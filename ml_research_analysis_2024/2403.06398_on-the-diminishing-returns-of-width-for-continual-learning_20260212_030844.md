---
ver: rpa2
title: On the Diminishing Returns of Width for Continual Learning
arxiv_id: '2403.06398'
source_url: https://arxiv.org/abs/2403.06398
tags:
- width
- learning
- continual
- forgetting
- diminishing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a framework for analyzing catastrophic forgetting in
  deep neural networks. We prove that the error incurred by a model trained on a sequence
  of datasets is directly proportional to the width of the model.
---

# On the Diminishing Returns of Width for Continual Learning

## Quick Facts
- arXiv ID: 2403.06398
- Source URL: https://arxiv.org/abs/2403.06398
- Authors: Etash Guha; Vihan Lakshman
- Reference count: 40
- Key outcome: Increasing model width reduces catastrophic forgetting with diminishing returns

## Executive Summary
This paper presents a theoretical framework for analyzing catastrophic forgetting in deep neural networks, proving that the error incurred during sequential task learning is directly proportional to model width. The authors demonstrate that while increasing width reduces forgetting, the benefit follows a power law relationship that leads to diminishing returns. Through both theoretical analysis and empirical validation on standard datasets, they show that the relationship between width and continual learning ability is more nuanced than previously thought, with important implications for model design in sequential learning scenarios.

## Method Summary
The authors develop a theoretical framework for analyzing catastrophic forgetting in feed-forward networks, deriving upper bounds on continual learning error as a function of width, depth, and sparsity. They validate their theory through experiments on four rotated datasets (MNIST, Fashion MNIST, SVHN, and GTSRB) with varying degrees of rotation (0° to 90° in 22.5° increments). Models are trained sequentially on each task for 5 epochs using SGD or Adam, with systematic variation of width (2^3 to 2^16), depth (1-3 layers), and row-wise sparsity. Performance is measured using Average Accuracy (AA), Average Forgetting (AF), Learning Accuracy (LA), and Joint Accuracy (JA).

## Key Results
- Increasing width reduces catastrophic forgetting, but benefits diminish at larger widths following a power law
- Deeper models exhibit higher continual learning error due to multiplicative accumulation of layer-wise errors
- Row-wise sparsity can mitigate forgetting but may reduce overall accuracy, with an optimal sparsity range existing
- The theoretical predictions about width-depth-sparsity relationships are empirically confirmed across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing model width reduces catastrophic forgetting, but the benefit diminishes at larger widths
- Mechanism: Implicit regularization during training causes wider models to remain closer to their initialization. This proximity acts as a functional regularizer, limiting the change in outputs for previously learned tasks when new tasks are added
- Core assumption: The distance from initialization decreases as width increases, following a power law of the form γW^−β
- Evidence anchors:
  - [abstract] "We prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns."
  - [section] "As the width of the neural network was increased, the distance from initialization was observed to decrease."
  - [corpus] "The impact of model size on catastrophic forgetting in Online Continual Learning" - This paper investigates how network size affects forgetting, supporting the idea that width is a key factor
- Break condition: If the distance from initialization does not decrease with width, or if the power law relationship does not hold, the mechanism breaks

### Mechanism 2
- Claim: The error incurred by a model trained on a sequence of datasets is directly proportional to the width of the model
- Mechanism: The error between models trained on different tasks scales with the spectral norm of the weight differences. As width increases, the spectral norm of the weight differences decreases, reducing the error
- Core assumption: The spectral norm of the weight differences is inversely proportional to a power of the width
- Evidence anchors:
  - [abstract] "We prove that the error incurred by a model trained on a sequence of datasets is directly proportional to the width of the model."
  - [section] "Using this assumption, we can demonstrate an upper bound in the matrix norm of the difference of the weights at a layer for two sequentially trained models."
  - [corpus] "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics" - This paper explores scaling laws in deep networks, which are relevant to understanding how model properties change with size
- Break condition: If the spectral norm of the weight differences does not decrease with width, or if the relationship is not a power law, the mechanism breaks

### Mechanism 3
- Claim: Increasing model depth increases continual learning error
- Mechanism: Deeper models accumulate more error through the layers due to the product of layer-wise errors. Each layer's error contributes multiplicatively to the final error
- Core assumption: The error accumulates multiplicatively through the layers
- Evidence anchors:
  - [abstract] "Our theoretical framework also predicts connections between model depth, number of tasks, and row-wise sparsity on continual learning error, which we empirically confirm."
  - [section] "Moreover, using simple triangle inequality, we expand this claim to general ϵt,t′ where t′ > t. Doing so yields our final theorem."
  - [corpus] "Continual Learning with Columnar Spiking Neural Networks" - This paper explores continual learning with a different architecture, which may have different depth-related properties
- Break condition: If the error does not accumulate multiplicatively, or if other factors dominate the error at higher depths, the mechanism breaks

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: This is the core problem the paper addresses. Understanding what catastrophic forgetting is and why it occurs is essential to understanding the significance of the results
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks trained on sequential tasks?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The paper mentions the NTK regime as a context for understanding the relationship between width and continual learning. Knowing what the NTK is and how it relates to infinite-width networks is helpful
  - Quick check question: What is the Neural Tangent Kernel, and how does it relate to the behavior of infinitely wide neural networks?

- Concept: Implicit Regularization
  - Why needed here: The paper relies on the idea that implicit regularization during training causes wider models to remain closer to their initialization. Understanding what implicit regularization is and how it affects training dynamics is crucial
  - Quick check question: What is implicit regularization, and how does it influence the training dynamics of neural networks?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (varying width) -> Output layer
- Critical path: Initialize model → Train on first task → Update for subsequent tasks → Measure error on previous tasks
- Design tradeoffs: Increasing width improves continual learning but with diminishing returns; increasing depth worsens continual learning; row-wise sparsity can mitigate forgetting but may reduce overall accuracy
- Failure signatures: If the distance from initialization does not decrease with width, the core mechanism breaks; if the error does not scale as predicted with width, depth, and sparsity, the theoretical framework is incomplete
- First 3 experiments:
  1. Verify the relationship between width and distance from initialization on a simple dataset (e.g., MNIST)
  2. Test the effect of increasing width on catastrophic forgetting on a rotated dataset (e.g., Rotated MNIST)
  3. Investigate the impact of depth on catastrophic forgetting by varying the number of hidden layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what width do the diminishing returns in continual learning error become most pronounced across different architectures and datasets?
- Basis in paper: Explicit - The paper demonstrates diminishing returns empirically at widths up to 2^16, but notes that previous studies stopped at 2048
- Why unresolved: The relationship between width and forgetting may depend on factors like depth, sparsity, and dataset characteristics, which weren't fully isolated in the experiments
- What evidence would resolve it: Systematic experiments varying width, depth, sparsity, and dataset complexity to identify the critical width threshold where diminishing returns become dominant

### Open Question 2
- Question: How does the diminishing returns phenomenon generalize to architectures beyond feedforward networks, such as ResNets or transformers?
- Basis in paper: Explicit - The paper's theoretical framework is limited to feedforward networks and only preliminary experiments were done with Wide ResNets
- Why unresolved: The lazy training phenomenon and implicit regularization that drive diminishing returns may behave differently in architectures with residual connections or attention mechanisms
- What evidence would resolve it: Extending the theoretical analysis to these architectures and conducting comprehensive empirical studies comparing forgetting across different model families

### Open Question 3
- Question: Can combining width scaling with other regularization methods (e.g., dropout, sparsity) overcome the diminishing returns in continual learning?
- Basis in paper: Explicit - The paper mentions this as a potential direction but doesn't explore it systematically
- Why unresolved: The interaction between architectural scaling and regularization techniques is complex and may yield non-linear effects on forgetting
- What evidence would resolve it: Controlled experiments varying both width and regularization parameters simultaneously to identify synergistic combinations that mitigate diminishing returns

## Limitations
- Theoretical analysis relies on assumptions about power law relationships that may not hold for all architectures or training regimes
- Empirical validation uses relatively shallow networks (depth ≤ 3) and focuses on rotation-based tasks
- The row-wise sparsity implementation details are somewhat underspecified in the experiments

## Confidence
- High confidence in the core finding that width reduces forgetting with diminishing returns, supported by both theory and empirical evidence
- Medium confidence in the depth-forgetting relationship, as the theoretical derivation assumes specific conditions that may not generalize
- Medium confidence in the sparsity recommendations, as the row-wise sparsity implementation details are somewhat underspecified

## Next Checks
1. Test the width-forgetting relationship on non-rotation continual learning tasks (e.g., permuted MNIST, task-incremental learning) to verify generalizability beyond angular transformations
2. Verify the power law relationship between width and distance from initialization across different activation functions (beyond ReLU) and optimization algorithms
3. Investigate whether the diminishing returns phenomenon persists when using modern continual learning techniques like elastic weight consolidation or experience replay as baselines for comparison