---
ver: rpa2
title: 'EMOVOME: A Dataset for Emotion Recognition in Spontaneous Real-Life Speech'
arxiv_id: '2403.02167'
source_url: https://arxiv.org/abs/2403.02167
tags:
- uni00000048
- speech
- emotion
- valence
- emovome
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces EMOVOME, the first publicly available dataset
  featuring spontaneous real-life emotions from natural voice conversations in Spanish.
  Collected from 100 speakers via WhatsApp, the dataset contains 999 audio messages
  labeled for valence and arousal by both expert and non-expert annotators.
---

# EMOVOME: A Dataset for Emotion Recognition in Spontaneous Real-Life Speech

## Quick Facts
- **arXiv ID:** 2403.02167
- **Source URL:** https://arxiv.org/abs/2403.02167
- **Reference count:** 40
- **Primary result:** First publicly available dataset of spontaneous real-life emotions from natural voice conversations in Spanish, featuring 999 audio messages from 100 speakers collected via WhatsApp

## Executive Summary
EMOVOME introduces the first publicly available dataset for emotion recognition in spontaneous real-life Spanish speech, collected from 100 speakers via WhatsApp voice messages. The dataset contains 999 audio messages labeled for valence and arousal by both expert and non-expert annotators. Using speaker-independent evaluation, the pre-trained UniSpeech-SAT-Large transformer model achieved the highest performance with 61.64% Unweighted Accuracy for 3-class valence prediction and 55.57% UA for arousal prediction. The dataset demonstrated lower performance compared to acted and elicited datasets like RA VDESS and IEMOCAP, highlighting the challenge of recognizing authentic emotions in real-life scenarios. Analysis showed that combining expert and non-expert annotations yielded better results and fairness in emotion recognition tasks.

## Method Summary
The EMOVOME dataset was collected from 100 Spanish speakers who submitted voice messages via WhatsApp, resulting in 999 audio recordings. Each message was annotated for valence and arousal by both expert and non-expert annotators. The study employed a speaker-independent approach for model evaluation, using baseline models and transformer-based architectures. The UniSpeech-SAT-Large pre-trained model demonstrated superior performance across valence, arousal, and emotion category predictions. Comparative analysis was conducted against the acted RA VDESS dataset and the elicited IEMOCAP dataset to assess performance differences between spontaneous and controlled emotion scenarios.

## Key Results
- UniSpeech-SAT-Large model achieved 61.64% UA for 3-class valence prediction and 55.57% UA for arousal prediction on EMOVOME
- For emotion categories, 42.58% UA was obtained on the EMOVOME dataset
- EMOVOME performed lower than RA VDESS and IEMOCAP datasets in emotion category prediction, but showed similar results for valence and arousal tasks
- Combining expert and non-expert annotations yielded better results and fairness compared to using either alone

## Why This Works (Mechanism)
The study's success stems from addressing the critical gap between controlled and spontaneous emotion expression by creating a real-life dataset. The combination of expert and non-expert annotations provides diverse perspectives that better capture the subjective nature of emotion recognition. The speaker-independent evaluation approach ensures models generalize across different speakers rather than memorizing individual patterns. Using transformer-based architectures, particularly the pre-trained UniSpeech-SAT-Large model, enables effective feature extraction from spontaneous speech patterns that differ from acted performances.

## Foundational Learning
- **Spontaneous vs. acted emotion recognition**: Understanding the fundamental differences in emotional expression between real-life conversations and controlled scenarios is crucial for developing effective emotion recognition systems. Quick check: Compare performance metrics across spontaneous and acted datasets.
- **Speaker-independent evaluation**: This approach prevents models from learning speaker-specific patterns and ensures generalizability. Quick check: Verify model performance remains consistent when tested on unseen speakers.
- **Multi-annotator labeling strategy**: Combining diverse annotator perspectives captures the subjective nature of emotion perception. Quick check: Analyze consistency between expert and non-expert annotations across emotion categories.
- **Transformer-based audio processing**: Pre-trained transformer models effectively capture complex acoustic patterns in speech. Quick check: Compare performance of transformer models versus traditional audio features.
- **Spanish language emotion modeling**: Language-specific acoustic and cultural factors influence emotion expression and recognition. Quick check: Evaluate model performance when transferred to other languages.

## Architecture Onboarding

**Component Map:** WhatsApp voice messages -> Audio preprocessing -> Annotation pipeline (expert + non-expert) -> Feature extraction (transformer-based) -> Classification models -> Performance evaluation

**Critical Path:** WhatsApp message collection → Audio preprocessing → Multi-annotator labeling → UniSpeech-SAT-Large feature extraction → Classification → Speaker-independent evaluation

**Design Tradeoffs:** Spontaneous real-life data provides authenticity but introduces noise and variability versus controlled acted data. Multi-annotator labeling increases robustness but introduces subjectivity. Pre-trained transformers offer strong feature extraction but require significant computational resources.

**Failure Signatures:** Poor generalization across speakers, inconsistent annotation patterns, performance degradation on spontaneous versus acted data, language-specific limitations.

**3 First Experiments:** 1) Train baseline model on EMOVOME and evaluate on IEMOCAP to assess cross-dataset generalization. 2) Compare performance using only expert annotations versus combined annotations. 3) Evaluate different transformer architectures (smaller vs larger) on the EMOVOME dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size of 999 audio messages may affect model generalizability compared to larger acted or elicited datasets
- Spanish language restriction creates potential language-specific biases in model performance
- Single-channel WhatsApp recordings may limit model transferability to different recording contexts
- Annotation subjectivity remains a challenge despite using both expert and non-expert annotators

## Confidence

**High Confidence Claims:**
- EMOVOME is the first publicly available spontaneous real-life emotion dataset in Spanish
- Dataset collection methodology via WhatsApp voice messages is accurately described
- Speaker-independent evaluation approach was correctly implemented
- Performance metrics for valence, arousal, and emotion categories are reliably reported
- Comparative analysis methodology with RA VDESS and IEMOCAP is sound

**Medium Confidence Claims:**
- Acted/elicited datasets consistently outperform real-life datasets in emotion recognition
- Combining expert and non-expert annotations yields optimal fairness
- Performance gap between controlled and real-life scenarios is significant and generalizable

## Next Checks
1. Conduct cross-validation study comparing EMOVOME-trained models on other spontaneous datasets versus acted datasets to verify the performance gap between real-life and controlled scenarios.

2. Perform language transferability test by training models on EMOVOME and evaluating them on spontaneous emotion datasets in other languages to assess language-specific biases.

3. Execute annotation consistency analysis comparing expert versus non-expert labeling patterns and their impact on model performance across different emotion categories.