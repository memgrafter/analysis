---
ver: rpa2
title: 'Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve
  Accuracy?'
arxiv_id: '2409.09221'
source_url: https://arxiv.org/abs/2409.09221
tags:
- speech
- noise
- audio
- modalities
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically examines how different input modalities
  (speech audio, visual context, lip movements, text from OCR) impact automatic speech
  recognition (ASR) accuracy across various noise conditions. The authors introduce
  a synthetic dataset (3-Equations) and use a real-world dataset (SlideAVSR) to investigate
  three research questions: (1) whether additional modalities always help ASR accuracy,
  (2) whether each modality provides uniform accuracy boosts across noise levels,
  and (3) how irrelevant visual information affects performance.'
---

# Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?

## Quick Facts
- arXiv ID: 2409.09221
- Source URL: https://arxiv.org/abs/2409.09221
- Authors: Yiwen Guan; Viet Anh Trinh; Vivek Voleti; Jacob Whitehill
- Reference count: 38
- Primary result: Multi-modal integration generally improves ASR accuracy, with image modality providing maximum benefit at moderate noise levels

## Executive Summary
This paper systematically examines how different input modalities (speech audio, visual context, lip movements, text from OCR) impact automatic speech recognition (ASR) accuracy across various noise conditions. The authors introduce a synthetic dataset (3-Equations) and use a real-world dataset (SlideAVSR) to investigate three research questions: (1) whether additional modalities always help ASR accuracy, (2) whether each modality provides uniform accuracy boosts across noise levels, and (3) how irrelevant visual information affects performance. Using a Discrete Multi-modal Language Model (DMLM) with a 125M parameter OPT backbone, they find that integrating more modalities generally increases accuracy, with the combination of audio, image context, and lip information showing particular benefit.

## Method Summary
The authors use a Discrete Multi-modal Language Model (DMLM) based on OPT-125M backbone to process multi-modal ASR inputs. They convert audio waveforms to discrete speech tokens using Seamless codec, images to image tokens using DALL-E encoder, and lip movements to lip tokens using AV-HuBERT. The model is fine-tuned on a mixture of multi-modal tasks before task-specific fine-tuning on the 3-Equations synthetic dataset and SlideAVSR real-world dataset. Evaluation uses Word Error Rate (WER) across different Signal-to-Noise Ratio (SNR) levels, with relative WER benefit calculated to assess improvements from adding extra modalities.

## Key Results
- Integrating more modalities generally increases ASR accuracy, with the combination of audio, image context, and lip information showing particular benefit
- Image modality provides greatest benefit at moderate noise levels, exhibiting a different trend from synchronized modalities like lip movements
- Performance improves when irrelevant visual information is filtered during preprocessing, especially at higher noise levels
- This work demonstrates for the first time the benefit of combining audio, image context, and lip information in a single ASR model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image modality provides greatest benefit at moderate noise levels, while lip modality benefits increase monotonically with noise.
- Mechanism: Image information helps disambiguate speech when noise is low-to-moderate by providing visual context (e.g., what equations were displayed), but at high noise levels the model cannot reliably align audio to image content. Lip movements, being synchronized with speech, provide complementary information that scales with noise level.
- Core assumption: The model can learn to attend to relevant cross-modal correspondences between speech and visual content, and that visual information quality is sufficient for this alignment.
- Evidence anchors:
  - [abstract]: "Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels; moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements"
  - [section]: "the benefit of lips in enhancing accuracy becomes more amplified, showing the same trend as discussed in previous works [15]. Conversely, the benefit of images follows a trend of first increasing and then decreasing, peaking in the middle when SNR=0dB"
  - [corpus]: Weak - no direct citations about modality synchronization trends in corpus
- Break condition: If visual information quality degrades below a threshold where cross-modal alignment becomes unreliable, or if the model cannot learn effective attention mechanisms for unsynchronized modalities.

### Mechanism 2
- Claim: Filtering irrelevant visual information improves ASR performance, particularly at higher noise levels.
- Mechanism: When visual inputs contain irrelevant information (e.g., 1/3 irrelevant equations in 3-Equations dataset), the model must filter this out to find correct correspondences. Preprocessing that removes irrelevant information reduces the search space and cognitive load, improving accuracy.
- Core assumption: The model benefits from reduced visual input complexity when searching for relevant information, especially when audio quality is poor.
- Evidence anchors:
  - [abstract]: "Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step"
  - [section]: "adding more irrelevant visual information will hinder the model from finding the correct information, especially in noisy environments"
  - [corpus]: Weak - no direct citations about preprocessing relevance filtering in corpus
- Break condition: If filtering removes relevant information, or if the preprocessing step introduces bias or errors in identifying relevance.

### Mechanism 3
- Claim: Combining multiple modalities (audio, image, lips) in a single model outperforms any single modality or two-modality combinations.
- Mechanism: Different modalities provide complementary information - audio provides primary speech content, images provide context about what was discussed/displayed, and lips provide synchronized visual speech cues. A unified model can learn optimal fusion strategies.
- Core assumption: The model architecture can effectively learn to combine information from multiple modalities without being overwhelmed by increased input complexity.
- Evidence anchors:
  - [abstract]: "Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information in one speech recognition model"
  - [section]: "when considering 3-modality combinations, we observe more consistent benefits: adding both image and lip (I+L+A →T), the model surpasses the audio-only model by +1.3%"
  - [corpus]: Weak - no direct citations about multi-modal fusion benefits in corpus
- Break condition: If the model cannot learn effective cross-modal attention, or if increased input length prevents effective processing of relevant information.

## Foundational Learning

- Concept: Cross-modal attention mechanisms
  - Why needed here: The model must learn to attend to relevant information across audio, image, and lip modalities to fuse complementary information effectively
  - Quick check question: How does a multi-head attention mechanism help a model determine which visual tokens are most relevant to a given audio token?

- Concept: Discrete tokenization of continuous modalities
  - Why needed here: The DMLM model requires discrete tokens for processing, so continuous audio waveforms and images must be converted to discrete representations using modality-specific encoders
  - Quick check question: What is the role of the Seamless codec in converting audio waveforms to discrete speech tokens for the DMLM architecture?

- Concept: Noise-robust feature extraction
  - Why needed here: The model must extract meaningful features from noisy audio signals and maintain recognition accuracy across varying SNR levels
  - Quick check question: How does the AV-HuBERT model contribute to extracting robust lip movement features that remain useful even when audio quality degrades?

## Architecture Onboarding

- Component map:
  - DMLM backbone (125M parameter OPT) -> Modality-specific encoders -> Task prompt concatenation -> Transformer decoder layers
  - Seamless codec (audio→speech tokens) -> DALL-E encoder (images→image tokens) -> AV-HuBERT (lips→lip tokens + lip-reading text)

- Critical path:
  1. Raw inputs (audio waveform, image, lip video) → modality-specific encoders
  2. Encoded tokens → concatenated with task prompt
  3. DMLM processes token sequence through cross-modal attention layers
  4. Output: predicted text tokens representing transcription

- Design tradeoffs:
  - Input length vs. complexity: More modalities increase input length, potentially making it harder to find relevant information
  - Synchronization vs. flexibility: Synchronized modalities (audio+lips) are easier to align but less flexible than asynchronous ones (audio+images)
  - Explicit vs. implicit visual representation: OCR provides explicit text but may miss visual context that image encoders capture

- Failure signatures:
  - Performance degradation when irrelevant visual information is not filtered
  - Inconsistent benefits across noise levels (images help at moderate noise, lips help at high noise)
  - Over-reliance on single modalities (as observed with Gemini model)

- First 3 experiments:
  1. Ablation study: Test audio-only vs. audio+image vs. audio+lip vs. audio+image+lip to verify multi-modal benefits
  2. Noise level sweep: Evaluate performance across SNR levels to confirm image benefits peak at moderate noise
  3. Preprocessing impact: Compare performance with and without irrelevant visual information filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the benefit of adding image modality vary depending on the type of mathematical symbols or equations present in the visual content?
- Basis in paper: [explicit] The paper notes that mathematical symbols may be difficult for standard image codecs to extract, and they explore different representations of visual modality (implicit DALL-E tokens, OCR text, oracle OCR). However, they don't systematically vary the complexity or type of mathematical content.
- Why unresolved: The study uses a fixed type of mathematical equations with consistent complexity. The effect of varying symbol types, equation complexity, or notation systems on the image modality benefit remains unexplored.
- What evidence would resolve it: Experiments varying the types of mathematical symbols (algebraic vs. geometric, simple vs. complex equations) and measuring how the benefit of image modality changes across these variations would provide clarity.

### Open Question 2
- Question: How does the temporal relationship between synchronized and non-synchronized modalities affect model performance in real-world scenarios beyond the controlled 2-noise dataset?
- Basis in paper: [explicit] The paper observes that lip movements (synchronized with speech) show different trends compared to image context (non-synchronized), with lips showing increasing benefit as noise increases while images peak at moderate noise levels.
- Why unresolved: The paper uses a synthetic dataset with controlled timing and a specific noise injection pattern. Real-world scenarios may have different temporal relationships, delays, or synchronization patterns that could affect how models leverage synchronized vs. non-synchronized modalities.
- What evidence would resolve it: Testing the model on real-world videos with varying degrees of asynchrony between speech and visual context, or videos where the temporal relationship changes dynamically, would reveal how temporal factors influence modality benefits.

### Open Question 3
- Question: What is the optimal strategy for filtering irrelevant visual information, and how does this strategy vary with noise levels or task complexity?
- Basis in paper: [explicit] The paper shows that filtering irrelevant OCR information improves performance, with stricter filtering (smaller K values) generally providing better results, but doesn't systematically explore different filtering strategies or their relationship to noise conditions.
- Why unresolved: The study uses a simple frequency-based filtering approach with limited parameter variation. Different filtering strategies (semantic relevance, attention-based filtering, etc.) might be more effective, and the optimal strategy might depend on noise levels or task complexity.
- What evidence would resolve it: Comparing different filtering algorithms (semantic matching, attention-weighted relevance, machine learning-based filtering) across various noise conditions and task complexities would identify optimal strategies for different scenarios.

### Open Question 4
- Question: How do the findings about modality benefits generalize to other language families or non-English speech recognition tasks?
- Basis in paper: [inferred] The experiments are conducted primarily on English datasets (LibriSpeech, SlideAVSR with technical English content). The paper's findings about modality benefits, optimal noise levels, and filtering strategies may not transfer to languages with different phonological or visual characteristics.
- Why unresolved: The study focuses exclusively on English language data. Languages with different writing systems, pronunciation patterns, or cultural visual contexts might interact differently with multi-modal ASR approaches.
- What evidence would resolve it: Replicating the experiments on datasets from different language families (Mandarin, Arabic, Japanese) with varying orthographic and phonological properties would reveal the cross-linguistic generalizability of the findings.

## Limitations

- The study relies heavily on synthetic data (3-Equations dataset) which may not fully capture real-world complexity
- The analysis focuses on specific modality combinations without exploring all possible permutations or alternative modality encodings
- The DMLM architecture is based on a relatively small 125M parameter model, limiting generalizability to larger-scale applications

## Confidence

**High Confidence**: The core finding that multi-modal integration generally improves ASR accuracy is well-supported by experiments on both synthetic and real-world datasets. The systematic analysis across noise levels provides robust evidence for modality-specific benefits.

**Medium Confidence**: The claim about image modality providing maximum benefit at moderate noise levels is supported by empirical results but lacks theoretical grounding. The trend observation is based on a limited number of noise levels (3 SNR values) and could benefit from more granular analysis.

**Low Confidence**: The assertion that this work is "the first to show the benefit of combining audio, image context, and lip information in one speech recognition model" cannot be independently verified due to incomplete citation context in the corpus signals.

## Next Checks

1. **Generalization Across Datasets**: Replicate the main findings on additional real-world datasets beyond SlideAVSR to verify that the observed modality benefits generalize across different domains and recording conditions.

2. **Scalability Analysis**: Evaluate whether the relative benefits of multi-modal integration scale with model size by testing the DMLM architecture with larger OPT backbones (e.g., 1.3B, 6.7B parameters) to determine if current limitations are architecture-specific.

3. **Modality Dependency Mapping**: Conduct a systematic ablation study across all possible modality combinations (8 permutations for 3 modalities) at each noise level to precisely map which modalities are essential versus redundant under different conditions.