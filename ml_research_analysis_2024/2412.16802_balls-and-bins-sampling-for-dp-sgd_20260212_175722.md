---
ver: rpa2
title: Balls-and-Bins Sampling for DP-SGD
arxiv_id: '2412.16802'
source_url: https://arxiv.org/abs/2412.16802
tags:
- sampling
- privacy
- batch
- order
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of privacy accounting for DP-SGD
  with shuffling, which can be significantly worse than with Poisson subsampling.
  The authors introduce Balls-and-Bins sampling, which achieves similar model utility
  to shuffling while enjoying better privacy guarantees similar to Poisson subsampling.
---

# Balls-and-Bins Sampling for DP-SGD
## Quick Facts
- **arXiv ID**: 2412.16802
- **Source URL**: https://arxiv.org/abs/2412.16802
- **Reference count**: 21
- **Key outcome**: Balls-and-Bins sampling achieves privacy amplification similar to Poisson subsampling while maintaining implementation simplicity similar to shuffling.

## Executive Summary
This paper addresses the problem of privacy accounting for DP-SGD with shuffling, which can be significantly worse than with Poisson subsampling. The authors introduce Balls-and-Bins sampling, which achieves similar model utility to shuffling while enjoying better privacy guarantees similar to Poisson subsampling. They develop importance sampling and order statistics sampling techniques for efficient Monte Carlo estimation of privacy parameters. The Balls-and-Bins sampler places each example in a random batch, making the marginal distribution over each batch the same as Poisson subsampling while maintaining implementation simplicity. Experimental results show that DP-SGDB achieves comparable utility to DP-SGDS at the same noise scale, while having better privacy guarantees than both DP-SGDS and DP-SGDP in practical regimes.

## Method Summary
The method introduces Balls-and-Bins sampling where each example is placed independently into a random batch, creating marginal distributions identical to Poisson subsampling while maintaining the simplicity of shuffling. Privacy accounting is performed using Monte Carlo estimation with importance sampling and order statistics sampling to compute the hockey stick divergence between neighboring distributions. The approach uses a tightly dominating pair (PB, QB) to characterize the privacy loss distribution ABLQB, enabling efficient numerical computation of privacy parameters. The implementation wraps this sampler around standard DP-SGD with fixed noise scale and batch size, using weighted loss to handle batch padding/truncation.

## Key Results
- Balls-and-Bins sampling achieves comparable model utility to shuffling (DP-SGDS) at the same noise scale
- Privacy guarantees for Balls-and-Bins are better than shuffling and comparable to Poisson subsampling (DP-SGDP) in practical regimes
- Monte Carlo estimation with importance sampling and order statistics sampling provides efficient privacy parameter computation
- The marginal distribution over each batch in Balls-and-Bins is exactly identical to Poisson subsampling

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Balls-and-Bins sampling achieves privacy amplification similar to Poisson subsampling while maintaining implementation simplicity similar to shuffling.
- Mechanism: Each example is placed independently into a random batch, making the marginal distribution over each batch identical to Poisson subsampling. This independence prevents non-differing examples from leaking information about the differing example's presence or absence in any given batch.
- Core assumption: The placement of each example into batches is independent and uniformly random across all batches.
- Evidence anchors:
  - [abstract] "The Balls-and-Bins sampler places each example in a random batch, making the marginal distribution over each batch the same as Poisson subsampling while maintaining implementation simplicity."
  - [section] "This sampler exhibits behavior similar to Shuffle, with each example appearing in precisely one batch, and is also similar to implementation... The marginal distribution over each batch is exactly the same as that of Poisson subsampling."
- Break condition: If example placement becomes correlated (e.g., through implementation shortcuts), the privacy amplification benefit is lost.

### Mechanism 2
- Claim: The hockey stick divergence for Balls-and-Bins sampling can be tightly characterized using a dominating pair, enabling precise privacy accounting.
- Mechanism: The pair (PB, QB) tightly dominates ABLQB, where PB is a mixture of Gaussians centered at standard basis vectors and QB is a single Gaussian at the origin. This characterization allows direct computation of privacy parameters.
- Core assumption: A tightly dominating pair exists for ABLQB that enables complete privacy characterization.
- Evidence anchors:
  - [section] "We identify a tightly dominating pair (Definition 2.2) for ABLQB, thereby allowing a tight privacy analysis."
  - [section] "Theorem 3.1. For all σ > 0 and T ≥ 1, it holds that (PB, QB) ≡ ABLQB"
- Break condition: If the dominating pair doesn't actually tightly dominate ABLQB in practice, the privacy accounting becomes conservative or inaccurate.

### Mechanism 3
- Claim: Monte Carlo estimation with importance sampling and order statistics sampling provides efficient privacy parameter estimation for large-scale problems.
- Mechanism: Importance sampling focuses computation on regions where the privacy loss is significant, while order statistics sampling reduces computational complexity when dealing with high-dimensional Gaussians by sampling from the joint distribution of order statistics.
- Core assumption: The privacy loss function has bounded support in certain regions that can be efficiently sampled.
- Evidence anchors:
  - [section] "Our key contributions here are to develop the techniques of importance sampling, to handle small δ values, and order statistics sampling, a new technique to handle a large number of steps."
  - [section] "For any distribution P with efficiently computable inverse of the cumulative density function CDF−1P (·), Algorithm 6 efficiently samples order statistics of P"
- Break condition: If the privacy loss distribution is heavy-tailed or has complex structure that these sampling methods cannot capture, the estimates become unreliable.

## Foundational Learning
- Concept: Differential Privacy and Privacy Accounting
  - Why needed here: The entire paper builds on understanding how different sampling mechanisms affect privacy guarantees and how to compute these guarantees efficiently.
  - Quick check question: What is the difference between (ε, δ)-DP and Rényi DP, and when would you use each for privacy accounting?

- Concept: Hockey Stick Divergence and Dominating Pairs
  - Why needed here: The paper uses hockey stick divergence to characterize privacy loss and relies on tightly dominating pairs to enable precise numerical computation of privacy parameters.
  - Quick check question: How does the hockey stick divergence relate to the definition of (ε, δ)-differential privacy?

- Concept: Monte Carlo Methods and Importance Sampling
  - Why needed here: The paper develops novel Monte Carlo techniques to estimate small privacy parameters efficiently, which is crucial for practical deployment.
  - Quick check question: Why does importance sampling reduce the sample complexity when estimating small probabilities, and what determines the factor of improvement?

## Architecture Onboarding
- Component map: Balls-and-Bins Sampler -> Privacy Accounting Module -> DP-SGD Implementation -> Evaluation Framework
- Critical path: 1) Generate batches using Balls-and-Bins sampler 2) Compute noisy gradients for each batch 3) Update model parameters 4) Periodically estimate privacy parameters using Monte Carlo methods 5) Output final model with privacy guarantees
- Design tradeoffs: Fixed vs variable batch sizes (fixed sizes improve efficiency but require padding/truncation); Single vs multiple epochs (single epoch is simpler but multiple epochs may be needed for convergence); Exact vs approximate privacy accounting (exact accounting is ideal but may be computationally prohibitive)
- Failure signatures: Poor privacy amplification (indicates correlation in batch sampling); High variance in Monte Carlo estimates (suggests need for more samples or better importance sampling); Model utility degradation (could indicate noise scale is too high relative to batch size)
- First 3 experiments: 1) Compare utility of DP-SGD with Balls-and-Bins vs Shuffling vs Poisson at same noise scale on a small dataset 2) Verify that Balls-and-Bins batch marginals match Poisson subsampling theoretically and empirically 3) Benchmark Monte Carlo estimation runtime vs accuracy for different numbers of order statistics

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can we obtain tight provable privacy accounting for ABLQB that matches or improves upon ABLQP in all relevant regimes?
- Basis in paper: Explicit - The authors state this as the main open problem in the discussion section.
- Why unresolved: Current methods provide high-probability bounds or approximations, not tight characterizations.
- What evidence would resolve it: A mathematical proof showing the exact privacy loss curve of ABLQB matches or dominates ABLQP across all parameter regimes.

### Open Question 2
- Question: Is there always an ε ≥ 0 such that δB(ε) > δP(ε) for all σ > 0 and T > 1?
- Basis in paper: Explicit - The authors note in Appendix D that while Theorem 3.4 shows δB(ε) < δP(ε) for large ε, it doesn't imply the reverse inequality exists.
- Why unresolved: The paper establishes that PB and PP are incomparable in terms of domination, but doesn't resolve the relationship between their corresponding δ values.
- What evidence would resolve it: A counterexample showing no such ε exists for some parameter choices, or a proof that such an ε must exist for all parameters.

### Open Question 3
- Question: Can we develop importance samplers for the multi-epoch case of ABLQB that improve upon Monte Carlo estimation?
- Basis in paper: Explicit - The authors mention in Appendix B.4 that importance sampling is not directly applicable for multiple epochs.
- Why unresolved: The single-epoch case uses importance sampling effectively, but extending this technique to multiple epochs presents technical challenges.
- What evidence would resolve it: A method for constructing effective importance samplers for the multi-epoch case, or a proof that such samplers cannot provide significant improvement.

## Limitations
- The privacy accounting relies on mathematical proofs of dominating pairs rather than empirical verification on real data
- Monte Carlo estimation methods may have convergence issues for very small δ values or very large numbers of steps
- The paper assumes single-epoch training, which may not be sufficient for all learning tasks
- Extension to multiple epochs introduces complexity not fully explored in the paper

## Confidence
- **High Confidence**: The Balls-and-Bins sampling mechanism and its implementation simplicity are well-established. The theoretical framework for hockey stick divergence and dominating pairs is rigorous.
- **Medium Confidence**: The privacy accounting results depend on the correctness of the Monte Carlo implementation and the tightness of the dominating pair characterization, which while proven, may have practical limitations.
- **Medium Confidence**: The experimental results showing comparable utility to shuffling are based on specific datasets and model architectures, limiting generalizability.

## Next Checks
1. **Dominating Pair Verification**: Empirically validate that (PB, QB) tightly dominates ABLQB on actual trained models by computing the hockey stick divergence directly for small ε values and comparing with the theoretical bound.
2. **Multi-Epoch Extension**: Test Balls-and-Bins sampling with multiple epochs of training to assess whether the privacy amplification benefits persist and how the privacy accounting needs to be adjusted.
3. **Generalization Across Tasks**: Evaluate the utility-privacy tradeoff of Balls-and-Bins sampling on diverse machine learning tasks beyond the pCTR datasets used, including computer vision and natural language processing benchmarks.