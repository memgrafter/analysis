---
ver: rpa2
title: 'Text2Cypher: Bridging Natural Language and Graph Databases'
arxiv_id: '2412.10064'
source_url: https://arxiv.org/abs/2412.10064
tags:
- cypher
- language
- datasets
- text2cypher
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating natural language
  queries into Cypher query language for graph databases, which traditionally requires
  specialized technical knowledge. The authors created a comprehensive Text2Cypher
  dataset by combining and cleaning 16 publicly available sources into 44,387 instances,
  then fine-tuned various large language models on this dataset.
---

# Text2Cypher: Bridging Natural Language and Graph Databases

## Quick Facts
- arXiv ID: 2412.10064
- Source URL: https://arxiv.org/abs/2412.10064
- Authors: Makbule Gulcin Ozsoy; Leila Messallem; Jon Besga; Gianandrea Minneci
- Reference count: 7
- Primary result: Fine-tuned models achieved up to 0.34 improvement in Google-BLEU score and 0.11 improvement in Exact Match score over baselines

## Executive Summary
This paper addresses the challenge of translating natural language queries into Cypher query language for graph databases, which traditionally requires specialized technical knowledge. The authors created a comprehensive Text2Cypher dataset by combining and cleaning 16 publicly available sources into 44,387 instances, then fine-tuned various large language models on this dataset. The results showed that all fine-tuned models outperformed their baseline versions, with the best models achieving up to 0.34 improvement in Google-BLEU score and 0.11 improvement in Exact Match score. The study demonstrates that high-quality datasets and fine-tuning are crucial for improving Text2Cypher performance, making graph databases more accessible to non-technical users.

## Method Summary
The authors systematically combined 16 publicly available datasets into a unified Text2Cypher corpus of 44,387 instances through deduplication, manual cleaning, and quality control. They evaluated six baseline language models using both translation-based (BLEU, ROUGE, Exact Match) and execution-based metrics, then fine-tuned selected models on the curated dataset. The evaluation included both text comparison and actual query execution against databases to ensure semantic correctness beyond syntactic validity.

## Key Results
- All fine-tuned models outperformed their baseline versions, achieving up to 0.34 increase in Google-BLEU score
- Exact Match score improved by up to 0.11 for fine-tuned models compared to baselines
- Execution-based evaluation confirmed that translation-only metrics can miss semantic errors in generated queries
- The best-performing fine-tuned model achieved a Google-BLEU score of 0.53

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset combination and cleaning improves model generalization by exposing models to diverse query patterns from multiple domains.
- Mechanism: By combining 16 different publicly available datasets into a unified corpus of 44,387 instances, the model encounters a wider variety of question types, schemas, and Cypher patterns than any single dataset would provide. The manual cleaning and de-duplication steps remove noise and inconsistencies that could confuse the model during training.
- Core assumption: Diverse training data from multiple sources provides better coverage of real-world query patterns than homogeneous datasets.
- Evidence anchors:
  - [abstract] "In this work, we show how we combined, cleaned and organized several publicly available datasets into a total of 44,387 instances"
  - [section 3.2] "Manual checks and updates: This step aims to produce more reliable and error-free output data"
  - [corpus] Weak - the corpus neighbors section shows related work but doesn't directly support the diversity claim
- Break condition: If the combined datasets share overlapping question patterns or the cleaning process removes too much domain-specific information, the diversity benefit diminishes.

### Mechanism 2
- Claim: Fine-tuning on domain-specific datasets outperforms zero/few-shot prompting for complex Cypher queries.
- Mechanism: The paper demonstrates that foundational LLMs struggle with complex, multi-hop queries when prompted directly, but fine-tuning on the Text2Cypher dataset allows the model to learn specific Cypher syntax patterns, schema relationships, and query construction strategies. This specialized training reduces the "hallucinations" and incomplete outputs common in zero-shot approaches.
- Core assumption: Domain-specific fine-tuning is more effective than general prompting for specialized query languages.
- Evidence anchors:
  - [abstract] "Fine-tuning LLMs on domain-specific datasets has proven to be a more promising approach, but the limited availability of high-quality, publicly available Text2Cypher datasets makes this challenging"
  - [section 5.2] "The results revealed that fine-tuned models consistently outperformed their baseline versions, achieving up to a 0.34 increase in Google-BLEU score"
  - [corpus] Weak - corpus neighbors discuss related work but don't directly compare fine-tuning vs prompting
- Break condition: If the fine-tuning dataset is too small or not representative of real-world queries, the performance gains may not materialize.

### Mechanism 3
- Claim: Execution-based evaluation provides more realistic performance assessment than translation-only metrics.
- Mechanism: The paper implements both translation-based evaluation (comparing generated vs reference Cypher text) and execution-based evaluation (running queries against databases and comparing results). This dual approach catches cases where syntactically correct Cypher produces semantically wrong results, which pure text comparison would miss.
- Core assumption: The ultimate measure of Text2Cypher performance is whether the generated queries return correct data, not just syntactically valid code.
- Evidence anchors:
  - [section 4.2] "We defined two types of evaluation procedures: Translation-based evaluation... Execution-based evaluation"
  - [section 5] "The evaluation procedures and metrics were identical to those used in benchmarking section"
  - [corpus] Weak - corpus neighbors don't discuss evaluation methodology
- Break condition: If database access is unavailable for test instances or if query results are too complex to compare reliably, execution-based evaluation becomes impractical.

## Foundational Learning

- Concept: Cypher query language fundamentals
  - Why needed here: Understanding Cypher syntax, pattern matching, and graph traversal concepts is essential for interpreting the task and evaluating model outputs
  - Quick check question: What is the difference between MATCH and CREATE clauses in Cypher?

- Concept: Graph database schema understanding
  - Why needed here: The model must understand how nodes, relationships, and properties are structured in different schemas to generate appropriate queries
  - Quick check question: How would you represent a "Person knows Person" relationship in a graph schema?

- Concept: Text-to-text generation evaluation metrics
  - Why needed here: BLEU, ROUGE, and Exact Match scores are used to quantify model performance, requiring understanding of their strengths and limitations
  - Quick check question: What's the key difference between BLEU score and Exact Match score?

## Architecture Onboarding

- Component map: 16 datasets → cleaning → splitting → 6 baseline models → fine-tuning → dual evaluation (translation + execution) → model deployment
- Critical path: Dataset preparation → Fine-tuning → Evaluation → Model release
- Design tradeoffs: Dataset size vs. quality (cleaning removes noise but may reduce quantity), open vs. closed models (cost vs. performance), translation-only vs. execution-based evaluation (speed vs. accuracy)
- Failure signatures: Poor BLEU scores indicate generation quality issues, low Exact Match in execution suggests semantic errors, overfitting shown by high training but low test performance
- First 3 experiments:
  1. Benchmark all 6 baseline models on the test set using both evaluation methods to establish performance baselines
  2. Fine-tune the top 3 performing baselines on the training set with different learning rates to find optimal parameters
  3. Compare fine-tuned models against baselines on a held-out validation set to verify generalization before final testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results change when evaluating on a dataset cleaned of duplicate questions with different Cypher outputs?
- Basis in paper: [explicit] The paper mentions this as a potential future direction, noting that current evaluation may unfairly penalize models for memorizing questions that have different correct answers.
- Why unresolved: The current dataset cleaning process only deduplicates by exact ["question", "cypher"] pairs, leaving potential duplicates with different Cypher outputs that could affect evaluation fairness.
- What evidence would resolve it: Re-running the benchmarking and fine-tuning experiments after removing instances where the same question appears with different Cypher outputs, then comparing performance changes.

### Open Question 2
- Question: What is the impact of model size on Text2Cypher performance, particularly for the fine-tuned models?
- Basis in paper: [inferred] The paper shows that larger models generally perform better in benchmarking but doesn't systematically analyze the relationship between model size and performance gains from fine-tuning.
- Why unresolved: The paper only fine-tunes a limited selection of models without exploring how performance scales with model size or whether smaller models can achieve comparable results with more training data.
- What evidence would resolve it: Conducting experiments that fine-tune models across a wider range of sizes (e.g., 3B, 7B, 13B, 34B parameters) using the same dataset and comparing both absolute performance and relative improvements.

### Open Question 3
- Question: How robust are the fine-tuned Text2Cypher models to distribution shifts in real-world queries?
- Basis in paper: [explicit] The paper acknowledges that both training and test sets are drawn from the same data distribution, raising concerns about performance in different contexts.
- Why unresolved: The evaluation only tests models on data from the same distribution as training, without assessing how well they generalize to queries from different domains, writing styles, or complexity levels.
- What evidence would resolve it: Testing the fine-tuned models on external datasets from different domains or sources, or creating synthetic variations of test queries to evaluate robustness to paraphrasing and complexity changes.

## Limitations
- The evaluation focuses on English-language queries only, with no assessment of cross-lingual or multilingual performance
- The dataset may have geographic or domain biases that aren't explicitly characterized
- Execution-based evaluation was limited to a subset of test cases due to database access constraints

## Confidence
- **High Confidence**: Performance improvements from fine-tuning (measured by BLEU and Exact Match scores) - directly supported by quantitative results
- **Medium Confidence**: Dataset quality improvements through cleaning - supported by methodology but not independently verified through ablation studies
- **Low Confidence**: Generalizability to real-world deployment - limited by evaluation scope and lack of production deployment data

## Next Checks
1. **Cross-Database Validation**: Test the fine-tuned models across different graph database implementations (Neo4j, Amazon Neptune, etc.) to verify schema-agnostic performance rather than schema-specific memorization.

2. **Long-Tail Query Analysis**: Analyze model performance on complex, multi-hop queries and rare query patterns that appear less than 5 times in the training data to assess true generalization capability.

3. **A/B Testing in Production**: Deploy the best-performing model in a real user environment and compare query success rates against traditional SQL-to-Cypher tools or expert-written queries over a 30-day period.