---
ver: rpa2
title: 'MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering'
arxiv_id: '2403.19116'
source_url: https://arxiv.org/abs/2403.19116
tags:
- table
- tables
- question
- answer
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MFORT-QA, a novel approach for multi-hop
  few-shot open rich table question answering. The method addresses the challenge
  of answering complex questions involving tables with nested hyperlinks by combining
  few-shot learning, chain-of-thought prompting, and retrieval-augmented generation.
---

# MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering

## Quick Facts
- arXiv ID: 2403.19116
- Source URL: https://arxiv.org/abs/2403.19116
- Reference count: 32
- Improves table QA F1 and EM by 12-30% on OTT-QA dataset

## Executive Summary
MFORT-QA introduces a novel approach for multi-hop few-shot open rich table question answering that combines few-shot learning, chain-of-thought prompting, and retrieval-augmented generation. The method addresses the challenge of answering complex questions involving tables with nested hyperlinks by first retrieving relevant tables and contexts, using few-shot examples to prompt an LLM, and then employing CoT to decompose complex questions while using RAG to retrieve additional contexts. Experimental results demonstrate significant improvements over traditional extractive table QA methods and LLM zero-shot learning.

## Method Summary
The MFORT-QA approach works in three stages: (1) Information retrieval using Sentence-BERT embeddings and cosine similarity to identify relevant tables and hyperlink contexts, (2) Few-shot learning where retrieved content is used to construct prompts with similar question-table-answer triples from training data, and (3) Multi-hop reasoning using Chain-of-Thought prompting to decompose complex questions into simpler sub-questions, with Retrieval-Augmented Generation providing additional context for each sub-question. The system processes questions by first attempting direct answer generation, then applying CoT decomposition if needed, and finally synthesizing answers from sub-question responses.

## Key Results
- Achieves 12-30% improvement in F1 and Exact Match metrics compared to traditional extractive table QA methods
- Significantly outperforms LLM zero-shot learning on the OTT-QA dataset
- Ablation studies show each component (FSL, CoT, RAG) contributes to overall performance improvement
- High hit rates (top-3) for table retrieval indicate effective information retrieval component

## Why This Works (Mechanism)

### Mechanism 1
- Few-shot learning with LLM prompts reduces complexity by providing concrete examples of expected answer format
- Retrieves relevant tables and similar question-table-answer triples from training set to construct prompts
- Core assumption: LLM can generalize from few-shot examples to answer new questions
- Evidence: [abstract] uses retrieved content to construct few-shot prompts; [section] retrieved pairs serve as few-shot prompt enabling LLM understanding
- Break condition: Retrieved examples insufficiently similar or LLM cannot generalize from few examples

### Mechanism 2
- Chain-of-thought prompting decomposes complex multi-hop questions into simpler sub-questions
- Breaks down complex questions into sequential simpler sub-questions with reasoning thoughts
- Core assumption: Complex questions can be effectively decomposed into answerable sub-questions
- Evidence: [abstract] leverages CoT to decompose complex question into sequential chain; [section] applies CoT to decompose original questions into simpler sub-questions
- Break condition: Decomposition creates still-complex sub-questions or incomplete reasoning chain

### Mechanism 3
- Retrieval-augmented generation provides additional context for sub-questions
- Uses RAG to retrieve relevant tables and hyperlink passages for each sub-question
- Core assumption: Retrieved context is relevant and helpful for answering original question
- Evidence: [abstract] RAG retrieves relevant tables and hyperlink contexts; [section] RAG algorithm gathers additional relevant table information
- Break condition: Retrieved context is irrelevant, noisy, or overwhelming

## Foundational Learning

- **Table embeddings and cosine similarity for retrieval**: Needed to identify which tables contain answers and which training examples are relevant for few-shot prompting. Quick check: How does Sentence-BERT convert tables into embeddings for comparison with question embeddings?

- **Prompt engineering for LLMs**: Critical for effective few-shot learning approach. Quick check: What information should be included in few-shot prompt template to maximize correct answer generation?

- **Question decomposition strategies**: Essential for handling complex questions. Quick check: What criteria determine if a question requires decomposition via CoT?

## Architecture Onboarding

- **Component map**: Information Retrieval → Few-shot Prompt Construction → LLM Answer Generation → (If needed) CoT Decomposition → Sub-question Retrieval → Augmented Prompt → Final Answer Generation

- **Critical path**: Question → Table Retrieval (Sentence-BERT + cosine similarity) → Few-shot Prompt Construction → LLM Answer Generation → (If needed) CoT Decomposition → Sub-question Retrieval (RAG) → Augmented Prompt → Final Answer Generation

- **Design tradeoffs**: Trades computational cost (multiple LLM calls, retrieval operations) for improved accuracy on complex questions. Choice of k for top-k retrieval affects both accuracy and efficiency.

- **Failure signatures**: No answer generated, irrelevant answers, overly verbose answers, or answers that don't reference table content properly. Suggests issues with retrieval quality, prompt construction, or LLM understanding.

- **First 3 experiments**:
  1. Test retrieval accuracy by measuring hit rates for top-1 and top-3 tables on filtered dataset where answers are known to be in specific tables
  2. Compare FSL prompting against zero-shot prompting on simple questions to establish baseline improvement from few-shot examples
  3. Test CoT decomposition on complex questions to verify sub-questions are actually simpler and can be answered with retrieved context

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions remain:

### Open Question 1
- How does MFORT-QA's performance compare when using different large language models (LLMs) beyond ChatGPT, such as LLaMA2 or Mistral-7B?
- Basis: Authors mention future work could explore open-source models like LLaMA2 or Mistral-7B
- Why unresolved: Only evaluates using ChatGPT without comparative results
- Evidence needed: Experimental results comparing MFORT-QA across multiple LLMs on OTT-QA dataset

### Open Question 2
- What is the impact of varying the number of top-k candidate tables and hyperlink passages retrieved on performance?
- Basis: Authors mention top three tables produces high hit rate but don't explore different k values
- Why unresolved: Only reports results for top 3 tables without k value analysis
- Evidence needed: Systematic evaluation using different k values (1, 3, 5, 10) for table and hyperlink passage retrieval

### Open Question 3
- How does performance scale with increasingly complex questions involving multiple nested hyperlinks?
- Basis: Authors mention MFORT-QA handles complex questions with nested hyperlinks but don't analyze performance on varying complexity levels
- Why unresolved: No comprehensive study on how performance is affected by number of nested hyperlinks or question structure complexity
- Evidence needed: Detailed evaluation on dataset with questions of varying complexity including nested hyperlinks and reasoning depth

## Limitations

- Lacks specific technical specifications for integrating FSL, CoT, and RAG components
- Results demonstrated only on OTT-QA dataset, limiting generalizability to other table QA domains
- Weak corpus evidence with average neighbor FMR of 0.468 and zero citations

## Confidence

- **High confidence**: Overall architecture combining FSL, CoT, and RAG is logically sound and addresses well-defined problem
- **Medium confidence**: Reported performance improvements (12-30% F1/EM gains) are substantial but may be specific to OTT-QA dataset characteristics
- **Low confidence**: Effectiveness of each individual component without synergistic effects of full system

## Next Checks

1. **Component ablation study**: Systematically test each component (FSL, CoT, RAG) independently on same dataset to quantify individual contributions versus combined performance

2. **Cross-dataset generalization**: Evaluate MFORT-QA on at least one other table QA dataset (WikiTableQuestions or TabFact) to assess generalizability beyond OTT-QA

3. **Retrieval quality analysis**: Measure precision@k for table retrieval component separately from end-to-end system to determine if retrieval accuracy drives overall performance improvements