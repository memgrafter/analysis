---
ver: rpa2
title: A Gradient Accumulation Method for Dense Retriever under Memory Constraint
arxiv_id: '2406.12356'
source_url: https://arxiv.org/abs/2406.12356
tags:
- memory
- bank
- contaccum
- training
- nlocal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Contrastive Accumulation (CONTACCUM), a memory-efficient
  method for training dense retrievers that outperforms both existing memory reduction
  methods and high-resource scenarios. The key innovation is using a dual memory bank
  structure to cache query and passage representations from previous training steps,
  allowing more negative samples without increasing memory usage.
---

# A Gradient Accumulation Method for Dense Retriever under Memory Constraint

## Quick Facts
- arXiv ID: 2406.12356
- Source URL: https://arxiv.org/abs/2406.12356
- Authors: Jaehee Kim; Yukyung Lee; Pilsung Kang
- Reference count: 40
- Primary result: Achieves up to 4.9 points improvement over baselines while using only 11GB memory

## Executive Summary
This paper proposes Contrastive Accumulation (CONTACCUM), a memory-efficient method for training dense retrievers that outperforms both existing memory reduction methods and high-resource scenarios. The key innovation is using a dual memory bank structure to cache query and passage representations from previous training steps, allowing more negative samples without increasing memory usage. Experiments on five information retrieval datasets show CONTACCUM achieves significant performance gains while using minimal memory and training faster than existing approaches like GradCache.

## Method Summary
CONTACCUM addresses memory constraints in dense retriever training by implementing a dual memory bank structure that caches query and passage representations from previous training steps. The method uses InfoNCE loss with gradient accumulation, storing representations in FIFO queues and applying stop-gradient operations. This allows the model to utilize more negative samples than the local batch size without exceeding memory limits, achieving better performance than both standard gradient accumulation and other memory reduction methods like GradCache.

## Key Results
- Achieves up to 4.9 points improvement over baselines on five information retrieval datasets
- Operates within 11GB memory constraint while outperforming high-resource scenarios
- Trains 34% faster than GradCache while using minimal additional memory (up to 0.5% more than GradAccum)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual memory bank stabilizes training by maintaining balanced gradient norms between query and passage encoders.
- Mechanism: The method caches representations from both query and passage encoders in FIFO queues, ensuring that the gradient norms ||∇ΘL(Sk)|| and ||∇ΛL(Sk)|| remain balanced during training.
- Core assumption: The gradient norm imbalance problem causes unstable training, and using memory banks of equal size resolves this issue.
- Evidence anchors:
  - [abstract]: "Theoretical analysis confirms the dual memory bank stabilizes training by maintaining balanced gradient norms between query and passage encoders."
  - [section]: "This ensures that the gradient norms of the two encoders remain similar and stabilizes the training process."
  - [corpus]: Weak evidence - related papers discuss memory bank compression and low-rank factorization but do not directly address gradient norm balance.
- Break condition: If the memory bank sizes Nqmemory and Npmemory are not equal, gradient norm imbalance reappears.

### Mechanism 2
- Claim: CONTACCUM outperforms high-resource scenarios by using more negative samples without increasing memory usage.
- Mechanism: By caching previously generated representations, CONTACCUM can use Nlocal + Npmemory - 1 negative passages per query, which can exceed the Ntotal - 1 negative samples used in high-resource settings.
- Core assumption: More negative samples in InfoNCE loss lead to better performance.
- Evidence anchors:
  - [abstract]: "Experiments on five information retrieval datasets show CONTACCUM achieves up to 4.9 points improvement over baselines while using only 11GB of memory."
  - [section]: "Moreover, if Npmemory > Nlocal × (K - 1), CONTACCUM can utilize more negative passages than the total batch, enabling superior performance in low-resource setting compared to high-resource scenario."
  - [corpus]: Weak evidence - related papers discuss contrastive learning and negative sampling but do not specifically address the relationship between negative samples and performance in low-resource settings.
- Break condition: If the memory bank size is too small to provide more negative samples than the total batch, the performance advantage diminishes.

### Mechanism 3
- Claim: CONTACCUM is time-efficient compared to existing memory reduction methods like GradCache.
- Mechanism: CONTACCUM reuses previously generated representations without requiring additional forward passes, reducing computational overhead compared to GradCache.
- Core assumption: The overhead of computing and storing gradients in GradCache significantly increases training time.
- Evidence anchors:
  - [abstract]: "The method also trains faster than existing approaches like GradCache, completing iterations 34% faster."
  - [section]: "CONTACCUM performs single iterations faster than GradCache in all total batch size."
  - [corpus]: Weak evidence - related papers discuss memory-efficient training but do not directly compare the training speed of CONTACCUM and GradCache.
- Break condition: If the additional computations involved in storing and retrieving representations from the memory bank become too large, the time efficiency advantage may be lost.

## Foundational Learning

- Concept: InfoNCE Loss
  - Why needed here: CONTACCUM uses InfoNCE loss for training dense retrievers, so understanding this loss function is crucial for comprehending the method.
  - Quick check question: How does the number of negative samples affect the performance of InfoNCE loss?

- Concept: Memory Bank
  - Why needed here: CONTACCUM employs a dual memory bank structure to cache query and passage representations, so understanding how memory banks work is essential.
  - Quick check question: What is the difference between using a single memory bank for passages and a dual memory bank for both queries and passages?

- Concept: Gradient Accumulation
  - Why needed here: CONTACCUM builds upon the concept of gradient accumulation to train models with limited memory, so understanding this technique is necessary.
  - Quick check question: How does gradient accumulation allow training with a total batch size that cannot fit into memory?

## Architecture Onboarding

- Component map:
  - Dual Memory Bank: FIFO queues storing query and passage representations
  - Encoder: Query encoder (fΘ) and passage encoder (gΛ) that generate representations
  - Loss Function: InfoNCE loss used for training
  - Gradient Accumulation: Process of decomposing the total batch into smaller batches for training

- Critical path:
  1. Encode queries and passages using the query and passage encoders
  2. Cache the generated representations in the dual memory bank
  3. Construct the similarity matrix using both current and stored representations
  4. Calculate the InfoNCE loss and perform backpropagation
  5. Update the model weights using the accumulated gradients

- Design tradeoffs:
  - Memory vs. Performance: Increasing the memory bank size improves performance but also increases memory usage
  - Time vs. Performance: Using more negative samples improves performance but may increase training time
  - Stability vs. Speed: The dual memory bank stabilizes training but may introduce additional computations

- Failure signatures:
  - Gradient norm imbalance: If the query and passage memory bank sizes are not equal, the gradient norms may become imbalanced, leading to unstable training
  - Insufficient negative samples: If the memory bank size is too small, CONTACCUM may not be able to use more negative samples than high-resource scenarios, reducing its performance advantage
  - High computational overhead: If the additional computations involved in storing and retrieving representations from the memory bank become too large, the time efficiency advantage may be lost

- First 3 experiments:
  1. Verify that the dual memory bank maintains balanced gradient norms by comparing the gradient norms of the query and passage encoders during training with and without the query memory bank
  2. Test the performance of CONTACCUM with different memory bank sizes to determine the optimal size for maximizing performance without exceeding memory constraints
  3. Compare the training speed of CONTACCUM and GradCache with various total batch sizes to confirm the time efficiency advantage of CONTACCUM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CONTACCUM's dual memory bank approach improve stability and performance during the pre-training phase of dense retrievers?
- Basis in paper: [inferred] The paper acknowledges that it only focuses on supervised fine-tuning and suggests that future work should investigate whether CONTACCUM can alleviate the gradient norm imbalance problem during pre-training.
- Why unresolved: The authors explicitly state that this is an area for future research and did not conduct experiments during the pre-training phase.
- What evidence would resolve it: Experiments comparing CONTACCUM's performance during pre-training against other memory reduction methods like GradCache and GradAccum would provide evidence of its effectiveness.

### Open Question 2
- Question: Can the reliance on the softmax operation in CONTACCUM be reduced to further improve computational efficiency?
- Basis in paper: [explicit] The paper mentions that CONTACCUM still relies on the softmax operation, which incurs high computational costs, and suggests investigating efficient training strategies to mitigate this burden.
- Why unresolved: The authors acknowledge this limitation but do not provide a solution or experimental results demonstrating a reduction in softmax dependency.
- What evidence would resolve it: Experiments showing improved training speed or reduced memory usage when using alternative similarity measures (e.g., dot product, cosine similarity) instead of softmax would resolve this question.

### Open Question 3
- Question: How does the size of the query and passage memory banks (N_q^memory and N_p^memory) impact CONTACCUM's performance and stability?
- Basis in paper: [inferred] The paper discusses the importance of balanced gradient norms and mentions that CONTACCUM uses memory banks of equal size (N_q^memory = N_p^memory = N_memory), but it does not explore the impact of varying the sizes of these memory banks.
- Why unresolved: The authors do not provide experimental results or analysis on the effect of different memory bank sizes on performance and stability.
- What evidence would resolve it: Experiments comparing CONTACCUM's performance with different ratios of N_q^memory and N_p^memory would provide insights into the optimal memory bank sizes for achieving the best results.

## Limitations
- Theoretical analysis of gradient norm balance lacks rigorous mathematical proof
- Performance gains primarily measured on academic datasets, limited real-world evaluation
- Method assumes static corpora during training, unclear adaptation to streaming data
- Scaling behavior for much larger models or datasets not thoroughly explored

## Confidence
- **High confidence**: Memory efficiency claims (verified through controlled experiments with 11GB constraint), time efficiency vs GradCache (34% faster iterations demonstrated), and basic functionality of dual memory bank structure
- **Medium confidence**: Claims about gradient norm balance stabilization (theoretical analysis provided but not rigorously proven), and the mechanism by which negative sample quantity directly translates to performance gains
- **Low confidence**: Claims about outperforming high-resource scenarios in all settings (only tested in low-resource configurations), and generalization to non-academic retrieval scenarios

## Next Checks
1. **Gradient Norm Analysis**: Implement detailed logging of gradient norms for both query and passage encoders during training with CONTACCUM vs GradAccum. Measure the variance and convergence behavior across multiple random seeds to statistically validate the claim of improved stability.

2. **Negative Sample Saturation Test**: Systematically vary the memory bank size (Nmemory) while keeping total batch size constant to identify the point of diminishing returns. Plot retrieval performance against negative sample count to verify the claimed relationship between more negatives and better performance.

3. **Scalability Evaluation**: Test CONTACCUM on a dataset 10× larger than MS Marco with models scaled to 2-3× the size. Measure memory usage, training time, and retrieval performance to confirm the method's effectiveness scales beyond the evaluated scenarios.