---
ver: rpa2
title: 'EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees'
arxiv_id: '2406.16858'
source_url: https://arxiv.org/abs/2406.16858
tags:
- draft
- eagle-2
- arxiv
- speculative
- eagle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EAGLE-2, a lossless acceleration algorithm
  for large language model (LLM) inference. It addresses the inefficiency of autoregressive
  decoding by introducing context-aware dynamic draft trees into speculative sampling.
---

# EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees

## Quick Facts
- arXiv ID: 2406.16858
- Source URL: https://arxiv.org/abs/2406.16858
- Authors: Yuhui Li; Fangyun Wei; Chao Zhang; Hongyang Zhang
- Reference count: 14
- Primary result: EAGLE-2 achieves 3.05x-4.26x speedup ratios on LLM inference, 20%-40% faster than EAGLE-1 while maintaining lossless acceleration.

## Executive Summary
EAGLE-2 introduces a context-aware dynamic draft tree mechanism to accelerate large language model inference through speculative sampling. By leveraging well-calibrated confidence scores from the draft model, EAGLE-2 dynamically adjusts the draft tree structure to increase the number of accepted draft tokens. Extensive experiments across six tasks (multi-turn conversation, code generation, mathematical reasoning, instruction following, summarization, and question answering) with three LLM series demonstrate significant speedup improvements while ensuring the distribution of generated text remains unchanged.

## Method Summary
EAGLE-2 improves upon speculative sampling by introducing context-aware dynamic draft trees that adjust based on confidence scores from the draft model. The method operates through three main phases: expansion (selecting top-k nodes with highest global acceptance probabilities), reranking (selecting top m tokens with highest values), and verification (checking draft tokens using the original LLM). The dynamic adjustment of the draft tree structure is based on the observation that acceptance rates are both position-dependent and context-dependent, allowing for more efficient token generation while maintaining consistency with vanilla autoregressive decoding.

## Key Results
- Achieves speedup ratios of 3.05x-4.26x across six diverse tasks
- Outperforms EAGLE-1 by 20%-40% in terms of speedup
- Maintains lossless acceleration with unchanged text distribution
- Demonstrates consistent performance across Vicuna, LLaMA2-Chat, and LLaMA3-Instruct series

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The draft model's confidence score approximates the acceptance rate of draft tokens.
- Mechanism: EAGLE-2 uses confidence scores from the draft model to estimate acceptance rates, enabling dynamic adjustment of the draft tree structure.
- Core assumption: The draft model is well-calibrated, meaning its confidence scores accurately reflect acceptance rates.
- Evidence anchors: Strong positive correlation between draft model confidence scores and acceptance rates; confidence scores approximate acceptance rates with small errors.

### Mechanism 2
- Claim: Context-aware dynamic draft trees yield better performance than static draft trees.
- Mechanism: Dynamically adjusts draft tree structure based on context-dependent acceptance rates of draft tokens.
- Core assumption: Acceptance rates depend not only on position but also on context.
- Evidence anchors: Acceptance rates found to be context-dependent in addition to position-dependent.

### Mechanism 3
- Claim: EAGLE-2 achieves lossless acceleration while maintaining unchanged text distribution.
- Mechanism: Only improves expansion and reranking phases without altering draft model training, inference, or verification stage.
- Core assumption: Adjustments to draft tree structure don't introduce bias or alter output distribution.
- Evidence anchors: Consistency with vanilla autoregressive decoding proven in Appendix A.1 of related work.

## Foundational Learning

- Concept: Speculative sampling
  - Why needed here: Understanding speculative sampling is crucial for grasping EAGLE-2's core improvement over existing methods.
  - Quick check question: What is the main goal of speculative sampling, and how does it achieve this goal?

- Concept: Autoregressive decoding
  - Why needed here: Autoregressive decoding is the baseline method that EAGLE-2 aims to accelerate.
  - Quick check question: What are the main drawbacks of autoregressive decoding, and how does it work?

- Concept: Tree attention
  - Why needed here: EAGLE-2 uses tree attention to expand the draft tree efficiently.
  - Quick check question: How does tree attention differ from standard attention, and what are its benefits in speculative decoding?

## Architecture Onboarding

- Component map: Original LLM -> Draft model -> Draft tree -> Expansion phase -> Reranking phase -> Verification stage -> Final output
- Critical path:
  1. Input prefix to draft model
  2. Generate draft tokens and confidence scores
  3. Expand draft tree based on top-k nodes with highest values
  4. Rerank draft tokens and select top m nodes
  5. Flatten selected tokens and adjust attention mask
  6. Input flattened tokens to original LLM for verification
  7. Determine acceptance of draft tokens and generate final output

- Design tradeoffs:
  - Tree depth vs. overhead: Deeper trees generate more tokens per cycle but increase draft model overhead
  - Number of candidates vs. accuracy: More candidates per node improve accuracy but increase draft tree complexity
  - Value-based expansion vs. confidence-based expansion: Value-based considers global acceptance probability, confidence-based considers local probability

- Failure signatures:
  - Low speedup ratio: Indicates insufficient accepted tokens or excessive draft model overhead
  - Inconsistent output distribution: Suggests bias introduction from draft tree adjustments
  - High rejection rate: Implies poor correlation between confidence scores and acceptance rates

- First 3 experiments:
  1. Compare acceptance rates of draft tokens at different positions to verify context-dependent nature
  2. Analyze correlation between draft model confidence scores and acceptance rates
  3. Implement and test value-based expansion and reranking phases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EAGLE-2's performance scale with larger or deeper draft trees?
- Basis in paper: [inferred] Fixed draft tree parameters used across experiments; performance impact of varying these parameters not investigated
- Why unresolved: Experiments used consistent draft tree parameters without exploring scalability
- What evidence would resolve it: Experiments with varying draft tree parameters (depth, nodes) comparing speedup ratios and acceptance lengths

### Open Question 2
- Question: How does EAGLE-2's performance compare to other methods when using a smaller draft model?
- Basis in paper: [explicit] Same draft model weights used as EAGLE; performance impact of smaller draft models not investigated
- Why unresolved: Experiments used draft models of varying sizes without exploring smaller alternatives
- What evidence would resolve it: Experiments with EAGLE-2 using smaller draft models compared to other speculative sampling methods

### Open Question 3
- Question: How does EAGLE-2's performance generalize to tasks beyond those evaluated?
- Basis in paper: [inferred] Limited to six specific tasks and datasets; performance on other domains unexplored
- Why unresolved: Focus on commonly used LLM benchmarks without exploring broader task diversity
- What evidence would resolve it: Experiments on wider range of tasks including complex reasoning and specialized domains

## Limitations

- Limited evaluation scope to six specific tasks and three LLM series, leaving generalization to other domains unexplored
- Computational overhead of draft model and dynamic draft tree management not explicitly quantified
- Well-calibrated nature of draft model assumed but not empirically validated across varying architectures or training distributions

## Confidence

- Speedup Claims: High confidence - well-supported by extensive experiments across multiple tasks and LLM series
- Lossless Acceleration: Medium confidence - theoretically sound but limited empirical validation across diverse scenarios
- Well-Calibrated Draft Model: Medium confidence - correlation shown but robustness across different conditions not thoroughly examined

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate EAGLE-2 on additional domains (long-form generation, code translation, medical text) measuring speedup ratios and validating text distribution consistency

2. **Calibration Robustness Analysis**: Systematically test calibration by introducing controlled perturbations to draft model (different training distributions, architectural modifications) measuring correlation impact and performance effects

3. **Overhead Quantification Study**: Measure computational overhead including draft model inference time, memory usage for draft tree management, and communication costs comparing against performance gains