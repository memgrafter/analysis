---
ver: rpa2
title: 'ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which
  Scales?'
arxiv_id: '2403.17368'
source_url: https://arxiv.org/abs/2403.17368
tags:
- chatgpt
- human
- explanation
- language
- clarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how well ChatGPT aligns with human evaluations
  of natural language explanations (NLEs) for AI decisions. The authors collect human
  annotations and ChatGPT-generated ratings for 300 NLE instances across three datasets,
  evaluating both informativeness and clarity.
---

# ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
## Quick Facts
- arXiv ID: 2403.17368
- Source URL: https://arxiv.org/abs/2403.17368
- Authors: Fan Huang; Haewoon Kwak; Kunwoo Park; Jisun An
- Reference count: 33
- Key outcome: ChatGPT aligns better with human evaluations of natural language explanations in coarse-grained scales (binary, ternary) than fine-grained scales, and excels in pairwise comparison tasks

## Executive Summary
This paper investigates how well ChatGPT aligns with human evaluations of natural language explanations (NLEs) for AI decisions. The authors collect human annotations and ChatGPT-generated ratings for 300 NLE instances across three datasets, evaluating both informativeness and clarity. Through correlation analysis, classification tasks at multiple granularities, and pairwise comparison experiments, the study reveals that ChatGPT performs better in coarse-grained classification and pairwise comparisons than in fine-grained rating tasks. The research also tests dynamic prompting to improve alignment, finding mixed results that depend on the specific task and dataset.

## Method Summary
The study evaluates ChatGPT's alignment with human assessments of natural language explanation quality using three datasets (e-SNLI, LIAR-PLUS, and Latent Hatred) with 300 sampled instances. Human annotators rated informativeness and clarity on 7-point Likert scales, and ChatGPT (gpt-3.5-turbo) generated ratings for the same instances. The evaluation framework includes correlation analysis (Pearson and Spearman), classification tasks at binary, ternary, and 7-way granularities, pairwise comparison experiments, and dynamic prompting tests where semantically similar examples are added to the prompt.

## Key Results
- ChatGPT aligns better with human assessments in coarse-grained scales (binary, ternary) than in fine-grained (7-Likert) scales
- ChatGPT excels in pairwise comparison tasks compared to direct rating tasks
- Dynamic prompting improves performance in some pairwise settings but not in raw score classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT aligns better with human assessments in coarse-grained scales (binary, ternary) than in fine-grained (7-Likert) scales.
- Mechanism: The model can effectively differentiate between broad quality categories but struggles with precise ordinal distinctions.
- Core assumption: ChatGPT's training on coarse-grained human preferences enables it to approximate human judgment at higher abstraction levels.
- Evidence anchors:
  - [abstract] "Our results show that ChatGPT aligns better with humans in more coarse-grained scales."
  - [section 4.2] "The ChatGPT performs very well, especially with 100% correctness in measuring clarity on a binary scale for the misinformation justification dataset."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism specifically.
- Break condition: Performance degrades significantly when scale granularity increases beyond ternary classification.

### Mechanism 2
- Claim: ChatGPT excels in pairwise comparison tasks compared to direct rating tasks.
- Mechanism: The model leverages contextual information from two examples to make more accurate quality judgments than when evaluating single instances.
- Core assumption: ChatGPT's reasoning capabilities are better activated in comparative contexts than in absolute scoring contexts.
- Evidence anchors:
  - [section 5] "Comparing instances is easier and more accurate than giving a score to an instance for human annotators."
  - [section 5] "ChatGPT and human experts generally perform better to distinguish the difference from NLE pairs."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism specifically.
- Break condition: Performance in pairwise comparison degrades when the difference between examples is very small (∆=1).

### Mechanism 3
- Claim: Dynamic prompting improves ChatGPT's performance in pairwise comparisons but not in direct classification.
- Mechanism: Providing semantically similar examples as context helps ChatGPT better understand quality differences between two instances.
- Core assumption: In-context learning through relevant examples enhances ChatGPT's comparative reasoning abilities.
- Evidence anchors:
  - [section 6] "Dynamic prompting shows sizable improvement for the misinformation justification dataset."
  - [section 6] "Dynamic prompting makes it even worse for the case that showed the worst performance: informativeness for logical reasoning and implicit hate speech explanation datasets."
  - [corpus] Weak - no direct corpus evidence supporting this mechanism specifically.
- Break condition: Dynamic prompting fails to improve performance when the examples provided are not sufficiently similar to the test instances.

## Foundational Learning

- Concept: Correlation analysis
  - Why needed here: To measure alignment between ChatGPT and human ratings before classification tasks.
  - Quick check question: What type of correlation (Pearson or Spearman) is more appropriate when comparing ordinal human ratings with continuous model outputs?

- Concept: Classification metrics (F1-score, RMSE)
  - Why needed here: To quantify ChatGPT's performance across different classification granularities.
  - Quick check question: Why might F1-score be preferred over accuracy when evaluating classification performance on imbalanced datasets?

- Concept: Pairwise comparison methodology
  - Why needed here: To evaluate ChatGPT's comparative reasoning abilities and potentially reduce annotation bias.
  - Quick check question: How does pairwise comparison potentially reduce the cognitive load on human annotators compared to direct rating?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (dataset sampling, annotation collection) -> ChatGPT interaction layer (prompt generation and API calls) -> Evaluation framework (correlation analysis, classification metrics, pairwise comparison) -> Dynamic prompting module (semantic similarity calculation and prompt construction)

- Critical path: Data sampling → Human annotation collection → ChatGPT annotation collection → Evaluation and analysis

- Design tradeoffs:
  - Coarse-grained vs. fine-grained classification: Easier alignment vs. more detailed quality assessment
  - Direct rating vs. pairwise comparison: Simpler evaluation vs. potentially more accurate results
  - Fixed vs. dynamic prompting: Reproducibility vs. potential performance improvements

- Failure signatures:
  - Low correlation between ChatGPT and human ratings
  - Poor performance in fine-grained classification tasks
  - Inconsistent results across different datasets or metrics

- First 3 experiments:
  1. Test ChatGPT's performance on binary classification of informativeness across all three datasets
  2. Evaluate ChatGPT's pairwise comparison accuracy for clarity ratings on the misinformation justification dataset
  3. Implement and test dynamic prompting on the logical reasoning dataset for ternary classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's performance in evaluating NLE quality generalize to other large language models, particularly open-source models?
- Basis in paper: [explicit] The paper states "This work primarily evaluates ChatGPT (i.e., gpt-3.5-turbo model), which is a closed-source language model. This raises concerns about the generalizability of our findings to other LLMs, particularly open-source models."
- Why unresolved: The study only tested ChatGPT, leaving open whether other LLMs, especially open-source ones, would perform similarly or differently.
- What evidence would resolve it: Testing the same evaluation tasks with a range of other LLMs, including open-source models, and comparing their performance to ChatGPT's results.

### Open Question 2
- Question: How does the design of prompts and the use of dynamic examples within the prompt influence ChatGPT's performance in evaluating NLE quality?
- Basis in paper: [explicit] The paper mentions "However, it is important to acknowledge that different prompt designs may influence ChatGPT's performance. Also, for the dynamic prompting, we only add the two examples in the prompt without altering any other information."
- Why unresolved: The study used a simple and replicable prompt design, but did not explore how variations in instructions and dynamic examples within the prompt might affect ChatGPT's responses.
- What evidence would resolve it: Conducting experiments with different prompt designs, including variations in instructions and the number and type of dynamic examples, and analyzing how these changes impact ChatGPT's performance in evaluating NLE quality.

### Open Question 3
- Question: Can incorporating more diverse and contextually sensitive evaluation metrics better capture the complexity of human language and explanation quality?
- Basis in paper: [explicit] The paper states "Though robust, the metrics used in our study, Informativeness (How helpful the NLE is while understanding the context with the given labels) and Clarity (How clearly the ideas in the NLE are expressed), might not fully capture the multifaceted nature of human language and explanation quality."
- Why unresolved: The study used only two metrics (informativeness and clarity) to evaluate NLE quality, which may not fully capture the nuances of human language and explanation quality.
- What evidence would resolve it: Developing and testing additional evaluation metrics that are tailored to different context-understanding tasks and are more sensitive to the contextual variations of language, and comparing their effectiveness to the current metrics in evaluating NLE quality.

## Limitations
- The study is limited to three specific datasets, which may not generalize to other domains of natural language explanations
- The human annotation process does not specify exact qualification criteria or inter-annotator agreement metrics
- The study uses only one version of ChatGPT (gpt-3.5-turbo) without testing other language models or versions

## Confidence
- Core claim (ChatGPT aligns better in coarse-grained scales): Medium
- Pairwise comparison advantage: Medium
- Dynamic prompting effectiveness: Low

## Next Checks
1. Test ChatGPT's performance on additional datasets from different domains (e.g., medical decision explanations, educational feedback) to assess generalizability beyond the three studied datasets.

2. Implement and evaluate a multi-model comparison using different versions of ChatGPT and other language models (e.g., Claude, Llama) to determine if the observed alignment patterns are specific to gpt-3.5-turbo or represent a broader trend.

3. Conduct ablation studies on the dynamic prompting approach by systematically varying the number and relevance of contextual examples to identify optimal conditions for improvement.