---
ver: rpa2
title: 'FLD+: Data-efficient Evaluation Metric for Generative Models'
arxiv_id: '2411.15584'
source_url: https://arxiv.org/abs/2411.15584
tags:
- images
- generated
- image
- real
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLD+, a novel evaluation metric for generative
  image models based on normalizing flows. Unlike traditional metrics such as FID,
  FLD+ computes exact log-likelihoods of images, enabling stable and reliable assessment
  with far fewer samples.
---

# FLD+: Data-efficient Evaluation Metric for Generative Models

## Quick Facts
- **arXiv ID**: 2411.15584
- **Source URL**: https://arxiv.org/abs/2411.15584
- **Reference count**: 32
- **Primary result**: FLD+ achieves stable evaluation of generative models with fewer than 300 images, compared to over 20,000 needed by FID.

## Executive Summary
This paper introduces FLD+, a novel evaluation metric for generative image models based on normalizing flows that computes exact log-likelihoods of images. Unlike traditional metrics such as FID, FLD+ demonstrates stable and reliable assessment with far fewer samples (under 300 vs. 20,000+ for FID). The key advantage is monotonic behavior with respect to image distortions and sensitivity to subtle quality differences between generative models of varying sizes, making it practical for real-time evaluation during model training and for data-scarce applications.

## Method Summary
FLD+ uses a pre-trained backbone network to extract features from images, which are then passed through a 2D average pooling layer to reduce dimensionality. These reduced-dimensional features are flattened and processed by a normalizing flow (specifically a rational-quadratic neural spline flow) to compute exact log-likelihoods. The metric compares real and generated image sets by computing the ratio of their average log-likelihoods, providing a scalar score that reflects distributional similarity. The method operates in a reduced-dimensional latent space rather than directly on full images, enabling computational efficiency while maintaining sensitivity to image quality variations.

## Key Results
- FLD+ achieves stable metric values with fewer than 300 images, compared to over 20,000 needed by FID
- Demonstrates monotonic behavior with respect to Gaussian noise, Gaussian blur, and salt-and-pepper noise
- Can be efficiently retrained on new domains like medical images with low data and compute requirements
- Outperforms previous metrics in sample efficiency, adaptability, and robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FLD+ computes exact log-likelihoods of images using normalizing flows, enabling stable and reliable assessment with far fewer samples than FID.
- **Mechanism**: Normalizing flows transform the data distribution into a simpler latent space where exact likelihoods can be computed via the change-of-variables formula. This scalar likelihood measure averages stably even with small sample sizes.
- **Core assumption**: The normalizing flow accurately models the real image distribution in the reduced-dimensional latent space.
- **Evidence anchors**:
  - [abstract]: "Unlike traditional metrics such as FID, FLD+ computes exact log-likelihoods of images, enabling stable and reliable assessment with far fewer samples."
  - [section]: "Because normalizing flows approximate the exact log-likelihood of each sample, which is a scalar quantity, it will likely produce a stable metric when averaged over just a few hundred images."
- **Break condition**: If the flow model fails to capture the true data distribution, likelihood estimates become unreliable regardless of sample size.

### Mechanism 2
- **Claim**: FLD+ exhibits monotonic behavior with respect to various image degradations including Gaussian noise, Gaussian blur, and salt-and-pepper noise.
- **Mechanism**: As image quality degrades, the likelihood under the real data distribution decreases monotonically because the flow model was trained exclusively on high-quality real images.
- **Core assumption**: The flow model trained on real images will assign systematically lower likelihoods to degraded versions.
- **Evidence anchors**:
  - [abstract]: "Thus, unlike FID, the proposed Flow-based Likelihood Distance Plus (FLD+) metric exhibits strongly monotonic behavior with respect to different types of image degradations..."
  - [section]: "Our results demonstrate that FLD+ maintains a consistent monotonic trend with increasing levels of Gaussian noise..."
- **Break condition**: If the flow model overfits to specific image patterns, it may not generalize monotonic behavior to all degradation types.

### Mechanism 3
- **Claim**: FLD+ can be efficiently retrained on new domains with low data and compute requirements.
- **Mechanism**: By operating on features extracted from a pre-trained backbone in a reduced-dimensional space, the normalizing flow requires fewer parameters and less training data to adapt to new domains.
- **Core assumption**: The feature extractor provides domain-agnostic representations that can be adapted with limited domain-specific data.
- **Evidence anchors**:
  - [abstract]: "Additionally, FLD+ can be efficiently retrained on new domains, such as medical images, with low data and compute requirements."
  - [section]: "We also show that FLD+ can easily be retrained on new domains, such as medical images, unlike the networks behind previous metrics..."
- **Break condition**: If the pre-trained backbone produces features that are not transferable to the new domain, retraining the flow will not improve performance.

## Foundational Learning

- **Normalizing flows and change-of-variables formula**:
  - Why needed here: Understanding how exact likelihoods are computed is essential to grasp FLD+'s advantage over distribution-distance metrics.
  - Quick check question: What mathematical formula allows normalizing flows to compute exact likelihoods in the original data space?

- **FrÃ©chet Inception Distance (FID) limitations**:
  - Why needed here: Understanding FID's assumptions and drawbacks clarifies why FLD+ was developed.
  - Quick check question: What key assumption about image embeddings does FID make that can lead to unreliable scores?

- **Backbone feature extraction and dimensionality reduction**:
  - Why needed here: FLD+ operates on lower-dimensional features rather than raw images, requiring understanding of this preprocessing step.
  - Quick check question: Why does FLD+ use a pre-trained backbone followed by pooling instead of directly processing images?

## Architecture Onboarding

- **Component map**: Pre-trained backbone -> Feature extraction -> 2D Average Pooling -> Flatten -> Normalizing Flow (Neural Spline Flow) -> Likelihood computation

- **Critical path**: Real images -> Backbone features -> Pooling -> Flow training -> Generated images -> Feature extraction -> Flow evaluation -> Likelihood ratio -> FLD+ score

- **Design tradeoffs**: Lower-dimensional latent space reduces computation but may lose some fine-grained information; frozen backbone ensures consistent feature extraction but may not be optimal for all domains; exact likelihood computation enables sample efficiency but requires careful flow model design

- **Failure signatures**: Non-monotonic behavior with image distortions indicates flow model misalignment; high variance in FLD+ scores with small sample sizes suggests insufficient flow training; domain adaptation failures indicate poor feature transferability

- **First 3 experiments**:
  1. Verify monotonic behavior by applying increasing Gaussian noise to a test image set and plotting FLD+ scores
  2. Test sample efficiency by computing FLD+ with varying numbers of images and measuring score stability
  3. Validate domain adaptation by training the flow on medical images and comparing performance to FID

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FLD+ perform when evaluating images from non-ImageNet-trained generative models in domains with significantly different visual characteristics?
- Basis in paper: [explicit] The authors note that FLD+ uses an ImageNet pre-trained backbone but hypothesize adaptability to new domains through the normalizing flow component.
- Why unresolved: The paper only demonstrates adaptability to medical images, not other domains with different visual characteristics from ImageNet.
- What evidence would resolve it: Testing FLD+ on generative models trained on datasets like satellite imagery, medical scans, or art datasets, comparing its performance to FID and human evaluations.

### Open Question 2
- Question: What is the theoretical relationship between FLD+ and the true likelihood of generated images under the real data distribution?
- Basis in paper: [inferred] The authors state that FLD+ computes exact log-likelihoods using normalizing flows, but the relationship between FLD+ and true likelihood is not rigorously established.
- Why unresolved: The paper focuses on empirical validation rather than theoretical analysis of the likelihood relationship.
- What evidence would resolve it: Mathematical proofs or extensive experiments showing how FLD+ values correlate with KL divergence or other information-theoretic measures of distributional distance.

### Open Question 3
- Question: How does the choice of feature extractor backbone architecture affect FLD+'s sensitivity to different types of image artifacts?
- Basis in paper: [explicit] The authors conducted ablation studies with ResNet-18 and MobileNetV3-Small, finding both maintained monotonicity.
- Why unresolved: The ablation only tested two architectures and didn't explore sensitivity to specific artifact types like color distortions or geometric transformations.
- What evidence would resolve it: Systematic testing of FLD+ with various backbone architectures (Vision Transformers, EfficientNet variants) across different artifact types, measuring which architectures show optimal sensitivity.

### Open Question 4
- Question: What is the optimal pooling strategy for FLD+ when evaluating images with significant spatial dependencies?
- Basis in paper: [explicit] The authors tested average pooling and max pooling, finding average pooling more sensitive to noise variations.
- Why unresolved: The study didn't explore more sophisticated pooling strategies or spatial-aware pooling that might better capture spatial dependencies in images.
- What evidence would resolve it: Comparative experiments testing FLD+ with learned pooling strategies, attention-based pooling, or multi-scale pooling against standard average pooling on datasets with strong spatial patterns.

## Limitations
- Core assumption that normalizing flows can accurately model real image distributions in reduced-dimensional latent spaces remains to be thoroughly validated across diverse domains
- Reliance on a frozen pre-trained backbone may limit adaptability to highly specialized domains where the backbone's feature representations are suboptimal
- Limited empirical evidence for computational efficiency claims, particularly regarding flow training times and memory requirements

## Confidence
- **Sample Efficiency**: Medium confidence - While the theoretical basis for stable likelihood averaging is sound, empirical validation across diverse generative models is limited to specific cases
- **Monotonic Behavior**: Medium confidence - The monotonic trend with image degradations is demonstrated, but the robustness across all degradation types and levels requires further validation
- **Domain Adaptation**: Low confidence - The claim of easy retraining on new domains like medical images is supported by the methodology but lacks comprehensive empirical validation

## Next Checks
1. **Cross-domain validation**: Test FLD+ on at least three additional domains (e.g., medical imaging, satellite imagery, and natural landscapes) to validate the domain adaptation claims and assess backbone transferability

2. **Computational overhead analysis**: Conduct a thorough analysis of FLD+ computational requirements including flow training time, memory usage, and inference speed, comparing these metrics against FID across different hardware configurations

3. **Failure mode characterization**: Systematically test FLD+ under various failure scenarios including out-of-distribution inputs, adversarial examples, and cases where the flow model significantly misestimates likelihoods to understand its robustness boundaries