---
ver: rpa2
title: A new approach for fine-tuning sentence transformers for intent classification
  and out-of-scope detection tasks
arxiv_id: '2410.13649'
source_url: https://arxiv.org/abs/2410.13649
tags:
- intent
- sentence
- training
- in-scope
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of out-of-scope (OOS) detection
  in virtual assistant systems, where queries outside the system's scope need to be
  rejected or redirected. The proposed method improves fine-tuning of transformer-based
  sentence encoders for intent classification by adding a reconstruction loss from
  an auxiliary autoencoder to the standard cross-entropy loss.
---

# A new approach for fine-tuning sentence transformers for intent classification and out-of-scope detection tasks

## Quick Facts
- arXiv ID: 2410.13649
- Source URL: https://arxiv.org/abs/2410.13649
- Reference count: 13
- Primary result: Improves OOS detection by 1-4% AUPR while maintaining intent classification accuracy

## Executive Summary
This work addresses out-of-scope (OOS) detection in virtual assistant systems by improving transformer-based sentence encoder fine-tuning. The proposed method adds an autoencoder reconstruction loss to the standard cross-entropy loss, encouraging in-scope embeddings to be more compact in the embedding space. Experiments on four datasets show improved OOS detection performance while maintaining intent classification accuracy.

## Method Summary
The approach fine-tunes BERT-base-uncased with max-pooling sentence embeddings using a joint cross-entropy and autoencoder reconstruction loss. The autoencoder head reconstructs sentence embeddings to regularize fine-tuning and reduce embedding dispersion. After fine-tuning, sentence embeddings are extracted and OOS detection uses Mahalanobis distance with class-specific means and universal covariance matrix.

## Key Results
- 1-4% improvement in AUPR for OOS rejection compared to baseline cross-entropy fine-tuning
- Maintains intent classification accuracy across all datasets
- Reconstruction loss weight α=0.1 provides optimal trade-off between OOS detection and classification accuracy
- Performance gains consistent across CLINC150, StackOverflow, MTOP, and Car Assistant datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding autoencoder reconstruction loss to cross-entropy fine-tuning forces in-scope embeddings to be more compact.
- Mechanism: The autoencoder head minimizes reconstruction error for in-scope embeddings, which implicitly constrains their dispersion in embedding space. This makes in-scope and OOS embeddings more separable using Mahalanobis distance.
- Core assumption: Reconstruction loss encourages embeddings to lie in a lower-dimensional manifold that preserves semantic information while reducing global variance.
- Evidence anchors:
  - [abstract] "Our work proposes to regularize the cross-entropy loss with an in-scope embedding reconstruction loss learned using an auto-encoder."
  - [section 3.1] "the sentence embedding si is passed to a second head which is comprised of an autoencoder network... The reconstruction loss computed using mean-squared error"
  - [corpus] Weak evidence - only indirect citations to general autoencoder use in NLP
- Break condition: If reconstruction loss dominates (high α), the model may overfit to reconstruction at the expense of classification accuracy.

### Mechanism 2
- Claim: Fine-tuning with cross-entropy alone disperses in-scope embeddings, causing overlap with OOS embeddings.
- Mechanism: Cross-entropy loss optimizes for class separation but doesn't constrain overall embedding variance, allowing in-scope classes to spread across embedding space and potentially overlap with OOS regions.
- Core assumption: Without regularization, fine-tuning tends to maximize class separation without regard for OOS separability.
- Evidence anchors:
  - [abstract] "Recent work has shown that while this produces suitable embeddings for the intent-classification task, it also tends to disperse in-scope embeddings over the full sentence embedding space."
  - [section 2] "fine-tuning without regularization could make the model forget some of the task-agnostic knowledge about general linguistic properties"
  - [corpus] Weak evidence - only general statements about embedding dispersion
- Break condition: If OOS distribution is very close to in-scope distribution, even reduced dispersion may not help.

### Mechanism 3
- Claim: Mahalanobis distance classification with learned covariance matrix is effective for OOS detection after our fine-tuning.
- Mechanism: The fine-tuned embeddings have reduced global dispersion, making the universal covariance matrix more reliable for measuring Mahalanobis distance. This improves OOS detection by providing a tighter confidence region around in-scope embeddings.
- Core assumption: Reduced embedding dispersion makes the covariance matrix estimate more stable and the Mahalanobis distance more discriminative.
- Evidence anchors:
  - [section 3.2-3.3] "Sentence embeddings using this transformer encoder are then generated for each training sample... These per-class sentence embeddings are then used to construct a set of C mean-vectors"
  - [section 4.3] "the most suitable sentence encoders for this purpose are transformer-based encoders"
  - [corpus] Weak evidence - no direct citations about Mahalanobis distance effectiveness with reduced dispersion
- Break condition: If the covariance matrix becomes singular due to very tight clustering, Mahalanobis distance becomes unreliable.

## Foundational Learning

- Concept: Transformer-based sentence encoders
  - Why needed here: The paper uses BERT-base-uncased followed by max-pooling to generate 768-dimensional sentence embeddings as the foundation for both intent classification and OOS detection.
  - Quick check question: What is the dimensionality of sentence embeddings produced by BERT-base-uncased with max-pooling?

- Concept: Cross-entropy loss for intent classification
  - Why needed here: Standard fine-tuning uses softmax over sentence embeddings with cross-entropy loss to optimize intent classification accuracy.
  - Quick check question: How does cross-entropy loss handle the classification of in-scope intents?

- Concept: Autoencoder reconstruction loss
  - Why needed here: The auxiliary autoencoder head reconstructs sentence embeddings to regularize fine-tuning and reduce embedding dispersion.
  - Quick check question: What type of loss function is used for the autoencoder reconstruction?

- Concept: Mahalanobis distance for OOS detection
  - Why needed here: After fine-tuning, OOS detection uses Mahalanobis distance with class-specific means and universal covariance matrix to determine if samples are in-scope or OOS.
  - Quick check question: How is the Mahalanobis distance calculated for OOS detection?

## Architecture Onboarding

- Component map: BERT-base-uncased → Max-pooling → Cross-entropy head (discarded) → Autoencoder head (discarded) → Sentence embeddings for inference
- Critical path: Input → Transformer → Pooling → Combined loss (CE + AE) → Backpropagation → Trained encoder
- Design tradeoffs: Adding autoencoder increases fine-tuning complexity and memory usage but improves OOS detection; removing both heads after training keeps inference lightweight
- Failure signatures: OOS detection fails when reconstruction loss dominates (high α) or when fine-tuning data is too limited to learn meaningful reconstruction
- First 3 experiments:
  1. Baseline: Fine-tune with cross-entropy only, measure AUPR and accuracy
  2. Full model: Fine-tune with CE+AE loss (α=0.1), measure AUPR and accuracy
  3. Ablation: Fine-tune with varying α values [0.01, 0.1, 0.2, 0.5, 0.9] to find optimal trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between autoencoder regularization strength (α) and maintaining task-agnostic linguistic knowledge in fine-tuned sentence transformers?
- Basis in paper: [explicit] The paper mentions that fine-tuning without regularization could make the model forget some task-agnostic knowledge about general linguistic properties which could help OOS detection, and that the autoencoder weight α was sensitive during hyperparameter optimization.
- Why unresolved: While the paper found α = 0.1 worked well across datasets, it doesn't explore the relationship between α and preservation of linguistic knowledge, or whether different α values might better preserve different types of task-agnostic features.
- What evidence would resolve it: Systematic experiments varying α across a wider range and measuring both OOS detection performance and task-agnostic linguistic feature preservation using probes or other evaluation methods.

### Open Question 2
- Question: How does the proposed fine-tuning approach generalize to compositional or compound queries containing both in-scope and out-of-scope elements?
- Basis in paper: [inferred] The paper mentions this as a limitation, noting that in most virtual assistant systems multi-intent queries are first broken into single-intent phrases, but there are not many studies on this use case.
- Why unresolved: The current approach hasn't been evaluated on compound queries, and it's unclear whether the autoencoder regularization would help or hinder the model's ability to recognize when a query contains mixed in-scope and out-of-scope content.
- What evidence would resolve it: Experiments with datasets containing compound queries, or synthetic generation of such queries, measuring both OOS detection and intent classification accuracy on these more complex inputs.

### Open Question 3
- Question: Can the autoencoder-based regularization be extended to few-shot learning scenarios where limited training examples per intent class are available?
- Basis in paper: [explicit] The paper states that the approach requires more than a few examples per intent class to make a significant impact, and that it's not suitable for few-shot learning, as evidenced by the CLINC150 dataset where OOS accuracy stayed the same.
- Why unresolved: The paper doesn't explore modifications to the autoencoder architecture or training procedure that might make it more effective with limited data, such as using pretrained autoencoders or incorporating contrastive learning.
- What evidence would resolve it: Comparative experiments on few-shot intent classification and OOS detection tasks, testing variations of the autoencoder-based fine-tuning approach with limited training data.

## Limitations

- The paper lacks comprehensive ablation studies on autoencoder architecture design choices
- The optimal reconstruction weight α=0.1 is based on validation performance but not systematically justified
- Assumes Mahalanobis distance with universal covariance matrix is optimal without comparing to alternative distance metrics

## Confidence

**High Confidence**: The experimental results showing 1-4% AUPR improvement are well-supported by the four dataset experiments and consistent performance gains across different α values.

**Medium Confidence**: The claim that cross-entropy fine-tuning alone causes excessive dispersion of in-scope embeddings is supported by theoretical reasoning but lacks direct empirical validation.

**Low Confidence**: The assertion that Mahalanobis distance is particularly effective for OOS detection with the proposed fine-tuning method is not strongly supported.

## Next Checks

1. **Ablation on Autoencoder Architecture**: Conduct experiments varying the autoencoder depth, width, and activation functions to determine the minimal effective architecture. Test whether simpler regularization approaches (L2 penalty on embeddings, contrastive loss) achieve similar OOS detection improvements.

2. **Embedding Dispersion Analysis**: Measure and visualize the actual dispersion of in-scope embeddings before and after reconstruction loss fine-tuning. Calculate metrics like embedding variance, class overlap, and nearest-neighbor distances to provide quantitative evidence for the proposed mechanism.

3. **Distance Metric Comparison**: Implement and evaluate alternative OOS detection methods including cosine distance with adaptive thresholds, learned threshold classifiers, and other distance metrics. Compare these approaches against Mahalanobis distance to determine if the proposed method's improvements are specific to Mahalanobis distance or generalize to other detection methods.