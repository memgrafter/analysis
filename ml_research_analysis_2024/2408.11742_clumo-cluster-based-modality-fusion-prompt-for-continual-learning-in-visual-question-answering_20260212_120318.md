---
ver: rpa2
title: 'CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual
  Question Answering'
arxiv_id: '2408.11742'
source_url: https://arxiv.org/abs/2408.11742
tags:
- prompt
- learning
- tasks
- task
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CluMo, a cluster-based modality fusion prompt
  method for continual learning in visual question answering (VQA). The method addresses
  the challenge of catastrophic forgetting when adapting large vision-language models
  (VLMs) to sequentially encountered tasks in a continual learning setting.
---

# CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering

## Quick Facts
- arXiv ID: 2408.11742
- Source URL: https://arxiv.org/abs/2408.11742
- Reference count: 11
- Primary result: Achieves state-of-the-art performance on CLOVE-scene and CLOVE-function benchmarks for VQA continual learning, with high average accuracy and low average forgetting rate.

## Executive Summary
This paper introduces CluMo, a cluster-based modality fusion prompt method for continual learning in visual question answering (VQA). The approach addresses catastrophic forgetting by using a two-stage training strategy with K-means clustering to create semantic prompt keys for visual and textual inputs. CluMo demonstrates state-of-the-art performance on two VQA continual learning benchmarks, effectively balancing accuracy and forgetting rate through its innovative prompt selection mechanism.

## Method Summary
CluMo employs a two-stage training strategy for continual learning in VQA. First, it trains visual and textual prompt keys using K-means clustering on unimodal features from each task. Second, it freezes these keys and uses their combination to select task-specific prompts from a prompt pool. The method also incorporates knowledge distillation between successive task models to prevent parameter drift. This approach enables effective adaptation to new tasks while minimizing forgetting of previously learned knowledge.

## Key Results
- Achieves state-of-the-art performance on CLOVE-scene and CLOVE-function benchmarks
- Demonstrates superior average accuracy and average forgetting rate compared to existing methods
- Ablation studies validate the effectiveness of prompt key clustering and knowledge distillation components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training strategy with frozen prompt keys prevents catastrophic forgetting while enabling task-specific adaptation
- Mechanism: First stage clusters unimodal features into semantic centers (prompt keys). Second stage freezes these keys and uses their combination to select from task-specific prompt pool
- Core assumption: Prompt keys learned via clustering remain semantically meaningful across task sequences when frozen
- Evidence anchors: [abstract] and [section 4.3] describe the two-stage training and K-means clustering process
- Break condition: Drastic domain changes between tasks could cause prompt keys to become semantically misaligned

### Mechanism 2
- Claim: Modality fusion prompt captures complementary semantic information from both vision and text inputs
- Mechanism: Each prompt is linked to one visual and one textual prompt key. Input features are matched separately to each key space, and their combination indexes into a task-specific prompt pool
- Core assumption: Visual and textual features encode complementary semantic aspects that provide richer context when combined
- Evidence anchors: [abstract] and [section 4.2] explain the Key-Key-Prompt pair association
- Break condition: If one modality becomes uninformative or noisy, fusion strategy may degrade performance

### Mechanism 3
- Claim: Knowledge distillation between successive task models reduces parameter drift and stabilizes performance
- Mechanism: After training on task T, a frozen copy MT-1 is kept. During training on task T, difference between MT and MT-1 outputs is added to loss via MSE
- Core assumption: Frozen previous-task model outputs serve as reliable target for current model, constraining weight updates
- Evidence anchors: [section 4.4] describes the KD implementation using MSE loss
- Break condition: If model architecture or input distribution changes substantially between tasks, KD target may become irrelevant

## Foundational Learning

- Concept: K-means clustering for semantic feature grouping
  - Why needed here: To generate prompt keys representing clusters of semantically similar visual/textual inputs for stable prompt selection
  - Quick check question: How does minibatch K-means differ from standard K-means, and why is it suitable for streaming data in continual learning?

- Concept: Prompt-based learning vs. full fine-tuning
  - Why needed here: Prompt tuning freezes backbone, reducing catastrophic forgetting while allowing task-specific adaptation via small learnable parameters
  - Quick check question: What are the memory and computational trade-offs between prompt tuning and adapter-based methods?

- Concept: Knowledge distillation for model stabilization
  - Why needed here: To penalize large parameter shifts between tasks, helping retain knowledge from previous tasks
  - Quick check question: How does MSE-based distillation differ from KL divergence-based distillation in multimodal settings?

## Architecture Onboarding

- Component map: Frozen ALBEF transformer (visual encoder, textual encoder, multimodal encoder, answer decoder) -> Visual prompt key pool (Kv) and Textual prompt key pool (Kt) -> Prompt pool (P) indexed by (visual key, textual key) pairs -> Knowledge distillation MSE loss between current and previous task models

- Critical path:
  1. On new task: Initialize new prompt keys and prompt pool
  2. First stage: Cluster visual/textual features → update Kv, Kt until convergence
  3. Second stage: For each batch, match inputs to nearest keys → select prompt → forward pass with prompt + encoders → compute loss (CE + KD) → backprop only prompt and classifier

- Design tradeoffs:
  - Prompt key size vs. prompt pool size: Larger keys increase granularity but also increase memory and computation
  - Clustering vs. learned keys: Clustering is data-driven but may not adapt well to distribution shifts; learned keys could be more flexible but risk overfitting
  - KD weighting: Too high → slow adaptation; too low → forgetting

- Failure signatures:
  - Accuracy plateau or drop after initial tasks → prompt keys may not generalize
  - Prompt selection instability → clustering convergence issues
  - High forgetting rate → KD weight too low or prompt keys frozen too early

- First 3 experiments:
  1. Vary visual and textual prompt key sizes (2x2, 3x3, 4x4) and measure accuracy/forgetting trade-offs
  2. Disable KD and compare forgetting rates to validate its contribution
  3. Replace clustering with random initialization of prompt keys and compare clustering error vs. accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CluMo's performance scale with increasing numbers of tasks and domain diversity in continual learning?
- Basis in paper: [inferred] Experiments limited to two specific benchmarks with six tasks each, not exploring larger task numbers
- Why unresolved: Paper doesn't test beyond CLOVE-scene and CLOVE-function benchmarks with fixed task counts
- What evidence would resolve it: Additional experiments with benchmarks containing more tasks, greater domain diversity, or different task distributions

### Open Question 2
- Question: What is the impact of different clustering algorithms (beyond k-means) on prompt key training effectiveness?
- Basis in paper: [explicit] Paper uses k-means clustering but doesn't explore or compare alternative clustering methods
- Why unresolved: Choice of k-means not justified or compared against other potential clustering algorithms
- What evidence would resolve it: Experiments comparing performance using different clustering algorithms (hierarchical clustering, DBSCAN, Gaussian Mixture Models)

### Open Question 3
- Question: How does CluMo handle task boundaries in task-free continual learning where task identities are unknown?
- Basis in paper: [inferred] Assumes domain-incremental learning with known task boundaries, not addressing unknown task identity scenarios
- Why unresolved: Real-world applications often involve streaming data without clear task boundaries
- What evidence would resolve it: Experiments in task-free continual learning setting where task identities are not provided

## Limitations

- Effectiveness heavily dependent on clustering quality and cluster number selection
- Two-stage training assumes frozen prompt keys remain semantically meaningful across all task sequences
- Knowledge distillation assumes frozen previous-task model outputs remain reliable targets

## Confidence

- High Confidence: Two-stage training strategy with frozen prompt keys is a sound approach to prevent catastrophic forgetting
- Medium Confidence: Modality fusion prompt captures complementary semantic information from vision and text inputs
- Low Confidence: Knowledge distillation component effectively reduces parameter drift and stabilizes performance

## Next Checks

1. Vary prompt key sizes and monitor stability across tasks to assess robustness of clustering-based approach
2. Evaluate performance on domain-shift scenarios (e.g., switching from indoor to outdoor scenes) to test frozen prompt key adaptability
3. Perform comprehensive hyperparameter sensitivity analysis across KD weight, prompt key sizes, and clustering batch size