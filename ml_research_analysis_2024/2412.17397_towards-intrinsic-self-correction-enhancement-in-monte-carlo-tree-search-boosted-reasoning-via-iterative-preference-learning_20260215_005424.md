---
ver: rpa2
title: Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted
  Reasoning via Iterative Preference Learning
arxiv_id: '2412.17397'
source_url: https://arxiv.org/abs/2412.17397
tags:
- arxiv
- learning
- preprint
- reasoning
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage framework to enhance the reasoning
  capabilities of Large Language Models (LLMs) through intrinsic self-correction and
  iterative preference learning. The first stage employs self-generated data to train
  LLMs for self-correction via reinforcement learning.
---

# Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning

## Quick Facts
- **arXiv ID:** 2412.17397
- **Source URL:** https://arxiv.org/abs/2412.17397
- **Reference count:** 5
- **Primary result:** Achieves accuracy improvements of 2.00%-2.28% on GSM8K and 4.18%-4.94% on MATH across different base models

## Executive Summary
This paper introduces a two-stage framework that enhances LLM reasoning capabilities through intrinsic self-correction and iterative preference learning. The approach first trains models to refine their own predictions using self-generated data, then applies this enhanced self-correction within a step-level Monte Carlo Tree Search (MCTS) framework for iterative preference learning. Experimental results demonstrate consistent improvements over existing methods on both GSM8K and MATH datasets, with ablation studies confirming the synergistic benefits of combining these approaches.

## Method Summary
The proposed framework operates in two stages. Stage I employs intrinsic self-correction by training LLMs to refine their predictions using only self-generated data through reinforcement learning, building intrinsic verification ability. Stage II applies this enhanced self-correct model as a reward function within a step-level MCTS-based iterative preference learning framework, providing granular feedback on intermediate reasoning steps. The approach is evaluated on GSM8K and MATH datasets using various base models including Llama-3.1-8B-Instruct and Mistral-7B-Instruct.

## Key Results
- Achieves accuracy improvements of 2.00%-2.28% on GSM8K across different base models
- Demonstrates 4.18%-4.94% accuracy gains on MATH dataset
- Ablation studies confirm superior synergy of combining intrinsic self-correction with step-level iterative preference learning
- Shows consistent performance improvements across multiple base model architectures

## Why This Works (Mechanism)

### Mechanism 1
The two-stage training procedure enables LLMs to develop self-correction capabilities that improve both intermediate step quality and final answer accuracy. Stage I trains the model to generate self-corrective attempts using only its own predictions as feedback, building intrinsic verification ability. Stage II then uses this enhanced self-correct model as a reward model within MCTS-based step-level iterative preference learning, providing richer feedback than sparse outcome supervision alone.

### Mechanism 2
Step-level MCTS with enhanced self-correction reward modeling provides more granular and informative feedback than instance-level approaches, leading to better reasoning performance. MCTS explores multiple reasoning paths at the step level, with rewards computed using both outcome correctness and self-evaluation from the enhanced self-correct model. This provides immediate feedback on intermediate reasoning steps rather than waiting for final answer verification.

### Mechanism 3
The combination of intrinsic self-correction with MCTS-based iterative preference learning creates a synergistic effect where each component amplifies the other's benefits. The self-correct model provides high-quality reward signals for MCTS exploration, while MCTS's systematic search and preference learning refine the model's reasoning strategies beyond what self-correction alone can achieve.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF) and its variants
  - **Why needed here:** The paper builds on iterative preference learning frameworks similar to RLHF but applies them at the step level rather than instance level
  - **Quick check:** What distinguishes preference learning from standard RL in the context of LLM reasoning?

- **Concept:** Monte Carlo Tree Search (MCTS) algorithm and its application to language model reasoning
  - **Why needed here:** MCTS forms the core search mechanism for exploring reasoning paths, with four key phases (selection, expansion, enhanced-self-verify, backup)
  - **Quick check:** How does the PUCT formula balance exploration and exploitation in the selection phase?

- **Concept:** Multi-turn Markov Decision Process (MDP) formulation for iterative reasoning
  - **Why needed here:** The self-correction framework is formalized as a multi-turn MDP where each turn represents an attempt to improve the response
  - **Quick check:** In the multi-turn MDP, what constitutes the state transition and reward signal at each turn?

## Architecture Onboarding

- **Component map:** Stage I (data generation → self-correct model training) → Stage II (root node initialization → MCTS loop → policy updates)
- **Critical path:** 1) Generate self-correction training data from base model outputs 2) Train Stage I self-correct model on this data 3) Initialize MCTS with Stage I model as reward function 4) Run MCTS exploration and preference learning iterations 5) Update policy model based on collected preference data
- **Design tradeoffs:** Self-correction data quality vs. quantity; MCTS computational budget vs. performance gain; Reward model complexity vs. stability
- **Failure signatures:** Stage I self-correct model fails to improve over base model; MCTS performance plateaus or degrades; MCTS worse with self-correct reward than baseline
- **First 3 experiments:** 1) Verify Stage I self-correction improves single-model accuracy on GSM8K before MCTS integration 2) Test MCTS with baseline reward model on MATH to establish performance floor 3) Run ablation comparing Stage I self-correct model vs. baseline reward model within MCTS on same problems

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The self-correction framework relies entirely on self-generated data, which may not capture the full complexity of reasoning errors
- Computational overhead of MCTS at the step level could be prohibitive for practical deployment
- Ablation studies don't clearly isolate which component (self-correction vs. MCTS structure) drives the improvements

## Confidence

- **High Confidence:** The baseline performance improvements on GSM8K and MATH datasets are well-documented and reproducible
- **Medium Confidence:** The theoretical mechanism of combining self-correction with step-level MCTS is sound, though empirical validation is limited
- **Low Confidence:** The claimed synergistic effects between components lack comprehensive ablation analysis to rule out alternative explanations

## Next Checks

1. **Ablation Study Expansion:** Run additional ablations isolating the contribution of self-correction reward modeling from MCTS structural improvements
2. **Cross-Domain Transfer:** Test whether the enhanced self-correct model trained on GSM8K transfers effectively to non-mathematical reasoning tasks
3. **Scalability Analysis:** Measure computational overhead of step-level MCTS versus instance-level approaches across different model sizes and problem complexities