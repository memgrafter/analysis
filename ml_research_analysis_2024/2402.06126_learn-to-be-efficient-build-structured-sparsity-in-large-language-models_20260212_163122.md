---
ver: rpa2
title: 'Learn To be Efficient: Build Structured Sparsity in Large Language Models'
arxiv_id: '2402.06126'
source_url: https://arxiv.org/abs/2402.06126
tags:
- sparsity
- layers
- training
- experts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have billions of parameters but incur
  high inference costs. A natural approach to reduce this cost is to leverage activation
  sparsity, where only parts of the parameters are involved in inference.
---

# Learn To be Efficient: Build Structured Sparsity in Large Language Models

## Quick Facts
- arXiv ID: 2402.06126
- Source URL: https://arxiv.org/abs/2402.06126
- Reference count: 40
- Large language models (LLMs) have billions of parameters but incur high inference costs. A natural approach to reduce this cost is to leverage activation sparsity, where only parts of the parameters are involved in inference. However, existing methods only utilize naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity. We hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity. To achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms state-of-the-art baselines. Along with our hardware-aware custom kernel implementation, LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity.

## Executive Summary
Large language models suffer from high inference costs due to their massive parameter counts. While activation sparsity naturally occurs in these models, existing methods only exploit this sparsity after training. LTE introduces a novel training algorithm that teaches models to be more efficient by achieving structured activation sparsity during training. By converting FFN layers to MoE layers and training routers to adaptively select experts, LTE enables models to activate fewer neurons while maintaining performance. Extensive experiments show LTE consistently outperforms baselines across language understanding, generation, and instruction tuning tasks, with 25% latency reduction on LLaMA2-7B at 50% sparsity when using custom kernels.

## Method Summary
LTE is a training algorithm that converts dense FFN layers to MoE layers by grouping neurons into experts and training routers to select the most relevant experts per input. The method uses a two-stage training approach: first jointly training the model and routers with soft expert selection and efficiency penalties, then adapting to discrete expert selection. LTE introduces an efficiency loss penalty to encourage sparse activation while maintaining task performance, and uses parameter clustering for expert grouping. A custom hardware-aware kernel implementation translates the structured sparsity into wall-clock latency reductions. The approach is evaluated across encoder-based models (RoBERTa), decoder-based models (GPT2-Medium, LLaMA-2-7B), and various tasks including GLUE, E2E, XSum, Wikitext103, and Tulu.

## Key Results
- LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity with custom kernels
- Consistently outperforms MoEfication and Deja Vu baselines across NLU, NLG, and instruction tuning tasks
- Achieves better trade-offs between sparsity and model performance than existing methods
- Enables efficient inference through structured sparsity patterns that map to custom kernel optimizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LTE achieves better sparsity than baseline methods by training routers to adaptively select experts per input and layer.
- Mechanism: LTE uses sigmoid routing with threshold-based selection, allowing flexible expert counts instead of fixed k experts per MoE layer. The efficiency loss penalty encourages sparse activation while maintaining task performance.
- Core assumption: Sparse activation in FFN layers can be increased without harming model quality, and more structured sparsity translates to wall-clock speedups.
- Evidence anchors:
  - [abstract] "We hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity."
  - [section] "LTE integrates an efficiency loss penalty, encouraging models to activate fewer neurons in their FFN layers while keeping good task performance."
  - [corpus] "Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters" - suggests activation sparsity is a promising approach.
- Break condition: If the efficiency loss causes significant performance degradation, or if the threshold-based selection becomes too restrictive for certain inputs.

### Mechanism 2
- Claim: LTE's two-stage training stabilizes router learning and adapts the model to discrete expert selection.
- Mechanism: Stage 1 uses "soft" expert selection (weighted sum of all experts) with efficiency and separability losses to train routers and model jointly. Stage 2 freezes routers and fine-tunes the model for discrete selection.
- Core assumption: Joint training of routers and model with soft selection can produce routers that generalize well to discrete selection.
- Evidence anchors:
  - [section] "Training the router poses three practical challenges... To tackle these challenges, we next propose a novel two-stage training algorithm..."
  - [section] "Stage 1: Model-router training... Stage 2: Model Adaptation."
  - [corpus] Weak - corpus papers focus on pruning/activation sparsity but don't detail two-stage training.
- Break condition: If the model fails to adapt in Stage 2, resulting in accuracy drops when switching to discrete selection.

### Mechanism 3
- Claim: LTE's structured sparsity enables efficient custom kernels that reduce wall-clock latency.
- Mechanism: By grouping neurons into experts and selecting subsets, LTE creates column/row sparsity patterns in weight matrices that custom kernels can exploit for coalesced memory access.
- Core assumption: The structured sparsity from LTE maps to actual latency reduction on GPUs when using custom kernels.
- Evidence anchors:
  - [section] "As illustrated in Figure 4, same as Deja Vu [21] and moefication [49], LTE provides very structured sparsity... our custom Triton kernel effectively translates the LTE sparsity to wall-clock time speed up."
  - [section] "LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity."
  - [corpus] "Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models" - suggests structured sparsity can be leveraged for efficiency.
- Break condition: If the custom kernel implementation doesn't achieve the expected speedup, or if unstructured sparsity from other methods proves equally efficient with simpler implementation.

## Foundational Learning

- Concept: Mixture of Experts (MoE)
  - Why needed here: LTE converts FFN layers to MoE layers by grouping neurons into experts and using routers to select experts. Understanding MoE is crucial for grasping LTE's architecture.
  - Quick check question: What is the main advantage of using MoE layers over dense FFN layers in transformers?

- Concept: Activation Sparsity
  - Why needed here: LTE exploits and enhances activation sparsity in FFN layers to improve inference efficiency. Knowing what activation sparsity is and how it arises is key to understanding LTE's motivation.
  - Quick check question: Why do ReLU-based models typically exhibit higher activation sparsity than models with soft activations like GeLU?

- Concept: Router Training in MoE
  - Why needed here: LTE introduces a novel two-stage training algorithm to stabilize router learning. Understanding the challenges of router training in MoE is essential for grasping LTE's innovations.
  - Quick check question: What are the main challenges in training routers for MoE layers, and how does LTE address them?

## Architecture Onboarding

- Component map: Input → Self-Attention Block → FFN Layer (MoEfied: Expert grouping → Router → Expert selection → Weighted sum of selected experts) → Output
- Critical path: Token → Self-Attention → FFN (with expert selection) → Next layer
- Design tradeoffs:
  - Expert grouping strategy: More experts → finer-grained sparsity but higher router overhead
  - Router selection method: Fixed k vs. threshold-based → simplicity vs. adaptability
  - Training stages: Joint training vs. separate → stability vs. flexibility
- Failure signatures:
  - Accuracy drops after MoEfication: Router not trained properly or threshold too restrictive
  - No latency improvement: Custom kernel not optimized or sparsity not structured enough
  - Memory issues: Too many experts or router overhead too high
- First 3 experiments:
  1. Implement LTE on a small model (e.g., GPT2-small) with fixed k expert selection to verify basic functionality.
  2. Compare LTE with baseline MoEfication on a held-out dataset to measure accuracy impact.
  3. Profile LTE's wall-clock latency with the custom kernel on a GPU to verify speedup claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the trade-off between sparsity and model quality differ across various activation functions in LLMs beyond ReLU, GeLU, and SwiGLU?
- Basis in paper: [explicit] The paper mentions that existing methods focus on ReLU-based models and that LTE can be applied to LLMs with non-ReLU activations like SwiGLU in LLaMA and GeGLU in Gemma.
- Why unresolved: The paper does not provide a comprehensive comparison of sparsity and model quality across different activation functions, leaving open the question of how LTE performs with other activation functions.
- What evidence would resolve it: Comparative studies evaluating LTE's performance with various activation functions on a range of tasks would provide insights into the trade-offs between sparsity and model quality.

### Open Question 2
- Question: What is the impact of training LTE on larger datasets, such as pretraining or RLHF data, on its performance in instruction tuning tasks?
- Basis in paper: [inferred] The paper suggests that training with a wider variety of data, like pretraining data, could further improve LTE performance, especially in instruction tuning tasks.
- Why unresolved: The paper does not explore the effects of training LTE on larger datasets, which could potentially enhance its generalization capabilities and performance in instruction tuning.
- What evidence would resolve it: Experiments training LTE on pretraining or RLHF datasets and evaluating its performance on instruction tuning tasks would provide insights into the potential benefits of larger-scale training.

### Open Question 3
- Question: How does the efficiency loss penalty hyperparameter η influence the sparsity and performance trade-off in LTE across different models and tasks?
- Basis in paper: [explicit] The paper discusses the role of the efficiency loss penalty hyperparameter η in controlling the trade-off between inference efficiency and task performance, noting that a higher η leads to a more sparse model.
- Why unresolved: While the paper mentions the impact of η on sparsity, it does not provide a detailed analysis of how this hyperparameter affects the sparsity-performance trade-off across various models and tasks.
- What evidence would resolve it: A systematic study varying η across different models and tasks, with detailed analysis of the resulting sparsity and performance, would clarify the influence of this hyperparameter on LTE's effectiveness.

## Limitations

- Scalability concerns: Method shows 25% latency reduction on LLaMA2-7B but doesn't thoroughly address how gains scale to larger models (70B+ parameters)
- Hardware specificity: Reported speedups rely on custom Triton kernels without fully characterizing performance across different GPU architectures
- Limited domain evaluation: Evaluation focuses on language tasks without exploring multimodal LLMs or domain-specific models (medical, scientific)

## Confidence

**High Confidence Claims**:
- LTE algorithm successfully trains models to achieve higher activation sparsity than baseline methods
- Two-stage training approach effectively stabilizes router learning
- Structured sparsity from LTE maps to actual latency reduction with custom kernels

**Medium Confidence Claims**:
- Trade-off between sparsity and model performance is optimal across all tested scenarios
- Efficiency loss penalty doesn't introduce bias in router selection that could affect downstream task performance
- Parameter clustering approach for expert grouping is optimal for all model architectures

**Low Confidence Claims**:
- LTE will maintain similar performance benefits when applied to models outside tested architectures
- Custom kernel implementation will be portable and equally effective across different hardware vendors
- Method's benefits will remain consistent as model sizes scale to frontier LLMs (e.g., GPT-4 class models)

## Next Checks

1. **Architecture generalization test**: Apply LTE to a diverse set of model architectures including Vision Transformers and multimodal models, then evaluate whether the learned sparsity patterns and router mechanisms transfer effectively beyond pure language models.

2. **Hardware portability assessment**: Implement and benchmark LTE's structured sparsity pattern on at least two different hardware platforms (e.g., NVIDIA A100 and AMD MI300X) using both custom kernels and standard cuSPARSE implementations to quantify hardware-specific benefits and limitations.

3. **Scaling analysis**: Systematically evaluate LTE across a spectrum of model sizes (from 1B to 70B parameters) while measuring not just latency and accuracy, but also memory consumption and router overhead to identify the optimal model size range where LTE provides maximum benefit.