---
ver: rpa2
title: Exploring Information-Theoretic Metrics Associated with Neural Collapse in
  Supervised Training
arxiv_id: '2409.16767'
source_url: https://arxiv.org/abs/2409.16767
tags:
- learning
- matrix
- entropy
- information
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies matrix entropy and mutual information metrics
  to analyze supervised learning dynamics. It introduces Cross-Model Alignment (CMA)
  loss for fine-tuning cross-modal models and proposes Matrix Mutual Information Ratio
  (MIR) and Matrix Entropy Difference Ratio (HDR) to characterize information interplay
  during training.
---

# Exploring Information-Theoretic Metrics Associated with Neural Collapse in Supervised Training

## Quick Facts
- arXiv ID: 2409.16767
- Source URL: https://arxiv.org/abs/2409.16767
- Reference count: 0
- Primary result: Introduces MIR and HDR metrics to characterize Neural Collapse dynamics, proposes CMA loss for cross-modal alignment, and shows these metrics serve as effective regularization terms

## Executive Summary
This paper introduces information-theoretic metrics to analyze supervised learning dynamics, particularly focusing on Neural Collapse phenomena. The authors propose Matrix Mutual Information Ratio (MIR) and Matrix Entropy Difference Ratio (HDR) as novel metrics that complement traditional matrix entropy calculations. They also introduce Cross-Model Alignment (CMA) loss for fine-tuning cross-modal models and demonstrate that MIR and HDR can serve as effective regularization terms in both supervised and semi-supervised learning scenarios. Theoretical analysis provides optimal values for these metrics under Neural Collapse conditions.

## Method Summary
The method centers on computing information-theoretic metrics from Gram matrices of normalized features and classifier weights. Matrix entropy is calculated from Gram matrix eigenvalues, while MIR and HDR provide normalized measures of mutual information and entropy difference respectively. The CMA loss minimizes matrix entropy within classes across modalities by collecting representations from different modalities and optimizing their joint entropy. The approach is validated through standard supervised learning experiments on CIFAR datasets, semi-supervised learning with FreeMatch framework, and cross-modal fine-tuning using CLIP models on 11 additional image classification datasets.

## Key Results
- MIR and HDR metrics effectively describe Neural Collapse phenomena and serve as regularization terms that improve both supervised and semi-supervised learning performance
- CMA loss improves cross-modal alignment performance, particularly effective for datasets with fewer categories but degrading on large datasets like ImageNet
- Theoretical analysis establishes optimal values for MIR and HDR under Neural Collapse conditions, with HDR approaching 0 and MIR approaching (1/(C-1)) + ((C-2)log(C-2))/((C-1)log(C-1))

## Why This Works (Mechanism)

### Mechanism 1
Matrix entropy captures intra-class feature similarity and correlates with clustering performance. Matrix entropy is calculated from the Gram matrix of normalized features, where lower entropy indicates higher similarity among representations, translating to better clustering within classes.

### Mechanism 2
MIR and HDR provide complementary metrics that better characterize information interplay during Neural Collapse. MIR measures normalized mutual information between data representations and classification weights, while HDR measures normalized entropy difference, together capturing alignment dynamics that matrix entropy alone misses.

### Mechanism 3
CMA loss improves cross-modal alignment by minimizing matrix entropy within classes across modalities. For each class, CMA collects representations from different modalities and minimizes their joint matrix entropy, encouraging representations from different modalities to align.

## Foundational Learning

- Concept: Matrix entropy and Gram matrix construction
  - Why needed here: Core metric for measuring information content and similarity in feature representations
  - Quick check question: How do you construct a Gram matrix from a set of normalized features?

- Concept: Neural Collapse theory and its conditions
  - Why needed here: Provides the theoretical framework for understanding the training dynamics being measured
  - Quick check question: What are the three main conditions of Neural Collapse?

- Concept: Mutual information and entropy difference ratios
  - Why needed here: Advanced metrics for characterizing the interaction between data representations and classification weights
  - Quick check question: How do MIR and HDR normalize mutual information and entropy difference respectively?

## Architecture Onboarding

- Component map: Feature extractor → Gram matrix construction → Matrix entropy/MIR/HDR calculation → Loss optimization
- Critical path: Feature extraction → Gram matrix computation → Metric calculation → Loss computation → Backpropagation
- Design tradeoffs: Matrix entropy vs. MIR/HDR (simpler vs. more comprehensive metrics), computational cost vs. accuracy
- Failure signatures: Metrics not converging, metrics not correlating with expected behavior, poor training performance despite good metric values
- First 3 experiments:
  1. Verify matrix entropy calculation on synthetic data with known clustering properties
  2. Test MIR and HDR behavior on data approaching Neural Collapse
  3. Implement CMA loss and validate cross-modal alignment on a simple multimodal dataset

## Open Questions the Paper Calls Out

### Open Question 1
How do MIR and HDR metrics behave in intermediate training regimes between early convergence and Neural Collapse, and can they predict when a model will reach Neural Collapse? The paper shows MIR increases and HDR decreases toward Neural Collapse but does not explore intermediate dynamics or predictive capabilities.

### Open Question 2
What is the optimal balance between cross-entropy loss and CMA loss weights for different numbers of classes and dataset complexities? The paper finds CMA works best for datasets with fewer categories but degrades performance on large datasets like ImageNet.

### Open Question 3
Can MIR and HDR be used as effective early stopping criteria for supervised learning, potentially reducing training time while maintaining performance? The paper demonstrates MIR and HDR track training progress toward Neural Collapse but doesn't explore their utility for stopping criteria.

## Limitations

- The paper lacks empirical validation for MIR and HDR metrics on diverse datasets beyond standard benchmarks
- Theoretical guarantees assume ideal Neural Collapse conditions that may not hold in practical scenarios with noisy or imbalanced data
- CMA loss implementation details are insufficiently specified, making exact reproduction challenging

## Confidence

- Matrix Entropy Correlation with Clustering: Medium confidence
- MIR and HDR as Effective Regularization: Low confidence
- CMA Loss for Cross-Modal Alignment: Medium confidence
- Theoretical Bounds for Neural Collapse Metrics: High confidence

## Next Checks

1. Evaluate MIR and HDR metrics on non-standard datasets (medical imaging, satellite imagery) to verify they capture meaningful information dynamics beyond CIFAR-like data distributions.

2. Systematically vary batch sizes during training and measure the stability of matrix entropy, MIR, and HDR values to identify the minimum batch size for reliable metric computation.

3. Replace L2 normalization with batch normalization or layer normalization in the feature extraction pipeline and assess how this affects the convergence of MIR and HDR metrics during training.