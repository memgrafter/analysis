---
ver: rpa2
title: 'FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs'
arxiv_id: '2410.16663'
source_url: https://arxiv.org/abs/2410.16663
tags:
- fastattention
- gpus
- matrix
- npus
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FastAttention, a new approach to extend FlashAttention2
  to NPUs and low-resource GPUs, addressing the limitations of existing FlashAttention
  methods. FastAttention introduces several strategies including a two-level tiling
  strategy, tiling-mask strategy, and tiling-AllReduce strategy for NPUs, and redesigns
  data layouts for low-resource GPUs.
---

# FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs

## Quick Facts
- arXiv ID: 2410.16663
- Source URL: https://arxiv.org/abs/2410.16663
- Authors: Haoran Lin, Xianzhi Yu, Kang Zhao, Lu Hou, Zongyuan Zhan, Stanislav Kamenev, Han Bao, Ting Hu, Mingkai Wang, Qixin Chang, Siyue Sui, Weihao Sun, Jiaxin Hu, Jun Yao, Zekun Yin, Cheng Qian, Ying Zhang, Yinfei Pan, Yu Yang, Weiguo Liu
- Reference count: 40
- Primary result: FastAttention achieves up to 10.7× speedup on Ascend NPUs and supports ultra-long sequences up to 256K tokens on V100 GPUs

## Executive Summary
FastAttention extends FlashAttention2 to Neural Processing Units (NPUs) and low-resource GPUs by introducing architecture-specific optimizations. The method addresses the limitations of existing FlashAttention approaches through three key strategies for NPUs: two-level tiling to reduce synchronization overhead, tiling-mask to eliminate large attention_mask storage, and tiling-AllReduce to overlap communication with computation. For low-resource GPUs like V100, FastAttention redesigns data layouts to accommodate different MMA instruction configurations and introduces a CPU-GPU cooperative strategy for ultra-long sequences.

## Method Summary
FastAttention introduces specialized optimizations for NPUs and low-resource GPUs to improve attention mechanism performance. For NPUs, it implements a two-level tiling strategy that uses larger blocks at the first level to reduce synchronization frequency and smaller blocks at the second level to fit memory constraints, while double-buffering overlaps data transfer and computation. The tiling-mask strategy generates a small M-mask matrix that can be shifted to produce required attention blocks on-the-fly, eliminating the need to store full attention_mask matrices. For multi-NPU scenarios, the tiling-AllReduce strategy splits AllReduce operations into multiple B-allreduce operations per block, overlapped with block computations using SDMA support. For V100 GPUs, FastAttention redesigns data layouts to accommodate m8n8k4 MMA instructions and introduces a CPU-GPU cooperative strategy for handling ultra-long sequences through intelligent memory management and task partitioning.

## Key Results
- On Ascend NPUs, FastAttention achieves up to 10.7× speedup over standard attention and up to 5.16× higher throughput
- On V100 GPUs, FastAttention provides up to 1.43× speedup over xformers' FlashAttention implementation
- FastAttention supports ultra-long sequences up to 256K tokens with 1.46× end-to-end speedup on 8 V100 GPUs
- The method demonstrates significant performance improvements across multiple models including PanGu-38B, PanGu-71B, LLaMA2-7B, and LLaMA2-70B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-level tiling reduces synchronization overhead and improves Cube unit utilization.
- Mechanism: First level uses larger blocks to reduce synchronization frequency; second level refines with smaller blocks to fit L0/L1 buffer constraints while double-buffering overlaps data transfer and computation.
- Core assumption: Larger first-level blocks allow the Cube unit to load more continuous data, better utilizing memory bandwidth, and that Vector-Cube synchronization is the main bottleneck.
- Evidence anchors:
  - [abstract] "two-level tiling strategy for runtime speedup"
  - [section 4.1] "The unified tiling strategy employs small block size, leading to the frequent data flow between Cube unit and Vector unit via L2 buffer, which in turn introduces significant synchronization overhead."

### Mechanism 2
- Claim: Tiling-mask strategy eliminates need to store full attention_mask matrix.
- Mechanism: A small M-mask matrix is generated and shifted to produce required B-mask blocks on-the-fly, skipping blocks where mask is all-0 or all-1.
- Core assumption: Attention_mask is lower-triangular and can be generated from a small M-mask; block size b is less than M.
- Evidence anchors:
  - [abstract] "tiling-mask strategy for memory saving"
  - [section 4.1] "Specifically, we implement an attention_mask generator that uses a small mask matrix with the dimensions of (2 ∗ M) ∗ (2 ∗ M) to substitute the complete attention_mask matrix"

### Mechanism 3
- Claim: Tiling-AllReduce overlaps communication with computation in multi-NPU scenarios.
- Mechanism: AllReduce is split into multiple B-allreduce operations per block, overlapped with block computations using SDMA support; first block given smaller tasks to minimize its impact.
- Core assumption: SDMA enables parallel computation and communication; communication overhead dominates multi-NPU performance.
- Evidence anchors:
  - [abstract] "tiling-AllReduce strategy for reducing communication overhead"
  - [section 4.2] "The B-allreduce operations are overlapped with block calculations to improve performance."

## Foundational Learning

- Concept: GPU vs NPU architectural differences (decoupled vs shared memory hierarchy)
  - Why needed here: FastAttention exploits these differences (e.g., Cube-Vector decoupling in NPUs) to redesign data flow.
  - Quick check question: What are the key architectural differences between Ascend NPUs and GPUs that impact attention optimization?

- Concept: Tiling and memory hierarchy optimization
  - Why needed here: FastAttention's performance gains rely on careful tiling strategies across multiple memory levels (L0, L1, L2, HBM).
  - Quick check question: How does two-level tiling improve performance compared to single-level tiling in attention mechanisms?

- Concept: CUDA MMA instructions and their limitations on V100
  - Why needed here: FastAttention adapts FlashAttention2 to V100 by redesigning data layouts for m8n8k4 instructions.
  - Quick check question: What are the key differences between Ampere's m16n8k16 MMA instructions and V100's m8n8k4 instructions?

## Architecture Onboarding

- Component map: FastAttention consists of attention kernel (with two-level tiling, tiling-mask, tiling-AllReduce for NPUs), data layout redesign for V100 GPUs, and CPU-GPU cooperative strategy for ultra-long sequences.
- Critical path: For NPUs: attention calculation → Linear → tiling-AllReduce; for V100: attention calculation with redesigned MMA instructions; for ultra-long sequences: CPU offload → GPU computation pipeline.
- Design tradeoffs: Memory vs computation tradeoff (tiling-mask saves memory but adds generation overhead); complexity vs performance (two-level tiling adds implementation complexity but improves utilization).
- Failure signatures: Memory allocation failures (tiling-mask issues), synchronization bottlenecks (two-level tiling problems), communication stalls (tiling-AllReduce problems).
- First 3 experiments:
  1. Benchmark operator latency on Ascend NPU with and without two-level tiling strategy across sequence lengths.
  2. Compare memory usage of attention_mask matrix vs M-mask for different sequence lengths.
  3. Measure AllReduce communication overhead with and without tiling-AllReduce strategy on multi-NPU setup.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several important questions remain unaddressed regarding the scalability of FastAttention beyond demonstrated limits, its applicability to training scenarios, and performance across different NPU architectures.

## Limitations
- Evaluation is limited to Ascend NPUs and V100 GPUs, leaving uncertainty about performance on modern GPU architectures where FlashAttention2 is already highly optimized.
- The paper lacks ablation studies isolating the contribution of each individual strategy (two-level tiling, tiling-mask, tiling-AllReduce).
- Memory savings from the tiling-mask strategy are described qualitatively but not quantified with specific numbers.

## Confidence

**High Confidence**: The architectural insights regarding NPU-specific optimizations (two-level tiling, tiling-mask) are well-grounded in the described hardware characteristics of Ascend NPUs. The multi-level tiling strategy and mask generation approach are clearly explained with logical reasoning.

**Medium Confidence**: Performance claims for V100 GPU optimizations are reasonable but limited by the dated hardware platform. The CPU-GPU cooperative strategy for ultra-long sequences is described but lacks detailed performance breakdown between CPU and GPU components.

**Low Confidence**: End-to-end inference speedup claims (1.46× for 256K sequences) lack sufficient context about baseline configurations and may be influenced by implementation-specific factors not fully disclosed.

## Next Checks

1. **Ablation study on NPU**: Measure individual and combined contributions of two-level tiling, tiling-mask, and tiling-AllReduce strategies on operator latency to isolate their respective performance impacts.

2. **Memory profiling validation**: Quantify actual memory savings from tiling-mask strategy across different sequence lengths and verify the claimed elimination of the full attention_mask matrix.

3. **Multi-NPU scaling analysis**: Evaluate strong scaling behavior with tiling-AllReduce across 2-8 NPUs to confirm that communication overhead is truly the bottleneck being addressed and that overlap effectiveness holds at larger scales.