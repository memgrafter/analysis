---
ver: rpa2
title: How well can a large language model explain business processes as perceived
  by users?
arxiv_id: '2401.12846'
source_url: https://arxiv.org/abs/2401.12846
tags:
- process
- explanations
- knowledge
- explanation
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the quality of explanations generated by
  Large Language Models (LLMs) for business process (BP) conditions as perceived by
  users. The study focuses on Situation-Aware eXplainability (SAX), which requires
  explanations to be causally sound and human-interpretable within the BP context.
---

# How well can a large language model explain business processes as perceived by users?

## Quick Facts
- arXiv ID: 2401.12846
- Source URL: https://arxiv.org/abs/2401.12846
- Reference count: 40
- One-line primary result: Providing LLMs with additional knowledge about process and causal dependencies improves perceived fidelity of explanations for business process conditions, but at the cost of interpretability.

## Executive Summary
This paper investigates the quality of explanations generated by Large Language Models (LLMs) for business process (BP) conditions as perceived by users. The study focuses on Situation-Aware eXplainability (SAX), which requires explanations to be causally sound and human-interpretable within the BP context. To address the limitations of LLMs in causal reasoning and tendency to hallucinate, the authors develop the SAX4BPM framework, which synthesizes various knowledge ingredients (process, causal, and XAI) to inform LLM prompts. A user study with 49 participants was conducted across three BP domains (pizza delivery, parking fines, and loan approval), using a newly developed scale to assess explanation quality. The results show that providing LLMs with additional knowledge about process and causal dependencies improves perceived fidelity of explanations, but this comes at the cost of interpretability. The improvement in fidelity is moderated by user trust in the LLM and curiosity about the problem. The study highlights a trade-off between fidelity and interpretability in LLM-generated BP explanations and identifies trust and curiosity as key factors influencing perceived quality.

## Method Summary
The study used the SAX4BPM framework to generate LLM explanations for business process conditions across three domains. The framework synthesizes process mining, causal discovery (LiNGAM), and XAI techniques to create knowledge ingredients that inform LLM prompts. A controlled experiment with 49 participants rated LLM explanations on a 24-item scale measuring fidelity and interpretability, while controlling for trust and curiosity as covariates. The study compared explanations generated with different combinations of knowledge ingredients (process+XAI vs XAI only; process+XAI+causal vs process+XAI; process+causal vs process only).

## Key Results
- Adding process and causal knowledge to LLM prompts improves perceived fidelity of explanations
- The improvement in fidelity comes at the cost of perceived interpretability
- User trust and curiosity moderate the perceived quality of explanations, with trust having a significant direct effect on both fidelity and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating additional knowledge ingredients (process, causal, XAI) into LLM prompts improves perceived fidelity of generated explanations for business process conditions.
- Mechanism: The SAX4BPM framework synthesizes multiple views (process mining, causal discovery, XAI feature importance) into a knowledge graph, then constructs structured prompts that guide the LLM to produce explanations grounded in both the execution flow and causal dependencies of the process, rather than relying solely on its pre-trained language patterns.
- Core assumption: The LLM can effectively integrate structured, domain-specific knowledge when presented in a coherent prompt format.
- Evidence anchors:
  - [abstract] "our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity."
  - [section 5.4] "The core functionality of the service is the synthesis of the input ingredients by invoking the corresponding SAX4BPM services and their textual streamlining via the LLM prompt toward the eventual elicitation of the explanation narrative."
  - [corpus] Weak - no direct neighbor evidence about multi-view prompt synthesis improving fidelity.
- Break condition: If the LLM cannot properly parse or integrate the structured knowledge, or if the synthesized prompt becomes too complex for effective generation.

### Mechanism 2
- Claim: Adding causal execution dependencies as a knowledge ingredient to LLM prompts improves the perceived completeness and soundness of explanations compared to using only process or XAI knowledge.
- Mechanism: Causal discovery (via LiNGAM) identifies true cause-effect relationships among process activities, which when included in the prompt, enables the LLM to generate explanations that reflect the underlying causal chains rather than just temporal precedence, leading to more accurate reasoning about why certain conditions occurred.
- Core assumption: The causal discovery algorithm accurately captures the true causal structure of the process, and the LLM can reason over these causal relationships.
- Evidence anchors:
  - [section 2] "Our previous work [10] demonstrated that relying only on time precedence between activities in a BP does not necessarily reflect the full cause-effect dependencies among the tasks and a more fundamental analysis of causal relationships among the tasks is required."
  - [section 6.2] "The generated text that serves as an input for the LLM prompt adheres to the template: {'Cause':(a:activity.unique-name), 'Effect':(b:activity.unique-name), 'Coefficient': coefficient(a.causes(b)) }"
  - [corpus] Weak - no direct neighbor evidence about causal knowledge improving explanation soundness.
- Break condition: If the causal discovery is inaccurate, or if the LLM treats causal relationships as mere correlations.

### Mechanism 3
- Claim: The perceived quality of LLM-generated explanations is moderated by user trust in the LLM and curiosity about the problem, with trust having a direct effect on both fidelity and interpretability perceptions.
- Mechanism: Users' pre-existing attitudes (trust and curiosity) act as psychological moderators that influence how they evaluate the explanations' quality, potentially amplifying or dampening the perceived benefits of the knowledge-enhanced prompts.
- Core assumption: User perceptions of explanation quality are not purely objective but are influenced by their subjective attitudes toward the explanation source and problem domain.
- Evidence anchors:
  - [abstract] "This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation."
  - [section 9.3.1] "Running the ANCOVA analysis also revealed that trust has a significant direct effect on both dependent variables: on fidelity, F(1,46) = 17.79, p < .001, η² = .29, and on interpretability, F(1,46) = 14.67, p < .001, η² = .25."
  - [corpus] Weak - no direct neighbor evidence about trust/curiosity moderating explanation quality.
- Break condition: If user trust is uniformly low, or if curiosity does not vary meaningfully across users.

## Foundational Learning

- Concept: Knowledge Graph (KG) as data model
  - Why needed here: The KG integrates multiple process-related views (process, causal, XAI) in a unified, queryable structure that can be efficiently synthesized into LLM prompts.
  - Quick check question: What are the three main types of nodes/relationships inferred in the KG schema for process views?

- Concept: Causal discovery vs. association mining
  - Why needed here: Causal discovery identifies true cause-effect relationships among process activities, which is essential for generating explanations that reflect why conditions occur, not just when they occur.
  - Quick check question: How does the LiNGAM algorithm differ from traditional process mining approaches in identifying dependencies?

- Concept: Prompt engineering for LLMs
  - Why needed here: Effective prompt engineering is crucial for guiding the LLM to synthesize and articulate the various knowledge ingredients into coherent, domain-specific explanations.
  - Quick check question: What are the key components that should be included in the blended prompt for SAX explanations?

## Architecture Onboarding

- Component map: SAX4BPM library (Process Discovery, Causal Process Discovery, XAI, CEP Enrich/Filter, Explanation Synthesize) → Knowledge Graph (Neo4j) → LLM (GPT4.0) → User-facing explanations
- Critical path: Event log → Knowledge Graph enrichment → Service invocation (Mining4Process, Causal4Process, X4Process) → NLP4X synthesis → LLM prompt → Explanation generation
- Design tradeoffs: Using a pre-trained LLM (no fine-tuning) for generality vs. potentially lower task-specific accuracy; knowledge graph complexity vs. prompt clarity; user study scale vs. generalizability
- Failure signatures: LLM generates generic/non-specific explanations; explanations contradict ground truth; users rate explanations poorly on fidelity/interpretability despite knowledge input
- First 3 experiments:
  1. Run the full SAX4BPM pipeline on a simple process log and inspect the generated LLM prompt for coherence and completeness
  2. Compare explanations generated with and without causal knowledge input on a known process to assess fidelity differences
  3. Conduct a small-scale user study (n=10) to validate the scale items and check for ceiling/floor effects in ratings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs balance fidelity and interpretability in business process explanations?
- Basis in paper: [explicit] The paper highlights a trade-off between fidelity and interpretability in LLM-generated BP explanations.
- Why unresolved: The study shows that increasing fidelity by providing LLMs with additional knowledge can decrease interpretability, but does not explore how to optimize this balance.
- What evidence would resolve it: Empirical testing of various prompt engineering techniques or LLM configurations that aim to optimize both fidelity and interpretability simultaneously.

### Open Question 2
- Question: How does the perceived quality of LLM-generated explanations vary across different business process domains?
- Basis in paper: [inferred] The study examines three different BP domains (pizza delivery, parking fines, and loan approval) but does not provide a comprehensive comparison of perceived quality across these domains.
- Why unresolved: The paper identifies domain-specific effects on fidelity and interpretability but does not systematically compare the overall perceived quality of explanations across all domains.
- What evidence would resolve it: A comparative analysis of user ratings for explanations generated across multiple BP domains, controlling for other factors like trust and curiosity.

### Open Question 3
- Question: Can LLM-generated explanations be further improved by incorporating additional knowledge perspectives beyond process, causal, and XAI?
- Basis in paper: [explicit] The study focuses on these three knowledge ingredients but acknowledges the potential for incorporating other perspectives.
- Why unresolved: The paper demonstrates the effectiveness of combining process, causal, and XAI knowledge but does not explore the impact of adding other types of knowledge.
- What evidence would resolve it: Empirical testing of LLM-generated explanations that incorporate additional knowledge perspectives, such as user feedback, historical data, or external context, and comparing their perceived quality to the current approach.

## Limitations
- The study's findings are based on a relatively small user sample (n=49) across three process domains, which may limit generalizability.
- The paper does not provide sufficient detail on the implementation of the SAX4BPM library services, particularly the integration of causal discovery with LiNGAM.
- The study focuses on pre-trained LLMs without fine-tuning, which may affect task-specific performance.

## Confidence
- **High confidence**: The trade-off between fidelity and interpretability in LLM-generated BP explanations is well-supported by user study results
- **Medium confidence**: The moderating effects of user trust and curiosity on perceived explanation quality, given the statistical analysis showing significant direct effects
- **Low confidence**: The exact mechanism by which causal knowledge improves explanation soundness, due to limited evidence on causal discovery accuracy

## Next Checks
1. Replicate the user study with a larger sample size (n>100) across more diverse process domains to test generalizability of the fidelity-interpretability trade-off
2. Implement the SAX4BPM library with detailed logging of causal discovery accuracy to verify that improved fidelity correlates with correct causal identification
3. Conduct a comparative study using fine-tuned LLMs versus pre-trained models to assess whether task-specific training improves the balance between fidelity and interpretability