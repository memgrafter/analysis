---
ver: rpa2
title: Learning Infinite-Horizon Average-Reward Linear Mixture MDPs of Bounded Span
arxiv_id: '2410.14992'
source_url: https://arxiv.org/abs/2410.14992
tags:
- learning
- lemma
- regret
- where
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies infinite-horizon average-reward reinforcement
  learning in linear mixture MDPs under the Bellman optimality condition. The authors
  propose UCLK-C, an algorithm that runs discounted extended value iteration with
  clipping to control the span of intermediate value functions.
---

# Learning Infinite-Horizon Average-Reward Linear Mixture MDPs of Bounded Span

## Quick Facts
- arXiv ID: 2410.14992
- Source URL: https://arxiv.org/abs/2410.14992
- Reference count: 40
- Key outcome: Achieves regret upper bound of $\widetilde{O}(d\sqrt{\mathrm{sp}(v^*)T})$ and matches new lower bound of $\Omega(d\sqrt{\mathrm{sp}(v^*)T})$ for weakly communicating MDPs of bounded span

## Executive Summary
This paper studies infinite-horizon average-reward reinforcement learning in linear mixture MDPs under the Bellman optimality condition. The authors propose UCLK-C, an algorithm that runs discounted extended value iteration with clipping to control the span of intermediate value functions. By leveraging weighted ridge regression with variance estimation and Bernstein-type confidence bounds, UCLK-C achieves a regret upper bound of $\widetilde{O}(d\sqrt{\mathrm{sp}(v^*)T})$ where $d$ is the feature dimension, $\mathrm{sp}(v^*)$ is the span of the optimal bias function, and $T$ is the horizon. This matches a new regret lower bound of $\Omega(d\sqrt{\mathrm{sp}(v^*)T})$ and thus establishes near minimax optimality for the class of weakly communicating MDPs of bounded span. The clipping operation is key to convergence of value iteration and bounding the variance term under random transitions.

## Method Summary
UCLK-C runs discounted extended value iteration with clipping to control the span of intermediate value functions in infinite-horizon average-reward linear mixture MDPs. The algorithm uses weighted ridge regression with variance estimation and Bernstein-type confidence bounds to construct optimistic parameter estimates. Each episode consists of a planning phase (discounted value iteration with clipping) followed by an execution phase (greedy policy based on the computed value function). The clipping threshold H is chosen as an upper bound on 2·sp(v*), ensuring convergence while maintaining theoretical guarantees.

## Key Results
- Achieves regret upper bound of $\widetilde{O}(d\sqrt{\mathrm{sp}(v^*)T})$ for linear mixture MDPs
- Establishes new regret lower bound of $\Omega(d\sqrt{\mathrm{sp}(v^*)T})$ for weakly communicating MDPs of bounded span
- Demonstrates near minimax optimality through matching upper and lower bounds
- Shows improved empirical performance over prior approaches like UCRL2-VTR

## Why This Works (Mechanism)

### Mechanism 1: Clipping operation stabilizes span
- **Claim**: The clipping operation in UCLK-C stabilizes the span of intermediate value functions, enabling convergence of discounted extended value iteration
- **Mechanism**: By capping the span of each value function to H (where H ≥ 2 · sp(v*)), the algorithm ensures that Bellman backups remain contractive in the span norm, preventing divergence due to unbounded value growth
- **Core assumption**: The clipping threshold H is chosen as an upper bound on 2 · sp(v*), the span of the optimal bias function
- **Evidence anchors**: Abstract states algorithm applies value iteration with clipping by span; section shows clipping bounds span at V(n) ≤ H
- **Break condition**: If H < 2 · sp(v*), clipping may truncate necessary components of value function, leading to suboptimal policies

### Mechanism 2: Variance-aware weighted ridge regression
- **Claim**: Weighted ridge regression with Bernstein-type confidence bounds provides tight parameter estimates under clipping
- **Mechanism**: The algorithm constructs confidence ellipsoids Ck using variance-aware weighted ridge regression, where weights depend on estimated conditional variances [VWk]. This ensures optimism while accounting for exploration-exploitation tradeoffs
- **Core assumption**: The variance estimator Et is an accurate upper bound on |[VWk](st, at) − [VtWk](st, at)|
- **Evidence anchors**: Section describes taking upper bound Et on error term and using weighted ridge regression for confidence sets
- **Break condition**: If variance estimator Et is too loose, confidence set Ck becomes too large, inflating regret

### Mechanism 3: Discounting bridges average and discounted MDPs
- **Claim**: Discounting with γ = 1 − √(d/(HT)) bridges average-reward and discounted MDPs, controlling the bias term
- **Mechanism**: By approximating average-reward MDP with discounted MDP, the algorithm inherits convergence properties of value iteration while bounding bias term (1 − γ)sp(v*) ≤ d√(sp(v*)T)
- **Core assumption**: Optimal bias function v* has bounded span, enabling tight connection between average and discounted returns
- **Evidence anchors**: Abstract mentions achieving Õ(d√(sp(v*)T)); section shows I1 ≤ T(1 − γ)sp(v*) ≤ d√(sp(v*)T)
- **Break condition**: If sp(v*) is large relative to d and T, bias term may dominate, degrading performance

## Foundational Learning

- **Concept: Bellman optimality condition**
  - Why needed here: The paper assumes existence of J*, v*, and q* satisfying Bellman equation, which characterizes optimal policy and enables span-based analysis
  - Quick check question: What is the relationship between optimal bias function v* and span sp(v*) in weakly communicating MDPs?

- **Concept: Linear mixture MDPs**
  - Why needed here: Transition probability is linear in feature maps, enabling efficient parameter estimation via ridge regression and Bernstein bounds
  - Quick check question: How does linear structure of P(s'|s,a) = ⟨ϕ(s,a,s'), θ*⟩ simplify computation of ⟨ϕF(s,a), θ*⟩ for any function F?

- **Concept: Span of a function**
  - Why needed here: Span sp(v*) captures maximum variation of optimal bias function and directly influences regret lower bound
  - Quick check question: Why is diameter D an upper bound on sp(v*), but potentially much larger in weakly communicating MDPs?

## Architecture Onboarding

- **Component map**: UCLK-C consists of discounted extended value iteration loop (with clipping) → weighted ridge regression estimator for θ* → Gram matrix update mechanism
- **Critical path**: 1) Update Gram matrix and confidence set Ck. 2) Run N rounds of discounted value iteration with clipping. 3) Execute greedy policy based on Vk. 4) Collect transition data and update estimators
- **Design tradeoffs**: Clipping ensures convergence but may introduce bias if H is too small; weighted ridge regression provides optimism but requires accurate variance estimation; discounting simplifies analysis but introduces bias term proportional to (1 − γ)sp(v*)
- **Failure signatures**: 1) Regret grows superlinearly if clipping threshold H is too small. 2) Excessive optimism (large Ck) leads to over-exploration and high regret. 3) Poor variance estimation causes underestimation of uncertainty, leading to exploitation of suboptimal actions
- **First 3 experiments**:
  1. Implement clipping operation and verify sp(V(n)) ≤ H for all n in simple 2-state MDP
  2. Test weighted ridge regression estimator on synthetic linear mixture MDP and check θ* ∈ Ck with high probability
  3. Run UCLK-C on small MDP instance and compare regret to UCRL2-VTR to confirm benefit of span control

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can discounted extended value iteration with clipping be proven to converge for other function approximation classes beyond linear mixture MDPs?
- Basis in paper: Authors note convergence for linear mixture MDPs but contrast with linear MDPs where convergence is not guaranteed due to clipping
- Why unresolved: Paper provides proof for linear mixture MDPs but doesn't explore whether similar convergence guarantees can be established for other function approximation frameworks
- What evidence would resolve it: Formal proof demonstrating convergence for linear MDPs or another function approximation class, or counterexample showing divergence

### Open Question 2
- Question: Is clipping threshold H = 2 · sp(v*) necessary for theoretical guarantees, or can tighter bounds be achieved with smaller threshold?
- Basis in paper: Authors state "We choose any upper bound H on 2 · sp(v*)" and use H = 2 · sp(v*) for analysis
- Why unresolved: Paper uses H = 2 · sp(v*) for analysis but doesn't investigate whether this is tightest possible bound or whether smaller thresholds could yield better practical performance
- What evidence would resolve it: Empirical or theoretical analysis showing effect of different clipping thresholds on regret bounds and convergence behavior

### Open Question 3
- Question: Can regret lower bound of Ω(d√(sp(v*)T)) be tightened for specific subclasses of weakly communicating MDPs with bounded span?
- Basis in paper: Authors present lower bound for general weakly communicating MDPs but note diameter can be arbitrarily larger than span
- Why unresolved: Presented lower bound applies to all weakly communicating MDPs of bounded span, but specific subclasses might admit tighter bounds
- What evidence would resolve it: Refined lower bound analysis for specific subclass of weakly communicating MDPs, or algorithm that achieves better regret for such subclasses

## Limitations
- Analysis critically depends on having accurate upper bound H on optimal bias function span, which may be difficult to obtain in practice
- Clipping operation could introduce significant bias if H is misspecified
- Variance estimation relies on potentially loose bounds that may inflate confidence sets and reduce practical efficiency
- Bellman optimality assumption excludes scenarios with suboptimal policies that satisfy Bellman equations but are not globally optimal

## Confidence
- High confidence in algorithmic framework and mathematical derivations
- Medium confidence in theoretical guarantees due to novel combination of techniques and lack of extensive empirical validation
- Medium confidence in practical applicability given dependence on span estimation and variance bounds

## Next Checks
1. Implement sensitivity analysis varying clipping threshold H across multiple orders of magnitude to identify robustness to misspecification
2. Compare empirical variance estimation performance against theoretical bounds to quantify potential looseness in Bernstein-type confidence construction
3. Evaluate algorithm on benchmark suite of linear mixture MDPs with known optimal bias function spans to verify practical relevance of sp(v*) dependence in regret bound