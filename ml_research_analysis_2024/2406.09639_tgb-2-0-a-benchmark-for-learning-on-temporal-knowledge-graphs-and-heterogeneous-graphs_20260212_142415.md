---
ver: rpa2
title: 'TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous
  Graphs'
arxiv_id: '2406.09639'
source_url: https://arxiv.org/abs/2406.09639
tags:
- datasets
- dataset
- temporal
- data
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TGB 2.0 introduces a benchmark framework for multi-relational temporal
  graphs, providing eight novel datasets (four TKGs, four THGs) significantly larger
  than existing benchmarks. It addresses evaluation inconsistencies by using a reproducible
  ranking-based protocol with edge-type-aware negative sampling.
---

# TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs

## Quick Facts
- arXiv ID: 2406.09639
- Source URL: https://arxiv.org/abs/2406.09639
- Reference count: 40
- Primary result: Introduces eight large-scale temporal graph datasets and a standardized evaluation protocol for multi-relational temporal graph learning

## Executive Summary
TGB 2.0 is a comprehensive benchmark for learning on temporal knowledge graphs (TKGs) and temporal heterogeneous graphs (THGs). It introduces eight novel datasets—four TKGs and four THGs—spanning five domains and featuring up to 53 million edges, significantly larger than existing benchmarks. The framework addresses evaluation inconsistencies by providing a reproducible, ranking-based protocol with edge-type-aware negative sampling. Experiments reveal that simple heuristic baselines often match or exceed complex methods, most existing approaches fail to scale to the largest datasets, and incorporating edge-type information is crucial for strong performance.

## Method Summary
TGB 2.0 introduces a benchmark framework for multi-relational temporal graphs. The method provides eight novel datasets (four TKGs, four THGs) with up to 53 million edges, spanning five domains. It uses a reproducible ranking-based evaluation protocol with edge-type-aware negative sampling to improve evaluation realism. The benchmark tests various methods including RE-GCN, CEN, TLogic, RecB, EdgeBank, TGN, and STHN, revealing that simple heuristics are often competitive and that most methods fail to scale to the largest datasets.

## Key Results
- Eight novel datasets introduced, significantly larger than existing benchmarks
- Simple heuristic baselines (RecB, EdgeBank) are competitive with learned methods
- Most methods fail to run on the largest datasets due to memory or time constraints
- Edge-type-aware negative sampling improves evaluation discriminative power

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge-type-aware negative sampling improves evaluation realism and discriminative power.
- Mechanism: Negatives are drawn from the same edge-type distribution as the positive query, reducing trivial negatives and aligning with the model's scoring focus.
- Core assumption: Negative examples from different edge types are inherently easier to rank correctly, thus degrading the signal quality for model comparison.
- Evidence anchors:
  - [abstract] "TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs... samples challenging negative edges based on the edge type information"
  - [section] "For the large TKG dataset tkgl-wikidata, we first identify possible tails for each edge type throughout the dataset and then sample the negatives based on the edge type of the query"
  - [corpus] Weak—no direct citation from neighboring works, but aligns with common negative sampling strategies in KG literature.
- Break condition: If edge-type distributions are uniform or highly imbalanced, the sampling heuristic may become less effective or introduce bias.

### Mechanism 2
- Claim: Including simple heuristic baselines (RecB, EdgeBank) provides necessary performance baselines and reveals method limitations.
- Mechanism: Heuristic methods use temporal recurrence or edge bank lookups without model training, offering interpretable performance floors against which to compare learned methods.
- Core assumption: If complex models do not significantly outperform simple heuristics, either the problem is inherently simple or the models are not leveraging relational information effectively.
- Evidence anchors:
  - [abstract] "simple heuristic baselines are often competitive with more complex methods"
  - [section] "Surprisingly, the RecB heuristic performs competitively across most datasets while being among the best performing on tkgl-smallpedia and tkgl-icews"
  - [corpus] Weak—neighbors do not mention baselines, but the practice is standard in ML benchmarking.
- Break condition: If the dataset contains highly non-recurrent or adversarial patterns, heuristics may fail entirely, making comparisons uninformative.

### Mechanism 3
- Claim: Large-scale datasets expose scalability bottlenecks in temporal multi-relational models.
- Mechanism: Datasets with millions of edges and nodes push memory and runtime limits, revealing methods that cannot generalize beyond toy settings.
- Core assumption: Existing methods are primarily optimized for small-scale graphs; scaling requires architectural redesign or approximation strategies.
- Evidence anchors:
  - [abstract] "most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods"
  - [section] "Most methods either are out of memory or out of time for these datasets"
  - [corpus] Weak—no explicit citation, but aligns with OGB and TGB scaling discussions.
- Break condition: If methods use aggressive sampling, streaming, or distributed architectures, they may avoid the observed failures.

## Foundational Learning

- Concept: Temporal graph representation learning
  - Why needed here: All methods encode node/edge evolution over time into vector representations for prediction.
  - Quick check question: How do TGN and STHN differ in how they incorporate time into their embeddings?
- Concept: Multi-relational graph modeling
  - Why needed here: Datasets contain multiple edge types; models must differentiate between them to make accurate predictions.
  - Quick check question: What role do inverse relations play in TKG evaluation, and why are they introduced?
- Concept: Ranking-based evaluation
  - Why needed here: Link prediction is framed as assigning higher scores to true edges than to negative samples.
  - Quick check question: How does the filtered MRR differ from standard MRR, and why is the filter needed?

## Architecture Onboarding

- Component map: Dataset loader → Preprocessing pipeline → Negative sampling generator → Model trainer → Evaluator (MRR) → Leaderboard updater
- Critical path: Preprocessing (chronological split) → Negative sampling (edge-type aware) → Model training → Evaluation (filtered MRR) → Result aggregation
- Design tradeoffs: 1-vs-all sampling is complete but intractable for large graphs; 1-vs-q sampling is scalable but requires careful negative selection to avoid bias
- Failure signatures: OOM → embedding dimension or history length too large; OOT → training loop inefficient or model complexity too high; Low MRR → negative sampling too easy or model underfitting
- First 3 experiments:
  1. Run RecBdefault on tkgl-smallpedia with 1-vs-all evaluation to verify pipeline
  2. Train TGNedge-type on thgl-software with small embedding dimensions to confirm GPU usage limits
  3. Evaluate EdgeBanktw on tkgl-wikidata with 1-vs-q sampling to confirm scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific scaling bottlenecks that prevent current methods from handling the largest TGB 2.0 datasets?
- Basis in paper: [explicit] The paper states that most methods fail to run on the largest datasets due to out of memory or out of time errors.
- Why unresolved: The paper identifies the problem but does not provide a detailed analysis of the computational complexity or memory usage patterns of different methods.
- What evidence would resolve it: Profiling studies showing memory usage and runtime for different methods on datasets of increasing size, identifying the specific operations or data structures causing bottlenecks.

### Open Question 2
- Question: How does the performance of heuristic baselines compare to learned methods across different types of temporal relations (fact-based vs. event-based)?
- Basis in paper: [inferred] The paper observes that heuristic baselines perform competitively, and mentions that datasets have different types of relations with varying recurrency degrees.
- Why unresolved: The paper does not analyze the performance of baselines relative to method types across different relation characteristics.
- What evidence would resolve it: Experiments comparing heuristic and learned method performance on datasets with varying proportions of fact-based vs. event-based relations, or relations with different recurrency characteristics.

### Open Question 3
- Question: What is the impact of different time granularities (yearly vs. daily vs. second-wise) on the performance of temporal graph learning methods?
- Basis in paper: [explicit] The paper discusses that THG datasets use continuous time representation while TKG datasets use discrete snapshots, and shows different edge count patterns over time.
- Why unresolved: The paper does not directly test how time granularity affects method performance or what adaptations might be needed.
- What evidence would resolve it: Experiments training and evaluating methods on the same dataset with different time granularities, or comparing methods designed for different time representations on appropriate datasets.

## Limitations
- The benchmark introduces large-scale datasets, but no direct evidence is provided that these datasets fully capture the diversity and complexity of real-world temporal multi-relational scenarios.
- While the paper reports that simple heuristics are competitive, the underlying reasons (e.g., dataset bias, model capacity) are not fully explored.
- Scalability claims are based on observed failures rather than exhaustive testing of alternative scalable architectures.

## Confidence
- **High Confidence**: The claim that existing methods fail to scale to the largest datasets is well-supported by direct experimental observations.
- **Medium Confidence**: The assertion that edge-type-aware negative sampling improves evaluation realism is supported by the described methodology but lacks comparative ablation studies.
- **Medium Confidence**: The finding that simple heuristics are competitive is directly observed, but deeper analysis of why is limited.

## Next Checks
1. **Scalability Exploration**: Test whether distributed or streaming variants of existing methods (e.g., sampled training, graph partitioning) can handle the largest TGB 2.0 datasets without failure.
2. **Negative Sampling Ablation**: Conduct experiments comparing edge-type-aware negative sampling against random sampling on the same models and datasets to quantify the impact on evaluation metrics.
3. **Dataset Realism Analysis**: Analyze the temporal and relational patterns in TGB 2.0 datasets and compare them to established real-world benchmarks to assess whether the new datasets introduce any biases or simplifications.