---
ver: rpa2
title: 'When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with
  Many Classes'
arxiv_id: '2404.12365'
source_url: https://arxiv.org/abs/2404.12365
tags:
- fastfit
- large
- setfit
- shot
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastFit is a method and Python package for fast and accurate few-shot
  text classification with many classes. It uses batch contrastive learning and token-level
  similarity scores to achieve 3-20x faster training than existing methods while improving
  accuracy on the FewMany benchmark and multilingual datasets.
---

# When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes

## Quick Facts
- arXiv ID: 2404.12365
- Source URL: https://arxiv.org/abs/2404.12365
- Authors: Asaf Yehudai; Elron Bendel
- Reference count: 27
- Key outcome: FastFit achieves 3-20x faster training than existing methods while improving accuracy on few-shot text classification with many classes.

## Executive Summary
FastFit is a method and Python package for fast and accurate few-shot text classification with many classes. It combines batch contrastive learning with token-level similarity scores to achieve significant speed improvements over existing approaches. The method outperforms fine-tuning methods like SetFit and few-shot prompting of large language models in both speed and accuracy on the FewMany benchmark and multilingual datasets. FastFit is available as an open-source package on GitHub and PyPI.

## Method Summary
FastFit uses batch contrastive learning to bring same-class texts closer and push apart different classes in embedding space, while employing token-level similarity metrics to capture fine-grained textual information. The method integrates data augmentation through batch repetition and includes class names as additional samples. It fine-tunes small language models using a contrastive loss function optimized for few-shot settings with many semantically similar classes.

## Key Results
- Achieves 3-20x faster training than existing methods
- Improves accuracy on FewMany benchmark and multilingual datasets
- Outperforms SetFit, traditional fine-tuning, and few-shot prompting of large language models

## Why This Works (Mechanism)

### Mechanism 1
FastFit uses batch contrastive learning to bring same-class texts closer and push apart different classes in embedding space. The loss function optimizes similarity between embeddings of same-class texts while increasing distance between different-class embeddings. Each batch includes augmented examples and class names as additional samples. Core assumption: Contrastive learning with batch negatives is effective for few-shot classification with many classes.

### Mechanism 2
Token-level similarity metrics capture fine-grained textual information better than CLS token representations. Similarity is computed as sum of cosine similarities between tokens of one text and most similar tokens of another text, leveraging dense token representations from the encoder. Core assumption: Fine-grained token-level information improves classification accuracy compared to CLS-level representations.

### Mechanism 3
Data augmentation through batch repetition improves model robustness and performance. Each text in the batch is repeated r times, treating dropout as minimal augmentation at representation level. Core assumption: Augmenting limited training data improves model generalization in few-shot settings.

## Foundational Learning

- Concept: Contrastive learning and its effectiveness for representation learning
  - Why needed here: FastFit's core mechanism relies on contrastive learning to create meaningful class embeddings
  - Quick check question: Can you explain how contrastive loss differs from standard classification loss?

- Concept: Token-level representations and their advantages over CLS-level embeddings
  - Why needed here: FastFit uses token-level similarity metrics which require understanding of token embeddings
  - Quick check question: What information might be lost when using only CLS token representations?

- Concept: Data augmentation techniques in NLP
  - Why needed here: FastFit uses batch repetition as augmentation, understanding augmentation is crucial for implementation
  - Quick check question: What are common data augmentation techniques used in NLP?

## Architecture Onboarding

- Component map: FastFitTrainer -> Model loading/config -> Dataset preprocessing -> Batch contrastive loss -> Token-level similarity -> Inference pipeline

- Critical path: 1) Load dataset and preprocess text 2) Initialize FastFitTrainer with model and parameters 3) Train using batch contrastive loss with token-level similarity 4) Save trained model 5) Load model for inference 6) Classify new texts by finding most similar class names

- Design tradeoffs: Batch size vs. computational cost; Token-level vs. CLS-level similarity; Number of augmentation repetitions

- Failure signatures: Poor performance on semantically similar classes; Slow training convergence; Memory errors during batch contrastive computation; Inconsistent results across different seeds

- First 3 experiments: 1) Train FastFit on a small dataset with default parameters and verify training completes in seconds 2) Compare accuracy of CLS vs. token-level similarity on a validation set 3) Test inference speed and compare with baseline methods on a sample text

## Open Questions the Paper Calls Out

- Question: How does FastFit's performance scale with increasing numbers of classes beyond 150?
  - Basis: The FewMany benchmark includes datasets with up to 150 classes, but the paper doesn't test performance on datasets with significantly more classes

- Question: What is the impact of different data augmentation strategies on FastFit's performance?
  - Basis: The paper mentions using batch repetitions as data augmentation but doesn't extensively explore other augmentation techniques or their relative effectiveness

- Question: How does FastFit perform on imbalanced datasets with significant class imbalance?
  - Basis: The paper focuses on balanced few-shot scenarios but doesn't address how FastFit handles datasets where some classes have many more examples than others

## Limitations

- Lack of detailed experimental comparisons against all relevant baselines with fully specified hyperparameters
- Limited ablation studies to isolate contributions of individual components like batch contrastive learning versus token-level similarity scoring
- Evaluation primarily relies on the FewMany benchmark and multilingual datasets without extensive testing on diverse domains

## Confidence

**High Confidence**: Claims about FastFit's implementation details and the general approach of combining batch contrastive learning with token-level similarity scoring.

**Medium Confidence**: Claims about 3-20x faster training speed compared to existing methods and improved accuracy on the FewMany benchmark and multilingual datasets.

**Low Confidence**: Claims about outperforming all baseline methods across all experimental conditions due to selective reporting and lack of comprehensive ablation studies.

## Next Checks

1. Implement and run FastFit alongside all major baselines (SetFit, traditional fine-tuning, few-shot prompting) on the same hardware with identical evaluation protocols to verify the claimed 3-20x speed improvement and accuracy gains.

2. Conduct controlled experiments removing individual components (batch contrastive learning, token-level similarity, augmentation) to quantify their specific contributions to overall performance.

3. Evaluate FastFit on diverse text classification tasks beyond the FewMany benchmark, including long-form document classification, code classification, and specialized technical domains, to assess its robustness and generalization capabilities.