---
ver: rpa2
title: Argumentative Causal Discovery
arxiv_id: '2405.11250'
source_url: https://arxiv.org/abs/2405.11250
tags:
- causal
- independence
- each
- graph
- abaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ABA-PC, an argumentative approach to causal
  discovery that uses assumption-based argumentation (ABA) to resolve inconsistencies
  in data. The method encodes causal graphs and d-separation as ABA frameworks, then
  uses Answer Set Programming to find all DAGs consistent with given independence
  tests.
---

# Argumentative Causal Discovery

## Quick Facts
- arXiv ID: 2405.11250
- Source URL: https://arxiv.org/abs/2405.11250
- Reference count: 35
- Outperforms baselines on worst-case SID in 3 of 4 benchmark datasets

## Executive Summary
This paper introduces ABA-PC, an argumentative approach to causal discovery that uses assumption-based argumentation (ABA) to resolve inconsistencies in data. The method encodes causal graphs and d-separation as ABA frameworks, then uses Answer Set Programming to find all DAGs consistent with given independence tests. When implemented with Majority-PC independence tests on four standard benchmark datasets, ABA-PC outperforms existing baselines in terms of Structural Intervention Distance, particularly in the worst-case scenario.

## Method Summary
The method combines Majority-PC for independence testing with an ABA framework encoded in ASP. Independence tests are weighted based on p-values and conditioning set size, then iteratively included in the ABAF. Stable extensions of this framework correspond to DAGs consistent with the tests. The approach handles conflicting tests by excluding low-confidence ones until a non-empty extension exists, balancing completeness and feasibility.

## Key Results
- ABA-PC achieves lower normalized SID than all baselines on three out of four datasets
- Performs significantly better than Majority-PC on all datasets
- Shows robust performance in worst-case SID scenarios while maintaining competitive best-case results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The causal ABAF framework encodes d-separation criteria directly into assumption-based argumentation rules.
- Mechanism: Rules in Ract enforce that an independence assumption (x⊥⊥y|Z) can only be accepted if there is no Z-active path between x and y. Conversely, when a dependence fact (x⊥⊥y|Z←) is added, blocked path rules ensure at least one path remains active, preventing false independences.
- Core assumption: The underlying data is faithful to some DAG, so all independencies correspond to d-separation.
- Evidence anchors:
  - [abstract] "We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs."
  - [section] "Proposition 3.11. Let σ ∈ {pr , stb}, S ∈ σ(Dds), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥⊥y | Z) ∈ S iff (x⊥⊥G y | Z)."
  - [corpus] Weak: no direct mention of argumentation encoding d-separation; relies on paper claim.
- Break condition: If data is not faithful to any DAG, the correspondence fails and extensions may include wrong independences.

### Mechanism 2
- Claim: Weighting p-values by both significance level and conditioning set size allows discrimination between true and spurious independence tests.
- Mechanism: The scoring function S(p,α,s,d) normalizes p-values around α and penalizes large conditioning sets, producing higher scores for more reliable tests. Weak constraints encode these scores so ASP prefers extensions consistent with higher-scoring tests.
- Core assumption: Test reliability decreases with conditioning set size and significance level.
- Evidence anchors:
  - [abstract] "Our experiments show that our method compares well against established baselines in Causal Discovery."
  - [section] "We use our final weights S to rank the test carried out by MPC. Then, our strategy is simple: exclude an incremental number of the lowest ranked tests until the returned extension is not empty."
  - [corpus] Weak: no external validation of the weighting heuristic; only internal consistency claim.
- Break condition: If the heuristic misranks tests, excluding wrong ones may remove true independences and bias results.

### Mechanism 3
- Claim: Using stable semantics in the ASP encoding ensures conflict-free extensions that correspond to valid DAGs.
- Mechanism: Stable extensions are ⊆-maximal and conflict-free; this guarantees that accepted assumptions (arrows, independences) form a consistent graph without cycles, as enforced by the acyclicity rules in Πdag.
- Core assumption: The stable extension corresponds uniquely to a DAG (one-to-one mapping under stable semantics).
- Evidence anchors:
  - [abstract] "Each stable extension corresponds to a DAG compatible with the fixed set of independence tests."
  - [section] "Lemma 3.5. σ(Ddag) = τ(Ddag) for σ, τ ∈ {pr , stb}."
  - [corpus] Weak: lemma refers to flat ABAF; non-flat case may not preserve this property without further proof.
- Break condition: If the ABAF is non-flat and the correspondence fails, extensions may not map to valid DAGs.

## Foundational Learning

- Concept: d-separation in DAGs
  - Why needed here: The core of the method is encoding d-separation into argumentation rules to capture causal independences.
  - Quick check question: Given a DAG and a conditioning set Z, how do you determine if two nodes are d-separated?

- Concept: Assumption-based argumentation (ABA) semantics
  - Why needed here: The method relies on stable (or preferred) extensions to extract consistent causal graphs from possibly conflicting independence tests.
  - Quick check question: What is the difference between stable and complete semantics in ABA, and why does the paper choose stable?

- Concept: Answer Set Programming (ASP) grounding and solving
  - Why needed here: The implementation encodes the ABAF into ASP rules and computes stable extensions via clingo.
  - Quick check question: How does the translation from ABA assumptions to ASP literals work in this encoding?

## Architecture Onboarding

- Component map:
  Data source → Majority-PC → independence tests with p-values → Weighting function S(p,α,s,d) → ranks tests → ASP encoder (Πdag, Πcol, Πap, Πbp) → grounded ABAF → clingo solver → stable extensions (DAGs) → Scoring loop → select best DAG by consistency score

- Critical path:
  1. Run MPC to get independence tests.
  2. Apply weighting function to rank tests.
  3. Iteratively encode tests as facts in ABAF.
  4. Ground ABAF in ASP, solve for stable extensions.
  5. Score extensions, return highest-scoring DAG.

- Design tradeoffs:
  - Excluding low-scoring tests to get non-empty extensions trades completeness for feasibility.
  - Using stable semantics guarantees consistency but may discard valid DAGs under other semantics.
  - Encoding all possible paths is exponential; optimization by pruning when edges are fixed.

- Failure signatures:
  - No stable extensions returned → too many conflicting tests.
  - All extensions score low → weighting function not discriminating enough.
  - Output DAGs differ greatly across runs → high variance in independence tests.

- First 3 experiments:
  1. Run ABA-PC on Asia dataset, compare NSID to MPC.
  2. Vary α and observe effect on number of excluded tests.
  3. Test scalability by running on synthetic DAGs of increasing size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the ABA-PC algorithm perform on datasets with more than 10 variables?
- Basis in paper: [inferred] The paper mentions scalability issues, stating "we cannot process more than 10 variables at the current state" and that "the main reasons for this are the complexity of both the grounding of logical rules and the calculation of the extensions."
- Why unresolved: The paper only tested the algorithm on datasets with 5-8 variables, and the scalability issues are mentioned as future work to be addressed.
- What evidence would resolve it: Testing the ABA-PC algorithm on larger datasets with more variables and comparing its performance to other methods would provide evidence of its scalability and potential limitations.

### Open Question 2
- Question: How would the performance of ABA-PC change if other argumentation semantics were used instead of stable semantics?
- Basis in paper: [explicit] The paper mentions that "other argumentation semantics in the literature" are planned to be explored as future work, suggesting that the current implementation uses stable semantics.
- Why unresolved: The paper only reports results using stable semantics, and the impact of using other semantics on the algorithm's performance is unknown.
- What evidence would resolve it: Implementing ABA-PC with different argumentation semantics and comparing their performance on benchmark datasets would provide evidence of the impact of semantics choice.

### Open Question 3
- Question: How would the ABA-PC algorithm handle datasets with latent confounders?
- Basis in paper: [inferred] The paper mentions extending the approach to deal with latent confounders as future work, and cites relevant literature (Colombo et al., 2012; Hyttinen, Eberhardt, and Järvisalo, 2014).
- Why unresolved: The current implementation assumes sufficiency (no latent confounders), and the performance of the algorithm on datasets with latent confounders is unknown.
- What evidence would resolve it: Testing the ABA-PC algorithm on datasets known to have latent confounders and comparing its performance to other methods that handle latent confounders would provide evidence of its ability to handle this scenario.

## Limitations

- Faithfulness assumption required for correct correspondence between d-separation and independences
- Weighting heuristic lacks external validation and may misrank tests
- Scalability limited to datasets with 10 or fewer variables due to grounding complexity

## Confidence

- Core mechanism (encoding d-separation into ABA): Medium
- Implementation approach (ASP encoding, stable semantics): High
- Empirical evaluation: Medium

## Next Checks

1. **Faithfulness sensitivity test**: Systematically evaluate ABA-PC on synthetic data with known faithfulness violations to quantify performance degradation and characterize failure modes.

2. **Weighting function ablation**: Compare ABA-PC performance with alternative weighting schemes (e.g., only p-value based, only conditioning set based) to isolate the contribution of the current heuristic.

3. **Semantic comparison study**: Implement ABA-PC with complete semantics instead of stable semantics on the same benchmarks to assess the impact of the choice on both accuracy and computational efficiency.