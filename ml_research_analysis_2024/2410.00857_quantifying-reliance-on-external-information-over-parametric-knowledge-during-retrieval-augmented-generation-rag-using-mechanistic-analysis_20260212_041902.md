---
ver: rpa2
title: Quantifying reliance on external information over parametric knowledge during
  Retrieval Augmented Generation (RAG) using mechanistic analysis
arxiv_id: '2410.00857'
source_url: https://arxiv.org/abs/2410.00857
tags:
- attention
- token
- context
- language
- parametric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper mechanistically examines how language models (LMs) leverage
  retrieved context versus their internal parametric knowledge during Retrieval Augmented
  Generation (RAG). The authors propose three methods - Causal Mediation Analysis,
  Attention Contributions, and Attention Knockouts - to quantify reliance on external
  information versus parametric memory.
---

# Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis

## Quick Facts
- arXiv ID: 2410.00857
- Source URL: https://arxiv.org/abs/2410.00857
- Reference count: 1
- Primary result: Language models show strong "shortcut" behavior in RAG, relying heavily on retrieved context while minimally using parametric memory

## Executive Summary
This paper presents a mechanistic analysis of how language models leverage retrieved context versus internal parametric knowledge during Retrieval Augmented Generation (RAG). The authors propose three methods - Causal Mediation Analysis, Attention Contributions, and Attention Knockouts - to quantify the reliance on external information. Using these methods on LLaMa-2 and Phi-2 models with 1209 samples, the study reveals that both models demonstrate a pronounced "shortcut" behavior, showing strong bias towards utilizing retrieved context while minimally relying on model priors. The findings suggest that RAG systems fundamentally alter how models process information, with implications for designing more effective retrieval-augmented systems.

## Method Summary
The paper introduces three mechanistic probing methods to examine how language models use retrieved context versus internal knowledge during RAG. The methods include Causal Mediation Analysis to measure indirect effects at different token positions, Attention Contributions to track information flow between tokens, and Attention Knockouts to assess the impact of removing specific attention connections. The study uses the Knowns 1000 dataset with GPT-4-generated context, comparing non-RAG and RAG settings across LLaMa-2 (7B) and Phi-2 (2.7B) models. The metrics include Average Indirect Effect (AIE), attention contribution magnitudes, and answer probability changes when attention edges are removed.

## Key Results
- Both LLaMa-2 and Phi-2 models show reduced reliance on parametric memory in RAG settings, with AIE at last subject token decreasing by 10X and 35X respectively
- Attention contributions from subject token to last token decrease significantly in RAG settings (1.6X for LLaMa-2, 7X for Phi-2)
- Attention knockouts show minimal impact (<5% drop) on answer probabilities in RAG settings, compared to 20-25% drops in non-RAG settings
- Information flow in RAG systems becomes more direct from attribute tokens to last token residual stream

## Why This Works (Mechanism)
None

## Foundational Learning
1. **Causal Mediation Analysis**: Method to decompose total effect into direct and indirect components by intervening on mediator variables
   - Why needed: To quantify how much information flows through retrieved context versus internal model knowledge
   - Quick check: Compare AIE values between RAG and non-RAG settings to verify expected reduction

2. **Attention Knockout Method**: Technique to assess the importance of specific attention connections by removing them and measuring performance impact
   - Why needed: To identify which attention pathways are critical for correct predictions
   - Quick check: Verify that removing attention from subject tokens causes larger drops in non-RAG vs RAG settings

3. **Residual Stream Analysis**: Examination of information flow through transformer layers by tracking attention patterns
   - Why needed: To understand how information propagates through the model architecture
   - Quick check: Confirm that RAG settings show more direct attribute-to-last-token flow compared to non-RAG

## Architecture Onboarding
**Component Map**: Input tokens -> Embedding layer -> Transformer blocks (self-attention + FFN) -> Output logits
**Critical Path**: Query tokens → Attention heads → Residual stream → Last token predictions
**Design Tradeoffs**: Direct information flow (faster, less reliance on memory) vs. integrated knowledge (more robust, slower)
**Failure Signatures**: 
- Incorrect AIE measurement leading to false conclusions about shortcut behavior
- Insufficient attention contribution reduction in RAG settings
- Disproportionate knockout effects that don't match reported patterns

**First Experiments**:
1. Measure AIE at subject and last token positions in non-RAG baseline
2. Compare attention contributions from subject to last token in RAG vs non-RAG
3. Perform attention knockouts on subject token and measure answer probability changes

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the reliance on parametric memory versus retrieved context scale with model size (e.g., comparing 7B vs 13B+ parameter models)?
- Basis in paper: [explicit] The authors note they plan to extend to larger LMs (>13B parameters) in future work
- Why unresolved: The current study only examined LLaMa-2 (7B) and Phi-2 (2.7B) models, leaving uncertainty about whether the "shortcut" behavior is consistent across different model scales
- What evidence would resolve it: Comparative mechanistic analysis of RAG behavior across multiple model sizes (7B, 13B, 70B+) using the same causal mediation and attention analysis methods

### Open Question 2
- How does context position affect reliance on retrieved information in RAG systems?
- Basis in paper: [explicit] The authors plan to study the impact of LM behavior in longer context and settings where language models exhibit primacy and recency bias
- Why unresolved: The current experiments used synthetic context generated by GPT-4 with controlled segment lengths, but didn't examine how information placement within longer contexts affects model reliance patterns
- What evidence would resolve it: Experiments varying context position (early vs. middle vs. late) within long documents and measuring changes in AIE and attention contributions across different token positions

### Open Question 3
- How does the "shortcut" behavior change when using conventional RAG pipelines versus synthetic context generation?
- Basis in paper: [explicit] The authors aim to replicate findings using a conventional RAG pipeline that automatically creates context rather than synthetically generating it using GPT-4
- Why unresolved: The current study used GPT-4-generated context which may not reflect the noise, relevance, or structure of real-world retrieved context from typical RAG implementations
- What evidence would resolve it: Replication of the mechanistic analysis (AIE, attention contributions, knockouts) using conventional RAG systems with real retrieval mechanisms and document corpora

## Limitations
- Analysis limited to small models (7B and 2.7B parameters), results may not generalize to larger models
- Dataset represents specific knowledge question types, may not capture full diversity of RAG use cases
- Mechanistic methods may not capture all aspects of how models integrate retrieved information with parametric knowledge

## Confidence
**Core finding (Medium-High)**: RAG systems show reduced reliance on parametric knowledge
**Quantitative claims (Medium)**: 10X, 35X, 1.6X, 7X decreases in various metrics

## Next Checks
1. **Cross-model validation**: Apply the same mechanistic analysis to larger models (e.g., LLaMa-2 13B, 70B) to verify if the "shortcut" behavior scales with model size
2. **Dataset diversity test**: Repeat the experiments using multiple knowledge datasets with varying question types to ensure the findings are not specific to the Knowns 1000 dataset
3. **Temporal stability analysis**: Conduct the same mechanistic analysis across multiple time points during model training to understand how the reliance on external information versus parametric knowledge evolves