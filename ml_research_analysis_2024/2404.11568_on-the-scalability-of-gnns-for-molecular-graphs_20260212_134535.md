---
ver: rpa2
title: On the Scalability of GNNs for Molecular Graphs
arxiv_id: '2404.11568'
source_url: https://arxiv.org/abs/2404.11568
tags:
- pearson
- auprc
- auroc
- scaling
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the scalability of graph neural networks (GNNs)
  for molecular graphs, addressing the challenge of scaling GNNs in supervised multi-task
  training on molecular graphs. The authors analyze message-passing networks, graph
  Transformers, and hybrid architectures on the largest public collection of 2D molecular
  graphs.
---

# On the Scalability of GNNs for Molecular Graphs

## Quick Facts
- **arXiv ID**: 2404.11568
- **Source URL**: https://arxiv.org/abs/2404.11568
- **Reference count**: 40
- **Primary result**: Proposed MolGPS model outperforms previous state-of-the-art models on 26 out of 38 downstream molecular property prediction tasks

## Executive Summary
This paper investigates the scalability of graph neural networks (GNNs) for molecular graphs, addressing the challenge of scaling GNNs in supervised multi-task training. The authors analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs. They demonstrate that GNNs benefit tremendously from increasing scale in depth, width, number of molecules, number of labels, and diversity in pretraining datasets. The proposed MolGPS model, derived from their findings, outperforms previous state-of-the-art models on 26 out of 38 highly competitive downstream tasks.

## Method Summary
The authors pretrain GNN models (MPNN++, Transformer, and GPS++) on the LargeMix dataset (5 million molecules) using multi-task supervised learning, then evaluate performance on 38 downstream tasks from TDC, Polaris2, and MoleculeNet benchmarks. They systematically scale model depth, width, and pretraining dataset size to study scaling behavior. The GPS++ architecture combines message-passing networks with self-attention to leverage both inductive bias and attention mechanisms. Performance is evaluated using metrics including Pearson, Spearman, AUROC, AUPRC, MAE, and average standardized scores.

## Key Results
- GNNs show consistent scaling benefits from increasing depth, width, number of molecules, and pretraining task diversity
- The 3 billion parameter models continue to show constant gains in molecular property prediction
- GPS++ architecture outperforms both pure MPNN++ and Transformer models
- MolGPS model achieves state-of-the-art performance on 26 out of 38 downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing width leads to better performance across all pretraining tasks
- Mechanism: Wider models can capture more complex patterns in molecular graphs by increasing the capacity of each layer to represent diverse chemical features
- Core assumption: The pretraining data contains sufficient diversity and quantity to avoid overfitting when increasing width
- Evidence anchors:
  - [abstract]: "we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets."
  - [section 4.1]: "increasing the width has a significant impact on model performance across all tasks."
- Break condition: If the dataset lacks diversity or is too small, wider models may overfit and performance could degrade

### Mechanism 2
- Claim: Multi-task pretraining on diverse molecular tasks improves downstream performance
- Mechanism: Multi-task pretraining creates rich embeddings that capture general molecular properties, which transfer well to new tasks
- Core assumption: The pretraining tasks are sufficiently diverse and cover relevant molecular features for downstream tasks
- Evidence anchors:
  - [abstract]: "We show that supervised pretraining over molecular graphs provides a rich fingerprint embedding, useful for MLP probing, and more expressive as we scale the model and datasets."
- Break condition: If pretraining tasks are too narrow or not representative of downstream tasks, the embeddings may not transfer effectively

### Mechanism 3
- Claim: Graph Transformers benefit more from increased width compared to MPNN++ and GPS++ models
- Mechanism: Transformers can leverage additional parameters to capture long-range interactions in molecular graphs more effectively
- Core assumption: The molecular graphs have meaningful long-range dependencies that Transformers can exploit
- Evidence anchors:
  - [section 4.1]: "Transformers being 'data-hungry' is consistent with recent works in domains such as natural language and computer vision [50, 2, 17]."
- Break condition: If molecular graphs are primarily local in nature, the benefits of increased width for Transformers may be limited

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Understanding how GNNs work is essential to grasp the paper's findings about their scalability
  - Quick check question: What is the key difference between how GNNs and traditional neural networks process data?

- Concept: Scaling Laws
  - Why needed here: The paper's core contribution is demonstrating that GNNs follow scaling laws similar to other deep learning models
  - Quick check question: What is the relationship between model size, dataset size, and performance according to scaling laws?

- Concept: Pretraining and Transfer Learning
  - Why needed here: The paper uses supervised pretraining on molecular graphs and then evaluates performance on downstream tasks
  - Quick check question: What is the purpose of pretraining a model on one task before fine-tuning it on another task?

## Architecture Onboarding

- Component map:
  - MPNN++: Message-passing network with edge and global features
  - Transformer: Self-attention based model with positional and structural encodings
  - GPS++: Hybrid model combining MPNN++ inductive bias with self-attention
  - Pretraining tasks: L1000_VCAP, L1000_MCF7, PCBA_1328, PCQM4M_G25, PCQM4M_N4
  - Downstream tasks: 38 datasets from TDC, Polaris2, and MoleculeNet

- Critical path:
  1. Pretrain GNN model on LargeMix dataset
  2. Extract fingerprints from pretrained model
  3. Probe fingerprints with MLP on downstream tasks
  4. Evaluate performance across different model sizes and architectures

- Design tradeoffs:
  - MPNN++ vs. Transformer: MPNN++ is more parameter and data efficient, while Transformers benefit more from increased width
  - Depth vs. width: Deeper models capture more complex patterns but may suffer from oversmoothing
  - Pretraining dataset size vs. diversity: Larger datasets improve performance, but diverse tasks are crucial for effective transfer learning

- Failure signatures:
  - Performance plateaus or degrades with increased depth (oversmoothing)
  - No improvement with increased width (insufficient data diversity or capacity saturation)
  - Poor transfer learning performance (pretraining tasks not representative of downstream tasks)

- First 3 experiments:
  1. Pretrain MPNN++, Transformer, and GPS++ models with varying widths on the LargeMix dataset and evaluate pretraining loss
  2. Extract fingerprints from pretrained models and probe with MLP on a subset of downstream tasks
  3. Compare performance of different architectures and widths on the probed downstream tasks to identify scaling trends

## Open Questions the Paper Calls Out
- What are the fundamental limitations of message-passing networks (MPNNs) that cause their performance to degrade with increasing depth, and how can these limitations be overcome?
- How does the choice of pretraining dataset affect the downstream performance of GNNs, and what are the key characteristics of a good pretraining dataset for molecular property prediction?
- How does the integration of phenomics data into pretraining impact the performance of GNNs on downstream molecular property prediction tasks, and what are the mechanisms behind this improvement?

## Limitations
- The paper doesn't establish where width scaling saturates or whether benefits are task-specific
- Claims about "constant gains" at 3B parameters may not generalize to even larger scales or different molecular property tasks
- The assertion that Transformers benefit more from width lacks exploration of the specific mechanisms for molecular graphs

## Confidence
- **High confidence**: The observation that GNNs benefit from scaling in molecular property prediction is well-supported by empirical results across 38 downstream tasks and multiple model families
- **Medium confidence**: Claims about Transformers being "data-hungry" and benefiting more from width are consistent with observations in other domains, but specific mechanisms for molecular graphs aren't fully explored
- **Low confidence**: The assertion that "constant gains" continue at 3B parameters is based on current results but may not generalize to even larger scales or different molecular property tasks

## Next Checks
1. **Diversity ablation study**: Create controlled pretraining datasets with varying degrees of task diversity and chemical space coverage to quantify the relationship between pretraining task diversity and downstream transfer performance
2. **Width scaling saturation analysis**: Systematically test the 3B parameter models on additional molecular property prediction tasks and other graph-based domains to determine if width scaling benefits generalize beyond the current task suite
3. **Depth vs. oversmoothing validation**: Implement and compare multiple depth-scaling strategies (residual connections, jumping knowledge, attention mechanisms) to isolate whether the observed depth benefits are due to architectural choices rather than inherent GNN scaling properties