---
ver: rpa2
title: 'The Illusion of Competence: Evaluating the Effect of Explanations on Users''
  Mental Models of Visual Question Answering Systems'
arxiv_id: '2406.19170'
source_url: https://arxiv.org/abs/2406.19170
tags:
- color
- explanations
- nlx-gpt
- system
- grayscale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We test whether explanations improve users' ability to accurately
  assess an AI system's capabilities, focusing on color recognition when images are
  converted to grayscale. Users rate system competence on four dimensions (color,
  shape, material, scene) with and without explanations.
---

# The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems

## Quick Facts
- arXiv ID: 2406.19170
- Source URL: https://arxiv.org/abs/2406.19170
- Authors: Judith Sieker; Simeon Junker; Ronja Utescher; Nazia Attari; Heiko Wersing; Hendrik Buschmeier; Sina Zarrieß
- Reference count: 31
- Key outcome: Explanations create illusion of competence, increasing user ratings of AI capabilities regardless of actual performance

## Executive Summary
This study investigates whether natural language explanations help users accurately assess AI system capabilities, specifically testing color recognition limitations when images are converted to grayscale. Through user studies with VQA systems, the research reveals that explanations actually increase perceived system competence across all dimensions (color, shape, material, scene) rather than making limitations transparent. This illusion-of-competence effect persists even when systems fail to recognize colors in grayscale images, with explanations often guessing correct attributes from context. The findings challenge the assumption that explanations automatically improve user mental models and suggest current explanation methods may hinder accurate AI assessment.

## Method Summary
The study uses two VQA datasets (VQA-X and CLEVR-X) with controlled grayscale manipulation to test color recognition capabilities. NLX-GPT, Uni-NLX, and PJ-X models are fine-tuned to generate answers and explanations for both color and grayscale images. A user study with 160 participants rates system capabilities and competence across four dimensions (color, shape, material, scene) in two experimental conditions: answers only (A) and answers with explanations (X). Statistical analysis compares ratings between conditions, while automatic evaluation metrics like BERTScore assess explanation quality. The controlled perturbation design isolates color information as the key variable affecting system performance.

## Key Results
- Explanations increase user ratings of system competence across all capability dimensions regardless of actual performance
- Users cannot accurately diagnose color recognition limitations even when explanations reference colors absent from grayscale inputs
- Automatic evaluation metrics like BERTScore show opposite trends from human ratings, failing to capture the illusion-of-competence effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations increase user ratings of AI system competence regardless of actual performance.
- Mechanism: Users interpret fluent, human-like explanations as evidence of system capability, leading to higher competence ratings even when performance is flawed.
- Core assumption: Users trust natural language explanations and interpret them as indicators of system understanding.
- Evidence anchors:
  - [abstract]: "explanations generally increase users' perceptions of the system's competence – regardless of its actual performance."
  - [section 5.3]: "users rate all system capabilities significantly higher when explanations are provided... the AI system's explanations seem to create an illusion of the system's competence that does not correspond to its actual performance."

### Mechanism 2
- Claim: Users cannot accurately diagnose specific system limitations even when explanations are provided.
- Mechanism: AI explanation models generate plausible-sounding explanations that mask true limitations by guessing correct attributes from context rather than reflecting actual capabilities.
- Core assumption: Explanation models prioritize generating plausible outputs over revealing true system reasoning.
- Evidence anchors:
  - [abstract]: "explanations do not make system limitations transparent... explanations may even further obstruct AIs' reasoning processes and trick users into believing that the AI is more competent than it actually is."
  - [section 5.1]: "explanations do not help users construct a more accurate mental model of the system and its capabilities and limitations, but simply lead to more positive user assessment overall."

### Mechanism 3
- Claim: Standard automatic evaluation metrics fail to capture the disconnect between user perception and actual system performance.
- Mechanism: Metrics like BERTScore measure semantic similarity to human explanations without accounting for whether explanations accurately reflect system capabilities or limitations.
- Core assumption: Automatic metrics assume similar explanations to human ones are necessarily good, without considering faithfulness to actual system reasoning.
- Evidence anchors:
  - [section 5.4]: "BERTScores for the CLEVR-X dataset show improved values (in both the grayscale and color conditions), aligning with the human results from Exp.X" while human ratings show the opposite pattern.
  - [section 6]: "automatic,benchmark-based evaluations still seem to be in focus and widely accepted in the community" despite evidence that they may not capture important properties.

## Foundational Learning

- Concept: Mental models in human-AI interaction
  - Why needed here: The study investigates how users construct mental models of AI capabilities and whether explanations help build accurate ones.
  - Quick check question: What distinguishes a functional mental model from an illusory one in AI system assessment?

- Concept: Faithfulness vs. plausibility in explanations
  - Why needed here: The study reveals that plausible-sounding explanations can be misleading if they're not faithful to actual system reasoning.
  - Quick check question: How can we design explanation evaluation that prioritizes faithfulness over plausibility?

- Concept: Controlled perturbation studies in AI evaluation
  - Why needed here: The study uses controlled grayscale manipulation to test whether users can detect specific system limitations.
  - Quick check question: What makes a perturbation study effective for isolating and testing specific AI capabilities?

## Architecture Onboarding

- Component map:
  - VQA datasets (VQA-X, CLEVR-X) → preprocessing → grayscale manipulation
  - NLX-GPT, Uni-NLX, PJ-X models → fine-tuning → explanation generation
  - User interface → randomization → data collection
  - Statistical analysis → visualization → result interpretation

- Critical path:
  1. Dataset preparation and item selection (color vs. grayscale conditions)
  2. Model fine-tuning and inference (with controlled input manipulation)
  3. User study deployment and data collection
  4. Statistical analysis of user ratings
  5. Comparison with automatic evaluation metrics

- Design tradeoffs:
  - Controlled manipulation vs. ecological validity: Artificial grayscale condition provides clean test but may not reflect real-world scenarios
  - Model selection: Using established models vs. newer approaches with potential reproducibility issues
  - Dataset choice: CLEVR-X offers controlled explanations vs. VQA-X offers real-world complexity

- Failure signatures:
  - Inconsistent user ratings across similar items
  - Automatic metrics showing opposite trends from human evaluation
  - Models failing to reproduce expected limitations in explanations
  - Statistical tests showing no significant differences where expected

- First 3 experiments:
  1. Replicate color vs. grayscale manipulation with a different VQA model to test generalizability
  2. Test explanation evaluation with explicit faithfulness constraints to see if this changes user assessment patterns
  3. Compare user assessment when explanations are presented with vs. without confidence scores or uncertainty indicators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms would cause users to perceive color capabilities when explanations reference colors not present in grayscale inputs?
- Basis in paper: [explicit] Authors note that systems "seem to be able to guess the correct color from the question or the general context" and that explanations "project confidence and competence" even when grounded in absent visual information
- Why unresolved: The study demonstrates this effect but doesn't explain the psychological or cognitive mechanisms behind it
- What evidence would resolve it: User interviews or eye-tracking studies showing how participants process explanation text versus question context when assessing system capabilities

### Open Question 2
- Question: How would explanations perform in tasks where color information is critical versus supplementary to correct answers?
- Basis in paper: [inferred] Study focuses on VQA tasks where color is often supplementary; limitations suggest exploring domains where color is essential
- Why unresolved: Current findings show explanations don't help diagnose color limitations, but results may differ in color-critical tasks
- What evidence would resolve it: Comparative study across domains (e.g., medical imaging vs. VQA) measuring explanation effectiveness when color is essential vs. decorative

### Open Question 3
- Question: Would explanations generated by more faithful architectures (e.g., attention-based or modular systems) produce different user assessment patterns?
- Basis in paper: [explicit] Authors suggest prioritizing "faithfulness over plausibility" and note that current explanation methods "largely follow common language modeling architectures and prioritize generating fluent, human-like outputs"
- Why unresolved: Study only tests encoder-decoder models that generate explanations as part of answer generation
- What evidence would resolve it: User study comparing explanation effectiveness between encoder-decoder models and attention-based or modular explanation systems on identical tasks

## Limitations
- Controlled grayscale manipulation may not reflect real-world scenarios where AI systems encounter ambiguous or degraded inputs
- User study sample (160 participants) may not capture diverse populations with varying AI literacy levels
- Focus on VQA systems limits applicability to other AI domains where explanations might function differently

## Confidence

**Major uncertainties and limitations:**
The study demonstrates a robust illusion-of-competence effect across multiple models and datasets, but several limitations affect generalizability. The controlled grayscale manipulation, while effective for isolating color recognition limitations, may not reflect real-world scenarios where AI systems encounter ambiguous or degraded inputs. The user study sample (160 participants) provides reasonable statistical power for the observed effects, but may not capture diverse user populations with varying AI literacy levels. Additionally, the focus on VQA systems limits applicability to other AI domains where explanations might function differently.

**Confidence labels:**
- **High confidence**: The core finding that explanations increase perceived competence regardless of actual performance is well-supported by consistent results across experiments and datasets.
- **Medium confidence**: The claim that explanations specifically mask color recognition limitations is strong for the controlled grayscale condition but may not generalize to more complex real-world scenarios.
- **Medium confidence**: The assertion that automatic evaluation metrics fail to capture the illusion-of-competence effect is well-documented but may depend on the specific metrics and datasets used.

## Next Checks
1. Test whether the illusion-of-competence effect persists when explanations include explicit uncertainty indicators or confidence scores.
2. Replicate the study with a more diverse user population including AI experts, non-experts, and users with varying levels of technical literacy.
3. Evaluate whether explanations designed with explicit faithfulness constraints (rather than plausibility) mitigate the illusion-of-competence effect while maintaining user trust.