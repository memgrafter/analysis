---
ver: rpa2
title: 'AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed
  Bandit'
arxiv_id: '2409.13447'
source_url: https://arxiv.org/abs/2409.13447
tags:
- question
- graph
- agents
- each
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AQA, an adaptive question answering framework
  that dynamically selects the most suitable answering strategy for each question
  using contextual multi-armed bandits. The framework frames the orchestration of
  multiple LLM agents as a graph optimization problem, where different graph configurations
  represent different answering strategies.
---

# AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit

## Quick Facts
- arXiv ID: 2409.13447
- Source URL: https://arxiv.org/abs/2409.13447
- Reference count: 15
- Key outcome: Dynamic selection of optimal answering strategy per question using contextual multi-armed bandits, outperforming static orchestration methods

## Executive Summary
This paper introduces AQA, a framework that adaptively selects the most suitable answering strategy for each question in a multi-LLM QA system. By framing orchestration as a graph optimization problem and using contextual multi-armed bandits (LinUCB), AQA learns to map question characteristics to optimal graph configurations. The framework balances answer quality and computational efficiency through a reward function that combines F1-score and execution time. Experiments demonstrate AQA's ability to identify optimal answering strategies per question type, successfully combining the superior performance of complex strategies while avoiding their costs when simpler strategies suffice.

## Method Summary
AQA frames multi-agent LLM orchestration as a graph optimization problem where different DAG configurations represent answering strategies. A LinUCB contextual bandit model learns to map question features to optimal graph configurations, treating each graph as an action. The reward function combines F1-score performance with execution time penalties, allowing the system to balance accuracy and computational efficiency. The framework is evaluated on a MMLU-based dataset with questions labeled by complexity levels, comparing against individual agents and static orchestration baselines.

## Key Results
- Successfully identifies optimal answering strategies per question type through adaptive orchestration
- Outperforms static orchestration methods by combining superior performance of complex strategies while avoiding unnecessary costs
- Demonstrates effective balance between answer quality and computational efficiency through learned time-accuracy tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMAB framework learns to map question characteristics to optimal graph configurations using question features as context
- Mechanism: LinUCB maintains parameter estimates for each graph configuration, computing upper confidence bounds on expected reward using question context vectors
- Core assumption: Linear relationship between question features and optimal graph configurations in feature space
- Evidence anchors: Abstract mentions training linear upper confidence bound model; Section 3.5 details LinUCB choice and reward signal formulation
- Break condition: Non-linear relationships between question features and optimal strategies cause suboptimal convergence

### Mechanism 2
- Claim: Framework balances effectiveness and efficiency by incorporating execution time into reward function
- Mechanism: Reward function rt = β · Pt - (1 - β) · Tt combines F1-score (Pt) with time penalty (Tt), with β controlling the tradeoff
- Core assumption: Execution time correlates meaningfully with computational cost and user experience for linear combination with accuracy
- Evidence anchors: Abstract discusses combining superior performance while avoiding costs; Section 3.5 specifies reward signal details; Section 4.4 sets β = 0.5
- Break condition: Non-linear time-user satisfaction relationships or fundamentally different time-accuracy tradeoffs across question types

### Mechanism 3
- Claim: Graph-based framework represents all meaningful communication patterns while avoiding invalid configurations
- Mechanism: Action space generated from all DAGs including final decision node with incoming edges, ensuring executability while covering unique strategies
- Core assumption: All valid answering strategies can be represented as DAGs under specified constraints without excluding useful strategies
- Evidence anchors: Section 3.3 describes node aggregation and cycle constraints; Section 3.4 defines graph constraints ensuring DAG structure and connectivity
- Break condition: Valid strategies cannot be represented as DAGs under constraints, or constraints are too restrictive

## Foundational Learning

- Concept: Multi-armed bandit algorithms and contextual bandits
  - Why needed here: Framework treats graph configurations as arms and uses question features as context, requiring understanding of exploration-exploitation balance
  - Quick check question: What is the difference between a standard multi-armed bandit and a contextual multi-armed bandit?

- Concept: Graph theory and DAGs
  - Why needed here: Orchestration framework represents agent interactions as directed acyclic graphs, essential for understanding implementation and debugging
  - Quick check question: Why must orchestration graphs be directed acyclic graphs rather than general directed graphs?

- Concept: Reinforcement learning vs contextual bandits
  - Why needed here: Paper contrasts CMAB approach with policy gradient methods, requiring understanding of appropriate use cases and tradeoffs
  - Quick check question: What are key differences between contextual multi-armed bandits and policy gradient methods for optimization?

## Architecture Onboarding

- Component map: Question processor -> Context vector generator -> LinUCB model -> Graph configuration generator -> Agent pool -> Execution engine -> Reward calculator -> Parameter update
- Critical path: Question → Context extraction → LinUCB selection → Graph execution → Performance measurement → Reward calculation → Parameter update
- Design tradeoffs: Linear vs non-linear CMAB models (simplicity vs expressiveness); pre-computation vs dynamic graph generation (runtime speed vs upfront cost); single vs multiple aggregation nodes (simplicity vs expressiveness)
- Failure signatures: Poor adaptation (check context feature predictiveness); computational inefficiency (verify time penalty scaling); convergence issues (check exploration rate and context representation)
- First 3 experiments: 1) Individual agents baseline (NoR, OneR, IRCoT) to establish performance baselines; 2) Context-agnostic orchestration using GPTSwarm for single graph optimization; 3) Time-agnostic reward optimization with β=1.0 to verify complexity-level mapping

## Open Questions the Paper Calls Out
The paper acknowledges that "Various features of a question can be important when choosing answering strategies" and mentions that "the number of agents and their possible interactions is generally limited," but doesn't empirically test scalability beyond the three-agent setup used in experiments.

## Limitations
- Linear approximation assumption may not capture complex relationships between question features and optimal strategies
- Uniform time-accuracy tradeoff assumption across question types may not reflect real-world user preferences
- Pre-computed action space may exclude valid answering strategies that don't fit specified DAG constraints

## Confidence
- High Confidence: Core CMAB mechanism and LinUCB implementation details are well-established and clearly specified
- Medium Confidence: Linear reward function combining accuracy and time is theoretically sound but may not capture complex user preferences
- Low Confidence: Performance on question types outside MMLU-based complexity labels is unknown; generalization to different domains remains untested

## Next Checks
1. **Non-linear CMAB validation:** Implement and compare against kernelized or neural contextual bandit to assess whether linear assumption limits adaptation performance

2. **Cross-domain generalization test:** Evaluate trained model on question-answering datasets from different domains (biomedical, legal) to assess generalization beyond MMLU complexity labels

3. **User preference calibration:** Conduct user studies to determine optimal β parameters for different user groups and validate whether current linear reward function adequately captures user satisfaction tradeoffs