---
ver: rpa2
title: Integrated Design and Governance of Agentic AI Systems through Adaptive Information
  Modulation
arxiv_id: '2409.10372'
source_url: https://arxiv.org/abs/2409.10372
tags:
- agents
- information
- cooperation
- manager
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework integrating adaptive governance
  into sociotechnical systems by dynamically modulating information flow between large
  language model (LLM) agents. The framework uses a reinforcement learning-based manager
  to control the type and level of information each agent receives during repeated
  strategic interactions, without altering the underlying network structure.
---

# Integrated Design and Governance of Agentic AI Systems through Adaptive Information Modulation

## Quick Facts
- **arXiv ID**: 2409.10372
- **Source URL**: https://arxiv.org/abs/2409.10372
- **Reference count**: 40
- **Primary result**: RL-based adaptive information governance improves cooperation and social welfare in multi-agent LLM systems compared to static information-sharing baselines

## Executive Summary
This paper introduces a framework that integrates adaptive governance into sociotechnical systems by dynamically modulating information flow between large language model (LLM) agents. The framework uses a reinforcement learning-based manager to control the type and level of information each agent receives during repeated strategic interactions, without altering the underlying network structure. Experimental results in a repeated Prisoner's Dilemma setting show that this RL-based information governance significantly enhances cooperation and social welfare compared to static information-sharing baselines. The RL manager adaptively selects among three information types—last actions, agent cooperation ratios, and neighborhood cooperation ratios—demonstrating that targeted information disclosure can effectively promote desired collective outcomes in multi-agent AI systems.

## Method Summary
The framework implements a reinforcement learning manager that dynamically adjusts information disclosure to LLM agents playing repeated Prisoner's Dilemma in a fixed network. The manager observes aggregated agent states and selects from three information types (last actions, agent cooperation ratios, neighborhood cooperation ratios) for each agent in each interaction. Using an Actor-Critic architecture, the RL manager learns to allocate information heterogeneously based on agent characteristics like degree centrality and historical cooperation. The approach preserves agent autonomy while promoting cooperation through information governance rather than structural or payoff interventions.

## Key Results
- RL-based adaptive information governance significantly improves social welfare compared to static baselines
- The manager learns to allocate LA+NR information to high-degree nodes and LA information to low-cooperation agents
- Information modulation achieves cooperation rates substantially above baseline methods without network restructuring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive information modulation steers agent behavior without altering network structure.
- Mechanism: A reinforcement learning manager dynamically adjusts the type and level of information each agent receives during repeated interactions, influencing decision-making through context rather than direct control.
- Core assumption: LLM agents respond predictably to changes in the information content of their prompts, allowing the manager to shape behavior by modulating what agents observe.
- Evidence anchors:
  - [abstract] "The governing agent learns to strategically adjust information disclosure at each timestep, determining what contextual or historical information each system agent can access."
  - [section] "Unlike conventional approaches that require direct structural interventions or payoff modifications, our framework preserves agent autonomy while promoting cooperation through adaptive information governance."
  - [corpus] Weak; neighboring papers focus on system-level governance architectures rather than information-flow-only approaches.
- Break condition: LLM agents stop responding consistently to information-level changes, or the RL manager cannot generalize across different network topologies.

### Mechanism 2
- Claim: Separation of interaction and information networks enables scalable, flexible governance.
- Mechanism: The interaction network remains fixed, while the information network is dynamically altered by the RL manager, allowing intervention without restructuring agent connections.
- Core assumption: The manager can observe aggregated agent states and select information types that influence outcomes without needing to modify the underlying interaction topology.
- Evidence anchors:
  - [abstract] "Our framework preserves agent autonomy while promoting cooperation through adaptive information governance."
  - [section] "Unlike conventional approaches that require direct structural interventions or payoff modifications, our framework preserves agent autonomy while promoting cooperation through adaptive information governance."
  - [corpus] Missing; no corpus papers directly discuss this separation of networks.
- Break condition: The manager cannot effectively target interventions without network-level information, or information modulation fails to improve collective welfare.

### Mechanism 3
- Claim: Reinforcement learning learns to allocate information heterogeneously based on agent characteristics.
- Mechanism: The RL manager tailors information disclosure to each agent's degree centrality and historical cooperation, assigning richer information to central or cooperative agents and simpler information to peripheral or less cooperative agents.
- Core assumption: Agent characteristics (degree, cooperation history) are predictive of how information should be allocated to maximize system-wide welfare.
- Evidence anchors:
  - [section] "Agents receiving LA+NR information show substantially higher pre-intervention cooperation rates... compared to those receiving LA information."
  - [section] "Nodes with higher degrees... are significantly more likely to receive neighborhood ratio information, while nodes with lower degrees... tend to receive last action information."
  - [corpus] Weak; neighboring papers focus on governance frameworks but not on agent-level information allocation strategies.
- Break condition: Agent heterogeneity no longer correlates with intervention effectiveness, or the manager cannot learn meaningful targeting rules.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The RL manager must act under uncertainty, observing only aggregated agent states and choosing actions that influence hidden agent states through information modulation.
  - Quick check question: In a POMDP, what does the agent observe at each step, and how does this differ from a fully observable MDP?

- Concept: Actor-Critic reinforcement learning
  - Why needed here: The RL manager uses an Actor-Critic architecture to learn both a policy for selecting information types and a value function for evaluating their impact on social welfare.
  - Quick check question: How does the Actor-Critic method combine policy-based and value-based approaches, and why is this useful for the manager's task?

- Concept: Multi-agent coordination and social dilemmas
  - Why needed here: The framework models repeated Prisoner's Dilemma interactions among LLM agents, where individual incentives conflict with collective welfare, requiring governance to promote cooperation.
  - Quick check question: Why is the Prisoner's Dilemma a canonical test case for studying cooperation and governance in multi-agent systems?

## Architecture Onboarding

- Component map:
  - LLM Agents -> Fixed interaction network -> Pairwise Prisoner's Dilemma games
  - RL Manager -> Observes aggregated agent states -> Selects information types
  - Information Types (LA, LA+AR, LA+NR) -> Integrated into prompts -> LLM decision-making
  - Prompts (system + individual) -> LLM agents -> Actions and payoffs

- Critical path:
  1. Initialize network and LLM agents.
  2. For each timestep:
     - RL manager observes aggregated agent state.
     - Manager selects information type for each agent in each interaction.
     - Prompts are constructed and provided to LLM agents.
     - LLM agents make decisions and receive payoffs.
     - System state and social welfare are updated.
  3. RL manager updates policy based on cumulative reward.

- Design tradeoffs:
  - Fixed interaction network vs. dynamic rewiring: Fixed structure preserves autonomy and scalability but limits direct control.
  - Information-only intervention vs. payoff modification: Information modulation is less invasive but may be less powerful than changing incentives.
  - Homogeneous vs. heterogeneous information: Tailoring information per agent improves performance but increases manager complexity.

- Failure signatures:
  - LLM agents ignore information-level changes (prompting ineffective).
  - RL manager policy collapses to a single information type (no adaptation).
  - Social welfare plateaus or declines despite manager intervention (misalignment or suboptimal policy).
  - High variance in agent responses across runs (lack of prompt robustness).

- First 3 experiments:
  1. Run with no manager intervention (all agents receive only last action info) to establish baseline cooperation and welfare.
  2. Run with static information assignment (e.g., all agents always get LA+NR) to compare with adaptive management.
  3. Run with the RL manager but only allow LA information to test if heterogeneity is necessary for performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework scale with network size and agent heterogeneity?
- Basis in paper: [explicit] The authors note that "the limited sample size of the evaluation (number of rounds) may introduce fluctuations" and suggest future work could explore "different network structures beyond random networks."
- Why unresolved: The current experiments use only 20 agents and 50 random network topologies, limiting understanding of performance with larger, more complex networks and diverse agent populations.
- What evidence would resolve it: Systematic experiments varying network size (hundreds to thousands of agents), network topology (scale-free, small-world, community-structured), and agent heterogeneity (different strategic dispositions, information processing capabilities).

### Open Question 2
- Question: How robust is the RL manager's performance when LLM agents have misaligned or adversarial objectives?
- Basis in paper: [inferred] The framework assumes LLM agents can be steered through information modulation, but doesn't test scenarios where agents might actively resist cooperation or pursue conflicting goals.
- Why unresolved: The current setup uses cooperative-oriented LLM agents, not testing how the governance mechanism performs when agents have heterogeneous or opposing objectives.
- What evidence would resolve it: Experiments introducing adversarial LLM agents, agents with different reward functions, or agents explicitly designed to resist cooperation, measuring whether the RL manager can still achieve positive social outcomes.

### Open Question 3
- Question: What are the long-term dynamics and convergence properties of the system under adaptive information governance?
- Basis in paper: [explicit] The authors note that "LLM agents tend to stabilize within 20 time steps for the majority of the experiments" but don't analyze long-term convergence patterns or potential for cyclical behavior.
- Why unresolved: The experiments run for only 20 timesteps and 50 rounds, insufficient to understand whether cooperation levels remain stable, improve, or degrade over extended periods.
- What evidence would resolve it: Extended simulations with hundreds or thousands of timesteps, tracking cooperation rates, welfare measures, and potential emergence of new behavioral patterns or system states over time.

## Limitations

- LLM behavior consistency across runs remains unclear despite micro-level validation
- Experiments limited to Erdos-Renyi random networks, performance on other topologies unknown
- Framework demonstrated only on Prisoner's Dilemma, generalizability to other social dilemmas unverified
- RL manager scalability to larger networks and longer time horizons not explored

## Confidence

- **High**: The RL manager learns to allocate information heterogeneously based on agent characteristics (degree centrality, cooperation history), and this targeting strategy correlates with improved social welfare.
- **Medium**: Adaptive information modulation improves cooperation and social welfare compared to static baselines, but the magnitude of improvement may depend on network structure and parameter choices.
- **Low**: The separation of interaction and information networks fundamentally enables scalable governance without structural intervention—this is theoretically sound but empirical validation beyond the current setup is needed.

## Next Checks

1. **Robustness check**: Run the full experimental pipeline across multiple network topologies (scale-free, small-world, modular) with 10 different instantiations each to verify the manager's performance is not topology-specific.

2. **Transfer check**: Replace the Prisoner's Dilemma with a different social dilemma (e.g., Stag Hunt or Snowdrift game) while keeping the same manager architecture and information types to test mechanism generalizability.

3. **Ablation check**: Implement a version where the RL manager can only select one global information type per timestep (rather than per agent) to quantify the value of heterogeneous information allocation.