---
ver: rpa2
title: 'DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with
  Language Models'
arxiv_id: '2410.05639'
source_url: https://arxiv.org/abs/2410.05639
tags:
- data
- arxiv
- training
- corpus
- tagging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DecorateLM, a data engineering method that
  refines large language model pretraining corpora through rating, tagging, and editing.
  The approach uses a small language model to rate texts across eight quality dimensions,
  tag them with a hierarchical labeling system, and edit them into more formalized
  formats.
---

# DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models

## Quick Facts
- arXiv ID: 2410.05639
- Source URL: https://arxiv.org/abs/2410.05639
- Reference count: 20
- One-line primary result: Improves model performance across benchmarks by up to 4.1 percentage points through data quality enhancement

## Executive Summary
DecorateLM is a data engineering framework that refines large language model pretraining corpora through three phases: rating texts against quality criteria, tagging with hierarchical labels, and editing for formalized format. The method uses a small language model (1.2B parameters) to process 100 billion tokens from raw corpus datasets, selecting 45 billion high-quality and diverse tokens for further training. Experiments demonstrate that DecorateLM significantly improves model performance across multiple benchmarks compared to baseline models trained on raw data.

## Method Summary
DecorateLM consists of two main components - a rating and tagging model, and an editing model. The rating and tagging model is trained jointly on 30,000 annotated samples for rating and tagging using MiniCPM-1.2B as base model, while the editing model is trained separately on 10,000 samples. The framework applies three phases to raw corpus data: first rating texts against eight quality dimensions (educational value, expertise, fact and trivia, reasoning level, scarcity, structural format, story-likeness, subjectivity), then tagging with a three-level hierarchical labeling system (21 first-level categories, 255 second-level tags, 793 third-level tags), and finally editing texts into more formalized formats. The decorated corpus is then used to train language models, with experiments showing performance improvements of 1.9-4.1 percentage points across benchmarks.

## Key Results
- Improves model performance across benchmarks by up to 4.1 percentage points compared to baseline models
- Reduces perplexity following the editing process, indicating improved data quality
- Demonstrates effectiveness of data quality enhancement approach for pretraining large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rating improves model performance by selecting data with high educational value, expertise, factual accuracy, and reasoning level.
- Mechanism: Higher-quality data provides better signal for learning representations, reducing noise in pretraining.
- Core assumption: The eight quality dimensions capture the most important aspects of useful pretraining data.
- Evidence anchors: Abstract mentions rating against quality criteria; section 3.2 shows Spearman correlation between model-provided scores and ground truth annotations.

### Mechanism 2
- Claim: Tagging ensures domain diversity and enables targeted data selection for specific tasks.
- Mechanism: A hierarchical tagging system allows sampling to balance representation across knowledge domains while maintaining coverage of important areas.
- Core assumption: The three-level tag hierarchy captures the structure of knowledge domains relevant to language model pretraining.
- Evidence anchors: Section 3.3 explains how tagging ensures diversity; section 4.3 describes sampling strategy among tags for balance.

### Mechanism 3
- Claim: Editing improves data quality by making texts more formal, clear, and consistent.
- Mechanism: Rephrasing noisy web data into more standardized formats reduces noise the model has to learn to ignore, improving signal-to-noise ratio.
- Core assumption: The editing model can successfully rephrase texts while preserving core information and meaning.
- Evidence anchors: Section 3.4 describes transformation goals; Figure 7 shows significant perplexity reduction after editing.

## Foundational Learning

- Concept: Supervised fine-tuning on labeled data
  - Why needed here: DecorateLM needs to learn to rate, tag, and edit texts based on human-defined criteria
  - Quick check question: What is the difference between supervised fine-tuning and unsupervised pretraining?

- Concept: Hierarchical classification
  - Why needed here: The tagging system uses a three-level hierarchy to organize knowledge domains
  - Quick check question: How does hierarchical classification differ from flat classification in terms of model architecture and evaluation?

- Concept: Text generation and paraphrasing
  - Why needed here: The editing component needs to rephrase texts while preserving meaning
  - Quick check question: What are the key challenges in text paraphrasing that need to be addressed by the editing model?

## Architecture Onboarding

- Component map: Raw corpus → Rating and Tagging → Filtering → Editing → Final decorated corpus → Model training
- Critical path: Data flows from raw corpus through three processing phases to produce enhanced training data
- Design tradeoffs: Using smaller model (1.2B parameters) for data processing vs. larger LLM; joint training of rating and tagging vs. separate models; random vs. targeted editing
- Failure signatures: Poor downstream performance despite high-quality data processing; model collapse during training; editing that changes meaning or introduces errors
- First 3 experiments:
  1. Test rating and tagging on small validation set to ensure working as expected
  2. Apply editing to small sample and manually check for quality and meaning preservation
  3. Train small model on decorated data subset and evaluate on downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DecorateLM handle the risk of introducing or amplifying biases present in the original data during rating, tagging, and editing processes?
- Basis in paper: [explicit] Ethical considerations section mentions biased data can lead to unfair outcomes
- Why unresolved: Paper acknowledges bias risk but doesn't provide specific detection or mitigation methods
- What evidence would resolve it: Details of bias detection/mitigation techniques integrated into DecorateLM's algorithms with evaluation results

### Open Question 2
- Question: What would be the impact on model performance if DecorateLM were applied to create a 1.1 trillion token dataset and used to train a model from scratch rather than just during decay stage?
- Basis in paper: [inferred] Limitations section suggests creating 1.1 trillion token dataset with DecorateLM
- Why unresolved: Only evaluated impact during decay stage, not initial pretraining
- What evidence would resolve it: Results from training significantly larger model from scratch using DecorateLM-processed dataset

### Open Question 3
- Question: How does DecorateLM's hierarchical three-level tagging system perform in specialized domains with complex content compared to more granular labeling approaches?
- Basis in paper: [explicit] Limitations section suggests exploring more granular labeling system for professional fields
- Why unresolved: Current three-level system may be insufficient for capturing complex real-world content
- What evidence would resolve it: Comparative studies showing performance with different tagging granularities on specialized domain data

## Limitations

- Data Quality Evaluation: Reliability of GPT-4 annotations for establishing ground truth remains uncertain
- Generalizability of Results: Experiments focus on Chinese language models, raising questions about performance in English/multilingual scenarios
- Trade-off Between Data Quantity and Quality: Reduction from 100B to 45B tokens may affect capabilities benefiting from broader data exposure

## Confidence

- Rating Effectiveness: High Confidence - Well-supported by experimental results showing 1.9-4.1 percentage point improvements
- Tagging Diversity Benefits: Medium Confidence - Demonstrates improved domain-specific performance but could be more thoroughly quantified
- Editing Quality Improvements: Medium Confidence - Perplexity reduction supports effectiveness but manual evaluation would strengthen confidence

## Next Checks

1. Apply DecorateLM to an English or multilingual corpus and evaluate whether similar performance improvements are observed to test generalizability beyond Chinese language context.

2. Conduct a blind human study where raters compare edited and original texts for quality, clarity, and meaning preservation to validate editing phase effectiveness beyond perplexity metrics.

3. Perform an ablation study systematically removing or modifying individual quality dimensions in rating criteria to determine which aspects contribute most to performance improvements and optimize the rating process.