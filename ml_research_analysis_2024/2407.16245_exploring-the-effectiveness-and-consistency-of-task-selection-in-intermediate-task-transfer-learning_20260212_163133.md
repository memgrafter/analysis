---
ver: rpa2
title: Exploring the Effectiveness and Consistency of Task Selection in Intermediate-Task
  Transfer Learning
arxiv_id: '2407.16245'
source_url: https://arxiv.org/abs/2407.16245
tags:
- task
- tasks
- transfer
- prompt
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We conduct a comprehensive evaluation of intermediate-task selection
  methods across 130 task combinations. Our analysis reveals significant performance
  variance in transfer learning across tasks, with embedding-based methods outperforming
  embedding-free and text embedding approaches.
---

# Exploring the Effectiveness and Consistency of Task Selection in Intermediate-Task Transfer Learning

## Quick Facts
- arXiv ID: 2407.16245
- Source URL: https://arxiv.org/abs/2407.16245
- Reference count: 38
- Key outcome: Embedding-based methods outperform embedding-free and text embedding approaches, with token-wise similarity achieving 82.5% nDCG in task prediction

## Executive Summary
This paper presents a comprehensive evaluation of intermediate-task selection methods across 130 task combinations using soft prompt tuning. The authors introduce a novel token-wise similarity approach (MAX) that outperforms traditional averaged embeddings by capturing richer task-specific patterns. Their analysis reveals that task embeddings constructed from fine-tuned weights better estimate transferability than embedding-free methods, while also highlighting significant performance variance due to training seed choice. The work identifies limitations of task embeddings for reasoning tasks and provides a framework for systematically evaluating transfer learning effectiveness.

## Method Summary
The study uses T5 Base with frozen weights and parameter-efficient soft prompt tuning (100 prompt tokens) for 30K steps across 13 source and 10 target tasks. Four task selection methods are implemented: RANDOM, SIZE, SEMB, and FEATURE, plus a novel MAX method based on token-wise similarity using maximum inner product search. Task embeddings are constructed from fine-tuned soft prompt weights and compared against embedding-free approaches. Performance is evaluated using nDCG scores to measure ranking quality of task selection methods, with Regret@k quantifying computational regret. The framework enables systematic comparison of how different task embedding construction methods predict transferability.

## Key Results
- Token-wise similarity in MAX method achieves highest average task prediction performance (82.5% nDCG)
- Task embeddings from fine-tuned weights improve prediction scores from 2.59% to 3.96% over embedding-free methods
- Training seed choice significantly impacts transfer performance, with up to 26.78% relative performance improvements for COPA
- Task embeddings do not consistently perform well on tasks requiring reasoning abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-wise similarity in MAX method outperforms averaged embeddings by capturing richer task-specific patterns
- Mechanism: MAX computes maximum cosine similarity for each token in target task against all tokens in source task, then averages these maxima, preserving fine-grained token interactions
- Core assumption: Most informative token-level alignment between tasks determines transferability better than global averaging
- Evidence anchors: Novel method achieves highest nDCG score of 82.5%; captures richer task information than FEATURE and UNIGRAM
- Break condition: If token distributions are highly uniform or tasks are too dissimilar at token level, maximum similarity scores may become uninformative

### Mechanism 2
- Claim: Task embeddings from fine-tuned weights outperform embedding-free and text embedding methods in predicting transferability
- Mechanism: Fine-tuned soft prompt weights encode task-specific transformations that reflect learned task representations better than static text encoders or metadata
- Core assumption: Learned prompt weights after fine-tuning capture latent task characteristics that correlate with transferability
- Evidence anchors: Embedding-based approaches show higher task prediction performance; FEATURE outperforms all other methods on average
- Break condition: If prompt tuning is unstable or training seeds cause large variance, embedding may not reliably reflect task properties

### Mechanism 3
- Claim: Training seed choice significantly impacts transfer performance, indicating instability in fine-tuning
- Mechanism: Different random seeds during prompt tuning lead to different local minima, causing variance in both intermediate and target task performance
- Core assumption: Fine-tuning process is non-deterministic enough that seed selection affects downstream transferability predictions
- Evidence anchors: Random seed significantly impacts transfer performance; different seeds lead to 7.69% to 26.78% relative performance improvements for COPA
- Break condition: If model or fine-tuning method is highly stable (e.g., large batch sizes, robust optimization), seed effects may be negligible

## Foundational Learning

- Concept: Task embeddings as vector representations of task characteristics
  - Why needed here: Enables quantitative comparison between source and target tasks for transferability prediction
  - Quick check question: If two tasks have embeddings with cosine similarity 0.9, does that guarantee good transfer?
    - Answer: No, high similarity does not guarantee transfer success; actual transfer experiments are needed.

- Concept: Parameter-efficient fine-tuning via soft prompts
  - Why needed here: Allows efficient adaptation of large PLMs without full fine-tuning, enabling large-scale intermediate-task experiments
  - Quick check question: Why freeze the PLM and only update prompt tokens in this work?
    - Answer: To reduce computational cost and avoid catastrophic forgetting while still capturing task-specific patterns.

- Concept: Normalized Discounted Cumulative Gain (nDCG) for ranking evaluation
  - Why needed here: Provides measure of how well predicted task rankings match ground truth ranking by transfer performance
  - Quick check question: What does an nDCG of 1.0 signify?
    - Answer: Perfect ranking matching the ideal order.

## Architecture Onboarding

- Component map: Pre-trained T5 Base (frozen) -> Soft prompt tokens (trainable, N=100) -> Task embedding constructors (FEATURE, UNIGRAM, MAX) -> Evaluation pipeline (transfer experiments, nDCG, Regret@k) -> Dataset loader (13 source, 10 target tasks)

- Critical path: 1. Fine-tune soft prompts on source tasks 2. Construct task embeddings from fine-tuned weights 3. Compute pairwise similarities between source and target task embeddings 4. Rank source tasks by similarity 5. Evaluate ranking quality against ground truth via nDCG

- Design tradeoffs: Prompt tuning vs full fine-tuning (speed vs expressiveness); Token-wise MAX vs averaging (granularity vs robustness); Embedding-free vs embedding-based (simplicity vs accuracy)

- Failure signatures: Low nDCG scores (<0.5) indicate poor embedding construction; High variance across seeds suggests instability in fine-tuning; Similar performance between embedding methods may indicate task similarity is not well captured

- First 3 experiments: 1. Implement FEATURE method and reproduce nDCG scores on subset of tasks 2. Compare MAX vs UNIGRAM embeddings on COPA and CB to observe token-level differences 3. Run prompt tuning with multiple seeds on single task to measure variance in task embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does effectiveness of token-wise similarity (MAX) compare to other task embedding construction methods when using different model architectures beyond T5 Base?
- Basis in paper: Study is limited to one specific model architecture (T5 Base), suggesting need to test robustness across different architectures
- Why unresolved: Different architectures may respond differently to task embedding methods
- What evidence would resolve it: Systematic evaluation of MAX and other task embedding methods across multiple model architectures (BERT, RoBERTa, GPT) would show if superiority is consistent or architecture-dependent

### Open Question 2
- Question: What specific characteristics of tasks requiring reasoning abilities make them resistant to task embeddings constructed from fine-tuned weights?
- Basis in paper: Paper observes that task embeddings do not consistently perform well on tasks requiring reasoning abilities
- Why unresolved: Paper identifies problem but does not investigate underlying reasons for poor performance on reasoning tasks
- What evidence would resolve it: Detailed analysis of differences in task embedding distributions between reasoning and non-reasoning tasks, along with correlation studies between reasoning complexity and embedding effectiveness

### Open Question 3
- Question: How does instability introduced by different random seeds during prompt transfer affect consistency of intermediate-task selection methods?
- Basis in paper: Paper highlights that training seed choice significantly impacts transfer performance
- Why unresolved: While paper notes impact of seed choice, it does not explore how this instability specifically affects reliability of task selection methods
- What evidence would resolve it: Comparative analysis of task selection performance across multiple random seeds and their correlation with transfer performance variance would quantify effect of seed-induced instability on task selection consistency

## Limitations
- Evaluation framework relies on relatively small set of 13 source and 10 target tasks, limiting generalizability
- Study focuses exclusively on soft prompt tuning with T5 Base, unclear whether findings extend to other PLMs or fine-tuning methods
- Does not systematically explore interaction between training seed variance and task difficulty or domain similarity
- Token-wise MAX method shows best performance but lacks theoretical justification for why maximum inner product similarity better captures transferability

## Confidence
- **High Confidence**: Training seed significantly impacts transfer performance (supported by quantitative variance analysis across 3 seeds)
- **Medium Confidence**: Embedding-based methods outperform embedding-free approaches (robust across multiple task pairs, though effect sizes vary)
- **Medium Confidence**: Task embeddings do not consistently perform well on reasoning tasks (observed pattern but limited reasoning task sample size)
- **Low Confidence**: Token-wise similarity captures richer task information (method is novel and shows best performance, but theoretical mechanism remains unexplained)

## Next Checks
1. **Replication with expanded task set**: Validate relative performance of FEATURE, UNIGRAM, and MAX methods on at least 50 additional task pairs beyond original 130 combinations to test generalizability of ranking patterns.

2. **Seed stability analysis**: Systematically vary training seeds (10+ seeds per task) and measure correlation between seed-induced embedding variance and task selection performance degradation to quantify relationship between stability and transferability prediction quality.

3. **Cross-model validation**: Apply same task selection framework to BERT and RoBERTa models with soft prompt tuning to determine whether superiority of token-wise MAX similarity is model-specific or represents more general principle of task representation.