---
ver: rpa2
title: Multi-Document Financial Question Answering using LLMs
arxiv_id: '2411.07264'
source_url: https://arxiv.org/abs/2411.07264
tags:
- arxiv
- question
- knowledge
- answering
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of multi-document financial
  question answering, which is more complex than single-document scenarios due to
  the need to integrate information from multiple sources. The authors propose two
  methods: RAGSEM, which combines semantic tagging with retrieval-augmented generation
  (RAG), and KGRAG, which integrates knowledge graphs with RAG and semantic tagging.'
---

# Multi-Document Financial Question Answering using LLMs

## Quick Facts
- arXiv ID: 2411.07264
- Source URL: https://arxiv.org/abs/2411.07264
- Authors: Shalin Shah; Srikanth Ryali; Ramasubbu Venkatesh
- Reference count: 40
- One-line primary result: Two proposed methods combining semantic tagging and knowledge graphs with RAG significantly outperform traditional RAG on multi-document financial QA tasks

## Executive Summary
This paper addresses the challenge of multi-document financial question answering, which is more complex than single-document scenarios due to the need to integrate information from multiple sources. The authors propose two methods: RAG_SEM, which combines semantic tagging with retrieval-augmented generation (RAG), and KG_RAG, which integrates knowledge graphs with RAG and semantic tagging. Both methods outperform traditional RAG across nine evaluation metrics, including faithfulness, correctness, relevance, and similarity. KG_RAG particularly excels, outperforming RAG_SEM in four out of nine metrics. The methods were tested on 18 10-K reports from six companies over three years, answering 111 complex questions.

## Method Summary
The paper proposes two methods for multi-document financial question answering: RAG_SEM and KG_RAG. RAG_SEM combines semantic tagging with retrieval-augmented generation, where documents and questions are tagged with semantic metadata (entities, dates, industries) before indexing to improve retrieval relevance. KG_RAG extends this by integrating knowledge graphs constructed through knowledge distillation from a large LLM to a smaller model, enabling multi-hop reasoning beyond what text chunks alone can provide. Both methods were tested against baseline RAG using 18 10-K reports from six companies (Apple, Microsoft, Alphabet, NVIDIA, Amazon, Tesla) for 2021-2023, answering 111 complex financial questions across nine evaluation metrics.

## Key Results
- KG_RAG outperforms RAG baseline on 8 out of 9 evaluation metrics
- KG_RAG outperforms RAG_SEM on 4 out of 9 metrics
- Both proposed methods show significant improvements in faithfulness, correctness, relevance, and similarity metrics
- Knowledge graph integration enables better handling of complex multi-hop reasoning questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic tagging improves RAG retrieval accuracy by filtering relevant documents based on entity, date, and domain tags
- Mechanism: Documents and questions are tagged with semantic metadata before indexing. During retrieval, query tags guide the vector search to retrieve more contextually relevant chunks
- Core assumption: Semantic tags meaningfully correlate with document relevance for a given question
- Evidence anchors: Abstract mentions tagging with industries, organization names, dates, etc.; section describes tagging questions with same prompt used for documents
- Break condition: If tags are noisy or misaligned with question intent, retrieval accuracy degrades

### Mechanism 2
- Claim: Knowledge graphs add fine-grained factual context that enables multi-hop reasoning beyond what RAG can retrieve from text chunks alone
- Mechanism: KG triples extracted from documents provide structured facts. These are combined with retrieved text chunks in the LLM context to support reasoning requiring linking multiple facts across documents
- Core assumption: KG triples capture salient relationships difficult to retrieve via vector similarity alone
- Evidence anchors: Abstract states KG triples are highly fine-grained for effective retrieval of needle-in-haystack facts
- Break condition: If KG construction is inaccurate or incomplete, multi-hop reasoning fails

### Mechanism 3
- Claim: Fine-tuning a small KG generator via knowledge distillation from a large LLM balances speed and accuracy for knowledge graph extraction
- Mechanism: A large teacher LLM extracts KG triples; a smaller student model is fine-tuned to mimic this behavior, enabling faster KG generation with less compute
- Core assumption: Knowledge distillation preserves sufficient quality while improving inference speed
- Evidence anchors: Abstract and section describe using small model fine-tuned via knowledge distillation from large model
- Break condition: If distillation quality drops, KG triples become unreliable

## Foundational Learning

- Concept: Semantic tagging and entity extraction
  - Why needed here: Tags filter retrieval to improve relevance in multi-document scenarios
  - Quick check question: Can you manually tag a sample sentence with entities, dates, and industries?

- Concept: Knowledge graph construction and triple representation
  - Why needed here: KGs encode structured relationships enabling multi-hop reasoning
  - Quick check question: Can you convert a sentence like "Apple acquired X" into a triple?

- Concept: Retrieval-Augmented Generation (RAG) workflow
  - Why needed here: RAG is the baseline; improvements are layered on top
  - Quick check question: What are the three main steps of a basic RAG pipeline?

## Architecture Onboarding

- Component map: Input Question -> Semantic Tagger -> Vector Database with Semantic Tags -> KG Generator -> KG Store -> LLM Synthesizer with Custom Prompt -> Output Answer

- Critical path: Tag question → Retrieve chunks → Retrieve KG triples → Generate answer

- Design tradeoffs:
  - Using semantic tags adds preprocessing overhead but improves retrieval precision
  - KG triples add retrieval complexity but enable multi-hop reasoning
  - Distilled KG model trades some accuracy for speed

- Failure signatures:
  - Poor tag quality → irrelevant chunks retrieved
  - KG errors → hallucinated facts or failed multi-hop reasoning
  - LLM synthesis issues → incoherent answers despite good context

- First 3 experiments:
  1. Run RAG baseline on a subset of 10-K reports and measure relevance and correctness
  2. Add semantic tagging to RAG and compare performance metrics
  3. Add KG retrieval to the semantic RAG pipeline and measure improvement in multi-hop questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KG_RAG compare to other graph-based RAG methods like GraphRAG [10] in terms of accuracy, latency, and computational efficiency?
- Basis in paper: The paper mentions that future work could include comparisons with methods like GraphRAG [10] for the metrics as well as latency
- Why unresolved: The paper does not include a direct comparison with other graph-based RAG methods, so it is unclear how KG_RAG performs relative to these alternatives
- What evidence would resolve it: Conducting experiments comparing KG_RAG to GraphRAG and other graph-based RAG methods on the same datasets and metrics, including latency and computational efficiency, would provide a clear comparison

### Open Question 2
- Question: What specific factors or model architectures contribute most to the observed performance gains in KG_RAG compared to RAG_SEM and RAG?
- Basis in paper: The paper suggests that future research could focus on understanding the specific factors or model architectures that contribute most to the observed performance gains
- Why unresolved: While the paper demonstrates that KG_RAG outperforms RAG_SEM and RAG, it does not delve into the specific factors or architectural elements responsible for these improvements
- What evidence would resolve it: Analyzing the impact of different components of KG_RAG, such as the knowledge graph construction process, semantic tagging, and the integration of knowledge graph triples, on the overall performance would help identify the key contributors to the gains

### Open Question 3
- Question: How does the fine-tuning process of the small language model using knowledge distillation from a large language model impact the quality and efficiency of knowledge graph construction?
- Basis in paper: The paper mentions that KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation from a large teacher model, and it highlights the advantages of using a small model in terms of speed and cost
- Why unresolved: The paper does not provide detailed insights into the fine-tuning process or how it affects the quality and efficiency of knowledge graph construction
- What evidence would resolve it: Investigating the fine-tuning process, including the choice of distillation techniques, the impact on the quality of knowledge graph triples, and the trade-offs between speed, cost, and accuracy, would provide a clearer understanding of the benefits and limitations of this approach

## Limitations

- The exact prompts for semantic tagging and knowledge distillation setup are not provided, making full replication challenging
- The evaluation relies heavily on LLM-based metrics which may introduce evaluation bias
- The dataset, while specific to financial 10-K reports, is relatively small (18 documents, 111 questions) and may not generalize to other domains or document types

## Confidence

- **High Confidence:** The general effectiveness of combining semantic tagging with RAG for improved retrieval accuracy in multi-document scenarios
- **Medium Confidence:** The specific performance improvements claimed for KG_RAG over RAG_SEM and baseline RAG, due to lack of detailed implementation specifics
- **Medium Confidence:** The scalability and generalization claims across different financial document types and question complexities

## Next Checks

1. **Implementation Verification:** Reconstruct the semantic tagging and knowledge graph generation pipelines using the described methodology to verify the claimed improvements hold across different LLM implementations

2. **Cross-Domain Testing:** Apply the KG_RAG method to non-financial multi-document QA tasks (e.g., legal documents, research papers) to assess domain transferability and identify limitations

3. **Ablation Study:** Systematically remove semantic tagging and knowledge graph components individually to quantify their independent contributions to the overall performance improvements