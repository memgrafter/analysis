---
ver: rpa2
title: Are Large Language Models Good Classifiers? A Study on Edit Intent Classification
  in Scientific Document Revisions
arxiv_id: '2410.02028'
source_url: https://arxiv.org/abs/2410.02028
tags:
- llms
- classification
- seqc
- table
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically investigates the use of large language
  models (LLMs) for classification tasks, focusing on edit intent classification (EIC)
  in scientific document revisions. The authors propose a general framework featuring
  four approaches: generation-based (Gen) and three encoding-based methods (SeqC,
  SNet, XNet).'
---

# Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions

## Quick Facts
- arXiv ID: 2410.02028
- Source URL: https://arxiv.org/abs/2410.02028
- Authors: Qian Ruan; Ilia Kuznetsov; Iryna Gurevych
- Reference count: 40
- Primary result: Encoding-based approaches, particularly SeqC, outperform both fully fine-tuned PLMs and instruction-tuned LLMs, achieving state-of-the-art performance with macro average F1 score of 84.3

## Executive Summary
This study systematically investigates whether large language models (LLMs) are effective classifiers by focusing on edit intent classification (EIC) in scientific document revisions. The authors propose a general framework with four approaches: generation-based (Gen) and three encoding-based methods (SeqC, SNet, XNet). They evaluate these approaches using eight LLMs and two PLMs across various training strategies. The results demonstrate that encoding-based approaches, particularly SeqC, achieve superior performance compared to traditional fine-tuned PLMs and instruction-tuned larger LLMs, with the best results obtained using 13B Llama2 or 8B Llama3 models.

## Method Summary
The authors propose a general framework for evaluating LLMs as classifiers, featuring four distinct approaches. The generation-based approach (Gen) treats classification as a generation task where LLMs generate classification labels as output. The encoding-based approaches include SeqC (sequence classification), SNet (sentence network), and XNet (extended network). These approaches leverage LLMs' token embeddings as feature representations by extracting the final token's hidden state and transforming it into classification vectors through linear layers or concatenation operations. The framework is evaluated across multiple LLMs (Llama2, Llama3, Mistral, Gemma) and PLMs (RoBERTa, BERT) with various training strategies and input formatting options.

## Key Results
- Encoding-based approaches, particularly SeqC, outperform both fully fine-tuned PLMs and instruction-tuned larger LLMs
- The best results are achieved using 13B Llama2 or 8B Llama3 models with SeqC approach
- SeqC demonstrates superior performance with perfect answer inclusion rate and better inference efficiency
- The authors create Re3-Sci2.0 dataset with 1,780 scientific document revisions and 94,482 labeled edits across multiple research domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding-based approaches enable LLMs to perform classification by leveraging their token embeddings as feature representations
- Mechanism: These approaches extract the final token's hidden state representation and transform it into a classification vector through linear layers or concatenation operations
- Core assumption: The final token's hidden state contains sufficient semantic information to distinguish between different classification labels
- Evidence anchors: "encoding-based approaches, particularly SeqC, outperform both fully fine-tuned PLMs and instruction-tuned larger LLMs"; "Approach SeqC treats the task as a sequence classification task using LLMs equipped with a linear classification layer on top. It utilizes the last hidden states of the last token"

### Mechanism 2
- Claim: Transformation functions that incorporate differences between sentence embeddings are more effective for edit intent classification
- Mechanism: Functions like `diffABS` (absolute difference) and `n-diffABS` (concatenation of new embedding with absolute difference) capture the semantic changes between original and edited sentences
- Core assumption: Edit intent classification fundamentally depends on understanding the differences between text versions rather than just the content of individual sentences
- Evidence anchors: "the EIC task relies on analyzing the variations between two versions of a text"; "Table 3(c) indicates that when using SNet, fdif f ABS substantially outperforms all other functions across all LLMs"

### Mechanism 3
- Claim: Structured input format with task instructions improves LLM classification performance by providing explicit task context
- Mechanism: Wrapping input text in structure tokens like `<instruction></instruction>`, `<old></old>`, and `<new></new>` helps LLMs better parse and understand the task requirements and input components
- Core assumption: LLMs benefit from explicit structural cues that delineate different parts of the input and the task objective
- Evidence anchors: "Table 2(a) shows that using structured input instead of natural language input improves performance for the Llama2 models in approach Gen"; "Table 2(b) shows that in approach SeqC, using structured inputs positively impacts RoBERTa and all LLMs except for Mistral-Instruct"

## Foundational Learning

- Concept: Edit Intent Classification (EIC)
  - Why needed here: This is the core task being studied - understanding why and how people modify text requires identifying the underlying purpose of edits
  - Quick check question: What are the five edit intent labels used in the Re3-Sci dataset?

- Concept: Fine-tuning vs Instruction-tuning
  - Why needed here: The paper systematically compares different fine-tuning approaches (Gen, SeqC, SNet, XNet) against instruction-tuning baselines to understand their relative effectiveness
  - Quick check question: What is the key difference between the Gen approach and the encoding-based approaches?

- Concept: Transformation functions for pair inputs
  - Why needed here: Understanding how to effectively combine embeddings from two related inputs (original and edited sentences) is crucial for the SNet and XNet approaches
  - Quick check question: Which transformation function was found to be most effective for the SNet approach?

## Architecture Onboarding

- Component map: Data preprocessing → Model selection (LLM/PLM + approach) → Fine-tuning with appropriate input format and transformation → Evaluation on validation set → Testing on held-out test set
- Critical path: Data preprocessing → Model selection (LLM/PLM + approach) → Fine-tuning with appropriate input format and transformation → Evaluation on validation set → Testing on held-out test set
- Design tradeoffs: Gen approach offers flexibility but suffers from AIR issues and lower efficiency; encoding approaches provide better control and efficiency but require careful design of transformation functions and input formatting
- Failure signatures: Low AIR scores indicate generation failures; poor performance on validation despite good training suggests overfitting or task misunderstanding; significant performance gaps between chat and non-chat versions may indicate instruction-following capability differences
- First 3 experiments:
  1. Fine-tune a small LLM (Llama2-7B) using SeqC approach with structured input on the EIC dataset, compare against baseline RoBERTa
  2. Test different transformation functions (diff, diffABS, n-diffABS) with SNet approach on same dataset to identify optimal combination strategy
  3. Evaluate the impact of adding task instructions to structured input by comparing performance with and without instructions across multiple LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of model size on the performance of LLMs for edit intent classification?
- Basis in paper: The authors systematically compare different fine-tuning approaches, training strategies, base PLMs and LLMs, and they find that the 13B Llama2 and 8B Llama3 models demonstrate the greatest potential and achieve the best results.
- Why unresolved: While the paper identifies the best-performing models, it does not provide a detailed analysis of how model size affects performance across different tasks or how the optimal size might vary depending on the complexity of the classification task.
- What evidence would resolve it: A comprehensive study varying model sizes systematically across multiple classification tasks, analyzing the trade-off between model size, performance, and computational efficiency.

### Open Question 2
- Question: How does the performance of LLMs for edit intent classification compare to other state-of-the-art methods specifically designed for this task?
- Basis in paper: The authors achieve state-of-the-art performance on the EIC task with a macro average F1 score of 84.3 using their best-performing model. However, they do not provide a direct comparison with other specialized methods for EIC.
- Why unresolved: The paper focuses on demonstrating the effectiveness of LLMs for EIC, but does not compare their performance against methods specifically designed for this task, such as those using feature engineering or other specialized architectures.
- What evidence would resolve it: A direct comparison of the proposed LLM-based approach with other state-of-the-art methods for EIC on the same dataset, evaluating their performance and identifying the strengths and weaknesses of each approach.

### Open Question 3
- Question: How well do the findings on edit intent classification generalize to other domains and languages?
- Basis in paper: The authors investigate the generalizability of their findings on five further classification tasks, but these tasks are all in English and related to scientific text. They acknowledge that their focus on English-language scientific publications stems from the limited availability of openly licensed scholarly publications in other languages.
- Why unresolved: While the paper demonstrates the effectiveness of their approach on various classification tasks, it does not explore its applicability to other domains or languages, which is crucial for real-world applications.
- What evidence would resolve it: Applying the proposed approach to edit intent classification in other domains (e.g., news articles, social media posts) and languages, evaluating its performance and identifying any domain-specific or language-specific challenges.

## Limitations
- The experimental scope is constrained to edit intent classification in scientific documents, which may not generalize to other domains or classification tasks
- The evaluation focuses on standard metrics without exploring robustness to adversarial examples, domain shifts, or temporal variations in scientific writing styles
- The analysis of Re3-Sci2.0 is based on a single snapshot of revisions and may not capture long-term editing patterns or evolution in scientific discourse

## Confidence

**High Confidence**: The core finding that encoding-based approaches (particularly SeqC) outperform both fully fine-tuned PLMs and instruction-tuned LLMs for edit intent classification. This is supported by consistent performance across multiple LLMs, clear statistical improvements in macro F1 scores, and validation through both quantitative metrics and qualitative analysis of the AIR metric.

**Medium Confidence**: The claim that structured input formats universally improve LLM performance. While the results show positive effects for most models, the exception of Mistral-Instruct suggests that model-specific factors influence the effectiveness of input formatting.

**Low Confidence**: The assertion that the absolute difference transformation function (diffABS) is universally optimal for all SNet implementations. The effectiveness of transformation functions appears to be model-dependent, with different functions showing varying performance across different LLMs.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the best-performing SeqC approach to edit intent classification tasks in non-scientific domains (e.g., legal documents, Wikipedia edits, or social media content) to assess the generalizability of the findings beyond scientific writing.

2. **Temporal Robustness Analysis**: Evaluate model performance on scientific document revisions from different time periods (e.g., pre-2000 vs post-2020) to determine whether the approach maintains effectiveness as scientific writing conventions evolve over time.

3. **Adversarial Edit Detection**: Design and test the models against intentionally obfuscated or misleading edits that preserve semantic meaning while changing surface form, to assess the robustness of the classification approach to adversarial editing strategies.