---
ver: rpa2
title: 'SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources
  with Retrieval and Semantic Parsing'
arxiv_id: '2406.00562'
source_url: https://arxiv.org/abs/2406.00562
tags:
- answer
- question
- linguistics
- gold
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPAGHETTI, a hybrid QA system that leverages
  large language models to answer questions using heterogeneous data sources including
  knowledge bases, text, tables, and infoboxes. SPAGHETTI uses a pipeline combining
  semantic parsing, retrieval, and LLM-based generation to gather evidence from these
  sources and generate answers.
---

# SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing

## Quick Facts
- arXiv ID: 2406.00562
- Source URL: https://arxiv.org/abs/2406.00562
- Reference count: 40
- State-of-the-art 56.5% exact match rate on COMP MIX dataset

## Executive Summary
SPAGHETTI is a hybrid QA system that leverages large language models to answer questions using heterogeneous data sources including knowledge bases, text, tables, and infoboxes. The system uses a pipeline combining semantic parsing, retrieval, and LLM-based generation to gather evidence from these sources and generate answers. On the COMP MIX dataset, SPAGHETTI achieves state-of-the-art performance with a 56.5% exact match rate, and manual analysis indicates over 90% accuracy, highlighting the limitations of exact match metrics for evaluating modern QA systems.

## Method Summary
SPAGHETTI implements a parallel pipeline that retrieves evidence from four sources: Wikidata using semantic parsing with LLM-enhanced entity linking, Wikipedia text using ColBERT retrieval with LLM verification, tables/infoboxes using fine-tuned ColBERT with LLM extraction, and LLM-generated claims with verification. The system fine-tunes ColBERT for table retrieval on NQ-Tables dataset and uses GPT-3.5/GPT-4 for answer generation and verification. All retrieved evidence is combined and fed to a final LLM-based answer generator using few-shot prompting.

## Key Results
- Achieves 56.5% exact match rate on COMP MIX dataset, outperforming state-of-the-art baselines
- Manual analysis on sample shows >90% accuracy, indicating exact match metrics are overly strict
- Identifies evidence retrieval and selection as key failure modes, with 154/388 errors due to evidence issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic parsing with LLM-based entity linking improves knowledge base retrieval accuracy by resolving ambiguous entity mentions
- Mechanism: LLM detects entity mentions and generates descriptions, then feeds to ReFinED for disambiguation and Wikidata linking
- Core assumption: LLMs possess sufficient world knowledge to accurately identify and describe entity mentions
- Evidence: Illustrative example shows ReFinED alone fails to identify "Academy Award" but succeeds with LLM-provided context

### Mechanism 2
- Claim: Combining evidence from multiple heterogeneous sources through verification and aggregation reduces individual source errors
- Mechanism: Retrieves evidence from all sources, applies verification steps, then combines verified evidence for final answer
- Core assumption: Different knowledge sources have complementary strengths and weaknesses
- Evidence: Mentions redundancy across sources can help reduce errors from earlier pipeline stages

### Mechanism 3
- Claim: GPT-4 matching better captures semantic equivalence than exact match evaluation
- Mechanism: Uses GPT-4 with few-shot prompt to determine semantic matching between generated and gold answers
- Core assumption: GPT-4 can accurately judge semantic equivalence accounting for paraphrasing and format differences
- Evidence: Manual analysis suggests >90% accuracy vs 56.5% exact match, indicating EM is no longer suitable

## Foundational Learning

- **Semantic parsing for knowledge base queries**: Required to convert natural language questions into structured queries for Wikidata. Quick check: Given "Who directed Inception?", what would be the corresponding SPARQL query pattern?

- **Information retrieval and ranking**: Needed for dense retrieval from Wikipedia text and tables using ColBERT. Quick check: What's the key difference between dense retrieval (like ColBERT) and traditional sparse retrieval?

- **Large language model few-shot prompting**: Essential for LLM-based answer generation and verification. Quick check: What are the key components of an effective few-shot prompt for an LLM?

## Architecture Onboarding

- **Component map**: Entity Linking -> Semantic Parsing -> Wikidata Query; ColBERT Retrieval -> LLM Verification -> Text Evidence; Fine-tuned ColBERT -> LLM Extraction -> Table Evidence; LLM Generation -> Verification -> Claims Evidence; All Evidence -> Answer Combiner -> Final Answer

- **Critical path**: Entity detection/linking → parallel retrieval from all sources → verification of evidence → final answer generation from combined evidence. LLM inference steps and retrieval indexing are most time-consuming.

- **Design tradeoffs**: Trades computational cost for accuracy by using multiple LLMs and retrieval steps; trades latency for robustness by parallelizing evidence gathering; uses fine-tuned ColBERT vs other retrievers for quality vs efficiency tradeoff.

- **Failure signatures**: Entity linking failures (empty KB responses), retrieval failures (relevant evidence not found), verification failures (incorrectly accepted/rejected claims), combiner failures (conflicting evidence leads to wrong answers).

- **First 3 experiments**:
  1. Run on 10 questions, verify each component produces output, check entity linking on simple questions
  2. Test retrieval components independently with known questions, verify fine-tuned table retriever improves over base ColBERT
  3. Test verification mechanism with known correct/incorrect claims, verify correct acceptance/rejection

## Open Questions the Paper Calls Out

- **Evidence selection and merging improvement**: How to handle conflicting or misleading evidence from heterogeneous sources when SPAGHETTI produces incorrect answers despite evidence containing correct answer (154/388 error cases).

- **Conversational question extension**: Can SPAGHETTI be extended to handle conversational questions and chitchat involving facts beyond single-turn QA?

- **Table preprocessing improvements**: How to improve preprocessing and linearization of Wikipedia tables to handle edge cases like multi-column cells and color-highlighted cells?

## Limitations

- Entity linking improvement relies heavily on LLM's ability to identify relevant entities, with no ablation studies quantifying contribution
- GPT-4 matching as evaluation metric is a black box and can make errors itself
- Manual analysis based on only 100 examples from 2,764 test examples is relatively small sample

## Confidence

**High Confidence**: Parallel retrieval from heterogeneous sources architecture is well-established; exact match metrics being overly strict is well-supported by prior work.

**Medium Confidence**: 56.5% EM performance and >90% accuracy claims; performance improvement over baselines is plausible but exact component contributions unclear without ablation studies.

**Low Confidence**: LLM-based entity linking significantly improves accuracy; only one illustrative example provided without quantification of success rate versus traditional methods.

## Next Checks

1. **Ablation Study on Entity Linking**: Run system with and without LLM-based entity description generation to quantify actual contribution to knowledge base retrieval accuracy.

2. **GPT-4 Evaluation Reliability Test**: Have human annotators independently evaluate answers and compare with GPT-4's matching decisions to assess reliability.

3. **Individual Component Performance Analysis**: Analyze 50 failed questions to identify which source contained gold answer and where pipeline failed, revealing main bottlenecks.