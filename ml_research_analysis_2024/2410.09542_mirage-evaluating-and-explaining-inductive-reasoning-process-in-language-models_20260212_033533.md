---
ver: rpa2
title: 'MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language
  Models'
arxiv_id: '2410.09542'
source_url: https://arxiv.org/abs/2410.09542
tags:
- reasoning
- inductive
- uni00000013
- uni00000011
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIRAGE, a synthetic dataset designed to comprehensively
  evaluate and analyze large language models' (LLMs) inductive reasoning capabilities.
  MIRAGE addresses limitations in previous work by providing both inductive and deductive
  evaluation tasks with flexible test data across various scenarios, including list
  transformations, real-world problems, code generation, and string transformations.
---

# MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models

## Quick Facts
- arXiv ID: 2410.09542
- Source URL: https://arxiv.org/abs/2410.09542
- Reference count: 40
- Large language models rely on neighbor-based rather than rule-based reasoning for inductive tasks

## Executive Summary
This paper introduces MIRAGE, a synthetic dataset designed to evaluate and analyze large language models' inductive reasoning capabilities. The dataset addresses limitations in previous work by providing both inductive and deductive evaluation tasks across multiple scenarios including list transformations, real-world problems, code generation, and string transformations. Through extensive experiments, the authors reveal that LLMs are poor rule-based reasoners, often achieving correct deductions without correctly inducing underlying rules. Instead, they find that LLMs are good neighbor-based reasoners, relying on observed facts close to test examples in feature space to perform reasoning, which enables strong performance within localized regions but lacks generalizability.

## Method Summary
The MIRAGE dataset is generated using five basic operations (Add, Copy, Map, Pad, Swap) to create rules and facts across four scenarios: List Transformations (LT), Real-World Problems (RP), Code Generation (CG), and String Transformations (ST). The evaluation framework consists of two tasks: Rule Induction (RI) where models must identify the transformation rule from observed facts, and Example Inference (EI) where models must predict outputs for new inputs based on observed facts. Models are evaluated using zero-shot and few-shot prompting with various methods (IO, ID, CoT, SC, SR, HR). The neighbor-based reasoning hypothesis is tested by analyzing how model performance varies with the distance between observed facts and test examples in feature space.

## Key Results
- LLMs achieve significantly higher accuracy on example inference (EI) compared to rule induction (RI) tasks across all scenarios
- Model performance on EI remains high even when RI accuracy is low, indicating deduction doesn't rely on correct induction
- Inductive reasoning transferability is form-dependent rather than rule-dependent, with best performance when observed facts and test cases share similar scenarios
- Neighbor-based reasoning mechanism is validated through experiments showing performance correlates with distance between observed facts and test examples in feature space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs rely on neighbor-based reasoning rather than rule-based reasoning for inductive tasks
- Mechanism: The model leverages observed facts close to the test example in feature space to predict outputs without needing to induce correct rules
- Core assumption: Continuous functions preserve neighborhood properties, allowing close inputs to have similar outputs
- Evidence anchors:
  - [abstract] "LLMs are good neighbor-based reasoners... the model tends to focus on observed facts that are close to the current test example in feature space"
  - [section] "continuous functions preserve the neighborhood property... the close distance between yt and y may allow LLM to predict yt based on y"
  - [corpus] Weak evidence - related papers focus on chain-of-thought reasoning failures rather than neighbor-based mechanisms
- Break condition: If the test example has no close neighbors in the observed fact set, the model's performance degrades significantly

### Mechanism 2
- Claim: Performance gap between rule induction and example inference tasks reveals lack of rule-based reasoning
- Mechanism: Models achieve high accuracy on example inference even when rule induction accuracy is low, indicating deduction doesn't rely on correct induction
- Core assumption: If reasoning were rule-based, correct induction would be prerequisite for correct deduction
- Evidence anchors:
  - [abstract] "when conducting inductive reasoning, they do not rely on a correct rule to answer the unseen case"
  - [section] "the model's performance on rule induction is noticeably worse than on example inference in almost all cases"
  - [corpus] Weak evidence - corpus focuses on reasoning failures but not specifically on rule vs neighbor-based distinctions
- Break condition: If the observed facts contain enough diverse examples, the model might accidentally infer the correct rule through pattern matching

### Mechanism 3
- Claim: Inductive reasoning transferability is form-dependent rather than rule-dependent
- Mechanism: Models perform better when observed facts and test cases share similar forms, even if underlying rules are identical
- Core assumption: LLMs rely on surface-level pattern matching rather than abstract rule understanding
- Evidence anchors:
  - [abstract] "LLMs lack transferability in inductive reasoning... the inductive reasoning process of the LLM is form-related"
  - [section] "the highest performance occurs when the scenarios of the observed and test facts are consistent"
  - [corpus] Weak evidence - related papers don't address transferability issues in inductive reasoning
- Break condition: If models are explicitly trained to focus on abstract rules rather than surface forms, transferability might improve

## Foundational Learning

- Concept: Continuous functions in mathematics
  - Why needed here: The neighbor-based reasoning mechanism relies on the property that continuous functions preserve neighborhood relationships
  - Quick check question: If f(x) = x², are inputs close to 2 mapped to outputs close to 4? (Yes, since f(1.9) = 3.61 and f(2.1) = 4.41)

- Concept: Feature space and distance metrics
  - Why needed here: Understanding how observed facts are positioned relative to test examples in feature space is crucial for the neighbor-based reasoning explanation
  - Quick check question: If we use Chebyshev distance and have vectors [3,4,7] and [3,3,6], what is their distance? (max(|3-3|, |4-3|, |7-6|) = 1)

- Concept: Inductive vs deductive reasoning distinction
  - Why needed here: The paper's core contribution is showing that LLMs fail at the inductive stage but succeed at deduction, which requires understanding this distinction
  - Quick check question: If given facts [A,B,C] → [B+C,B+C,C], [0,1,3] → [4,4,3], and [3,4,7] → ?, what type of reasoning is needed for each part? (Induction for first, deduction for second)

## Architecture Onboarding

- Component map: Rule generation -> Fact generation -> Data filtering -> Scenario transformation -> Evaluation tasks
- Critical path: Rule generation → Fact generation → Data filtering → Scenario transformation → Evaluation tasks
- Design tradeoffs: Synthetic dataset allows controlled experiments but may not fully capture real-world complexity; multiple scenarios increase validity but add complexity
- Failure signatures: High rule induction error with low example inference error indicates neighbor-based rather than rule-based reasoning; poor cross-scenario performance indicates form-dependency
- First 3 experiments:
  1. Replicate the rule induction vs example inference accuracy comparison on a small subset of the dataset
  2. Test neighbor-based reasoning by comparing performance when removing close vs distant observed facts
  3. Validate the continuous function property by checking if nearby inputs produce similar outputs for generated rules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of neighbor-based reasoning scale with the dimensionality of input vectors?
- Basis in paper: [inferred] The paper demonstrates neighbor-based reasoning across dimensions D=3, 5, 8, showing varying performance, but doesn't explore higher dimensions systematically.
- Why unresolved: The paper only tests up to D=8, leaving open questions about performance at much higher dimensions where vector spaces become more sparse.
- What evidence would resolve it: Systematic experiments varying D from 3 to 50+ while measuring performance on both rule induction and example inference tasks.

### Open Question 2
- Question: Can models be trained to develop rule-based reasoning capabilities instead of relying on neighbor-based matching?
- Basis in paper: [explicit] The paper explicitly states "Future work could explore methods to encourage the model to follow rules more closely during reasoning" in the Limitations section.
- Why unresolved: The paper only evaluates existing models without attempting to modify their reasoning mechanisms through training interventions.
- What evidence would resolve it: Training studies comparing models trained with rule-focused objectives versus standard objectives, measuring both rule induction accuracy and generalization.

### Open Question 3
- Question: How do different distance metrics for neighbor identification affect reasoning performance?
- Basis in paper: [explicit] The paper tests Euclidean, Manhattan, and Minkowski distances in supplementary experiments, finding their performance inferior to Chebyshev distance.
- Why unresolved: The paper only compares three alternatives and doesn't explore the theoretical reasons why Chebyshev distance works better or whether even better metrics exist.
- What evidence would resolve it: Systematic comparison of various distance metrics (cosine similarity, Mahalanobis distance, learned metrics) with theoretical analysis of their properties.

### Open Question 4
- Question: Does the neighbor-based reasoning mechanism generalize to other reasoning tasks beyond inductive transformations?
- Basis in paper: [inferred] The paper focuses exclusively on inductive reasoning tasks, but the neighbor-based mechanism could theoretically apply to other reasoning domains.
- Why unresolved: The study is limited to the specific transformation tasks in MIRAGE, leaving open whether this is a general reasoning mechanism or task-specific.
- What evidence would resolve it: Testing the neighbor-based reasoning hypothesis on mathematical word problems, logical reasoning tasks, and causal inference problems.

## Limitations
- The synthetic nature of MIRAGE may not fully capture the complexity of real-world inductive reasoning tasks
- The theoretical foundation connecting continuous function properties to LLM behavior is not rigorously established
- Form-dependency findings may be partially explained by the specific construction of the synthetic dataset

## Confidence
- **High Confidence**: The empirical finding that LLMs achieve high example inference accuracy despite low rule induction accuracy is well-supported by experimental results
- **Medium Confidence**: The neighbor-based reasoning explanation for LLM behavior is plausible but lacks rigorous theoretical grounding
- **Low Confidence**: The generalizability of form-dependency findings beyond the specific synthetic dataset used in MIRAGE

## Next Checks
1. Cross-dataset validation: Test the neighbor-based reasoning hypothesis on non-synthetic datasets with clear rule structures to determine if the mechanism generalizes beyond MIRAGE's controlled environment

2. Feature space analysis: Conduct ablation studies removing observed facts at varying distances from test examples to quantify the relationship between neighborhood properties and prediction accuracy

3. Theoretical formalization: Develop mathematical proofs showing how LLM architectures might preserve neighborhood properties in their feature space, particularly focusing on transformer attention mechanisms and their continuity properties