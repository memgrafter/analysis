---
ver: rpa2
title: 'LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior'
arxiv_id: '2410.21264'
source_url: https://arxiv.org/abs/2410.21264
tags:
- larp
- video
- generation
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LARP, a video tokenizer designed for autoregressive
  generative models. Unlike traditional patchwise tokenizers that encode local patches
  into discrete tokens, LARP uses learned holistic queries to capture global and semantic
  video representations.
---

# LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior

## Quick Facts
- arXiv ID: 2410.21264
- Source URL: https://arxiv.org/abs/2410.21264
- Reference count: 15
- One-line primary result: Achieves state-of-the-art Frechet Video Distance (FVD) of 57 on UCF101 class-conditional video generation, outperforming all existing autoregressive methods

## Executive Summary
This paper introduces LARP (Learned AutoRegressive Prior), a video tokenizer specifically designed for autoregressive generative models. Unlike traditional patchwise tokenizers that encode local patches into discrete tokens, LARP uses learned holistic queries to capture global and semantic video representations. It integrates a lightweight autoregressive transformer as a training-time prior model to align its latent space with downstream autoregressive generation tasks, automatically determining an optimal token order. Experiments show LARP achieves state-of-the-art Frechet Video Distance (FVD) of 57 on UCF101 class-conditional video generation, outperforming all existing autoregressive methods.

## Method Summary
LARP addresses the limitation of current video tokenizers for autoregressive generative models by introducing a holistic tokenization scheme. Instead of directly encoding individual patches into discrete tokens, LARP learns a set of holistic queries that capture global semantic information from videos. These queries are concatenated with patch embeddings and processed through a transformer encoder to produce holistic discrete tokens. During training, LARP incorporates a lightweight AR transformer as a prior model that predicts the next token on its discrete latent space, pushing the latent space toward a structure optimized for autoregressive generation. This prior model is discarded during inference. The method uses Stochastic Vector Quantization (SVQ) instead of deterministic quantization to introduce stochasticity and allow exploration of inter-code correlations.

## Key Results
- Achieves state-of-the-art FVD of 57 on UCF101 class-conditional video generation
- Outperforms all existing autoregressive methods on video generation benchmarks
- Demonstrates superior reconstruction quality compared to patchwise tokenizers

## Why This Works (Mechanism)

### Mechanism 1
LARP's holistic tokenization scheme allows tokens to capture global and semantic video representations rather than being limited to local patch-level information. By using learned holistic queries instead of directly encoding individual patches, each token can represent any video patch and capture higher-level semantic information. This decouples the direct correspondence between discrete tokens and input patches, enabling more effective global semantic representation.

### Mechanism 2
The AR prior model during training optimizes LARP's latent space to be more conducive to autoregressive generation. The lightweight AR transformer predicts the next token on the discrete latent space and provides gradients to push the latent space toward a structure optimized for AR generation. This co-training process ensures the latent space is not only optimized for video reconstruction but also structured for effective autoregressive generation.

### Mechanism 3
LARP automatically determines an optimal token order for autoregressive generation without requiring manual flattening order definition. By combining holistic tokenization with co-training of the AR prior model, LARP defines a sequential order for discrete tokens during training. This process progressively pushes tokens toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time.

## Foundational Learning

- **Concept: Vector Quantization and Stochastic Vector Quantization**
  - Why needed here: LARP needs to convert continuous video representations into discrete tokens for autoregressive modeling. SVQ is used instead of deterministic VQ to introduce stochasticity and allow exploration of inter-code correlations.
  - Quick check question: What is the key difference between standard VQ and SVQ, and why does this matter for video tokenization?

- **Concept: Autoregressive Modeling and Exposure Bias**
  - Why needed here: LARP is designed specifically for autoregressive generative models, and understanding AR models is fundamental. The paper addresses exposure bias in the AR prior model through scheduled sampling.
  - Quick check question: How does scheduled sampling help mitigate exposure bias in autoregressive models, and why is this particularly important for the AR prior model in LARP?

- **Concept: Transformer Architecture and Positional Encoding**
  - Why needed here: LARP uses transformer encoders for both the main tokenizer and the AR prior model. Understanding how transformers handle sequential data and positional information is crucial for grasping LARP's design.
  - Quick check question: Why does LARP use absolute learned positional encodings in the AR generative model instead of fixed positional encodings, and how does this differ from the positional encoding approach in the LARP tokenizer?

## Architecture Onboarding

- **Component map**: Video Input → Patchifier (spatial-temporal patch extraction) → Patch Embeddings → Holistic Queries → LARP Encoder → SVQ Quantizer → Discrete Tokens → AR Prior Model (training only) → LARP Decoder → Video Output
- **Critical path**: Input Video → Patch Embeddings + Holistic Queries → LARP Encoder → SVQ → Discrete Tokens → AR Prior Model (during training) → LARP Decoder → Reconstructed Video
- **Design tradeoffs**:
  - Holistic vs. Patchwise: Holistic tokenization offers global semantic representation but requires learned queries and more complex encoding; patchwise is simpler but limited to local information
  - Stochastic vs. Deterministic Quantization: SVQ introduces stochasticity for better semantic codes but adds complexity compared to standard VQ
  - Training-time Prior vs. No Prior: AR prior model optimizes latent space for AR generation but adds training complexity and computational overhead
- **Failure signatures**:
  - Poor reconstruction quality despite good AR generation (or vice versa) indicates misalignment between reconstruction and AR optimization objectives
  - Unstable training with the AR prior model suggests issues with gradient flow or the scheduled sampling strategy
  - No improvement over patchwise tokenizers indicates the holistic queries aren't capturing meaningful global information
- **First 3 experiments**:
  1. Implement the basic LARP architecture without the AR prior model and compare reconstruction quality against a standard patchwise tokenizer on a small dataset
  2. Add the AR prior model with scheduled sampling and evaluate whether it improves AR generation metrics without degrading reconstruction
  3. Test different numbers of holistic queries (varying the number of discrete tokens) to find the optimal balance between generation quality and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does LARP's learned autoregressive prior affect the generalization of the generated videos to unseen classes or datasets?
- **Basis in paper**: The paper shows that LARP achieves state-of-the-art performance on UCF101 class-conditional video generation, but does not evaluate its performance on unseen classes or datasets.
- **Why unresolved**: The paper focuses on evaluating LARP's performance on the UCF101 dataset and does not explore its ability to generalize to unseen classes or datasets.
- **What evidence would resolve it**: Evaluating LARP's performance on a held-out set of classes from UCF101 or on a different dataset, such as Kinetics-600, would provide insights into its generalization capabilities.

### Open Question 2
- **Question**: What is the impact of the number of holistic queries on the quality of the generated videos?
- **Basis in paper**: The paper mentions that LARP supports an arbitrary number of discrete tokens, but does not provide a detailed analysis of the impact of the number of holistic queries on the generated video quality.
- **Why unresolved**: While the paper discusses the trade-off between the number of tokens and reconstruction quality, it does not explore the specific impact of the number of holistic queries on the generated video quality.
- **What evidence would resolve it**: Conducting experiments with different numbers of holistic queries and evaluating the resulting video quality using metrics such as FVD would provide insights into the optimal number of queries for different tasks.

### Open Question 3
- **Question**: How does the scheduled sampling technique affect the training stability and convergence of the AR prior model?
- **Basis in paper**: The paper mentions the use of scheduled sampling in the AR prior model to reduce exposure bias, but does not provide a detailed analysis of its impact on training stability and convergence.
- **Why unresolved**: While the paper demonstrates the effectiveness of scheduled sampling in reducing exposure bias, it does not explore its impact on training stability and convergence.
- **What evidence would resolve it**: Comparing the training stability and convergence of the AR prior model with and without scheduled sampling, using metrics such as loss curves and training time, would provide insights into its impact on training dynamics.

## Limitations

- The paper's claims about LARP's superiority rely heavily on comparisons against specific baselines, but generalization to other video datasets or longer video sequences remains unclear.
- The computational overhead introduced by the AR prior model during training, while discarded at inference, could impact training efficiency significantly, though the paper doesn't provide detailed analysis of training time or memory requirements.
- The choice of hyperparameters for the AR prior model (such as the scheduled sampling rate and loss weighting) appears critical to performance, but the sensitivity analysis to these parameters is not thoroughly explored.

## Confidence

**High Confidence**: The core mechanism of using learned holistic queries for tokenization is well-supported by the paper's ablation studies and theoretical grounding in previous work on set-based representations. The experimental results showing LARP's superior reconstruction quality and generation performance on UCF-101 and K600 are robust and clearly demonstrated.

**Medium Confidence**: The claim that the AR prior model significantly improves AR generation performance is supported by experiments, but the magnitude of improvement relative to other potential optimization strategies is not fully established. The paper demonstrates that LARP outperforms existing autoregressive methods, but the comparison could be strengthened by including more recent non-autoregressive video generation methods as baselines.

**Low Confidence**: The assertion that LARP's learned token order is definitively "optimal" for AR generation is difficult to verify. While the paper shows that the learned order performs better than random orders, the evaluation of what constitutes "optimal" is limited to performance metrics rather than qualitative analysis of the learned order's semantic coherence or temporal structure.

## Next Checks

1. **Ablation Study on AR Prior Model**: Conduct a systematic ablation study varying the AR prior model's scheduled sampling rate, loss weight (α), and architecture depth. This would help determine the sensitivity of LARP's performance to these hyperparameters and identify the minimum viable configuration for the AR prior model.

2. **Cross-Dataset Generalization Test**: Evaluate LARP's performance on additional video datasets beyond UCF-101 and K600, particularly datasets with different characteristics (e.g., longer videos, different frame rates, or different domains like sports vs. cooking videos). This would test the robustness and generalizability of LARP's learned tokenization scheme.

3. **Qualitative Analysis of Learned Token Order**: Perform a qualitative analysis of the token order learned by the AR prior model, including visualization of how tokens evolve during training and analysis of the semantic coherence of the learned sequence. This could include examining whether the learned order captures natural temporal progression or semantic grouping in the video content.