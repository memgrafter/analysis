---
ver: rpa2
title: Fast Vocabulary Transfer for Language Model Compression
arxiv_id: '2402.09977'
source_url: https://arxiv.org/abs/2402.09977
tags:
- vocabulary
- language
- size
- compression
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a vocabulary transfer (VT) method for compressing
  language models by adapting their tokenizers to in-domain datasets. The core idea
  is to train a tokenizer on the target domain and transfer embeddings from the original
  tokenizer to the new one, resulting in shorter sequences and a smaller embedding
  matrix.
---

# Fast Vocabulary Transfer for Language Model Compression

## Quick Facts
- arXiv ID: 2402.09977
- Source URL: https://arxiv.org/abs/2402.09977
- Authors: Leonidas Gee; Andrea Zugarini; Leonardo Rigutini; Paolo Torroni
- Reference count: 10
- Primary result: Vocabulary transfer reduces model size by up to 55% and speeds up inference by up to 2.75x with minimal performance impact

## Executive Summary
This paper introduces a vocabulary transfer method to compress language models by adapting their tokenizers to in-domain datasets. The core innovation involves training a tokenizer on target domain data and transferring embeddings from the original tokenizer to create a compact embedding matrix. Experiments on medical, legal, and news datasets demonstrate that this approach can significantly reduce model size and inference time while maintaining or even improving performance. The method is particularly effective for specialized domains and can be combined with knowledge distillation for additional compression benefits.

## Method Summary
The method involves training a tokenizer on an in-domain dataset and transferring embeddings from a pre-trained tokenizer to initialize the new tokenizer's embeddings. For tokens present in both vocabularies, embeddings are directly transferred. For new tokens, embeddings are computed as the average of the embeddings of the corresponding subwords from the original tokenizer. The model is then fine-tuned on the downstream task using the adapted tokenizer. Optionally, knowledge distillation can be applied for further compression. The approach aims to reduce sequence length and embedding matrix size while preserving semantic information.

## Key Results
- Model size reduction of up to 55% with minimal performance impact
- Inference speedup of up to 2.75x achieved through sequence length reduction
- In the medical domain, average token sequence reduction of 32%
- Combining with knowledge distillation yields even greater compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting tokenizers to domain-specific vocabulary reduces sequence length, improving speed and size
- Mechanism: Domain-specific tokenizers tokenize domain terms more efficiently, resulting in fewer tokens and smaller embedding matrices
- Core assumption: Domain vocabulary distributions differ from general-purpose language
- Evidence anchors:
  - [abstract]: "memory and time complexity of attention layers grows quadratically with respect to the sequence length"
  - [section]: "In the medical domain, which is particularly specialized, we notice a remarkable 32% reduction of the average number of tokens per sequence"
- Break condition: If domain vocabulary is similar to general-purpose language, sequence length reduction may be minimal

### Mechanism 2
- Claim: Transferring embeddings from general-purpose to domain-specific tokenizer preserves semantic information while reducing vocabulary size
- Mechanism: Direct transfer for common tokens, average of subword embeddings for new tokens
- Core assumption: General-purpose tokenizer embedding space captures sufficient semantic information for transfer
- Evidence anchors:
  - [abstract]: "VT aims to initialize Vin by re-using most of the information learned from the LM pre-trained on Dgen"
  - [section]: "Ein(ti) = 1/|Tgen(ti)| · Σtj∈Tgen(ti) Egen(tj)"
- Break condition: If general-purpose embeddings don't capture relevant domain semantics, transfer may result in poor initialization

### Mechanism 3
- Claim: Combining vocabulary transfer with knowledge distillation enables greater compression without significant performance loss
- Mechanism: Vocabulary transfer reduces vocabulary size, then knowledge distillation further compresses while maintaining performance
- Core assumption: Knowledge distillation works best when student and teacher share the same vocabulary
- Evidence anchors:
  - [abstract]: "this guarantees a full exploitation of FVT in the scope of language model compression"
  - [section]: "Our choice of applying VT after KD is based on findings by Kim and Hassan (2020)"
- Break condition: If knowledge distillation process isn't properly aligned with vocabulary transfer, combined method may not achieve expected benefits

## Foundational Learning

- Concept: Tokenizer and Subword Tokenization
  - Why needed here: Understanding how tokenizers work and subword benefits is crucial for grasping vocabulary transfer
  - Quick check question: What are the advantages of subword tokenization over word-level tokenization in handling out-of-vocabulary words?

- Concept: Knowledge Distillation
  - Why needed here: Knowledge distillation is key for combining with vocabulary transfer to achieve greater compression
  - Quick check question: How does the teacher-student framework in knowledge distillation work to maintain performance while reducing model size?

- Concept: Sequence Length and Computational Complexity
  - Why needed here: The relationship between sequence length and computational complexity is fundamental to understanding efficiency gains
  - Quick check question: Why does the time complexity of attention layers grow quadratically with respect to sequence length?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding Matrix -> Language Model -> Knowledge Distillation
- Critical path:
  1. Train tokenizer on in-domain dataset
  2. Transfer embeddings from general-purpose to domain-specific tokenizer using FVT
  3. Fine-tune model on downstream task with adapted tokenizer
  4. (Optional) Apply knowledge distillation for additional compression
- Design tradeoffs:
  - Vocabulary Size vs. Performance: Smaller vocabularies enable greater compression but may impact performance
  - Domain Specialization vs. Generalization: Specialized tokenizers may perform better on domain tasks but worse on general tasks
  - Complexity vs. Efficiency: Vocabulary transfer adds complexity but enables significant efficiency gains
- Failure signatures:
  - Poor downstream performance despite vocabulary transfer
  - Minimal sequence length reduction after tokenizer adaptation
  - Unexpected performance degradation when combining VT with knowledge distillation
- First 3 experiments:
  1. Train tokenizer on domain-specific dataset and measure average sequence length reduction
  2. Apply FVT method to transfer embeddings and evaluate impact on model performance
  3. Combine vocabulary transfer with knowledge distillation and measure compression rate and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size reduction for maximizing performance while achieving significant model compression?
- Basis in paper: [explicit] The paper shows that reducing vocabulary size doesn't always lead to lower performance, and in some cases, a 50% reduction yields better results than no reduction
- Why unresolved: The paper doesn't provide a systematic method for determining the optimal vocabulary size reduction for different domains and tasks
- What evidence would resolve it: A comprehensive study across various domains and tasks to determine the optimal vocabulary size reduction that balances performance and compression

### Open Question 2
- Question: How does the choice of tokenizer (e.g., SentencePiece, WordPiece) affect the effectiveness of vocabulary transfer?
- Basis in paper: [inferred] The paper uses BERT's tokenizer (WordPiece) but mentions that the method is independent of the tokenizer type
- Why unresolved: The paper doesn't compare the effectiveness of vocabulary transfer across different tokenizer types
- What evidence would resolve it: Experiments comparing vocabulary transfer effectiveness using different tokenizer types (e.g., SentencePiece, WordPiece) on the same tasks and domains

### Open Question 3
- Question: Can vocabulary transfer be integrated into the knowledge distillation process to maximize information transfer during learning?
- Basis in paper: [explicit] The paper mentions plans to fully integrate vocabulary transfer within knowledge distillation during the learning process to maximize information transfer
- Why unresolved: The paper only discusses combining vocabulary transfer and knowledge distillation sequentially, not integrating them into a single learning process
- What evidence would resolve it: Development and evaluation of a model compression method that integrates vocabulary transfer and knowledge distillation into a single learning process

## Limitations
- Effectiveness on specialized domains beyond medical, legal, and news remains unproven
- Optimal vocabulary size reduction for balancing compression and performance is not determined
- Assumption that knowledge distillation works best with same vocabulary lacks extensive empirical validation

## Confidence
- High Confidence: Core mechanism of vocabulary transfer and basic effectiveness in reducing sequence length and model size
- Medium Confidence: Claim that combining VT with knowledge distillation yields even greater compression
- Medium Confidence: Assertion that VT is particularly effective for specialized domains

## Next Checks
1. **Cross-domain validation**: Apply FVT method to a new domain (e.g., technical documentation) and measure sequence length reduction and performance impact compared to original tokenizer
2. **Vocabulary size sensitivity analysis**: Systematically vary vocabulary size beyond tested levels to identify optimal trade-off point between compression ratio and performance degradation
3. **Ablation study on embedding transfer**: Compare FVT method against random initialization and alternative transfer approaches to isolate contribution of embedding transfer mechanism to overall performance