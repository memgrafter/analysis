---
ver: rpa2
title: 'JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language
  Models'
arxiv_id: '2406.02050'
source_url: https://arxiv.org/abs/2406.02050
tags:
- japanese
- bias
- social
- llms
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study constructed the Japanese Bias Benchmark for Question
  Answering (JBBQ) by semi-automatically translating and adapting the English BBQ
  dataset, focusing on five social bias categories: age, disability, gender, physical
  appearance, and sexual orientation. Evaluation on eight open Japanese LLMs showed
  that larger models and instruction tuning improved QA accuracy but also increased
  bias scores.'
---

# JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models

## Quick Facts
- **arXiv ID**: 2406.02050
- **Source URL**: https://arxiv.org/abs/2406.02050
- **Reference count**: 14
- **Primary result**: Evaluation on eight open Japanese LLMs showed that larger models and instruction tuning improved QA accuracy but also increased bias scores, while chain-of-thought prompting reduced bias by requiring explicit context use.

## Executive Summary
This paper introduces the Japanese Bias Benchmark for Question Answering (JBBQ), a dataset constructed by semi-automatically translating and adapting the English BBQ dataset to evaluate social biases in Japanese large language models. The benchmark focuses on five social bias categories: age, disability, gender, physical appearance, and sexual orientation. Through evaluation of eight open Japanese LLMs and two commercial models, the study reveals that while larger models and instruction tuning improve question-answering accuracy, they also amplify bias scores. The research demonstrates that prompt engineering techniques, particularly chain-of-thought prompting and warnings about social bias, can reduce biased outputs, though models struggle with extracting correct evidence from contexts.

## Method Summary
The JBBQ dataset was created through semi-automatic translation of the English BBQ dataset, followed by manual modification to ensure Japanese cultural relevance and filtering to maintain quality. The dataset contains 245 templates and 50,856 question pairs across five social bias categories. Eight open Japanese LLMs and two commercial models were evaluated using three prompt settings: basicP (3-shot), paraP (with bias warning), and CoT (chain-of-thought with reasoning steps). The evaluation framework used llm-jp-eval v1.4.15 to measure accuracy, diff-bias scores, and out-of-choice ratios for both ambiguous and disambiguated contexts.

## Key Results
- Larger model size and instruction tuning improve accuracy on JBBQ but also increase bias scores
- Chain-of-thought prompting reduces bias by requiring models to explicitly use context evidence
- Prompts with warnings about social biases help models select unknown labels for ambiguous questions
- Models struggle to extract correct evidence from contexts, with bias detection tasks proving more challenging than standard QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger model size and instruction tuning improve accuracy on JBBQ but also increase bias scores
- Mechanism: Parameter count increases model capacity to encode and reproduce societal stereotypes present in training data. Instruction tuning teaches models to generate more coherent and confident responses, which amplifies both correct and biased outputs.
- Core assumption: Training data contains more stereotypical associations than non-stereotypical ones, and larger models can encode these patterns more completely
- Evidence anchors:
  - [abstract] "current open Japanese LLMs with more parameters show improved accuracies on JBBQ, their bias scores increase"
  - [section 5.1] "larger model size gives better accuracies, it also gives higher diff-bias scores"
  - [corpus] Weak - corpus neighbors do not directly address size-bias relationships
- Break condition: If training data is carefully curated to minimize stereotypical patterns, or if bias mitigation techniques are applied during pre-training

### Mechanism 2
- Claim: Chain-of-thought prompting reduces bias by requiring models to explicitly use context evidence
- Mechanism: By forcing models to output reasoning steps that must reference the provided context, CoT prompting constrains the model from relying on memorized stereotypical associations and instead grounds responses in the specific textual evidence
- Core assumption: Models default to pattern matching against stereotypical associations when not explicitly required to ground reasoning in context
- Evidence anchors:
  - [abstract] "chain-of-thought prompting reduces the effect of biases in model outputs, but there is room for improvement in extracting the correct evidence from contexts in Japanese"
  - [section 5.2] "CoT prompting requires models to explicitly use contexts as output, and the models are less prone to incorrect predictions based on social bias ignoring the given contexts"
  - [corpus] Weak - corpus does not directly address CoT effects on bias
- Break condition: If context is ambiguous or does not contain sufficient disambiguating information, models may still rely on stereotypical associations

### Mechanism 3
- Claim: Prompts with warnings about social biases help models select unknown labels for ambiguous questions
- Mechanism: Explicit warnings activate model's awareness of bias considerations, leading to more cautious responses when context is insufficient to determine correct answers without relying on stereotypes
- Core assumption: Models can recognize when questions cannot be answered without stereotyping and have learned to respond with unknown labels when explicitly instructed
- Evidence anchors:
  - [abstract] "prompts with a warning about social biases... reduce the effect of biases in model outputs"
  - [section 5.2] "the paraP prompt encourages models to answer unknown labels, and correct answers for questions in ambiguous contexts are only unknown labels"
  - [corpus] Weak - corpus does not directly address prompt warning effects
- Break condition: If warnings conflict with other task requirements (such as bias detection tasks), models may become confused about the appropriate response

## Foundational Learning

- Concept: Social bias in language models as stereotype reproduction
  - Why needed here: Understanding that model outputs reflect patterns in training data, not objective truth
  - Quick check question: Why might a language model produce biased outputs even when given disambiguating context?

- Concept: Prompt engineering effects on model behavior
  - Why needed here: Different prompting strategies can significantly alter model outputs and bias manifestations
  - Quick check question: How might a warning prompt about biases affect a model's response to an ambiguous question?

- Concept: Multiple-choice QA task design for bias evaluation
  - Why needed here: Understanding how ambiguous vs disambiguated contexts test model reliance on stereotypes
  - Quick check question: What distinguishes a biased answer from a counter-biased answer in the JBBQ framework?

## Architecture Onboarding

- Component map: JBBQ dataset (templates + vocabulary) -> Prompt generation (basicP, paraP, CoT) -> Model inference (Japanese LLMs with various sizes/instruction tuning) -> Evaluation metrics (accuracy, diff-bias score, bias detection task)
- Critical path: Dataset creation → Prompt selection → Model inference → Metric calculation → Analysis of size/instruction tuning/prompt effects
- Design tradeoffs: Larger models give better accuracy but more bias; CoT prompting reduces bias but requires more complex prompt engineering; instruction tuning helps with unknown labels but has limited effect on disambiguated questions
- Failure signatures: High OoC ratios indicate models not recognizing multiple-choice format; high diff-bias scores in ambiguous contexts indicate over-reliance on stereotypes; low accuracy in disambiguated contexts indicates poor context understanding
- First 3 experiments:
  1. Compare baseline accuracy and diff-bias scores across all eight Japanese LLMs with basic 3-shot prompts
  2. Test effect of paraP prompts on the top-performing model to measure warning prompt impact
  3. Apply CoT prompting to the same top model to assess reasoning step requirements on bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Japanese LLMs perform on intersectional bias detection tasks that combine multiple social categories (e.g., gender + age)?
- Basis in paper: Explicit - The paper explicitly mentions that JBBQ does not address intersectional bias and states "it is necessary to create data to evaluate such intersectional bias in the future."
- Why unresolved: The dataset construction focused on single-category biases and excluded intersectional cases, leaving a gap in understanding how models handle compound social stereotypes.
- What evidence would resolve it: Evaluation results on a dataset containing intersectional bias examples would reveal whether model performance degrades when multiple social categories interact.

### Open Question 2
- Question: What specific cultural differences in Japanese society make age-related bias templates more prevalent in JBBQ compared to other categories?
- Basis in paper: Explicit - The paper notes "The reason for the relatively large number of templates in the age category is that our JBBQ dataset reflects many age-related harmful biases that exist in Japanese society (Sussman et al., 1980)."
- Why unresolved: While the paper acknowledges higher prevalence, it doesn't analyze which specific Japanese cultural factors (workplace norms, family structures, etc.) drive this bias concentration.
- What evidence would resolve it: Detailed analysis of Japanese social structures and media representations could identify which cultural elements most strongly correlate with age bias patterns.

### Open Question 3
- Question: How does the order bias observed in model predictions (favoring first options, avoiding third options) vary across different Japanese LLM architectures and training methodologies?
- Basis in paper: Explicit - The paper reports "All the models made somewhat imbalanced predictions in terms of the order of options" and provides detailed statistics showing consistent order preferences.
- Why unresolved: The paper identifies the order bias exists but doesn't investigate whether it stems from training data ordering, tokenizer design, or architectural differences between models.
- What evidence would resolve it: Systematic experiments varying answer position across models with different architectures and training data would reveal whether order bias is model-specific or universal.

## Limitations

- The dataset construction relied heavily on semi-automatic translation from English BBQ, with only 15 additional templates added for Japanese cultural relevance, potentially missing nuanced cultural biases.
- The evaluation focused on eight Japanese models and two commercial models, limiting cross-linguistic comparisons and generalizability of findings.
- The paper acknowledges room for improvement in extracting correct evidence from contexts in Japanese, suggesting the language may present unique challenges not fully addressed.

## Confidence

- **High Confidence**: The observation that larger models show improved accuracy but increased bias scores is well-supported by multiple evaluation settings and consistent across different prompt types.
- **Medium Confidence**: The effectiveness of chain-of-thought prompting in reducing bias relies on models successfully extracting context evidence, which the paper acknowledges as an area needing improvement.
- **Low Confidence**: The cultural adaptation claims are limited by the small number of additional Japanese-specific templates and rely more on author judgment than systematic validation.

## Next Checks

1. **Cross-linguistic validation**: Evaluate the same JBBQ methodology on non-Japanese language pairs (e.g., Spanish, French) to determine whether the observed size-bias relationships and prompt engineering effects generalize beyond Japanese.

2. **Bias mitigation technique comparison**: Systematically compare CoT prompting against other established bias mitigation techniques (such as counterfactual data augmentation or adversarial debiasing) to establish relative effectiveness.

3. **Longitudinal bias tracking**: Re-run the evaluation on the same models after additional fine-tuning cycles to measure whether bias scores change predictably over time, helping establish whether observed patterns are stable or transient.