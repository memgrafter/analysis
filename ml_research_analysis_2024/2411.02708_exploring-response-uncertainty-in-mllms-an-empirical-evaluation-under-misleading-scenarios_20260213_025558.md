---
ver: rpa2
title: 'Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading
  Scenarios'
arxiv_id: '2411.02708'
source_url: https://arxiv.org/abs/2411.02708
tags:
- misleading
- rate
- answer
- implicit
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a systematic evaluation of multimodal large\
  \ language models' (MLLMs) vulnerability to misleading instructions, revealing that\
  \ 65% of previously correct answers are overturned after a single deceptive cue.\
  \ A two-stage pipeline is proposed: first recording unperturbed responses, then\
  \ injecting explicit (false-answer hints) and implicit (contextual contradictions)\
  \ misleading instructions, and measuring the misleading rate\u2014the fraction of\
  \ correct-to-incorrect flips."
---

# Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios

## Quick Facts
- arXiv ID: 2411.02708
- Source URL: https://arxiv.org/abs/2411.02708
- Reference count: 40
- 65% of correct answers are overturned by a single deceptive cue

## Executive Summary
This paper investigates how multimodal large language models (MLLMs) respond to misleading instructions. Through a two-stage evaluation pipeline, the authors systematically measure how often models flip from correct to incorrect answers when exposed to either explicit false-answer hints or implicit contextual contradictions. They find that 65% of previously correct answers are overturned after a single deceptive cue, with explicit misleading rates over 67% and implicit over 81%. To address this vulnerability, they propose a compact 2000-sample fine-tuning dataset that reduces misleading rates to under 7% for explicit and 33% for implicit cues, while also improving consistency and accuracy.

## Method Summary
The authors develop a two-stage evaluation pipeline: first recording unperturbed model responses, then injecting explicit (false-answer hints) and implicit (contextual contradictions) misleading instructions to measure the "misleading rate" - the fraction of correct-to-incorrect flips. Using examples with the highest misleading rates, they construct a Multimodal Uncertainty Benchmark (MUB) stratified by difficulty. They then fine-tune all open-source models on a compact 2000-sample mixed-instruction dataset using LoRA, achieving significant robustness improvements while slightly improving standard benchmark accuracy.

## Key Results
- 65% of previously correct answers are overturned by a single deceptive cue
- Explicit misleading rates exceed 67.19% and implicit exceed 80.67% across models
- Fine-tuning reduces misleading rates to 6.97% (explicit) and 32.77% (implicit)
- Fine-tuned models show 29.37% improvement in consistency on highly deceptive inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Misleading instructions systematically flip correct answers by exploiting model sensitivity to explicit or implicit cues
- Mechanism: The two-stage pipeline captures original responses, then introduces explicit cues (e.g., "The true answer is No") or implicit contextual contradictions, measuring correct-to-incorrect flips
- Core assumption: Models' internal knowledge is sufficiently plastic that deceptive cues can override correct reasoning without robust alignment
- Evidence anchors: [abstract] shows 65% flip rate; [section] describes two-stage pipeline; related work on visual deception supports mechanism plausibility
- Break condition: Fine-tuning on mixed instructions drops rates to ~7% explicit, ~33% implicit

### Mechanism 2
- Claim: Fine-tuning on compact high-misleading-rate examples substantially improves robustness
- Mechanism: Mixing explicit and implicit instructions in 2000 samples teaches models to recognize and resist deceptive cues
- Core assumption: Small, high-quality dataset targeting deceptive examples is sufficient to recalibrate response patterns
- Evidence anchors: [abstract] shows reduction to 6.97% explicit and 32.77% implicit; [section] describes fine-tuning procedure
- Break condition: Using only explicit instructions leaves models vulnerable to implicit misleading

### Mechanism 3
- Claim: Misleading rate metric provides more direct and efficient uncertainty measure than consistency rates
- Mechanism: Compares correctness before/after misleading cues in single pass rather than requiring multiple runs
- Core assumption: Correctness state is stable enough between unperturbed and misleading conditions for single comparison
- Evidence anchors: [abstract] defines misleading rate and links to accuracy; [section] explains metric advantages
- Break condition: If internal state changes drastically between runs, metric may not reflect true uncertainty

## Foundational Learning

- **Multimodal Large Language Models (MLLMs)**: Understanding how MLLMs integrate visual and textual inputs is essential to grasp susceptibility to multimodal deceptive cues
  - Quick check: How does an MLLM typically integrate visual features and text embeddings to produce a final answer?

- **Explicit vs. Implicit Misleading Instructions**: Differentiating between overt false-answer hints and subtle contextual contradictions is key to understanding the evaluation pipeline
  - Quick check: Give an example of an explicit misleading instruction and an implicit one for the same image-question pair

- **Misleading Rate (MR) Metric**: The core evaluation metric; understanding its calculation and interpretation is necessary to follow results
  - Quick check: If a model answers 10 questions correctly originally, and 7 become incorrect after misleading instructions, what is the MR(T→F)?

## Architecture Onboarding

- **Component map**: Image + Question → Unperturbed inference → Original response → Misleading instruction injection → Misleading inference → Correctness evaluation → Misleading rate calculation → Optional fine-tuning → Updated MLLM
- **Critical path**: Data preparation → Unperturbed inference → Misleading instruction generation → Misleading inference → Correctness evaluation and misleading rate calculation
- **Design tradeoffs**: Data volume vs. robustness (2000 samples sufficient); explicit vs. implicit mix (combining explicit reduces data but may oversimplify); evaluation speed vs. accuracy (single comparison vs. multiple runs)
- **Failure signatures**: High misleading rate indicates vulnerability; low rate after fine-tuning indicates success; persistent high rate suggests insufficient or unrepresentative training data
- **First 3 experiments**: 1) Run two-stage pipeline on held-out dataset to verify MR calculation and consistency correlation; 2) Fine-tune single MLLM on mixed dataset and evaluate MR before/after; 3) Vary explicit/implicit data proportions to find optimal robustness mix

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different fine-tuning strategies impact robustness against implicit misleading instructions? The paper compares S5, C5, C10 strategies but doesn't comprehensively evaluate transfer to implicit misleading scenarios.
- **Open Question 2**: What is the long-term generalization capability of fine-tuned MLLMs on novel misleading scenarios not seen during training? The study evaluates on additional datasets but doesn't test entirely new misleading instruction types.
- **Open Question 3**: How does multimodal misleading information (visual cues + textual instructions) affect misleading rates compared to unimodal misleading? The study focuses on textual instructions and isolated visual manipulations.

## Limitations
- Dataset scope may not cover full diversity of real-world misleading scenarios despite using nine standard datasets and 2.5k MUB samples
- Exact prompt templates and their sensitivity to model-specific nuances are not fully disclosed, limiting reproducibility
- Long-term stability and generalization of fine-tuning improvements to novel deceptive inputs remains uncertain

## Confidence
- **High**: Core finding of 65% correct-to-incorrect flips is well-supported by extensive evaluation across twelve open-source and five closed-source models with clear quantitative metrics
- **Medium**: Fine-tuning effectiveness is supported experimentally but long-term generalization remains to be validated
- **Low**: Exact prompt templates and model sensitivity are not fully specified, limiting reproducibility and generalizability

## Next Checks
1. Evaluate fine-tuned models on a separate, held-out set of misleading examples not seen during training to assess robustness to novel deceptive inputs
2. Systematically vary explicit and implicit prompt templates and measure their impact on misleading rates across different MLLM architectures to identify the most robust approaches
3. Re-evaluate models' misleading rates after a period of time or after additional fine-tuning on standard datasets to check for degradation or overfitting