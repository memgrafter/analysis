---
ver: rpa2
title: 'Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent
  Path Finding'
arxiv_id: '2410.21415'
source_url: https://arxiv.org/abs/2410.21415
tags:
- agents
- pibt
- learning
- path
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Scalable Imitation Learning for Lifelong Multi-Agent
  Path Finding (SILLM), a learning-based approach capable of managing up to 10,000
  agents. SILLM combines imitation learning with a novel Spatially Sensitive Communication
  module and integrates global guidance techniques with heuristic search improvements.
---

# Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding

## Quick Facts
- arXiv ID: 2410.21415
- Source URL: https://arxiv.org/abs/2410.21415
- Reference count: 40
- SILLM achieves 137.7% throughput improvement over learning-based baselines and 16.0% over search-based solvers for up to 10,000 agents

## Executive Summary
This paper presents SILLM, a scalable imitation learning approach for Lifelong Multi-Agent Path Finding (LMAPF) that can handle up to 10,000 agents. The system combines a neural policy with Spatially Sensitive Communication (SSC) and global guidance heuristics, trained via imitation learning from the state-of-the-art Windowed MAPF-LNS algorithm. SILLM outperforms both learning- and search-based baselines on six large-scale maps, achieving an average throughput improvement of 137.7% over learning methods and 16.0% over search methods. The approach was also validated in real-world environments with 10 real and 100 virtual robots.

## Method Summary
SILLM employs imitation learning to train a neural policy that imitates the Windowed MAPF-LNS algorithm. The policy architecture includes CNN feature extractors, a Spatially Sensitive Communication (SSC) module that preserves spatial relationships between agents, and global guidance encoding (Backward Dijkstra heuristics, Static Guidance via crisscross highways, and Dynamic Guidance for traffic patterns). Collision resolution is handled by a Learnable PIBT module that outputs action probabilities safeguarded by PIBT before execution. The system is trained on downscaled maps with 600 agents and evaluated on large maps with 10,000 agents.

## Key Results
- Achieves 137.7% average throughput improvement over learning-based baselines
- Achieves 16.0% average throughput improvement over search-based baselines
- Outperforms winning solution of 2023 League of Robot Runners competition
- Successfully deployed in real-world environment with 10 real and 100 virtual robots

## Why This Works (Mechanism)

### Mechanism 1
Spatially Sensitive Communication (SSC) improves learning of cooperative behaviors by explicitly encoding spatial relationships between agents. Instead of using attention-based communication that only implicitly reasons with spatial information, SSC creates a matrix filled with -1 and inserts neighboring agents' feature vectors into their corresponding relative positions in the observer's field of view. This preserves precise spatial relationships when aggregating information.

### Mechanism 2
Imitation learning from scalable search-based solvers outperforms learning from weak bounded-suboptimal algorithms. The system imitates Windowed MAPF-LNS, an anytime search-based algorithm that scales well to large instances due to its planning window and anytime behavior. This provides high-quality training data that matches the target application scale.

### Mechanism 3
Combining global guidance heuristics with learned policies improves throughput by reducing congestion. Three types of global guidance are encoded into observations: Backward Dijkstra heuristics for shortest path distances, Static Guidance using crisscross highways to encourage directional movement, and Dynamic Guidance that encodes real-time traffic information to avoid congestion.

## Foundational Learning

- **Imitation Learning**: Why needed - The problem requires learning complex cooperative behaviors from experienced agents (search-based solvers) rather than exploring the vast joint action space through trial and error. Quick check - What is the key difference between imitation learning and reinforcement learning in terms of data requirements?

- **Spatial Reasoning in Neural Networks**: Why needed - Multi-agent pathfinding requires understanding relative positions and potential collisions, which is best captured through explicit spatial encoding rather than implicit attention mechanisms. Quick check - How does the SSC module differ from standard attention-based communication in terms of spatial information preservation?

- **Heuristic Search and Planning**: Why needed - The system needs to integrate search-based techniques (like PIBT) with learning to resolve collisions and provide global guidance while maintaining fast planning times. Quick check - Why does the system use PIBT for collision resolution rather than relying solely on the learned policy?

## Architecture Onboarding

- **Component map**: Observation → CNN → SSC → Global Guidance Encoding → Policy Network → Action Probabilities → CS-PIBT → Execution
- **Critical path**: The observation pipeline through CNN feature extraction, spatial communication aggregation, global guidance encoding, policy inference, and collision-safe execution
- **Design tradeoffs**: The SSC module trades memory (storing feature matrices) for spatial precision. The hybrid approach trades some planning speed for higher solution quality compared to pure learning methods.
- **Failure signatures**: Poor performance on maps with specific obstacle patterns may indicate inadequate global guidance. Excessive planning time suggests inefficient collision resolution or oversized field of view.
- **First 3 experiments**:
  1. Replace SSC with attention-based communication and measure throughput impact on simple maps
  2. Disable global guidance entirely and evaluate performance degradation
  3. Test CS-PIBT with different priority schemes to find optimal collision resolution strategy

## Open Questions the Paper Calls Out

1. How can reinforcement learning methods be improved to match or surpass the performance of imitation learning approaches for large-scale LMAPF? The paper only tests a basic MAPPO implementation and does not explore more advanced RL techniques.

2. What is the optimal communication range and mechanism for SSC in different LMAPF scenarios with varying agent densities and map structures? The paper sets communication range equal to field of view for simplicity without systematic exploration.

3. How can global guidance heuristics be automatically optimized for specific LMAPF instances rather than relying on manual design? The paper uses manually designed heuristics without exploring automated optimization methods.

## Limitations

- SSC module's spatial encoding assumes fixed field of view that may not scale optimally for extremely dense agent scenarios
- Global guidance heuristics require careful tuning of edge costs and may not generalize well to all map topologies
- Reliance on imitation learning creates dependency on quality and availability of search-based solvers for new problem instances

## Confidence

- **High Confidence**: Superior throughput compared to both learning-based (137.7% improvement) and search-based (16.0% improvement) solvers
- **Medium Confidence**: Mechanism claims about SSC's spatial precision benefits and global guidance heuristics effectiveness
- **Low Confidence**: Effectiveness in real-world environments with 10 real and 100 virtual robots based on single demonstration

## Next Checks

1. Conduct more granular ablation studies that isolate the contribution of each global guidance heuristic and the SSC module to better understand their individual impacts on performance.

2. Systematically evaluate SILLM's performance across multiple real-world deployment scenarios with varying agent densities and environmental conditions to validate real-world robustness claims.

3. Test the system's performance limits by evaluating on maps with agent counts exceeding 10,000 and with varying density levels to identify potential scalability bottlenecks in SSC module and collision resolution mechanisms.