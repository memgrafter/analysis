---
ver: rpa2
title: 'Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents'
arxiv_id: '2402.12327'
source_url: https://arxiv.org/abs/2402.12327
tags:
- uni00000013
- agents
- arxiv
- communication
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language model (LLM) agents
  can spontaneously develop cooperative behaviors in competitive scenarios without
  explicit instructions. The authors examine three scenarios - Keynesian beauty contest,
  Bertrand competition, and emergency evacuation - using GPT-4 as agents.
---

# Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents

## Quick Facts
- arXiv ID: 2402.12327
- Source URL: https://arxiv.org/abs/2402.12327
- Reference count: 40
- Primary result: LLM agents spontaneously develop cooperative behaviors in competitive scenarios without explicit instructions

## Executive Summary
This study investigates whether large language model agents can spontaneously develop cooperative behaviors in competitive scenarios without explicit instructions. The authors examine three scenarios - Keynesian beauty contest, Bertrand competition, and emergency evacuation - using GPT-4 as agents. They find that LLM agents gradually converge on cooperative strategies through communication and in-context learning, evidenced by decreasing variance in choices, converging prices, and improved evacuation outcomes. The results align with human behavioral data and demonstrate that minimizing explicit instructions in LLM prompts yields more human-like performance. This work highlights the importance of debiased LLM agents for realistic social simulations and offers a novel method to assess LLMs' deliberate reasoning capabilities.

## Method Summary
The study employs large language model agents (primarily GPT-4) in three competitive scenarios - Keynesian beauty contest, Bertrand competition, and emergency evacuation - using a Self-Adaptive Behavior Model (SABM) framework. Agents interact through communication phases where they exchange information and formulate strategies, then execute actions based on their planning. The experiments compare outcomes with and without communication, and with varying levels of explicit instructions. Performance is measured through convergence metrics (variance in choices, price convergence) and alignment with human behavioral data.

## Key Results
- LLM agents spontaneously converge on cooperative strategies through communication and in-context learning across all three scenarios
- Minimizing explicit instructions in LLM prompts yields more human-like performance compared to instructed agents
- Cooperation emerges even in competitive environments, with agents recognizing mutual benefits through shared information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents can develop cooperative behaviors through communication in competitive scenarios without explicit instructions.
- Mechanism: Through in-context learning and long-term interaction, agents gradually recognize mutual benefits of cooperation and adjust strategies accordingly.
- Core assumption: Agents possess sufficient in-context learning capability to adapt strategies based on historical interactions.
- Evidence anchors:
  - [abstract]: "LLM agents gradually converge on cooperative strategies through communication and in-context learning"
  - [section]: "we observe the spontaneous cooperation between LLM agents in diverse competitive scenarios, which reflects LLM's potential in long-horizon in-context learning tasks"
  - [corpus]: Weak evidence. Related papers focus on embodied cooperation and reputation systems but don't directly address spontaneous cooperation without explicit instructions.
- Break condition: If agents fail to recognize mutual benefits or if communication does not lead to strategy adaptation.

### Mechanism 2
- Claim: Minimizing explicit instructions in prompts yields more human-like performance in social simulations.
- Mechanism: Without explicit instructions, agents rely on their inherent knowledge and reasoning abilities, leading to behaviors that more closely mirror human decision-making processes.
- Core assumption: LLMs have been trained on sufficient human behavioral data to produce realistic social interactions without guidance.
- Evidence anchors:
  - [abstract]: "minimizing explicit instructions in LLM prompts yields more human-like performance"
  - [section]: "when using specific LLMs (e.g., GPT-4) for social simulations, minimizing instructions could better reflect real-world situations"
  - [corpus]: No direct evidence. Related papers focus on team simulations but don't compare instructed vs. non-instructed agents.
- Break condition: If instructed agents consistently outperform non-instructed agents in human-like behavior metrics.

### Mechanism 3
- Claim: Spontaneous cooperation can emerge in competitive environments as agents recognize the benefits of mutual adaptation.
- Mechanism: Agents engage in strategic discussions, share information, and adjust their actions to achieve better collective outcomes without formal agreements.
- Core assumption: Agents can infer mutual benefits from shared information and historical context.
- Evidence anchors:
  - [abstract]: "spontaneous cooperation across three competitive scenarios"
  - [section]: "cooperation can naturally develop among agents, even in a competitive environment"
  - [corpus]: Weak evidence. Related papers discuss cooperation in teams but don't specifically address spontaneous cooperation in competitive settings.
- Break condition: If agents continue competitive behaviors despite communication and shared information.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Agents must adapt strategies based on historical interactions without explicit retraining.
  - Quick check question: How do agents use past communication history to inform current decisions?

- Concept: Nash equilibrium
  - Why needed here: Understanding competitive dynamics helps analyze when and how cooperation emerges.
  - Quick check question: What conditions lead agents to deviate from Nash equilibrium toward cooperative strategies?

- Concept: Game theory
  - Why needed here: Provides framework for analyzing strategic interactions between agents.
  - Quick check question: How do payoff structures influence agents' decisions to cooperate or compete?

## Architecture Onboarding

- Component map: Scenario modules (KBC, BC, EE) -> Communication phase -> Planning phase -> Action phase -> Update phase -> Repeat
- Critical path: Initialize scenario and agents -> Execute communication phase -> Execute planning phase -> Execute action phase -> Update scenario state -> Repeat until termination condition
- Design tradeoffs:
  - Communication frequency vs. computational cost
  - Prompt specificity vs. spontaneous behavior emergence
  - Scenario complexity vs. agent comprehension
  - Number of agents vs. token limits
- Failure signatures:
  - Agents fail to cooperate despite communication
  - Agents produce unrealistic or nonsensical outputs
  - Simulation fails to terminate or progresses incorrectly
  - Performance degrades with increased agents or rounds
- First 3 experiments:
  1. Run KBC with varying communication rounds (k=0,1,2,3) to observe cooperation emergence
  2. Compare BC outcomes with and without communication to identify tacit collusion
  3. Test EE with different agent counts to evaluate scalability and cooperation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different large language models (LLMs) perform in simulating spontaneous cooperation across various competitive scenarios, and what factors contribute to their performance differences?
- Basis in paper: [explicit] The paper compares GPT-4, Claude 3, and Gemini Pro in the Keynesian Beauty Contest (KBC) scenario, noting performance differences.
- Why unresolved: The study primarily focuses on GPT-4 and provides limited comparisons with other models. A comprehensive analysis across different models and scenarios is needed to understand the generalizability of spontaneous cooperation.
- What evidence would resolve it: Extensive experiments using a diverse set of LLMs across all three scenarios (KBC, Bertrand Competition, and Emergency Evacuation) to evaluate their ability to exhibit spontaneous cooperation.

### Open Question 2
- Question: What is the impact of varying communication frequencies and forms on the emergence and strength of spontaneous cooperation in competitive scenarios?
- Basis in paper: [explicit] The paper mentions that different forms of communication (one-on-one, group chat, broadcast) are used in the three scenarios but does not systematically analyze their impact on cooperation.
- Why unresolved: The study does not isolate the effect of communication frequency and form on cooperation, leaving open the question of how these factors influence the emergence and strength of cooperative behavior.
- What evidence would resolve it: Controlled experiments manipulating communication frequency and form across the three scenarios to measure their effect on cooperation outcomes.

### Open Question 3
- Question: How does the initial positioning and distribution of agents affect the development of spontaneous cooperation in the Emergency Evacuation (EE) scenario?
- Basis in paper: [inferred] The paper mentions using different initial seeds in the EE scenario but does not analyze the impact of initial positioning on cooperation.
- Why unresolved: The study does not explore how the initial distribution of agents in the grid environment influences their ability to cooperate and evacuate efficiently.
- What evidence would resolve it: Experiments with varying initial agent distributions in the EE scenario to analyze the relationship between initial positioning and the emergence of cooperative evacuation strategies.

## Limitations
- Limited comparison with alternative LLM models beyond GPT-4 makes it difficult to assess whether results are model-specific
- The role of communication structure and format in enabling cooperation remains underexplored
- Human behavioral data comparison is qualitative rather than quantitative, making precise alignment assessment challenging
- The study demonstrates spontaneous cooperation emergence but doesn't fully characterize the boundary conditions where cooperation fails or breaks down

## Confidence

- **High confidence**: LLMs can exhibit spontaneous cooperation in competitive scenarios without explicit instructions
- **Medium confidence**: Minimizing instructions yields more human-like performance, based on qualitative alignment with human data
- **Medium confidence**: In-context learning enables strategy adaptation, though the learning mechanisms remain partially characterized

## Next Checks
1. Test cooperation emergence across different LLM models (Claude, Llama, Gemini) to assess generalizability
2. Conduct quantitative comparison between LLM and human behavioral data using statistical measures of convergence and variance
3. Systematically vary communication frequency and format to identify optimal conditions for spontaneous cooperation emergence