---
ver: rpa2
title: 'Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and
  Hypothetical Reasoning'
arxiv_id: '2402.12554'
source_url: https://arxiv.org/abs/2402.12554
tags:
- reasoning
- archer
- questions
- ct-3
- select
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Archer, a bilingual text-to-SQL dataset designed
  to challenge models with complex reasoning tasks including arithmetic, commonsense,
  and hypothetical reasoning. The dataset contains 1,042 English and 1,042 Chinese
  questions paired with 521 SQL queries across 20 domains.
---

# Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning

## Quick Facts
- arXiv ID: 2402.12554
- Source URL: https://arxiv.org/abs/2402.12554
- Authors: Danna Zheng; Mirella Lapata; Jeff Z. Pan
- Reference count: 24
- Even top-performing models achieve only 6.73% execution accuracy on Archer

## Executive Summary
Archer is a bilingual text-to-SQL dataset that introduces unprecedented complexity through arithmetic, commonsense, and hypothetical reasoning requirements. With 1,042 English and 1,042 Chinese questions paired with 521 SQL queries across 20 domains, Archer challenges models to go beyond simple SQL translation and engage in multi-step reasoning with mathematical calculations and implicit knowledge integration.

The dataset demonstrates that current state-of-the-art text-to-SQL models struggle significantly with these advanced reasoning tasks, with the best-performing model (GPT-4 with CT-3+COT prompt) achieving only 6.73% execution accuracy. This highlights Archer's potential as a benchmark for advancing research in complex reasoning capabilities for natural language interfaces to databases.

## Method Summary
Archer was created by generating questions across 20 databases from the Spider dataset, ensuring each question requires arithmetic reasoning and many incorporate commonsense or hypothetical reasoning. The questions are designed without explicitly quoting exact values, requiring models to predict values during SQL generation. The dataset provides both English and Chinese questions targeting the same English databases, enabling cross-linguistic evaluation. Questions were evaluated by annotators for quality and reasoning requirements, with all questions requiring arithmetic reasoning and 44.0% involving hypothetical reasoning.

## Key Results
- GPT-4 with CT-3+COT prompt achieves only 6.73% execution accuracy on Archer
- All Archer questions require arithmetic reasoning, with 44.0% involving hypothetical reasoning
- Archer questions have an average of 6.21 value slots per SQL query, emphasizing value prediction difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Archer's inclusion of arithmetic, commonsense, and hypothetical reasoning creates multi-layered complexity that exceeds existing text-to-SQL datasets.
- Mechanism: By requiring models to integrate mathematical operations, implicit knowledge, and counterfactual thinking, Archer forces deeper semantic understanding beyond simple SQL translation.
- Core assumption: Existing datasets' exclusion of these reasoning types creates an evaluation gap where current models appear more capable than they truly are.
- Evidence anchors:
  - [abstract] states Archer demonstrates "significantly higher level of complexity compared to existing publicly available datasets"
  - [section] notes "All questions in Archer require arithmetic reasoning" and "44.0% of the questions involve hypothetical reasoning"
  - [corpus] shows related work in this specific reasoning combination

### Mechanism 2
- Claim: The bilingual nature of Archer (English and Chinese questions) provides cross-linguistic generalization testing that monolingual datasets cannot offer.
- Mechanism: By requiring models to map questions in two languages to the same SQL logic, Archer exposes whether models truly understand semantic meaning versus memorizing language-specific patterns.
- Core assumption: Language-specific patterns in text-to-SQL datasets have allowed models to succeed through pattern matching rather than genuine understanding.
- Evidence anchors:
  - [abstract] states Archer "provides both English and Chinese questions"
  - [section] explains "Archer provides both English and Chinese questions to query English databases across various domains"
  - [corpus] shows related work exists in bilingual/multilingual text-to-SQL

### Mechanism 3
- Claim: Archer's emphasis on value prediction and complex SQL grammar creates a more realistic evaluation of text-to-SQL capabilities.
- Mechanism: By requiring models to handle implicit values (not quoted in questions) and complex SQL structures (GROUP BY, ORDER BY, nested queries), Archer better reflects real-world database interaction scenarios.
- Core assumption: Existing datasets' simplified value handling and SQL structures create inflated performance metrics that don't translate to practical applications.
- Evidence anchors:
  - [section] states Archer questions "do not explicitly quote exact values" and "emphasizes the importance of values, with an average of 6.21 value slots per SQL"
  - [section] notes "Archer exhibits a high usage rate of complex SQL grammar features such as GROUP BY and ORDER BY"
  - [corpus] provides limited evidence of datasets with similar value prediction emphasis

## Foundational Learning

- Concept: Relational database schema understanding
  - Why needed here: Archer requires mapping natural language questions to complex SQL queries across 20 databases with 7.55 tables and 45.25 columns on average
  - Quick check question: Given a database schema with tables A, B, and C, can you explain how foreign keys enable joins between these tables?

- Concept: Arithmetic reasoning in computational contexts
  - Why needed here: All Archer questions require arithmetic reasoning, from simple calculations to complex multi-step operations involving database values
  - Quick check question: If a question asks "How much slower is X than Y?" what mathematical operation would you use to compute this from database values?

- Concept: Commonsense knowledge integration
  - Why needed here: Archer questions require understanding implicit knowledge like "Fuel used is calculated by dividing distance driven by fuel consumption"
  - Quick check question: When a question mentions "300 miles" but the database only has "MPG" values, what commonsense knowledge would you need to derive fuel consumption?

## Architecture Onboarding

- Component map: Question parser -> Schema linker -> Reasoning engine -> SQL generator -> Value predictor -> Execution validator
- Critical path: Question understanding -> SQL structure generation -> Value prediction -> Execution validation
- Design tradeoffs: Bilingual support vs. performance optimization, explicit reasoning steps vs. end-to-end generation, value prediction accuracy vs. SQL syntax correctness
- Failure signatures: Low execution accuracy despite high valid SQL rate indicates value prediction issues; high valid SQL rate but low execution accuracy suggests reasoning gaps
- First 3 experiments:
  1. Compare zero-shot performance of LLMs with different reasoning-specific prompts on English vs. Chinese subsets
  2. Evaluate value prediction accuracy separately from SQL structure generation
  3. Test performance on arithmetic-only questions vs. questions requiring all three reasoning types to isolate difficulty sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of external knowledge bases impact the performance of text-to-SQL models on Archer's commonsense reasoning questions?
- Basis in paper: Explicit - The paper discusses that explicitly stating knowledge within questions can aid in generating correct SQL queries, suggesting potential benefits of external knowledge bases.
- Why unresolved: The paper notes that incorporating external knowledge presents significant challenges, including identifying when external knowledge is needed, extracting relevant information, and integrating it into the generation process.
- What evidence would resolve it: Empirical experiments comparing model performance with and without access to external knowledge bases on Archer's commonsense reasoning subset.

### Open Question 2
- Question: What is the optimal prompt strategy for LLMs on Archer's hypothetical reasoning questions?
- Basis in paper: Explicit - The paper shows that CT-3+COT performs worse than CT-3 specifically on hypothetical reasoning questions, indicating prompt sensitivity.
- Why unresolved: The paper only tests three prompt variations and doesn't explore other prompt engineering techniques or hypothetical reasoning-specific prompts.
- What evidence would resolve it: Systematic evaluation of different prompt engineering approaches (chain-of-thought variations, few-shot examples, instruction tuning) specifically for hypothetical reasoning questions.

### Open Question 3
- Question: How would Archer's difficulty scale with database size and complexity?
- Basis in paper: Explicit - The paper notes Archer uses databases from Spider but doesn't explore how performance changes with database complexity.
- Why unresolved: The paper focuses on Archer's current difficulty level but doesn't investigate whether larger, more complex databases would further challenge models.
- What evidence would resolve it: Performance benchmarking across databases of varying sizes, table counts, and schema complexity to establish difficulty scaling relationships.

## Limitations
- Evaluation methodology shows critical gaps in execution accuracy measurement, with concerns about measurement artifacts affecting reported performance
- Dataset's relatively small size (521 SQL queries across 20 domains) may limit generalizability despite handcrafted quality
- Practical utility for real-world applications is uncertain given extreme difficulty even for state-of-the-art models

## Confidence
- High confidence: Archer successfully introduces novel reasoning types (arithmetic, commonsense, hypothetical) not commonly found in existing text-to-SQL datasets
- Medium confidence: The claim that Archer demonstrates "significantly higher complexity" compared to existing datasets is supported by evaluation results but requires independent verification
- Low confidence: The practical utility of Archer for real-world applications is uncertain given extreme difficulty and limited domain coverage

## Next Checks
1. Implement a robust execution accuracy measurement that accounts for SQL result permutations and verify reported performance metrics across different evaluation frameworks
2. Conduct detailed analysis of model failures across arithmetic, commonsense, and hypothetical reasoning questions to identify which reasoning types pose the greatest challenges
3. Test whether models trained or fine-tuned on Archer show improved performance on existing text-to-SQL datasets to evaluate transfer learning potential