---
ver: rpa2
title: 'JetMoE: Reaching Llama2 Performance with 0.1M Dollars'
arxiv_id: '2404.07413'
source_url: https://arxiv.org/abs/2404.07413
tags:
- training
- arxiv
- language
- jetmoe-8b
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report introduces JetMoE-8B, an 8-billion parameter open-source
  MoE language model trained with a $100k budget. The model uses sparse activation
  in both attention and feed-forward layers, activating only 2 billion parameters
  per token, reducing inference computation by about 70% compared to Llama2-7B.
---

# JetMoE: Reaching Llama2 Performance with 0.1M Dollars

## Quick Facts
- arXiv ID: 2404.07413
- Source URL: https://arxiv.org/abs/2404.07413
- Reference count: 14
- Key outcome: 8B MoE model matching Llama2-7B performance at 70% lower inference cost

## Executive Summary
This report introduces JetMoE-8B, an 8-billion parameter open-source MoE language model trained with a $100k budget. The model uses sparse activation in both attention and feed-forward layers, activating only 2 billion parameters per token, reducing inference computation by about 70% compared to Llama2-7B. JetMoE-8B is trained on 1.25 trillion tokens from carefully mixed open-source datasets and achieves impressive performance, outperforming Llama2-7B and surpassing Llama2-13B-Chat in chat capabilities. The model demonstrates strong results on the OpenLLM leaderboard and code benchmarks, achieving the highest MBPP scores in Python programming.

## Method Summary
JetMoE-8B is an 8B parameter MoE model that uses sparse activation in both attention and feed-forward layers, activating only 2B parameters per token. Trained with 1.25T tokens from mixed open-source datasets using 96 H100 GPUs for 30,000 GPU hours, the model implements a two-phase training approach with increased weight on high-quality data during the learning rate decay phase. The architecture employs Megatron framework with Megablock for MoE support, using 8 experts with top-k=2 selection, and incorporates load balancing mechanisms including frequency-based auxiliary loss and z-loss.

## Key Results
- Achieves 70% reduction in inference computation compared to Llama2-7B through sparse activation
- Outperforms Llama2-7B and surpasses Llama2-13B-Chat on chat capabilities
- Achieves highest MBPP scores among evaluated models on Python programming benchmarks
- Demonstrates strong performance on OpenLLM leaderboard tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse activation in both attention and feed-forward layers reduces inference computation by ~70% compared to dense models.
- Mechanism: JetMoE replaces standard self-attention and MLP layers with MoE layers that activate only a subset of parameters per token. This allows the model to maintain 8B total parameters while only activating ~2B per input token.
- Core assumption: The router can effectively select the most relevant experts for each token without degrading performance.
- Evidence anchors:
  - [abstract] "Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
  - [section 2.1] "When g(e | x) = 0, fe(x) will not need to be evaluated, thus reducing computation cost during training and inference."
  - [corpus] Weak evidence - related papers focus on general MoE benefits but don't provide specific 70% reduction data.

### Mechanism 2
- Claim: Training with 1.25T tokens from carefully mixed open-source datasets achieves performance comparable to larger, more expensive models.
- Mechanism: The two-phase training approach increases the weight of high-quality data during the learning rate decay phase, allowing the model to focus on learning from the most informative examples.
- Core assumption: The carefully curated data mixture contains sufficient high-quality data to achieve strong performance despite the smaller token count compared to larger models.
- Evidence anchors:
  - [abstract] "JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B outperforms the Llama2-7B model."
  - [section 4.3] "Similar to the approach advocated in miniCPM (Hu et al., 2024) and Gemma (Team et al., 2024), we increase the weight of high-quality data during the learning rate decay phase."
  - [corpus] Weak evidence - related papers discuss MoE efficiency but don't provide specific data mixing strategies.

### Mechanism 3
- Claim: Sharing key-value projections across attention experts improves training stability and efficiency.
- Mechanism: By sharing Wk and Wv matrices across all attention experts while keeping Weq and Weo unique to each expert, the model reduces memory usage and improves load balancing.
- Core assumption: The shared key-value projections don't significantly limit the model's ability to learn diverse attention patterns.
- Evidence anchors:
  - [section 2.3] "We adapt MoA for our purposes, generalizing it to allow for multiple heads per expert and introducing RoPE relative positioning into the attention computation. Among these matrices, Weq and Weo are owned by each expert, but Wk and Wv are shared across experts to improve the training and inference efficiency."
  - [corpus] Weak evidence - related papers mention MoE architectures but don't discuss shared projections specifically.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how JetMoE's sparse activation differs from dense models is crucial for grasping its efficiency gains.
  - Quick check question: What is the main advantage of using MoE over a dense architecture in terms of computational efficiency?

- Concept: Load balancing in MoE models
  - Why needed here: Proper load balancing is essential for preventing expert underutilization and ensuring stable training.
  - Quick check question: How does JetMoE's use of frequency-based auxiliary loss and z-loss help maintain balanced expert utilization?

- Concept: Data mixing strategies in LLM training
  - Why needed here: Understanding how different data sources contribute to model performance is key to replicating or improving JetMoE's results.
  - Quick check question: Why does JetMoE increase the weight of high-quality data during the learning rate decay phase?

## Architecture Onboarding

- Component map:
  Router layer -> Expert selection -> Sparse attention experts -> Sparse feed-forward experts -> Load balancing mechanisms -> Final output generation

- Critical path:
  1. Input token → Router → Expert selection
  2. Selected experts process token in parallel
  3. Outputs combined and passed to next layer
  4. Repeat through all layers
  5. Final output generation

- Design tradeoffs:
  - MoE sparsity vs. model capacity: Balancing the number of experts with computational efficiency
  - Shared vs. unique projections: Trade-off between memory efficiency and representational power
  - Data quality vs. quantity: Prioritizing high-quality data within budget constraints
  - Training stability vs. performance: Implementing load balancing mechanisms to maintain stable training

- Failure signatures:
  - Uneven expert utilization: Some experts are rarely selected, wasting capacity
  - Performance degradation: Model struggles with certain tasks despite MoE architecture
  - Training instability: Loss spikes or failure to converge due to load imbalance
  - Memory issues: Excessive memory usage due to poor expert selection or data handling

- First 3 experiments:
  1. Baseline dense model comparison: Train a dense model with similar parameters to JetMoE-8B to establish performance and computational baselines.
  2. MoE ablation study: Train JetMoE variants with different numbers of experts and top-k values to find optimal configuration.
  3. Data mixing experiment: Train models with different data mixtures to validate the effectiveness of JetMoE's two-phase approach and data weighting strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual contribution of sparse activation in both attention and feed-forward layers compared to sparse activation in only feed-forward layers?
- Basis in paper: [explicit] The paper states that JetMoE-8B leverages sparse activation in both components to further reduce computational costs while maintaining performance, but notes they cannot afford ablation studies due to budget constraints.
- Why unresolved: The authors explicitly state they cannot afford ablation studies to measure the individual contribution of sparse activation in attention layers.
- What evidence would resolve it: A controlled experiment comparing JetMoE with only feed-forward layer sparsity against the current dual-sparse architecture, measuring both performance and computational efficiency.

### Open Question 2
- Question: How does the optimal data mixture for JetMoE-8B change with different training budgets and compute resources?
- Basis in paper: [explicit] The paper acknowledges that given the limited computing budget, their data mixture might not be ideal and serves as a good starting point for future optimization.
- Why unresolved: The data mixture was chosen based on empirical results from previous works rather than systematic optimization for the JetMoE architecture.
- What evidence would resolve it: A comprehensive study varying the data mixture proportions and training budgets to identify optimal configurations for different resource constraints.

### Open Question 3
- Question: What are the scaling laws for JetMoE architecture in terms of parameter count, compute efficiency, and performance across different model sizes?
- Basis in paper: [inferred] The paper presents results for a single 8B parameter model but does not explore how the architecture performs at different scales.
- Why unresolved: The paper only reports results for JetMoE-8B without exploring how the architecture scales to larger or smaller models.
- What evidence would resolve it: A systematic study training JetMoE models of varying sizes (e.g., 1B, 4B, 16B, 32B parameters) and measuring performance, activation patterns, and computational efficiency across the scale spectrum.

## Limitations

- Data Quality Uncertainty: The specific quality and curation of the mixed datasets remains unclear, which could significantly influence performance gains.
- Sparse Activation Implementation: The actual implementation details of the MoE architecture are not fully described, making it difficult to verify the claimed efficiency gains independently.
- Benchmark Context: The specific benchmark conditions and evaluation methodology are not fully detailed, making it challenging to assess whether performance claims are directly comparable.

## Confidence

**High Confidence Claims**:
- JetMoE-8B uses a Mixture-of-Experts architecture with sparse activation in both attention and feed-forward layers
- The model was trained with approximately 1.25T tokens on a budget of $100k using 96 H100 GPUs
- JetMoE-8B achieves strong performance on code benchmarks (MBPP, HumanEval) and the OpenLLM leaderboard

**Medium Confidence Claims**:
- JetMoE-8B outperforms Llama2-7B and surpasses Llama2-13B-Chat in chat capabilities
- The model reduces inference computation by about 70% compared to dense models
- The two-phase training approach with data mixing is effective for achieving high performance

**Low Confidence Claims**:
- The exact computational efficiency gains in real-world deployment scenarios
- The reproducibility of the $100k budget claim given undisclosed infrastructure costs
- The long-term stability and performance of the model on diverse, real-world tasks

## Next Checks

1. **Architecture Verification**: Implement a minimal JetMoE architecture with the described sparse activation mechanism and verify the 70% computation reduction claim through controlled experiments comparing dense vs. sparse inference times on representative workloads.

2. **Data Quality Analysis**: Conduct a detailed analysis of the data mixing strategy by training JetMoE variants with different data weightings during the learning rate decay phase to validate the effectiveness of the two-phase approach and identify the most critical data sources.

3. **Benchmark Reproducibility**: Replicate the OpenLLM leaderboard and code benchmark evaluations using standardized prompts and evaluation protocols to verify the claimed performance advantages over Llama2 models.