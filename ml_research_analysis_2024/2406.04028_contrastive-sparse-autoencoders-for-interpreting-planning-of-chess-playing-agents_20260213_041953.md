---
ver: rpa2
title: Contrastive Sparse Autoencoders for Interpreting Planning of Chess-Playing
  Agents
arxiv_id: '2406.04028'
source_url: https://arxiv.org/abs/2406.04028
tags:
- features
- feature
- figure
- concepts
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces contrastive sparse autoencoders (CSAE) for
  interpreting planning concepts in chess-playing agents. The main idea is to study
  pairs of latent representations from optimal and suboptimal game trajectories to
  discover features that differentiate between them.
---

# Contrastive Sparse Autoencoders for Interpreting Planning of Chess-Playing Agents

## Quick Facts
- arXiv ID: 2406.04028
- Source URL: https://arxiv.org/abs/2406.04028
- Reference count: 32
- The paper introduces contrastive sparse autoencoders (CSAE) for interpreting planning concepts in chess-playing agents

## Executive Summary
This paper presents a novel method for interpreting the planning mechanisms of chess-playing agents using contrastive sparse autoencoders. The key innovation is separating latent features into common ones (shared across trajectories) and differentiating ones (that distinguish optimal from suboptimal moves). By training on pairs of optimal and suboptimal chess trajectories, the method discovers interpretable chess concepts like piece safety and rook threats. Automated clustering creates a taxonomy of these concepts, providing insights into how chess agents plan multi-step strategies.

## Method Summary
The method trains contrastive sparse autoencoders on latent representations extracted from chess-playing agents. It processes pairs of optimal and suboptimal trajectories, decomposing their latent representations into two sets of features: common features (c-features) shared between both trajectories, and differentiating features (d-features) that capture what makes the optimal trajectory better. A contrastive loss encourages d-features to be active in only one representation, while c-features remain active in both. The approach includes automated feature clustering and sanity checks to evaluate feature quality and remove spurious activations.

## Key Results
- Successfully identifies interpretable chess concepts including piece safety and rook threats through activation maximization
- Automated feature clustering creates a taxonomy of discovered planning concepts
- Sanity checks reveal that c-features tend to overfit specific trajectories while d-features better capture general planning concepts
- The method provides a scalable approach to interpreting multi-step reasoning in deep neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive sparse autoencoders separate planning features from common features by using pairs of optimal and suboptimal trajectories.
- Mechanism: The model encodes pairs of latent representations into common features (c-features) shared between optimal and suboptimal trajectories, and differentiating features (d-features) that capture what makes the optimal trajectory different. This separation is enforced through a contrastive loss.
- Core assumption: The latent space contains features that are shared across trajectories and features specific to planning quality.
- Evidence anchors: [abstract] "CSAE architecture separates features into common ones (c-features) and differentiating ones (d-features) using a contrastive loss."
- Break condition: If optimal and suboptimal trajectories share too many features, the contrastive loss becomes ineffective.

### Mechanism 2
- Claim: The CSAE architecture discovers interpretable chess concepts like piece safety and rook threats through activation maximization.
- Mechanism: After training, individual features are interpreted by finding chess positions that maximally activate them, revealing semantic meanings like "protection" or "rook threat" concepts.
- Core assumption: The latent space contains linear representations of interpretable chess concepts.
- Evidence anchors: [abstract] "The method successfully identifies interpretable features linked to chess strategies like piece safety and rook threats."
- Break condition: If features are too entangled or the latent space is not linearly organized, activation maximization won't reveal interpretable concepts.

### Mechanism 3
- Claim: The CSAE architecture provides automated sanity checks to ensure feature quality.
- Mechanism: The paper proposes metrics including activation frequency, entropy of activation distribution, and linear probe performance to evaluate features and identify spurious ones.
- Core assumption: Feature quality can be evaluated using statistical metrics on activation patterns.
- Evidence anchors: [abstract] "Sanity checks show that c-features tend to overfit specific trajectories while d-features better capture planning concepts."
- Break condition: If metrics aren't sensitive enough to detect poor quality features, spurious features may be retained.

## Foundational Learning

- Concept: Sparse autoencoders (SAE)
  - Why needed here: SAEs decompose latent activations into interpretable features with sparsity constraints ensuring each feature captures specific concepts
  - Quick check question: What is the main difference between a regular autoencoder and a sparse autoencoder?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning separates common from differentiating features by comparing pairs of representations (optimal vs. suboptimal trajectories)
  - Quick check question: How does the contrastive loss encourage the separation of c-features and d-features?

- Concept: Activation maximization
  - Why needed here: Activation maximization interprets individual features by finding inputs that maximally activate them, revealing semantic meaning
  - Quick check question: What is the difference between activation maximization and gradient ascent for feature interpretation?

## Architecture Onboarding

- Component map: Input (latent pairs) -> Encoder (c/d-feature decomposition) -> Contrastive loss (feature separation) -> Decoder (reconstruction) -> Linear probe (evaluation) -> Output (interpretable features)

- Critical path:
  1. Sample optimal and suboptimal chess trajectories from the agent
  2. Extract latent representations from the agent's neural network
  3. Train CSAE to decompose latent representations into c-features and d-features
  4. Interpret individual features using activation maximization
  5. Cluster features to create a taxonomy of concepts
  6. Evaluate feature quality using sanity checks

- Design tradeoffs:
  - Sparsity vs. reconstruction accuracy: Higher sparsity leads to more interpretable features but may reduce reconstruction quality
  - Common vs. differentiating features: More c-features may improve reconstruction but reduce focus on planning concepts
  - Trajectory sampling strategy: Different strategies for sampling suboptimal trajectories may affect types of features discovered

- Failure signatures:
  - Dead features: Features that never activate indicate overly sparse dictionary
  - Overactive features: Features that activate on too many inputs indicate overly general concepts
  - Trajectory-specific features: Features that only activate on specific trajectories indicate overfitting
  - Poor reconstruction: High reconstruction error indicates missing important features

- First 3 experiments:
  1. Train CSAE on a small dataset of chess trajectories and visualize learned c-features and d-features
  2. Interpret individual d-features using activation maximization and manually label with chess concepts
  3. Evaluate feature quality using proposed sanity checks and remove spurious features based on metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the contrastive loss in CSAE architecture affect interpretability compared to traditional sparse autoencoders?
- Basis in paper: [explicit] The paper introduces CSAE with contrastive loss claiming improved interpretability
- Why unresolved: No direct quantitative comparison with traditional SAEs on interpretability metrics provided
- What evidence would resolve it: Systematic comparison using established interpretability metrics (feature correlation, specificity)

### Open Question 2
- Question: What is the optimal strategy for sampling suboptimal trajectories to pair with optimal ones in CSAE training?
- Basis in paper: [explicit] The paper mentions need to quantify impact of different strategies for suboptimal sampling
- Why unresolved: Choice of sampling strategy can introduce inductive biases and affect concept quality
- What evidence would resolve it: Comprehensive study comparing different suboptimal sampling strategies and their impact

### Open Question 3
- Question: How do CSAE-extracted concepts generalize across different chess models and layers within the same model?
- Basis in paper: [explicit] The paper briefly investigates feature correlation across layers and checkpoints
- Why unresolved: Understanding concept generalizability is crucial for assessing robustness and scalability
- What evidence would resolve it: Systematic analysis across diverse chess models using concept overlap and transferability metrics

### Open Question 4
- Question: How can the feature interpretation process be automated to reduce reliance on human expertise?
- Basis in paper: [explicit] The paper acknowledges limitations of human analysis and suggests exploring automated methods
- Why unresolved: While automated feature taxonomy is proposed, fully automated interpretation pipeline is not presented
- What evidence would resolve it: Developing and evaluating automated feature interpretation methods on large scale

## Limitations

- Limited quantitative validation showing that discovered d-features actually improve chess performance or planning quality
- Potential bias from cherry-picking interpretable features without reporting full distribution of discovered concepts
- Unknown generalization to other game domains or non-chess planning tasks

## Confidence

- Mechanism 1 (contrastive separation): Medium - theoretically sound but limited empirical validation
- Mechanism 2 (feature interpretability): Low-Medium - qualitative examples provided but no systematic coverage analysis
- Mechanism 3 (sanity checks): Medium - proposed metrics are reasonable but effectiveness not fully demonstrated

## Next Checks

1. Quantify the proportion of discovered features that are genuinely interpretable versus random activations across the full feature set
2. Test whether selectively amplifying d-features improves actual chess move quality in held-out positions
3. Evaluate the method on a simpler planning domain (e.g., Sokoban or grid navigation) where ground truth planning steps are known