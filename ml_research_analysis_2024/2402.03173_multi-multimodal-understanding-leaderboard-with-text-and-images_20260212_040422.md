---
ver: rpa2
title: 'MULTI: Multimodal Understanding Leaderboard with Text and Images'
arxiv_id: '2402.03173'
source_url: https://arxiv.org/abs/2402.03173
tags:
- questions
- question
- image
- answer
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTI, a large-scale Chinese multimodal benchmark
  designed to evaluate the performance of multimodal large language models (MLLMs)
  on real-world scientific knowledge tasks. The benchmark comprises 18,430 carefully
  curated questions from authentic examination sources, covering diverse subjects
  and question formats, including multiple-choice, fill-in-the-blank, and open-ended
  writing questions.
---

# MULTI: Multimodal Understanding Leaderboard with Text and Images

## Quick Facts
- arXiv ID: 2402.03173
- Source URL: https://arxiv.org/abs/2402.03173
- Reference count: 40
- Primary result: Benchmark of 18,430 Chinese multimodal questions shows MLLMs still lag human experts

## Executive Summary
MULTI is a large-scale Chinese multimodal benchmark designed to evaluate multimodal large language models (MLLMs) on real-world scientific knowledge tasks. The benchmark comprises 18,430 carefully curated questions from authentic examination sources, covering diverse subjects and question formats. Two specialized subsets, Multi-Elite and Multi-Extend, further challenge models with difficult reasoning tasks and in-context learning capabilities. Experiments with 25 state-of-the-art models show that while MLLMs have made progress, they still lag behind human expert performance, with the best model, Qwen2-VL-72B, achieving 76.9% accuracy on the full benchmark and 53.1% on Multi-Elite.

## Method Summary
The MULTI benchmark was constructed by collecting 18,430 questions from authentic examination materials, including college entrance exams, high school tests, and science competitions. The questions were categorized into three main types: multiple-choice, fill-in-the-blank, and open-ended writing questions. The benchmark includes two specialized subsets: Multi-Elite, containing 423 difficult reasoning questions, and Multi-Extend, designed to test in-context learning capabilities with 1,081 questions. The evaluation process involved 25 state-of-the-art MLLMs, with performance measured using accuracy metrics. The benchmark focuses on Chinese language content and real-world scientific knowledge domains.

## Key Results
- Qwen2-VL-72B achieved the highest accuracy at 76.9% on the full MULTI benchmark
- Best model scored only 53.1% on the challenging Multi-Elite subset
- All tested MLLMs significantly underperformed human expert levels across all question types

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its use of authentic examination materials that represent real-world multimodal reasoning challenges. By incorporating diverse question formats and subject domains, the benchmark captures the complexity of human-like multimodal understanding. The specialized subsets push models to demonstrate advanced capabilities like complex reasoning and in-context learning that go beyond simple pattern matching.

## Foundational Learning

1. **Multimodal Alignment**
   - Why needed: Enables models to correlate visual and textual information effectively
   - Quick check: Test model's ability to correctly associate objects in images with their textual descriptions

2. **Visual Reasoning**
   - Why needed: Required for solving complex problems that combine image analysis with logical deduction
   - Quick check: Evaluate model performance on questions requiring multi-step reasoning from visual cues

3. **In-Context Learning**
   - Why needed: Allows models to adapt to new tasks without fine-tuning by leveraging provided examples
   - Quick check: Measure accuracy improvement when examples are provided in the prompt

4. **Cross-Modal Integration**
   - Why needed: Essential for combining information from different modalities to form coherent understanding
   - Quick check: Assess model's ability to synthesize information from text and images in open-ended questions

5. **Domain-Specific Knowledge**
   - Why needed: Scientific questions require understanding of specialized concepts across multiple fields
   - Quick check: Test performance on questions from different scientific disciplines

## Architecture Onboarding

Component map: Input images and text -> Visual encoder -> Language model -> Output generator

Critical path: Image processing pipeline -> Multimodal fusion layer -> Reasoning module -> Answer generation

Design tradeoffs:
- Accuracy vs. inference speed: Larger models achieve better accuracy but require more computational resources
- Specialization vs. generalization: Domain-specific models may outperform general models on certain question types
- Parameter efficiency vs. performance: Smaller models are more deployable but may struggle with complex reasoning

Failure signatures:
- Incorrect visual grounding leading to wrong associations between text and images
- Inability to handle ambiguous or context-dependent questions
- Struggles with questions requiring common-sense reasoning beyond provided information

3 first experiments:
1. Test model performance on single-modality questions (text-only or image-only) to identify modality-specific weaknesses
2. Evaluate few-shot learning capabilities by providing varying numbers of examples in prompts
3. Assess robustness by introducing controlled perturbations to question formats and image qualities

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on Chinese-language content may limit generalizability to other languages and cultural contexts
- Examination-style questions may not fully capture the complexity and ambiguity of real-world multimodal scenarios
- Potential bias in curated question selection from specific examination materials

## Confidence

High confidence in the benchmark construction methodology and dataset curation process
Medium confidence in the reported model performance comparisons due to potential implementation variations
Medium confidence in the conclusion about room for improvement, given the controlled experimental conditions
Low confidence in the generalizability of results to non-Chinese contexts

## Next Checks

1. Conduct cross-linguistic validation by translating a subset of questions to English and evaluating model performance consistency across languages
2. Perform ablation studies to isolate the impact of different question types (multiple-choice, fill-in-the-blank, writing) on model performance
3. Implement error analysis protocols to categorize and quantify specific failure modes across different model architectures