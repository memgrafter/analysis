---
ver: rpa2
title: 'AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts'
arxiv_id: '2404.05993'
source_url: https://arxiv.org/abs/2404.05993
tags:
- safety
- content
- dataset
- categories
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AEGIS, a framework for online adaptive content
  safety moderation using an ensemble of large language models (LLMs). The authors
  address the lack of high-quality datasets and benchmarks for LLM safety by creating
  AEGISSAFETYDATASET, a human-annotated dataset with 26,000 instances across 13 major
  and 9 sub-categories of safety risks.
---

# AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts

## Quick Facts
- arXiv ID: 2404.05993
- Source URL: https://arxiv.org/abs/2404.05993
- Reference count: 35
- Primary result: Novel online adaptive framework for LLM safety moderation using ensemble of experts with strong theoretical guarantees

## Executive Summary
This paper introduces AEGIS, a framework for online adaptive content safety moderation using an ensemble of large language models (LLMs). The authors address the lack of high-quality datasets and benchmarks for LLM safety by creating AEGISSAFETYDATASET, a human-annotated dataset with 26,000 instances across 13 major and 9 sub-categories of safety risks. They instruction-tune multiple LLM-based safety models on this dataset, achieving competitive or superior performance compared to state-of-the-art safety models on various benchmarks. The framework demonstrates strong theoretical guarantees and improved robustness against jailbreak attacks.

## Method Summary
AEGIS combines instruction-tuned LLM safety experts with a no-regret online learning framework to dynamically select the most appropriate expert for each moderation task. The system uses human-annotated safety data to train multiple expert models, then employs an online adaptation algorithm that adjusts expert weights based on oracle feedback. The framework introduces a "Needs Caution" category to reduce over-defensiveness while maintaining safety standards.

## Key Results
- Instruction-tuned AEGISSAFETYEXPERTS outperform baseline models on ToxicChat, OpenAI Moderation Dataset, and SimpleSafetyTests
- AEGIS framework achieves strong theoretical guarantees for regret minimization in expert selection
- Models demonstrate improved robustness against jailbreak attacks compared to single-model approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AEGIS uses online no-regret learning to dynamically select the best safety expert for each moderation task.
- Mechanism: At each round, AEGIS receives predictions from all safety experts, selects one expert based on a weighted probability distribution, and updates the weights according to the feedback received from an oracle.
- Core assumption: The feedback oracle provides accurate and timely ground truth labels for each moderation instance.
- Evidence anchors:
  - [abstract] "we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment"
  - [section] "At each round t = 1, 2, ...,T , a learner selects an expert It for advice from predetermined set E of experts...The goal of the learner it to minimize its expected regret...One of the most renowned algorithms in this framework is the Exponential Weighs (EW) algorithm or Hedge"
- Break condition: If the oracle feedback is delayed, noisy, or inconsistent, the weight updates become unreliable and the algorithm may converge to suboptimal experts.

### Mechanism 2
- Claim: Instruction tuning diverse LLM models on AEGIS SAFETY DATASET improves their ability to detect multiple safety risk categories.
- Mechanism: By fine-tuning models like Llama Guard and NEMO43B on the human-annotated dataset, the models learn to recognize patterns across 13 major and 9 sub-categories of safety risks, enabling them to generalize beyond their original training.
- Core assumption: The dataset covers a representative and balanced distribution of safety risks encountered in real LLM interactions.
- Evidence anchors:
  - [abstract] "we instruction-tune multiple LLM-based safety models (AEGISSAFETYEXPERTS) on this dataset, achieving competitive or superior performance compared to state-of-the-art safety models"
  - [section] "We train our models using Low-Rank Adapters (Hu et al., 2021) for our dataset and taxonomy, fine-tuning the LoRA weights on 11, 000 samples...Our AEGIS SAFETY EXPERTS are not finetuned on the OpenAI Mod or ToxicChat datasets, but when instruction tuned using just half of our AEGIS SAFETY DATASET, outperform the base models"
- Break condition: If the dataset contains annotation errors or is biased toward certain risk categories, the models will learn incorrect patterns and perform poorly on unseen risks.

### Mechanism 3
- Claim: Introducing a "Needs Caution" category reduces over-defensiveness and improves user experience.
- Mechanism: Instead of outright blocking ambiguous or borderline content, the system labels it as "Needs Caution", allowing downstream systems to decide whether to block or allow based on context.
- Core assumption: Many borderline cases can be safely handled with additional scrutiny rather than automatic blocking.
- Evidence anchors:
  - [abstract] "The introduction of the category Needs Caution is driven by the motivation of not having to categorize ambiguous instances as unsafe and thereby blocking the user query"
  - [section] "Although, we have a total of 14 explicit categories, we also have introduced an extensible categoryOther to handle unsafe categories that are not captured in our taxonomy...Our safety guidelines contain the definitions, descriptions, the rules of inclusion and exclusion and an elaborate list of examples to indicate the hard negatives with each category"
- Break condition: If "Needs Caution" is overused, it may dilute the system's ability to catch truly harmful content, or if underused, it may still block too many benign cases.

## Foundational Learning

- Concept: Multi-class classification for safety risk detection
  - Why needed here: The system must categorize each input into one or more of 13 major and 9 sub-categories of safety risks, requiring a model that can handle multiple overlapping labels.
  - Quick check question: Can the model output multiple risk categories for a single input, or is it limited to one label per input?

- Concept: Online learning with experts (no-regret framework)
  - Why needed here: The system must adapt in real-time to changing data distributions and feedback, which requires a learning algorithm that can dynamically adjust expert weights without needing to retrain from scratch.
  - Quick check question: How does the algorithm ensure that the regret (difference between cumulative loss of the learner and the best expert) remains bounded over time?

- Concept: Adversarial robustness and jailbreak detection
  - Why needed here: Even with safety-trained models, attackers can craft inputs to bypass moderation, so the system must be resilient to such attacks.
  - Quick check question: What specific techniques (e.g., TAP, GCG) were used to test the models' resilience to jailbreaks?

## Architecture Onboarding

- Component map: Input text -> Expert models (Llama Guard, NEMO43B) -> AEGIS online adaptation -> Oracle feedback -> Updated expert weights -> Output safety categories

- Critical path: 1. Receive input text 2. Generate predictions from all expert models 3. Select expert based on weighted probability distribution 4. Apply selected expert's prediction 5. Receive oracle feedback 6. Update expert weights

- Design tradeoffs:
  - Using multiple expert models increases coverage but adds inference latency
  - Fine-tuning on AEGIS dataset improves performance but requires maintaining and updating the dataset
  - Online adaptation allows real-time learning but depends on oracle feedback availability

- Failure signatures:
  - High false positive rate: Model is over-defensive, blocking too many benign cases
  - High false negative rate: Model is under-defensive, missing harmful content
  - Slow adaptation: Weights do not update quickly enough to respond to new attack patterns

- First 3 experiments:
  1. Evaluate each expert model's performance on AEGIS test set to establish baseline F1 scores
  2. Test the online adaptation algorithm with synthetic oracle feedback to verify weight updates
  3. Measure the impact of the "Needs Caution" category on overall false positive/negative rates

## Open Questions the Paper Calls Out

None

## Limitations

- The framework's performance degrades if oracle feedback is delayed, noisy, or inconsistent, but robustness analysis under these conditions is lacking
- The dataset's coverage of real-world safety risks may be incomplete, potentially leading to blind spots in the expert models
- Computational overhead and latency implications of running multiple expert models in production are not discussed

## Confidence

- High confidence: The instruction-tuning approach on diverse safety categories and the creation of a multi-category taxonomy are well-supported by empirical results showing improved F1 scores over baseline models.
- Medium confidence: The online adaptation mechanism works as described, but its robustness to imperfect oracle feedback remains uncertain.
- Low confidence: The "Needs Caution" category's impact on reducing over-defensiveness is conceptually sound but lacks quantitative validation of user experience improvements.

## Next Checks

1. Stress-test the online adaptation algorithm with synthetic oracle feedback that includes noise, delays, and adversarial patterns to measure performance degradation and convergence properties.
2. Conduct a thorough bias and coverage analysis of the AEGISSAFETYDATASET to identify underrepresented safety categories and potential annotation inconsistencies.
3. Benchmark the end-to-end inference latency of the ensemble approach compared to single-model baselines under realistic throughput requirements.