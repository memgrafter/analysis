---
ver: rpa2
title: Cost-Effective Online Multi-LLM Selection with Versatile Reward Models
arxiv_id: '2405.16587'
source_url: https://arxiv.org/abs/2405.16587
tags:
- uni00000013
- uni00000015
- uni00000011
- uni00000026
- uni00000010
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C2MAB-V, a cost-effective combinatorial multi-armed
  bandit framework for optimal LLM selection and usage. The framework addresses the
  challenge of selecting multiple LLMs over a combinatorial search space for various
  collaborative task types while balancing cost and reward.
---

# Cost-Effective Online Multi-LLM Selection with Versatile Reward Models

## Quick Facts
- arXiv ID: 2405.16587
- Source URL: https://arxiv.org/abs/2405.16587
- Authors: Xiangxiang Dai; Jin Li; Xutong Liu; Anqi Yu; John C. S. Lui
- Reference count: 40
- Primary result: Introduces C2MAB-V framework achieving 64.72% reward/violation ratio improvement over baselines

## Executive Summary
This paper presents C2MAB-V, a cost-effective combinatorial multi-armed bandit framework for optimal LLM selection and usage. The framework addresses the challenge of selecting multiple LLMs over a combinatorial search space for various collaborative task types while balancing cost and reward. C2MAB-V employs an online feedback mechanism and confidence bound technique to manage the exploration-exploitation trade-off across different models. The method decomposes the NP-hard integer linear programming problem into a relaxed form, utilizes a discretization rounding scheme, and updates selections based on continual feedback.

## Method Summary
C2MAB-V is an online learning framework that solves the combinatorial multi-armed bandit problem for LLM selection. The method uses a local-cloud architecture where the local server performs continuous relaxation and cost estimation using confidence bounds, while the scheduling cloud handles discretization and final selection. The framework implements three versatile reward models (AWC, SUC, AIC) to capture different collaborative task structures. The algorithm updates selections based on continual feedback, achieving superior performance in balancing performance and cost-efficiency across nine LLMs and three application scenarios.

## Key Results
- Achieves reward/violation ratio improvement of at least 64.72% over baselines
- Demonstrates superior performance in balancing performance and cost-efficiency with nine LLMs across three application scenarios
- Theoretical analysis proves strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The local-cloud architecture enables efficient online LLM selection by decomposing the NP-hard problem into manageable sub-problems
- Mechanism: The local server performs continuous relaxation and cost estimation using confidence bounds, while the scheduling cloud handles discretization and final selection, leveraging their respective computational strengths
- Core assumption: The local server has limited computational resources but can efficiently solve continuous relaxation problems, while the cloud can handle discrete rounding with better resources
- Evidence anchors:
  - [abstract]: "decomposing the integer problem into a relaxed form by the local server" and "utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud"
  - [section]: "To mitigate the computational hardness of the LLM selection problem, the local server adopts a relaxed strategy" and "the synchronized cloud employs specialized discretization rounding algorithms"
  - [corpus]: Weak - corpus papers focus on routing and selection but don't discuss this specific two-tier decomposition architecture
- Break condition: If the communication overhead between local server and cloud exceeds the computational savings from decomposition, or if the discretization rounding loses too much precision

### Mechanism 2
- Claim: Confidence bound techniques enable effective exploration-exploitation tradeoff in online LLM selection
- Mechanism: Optimistic reward bounds (μ̄ₜ,ₖ = min{μ̂ₜ,ₖ + αμρₜ,μₖ, 1}) and pessimistic cost bounds (c̄ₜ,ₖ = max{ĉₜ,ₖ - αcρₜ,cₖ, 0}) create adaptive selection criteria that balance trying new models with exploiting known good ones
- Core assumption: The confidence bounds correctly capture the uncertainty in reward and cost estimates, with high probability events Nμ and Nc occurring
- Evidence anchors:
  - [abstract]: "online feedback mechanism and confidence bound technique to manage the exploration-exploitation trade-off"
  - [section]: "we implement the confidence bound (CB) method" and "The confidence radius of CB as ρt,μₖ = √(ln(2π²Kt³/3δ)/2Tt,μₖ)"
  - [corpus]: Moderate - CUCB and related bandit algorithms use similar confidence bounds, but not specifically for this cost-aware multi-LLM setting
- Break condition: If the exploration parameter α is poorly tuned, leading to either insufficient exploration or excessive exploitation, or if the cost uncertainty is much larger than reward uncertainty

### Mechanism 3
- Claim: The versatile reward models (AWC, SUC, AIC) capture different collaborative task structures and enable optimal multi-LLM combinations
- Mechanism: Each reward model encodes different success criteria - any win (AWC) maximizes user satisfaction through redundancy, sum up (SUC) speeds task completion through parallelization, and all in (AIC) ensures project success through complete collaboration
- Core assumption: The different task structures can be effectively modeled by these three reward functions, and the combinatorial selection can optimize for each structure
- Evidence anchors:
  - [abstract]: "versatile reward models for different collaborative task types" and specific formulations for AWC, SUC, AIC
  - [section]: "We consider versatile reward models for different combinations of LLMs in multi-LLM tasks below" with detailed formulations
  - [corpus]: Strong - multiple related papers (IRT-Router, Router-R1) address multi-LLM routing but with different formulations; this paper's approach of modeling different task structures is novel
- Break condition: If real-world tasks don't fit these three categories well, or if the combinatorial optimization doesn't scale to more complex reward structures

## Foundational Learning

- Concept: Multi-armed bandit problem and combinatorial bandit extensions
  - Why needed here: The core selection problem is framed as a combinatorial multi-armed bandit where each "arm" is an LLM and actions are combinations of LLMs
  - Quick check question: What's the key difference between standard MAB and combinatorial MAB in this context?

- Concept: Submodular optimization and greedy approximation algorithms
  - Why needed here: The reward functions exhibit diminishing marginal returns (submodularity), allowing efficient approximation via greedy algorithms
  - Quick check question: Why does the AWC reward function satisfy the submodular property?

- Concept: Confidence bounds and concentration inequalities
  - Why needed here: Essential for the exploration-exploitation tradeoff, providing statistical guarantees on reward and cost estimates
  - Quick check question: What's the relationship between the confidence radius formula and the Chernoff-Hoeffding inequality?

## Architecture Onboarding

- Component map: Local Server -> Scheduling Cloud -> LLMs
- Critical path: User query → Local server relaxation → Cloud discretization → LLM selection → Query execution → Feedback collection → Server update → Next round
- Design tradeoffs:
  - Centralization vs. distribution: Local-cloud architecture trades off communication overhead for computational efficiency
  - Precision vs. tractability: Relaxation introduces approximation but makes the problem solvable in polynomial time
  - Exploration vs. exploitation: Confidence bounds must be carefully tuned for optimal performance
- Failure signatures:
  - High communication latency between local and cloud components
  - Poor discretization rounding leading to suboptimal selections
  - Incorrect confidence bound parameters causing inefficient exploration
- First 3 experiments:
  1. Baseline test: Run with only the local server making selections (no cloud) to measure the value of the decomposition
  2. Parameter sensitivity: Vary αμ and αc to find optimal exploration-exploitation balance
  3. Reward model comparison: Test all three reward models (AWC, SUC, AIC) on same task to validate their appropriateness for different scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does C2MAB-V perform in scenarios with more than nine LLMs or with different distributions of LLM capabilities?
- Basis in paper: [explicit] The paper evaluates C2MAB-V with nine LLMs across three application scenarios, but does not explore scenarios with a larger number of LLMs or different capability distributions
- Why unresolved: The paper focuses on a specific set of nine LLMs and does not provide insights into how the framework scales with more LLMs or adapts to different distributions of capabilities
- What evidence would resolve it: Experiments with a larger set of LLMs or simulations with varied capability distributions would provide insights into the scalability and adaptability of C2MAB-V

### Open Question 2
- Question: How does the performance of C2MAB-V change with different types of user feedback mechanisms, such as implicit feedback from user behavior?
- Basis in paper: [explicit] The paper mentions that feedback can include direct user input and data from techniques that quantify user behavior, but does not explore the impact of different feedback mechanisms on performance
- Why unresolved: The paper primarily focuses on explicit user feedback and does not investigate how implicit feedback or other feedback types might influence the algorithm's performance
- What evidence would resolve it: Comparative studies using different feedback mechanisms, including implicit feedback, would clarify the impact on C2MAB-V's performance

### Open Question 3
- Question: How does C2MAB-V handle scenarios where the cost structures of LLMs change dynamically over time?
- Basis in paper: [inferred] The paper discusses a statistically-based cost model and the importance of cost considerations, but does not address dynamic changes in cost structures
- Why unresolved: The cost model is based on static assumptions, and the paper does not explore how the framework adapts to dynamic cost changes
- What evidence would resolve it: Simulations or experiments with dynamic cost structures would reveal how C2MAB-V adapts to changes in LLM pricing over time

### Open Question 4
- Question: What are the implications of using C2MAB-V in real-world applications with privacy concerns and data protection regulations?
- Basis in paper: [explicit] The paper mentions that the cloud does not have access to original sensitive user data, enhancing privacy protection prospects, but does not delve into compliance with data protection regulations
- Why unresolved: The discussion on privacy is brief and does not address the broader implications of data protection regulations like GDPR or CCPA
- What evidence would resolve it: Case studies or analyses of C2MAB-V in regulated environments would provide insights into its compliance and privacy implications

## Limitations

- Framework effectiveness depends heavily on accurate cost estimation and confidence bound calibration
- Local-cloud architecture introduces communication overhead that could impact real-time performance
- Three reward models may not capture all possible task structures, and combinatorial optimization becomes increasingly complex as LLM count grows

## Confidence

- **High confidence** in the theoretical framework and regret/violation bounds (proven via mathematical analysis)
- **Medium confidence** in practical performance claims (based on experimental results with 9 LLMs on SciQ dataset)
- **Medium confidence** in the general applicability across diverse task types (validated on three specific scenarios)

## Next Checks

1. **Communication Overhead Analysis**: Measure the actual latency and bandwidth costs of the local-cloud communication pattern to quantify the trade-off between computational efficiency and network overhead
2. **Scalability Testing**: Evaluate performance as the number of LLMs increases beyond 9 to identify the point where the combinatorial complexity becomes prohibitive
3. **Reward Model Generality**: Test the framework on task types that don't clearly fit AWC, SUC, or AIC categories to assess how well the reward models generalize beyond the three specific structures