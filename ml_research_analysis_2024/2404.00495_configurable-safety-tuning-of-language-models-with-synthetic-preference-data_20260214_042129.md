---
ver: rpa2
title: Configurable Safety Tuning of Language Models with Synthetic Preference Data
arxiv_id: '2404.00495'
source_url: https://arxiv.org/abs/2404.00495
tags:
- safety
- system
- preference
- data
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Configurable Safety Tuning (CST) addresses the inflexibility of
  standard fine-tuning methods like Direct Preference Optimization (DPO) that hard-code
  specific behaviors into language models. CST introduces a system prompt-based approach
  that enables dynamic safety configuration at inference time without requiring additional
  synthetic data beyond what is already used in DPO.
---

# Configurable Safety Tuning of Language Models with Synthetic Preference Data

## Quick Facts
- arXiv ID: 2404.00495
- Source URL: https://arxiv.org/abs/2404.00495
- Authors: Victor Gallego
- Reference count: 6
- One-line primary result: CST achieves configurable safety with near-perfect safe response generation while preserving uncensored capabilities when requested

## Executive Summary
Configurable Safety Tuning (CST) addresses the inflexibility of standard fine-tuning methods like Direct Preference Optimization (DPO) that hard-code specific behaviors into language models. CST introduces a system prompt-based approach that enables dynamic safety configuration at inference time without requiring additional synthetic data beyond what is already used in DPO. By leveraging self-critique to generate preference pairs and conditioning the preference probability on both user prompts and system prompts, CST allows deployment teams to enable or disable safety behaviors simply by changing the system prompt.

Experimental results on OpenHermes-2.5-Mistral-7B and SOLAR-Instruct-10.7B models show that CST achieves near-perfect performance (S1 scores of 1.00) for generating safe responses while maintaining the ability to produce uncensored content (S0 scores of 0.92-1.00) when requested. In multi-task scenarios, CST outperforms DPO with average scores of 0.98-0.96 versus 0.74-0.72, and maintains general capabilities on ARC, HellaSwag, MMLU, and TruthfulQA benchmarks with scores comparable to or slightly better than baseline models (66.2→68.6 and 74.2→74.3).

## Method Summary
CST modifies the standard DPO framework by conditioning preference probabilities on system prompts that specify safety configurations. The method uses self-critique to generate synthetic preference pairs, then creates dual training entries by reversing preferences for opposing system prompts. This allows a single model to learn both safe and uncensored behaviors using the same data. The loss function is augmented to include system prompt conditioning, enabling dynamic safety configuration at inference time without requiring additional synthetic preference data compared to standard DPO pipelines.

## Key Results
- Achieves near-perfect safe response generation (S1 scores of 1.00) while maintaining uncensored capabilities (S0 scores of 0.92-1.00)
- Outperforms DPO in multi-task scenarios with average scores of 0.98-0.96 versus 0.74-0.72
- Maintains general capabilities on ARC, HellaSwag, MMLU, and TruthfulQA benchmarks with comparable or slightly improved scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CST enables dynamic safety configuration by conditioning preference probabilities on system prompts
- Mechanism: The method modifies DPO's preference probability from ˆpθ(y1 ≻ y0|x) to ˆpθ(y1 ≻ y0|x, s), allowing the same model to generate both safe and uncensored responses based on the system prompt provided at inference time
- Core assumption: The LLM can learn to associate specific system prompts with corresponding safety behaviors through synthetic preference data
- Evidence anchors:
  - [abstract] "CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations"
  - [section II] "the preference probability ˆpθ(y1 ≻ y0|x) depends on the context, represented by a system prompt s that specifies the safety configuration"
  - [corpus] Weak - related papers discuss preference tuning but don't specifically address system prompt conditioning
- Break condition: If the model fails to learn the association between system prompts and desired behaviors, or if the synthetic preference pairs don't capture the full range of safety configurations needed

### Mechanism 2
- Claim: CST achieves safety tuning without requiring additional synthetic data beyond standard DPO
- Mechanism: By using the same synthetic preference pairs but reversing the preference relation ≻ for opposing system prompts, CST creates dual-purpose training data that teaches both safe and uncensored behaviors
- Core assumption: The self-critique method can generate high-quality preference pairs that, when reversed, provide sufficient signal for both safety configurations
- Evidence anchors:
  - [section II] "we can leverage the synthetic data pairs by setting ˆpθ(y1 ≻ y0|x, s1) = 1 − ˆpθ(y1 ≻ y0|x, s0)"
  - [abstract] "CST... facilitates the flexible and controlled adjustment of language models' safety levels, using only synthetic preference data"
  - [corpus] Weak - related work mentions synthetic data generation but doesn't specifically address data efficiency through preference reversal
- Break condition: If the synthetic preference pairs are of low quality or if the preference reversal doesn't capture meaningful behavioral differences

### Mechanism 3
- Claim: CST preserves general capabilities while enabling configurable safety
- Mechanism: By maintaining the original DPO loss function structure and only adding system prompt conditioning, CST avoids catastrophic forgetting of general knowledge and reasoning abilities
- Core assumption: The model's general capabilities are sufficiently robust to survive fine-tuning that primarily focuses on safety behavior modification
- Evidence anchors:
  - [abstract] "CST successfully manages different safety configurations and retains the original functionality of LLMs"
  - [section III] "CST not only enables safety configuration of the models at inference time, it also doesn't degrade performance in downstream tasks"
  - [section III] "Results in Table III... evidences that CST... doesn't degrade performance in downstream tasks such as general knowledge question-answering or reasoning"
- Break condition: If the safety tuning overpowers the model's general capabilities, leading to degradation in tasks like ARC, HellaSwag, MMLU, and TruthfulQA

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CST builds directly on DPO by modifying its preference probability conditioning to include system prompts
  - Quick check question: How does DPO differ from standard reinforcement learning from human feedback (RLHF)?

- Concept: Self-critique for synthetic preference generation
  - Why needed here: CST uses self-critique to generate the initial preference pairs that are then augmented with system prompts
  - Quick check question: What is the key advantage of using self-critique over human-annotated preference data?

- Concept: System prompt conditioning in LLMs
  - Why needed here: The core innovation of CST relies on the model's ability to interpret and respond to system prompts
  - Quick check question: How do system prompts typically influence LLM behavior compared to user prompts?

## Architecture Onboarding

- Component map:
  - Self-critique module -> Synthetic preference pairs (xi, yi0, yi1)
  - System prompt encoder -> Safety configuration specification
  - Preference probability calculator -> Computes ˆpθ(y1 ≻ y0|x, s) with system prompt conditioning
  - DPO optimizer -> Updates model parameters using the modified loss function

- Critical path:
  1. Generate synthetic preference pairs using self-critique
  2. Create dual entries with opposite system prompts and reversed preferences
  3. Train model with modified DPO loss incorporating system prompt conditioning
  4. Validate safety configuration switching at inference time

- Design tradeoffs:
  - Data efficiency vs. behavioral coverage: Using preference reversal saves data but may miss nuanced safety behaviors
  - Safety vs. general capability preservation: Balancing safety tuning strength to avoid degrading other capabilities
  - System prompt simplicity vs. expressiveness: Simple prompts work better but may not capture complex safety requirements

- Failure signatures:
  - Model fails to switch behaviors when system prompts change
  - Performance degradation on general capability benchmarks
  - Inconsistent safety behavior across similar prompts
  - Over-conservatism when "safe" mode is enabled

- First 3 experiments:
  1. Test safety configuration switching: Use harmful behavior test set with both system prompts to verify S0 and S1 scores
  2. Validate general capability preservation: Run ARC, HellaSwag, MMLU, and TruthfulQA benchmarks on CST-tuned model
  3. Compare with DPO ablation: Train identical model with only safety-focused system prompt to isolate benefit of dual configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CST perform with more than two system prompts representing different safety levels or behavioral modes?
- Basis in paper: [inferred] The paper mentions that "Further work shall study more fine-grained controls of safety, i.e., depending on semantic topics," suggesting this has not been tested
- Why unresolved: The current implementation only tests two opposing system prompts (s0 and s1), and the paper explicitly identifies exploring more nuanced configurations as future work
- What evidence would resolve it: Experiments testing CST with 3+ system prompts representing different safety levels (e.g., fully uncensored, moderate safety, maximum safety) and measuring performance across these configurations

### Open Question 2
- Question: What is the impact of CST on the model's computational efficiency during inference, particularly when switching between safety configurations?
- Basis in paper: [inferred] The paper does not discuss inference time or computational overhead differences between CST and standard fine-tuning methods
- Why unresolved: While the paper demonstrates CST's effectiveness, it does not report any measurements of inference latency or computational costs when using different system prompts
- What evidence would resolve it: Benchmark comparisons of inference time and memory usage for CST-tuned models versus baseline models under different system prompt configurations

### Open Question 3
- Question: How does CST perform when the system prompt and user prompt contain conflicting instructions or when safety requirements change mid-conversation?
- Basis in paper: [inferred] The paper tests CST with predefined system prompts but does not explore scenarios where safety requirements dynamically change during interaction
- Why unresolved: The experimental setup uses static system prompts applied to test sets, but real-world applications may involve dynamic safety requirement changes or conflicting instructions
- What evidence would resolve it: Testing CST in conversational scenarios where system prompts change between turns or where user prompts explicitly contradict system prompt safety instructions

### Open Question 4
- Question: What is the minimum amount of synthetic preference data needed for CST to maintain its configurable safety capabilities while still outperforming standard DPO?
- Basis in paper: [explicit] The paper states "CST enables controlling the safety behavior of LLMs, with no need for additional synthetic preference data compared with what is already available in current fine-tuning pipelines"
- Why unresolved: While the paper confirms CST doesn't need more data than DPO, it doesn't explore whether less data could still achieve the same benefits
- What evidence would resolve it: Experiments varying the amount of synthetic preference data used for CST training and measuring performance degradation thresholds compared to full data training

### Open Question 5
- Question: How does CST perform on safety-sensitive domains beyond the tested Harmful Behaviors dataset, such as medical advice or legal consultation?
- Basis in paper: [inferred] The paper tests on general safety tasks but doesn't explore specialized domain-specific safety requirements
- Why unresolved: The experimental evaluation focuses on general safety benchmarks and multi-task scenarios, but doesn't test domain-specific safety-critical applications
- What evidence would resolve it: Evaluations of CST-tuned models on domain-specific safety benchmarks for medical, legal, financial, or other specialized fields with unique safety requirements

## Limitations

- The method relies heavily on synthetic data quality and the assumption that preference reversal provides sufficient behavioral coverage
- Evaluation uses GPT-4 classification for safety assessment, introducing potential subjectivity and reliability concerns
- Performance on truly novel harmful prompts versus the curated test set remains unclear
- Claim that no additional synthetic data is required may be overstated if synthetic pairs are insufficient for capturing safety complexity

## Confidence

- **High confidence**: The core mechanism of system prompt conditioning for safety configuration (Mechanism 1) - the mathematical formulation and implementation are clearly specified and the experimental results support this claim.
- **Medium confidence**: The claim that no additional synthetic data is required beyond standard DPO (Mechanism 2) - while the preference reversal approach is clever, its sufficiency for comprehensive safety coverage needs further validation.
- **Medium confidence**: The preservation of general capabilities during safety tuning (Mechanism 3) - the benchmark results show maintenance of capabilities, but the evaluation scope may not capture all potential degradation scenarios.

## Next Checks

1. **Generalization to novel harmful prompts**: Test CST-tuned models on a separate set of harmful prompts not present in the training data to verify that safety behaviors generalize beyond the specific examples used in synthetic preference generation.

2. **Human evaluation of safety configurations**: Replace GPT-4 classification with human annotators to assess the quality and consistency of safety behavior switching, addressing potential reliability issues with automated safety assessment.

3. **Long-term stability analysis**: Evaluate model behavior after extended deployment to check for "safety drift" where the model gradually loses its ability to maintain the desired safety configurations across different system prompts.