---
ver: rpa2
title: Construction of Domain-specified Japanese Large Language Model for Finance
  through Continual Pre-training
arxiv_id: '2404.10555'
source_url: https://arxiv.org/abs/2404.10555
tags:
- japanese
- tuned
- financial
- language
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the absence of Japanese financial-specific
  large language models by constructing one through domain-focused continual pre-training.
  The approach involves building a Japanese financial corpus from diverse sources
  (speeches, reports, web articles, etc.) and using it to fine-tune a state-of-the-art
  Japanese LLM (rinna/nekomata-14b).
---

# Construction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training

## Quick Facts
- arXiv ID: 2404.10555
- Source URL: https://arxiv.org/abs/2404.10555
- Reference count: 40
- Primary result: Domain-specific continual pre-training improves Japanese LLM performance on financial benchmarks

## Executive Summary
This study addresses the absence of Japanese financial-specific large language models by constructing one through domain-focused continual pre-training. The approach involves building a Japanese financial corpus from diverse sources (speeches, reports, web articles, etc.) and using it to fine-tune a state-of-the-art Japanese LLM (rinna/nekomata-14b). The tuned model was evaluated on Japanese financial benchmarks and showed improved performance over the base model across all tasks, achieving an overall score of 0.4716 versus 0.4335 for the original. Qualitative output comparisons indicated the tuned model's answers were more informative and accurate, though some domain-specific questions remained challenging. These findings demonstrate that domain-specific continual pre-training is effective for adapting LLMs to specialized domains. The tuned model is publicly available on Hugging Face.

## Method Summary
The method involves constructing a Japanese financial corpus from diverse sources including speeches, reports, web articles, Wikipedia dumps, and official documents, then using this corpus to perform continual pre-training on the rinna/nekomata-14b base model. The training uses 4x A100 80GB GPUs with DeepSpeed and Accelerate, employing hyperparameters including a learning rate starting from 5e-7 decaying to 0, 5 epochs, batch size 24, and gradient checkpointing enabled. The model is evaluated on Japanese financial benchmarks (Chabsa, CMA basics, CPA audit, FP2, Security Sales 1) and through qualitative output comparisons.

## Key Results
- The tuned model achieved an overall benchmark score of 0.4716 versus 0.4335 for the base model
- Improved performance across all Japanese financial benchmark tasks compared to the original model
- Qualitative output comparisons showed the tuned model generated more informative and longer responses

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific continual pre-training improves LLM performance on specialized tasks by exposing the model to task-relevant vocabulary and concepts. The pre-training process updates the model's weights using financial corpora, allowing it to internalize domain-specific language patterns and factual knowledge. Core assumption: The base model (rinna/nekomata-14b) has sufficient capacity to learn new domain knowledge without catastrophic forgetting of general language abilities. Evidence: The tuned model performed better than the original model on Japanese financial benchmarks. Break condition: If the financial corpus lacks diversity or contains noisy/incorrect information, the model may learn erroneous patterns or fail to generalize beyond memorized examples.

### Mechanism 2
Continual pre-training with task-specific data improves the model's ability to generate longer, more informative outputs in the target domain. Exposure to diverse financial documents teaches the model appropriate discourse structures and response patterns for financial queries. Core assumption: The model's generation capabilities can be shaped by the style and content of the pre-training data. Evidence: The tuned model's outputs tend to be better than the original model's outputs in terms of quality and length. Break condition: If the corpus contains predominantly short, fragmented texts, the model may not learn to generate comprehensive responses.

### Mechanism 3
Using a strong base model minimizes the amount of domain-specific data needed to achieve meaningful performance gains. A model already proficient in Japanese and general language tasks can adapt more efficiently to specialized domains with less data. Core assumption: Transfer learning benefits from pre-existing language proficiency and reduce the sample complexity for domain adaptation. Evidence: The rinna/nekomata-14b achieved state-of-the-art performance on Japanese financial benchmarks among 10-billion-class-parameter models. Break condition: If the base model lacks sufficient capacity or relevant pre-training, even large domain-specific datasets may not yield improvements.

## Foundational Learning

- **Concept**: Continual pre-training vs. fine-tuning
  - Why needed here: The paper distinguishes between standard fine-tuning (which updates weights for specific tasks) and continual pre-training (which extends the pre-training phase with domain data). Understanding this difference is crucial for interpreting the methodology and results.
  - Quick check question: What is the key difference between continual pre-training and instruction tuning, and why might continual pre-training be preferred for adding domain knowledge?

- **Concept**: Catastrophic forgetting
  - Why needed here: When adapting a pre-trained model to a new domain, there's a risk that the model will lose its ability to perform general tasks. The paper's success implies this wasn't a major issue, but understanding the concept helps evaluate the approach.
  - Quick check question: What techniques could be used to prevent catastrophic forgetting when performing domain-specific continual pre-training?

- **Concept**: Evaluation metrics for LLMs
  - Why needed here: The paper uses both benchmark accuracy/F1 scores and qualitative output comparisons. Understanding what these metrics measure and their limitations is essential for interpreting the results.
  - Quick check question: What are the advantages and disadvantages of using benchmark scores versus qualitative output comparisons to evaluate domain-adapted LLMs?

## Architecture Onboarding

- **Component map**: Financial corpus construction -> Data cleaning and formatting -> Base model loading (rinna/nekomata-14b) -> Distributed training with DeepSpeed -> Evaluation on benchmarks -> Results analysis
- **Critical path**: Corpus construction and cleaning (most time-consuming) -> Base model loading and configuration -> Distributed training setup with DeepSpeed -> Evaluation pipeline implementation -> Results analysis and comparison
- **Design tradeoffs**: Data quality vs. quantity: The corpus is relatively small (370M tokens) but carefully curated; Training efficiency vs. thoroughness: 5 epochs chosen as a balance between adaptation and overfitting; Evaluation comprehensiveness vs. simplicity: Benchmark tasks provide quantitative measures, but qualitative analysis adds interpretive depth
- **Failure signatures**: Loss spikes during training (indicating numerical instability); Benchmark scores dropping below base model performance (indicating catastrophic forgetting); Generated outputs containing irrelevant or incorrect information (indicating poor domain adaptation); Training taking significantly longer than expected (indicating data or hardware bottlenecks)
- **First 3 experiments**: Train on a subset of the financial corpus (e.g., 10% of documents) for 1 epoch to verify the training pipeline works; Compare loss curves and validation metrics between the base model and the tuned model on a held-out validation set; Generate outputs for 10 sample prompts from the benchmarks and compare qualitatively before running full benchmark evaluation

## Open Questions the Paper Calls Out

- **Open Question 1**: How effective is domain-specific continual pre-training for 100-billion-class-parameter models compared to smaller models like the 14-billion-parameter model used in this study? The study only evaluated a 14-billion-parameter model, and the effectiveness for larger models like GPT-4 remains untested. Comparative experiments on both smaller and larger parameter models would resolve this.

- **Open Question 2**: What is the optimal dataset size and diversity for effective financial domain adaptation of Japanese LLMs? The study used a dataset with approximately 8.1 million documents and 370 million tokens, but the relationship between dataset size/diversity and model performance is not fully explored. Systematic experiments varying dataset characteristics would provide answers.

- **Open Question 3**: How does instruction tuning compare to continual pre-training for financial domain adaptation of Japanese LLMs? The study only employed continual pre-training and did not explore instruction tuning methods. Direct comparison experiments would determine which approach is more effective.

## Limitations

- The financial corpus size (370M tokens) is relatively small compared to typical pre-training datasets, raising questions about handling rare financial scenarios
- The evaluation relies on benchmark tasks and qualitative comparisons without systematic human evaluation of output quality across diverse financial queries
- The extent to which the model retains general Japanese language capabilities after domain adaptation remains unclear

## Confidence

- **High confidence**: The claim that continual pre-training improves financial benchmark performance is well-supported by quantitative results showing consistent improvements across all tested tasks
- **Medium confidence**: The assertion that the tuned model generates more informative and longer outputs is supported by qualitative examples, but lacks systematic measurement and statistical significance testing
- **Medium confidence**: The conclusion that this approach is effective for adapting LLMs to specialized domains is reasonable given the results, but would benefit from testing on additional domains and comparing with alternative adaptation methods

## Next Checks

1. **Quantitative Output Quality Assessment**: Develop a rubric to systematically score the quality, accuracy, and completeness of model outputs on financial queries, then statistically compare base vs. tuned model performance across a larger sample of prompts.

2. **Generalization Testing**: Evaluate the tuned model on non-financial Japanese language tasks to assess potential degradation in general language capabilities and quantify any catastrophic forgetting effects.

3. **Cross-Domain Transfer**: Apply the same continual pre-training approach to a different specialized domain (e.g., medical or legal) using the same base model to test whether the methodology generalizes beyond finance-specific applications.