---
ver: rpa2
title: 'SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning
  of Large Language Models'
arxiv_id: '2411.06171'
source_url: https://arxiv.org/abs/2411.06171
tags:
- attention
- learning
- seekr
- continual
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in continual learning
  of large language models (LLMs) by proposing a novel knowledge retention method
  called SEEKR. The key insight is that attention weights in LLMs play a crucial role
  in preserving knowledge across tasks.
---

# SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models

## Quick Facts
- arXiv ID: 2411.06171
- Source URL: https://arxiv.org/abs/2411.06171
- Reference count: 18
- Achieves comparable or better performance with only 1% replay data versus 10% used by other methods

## Executive Summary
This paper introduces SEEKR, a novel approach to continual learning for large language models that addresses catastrophic forgetting through selective attention distillation. The method identifies and distills attention from important heads using a two-dimensional measure combining forgettability and task sensitivity. SEEKR achieves superior performance compared to existing methods while being highly data-efficient, requiring only 1% replay data versus 10% used by other approaches. The approach maintains both task-specific performance and general language understanding abilities during sequential training on new tasks.

## Method Summary
SEEKR tackles catastrophic forgetting by selectively distilling attention weights from important heads identified through forgettability and task sensitivity measures. The method computes forgettability by tracking cumulative changes in attention weights across tasks, and task sensitivity by measuring gradients of task loss with respect to attention weights. A hierarchical budget allocation strategy selects important layers and heads for distillation, optimizing training cost while maintaining performance. The approach uses a memory buffer to store replay samples and distillation signals, enabling effective knowledge transfer from previous task versions to the current model.

## Key Results
- Outperforms existing continual learning methods on two benchmarks while using only 1% replay data
- Maintains both task-specific performance and general language understanding abilities
- Important attention heads are concentrated in middle and deep layers of the model
- Achieves comparable or better performance than methods using 10x more replay data

## Why This Works (Mechanism)

### Mechanism 1
Attention weights are critical for knowledge retention during continual learning of LLMs. By selectively distilling attention from important heads identified through forgettability and task sensitivity measures, the model preserves crucial knowledge while adapting to new tasks. Different attention heads exhibit varying levels of task sensitivity and forgettability, making selective distillation more effective than uniform approaches.

### Mechanism 2
Forgettability and task sensitivity measures effectively identify important attention heads for distillation. Forgettability (measured by cumulative changes in attention weights) identifies heads with general knowledge, while task sensitivity (measured by gradient magnitude) identifies heads crucial for specific tasks. Stable attention heads encode general knowledge less prone to forgetting, while sensitive heads are critical for task performance.

### Mechanism 3
Hierarchical budget allocation optimizes training cost while maintaining performance. Two-step selection (top-BL layers, then top-BH heads within those layers) combined with query budget BT reduces computational costs without sacrificing effectiveness. Important attention heads are concentrated in specific layers, allowing efficient selection.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses catastrophic forgetting in continual learning of LLMs
  - Quick check question: What happens to model performance on previous tasks when sequentially training on new tasks without any mitigation strategies?

- Concept: Knowledge distillation
  - Why needed here: SEEKR uses attention distillation to transfer knowledge from old models to the current model
  - Quick check question: How does knowledge distillation help in mitigating catastrophic forgetting?

- Concept: Attention mechanisms in transformers
  - Why needed here: The method relies on analyzing and distilling attention weights in LLM attention heads
  - Quick check question: What role do attention weights play in the transformer architecture?

## Architecture Onboarding

- Component map: Input -> Sequential task datasets -> Memory buffer -> Transformer LLM with attention heads -> SEEKR module -> Continually learned model

- Critical path: 1) Sequentially finetune on each task 2) Store replay samples and distillation signals 3) Compute forgettability and task sensitivity scores 4) Select important heads using hierarchical budget allocation 5) Perform attention distillation on selected heads and queries 6) Evaluate performance on old and new tasks

- Design tradeoffs: Replay ratio vs. performance (lower ratios save memory but may impact performance), head budget vs. effectiveness (fewer heads reduce computation but may miss important information), layer budget vs. cost (fewer layers save computation but may miss important heads in excluded layers)

- Failure signatures: Performance degradation on old tasks indicates insufficient knowledge retention, high computational cost suggests inefficient head selection, poor performance on new tasks indicates excessive focus on old knowledge

- First 3 experiments: 1) Ablation study comparing SEEKR with random head selection vs. importance-based selection 2) Budget sensitivity varying head and layer budgets to find optimal tradeoff 3) Replay efficiency comparing SEEKR performance with different replay ratios (1%, 5%, 10%)

## Open Questions the Paper Calls Out

### Open Question 1
How does SEEKR's performance scale when applied to multimodal large language models for continual learning tasks involving both text and images? The paper mentions that applying SEEKR to multimodal LLMs "remains to be explored in the future," indicating this is currently unknown. The paper's experiments focus solely on text-based LLMs, with no evaluation on multimodal models that could process images alongside text.

### Open Question 2
Does the effectiveness of SEEKR's attention head selection vary significantly across different model architectures (e.g., encoder-only, decoder-only, encoder-decoder) or is it consistently effective regardless of architecture? The paper validates SEEKR on two decoder-only transformer architectures but doesn't test other architectures. All experiments use the same decoder-only transformer architecture, leaving open whether the attention head importance measures generalize to other architectures like BERT or T5.

### Open Question 3
What is the relationship between the forgettability measure and actual catastrophic forgetting as measured by performance degradation on old tasks - do high-forgettability heads always correlate with task performance drops? The paper proposes forgettability as a measure but doesn't empirically validate whether it directly predicts forgetting behavior or just correlates with attention weight changes. The paper shows that heads with higher forgettability receive more distillation, but doesn't test whether these heads are actually the ones causing forgetting when not distilled.

## Limitations

- Core assumption that forgettability and task sensitivity measures accurately identify important attention heads needs rigorous validation
- Effectiveness of hierarchical budget allocation across layers, heads, and queries is assumed rather than empirically proven
- Limited discussion of potential negative impacts on general language understanding abilities during sequential task learning

## Confidence

**High Confidence:** The fundamental mechanism of using attention distillation for knowledge retention is well-established in the literature, and the paper's approach to selective distillation is theoretically sound.

**Medium Confidence:** The specific formulations of forgettability and task sensitivity metrics are reasonable but may have edge cases where they fail to identify truly important heads.

**Low Confidence:** The paper doesn't adequately address potential negative impacts on general language understanding abilities during sequential task learning.

## Next Checks

1. Cross-Architecture Validation: Test SEEKR on transformer architectures beyond the base model used in experiments (e.g., BERT, RoBERTa) to verify that the middle-to-deep layer concentration of important heads holds across architectures.

2. Stress Test with Adversarial Tasks: Design sequential task scenarios where tasks are deliberately chosen to interfere with each other, testing whether SEEKR can maintain performance when knowledge retention is most challenging.

3. Long-Term Stability Analysis: Extend evaluation beyond immediate task transitions to measure how knowledge retention degrades over extended training periods with many task sequences, particularly focusing on general language understanding capabilities.