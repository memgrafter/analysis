---
ver: rpa2
title: Generalizations across filler-gap dependencies in neural language models
arxiv_id: '2410.18225'
source_url: https://arxiv.org/abs/2410.18225
tags:
- filler
- island
- filler-gap
- effects
- dependencies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether neural language models (NLMs) can
  learn shared representations for filler-gap dependencies across different syntactic
  constructions. Researchers trained RNN models on controlled linguistic input and
  tested their ability to generalize knowledge of filler-gap dependencies (e.g., Wh-movement,
  clefting, tough-movement, topicalization) across constructions.
---

# Generalizations across filler-gap dependencies in neural language models

## Quick Facts
- arXiv ID: 2410.18225
- Source URL: https://arxiv.org/abs/2410.18225
- Reference count: 14
- Primary result: Neural language models fail to generalize shared representations for filler-gap dependencies across syntactic constructions, instead relying on construction-specific surface patterns

## Executive Summary
This study investigates whether neural language models can learn shared representations for filler-gap dependencies across different syntactic constructions. Researchers trained RNN models on controlled linguistic input and tested their ability to generalize knowledge of filler-gap dependencies (e.g., Wh-movement, clefting, tough-movement, topicalization) across constructions. The key finding was that NLMs relied on superficial properties of input rather than a shared structural generalization, showing success in individual constructions but failing to systematically apply constraints across them. This suggests that language-specific inductive biases may be necessary for modeling language acquisition, as NLMs cannot infer shared representations from finite input alone.

## Method Summary
The study used Gulordava et al.'s (2018) 2-layer LSTM RNN architecture (650 units each) trained on a Wikipedia corpus (90 million tokens). Researchers created controlled testing sets with 486 clefting items, 486 topicalization with intro, 161 topicalization without intro, and 243 tough-movement items. They computed surprisal values at critical regions and calculated filler effects (difference in surprisal between sentences with/without fillers). Linear mixed-effects regression models analyzed interaction terms following Wilcox et al.'s (2023) methodology. The study also augmented training data with 864 examples each of clefting and topicalization sentences and retrained models to test for generalization effects.

## Key Results
- NLMs showed construction-specific learning patterns, with no systematic generalization of filler-gap dependencies across different syntactic constructions
- Training on clefting improved island constraint knowledge only for clefting itself, with no transfer to other constructions
- Topic-RNN failed to learn simple topicalization dependencies despite direct evidence in training data
- Models tracked filler-gap co-occurrence statistics within each construction separately rather than abstracting shared structural representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural language models rely on surface-level distributional cues rather than abstract syntactic representations for filler-gap dependencies.
- Mechanism: When trained on controlled input, NLMs develop distinct statistical patterns for each construction type (clefting, topicalization, etc.) without generalizing across them. The models track filler-gap co-occurrence statistics within each construction separately.
- Core assumption: The model learns from local context and cannot abstract away from surface forms to create shared structural representations.
- Evidence anchors:
  - [abstract] "NLMs relied on superficial properties of input rather than a shared structural generalization"
  - [section] "Training on clefting had no significant effect on knowledge of the dependency in simple sentences of any construction"
  - [corpus] Weak evidence - corpus analysis shows construction-specific frequency patterns but no clear evidence of shared structural learning
- Break condition: If the model could generalize from one construction to another without explicit training on the target construction, this mechanism would fail.

### Mechanism 2
- Claim: Piecemeal learning through construction-specific frequency drives NLM performance on filler-gap dependencies.
- Mechanism: The model's ability to correctly identify grammatical filler-gap dependencies depends on the frequency of each construction type in training data. Higher frequency leads to better performance for that specific construction.
- Core assumption: Each construction is learned as a separate pattern rather than as instances of a shared dependency type.
- Evidence anchors:
  - [abstract] "piecemeal learning process: learning each construction separately"
  - [section] "Ozaki et al. (2022) argue the model's ability to approximate human behavior is dependent on the availability of each construction type in the input"
  - [corpus] Moderate evidence - corpus analysis shows correlation between construction frequency and model performance
- Break condition: If performance improved across constructions when training on one, this mechanism would fail.

### Mechanism 3
- Claim: Island constraints are learned through distributional patterns specific to each construction, not through a shared structural principle.
- Mechanism: The model learns which contexts block filler-gap dependencies based on the specific lexical and syntactic patterns it encounters, rather than through an abstract notion of "islandhood."
- Core assumption: The model cannot generalize the concept of structural constraints across different surface forms.
- Evidence anchors:
  - [abstract] "NLMs do have success differentiating grammatical from ungrammatical filler-gap dependencies, they rely on superficial properties"
  - [section] "Cleft-RNN's failure to capture islands in tough-movement relative to the pretrained RNN highlights... exposure to one construction type can cause a degradation in an NLM's knowledge of a different type"
  - [corpus] Weak evidence - corpus patterns show construction-specific island behaviors but don't clearly demonstrate shared structural learning
- Break condition: If the model showed consistent island effects across all constructions after training on one, this mechanism would fail.

## Foundational Learning

- Concept: Filler-gap dependency
  - Why needed here: The entire study tests whether models can learn this syntactic relationship across different constructions
  - Quick check question: Can you explain what a filler-gap dependency is and give an example from the text?

- Concept: Surprisal as a measure of grammaticality
  - Why needed here: The study uses surprisal at critical points to evaluate whether models recognize grammatical vs ungrammatical sentences
  - Quick check question: How does surprisal help distinguish between grammatical and ungrammatical filler-gap dependencies?

- Concept: Mixed-effects regression analysis
  - Why needed here: The statistical analysis tests for specific interactions between fillers, gaps, and islands
  - Quick check question: What does a negative filler-gap interaction term indicate about model behavior?

## Architecture Onboarding

- Component map: Wikipedia corpus (90M tokens) → 2-layer LSTM RNN (650 units each) → controlled testing sets → surprisal calculation → linear mixed-effects regression analysis
- Critical path: Input sentences → LSTM processing → next-word prediction → surprisal calculation at critical regions → statistical aggregation → interpretation of filler effects
- Design tradeoffs: Smaller, simpler RNN vs larger transformer models; controlled training data vs naturalistic input; focused syntactic tests vs broader language modeling
- Failure signatures: Lack of cross-construction generalization; construction-specific performance patterns; degradation when exposed to conflicting surface patterns
- First 3 experiments:
  1. Replicate baseline filler-gap dependency tests on pretrained RNN across all construction types
  2. Augment training data with clefting examples and test generalization to other constructions
  3. Augment training data with topicalization examples and test dependency learning in that construction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the amount of training data for a single construction improve the model's ability to generalize to other constructions with filler-gap dependencies?
- Basis in paper: [explicit] The paper states that Cleft-RNN's improvement in island constraints for clefting did not generalize to other constructions, and Topic-RNN failed to learn the simple topicalization dependency even with direct evidence.
- Why unresolved: The paper only tested with a fixed amount of additional training data (864 examples) for each construction. It's unclear if more examples would lead to better generalization.
- What evidence would resolve it: Training models with varying amounts of additional data for one construction and testing their performance on other constructions would show if more data leads to better generalization.

### Open Question 2
- Question: Do Transformer models, when trained on the same data as RNNs, show better generalization of filler-gap dependencies across constructions?
- Basis in paper: [explicit] The paper mentions that they tested a pretrained GPT-2 model and found similar qualitative patterns to the RNN, but they didn't train a Transformer model on the same data.
- Why unresolved: The paper only compared a pretrained GPT-2 to the RNN, not a Transformer trained on the same data. It's unclear if the architecture itself would lead to better generalization.
- What evidence would resolve it: Training a Transformer model on the same data as the RNN and comparing their performance on filler-gap dependencies would show if architecture affects generalization.

### Open Question 3
- Question: How does the presence of noisy or ungrammatical data in the training corpus affect the model's ability to learn and generalize filler-gap dependencies?
- Basis in paper: [inferred] The paper mentions that the Wikipedia corpus used for training is less noisy than the data a child is exposed to, and they speculate that more varied input might show similar or worse effects.
- Why unresolved: The paper only tested on clean, grammatical data. It's unclear how the model would perform with noisy or ungrammatical data, which is more representative of natural language.
- What evidence would resolve it: Training models on corpora with varying degrees of noise and ungrammaticality, and testing their performance on filler-gap dependencies, would show how robust the models are to different types of input.

## Limitations

- The controlled training environment may not reflect naturalistic language acquisition
- Surprisal as a proxy for grammaticality judgment has known limitations, particularly for island constraints
- The study focuses on specific construction types that may not represent the full complexity of human language learning

## Confidence

**High confidence**: The core finding that NLMs show construction-specific learning patterns rather than shared structural representations is well-supported by the experimental data and statistical analysis.

**Medium confidence**: The claim that this implies humans require language-specific inductive biases is an extrapolation from NLM behavior.

**Low confidence**: The specific mechanism of "superficial property reliance" is inferred rather than directly measured.

## Next Checks

1. Cross-linguistic validation: Test whether similar NLM patterns emerge with languages that have different filler-gap dependency structures (e.g., languages with scrambling or free word order) to determine if the findings generalize beyond English.

2. Intermediate training analysis: Track NLM performance during training rather than just pre/post augmentation to identify exactly when and how construction-specific patterns emerge.

3. Human comparison study: Directly compare NLM performance patterns with human learners on the same controlled input to determine if humans show similar construction-specific learning or if they indeed develop shared representations as the paper suggests.