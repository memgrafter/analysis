---
ver: rpa2
title: Partially Trained Graph Convolutional Networks Resist Oversmoothing
arxiv_id: '2410.13416'
source_url: https://arxiv.org/abs/2410.13416
tags:
- node
- layer
- trained
- graph
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates partially trained graph convolutional networks
  (GCNs), where only a single layer is trained while others remain frozen. The authors
  theoretically analyze the behavior of untrained weight matrices and their effect
  on node embeddings, showing that increasing model width helps reduce oversmoothing
  by decreasing correlation between node representations.
---

# Partially Trained Graph Convolutional Networks Resist Oversmoothing

## Quick Facts
- arXiv ID: 2410.13416
- Source URL: https://arxiv.org/abs/2410.13416
- Reference count: 39
- Primary result: Partially trained GCNs with only one trainable layer outperform fully trained models, especially in cold-start scenarios

## Executive Summary
This paper introduces partially trained Graph Convolutional Networks (GCNs) where only a single layer is trained while others remain frozen with Glorot initialization. The authors theoretically analyze how untrained weight matrices create decorrelation effects that resist oversmoothing, showing that increasing model width helps preserve discriminative information. Experiments demonstrate that deep partially trained GCNs outperform fully trained ones, particularly when unlabeled nodes lack features. The optimal trainable layer position is found to be the second layer, offering a new perspective on GCN training that could benefit scenarios with limited labeled data.

## Method Summary
Partially trained GCNs consist of a standard GCN architecture where only one layer receives gradient updates while the remaining layers maintain their initial Glorot weights. The model is trained for 200 epochs using Cross Entropy loss with a learning rate of 0.1 for partially trained models (compared to 0.001 for fully trained ones). The approach is tested across multiple benchmark datasets with varying widths (64-8192) and depths (2-32 layers), comparing against fully trained baselines and examining the cold-start scenario where unlabeled nodes lack features.

## Key Results
- Deep partially trained GCNs (8+ layers) significantly outperform fully trained models, with accuracy gains of up to 10% on cold-start datasets
- Model width has a strong positive impact on performance, with gains stabilizing around width 2048
- The second layer is consistently the optimal position for the trainable layer across all tested datasets
- Partially trained models show superior performance in cold-start scenarios where labeled and unlabeled nodes have different feature distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Untrained weight matrices act as decorrelation layers, reducing correlation between node embeddings
- Mechanism: Random weight matrices introduce Gaussian-distributed transformations that counteract smoothing from repeated adjacency matrix multiplications
- Core assumption: Product of untrained weight matrices maintains Gaussian distribution with variance scaling as 1/d
- Evidence anchors: [abstract] mentions network width influences embedding dissimilarity; [section] shows Lemma 7 proves Gaussian distribution with variance; corpus provides weak evidence on decorrelation
- Break condition: If untrained layers are too narrow, Gaussian approximation breaks down and decorrelation effect diminishes

### Mechanism 2
- Claim: Partially trained GCNs resist oversmoothing by maintaining large singular values in untrained weight matrices
- Mechanism: Untrained matrices keep largest singular values above 2 as width increases, preventing convergence to oversmoothing subspace
- Core assumption: Product of largest singular values across untrained layers remains large enough to prevent sλ < 1 condition
- Evidence anchors: [abstract] establishes connection between partially trained GCNs and oversmoothing reduction; [section] shows Corollary 4 proves singular values stay above 2; corpus provides moderate evidence from rank-based oversmoothing work
- Break condition: Excessive depth relative to width may still cause eventual oversmoothing despite large singular values

### Mechanism 3
- Claim: Trainable layer positioned early learns from aggregated but still discriminative representations
- Mechanism: Position 2 placement allows learning from information aggregated through one untrained layer but retaining diversity due to decorrelation effect
- Core assumption: Early trainable layer placement enables effective learning before excessive correlation develops
- Evidence anchors: [abstract] identifies second layer as optimal position; [section] shows Table 2 confirms position 2 is generally optimal; corpus provides weak evidence on trainable layer positioning
- Break condition: Trainable layer placed too late results in embeddings that are too correlated for effective learning

## Foundational Learning

- Concept: Graph Convolutional Networks and message passing
  - Why needed here: Entire paper builds on GCN mechanics and information flow through graph convolutions
  - Quick check question: What does the Â matrix represent in the GCN equation H(l+1) = σ(ÂH(l)W(l))?

- Concept: Singular value decomposition and matrix norms
  - Why needed here: Oversmoothing analysis relies heavily on singular value properties and their products across layers
  - Quick check question: How does the largest singular value of a weight matrix affect the rate of convergence to the oversmoothing subspace?

- Concept: Central Limit Theorem and random matrix theory
  - Why needed here: Theoretical analysis shows products of untrained weight matrices converge to Gaussian distributions, crucial for understanding decorrelation mechanism
  - Quick check question: What happens to the distribution of the average of n independent random variables as n approaches infinity?

## Architecture Onboarding

- Component map:
  Input features X (NxC) -> Untrained layers with random weights -> Trainable layer with learnable weights -> Untrained layers with random weights -> Output node embeddings

- Critical path:
  1. Node features pass through untrained layers (random transformations)
  2. Information aggregates through graph convolutions
  3. Trainable layer learns from aggregated but diverse representations
  4. Final untrained layers distribute learned information while maintaining decorrelation

- Design tradeoffs:
  - Width vs. depth: Wider untrained layers provide better decorrelation but increase computational cost
  - Trainable layer position: Earlier positions allow learning from more local information, later positions from more aggregated information
  - Number of trainable layers: Single trainable layer simplifies training but may limit representational capacity

- Failure signatures:
  - Accuracy plateaus at low values: Likely insufficient width or incorrect trainable layer position
  - High variance across runs: Training instability, may need learning rate adjustment
  - Performance worse than fully trained model: Width too small or trainable layer position suboptimal

- First 3 experiments:
  1. Test width impact: Run 16-layer model with trainable layer at position 2, varying width from 64 to 8192 on Cora dataset
  2. Test trainable layer position: Fix width at 512, test positions 1, 2, 4, 8, 16 on CiteSeer dataset
  3. Test cold-start scenario: Remove features from unlabeled nodes, compare partially trained vs fully trained models on Photo dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes large layer widths for Gaussian approximations, but practical regime may operate in middle ground
- Decorrelation mechanism relies on untrained layers maintaining random characteristics, but extended training might introduce subtle dependencies
- Cold-start performance gains were tested on single dataset variant without extensive ablation studies

## Confidence

- High confidence: Partially trained GCNs outperform fully trained models when depth exceeds 8 layers (supported by multiple experiments)
- Medium confidence: Second-layer placement is optimal across datasets (consistent pattern but limited dataset diversity)
- Medium confidence: Width has strong impact on oversmoothing resistance (theoretically sound but empirical validation limited to specific ranges)

## Next Checks
1. Test cold-start hypothesis across diverse graph types (heterophilic vs. homophilic) to verify generalizability beyond single dataset
2. Measure actual correlation decay across layers in partially trained vs fully trained models to empirically validate decorrelation mechanism
3. Evaluate whether partially trained approach transfers to other GNN architectures (GraphSAGE, GAT) or is specific to GCN message-passing