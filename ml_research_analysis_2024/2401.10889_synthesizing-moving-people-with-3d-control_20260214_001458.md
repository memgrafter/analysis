---
ver: rpa2
title: Synthesizing Moving People with 3D Control
arxiv_id: '2401.10889'
source_url: https://arxiv.org/abs/2401.10889
tags:
- human
- diffusion
- image
- texture
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents 3DHM, a diffusion-based framework for synthesizing
  moving people from a single image and target 3D motion sequence. The method addresses
  the challenge of generating realistic animations of arbitrary people by learning
  priors about invisible body parts and clothing.
---

# Synthesizing Moving People with 3D Control

## Quick Facts
- **arXiv ID**: 2401.10889
- **Source URL**: https://arxiv.org/abs/2401.10889
- **Reference count**: 40
- **Primary result**: 3DHM, a diffusion-based framework that synthesizes moving people from a single image and 3D motion sequence, achieving state-of-the-art performance in generation quality, video consistency, and pose accuracy.

## Executive Summary
This paper introduces 3DHM, a novel two-stage diffusion framework for synthesizing realistic animations of moving people from a single input image and target 3D motion sequence. The method learns to complete partial texture maps from single-view images and uses 3D pose control to generate temporally consistent videos. By disentangling texture map inpainting from rendering, 3DHM achieves superior performance compared to existing methods while maintaining visual fidelity to both the input image and target motion. The approach is fully self-supervised and scalable without requiring additional annotations beyond input images and videos.

## Method Summary
3DHM employs a two-stage diffusion framework to animate a target person (imitator) following an actor's motion. Stage 1 trains a diffusion inpainting model to complete the imitator's texture map from a single view image using synthetic 3D human videos. Stage 2 uses a ControlNet-like architecture with 3D pose control and appearance alignment to generate realistic renderings. The method leverages 4DHumans for 3D pose extraction and applies temporal attention layers for video consistency. Training occurs on ~2,205 synthetic 3D human videos and ~1,000 real human videos, requiring 2 weeks on 8 NVIDIA A100 GPUs.

## Key Results
- Achieves state-of-the-art performance on frame-wise metrics (PSNR↑, SSIM↑, FID↓, LPIPS↓) and video-level metrics (FID-VID↓, FVD↓)
- Demonstrates superior pose accuracy with lower MPVPE and PA-MPVPE compared to baseline methods
- Shows resilience in generating prolonged motions and varied challenging poses with strong temporal consistency
- Outperforms prior methods in visual quality while preserving clothing appearance and inpainting unseen regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Texture map inpainting learns priors about human appearance and clothing from partial views.
- Mechanism: The diffusion model is trained to hallucinate unseen regions of a texture map by leveraging the geometric consistency of 3D meshes and the visual coherence of clothing textures.
- Core assumption: Human clothing textures are geometrically consistent and can be inferred from visible portions of the texture map.
- Evidence anchors: [abstract] "We learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image." [section] "To get the complete texture map of the imitator from a single view image, we learn a diffusion model to in-fill the unseen regions of the texture map."

### Mechanism 2
- Claim: 3D pose control enables accurate motion transfer and temporal consistency.
- Mechanism: The diffusion model uses 3D human poses as control signals to generate realistic renderings of novel poses, ensuring that the generated images accurately reflect the target motion.
- Core assumption: 3D human poses provide a dense and accurate flow of motion that can be effectively used to control the diffusion model.
- Evidence anchors: [abstract] "This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions." [section] "Our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods."

### Mechanism 3
- Claim: Disentangled two-stage approach improves generation quality and consistency.
- Mechanism: The two-stage approach separates the tasks of texture map completion and realistic rendering, allowing each stage to focus on specific aspects of the generation process.
- Core assumption: Separating the tasks of texture map completion and realistic rendering leads to improved generation quality and consistency compared to a single-stage approach.
- Evidence anchors: [abstract] "This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity." [section] "We find out such a simple framework could successfully synthesize realistic and faithful human videos, particularly for long video generations."

## Foundational Learning

- **Concept**: Diffusion models
  - Why needed here: Diffusion models are used to learn the priors about human appearance and clothing, and to generate realistic renderings of novel poses.
  - Quick check question: What is the key idea behind diffusion models, and how do they differ from other generative models?

- **Concept**: 3D human pose estimation
  - Why needed here: 3D human pose estimation is used to extract accurate motion signals from videos, which are then used to control the diffusion model.
  - Quick check question: What are the challenges in 3D human pose estimation, and how do state-of-the-art methods address these challenges?

- **Concept**: Texture mapping
  - Why needed here: Texture mapping is used to represent the appearance of human clothing and skin, and to complete the texture map from partial views.
  - Quick check question: What are the different types of texture mapping techniques, and how do they differ in terms of their applications and limitations?

## Architecture Onboarding

- **Component map**: 4DHumans -> Stage 1 (Texture Map Inpainting) -> Stage 2 (Human Rendering with 3D Pose Control) -> ReferenceNet -> Temporal Model -> Output Video

- **Critical path**: 1) Extract 3D poses from input video using 4DHumans. 2) Generate intermediate renderings using Stage 1 texture map and 3D poses. 3) Generate realistic renderings using Stage 2 diffusion model with 3D pose control and appearance alignment. 4) Apply temporal consistency using the temporal model.

- **Design tradeoffs**:
  - Two-stage vs. single-stage approach: The two-stage approach allows for better separation of concerns but may introduce inconsistencies between stages.
  - 3D pose control vs. other control signals: 3D pose control provides accurate motion information but may be more computationally expensive than other control signals.
  - Self-supervised vs. supervised training: Self-supervised training allows for scalability but may lead to lower generation quality compared to supervised training.

- **Failure signatures**:
  - Inconsistent clothing textures: May indicate issues with the texture map inpainting stage.
  - Inaccurate motion transfer: May indicate issues with the 3D pose estimation or the rendering stage.
  - Jittering or flickering in generated videos: May indicate issues with the temporal consistency module.

- **First 3 experiments**:
  1. Ablation study: Remove the texture map inpainting stage and generate renderings directly from the input image.
  2. Control experiment: Replace the 3D pose control with 2D pose control and compare the generation quality.
  3. Scaling experiment: Increase the size of the training dataset and evaluate the impact on generation quality and consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 3DHM scale with increasing amounts of training data, particularly with more diverse human appearances and clothing types?
- Basis in paper: [inferred] The paper mentions that 3DHM is trained on a limited dataset (2,205 synthetic humans and 1,000 real humans) and suggests that training with more human videos could boost model generalization.
- Why unresolved: The paper does not provide experiments or analysis on the impact of training data scale on model performance.
- What evidence would resolve it: Experiments training 3DHM on progressively larger and more diverse datasets, measuring performance improvements in terms of generation quality, pose accuracy, and generalization to unseen humans.

### Open Question 2
- Question: How does 3DHM handle occlusions and partial visibility of the human body in the input image, and what are the limitations of the texture inpainting stage in these cases?
- Basis in paper: [explicit] The paper discusses the challenge of generating a complete texture map from a single view image, where only a part of the body is visible. It mentions that the inpainting diffusion model learns to hallucinate unseen parts of the person.
- Why unresolved: The paper does not provide detailed analysis or experiments on the quality of the inpainted texture map in cases of severe occlusions or complex poses.
- What evidence would resolve it: Qualitative and quantitative evaluation of the inpainted texture maps on images with varying degrees of occlusion, comparing the generated textures to ground truth when available.

### Open Question 3
- Question: Can 3DHM be extended to handle multiple people in a scene, and what are the challenges in scaling the method to multi-person scenarios?
- Basis in paper: [inferred] The paper focuses on animating a single person based on a single input image and target 3D motion sequence. It does not address the case of multiple people in the scene.
- Why unresolved: The paper does not discuss the feasibility or challenges of extending 3DHM to multi-person scenarios, such as handling interactions between people, occlusions, and maintaining consistency across multiple generated humans.
- What evidence would resolve it: Experiments applying 3DHM to images with multiple people, evaluating the quality of the generated animations and identifying the specific challenges encountered in multi-person scenarios.

## Limitations

- Generalization to diverse clothing styles remains uncertain due to limited discussion of performance on complex or culturally specific attire
- Computational requirements are substantial (8 A100 GPUs, 2 weeks training), limiting practical deployment potential
- Limited quantitative evidence for performance on extreme occlusions and highly unusual poses

## Confidence

- **High confidence**: Core mechanism of 3D pose control for motion transfer and two-stage diffusion framework is well-supported by experimental results
- **Medium confidence**: Texture map inpainting mechanism is plausible but relies on assumptions about clothing geometric consistency
- **Low confidence**: Claims about generalization to "arbitrary" people and handling all clothing types are not fully supported by experimental evidence

## Next Checks

1. **Cross-dataset generalization test**: Evaluate 3DHM on datasets with significantly different clothing styles and demographics than the training data (e.g., fashion photography datasets, cultural dance videos) to assess true generalization capabilities.

2. **Extreme pose and occlusion benchmark**: Create a benchmark with highly challenging poses and occlusion scenarios (multiple overlapping people, extreme limb positions) to test the method's robustness limits quantitatively.

3. **Computational efficiency analysis**: Measure inference time and resource usage on different hardware configurations, and analyze the trade-offs between quality and speed to assess practical deployment potential.