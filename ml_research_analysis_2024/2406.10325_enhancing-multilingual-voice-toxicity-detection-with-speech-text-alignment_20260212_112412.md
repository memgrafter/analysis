---
ver: rpa2
title: Enhancing Multilingual Voice Toxicity Detection with Speech-Text Alignment
arxiv_id: '2406.10325'
source_url: https://arxiv.org/abs/2406.10325
tags:
- speech
- toxicity
- text
- encoder
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cross-modal learning framework for multilingual
  voice toxicity detection that integrates semantic text embeddings into speech toxicity
  classifiers during training. The method uses text injection with contrastive loss
  to align speech and text representations, enabling improved performance without
  requiring text input during inference.
---

# Enhancing Multilingual Voice Toxicity Detection with Speech-Text Alignment

## Quick Facts
- arXiv ID: 2406.10325
- Source URL: https://arxiv.org/abs/2406.10325
- Reference count: 0
- Primary result: Text injection with contrastive loss improves multilingual voice toxicity detection performance across five languages

## Executive Summary
This paper introduces a cross-modal learning framework that integrates semantic text embeddings into speech toxicity classifiers during training. The method uses text injection with contrastive loss to align speech and text representations, enabling improved performance without requiring text input during inference. Experiments on large-scale real-world datasets across five languages (English, Spanish, French, German, Portuguese) show significant improvements over audio-only baselines, with relative gains up to 23% for certain toxicity categories like bullying in German and 11% overall.

## Method Summary
The framework uses a pre-trained speech encoder (Whisper variant) and frozen multilingual text encoder (E5 Large) with a learnable projection layer to align dimensions. Text embeddings are injected at the final layer of the speech encoder during training using contrastive loss. The model is trained with a combined loss of classification BCE loss and text injection loss, where α controls the weighting between them. During inference, only the speech encoder is used. The approach was evaluated on real-world datasets with human-labeled test sets across five languages and six toxicity categories.

## Key Results
- Significant performance improvements over audio-only baselines across all five languages tested
- Relative gains up to 23% for specific toxicity categories like bullying in German
- Contrastive loss outperforms MSE loss for text injection alignment
- Text injection at the final layer yields optimal performance compared to intermediate layers
- Best results achieved with α values between 0.1-0.3, suggesting lower weights on text injection loss improve performance

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal contrastive learning aligns speech and text embeddings in a shared semantic space for toxicity classification. During training, text embeddings are injected via a projection layer and aligned to speech embeddings using contrastive loss across the batch. This forces the speech encoder to produce representations semantically aligned with text, enabling toxicity detection without text at inference. The core assumption is that speech and text embeddings from general-purpose encoders already capture overlapping semantic features relevant to toxicity.

### Mechanism 2
Lower α values improve performance because the model can prioritize robust speech embeddings over joint speech-text loss. The combined loss weights classifier loss and text injection loss; setting α < 1 allows more focus on speech embedding quality while still leveraging text alignment. The core assumption is that the speech encoder already learns a representation close to text embeddings for toxicity tasks, so joint training benefits come from fine-tuning rather than replacing speech learning.

### Mechanism 3
Injecting text at the final encoder layer yields the best performance because this layer already contains the highest-level semantic representation. The final layer outputs capture global semantic content of the utterance, which aligns well with sentence-level text embeddings used in cross-modal tasks. The core assumption is that semantic features for toxicity are primarily encoded in the top layers of the speech encoder, not in intermediate or lower-level features.

## Foundational Learning

- Concept: Contrastive loss for cross-modal alignment
  - Why needed here: Enables learning a shared semantic space between speech and text without forcing direct numerical alignment (unlike MSE), improving domain fit for toxicity.
  - Quick check question: Why does contrastive loss outperform MSE in this framework?
    - Expected answer: It allows the model to learn its own optimal representation aligned to text semantics rather than copying text embeddings directly.

- Concept: Multi-label classification with BCE loss
  - Why needed here: Each audio segment can contain multiple toxicity types; BCE allows independent prediction per label.
  - Quick check question: What loss would be inappropriate if audio could have multiple toxicity labels simultaneously?
    - Expected answer: Categorical cross-entropy, because it assumes one label per sample.

- Concept: Bootstrap confidence intervals for AP
  - Why needed here: Provides statistically robust evaluation of average precision given variability in toxicity labeling and data splits.
  - Quick check question: How does bootstrapping improve metric reliability compared to a single train/test split?
    - Expected answer: It estimates metric variability across resampled datasets, yielding confidence bounds.

## Architecture Onboarding

- Component map: Audio input -> Speech encoder (Whisper) -> Projection layer -> Classification head; Text encoder (E5 Large) -> Projection layer -> Contrastive loss alignment with speech encoder
- Critical path: ASR transcript generation → text embedding extraction → speech embedding extraction → contrastive alignment → classification training → inference via speech encoder only
- Design tradeoffs: Using frozen text encoder vs joint fine-tuning (faster training, less adaptation); Contrastive loss vs MSE (better domain fit, more sensitive to batch size); Text injection at final layer vs intermediate (simpler, may miss low-level features)
- Failure signatures: Degraded performance on semantic-heavy categories suggests poor alignment; Performance drop in low-resource languages indicates dependence on multilingual text encoder coverage; High variance in AP across runs indicates instability
- First 3 experiments: 1) Switch α from 0.1 to 0.9 and observe impact on mAP and semantic category performance; 2) Move text injection from final to middle layer and measure performance change; 3) Replace contrastive loss with MSE and compare improvement magnitude

## Open Questions the Paper Calls Out

- Question: How do other cross-modal alignment methods (such as joint training of speech and text toxicity classifiers) compare to the text injection framework in terms of performance and scalability?
  - Basis in paper: The authors mention plans to investigate other cross-modal alignment methods, including joint training of speech and text toxicity classifiers.
  - Why unresolved: The paper only evaluates the text injection framework and does not compare it to other cross-modal alignment approaches.
  - What evidence would resolve it: Empirical results comparing the text injection framework to other cross-modal alignment methods on the same datasets and evaluation metrics.

- Question: What is the optimal architecture for the speech toxicity classifier when using text injection, and how does it compare to architectures used in other speech processing tasks?
  - Basis in paper: The authors use a pre-trained speech encoder and do not explore different architectures for the speech toxicity classifier.
  - Why unresolved: The paper does not investigate the impact of different speech encoder architectures on the performance of the text injection framework.
  - What evidence would resolve it: Experimental results comparing the performance of the text injection framework using different speech encoder architectures on the same datasets and evaluation metrics.

- Question: How does the text injection framework perform on low-resource languages with limited training data?
  - Basis in paper: The authors demonstrate the framework's effectiveness across five languages but do not specifically address low-resource language scenarios.
  - Why unresolved: The paper does not provide a detailed analysis of the framework's performance on languages with limited training data.
  - What evidence would resolve it: Experimental results evaluating the text injection framework on low-resource languages, comparing its performance to that on high-resource languages.

## Limitations

- Performance depends heavily on the quality of the underlying ASR system and semantic alignment between speech and text embeddings
- Framework requires careful hyperparameter tuning and sufficient batch diversity for stable contrastive loss training
- Frozen text encoders limit adaptation to speech-specific nuances and miss opportunities for joint optimization
- Evaluation focuses on average precision metrics that may not capture practical deployment impacts

## Confidence

- **High Confidence**: Core finding that text injection during training improves speech toxicity classification performance
- **Medium Confidence**: Claim that lower α values consistently improve performance across all languages and categories
- **Medium Confidence**: Assertion that text injection at the final layer is optimal for this task

## Next Checks

1. Test framework performance on speech domains significantly different from training data to assess robustness of cross-modal alignment
2. Replace frozen Multilingual E5 Large with jointly fine-tuned speech-text encoder to quantify performance gap
3. Systematically vary batch sizes during training to measure impact on contrastive loss stability and final performance