---
ver: rpa2
title: 'LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model
  Fine-Tuning'
arxiv_id: '2403.17919'
source_url: https://arxiv.org/abs/2403.17919
tags:
- lisa
- lora
- layers
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high memory consumption of fine-tuning
  large language models (LLMs), which typically requires at least 60 GB of GPU memory
  for a 7B model with full parameter training. The authors propose LISA (Layerwise
  Importance Sampled AdamW), a parameter-efficient fine-tuning method that randomly
  freezes most middle layers during optimization based on importance sampling.
---

# LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2403.17919
- Source URL: https://arxiv.org/abs/2403.17919
- Authors: Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
- Reference count: 34
- Primary result: LISA achieves 10%-35% better performance than LoRA on MT-Bench while maintaining similar or lower memory consumption

## Executive Summary
LISA addresses the high memory consumption of LLM fine-tuning by introducing layerwise importance sampling, which randomly freezes most middle layers during optimization. The method achieves performance competitive with or superior to LoRA while using similar or less memory. LISA works across different model sizes (7B-70B) and demonstrates strong performance on multiple benchmarks including MT-Bench, MMLU, AGIEval, and WinoGrande.

## Method Summary
LISA (Layerwise Importance Sampled AdamW) is a parameter-efficient fine-tuning method that randomly freezes most middle layers during optimization based on importance sampling. Unlike LoRA which confines parameter updates to low-rank subspaces, LISA samples important layers for full updates while freezing others, preserving the model's full representational capacity. The method initializes with all layers frozen except embedding and LM head, then samples γ intermediate layers to unfreeze based on importance distribution, running AdamW optimizer for K iterations before repeating the sampling process.

## Key Results
- LISA outperforms LoRA by 10%-35% on MT-Bench across multiple model sizes
- LISA achieves better or comparable performance on MMLU, AGIEval, and WinoGrande compared to LoRA
- On 70B models, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA
- Memory consumption similar to or lower than LoRA while outperforming Full-parameter Training in certain settings

## Why This Works (Mechanism)

### Mechanism 1
LISA exploits layerwise importance by freezing less important layers during training. The method randomly freezes most middle layers during optimization based on layerwise importance sampling, allowing critical layers to update while maintaining memory efficiency similar to LoRA. This works because different layers in LLMs have varying importance during fine-tuning, with embedding and language modeling head layers being most critical.

### Mechanism 2
LISA maintains representation power by using full parameter space while selectively updating. Unlike LoRA which confines parameter search to low-rank subspaces, LISA samples important layers for full updates while freezing others, preserving the model's full representational capacity. This is effective because full parameter updates are necessary for capturing complex patterns, but not all layers need simultaneous updates.

### Mechanism 3
LISA achieves implicit regularization through restricted layer updates. By limiting the number of simultaneously unfrozen layers, LISA creates an implicit dropout-like effect that prevents overfitting while maintaining performance. This works because restricting parameter updates can improve generalization by preventing co-adaptation of layers.

## Foundational Learning

- Concept: Importance sampling in optimization
  - Why needed here: LISA uses importance sampling to determine which layers to freeze/unfreeze during training
  - Quick check question: What is the key difference between uniform sampling and importance sampling in the context of layerwise optimization?

- Concept: Low-rank adaptation and its limitations
  - Why needed here: Understanding why LoRA works but has limitations helps explain LISA's design choices
  - Quick check question: How does LoRA's low-rank constraint limit its representation power compared to full parameter training?

- Concept: Memory-efficient training techniques
  - Why needed here: LISA must balance memory savings with performance, requiring understanding of GPU memory constraints
  - Quick check question: What are the main contributors to GPU memory consumption during LLM fine-tuning?

## Architecture Onboarding

- Component map: Layerwise importance sampler -> AdamW optimizer -> Memory manager -> Sampling controller
- Critical path: 1) Initialize model with all layers frozen except embedding and LM head 2) Sample γ intermediate layers to unfreeze based on importance distribution 3) Run AdamW optimizer for K iterations on unfrozen layers 4) Repeat sampling and optimization until training completes
- Design tradeoffs:
  - Memory vs. Performance: More unfrozen layers → better performance but higher memory usage
  - Sampling frequency vs. Stability: More frequent sampling → better adaptation but potential instability
  - Layer selection strategy vs. Simplicity: Complex importance metrics vs. simple uniform sampling
- Failure signatures:
  - Memory errors: Insufficient GPU memory for unfrozen layers
  - Training instability: Loss spikes when sampling changes layers
  - Performance degradation: Too many critical layers frozen simultaneously
- First 3 experiments:
  1. Baseline comparison: Run LISA vs LoRA on small model (GPT2-Small) with identical memory budget
  2. Sampling sensitivity: Test different γ values (2, 4, 8) on TinyLlama to find optimal balance
  3. Memory profiling: Measure actual GPU memory usage across different layer configurations to validate theoretical savings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for layer selection in LISA that goes beyond uniform random sampling of intermediate layers?
- Basis in paper: The paper mentions that the current strategy of randomly sampling intermediate layers may not be optimal and that considering data sources and model architecture could improve efficiency.
- Why unresolved: The paper does not explore alternative strategies for layer selection, such as prioritizing layers based on their importance or relevance to specific tasks.
- What evidence would resolve it: Experiments comparing LISA's performance using different layer selection strategies, including task-specific importance weighting or architectural analysis, would provide insights into optimal layer selection.

### Open Question 2
- Question: How does LISA's performance compare to LoRA when combined with quantization techniques like QLoRA?
- Basis in paper: The paper mentions that the major bottleneck of LISA is similar to LoRA, where forward pass still requires the model to be present in memory, and suggests that techniques like QLoRA could compensate for this limitation.
- Why unresolved: The paper does not conduct experiments to evaluate LISA's performance when combined with quantization techniques, leaving its effectiveness in this context unknown.
- What evidence would resolve it: Empirical comparisons of LISA and LoRA's performance and memory efficiency when combined with quantization techniques would clarify the benefits of using LISA in quantized settings.

### Open Question 3
- Question: What is the theoretical explanation for LISA's superior performance in memorization tasks compared to LoRA?
- Basis in paper: The paper observes that LISA outperforms LoRA in memorization-centered tasks like Writing or depicting image details, while the gap is smaller in reasoning-centered tasks.
- Why unresolved: The paper does not provide a theoretical explanation for this observation, leaving the underlying reasons for LISA's advantage in memorization tasks unexplored.
- What evidence would resolve it: Theoretical analysis or empirical studies investigating the relationship between layer-wise width, depth, and their impact on memorization versus reasoning tasks would shed light on LISA's performance characteristics.

## Limitations

- Lack of comprehensive ablation studies on the sampling mechanism's sensitivity to hyperparameters
- Insufficient detailed memory profiling across different GPU architectures and batch sizes
- Limited evaluation scope focused primarily on chat-oriented and general knowledge benchmarks

## Confidence

- High Confidence: The fundamental insight that different layers have varying importance during fine-tuning is well-supported by observed skewed weight-norm distributions
- Medium Confidence: Performance claims against LoRA are credible given theoretical advantages of full parameter space access
- Low Confidence: The claim that LISA "even outperforms Full-parameter Training under certain settings" lacks sufficient empirical support

## Next Checks

1. **Layer Importance Sensitivity Analysis**: Conduct systematic ablation studies varying the number of unfrozen layers (γ) and sampling frequencies across multiple tasks to determine optimal configurations and robustness boundaries.

2. **Memory Profiling Under Varying Conditions**: Measure actual GPU memory consumption across different batch sizes, sequence lengths, and GPU architectures to validate theoretical memory savings and identify implementation bottlenecks.

3. **Cross-Domain Generalization Test**: Evaluate LISA on specialized domains (scientific reasoning, code generation, multilingual tasks) beyond the general benchmarks to assess whether layerwise importance patterns generalize across diverse applications.