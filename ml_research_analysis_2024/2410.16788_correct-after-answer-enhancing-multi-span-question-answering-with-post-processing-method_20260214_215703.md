---
ver: rpa2
title: 'Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing
  Method'
arxiv_id: '2410.16788'
source_url: https://arxiv.org/abs/2410.16788
tags:
- predictions
- correct
- answer
- framework
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving performance in multi-span
  question answering (MSQA) by reducing incorrect predictions. The authors observe
  that stronger MSQA models don't necessarily predict fewer incorrect answers, indicating
  room for improvement through post-processing strategies.
---

# Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method

## Quick Facts
- arXiv ID: 2410.16788
- Source URL: https://arxiv.org/abs/2410.16788
- Authors: Jiayi Lin; Chenyang Zhang; Haibo Tong; Dongyu Zhang; Qingqing Hong; Bingxuan Hou; Junli Wang
- Reference count: 34
- Primary result: ACC framework improves Exact Match F1 scores across multiple MSQA datasets

## Executive Summary
This paper addresses the problem of incorrect predictions in multi-span question answering (MSQA) by proposing a post-processing framework called Answer-Classifying-Correcting (ACC). The authors observe that stronger MSQA models don't necessarily predict fewer incorrect answers, indicating room for improvement through post-processing strategies. The ACC framework employs a three-step approach: classifying predictions into correct, partially correct, and wrong categories, modifying partially correct predictions, and excluding wrong predictions. Experimental results show significant improvements across multiple MSQA datasets, with the framework effectively reducing incorrect predictions while maintaining correct ones.

## Method Summary
The paper proposes the Answer-Classifying-Correcting (ACC) framework as a post-processing strategy for MSQA models. The framework consists of two main components: a classifier and a corrector. The classifier categorizes predictions into correct, partially correct, and wrong categories, while the corrector modifies imperfect predictions. The method is trained using automatically constructed silver-labeled datasets through an annotation approach that identifies and labels prediction errors. The ACC framework is applied to baseline MSQA models' outputs, improving their performance by reducing incorrect predictions and enhancing partially correct ones. The approach shows promise when adapted for use with large language models through prompting strategies.

## Key Results
- ACC framework increased Exact Match F1 scores from 69.05% to 72.26% when applied to Tagger-RoBERTa
- For BART-base, improvements went from 65.57% to 67.31% on MultiSpanQA dataset
- The framework effectively reduces incorrect predictions while maintaining correct ones, as demonstrated through statistical analysis of prediction distributions
- ACC shows consistent improvements across multiple MSQA datasets including MultiSpanQA, MultiSpanQA-Expand, MAMRC, and MAMRC-Multi

## Why This Works (Mechanism)
The ACC framework works by addressing a key observation that stronger MSQA models don't necessarily predict fewer incorrect answers. By implementing a post-processing approach that systematically classifies, modifies, and excludes predictions, the framework targets the specific problem of incorrect answers. The classifier component identifies which predictions need correction, while the corrector component learns to improve partially correct predictions. This targeted approach allows for more precise error handling compared to model training alone, effectively reducing the noise in final predictions without sacrificing correct answers.

## Foundational Learning
- **Multi-span question answering**: Task requiring extraction of multiple answer spans from context text - needed to understand the specific challenge ACC addresses; quick check: verify dataset contains questions with multiple gold answer spans
- **Post-processing strategies**: Methods applied after initial model predictions to improve output quality - needed to grasp ACC's approach as a correction mechanism; quick check: identify what transformations occur between raw predictions and final outputs
- **Silver-labeled datasets**: Automatically constructed training data with predicted labels - needed to understand how ACC components are trained; quick check: examine the automatic annotation approach used to create training data
- **Exact Match metrics**: Evaluation metrics measuring precise match between predictions and gold answers - needed to assess ACC's performance improvements; quick check: calculate EM scores on a sample dataset
- **BERTScore evaluation**: Metric using BERT embeddings to measure semantic similarity - needed to complement exact match with semantic quality assessment; quick check: compare EM and BERTScore results for same predictions

## Architecture Onboarding

**Component Map:**
Classifier -> Corrector -> Final Output

**Critical Path:**
Input predictions → Classification (correct/partially correct/wrong) → Modification of partially correct predictions → Exclusion of wrong predictions → Final output

**Design Tradeoffs:**
- Pointer-based vs generative models for correction: Pointer models offer precise span modification but limited expressiveness, while generative models provide flexibility but may introduce errors
- Classification granularity: Three-way classification balances complexity with effectiveness, though more granular categories could provide finer control
- Automatic annotation quality: Silver-labeled datasets enable training but may contain noise, requiring careful quality control

**Failure Signatures:**
- Precision gains without recall improvement indicates over-aggressive exclusion of predictions
- Degradation in Word Overlap scores suggests corrector is modifying correct spans incorrectly
- Inconsistent improvements across datasets indicate sensitivity to domain or question types

**First Experiments:**
1. Evaluate classification accuracy breakdown across prediction types (correct, partially correct, wrong)
2. Compare performance of pointer-based vs generative corrector models
3. Test ACC framework on out-of-domain questions to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks detailed ablation studies to isolate the contribution of each ACC component
- The automatic annotation approach for constructing silver-labeled datasets lacks full specification, making exact replication challenging
- The framework's performance on out-of-domain questions is not evaluated, limiting understanding of generalization capabilities

## Confidence
- **High confidence**: The core observation that stronger MSQA models don't necessarily predict fewer incorrect answers is well-supported by the data
- **Medium confidence**: The effectiveness of the ACC framework is demonstrated through multiple experiments, though the improvement magnitudes vary across datasets
- **Medium confidence**: The claim that ACC reduces incorrect predictions while maintaining correct ones is supported by distribution analysis, but the analysis could be more comprehensive

## Next Checks
1. Conduct ablation studies to measure the individual impact of the classifier and corrector components on overall performance
2. Test the ACC framework's robustness on questions requiring reasoning beyond direct text matching to evaluate generalization
3. Implement the automatic annotation approach independently to verify reproducibility and assess whether the silver-labeled datasets consistently produce similar improvements