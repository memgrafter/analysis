---
ver: rpa2
title: 'Exploring speech style spaces with language models: Emotional TTS without
  emotion labels'
arxiv_id: '2405.11413'
source_url: https://arxiv.org/abs/2405.11413
tags:
- emotional
- speech
- emotion
- style
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TEMOTTS, a text-aware emotional text-to-speech
  (E-TTS) framework that eliminates the need for emotion labels during training and
  auxiliary inputs during inference. The core idea is to transfer knowledge between
  the linguistic space learned by BERT and the emotional style space constructed by
  global style tokens (GST).
---

# Exploring speech style spaces with language models: Emotional TTS without emotion labels

## Quick Facts
- arXiv ID: 2405.11413
- Source URL: https://arxiv.org/abs/2405.11413
- Reference count: 0
- Primary result: Two-stage TEMOTTS framework achieves MOS of 3.75 and 80.95% best vote rate in emotional TTS without emotion labels

## Executive Summary
TEMOTTS presents a novel text-aware emotional text-to-speech framework that eliminates the need for emotion labels during training and auxiliary inputs during inference. The framework leverages knowledge transfer between linguistic space learned by DistilRoBERTa and emotional style space constructed by global style tokens (GST). Through a two-stage training approach, TEMOTTS first constructs an emotional style space from speech data, then maps emotional text embeddings to corresponding style representations. The system achieves significant improvements in emotional accuracy and naturalness compared to baseline models.

## Method Summary
TEMOTTS operates through a two-stage training strategy. Stage I trains a FastSpeech2 backbone with GST to learn unsupervised emotional style representations from speech data. Stage II fine-tunes DistilRoBERTa to predict emotion class probabilities, then uses an adaptation module to map emotional text embeddings to GST weight distributions. During inference, the system takes only text input, generates emotional embeddings through DistilRoBERTa, converts these to style tokens via the adaptation module, and synthesizes speech with appropriate emotional prosody through the FastSpeech2 decoder.

## Key Results
- Achieved MOS of 3.75 on naturalness evaluation
- Obtained 80.95% best vote rate in Best-Worst Scaling tests
- Outperformed baseline models in both emotional accuracy and naturalness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GST weight prediction replaces emotion labels by encoding emotional prosody into style tokens
- Mechanism: Stage I trains FastSpeech2 with GST to learn unsupervised style representations from emotional speech; Stage II maps DistilRoBERTa embeddings to these GST weights via an adaptation module
- Core assumption: Emotional speaking styles are statistically correlated with emotional content in text
- Evidence anchors:
  - "Our proposed method performs knowledge transfer between the linguistic space learned by BERT and the emotional style space constructed by global style tokens"
  - t-SNE visualization shows emotional style space created by GST weight distribution alongside emotional text embeddings
- Break condition: If emotional content in text does not correlate with emotional prosody in speech, the adaptation module will fail to learn a mapping

### Mechanism 2
- Claim: Emotion data pruning selects utterances with strong emotional content for Stage II
- Mechanism: Fine-tuned DistilRoBERTa predicts emotion class probabilities; only utterances where dominant emotion probability exceeds threshold Pth are used for adaptation module training
- Core assumption: High-confidence emotion predictions indicate utterances where emotional prosody and content are well-aligned
- Evidence anchors:
  - "If the probability of the dominant emotion class exceeds an experimentally chosen threshold Pth, we include the example in the training dataset for Stage II"
- Break condition: If Pth is too high, too few examples are available; if too low, noise in emotion alignment corrupts GST mapping

### Mechanism 3
- Claim: Two-stage training decouples style space construction from style-to-text mapping
- Mechanism: Stage I learns emotional style space from unlabeled speech; Stage II learns mapping from emotional text embeddings to style tokens; inference uses only text input
- Core assumption: Prosody and linguistic emotion can be modeled independently
- Evidence anchors:
  - "Our proposed two-stage training strategy is inspired by these methods, which have proven to be effective in addressing prosody-related challenges in E-TTS"
- Break condition: If the style space from Stage I is not emotionally discriminative, Stage II mapping cannot recover emotion from text

## Foundational Learning

- Concept: Emotional prosody modeling
  - Why needed here: TTS must generate pitch, duration, and energy variations that convey emotions; without explicit labels, the model must learn these implicitly
  - Quick check question: What low-level prosody features must change to convey "sad" versus "happy" speech?

- Concept: Language model emotion understanding
  - Why needed here: DistilRoBERTa predicts emotion class probabilities from text, enabling selection of high-emotion utterances and generation of emotional embeddings for inference
  - Quick check question: How does mean pooling over the last BERT layer produce a fixed-size emotional embedding?

- Concept: Global Style Tokens (GST)
  - Why needed here: GST constructs a learned style space without labels; style tokens are weighted by a reference encoder to condition TTS on emotional speaking styles
  - Quick check question: How does the reference encoder produce a weight distribution over style tokens from speech?

## Architecture Onboarding

- Component map:
  Text → DistilRoBERTa → Adaptation module → Style embedding → FastSpeech2 decoder → Vocoder → Waveform

- Critical path: Text → DistilRoBERTa → Adaptation module → Style embedding → FastSpeech2 decoder → Vocoder → Waveform

- Design tradeoffs:
  - Using DistilRoBERTa vs. full BERT: smaller model, faster inference, but potentially less nuanced emotion understanding
  - Ntokens=16 vs. higher/lower: too few tokens constrain style space; too many hurt emotion expressiveness
  - Pth=0.7 vs. other values: balances data quantity and emotion alignment quality

- Failure signatures:
  - Poor MOS or high WER → adaptation module not mapping text to appropriate style tokens
  - Confusion matrix shows random emotion synthesis → GST space not emotionally discriminative or adaptation mapping is wrong
  - Model produces flat prosody → variance adaptors not being properly conditioned by style embedding

- First 3 experiments:
  1. Train Stage I only with GST on emotional dataset; verify style space is emotionally clustered via t-SNE
  2. Train adaptation module on emotion-pruned subset; measure classification accuracy of predicted GST weights
  3. End-to-end inference test: generate speech from high-emotion sentences; evaluate with SER to confirm correct emotion synthesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of style tokens (Ntokens) for balancing emotional expressiveness and model capacity in TEMOTTS?
- Basis in paper: The authors experimented with Ntokens values of 8, 16, 32, and 64, finding that Ntokens = 16 provided the best results.
- Why unresolved: The paper only tested a limited range of Ntokens values and did not explore the full spectrum or provide a systematic method for determining the optimal number.
- What evidence would resolve it: A comprehensive study testing a wider range of Ntokens values and analyzing their impact on emotional expressiveness and model performance.

### Open Question 2
- Question: How does TEMOTTS perform on datasets with limited emotional variation or highly expressive emotional content?
- Basis in paper: The authors acknowledge that the model is designed to leverage any conversational speech dataset with sufficient emotional variation, but do not evaluate its performance on datasets with limited or highly expressive emotional content.
- Why unresolved: The paper only tests the model on a specific emotional TTS dataset and does not explore its generalization capabilities on different types of emotional speech data.
- What evidence would resolve it: Experiments on datasets with varying levels of emotional variation and expressiveness to assess the model's performance and generalization capabilities.

### Open Question 3
- Question: Can TEMOTTS effectively handle multiple speakers with different emotional speaking styles?
- Basis in paper: The authors do not discuss the model's ability to handle multiple speakers or different emotional speaking styles in their experiments.
- Why unresolved: The paper only uses a single speaker dataset and does not evaluate the model's performance on multi-speaker scenarios.
- What evidence would resolve it: Experiments on multi-speaker datasets with varying emotional speaking styles to assess the model's ability to adapt to different speakers and emotional expressions.

## Limitations

- The emotion data pruning threshold (Pth=0.7) and style token configuration (Ntokens=16) were determined empirically rather than through principled analysis
- The framework assumes emotional content in text statistically correlates with emotional prosody in speech, which may not hold across languages or cultural contexts
- Reliance on DistilRoBERTa for emotion understanding introduces potential brittleness if the language model's emotional understanding differs from human perception

## Confidence

- **High Confidence**: The two-stage training architecture and use of GST for unsupervised style space construction are well-established techniques with solid theoretical foundations
- **Medium Confidence**: The emotion data pruning strategy is empirically justified but lacks theoretical grounding; the mapping from text embeddings to GST weights is novel but not extensively validated
- **Medium Confidence**: MOS and BWS results are positive but based on a single dataset (EMO-DB); generalization to other emotional speech corpora remains unproven

## Next Checks

1. **Cross-Corpus Validation**: Evaluate TEMOTTS on multiple emotional speech datasets (e.g., IEMOCAP, RAVDESS) to verify that the text-to-style mapping generalizes beyond EMO-DB, testing both German and English emotional speech.

2. **Ablation Study**: Systematically vary Pth (0.5, 0.6, 0.7, 0.8, 0.9) and Ntokens (8, 12, 16, 20, 24) to quantify their impact on emotional accuracy and naturalness, identifying optimal configurations for different emotional intensities.

3. **Human Perception Study**: Conduct a detailed listener study where participants rate not only emotion correctness but also naturalness and speaker similarity across different emotion intensities, to understand whether the framework can produce subtle emotional variations or only coarse-grained categories.