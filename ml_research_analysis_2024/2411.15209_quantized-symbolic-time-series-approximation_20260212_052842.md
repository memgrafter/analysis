---
ver: rpa2
title: Quantized symbolic time series approximation
arxiv_id: '2411.15209'
source_url: https://arxiv.org/abs/2411.15209
tags:
- time
- qabba
- series
- symbolic
- abba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QABBA, a quantization-based variant of the
  ABBA symbolic time series representation method. QABBA improves storage efficiency
  by replacing floating-point symbolic centers with low-bit-width integer representations
  while maintaining the accuracy and speed of the original ABBA approach.
---

# Quantized symbolic time series approximation

## Quick Facts
- arXiv ID: 2411.15209
- Source URL: https://arxiv.org/abs/2411.15209
- Authors: Erin Carson; Xinye Chen; Cheng Kang
- Reference count: 40
- Key outcome: QABBA achieves significant storage savings through quantization while maintaining accuracy and enabling state-of-the-art LLM regression performance

## Executive Summary
This paper introduces QABBA, a quantization-based variant of the ABBA symbolic time series representation method. QABBA improves storage efficiency by replacing floating-point symbolic centers with low-bit-width integer representations while maintaining the accuracy and speed of the original ABBA approach. The authors prove an upper bound for the error introduced by quantization and discuss how to choose the number of bits to balance this error with other sources. They also present an application of QABBA combined with large language models (LLMs) for time series regression, achieving state-of-the-art results on the Monash regression dataset.

## Method Summary
QABBA builds on ABBA's symbolic time series representation by adding quantization to the symbolic centers. The method first applies ABBA's compression phase to create (length, increment) tuples, then uses vector quantization to cluster these tuples into symbolic centers. QABBA then quantizes these symbolic centers using symmetric quantization with 8-bit width for lengths and 12-bit width for increments. The quantized representation is stored and later inverse-quantized for reconstruction. The authors prove an upper bound on quantization error and apply the method to both univariate and multivariate time series, including an LLM-based regression application.

## Key Results
- QABBA achieves significant storage savings (up to 75%) compared to non-quantized methods while maintaining reconstruction accuracy
- The method provides state-of-the-art results on the Monash regression dataset when combined with LLMs
- Extensive experiments show QABBA's advantages across various datasets with different characteristics
- The quantization error is bounded and controllable through bit-width selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QABBA achieves significant storage savings by quantizing symbolic centers while preserving approximation accuracy.
- Mechanism: QABBA replaces the floating-point symbolic centers (C) from ABBA with low-bit-width integer representations (eC) using symmetric quantization. This reduces storage from 32 bits per value to as low as 8 or 12 bits while maintaining reconstruction quality.
- Core assumption: The quantization error introduced is dominated by the quantization step itself, not floating-point rounding errors.
- Evidence anchors:
  - [abstract]: "improves storage efficiency by replacing floating-point symbolic centers with low-bit-width integer representations"
  - [section]: "Our method is motivated by techniques used for model quantization in deep neural network training"
  - [corpus]: Weak - corpus papers discuss symbolic representation but not quantization-specific mechanisms
- Break condition: When quantization bit-width is too low (e.g., below 8 bits for lengths, below 12 bits for increments) the reconstruction error becomes unacceptable.

### Mechanism 2
- Claim: The quantization error can be bounded and controlled through bit-width selection.
- Mechanism: QABBA provides an upper bound on quantization error using the Frobenius norm, showing the relationship between quantization error and bit-width choice. The error scales as 1/(2^ω-2), allowing predictable control over accuracy.
- Core assumption: The symbolic centers' values fall within a known range, enabling proper quantization scaling.
- Evidence anchors:
  - [abstract]: "We prove an upper bound for the error introduced by quantization"
  - [section]: "Accordingly, given C = [c1, . . . , ck] ∈ R2×k, the error from the quantization in terms of the Frobenius norm is..."
  - [corpus]: Weak - corpus lacks specific error bounding for quantization in symbolic time series
- Break condition: When the distribution of symbolic center values exceeds assumed bounds, causing clipping and unbounded errors.

### Mechanism 3
- Claim: QABBA enables efficient fine-tuning of LLMs for time series regression by providing meaningful symbolic pattern chains.
- Mechanism: By converting time series to symbolic pattern chains, QABBA allows LLMs to leverage their sequence modeling capabilities without requiring separate embedding training, achieving state-of-the-art results on regression tasks.
- Core assumption: LLMs can effectively process symbolic representations of time series data for regression tasks.
- Evidence anchors:
  - [abstract]: "achieves state-of-the-art results on the Monash regression dataset" and "enables efficient fine-tuning of LLMs"
  - [section]: "QABBA not only avoids the training of embedding from scratch, but also achieves a new state-of-the-art on Monash regression dataset"
  - [corpus]: Moderate - related corpus includes papers on symbolic regression and LLM applications to time series
- Break condition: When the symbolic representation loses too much temporal information for the specific regression task.

## Foundational Learning

- Concept: Symbolic time series representation (SAX, ABBA, 1d-SAX)
  - Why needed here: QABBA builds directly on ABBA's symbolic approximation framework, so understanding how symbolic representation works is essential
  - Quick check question: How does ABBA's compression phase reduce dimensionality while maintaining reconstruction quality?

- Concept: Vector quantization and k-means clustering
  - Why needed here: The digitization phase in ABBA (and QABBA) uses vector quantization to cluster normalized (length, increment) pairs into symbolic centers
  - Quick check question: What is the relationship between the number of clusters k and the storage efficiency of the symbolic representation?

- Concept: Quantization in neural networks
  - Why needed here: QABBA applies similar quantization principles to symbolic time series, so understanding quantization mechanics and error bounds is crucial
  - Quick check question: How does symmetric quantization differ from asymmetric quantization in terms of zero-point handling and error characteristics?

## Architecture Onboarding

- Component map: T → Compression → Digitization → Quantization → Storage → Inverse-quantization → Reconstruction → bT
- Critical path: Time series undergoes compression to (len, inc) tuples, digitization through clustering, quantization to low-bit-width integers, storage, inverse-quantization, and reconstruction
- Design tradeoffs:
  - Bit-width selection: Lower bits save storage but increase quantization error
  - Clustering method: VQ provides better SSE but slower than GA
  - Tolerance parameter: Controls compression ratio vs. reconstruction error
  - Weighting (scl): Balances importance of length vs. increment in clustering
- Failure signatures:
  - Excessive reconstruction error: Bit-widths too low, tolerance too high, or inappropriate scl parameter
  - Clustering instability: k too large or too small relative to data characteristics
  - LLM regression poor performance: Symbolic representation losing task-relevant information
- First 3 experiments:
  1. Test quantization error bounds: Apply QABBA to synthetic data with known distribution, measure reconstruction error vs. bit-width to validate theoretical bounds
  2. Storage efficiency validation: Compare storage requirements of QABBA vs. ABBA on real-world datasets, measure compression ratios
  3. LLM regression capability: Fine-tune Mistral-7B on QABBA-transformed time series regression task, compare performance to baseline methods on Monash dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal bit-width allocation between length and increment values for QABBA across different types of time series data?
- Basis in paper: [explicit] The paper states "The safety margin of the quantization bits for len quantization in this dataset is in [8, 32) whereas for inc quantization it is [12, 32)" but notes that "in practice, one will have to tune these parameters to a specific dataset"
- Why unresolved: The paper only tests one dataset and suggests different optimal ranges for length vs increment values, but doesn't provide a systematic method for determining optimal bit-width allocation for different time series characteristics
- What evidence would resolve it: A comprehensive study testing various bit-width combinations across diverse time series datasets with different characteristics (trend, seasonality, noise levels) to establish guidelines or heuristics for optimal allocation

### Open Question 2
- Question: How does QABBA's symbolic representation compare to other compression methods like LZW for time series data?
- Basis in paper: [explicit] The paper states "we aims to preserve the time series structure and patterns under a further compression, and a further compression can be further obtained from LZW compression"
- Why unresolved: The paper focuses on comparing QABBA to ABBA/fABBA but doesn't benchmark against other compression algorithms like LZW that could be applied to the symbolic representation
- What evidence would resolve it: Direct comparison of QABBA+LZW vs standalone LZW vs other compression methods on the same time series datasets measuring compression ratio and reconstruction accuracy

### Open Question 3
- Question: What is the theoretical relationship between quantization error and symbolic approximation error in QABBA?
- Basis in paper: [explicit] The paper derives "an upper bound for the error arising from quantization" and discusses "how the number of bits should be chosen to balance this with other errors"
- Why unresolved: While the paper provides theoretical bounds for quantization error, it doesn't fully explore the interaction between quantization error and the original symbolic approximation error from ABBA
- What evidence would resolve it: A formal mathematical framework showing how quantization error compounds with symbolic approximation error, potentially leading to guidelines for error budgeting across both sources

## Limitations

- The optimal bit-width allocation for different time series characteristics is not systematically determined
- The LLM regression results are based on a single dataset without comprehensive ablation studies
- The multivariate extension relies on joint symbolic representation from an external reference not fully detailed in the paper

## Confidence

**High Confidence**: The core quantization mechanism and its basic implementation are well-defined and theoretically grounded. The storage efficiency improvements through bit-width reduction are straightforward to verify.

**Medium Confidence**: The error bounding claims are mathematically sound but their practical applicability across diverse datasets needs more validation. The storage vs. accuracy tradeoff appears reasonable but may vary significantly with data characteristics.

**Low Confidence**: The LLM application's state-of-the-art claims are based on a single dataset comparison. Without broader benchmarking across multiple regression tasks and comparison to specialized time series models, these claims remain tentative.

## Next Checks

1. **Error Bound Validation**: Apply QABBA to synthetic time series with controlled distributions (Gaussian, uniform, heavy-tailed) and systematically measure reconstruction error across varying bit-widths to verify the theoretical error bounds hold in practice and identify breaking points.

2. **Cross-Dataset LLM Regression**: Test the LLM regression approach beyond Monash on diverse regression datasets (both time series and non-time series converted to symbolic representation) to determine if the performance gains generalize or are dataset-specific.

3. **Parameter Sensitivity Analysis**: Conduct systematic ablation studies varying k (number of clusters), scl (length/increment weighting), and bit-width combinations to identify optimal configurations and understand the robustness of QABBA across different time series characteristics.