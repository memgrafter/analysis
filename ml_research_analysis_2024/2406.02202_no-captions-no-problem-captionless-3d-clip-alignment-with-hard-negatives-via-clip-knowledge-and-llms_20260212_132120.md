---
ver: rpa2
title: 'No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard Negatives
  via CLIP Knowledge and LLMs'
arxiv_id: '2406.02202'
source_url: https://arxiv.org/abs/2406.02202
tags:
- clip
- similarity
- learning
- hard
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to 3D-CLIP alignment without
  relying on textual descriptions of 3D objects. The authors propose two unsupervised
  methods, I2I and (I2L)2, which leverage CLIP's knowledge of textual and 2D data
  to compute similarities between 3D samples.
---

# No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard Negatives via CLIP Knowledge and LLMs

## Quick Facts
- arXiv ID: 2406.02202
- Source URL: https://arxiv.org/abs/2406.02202
- Authors: Cristian Sbrolli; Matteo Matteucci
- Reference count: 31
- Key outcome: Proposed captionless 3D-CLIP alignment methods achieve comparable 3D classification and significantly improved cross-modal retrieval without explicit text supervision

## Executive Summary
This paper introduces a novel approach to 3D-CLIP alignment that eliminates the need for textual descriptions of 3D objects. The authors propose two unsupervised methods, I2I and (I2L)2, which leverage CLIP's knowledge of textual and 2D data to compute similarities between 3D samples. These similarities are then used to mine hard negatives, enhancing the contrastive training process. The method is evaluated on various benchmarks, including 3D classification and cross-modal retrieval tasks, demonstrating competitive performance without explicit text alignment.

## Method Summary
The method computes 3D-to-3D similarities using CLIP embeddings from rendered views of 3D objects. Two approaches are proposed: I2I uses image-to-image similarity across views, while (I2L)2 uses image-to-landmark text similarity with LLM-generated category descriptions. These precomputed similarities weight contrastive losses during training, emphasizing harder negative pairs. The approach trains a point cloud encoder using hard negative weighted InfoNCE loss, with CLIP encoders frozen throughout training.

## Key Results
- Achieved 87.8% accuracy on ModelNet40 zero-shot 3D classification, comparable to supervised methods
- Improved cross-modal retrieval performance, with shape-to-image mAP of 0.421 on Pix3D
- Demonstrated that hard negative mining via precomputed 3D similarities enhances multimodal alignment

## Why This Works (Mechanism)

### Mechanism 1
CLIP's latent space can be repurposed to measure 3D object similarity without explicit text supervision. The method leverages CLIP's pretrained image encoder to generate view-level embeddings for each 3D object. By averaging cosine similarities across corresponding views between two 3D objects, it computes a perceptual similarity score that correlates with semantic similarity.

### Mechanism 2
Using LLM-generated category-specific text prompts as "landmarks" enables fine-grained structural comparison without color/texture bias. Instead of captioning individual views, the method generates a small set of detailed category descriptions focusing on structural features. Each 3D view is compared against these landmarks using CLIP's text encoder, creating a descriptor that captures structural similarity while ignoring visual attributes.

### Mechanism 3
Hard negative mining via precomputed 3D similarities improves multimodal alignment by forcing the model to learn finer-grained distinctions. Precomputed similarity scores between all 3D samples are used to weight contrastive losses, increasing the gradient contribution from harder negative pairs. This focuses learning on distinguishing structurally similar objects rather than easy, obviously different ones.

## Foundational Learning

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: The entire training pipeline relies on maximizing similarity between matching image-3D pairs while minimizing similarity to negatives. Understanding InfoNCE is essential for grasping how hard negative weighting modifies the loss.
  - Quick check question: What happens to the gradient contribution of a negative sample when its similarity to the anchor increases?

- Concept: CLIP Architecture and Multimodal Embeddings
  - Why needed here: The method repurposes CLIP's image and text encoders for 3D tasks. Knowing how CLIP creates aligned embeddings and what features it captures is crucial for understanding why the similarity metrics work.
  - Quick check question: Why can CLIP's image encoder be used to compare 3D objects via rendered views?

- Concept: Hard Negative Mining in Representation Learning
  - Why needed here: The novelty hinges on mining hard negatives for 3D data, which is non-trivial. Understanding why hard negatives improve learning and how they're typically mined in other domains contextualizes the method's contribution.
  - Quick check question: How does hard negative mining change the optimization landscape compared to random negative sampling?

## Architecture Onboarding

- Component map: 3D Point Clouds -> 30 Rendered Views per Object -> OpenCLIP Image Encoder -> CLIP Embeddings -> I2I/(I2L)2 Similarity Matrix -> Hard Negative Weighted Loss -> PointNeXt Encoder -> Aligned Multimodal Embeddings

- Critical path: 1. Precompute 3D similarities using I2I and (I2L)2 2. During training, sample image-3D pairs and apply hard negative weights 3. Update point cloud encoder via modified InfoNCE loss 4. Evaluate on downstream tasks (classification, retrieval)

- Design tradeoffs: Using frozen CLIP encoders limits architectural flexibility but leverages strong pretrained features; Precomputing similarities increases memory usage but speeds up training; (I2L)2 adds complexity but removes 2D visual biases; I2I is simpler but prone to texture bias

- Failure signatures: Poor retrieval performance suggests similarity metrics aren't capturing relevant features; Degraded classification accuracy indicates hard negative weighting is amplifying noise; High memory usage or slow training may reveal inefficiencies in similarity storage/computation

- First 3 experiments: 1. Train with only I2I similarity and no hard negative weighting; verify baseline performance 2. Add hard negative weighting to I2I model; measure improvement in retrieval metrics 3. Replace I2I with (I2L)2 and compare retrieval/classification results to identify bias differences

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Precomputing similarity matrices requires significant memory overhead and may not scale well to larger datasets
- Performance gains over baselines, while statistically significant, are modest in some metrics
- Dependence on LLM-generated landmarks introduces potential variability based on prompt quality and generation consistency

## Confidence

- High confidence: The core methodology of using CLIP embeddings for 3D similarity computation and the mathematical formulation of the weighted contrastive loss
- Medium confidence: The effectiveness of hard negative mining for 3D data and the superiority of the proposed methods over baselines, given the relatively small margin in some metrics
- Low confidence: The generalizability of results to larger, more diverse 3D datasets beyond the benchmark collections used

## Next Checks

1. Evaluate model performance on a significantly larger 3D dataset (e.g., Objaverse or a custom collection) to assess scalability and robustness beyond benchmark settings
2. Conduct ablation studies comparing hard negative mining against other negative sampling strategies (e.g., hard example mining via gradient-based methods) to isolate the specific contribution of the proposed approach
3. Test the sensitivity of (I2L)2 to variations in LLM prompt quality by systematically modifying the landmark generation prompts and measuring performance impact