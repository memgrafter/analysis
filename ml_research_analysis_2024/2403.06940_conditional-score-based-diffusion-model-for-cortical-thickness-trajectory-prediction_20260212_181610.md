---
ver: rpa2
title: Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction
arxiv_id: '2403.06940'
source_url: https://arxiv.org/abs/2403.06940
tags:
- cortical
- diffusion
- prediction
- thickness
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conditional score-based diffusion model
  to predict cortical thickness trajectories in Alzheimer's disease, addressing the
  challenge of longitudinal data sparsity and incompleteness. The model uses baseline
  clinical information (age, sex, initial diagnosis, and initial cortical thickness)
  to guide a reverse diffusion process for trajectory prediction.
---

# Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction

## Quick Facts
- arXiv ID: 2403.06940
- Source URL: https://arxiv.org/abs/2403.06940
- Reference count: 16
- Mean absolute error of 0.092±0.032 across all subjects for cortical thickness trajectory prediction

## Executive Summary
This paper introduces a conditional score-based diffusion model to predict cortical thickness trajectories in Alzheimer's disease, addressing the challenge of longitudinal data sparsity and incompleteness. The model uses baseline clinical information (age, sex, initial diagnosis, and initial cortical thickness) to guide a reverse diffusion process for trajectory prediction. The method was evaluated on the TADPOLE cohort and demonstrated superior performance compared to existing approaches, achieving a mean absolute error of 0.092±0.032 across all subjects. Bland-Altman analysis revealed near-zero bias with narrow confidence intervals (mean difference = 0.01, 95% CI = ±0.24-0.28), and strong correlations (R² > 0.9) between predicted and actual measurements. The model's stochastic nature also enables uncertainty quantification for patient-specific predictions.

## Method Summary
The method employs a conditional score-based diffusion model using the EDM framework with a 1D Attention U-net architecture. The model is trained on baseline clinical features including age, sex, initial diagnosis, initial cortical thickness, and time intervals. During inference, the model generates cortical thickness trajectories through reverse diffusion sampling without requiring intermediate measurements. The model was trained for 8192 epochs on NVIDIA A100 GPU with a batch size of 64 and learning rate of 0.001, using mean squared error loss.

## Key Results
- Achieved mean absolute error of 0.092±0.032 across all subjects for cortical thickness trajectory prediction
- Bland-Altman analysis showed near-zero bias with narrow confidence intervals (mean difference = 0.01, 95% CI = ±0.24-0.28)
- Strong correlations (R² > 0.9) between predicted and actual measurements
- Model enables uncertainty quantification through stochastic sampling

## Why This Works (Mechanism)

### Mechanism 1
The conditional diffusion model leverages baseline clinical features to guide trajectory generation, overcoming sparsity and incompleteness in longitudinal data. By conditioning on baseline CTh, age, sex, diagnosis, and time interval, the model learns a conditional score function that maps these inputs to the distribution of future CTh changes. During inference, this score function guides a reverse diffusion process that denoises latent variables into realistic CTh trajectories without requiring intermediate measurements.

### Mechanism 2
The stochastic reverse diffusion process enables uncertainty quantification for patient-specific predictions. Unlike deterministic models, the diffusion process samples multiple trajectories from the learned conditional distribution. Each sample represents a plausible CTh evolution path, and the variance across samples quantifies prediction uncertainty. This stochastic nature allows clinicians to assess the confidence of predictions.

### Mechanism 3
Conditioning on time intervals allows flexible, continuous prediction across arbitrary future points without retraining. By including the time difference between baseline and target as part of the conditioning vector, the same trained model can generate predictions for any time horizon within the trained range. This avoids the need for separate models or interpolation between discrete time points.

## Foundational Learning

- **Score-based generative models and diffusion processes**: Understanding how the forward diffusion adds noise and the reverse diffusion denoises to generate data is essential for implementing and debugging the model. *Quick check*: What is the role of the score function in the reverse diffusion process?

- **Conditional modeling and conditioning vectors**: The model must correctly encode and utilize baseline features to guide trajectory generation; improper conditioning leads to poor predictions. *Quick check*: How does the model condition on both static features (age, sex) and dynamic features (initial CTh, time interval)?

- **Uncertainty quantification and probabilistic predictions**: Clinicians need to understand the confidence of predictions; knowing how to interpret and visualize uncertainty is critical for practical deployment. *Quick check*: How is uncertainty quantified in a stochastic generative model, and what does it represent clinically?

## Architecture Onboarding

- **Component map**: Input preprocessing -> Conditioning encoder -> 1D Attention U-net -> Reverse diffusion sampler -> Uncertainty estimator -> Output postprocessing

- **Critical path**: 
  1. Preprocess baseline data and time intervals
  2. Encode and concatenate conditioning features
  3. Pass through 1D Attention U-net to estimate score
  4. Perform reverse diffusion to generate CTh trajectories
  5. Sample multiple realizations for uncertainty quantification
  6. Compare predictions to ground truth for evaluation

- **Design tradeoffs**:
  - Deterministic vs. stochastic: Deterministic models are simpler but cannot quantify uncertainty; stochastic diffusion models are more complex but provide richer outputs
  - Fixed vs. flexible time horizons: Fixed models require retraining for new time points; flexible conditioning allows continuous prediction but may sacrifice accuracy for distant horizons
  - Attention vs. no attention: Attention modules improve accuracy but increase model complexity and training time

- **Failure signatures**:
  - High variance in predictions may indicate insufficient conditioning or overfitting
  - Systematic bias in certain subgroups suggests the model is not capturing subgroup-specific progression patterns
  - Poor performance on MCI/AD subgroups may reflect the increased heterogeneity in these groups

- **First 3 experiments**:
  1. Train and evaluate a baseline deterministic U-net (no attention) for direct CTh change prediction; compare MAE to diffusion model
  2. Train the conditional diffusion model with and without attention; analyze the impact on accuracy and uncertainty calibration
  3. Perform ablation studies on conditioning features (e.g., remove time interval or diagnosis) to assess their contribution to prediction quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed conditional score-based diffusion model perform when trained and evaluated on datasets with varying degrees of missing data? The paper mentions that longitudinal data in AD studies frequently suffer from sparsity and incompleteness, but does not explicitly test the model's performance under different levels of missing data.

### Open Question 2
Can the proposed model effectively handle other biomarkers or clinical variables beyond cortical thickness for predicting Alzheimer's disease progression? The paper focuses solely on predicting cortical thickness trajectories and does not explore the model's applicability to other biomarkers or clinical variables.

### Open Question 3
How does the proposed model's performance compare to other state-of-the-art deep learning approaches for longitudinal prediction in Alzheimer's disease? While the paper demonstrates the superiority of the proposed model over the compared methods, it does not explore its performance relative to other advanced deep learning techniques.

## Limitations
- Model relies heavily on accurate baseline clinical features and T1-weighted MRI data quality
- Performance may degrade for patient subgroups with highly variable progression patterns
- Model's accuracy is constrained by training data range (1.5-6 years)

## Confidence
- **Prediction Accuracy (MAE = 0.092±0.032)**: High confidence - supported by direct evaluation on held-out test set
- **Uncertainty Quantification**: Medium confidence - enabled by model's stochastic nature but calibration requires further validation
- **Generalizability to New Patients**: Low-Medium confidence - validated only on TADPOLE cohort

## Next Checks
1. Evaluate model performance on an independent AD cohort with different MRI acquisition parameters to assess generalizability
2. Perform reliability diagrams and calibration metrics to assess whether predicted uncertainties match observed errors
3. Conduct a clinician-in-the-loop study to evaluate whether model's predictions and uncertainty estimates improve clinical decision-making