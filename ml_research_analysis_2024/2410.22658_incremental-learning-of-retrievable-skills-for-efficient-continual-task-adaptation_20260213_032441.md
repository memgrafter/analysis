---
ver: rpa2
title: Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation
arxiv_id: '2410.22658'
source_url: https://arxiv.org/abs/2410.22658
tags:
- skill
- learning
- task
- tasks
- evolving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IsCiL, a continual imitation learning (CiL)
  framework designed to address challenges of data inefficiency, non-stationarity,
  and privacy concerns. The key idea is to incrementally learn shareable skills from
  demonstrations using a prototype-based skill retrieval method.
---

# Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation

## Quick Facts
- **arXiv ID**: 2410.22658
- **Source URL**: https://arxiv.org/abs/2410.22658
- **Reference count**: 40
- **Primary result**: IsCiL achieves robust task adaptation and sample efficiency in non-stationary continual imitation learning by incrementally learning shareable skills with prototype-based retrieval and parameter-efficient adapters.

## Executive Summary
This paper proposes IsCiL, a continual imitation learning framework that addresses data inefficiency, non-stationarity, and privacy concerns through incremental skill learning. The key innovation is a prototype-based skill retrieval method that captures multifaceted skill prototypes and associated adapters, enabling accurate skill matching and retrieval during policy evaluation. The framework supports task adaptation and unlearning via parameter-efficient skill adapters, achieving superior performance compared to adapter-based continual learning baselines on Evolving Kitchen and Evolving World environments.

## Method Summary
IsCiL uses a two-level hierarchical architecture with a skill retriever and skill decoder. The retriever incrementally learns multifaceted skill prototypes using KMeans clustering over state-action demonstrations, where each prototype consists of multiple bases to capture multi-modal skill distributions. During evaluation, the retriever selects the most similar skill prototype for a given state, which is then combined with a LoRA adapter to produce actions through the skill decoder. This approach isolates skill-specific parameters in dedicated adapters while keeping the base model frozen, mitigating catastrophic forgetting. The framework supports task adaptation by modifying goal embeddings at evaluation time and enables unlearning by removing specific skill adapters.

## Key Results
- IsCiL outperforms adapter-based continual learning baselines in task adaptation and sample efficiency on Evolving Kitchen and Evolving World environments
- The multifaceted skill prototype design with multiple bases improves retrieval accuracy under non-stationary conditions
- Parameter isolation through LoRA adapters successfully mitigates catastrophic forgetting across stages
- Task adaptation is achieved by modifying goal embeddings without retraining the base model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: IsCiL mitigates catastrophic forgetting by isolating skill-specific parameters in dedicated adapters while retaining a fixed base model.
- **Mechanism**: Each skill prototype is paired with a LoRA adapter that is only updated when that skill is encountered. The base model parameters remain frozen, so past knowledge is preserved.
- **Core assumption**: Skill distributions are stable enough that frozen state embeddings (from the fixed encoder) remain effective for retrieval.
- **Evidence anchors**:
  - [abstract] "adapter-based learning approach allows for parameter isolation for individual tasks, thus enabling to mitigate catastrophic forgetting"
  - [section 3.3] "skill adapter parameters θz and the pre-trained base model θpre, using the Low-Rank Adaptation"
  - [corpus] "class-incremental learning (CIL) ... suffer from catastrophic forgetting (CF) due to task distribution shifts"
- **Break condition**: If the state embedding function drifts or if tasks have overlapping sub-goals that cause ambiguous skill matches, the fixed retrieval mechanism could fail.

### Mechanism 2
- **Claim**: Multi-modal skill prototypes with multiple bases improve retrieval accuracy under non-stationary conditions.
- **Mechanism**: Each skill prototype consists of several bases; the similarity function takes the max over all bases, allowing robust matching even when sub-goal demonstrations vary.
- **Core assumption**: The KMeans clustering over skill demonstrations yields a compact set of representative bases that span the skill's state distribution.
- **Evidence anchors**:
  - [section 3.3] "Each χz consists of multiple bases (e.g., 20 bases), and each basis b is a representative vector containing its corresponding centroid"
  - [section 3.3] "This multifaceted set of bases allows the skill prototype to capture an accurate multi-modal distribution of the skill"
  - [corpus] "skill prototype ... consists of several bases; the similarity function takes the max over all bases"
- **Break condition**: If skill distributions are too sparse or highly overlapping, the max similarity may yield ambiguous matches and hurt performance.

### Mechanism 3
- **Claim**: Task adaptation is achieved by modifying the goal embedding at evaluation time, enabling the retriever to select the appropriate skill without retraining.
- **Mechanism**: Given a new task's sub-goal sequence, the decoder conditions on the current sub-goal and retrieves the relevant skill from the learned prototypes.
- **Core assumption**: The language-conditioned sub-goal embeddings encode sufficient information to disambiguate between skills across tasks.
- **Evidence anchors**:
  - [abstract] "The skill decoder is responsible for producing short-horizon actions for state-skill pairs"
  - [section 3.4] "This adjustment enables the inference of appropriate current actions, in a manner of similar to handling learned tasks"
  - [corpus] "language-conditioned policies" and "sub-goal information is also expressed using language embedding"
- **Break condition**: If sub-goal language embeddings are ambiguous or poorly aligned with the skill space, the retriever may select incorrect skills.

## Foundational Learning

- **Concept**: Continual learning and catastrophic forgetting
  - **Why needed here**: IsCiL explicitly targets the forgetting problem by isolating adapters per skill.
  - **Quick check question**: What happens to the base model parameters during skill updates?

- **Concept**: Parameter-efficient tuning (PET) and LoRA
  - **Why needed here**: LoRA adapters allow fast adaptation without retraining the entire model.
  - **Quick check question**: How does the rank of the LoRA adapter affect adaptation quality vs. memory usage?

- **Concept**: Skill-based hierarchical policies
  - **Why needed here**: The two-level architecture (retriever + decoder) enables modular, reusable skills.
  - **Quick check question**: How does the retriever decide which skill adapter to activate given a state?

## Architecture Onboarding

- **Component map**: State encoder (fixed) -> Skill retriever (prototype-based) -> LoRA skill adapter pool -> Skill decoder (base + LoRA) -> Action
- **Critical path**: 1. Encode (o_t, g_t) -> s_t 2. Retrieve most similar skill prototype -> θ_z 3. Decode with θ_pre + θ_z -> action a_t
- **Design tradeoffs**:
  - Fixed encoder vs. adaptive embedding: Stability vs. potential mismatch under distribution shift.
  - Number of bases per prototype: More bases -> better coverage but higher memory.
  - LoRA rank: Higher rank -> richer adaptation but more parameters.
- **Failure signatures**:
  - Retrieval consistently wrong skill -> Check base clustering or similarity function.
  - Performance degrades over stages -> Adapter isolation might be insufficient; consider rehearsal.
  - Slow inference -> Prototype pool too large; consider indexing or pruning.
- **First 3 experiments**:
  1. Ablation: Compare retrieval accuracy with 1 vs. 20 bases per skill prototype.
  2. Scalability: Measure inference time vs. prototype pool size.
  3. Unlearning: Remove a skill adapter and confirm task performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of bases per skill prototype (χz) for different environments and task complexities?
- **Basis in paper**: [explicit] Table 4 shows ablation results varying |χz| from 1 to 50, demonstrating performance differences across different numbers of bases.
- **Why unresolved**: The paper shows performance varies with |χz| but doesn't identify a universal optimal value, and different environments (Evolving Kitchen vs Evolving World) show different optimal ranges.
- **What evidence would resolve it**: Systematic experiments across multiple environments varying |χz|, identifying patterns between environment complexity, task diversity, and optimal base count.

### Open Question 2
- **Question**: How does the quality of the pre-trained base model affect IsCiL's performance, and what is the minimum viable quality threshold?
- **Basis in paper**: [explicit] Table 15 shows ablation results varying pre-trained model quality by reducing the number of objects from 4 to 1, demonstrating performance degradation.
- **Why unresolved**: The paper demonstrates quality matters but doesn't establish how much degradation is acceptable or what specific aspects of pre-trained model quality (e.g., action distribution, state representation) are most critical.
- **What evidence would resolve it**: Systematic degradation studies varying different aspects of pre-trained model quality, establishing performance thresholds and identifying which components are most critical.

### Open Question 3
- **Question**: What is the trade-off between skill adapter rank and performance in different task domains?
- **Basis in paper**: [explicit] Table 14 shows 1-rank adapters work well in Evolving Kitchen but poorly in Evolving World, suggesting domain-dependent rank requirements.
- **Why unresolved**: The paper shows rank matters but doesn't explain why different environments require different ranks or establish guidelines for selecting appropriate ranks.
- **What evidence would resolve it**: Analysis correlating adapter rank requirements with task complexity metrics, environment characteristics, or action distribution properties.

### Open Question 4
- **Question**: How does IsCiL's performance scale with the number of tasks and task diversity in the data stream?
- **Basis in paper**: [inferred] The paper evaluates on scenarios with 20 stages but doesn't systematically explore scaling behavior or test with varying levels of task similarity/diversity.
- **Why unresolved**: Limited evaluation scenarios don't reveal how performance degrades or improves with increasing task count or diversity.
- **What evidence would resolve it**: Experiments varying the number of tasks and measuring performance across different diversity levels, identifying scaling limits and diversity thresholds.

## Limitations

- The claim that IsCiL fully mitigates catastrophic forgetting relies on the assumption that skill distributions remain stable across stages, but this is not empirically validated beyond the reported metrics.
- The prototype-based retrieval mechanism's robustness to ambiguous skill matches is assumed rather than proven, with no reported retrieval accuracy or qualitative analysis of skill retrieval errors.
- The evaluation focuses on specific benchmarks (Evolving Kitchen and Evolving World) without ablation studies on the impact of key design choices like the number of bases per prototype or LoRA rank.

## Confidence

- **High**: The mechanism of using LoRA adapters to isolate skill-specific parameters and prevent catastrophic forgetting is well-supported by the literature and experimental results.
- **Medium**: The effectiveness of multifaceted skill prototypes in improving retrieval accuracy is plausible but not thoroughly validated with ablation studies or retrieval error analysis.
- **Low**: The claim that the retriever can reliably disambiguate tasks via sub-goal embeddings is weakly supported, as the paper does not analyze cases where sub-goal language embeddings might be ambiguous.

## Next Checks

1. **Retrieval accuracy analysis**: Measure and visualize the skill retriever's performance on state spaces across stages to identify ambiguous or failed matches.
2. **Ablation on prototype design**: Compare retrieval and task performance with varying numbers of bases per skill prototype (e.g., 1 vs. 20) to quantify the impact of multifaceted bases.
3. **Unlearning validation**: Remove a skill adapter and confirm task performance degradation, ensuring the unlearning mechanism works as intended.