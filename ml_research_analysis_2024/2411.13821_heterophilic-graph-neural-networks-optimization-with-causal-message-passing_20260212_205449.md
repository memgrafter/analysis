---
ver: rpa2
title: Heterophilic Graph Neural Networks Optimization with Causal Message-passing
arxiv_id: '2411.13821'
source_url: https://arxiv.org/abs/2411.13821
tags:
- graph
- causal
- node
- heterophilic
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CausalMP, a causal inference-based approach
  to improve graph neural networks (GNNs) on heterophilic graphs. The key idea is
  to detect heterophilic edges by analyzing asymmetric node dependencies using intervention-based
  causal inference.
---

# Heterophilic Graph Neural Networks Optimization with Causal Message-passing

## Quick Facts
- arXiv ID: 2411.13821
- Source URL: https://arxiv.org/abs/2411.13821
- Reference count: 40
- Primary result: CausalMP achieves higher AUC scores and better node classification accuracy on heterophilic graphs through causal inference-based structure learning

## Executive Summary
This paper introduces CausalMP, a novel approach that leverages causal inference to improve graph neural networks on heterophilic graphs. The key innovation is using intervention-based experiments and conditional entropy analysis to detect asymmetric dependencies between nodes, allowing the model to identify heterophilic edges and convert them to directed edges. By learning a causal structure that represents true node relationships rather than spurious correlations, CausalMP achieves superior performance on both link prediction and node classification tasks across 9 datasets spanning both homophilic and heterophilic graph types.

## Method Summary
CausalMP iteratively learns explicit causal structures by conducting multiple intervention experiments where Gaussian noise is added to node features. For each edge, the model computes conditional entropy differences between node pairs in both directions (H(Xj|Xi) vs H(Xi|Xj)). A large asymmetric difference indicates heterophily, as mutual information is lower between heterophilic pairs. These detected edges are converted to directed edges, and the graph structure is further optimized by adding edges based on mutual information. The model then trains separate encoder-decoder frameworks for both original and causal graphs, with a consistency penalty ensuring stable node embeddings. This dual optimization approach combines reconstruction losses with a mean squared error consistency term.

## Key Results
- CausalMP outperforms existing models on link prediction across 9 datasets, achieving higher AUC scores
- The method shows better node classification accuracy, particularly in few-shot settings
- Learned causal structures enhance generalization across different base models
- CausalMP effectively handles both homophilic and heterophilic graphs, with particular strength on heterophilic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal inference detects heterophilic edges by quantifying asymmetric dependencies between node pairs
- Mechanism: The model uses conditional entropy differences between node pairs in both directions (H(Xj|Xi) vs H(Xi|Xj)). A large asymmetric difference indicates heterophily, as mutual information is lower between heterophilic pairs. These detected edges are then converted to directed edges to improve message passing
- Core assumption: Heterophilic edges exhibit asymmetric dependency where one node depends more strongly on the other than vice versa
- Evidence anchors:
  - [abstract]: "By leveraging cause-effect analysis, we can discern heterophilic edges based on asymmetric node dependency"
  - [section 3.3]: "We construct a criterion for heterophilic links based on dependency estimated by their conditional entropy" and "a large difference in conditional entropy suggests heterophily"
  - [corpus]: Weak evidence - the corpus contains related work on heterophilic graph learning but no direct evidence about causal inference detecting asymmetric dependencies
- Break condition: If heterophilic edges exhibit symmetric dependencies rather than asymmetric ones, this mechanism would fail to detect them

### Mechanism 2
- Claim: The causal structure learned through intervention-based experiments improves GNN generalization across both homophilic and heterophilic graphs
- Mechanism: By conducting multiple intervention experiments (adding noise to node features) and measuring information gain through conditional entropy, the model learns a causal structure that represents true node relationships rather than spurious correlations. This structure is then used to modify the graph topology for better message passing
- Core assumption: The learned causal structure captures invariant relationships that generalize across different graph types and tasks
- Evidence anchors:
  - [abstract]: "The learned causal structure offers more accurate relationships among nodes" and "Training on causal structure can also enhance node representation in classification task across different base models"
  - [section 4.1]: "By leveraging the KDE estimates, we compute the conditional entropy between the node pairs" and "For each edge, we obtain a corresponding dependency score"
  - [section 3.2]: "We aim to learn the parameter of the causal structure" and "The posterior of the final causal structure"
- Break condition: If the causal structure overfits to specific graph characteristics and fails to generalize to new graph types

### Mechanism 3
- Claim: The consistency penalty in the optimization objective ensures that causal structure modification doesn't disrupt learned node representations
- Mechanism: After modifying the graph structure to create a causal structure, the model trains separate encoder-decoder frameworks for both the original and causal graphs. A mean squared error consistency term ensures the node embeddings remain stable despite structural changes
- Core assumption: The node embeddings should remain consistent when the graph structure is modified according to causal relationships
- Evidence anchors:
  - [section 4.1]: "The consistency penalty is quantified by the mean squared error (MSE) as Lcons = MSE(EN(ùê¥ùëê), EN(ùê¥))"
  - [section 3.2]: "we seek to minimize the entropy of the second term" and "the augmentation introduced by the causal structure does not disrupt the node representation"
  - [section 4.1]: "The optimization target is weighted summation of the reconstruction loss of two graphs and consistency penalty"
- Break condition: If the consistency penalty is too strong, it may prevent the model from learning beneficial structural modifications

## Foundational Learning

- Concept: Conditional entropy and mutual information
  - Why needed here: These information-theoretic measures are used to quantify the asymmetric dependencies between node pairs and detect heterophily
  - Quick check question: How does conditional entropy differ from mutual information, and why is the difference between H(Xj|Xi) and H(Xi|Xj) meaningful for detecting heterophily?

- Concept: Intervention-based causal inference
  - Why needed here: The method uses interventions (adding noise to node features) to estimate causal effects and learn the dependency structure
  - Quick check question: What is the difference between association and causation, and how does intervention help establish causal relationships in graphs?

- Concept: Graph neural network message passing
  - Why needed here: The entire method aims to improve the message passing mechanism in GNNs by modifying the graph structure based on causal analysis
  - Quick check question: How does standard GNN message passing work, and why does it struggle with heterophilic graphs where connected nodes have different labels?

## Architecture Onboarding

- Component map: Graph ‚Üí Embedding ‚Üí Intervention experiments ‚Üí Dependency estimation ‚Üí Causal structure modification ‚Üí Dual optimization ‚Üí Final model
- Critical path: Graph ‚Üí Embedding ‚Üí Intervention experiments ‚Üí Dependency estimation ‚Üí Causal structure modification ‚Üí Dual optimization ‚Üí Final model
- Design tradeoffs:
  - Computational cost vs accuracy: More intervention experiments (M) and iterations (T) improve accuracy but increase computation time
  - Homophily vs heterophily performance: The model must balance improvements for both graph types
  - Structure modification vs representation stability: The consistency penalty trades off between learning new structure and maintaining stable embeddings
- Failure signatures:
  - Performance degradation on homophilic graphs: May indicate overcorrection for heterophily
  - High variance in results: Could suggest instability in dependency estimation
  - No improvement over baselines: Might indicate issues with the conditional entropy estimation or intervention strategy
- First 3 experiments:
  1. Run on a small heterophilic dataset (Cornell) with M=4, T=3, r_c=0.05 to verify basic functionality and dependency detection
  2. Compare dependency scores between homophilic and heterophilic edges on Texas dataset to validate the asymmetric dependency hypothesis
  3. Test different center node ratios (r_c=0.02, 0.05, 0.1) on Actor dataset to find optimal balance between coverage and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CausalMP's performance scale with graph size, and what are the computational bottlenecks that limit its applicability to very large graphs?
- Basis in paper: [inferred] The paper discusses computational complexity, estimating it as O(N^2) where N is the number of nodes. It mentions that the main overhead comes from the KDE estimation process
- Why unresolved: The paper only provides theoretical complexity analysis and does not present empirical results on extremely large graphs. The scalability of the method to graphs with millions of nodes is not demonstrated
- What evidence would resolve it: Experiments on graphs with varying sizes (e.g., from 10,000 to 1,000,000 nodes) showing the runtime and memory usage of CausalMP, along with comparisons to baseline methods on the same graphs

### Open Question 2
- Question: What is the impact of the number of intervention experiments (M) and iterations (T) on the quality of the learned causal structure and the overall performance of CausalMP?
- Basis in paper: [explicit] The paper mentions that CausalMP performs M intervention experiments for T iterations, but does not provide a detailed analysis of how varying these parameters affects performance
- Why unresolved: The paper does not conduct an ablation study to determine the optimal values of M and T, or how sensitive the performance is to these hyperparameters
- What evidence would resolve it: An ablation study showing the performance of CausalMP with different combinations of M and T on various datasets, along with an analysis of the trade-off between performance and computational cost

### Open Question 3
- Question: How does CausalMP handle graphs with dynamic structures, where the edges and node features change over time?
- Basis in paper: [inferred] The paper focuses on static graphs and does not address the scenario of dynamic graphs. The proposed method relies on a fixed causal structure learned from the initial graph
- Why unresolved: The paper does not discuss how to adapt the learned causal structure to changes in the graph topology or node features over time
- What evidence would resolve it: An extension of CausalMP to handle dynamic graphs, along with experiments on temporal graph datasets showing the performance of the adapted method compared to the original version

## Limitations
- The asymmetric dependency assumption may not hold universally across all graph types
- Computational complexity with M=8 Monte Carlo experiments per iteration could limit scalability to larger graphs
- KDE-based conditional entropy estimation may be sensitive to bandwidth selection and sample size

## Confidence
- Mechanism 1 (asymmetric dependency detection): Medium - Supported by theoretical framework but requires empirical validation across diverse graph types
- Mechanism 2 (causal structure generalization): High - Strong experimental evidence across 9 datasets and multiple base models
- Mechanism 3 (consistency penalty effectiveness): Medium - Theoretical soundness but limited ablation study on penalty coefficient sensitivity

## Next Checks
1. Conduct systematic experiments varying M (number of Monte Carlo experiments) and T (iterations) to establish computational-performance tradeoffs
2. Perform ablation studies on the consistency penalty coefficient to determine optimal balance between structural modification and representation stability
3. Test CausalMP on additional heterophilic graph datasets with different characteristics (e.g., larger graphs, different feature distributions) to assess generalization limits