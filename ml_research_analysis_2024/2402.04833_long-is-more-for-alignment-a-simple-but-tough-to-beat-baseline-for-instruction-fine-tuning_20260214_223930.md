---
ver: rpa2
title: 'Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction
  Fine-Tuning'
arxiv_id: '2402.04833'
source_url: https://arxiv.org/abs/2402.04833
tags:
- lima-1k
- alpaca-1k-longest
- instruction
- alpaca-52k
- alpagasus-1k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of instruction length on the
  performance of instruction fine-tuning (IFT) for large language models (LLMs). The
  authors propose a simple baseline of selecting the 1,000 instructions with longest
  responses from standard datasets for IFT, hypothesizing that longer responses contain
  more learnable information and are harder to overfit.
---

# Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning

## Quick Facts
- arXiv ID: 2402.04833
- Source URL: https://arxiv.org/abs/2402.04833
- Reference count: 40
- Primary result: Selecting and fine-tuning on the 1,000 longest instruction responses from standard datasets consistently outperforms state-of-the-art instruction fine-tuning methods, achieving strong performance with minimal computational cost.

## Executive Summary
This paper introduces a simple yet effective baseline for instruction fine-tuning (IFT) of large language models (LLMs): selecting the 1,000 instructions with the longest responses from standard datasets. The authors demonstrate that this approach, which they call "Long Is More," consistently outperforms complex, curated datasets such as AlpaGasus-1k and LIMA-1k across multiple model sizes (Llama-2-7B, Llama-2-13B, Mistral-7B) and evaluation protocols (GPT-4, PaLM-2 judges). By leveraging the informativeness and difficulty of longer responses, the method achieves strong performance on both open benchmarks and AlpacaEval 2.0, ranking second among Llama-2-7B-based models with only 1,000 examples and no extra preference data. The authors also propose a lightweight refinement using introspection and NEFTune noise augmentation to further improve results.

## Method Summary
The authors propose a straightforward method for instruction fine-tuning: from standard datasets (Alpaca-52k or Evol-Instruct-70k), select the 1,000 instructions with the longest model responses. These are then used to fine-tune LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B). A refinement step involves introspective self-improvement of these instructions and NEFTune noise augmentation during training. Performance is evaluated using GPT-4 and PaLM-2 as judges, as well as on open LLM benchmarks and AlpacaEval 2.0.

## Key Results
- Fine-tuning on the 1,000 longest responses from Alpaca-52k or Evol-Instruct-70k consistently outperforms state-of-the-art methods (AlpaGasus-1k, LIMA-1k) according to GPT-4 and PaLM-2 judges.
- The approach achieves the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 with only 1,000 examples and no extra preference data.
- Lightweight refinement via introspection and NEFTune noise augmentation further improves performance, demonstrating the robustness of the method across model sizes and evaluation protocols.

## Why This Works (Mechanism)
The authors hypothesize that longer instruction responses contain more detailed and diverse information, making them more informative and harder to overfit during fine-tuning. This increased complexity may lead to better generalization and alignment with user intent. The introspective refinement step further enhances instruction quality by leveraging the model's own understanding, while NEFTune noise augmentation helps improve robustness. However, the possibility that judges (especially GPT-4) may have a bias toward longer responses is acknowledged and addressed through analysis, though not fully ruled out.

## Foundational Learning
- **Instruction fine-tuning (IFT)**: Why needed: Adapts LLMs to follow instructions; Quick check: Does the model produce outputs matching the desired format and content?
- **Dataset curation**: Why needed: Quality and diversity of training data affect model performance; Quick check: Are the selected instructions representative and informative?
- **Response length as a proxy for informativeness**: Why needed: Longer responses may contain more useful information; Quick check: Do longer responses correlate with better model outputs?
- **Self-improvement via introspection**: Why needed: Enables the model to refine its own instructions; Quick check: Do introspective refinements lead to measurable gains?
- **NEFTune noise augmentation**: Why needed: Improves model robustness and generalization; Quick check: Does noise augmentation reduce overfitting and improve evaluation metrics?
- **Judge bias analysis**: Why needed: Ensures evaluation is not artificially inflated by judge preferences; Quick check: Do results hold under different judging protocols?

## Architecture Onboarding

### Component Map
Standard dataset (Alpaca-52k/Evol-Instruct-70k) -> Longest response selection (1,000) -> Fine-tuning (LLM) -> Evaluation (GPT-4, PaLM-2, Open LLM, AlpacaEval 2.0) -> Introspective refinement + NEFTune (optional)

### Critical Path
Data selection (longest responses) -> Fine-tuning (LLM) -> Evaluation (judges, benchmarks)

### Design Tradeoffs
- **Pros**: Simple, computationally efficient, uses readily available datasets, strong performance across models and evaluations
- **Cons**: Relies on a single selection criterion (response length), potential judge bias, limited sample size (1,000 examples)

### Failure Signatures
- Poor performance if longest responses are not genuinely more informative or if they overfit
- Evaluation results may be inflated if judges favor longer responses
- Limited generalization if the dataset or instruction types are not representative

### First Experiments
1. Fine-tune Llama-2-7B on 1,000 longest responses from Alpaca-52k and evaluate with GPT-4 and PaLM-2.
2. Apply introspective refinement and NEFTune to the fine-tuned model and re-evaluate.
3. Compare results to state-of-the-art methods (AlpaGasus-1k, LIMA-1k) on Open LLM benchmarks and AlpacaEval 2.0.

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies on a single selection criterion (response length), which may not always correlate with instruction quality.
- The sample size of 1,000 examples may limit generalization, especially for more complex or diverse tasks.
- The possibility of judge bias toward longer responses, despite attempts to address it, remains a concern.
- The introspective refinement and NEFTune augmentation add complexity, and their individual contributions are not fully isolated.

## Confidence
- **High confidence**: The core empirical result that selecting and fine-tuning on the longest responses from existing datasets yields strong performance relative to curated datasets, and that this approach is robust across model sizes and evaluation methods.
- **Medium confidence**: The claim that the performance gain is primarily due to the informativeness and difficulty of longer responses, as opposed to other correlated factors, and that the introspective refinement meaningfully improves results beyond the baseline.
- **Low confidence**: The assertion that the effect is not substantially driven by judge bias toward longer responses, given the acknowledged limitations in the analysis.

## Next Checks
1. Conduct ablation studies using response length-matched subsets from both long and short response groups to isolate the effect of length from other response properties.
2. Re-evaluate using human annotators or additional judges not known to favor longer responses, and test the impact of response truncation during evaluation.
3. Expand the analysis to additional datasets and instruction types (e.g., non-English, specialized domains) to assess generalizability beyond the current benchmarks.