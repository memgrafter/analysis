---
ver: rpa2
title: 'Packing Analysis: Packing Is More Appropriate for Large Models or Datasets
  in Supervised Fine-tuning'
arxiv_id: '2410.08081'
source_url: https://arxiv.org/abs/2410.08081
tags:
- packing
- training
- padding
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of packing methods during supervised
  fine-tuning of large language models. Packing combines multiple training samples
  into one to maximize hardware efficiency.
---

# Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning

## Quick Facts
- arXiv ID: 2410.08081
- Source URL: https://arxiv.org/abs/2410.08081
- Reference count: 12
- Models tested: LLaMA-3-8B and LLaMA-3-70B
- Datasets: WildChat (69K), TULU (326K), WildChat Full (652K), Open-source 1M (1.2M)
- Key finding: Packing methods improve SFT efficiency and performance, especially for large models and datasets

## Executive Summary
This paper analyzes the impact of packing methods during supervised fine-tuning of large language models. Packing combines multiple training samples into one to maximize hardware efficiency. The study compares padding and packing (random and greedy) across models from 8B to 70B and datasets from 69K to 1.2M. Key findings show that packing generally improves performance across benchmarks, with larger models benefiting more. Greedy packing outperforms random packing, particularly on instruction-following tasks. Packing significantly reduces training time (e.g., 97893s to 25124s on LLaMA-3-70B for WildChat dataset), enabling efficient fine-tuning of large models on large datasets.

## Method Summary
The study compares three methods: padding, random packing, and greedy packing for supervised fine-tuning. Datasets were preprocessed with special tokens ([EOS], [PAD]) and chat templates. Models (LLaMA-3-8B and LLaMA-3-70B) were fine-tuned using specified hyperparameters. Performance was evaluated across multiple benchmarks (MMLU, GSM8K, MATH, BBH, HumanEval, IFEval, WildBench) with training time as a key efficiency metric.

## Key Results
- Packing significantly reduces training time (e.g., 97893s to 25124s on LLaMA-3-70B for WildChat dataset)
- Larger models (70B) benefit more from packing than smaller models (8B)
- Greedy packing outperforms random packing, especially on instruction-following tasks
- Packing is more suitable for large datasets; padding may be more time-efficient for small datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Packing reduces per-sample processing time by combining multiple sequences into one, allowing more efficient GPU utilization during SFT.
- Mechanism: During packing, multiple training conversations are concatenated into a single sequence using special tokens like [EOS]. This packed sequence is then truncated or padded to the model's maximum input length, enabling the model to process more data per forward/backward pass.
- Core assumption: The packing method does not significantly degrade model performance while reducing training time.
- Evidence anchors:
  - [abstract] "packing significantly reduces training time (e.g., 97893s to 25124s on LLaMA-3-70B for WildChat dataset)"
  - [section] "Compared to padding, the packing method greatly reduces training time, making it possible to fine-tune large models on large datasets."
- Break condition: If packing causes significant performance degradation or if the overhead of packing/unpacking exceeds the benefits.

### Mechanism 2
- Claim: Larger models benefit more from packing because they can better utilize the extended contexts created by combining multiple samples.
- Mechanism: As model size increases, their ability to effectively utilize extended contexts improves. Packing exposes these larger models to a wider variety of contextual combinations within each training sample, enhancing their generalization capabilities.
- Core assumption: Larger models have sufficient capacity to process and learn from the more diverse contextual information provided by packed sequences.
- Evidence anchors:
  - [abstract] "As the model size grows, the performance gap between padding and packing-based models on the benchmark increases."
  - [section] "This is due to enhanced contextual efficiency. As the model size increases, its ability to effectively utilize extended contexts improves, thereby magnifying the advantages of the diverse contextual combinations brought by packing."
- Break condition: If the model's attention mechanism becomes overwhelmed by too much context or if the packed sequences exceed the model's attention window.

### Mechanism 3
- Claim: Greedy packing outperforms random packing because it preserves the coherence of multi-turn conversations, which is crucial for instruction-following tasks.
- Mechanism: Greedy packing sorts training conversations by length and packs them together as much as possible without exceeding the maximum input length. This approach maintains the integrity of multi-turn conversations, unlike random packing which may split such conversations across different training samples.
- Core assumption: Maintaining the coherence of multi-turn conversations is essential for the model to learn when to use prior context effectively.
- Evidence anchors:
  - [abstract] "Greedy packing outperforms random packing, particularly on instruction-following tasks."
  - [section] "This advantage of the greedy packing method lies in its ability to maintain the coherence of multi-turn conversations, unlike random packing which may split such conversations across different training samples."
- Break condition: If the sorting and organizing process introduces significant computational overhead or if the benefits of coherence preservation are outweighed by the loss of training data diversity.

## Foundational Learning

- Concept: Sequence packing and padding in transformer-based language models
  - Why needed here: Understanding how packing and padding work is fundamental to grasping why packing can be more efficient for SFT.
  - Quick check question: What is the main difference between packing and padding in terms of how they handle variable-length sequences?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The effectiveness of packing depends on how well the model's attention mechanism can handle the extended contexts created by combining multiple samples.
  - Quick check question: How does the attention mechanism in transformers handle sequences that exceed the maximum input length?

- Concept: Supervised fine-tuning (SFT) process
  - Why needed here: Understanding the SFT process is crucial for comprehending how packing fits into the overall fine-tuning workflow.
  - Quick check question: What are the main steps in the supervised fine-tuning process, and how does packing affect each step?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Model training loop -> Evaluation framework -> Resource management

- Critical path:
  1. Data preprocessing: Pack sequences and create batches
  2. Model training: Process packed sequences through forward/backward passes
  3. Parameter updates: Adjust model weights based on computed gradients
  4. Evaluation: Assess model performance on benchmarks

- Design tradeoffs:
  - Packing efficiency vs. potential performance degradation
  - Computational overhead of packing/unpacking vs. reduced training time
  - Model size benefits vs. increased memory requirements

- Failure signatures:
  - Performance degradation compared to padding method
  - Increased training time due to packing overhead
  - Memory issues when packing sequences exceed model capacity

- First 3 experiments:
  1. Compare training time and performance of LLaMA-3-8B on WildChat dataset using padding vs. greedy packing
  2. Evaluate the impact of packing on instruction-following tasks (IFEval benchmark) for different model sizes
  3. Test the scalability of packing by fine-tuning LLaMA-3-70B on increasingly large datasets (TULU, WildChat, Open-source 1M)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relationship between batch size and learning rate in packing methods follow a different mathematical pattern than in padding methods, and if so, what is the precise functional form?
- Basis in paper: [explicit] The paper states "In packing mode, the batch size is no longer directly proportional to the learning rate" and shows experimental evidence that this linear relationship breaks down.
- Why unresolved: While the paper demonstrates that the relationship is not linear in packing mode, it does not provide a mathematical model or precise functional form that describes how batch size and learning rate should be adjusted together.
- What evidence would resolve it: Controlled experiments varying batch size and learning rate combinations across multiple packing scenarios, combined with mathematical modeling to derive the optimal scaling relationship.

### Open Question 2
- Question: What is the optimal packing strategy (random vs greedy) for instruction-following benchmarks when the training dataset contains both single-turn and multi-turn conversations?
- Basis in paper: [explicit] The paper finds that greedy packing generally performs better on instruction-following benchmarks, but notes that this advantage comes from preserving multi-turn conversation integrity.
- Why unresolved: The paper doesn't provide specific guidance on how to optimize packing strategies when datasets contain mixed conversation types, nor does it explore hybrid approaches that might combine benefits of both methods.
- What evidence would resolve it: Systematic experiments comparing performance across different ratios of single-turn to multi-turn conversations using various packing strategies, including potential hybrid approaches.

### Open Question 3
- Question: How does the quality and diversity of the training dataset influence the effectiveness of packing methods compared to padding?
- Basis in paper: [explicit] The paper mentions that "if your goal is to fine-tune a small model (e.g., 6B, 8B, or 9B) on a small dataset (e.g., 20K or 30K), using the padding method might be more time-efficient" and notes differences in performance based on dataset size.
- Why unresolved: While the paper observes performance differences based on dataset size, it doesn't systematically investigate how dataset quality metrics (like diversity, complexity, or instruction variety) interact with packing effectiveness.
- What evidence would resolve it: Controlled experiments varying dataset quality metrics while keeping size constant, measuring packing vs padding performance across different quality levels.

## Limitations

- Relatively small number of datasets tested (4 total) may not capture full spectrum of LLM applications
- Focus on only two model sizes (8B and 70B parameters) limits generalizability across the full range of LLM architectures
- Does not address potential long-term effects of packing on model robustness or generalization beyond tested benchmarks

## Confidence

- High confidence: The core finding that packing reduces training time while maintaining or improving performance, particularly for larger models and datasets
- Medium confidence: The claim that greedy packing outperforms random packing on instruction-following tasks, as this is based on specific benchmark results
- Medium confidence: The assertion that larger models benefit more from packing due to enhanced contextual efficiency, though the mechanism is well-supported by the data

## Next Checks

1. Test packing methods across a wider range of model sizes (e.g., 3B, 30B, 65B) to better understand the scaling relationship
2. Evaluate packing performance on additional datasets, particularly those with different characteristics (e.g., code, multilingual, specialized domains)
3. Investigate the impact of packing on model calibration and uncertainty estimates, which were not addressed in the current analysis