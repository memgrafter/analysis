---
ver: rpa2
title: Neural DDEs with Learnable Delays for Partially Observed Dynamical Systems
arxiv_id: '2410.02843'
source_url: https://arxiv.org/abs/2410.02843
tags:
- delays
- dynamics
- equation
- neural
- delay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning dynamics from partially
  observed systems, where the full state is not available. It introduces a theoretical
  framework based on the Mori-Zwanzig formalism, showing that constant lag Neural
  Delay Differential Equations (NDDEs) naturally model such systems.
---

# Neural DDEs with Learnable Delays for Partially Observed Dynamical Systems
## Quick Facts
- arXiv ID: 2410.02843
- Source URL: https://arxiv.org/abs/2410.02843
- Reference count: 40
- Primary result: NDDEs outperform LSTM, Neural ODEs, and Latent ODEs on partially observed dynamical systems

## Executive Summary
This paper addresses the challenge of learning dynamics from partially observed systems where full state information is unavailable. The authors introduce a theoretical framework based on the Mori-Zwanzig formalism showing that constant lag Neural Delay Differential Equations (NDDEs) naturally model such systems. They demonstrate that NDDEs significantly outperform existing methods including LSTM, Neural ODEs, and Latent ODEs on both synthetic and experimental data. A key innovation is learning the delays jointly with the model dynamics, which proves crucial for accurate modeling. The approach is validated across multiple experiments including the chaotic Kuramoto-Sivashinsky system and experimental fluid dynamics data.

## Method Summary
The authors leverage the Mori-Zwanzig formalism to establish that constant lag Neural Delay Differential Equations (NDDEs) naturally arise when modeling partially observed dynamical systems. They implement NDDEs using the torchdde library with numerically robust solvers and adjoint methods. The model learns both the system dynamics and optimal delay values simultaneously through backpropagation. This approach is applied to learn from partial observations of the full state, with the learned delays capturing the temporal correlations inherent in the reduced observations. The method is compared against LSTM, Neural ODEs, and Latent ODEs on synthetic and experimental datasets.

## Key Results
- NDDEs outperform LSTM, Neural ODEs, and Latent ODEs on both synthetic and experimental data
- The learned delays are shown to be crucial for accurate modeling of partially observed systems
- Superior performance in capturing system statistics and dynamics on the chaotic Kuramoto-Sivashinsky system
- Strong results on experimental fluid dynamics data, demonstrating real-world applicability

## Why This Works (Mechanism)
The Mori-Zwanzig formalism provides a theoretical foundation showing that when projecting high-dimensional dynamical systems onto lower-dimensional observations, the resulting equations naturally involve delays. This mathematical framework explains why constant lag NDDEs are well-suited for partially observed systems. The learned delays capture the temporal correlations that emerge from the projection of full-state dynamics onto partial observations. By learning these delays jointly with the system dynamics, the model can adapt to the specific temporal structure of each observation scenario, leading to superior performance compared to models that assume Markovian dynamics or fixed delays.

## Foundational Learning
- Mori-Zwanzig formalism: Mathematical framework for reducing high-dimensional dynamical systems; needed to theoretically justify NDDEs for partially observed systems; quick check: verify the projection operator properties
- Neural Delay Differential Equations: Neural networks combined with DDEs; needed to model systems with memory effects; quick check: ensure numerical stability of solvers
- Adjoint methods: Gradient computation through differential equation solvers; needed for efficient backpropagation; quick check: validate gradient accuracy
- Chaotic dynamical systems: Systems with sensitive dependence on initial conditions; needed as testbeds for modeling capability; quick check: verify Lyapunov exponents
- Partial observations: Limited access to system state; needed to simulate real-world measurement constraints; quick check: test with different observation dimensions

## Architecture Onboarding
Component map: Observations -> NDDE Solver -> Loss Function -> Backpropagation -> Delay and Parameter Updates

Critical path: The NDDE solver with learned delays forms the core computational path, where the delays directly influence the solution trajectory and consequently the loss. The adjoint method enables efficient gradient computation through this path.

Design tradeoffs: Learning delays jointly provides adaptability but increases parameter count and computational cost compared to fixed delays. The choice between stiff and non-stiff solvers affects numerical stability and efficiency. The observation dimension versus delay length tradeoff impacts model complexity.

Failure signatures: Poor performance may indicate inappropriate delay values, numerical instability in the solver, or insufficient model capacity. Chaotic systems may require longer training or adaptive step sizes. Local minima in delay optimization can lead to suboptimal solutions.

Three first experiments:
1. Validate on a simple linear system with known delay to verify correct delay learning
2. Test with synthetic observations of the Lorenz system to assess chaotic system modeling
3. Perform ablation study removing learned delays to quantify their importance

## Open Questions the Paper Calls Out
The paper acknowledges that while the method shows strong performance on tested systems, its generalizability to diverse dynamical systems remains to be fully established. The learned delays may not always converge to meaningful physical interpretations, and the computational efficiency compared to other methods on larger-scale problems requires further investigation. The authors suggest testing on additional diverse dynamical systems, including high-dimensional chaotic systems and stochastic processes, to assess broader applicability.

## Limitations
- Generalizability beyond tested systems is uncertain
- Learned delays may not have clear physical interpretations
- Computational efficiency on larger-scale problems not extensively discussed
- Limited testing on diverse dynamical system types

## Confidence
High: Theoretical framework connecting Mori-Zwanzig formalism to constant lag NDDEs is sound; demonstrated superiority over baselines on tested datasets
Medium: Claim that learning delays jointly is essential is supported but needs broader validation
Low: Broad applicability claims require testing on more varied dynamical systems

## Next Checks
1. Test the method on additional diverse dynamical systems, including high-dimensional chaotic systems and stochastic processes, to assess generalizability
2. Conduct ablation studies systematically removing learned delays to quantify their impact across different observation scenarios
3. Compare computational efficiency and scalability with other methods on larger datasets to establish practical advantages