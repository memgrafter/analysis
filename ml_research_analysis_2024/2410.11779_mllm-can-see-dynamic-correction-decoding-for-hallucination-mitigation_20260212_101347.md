---
ver: rpa2
title: MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation
arxiv_id: '2410.11779'
source_url: https://arxiv.org/abs/2410.11779
tags:
- image
- mllms
- deco
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLLMs exhibit awareness of hallucinated objects in early layers
  but suppress this recognition in later layers, leading to hallucinations. Dynamic
  Correction Decoding with Preceding-Layer Knowledge (DeCo) adaptively selects high-confidence
  preceding layers and integrates their knowledge into the final layer to correct
  logits.
---

# MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation

## Quick Facts
- **arXiv ID:** 2410.11779
- **Source URL:** https://arxiv.org/abs/2410.11779
- **Reference count:** 40
- **Primary result:** DeCo reduces hallucination rates by up to 17.6% on CHAIR and improves POPE F1 scores by up to 18.9%, outperforming baselines while adding only ~1.2x latency.

## Executive Summary
This paper identifies a key mechanism behind MLLM hallucinations: while MLLMs can recognize visual objects in early layers, language model priors suppress this recognition in later layers, leading to hallucinated outputs. The authors propose Dynamic Correction Decoding (DeCo), which adaptively selects high-confidence tokens from preceding layers (20-28) and integrates their knowledge into the final layer to correct logits. DeCo significantly reduces hallucinations while preserving the model's original generative style, achieving substantial improvements on CHAIR, AMBER, and POPE benchmarks.

## Method Summary
DeCo addresses MLLM hallucinations by leveraging the observation that visual object recognition capability exists in early transformer layers but gets overridden by language model priors in deeper layers. The method dynamically selects an anchor layer from preceding layers (20-28 for 7B models), identifies the highest-probability candidate tokens in that layer, and integrates this information into the final layer's logits using a dynamic modulation coefficient based on the maximum probability from the anchor layer. This approach corrects the logit distribution without fundamentally altering the model architecture or training process, making it broadly applicable across different MLLM architectures.

## Key Results
- DeCo reduces hallucination rates by up to 17.6% on the CHAIR benchmark compared to baseline methods
- POPE F1 scores improve by up to 18.9% with DeCo integration
- The method adds only ~1.2x latency overhead while maintaining or improving overall output quality
- DeCo consistently outperforms established baselines (DoLa, OPERA, VCD) across multiple benchmarks including AMBER and MME

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MLLMs retain visual object recognition capability in early layers but suppress it in later layers due to language model priors.
- **Mechanism:** The model's visual perception capability exists in preceding layers but gets overridden by strong language priors in deeper layers during decoding.
- **Core assumption:** The knowledge priors embedded in the MLLM actively suppress visual information that would otherwise be correctly recognized in earlier layers.
- **Evidence anchors:**
  - [abstract] "we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers"
  - [section 2.1] "We further observe that the confidence of generated tokens is influenced by the knowledge priors of MLLMs, leading to a reduction in the probability of ground truth tokens in the deeper layers"
  - [corpus] Weak - related papers focus on hallucination mitigation but don't directly address this specific mechanism
- **Break condition:** If the model architecture fundamentally changes such that visual and language modalities are processed independently throughout all layers, or if the knowledge priors are substantially weakened during training.

### Mechanism 2
- **Claim:** Dynamic Correction Decoding (DeCo) selects high-confidence tokens from preceding layers and uses them to correct logits in the final layer.
- **Mechanism:** DeCo dynamically selects an anchor layer from the preceding layers (20-28), extracts its probability distribution, and proportionally integrates this information into the final layer's logits using a modulation coefficient.
- **Core assumption:** The tokens with highest probability in selected preceding layers correspond to ground truth tokens that are being suppressed in the final layer.
- **Evidence anchors:**
  - [abstract] "DeCo adaptively selects high-confidence preceding layers and integrates their knowledge into the final layer to correct logits"
  - [section 3.2] "we integrate information from these layers into the final layer to correct the logit distribution"
  - [corpus] Weak - related papers discuss contrastive decoding but don't specifically use preceding-layer knowledge in this dynamic correction manner
- **Break condition:** If the preceding layers don't consistently contain high-confidence ground truth tokens, or if the modulation coefficient becomes unstable across different input types.

### Mechanism 3
- **Claim:** The dynamic soft modulation coefficient preserves the original generative style while correcting hallucinations.
- **Mechanism:** Uses the maximum probability from the anchor layer as a dynamic modulation coefficient to scale the contribution of preceding-layer logits when correcting the final layer.
- **Core assumption:** The maximum probability from the anchor layer provides an appropriate scaling factor that prevents hard changes in logits while still enabling meaningful correction.
- **Evidence anchors:**
  - [section 3.2] "We introduce a dynamic modulation coefficient, defaulting to the maximum probability"
  - [section 4.4] "This coefficient can help prevent hard changes in logits, particularly when the probability differences between candidate tokens in preceding layers are insignificant"
  - [corpus] Weak - related papers don't explicitly discuss dynamic soft modulation using max probability as scaling factor
- **Break condition:** If the max probability becomes too small (preventing correction) or too large (causing over-correction), or if the scaling relationship breaks down for certain types of inputs.

## Foundational Learning

- **Concept:** Transformer layer architecture and attention mechanisms
  - **Why needed here:** Understanding how information flows through transformer layers is critical to grasp why visual information is recognized in early layers but suppressed in later layers
  - **Quick check question:** How does the attention mechanism in transformer layers enable both visual and language information to interact, and what architectural features might cause this interaction to favor language priors over visual information?

- **Concept:** Contrastive decoding and probability truncation strategies
  - **Why needed here:** DeCo builds on contrastive decoding principles but applies them in a novel way using preceding-layer knowledge; understanding truncation strategies like top-p is essential
  - **Quick check question:** What is the difference between top-k and top-p truncation, and how does the choice of truncation strategy affect the candidate token selection in DeCo's approach?

- **Concept:** Knowledge priors in language models and their influence on generation
  - **Why needed here:** The core mechanism relies on understanding how pre-existing knowledge in the language model can override or suppress visual information from the image encoder
  - **Quick check question:** How do language model priors typically influence token generation probabilities, and what mechanisms could cause these priors to specifically suppress correct visual information?

## Architecture Onboarding

- **Component map:** Visual encoder → Projection layer → Concatenation with text tokens → 32-layer transformer decoder → Affine projection to vocabulary → DeCo correction module (optional)
- **Critical path:** Visual tokens → Early transformer layers (20-28) → Candidate token selection → Anchor layer identification → Logit correction → Final token generation
- **Design tradeoffs:**
  - Layer interval selection (20-28) balances computational efficiency with hallucination mitigation effectiveness
  - Dynamic soft modulation coefficient prevents hard changes but may reduce correction strength
  - Model-agnostic design enables broad applicability but may miss model-specific optimizations
- **Failure signatures:**
  - If hallucinations increase despite DeCo application: likely issues with anchor layer selection or modulation coefficient
  - If output becomes incoherent: possible over-correction from preceding-layer influence
  - If performance matches baseline: likely issues with candidate token acquisition or layer interval selection
- **First 3 experiments:**
  1. **Layer interval ablation:** Test different layer intervals (e.g., 15-25, 25-32) to identify optimal range for anchor layer selection
  2. **Modulation coefficient sensitivity:** Vary α from 0.1 to 1.0 to find optimal balance between correction strength and output coherence
  3. **Model compatibility test:** Apply DeCo to different MLLM architectures (different sizes, different vision backbones) to verify model-agnostic claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which LLM priors suppress visual information in deeper layers of MLLMs?
- Basis in paper: [explicit] The paper demonstrates that MLLMs recognize objects in early layers but generate hallucinations in final layers, and that this suppression is due to LLM priors reducing probabilities of ground truth tokens in deeper layers.
- Why unresolved: While the paper shows correlation between layer depth and hallucination probability, it does not explain the specific neural or architectural mechanism (e.g., attention patterns, residual connections, or token mixing) that causes this suppression.
- What evidence would resolve it: Detailed attention weight analysis across layers showing how visual token information is diluted or overridden by textual priors, or ablation studies isolating specific architectural components responsible for the suppression.

### Open Question 2
- Question: Does the Dynamic Correction Decoding approach generalize to larger MLLMs (e.g., 30B+ parameters) and different vision encoder resolutions?
- Basis in paper: [inferred] The paper tests DeCo on 7B parameter models and mentions that visual resolution scaling improves performance, but does not evaluate scalability to larger models or systematically test different vision encoder resolutions.
- Why unresolved: The paper's experiments are limited to 7B parameter models, and the impact of scaling both the language model size and vision encoder resolution on DeCo's effectiveness is unknown.
- What evidence would resolve it: Experiments applying DeCo to 30B+ parameter MLLMs and testing across a range of vision encoder resolutions (e.g., 224px, 336px, 512px) to determine if performance gains scale proportionally.

### Open Question 3
- Question: How does DeCo's performance compare when integrated with more advanced decoding strategies beyond greedy, beam search, and nucleus sampling?
- Basis in paper: [explicit] The paper integrates DeCo with greedy decoding, beam search, and nucleus sampling, showing consistent improvements, but does not test other decoding methods like contrastive decoding or top-k sampling.
- Why unresolved: The paper only evaluates three common decoding strategies, leaving open the question of whether DeCo's effectiveness holds across the broader space of decoding methods.
- What evidence would resolve it: Empirical comparison of DeCo's hallucination reduction and output quality when combined with alternative decoding strategies such as contrastive decoding, top-k sampling, or dynamic blocking.

## Limitations

- The anchor layer selection mechanism (layers 20-28) appears somewhat arbitrary without systematic justification for why these specific layers would contain the most reliable visual information
- The dynamic modulation coefficient approach, while elegant in concept, lacks rigorous theoretical grounding for why maximum probability is the optimal scaling factor
- The paper only tests four specific MLLM architectures, raising questions about the true model-agnostic nature of the approach and whether the 20-28 layer recommendation generalizes across different architectures

## Confidence

**High Confidence:** The empirical results demonstrating hallucination reduction (up to 17.6% on CHAIR, 18.9% F1 improvement on POPE) are well-supported by the experimental data presented.

**Medium Confidence:** The proposed mechanism explaining why MLLMs hallucinate - that visual information is recognized in early layers but suppressed by language priors in later layers - is plausible based on the evidence provided.

**Low Confidence:** The claim that DeCo is "model-agnostic" and can be applied to any MLLM without architectural modifications is not fully validated given the limited model diversity in experiments.

## Next Checks

1. **Layer Interval Sensitivity Analysis:** Systematically vary the anchor layer selection interval (e.g., 15-25, 25-32, 10-20) across different model sizes (1B, 7B, 13B, 34B) to determine whether the 20-28 recommendation is optimal or merely coincidental for the tested models.

2. **Architectural Ablation Study:** Implement DeCo on MLLMs with different architectural designs (different vision backbones, different fusion mechanisms, different layer counts) to test the true model-agnostic nature of the approach and identify any architectural constraints.

3. **Cross-Modality Attention Analysis:** Conduct a detailed analysis of cross-attention patterns across layers to directly visualize whether visual information is indeed being suppressed in later layers, using attention visualization techniques to map how visual tokens are processed throughout the network.