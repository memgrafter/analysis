---
ver: rpa2
title: Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance
  on In-Context Learning Tasks
arxiv_id: '2411.03945'
source_url: https://arxiv.org/abs/2411.03945
tags:
- gpt-2
- regression
- mamba
- linear
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how architectural changes in sequence models
  affect in-context learning (ICL) performance. The authors extend prior work by testing
  hybrid architectures combining GPT-2, Llama, and Mamba components across six regression
  tasks.
---

# Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks

## Quick Facts
- arXiv ID: 2411.03945
- Source URL: https://arxiv.org/abs/2411.03945
- Reference count: 40
- Some hybrid architectures converge to suboptimal predictors instead of optimal ICL solutions

## Executive Summary
This paper investigates how architectural modifications in sequence models affect in-context learning (ICL) performance. The authors extend prior work by testing hybrid architectures combining GPT-2, Llama, and Mamba components across six regression tasks. They introduce the ICL regression score, a metric summarizing model performance relative to baselines. Experiments reveal that certain architectural modifications lead to suboptimal convergence or reduced training efficiency. Some hybrid models show degraded ICL accuracy, while others exhibit potential improvements. The study provides a modular codebase to facilitate reproducible research and suggests that architectural compatibility and inductive biases significantly influence ICL ability.

## Method Summary
The study systematically replaces three architectural components—positional embeddings, feed-forward networks, and normalization layers—across GPT-2, Llama, and Mamba base architectures. Nine hybrid models plus three unmodified baselines are trained on six regression tasks for up to 500k steps using ADAM optimizer. Performance is evaluated using a novel ICL regression score that normalizes model error against baseline predictors across different context lengths. The modular codebase allows component-level architectural changes to isolate effects on ICL performance.

## Key Results
- Certain architectural modifications cause models to converge to suboptimal predictors instead of optimal ICL solutions
- GPT-2 SwiGLU on Sparse Linear Regression initially adopts least squares scheme but eventually unlearns it with extended training
- ICL regression score effectively captures qualitative differences in model performance across tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain hybrid architectures converge to suboptimal predictors instead of optimal ICL solutions
- Mechanism: Architectural modifications introduce inductive biases that favor simpler but less accurate regression schemes during training
- Core assumption: Training dynamics are sensitive to architectural details in ways that affect which local minima models converge to
- Evidence anchors:
  - [abstract] "certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors"
  - [section] "We find that some model-task pairs produce suboptimal predictions not as a result of insufficient training"
  - [corpus] Weak - no direct corpus evidence found for this specific convergence behavior
- Break condition: If training duration is extended significantly or if architectural modifications are reverted to original configurations

### Mechanism 2
- Claim: Models can escape suboptimal regression schemes through extended training
- Mechanism: Initial convergence to simpler solutions can be overcome as training progresses, allowing models to discover more complex optimal solutions
- Core assumption: The loss landscape has pathways from suboptimal to optimal solutions that become accessible over time
- Evidence anchors:
  - [abstract] "We also find certain hybrids showing optimistic performance improvements"
  - [section] "GPT-2 SwiGLU (model 1.3) on Sparse Linear Regression adopts a suboptimal regression scheme (least squares) partway in training, eventually unlearning this scheme"
  - [corpus] Weak - limited corpus evidence for this specific unlearning phenomenon
- Break condition: If training is terminated before sufficient steps for unlearning to occur, or if architectural constraints permanently block optimal solution pathways

### Mechanism 3
- Claim: ICL regression score effectively captures qualitative differences in model performance across tasks
- Mechanism: By normalizing model error against baseline and zero estimators across context lengths, the metric provides a scalar summary that reflects overall ICL ability
- Core assumption: The baseline estimator represents the best possible performance for each task, and deviations from it meaningfully indicate ICL capability
- Evidence anchors:
  - [abstract] "we propose the 'ICL regression score', a scalar metric describing a model's whole performance on a specific task"
  - [section] "ICL Regression Scores reflect qualitative information contained in squared-error plots"
  - [corpus] Weak - limited external validation of this specific metric design
- Break condition: If baseline estimators are poorly chosen or if tasks have fundamentally different difficulty scales that aren't captured by normalization

## Foundational Learning

- Concept: In-Context Learning (ICL) fundamentals
  - Why needed here: The entire study examines how architectural changes affect a model's ability to learn from context without parameter updates
  - Quick check question: What distinguishes ICL from traditional fine-tuning in terms of parameter updates and learning mechanism?

- Concept: Transformer architecture components and their roles
  - Why needed here: The paper systematically replaces components (positional embeddings, FFN, normalization) to understand their impact on ICL
  - Quick check question: How do absolute positional embeddings differ functionally from rotary positional embeddings in transformers?

- Concept: Regression analysis and error metrics
  - Why needed here: The evaluation framework relies on understanding squared error, baseline predictors, and how to compare model performance across different tasks
  - Quick check question: Why is it important to normalize model error against a baseline when computing the ICL regression score?

## Architecture Onboarding

- Component map: The architectures explored consist of three modifiable blocks: positional embeddings (absolute, rotary, or none) -> feed-forward network (MLP with GELU/SwGLU activations or Mamba Mixer) -> normalization (LayerNorm or RMSNorm). These can be mixed and matched across GPT-2, Llama, and Mamba base architectures.

- Critical path: For reproducing results, the essential workflow is: (1) define function class and task parameters, (2) select model architecture variant, (3) configure training parameters (optimizer, learning rate, steps), (4) run training with logging, (5) evaluate across context lengths, (6) compute ICL regression score.

- Design tradeoffs: The paper balances architectural exploration with computational constraints - using 12-layer models with ~10M parameters rather than full-scale models allows broader architectural experimentation but may miss phenomena that only emerge at scale.

- Failure signatures: Suboptimal convergence manifests as error profiles that plateau at levels significantly above optimal baselines, or as error profiles that match simpler regression schemes (like least squares) when more sophisticated solutions exist. Complete failure shows consistently high error across all context lengths.

- First 3 experiments:
  1. Train GPT-2 baseline on Linear Regression task to verify replication of Garg et al. results
  2. Train Llama baseline on the same task to establish control for architectural modifications
  3. Create and train GPT-2-Llama hybrid with only positional embeddings changed (absolute to rotary) on Linear Regression to isolate effects of this single modification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific architectural changes consistently improve or degrade ICL performance across multiple function classes?
- Basis in paper: [explicit] The paper identifies that certain architectural modifications lead to suboptimal convergence or reduced training efficiency, while others show potential improvements.
- Why unresolved: The paper only conducted single training runs per model-task pair, making it unclear if observed phenomena are consistent or due to random variation.
- What evidence would resolve it: Multiple independent training runs with statistical analysis of performance differences across architectures and function classes.

### Open Question 2
- Question: What is the relationship between model convergence behavior and the quality of the learned regression scheme?
- Basis in paper: [explicit] The paper observes that some models converge to suboptimal regression schemes (like least squares instead of LASSO) and others fail to converge within the training horizon.
- Why unresolved: The paper only examined models up to 500K training steps and didn't systematically analyze the relationship between convergence speed and regression quality.
- What evidence would resolve it: Extended training experiments with convergence analysis, tracking both training efficiency and regression scheme quality over time.

### Open Question 3
- Question: How do architectural changes affect the inductive biases that influence which regression schemes models prefer?
- Basis in paper: [explicit] The paper notes that certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in extreme convergence times when these solution forms greatly vary from the optimal predictor's form.
- Why unresolved: The paper only examined a limited set of architectures and didn't systematically investigate how specific architectural components influence learned solution forms.
- What evidence would resolve it: Systematic ablation studies varying individual architectural components and measuring their effects on learned regression schemes across multiple function classes.

## Limitations
- Limited to single training runs per model-task pair, introducing uncertainty about result consistency
- Restricted training duration (500k steps) may be insufficient for some architectures to converge to optimal solutions
- Scale limitations using 12-layer models with ~10M parameters rather than full-scale models

## Confidence
- High Confidence: The experimental methodology is clearly specified and reproducible, with modular code architecture and well-defined evaluation procedures
- Medium Confidence: Claims about specific architectural components causing particular failure modes require more extensive ablation studies
- Low Confidence: Speculation about why certain architectures perform better lacks rigorous theoretical grounding

## Next Checks
1. **Extended Training Duration**: Re-run the most problematic model-task pairs for 2-3x the original training steps to determine whether suboptimal convergence is temporary or permanent

2. **Multiple Random Seeds**: Conduct 5-10 training runs for each model-task pair using different random seeds to establish confidence intervals for ICL regression scores

3. **Full-Scale Model Testing**: Select 2-3 promising hybrid architectures and scale them to 24+ layers with ~100M+ parameters to verify whether architectural advantages/disadvantages persist at realistic model sizes