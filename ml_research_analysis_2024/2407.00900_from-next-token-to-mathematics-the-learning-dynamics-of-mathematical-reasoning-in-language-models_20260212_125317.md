---
ver: rpa2
title: 'From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning
  in Language Models'
arxiv_id: '2407.00900'
source_url: https://arxiv.org/abs/2407.00900
tags:
- problems
- training
- skills
- problem
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathCAMPS, a fine-grained synthetic dataset
  of 4,900 mathematical problems grounded in the Common Core curriculum for grades
  K-8. The dataset was constructed using an attribute grammar approach that encodes
  44 Common Core standards, allowing for automatic generation of symbolic problems
  that are then converted into natural language using GPT-4.
---

# From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models

## Quick Facts
- arXiv ID: 2407.00900
- Source URL: https://arxiv.org/abs/2407.00900
- Authors: Shubhra Mishra; Gabriel Poesia; Noah D. Goodman
- Reference count: 40
- Primary result: Models learn mathematical skills in an order correlating with human curriculum despite random training data ordering

## Executive Summary
This paper introduces MathCAMPS, a synthetic dataset of 4,900 mathematical problems aligned with Common Core standards K-8, to analyze how language models develop mathematical reasoning skills during pre-training and post-training. The dataset is generated using attribute grammars and validated through cycle-consistency to ensure problem quality. The authors evaluate multiple open-weight models across training checkpoints, revealing that mathematical skills evolve smoothly rather than discretely emerging, with skill acquisition order correlating with the human curriculum. The study also finds that instruction tuning has variable effects on different mathematical skills, improving some while hurting others, and that models show robust reasoning abilities that improve with follow-up questions.

## Method Summary
The paper constructs MathCAMPS by encoding 44 Common Core standards as attribute grammars, generating symbolic math problems that are converted to natural language using GPT-4 and validated via cycle-consistency