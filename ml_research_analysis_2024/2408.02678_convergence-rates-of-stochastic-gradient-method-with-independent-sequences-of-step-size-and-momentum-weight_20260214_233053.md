---
ver: rpa2
title: Convergence rates of stochastic gradient method with independent sequences
  of step-size and momentum weight
arxiv_id: '2408.02678'
source_url: https://arxiv.org/abs/2408.02678
tags:
- convergence
- momentum
- learning
- step-size
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes convergence rates of stochastic gradient method
  with momentum for strongly convex functions. The authors consider two step-size
  sequences: diminishing-to-zero and constant-and-drop.'
---

# Convergence rates of stochastic gradient method with independent sequences of step-size and momentum weight

## Quick Facts
- **arXiv ID**: 2408.02678
- **Source URL**: https://arxiv.org/abs/2408.02678
- **Reference count**: 40
- **Primary result**: Proves convergence rates for stochastic gradient method with momentum, showing that independent sequences of step-size and momentum weight can achieve convergence with different rates depending on decay conditions.

## Executive Summary
This paper provides a theoretical analysis of convergence rates for the stochastic gradient method with momentum (SGM) applied to strongly convex functions. The authors examine two step-size strategies - diminishing-to-zero and constant-and-drop - and derive convergence bounds that depend on both step-size and momentum weight sequences. The analysis reveals that SGM achieves the same worst-case convergence order as standard stochastic gradient descent (SGD) when momentum weights decay sufficiently fast, while maintaining practical acceleration benefits. The paper also establishes conditions for convergence in the constant-and-drop strategy and justifies the use of default momentum settings in popular machine learning frameworks.

## Method Summary
The paper analyzes stochastic gradient method with momentum for strongly convex functions using two distinct step-size strategies. For diminishing-to-zero step-size sequences (satisfying ∑t_i = ∞ and ∑t_i² < ∞), the authors derive convergence bounds showing that the rate depends on both the exponential decay from step-size and polynomial growth from momentum weights. For constant-and-drop strategies, they prove convergence when momentum weights satisfy ∑η_i/(j+1) → 0 at each stage. The analysis assumes strongly convex functions over compact convex sets with bounded sub-gradients and uses projection operators to maintain iterates within feasible regions.

## Key Results
- Convergence rate for diminishing-to-zero step-size is O(e^(-∑t_i)(1+∑η_i)), showing exponential dependence on step-size and polynomial dependence on momentum
- For constant-and-drop step-size strategy, mean-squared error converges when momentum weights satisfy ∑η_i/(j+1) → 0 at each stage
- The analysis justifies default momentum settings in machine learning software by proving worst-case convergence order equivalence with SGD
- Independent sequences of step-size and momentum weight achieve optimal convergence rates under appropriate decay conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The diminishing-to-zero step-size sequence with momentum converges because the exponential decay in the error bound dominates the polynomial growth from momentum terms when momentum decays sufficiently fast.
- Mechanism: The error bound combines an exponential decay term e^(-m*∑t_i) from the standard SGD analysis with a polynomial factor (1 + ∑η_i) from momentum. When ∑η_i converges (fast momentum decay), the exponential dominates and convergence occurs. When ∑η_i diverges slowly (η_i = 1/(i+1)^β with β ≤ 1), the convergence rate degrades but remains convergent.
- Core assumption: The momentum weights form a non-increasing diminishing-to-zero sequence that satisfies e^(-m*∑t_i) * ∑η_i → 0 as N → ∞.
- Evidence anchors:
  - [abstract]: "For the former, we show that the convergence rate can be written as a product of exponential in step-size and polynomial in momentum weight."
  - [section 3.1]: "Our analysis reveals that the convergence rate of SGM is the best as good as that of SG."
  - [corpus]: Weak evidence - related papers discuss convergence but don't directly support this specific mechanism.
- Break condition: If momentum weights decay too slowly such that e^(-m*∑t_i) * ∑η_i does not approach zero, or if the step-size sequence doesn't satisfy ∑t_i = ∞ and ∑t_i² < ∞.

### Mechanism 2
- Claim: In constant-and-drop learning rate strategy, convergence occurs when the momentum weight sequence satisfies ∑η_i/(j+1) → 0 at each stage, allowing the suffix average update to converge.
- Mechanism: At each stage with constant step-size a, the suffix average update accumulates momentum effects across iterations. The condition ∑η_i/(j+1) → 0 ensures that the momentum contribution becomes negligible relative to the step-size, allowing the average to converge to the optimal solution within the stage.
- Core assumption: Momentum weights at each stage satisfy η_j < 1 and ∑η_i/(j+1) → 0 as j approaches infinity, with step-size a satisfying 1/m > a > 0.
- Evidence anchors:
  - [abstract]: "For constant-and-drop step-size, they prove convergence of mean-squared error if momentum weights satisfy ∑η_i/(j+1)→0 at each stage."
  - [section 3.2]: "Our analysis shows that the mean-squared-error sequence can converge, as long as momentum weights η_j at the stage satisfy ∑i=0^η_i/(j+1) → 0 with η_i < 1."
  - [corpus]: Weak evidence - related papers discuss momentum but don't directly support this specific stage-wise convergence mechanism.
- Break condition: If momentum weights don't decay sufficiently fast relative to the number of iterations in a stage, or if step-size is not appropriately chosen within the bounds.

### Mechanism 3
- Claim: The convergence analysis justifies the default momentum settings in machine learning software by showing that even with relatively slow momentum decay, the convergence rate remains practical for deep learning applications.
- Mechanism: The theoretical analysis demonstrates that practical momentum settings (like those in Adam, TensorFlow, and PyTorch) with η_j = k*t_j are justified because they provide a balance between convergence speed and stability, even though the worst-case analysis shows SG can achieve the same order of convergence.
- Core assumption: The practical momentum settings used in software implementations satisfy the convergence conditions derived in the theoretical analysis, even if not optimal in the worst-case sense.
- Evidence anchors:
  - [abstract]: "Our analysis justifies the convergence of using the default momentum weight setting and the diminishing-to-zero step-size sequence in large-scale machine learning software."
  - [section 1.1]: "Our analysis supports the default setting η_j = k*t_j in Adam [14], TensorFlow [15], and PyTorch [16] software."
  - [corpus]: Weak evidence - related papers discuss practical implementations but don't directly support this specific justification mechanism.
- Break condition: If practical settings violate the theoretical conditions, or if the objective functions in DNNs have properties significantly different from the assumptions (strongly convex, bounded gradients).

## Foundational Learning

- Concept: Strongly convex functions and their properties
  - Why needed here: The entire convergence analysis relies on the m-strongly convex assumption to derive error bounds and equivalence between function sub-optimality and mean-squared error convergence.
  - Quick check question: What property of strongly convex functions allows the equivalence between E{f(θ_j) - f(θ*)} → 0 and E{∥θ_j - θ*∥²} → 0?

- Concept: Stochastic approximation and unbiased gradient estimation
  - Why needed here: The analysis assumes g(θ_j; z_j) = s_j + n_j where n_j is zero-mean noise, which is fundamental to deriving the convergence bounds for both SGD and SGM.
  - Quick check question: Why is the assumption that the variance of n_j is bounded by σ² crucial for the convergence analysis?

- Concept: Polyak's momentum and its acceleration properties
  - Why needed here: The paper analyzes Polyak's momentum specifically, and understanding how momentum accumulates velocity in directions of persistent reduction is key to interpreting the convergence results.
  - Quick check question: How does Polyak's momentum differ from Nesterov's momentum in terms of information usage for the next update?

## Architecture Onboarding

- Component map:
  - Assumptions module -> Handles m-strongly convex functions, bounded gradients, compact convex set D
  - Step-size controller -> Manages diminishing-to-zero (∑t_i = ∞, ∑t_i² < ∞) and constant-and-drop strategies
  - Momentum weight scheduler -> Implements non-increasing diminishing-to-zero sequences or stage-wise decay
  - Error bound calculator -> Computes the exponential-polynomial product bound E{∥θ_N - θ*∥²} ≤ c₀e^(-(m∑t_i + m²/2∑t_i²))(1 + ∑η_i)
  - Convergence checker -> Verifies conditions like e^(-m∑t_i)∑η_i → 0 or ∑η_i/(j+1) → 0

- Critical path:
  1. Initialize θ_0 in compact convex set D
  2. Select step-size sequence (diminishing-to-zero or constant-and-drop)
  3. Choose momentum weight sequence satisfying convergence conditions
  4. At each iteration j: compute g(θ_j; z_j), update θ_{j+1} = PD(θ_j - t_j*g(θ_j; z_j) + η_j(θ_j - θ_{j-1}))
  5. Monitor error bound and verify convergence conditions

- Design tradeoffs:
  - Fast momentum decay (∑η_i < ∞) gives same convergence order as SGD but may lose acceleration benefits
  - Slow momentum decay (η_i = 1/(i+1)^β with β ≤ 1) maintains practical performance but has inferior worst-case rate
  - Constant-and-drop strategy allows larger initial step-sizes but requires careful stage management

- Failure signatures:
  - Divergence: Momentum weights don't satisfy ∑η_i/(j+1) → 0 in constant-and-drop strategy
  - Slow convergence: Momentum weights decay too fast, losing acceleration benefits
  - Instability: Step-size sequence violates ∑t_i = ∞ or ∑t_i² < ∞ conditions
  - Poor performance: Actual function properties differ significantly from strongly convex assumption

- First 3 experiments:
  1. Verify diminishing-to-zero convergence: Run SGM with η_j = 1/(j+1) and t_j = 1/(m(j+1)), measure E{∥θ_N - θ*∥²} vs N to confirm O(log(N+1)/(N+1)) rate
  2. Test constant-and-drop strategy: Implement stage-wise SGM with fixed step-size a and η_j = 1/(j+1)^β, verify convergence when ∑η_i/(j+1) → 0
  3. Compare with SGD baseline: Run both SGD and SGM with same step-size sequence, measure practical convergence speed to assess momentum benefits beyond worst-case analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does SGM converge faster than SG in practice despite having the same worst-case convergence rate?
- Basis in paper: [explicit] The paper shows SGM's worst-case convergence rate is the same as SG, yet acknowledges SGM's practical success in large-scale learning.
- Why unresolved: The paper conjectures reasons (ravines, expected complexity, constant size) but doesn't prove which explanation is correct.
- What evidence would resolve it: Empirical studies comparing SG vs SGM on functions with different geometric properties, or theoretical bounds on expected complexity that favor SGM.

### Open Question 2
- Question: How can we theoretically justify the default momentum weight setting ηj = ktj used in popular ML frameworks?
- Basis in paper: [explicit] The paper justifies this setting for diminishing-to-zero step-sizes but doesn't explain why it works well in practice across different problems.
- Why unresolved: The analysis focuses on worst-case bounds, while practical performance depends on problem structure and data distribution.
- What evidence would resolve it: Empirical validation showing ηj = ktj performs well across diverse problem classes, or theoretical analysis under specific data distributions.

### Open Question 3
- Question: What is the optimal momentum weight sequence for the constant-and-drop learning rate strategy?
- Basis in paper: [explicit] The paper only provides sufficient conditions (∑ηi/(j+1)→0) for convergence, not optimal sequences.
- Why unresolved: The paper proves convergence but doesn't optimize for speed, leaving practical tuning to heuristics.
- What evidence would resolve it: Comparative studies of different momentum sequences (constant, linear decay, polynomial decay) on benchmark problems to identify optimal decay rates.

## Limitations

- The analysis assumes strongly convex functions, which may not hold for deep neural networks where most practical applications of momentum exist.
- The theoretical justification for default momentum settings relies on worst-case analysis that may not reflect practical benefits from problem-specific structures.
- The mechanism by which practical momentum settings achieve acceleration despite worst-case analysis showing no improvement over SGD remains unexplained.

## Confidence

- **High confidence**: Convergence results for diminishing-to-zero step-size when momentum weights satisfy ∑ηi/(j+1) → 0
- **Medium confidence**: Justification of default momentum settings in practice, as this relies on worst-case analysis that may not reflect practical benefits
- **Low confidence**: The mechanism by which practical momentum settings achieve acceleration despite worst-case analysis showing no improvement over SGD

## Next Checks

1. Empirical validation on deep learning benchmarks to test whether the theoretical momentum settings actually improve convergence in practice
2. Extension of analysis to weakly convex or non-convex functions to assess applicability to neural network training
3. Sensitivity analysis varying the decoupling between step-size and momentum weight sequences to test the independence assumption in the convergence proofs