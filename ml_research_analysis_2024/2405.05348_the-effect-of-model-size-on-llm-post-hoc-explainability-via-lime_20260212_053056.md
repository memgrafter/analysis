---
ver: rpa2
title: The Effect of Model Size on LLM Post-hoc Explainability via LIME
arxiv_id: '2405.05348'
source_url: https://arxiv.org/abs/2405.05348
tags:
- explanations
- language
- lime
- comprehensiveness
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how post-hoc explainability methods like
  LIME perform across different model sizes in language models. The authors focus
  on DeBERTaV3 models of varying sizes, testing them on natural language inference
  (NLI) and zero-shot classification (ZSC) tasks.
---

# The Effect of Model Size on LLM Post-hoc Explainability via LIME

## Quick Facts
- **arXiv ID**: 2405.05348
- **Source URL**: https://arxiv.org/abs/2405.05348
- **Reference count**: 11
- **Primary result**: Model performance improves with size, but LIME explanation plausibility does not, suggesting misalignment between explanations and larger models' internal processes.

## Executive Summary
This paper investigates how post-hoc explainability methods like LIME perform across different model sizes in language models. The authors focus on DeBERTaV3 models of varying sizes, testing them on natural language inference (NLI) and zero-shot classification (ZSC) tasks. They evaluate explanations based on faithfulness, which measures how well the explanation reflects the model's internal decision process, and plausibility, which assesses agreement with human explanations. The primary finding is that while model performance improves with size, the agreement between human and LIME-generated explanations does not, suggesting a misalignment between the explanations and the true internal decision processes of larger models. Additionally, the study identifies limitations in faithfulness metrics, particularly comprehensiveness, in the context of NLI tasks, where the removal of important tokens may not significantly alter predictions for neutral sentence pairs.

## Method Summary
The study fine-tunes DeBERTaV3 models of four different sizes (22M to 304M parameters) on MNLI, e-SNLI, and CoS-e datasets. LIME explanations are generated for each model on a subset of 100 test samples from each dataset. Comprehensiveness is calculated as the difference in prediction probability before and after removing top-k important tokens. IOU measures the agreement between generated and human-annotated highlights. The faithfulness and plausibility metrics are then compared across model sizes to analyze the effect of model size on LIME explanations.

## Key Results
- Model performance improves monotonically with increasing model size for all three datasets.
- Increased model size does not correlate with plausibility despite improved model performance.
- Comprehensiveness metric shows limitations in NLI contexts, particularly for neutral sentence pairs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger model size increases faithfulness of LIME explanations.
- Mechanism: As models grow in size, their internal representations become more complex, potentially leading to more distinct and consistent decision processes that LIME can approximate with higher fidelity.
- Core assumption: The local linear approximation used by LIME captures more of the model's decision process as model complexity increases.
- Evidence anchors:
  - [abstract] "The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases."
  - [section] "We find that, as expected, performance improves monotonically with increasing model size for all three datasets. We can conclude that the models' capabilities are different enough to reason about the effect of model size on the LIME explanations."
  - [corpus] No direct corpus evidence on faithfulness improving with size, but related work on post-hoc explainability across model sizes supports this direction.
- Break condition: If the complexity of larger models' internal processes exceeds what local linear approximations can capture, faithfulness may plateau or decrease.

### Mechanism 2
- Claim: Plausibility (agreement with human explanations) does not improve with model size.
- Mechanism: Human reasoning and LLM decision processes diverge as models become more complex, leading to explanations that may be faithful but not plausible.
- Core assumption: Human explanations represent a consistent reference point that does not scale with model size.
- Evidence anchors:
  - [abstract] "The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases."
  - [section] "IOU, on the other hand, stays almost constant across all model sizes for both datasets indicating that the plausibility of the LIME explanations is uncorrelated with model size."
  - [corpus] "Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales" suggests plausibility may not scale with performance.
- Break condition: If human explanations evolve alongside model capabilities or if new explanation methods better capture human reasoning patterns.

### Mechanism 3
- Claim: Comprehensiveness metric has limitations in NLI tasks, particularly for neutral sentence pairs.
- Mechanism: Removing important tokens from neutral sentence pairs may not significantly alter predictions, limiting the metric's ability to measure faithfulness.
- Core assumption: The semantic content of neutral pairs is more resilient to token removal than contradiction or entailment pairs.
- Evidence anchors:
  - [abstract] "Our results further suggest limitations regarding faithfulness metrics in NLI contexts."
  - [section] "Splitting the metrics by labels could reveal potential flaws with the comprehensiveness metric in the NLI setting as we found significantly lower scores for neutral sentence pairs."
  - [corpus] No direct corpus evidence on comprehensiveness limitations, but related work on faithfulness metrics suggests task dependency.
- Break condition: If alternative faithfulness metrics or token removal strategies prove more effective for neutral pairs.

## Foundational Learning

- Concept: Post-hoc explainability methods
  - Why needed here: Understanding how LIME approximates model decisions is crucial for interpreting the study's findings on faithfulness and plausibility.
  - Quick check question: What is the fundamental difference between faithfulness and plausibility in evaluating explanations?

- Concept: Faithfulness vs. Plausibility
  - Why needed here: The study evaluates explanations using both metrics, which measure different aspects of explanation quality.
  - Quick check question: How does comprehensiveness measure faithfulness, and what are its limitations?

- Concept: Natural Language Inference (NLI)
  - Why needed here: The study uses NLI tasks, which have specific characteristics that may affect explanation metrics.
  - Quick check question: What are the three types of NLI sentence pairs, and how might they differ in terms of explanation faithfulness?

## Architecture Onboarding

- Component map:
  DeBERTaV3 models (22M-304M parameters) -> LIME explainer -> Comprehensiveness and IOU metrics -> Analysis of faithfulness and plausibility across model sizes

- Critical path:
  1. Fine-tune DeBERTaV3 models on NLI tasks
  2. Generate LIME explanations for model predictions
  3. Calculate faithfulness and plausibility metrics
  4. Analyze correlation between model size and explanation quality

- Design tradeoffs:
  - Using perturbation-based LIME vs. gradient-based methods
  - Focusing on faithfulness vs. plausibility
  - Evaluating on NLI tasks vs. other NLP tasks
  - Using smaller models due to computational constraints

- Failure signatures:
  - Faithfulness metrics not improving with model size
  - Plausibility metrics not correlating with model performance
  - Comprehensiveness metric showing task-specific limitations
  - Computational constraints limiting model size exploration

- First 3 experiments:
  1. Replicate the study with a different perturbation-based explainability method (e.g., SHAP)
  2. Evaluate explanations on additional NLP tasks beyond NLI and ZSC
  3. Investigate the effect of RLHF on explanation plausibility metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the faithfulness of LIME explanations change with model size when considering specific classes or labels within the NLI tasks?
- Basis in paper: [explicit] The paper mentions that faithfulness, measured by comprehensiveness, improves with model size for all datasets, but it also identifies limitations of the comprehensiveness metric in NLI contexts, particularly for neutral sentence pairs.
- Why unresolved: The paper does not provide a detailed analysis of how faithfulness varies across different labels within the NLI tasks, such as entailment, contradiction, and neutral.
- What evidence would resolve it: A detailed breakdown of comprehensiveness scores for each label within the NLI tasks across different model sizes would provide insights into how faithfulness varies with model size and label.

### Open Question 2
- Question: How do other post-hoc explainability methods, such as Anchors or SHAP, compare to LIME in terms of faithfulness and plausibility across different model sizes?
- Basis in paper: [inferred] The paper focuses on LIME as a perturbation-based post-hoc explainability method but acknowledges that other techniques might present different challenges and opportunities for explainability.
- Why unresolved: The paper only investigates LIME and does not compare its performance with other explainability methods like Anchors or SHAP.
- What evidence would resolve it: Conducting experiments using Anchors and SHAP on the same datasets and model sizes as LIME, and comparing their faithfulness and plausibility scores, would provide insights into the relative effectiveness of these methods.

### Open Question 3
- Question: How does the inclusion of Reinforcement Learning from Human Feedback (RLHF) affect the plausibility of explanations generated by larger language models?
- Basis in paper: [inferred] The paper suggests that the results on plausibility might significantly change with RLHF, indicating that this is an area of potential impact but not yet explored.
- Why unresolved: The paper does not explore the effects of RLHF on the plausibility of explanations, leaving a gap in understanding how human feedback might align model explanations with human understanding.
- What evidence would resolve it: Training models with RLHF and evaluating the plausibility of their explanations using IOU scores compared to models without RLHF would reveal the impact of human feedback on explanation alignment.

## Limitations
- Computational constraints limited exploration to DeBERTaV3 models up to 304M parameters, preventing generalization to larger LLMs.
- Comprehensiveness metric showed significant limitations in NLI tasks, particularly for neutral sentence pairs where token removal may not meaningfully affect predictions.
- Human annotation quality may not perfectly represent LLM reasoning processes, especially as model complexity increases.

## Confidence
- High confidence: The observation that model performance improves monotonically with size while explanation plausibility remains constant.
- Medium confidence: The conclusion about comprehensiveness metric limitations in NLI tasks.
- Medium confidence: The suggestion that larger models may have more complex internal processes that LIME cannot adequately capture.

## Next Checks
1. Expand model size range: Replicate the study with larger models (1B+ parameters) when computational resources permit, to test whether the observed trends continue or reverse at scale.
2. Alternative faithfulness metrics: Evaluate explanations using metrics that don't rely on token removal (e.g., causal tracing or activation-based methods) to determine if comprehensiveness limitations are method-specific.
3. Cross-task validation: Apply the same experimental framework to text generation tasks or other NLP domains to assess whether the observed patterns hold beyond NLI and classification.