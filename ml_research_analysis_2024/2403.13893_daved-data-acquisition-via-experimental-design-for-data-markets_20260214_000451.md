---
ver: rpa2
title: 'DAVED: Data Acquisition via Experimental Design for Data Markets'
arxiv_id: '2403.13893'
source_url: https://arxiv.org/abs/2403.13893
tags:
- data
- test
- should
- error
- buyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a federated data acquisition framework for
  data markets that enables buyers to select the most valuable seller data points
  for their specific prediction tasks. The core method, DA VED, uses linear experimental
  design to directly optimize data selection based on unlabeled test queries, eliminating
  the need for labeled validation data that causes overfitting in existing approaches.
---

# DAVED: Data Acquisition via Experimental Design for Data Markets

## Quick Facts
- arXiv ID: 2403.13893
- Source URL: https://arxiv.org/abs/2403.13893
- Reference count: 40
- Primary result: Federated data acquisition framework that selects most valuable seller data points for buyer's prediction tasks without requiring labeled validation data

## Executive Summary
This paper introduces DAVED (Data Acquisition via Experimental Design), a federated framework for data markets that enables buyers to select optimal training data from multiple sellers without requiring labeled validation data. The core innovation is using linear experimental design to directly optimize data selection based on unlabeled test queries, eliminating the overfitting problems inherent in validation-based approaches. DAVED employs a fast, distributed Frank-Wolfe optimization procedure that scales to large datasets and can be implemented in a federated manner. Experiments on synthetic and real-world medical datasets demonstrate that DAVED achieves lower prediction error than state-of-the-art data valuation methods while requiring significantly less runtime.

## Method Summary
DAVED uses linear experimental design to construct a proxy loss function that approximates expected test error using only unlabeled test queries and training data. The method employs Frank-Wolfe optimization with Sherman-Morrison updates for efficient distributed computation. It operates in two variants: a multi-step iterative optimization for maximum accuracy, and a single-step variant for extremely fast runtime. The framework assumes conditional distribution Dy|x is identical across training and test sets, allowing direct optimization without labeled validation data.

## Key Results
- DAVED achieves lower test error than Data Shapley, DVRL, and other baselines on multiple datasets
- Single-step variant provides 10-100x speedup while maintaining competitive accuracy
- Runtime scales favorably with dataset size, showing O(n^2) complexity
- Performance particularly strong when budgets are small relative to data dimensionality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DA VED directly optimizes data selection for test set prediction without requiring labeled validation data, eliminating overfitting from "inference after selection."
- **Mechanism**: Uses linear experimental design to build a proxy loss function that approximates expected test error using only unlabeled test queries and training data.
- **Core assumption**: The conditional distribution Dy|x is identical across Ztrain and Ztest, allowing use of V-optimal experiment design framework.
- **Evidence anchors**:
  - [abstract]: "Our proposed data acquisition method achieves lower prediction error without requiring labeled validation data"
  - [section 2]: "Our proposed DA VED achieves lower test error as more seller training data is selected"
  - [corpus]: Weak - corpus contains related market simulation papers but no direct experimental design validation
- **Break condition**: If the conditional distribution Dy|x differs between train and test sets, the problem becomes intractable as the same data point could have very different labels.

### Mechanism 2
- **Claim**: The Frank-Wolfe optimization procedure provides provable approximation guarantees while being amenable to federated implementation.
- **Mechanism**: Continuous relaxation of discrete selection problem followed by iterative Frank-Wolfe updates that communicate only O(d) per round using Sherman-Morrison updates.
- **Core assumption**: The curvature constant Cl is finite, meaning the algorithm and optimum are bounded away from the boundary of the constraint set.
- **Evidence anchors**:
  - [section 4]: "we can use the theory from [8, 27] to analyze the above procedure and show the following" with Theorem 2 providing O(log t/t) approximation guarantee
  - [section 4]: "The bottleneck to efficiently implementing (6) is computing the gradient" followed by Sherman-Morrison update formula
  - [corpus]: Weak - corpus lacks specific Frank-Wolfe implementation details for experimental design
- **Break condition**: If the problem dimension d is extremely large relative to the budget, the communication cost O(d) per round may become prohibitive.

### Mechanism 3
- **Claim**: The single-step variant provides extremely fast runtime while maintaining reasonable performance compared to multi-step optimization.
- **Mechanism**: Linear approximation of the cost function that selects top-k datapoints under budget constraint without iterative optimization.
- **Core assumption**: The linear approximation of the cost function is sufficiently accurate for the selection task.
- **Evidence anchors**:
  - [section 4]: "We can also forgo the iterative process and instead linearly approximate the cost function" with Equation 11
  - [section 5]: Figure 3 shows single-step variant has lower test errors than other baselines while Figure 5 shows it has fastest runtime
  - [corpus]: Weak - corpus contains no direct comparison of single-step vs iterative approaches
- **Break condition**: If the budget is very small relative to dimensionality, the linear approximation may miss important selection nuances that iterative optimization would capture.

## Foundational Learning

- **Concept**: Linear experimental design and Fisher information matrix
  - **Why needed here**: Provides the theoretical foundation for constructing the proxy loss function that approximates expected test error without labels
  - **Quick check question**: Why does the Fisher information matrix I(w) appear in the expected test error formula for linear regression?

- **Concept**: Kernelized linear regression and Neural Tangent Kernel (eNTK)
  - **Why needed here**: Allows modeling complex deep learning training dynamics with linear approximations when feature extractor ϕ is chosen appropriately
  - **Quick check question**: How does the empirical Neural Tangent Kernel approximate fine-tuning dynamics of pre-trained models?

- **Concept**: Frank-Wolfe optimization and its convergence properties
  - **Why needed here**: Enables efficient approximation of the NP-hard discrete optimization problem with provable guarantees while supporting federated implementation
  - **Quick check question**: What is the significance of the curvature constant Cl in Frank-Wolfe convergence analysis?

## Architecture Onboarding

- **Component map**: Data buyer -> Platform -> Data sellers
- **Critical path**: 
  1. Buyer submits Xtest and B to platform
  2. Platform runs DA VED optimization (Algorithm 4)
  3. Platform selects seller data based on weights wT
  4. Platform trains regression model on selected data
  5. Platform makes predictions on buyer's test data

- **Design tradeoffs**:
  - Multi-step vs single-step: accuracy vs runtime
  - Regularization strength λReg: stability vs overfitting
  - Feature extractor choice: approximation quality vs computational cost
  - Number of optimization steps: convergence vs efficiency

- **Failure signatures**:
  - Poor test error despite large budget: potential distributional shift between train/test
  - Runtime degradation: check dimensionality d vs budget ratio
  - Communication failures: verify Sherman-Morrison update implementation
  - Numerical instability: check regularization λReg and initial conditions

- **First 3 experiments**:
  1. Gaussian synthetic data with homogeneous costs: verify basic functionality and compare to baselines
  2. MIMIC dataset with varying regularization: tune λReg parameter for optimal performance
  3. Real image data (RSNA/Fitzpatrick) with single-step variant: test scalability and practical performance

## Open Questions the Paper Calls Out
The paper identifies several limitations and future directions, including the need for privacy guarantees for both buyers and sellers, extension to non-linear feature representations beyond kernelized linear regression, and integration of local steps like FedAvg or Scaffold for improved communication efficiency. The authors also note that the current implementation lacks formal privacy guarantees and could benefit from extensions to handle more complex model classes and optimization procedures.

## Limitations
- No analysis of distributional shift between buyer test data and seller training data
- Computational complexity scaling poorly when dimensionality approaches budget size
- Reliance on accurate feature extraction without validation of embedding quality

## Confidence

- Eliminating validation data requirement: **Medium** confidence - theoretically sound but assumes identical conditional distributions
- Frank-Wolfe optimization guarantees: **High** confidence for continuous relaxation, but discrete gap unquantified
- Single-step runtime claims: **High** confidence based on Figure 5, but accuracy tradeoffs not fully characterized

## Next Checks

1. **Distributional Robustness**: Run experiments with controlled covariate shift between Ztrain and Ztest to quantify performance degradation when the conditional distribution assumption breaks.

2. **Scaling Analysis**: Systematically vary the ratio d/B to identify the threshold where Sherman-Morrison updates become computationally prohibitive.

3. **Embedding Sensitivity**: Compare performance using different feature extractors (eNTK vs raw features vs alternative pre-trained models) to assess dependence on embedding quality.