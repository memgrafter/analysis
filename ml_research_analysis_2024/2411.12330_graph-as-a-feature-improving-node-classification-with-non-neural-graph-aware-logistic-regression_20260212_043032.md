---
ver: rpa2
title: 'Graph as a feature: improving node classification with non-neural graph-aware
  logistic regression'
arxiv_id: '2411.12330'
source_url: https://arxiv.org/abs/2411.12330
tags:
- graph
- node
- homophily
- networks
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graph-aware Logistic Regression (GLR), a
  non-neural model that leverages both node features and graph topology for node classification.
  Unlike traditional methods that use only one type of information, GLR concatenates
  each node's neighborhood representation from the adjacency matrix with its feature
  vector before applying logistic regression.
---

# Graph as a feature: improving node classification with non-neural graph-aware logistic regression

## Quick Facts
- arXiv ID: 2411.12330
- Source URL: https://arxiv.org/abs/2411.12330
- Authors: Simon Delarue; Thomas Bonald; Tiphaine Viard
- Reference count: 26
- Primary result: GLR outperforms both traditional graph algorithms and sophisticated GNNs while achieving up to two orders of magnitude faster computation

## Executive Summary
This paper introduces Graph-aware Logistic Regression (GLR), a non-neural model that leverages both node features and graph topology for node classification. Unlike traditional methods that use only one type of information, GLR concatenates each node's neighborhood representation from the adjacency matrix with its feature vector before applying logistic regression. Extensive experiments on diverse datasets show GLR outperforms both traditional graph algorithms and sophisticated GNNs while achieving up to two orders of magnitude faster computation. The model demonstrates strong generalization across various graph characteristics including homophily levels.

## Method Summary
GLR works by concatenating each node's adjacency row (representing its neighborhood) with its feature vector, creating an augmented feature representation that captures both topology and attributes. This concatenated representation is then fed into a standard logistic regression classifier. The method leverages information from multiple levels of the original graph - the node's own features and its local neighborhood structure - without requiring expensive message passing or complex neural architectures. The model learns to balance the importance of feature information versus topological information during training.

## Key Results
- GLR achieves up to 13% higher accuracy than GCN on heterophilous datasets
- Computation time is 10-100x faster than GNNs on benchmark datasets
- Strong performance across all tested datasets regardless of homophily level
- Outperforms traditional graph algorithms like PageRank and Label Propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLR outperforms GNNs by avoiding neighborhood aggregation in favor of direct feature concatenation.
- Mechanism: Instead of message passing that aggregates signals from neighbors, GLR concatenates each node's adjacency row (neighborhood representation) with its feature vector, preserving raw feature information while incorporating graph topology.
- Core assumption: When node features contain substantial information, message passing can introduce noise and dilute useful signals.
- Evidence anchors:
  - [abstract] "unlike neural methods, GLR offers significant benefits in terms of simplicity, computational efficiency, and ease of hyperparameter tuning"
  - [section] "Unlike the message passing scheme, the proposed architecture does not involves signal aggregation from direct neighbourhood. While such signal aggregation can be desirable in homophilous graphs, it may introduce uninformative and noisy representations in heterophilous settings."
- Break condition: If graph topology is the primary signal and node features are sparse or uninformative, message passing aggregation may outperform concatenation.

### Mechanism 2
- Claim: GLR achieves better generalization by learning to balance feature and topology weights dynamically.
- Mechanism: The logistic regression learns optimal weights for both the neighborhood representation and node features during training, allowing the model to emphasize whichever signal is more informative for each dataset.
- Core assumption: Different graphs have varying proportions of useful information in features vs. topology, and a single model should adapt to these differences.
- Evidence anchors:
  - [abstract] "Our method leverages information from multiple levels of the original graph"
  - [section] "In the presence of strong homophily, our model may gives more importance to the neighbourhood structure, and conversely, more emphasis will be placed on the node's features in the presence of strong heterophily."
- Break condition: If the dataset has extreme homophily or heterophily, the learned balance may still not match specialized architectures designed for those specific conditions.

### Mechanism 3
- Claim: GLR's computational efficiency stems from avoiding expensive message passing operations.
- Mechanism: GLR requires O(n(n + L)) time complexity where n is number of nodes and L is feature dimension, while GNNs require O(mL + nLd) per layer due to neighborhood aggregation and weight matrix multiplication.
- Core assumption: For large graphs with many nodes and high-dimensional features, the difference in computational complexity becomes significant.
- Evidence anchors:
  - [section] "With GLR, the training time complexity is inO(n(n +L)), wheren +L corresponds to the number of parameters to learn. In GNNs, one training pass of a single-layer network induces anO(mL +nLd) cost"
  - [abstract] "achieving up to two orders of magnitude faster computation"
- Break condition: For small graphs or low-dimensional features, the computational advantage may not justify using GLR over simpler approaches.

## Foundational Learning

- Concept: Logistic regression fundamentals
  - Why needed here: GLR builds directly on logistic regression as its base model
  - Quick check question: What does the softmax function output in a multi-class logistic regression?

- Concept: Graph topology representation
  - Why needed here: Understanding how adjacency matrices encode node relationships
  - Quick check question: What does each row in an adjacency matrix represent?

- Concept: Feature concatenation and dimensionality
  - Why needed here: GLR combines adjacency rows with feature vectors
  - Quick check question: If a node has a 500-dimensional feature vector and 10 neighbors, what is the dimension of its GLR representation?

## Architecture Onboarding

- Component map: Adjacency matrix + Feature matrix -> Concatenation -> Logistic Regression -> Class probabilities
- Critical path:
  1. Load graph data (adjacency and features)
  2. For each node, extract its row from adjacency matrix
  3. Concatenate with corresponding feature vector
  4. Train logistic regression on concatenated representations
  5. Predict class probabilities for test nodes
- Design tradeoffs:
  - Simplicity vs. expressivity: GLR is simple but may miss complex patterns that GNNs can capture
  - Feature preservation vs. smoothing: GLR preserves raw features while GNNs smooth signals
  - Computational efficiency vs. scalability: GLR is fast but may not scale to extremely large graphs
- Failure signatures:
  - Poor performance when graph structure is the dominant signal and features are weak
  - Memory issues with extremely large feature dimensions
  - Inability to capture long-range dependencies that multi-hop message passing can model
- First 3 experiments:
  1. Compare GLR vs. logistic regression on Cora dataset to verify topology contribution
  2. Test GLR on a synthetic graph with controlled homophily to understand performance bounds
  3. Benchmark GLR against GCN on a large sparse graph to measure computational efficiency gains

## Open Questions the Paper Calls Out
The paper explicitly mentions that future work includes extension of this study to other graph-related tasks beyond node classification.

## Limitations
- Performance depends heavily on quality of node features - may underperform when features are sparse or uninformative
- Limited ability to capture long-range dependencies and complex non-linear patterns that multi-layer GNNs can learn
- Some implementation details like adjacency matrix normalization and feature homophily calculation are not fully specified

## Confidence

**Confidence Labels**:
- High confidence: The core GLR methodology (concatenating adjacency rows with features before logistic regression) and its basic computational complexity analysis
- Medium confidence: The empirical superiority claims across all 13 datasets, as some dataset-specific conditions may affect generalization
- Low confidence: The claim about avoiding "noisy representations in heterophilous settings" without providing detailed ablation studies showing the impact of different normalization schemes

## Next Checks
1. Reproduce the GLR implementation on Cora dataset with controlled adjacency normalization variants to verify the impact of different graph topology representations on performance
2. Conduct ablation studies comparing GLR with and without feature normalization to quantify the contribution of feature preprocessing to overall accuracy
3. Benchmark GLR against GCN on graphs with varying homophily ratios to validate the claim that GLR specifically excels in heterophilous settings while maintaining competitive performance in homophilous scenarios