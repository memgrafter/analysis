---
ver: rpa2
title: A preliminary study on continual learning in computer vision using Kolmogorov-Arnold
  Networks
arxiv_id: '2409.13550'
source_url: https://arxiv.org/abs/2409.13550
tags:
- task
- learning
- networks
- which
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance of Kolmogorov-Arnold Networks
  (KANs) in continual learning scenarios within computer vision, specifically using
  the MNIST dataset. Unlike traditional multi-layer perceptrons (MLPs), KANs are based
  on the Kolmogorov-Arnold Theorem and utilize learnable activation functions on edges
  rather than fixed activations on nodes.
---

# A preliminary study on continual learning in computer vision using Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2409.13550
- Source URL: https://arxiv.org/abs/2409.13550
- Reference count: 10
- Key outcome: This study investigates the performance of Kolmogorov-Arnold Networks (KANs) in continual learning scenarios within computer vision, specifically using the MNIST dataset. Unlike traditional multi-layer perceptrons (MLPs), KANs are based on the Kolmogorov-Arnold Theorem and utilize learnable activation functions on edges rather than fixed activations on nodes. The research compares MLPs with two KAN-based models, ensuring comparable architectures in terms of trainable parameters. The findings reveal that an efficient version of KAN, EffKAN, outperforms both traditional MLPs and the original KAN implementation, achieving 52% accuracy in class-incremental learning tasks. The study also highlights the importance of hyperparameter selection, such as learning rate and grid size, in optimizing KAN performance. Additionally, the research provides insights into the behavior of KAN-based convolutional networks, demonstrating their potential as a viable alternative to traditional convolutional neural networks. The results suggest that KANs, particularly EffKAN, offer promising capabilities for continual learning in complex computer vision tasks.

## Executive Summary
This paper explores the application of Kolmogorov-Arnold Networks (KANs) to continual learning tasks in computer vision. KANs differ from traditional neural networks by placing learnable activation functions on edges rather than nodes, based on the Kolmogorov-Arnold representation theorem. The study evaluates KAN performance against traditional MLPs on the MNIST dataset for class-incremental learning, finding that an efficient KAN variant (EffKAN) achieves 52% accuracy while highlighting the importance of hyperparameter selection for optimal performance.

## Method Summary
The study compares traditional MLPs with two KAN-based models using MNIST dataset for continual learning tasks. The researchers ensure architectural comparability by matching trainable parameters across models. They evaluate both standard KANs and an efficient variant (EffKAN) across class-incremental learning scenarios. The experimental setup systematically varies hyperparameters including learning rate and grid size to assess their impact on KAN performance. The study also investigates KAN-based convolutional networks as potential alternatives to traditional CNNs.

## Key Results
- EffKAN achieved 52% accuracy in class-incremental learning tasks on MNIST
- KANs with learnable edge activations outperformed traditional MLPs in continual learning scenarios
- Hyperparameter selection (learning rate and grid size) significantly impacts KAN performance

## Why This Works (Mechanism)
The Kolmogorov-Arnold Theorem states that any multivariate continuous function can be represented as a finite composition of univariate functions. KANs leverage this by placing learnable univariate functions (activations) on edges rather than using fixed activation functions on nodes. This architecture allows for more flexible function approximation and potentially better handling of catastrophic forgetting in continual learning scenarios. The learnable edge activations can adapt to the specific patterns in each task, reducing interference between tasks learned sequentially.

## Foundational Learning
1. **Kolmogorov-Arnold Theorem** - Mathematical foundation for KANs
   - Why needed: Provides theoretical basis for representing complex functions
   - Quick check: Verify theorem conditions hold for the problem domain

2. **Catastrophic Forgetting** - Core challenge in continual learning
   - Why needed: Defines the problem KANs aim to address
   - Quick check: Measure performance drop when learning new tasks

3. **Class-Incremental Learning** - Specific continual learning scenario tested
   - Why needed: Standard benchmark for evaluating continual learning methods
   - Quick check: Track accuracy on previously seen classes after new classes are learned

4. **Learnable Activation Functions** - Key architectural difference of KANs
   - Why needed: Enables adaptation to task-specific patterns
   - Quick check: Compare with fixed activation functions on same architecture

5. **Hyperparameter Sensitivity** - Impact of learning rate and grid size
   - Why needed: Critical for practical deployment and optimization
   - Quick check: Perform grid search to identify optimal hyperparameter ranges

## Architecture Onboarding
**Component Map**: Input -> KAN Layers (with learnable edge activations) -> Output
**Critical Path**: Data input → Edge activation functions → Linear combinations → Task-specific adaptation → Classification output
**Design Tradeoffs**: KANs trade increased parameter count (learnable activations) for improved function approximation capability and potentially better continual learning performance. The efficient variant (EffKAN) reduces computational overhead while maintaining performance.
**Failure Signatures**: Poor hyperparameter selection leads to catastrophic forgetting; fixed grid sizes may limit expressiveness; excessive parameters can cause overfitting to individual tasks.
**First Experiments**: 1) Compare EffKAN vs standard KAN on MNIST with varying grid sizes, 2) Test KAN performance on more complex datasets (CIFAR-10), 3) Evaluate memory requirements and computational efficiency against traditional MLPs.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to MNIST dataset, which represents simple computer vision tasks
- No comparison with state-of-the-art continual learning methods
- Lacks detailed analysis of computational efficiency and memory requirements

## Confidence
- Performance claims: Medium
- Scalability assertions: Low
- Hyperparameter findings: Medium

## Next Checks
1. Test EffKAN on more complex vision datasets (e.g., CIFAR-10, ImageNet subsets) to evaluate scalability and robustness across different task complexities
2. Compare EffKAN's performance against established continual learning approaches like Elastic Weight Consolidation (EWC), Synaptic Intelligence, or rehearsal-based methods to establish relative effectiveness
3. Conduct ablation studies to isolate the contribution of KAN-specific components (learnable edge activations) versus other architectural choices to the observed performance gains