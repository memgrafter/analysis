---
ver: rpa2
title: 'Model Merging and Safety Alignment: One Bad Model Spoils the Bunch'
arxiv_id: '2406.14563'
source_url: https://arxiv.org/abs/2406.14563
tags:
- alignment
- merging
- data
- safety
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effects of model merging on alignment,
  demonstrating that existing techniques propagate misalignment alongside domain expertise.
  We propose a safety-aware merging pipeline that treats alignment as a task to be
  optimized during merging.
---

# Model Merging and Safety Alignment: One Bad Model Spoils the Bunch

## Quick Facts
- arXiv ID: 2406.14563
- Source URL: https://arxiv.org/abs/2406.14563
- Reference count: 29
- This work investigates the effects of model merging on alignment, demonstrating that existing techniques propagate misalignment alongside domain expertise.

## Executive Summary
This paper addresses a critical gap in model merging research: the preservation of safety alignment when combining multiple expert language models. The authors demonstrate that standard merging techniques, which optimize only for domain accuracy, inadvertently dilute or eliminate alignment behaviors encoded in the source models. To solve this, they propose a safety-aware merging pipeline that treats alignment as an explicit optimization objective, using synthetic data generation to create both safety and domain-specific training sets. Their approach significantly improves alignment metrics (up to 96.1) while maintaining or improving domain performance compared to baselines, particularly when merging three or more models.

## Method Summary
The method involves generating synthetic safety data by having an uncensored model create harmful prompts and collecting refusals from aligned expert models. Domain-specific synthetic data is also generated for each expert. These datasets are then incorporated into a data-driven merging optimization (using EvoMM or LM-Cocktail) that balances both domain accuracy and alignment through a weighted loss function with parameter α. The merged model is evaluated on both domain benchmarks and the BeaverTails30K alignment dataset.

## Key Results
- Safety-aware merging achieves up to 96.1 alignment score while maintaining domain accuracy
- The method is particularly effective for merging three or more models, consistently achieving highest alignment across scenarios
- Existing merging techniques propagate misalignment alongside domain expertise, confirming the need for alignment-aware optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alignment is lost during model merging because task vectors combine both domain expertise and alignment behavior, and the optimization focuses only on domain accuracy.
- Mechanism: When merging models, the task vector arithmetic combines parameters that encode both domain knowledge and alignment. If the optimization criterion only maximizes domain accuracy, the alignment-related components get diluted or overwritten by the dominant domain vectors.
- Core assumption: Alignment can be represented as a learnable component in the parameter space that gets optimized alongside domain expertise.
- Evidence anchors:
  - [abstract] "existing methods do not only transfer domain expertise but also propagate misalignment"
  - [section 4.4] "We combine the two terms into a single loss, using a factor α to balance each contribution"
  - [corpus] Found 25 related papers with average neighbor FMR=0.441, suggesting alignment is a recognized concern in model merging literature
- Break condition: If alignment is not representable as a task vector or cannot be optimized with synthetic data, the mechanism fails.

### Mechanism 2
- Claim: Synthetic data generation preserves alignment behavior by creating a feedback loop where aligned models refuse harmful prompts.
- Mechanism: The system generates unsafe prompts using an uncensored model, then collects refusals from aligned models. This creates a dataset that teaches the merged model to refuse similar prompts, effectively transferring the alignment behavior through data rather than direct parameter manipulation.
- Core assumption: Refusals from aligned models can be effectively transferred to the merged model through supervised learning on synthetic data.
- Evidence anchors:
  - [section 4.2] "We then use qsafety ∼ Qsafety as input for all f ∈ F, collecting a set of replies for each prompt qsafety"
  - [section 5.1] "For safety data, we use an uncensored LLM to generate harmful questions, and collect refusals of the F experts"
  - [corpus] Related work on SafeMERGE and AlignMerge suggests data-driven alignment preservation is an active research area
- Break condition: If the uncensored model cannot generate diverse harmful prompts or aligned models fail to refuse consistently, the synthetic data pipeline breaks down.

### Mechanism 3
- Claim: The data-aware merging optimization balances domain expertise and alignment by using both datasets as optimization objectives.
- Mechanism: Instead of optimizing only on domain-specific data, the merging process uses a combined loss that includes both domain accuracy and alignment performance. This forces the optimization to find task weights that satisfy both objectives simultaneously.
- Core assumption: The optimization algorithm can find a Pareto-optimal solution that maintains both high domain accuracy and high alignment.
- Evidence anchors:
  - [section 4.4] "We combine the two terms into a single loss, using a factor α to balance each contribution"
  - [section 5.2] "Our safety-aware merging achieves the highest alignment across all scenarios"
  - [corpus] LED-Merging paper suggests safety-utility conflicts are a known challenge in model merging
- Break condition: If the optimization gets stuck in local minima that favor one objective heavily over the other, or if α cannot be properly tuned, the balance fails.

## Foundational Learning

- Concept: Task vector arithmetic in model merging
  - Why needed here: Understanding how expert models are combined through parameter differences is fundamental to grasping why alignment gets lost
  - Quick check question: If θmerged = θbase + λ1τ1 + λ2τ2, what happens to alignment when λ values are optimized only for domain accuracy?

- Concept: Synthetic data generation for alignment
  - Why needed here: The paper's key innovation relies on creating artificial prompts and collecting model refusals to teach alignment
  - Quick check question: Why use an uncensored model to generate harmful prompts instead of using existing harmful datasets?

- Concept: Multi-objective optimization in neural networks
  - Why needed here: The merging process optimizes for two competing objectives (domain accuracy vs alignment), requiring understanding of loss balancing
  - Quick check question: What role does the α parameter play in balancing the two loss terms?

## Architecture Onboarding

- Component map:
  Expert models pool (F) -> Synthetic data generator (for Dsafety and Dexpert) -> LLaMA-Guard classifier (for identifying refusals) -> Merging optimizer (EvoMM or LM-Cocktail) -> Evaluation pipeline (BeaverTails30K for alignment, domain benchmarks for accuracy)

- Critical path: Generate synthetic data → Merge with safety-aware loss → Evaluate alignment and accuracy

- Design tradeoffs:
  - Synthetic vs real data: Synthetic data avoids external dependencies but may miss edge cases
  - α parameter tuning: Higher α improves alignment but may reduce domain accuracy
  - Number of optimization steps: More steps improve alignment transfer but increase computation time

- Failure signatures:
  - Low alignment with high domain accuracy: Optimization focused too heavily on domain loss
  - Both alignment and accuracy poor: Synthetic data generation failed or task weights poorly optimized
  - High alignment but domain performance worse than individual experts: Task vectors may have canceled each other out

- First 3 experiments:
  1. Run merging with only Dexpert (no safety data) to establish baseline performance degradation
  2. Test different α values (0.1, 0.3, 0.5, 1.0) to find optimal balance point
  3. Compare synthetic data generation vs using real BeaverTails training set for Dsafety to validate approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed safety-aware merging pipeline perform when merging models with varying architectures and prompt templates?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that the current approach is limited to models with the same architectures and requires the use of the same chat template across models. This limitation restricts the applicability of the method in scenarios involving diverse model architectures or heterogeneous prompt templates.
- What evidence would resolve it: Experimental results demonstrating the performance of the safety-aware merging pipeline when applied to models with different architectures and prompt templates would provide insights into the generalizability and limitations of the proposed approach.

### Open Question 2
- Question: What is the impact of the number of optimization steps in EvoMM on the alignment and accuracy of the merged model?
- Basis in paper: [explicit]
- Why unresolved: The paper investigates the impact of the number of optimization steps in EvoMM on the alignment and accuracy of the merged model, but the results are not conclusive. The paper suggests that more iterations benefit alignment transfer while accuracy decreases, but further investigation is needed to understand the optimal number of steps for achieving the best trade-off between alignment and accuracy.
- What evidence would resolve it: A comprehensive study analyzing the alignment and accuracy of the merged model as a function of the number of optimization steps in EvoMM would provide insights into the optimal number of steps for achieving the desired performance.

### Open Question 3
- Question: How does the proposed safety-aware merging pipeline perform when merging models with varying levels of alignment?
- Basis in paper: [inferred]
- Why unresolved: The paper assumes that at least one model in the merging pool is sufficiently aligned, but this prerequisite may not always be met. The performance of the proposed pipeline when merging models with varying levels of alignment is not explored, which limits the understanding of the robustness of the approach.
- What evidence would resolve it: Experimental results demonstrating the performance of the safety-aware merging pipeline when merging models with different levels of alignment would provide insights into the robustness and limitations of the proposed approach.

## Limitations
- The approach relies heavily on synthetic data generation, which may not capture the full complexity of real-world harmful prompts
- The quality of alignment preservation depends on the uncensored model's ability to generate diverse safety scenarios and aligned models' consistency in refusing harmful requests
- The study focuses on LLM merging rather than other architectures, limiting generalizability

## Confidence
- High confidence: The observation that existing merging techniques propagate misalignment is well-supported by experimental results across multiple scenarios and metrics
- Medium confidence: The proposed safety-aware merging pipeline's effectiveness, while demonstrated, relies on assumptions about synthetic data quality and optimal α values that weren't exhaustively validated
- Low confidence: The scalability of the approach to more than three experts and its performance on non-LLM architectures remains unproven

## Next Checks
1. Test the safety-aware merging approach on a larger set of domain pairs (8-10) to establish robustness across different expertise combinations
2. Conduct ablation studies varying α systematically across multiple orders of magnitude to identify optimal balancing strategies
3. Compare synthetic safety data quality against real-world harmful prompt datasets to validate the generation approach's coverage and effectiveness