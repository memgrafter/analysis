---
ver: rpa2
title: 'Unchosen Experts Can Contribute Too: Unleashing MoE Models'' Power by Self-Contrast'
arxiv_id: '2405.14507'
source_url: https://arxiv.org/abs/2405.14507
tags:
- routing
- scmoe
- experts
- activation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of underutilizing experts in Mixture-of-Experts
  (MoE) models, where unchosen experts do not contribute to the output during inference.
  The authors propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy
  that leverages the contrastive information between strong and weak activations within
  the same MoE model to enhance performance.
---

# Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast

## Quick Facts
- arXiv ID: 2405.14507
- Source URL: https://arxiv.org/abs/2405.14507
- Authors: Chufan Shi; Cheng Yang; Xinyu Zhu; Jiahao Wang; Taiqiang Wu; Siheng Li; Deng Cai; Yujiu Yang; Yu Meng
- Reference count: 40
- Primary result: SCMoE improves Mixtral 8x7B accuracy from 61.79 to 66.94 on GSM8K with only 1.30x latency increase

## Executive Summary
This paper addresses the underutilization of experts in Mixture-of-Experts (MoE) models, where unchosen experts during inference do not contribute to the output. The authors propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that leverages contrastive information between strong (top-2 routing) and weak (rank-k routing) activations within the same MoE model. By computing the difference between logits from these two routing strategies, SCMoE transforms the potentially negative or neutral contributions of unchosen experts into beneficial information, achieving consistent performance improvements across multiple reasoning benchmarks while maintaining practical latency.

## Method Summary
SCMoE is a training-free inference strategy for MoE models that computes next-token probabilities by contrasting outputs from strong activation (top-2 routing) and weak activation (rank-k routing). The method calculates contrastive logits using both routing strategies, with a contrastive penalty parameter β that controls the strength of the weak activation contribution. The vocabulary mask parameter α prevents sampling from low-probability tokens. SCMoE requires only two forward passes per token and simple logit adjustment, incurring a 1.30x latency increase versus greedy decoding while improving accuracy on reasoning tasks.

## Key Results
- GSM8K accuracy improves from 61.79 to 66.94 using Mixtral 8x7B
- MBPP pass@1 accuracy increases from 46.20 to 48.80
- Only 1.30x latency increase compared to greedy decoding
- Consistent improvements across GSM8K, StrategyQA, MBPP, and HumanEval benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unchosen experts can contribute positively to the output when leveraged via contrastive self-computation within the same MoE model.
- Mechanism: The self-contrast method computes logits by taking the difference between strong (top-2 routing) and weak (rank-k routing) activations. The weak activation selectively engages unchosen experts, whose divergent output logits from strong activation are used to amplify or correct the final prediction. This transforms the potentially negative or neutral contributions of unchosen experts into beneficial information.
- Core assumption: Unchosen experts contain complementary information to strong experts, even if their activation was initially deemed unnecessary. The routing mechanism can suppress useful but non-dominant contributions during greedy decoding.
- Evidence anchors:
  - [abstract] states that unchosen experts do not contribute to output, but proposes SCMoE to leverage them via self-contrast.
  - [section] describes using contrastive information between strong and weak activations to determine next-token probabilities.
  - [corpus] includes related work on MoE pruning and routing, suggesting underutilization of experts is a recognized problem.
- Break condition: If the weak activation's logits are uncorrelated or detrimental to the strong activation's predictions, the contrastive penalty could degrade performance. Also, if the routing mechanism is too accurate in selecting the best experts, unchosen experts may add noise rather than signal.

### Mechanism 2
- Claim: Different routing strategies within the same MoE model produce substantially different next-token distributions, especially in reasoning-heavy sequences.
- Mechanism: By calculating the Kullback-Leibler divergence between distributions from top-2 routing and rank-k routing, the authors show that reasoning sequences (e.g., mathematical expressions) have higher divergence, indicating distinct expert contributions. SCMoE exploits this divergence by contrasting logits from different routing strategies.
- Core assumption: The routing mechanism in MoE models does not fully capture all useful information, and weaker routing strategies still produce meaningful, divergent predictions that can be used constructively.
- Evidence anchors:
  - [section] provides heatmaps showing high KLD for reasoning tokens when comparing top-2 and rank-k routing.
  - [abstract] mentions that different experts do not always act synergistically, implying divergence in behavior.
  - [corpus] lacks direct evidence of KLD measurements but supports MoE diversity research.
- Break condition: If the routing strategies converge in their predictions (low KLD across all tokens), the contrastive approach loses its advantage and may introduce unnecessary computational overhead.

### Mechanism 3
- Claim: The contrastive decoding method is computationally lightweight compared to other decoding methods like contrastive search or contrastive decoding, while still improving performance.
- Mechanism: SCMoE only requires two forward passes per token (one for strong activation, one for weak activation) and simple logit adjustment, incurring a 1.30x latency increase versus greedy decoding. This is more efficient than methods requiring additional models or complex search.
- Core assumption: The overhead of dual routing is acceptable given the performance gains, and the method remains practical for deployment.
- Evidence anchors:
  - [abstract] explicitly states SCMoE incurs only a 1.30x latency increase compared to greedy decoding.
  - [section] compares latency with other methods, showing SCMoE is competitive.
  - [corpus] lacks direct latency comparisons but includes related MoE efficiency research.
- Break condition: If the latency penalty becomes prohibitive for large-scale deployment or real-time applications, or if the performance gains do not justify the computational cost.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture and sparse activation
  - Why needed here: SCMoE operates within the MoE framework, leveraging its routing mechanism and expert diversity. Understanding how experts are selected and how sparse activation works is essential to grasp why unchosen experts might still hold value.
  - Quick check question: In a standard MoE layer, how are experts selected for a given token, and what happens to the unchosen experts during inference?

- Concept: Kullback-Leibler (KL) divergence as a measure of distributional difference
  - Why needed here: The paper uses KLD to quantify the difference between output distributions from different routing strategies. Understanding KLD helps explain why contrastive methods can be effective.
  - Quick check question: What does a high KL divergence between two output distributions indicate about the underlying models or routing strategies?

- Concept: Contrastive learning and decoding in NLP
  - Why needed here: SCMoE is a form of contrastive decoding that leverages differences between model behaviors. Familiarity with contrastive methods helps contextualize SCMoE's innovation.
  - Quick check question: How does contrastive decoding differ from standard greedy or beam search decoding in language models?

## Architecture Onboarding

- Component map:
  Router -> Top-2 routing -> Strong activation logits
  Router -> Rank-k routing -> Weak activation logits
  Strong + Weak activation -> Contrastive logits computation
  Contrastive logits -> Vocabulary masking -> Next token sampling

- Critical path:
  1. Token input → Router computes gating scores.
  2. Apply top-2 routing → Generate strong activation logits.
  3. Apply rank-k routing → Generate weak activation logits.
  4. Compute contrastive logits via Equation 5.
  5. Mask invalid tokens → Apply vocabulary restriction.
  6. Sample or select next token → Proceed to next step.

- Design tradeoffs:
  - Performance vs. latency: SCMoE improves accuracy but adds computational overhead.
  - Routing strategy selection: Choice of weak activation (rank-k) affects results.
  - Vocabulary masking (alpha): Controls trade-off between exploration and noise.

- Failure signatures:
  - Degraded performance on non-reasoning tasks (low KLD scenarios).
  - Increased latency without commensurate accuracy gains.
  - Overfitting to specific routing strategies, reducing generalization.

- First 3 experiments:
  1. Reproduce KLD analysis comparing top-2 and rank-k routing on GSM8K to validate distributional differences.
  2. Implement SCMoE with fixed rank-2 weak activation and test on GSM8K, StrategyQA, MBPP, and HumanEval.
  3. Compare SCMoE latency against greedy, contrastive search, and contrastive decoding baselines.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Performance improvements are relatively modest (5.15 percentage points on GSM8K, 2.6 percentage points on MBPP)
- Limited investigation of different weak activation strategies beyond rank-2
- Only tested on Mixtral 8x7B model, leaving scalability to larger models unexplored

## Confidence
- **High Confidence**: The experimental results showing consistent improvements across multiple benchmarks using the Mixtral 8x7B model. The latency measurements appear reliable and well-documented.
- **Medium Confidence**: The theoretical justification for why unchosen experts contain useful information. While the contrastive approach is plausible, the paper provides limited mechanistic explanation for why this works beyond distributional divergence.
- **Medium Confidence**: The claim that SCMoE is "training-free" and practical for deployment. While technically true, the additional computational overhead and complexity of dual routing may impact real-world adoption.

## Next Checks
1. **Cross-architecture validation**: Test SCMoE on a different MoE architecture (e.g., DeepSeekMoE or a custom MoE with different expert counts) to verify if the improvements generalize beyond Mixtral 8x7B.

2. **Routing strategy sensitivity**: Systematically evaluate the impact of different weak activation strategies (rank-3, rank-4, random expert selection) and contrastive penalty strengths (β values) to identify optimal configurations and robustness.

3. **Knowledge attribution analysis**: Conduct ablation studies where specific experts are removed or modified to determine whether the performance gains truly stem from unchosen experts or are artifacts of the contrastive computation itself.