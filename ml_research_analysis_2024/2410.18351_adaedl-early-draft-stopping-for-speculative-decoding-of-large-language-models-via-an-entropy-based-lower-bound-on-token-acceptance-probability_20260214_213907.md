---
ver: rpa2
title: 'AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models
  via an Entropy-based Lower Bound on Token Acceptance Probability'
arxiv_id: '2410.18351'
source_url: https://arxiv.org/abs/2410.18351
tags:
- draft
- adaedl
- decoding
- token
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaEDL addresses the challenge of inefficient draft lengths in
  speculative decoding by introducing an entropy-based early stopping criterion. The
  method estimates a lower bound on the acceptance probability of drafted tokens using
  the entropy of the draft model's logits, stopping the drafting process when this
  bound falls below a threshold.
---

# AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability

## Quick Facts
- arXiv ID: 2410.18351
- Source URL: https://arxiv.org/abs/2410.18351
- Reference count: 40
- Token generation rates improved by 10%-57% compared to static draft lengths

## Executive Summary
AdaEDL addresses the challenge of inefficient draft lengths in speculative decoding by introducing an entropy-based early stopping criterion. The method estimates a lower bound on the acceptance probability of drafted tokens using the entropy of the draft model's logits, stopping the drafting process when this bound falls below a threshold. This approach adapts the draft length dynamically during inference without requiring training or model-specific tuning. Experiments across multiple datasets, sampling temperatures, and draft/target model configurations show that AdaEDL improves token generation rates by 10%-57% compared to static draft lengths and outperforms other training-free draft-stopping techniques by up to 10%.

## Method Summary
AdaEDL uses entropy of draft model logits to estimate a lower bound on token acceptance probability in speculative decoding. The draft stops when 1 - sqrt(γ * H) < λ, where H is entropy and γ is a hyperparameter. A dynamic threshold λ is maintained through exponential moving average based on acceptance rate statistics, adapting to changing conditions during generation. The method requires only the draft model's logits and works across different sampling temperatures without additional training or fine-tuning.

## Key Results
- 10%-57% improvement in token generation rates compared to static draft lengths
- Outperforms other training-free draft-stopping techniques by up to 10%
- Robust to hyperparameter choices and maintains performance across sampling temperatures from 0.7 to 1.7

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy of draft model logits correlates inversely with acceptance probability of draft tokens
- Mechanism: Draft model logits entropy serves as a proxy for uncertainty in predictions; higher entropy indicates lower confidence in any particular token, leading to lower acceptance rates
- Core assumption: The relationship between draft model entropy and acceptance probability follows a predictable pattern that can be bounded mathematically
- Evidence anchors:
  - [abstract]: "entropy-based early stopping criterion"
  - [section]: "1 − √γHDM (x) serves as an approximate lower bound on the expected acceptance rate"
  - [corpus]: Weak - related papers mention entropy but don't establish the specific bound relationship
- Break condition: When the entropy-based lower bound falls below threshold λ, indicating insufficient expected acceptance probability

### Mechanism 2
- Claim: Dynamic adjustment of stopping threshold λ improves performance over static thresholds
- Mechanism: Acceptance rate statistics are tracked and used to update λ via exponential moving average, adapting to changing conditions during generation
- Core assumption: Acceptance rate varies across different contexts and can be predicted from recent history
- Evidence anchors:
  - [abstract]: "adapts the draft length dynamically during inference"
  - [section]: "Algorithm 1: Dynamic updates for stopping threshold λ"
  - [corpus]: Weak - no direct evidence of similar dynamic threshold adaptation strategies
- Break condition: When AR (acceptance rate) ≥ α (target acceptance rate) and nacc = L (maximum draft length reached)

### Mechanism 3
- Claim: Entropy-based stopping criterion generalizes better across different sampling temperatures than confidence-based methods
- Mechanism: Entropy captures distributional properties of all tokens rather than just the most likely token, making it more robust to temperature variations
- Core assumption: Sampling temperature affects the entire probability distribution in a way that entropy can capture but single-token confidence cannot
- Evidence anchors:
  - [abstract]: "preserves performance in high-sampling-temperature scenarios"
  - [section]: "Figure 5 we see that as we increase the sampling temperature from 0.7 to 1.7"
  - [corpus]: Weak - no corpus evidence directly comparing temperature robustness of entropy vs confidence methods
- Break condition: When 1 − √γHDM(x) < λ, where λ is dynamically adjusted

## Foundational Learning

- Concept: Speculative decoding and rejection sampling
  - Why needed here: The entire approach relies on understanding how draft tokens are verified against the target model using rejection sampling
  - Quick check question: How does rejection sampling guarantee that accepted draft tokens follow the target model's distribution?

- Concept: Entropy and information theory
  - Why needed here: The stopping criterion uses entropy as a proxy for acceptance probability, requiring understanding of entropy's properties
  - Quick check question: What does high entropy in a probability distribution indicate about the certainty of predictions?

- Concept: Total variation distance and Pinsker's inequality
  - Why needed here: The derivation of the entropy-based bound relies on relating acceptance probability to total variation distance via Pinsker's inequality
  - Quick check question: How does Pinsker's inequality relate total variation distance to KL divergence?

## Architecture Onboarding

- Component map:
  - Draft model -> Entropy calculator -> Threshold controller -> Token selector
  - Target model (parallel verification)
  - Acceptance/rejection sampler

- Critical path:
  1. Draft model generates token
  2. Entropy calculator computes HDM(x)
  3. Stopping criterion check: 1 − √γHDM(x) < λ?
  4. If yes, stop drafting and verify; if no, continue drafting
  5. Acceptance/rejection sampling performed
  6. Threshold λ updated based on acceptance statistics

- Design tradeoffs:
  - Entropy computation cost vs. accuracy of acceptance probability estimation
  - Frequency of threshold updates vs. responsiveness to changing conditions
  - Value of γ hyperparameter vs. tightness of the bound approximation
  - Maximum draft length setting vs. opportunity for early stopping

- Failure signatures:
  - Poor performance on high-temperature sampling indicates threshold adaptation issues
  - Inconsistent gains across datasets suggests entropy calculation or bound approximation problems
  - Sensitivity to λ initialization points to insufficient adaptation mechanism
  - Degradation with larger draft models may indicate incorrect assumptions about entropy-acceptance relationship

- First 3 experiments:
  1. Compare token acceptance rates between static and dynamic λ settings on CNN-DM dataset
  2. Test sensitivity to γ values (0.1, 0.2, 0.5) while holding other parameters constant
  3. Evaluate performance across sampling temperatures (0.7, 1.0, 1.3) with fixed maximum draft length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the entropy factor γ impact AdaEDL's performance across different datasets and model architectures?
- Basis in paper: [explicit] The paper states "In all our experiments, we set γ = 0.2" and notes that "values in the range γ ∈ (0, 1) typically work best."
- Why unresolved: The paper uses a fixed value of γ = 0.2 without exploring how different values affect performance or whether dynamic adjustment of γ could improve results.
- What evidence would resolve it: Systematic experiments varying γ across multiple datasets, model sizes, and tasks to identify optimal values and test whether adaptive γ adjustment improves token generation rates.

### Open Question 2
- Question: Can larger draft models finetuned for specific tasks achieve higher acceptance rates when used with AdaEDL compared to smaller draft models?
- Basis in paper: [inferred] The paper suggests that "larger draft models, which would presumably lead to higher acceptance rates without sacrificing performance if AdaEDL were to be enabled" and mentions this as "a promising direction of investigation for future work."
- Why unresolved: The experiments only test AdaEDL with a 115M parameter draft model, leaving the potential benefits of larger draft models unexplored.
- What evidence would resolve it: Experiments comparing AdaEDL performance using draft models of varying sizes (e.g., 1B, 3B, 7B parameters) both with and without task-specific finetuning, measuring acceptance rates and token generation speeds.

### Open Question 3
- Question: How does hardware choice impact the relative performance of AdaEDL versus other speculative decoding methods?
- Basis in paper: [explicit] The paper acknowledges that "results may vary on different hardware which may lead to faster or slower inference of a draft or target model" and discusses draft model cost across processors as an important consideration.
- Why unresolved: All experiments were conducted on a single NVIDIA A100 with 80GB of GPU memory, and the paper notes that hardware differences could significantly impact relative performance.
- What evidence would resolve it: Comparative experiments running AdaEDL and baseline methods on different hardware configurations (e.g., various GPU models, CPUs, specialized AI accelerators) to quantify how hardware affects speedup and relative performance.

## Limitations

- Limited empirical validation across different model architectures beyond Llama2 family
- Experimental scope restricted to three datasets and single draft-target model pairing
- Missing ablation studies on the γ hyperparameter and its interaction with draft model capacities

## Confidence

**High Confidence** (Strong theoretical grounding and consistent experimental results):
- The entropy-based stopping criterion provides a mathematically justified lower bound on acceptance probability
- AdaEDL demonstrates consistent TPS improvements over static draft length methods across all tested datasets and temperatures
- The method successfully handles high-sampling-temperature scenarios where confidence-based methods fail

**Medium Confidence** (Reasonable evidence but limited scope):
- Dynamic threshold adaptation improves performance over fixed thresholds
- AdaEDL outperforms other training-free draft-stopping techniques by up to 10%
- The method is robust to hyperparameter choices within reasonable ranges

**Low Confidence** (Theoretical justification present but limited empirical validation):
- Generalization to different model architectures beyond Llama2 family
- Performance with draft models substantially larger than 115M parameters
- Behavior in multilingual or code generation contexts
- Long-term stability of the dynamic threshold adaptation mechanism

## Next Checks

1. **Cross-Architecture Validation**: Test AdaEDL with different draft-target model pairs (e.g., OPT draft model with GPT-3 target, or Bloom draft with Llama target) to verify the entropy-acceptance probability relationship holds across architectures. Measure whether the same γ value (0.2) provides optimal performance or requires tuning.

2. **Scaling Analysis**: Evaluate AdaEDL performance with draft models of varying sizes (34M, 350M, 1.3B parameters) while keeping the target model fixed. This will reveal whether the entropy-based stopping criterion scales appropriately with draft model capacity and whether the theoretical bound remains tight across different draft model sizes.

3. **Ablation of Dynamic Thresholding**: Implement a static threshold variant of AdaEDL using λ values optimized through grid search for each dataset. Compare this against the dynamic adaptation mechanism to quantify the contribution of the adaptive component and test its necessity across different generation conditions.