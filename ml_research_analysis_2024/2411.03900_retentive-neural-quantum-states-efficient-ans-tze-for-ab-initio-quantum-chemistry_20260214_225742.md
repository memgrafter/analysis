---
ver: rpa2
title: "Retentive Neural Quantum States: Efficient Ans\xE4tze for Ab Initio Quantum\
  \ Chemistry"
arxiv_id: '2411.03900'
source_url: https://arxiv.org/abs/2411.03900
tags:
- retnet
- ansatz
- neural
- transformer
- quantum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of retentive networks (RetNet), a recurrent
  alternative to transformers, as an ansatz for solving electronic ground state problems
  in ab initio quantum chemistry. Unlike transformers, RetNets overcome the quadratic
  time complexity bottleneck by processing data in parallel during training and recurrently
  during inference.
---

# Retentive Neural Quantum States: Efficient Ansätze for Ab Initio Quantum Chemistry

## Quick Facts
- **arXiv ID:** 2411.03900
- **Source URL:** https://arxiv.org/abs/2411.03900
- **Reference count:** 37
- **Key outcome:** Retentive networks (RetNet) provide linear-time inference for NQS ansatze, outperforming transformers on electronic ground state problems when problem size exceeds model size by a factor of 1.75

## Executive Summary
This work introduces retentive networks (RetNet) as an efficient alternative to transformers for solving electronic ground state problems in ab initio quantum chemistry. By replacing the quadratic attention mechanism with linear-time retention, RetNets enable practical scaling of neural quantum states (NQS) for larger molecular systems. The authors demonstrate that variational neural annealing significantly improves training robustness and accuracy, achieving competitive results with smaller model sizes than previously reported in the literature.

## Method Summary
The authors replace transformer blocks with RetNet blocks in NQS ansatze, maintaining the same overall architecture (embedding layers, multi-block processing, output projection) while substituting attention with retention mechanisms. Retention uses vector recurrence instead of query-key comparisons, achieving O(1) time per token versus O(n) for attention. The approach is combined with variational neural annealing, which gradually reduces entropy regularization during training to prevent ansatz collapse while encouraging exploration early in training.

## Key Results
- RetNet inference time complexity is linear versus quadratic for transformers when n > 1.75 × dmodel
- Variational neural annealing significantly improves NQS accuracy and training robustness
- RetNet and transformer perform comparably on benchmark molecules (H₂, LiH, BeH₂, H₂O, NH₃) with smaller model sizes
- MADE with neural annealing achieves accuracy beyond previously known levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RetNet's linear inference time complexity enables practical scaling of NQS for electronic structure problems
- Mechanism: RetNet uses a dual formulation with parallel processing during training and recurrent processing during inference. The recurrent formulation computes retention in O(1) time per token using vector recurrence instead of O(n) query-key product comparisons
- Core assumption: The number of qubits n is large enough that n > 1.75 × dmodel, making the recurrent formulation more efficient than the quadratic transformer
- Evidence anchors:
  - [abstract] "RetNets overcome this time complexity bottleneck by processing data in parallel during training, and recurrently during inference"
  - [section 3.4] "the iterative input processing executes in linear time, with constant additional memory constraints"
  - [section 4.1] "there exists a threshold past which the parallel RetNet operates using fewer FLOPs than the transformer"

### Mechanism 2
- Claim: Variational neural annealing prevents ansatz collapse during training by maintaining entropy regularization
- Mechanism: The annealing schedule gradually reduces entropy regularization β from an initial positive value to zero using polynomial decay. This encourages exploration early in training when the ansatz distribution is uniform, then allows convergence to the ground state as β approaches zero
- Core assumption: Electronic ground states occupy small subspaces of the overall state space, making ansatz collapse likely without regularization
- Evidence anchors:
  - [section 3.2] "entropy regularization encourages the model to explore a larger variety of actions during training"
  - [section 4.2] "the perturbation of the NQS loss by the ansatz entropy is controlled through the regularization parameter β"
  - [section 5] "variational neural annealing significantly improves the robustness—and, indirectly, the practically attainable accuracy—of both the transformer and RetNet ansatze"

### Mechanism 3
- Claim: The structural similarity between RetNet and transformer allows direct architectural substitution in NQS pipelines
- Mechanism: Both architectures use embedding layers, multi-block processing, and linear output projections. RetNet replaces attention with retention heads while maintaining the same residual connections, layer normalization, and feedforward layers
- Core assumption: The NQS problem structure (small vocabulary size, fixed sequence lengths) is simpler than typical LLM applications, making RetNet a suitable substitute
- Evidence anchors:
  - [section 4] "Due to the architectural similarity between RetNets and transformers discussed in section 3.4, one may consider simply replacing transformers with RetNets inside any larger deep learning pipeline"
  - [section 3.4] "The RetNet block, as depicted within fig. 1b, is almost entirely identical to a decoder-only transformer block"
  - [section 5] "all models perform comparably well, with RetNet and transformer essentially performing in lockstep"

## Foundational Learning

- Concept: Variational Monte Carlo and quantum expectation values
  - Why needed here: The NQS framework optimizes wavefunction parameters by minimizing the quantum expectation value of the Hamiltonian, requiring understanding of how to estimate these values using Monte Carlo sampling
  - Quick check question: How does the local energy estimator lθ(x) = ⟨x|H|ψθ⟩/⟨x|ψθ⟩ relate to the quantum expectation value ⟨ψθ|H|ψθ⟩?

- Concept: Autoregressive neural networks and sequence modeling
  - Why needed here: NQS ansatze are autoregressive models that generate conditional probabilities for qubit spin configurations, requiring understanding of how to preserve the autoregressive property while maintaining expressiveness
  - Quick check question: What is the difference between MADE's binary mask approach and transformer's attention mechanism for preserving autoregressive properties?

- Concept: Computational complexity analysis and FLOP counting
  - Why needed here: The paper's main contribution is improving inference time complexity from quadratic to linear, requiring understanding of how to analyze and compare computational costs across architectures
  - Quick check question: Why does the attention mechanism in transformers have O(n²) complexity while retention in RetNet achieves O(n) complexity?

## Architecture Onboarding

- Component map: Input embedding → RetNet blocks (retention + feedforward) → output linear layer → softmax probability vectors
- Critical path: Forward pass during inference → recurrent retention computation → feedforward processing → output generation
- Design tradeoffs: RetNet vs transformer: linear vs quadratic inference time complexity, potentially reduced expressiveness vs improved scalability. MADE vs RetNet: simple fixed structure vs learned attention-like mechanisms
- Failure signatures: Poor convergence indicates annealing schedule issues, while slow inference suggests dmodel is too large relative to n. Accuracy degradation may indicate retention cannot capture necessary correlations
- First 3 experiments:
  1. Compare inference times of RetNet vs transformer on a small NQS problem with varying n to verify the threshold condition
  2. Test different annealing schedules (r values) on a fixed problem to find optimal decay rate
  3. Implement both parallel and recurrent RetNet forward passes to measure actual FLOP counts and verify theoretical estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RetNet compare to transformers when applied to NQS problems beyond electronic structure calculations?
- Basis in paper: [explicit] The paper states that NQS is fundamentally problem-agnostic and expresses hope that this work will inspire further exploration of novel autoregressive NQS ansatze, including RetNet, on other types of NQS problems.
- Why unresolved: The paper focuses specifically on ab initio quantum chemistry and does not explore RetNet's performance on other NQS applications.
- What evidence would resolve it: Empirical studies comparing RetNet and transformer performance on various NQS problems, such as Ising models, combinatorial optimization, or high-dimensional linear algebra, would provide concrete evidence of RetNet's general applicability and advantages.

### Open Question 2
- Question: What is the optimal annealing schedule for variational neural annealing in the context of electronic ground state problems?
- Basis in paper: [explicit] The paper introduces a quartic-schedule annealing schedule but notes that a more comprehensive study of neural annealing schedules would be helpful for understanding the amount of improvement that VNA can provide in practice.
- Why unresolved: While the paper demonstrates the benefits of VNA, it does not explore the full range of possible annealing schedules or determine the optimal schedule for different types of problems or model architectures.
- What evidence would resolve it: Systematic experiments comparing the performance of different annealing schedules (e.g., linear, polynomial with different exponents, exponential) on various electronic ground state problems and with different model architectures would help identify the optimal schedule.

### Open Question 3
- Question: Can MADE, a simpler architecture than transformers or RetNets, be effectively used as a competitive NQS ansatz for electronic structure problems?
- Basis in paper: [explicit] The paper demonstrates that neural annealing improves the accuracy of MADE to levels beyond what was previously known, suggesting that MADE could be a viable alternative to more complex architectures.
- Why unresolved: While the paper shows promising results for MADE with neural annealing, it does not conduct a comprehensive comparison of MADE's performance against transformers and RetNets on a wide range of electronic ground state problems.
- What evidence would resolve it: Extensive experiments comparing the performance of MADE, transformers, and RetNets on various electronic ground state problems, considering factors such as accuracy, computational efficiency, and memory usage, would provide a clearer picture of MADE's competitiveness as an NQS ansatz.

## Limitations

- The computational complexity analysis assumes ideal conditions that may not hold in practice, with theoretical FLOP counts not accounting for hardware-specific optimizations or memory access patterns
- Variational neural annealing shows empirical success but lacks theoretical guarantees about optimal annealing schedules or convergence properties
- Results are limited to small molecules (H₂, LiH, BeH₂, H₂O, NH₃) and may not generalize to larger, more complex systems

## Confidence

*High confidence* in the architectural similarity between RetNet and transformer blocks, as this is well-documented in the original RetNet literature and the structural differences are minimal and clearly defined.

*Medium confidence* in the empirical results demonstrating RetNet's effectiveness as NQS ansatze. While the experimental setup is sound and comparisons are fair, the results are limited to small molecules and may not generalize to larger systems.

*Medium confidence* in the claim that RetNet provides superior scaling for NQS problems. The theoretical analysis supports this, but practical considerations like memory constraints, parallelization opportunities, and specific hardware implementations could affect the real-world advantage.

## Next Checks

1. **Cross-model complexity validation**: Implement both parallel and recurrent RetNet forward passes and measure actual FLOP counts on a range of problem sizes to verify the theoretical threshold condition. Compare these measurements against the theoretical estimates for transformers of equivalent model sizes to identify any discrepancies.

2. **Annealing schedule sensitivity analysis**: Conduct systematic experiments varying the polynomial decay rate r from 0.01 to 0.2 and initial regularization β₀ from 1.0 to 10.0 across all benchmark molecules. Measure convergence rates, final energies, and ansatz collapse occurrences to determine optimal schedules for different problem characteristics.

3. **Scalability benchmark**: Test RetNet and transformer NQS ansatze on progressively larger molecular systems (H₂O, NH₃, and additional molecules like CH₄ or C₂H₄) while measuring both inference time and energy accuracy. Verify whether the computational advantages scale as predicted and whether accuracy degradation occurs at larger system sizes.