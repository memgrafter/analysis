---
ver: rpa2
title: Causal Explanations for Image Classifiers
arxiv_id: '2411.08875'
source_url: https://arxiv.org/abs/2411.08875
tags:
- image
- explanation
- explanations
- responsibility
- pixels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a black-box explanation approach for image
  classifiers based on formal causality theory. The authors define explanations as
  minimal pixel sets sufficient for the model's classification, then develop a compositional
  algorithm (ReX) to approximate these explanations without opening the model's black
  box.
---

# Causal Explanations for Image Classifiers

## Quick Facts
- arXiv ID: 2411.08875
- Source URL: https://arxiv.org/abs/2411.08875
- Reference count: 16
- Key outcome: ReX produces the smallest explanations (average ~5% of image size) compared to 20-80% for competitors while achieving high quality metrics

## Executive Summary
This paper introduces ReX, a black-box explanation method for image classifiers based on formal causality theory. The approach defines explanations as minimal pixel sets sufficient for classification and uses a compositional algorithm to approximate these explanations without accessing model internals. ReX achieves near-minimal explanations through iterative superpixel refinement and produces consistently smaller explanations than eight state-of-the-art methods while maintaining high quality metrics.

## Method Summary
ReX partitions images into superpixels and computes their responsibility for classification, then iteratively refines high-responsibility areas through recursive subdivision. The algorithm runs 20 iterations with random partitions, averages responsibility scores, and extracts minimal sufficient explanations using a greedy approach. This black-box method is evaluated against eight competing approaches across four datasets and three model architectures, demonstrating superior explanation size and quality metrics.

## Key Results
- ReX explanations average 5% of image size versus 20-80% for competitors
- Highest insertion curve AUC values across all datasets and models
- Near-perfect overlap with occlusion regions in Photobombing dataset (0.97 OUT)
- Strong segmentation overlap on VOC2012 (0.64 IN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative refinement process achieves near-minimal explanations by focusing computational effort on high-responsibility superpixels
- Mechanism: The algorithm partitions images into superpixels, computes their responsibility scores, then recursively refines only those superpixels exceeding a responsibility threshold
- Core assumption: Pixels with high responsibility tend to cluster within high-responsibility superpixels, making refinement efficient
- Evidence anchors:
  - [abstract]: "We prove termination of our algorithm and discuss its complexity and the amount of approximation compared to the precise definition"
  - [section 5.2]: "We handle this problem by holding one superpixel to its original value while refining the other and then flipping the procedure"
- Break condition: If high-responsibility pixels are scattered across multiple low-responsibility superpixels, the algorithm loses efficiency and produces larger-than-minimal explanations

### Mechanism 2
- Claim: Averaging responsibility scores across multiple random partitions reduces the impact of any single partition choice on explanation quality
- Mechanism: The algorithm runs 20 iterations with independently sampled partitions, then averages the responsibility scores across all iterations to create a more robust ranking
- Core assumption: Random partitioning distributes responsibility measurement errors evenly, allowing averaging to converge toward true responsibility
- Evidence anchors:
  - [abstract]: "The tool ReX is open source and available at https://github.com/ReX-XAI/ReX"
  - [section 5.3]: "We ameliorate the influence of the choice of any particular partition by iterating the algorithm over a set of partitions"
- Break condition: If the underlying image has strong directional features that align poorly with random partitions, averaging may not adequately capture true responsibility distribution

### Mechanism 3
- Claim: The greedy explanation extraction guarantees minimal sufficiency while maintaining computational tractability
- Mechanism: After computing pixel responsibility rankings, the algorithm greedily adds pixels to the explanation set until the classifier's output matches the original classification
- Core assumption: The greedy approach finds the minimal sufficient set because pixels are added in descending responsibility order
- Evidence anchors:
  - [abstract]: "ReX consistently produces the smallest explanations (average ~5% of image size versus 20-80% for competitors)"
  - [section 5.3]: "We construct a subset of pixelsE to explain N's output on this particular input x greedily"
- Break condition: If multiple non-overlapping pixel sets can each independently produce the classification, the greedy approach may select a larger-than-minimal explanation

## Foundational Learning

- Concept: Actual Causality Theory
  - Why needed here: Provides the formal definition of explanations as minimal sufficient causes, grounding the entire approach in rigorous theoretical foundations
  - Quick check question: What are the three conditions (AC1, AC2, AC3) that must hold for an actual cause according to Halpern's framework?

- Concept: Superpixel Partitioning
  - Why needed here: Enables efficient computation by reducing the pixel space to manageable blocks while preserving spatial coherence
  - Quick check question: How does the algorithm determine which superpixels to refine in subsequent iterations?

- Concept: Responsibility Scoring
  - Why needed here: Provides a quantitative measure of each pixel's contribution to classification, enabling principled ranking and explanation extraction
  - Quick check question: What is the mathematical formula for responsibility according to Definition 4.4?

## Architecture Onboarding

- Component map: Input processor -> Partition generator -> Responsibility calculator -> Refinement engine -> Averaging module -> Greedy extractor -> Output formatter
- Critical path:
  1. Random partition generation (N times)
  2. Superpixel responsibility computation
  3. Iterative refinement of passing combinations
  4. Responsibility averaging across partitions
  5. Greedy explanation extraction
  6. Quality metric calculation
- Design tradeoffs:
  - Partition size vs. computational cost: Larger partitions reduce computation but may miss fine-grained explanations
  - Number of iterations vs. robustness: More iterations improve reliability but increase runtime
  - Termination threshold vs. explanation quality: Stricter thresholds produce better explanations but may miss valid explanations
- Failure signatures:
  - Uniform responsibility across all superpixels indicates poor partition selection or uninformative images
  - Extremely large explanations suggest the algorithm failed to identify true causal factors
  - High variance across partitions indicates unstable responsibility measurement
- First 3 experiments:
  1. Run ReX on a simple image with a single dominant object to verify basic functionality
  2. Compare explanation sizes and quality metrics against a baseline XAI tool on the same image
  3. Test the impact of varying the number of partitions (N) on explanation quality and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ReX algorithm's performance compare on image classification tasks beyond the three tested architectures (ResNet50, ViT, ConvNext)?
- Basis in paper: [inferred] The authors note they will apply ReX to other modalities and domains, including healthcare AI and autonomous vehicles, suggesting broader applicability
- Why unresolved: The paper only evaluates on three specific model architectures and does not test ReX on other types of image classifiers or different data distributions
- What evidence would resolve it: Experimental results comparing ReX against other XAI methods on diverse model architectures (e.g., Vision Transformers, EfficientNets, CLIP) and varied image datasets would clarify its generalizability

### Open Question 2
- Question: Can the compositional explanation approach be adapted for non-image data modalities where causal independence assumptions do not hold?
- Basis in paper: [explicit] The authors state that causal independence between pixels is a non-trivial assumption that might not hold for tabular or spectral data, and they plan to explore other modalities
- Why unresolved: The paper focuses exclusively on image data where pixel independence is argued to hold, leaving open how the method would perform on interdependent features
- What evidence would resolve it: Implementation and evaluation of ReX-like algorithms on tabular datasets, time series, or spectroscopy data with known feature dependencies would demonstrate its adaptability

### Open Question 3
- Question: What is the optimal balance between explanation quality and computational cost when using different numbers of initial partitions (N) in Algorithm 3?
- Basis in paper: [explicit] The authors use N=20 iterations but acknowledge this is a parameter choice, and they mention exploring different partitioning distributions
- Why unresolved: The paper uses a fixed value of N=20 without systematic exploration of how varying this parameter affects both explanation quality metrics and runtime efficiency
- What evidence would resolve it: A comprehensive ablation study varying N and measuring the corresponding trade-offs in explanation size, AUC scores, and computation time would identify optimal parameter settings

## Limitations
- The method's reliance on superpixel partitioning may miss fine-grained explanations when important features span multiple superpixels
- Computational cost scales poorly with image resolution, limiting deployment on high-resolution imagery
- The theoretical framework assumes perfect knowledge of the classifier's output, which may not hold in real-world scenarios with noisy predictions

## Confidence
- **High confidence**: The core algorithmic approach and its termination properties are well-defined and theoretically sound
- **Medium confidence**: The empirical evaluation results are robust across multiple datasets and models, though some metrics show variability
- **Low confidence**: The comparison with state-of-the-art methods may be affected by implementation differences in baseline tools

## Next Checks
1. **Runtime scaling analysis**: Test the algorithm on progressively larger images to quantify the superlinear scaling behavior and identify practical limits for deployment
2. **Partition strategy ablation**: Systematically vary the number and size of superpixels to measure the tradeoff between explanation quality and computational efficiency
3. **Cross-domain robustness**: Evaluate ReX on non-natural image datasets (medical imaging, satellite imagery) to assess generalizability beyond the current evaluation domains