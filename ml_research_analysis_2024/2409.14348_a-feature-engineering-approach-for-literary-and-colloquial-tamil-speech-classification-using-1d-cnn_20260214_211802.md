---
ver: rpa2
title: A Feature Engineering Approach for Literary and Colloquial Tamil Speech Classification
  using 1D-CNN
arxiv_id: '2409.14348'
source_url: https://arxiv.org/abs/2409.14348
tags:
- features
- speech
- tamil
- feature
- prosodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for a system that can distinguish
  between literary Tamil (LT) and colloquial Tamil (CT) speech, a crucial step for
  effective human-computer interaction in Tamil. The authors propose using a one-dimensional
  convolutional neural network (1D-CNN) trained on handcrafted features that capture
  the unique characteristics of both LT and CT speech.
---

# A Feature Engineering Approach for Literary and Colloquial Tamil Speech Classification using 1D-CNN

## Quick Facts
- arXiv ID: 2409.14348
- Source URL: https://arxiv.org/abs/2409.14348
- Reference count: 40
- This paper proposes using a 1D-CNN trained on handcrafted features to distinguish literary Tamil from colloquial Tamil speech with high accuracy.

## Executive Summary
This paper addresses the need for a system that can distinguish between literary Tamil (LT) and colloquial Tamil (CT) speech, a crucial step for effective human-computer interaction in Tamil. The authors propose using a one-dimensional convolutional neural network (1D-CNN) trained on handcrafted features that capture the unique characteristics of both LT and CT speech. These features include prosodic (fundamental frequency, energy, voicing probability), voice quality (jitter, shimmer, harmonic-to-noise ratio), spectral (spectral flux, psychoacoustic sharpness), and temporal (zero crossing rate) features. The 1D-CNN is trained to learn the trends of these features over time, effectively capturing the distinguishing patterns between LT and CT. The proposed approach achieves an F1 score of 0.9803 when trained on the handcrafted features, outperforming the MFCC-based approach with an F1 score of 0.9895. Further feature combination experiments, combining the best-ranked handcrafted features with MFCC, yield the best results with an F1 score of 0.9946. This demonstrates the effectiveness of the proposed 1D-CNN architecture and the complementary nature of handcrafted and MFCC features for LT and CT speech classification.

## Method Summary
The proposed method involves extracting handcrafted features from Tamil speech utterances, segmenting them into fixed-duration chunks, and feeding them to a 1D-CNN for dialect classification. The handcrafted features include prosodic (fundamental frequency, energy, voicing probability), voice quality (jitter, shimmer, harmonic-to-noise ratio), spectral (spectral flux, psychoacoustic sharpness), and temporal (zero crossing rate) features. The 1D-CNN architecture consists of two sets of convolutional, pooling, and dropout layers, followed by dense layers. The model is trained to learn the trends of the proposed features over time, effectively capturing the distinguishing patterns between literary and colloquial Tamil. The model is evaluated using precision, recall, F1 score, and accuracy metrics.

## Key Results
- The proposed 1D-CNN approach achieves an F1 score of 0.9803 when trained on handcrafted features.
- The MFCC-based approach outperforms the handcrafted feature-based approach with an F1 score of 0.9895.
- Combining the top 3 handcrafted features (energy, HNR, F0) with MFCC yields the best results with an F1 score of 0.9946.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 1D-CNN learns meaningful time-series patterns from handcrafted features that capture both prosodic and spectral characteristics of Tamil speech.
- Mechanism: By using 1D convolution with kernel sizes tuned to phoneme/syllable durations (150-110ms), the network captures envelope trends of features like F0, energy, jitter, shimmer, and spectral flux across time, enabling discrimination between literary and colloquial Tamil.
- Core assumption: The distinguishing patterns between LT and CT manifest as measurable trends in acoustic features over time.
- Evidence anchors:
  - [abstract] "A one-dimensional convolutional neural network (1D-CNN) that learns the envelope of features across time"
  - [section] "The role of the 1D-CNN here is to learn the trend of the proposed features over time"
  - [corpus] Weak - no corpus-specific evidence of trend learning performance
- Break condition: If kernel sizes are too small to capture syllable-level patterns, or if feature extraction quality degrades (e.g., noise in pitch estimation), the learned trends become unreliable.

### Mechanism 2
- Claim: Handcrafted features complement MFCC by encoding prosodic and voice quality aspects that spectral coefficients alone miss.
- Mechanism: Prosodic features (F0, energy, voicing probability) and voice quality features (jitter, shimmer, HNR) capture stylistic differences (effort, fluency, nasalization) between LT and CT that MFCC's vocal tract modeling doesn't explicitly encode.
- Core assumption: LT and CT differ significantly in speaking style and effort, which are reflected in prosodic and voice quality features.
- Evidence anchors:
  - [abstract] "handcrafted features complement the information offered by MFCC"
  - [section] "In ideal human computer interaction (HCI), the colloquial form of a language would be preferred by most users"
  - [corpus] Weak - no corpus-specific evidence of complementary feature performance
- Break condition: If LT and CT don't actually differ in the targeted prosodic/voice quality aspects, the handcrafted features add no value over MFCC alone.

### Mechanism 3
- Claim: Feature ablation through independent evaluation identifies the most discriminative features without overfitting to the full feature set.
- Mechanism: By evaluating each feature independently and ranking by accuracy, the approach avoids the ambiguity of recursive feature elimination where feature interactions obscure individual contributions.
- Core assumption: Features contribute independently to classification accuracy, allowing meaningful ranking through isolated evaluation.
- Evidence anchors:
  - [section] "In IFE, the importance of each feature is determined by doing the opposite of the RFE method, that is by evaluating only one feature instead of nf âˆ’ r features in each round of evaluation"
  - [section] "The accuracies now fall in the range 0.9310 to 0.9922, a difference of 6.12%"
  - [corpus] Weak - no corpus-specific evidence of feature independence
- Break condition: If features are highly correlated, independent evaluation may misrepresent their joint contribution, leading to suboptimal feature selection.

## Foundational Learning

- Concept: Time-series feature extraction and normalization
  - Why needed here: The 1D-CNN processes sequences of acoustic features extracted from variable-length utterances, requiring consistent frame-level feature extraction and segment normalization
  - Quick check question: How do you ensure features are extracted at consistent time intervals across utterances of different durations?

- Concept: CNN kernel size selection for speech signals
  - Why needed here: Kernel sizes (10 and 5) are chosen based on syllable/phoneme duration ranges to capture meaningful temporal patterns in the feature envelopes
  - Quick check question: What speech unit duration justifies the chosen kernel sizes of 10 and 5 in the first and second convolutional layers?

- Concept: Dataset balancing and its impact on dialect classification
  - Why needed here: The original dataset is imbalanced (31h LT vs 8h CT), and balancing improves CT recall by preventing model bias toward the majority class
  - Quick check question: Why does balancing the dataset improve CT classification performance even though overall accuracy remains similar?

## Architecture Onboarding

- Component map:
  - Feature extraction layer: Computes 10 handcrafted features (prosodic, voice quality, spectral, temporal) and MFCC from speech frames
  - 1D-CNN backbone: Two sets of conv+pool+dropout layers with kernel sizes [10,5] and [7,3], followed by dense layers
  - Segment processing pipeline: Splits variable-length utterances into fixed-duration segments (1.87s) for CNN input
  - Inference aggregation: Averages segment-level predictions to produce utterance-level dialect classification

- Critical path:
  1. Load and preprocess speech utterances
  2. Extract handcrafted and MFCC features frame-by-frame
  3. Segment utterances into fixed-duration chunks
  4. Pass segments through 1D-CNN for dialect prediction
  5. Average segment predictions for final utterance classification

- Design tradeoffs:
  - Handcrafted vs MFCC: Handcrafted features capture prosodic/voice quality but require careful engineering; MFCC is universal but may miss stylistic nuances
  - Segment duration: Longer segments capture more context but reduce temporal resolution and increase memory usage
  - Kernel size: Larger kernels capture broader patterns but may miss fine-grained temporal variations

- Failure signatures:
  - Low recall on CT despite high overall accuracy: Indicates dataset imbalance or CT-specific feature extraction issues
  - Similar performance with random feature subsets: Suggests features are redundant or don't capture distinguishing patterns
  - Performance degradation with longer utterances: May indicate segment aggregation method issues or overfitting to segment length

- First 3 experiments:
  1. Train CA03B on balanced dataset with handcrafted features only; verify F1 > 0.98
  2. Replace handcrafted features with MFCC only; verify F1 > 0.99
  3. Combine top 3 handcrafted features (energy, HNR, F0) with MFCC; verify F1 > 0.994

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would the proposed 1D-CNN approach generalize to other Tamil dialects beyond literary and colloquial Tamil?
- Basis in paper: [inferred] The paper focuses specifically on distinguishing literary Tamil (LT) and colloquial Tamil (CT) using a 1D-CNN trained on handcrafted features. The authors do not explore the model's performance on other Tamil dialects.
- Why unresolved: The paper only evaluates the model on LT and CT data. The authors do not investigate whether the learned features and model architecture would be effective for classifying other Tamil dialects.
- What evidence would resolve it: Evaluating the proposed 1D-CNN approach on a dataset containing other Tamil dialects (e.g., dialects from different regions) and comparing its performance to other classification methods.

### Open Question 2
- Question: How do the proposed handcrafted features compare to automatically learned features from a deep neural network in terms of performance and interpretability?
- Basis in paper: [explicit] The paper compares the proposed handcrafted features to Mel-frequency cepstral coefficients (MFCCs), which are commonly used in automatic speech recognition. However, it does not explore automatically learned features from a deep neural network.
- Why unresolved: The paper does not investigate whether the proposed handcrafted features offer any advantages over automatically learned features from a deep neural network, such as improved interpretability or performance.
- What evidence would resolve it: Training a deep neural network (e.g., a convolutional neural network or a recurrent neural network) on the same dataset and comparing its performance and interpretability to the proposed approach using handcrafted features.

### Open Question 3
- Question: How would the proposed approach perform on a dataset with a larger number of speakers and more diverse recording conditions?
- Basis in paper: [inferred] The paper uses a dataset containing speech recordings from a limited number of speakers and recording conditions. The authors do not explore the model's robustness to speaker variability and recording conditions.
- Why unresolved: The paper does not investigate how well the proposed approach would generalize to a dataset with a larger number of speakers and more diverse recording conditions, such as different microphones, environments, and noise levels.
- What evidence would resolve it: Evaluating the proposed approach on a larger and more diverse dataset and comparing its performance to other classification methods under different recording conditions.

## Limitations
- The corpus consists of read speech from a single female speaker, limiting generalizability to spontaneous speech and diverse speaker populations.
- The feature independence assumption in IFE may not hold if acoustic features are correlated, potentially affecting feature ranking reliability.
- Handcrafted feature extraction relies on accurate pitch and formant tracking, which can degrade in noisy conditions.

## Confidence
- High confidence: The 1D-CNN architecture effectively learns temporal patterns from acoustic features (F1 > 0.98 on balanced data)
- Medium confidence: Handcrafted features provide complementary information to MFCC for dialect classification
- Medium confidence: Feature ablation through independent evaluation reliably identifies discriminative features

## Next Checks
1. Test the model on spontaneous colloquial Tamil speech to assess real-world performance beyond read speech
2. Evaluate feature correlation structure to verify the independence assumption underlying the IFE method
3. Conduct cross-speaker validation with male speakers and varying age groups to establish speaker independence