---
ver: rpa2
title: 'BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis'
arxiv_id: '2408.08964'
source_url: https://arxiv.org/abs/2408.08964
tags:
- code-mixed
- data
- sentiment
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present BnSentMix, a large-scale sentiment analysis
  dataset for Bengali-English code-mixed text consisting of 20,000 samples with four
  sentiment labels. The dataset is collected from Facebook, YouTube, and e-commerce
  platforms and filtered using a novel code-mixed detection method.
---

# BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis

## Quick Facts
- arXiv ID: 2408.08964
- Source URL: https://arxiv.org/abs/2408.08964
- Authors: Sadia Alam, Md Farhan Ishmam, Navid Hasin Alvee, Md Shahnewaz Siddique, Md Azam Hossain, Abu Raihan Mostofa Kamal
- Reference count: 24
- Primary result: 20,000-sample Bengali-English code-mixed dataset with 4 sentiment labels; BERT-CMB model achieves 69.8% accuracy and 69.1% F1 score

## Executive Summary
This paper introduces BnSentMix, a large-scale sentiment analysis dataset for Bengali-English code-mixed text containing 20,000 samples across four sentiment labels (positive, negative, neutral, mixed). The dataset is collected from Facebook, YouTube, and e-commerce platforms and filtered using a novel code-mixed detection method based on fine-tuned mBERT. The authors propose 14 baseline methods including three transformer encoders further pre-trained on code-mixed Bengali-English data, with BERT-CMB achieving the best performance at 69.8% accuracy and 69.1% F1 score. The dataset will be publicly available under a Creative Commons license.

## Method Summary
The authors collected 20,000 code-mixed Bengali-English samples from Facebook, YouTube, and e-commerce platforms, then filtered them using a novel detection method based on fine-tuned mBERT. They trained 14 baseline models including traditional ML (Logistic Regression, Random Forest, SVM), RNNs (RNN, LSTM), transformer-based models (BERT, mBERT, XLM-RoBERTa, BanglaBERT, BanglishBERT, DistilBERT), and further pre-trained CMB variants. Models were trained with Adam optimizer, batch size 32, and learning rates of 1E-5 for ML/RNN models and 1.5E-6 for transformer models, using a 70:15:15 train-validation-test split.

## Key Results
- BnSentMix dataset contains 20,000 code-mixed samples across four sentiment labels
- BERT-CMB model achieves best performance with 69.8% accuracy and 69.1% F1 score
- Code-mixed detection using fine-tuned mBERT shows substantially higher accuracy than other approaches
- Further pre-training on code-mixed data improves performance across all three transformer models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-mixed Bengali-English text poses unique challenges that standard language models struggle to process accurately
- Mechanism: A novel code-mixed detection method using fine-tuned mBERT ensures only relevant code-mixed text is used for sentiment analysis
- Core assumption: mBERT's multilingual capabilities effectively handle code-mixed text nuances better than monolingual models
- Evidence: mBERT shows substantially higher accuracy and F1 score in code-mixed detection compared to alternatives

### Mechanism 2
- Claim: Further pre-training transformer models on code-mixed data enhances sentiment analysis performance
- Mechanism: Masked Language Modeling (MLM) loss exposes models to specific linguistic patterns of code-mixed text
- Core assumption: Additional exposure to code-mixed data through MLM pre-training provides significant performance boost
- Evidence: Further pre-training translates to performance increases for all three transformer models tested

### Mechanism 3
- Claim: Dataset diversity across multiple platforms ensures robust model performance
- Mechanism: Including data from Facebook, YouTube, and e-commerce captures various contexts and linguistic styles
- Core assumption: Models trained on diverse data generalize better to unseen data
- Evidence: Data collected from multiple sources reflects realistic code-mixed texts commonly found in digital spaces

## Foundational Learning

- Concept: Code-mixing and its challenges in NLP
  - Why needed here: Understanding code-mixed text characteristics is crucial for developing effective sentiment analysis models
  - Quick check question: What are the two main types of code-switching mentioned in the paper, and how do they differ?

- Concept: Transformer-based models and their pre-training
  - Why needed here: The paper leverages transformer models and further pre-trains them on code-mixed data
  - Quick check question: How does Masked Language Modeling (MLM) loss contribute to the pre-training of transformer models?

- Concept: Evaluation metrics in text classification
  - Why needed here: The paper uses accuracy and F1-score to evaluate model performance
  - Quick check question: Why might F1-score be a more appropriate metric than accuracy for evaluating models on imbalanced datasets?

## Architecture Onboarding

- Component map: Data collection -> Code-mixed detection (mBERT) -> Dataset filtering -> Sentiment model training (14 baselines) -> Evaluation

- Critical path: 1) Collect and preprocess data from multiple sources 2) Fine-tune mBERT for code-mixed detection 3) Filter and annotate dataset 4) Train and evaluate sentiment models 5) Analyze results

- Design tradeoffs: Using multilingual mBERT vs. Bengali-specific model for detection; further pre-training vs. direct use of pre-trained models; dataset size/diversity vs. computational resources

- Failure signatures: Low detection accuracy leading to irrelevant training data; overfitting due to lack of diversity; underperformance on certain sentiment labels due to imbalance

- First 3 experiments: 1) Evaluate different pre-trained models on BnSentMix without further pre-training 2) Fine-tune mBERT on code-mixed detection dataset 3) Further pre-train best-performing model on code-mixed corpus and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the inclusion of emojis and emoticons affect sentiment analysis performance on code-mixed Bengali-English text?
- Basis: Authors removed emojis but acknowledge their potential impact
- Why unresolved: Paper does not explore effects of including emojis/emoticons
- What evidence would resolve it: Experiments comparing performance on datasets with and without emojis/emoticons

### Open Question 2
- Question: What is the impact of different data augmentation techniques on code-mixed sentiment analysis performance?
- Basis: Authors mention data augmentation has been explored for Bengali code-mixed datasets
- Why unresolved: Paper does not investigate specific impact on BnSentMix
- What evidence would resolve it: Experiments comparing performance with and without various augmentation techniques

### Open Question 3
- Question: How does model performance vary across different code-mixing ratios in the dataset?
- Basis: Sentences considered code-mixed if containing at least 30% English words
- Why unresolved: Paper does not analyze performance on subsets with varying code-mixing ratios
- What evidence would resolve it: Experiments evaluating performance on subsets with different code-mixing ratios

## Limitations

- Performance remains modest (69.8% accuracy, 69.1% F1) suggesting current approaches still struggle with code-mixed text
- Significant class imbalance, particularly for the "mixed" sentiment class (9.2% of samples)
- Computational resources required for further pre-training CMB models may limit accessibility

## Confidence

- High Confidence: Dataset collection methodology and mBERT detection effectiveness
- Medium Confidence: Performance improvements from further pre-training and dataset diversity benefits
- Medium Confidence: Generalizability of results to other code-mixed language pairs

## Next Checks

1. Evaluate each sentiment classification model separately on test data from each source (Facebook, YouTube, e-commerce) to quantify dataset diversity impact

2. Conduct experiments with stratified sampling and oversampling techniques targeting the "mixed" sentiment class to assess performance improvements through better class balance

3. Test trained models on an external code-mixed Bengali-English dataset or different domain to evaluate generalizability beyond BnSentMix corpus