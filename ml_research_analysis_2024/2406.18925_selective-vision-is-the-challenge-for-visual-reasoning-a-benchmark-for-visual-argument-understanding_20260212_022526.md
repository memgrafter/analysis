---
ver: rpa2
title: 'Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual
  Argument Understanding'
arxiv_id: '2406.18925'
source_url: https://arxiv.org/abs/2406.18925
tags:
- visual
- premises
- image
- conclusion
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisArgs, a benchmark for evaluating AI systems'
  ability to understand visual arguments. The authors created a dataset of 1,611 images
  annotated with visual premises, commonsense premises, and reasoning trees connecting
  them to conclusions.
---

# Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding

## Quick Facts
- arXiv ID: 2406.18925
- Source URL: https://arxiv.org/abs/2406.18925
- Reference count: 40
- Key outcome: AI systems struggle with selective vision when identifying relevant visual cues for visual arguments, showing 79.5% accuracy vs 98% for humans

## Executive Summary
This paper introduces VisArgs, a benchmark designed to evaluate AI systems' ability to understand visual arguments. The benchmark consists of 1,611 images annotated with visual premises, commonsense premises, and reasoning trees connecting them to conclusions. Through three distinct tasks—premise localization, premise identification, and conclusion deduction—the authors demonstrate that current AI systems face significant challenges in identifying relevant visual stimuli. The research reveals that selective vision, the ability to focus on pertinent visual information while ignoring irrelevant details, is the primary bottleneck for visual argument understanding in existing AI systems.

## Method Summary
The authors created VisArgs by collecting images depicting everyday scenarios and annotating them with premises and conclusions that form visual arguments. Each image contains visual premises (specific regions), commonsense premises (general knowledge), and reasoning trees showing how premises support conclusions. The benchmark evaluates three tasks: (1) Premise localization - identifying regions in images that support premises using IoU metrics; (2) Premise identification - determining which premises support given conclusions using accuracy metrics; and (3) Conclusion deduction - inferring conclusions from various input levels using BERTScore. The authors test multiple models including CLIP, LLaVA, and OFA across these tasks to establish baseline performance.

## Key Results
- Human annotators achieved 98% accuracy on premise identification compared to 79.5% for current AI models
- Providing relevant visual premises significantly improved model performance on conclusion deduction tasks
- Models performed better with global negative sampling strategies than local ones, indicating difficulty distinguishing relevant visual cues

## Why This Works (Mechanism)
The VisArgs benchmark works by explicitly separating visual premises from commonsense premises and requiring models to understand how both types of premises support conclusions through reasoning trees. The selective vision challenge emerges because models must learn to focus on specific visual regions while filtering out irrelevant background information. The task structure forces models to demonstrate not just object recognition but the ability to identify which objects and regions are actually relevant to the argument being made.

## Foundational Learning
- Visual argument understanding: Ability to interpret images as structured arguments with premises supporting conclusions
  * Why needed: Core task being evaluated by VisArgs benchmark
  * Quick check: Can you identify premises and conclusions in a simple image?
- Selective vision: Focusing on relevant visual stimuli while ignoring irrelevant information
  * Why needed: Identified as the main bottleneck for AI systems
  * Quick check: Can you distinguish between relevant and irrelevant visual regions in an image?
- Reasoning trees: Hierarchical structures showing how premises support conclusions
  * Why needed: Core annotation structure in VisArgs dataset
  * Quick check: Can you map premises to conclusions in a given reasoning tree?

## Architecture Onboarding
- Component map: CLIP/LLaVA/OFA -> Premise Localization -> Premise Identification -> Conclusion Deduction
- Critical path: Image input → Premise localization → Premise identification → Conclusion deduction
- Design tradeoffs: Balance between semantic understanding vs object detection for premise localization
- Failure signatures: Poor performance on open-set grounding, difficulty distinguishing relevant visual cues
- First experiments: 1) Test baseline models on premise localization task, 2) Evaluate premise identification with different negative sampling strategies, 3) Run conclusion deduction experiments with various input levels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the main text. However, the authors' emphasis on selective vision as a bottleneck suggests implicit questions about whether this challenge is fundamental to visual reasoning or specific to current model architectures. The performance gap between humans and models also raises questions about what additional capabilities humans employ beyond selective vision.

## Limitations
- Small scale of VisArgs benchmark (1,611 images) may limit generalizability
- Complex annotation process makes dataset expansion challenging
- Unclear whether performance gaps are due to model capacity limitations or inherent task difficulty
- The benchmark may favor certain types of visual arguments over others
- Limited diversity in the scenarios depicted across the dataset

## Confidence
- High confidence: The core finding that selective vision is a bottleneck for visual reasoning
- Medium confidence: The relative effectiveness of different negative sampling strategies
- Low confidence: The absolute performance numbers on premise identification due to small test set size
- Medium confidence: The conclusion that selective vision is the primary bottleneck, though other factors may contribute

## Next Checks
1. Test whether increasing the VisArgs dataset size by 2-3x would close the performance gap between models and humans
2. Conduct ablation studies to isolate whether selective vision or commonsense reasoning is the primary bottleneck
3. Evaluate whether models trained on VisArgs show transfer to other visual reasoning benchmarks
4. Investigate whether fine-tuning existing models on VisArgs improves selective vision capabilities
5. Explore whether incorporating attention mechanisms specifically designed for selective vision could improve performance