---
ver: rpa2
title: Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems
  with Reference-Free Metrics
arxiv_id: '2410.10867'
source_url: https://arxiv.org/abs/2410.10867
tags:
- human
- metrics
- evaluation
- linguistics
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reference-free metric for evaluating abstractive
  summarization systems, addressing the limitations of existing reference-based metrics
  that rely on potentially noisy or extractive gold summaries. The proposed method
  uses n-gram importance weighting (via tf-idf or bm-25) to score summaries based
  on their weighted lexical overlap with semantically important n-grams from source
  documents, while applying a length penalty to prevent trivial solutions.
---

# Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics

## Quick Facts
- arXiv ID: 2410.10867
- Source URL: https://arxiv.org/abs/2410.10867
- Reference count: 14
- Primary result: Reference-free metric using n-gram importance weighting correlates well with human judgments and improves robustness when combined with reference-based metrics

## Executive Summary
This paper introduces a reference-free metric for evaluating abstractive summarization systems that addresses limitations of existing reference-based metrics. The proposed method uses n-gram importance weighting (via tf-idf or bm-25) to score summaries based on their weighted lexical overlap with semantically important n-grams from source documents, while applying a length penalty to prevent trivial solutions. Experiments show the metric correlates well with human relevance judgments across multiple datasets and settings, achieving comparable results to expensive LLM-as-a-Judge methods. Importantly, the metric can be combined with reference-based metrics like ROUGE-1 to improve their robustness when references are of low quality, and it demonstrates high complementarity with other evaluation metrics.

## Method Summary
The proposed metric evaluates summaries by computing weighted lexical overlap between summary n-grams and source document n-grams, where the weights are determined by n-gram importance using tf-idf or bm-25 scoring. The method extracts n-grams from both source documents and summaries, assigns importance scores based on their semantic significance within the document corpus, and calculates a weighted overlap score. A length penalty function is applied to prevent the trivial solution of scoring entire documents highly. The final score is normalized and can be used independently or combined with reference-based metrics to improve robustness when reference quality is low.

## Key Results
- The reference-free metric achieves comparable correlation with human judgments to expensive LLM-as-a-Judge methods across multiple datasets
- When combined with ROUGE-1, the metric significantly improves robustness to low-quality references, with mixed ROUGE-1+REF metric showing superior performance to either metric alone
- The metric demonstrates high complementarity with other evaluation metrics, suggesting it captures unique aspects of summary quality
- Length penalty resolves the issue of entire documents receiving maximum scores while improving correlation with human judgment at the system level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-gram importance weighting captures semantic meaning better than raw lexical overlap.
- Mechanism: The metric uses tf-idf or bm-25 to assign weights to n-grams based on their semantic importance within the source document corpus. This prioritizes n-grams that carry more meaning rather than common words.
- Core assumption: N-grams with higher tf-idf/bm-25 scores express more semantic meaning in the context of the document corpus.
- Evidence anchors:
  - [abstract]: "We rate n-grams of the source documents relative to how much semantic meaning they express, as measured by tf-idf (Sparck Jones, 1972)"
  - [section 4]: "We propose to use n-gram importance weighting methods, such as tf-idf (Sparck Jones, 1972) or bm-25 (Robertson and Jones, 1976), to extract the n-grams expressing most of the semantic meaning of the source document."
  - [corpus]: Weak - The paper doesn't provide empirical evidence that tf-idf/bm-25 specifically capture semantic meaning better than alternatives.
- Break condition: If the tf-idf/bm-25 weighting doesn't correlate with human judgments of semantic importance, the metric would fail to capture relevance effectively.

### Mechanism 2
- Claim: Length penalty prevents the trivial solution of scoring entire documents highly.
- Mechanism: The metric multiplies the weighted overlap score by a length penalty term that decreases as the summary length approaches the document length, preventing the metric from simply rewarding copying the entire source.
- Core assumption: The length penalty function is properly calibrated to prevent trivial solutions while not unfairly penalizing genuinely relevant longer summaries.
- Evidence anchors:
  - [section 4]: "By design this score will be maximized for a summary consisting of the full document. To alleviate this issue, we penalize longer summaries"
  - [section 4]: "We observe that this length penalty not only resolves the issue related to the scoring of entire documents but also shows a stronger correlation with human judgment at the system level."
  - [corpus]: Weak - The paper mentions observation of improved correlation but doesn't provide detailed analysis of the length penalty's effectiveness across different summary lengths.
- Break condition: If the length penalty is too aggressive, it might penalize genuinely informative longer summaries; if too weak, it might allow trivial solutions.

### Mechanism 3
- Claim: Reference-free metrics are robust to reference quality variations.
- Mechanism: Since the metric doesn't rely on reference summaries, it remains stable even when reference quality degrades, unlike ROUGE which shows significant correlation drops with noisy references.
- Core assumption: Reference quality variations significantly impact reference-based metrics but not reference-free metrics.
- Evidence anchors:
  - [section 5.2]: "Our metric is not sensitive to altered references by design, contrary to ROUGE-1. When mixed with it, it improves the robustness of ROUGE-1 to low quality references."
  - [section 3]: "Reference-based metrics such as ROUGE-1 are sensitive to the quality of the references."
  - [corpus]: Moderate - The paper shows experimental evidence with RAND-3, LEAD-3, and TAIL-3 alterations demonstrating ROUGE-1's sensitivity versus their metric's stability.
- Break condition: If reference-free metrics have inherent biases or limitations that make them unreliable regardless of reference quality, the robustness claim would be invalid.

## Foundational Learning

- Concept: N-gram importance weighting (tf-idf/bm-25)
  - Why needed here: These weighting schemes determine which n-grams contribute most to the relevance score, making the metric focus on semantically important content rather than common words.
  - Quick check question: How does tf-idf differ from bm-25, and when might you prefer one over the other for summarization evaluation?

- Concept: Length penalty functions in evaluation metrics
  - Why needed here: Understanding how length penalties work is crucial for preventing trivial solutions while maintaining fair evaluation of summaries of varying lengths.
  - Quick check question: What characteristics should an ideal length penalty function have for summarization evaluation?

- Concept: System-level correlation metrics
  - Why needed here: The paper evaluates metrics based on their correlation with human judgments at the system level, requiring understanding of how to aggregate summary-level scores to system-level comparisons.
  - Quick check question: How do you calculate system-level Spearman correlation from individual summary scores?

## Architecture Onboarding

- Component map:
  - N-gram extraction module: Tokenizes documents and extracts n-grams (bigrams, trigrams, 4-grams)
  - Importance weighting engine: Computes tf-idf or bm-25 scores for each n-gram
  - Scoring module: Calculates weighted overlap between summary n-grams and source document n-grams
  - Length penalty calculator: Applies length-based adjustment to prevent trivial solutions
  - Corpus manager: Maintains and updates the document corpus for importance weighting

- Critical path:
  1. Tokenize source document and summary
  2. Extract n-grams from both
  3. Compute n-gram importance scores using tf-idf/bm-25
  4. Calculate weighted overlap score
  5. Apply length penalty
  6. Normalize and output final score

- Design tradeoffs:
  - Tokenization granularity: Character vs word vs sentence tokenization affects n-gram quality
  - N-gram size selection: Larger n-grams capture more context but may be too sparse
  - Corpus size: Larger corpora provide better importance weighting but increase computation
  - Length penalty steepness: Steeper penalties prevent trivial solutions better but may unfairly penalize longer summaries

- Failure signatures:
  - Low variance in scores across diverse summaries indicates broken weighting or scoring
  - Scores that increase monotonically with summary length suggest insufficient length penalty
  - High correlation with summary length rather than relevance indicates metric failure

- First 3 experiments:
  1. Test metric on a synthetic dataset with known extractive vs abstractive summaries to verify it penalizes extractive content appropriately
  2. Evaluate metric's sensitivity to reference quality by comparing scores on original vs corrupted references
  3. Measure correlation with human judgments across different document lengths to verify length penalty effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed metric's performance vary with different summarization datasets that have varying compression ratios, extractiveness, and styles?
- Basis in paper: [explicit] The authors acknowledge that "Our metric is also specific to the task of summarization and might correlate differently with human judgement on summarization tasks with different compression ratio, extractiveness, or style."
- Why unresolved: The authors did not have access to the data from Ladhak et al. (2022) to test the metric's sensitivity to different levels of extractiveness, and they did not conduct experiments across datasets with varying summarization characteristics.
- What evidence would resolve it: Systematic evaluation of the metric across multiple summarization datasets with different compression ratios, extractiveness levels, and writing styles, comparing its correlation with human judgment in each case.

### Open Question 2
- Question: What is the optimal n-gram size and importance weighting method for maximizing correlation with human judgment across different summarization domains?
- Basis in paper: [inferred] The authors tested various settings including different n-gram sizes (bigrams, trigrams, 4-grams) and importance weighting methods (tf-idf, bm-25), but only reported results using trigrams and tf-idf.
- Why unresolved: The paper only presents results using one combination of n-gram size and weighting method, without exploring which configuration works best across different domains or summarization tasks.
- What evidence would resolve it: Comparative analysis showing how different n-gram sizes and weighting methods perform across various summarization domains, with quantitative measurements of which combinations yield the highest correlation with human judgment.

### Open Question 3
- Question: How does the proposed metric compare to other reference-free metrics when evaluating summaries in multimodal settings (e.g., text and images)?
- Basis in paper: [explicit] The authors mention that "The prospects for future research include further exploration of the behaviour of reference-based, reference-free and hybrid metrics with references of varying quality, as well as potential extensions to multimodal settings such as the evaluation of vision-language systems."
- Why unresolved: The paper only evaluates the metric in text-only summarization settings and does not explore its applicability to multimodal summarization tasks.
- What evidence would resolve it: Experimental comparison of the metric against other reference-free metrics when evaluating multimodal summaries, measuring correlation with human judgment for both text and visual components.

### Open Question 4
- Question: What is the impact of the length penalty function on the metric's performance, and how sensitive is the metric to different penalty formulations?
- Basis in paper: [inferred] The authors tested different length penalty options but only reported using one specific formulation (αˆs,d = f(|ˆs|, |d|)), without exploring the sensitivity to different penalty functions.
- Why unresolved: The paper does not provide an analysis of how different length penalty formulations affect the metric's correlation with human judgment or its robustness to summary length variations.
- What evidence would resolve it: Comparative evaluation of different length penalty formulations, showing how each affects the metric's performance across various summarization tasks and summary length distributions.

## Limitations
- The effectiveness of tf-idf/bm-25 weighting in capturing semantic meaning versus simpler frequency-based approaches is not rigorously validated through ablation studies
- The length penalty mechanism lacks detailed sensitivity analysis across different document lengths and summary types
- The method's performance on languages other than English or on domains with different n-gram characteristics remains untested

## Confidence
- High confidence: The metric's ability to improve robustness when combined with reference-based metrics like ROUGE-1, supported by concrete experimental evidence with degraded references
- Medium confidence: The claim that n-gram importance weighting captures semantic meaning better than raw lexical overlap, as this relies on established tf-idf/bm-25 literature rather than direct validation
- Medium confidence: The length penalty effectively prevents trivial solutions while maintaining fair evaluation, though the exact calibration appears sensitive to implementation details

## Next Checks
1. Conduct ablation studies comparing tf-idf/bm-25 weighting against simpler frequency-based or uniform weighting schemes to isolate the contribution of semantic importance weighting to overall performance
2. Perform systematic sensitivity analysis on the length penalty parameters across varying document lengths and summary characteristics to identify optimal calibration ranges
3. Test the metric's generalization to non-English languages and diverse domains (e.g., scientific abstracts, news articles, meeting transcripts) to assess robustness beyond the tested CNN/DailyMail, ArXiv, and GovReport datasets