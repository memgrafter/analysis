---
ver: rpa2
title: How good is GPT at writing political speeches for the White House?
arxiv_id: '2412.14617'
source_url: https://arxiv.org/abs/2412.14617
tags:
- presidents
- language
- versions
- more
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the writing styles of two GPT models (GPT-3.5
  and GPT-4.0) with real US presidential speeches. Researchers generated State of
  the Union addresses for six presidents and analyzed them using linguistic complexity
  metrics, pronoun usage, emotional tone, and intertextual distance.
---

# How good is GPT at writing political speeches for the White House?
## Quick Facts
- arXiv ID: 2412.14617
- Source URL: https://arxiv.org/abs/2412.14617
- Authors: Jacques Savoy
- Reference count: 0
- Key finding: GPT-generated speeches remain distinguishable from authentic presidential addresses

## Executive Summary
This study evaluated the writing quality of GPT models compared to authentic US presidential speeches by generating State of the Union addresses for six presidents using both GPT-3.5 and GPT-4.0. The research analyzed linguistic complexity, emotional tone, pronoun usage, and intertextual distance to determine how closely AI-generated speeches matched authentic presidential rhetoric. Results showed that while GPT models produced more complex and emotionally positive messages with higher social status language, they still generated distinct patterns that made them easily distinguishable from real presidential speeches.

The findings reveal both capabilities and limitations of current language models for political speechwriting, with GPT-4.0 showing improvements over GPT-3.5 but neither model achieving authentic presidential style. The study highlights systematic differences in language patterns, particularly in the overuse of collective pronouns and avoidance of personal references, suggesting that while AI can assist in speech generation, it cannot yet fully replicate the nuanced rhetorical styles of US presidents.

## Method Summary
The research compared authentic State of the Union addresses from six US presidents (Barack Obama, Donald Trump, Joe Biden, George W. Bush, Bill Clinton, and Ronald Reagan) with AI-generated versions created using GPT-3.5 and GPT-4.0. Researchers used identical prompts for each president and analyzed the outputs using various linguistic metrics including lexical complexity, emotional tone, pronoun usage, and intertextual distance. The study examined how closely the AI-generated speeches matched authentic presidential rhetoric across multiple dimensions of language use and political messaging.

## Key Results
- GPT-generated speeches were shorter, more complex, and had higher language complexity than authentic presidential addresses
- AI models produced more positive emotional tone and higher social status language compared to real presidents
- GPT-4.0 showed improvements over GPT-3.5 including longer messages and more political terminology
- Both models overused collective pronouns ("we") and avoided personal references, making them distinguishable from authentic speeches

## Why This Works (Mechanism)
The mechanism behind GPT's speechwriting involves pattern recognition from training data rather than true understanding of political context or rhetorical strategy. The models generate text based on statistical correlations in language patterns rather than genuine comprehension of political messaging, leading to systematic differences from authentic presidential rhetoric.

## Foundational Learning
- Linguistic complexity analysis: Needed to quantify differences in language sophistication between AI and human writing; quick check through readability scores
- Emotional tone detection: Required to measure sentiment differences in political messaging; quick check using sentiment analysis tools
- Pronoun usage patterns: Essential for understanding how AI models handle collective vs. personal references; quick check through frequency analysis
- Intertextual distance metrics: Necessary to measure semantic similarity between authentic and AI-generated speeches; quick check through vector space analysis
- State of the Union address structure: Important for understanding the specific speech genre being analyzed; quick check through content categorization
- Political terminology identification: Required to assess how well AI captures political language; quick check through keyword frequency analysis

## Architecture Onboarding
Component map: Input prompt -> Language model (GPT-3.5/GPT-4.0) -> Text generation -> Linguistic analysis pipeline -> Comparative metrics
Critical path: Prompt design → Model selection → Text generation → Linguistic feature extraction → Statistical comparison
Design tradeoffs: Model version choice (3.5 vs 4.0) → Complexity vs authenticity → Emotional tone calibration → Pronoun usage patterns
Failure signatures: Over-complexity in simple contexts → Excessive positivity → Collective pronoun overuse → Political terminology misuse
First experiments: 1) Test different temperature settings for variation; 2) Compare different prompt engineering approaches; 3) Analyze multiple speech genres beyond State of the Union

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to State of the Union addresses, which may not represent all presidential speech contexts
- Only examined two specific GPT versions (3.5 and 4.0), excluding newer models
- Results may vary with different prompt engineering approaches or temperature settings

## Confidence
- Confidence in GPT-generated speeches being distinguishable from authentic addresses: High
- Confidence in observed differences in emotional tone and pronoun usage: Medium
- Confidence in GPT-4.0 improvements over GPT-3.5: Low-Medium

## Next Checks
1. Test additional prompt variations and temperature settings to determine if observed differences are robust across different generation parameters
2. Expand analysis to include other types of presidential speeches (inaugural addresses, press conferences, etc.) to assess whether patterns hold across different speech contexts
3. Conduct blind evaluations with human readers to determine whether the linguistic differences identified translate to perceptible differences in speech quality or authenticity