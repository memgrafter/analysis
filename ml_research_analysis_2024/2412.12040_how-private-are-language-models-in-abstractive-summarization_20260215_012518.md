---
ver: rpa2
title: How Private are Language Models in Abstractive Summarization?
arxiv_id: '2412.12040'
source_url: https://arxiv.org/abs/2412.12040
tags:
- linguistics
- computational
- summarization
- association
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the extent of personally identifiable information
  (PII) leakage in language model-generated summaries across sensitive domains. The
  authors conduct a comprehensive evaluation using two closed-source and three open-weight
  models, experimenting with various prompting strategies and instruction fine-tuning
  to improve privacy preservation.
---

# How Private are Language Models in Abstractive Summarization?

## Quick Facts
- arXiv ID: 2412.12040
- Source URL: https://arxiv.org/abs/2412.12040
- Authors: Anthony Hughes; Ning Ma; Nikolaos Aletras
- Reference count: 40
- Language models frequently leak PII in abstractive summarization, even when instructed to protect privacy

## Executive Summary
This study investigates PII leakage in language model-generated summaries across sensitive domains including medical, legal, and news data. The authors evaluate two closed-source and three open-weight models using various prompting strategies and instruction fine-tuning to improve privacy preservation. Their comprehensive analysis reveals that most models leak PII despite privacy instructions, with open-weight models showing higher vulnerability than closed-source models. Instruction fine-tuning significantly improves both privacy protection and summary quality for open-weight models, enabling them to match or exceed closed-source model performance.

## Method Summary
The study evaluates PII leakage using four summarization datasets (Brief Hospital Course, Discharge Instructions, Legal Contracts, CNN/DailyMail) across two closed-source models (GPT-4o, Claude Sonnet 3.5) and three open-weight models (Llama-3.1-8B, Llama-3.1-70B, Mistral 7B). The evaluation employs four prompting strategies: 0-shot summary, 0-shot private summary, 1-shot private summary, and Anonymize & Summarize. Privacy is measured using PTR, LDR, and TPR metrics with NER tools (spaCy, FLAIR) for PII detection. Instruction fine-tuning is applied to open-weight models using pseudonymized summary pairs to improve privacy preservation.

## Key Results
- Open-weight models leak more PII than closed-source models, even when instructed to protect privacy
- Instruction fine-tuning significantly improves both privacy preservation and summary quality for open-weight models
- The Anonymize & Summarize approach reduces PII leakage but may introduce new PII through hallucination
- Larger models (GPT-4o, Claude Sonnet 3.5, Llama-3.1-70B) are more effective at preventing PII leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction fine-tuning significantly improves privacy preservation and summary quality for open-weight models by explicitly teaching them to exclude PII during generation.
- Mechanism: By training open-weight models on paired prompts and target pseudonymized summaries, the models learn to recognize and avoid PII tokens during inference, leading to lower PTR and LDR scores.
- Core assumption: The instruction fine tuned dataset contains sufficient diverse examples of PII categories to generalize privacy-preserving behavior across domains.
- Evidence anchors: [abstract]: "Instruction fine-tuning significantly improves both privacy protection and summary quality for open-weight models"; [section]: "we use the data constructed in Subsection 3.3 to teach LMs how to generate private summaries"

### Mechanism 2
- Claim: Larger models are more effective at preventing PII leakage due to their broader knowledge and better contextual understanding.
- Mechanism: The increased model capacity and training data exposure enable these models to better distinguish between relevant summary content and PII that should be excluded, resulting in lower TPR and PTR scores.
- Core assumption: Model size correlates with improved ability to understand context-dependent privacy requirements and apply them consistently across domains.
- Evidence anchors: [abstract]: "closed-source models showing higher vulnerability than closed-source models"; [section]: "Larger models (GPT-4o, Sonnet-3.5, and Llama-3.1-70B) are more effective at privatization"

### Mechanism 3
- Claim: Using LMs to anonymize source documents before summarization reduces PII leakage more effectively than post-hoc anonymization or prompting alone.
- Mechanism: By removing PII tokens from the source document first, the subsequent summarization task has no PII content to leak, achieving the lowest PTR and LDR scores.
- Core assumption: The anonymization step can reliably identify and replace all PII tokens without degrading summary quality or introducing false positives.
- Evidence anchors: [abstract]: "Our quantitative and qualitative analysis... shows that LMs frequently leak personally identifiable information in their summaries"; [section]: "Anonymize & Summarize... consists of two steps: (1) the LM is first instructed to anonymize the input document by removing PII"

## Foundational Learning

- Concept: PII classification and detection using NER tools (spaCy, FLAIR)
  - Why needed here: The study relies on these tools to quantify PII leakage through PTR, LDR, and TPR metrics
  - Quick check question: What are the limitations of using spaCy and FLAIR for PII detection in domain-specific contexts?

- Concept: Instruction fine-tuning methodology and LoRA adapters
  - Why needed here: The study uses IFT to improve open-weight model privacy performance, requiring understanding of how to format prompts and apply LoRA
  - Quick check question: How does LoRA's rank and alpha configuration affect the fine-tuning process for privacy preservation?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The study tests multiple prompting strategies to evaluate their impact on privacy preservation
  - Quick check question: How do different prompt variations affect the model's ability to recognize and exclude PII?

## Architecture Onboarding

- Component map: Data preprocessing (pseudonymization/anonymization) -> Model inference (4 prompting methods × 5 models) -> PII detection -> Metrics calculation -> Human evaluation
- Critical path: Data preprocessing → Model inference → PII detection → Metrics calculation → Human evaluation
- Design tradeoffs: Open-weight models offer customization through IFT but require more computational resources, while closed-source models provide better baseline privacy but less flexibility
- Failure signatures: High PTR scores indicate PII leakage, low ROUGE/BERTScore suggests quality degradation, high TPR suggests context-dependent privacy failures
- First 3 experiments:
  1. Run 0-shot private summary generation on a small subset of medical data to establish baseline privacy performance
  2. Apply instruction fine-tuning to Mistral-7B and evaluate on legal contracts to measure IFT impact
  3. Test Anonymize & Summarize method on CNN/DM to assess pre-anonymization effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause instruction fine-tuning to reduce privacy token ratio (PTR) in open-weight models while sometimes increasing true positive rate (TPR)?
- Basis in paper: Inferred from results showing IFT improves PTR but increases TPR in some cases
- Why unresolved: The paper observes these conflicting effects but doesn't explain the underlying mechanism
- What evidence would resolve it: Ablation studies isolating different components of IFT training or analysis of model attention patterns during privacy-sensitive generation

### Open Question 2
- Question: How do different types of PII (names, dates, locations, organizations) leak at different rates across models and domains?
- Basis in paper: Inferred from mention of analyzing name, date, and location exposure but no detailed breakdown provided
- Why unresolved: The paper only provides aggregate metrics without granular analysis of specific PII categories
- What evidence would resolve it: Detailed per-category leakage statistics across all models, tasks, and prompting methods

### Open Question 3
- Question: What is the relationship between summary quality (ROUGE/BERTScore) and privacy preservation across different prompting strategies?
- Basis in paper: Inferred from observation that some methods improve both metrics while others trade off between them
- Why unresolved: The paper shows correlation but doesn't analyze whether better summaries inherently leak more or less PII
- What evidence would resolve it: Systematic analysis of the quality-privacy trade-off across the full range of model performances

## Limitations
- PII detection reliability is limited by NER tools generating false positives and negatives, particularly for context-dependent PII
- Instruction fine-tuning may not generalize to rare PII types or domain-specific privacy risks not covered in the training data
- Prompt variations are not fully specified, making it difficult to reproduce or optimize prompting strategies

## Confidence
- High Confidence: Open-weight models leak more PII than closed-source models; larger models perform better at privacy preservation
- Medium Confidence: Instruction fine-tuning significantly improves both privacy preservation and summary quality for open-weight models
- Low Confidence: The effectiveness of the Anonymize & Summarize method for reducing PII leakage

## Next Checks
1. Conduct a comprehensive manual review of PII detection across all datasets to establish ground truth PII spans and calculate the precision, recall, and F1-score of the NER tools
2. Evaluate instruction fine-tuned models on domains and PII types not present in the fine-tuning dataset to assess generalization capability
3. Systematically test a broader range of prompt variations to determine the optimal prompt configuration for privacy preservation while maintaining summary quality