---
ver: rpa2
title: Target-driven Attack for Large Language Models
arxiv_id: '2411.07268'
source_url: https://arxiv.org/abs/2411.07268
tags:
- attack
- text
- problem
- language
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a target-driven black-box attack method for
  large language models that maximizes the Kullback-Leibler (KL) divergence between
  clean text and adversarial text. The approach transforms the attack problem into
  a convex optimization problem and uses projected gradient descent to solve for the
  optimal adversarial text representation.
---

# Target-driven Attack for Large Language Models

## Quick Facts
- arXiv ID: 2411.07268
- Source URL: https://arxiv.org/abs/2411.07268
- Reference count: 40
- Primary result: Proposed target-driven black-box attack achieves up to 81.48% attack success rate on mathematical problems using token manipulation

## Executive Summary
This paper introduces a novel target-driven black-box attack method for large language models (LLMs) that maximizes the Kullback-Leibler (KL) divergence between clean and adversarial text. The approach transforms the attack problem into a convex optimization problem and uses projected gradient descent to solve for the optimal adversarial text representation. The method includes two attack strategies: token manipulation through synonym replacement and misinformation attacks using context-aware misleading prompts. Experimental results demonstrate significant attack success rates across multiple LLM models and datasets, with the advantage of being query-free, making it more efficient than traditional black-box attacks.

## Method Summary
The method works by maximizing KL divergence between clean and attack text conditional probabilities, which is theoretically equivalent to maximizing Mahalanobis distance between their embeddings. The attack problem is transformed into a convex optimization problem solved via projected gradient descent. Two black-box attack strategies are employed: token manipulation (synonym replacement on subject/predicate/object keywords) and misinformation attack (inserting context-aware misleading prompts generated by an assistant model). The algorithm iteratively updates attack vectors and covariance matrices until convergence, then generates adversarial text that approximates the optimal embedding vector.

## Key Results
- Token manipulation attack achieves 81.48% attack success rate on mathematical problems
- Misinformation attack achieves 79.05% attack success rate on SQuAD2.0 dataset
- Strong transferability across different models (ChatGPT, Llama-2)
- Query-free method demonstrating efficiency advantages over traditional black-box attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Maximizing KL divergence between clean and attack text conditional probabilities is equivalent to maximizing Mahalanobis distance between their embeddings
- **Mechanism**: The paper proves through Taylor expansion that KL divergence can be approximated as a quadratic form involving the inverse Fisher information matrix, which is equivalent to Mahalanobis distance when using the covariance matrix
- **Core assumption**: The LLM output distribution can be locally approximated as Gaussian around the clean input embedding
- **Evidence anchors**:
  - [abstract]: "maximize the KL divergence between the conditional probabilities of the clean text and the attack text"
  - [section]: "maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between x and z"
  - [corpus]: Weak - no direct evidence in cited papers, but related work on adversarial attacks exists
- **Break condition**: If the LLM's output distribution deviates significantly from Gaussian assumptions or if the embedding space doesn't follow the statistical properties assumed in the proof

### Mechanism 2
- **Claim**: Projected gradient descent can solve the convex optimization problem to find optimal attack embedding vectors
- **Mechanism**: The optimization problem transforms into finding vectors that maximize distance while constrained to unit norm, solved via iterative gradient updates with projection onto the constraint set
- **Core assumption**: The transformed optimization problem is convex and has a unique global optimum
- **Evidence anchors**:
  - [section]: "The projected gradient descent algorithm solves the vector corresponding to the attack text"
  - [section]: "we transformed the original problem into a convex optimization problem"
  - [corpus]: Weak - while PGD is common in adversarial attacks, the specific convex formulation here isn't directly evidenced in cited work
- **Break condition**: If the objective function or constraints change such that the problem becomes non-convex, or if gradient calculations become unstable

### Mechanism 3
- **Claim**: Two black-box attack strategies (token manipulation and misinformation) can generate adversarial text that approximates the optimal embedding vector
- **Mechanism**: Token manipulation uses synonym replacement on subject/predicate/object keywords; misinformation attack inserts context-aware misleading prompts generated by an assistant model
- **Core assumption**: The generated text embeddings will be close to the optimal vector found by optimization
- **Evidence anchors**:
  - [section]: "Two attack algorithms are designed including token manipulation attack and misinformation attack"
  - [section]: "we use the existing jailbreak template dataset to generate a candidate question set"
  - [corpus]: Moderate - related work on token manipulation exists but the specific misinformation approach is novel
- **Break condition**: If the text generation process fails to produce semantically constrained variants, or if the embedding space doesn't preserve semantic similarity well

## Foundational Learning

- **Concept**: KL divergence and its relationship to information theory
  - **Why needed here**: Forms the theoretical foundation for the attack objective function
  - **Quick check question**: What does KL divergence measure between two probability distributions?

- **Concept**: Mahalanobis distance and covariance matrix properties
  - **Why needed here**: Provides the geometric interpretation of the optimization problem
  - **Quick check question**: How does Mahalanobis distance differ from Euclidean distance?

- **Concept**: Projected gradient descent and convex optimization
  - **Why needed here**: The algorithm used to solve the constrained optimization problem
  - **Quick check question**: What makes an optimization problem convex and why does this matter for finding global optima?

## Architecture Onboarding

- **Component map**: Attack objective → Convex optimization transformation → Projected gradient descent → Text generation → Embedding similarity search
- **Critical path**: Problem formulation → Algorithm implementation → Attack strategy execution → Evaluation
- **Design tradeoffs**: Query-free vs query-based attacks (efficiency vs effectiveness), token manipulation vs misinformation (semantic preservation vs attack strength)
- **Failure signatures**: Low attack success rate despite high optimization values, text generation failing to produce valid candidates, embedding similarity not matching expected results
- **First 3 experiments**:
  1. Verify KL divergence ≈ Mahalanobis distance relationship on synthetic distributions
  2. Test projected gradient descent convergence on simple quadratic forms
  3. Validate text generation strategies produce embeddings close to optimal vectors on toy examples

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Theoretical foundation relies on local Gaussian approximations of LLM output distributions that may not hold across diverse model architectures
- Limited investigation of how architecture differences, training data overlap, or model scaling affect transferability patterns
- Heuristic attack strategies' approximation quality to optimal solutions lacks rigorous evaluation

## Confidence

**High Confidence (4/5)**: The optimization algorithm implementation and its convergence properties are well-specified. The projected gradient descent approach follows established methods, and the mathematical transformations are clearly articulated. Experimental results showing attack success rates are reproducible given the specified implementation.

**Medium Confidence (3/5)**: The theoretical framework connecting KL divergence to Mahalanobis distance is mathematically sound under stated assumptions, but the practical validity of these assumptions across diverse LLM deployments is uncertain. The attack strategies are innovative but their approximation quality to optimal solutions lacks rigorous evaluation.

**Low Confidence (2/5)**: Claims about the method being "more efficient than traditional black-box attacks" due to query-free operation are supported by comparisons to unspecified baselines. The paper doesn't benchmark against established query-based black-box attack methods or quantify the trade-off between efficiency and attack strength.

## Next Checks

1. **Approximation Quality Validation**: Generate synthetic data where the optimal attack vector is known, then measure the embedding distance between the algorithm's output and the optimal solution across varying input characteristics and model architectures.

2. **Transferability Mechanism Analysis**: Systematically vary model architectures (different families, sizes, training datasets) and quantify how attack success rates correlate with model similarity metrics, pretraining data overlap, and architectural features.

3. **Baseline Comparison Study**: Implement established query-based black-box attack methods (e.g., gradient estimation, evolutionary strategies) and compare against the proposed method across multiple dimensions: attack success rate, query efficiency, transferability, and robustness to defenses.