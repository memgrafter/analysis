---
ver: rpa2
title: 'Synergetic Event Understanding: A Collaborative Approach to Cross-Document
  Event Coreference Resolution with Large Language Models'
arxiv_id: '2406.02148'
source_url: https://arxiv.org/abs/2406.02148
tags:
- event
- mentions
- mention
- coreference
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-document event coreference
  resolution, where the goal is to cluster event mentions across multiple documents
  that refer to the same real-world events. The proposed method leverages both a large
  language model (LLM) and a small language model (SLM) in a collaborative approach.
---

# Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models

## Quick Facts
- arXiv ID: 2406.02148
- Source URL: https://arxiv.org/abs/2406.02148
- Reference count: 29
- Proposed method achieves state-of-the-art performance on cross-document event coreference resolution, improving CoNLL F1 by 1-7% on three datasets

## Executive Summary
This paper tackles the challenge of cross-document event coreference resolution by introducing a novel collaborative approach that leverages both large language models (LLMs) and small language models (SLMs). The method employs LLMs to generate concise event summaries for each mention, which are then integrated with the original documents to create joint representations for SLM-based coreference resolution. Experiments demonstrate significant improvements over existing methods across multiple datasets, highlighting the effectiveness of combining LLM-generated contextual summaries with SLM's discriminative capabilities.

## Method Summary
The proposed method follows a two-stage process to resolve cross-document event coreference. First, a large language model generates concise summaries for each event mention through a carefully designed two-step prompting process that captures essential context. These summaries are then integrated with the original document content to create enhanced representations. A small language model learns these joint representations and performs the coreference resolution task, clustering event mentions that refer to the same real-world events. This collaborative framework leverages the LLM's superior understanding of complex contexts while utilizing the SLM's efficiency for the final classification task.

## Key Results
- Achieved 1% improvement in CoNLL F1 score on ECB+ dataset compared to best existing methods
- Obtained 2.7% improvement on GVC dataset
- Demonstrated 7% improvement on FCC dataset

## Why This Works (Mechanism)
The approach works by effectively bridging the gap between complex contextual understanding and efficient classification. LLMs excel at capturing nuanced contextual information through their summary generation capability, which addresses the challenge of sparse context in individual documents. By providing these rich summaries as additional context, the SLM can make more informed coreference judgments without requiring the computational resources of a large model. This division of labor allows the system to leverage the strengths of both model types while mitigating their individual limitations.

## Foundational Learning

**Event Coreference Resolution**: The task of identifying event mentions across documents that refer to the same real-world event. Essential because it enables understanding of event narratives across multiple sources. Quick check: Can identify clusters of mentions referring to the same event across documents.

**Joint Representation Learning**: Combining multiple sources of information (original text and LLM summaries) into unified representations. Needed to provide comprehensive context for coreference decisions. Quick check: Representations capture both original document content and summary insights.

**Two-stage Prompting**: A specific prompting strategy for guiding LLM summary generation. Required to ensure summaries capture relevant contextual information without being overly verbose. Quick check: Summaries are concise yet informative enough for coreference resolution.

## Architecture Onboarding

**Component Map**: Document Mentions -> LLM Summary Generation -> Joint Representation Construction -> SLM Coreference Resolution -> Event Clusters

**Critical Path**: The LLM summary generation must complete successfully before joint representations can be constructed. The quality of LLM summaries directly impacts SLM performance, making this the most critical path in the pipeline.

**Design Tradeoffs**: The approach trades increased computation (LLM summary generation for each mention) for improved accuracy. Alternative designs might use a single large model or simpler context augmentation methods, but these would either be computationally prohibitive or less effective at capturing complex contexts.

**Failure Signatures**: Poor coreference performance indicates either inadequate LLM summaries (missing key context) or SLM's inability to effectively learn from the joint representations. Systematic errors in specific domains suggest domain adaptation issues.

**3 First Experiments**: 1) Evaluate summary quality independently using human or automated metrics. 2) Test coreference performance with and without summaries to isolate their impact. 3) Compare different SLM architectures to identify optimal model for joint representation learning.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on LLM-generated summaries may introduce errors if summarization fails to capture essential context
- Performance and generalizability on non-news domains remains untested
- Computational cost of generating LLM summaries for each mention may be prohibitive for large-scale applications

## Confidence
- High confidence in observed performance improvements on tested datasets (specific metrics provided)
- Medium confidence in claims about general applicability to other domains or languages (limited experimental scope)
- High confidence in the effectiveness of the collaborative LLM-SLM framework based on experimental results

## Next Checks
1. Evaluate the approach on datasets from diverse domains (e.g., scientific literature, social media) to assess generalizability
2. Conduct ablation studies to quantify the impact of LLM-generated summaries on overall performance
3. Analyze the computational efficiency and scalability of the method for large-scale event coreference tasks