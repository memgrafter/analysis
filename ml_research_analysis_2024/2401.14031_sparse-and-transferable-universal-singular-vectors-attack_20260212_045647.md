---
ver: rpa2
title: Sparse and Transferable Universal Singular Vectors Attack
arxiv_id: '2401.14031'
source_url: https://arxiv.org/abs/2401.14031
tags:
- attack
- adversarial
- sparse
- universal
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel sparse universal adversarial attack
  method based on truncated power iteration for computing sparse (p,q)-singular vectors
  of hidden layer Jacobians. The method generates perturbations that damage only 5%
  of pixels while achieving 50% fooling rate on ImageNet, outperforming dense baselines.
---

# Sparse and Transferable Universal Singular Vectors Attack

## Quick Facts
- arXiv ID: 2401.14031
- Source URL: https://arxiv.org/abs/2401.14031
- Authors: Kseniia Kuvshinova; Olga Tsymboi; Ivan Oseledets
- Reference count: 19
- Primary result: >50% fooling rate on ImageNet while damaging only 5% of pixels

## Executive Summary
This paper introduces a novel sparse universal adversarial attack method based on truncated power iteration for computing sparse (p,q)-singular vectors of hidden layer Jacobians. The attack generates perturbations that damage only 5% of pixels while achieving over 50% fooling rate on ImageNet, significantly outperforming dense baselines. The method demonstrates high transferability across different models and requires only 256 training samples, making it both efficient and effective.

## Method Summary
The proposed approach computes sparse singular vectors of hidden layer Jacobians using truncated power iteration, creating universal perturbations that can attack multiple images simultaneously. The method generates sparse perturbations that maintain human perception while effectively fooling deep learning models. By focusing on sparse attacks rather than dense perturbations, the technique achieves higher attack magnitudes without compromising visual quality, addressing the limitations of traditional universal adversarial attacks.

## Key Results
- Achieves >50% fooling rate on ImageNet with only 5% pixel damage
- Outperforms dense baseline attacks while maintaining human perception
- Demonstrates high transferability across different model architectures

## Why This Works (Mechanism)
The method leverages the mathematical properties of singular vectors in Jacobian matrices of hidden layers to identify the most influential perturbation directions. By using truncated power iteration, it efficiently computes sparse singular vectors that correspond to the most vulnerable directions in the model's decision space. The sparsity constraint ensures that only the most critical pixels are modified, making the attack both effective and perceptually subtle. The universal nature of the perturbations allows them to generalize across multiple images, while their transferability across models indicates that they exploit fundamental vulnerabilities in neural network architectures.

## Foundational Learning
- Singular Value Decomposition (SVD): Understanding matrix decomposition is crucial for grasping how singular vectors identify important directions in high-dimensional spaces
  - Why needed: The attack fundamentally relies on computing singular vectors of Jacobian matrices
  - Quick check: Can you explain how SVD helps identify the most influential directions in a matrix?

- Jacobian Matrices: Knowledge of how Jacobians represent gradients in neural networks is essential
  - Why needed: The attack targets the Jacobian of hidden layers to find vulnerability directions
  - Quick check: How does the Jacobian matrix relate to model sensitivity and vulnerability?

- Truncated Power Iteration: Understanding this optimization technique is key to comprehending the sparse computation method
  - Why needed: This is the core algorithm used to compute sparse singular vectors efficiently
  - Quick check: What advantages does truncated power iteration offer over standard power iteration for sparse computations?

- Universal Adversarial Attacks: Familiarity with the concept of attacks that generalize across multiple inputs
  - Why needed: The method creates perturbations that work across many images simultaneously
  - Quick check: How do universal attacks differ from instance-specific adversarial attacks?

## Architecture Onboarding

Component Map:
Data Preparation -> Jacobian Computation -> Truncated Power Iteration -> Sparse Vector Generation -> Universal Perturbation Application

Critical Path:
The critical path involves computing the Jacobian of hidden layers, followed by truncated power iteration to find sparse singular vectors, and finally generating the universal perturbation. The efficiency of the truncated power iteration step is crucial for practical implementation.

Design Tradeoffs:
- Sparsity vs. Effectiveness: Higher sparsity reduces pixel damage but may decrease attack success rate
- Computational Cost vs. Accuracy: More iterations in power iteration improve accuracy but increase computation time
- Universal vs. Instance-Specific: Universal attacks are more efficient but may be slightly less effective than tailored attacks

Failure Signatures:
- Poor transferability across models indicates the attack may be exploiting model-specific vulnerabilities
- Low fooling rates despite high sparsity suggest the singular vector computation may not be capturing the most influential directions
- Computational instability during power iteration could indicate numerical precision issues

First Experiments:
1. Test the attack on a small subset of ImageNet with varying sparsity levels to observe the tradeoff curve
2. Compare the transferability of sparse vs. dense universal perturbations across different model architectures
3. Measure the computational time for truncated power iteration with different iteration counts and convergence criteria

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalization of sparse universal attacks to different datasets and model architectures, the theoretical understanding of why sparse attacks are more transferable, and the potential for developing robust defenses against such attacks.

## Limitations
- The attack's generalizability across different datasets beyond ImageNet remains uncertain
- The method's computational efficiency and scalability to larger models are not fully explored
- The perceptual thresholds for different image types and human observers need more comprehensive validation

## Confidence

| Claim | Confidence |
|-------|------------|
| >50% fooling rate with 5% pixel damage | High |
| Superior performance vs dense baselines | High |
| High transferability across models | Medium |
| Only 256 training samples needed | Medium |
| Higher attack magnitudes without affecting perception | Medium |

## Next Checks
1. Test the attack method across diverse datasets beyond ImageNet to evaluate generalization
2. Conduct experiments with varying training sample sizes to determine minimum requirements for effective attacks
3. Perform large-scale transferability tests across a wider range of model architectures and families