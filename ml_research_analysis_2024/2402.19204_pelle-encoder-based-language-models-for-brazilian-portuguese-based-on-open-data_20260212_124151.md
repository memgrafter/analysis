---
ver: rpa2
title: 'PeLLE: Encoder-based language models for Brazilian Portuguese based on open
  data'
arxiv_id: '2402.19204'
source_url: https://arxiv.org/abs/2402.19204
tags:
- data
- corpus
- language
- portuguese
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces PeLLE, a family of large language models
  for Brazilian Portuguese based on the RoBERTa architecture, trained on the curated
  Carolina Corpus. The authors present three models: pPeLLE (trained from scratch),
  xPeLLE (extended from XLM-R), and mPeLLE (extended from mBERT), all using the same
  open, curated dataset.'
---

# PeLLE: Encoder-based language models for Brazilian Portuguese based on open data

## Quick Facts
- arXiv ID: 2402.19204
- Source URL: https://arxiv.org/abs/2402.19204
- Authors: Guilherme Lamartine de Mello; Marcelo Finger; Felipe Serras; Miguel de Mello Carpi; Marcos Menon Jose; Pedro Henrique Domingues; Paulo Cavalim
- Reference count: 5
- Primary result: PeLLE models trained on curated Carolina Corpus achieve competitive performance on Brazilian Portuguese NLP tasks

## Executive Summary
This paper introduces PeLLE, a family of encoder-based language models for Brazilian Portuguese built on the RoBERTa architecture. The authors present three models: pPeLLE (trained from scratch), xPeLLE (extended from XLM-R), and mPeLLE (extended from mBERT), all using the same curated Carolina Corpus. These models are evaluated on downstream NLP tasks including semantic similarity, natural language inference, hate speech detection, and legal document classification. Results demonstrate that while larger models generally perform better, the curated nature of the training data enables competitive performance even with smaller models, particularly for domain-specific tasks like legal document classification.

## Method Summary
The PeLLE models are based on the RoBERTa architecture with 12 transformer layers and 768 hidden size. pPeLLE is trained from scratch using a Byte-Pair Encoding vocabulary of 50k sub-word units, while xPeLLE and mPeLLE are extended from multilingual models (XLM-R and mBERT respectively). All models are trained on the Carolina Corpus, a curated dataset of 823M tokens spanning juridical, instructional, and journalistic domains. The pretraining uses Masked Language Modeling, followed by fine-tuning on downstream tasks with task-specific heads. Fine-tuning employs Population Based Training for semantic similarity tasks and fixed hyperparameters for hate speech detection and legal document classification.

## Key Results
- pPeLLE achieves competitive performance despite being trained from scratch on curated data
- xPeLLE outperforms other PeLLE variants, validating multilingual initialization for Portuguese adaptation
- Larger models generally perform better, but curated data compensates for smaller sizes on domain-specific tasks
- pPeLLE shows particularly strong performance on legal document classification tasks

## Why This Works (Mechanism)

### Mechanism 1
Curated, high-quality open data (Carolina Corpus) yields competitive downstream performance even with smaller models. Quality data reduces noise and domain mismatch, enabling better generalization from fewer tokens. The corpus includes typological diversity (juridical, instructional, journalistic) explicitly designed for NLP pretraining.

### Mechanism 2
Weight initialization from multilingual models (mBERT/XLM-R) provides strong baseline for Portuguese adaptation. Multilingual pretraining provides cross-lingual transfer knowledge that can be fine-tuned for Portuguese-specific tasks. This approach leverages cross-lingual representations learned during multilingual pretraining.

### Mechanism 3
Larger model sizes generally improve performance, but domain-specific curation can compensate for smaller sizes. Model capacity and data quality interact - high-quality data can partially offset limited model size. This creates a trade-off curve where good curation can substitute for scale in certain tasks.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) pretraining objective
  - Why needed here: pPeLLE is trained from scratch using MLM, and understanding this objective is crucial for interpreting pretraining choices
  - Quick check question: What does MLM do during pretraining, and how does it differ from autoregressive objectives?

- Concept: Cross-lingual transfer learning
  - Why needed here: xPeLLE and mPeLLE extend multilingual models, and understanding when cross-lingual transfer works is key to interpreting results
  - Quick check question: Under what conditions does initializing from multilingual models help versus hurting Portuguese-specific performance?

- Concept: Domain adaptation and fine-tuning strategies
  - Why needed here: The paper evaluates models on downstream tasks requiring adaptation from pretraining to specific domains (legal, hate speech, semantic similarity)
  - Quick check question: How does fine-tuning differ from continued pretraining, and when would you choose one over the other?

## Architecture Onboarding

- Component map: Data curation -> vocabulary building -> MLM pretraining -> fine-tuning evaluation
- Critical path: Data curation → vocabulary building → MLM pretraining → fine-tuning evaluation
- Design tradeoffs:
  - Open vs proprietary data: Enables reproducibility but may limit domain coverage
  - Size vs curation: Smaller curated corpus vs larger web-crawled data (BrWaC)
  - Multilingual vs monolingual: Cross-lingual transfer vs Portuguese-specific optimization
  - Model size vs compute budget: Base vs large models for different deployment scenarios
- Failure signatures:
  - Poor downstream performance despite good pretraining loss: Domain mismatch or task-specific feature learning failure
  - Overfitting on pretraining data: Too small or too domain-specific pretraining corpus
  - Suboptimal multilingual initialization: Cross-lingual representations don't transfer well to Portuguese
  - Unicode normalization issues: Portuguese-specific characters not properly handled
- First 3 experiments:
  1. Compare pretraining loss curves for pPeLLE vs mPeLLE/xPeLLE to understand quality vs quantity effects
  2. Ablation study: Pretrain on subsets of Carolina Corpus (different domains) to identify most valuable data types
  3. Cross-lingual probing: Test mPeLLE/xPeLLE on non-Portuguese languages to quantify cross-lingual transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of PeLLE models scale with increased training data size from the Carolina Corpus? The current study only evaluates models trained on version 1.2 (0.82B tokens). Training and evaluating PeLLE models on successive versions with increasing token sizes would identify scaling trends.

### Open Question 2
Which specific characteristics of the Carolina Corpus contribute most to improved performance on legal-domain tasks like Acórdãos TCU? While the paper mentions juridical texts, it doesn't analyze which specific document types, vocabulary, or linguistic features drive the legal-domain performance advantage.

### Open Question 3
How does the curated nature of Carolina Corpus compare to non-curated large-scale web corpora in terms of pretraining efficiency and downstream task performance? The study contrasts smaller-but-curated pretraining data with larger non-curated datasets but doesn't systematically vary corpus size while controlling for curation level.

## Limitations

- The 823M token corpus size is significantly smaller than typical pretraining corpora, raising questions about generalization to out-of-domain scenarios
- Focus on Brazilian Portuguese may limit cross-lingual transfer capabilities, particularly for European Portuguese variants
- Evaluation on only four downstream tasks doesn't comprehensively test the models' capabilities across the full spectrum of language understanding challenges

## Confidence

**High Confidence**: Larger models generally perform better aligns with established literature and is supported by consistent performance patterns across multiple tasks.

**Medium Confidence**: Curated data can compensate for smaller model sizes is supported by task-specific results, but generalizability to other domains or languages remains uncertain.

**Low Confidence**: The Carolina Corpus being "highly curated" lacks quantitative measures of curation quality, making it difficult to assess whether curation process itself is responsible for observed benefits.

## Next Checks

1. **Domain Generalization Test**: Evaluate PeLLE models on out-of-domain Portuguese text (e.g., social media, technical documentation) to quantify the impact of curated training data on generalization.

2. **Curation Quality Analysis**: Conduct ablation studies by pretraining models on progressively less-curated versions of the corpus to quantify the contribution of curation versus raw data quantity.

3. **Cross-Lingual Transfer Verification**: Systematically test xPeLLE and mPeLLE on European Portuguese and other Romance languages to quantify cross-lingual transfer capabilities.