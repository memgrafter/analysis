---
ver: rpa2
title: Mitigating the Bias in the Model for Continual Test-Time Adaptation
arxiv_id: '2403.01344'
source_url: https://arxiv.org/abs/2403.01344
tags:
- leftr
- target
- adaptation
- test-time
- eata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bias in continual test-time adaptation (CTA)
  models, which can lead to overconfident predictions favoring certain classes. The
  proposed method uses class-wise EMA target prototypes to cluster target features
  and aligns target distributions to source prototypes via prototype matching.
---

# Mitigating the Bias in the Model for Continual Test-Time Adaptation

## Quick Facts
- arXiv ID: 2403.01344
- Source URL: https://arxiv.org/abs/2403.01344
- Reference count: 24
- Key outcome: Proposes method to reduce bias in continual test-time adaptation using class-wise EMA target prototypes and source distribution alignment, significantly improving accuracy and calibration on ImageNet-C and CIFAR100-C benchmarks.

## Executive Summary
This paper addresses the issue of bias in continual test-time adaptation (CTA), where models become overconfident and favor certain classes during adaptation to streaming target data. The authors propose a method that uses class-wise exponential moving average (EMA) target prototypes to cluster target features and aligns target distributions to source prototypes via prototype matching. This approach significantly improves accuracy and calibration on standard benchmarks (ImageNet-C and CIFAR100-C) when applied to existing CTA methods, with minimal adaptation overhead.

## Method Summary
The method addresses bias in CTA by introducing class-wise EMA target prototypes and source distribution alignment. For each class, EMA target prototypes are maintained using reliable (low-entropy) target samples, which guide feature clustering by pulling samples toward their class prototype while pushing them away from others. Additionally, target features are aligned to their corresponding source prototypes via MSE minimization to prevent excessive drift from the original source distribution. The approach is compatible with existing CTA methods and can be applied as a plug-and-play component. Hyperparameters include α=0.996 for EMA update rate, entropy threshold E0=0.4×lnC for filtering reliable samples, and alignment weights λema=2.0 and λsrc=50.

## Key Results
- Significantly improves accuracy on ImageNet-C and CIFAR100-C benchmarks when applied to existing CTA methods
- Mitigates overconfident predictions and calibration issues in model predictions
- Maintains minimal adaptation overhead while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using class-wise EMA target prototypes enables the model to capture the changing target distribution more robustly, reducing class-specific bias.
- Mechanism: The model continuously updates prototypes for each class using reliable target samples via EMA. These prototypes then guide feature clustering by pulling samples closer to their corresponding class prototype while pushing them away from others. This ensures the model adapts to the evolving target domain without overfitting to current data.
- Core assumption: The target domain features can be meaningfully clustered into classes, and reliable samples (low entropy) are representative of their class.
- Evidence anchors:
  - [abstract] "To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely."
  - [section] "These EMA target prototypes are utilized to organize the target features into distinct classes by pulling them closer to their corresponding EMA prototypes while simultaneously pushing them away from other irrelevant prototypes."
- Break condition: If the target domain shifts drastically or samples become too ambiguous to cluster reliably, the EMA prototypes may become outdated or noisy, breaking the clustering mechanism.

### Mechanism 2
- Claim: Aligning target features to source prototypes prevents the model from drifting too far from the original source distribution.
- Mechanism: Source prototypes are precomputed from the source domain and stored. During adaptation, each target feature is pulled toward its corresponding source prototype via MSE minimization. This regularizes the feature space to remain consistent with the source distribution.
- Core assumption: The source domain contains representative class prototypes that remain relevant despite target domain shifts.
- Evidence anchors:
  - [abstract] "Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype."
  - [section] "We align the target data distribution to the source distribution by minimizing the distance between the target feature and its corresponding source prototype."
- Break condition: If the source and target domains become fundamentally different (e.g., new classes or drastically different data), aligning to source prototypes may harm adaptation.

### Mechanism 3
- Claim: Filtering out high-entropy (low-confidence) samples improves the reliability of prototype updates and loss computations.
- Mechanism: Only samples with entropy below a threshold are used for computing both EMA target prototypes and alignment losses. This ensures that only confident predictions contribute to adaptation.
- Core assumption: High-entropy samples are likely to be mislabeled or ambiguous, and excluding them improves adaptation quality.
- Evidence anchors:
  - [section] "Before computing the loss, we first identify reliable target samples as proposed in (Niu et al., 2022), which excludes samples with high entropy, thus low confidence."
- Break condition: If reliable samples become too scarce (e.g., early in adaptation or in highly uncertain domains), the method may overfit to a small subset or fail to adapt.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA is used to update target prototypes incrementally with streaming data, ensuring smooth adaptation to distribution shifts.
  - Quick check question: How does EMA differ from a simple average, and why is it better for streaming data?
- Concept: Entropy as confidence measure
  - Why needed here: Entropy quantifies prediction uncertainty; low-entropy samples are deemed reliable for prototype updates.
  - Quick check question: What does high entropy indicate about a model's prediction, and how is it used to filter samples?
- Concept: Prototype-based feature clustering
  - Why needed here: Prototypes represent class centroids; pulling features toward them encourages class-specific clustering in feature space.
  - Quick check question: How does dot-product similarity between features and prototypes relate to classification confidence?

## Architecture Onboarding

- Component map:
  - Feature extractor `fϕ` (backbone)
  - Classification head `hω` (class prototypes)
  - EMA target prototype buffer `P^t` (class-wise, updated online)
  - Source prototype buffer `P^s` (precomputed, static)
  - Entropy filter (confidence threshold)
  - Loss components: unsupervised loss `L_unsup`, EMA loss `L_ema`, source alignment loss `L_src`
- Critical path:
  1. Input → feature extraction → prediction
  2. Entropy filtering → select reliable samples
  3. Compute EMA loss using `P^t` → update `P^t` via EMA
  4. Compute source alignment loss using `P^s`
  5. Backpropagate combined loss to update `fϕ` only
- Design tradeoffs:
  - Using only `fϕ` updates reduces adaptation overhead but may limit classifier flexibility.
  - Relying on pseudo-labels risks error propagation if unreliable samples slip through.
  - EMA smoothing trades responsiveness for stability.
- Failure signatures:
  - Overconfident but wrong predictions → high accuracy drop, low entropy → check prototype quality and entropy threshold.
  - Class imbalance in predictions → inspect EMA updates and prototype initialization.
  - Slow adaptation → increase EMA update rate or lower entropy threshold.
- First 3 experiments:
  1. Baseline: Run without `L_ema` and `L_src` to measure baseline bias.
  2. Ablation: Remove entropy filtering to see impact of unreliable samples.
  3. Sensitivity: Vary EMA update rate `α` and alignment weight `λ_src` to tune adaptation stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method handle class imbalance in target domains, and what impact does this have on overall performance?
- Basis in paper: [inferred] The paper mentions that the model shows bias towards certain classes and discusses the mitigation of this bias. However, it does not explicitly address class imbalance in target domains or its impact on performance.
- Why unresolved: The paper focuses on mitigating bias in predictions but does not delve into the specific issue of class imbalance in target domains or its potential effects on the model's performance.
- What evidence would resolve it: Conducting experiments with imbalanced target domains and analyzing the model's performance and bias mitigation in such scenarios would provide insights into how the proposed method handles class imbalance.

### Open Question 2
- Question: How sensitive is the proposed method to the choice of the entropy threshold (E0) for identifying reliable samples, and what is the optimal range for this parameter?
- Basis in paper: [explicit] The paper mentions that the entropy threshold E0 is set to 0.4 × ln C, following a previous work (Niu et al., 2022). However, it does not explore the sensitivity of the method to this parameter or provide an optimal range.
- Why unresolved: The paper does not conduct an in-depth analysis of the impact of different entropy threshold values on the performance of the proposed method, leaving the sensitivity and optimal range of this parameter unexplored.
- What evidence would resolve it: Performing experiments with varying entropy threshold values and analyzing the corresponding performance of the proposed method would help determine its sensitivity to this parameter and identify the optimal range for E0.

### Open Question 3
- Question: How does the proposed method compare to other state-of-the-art methods for test-time adaptation in terms of calibration and uncertainty estimation?
- Basis in paper: [inferred] The paper discusses the mitigation of overconfident predictions and the improvement in calibration through the proposed method. However, it does not directly compare the calibration and uncertainty estimation capabilities of the proposed method with other state-of-the-art approaches.
- Why unresolved: While the paper highlights the improvement in calibration achieved by the proposed method, it does not provide a comprehensive comparison with other methods in terms of calibration and uncertainty estimation.
- What evidence would resolve it: Conducting experiments to evaluate the calibration and uncertainty estimation performance of the proposed method alongside other state-of-the-art approaches would provide a clearer understanding of its strengths and limitations in these aspects.

## Limitations
- The method assumes target domain features can be meaningfully clustered into classes, which may not hold for drastically shifted or ambiguous domains.
- Source distribution alignment may harm adaptation if source and target domains become fundamentally different.
- Effectiveness depends on reliable sample availability; scarcity of low-entropy samples may lead to overfitting or adaptation failure.

## Confidence
- High confidence: The mechanism of using EMA target prototypes to reduce class-specific bias and improve adaptation robustness is well-supported.
- Medium confidence: The effectiveness of source prototype alignment depends on the relevance of source prototypes to the target domain, introducing uncertainty.
- Medium confidence: The reliability of entropy filtering depends on the quality of pseudo-labels and the chosen threshold, affecting its effectiveness.

## Next Checks
1. **Ablation study on entropy threshold**: Systematically vary the entropy threshold E0 and measure its impact on adaptation accuracy and calibration to determine the optimal balance between reliability and adaptation speed.
2. **Robustness to domain shift**: Test the method on a dataset with known and controlled domain shifts (e.g., gradual corruption increase) to evaluate the stability of EMA prototypes and the effectiveness of source alignment under varying shift magnitudes.
3. **Sample efficiency analysis**: Evaluate the method's performance with varying batch sizes to quantify the minimum number of reliable samples needed for effective prototype updates and identify potential failure modes with small batch sizes.