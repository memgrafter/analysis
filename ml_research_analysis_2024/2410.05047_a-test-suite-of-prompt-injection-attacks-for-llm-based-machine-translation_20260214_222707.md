---
ver: rpa2
title: A test suite of prompt injection attacks for LLM-based machine translation
arxiv_id: '2410.05047'
source_url: https://arxiv.org/abs/2410.05047
tags:
- shot
- json
- format
- direct
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a test suite of prompt injection attacks (PIAs)
  for LLM-based machine translation systems, extending prior work to all language
  pairs in the WMT 2024 General Machine Translation task. The authors evaluate five
  PIA variants plus a clean baseline on submissions from 50+ systems, measuring translation
  quality degradation using BLEU, chrF++, and task-specific metrics.
---

# A test suite of prompt injection attacks for LLM-based machine translation

## Quick Facts
- arXiv ID: 2410.05047
- Source URL: https://arxiv.org/abs/2410.05047
- Reference count: 5
- This paper presents a comprehensive test suite of prompt injection attacks for LLM-based machine translation systems across all WMT 2024 General Machine Translation language pairs.

## Executive Summary
This paper introduces a test suite of prompt injection attacks (PIAs) specifically designed to evaluate the robustness of LLM-based machine translation systems. The authors extend previous work by creating attack variants for all language pairs in the WMT 2024 General Machine Translation task and evaluate 50+ system submissions using multiple quality metrics. The study reveals that commercial online MT systems demonstrate the strongest resistance to PIAs, while LLM-based systems fine-tuned on translation data show intermediate robustness. The test suite and evaluation code are made publicly available for future research.

## Method Summary
The authors developed a test suite consisting of five prompt injection attack variants plus a clean baseline, designed to be language-agnostic and applicable to all language pairs in the WMT 2024 General Machine Translation task. Each attack variant was constructed by modifying system prompts with specific instructions that contradict the intended translation task. The evaluation used submissions from 50+ systems participating in the WMT shared task, measuring translation quality degradation through BLEU, chrF++, and task-specific metrics. The attacks were applied as single-round, single-line interactions due to competition format constraints, preventing exploration of more complex multi-turn or multi-line attack formats.

## Key Results
- Commercial online MT systems demonstrated the strongest robustness against prompt injection attacks
- LLM-based systems fine-tuned with translation data showed intermediate resistance, outperforming other LLM-based approaches
- English-source attacks proved more effective than non-English attacks, particularly against commercial systems
- BLEU scores dropped below 10 for some language pairs and attack combinations
- A positive correlation was observed between baseline translation quality and resistance to prompt injection attacks

## Why This Works (Mechanism)
The effectiveness of prompt injection attacks stems from the fundamental design of LLM-based translation systems, which rely on interpreting and following instructions embedded in prompts. When attackers inject contradictory or task-disrupting instructions into the prompt, they exploit the model's tendency to prioritize recent or prominently placed instructions over its core translation objective. This vulnerability is particularly pronounced in systems that haven't been specifically trained or fine-tuned to resist such adversarial manipulations, allowing the injected instructions to override the translation task.

## Foundational Learning
- **Prompt injection attack variants**: Different methods of embedding adversarial instructions in translation prompts; needed to understand attack diversity and effectiveness patterns.
- **Translation quality metrics (BLEU, chrF++)**: Standardized evaluation measures for translation output; needed to quantify attack impact across different system types.
- **LLM-based MT architectures**: Neural translation systems built on large language models; needed to understand why these systems are vulnerable to prompt injection.
- **Fine-tuning strategies**: Different approaches to adapting LLMs for translation tasks; needed to explain variation in attack resistance across systems.
- **Language pair resource levels**: The amount of training data available for different language combinations; needed to contextualize performance differences.
- **Instruction following behavior**: How LLMs process and prioritize instructions in prompts; needed to explain the mechanism of prompt injection attacks.

## Architecture Onboarding

Component Map: Input Prompt -> Instruction Processing -> Translation Generation -> Output

Critical Path: The sequence from prompt injection through instruction processing to final translation output represents the critical vulnerability chain. Attacks succeed when injected instructions disrupt normal processing flow.

Design Tradeoffs: The study reveals a fundamental tension between instruction-following flexibility (which makes LLMs useful for diverse tasks) and robustness to adversarial manipulations. Systems prioritizing translation accuracy may be more vulnerable to instruction overrides.

Failure Signatures: Quality degradation below BLEU score 10, generation of non-translation outputs, and complete failure to produce coherent translations are key failure indicators. Commercial systems show fewer such failures.

First Experiments:
1. Test the same prompt injection attacks on a small set of non-WMT domain data to assess domain-specific vulnerabilities
2. Apply individual attack components in isolation to identify the most critical vulnerability factors
3. Compare attack effectiveness across different fine-tuning strategies while controlling for model size

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation was constrained by WMT shared task format, limiting attacks to single-round, single-line interactions
- Results may not generalize beyond the WMT 2024 General Machine Translation task domain
- The study didn't segment results by resource level of language pairs
- Only 50+ systems from a single competition were evaluated, potentially missing broader patterns

## Confidence
- High confidence in the empirical results for the specific test suite and evaluation methodology
- Medium confidence in the comparative robustness rankings between commercial and LLM-based systems
- Medium confidence in the generalizability of attack effectiveness patterns across language pairs
- Low confidence in the theoretical explanations for observed correlations and attack effectiveness differences

## Next Checks
1. Test the same prompt injection attacks on non-WMT domain data (medical, legal, technical) to assess domain robustness
2. Replicate the evaluation with alternative PIA variants beyond the five tested to validate the attack effectiveness hierarchy
3. Conduct ablation studies removing individual components of the prompt injection attacks to identify the most critical vulnerability factors