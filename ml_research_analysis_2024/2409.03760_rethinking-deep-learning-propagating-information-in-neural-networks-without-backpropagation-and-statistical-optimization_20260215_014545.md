---
ver: rpa2
title: 'Rethinking Deep Learning: Propagating Information in Neural Networks without
  Backpropagation and Statistical Optimization'
arxiv_id: '2409.03760'
source_url: https://arxiv.org/abs/2409.03760
tags:
- layers
- output
- information
- neural
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the information propagation capabilities of
  neural networks (NNs) without using statistical weight optimization techniques like
  backpropagation. A simple fully connected NN with 0-15 hidden layers and step function
  activation was tested on the MNIST handwritten digit recognition task.
---

# Rethinking Deep Learning: Propagating Information in Neural Networks without Backpropagation and Statistical Optimization

## Quick Facts
- **arXiv ID**: 2409.03760
- **Source URL**: https://arxiv.org/abs/2409.03760
- **Reference count**: 28
- **Primary result**: Simple neural networks without backpropagation can achieve ~80% accuracy on MNIST using vector similarity classification

## Executive Summary
This study investigates whether neural networks can propagate information correctly without using backpropagation or statistical weight optimization. The research demonstrates that a fully connected neural network with step function activation and randomly initialized weights can achieve approximately 80% accuracy on the MNIST handwritten digit recognition task. The accuracy is computed by comparing average output vectors of training data for each label with test data outputs using vector similarity. Results show that accuracy decreases as the number of hidden layers increases, attributed to reduced variance in output vectors, indicating information smoothing.

## Method Summary
The method employs a fully connected neural network with 784 input nodes, 0-15 hidden layers, and 784 output nodes. Weights are initialized from a uniform distribution U(-sqrt(1/784), sqrt(1/784)) and remain fixed throughout training. The network uses step function activation, creating binary outputs. Classification is performed by computing average output vectors for each class from training data, then measuring Euclidean distance between test outputs and these class averages. The label with minimum distance is assigned to each test sample.

## Key Results
- Maximum accuracy of ~80% achieved with 0-2 hidden layers on MNIST
- Accuracy decreases as number of hidden layers increases
- Variance in output vectors decreases with more hidden layers, correlating with accuracy decline
- Information propagation occurs without backpropagation or statistical weight optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks can propagate information correctly without backpropagation or statistical weight optimization.
- Mechanism: Step-function activation with randomly initialized weights can maintain discriminative signal through the network, allowing similarity-based classification without learned weights.
- Core assumption: The variance in output vectors preserves sufficient discriminative information for similarity-based classification.
- Evidence anchors:
  - [abstract] "This study explored the information propagation capabilities of neural networks (NNs) without using statistical weight optimization techniques like backpropagation."
  - [section] "The results showed that the maximum accuracy achieved is around 80%... indicates that NNs can propagate information correctly without using statistical weight optimization."
  - [corpus] Weak evidence: corpus papers are not directly on non-backprop classification, but on related topics like NN architecture, residual networks, and inference-time leakage.
- Break condition: If output variance drops below a threshold that destroys discriminative power, accuracy will fall to chance levels.

### Mechanism 2
- Claim: Accuracy decreases with more hidden layers due to smoothing of output vectors.
- Mechanism: Each additional layer with step function activation and random weights progressively reduces variance in the output space, causing different classes to become less distinguishable.
- Core assumption: Variance in output vectors is a proxy for the amount of discriminative information retained.
- Evidence anchors:
  - [section] "The accuracy decreased with an increasing number of hidden layers... attributed to the decrease in the variance of the output vectors as the number of hidden layers increases."
  - [section] "Figure 2... shows the average variance of the calculated output vectors was computed for each number of hidden layers... the variance decreases as the number of hidden layers increases up to around 10 layers."
  - [corpus] No direct corpus support for variance-smoothing relationship; this is a novel claim in the paper.
- Break condition: If variance stabilizes (as observed after ~10 layers), accuracy will plateau at a low level rather than continuing to decrease.

### Mechanism 3
- Claim: Distributed representation similarity can be used for classification without softmax or learned weights.
- Mechanism: By computing average output vectors for each class during training and comparing test outputs to these averages using Euclidean distance, the network can classify based on proximity in output space.
- Core assumption: Random weights and step functions produce consistent clustering of classes in output space, making class averages meaningful.
- Evidence anchors:
  - [abstract] "The accuracy is calculated by comparing the average output vectors of the training data for each label with the output vectors of the test data, based on vector similarity."
  - [section] "This verification method is inspired by distributed representation... In this study, distributed representation is applied to calculate the accuracy rate from the similarity between output vectors."
  - [corpus] No corpus support; this is a novel application of distributed representation to non-learned networks.
- Break condition: If random initialization produces inconsistent clustering across runs, class averages become unreliable and accuracy will degrade.

## Foundational Learning

- Concept: Vector similarity and Euclidean distance
  - Why needed here: The classification method relies on comparing output vectors using Euclidean distance to compute similarity.
  - Quick check question: Given two vectors A = [1, 2, 3] and B = [4, 5, 6], what is their Euclidean distance?

- Concept: Step function activation and its effects
  - Why needed here: The network uses step functions, which create binary outputs that affect information propagation and variance.
  - Quick check question: What is the output of a step function with threshold 0 when the input is -0.5? What about 0.5?

- Concept: Variance as a measure of information content
  - Why needed here: The paper uses variance of output vectors to explain why accuracy decreases with more layers.
  - Quick check question: If all output vectors are identical, what is their variance? What does this imply about discriminative information?

## Architecture Onboarding

- Component map:
  - Input layer: 784 dimensions (MNIST images flattened)
  - Hidden layers: 0-15 fully connected layers with step function activation
  - Output layer: 784 dimensions (same as input)
  - Weight initialization: Uniform distribution U(-sqrt(1/784), sqrt(1/784))
  - No weight updates during training
  - Similarity calculation: Euclidean distance between test outputs and class average vectors

- Critical path:
  1. Load and preprocess MNIST data
  2. Initialize network with random weights
  3. Forward propagate training data to compute class average vectors
  4. Forward propagate test data
  5. Compute Euclidean distance between each test output and all class averages
  6. Assign label based on minimum distance
  7. Calculate accuracy

- Design tradeoffs:
  - Step function vs. continuous activation: Step functions create binary outputs that may reduce gradient-like information flow but maintain clear class boundaries.
  - Fixed random weights vs. learned weights: Eliminates optimization complexity but may limit representational capacity.
  - Fully connected vs. specialized architectures: Simple to implement but may not capture spatial patterns as effectively as CNNs.

- Failure signatures:
  - Accuracy at chance level (~10% for 10 classes): Indicates loss of discriminative information in output vectors
  - Inconsistent accuracy across runs: Suggests random initialization is producing unreliable clustering
  - High accuracy with 0 hidden layers but low with 1+ layers: Indicates the network structure itself is degrading information

- First 3 experiments:
  1. Replicate the paper's results with 0, 1, and 2 hidden layers to verify the ~80% accuracy claim
  2. Measure output vector variance for each layer count to confirm the smoothing relationship
  3. Test with different weight initialization ranges to see if this affects accuracy or variance trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance of output vectors change as the number of hidden layers increases beyond 15?
- Basis in paper: [explicit] The study observed a plateau in variance at around 10 hidden layers, but did not investigate further.
- Why unresolved: The study only tested up to 15 hidden layers, leaving the behavior of variance at higher layer counts unknown.
- What evidence would resolve it: Additional experiments with 20+ hidden layers to determine if variance continues to plateau or exhibits new behavior.

### Open Question 2
- Question: How do different activation functions affect information propagation in neural networks without backpropagation?
- Basis in paper: [explicit] The study used only step functions as activation functions, suggesting room for improvement with other activation functions.
- Why unresolved: The study's simplified architecture limits understanding of how other activation functions might influence information propagation.
- What evidence would resolve it: Experiments comparing various activation functions (e.g., ReLU, sigmoid, tanh) in similar network structures without backpropagation.

### Open Question 3
- Question: How does the inclusion of convolutional layers affect the information propagation capabilities of neural networks without backpropagation?
- Basis in paper: [explicit] The study suggests potential improvements using convolutional neural networks.
- Why unresolved: The study only used fully connected layers, not exploring the impact of convolutional layers on information propagation.
- What evidence would resolve it: Experiments with convolutional layers in similar network structures without backpropagation to compare performance and information propagation.

## Limitations
- The proposed mechanism linking variance reduction to accuracy degradation is speculative and lacks independent validation
- Claims about information propagation without backpropagation remain uncertain due to limited experimental scope
- The study doesn't address whether observed accuracy could be achieved through simpler mechanisms like random chance clustering

## Confidence
- **High Confidence**: The experimental methodology for computing accuracy using class average vectors is clearly specified and reproducible
- **Medium Confidence**: The observation that accuracy decreases with more hidden layers is directly supported by the experimental results
- **Low Confidence**: The proposed mechanism linking variance reduction to accuracy degradation is speculative and lacks independent validation

## Next Checks
1. **Replicate variance-accuracy relationship**: Systematically measure output vector variance and accuracy across 0-15 hidden layers on MNIST, then test whether artificially reducing variance in the 0-layer case decreases accuracy proportionally
2. **Cross-dataset validation**: Test the same network architecture on CIFAR-10 and Fashion-MNIST to determine if the ~80% baseline and layer-dependent degradation generalize beyond handwritten digits
3. **Alternative activation comparison**: Replace step functions with ReLU or sigmoid activations while keeping random weights fixed to isolate whether the step function is essential for the observed information propagation behavior