---
ver: rpa2
title: 'ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models'
arxiv_id: '2407.04693'
source_url: https://arxiv.org/abs/2407.04693
tags:
- hallucination
- annotator
- arxiv
- data
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of hallucination in large language\
  \ models by introducing an iterative self-training framework to scale hallucination\
  \ annotation datasets and improve annotation accuracy. The core method uses an Expectation-Maximization\
  \ algorithm to progressively scale data across three dimensions\u2014response diversity,\
  \ topic coverage, and annotator accuracy\u2014while employing self-consistency for\
  \ robust annotation estimation."
---

# ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models

## Quick Facts
- arXiv ID: 2407.04693
- Source URL: https://arxiv.org/abs/2407.04693
- Authors: Yuzhe Gu; Ziwei Ji; Wenwei Zhang; Chengqi Lyu; Dahua Lin; Kai Chen
- Reference count: 40
- Primary result: 7B annotator surpasses GPT-4 with 89.30% F1 and 89.55% accuracy

## Executive Summary
ANAH-v2 introduces an iterative self-training framework that scales hallucination annotation datasets while improving annotation accuracy. The system employs an Expectation-Maximization algorithm to progressively scale data across three dimensions: response diversity, topic coverage, and annotator accuracy. Using self-consistency for robust annotation estimation, the framework produces a 7B parameter annotator that achieves state-of-the-art zero-shot results on HaluEval (81.54%) and HalluQA (94.44%). The system also demonstrates a 25-37% reduction in hallucination rates through re-ranking generated responses.

## Method Summary
The core methodology leverages an EM algorithm with self-consistency estimation to iteratively improve annotation quality. In the E-step, multiple independent outputs are generated for each response using different sampling strategies, and a weighted aggregation determines the final hallucination label. The M-step then updates the annotator parameters using the refined labels. Progressive data scaling is achieved through a curriculum learning approach that first diversifies responses, then expands topic coverage, and finally improves annotator accuracy. The framework incorporates annotator self-evaluation to enhance reliability and employs a re-ranking strategy that filters hallucinatory responses while maintaining diversity.

## Key Results
- ANAH-v2 7B annotator surpasses GPT-4 with 89.30% F1 and 89.55% accuracy on in-domain data
- Achieves new state-of-the-art zero-shot results: HaluEval (81.54%) and HalluQA (94.44%)
- Reduces hallucination in generated responses from 25% to 37% using re-ranking strategy
- Comprehensive benchmark provided for evaluating hallucination across multiple open-source models

## Why This Works (Mechanism)
The EM-based self-training framework progressively improves annotation quality through three complementary scaling dimensions. Self-consistency across multiple sampled outputs provides robustness against individual sampling biases, while the progressive curriculum ensures the model encounters increasingly diverse and challenging examples. The annotator self-evaluation component creates a feedback loop that reinforces reliable judgments, and the re-ranking mechanism effectively filters hallucinatory content without sacrificing response diversity.

## Foundational Learning
- **Expectation-Maximization algorithm**: Iterative optimization framework that alternates between estimating latent variables (E-step) and updating model parameters (M-step) - needed for handling incomplete or noisy annotation data
- **Self-consistency estimation**: Using multiple independent model outputs to improve prediction reliability through aggregation - needed to reduce sampling variance in hallucination detection
- **Curriculum learning for data scaling**: Progressive exposure to increasingly complex examples - needed to prevent catastrophic forgetting during large-scale dataset expansion
- **Annotator self-evaluation**: Models assessing their own confidence in predictions - needed to filter unreliable annotations and improve overall quality
- **Progressive scaling dimensions**: Simultaneous improvement across diversity, coverage, and accuracy - needed to ensure comprehensive representation of hallucination scenarios
- **Re-ranking for hallucination mitigation**: Post-generation filtering based on hallucination likelihood - needed to reduce hallucinations without requiring architectural changes

## Architecture Onboarding

**Component Map**
Seed Dataset -> EM Self-training Loop -> Progressive Scaling Module -> Annotator Model -> Re-ranking Filter -> Benchmark Evaluation

**Critical Path**
The critical execution path flows through the EM loop: seed data enters E-step (self-consistency estimation) → M-step (parameter updates) → progressive scaling module (diversity/coverage/accuracy enhancement) → annotator model training → re-ranking application → benchmark evaluation.

**Design Tradeoffs**
- Model size vs. performance: 7B parameters chosen as sweet spot between capability and efficiency
- Self-consistency samples (K) vs. computational cost: More samples improve accuracy but increase inference time
- Progressive vs. batch scaling: Progressive approach shows better performance but requires more training iterations
- Re-ranking strictness vs. response utility: Stricter filtering reduces hallucinations but may remove valid responses

**Failure Signatures**
- Poor initial seed data quality propagates through EM iterations
- Overfitting to training domains reduces zero-shot generalization
- Excessive self-consistency samples cause diminishing returns
- Aggressive re-ranking eliminates diverse but valid responses

**First 3 Experiments**
1. Validate self-consistency estimation improves over single-sample baseline on held-out validation set
2. Compare progressive scaling approach against non-progressive scaling with equal total data volume
3. Test re-ranking effectiveness by measuring hallucination reduction vs. response utility trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-consistency method's performance vary with different numbers of candidate outputs (K) in the E-step of the EM algorithm?
- Basis in paper: The paper mentions using self-consistency with multiple samplings to yield K independent outputs but does not explore the impact of varying K.
- Why unresolved: The paper does not provide experiments or analysis on how the choice of K affects the accuracy and stability of the hallucination annotation estimation.
- What evidence would resolve it: Empirical results comparing the performance of the annotator with different values of K (e.g., K=1, 5, 10, 20) on a held-out test set would clarify the optimal number of candidate outputs for self-consistency.

### Open Question 2
- Question: How does the progressive data scaling approach compare to non-progressive data scaling in terms of annotator performance when the total amount of training data is held constant?
- Basis in paper: The paper describes the progressive data scaling approach but does not directly compare it to a non-progressive approach with the same total data volume.
- Why unresolved: While the paper shows that progressive data scaling outperforms non-progressive scaling, it does not control for the total amount of data, leaving open the question of whether the improvement is due to the progressive nature or simply the larger dataset size.
- What evidence would resolve it: A controlled experiment comparing progressive and non-progressive data scaling approaches with equal total training data would determine if the progressive approach offers additional benefits beyond increased data volume.

### Open Question 3
- Question: What is the impact of different underlying model architectures and parameter sizes on the performance of the hallucination annotator?
- Basis in paper: The paper uses InternLM2-7B as the backbone for the hallucination annotator but does not explore the effects of using different architectures or model sizes.
- Why unresolved: The choice of InternLM2-7B may influence the annotator's performance, and it is unclear how other models (e.g., Llama, Mistral) or larger parameter sizes would affect the results.
- What evidence would resolve it: Experiments training the hallucination annotator using different backbone models and parameter sizes (e.g., 7B, 13B, 70B) would reveal the impact of model choice on performance and generalization.

## Limitations
- Results may be biased toward English-language models and datasets, limiting multilingual generalization
- EM framework relies on correlation between model agreement and annotation accuracy, which may not hold in all cases
- Improvements over GPT-4 measured primarily on curated in-domain datasets, raising overfitting concerns
- Re-ranking approach may filter out diverse but potentially valid responses

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Mathematical formulation of EM algorithm and self-consistency is sound | High |
| Reported benchmark results are reproducible with claimed datasets | Medium |
| Relative improvements over baseline models are meaningful but dataset-dependent | Medium |

## Next Checks

1. Test the ANAH-v2 annotator on truly out-of-domain hallucinations (e.g., biomedical or legal domains) not represented in the training data
2. Perform ablation studies to quantify the individual contributions of response diversity, topic coverage, and annotator accuracy scaling dimensions
3. Evaluate the trade-off between hallucination reduction and response utility in the re-ranking approach using human preference studies on downstream task performance