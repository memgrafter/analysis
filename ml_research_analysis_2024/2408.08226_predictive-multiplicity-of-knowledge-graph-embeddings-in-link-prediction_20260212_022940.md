---
ver: rpa2
title: Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction
arxiv_id: '2408.08226'
source_url: https://arxiv.org/abs/2408.08226
tags:
- predictive
- borda
- multiplicity
- major
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates predictive multiplicity in knowledge graph
  embeddings (KGE) for link prediction. The authors define predictive multiplicity
  as the phenomenon where multiple KGE models with similar performance provide conflicting
  predictions for individual queries.
---

# Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction

## Quick Facts
- arXiv ID: 2408.08226
- Source URL: https://arxiv.org/abs/2408.08226
- Reference count: 40
- Primary result: 8% to 39% of testing queries exhibit conflicting predictions across multiple KGE models

## Executive Summary
This paper investigates predictive multiplicity in knowledge graph embeddings (KGE) for link prediction tasks. The authors define predictive multiplicity as the phenomenon where multiple KGE models with similar performance provide conflicting predictions for individual queries. They introduce two metrics, ambiguity and discrepancy, to measure predictive multiplicity and derive an upper bound for discrepancy. The study evaluates six representative KGE methods on commonly used benchmark datasets and finds significant predictive multiplicity. To address this issue, the authors leverage voting methods from social choice theory, demonstrating that they can significantly mitigate conflicts by 66% to 78% in their experiments.

## Method Summary
The study trains multiple KGE models with different random seeds for six representative methods (TransE, RotatE, RESCAL, DistMult, ComplEx, ConvE) on four benchmark datasets (WN18, WN18RR, FB15k, FB15k-237). The authors construct an ϵ-level set of models with similar performance and evaluate predictive multiplicity using ambiguity (αϵ) and discrepancy (δϵ) metrics. Voting methods from social choice theory (majority, Borda, range voting) are then applied to aggregate rankings and reduce predictive multiplicity. The theoretical upper bound on discrepancy is derived based on the baseline model's Hits@K performance.

## Key Results
- 8% to 39% of testing queries exhibit conflicting predictions across multiple KGE models
- Predictive multiplicity is significantly reduced by 66% to 78% using voting methods
- More expressive KGE models (RESCAL, ComplEx) show larger ambiguity and discrepancy values
- The theoretical upper bound on discrepancy is validated empirically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictive multiplicity occurs when multiple KGE models with similar overall link prediction performance produce conflicting predictions for individual queries.
- Mechanism: Training KGE models introduces randomness through random parameter initialization, random sequences of positive samples, and random negative sampling. Due to the non-convexity of the training problem, the same KG may converge to different local minima, producing different embeddings that fit the training graph equally well but disagree on specific predictions.
- Core assumption: The KGE training problem is non-convex and sensitive to initialization, leading to multiple local optima with similar training loss but different parameter configurations.
- Evidence anchors:
  - [abstract] "The training of the KG embedding introduces randomness into the resulting model. Sources of randomness include randomized parameter initialization, randomized sequences of positive samples, and randomized negative sampling. Given the non-convexity of the training problem, the same KG may lead to various KG embeddings because of the convergence of the training in different local minima."
  - [section 1] "While learned embeddings may exhibit comparable performance in link prediction, they may suggest conflicting predictions for an individual query."
- Break condition: If KGE training were convex or if random factors were eliminated, this mechanism would not produce multiplicity.

### Mechanism 2
- Claim: Voting methods from social choice theory can reduce predictive multiplicity by aggregating individual model rankings to produce more robust collective rankings.
- Mechanism: Voting methods aggregate individual model preferences (rankings) to determine a group's overall preference. By training multiple models with different random seeds and aggregating their rankings, candidate entities consistently ranked highly across models will also be ranked highly in the final aggregated ranking, smoothing out randomness.
- Core assumption: Models trained with different random seeds sample from the same distribution of plausible embeddings, and aggregating their rankings captures consensus rather than individual model idiosyncrasies.
- Evidence anchors:
  - [abstract] "To address this issue, the authors leverage voting methods from social choice theory, demonstrating that they can significantly mitigate conflicts by 66% to 78% in their experiments."
  - [section 5.1] "Voting methods from social choice theory can help 'smooth out' the randomness in rankings by aggregating individual models."
- Break condition: If the models trained with different seeds are not representative samples from the hypothesis space, or if the aggregation method doesn't properly capture consensus, voting may not reduce multiplicity.

### Mechanism 3
- Claim: The extent of predictive multiplicity is bounded by the baseline model's accuracy and the error tolerance threshold.
- Mechanism: The discrepancy between a baseline model and competing models in the ϵ-level set is bounded by 2(1 - HK(M*)) + ϵ, where HK(M*) is the baseline model's Hits@K performance. Less accurate baseline models or larger ϵ values theoretically allow for greater potential predictive multiplicity.
- Core assumption: The bound derivation assumes the competing models are within the specified performance tolerance and that Hits@K provides a reasonable measure of overall model accuracy.
- Evidence anchors:
  - [section 4.4] "The upper bound illustrates how the extent of predictive multiplicity depends on Hits@K of the baseline model. Specifically, a less accurate baseline model theoretically provides greater potential for predictive multiplicity."
  - [proposition 1] "The discrepancy between the baseline model M* and any competing model Mθ ∈ Sϵ(M*) obeys: δϵ(M*) ≤ 2 · (1 − HK(M*)) + ϵ"
- Break condition: If the ϵ-level set definition or Hits@K metric doesn't accurately capture model similarity, the bound may not hold.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE) map entities and relations into low-dimensional vectors while preserving semantic and structural information.
  - Why needed here: Understanding KGE fundamentals is essential to grasp why multiple embeddings can fit the same KG differently and why they're used for link prediction.
  - Quick check question: What is the primary purpose of mapping entities and relations to low-dimensional vectors in KGE?

- Concept: Social Choice Theory provides methods for aggregating individual preferences to determine collective preferences.
  - Why needed here: Voting methods from social choice theory are the core mechanism for mitigating predictive multiplicity by aggregating individual model rankings.
  - Quick check question: What property must a scoring rule satisfy according to Young's theorem?

- Concept: Predictive Multiplicity refers to the phenomenon where multiple models with similar overall performance produce conflicting predictions for individual instances.
  - Why needed here: This is the central phenomenon being studied and mitigated in the paper.
  - Quick check question: How is predictive multiplicity formally defined in the context of link prediction?

## Architecture Onboarding

- Component map: KGE training pipelines (with random seeds) -> Evaluation metrics (Hits@K, ambiguity, discrepancy) -> Voting method implementations (majority, Borda, range voting) -> Aggregation procedures
- Critical path: Baseline model → ϵ-level set construction → predictive multiplicity measurement → voting method application → multiplicity mitigation evaluation
- Design tradeoffs: Voting methods reduce predictive multiplicity but require training multiple models (increased computational cost) and may slightly reduce Hits@K (accuracy vs. robustness tradeoff)
- Failure signatures: Persistent high ambiguity/discrepancy values despite voting, significant Hits@K degradation after voting, or failure to construct a sufficient ϵ-level set
- First 3 experiments:
  1. Train 10 models with different seeds for a chosen KGE method and dataset, verify they form an ϵ-level set
  2. Apply each voting method to the trained models and measure changes in ambiguity and discrepancy
  3. Test the effect of varying the number of aggregated models (2, 4, 6, 8, 10) on predictive multiplicity metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the expressiveness of KGE models and systematically analyze their impact on predictive multiplicity?
- Basis in paper: [inferred] The paper mentions expressiveness as a factor that might influence predictive multiplicity, observing that more expressive models like RESCAL and ComplEx tend to have larger ambiguity and discrepancy values.
- Why unresolved: Quantifying expressiveness and establishing a systematic analysis framework falls outside the scope of this paper but is identified as a promising avenue for future research.
- What evidence would resolve it: A comprehensive study quantifying the expressiveness of various KGE models and correlating it with their predictive multiplicity metrics across multiple datasets would provide empirical evidence to support or refute this hypothesis.

### Open Question 2
- Question: How does the choice of scoring-based threshold τ affect predictive multiplicity in the context of query answering, and is there an optimal way to set this threshold?
- Basis in paper: [inferred] The paper extends its analysis to link prediction for query answering, introducing a scoring-based threshold τ to determine answer sets. It observes more significant predictive multiplicity in this setting, heavily relying on the threshold.
- Why unresolved: The paper does not explore how different threshold values impact predictive multiplicity or propose methods for setting an optimal threshold.
- What evidence would resolve it: An empirical study investigating the relationship between threshold values and predictive multiplicity metrics, along with experiments on threshold optimization techniques, would provide insights into this open question.

### Open Question 3
- Question: Can computational argumentation frameworks be effectively used to find a set of conflict-free explanations for KGE predictions, addressing the variability introduced by model multiplicity?
- Basis in paper: [inferred] The paper suggests using computational argumentation frameworks to find conflict-free explanations as a promising direction for future research, given that explanations can vary depending on random seeds used during training.
- Why unresolved: This is a proposed future direction that has not been explored in the paper.
- What evidence would resolve it: Implementing and evaluating computational argumentation frameworks for generating consistent explanations across multiple KGE models trained with different random seeds would provide evidence for the feasibility and effectiveness of this approach.

## Limitations

- The theoretical upper bound on discrepancy provides useful insight but may not fully capture the complexity of real-world predictive multiplicity
- The study focuses on single-relation queries, which may not represent the full spectrum of link prediction tasks in KGE
- The computational overhead of training multiple models and potential trade-off with Hits@K performance are not fully explored

## Confidence

**High Confidence**: The experimental results showing significant predictive multiplicity (8% to 39% of testing queries with conflicting predictions) are well-supported by the methodology and consistent across different datasets and KGE methods.

**Medium Confidence**: The effectiveness of voting methods in reducing predictive multiplicity (66% to 78% reduction) is demonstrated, but the generalizability to other KGE methods and the long-term stability of these reductions require further validation.

**Low Confidence**: The theoretical upper bound on discrepancy provides a useful framework but may not accurately predict the extent of predictive multiplicity in all scenarios, particularly for more complex KGE methods or datasets with different characteristics.

## Next Checks

1. **Cross-dataset validation**: Test the predictive multiplicity phenomenon and voting method effectiveness on additional KG datasets beyond the four benchmark datasets used in the study, particularly those with different characteristics (e.g., dense vs. sparse graphs, different relation types).

2. **Method-specific analysis**: Investigate whether certain KGE methods (e.g., TransE vs. ComplEx) exhibit systematically different levels of predictive multiplicity and whether voting methods are equally effective across all methods.

3. **Multi-relation query evaluation**: Extend the analysis to multi-relation queries (chains of relations) to determine if predictive multiplicity is more pronounced or mitigated in these more complex prediction scenarios.