---
ver: rpa2
title: 'MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory'
arxiv_id: '2404.11672'
source_url: https://arxiv.org/abs/2404.11672
tags:
- memory
- relation
- language
- knowledge
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemLLM finetunes LLMs to use an explicit read-write memory module
  with a structured triple-based schema, enabling interpretable knowledge editing
  and reducing hallucinations. It learns to extract relations from text and store
  them as structured triples, and to query this memory for factual support during
  generation.
---

# MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory

## Quick Facts
- arXiv ID: 2404.11672
- Source URL: https://arxiv.org/abs/2404.11672
- Reference count: 40
- Key outcome: MemLLM improves perplexity by up to 0.524 on target entities and achieves 0.84 average score on ZsRE knowledge editing

## Executive Summary
MemLLM introduces a finetuning approach that enables large language models to use an explicit read-write memory module with a structured triple-based schema. By learning to extract relations from text and store them as structured triples, and to query this memory for factual support during generation, MemLLM reduces hallucinations and enables interpretable knowledge editing. The method improves language modeling performance on entity-heavy text and outperforms model editing approaches on knowledge editing tasks while preserving unrelated knowledge.

## Method Summary
MemLLM finetunes LLM models (Mistral-7B) to generate memory-write and memory-read API calls using a structured triple-based schema. The approach involves finetuning with standard language modeling objectives while incorporating memory operations, populating memory from Wikipedia using the memory-write model, and evaluating on language modeling and knowledge editing tasks. The memory stores information as subject-relation-object triples with vector embeddings, enabling both human-readable inspection and efficient similarity-based retrieval.

## Key Results
- Improves perplexity by up to 0.524 on target entities in Re-DocRED
- Achieves 0.84 average score on ZsRE knowledge editing, outperforming model editing methods
- Enables interpretable knowledge editing while preserving unrelated knowledge

## Why This Works (Mechanism)

### Mechanism 1
Finetuning enables LLMs to learn explicit memory operations by providing structured training data with memory-write and memory-read API calls. The LLM learns to generate these calls as part of its language modeling process through standard finetuning approaches.

### Mechanism 2
Structured triple-based memory improves interpretability and editing by storing facts as subject-relation-object triples with vector embeddings. This enables both human-readable inspection and efficient similarity-based retrieval for factual support during generation.

### Mechanism 3
Memory augmentation reduces hallucinations by providing explicit factual support. When generating text that requires factual information, the LLM can query the memory for relevant triples and use them to ground its generation, making the output generation process more interpretable.

## Foundational Learning

- Concept: Relation extraction from text
  - Why needed here: The memory-write component requires extracting structured triples from unstructured text
  - Quick check question: How would you extract the triple ⟨Washington D.C., capital of, United States⟩ from the sentence "Washington D.C. is the capital of the United States"?

- Concept: Vector similarity and embedding retrieval
  - Why needed here: Memory queries use vector similarity rather than exact string matching to handle entity variations
  - Quick check question: If entity embeddings are normalized, what is the range of cosine similarity values between two embeddings?

- Concept: Fine-tuning with custom loss functions
  - Why needed here: The model needs to learn to generate specific API calls while maintaining language modeling capabilities
  - Quick check question: How would you structure the loss function to encourage API call generation without disrupting normal text generation?

## Architecture Onboarding

- Component map: LLM base model (e.g., Mistral-7B) -> Memory component (triple store with entity/relation tables) -> Finetuning pipeline (data generation and training) -> API layer (memory-write and memory-read interfaces)

- Critical path: 1. Data generation (extract relations, create API training examples) 2. Memory-write finetuning (learn to extract and store facts) 3. Memory-read finetuning (learn to query and use memory) 4. Inference pipeline (integrate memory operations into generation)

- Design tradeoffs:
  - Structured vs unstructured memory (interpretability vs flexibility)
  - Vector similarity thresholds (precision vs recall)
  - API verbosity (information richness vs generation flow)
  - Memory population strategy (automated extraction vs manual curation)

- Failure signatures:
  - High perplexity despite memory availability (memory not being used effectively)
  - Incorrect or missing API calls during generation (finetuning issues)
  - Memory overflow or slow retrieval (scalability problems)
  - Contradictory information between memory and generation (consistency issues)

- First 3 experiments:
  1. Ablation study: Compare perplexity with and without memory component on entity-heavy text
  2. API call analysis: Measure frequency and accuracy of memory-write/read calls during generation
  3. Memory population test: Evaluate memory-write model on held-out relation extraction tasks

## Open Questions the Paper Calls Out

- Question: How does MemLLM's structured memory schema scale when handling billions of triples compared to traditional vector-based retrieval methods?
  - Basis in paper: The paper mentions memory efficiency benefits at 111M triples but lacks scaling experiments at massive scales
  - Why unresolved: Only demonstrates benefits at Wikipedia scale, not billions of triples needed for comprehensive knowledge coverage

- Question: Can MemLLM effectively handle composite relations that require inference across multiple stored triples?
  - Basis in paper: The paper acknowledges this as a limitation with 96 relation types
  - Why unresolved: Current schema limits ability to handle complex relational reasoning requiring chaining or combining existing facts

- Question: How does MemLLM's performance degrade when the memory-write process has low precision and recall?
  - Basis in paper: Shows imperfect memory writes degrade perplexity but doesn't quantify minimum quality thresholds
  - Why unresolved: Doesn't establish at what point memory-write quality becomes too poor to provide benefits

## Limitations
- The approach relies on standard finetuning without architectural innovations, making scalability to complex reasoning tasks uncertain
- Memory retrieval performance depends heavily on threshold hyperparameters that are not fully specified
- The structured triple schema may be too rigid for certain types of knowledge, particularly complex or contextual information

## Confidence
- **High**: MemLLM successfully learns to generate memory API calls during finetuning and demonstrates measurable perplexity improvements on entity-heavy text
- **Medium**: The structured triple schema enables interpretable knowledge editing and reduces hallucinations as demonstrated on ZsRE
- **Low**: Claims about scalability and applicability to diverse knowledge domains lack empirical validation beyond tested datasets

## Next Checks
1. Conduct an ablation study varying memory retrieval thresholds (τe, τt, τr, Qthr) to determine optimal settings and understand their impact on performance
2. Test memory consistency by measuring how well the model maintains factual accuracy across multiple generations using the same memory state
3. Evaluate the approach on a more diverse set of knowledge domains (e.g., scientific, technical, or procedural knowledge) to assess schema flexibility and retrieval effectiveness beyond entity-relation triples