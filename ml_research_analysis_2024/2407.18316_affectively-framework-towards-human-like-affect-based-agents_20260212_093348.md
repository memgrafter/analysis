---
ver: rpa2
title: 'Affectively Framework: Towards Human-like Affect-Based Agents'
arxiv_id: '2407.18316'
source_url: https://arxiv.org/abs/2407.18316
tags:
- agent
- agents
- game
- affect
- arousal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Affectively Framework, a set of Open-AI
  Gym environments designed to train reinforcement learning agents that consider both
  in-game performance and human affect. The framework incorporates a human-sourced
  model of arousal into its observation space, using gameplay data from the AGAIN
  dataset annotated for arousal.
---

# Affectively Framework: Towards Human-like Affect-Based Agents

## Quick Facts
- arXiv ID: 2407.18316
- Source URL: https://arxiv.org/abs/2407.18316
- Reference count: 16
- Key outcome: Introduces the Affectively Framework, a set of OpenAI Gym environments for training RL agents that balance gameplay performance and human affect, using arousal models from the AGAIN dataset.

## Executive Summary
This paper introduces the Affectively Framework, a set of OpenAI Gym environments designed to train reinforcement learning agents that consider both in-game performance and human affect. The framework incorporates a human-sourced model of arousal into its observation space, using gameplay data from the AGAIN dataset annotated for arousal. Three game environments are provided: a side-scrolling platformer (Pirates), a first-person shooter (Heist), and a racing game (Solid Rally). Baseline experiments using Proximal Policy Optimisation (PPO) agents show that while agents can optimise for arousal, they often exhibit poor gameplay, highlighting the challenge of balancing behaviour and affect rewards. The framework offers a foundation for future research into affect-aware game-playing agents, with potential improvements including better reward balancing and exploration strategies.

## Method Summary
The Affectively Framework integrates a KNN-based arousal model into OpenAI Gym environments, using human gameplay traces from the AGAIN dataset. Agents receive a combined reward of behaviour and affect, weighted by a λ parameter. PPO agents are trained to optimise both performance and arousal, with experiments across three games showing the difficulty of balancing these objectives.

## Key Results
- Agents trained to maximise arousal significantly improved arousal values but exhibited poor gameplay (RE = 0).
- Balancing behaviour and affect rewards remains a core challenge in affect-aware RL.
- The framework provides a foundation for future research into affect-aware game-playing agents.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework successfully integrates human affect into reinforcement learning observation space by embedding arousal transitions derived from gameplay traces.
- Mechanism: Arousal is modeled using a k-nearest neighbors (KNN) algorithm that maps game-state transitions to arousal changes observed in human playthroughs, creating an affect reward signal for agents.
- Core assumption: Human affect states can be reliably inferred from game-state variables and used to guide agent behavior.
- Evidence anchors:
  - [abstract] "This paper introduces the Affectively Framework, a set of Open-AI Gym environments designed to train reinforcement learning agents that consider both in-game performance and human affect."
  - [section II-B] "The affect model is built using the k-nearest neighbours algorithm [14] (KNN), with k = 5. We follow the same methodology for deriving this affect model across all three games, based on successful arousal modelling attempts when the dataset was collected [4]."
  - [corpus] Weak. No neighbor papers discuss affect modeling in reinforcement learning environments; the corpus evidence is largely irrelevant to this mechanism.
- Break condition: If the KNN model cannot find sufficiently similar game-state transitions in the human dataset, the affect prediction becomes unreliable and the reward signal degrades.

### Mechanism 2
- Claim: The framework's reward structure enables balancing between gameplay performance and affect optimization through a tunable λ parameter.
- Mechanism: The total reward is a linear combination of behavior reward (RB) and affect reward (RA), weighted by λ, allowing agents to optimize for either pure performance, pure affect, or a blend.
- Core assumption: A linear reward combination is sufficient to guide agents toward desired trade-offs between performance and affect.
- Evidence anchors:
  - [section II] "Using these components, the total reward Rt to the agent is calculated at each time step as per Eq. (1). Rt = (1 − λ) · n(RB) + λ · RA"
  - [section IV] "When PPO agents were trained to maximise arousal (RA), the agents significantly improved their average arousal values, surpassing the random agent and the human demonstrations alike."
  - [corpus] Missing. No neighbor papers provide evidence for reward balancing strategies in affect-aware reinforcement learning.
- Break condition: If λ is poorly tuned or if one reward component dominates, the agent's behavior may become unstable or converge to suboptimal policies.

### Mechanism 3
- Claim: The Affectively Framework can expose the limitations of current reinforcement learning algorithms in balancing affect and behavior rewards.
- Mechanism: Baseline experiments show that PPO agents optimizing for arousal alone achieve high affect scores but fail to perform well in gameplay, revealing the challenge of affect-aware training.
- Core assumption: Observed failure modes in baseline agents are informative for future research into affect-aware RL.
- Evidence anchors:
  - [section IV] "When PPO agents were trained to maximise arousal (RA), the agents significantly improved their average arousal values... However, the behaviour of these agents was very poor across games (with RE = 0 in all 30 runs and all three games)."
  - [section V] "Experiments with the built-in agents and reward systems provided by the Affectively Framework highlighted the inherent challenge of optimising affect in gameplaying agents."
  - [corpus] Weak. No neighbor papers discuss limitations of RL algorithms in affect-aware environments.
- Break condition: If the affect reward is too sparse or deceptive, agents may converge to degenerate policies that exploit the reward without meaningful gameplay.

## Foundational Learning

- Concept: k-Nearest Neighbors (KNN) for affect modeling
  - Why needed here: KNN is used to predict arousal transitions from human gameplay data based on similarity of game-state vectors.
  - Quick check question: How does the KNN model decide which human gameplay traces are most relevant for a given agent state?
- Concept: Proximal Policy Optimization (PPO) algorithm
  - Why needed here: PPO is the reinforcement learning algorithm used to train agents within the framework, balancing exploration and exploitation.
  - Quick check question: What is the role of the clipped objective in PPO, and why is it important for stable training?
- Concept: Reward shaping and balancing
  - Why needed here: Proper reward design is critical to guide agents toward both high performance and human-like affect responses.
  - Quick check question: How does the λ parameter influence the trade-off between behavior and affect rewards?

## Architecture Onboarding

- Component map:
  - OpenAI Gym-compatible environment wrapper -> KNN-based affect model (3-second time window) -> Behavior reward calculators per game (RB) -> Affect reward generator (RA) -> Total reward combiner (Rt) -> PPO agent trainer (via Stable Baselines3)
- Critical path:
  1. Agent interacts with environment (receives St, produces At)
  2. Environment updates state, computes RE
  3. Affect model generates RA every 3 seconds
  4. RB is computed based on domain-specific rules
  5. Rt is formed using Eq. (1)
  6. PPO updates policy based on Rt
- Design tradeoffs:
  - Sparse affect rewards (every 3 seconds) vs. more frequent but potentially noisier rewards
  - Linear reward combination vs. more complex multi-objective optimization
  - Use of pre-computed KNN model vs. online affect prediction
- Failure signatures:
  - Agent gets stuck in high-arousal but low-performance states
  - Affect reward dominates and leads to degenerate gameplay
  - KNN model fails to find relevant neighbors, causing reward instability
- First 3 experiments:
  1. Train a PPO agent with λ = 0 (pure behavior) on Pirates and evaluate RE and RA.
  2. Train a PPO agent with λ = 1 (pure affect) on Heist and observe behavior collapse.
  3. Train a PPO agent with λ = 0.5 (blended) on Solid Rally and analyze the balance between RE and RA.

## Open Questions the Paper Calls Out
None

## Limitations
- The KNN-based affect model's generalization capability is untested beyond the AGAIN dataset; performance may degrade if agent states diverge significantly from human traces.
- The sparse arousal reward signal (every 3 seconds) may limit learning efficiency, especially for complex games like Heist.
- No validation of affect reward reliability in scenarios where the KNN model cannot find sufficiently similar neighbors.

## Confidence
- High confidence in the framework's architectural design and reward balancing mechanism.
- Medium confidence in the affect modeling approach, given its dependence on dataset coverage and similarity assumptions.
- Low confidence in the reproducibility of baseline results without access to specific PPO hyperparameters and preprocessing details.

## Next Checks
1. Replicate the baseline experiments (behavior-only, arousal-only, blended) and verify RE and RA trends across games.
2. Test the KNN affect model's robustness by evaluating neighbor similarity distributions for out-of-distribution agent states.
3. Explore alternative reward schedules (e.g., decaying λ) to assess stability and performance trade-offs in affect-aware training.