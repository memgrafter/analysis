---
ver: rpa2
title: Unitary convolutions for learning on graphs and groups
arxiv_id: '2410.05499'
source_url: https://arxiv.org/abs/2410.05499
tags:
- unitary
- graph
- convolution
- matrix
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces unitary group convolutions as a method to
  enhance stability in graph neural networks and general group-convolutional architectures.
  The authors propose two variants of unitary graph convolution: Separable unitary
  convolution (UniConv) and Lie orthogonal/unitary convolution (Lie UniConv), which
  replace standard group convolution operators with unitary ones.'
---

# Unitary convolutions for learning on graphs and groups

## Quick Facts
- arXiv ID: 2410.05499
- Source URL: https://arxiv.org/abs/2410.05499
- Authors: Bobak T. Kiani; Lukas Fesser; Melanie Weber
- Reference count: 40
- Key outcome: Introduces unitary group convolutions to prevent over-smoothing in graph neural networks and improve stability during training

## Executive Summary
This paper introduces unitary group convolutions as a method to enhance stability in graph neural networks and general group-convolutional architectures. The authors propose two variants of unitary graph convolution: Separable unitary convolution (UniConv) and Lie orthogonal/unitary convolution (Lie UniConv), which replace standard group convolution operators with unitary ones. These unitary operations are norm-preserving and invertible, preventing over-smoothing effects and improving stability during training. The method demonstrates competitive performance compared to state-of-the-art graph neural networks, with improvements in tasks requiring long-range dependencies.

## Method Summary
The method introduces unitary graph convolutions that provably avoid oversmoothing by preserving the Rayleigh quotient through isometric operations. Two variants are proposed: UniConv for simpler implementation and Lie UniConv for full unitarity. The approach uses the exponential map to enforce unitarity in message-passing layers, ensuring dynamical isometry and preventing vanishing/exploding gradients. The method is evaluated across multiple graph learning tasks including node classification, graph classification, and graph distance regression on diverse benchmark datasets.

## Key Results
- Unitary graph convolutions provably avoid over-smoothing by preserving the Rayleigh quotient
- The method prevents vanishing/exploding gradients through dynamical isometry
- Achieves competitive performance on LRGB, TU Datasets, and heterophilous graph datasets
- Improves ability to learn long-range dependencies compared to standard message-passing architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unitary graph convolutions provably avoid oversmoothing by preserving the Rayleigh quotient
- Mechanism: Isometries preserve the Rayleigh quotient, preventing rapid convergence of node representations that characterizes oversmoothing
- Core assumption: The Rayleigh quotient is a valid measure of oversmoothing for this class of graph neural networks
- Evidence anchors: [abstract] "unitary graph convolutions provably avoid over-smoothing", [section] "Proposition 6 (Invariance of Rayleigh quotient)"

### Mechanism 2
- Claim: Unitary convolutions enhance stability and prevent vanishing/exploding gradients by ensuring dynamical isometry
- Mechanism: Unitary operators have bounded operator norm (norm 1), keeping the Jacobian of network composition close to an orthogonal matrix
- Core assumption: The network architecture allows composition of unitary layers to maintain dynamical isometry
- Evidence anchors: [abstract] "allow for deeper networks that are more stable during training", [section] "Definition 8 (Dynamical isometry)"

### Mechanism 3
- Claim: Unitary convolutions improve ability to learn long-range dependencies by enabling deeper networks without instability
- Mechanism: Preventing oversmoothing and gradient issues allows more layers, enabling propagation of information over longer distances
- Core assumption: Tasks require learning long-range dependencies and graph structures support such dependencies
- Evidence anchors: [abstract] "often struggle to learn long range dependencies in data", [section] "We further describe how generalized unitary convolutions avoid vanishing and exploding gradients"

## Foundational Learning

- Concept: Group theory and Lie groups
  - Why needed here: The paper builds on mathematical framework of group theory and Lie groups to define and analyze unitary convolutions on graphs and general groups
  - Quick check question: What is the difference between a Lie group and a Lie algebra, and how are they related in the context of this paper?

- Concept: Graph neural networks and message passing
  - Why needed here: The paper focuses on applying unitary convolutions to graph neural networks, requiring understanding of how GNNs work and the message passing framework
  - Quick check question: How does the standard message passing update in a GNN differ from the unitary message passing update proposed in this paper?

- Concept: Spectral graph theory and the Rayleigh quotient
  - Why needed here: The paper uses the Rayleigh quotient as a measure of oversmoothing and proves unitary convolutions preserve it
  - Quick check question: How is the Rayleigh quotient defined for a graph, and what does it measure in the context of graph neural networks?

## Architecture Onboarding

- Component map: Input features -> UniConv/Lie UniConv layers -> Activation functions (GroupSort) -> Batch normalization -> Output
- Critical path: Forward pass through unitary convolution layers processing node features and adjacency matrix, with backward pass relying on stable gradients
- Design tradeoffs: Stability vs expressiveness - unitary convolutions provide stability but may limit learning of non-isometric/invertible functions
- Failure signatures: Numerical instability in exponential map approximation, poor performance if unitarity not enforced in message passing, potential expressiveness limitations
- First 3 experiments:
  1. Implement simple GNN with unitary convolution layers on synthetic ring graph dataset to test oversmoothing
  2. Compare unitary GNN with standard GNN on TU Datasets benchmark for stability and performance
  3. Test unitary GNN on heterophilous graph dataset to evaluate handling of dissimilar connected nodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of unitary convolutions compare to orthogonal convolutions in practice?
- Basis in paper: [explicit] The authors compare unitary and orthogonal convolutions on Peptides-func and Peptides-struct datasets in Table 4, finding similar performance levels
- Why unresolved: While the authors provide some empirical evidence, a comprehensive comparison across various datasets and tasks is lacking
- What evidence would resolve it: A large-scale study comparing unitary and orthogonal convolutions across diverse datasets and tasks

### Open Question 2
- Question: How do unitary convolutions perform on directed graphs?
- Basis in paper: [inferred] The authors mention directed graphs present challenges due to non-symmetric adjacency matrices and propose a theoretical method to handle this
- Why unresolved: The proposed method for handling directed graphs is theoretical, and its practical effectiveness is unknown
- What evidence would resolve it: Experiments comparing unitary convolutions on directed graphs to other methods using various directed graph datasets

### Open Question 3
- Question: What is the optimal way to initialize unitary and orthogonal convolutions?
- Basis in paper: [explicit] The authors mention various forms of initialization for unitary and orthogonal matrices have been proposed, including 2x2 blocks along the diagonal
- Why unresolved: While the authors provide some guidance, the optimal initialization strategy likely depends on specific task and architecture
- What evidence would resolve it: A comprehensive study comparing performance of different initialization methods across various tasks and architectures

## Limitations
- Theoretical claims about oversmoothing avoidance rely heavily on invariance of the Rayleigh quotient, which may not be appropriate across all graph structures
- Dynamical isometry claims assume full composition of unitary layers, which may not hold with standard activation functions or non-isometric components
- Experimental results show competitive performance but lack direct comparisons with specialized oversmoothing solutions

## Confidence
- High confidence: Basic unitary convolution implementation and standard GNN baselines
- Medium confidence: Over-smoothing avoidance claims and stability improvements
- Low confidence: Long-range dependency learning benefits and performance on heterophilous graphs

## Next Checks
1. Verify Rayleigh quotient preservation empirically by tracking node representation similarity across layers in both unitary and standard GNNs on synthetic ring graphs
2. Test dynamical isometry by monitoring gradient norms and spectral properties of Jacobians across network depths during training
3. Conduct ablation studies isolating the effects of unitary message passing versus unitary feature transformation on heterophilous graph datasets