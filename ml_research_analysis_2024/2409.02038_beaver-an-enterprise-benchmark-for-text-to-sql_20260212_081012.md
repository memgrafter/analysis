---
ver: rpa2
title: 'BEAVER: An Enterprise Benchmark for Text-to-SQL'
arxiv_id: '2409.02038'
source_url: https://arxiv.org/abs/2409.02038
tags:
- tables
- table
- building
- column
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEAVER, the first enterprise text-to-SQL
  benchmark sourced from real private enterprise data warehouses. The dataset includes
  natural language queries and their correct SQL statements collected from actual
  query logs, providing a more realistic evaluation environment compared to existing
  benchmarks based on web tables.
---

# BEAVER: An Enterprise Benchmark for Text-to-SQL

## Quick Facts
- arXiv ID: 2409.02038
- Source URL: https://arxiv.org/abs/2409.02038
- Reference count: 12
- First enterprise text-to-SQL benchmark sourced from real private enterprise data warehouses

## Executive Summary
BEAVER introduces the first enterprise-focused text-to-SQL benchmark derived from actual private enterprise data warehouses. The dataset comprises natural language queries and their corresponding SQL statements collected from real query logs, providing a more realistic evaluation environment than existing benchmarks based on public web tables. When tested on state-of-the-art LLMs including GPT-4o and Llama3.1-70B-Instruct, models demonstrated significantly poor performance even with standard prompt engineering and RAG techniques, revealing substantial gaps in enterprise text-to-SQL capabilities.

## Method Summary
The authors created BEAVER by collecting natural language queries and their correct SQL translations from actual enterprise query logs, establishing a benchmark that reflects real-world enterprise data warehouse complexity. The evaluation methodology involved testing off-the-shelf LLMs on this dataset using standard prompt engineering and retrieval-augmented generation approaches. Performance was measured by comparing generated SQL against ground truth SQL statements, with particular attention to failures in table retrieval, column mapping, and handling complex queries involving joins, aggregations, and nested operations typical of business intelligence use cases.

## Key Results
- Off-the-shelf LLMs like GPT-4o and Llama3.1-70B-Instruct performed poorly on enterprise text-to-SQL tasks
- Three primary factors contributed to poor performance: complex enterprise schemas, business-oriented queries requiring sophisticated SQL operations, and inability to train on private enterprise data
- Models struggled specifically with table retrieval, column mapping, and complex query execution in enterprise contexts

## Why This Works (Mechanism)
The benchmark reveals fundamental limitations in current LLMs when applied to enterprise environments, demonstrating that existing models trained on public data cannot effectively translate business queries to SQL for private enterprise schemas.

## Foundational Learning
- Enterprise schema complexity: Business data models are more intricate than public datasets, requiring understanding of domain-specific relationships
  - Why needed: Enterprise data warehouses contain complex relationships not present in web tables
  - Quick check: Compare schema complexity metrics between BEAVER and existing benchmarks
- Business query patterns: Enterprise queries typically involve joins, aggregations, and nested queries
  - Why needed: Business intelligence questions require sophisticated SQL operations beyond simple lookups
  - Quick check: Analyze query complexity distributions across benchmarks
- Private data constraints: Public LLMs cannot access proprietary enterprise data for training
  - Why needed: Models lack exposure to enterprise-specific terminology and schema patterns
  - Quick check: Measure performance gap between enterprise and public domain adaptation

## Architecture Onboarding

Component map: Enterprise Schema -> Natural Language Query -> SQL Translation -> Evaluation

Critical path: The transformation from business queries to executable SQL depends critically on accurate schema understanding and query complexity handling.

Design tradeoffs: The benchmark prioritizes real-world enterprise complexity over generalizability, accepting that results may not transfer to simpler domains.

Failure signatures: Common failure modes include incorrect table selection, column mapping errors, and inability to construct appropriate join operations for business logic.

First experiments:
1. Evaluate schema simplification impact on model performance
2. Test RAG effectiveness with enterprise-specific retrieval mechanisms
3. Compare performance across different enterprise domains to assess generalizability

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of BEAVER findings across different enterprise contexts, the potential for privacy-preserving training approaches on enterprise data, and the relative importance of schema complexity versus query sophistication in driving performance degradation.

## Limitations
- Limited specification of dataset diversity across enterprise sources and industry domains
- Absence of comprehensive ablation studies to isolate performance degradation factors
- Focus on only two model architectures without exploring domain-adapted alternatives

## Confidence

High confidence: The existence of BEAVER as a new benchmark dataset
Medium confidence: The three identified reasons for poor LLM performance
Low confidence: The generalizability of findings across different enterprise contexts

## Next Checks

1. Conduct ablation studies varying schema complexity, query types, and domain specificity to quantify their individual contributions to performance degradation
2. Test additional model architectures including domain-adapted smaller models to determine if model scale or adaptation strategy matters more than general capability
3. Compare BEAVER performance against existing benchmarks after normalizing for schema and query complexity to establish the relative difficulty contribution of enterprise-specific factors