---
ver: rpa2
title: 'Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in
  Datasets and Large Language Models'
arxiv_id: '2403.09750'
source_url: https://arxiv.org/abs/2403.09750
tags:
- knowledge
- procedural
- declarative
- tasks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a meta-cognitive analysis framework for evaluating
  declarative and procedural knowledge in large language models (LLMs). The authors
  define declarative knowledge as fundamental facts and procedural knowledge as generalized
  problem-solving strategies, then use GPT-4 to decompose reasoning processes in QA
  datasets into these two knowledge types.
---

# Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models

## Quick Facts
- arXiv ID: 2403.09750
- Source URL: https://arxiv.org/abs/2403.09750
- Reference count: 4
- Primary result: Declarative knowledge provides greater benefits than procedural knowledge in most tasks, with benefits scaling differently with model size and pre-training steps

## Executive Summary
This paper introduces a meta-cognitive analysis framework to evaluate how large language models utilize declarative (factual) and procedural (problem-solving) knowledge. The authors decompose reasoning processes in QA datasets using GPT-4, then measure model performance improvement when providing ground truth knowledge hints. Experiments across 32 models and 13 datasets reveal that declarative knowledge benefits are generally greater than procedural, except in simple reasoning tasks where procedural knowledge excels. The framework also shows that both knowledge types improve with model size and pre-training, but declarative knowledge utilization scales faster.

## Method Summary
The framework uses GPT-4 to decompose QA pairs into declarative (facts) and procedural (step-by-step reasoning) knowledge components. These are converted into hints injected into prompts, and models are evaluated with three conditions: no hints, declarative hints, and procedural hints. Performance is measured by error rate reduction, with scores calculated as the percentage improvement over baseline. The approach is applied across 32 LLMs and 13 diverse datasets to analyze knowledge utilization patterns.

## Key Results
- Declarative knowledge provides greater benefits than procedural knowledge in 9 out of 13 datasets
- Procedural knowledge is more beneficial only in reasoning tasks with simple logic (e.g., GSM8K, MultiArith)
- Both knowledge utilization improve with model size and pre-training steps, but declarative knowledge scales faster
- Benefits from combined knowledge hints exceed those from individual knowledge types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing ground-truth declarative knowledge as hints improves model performance more than procedural hints in most tasks.
- Mechanism: By explicitly supplying the declarative facts needed to solve a problem, the model can bypass the need to recall or infer these facts from context, reducing error rates.
- Core assumption: The model's knowledge gaps are primarily declarative in nature for the evaluated tasks.
- Evidence anchors:
  - [abstract] "In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge."
  - [section] "declarative score is larger than procedural score in 9 datasets."
- Break condition: If a task's difficulty is driven by logical inference or multi-step reasoning rather than missing facts, procedural hints may outperform declarative hints.

### Mechanism 2
- Claim: Procedural knowledge benefits are larger than declarative knowledge only in reasoning tasks with simple logic.
- Mechanism: In tasks where the main challenge is understanding the sequence of operations rather than recalling facts, providing step-by-step procedural hints directly addresses the bottleneck.
- Core assumption: Simple reasoning tasks are logic-heavy but fact-light.
- Evidence anchors:
  - [abstract] "Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic."
  - [section] "in simple mathematical datasets such as GSM8K and MultiArith... benefits from procedural knowledge are more than other datasets."
- Break condition: When reasoning complexity increases, models struggle to parse procedural hints, reducing their effectiveness.

### Mechanism 3
- Claim: Model ability to utilize both declarative and procedural knowledge improves with size and pre-training steps, but at different rates.
- Mechanism: Larger models and more pre-training provide richer internal representations, allowing better integration of external knowledge hints. However, declarative knowledge utilization improves faster than procedural.
- Core assumption: Model scale and training time affect knowledge utilization differentially.
- Evidence anchors:
  - [abstract] "As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed."
  - [section] "models with more parameters show clear improvement in capturing both declarative and procedural knowledge... improvement in capturing declarative knowledge is significantly higher than procedural knowledge."
- Break condition: If model architecture or training method does not scale knowledge utilization, benefits may plateau.

## Foundational Learning

- Concept: Metacognitive theory (declarative vs procedural knowledge)
  - Why needed here: The paper's framework hinges on distinguishing these two knowledge types to analyze LLM performance.
  - Quick check question: Can you define declarative knowledge as "knowing that" and procedural knowledge as "knowing how" without looking at the paper?

- Concept: In-context learning and knowledge injection
  - Why needed here: The evaluation method relies on providing hints within the prompt to measure knowledge effects.
  - Quick check question: What is the difference between providing declarative hints versus procedural hints in a prompt?

- Concept: Error rate improvement calculation
  - Why needed here: The score metric is based on error reduction when hints are provided.
  - Quick check question: If the original error rate is 0.4 and the error rate with hints is 0.2, what is the procedural score?

## Architecture Onboarding

- Component map:
  - Original QA pairs -> GPT-4 decomposition -> Hint-injected prompts -> Model evaluation -> Score aggregation

- Critical path:
  1. Decompose each QA pair into declarative and procedural knowledge using GPT-4.
  2. Generate hint-injected prompts for each knowledge type.
  3. Run inference on each model-dataset pair with each hint type.
  4. Compute error rates and derive procedural/declarative/combined scores.
  5. Aggregate scores across datasets for model-level insights.

- Design tradeoffs:
  - Using GPT-4 for decomposition adds cost and dependency but enables scalable annotation.
  - Providing both knowledge types together may mask individual contributions; hence separate evaluation is critical.
  - Noise is introduced in declarative hints to avoid over-representation of procedural content.

- Failure signatures:
  - Decomposition errors: GPT-4 fails to correctly identify knowledge types → invalid scores.
  - Hint redundancy: Procedural hints embedded in declarative hints → inflated declarative scores.
  - Model confusion: Overly complex hints overwhelm the model → no improvement or degradation.

- First 3 experiments:
  1. Run decomposition on a small sample of QA pairs and manually verify correctness.
  2. Evaluate one model (e.g., Llama-7B) on one dataset (e.g., GSM8K) with all three hint types to validate the scoring logic.
  3. Compare scores between two model sizes (e.g., 7B vs 13B) on one dataset to confirm size effect trend.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific relationship between model size and the ability to utilize procedural knowledge?
- Basis in paper: [explicit] The paper states that larger models show improvement in capturing both declarative and procedural knowledge, with the improvement in declarative knowledge being significantly higher than procedural knowledge.
- Why unresolved: The paper does not provide a detailed analysis of how model size specifically affects the ability to utilize procedural knowledge compared to declarative knowledge.
- What evidence would resolve it: A detailed study comparing the improvement rates of procedural and declarative knowledge utilization across different model sizes.

### Open Question 2
- Question: How does the complexity of logical reasoning in tasks affect the utilization of procedural knowledge?
- Basis in paper: [explicit] The paper mentions that in tasks with more complex logic, such as high school mathematics, the benefits from procedural knowledge are lower compared to simpler logic tasks.
- Why unresolved: The paper does not provide a clear explanation of how task complexity specifically impacts the utilization of procedural knowledge.
- What evidence would resolve it: An analysis of the relationship between task complexity and the effectiveness of procedural knowledge hints in various tasks.

### Open Question 3
- Question: What is the impact of pre-training steps on the ability to utilize both declarative and procedural knowledge?
- Basis in paper: [explicit] The paper indicates that as pre-training progresses, the ability to utilize both types of knowledge improves, but at different rates.
- Why unresolved: The paper does not provide a detailed breakdown of how each pre-training step specifically affects the utilization of declarative and procedural knowledge.
- What evidence would resolve it: A detailed study tracking the improvement in knowledge utilization at each pre-training step.

### Open Question 4
- Question: How do different types of knowledge hints (declarative, procedural, or combined) affect the performance of models on various tasks?
- Basis in paper: [explicit] The paper evaluates the effectiveness of different types of knowledge hints on model performance.
- Why unresolved: The paper does not provide a comprehensive analysis of how each type of hint affects performance across different tasks and models.
- What evidence would resolve it: A detailed comparison of model performance with each type of knowledge hint across a wide range of tasks and models.

## Limitations
- The decomposition of QA pairs into declarative and procedural knowledge relies entirely on GPT-4, with no human validation or alternative decomposition methods tested
- The framework assumes knowledge types are mutually exclusive, but some hints may contain both elements
- Results are based on error rate improvements, but the relationship between error reduction and actual knowledge acquisition is not explicitly established

## Confidence
- **High confidence:** The observation that declarative knowledge generally provides greater benefits than procedural knowledge across most tasks
- **Medium confidence:** The claim that procedural knowledge is more beneficial only in simple reasoning tasks, due to potential confounding factors in task complexity measurement
- **Medium confidence:** The differential scaling of declarative versus procedural knowledge utilization with model size, as the mechanism for this difference is not fully explained

## Next Checks
1. **Decomposition validation:** Manually verify GPT-4's decomposition accuracy on a stratified sample of 100+ QA pairs across different dataset types
2. **Cross-decomposer consistency:** Compare results using multiple decomposition approaches (e.g., different prompts, smaller LLMs) to assess sensitivity to the decomposition method
3. **Knowledge type overlap analysis:** Quantitatively measure the actual overlap between declarative and procedural hints to validate the mutual exclusivity assumption