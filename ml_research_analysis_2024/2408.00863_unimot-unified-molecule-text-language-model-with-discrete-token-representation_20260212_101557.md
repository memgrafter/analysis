---
ver: rpa2
title: 'UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation'
arxiv_id: '2408.00863'
source_url: https://arxiv.org/abs/2408.00863
tags:
- molecule
- unimot
- molecular
- text
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniMoT, a unified molecule-text language
  model that treats molecular structures as a distinct language. The key innovation
  is a tokenizer-based architecture that converts molecules into discrete tokens with
  causal dependency using a Vector Quantization-driven tokenizer with a Q-Former to
  bridge molecule-text modality gaps.
---

# UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation

## Quick Facts
- arXiv ID: 2408.00863
- Source URL: https://arxiv.org/abs/2408.00863
- Authors: Shuhan Guo; Yatao Bian; Ruibing Wang; Nan Yin; Zhen Wang; Quanming Yao
- Reference count: 34
- Primary result: Achieves 71.37% ROC-AUC on molecular property prediction across 7 MoleculeNet datasets

## Executive Summary
UniMoT introduces a unified molecule-text language model that treats molecular structures as a distinct language through discrete token representation. The model employs a Vector Quantization-driven tokenizer with a Q-Former to bridge the modality gap between molecules and text, converting molecular structures into causal sequences of tokens that can be processed by large language models. Through a four-stage training scheme, UniMoT achieves state-of-the-art performance across multiple molecular tasks including property prediction, molecule captioning, caption-guided generation, and reagent prediction, demonstrating the effectiveness of unifying molecule and text modalities under a shared autoregressive training paradigm.

## Method Summary
UniMoT converts molecular structures into discrete tokens using a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the molecule-text modality gap. The model employs a four-stage training scheme: (1) pretraining the Causal Q-Former with tailored objectives (MTC, MTM, MTG), (2) optimizing the molecule tokenizer with frozen encoders, (3) integrating the tokenizer with a language model for multi-modal pretraining, and (4) fine-tuning for specific tasks. By expanding the LLM vocabulary with molecule tokens mapped from a learnable codebook, UniMoT enables both molecule-to-text and text-to-molecule tasks using the same next-token-prediction training objective, achieving unified autoregressive pretraining across modalities.

## Key Results
- Achieves 71.37% ROC-AUC on molecular property prediction across 7 MoleculeNet datasets
- Attains 31.3% BLEU-4 score on molecule captioning (PubChem dataset)
- Demonstrates strong generation capabilities with 0.698 BLEU for caption-guided molecule generation and 0.728 BLEU for reagent prediction

## Why This Works (Mechanism)

### Mechanism 1
UniMoT achieves state-of-the-art performance by treating molecules as a distinct language with causal dependency through vector quantization-driven tokenization. The model converts molecules into discrete tokens using a Vector Quantization-driven tokenizer with a Q-Former to bridge the modality gap between molecule and text. This creates causal sequences of tokens that can be processed by LLMs using autoregressive training. The core assumption is that molecules can be effectively represented as discrete tokens with causal dependencies similar to text sequences.

### Mechanism 2
UniMoT unifies molecule and text modalities under a shared token representation and autoregressive training paradigm. By expanding the LLM vocabulary with molecule tokens mapped from the learned codebook, the model can perform both molecule-to-text and text-to-molecule tasks using the same next-token-prediction training objective. The core assumption is that a unified discrete token representation enables effective molecule-text interactions and alignment through autoregressive pretraining.

### Mechanism 3
The four-stage training scheme progressively aligns molecule and text representations while maintaining model stability. Stage-1 pretrains the Causal Q-Former with tailored objectives, Stage-2 optimizes the molecule tokenizer with frozen encoders, Stage-3 integrates the tokenizer with LLM for multi-modal pretraining, and Stage-4 fine-tunes for specific tasks. The core assumption is that progressive training allows for stable alignment of modalities while preserving learned representations.

## Foundational Learning

- **Vector Quantization (VQ)**: Essential for converting continuous molecular feature representations into discrete tokens that can be processed by LLMs. *Quick check: What is the main advantage of using VQ for molecule tokenization compared to continuous representations?*

- **Causal attention mechanisms**: Ensures tokens can only attend to previous tokens, matching the autoregressive nature of language models. *Quick check: How does causal masking in the Q-Former differ from bidirectional attention in standard transformers?*

- **Multi-modal contrastive learning**: Contrastive objectives help align molecule and text representations in a shared semantic space. *Quick check: What are the three tailored objectives used for Causal Q-Former pretraining and what do they optimize?*

## Architecture Onboarding

- **Component map**: Molecule → Graph → Features → Causal Q-Former → Queries → Vector Quantization → Tokens → LLM → Output
- **Critical path**: Molecule → Graph → Features → Causal Q-Former → Queries → Vector Quantization → Tokens → LLM → Output
- **Design tradeoffs**: Discrete vs continuous representations (discrete enables autoregressive training but loses some information); Codebook size (larger captures more nuances but increases computational cost); Query count (more queries capture more information but increase latency)
- **Failure signatures**: Poor molecular property prediction (token quantization losing structural information); Low BLEU scores in captioning (misalignment between molecule and text modalities); Invalid molecule generation (issues with reconstruction pipeline or codebook mapping)
- **First 3 experiments**: (1) Test molecule tokenizer with small molecule dataset to verify discrete token generation and reconstruction; (2) Evaluate causal query generation by comparing with bidirectional attention baselines; (3) Validate unified vocabulary expansion by testing simple molecule-text sequence processing

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of UniMoT vary when scaling the LLM backbone to 13B parameters versus using LoRA-based adaptation? The paper mentions that scaling up the LLM to 13B or adopting a full fine-tuning strategy yields only marginal improvements compared to using Llama-2-7B with LoRA, but doesn't provide quantitative performance data comparing the two approaches.

### Open Question 2
What is the optimal codebook size for the molecule tokenizer that balances performance and model complexity across different molecular tasks? While the paper identifies 2048 as optimal for their experiments, it doesn't explore how this choice varies with different molecular domains, dataset sizes, or specific task requirements.

### Open Question 3
How does UniMoT perform on molecule editing tasks that require precise structural modifications compared to its current capabilities? The paper explicitly states UniMoT has yet to be extensively tested on more complex tasks like molecule editing that require precise structural modifications.

## Limitations

- The conversion of continuous molecular features to discrete tokens through vector quantization may result in significant information loss, particularly for subtle molecular substructures
- Performance on molecules with rare substructures or those outside the training distribution is not adequately addressed
- The four-stage training procedure requires substantial computational resources and may lead to catastrophic forgetting if not carefully managed

## Confidence

**High Confidence Claims:**
- The unified autoregressive training paradigm can handle both molecule-to-text and text-to-molecule tasks effectively
- The four-stage training approach provides a reasonable framework for progressive modality alignment
- The discrete token representation enables LLM processing of molecular structures

**Medium Confidence Claims:**
- State-of-the-art performance on molecular property prediction (71.37% ROC-AUC on 7 MoleculeNet datasets)
- Strong captioning performance (31.3% BLEU-4 on PubChem)
- Effective generation capabilities (0.698 BLEU for caption-guided generation)

**Low Confidence Claims:**
- The specific architectural choices (Q-Former design, codebook size, etc.) are optimal for the task
- The model will generalize equally well to all chemical domains
- The training procedure is robust to hyperparameter variations

## Next Checks

**Validation Check 1: Information Retention Analysis**
Implement a systematic evaluation measuring the information content preserved during the molecule→tokens→reconstruction pipeline. Use metrics like reconstruction accuracy, KL divergence between original and reconstructed molecular distributions, and substructure matching scores. Test this across molecules of varying complexity and size to identify the limits of the tokenization approach.

**Validation Check 2: Out-of-Distribution Performance Testing**
Evaluate UniMoT on molecules from underrepresented classes in the training data, including rare heterocycles, organometallic compounds, and molecules with unusual functional groups. Compare performance against continuous representation methods and assess the rate of invalid molecule generation. This will reveal whether the discrete tokenization approach has inherent biases toward common molecular scaffolds.

**Validation Check 3: Ablation Study on Training Stages**
Systematically disable or modify each training stage to quantify their individual contributions to final performance. Specifically: (a) Train without Stage-1 Causal Q-Former pretraining to test if the tailored objectives are essential, (b) Remove Stage-2 tokenizer optimization to assess if the progressive approach is necessary, and (c) Skip Stage-4 fine-tuning to determine the importance of instruction tuning. This will reveal whether the four-stage approach is truly necessary or if a simpler training procedure would suffice.