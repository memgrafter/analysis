---
ver: rpa2
title: Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales
  Inventory Control
arxiv_id: '2406.18351'
source_url: https://arxiv.org/abs/2406.18351
tags:
- uni00000013
- uni00000018
- uni00000014
- uni00000015
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the sample inefficiency challenge in applying
  reinforcement learning (RL) to lost-sales inventory control, where demand may be
  censored due to insufficient inventory. The authors propose a framework that combines
  RL with feedback graphs (RLFG) and intrinsically motivated exploration (IME) to
  improve sample efficiency.
---

# Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales Inventory Control

## Quick Facts
- arXiv ID: 2406.18351
- Source URL: https://arxiv.org/abs/2406.18351
- Reference count: 35
- One-line primary result: RL with feedback graphs and intrinsically motivated exploration significantly improves sample efficiency in lost-sales inventory control

## Executive Summary
This paper addresses the sample inefficiency challenge in applying reinforcement learning to lost-sales inventory control, where demand may be censored due to insufficient inventory. The authors propose a framework that combines reinforcement learning with feedback graphs and intrinsically motivated exploration to improve sample efficiency. The method constructs a feedback graph tailored to the lost-sales problem to generate additional "side experiences" and designs an intrinsic reward that encourages the RL agent to explore state-action spaces that yield more side experiences. Experiments across single-item, multi-item, and multi-echelon inventory environments show significant improvements in sample efficiency and final performance.

## Method Summary
The proposed method combines reinforcement learning with feedback graphs (RLFG) and intrinsically motivated exploration (IME). A feedback graph module dynamically constructs connections between state-action pairs based on observed demand - complete connectivity when demand is uncensored, and partial connectivity when censored. The FG module generates side experiences for state-action pairs with inventory levels less than or equal to observed demand. An intrinsic reward module computes rewards based on prediction error (curiosity) and the quantity of side experiences generated. The RL agent uses both extrinsic (cost) and intrinsic rewards to update policies using off-policy algorithms like TD3 and Rainbow.

## Key Results
- RLFG achieves 1.5-2.5× faster convergence compared to baseline RL methods across all inventory settings
- TD3-FG outperforms all baseline methods including heuristic policies in multi-echelon inventory control
- The method demonstrates robustness to varying lead times (L=2,3,4) and demand distributions
- Sample efficiency improvements are most pronounced in multi-item and multi-echelon settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feedback graphs reduce sample complexity by improving update probabilities across state-action pairs
- Mechanism: When an experience is sampled, the FG module generates side experiences for all state-action pairs with inventory levels less than or equal to the observed demand
- Core assumption: The demand distribution is independent of state-action pairs
- Evidence anchors:
  - [abstract] "Theoretical analysis demonstrates that the feedback graph improves update probabilities across state-action pairs"
  - [section] "The update probability for Q-learning with FG under scenario 6 is Equation (5)...˜µ(s, a) = Ed∼Pd[˜µ(s, a|d)] ≥ Ed∼Pd[µ(s, a|d)] = µ(s, a)"
  - [corpus] No direct evidence for FG reducing sample complexity in inventory control
- Break condition: If demand becomes state-dependent or correlated with inventory levels

### Mechanism 2
- Claim: Intrinsically motivated exploration directs the agent to state-action spaces that yield more side experiences
- Mechanism: The intrinsic reward combines curiosity (prediction error) with the quantity of side experiences generated
- Core assumption: Curiosity-driven exploration is effective in inventory control environments with censored demand
- Evidence anchors:
  - [abstract] "designs an intrinsic reward that encourages the RL agent to explore state-action spaces that yield more side experiences"
  - [section] "To satisfy both conditions, manually designing a behavior policy is difficult and not general enough. Instead, we propose an intrinsic reward given in Equation 7 promoting side experience generation"
  - [corpus] No direct evidence for intrinsic rewards improving sample efficiency in inventory control
- Break condition: If the intrinsic reward weight is too large, the agent may prioritize exploration over exploitation

### Mechanism 3
- Claim: Dynamic FG construction adapts to censored demand, maintaining effectiveness across different demand levels
- Mechanism: When demand is censored (do_t = yt), FG connects only state-action pairs with inventory levels less than or equal to yt; when demand is uncensored, FG forms a complete graph
- Core assumption: The lost-sales property can be handled by restricting side experience generation to observable inventory levels
- Evidence anchors:
  - [abstract] "we design the feedback graph (FG) specially for lost-sales IC problems to generate abundant side experiences aid RL updates"
  - [section] "When do_t is uncensored, FG forms a complete graph...When do_t is censored, only the state-action pairs with smaller inventory levels than st can be utilized to generate side experiences"
  - [corpus] No direct evidence for dynamic FG construction in inventory control
- Break condition: If demand distribution has high variance, the FG may frequently switch between complete and partial connectivity

## Foundational Learning

- Concept: Reinforcement Learning with Feedback Graphs
  - Why needed here: Standard RL methods suffer from low sample efficiency, which is critical in inventory control where online experience collection is expensive
  - Quick check question: What is the main advantage of using feedback graphs in reinforcement learning?

- Concept: Lost-Sales Inventory Control
  - Why needed here: The lost-sales phenomenon creates censored demand, making learning more challenging and requiring specialized approaches
  - Quick check question: How does the lost-sales property affect the observability of demand in inventory control?

- Concept: Intrinsically Motivated Exploration
  - Why needed here: The FG module generates side experiences, but the agent needs guidance to explore state-action spaces that yield more side experiences
  - Quick check question: What is the purpose of combining curiosity-driven exploration with feedback graphs?

## Architecture Onboarding

- Component map: Environment -> RL Module -> FG Module -> Intrinsic Reward Module -> Replay Buffers -> RL Module

- Critical path:
  1. Agent interacts with environment, collects experience (s, a, r, s', do)
  2. FG module generates side experiences based on observed demand
  3. Intrinsic reward module calculates intrinsic rewards for experiences
  4. Experiences and side experiences stored in replay buffers
  5. RL module samples mini-batch from replay buffers
  6. RL module updates policy using combined rewards (extrinsic + intrinsic)

- Design tradeoffs:
  - Complete vs. partial FG connectivity: Complete graphs maximize side experiences but may include invalid transitions
  - Intrinsic reward weight: Balances exploration and exploitation; too high harms final performance
  - Side experience buffer size: Larger buffers improve sample efficiency but increase memory usage

- Failure signatures:
  - Poor sample efficiency: FG module not generating enough valid side experiences
  - Unstable learning: Intrinsic reward weight too high, causing excessive exploration
  - Suboptimal final performance: Insufficient exploitation due to strong intrinsic rewards

- First 3 experiments:
  1. Test FG module with different demand distributions to verify side experience generation
  2. Evaluate intrinsic reward effectiveness by comparing learning curves with/without intrinsic rewards
  3. Test different FG connectivity levels (complete vs. partial) to find optimal balance between sample efficiency and validity

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of RLFG and IME scale with increasing lead times beyond L=4 in lost-sales inventory control?
  - Basis in paper: [inferred] The paper experiments with L=2, 3, and 4, showing improved sample efficiency and performance, but does not test longer lead times
  - Why unresolved: The paper's experiments are limited to a maximum lead time of L=4
  - What evidence would resolve it: Experimental results demonstrating the performance and sample efficiency of RLFG and IME for lead times significantly longer than L=4

- Open Question 2: What is the impact of demand distribution characteristics (e.g., variance, skewness) on the effectiveness of RLFG and IME in lost-sales inventory control?
  - Basis in paper: [explicit] The paper mentions that demand follows Poisson distribution but does not explore how different demand distributions affect the method's performance
  - Why unresolved: The experiments use a single demand distribution (Poisson)
  - What evidence would resolve it: Comparative experiments using different demand distributions to evaluate how distribution properties influence RLFG and IME's performance

- Open Question 3: How does the computational overhead of constructing and maintaining the feedback graph scale with problem size in multi-item and multi-echelon inventory control?
  - Basis in paper: [inferred] The paper demonstrates effectiveness in multi-item and multi-echelon settings but does not discuss the computational cost
  - Why unresolved: While performance improvements are shown, the paper does not quantify the computational resources required
  - What evidence would resolve it: Analysis of computational time and memory usage for feedback graph operations as the number of items and echelons increases

## Limitations

- Theoretical claims about feedback graphs improving update probabilities lack formal proofs for the specific inventory control setting
- Effectiveness of intrinsically motivated exploration for inventory control lacks direct domain-specific validation
- Dynamic FG construction's adaptability across varying demand distributions remains unproven
- Computational overhead of maintaining multiple replay buffers and computing intrinsic rewards is not quantified

## Confidence

- Mechanism 1 (FG improves update probabilities): **Medium** - Theoretical claims are made but not rigorously proven for the specific inventory control setting
- Mechanism 2 (IME directs exploration): **Low-Medium** - No domain-specific validation of intrinsic rewards in inventory control
- Mechanism 3 (Dynamic FG adapts to censored demand): **Medium** - Algorithm description is clear but effectiveness across demand distributions is not validated
- Experimental results: **High** - Results are presented with appropriate baselines and statistical analysis

## Next Checks

1. **Theoretical validation**: Derive formal sample complexity bounds for the RLFG algorithm in inventory control settings, comparing against standard RL methods

2. **Intrinsic reward ablation**: Run experiments with different intrinsic reward weights (0, 0.1, 0.5, 1.0) to identify optimal balance between exploration and exploitation

3. **FG connectivity analysis**: Test different levels of FG connectivity (partial graphs with varying connection thresholds) to quantify the tradeoff between sample efficiency and side experience validity