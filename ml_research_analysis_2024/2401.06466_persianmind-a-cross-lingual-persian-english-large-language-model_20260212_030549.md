---
ver: rpa2
title: 'PersianMind: A Cross-Lingual Persian-English Large Language Model'
arxiv_id: '2401.06466'
source_url: https://arxiv.org/abs/2401.06466
tags:
- persian
- language
- persianmind
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PersianMind, an open-source bilingual Persian-English
  large language model built on LLaMa2-7B-chat. The authors expanded the tokenizer
  with 10,000 Persian tokens and trained the model on nearly 2 billion Persian tokens
  using LoRA-based parameter-efficient fine-tuning.
---

# PersianMind: A Cross-Lingual Persian-English Large Language Model

## Quick Facts
- arXiv ID: 2401.06466
- Source URL: https://arxiv.org/abs/2401.06466
- Reference count: 22
- Primary result: State-of-the-art Persian-English bilingual model matching GPT-3.5-turbo in Persian reading comprehension

## Executive Summary
PersianMind is an open-source bilingual Persian-English large language model built on LLaMa2-7B-chat through token expansion and LoRA-based fine-tuning. The model expands the tokenizer with 10,000 Persian tokens and trains on nearly 2 billion Persian tokens while preserving English capabilities. PersianMind achieves state-of-the-art performance on Persian benchmarks and demonstrates strong cross-lingual semantic textual similarity capabilities.

## Method Summary
The method involves expanding LLaMa2-7B-chat's tokenizer with 10,000 Persian subwords, expanding input/output embeddings, and using LoRA fine-tuning with rank-8 decomposition for parameter-efficient adaptation. The model is trained in three iterative steps: first on plain Persian text corpus, then on instruction-tuned datasets including machine-translated materials. Training uses data parallelism across 4 RTX 3090 GPUs, consuming 960 GPU-hours and emitting approximately 232 kg CO2 equivalent.

## Key Results
- Achieves state-of-the-art performance on Persian subset of Belebele benchmark
- Matches GPT-3.5-turbo performance on ParsiNLU multiple-choice QA task
- Outperforms previous masked language models in cross-lingual semantic textual similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding tokenizer with Persian subwords preserves model capacity while enabling efficient training on Persian data
- Mechanism: Adding 10,000 Persian tokens enables subword-level processing, reducing sequence length and training time while maintaining semantic granularity
- Core assumption: Persian words can be effectively segmented into subwords that capture meaningful linguistic units
- Evidence anchors: Abstract mentions tokenizer expansion; section 3.1 describes BPE tokenizer training; weak corpus analysis evidence
- Break condition: If Persian morphology is too complex for BPE to capture meaningful subword units

### Mechanism 2
- Claim: LoRA fine-tuning preserves English knowledge while enabling Persian language learning
- Mechanism: Freezing base model parameters while training only low-rank adaptation matrices allows Persian learning without overwriting English representations
- Core assumption: Language knowledge is distributed across transformer layers in a way that allows selective adaptation
- Evidence anchors: Abstract confirms English knowledge preservation; section 3.2 describes LoRA training; weak English capability retention evidence
- Break condition: If English knowledge is stored in updated parameters or rank-8 decomposition is insufficient

### Mechanism 3
- Claim: Multilingual transfer learning improves cross-lingual task performance
- Mechanism: Training on Persian data enhances English task performance through shared semantic representations
- Core assumption: Core linguistic and reasoning capabilities transfer across languages through shared representations
- Evidence anchors: Section 4.1 shows English performance improvement after Persian training; abstract mentions transfer learning; moderate evidence from performance improvements
- Break condition: If task-specific knowledge is too language-dependent or shared representations are insufficient

## Foundational Learning

- Concept: Tokenization and vocabulary design
  - Why needed here: Tokenizer choice directly impacts model efficiency and representation quality for Persian text
  - Quick check question: What happens to sequence length if Persian text is tokenized at character level instead of subword level?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: LoRA enables cost-effective adaptation while preserving base model capabilities
  - Quick check question: How does freezing base parameters during LoRA training prevent catastrophic forgetting?

- Concept: Multilingual model evaluation
  - Why needed here: Proper evaluation across languages is crucial to verify cross-lingual capabilities and transfer learning effects
  - Quick check question: Why is it important to evaluate model performance on both source and target languages after cross-lingual fine-tuning?

## Architecture Onboarding

- Component map: LLaMa2-7B-chat base model + expanded tokenizer (41,510 tokens) + LoRA adapters + expanded input/output embeddings
- Critical path: Tokenizer expansion → LoRA initialization → Persian corpus training → Instruction tuning → Evaluation
- Design tradeoffs: Larger tokenizer improves Persian representation but increases memory usage; LoRA reduces trainable parameters but may limit adaptation capacity
- Failure signatures: Poor perplexity scores indicating tokenization issues; catastrophic forgetting shown by degraded English performance; low cross-lingual transfer indicating inadequate shared representations
- First 3 experiments:
  1. Evaluate perplexity on Persian and English validation sets after each training phase to monitor language preservation
  2. Test cross-lingual semantic similarity on parallel sentence pairs to verify transfer learning
  3. Compare BLEU scores for zero-shot vs fine-tuned translation to assess adaptation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PersianMind's performance compare to GPT-4 or Claude in Persian language tasks?
- Basis in paper: The paper only compares to GPT-3.5-turbo, leaving performance gap with newer models unknown
- Why unresolved: No direct comparison with more advanced models like GPT-4 or Claude provided
- What evidence would resolve it: Direct evaluations of PersianMind against GPT-4 and Claude on same benchmarks

### Open Question 2
- Question: How does PersianMind's carbon footprint compare to other similar-sized models?
- Basis in paper: Paper provides detailed carbon footprint breakdown but no comparative analysis
- Why unresolved: No comparison with carbon footprints of LLaMA2-7B or GPT-3.5-turbo training
- What evidence would resolve it: Comparative analysis of carbon footprints across various large language models

### Open Question 3
- Question: How does PersianMind's English performance compare to original LLaMA2-7B-chat?
- Basis in paper: Claims English preservation but lacks direct comparison on English tasks
- Why unresolved: No direct performance comparison on English language benchmarks provided
- What evidence would resolve it: Direct evaluations of PersianMind and LLaMA2-7B-chat on same English benchmarks

## Limitations

- Evaluation scope limited to specific Persian benchmarks without broader language capability assessment
- Reliance on machine-translated datasets for instruction tuning may introduce quality issues
- Environmental impact assessment only considers GPU energy consumption, omitting other factors

## Confidence

**High Confidence**: Tokenizer expansion and LoRA fine-tuning implementation is well-documented and follows established practices; performance improvements on Persian benchmarks are measurable and reproducible.

**Medium Confidence**: Cross-lingual transfer learning claims are supported by experimental results but require more extensive validation across diverse tasks; GPT-3.5-turbo matching claim is demonstrated only on specific Persian benchmarks.

**Low Confidence**: Broader claims about general language understanding capabilities and foundation for Persian NLP applications lack sufficient support from current evaluation scope.

## Next Checks

1. Evaluate PersianMind on broader Persian language tasks including creative writing, factual QA across multiple domains, and complex reasoning to verify general language understanding beyond current benchmarks.

2. Conduct thorough testing of English language performance across multiple domains using established English benchmarks to confirm English capabilities were preserved after Persian fine-tuning.

3. Perform detailed analysis of Persian text processing quality by examining tokenization outputs, attention patterns, and handling of complex Persian morphological constructions to verify adequate capture of Persian linguistic features.