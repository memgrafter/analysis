---
ver: rpa2
title: Last-Iterate Convergence of Payoff-Based Independent Learning in Zero-Sum Stochastic
  Games
arxiv_id: '2409.01447'
source_url: https://arxiv.org/abs/2409.01447
tags:
- amax
- have
- lemma
- learning
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops payoff-based, convergent, rational, and symmetric
  learning dynamics for two-player zero-sum matrix and stochastic games. The key innovation
  is a coupled Lyapunov-based approach to handle multiple sets of coupled and stochastic
  iterates evolving on different time scales.
---

# Last-Iterate Convergence of Payoff-Based Independent Learning in Zero-Sum Stochastic Games

## Quick Facts
- arXiv ID: 2409.01447
- Source URL: https://arxiv.org/abs/2409.01447
- Reference count: 40
- One-line primary result: Develops first last-iterate finite-sample guarantees for payoff-based independent learning in zero-sum matrix and stochastic games

## Executive Summary
This paper establishes the first last-iterate finite-sample guarantees for payoff-based independent learning in zero-sum matrix and stochastic games. The key innovation is a coupled Lyapunov-based approach that handles multiple sets of coupled and stochastic iterates evolving on different time scales. The authors develop independent, rational, and symmetric learning dynamics that achieve polynomial sample complexity, overcoming previous limitations where exponential dependencies prevented efficient learning. The results provide convergence rates of O(ε⁻¹) for finding Nash distributions in matrix games and Õ(ε⁻⁸) for finding Nash equilibria in stochastic games.

## Method Summary
The method uses a coupled Lyapunov framework to analyze multiple stochastic iterates evolving on different time scales. For matrix games, it employs smoothed best-response dynamics with exploration to achieve polynomial sample complexity. For stochastic games, it combines value iteration with smoothed best-response dynamics, using coupled Lyapunov drift inequalities to track convergence. The approach modifies standard softmax policies to include exploration components that prevent exponentially small probabilities, enabling polynomial rather than exponential sample complexity bounds.

## Key Results
- Achieves O(ε⁻¹) sample complexity to find Nash distribution in zero-sum matrix games
- Achieves Õ(ε⁻⁸) sample complexity to find Nash equilibrium in zero-sum stochastic games
- First last-iterate finite-sample guarantees for payoff-based independent learning
- Rational dynamics that converge to best responses when opponent is stationary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The coupled Lyapunov drift inequalities enable tracking convergence of multiple stochastic iterates evolving on different time scales.
- Mechanism: By constructing a Lyapunov function for each set of stochastic iterates and establishing a Lyapunov drift inequality for each, then carefully combining these coupled inequalities, the authors derive finite-sample bounds.
- Core assumption: The individual Lyapunov drift inequalities capture the true evolution of each iterate set under the coupled updates.
- Evidence anchors:
  - [abstract] "To overcome this challenge, we developed a coupled Lyapunov-based approach, which involves constructing a Lyapunov function for each set of stochastic iterates and establishing a Lyapunov drift inequality for each."
  - [section] "Although complete proofs of all our theorems are presented in the appendix, here we use Theorem 3.1 as an example to present a detailed proof sketch to elaborate on our technical novelties."
- Break condition: If the coupling between iterates is too strong, the drift inequalities may not decouple cleanly, invalidating the bound derivation.

### Mechanism 2
- Claim: The modification from σ_τ to σ_τ^ε in Algorithm 2 prevents exponentially small exploration probabilities, enabling polynomial sample complexity.
- Mechanism: The uniform component ε/|A_i| in σ_τ^ε ensures a lower bound ℓ_τ,ε that is polynomial in ε rather than exponential, removing the smoothing bias that prevented polynomial bounds.
- Core assumption: The exploration margin ε can be tuned to control the lower bound without sacrificing convergence.
- Evidence anchors:
  - [section] "Due to the exponential nature of softmax policies, the parameter ℓ_τ is also an exponential function of the temperature τ."
  - [section] "The parameter ℓ_τ will be important in our analysis as it captures the exploration capabilities of Algorithm 1."
- Break condition: If ε is chosen too small, the lower bound may again become exponentially small, reintroducing the bias.

### Mechanism 3
- Claim: The independent, payoff-based learning dynamics are rational in the sense of Bowling and Veloso (2001).
- Mechanism: By using on-policy updates that move each player's policy toward the best response to the opponent's current policy, the dynamics naturally find best responses when the opponent's policy is stationary.
- Core assumption: The opponent's policy is stationary or converges to stationarity during learning.
- Evidence anchors:
  - [abstract] "our learning dynamics are independent, requiring no coordination between agents during learning, and rational, meaning each agent converges to the (smoothed) best response to the opponent if the opponent plays an (asymptotically) stationary policy."
  - [section] "Intuitively, the reason that our algorithm is rational is that it performs an on-policy update in RL."
- Break condition: If the opponent's policy is non-stationary or changes adversarially, the on-policy updates may not converge to best responses.

## Foundational Learning

- Concept: Lyapunov drift analysis
  - Why needed here: To prove convergence of stochastic approximation algorithms with coupled iterates
  - Quick check question: What property must the expected Lyapunov drift have for convergence?

- Concept: Stochastic approximation with Markovian noise
  - Why needed here: To handle the time-varying policies that induce time-inhomogeneous Markov chains
  - Quick check question: How does time-inhomogeneity affect the standard stochastic approximation framework?

- Concept: Entropy regularization and Nash gap
  - Why needed here: To measure performance of learning dynamics in matrix and stochastic games
  - Quick check question: What is the relationship between the regularized Nash gap and the Nash gap?

## Architecture Onboarding

- Component map:
  Outer loop: Minimax value iteration approximation
  Inner loop: Matrix game solver using smoothed best response dynamics
  Policy update: Softmax best response with exploration
  Value function update: Temporal difference learning
  Lyapunov monitors: L_v, L_sum, L_q, L_π for convergence tracking

- Critical path:
  1. Initialize value functions and policies
  2. For each time step:
     a. Solve inner matrix game to update policies
     b. Update value functions via TD learning
     c. Monitor Lyapunov functions
  3. Return final policies

- Design tradeoffs:
  - Constant vs diminishing stepsizes: Constant stepsizes give geometric convergence of value iteration but statistical error; diminishing stepsizes eliminate both but converge slower
  - Temperature τ: Higher τ gives better exploration but larger smoothing bias; lower τ reduces bias but may hurt exploration
  - Exploration parameter ε: Balances between uniform exploration and following softmax best response

- Failure signatures:
  - Lyapunov functions not decreasing: Stepsizes too large or coupling too strong
  - Policies collapsing to single actions: Temperature too low or ε too small
  - Value functions diverging: Stepsizes too large or insufficient exploration

- First 3 experiments:
  1. Verify Lyapunov drift inequalities hold empirically for Algorithm 3 with synthetic game
  2. Test sensitivity of sample complexity to temperature τ and exploration parameter ε
  3. Compare convergence rates of constant vs diminishing stepsizes on a small stochastic game

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental limit of last-iterate convergence for payoff-based independent learning in zero-sum matrix games?
- Basis in paper: The paper states "An immediate future direction is to improve the convergence rate (either by developing new analysis techniques or new algorithmic ideas) and investigate the fundamental limit (i.e., lower bounds) for the last-iterate convergence of payoff-based independent learning in these games."
- Why unresolved: The authors achieve a sample complexity of $\tilde{O}(\epsilon^{-8})$ but explicitly state this is unlikely to be tight. No lower bounds are provided or referenced.
- What evidence would resolve it: A matching lower bound proof showing $\tilde{\Omega}(\epsilon^{-8})$ sample complexity, or a new algorithm achieving better sample complexity.

### Open Question 2
- Question: Can provable results be developed for payoff-based independent learning in zero-sum stochastic games when the opponent follows an arbitrary (non-stationary) policy?
- Basis in paper: The authors note "Our results are presented for the tabular setting... It is an interesting future direction to investigate whether provable results can be developed under weaker assumptions about the opponent. This can be challenging in general for arbitrary opponents, as achieving sublinear regret in such stochastic games is known to be hard."
- Why unresolved: The current theoretical guarantees only apply when (1) the opponent uses the same learning dynamics or (2) follows a stationary policy. The paper acknowledges this limitation and cites hardness results for general opponents.
- What evidence would resolve it: A new algorithm and analysis showing last-iterate convergence guarantees under weaker assumptions about the opponent's behavior, or a hardness result proving such guarantees are impossible.

### Open Question 3
- Question: How can function approximation be incorporated into payoff-based independent learning for zero-sum games while maintaining provable performance guarantees?
- Basis in paper: The authors state "In real applications, tabular RL algorithms often suffer greatly from the curse of dimensionality. Therefore, we are interested in seeing if it is possible to incorporate function approximation into the algorithm design and to obtain provable performance guarantees."
- Why unresolved: The current results are presented only for tabular settings. The paper explicitly identifies this as a limitation for real-world applications and an interesting direction for future work.
- What evidence would resolve it: A new algorithm that uses function approximation (e.g., neural networks, linear features) with provable sample complexity bounds comparable to the tabular case.

## Limitations

- The coupled Lyapunov framework relies on carefully tracking multiple stochastic iterates evolving on different time scales, with uncertain practical applicability in highly non-stationary environments
- Optimal tuning of the exploration parameter ε remains an open question, requiring careful balancing of exploration and exploitation
- The analysis is specifically tailored to zero-sum games and would require significant modifications to extend to general-sum games

## Confidence

**Sample Complexity Bounds (High confidence)**: The O(ε⁻¹) and O(ε⁻⁸) bounds for matrix games, and Õ(ε⁻⁸) for stochastic games are rigorously derived from the coupled Lyapunov analysis and supported by the mathematical proofs.

**Rationality Property (High confidence)**: The claim that the dynamics are rational in the sense of Bowling and Veloso is well-established through the on-policy update structure and convergence to best responses when opponents are stationary.

**Coupled Lyapunov Framework (Medium confidence)**: While the theoretical framework is sound, its practical implementation and numerical stability require further validation through experiments.

## Next Checks

1. **Empirical Lyapunov Drift Verification**: Implement a simulation framework to track the evolution of Lyapunov functions L_v, L_sum, L_q, and L_π during learning. Measure whether the theoretical drift inequalities hold empirically across different game instances and parameter settings.

2. **Exploration Parameter Sensitivity Analysis**: Systematically vary the exploration parameter ε and temperature τ across multiple orders of magnitude. Quantify the impact on convergence speed, final Nash gap, and whether the polynomial sample complexity bounds are achieved in practice.

3. **Extension to Stochastic Games with Function Approximation**: Test the value iteration with smoothed best-response dynamics on stochastic games where the value function is represented using function approximation (e.g., neural networks). Evaluate whether the coupled Lyapunov approach remains effective when exact value function representation is not available.