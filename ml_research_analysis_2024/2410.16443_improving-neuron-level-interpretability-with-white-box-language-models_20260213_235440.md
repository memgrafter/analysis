---
ver: rpa2
title: Improving Neuron-level Interpretability with White-box Language Models
arxiv_id: '2410.16443'
source_url: https://arxiv.org/abs/2410.16443
tags:
- layer
- gpt-2
- interpretability
- crate
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a white-box transformer-like architecture named
  CRATE (Coding RAte TransformEr) that builds sparse coding directly into the model
  architecture to improve neuron-level interpretability in language models. Unlike
  post-hoc sparse auto-encoders, CRATE integrates a mathematically principled sparse
  coding framework that compresses and sparsifies token representations across layers,
  resulting in representations aligned on distinct semantic axes.
---

# Improving Neuron-level Interpretability with White-box Language Models

## Quick Facts
- arXiv ID: 2410.16443
- Source URL: https://arxiv.org/abs/2410.16443
- Authors: Hao Bai; Yi Ma
- Reference count: 40
- Key outcome: CRATE achieves up to 103% relative improvement in neuron-level interpretability compared to GPT-2 across multiple evaluation metrics

## Executive Summary
This paper addresses the challenge of improving neuron-level interpretability in language models by proposing a white-box transformer architecture called CRATE (Coding RAte TransformEr) that integrates sparse coding directly into the model design. Unlike post-hoc sparse auto-encoders that introduce reconstruction loss and reduce fidelity, CRATE builds on a mathematically principled framework that compresses and sparsifies token representations across layers. The approach replaces standard transformer components with a Compression block (MSSA) and an overcomplete ISTA block that learns sparse representations end-to-end during training. Extensive experiments demonstrate that CRATE achieves significantly better interpretability across multiple metrics while maintaining comparable performance to standard transformers, with interpretability gains that are steady across different layers and model sizes.

## Method Summary
CRATE modifies the standard transformer architecture by replacing the MLP block with an ISTA (Iterative Shrinkage-Thresholding Algorithm) block that learns sparse representations, and replacing standard multi-head attention with a Compression block (MSSA) that projects token embeddings into subspaces. The ISTA block uses an overcomplete dictionary (h = 4d) to create sparse activation patterns, while the MSSA block compresses token representations into subspaces. Together, these components create a white-box model where neuron activations can be interpreted more reliably. The model is pre-trained on The Pile dataset using Adam optimizer with context window of 1,024 tokens, and evaluated on neuron-level interpretability using OpenAI's Top-and-Random and Random-only metrics, as well as Anthropic's metric.

## Key Results
- CRATE achieves up to 103% relative improvement in neuron-level interpretability compared to GPT-2 across multiple evaluation metrics
- Interpretability improvements are steady across different layers and model sizes (1L to 12L configurations)
- CRATE shows enhanced ability to consistently activate on relevant tokens while avoiding activation on semantically irrelevant ones
- The model can be steered without loss of fidelity unlike SAE-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding sparse coding directly into the transformer architecture avoids reconstruction loss introduced by post-hoc SAEs
- Mechanism: The ISTA block in CRATE learns sparse representations without requiring a separate decoder to reconstruct activations, eliminating the reconstruction loss that degrades fidelity in SAE-based approaches
- Core assumption: The sparse coding can be learned end-to-end during language model training without sacrificing performance
- Evidence anchors:
  - [abstract] "Unlike post-hoc sparse auto-encoders, CRATE integrates a mathematically principled sparse coding framework that compresses and sparsifies token representations across layers"
  - [section 2] "As a post-hoc method, sparse auto-encoders introduce non-negative reconstruction loss, causing noise and reducing fidelity when interpreting neurons"
  - [section 4] "To avoid adding inconsistency into the language model, we develop on top of a mathematically principled white-box model framework"

### Mechanism 2
- Claim: The CRATE architecture achieves more steady interpretability across layers compared to GPT-2
- Mechanism: The built-in sparse coding framework introduces consistent and specific neuron-level behaviors through iterative compression (MSSA) and sparsification (ISTA) operations
- Core assumption: The iterative process of compression followed by sparsification creates more coherent semantic representations
- Evidence anchors:
  - [section 5.3] "Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size"
  - [section 4] "Repeated across layers, this culminates in distinct token representations aligned on unique semantic axes"
  - [corpus] "The interpretability of GPT2-SAE on the Anthropic metric decreases significantly when ℓ increases, while CRATE remains steady"

### Mechanism 3
- Claim: CRATE achieves higher mono-semanticity of neurons compared to standard transformers
- Mechanism: The ISTA block's overcomplete dictionary (h = 4d) and iterative shrinkage thresholding creates more specific activation patterns that fire on semantically relevant tokens only
- Core assumption: The increased dimensionality and sparsity constraints force neurons to specialize on specific semantic concepts
- Evidence anchors:
  - [section 4] "To investigate the neuron interpretability of the activation matrix A, we design an overcomplete version of the original ISTA block with Dℓ ∈ Rd×h where h = nd, and n = 4"
  - [section 5.3] "The larger interpretability gap of CRATE and GPT-2 on the OpenAI Random-only metric versus the Top-and-Random metric highlights the specificity of CRATE in avoiding firing on irrelevant tokens"
  - [section 1] "Recent research has also proposed that sparse auto-encoder-based dictionary learning effectively promotes mono-semanticity of neurons"

## Foundational Learning

- Concept: Sparse coding and dictionary learning
  - Why needed here: Understanding how sparse representations are learned and why they improve interpretability is crucial for grasping CRATE's design
  - Quick check question: What is the key difference between sparse coding learned during training versus applied post-hoc as in SAEs?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: CRATE modifies standard transformer components (attention and MLP blocks), so understanding these components is essential
  - Quick check question: How does the MSSA block in CRATE differ from standard multi-head self-attention?

- Concept: Neuron-level interpretability metrics
  - Why needed here: The paper evaluates interpretability using specific metrics (OpenAI and Anthropic), so understanding these metrics is important
  - Quick check question: What is the key difference between the OpenAI Random-only and Top-and-Random metrics?

## Architecture Onboarding

- Component map: Embedding → MSSA → ISTA → [MSSA → ISTA] × (L-1) → Head
- Critical path: Token → Embedding → [MSSA → ISTA] × L → Head
- Design tradeoffs:
  - Parameter efficiency: CRATE uses ~2/3 the parameters of GPT-2 due to fewer matrices in attention and MLP
  - Interpretability vs performance: Built-in sparsity may reduce next-token prediction accuracy
  - Sparsity level: Overcomplete ISTA (h = 4d) vs standard MLP affects both interpretability and computational cost

- Failure signatures:
  - Poor next-token prediction: Sparse coding may not capture all necessary information
  - Unstable training: The combined compression-sparsification process may be difficult to optimize
  - Non-sparse activations: ISTA may not effectively enforce sparsity

- First 3 experiments:
  1. Train CRATE-1L and GPT-2-1L on a small dataset, compare validation loss and activation sparsity
  2. Evaluate interpretability of both models using the OpenAI Random-only metric on the same dataset
  3. Visualize attention patterns and ISTA activations to verify compression and sparsification are occurring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the built-in sparse coding in CRATE affect long-term context modeling compared to GPT-2?
- Basis in paper: [explicit] The paper notes that CRATE maintains interpretability across layers but does not explicitly compare long-range dependencies or context retention.
- Why unresolved: While CRATE achieves high interpretability, the paper does not provide evidence on whether sparse coding impacts the model's ability to maintain coherence over longer sequences or its capacity for complex reasoning tasks requiring extended context.
- What evidence would resolve it: Comparative experiments on tasks requiring long-range reasoning (e.g., multi-hop question answering, story continuation) or analysis of attention patterns across extended contexts.

### Open Question 2
- Question: What is the optimal trade-off between sparsity level and model performance in CRATE?
- Basis in paper: [inferred] The paper shows that forcing sparsity may lead to higher computational cost and potentially lower performance on next-token prediction, but does not explore the full spectrum of sparsity levels or their impact on different tasks.
- Why unresolved: The paper uses fixed sparsity parameters but doesn't investigate how varying these affects performance across different model sizes or tasks, leaving open questions about the most effective balance.
- What evidence would resolve it: Systematic ablation studies varying sparsity regularization parameters across different model sizes and tasks, measuring both performance and interpretability metrics.

### Open Question 3
- Question: How does CRATE's interpretability scale to larger language models (e.g., GPT-4 scale)?
- Basis in paper: [explicit] The paper demonstrates CRATE's effectiveness up to 12-layer models but explicitly notes that post-hoc SAE methods face scalability challenges in deeper models.
- Why unresolved: While CRATE shows improved interpretability in smaller models, the paper doesn't address whether the architectural advantages persist as model size increases to hundreds of billions of parameters.
- What evidence would resolve it: Scaling experiments with CRATE architecture applied to larger models, measuring interpretability metrics and comparing them to both standard transformers and post-hoc SAE approaches at similar scales.

## Limitations

- Training Stability and Scalability: The paper reports successful training of CRATE models up to 12 layers, but does not provide detailed analysis of training stability across different configurations. The combined compression-sparsification process (MSSA + ISTA) represents a complex optimization landscape that may be difficult to train effectively, particularly for larger models.
- Generalization Beyond The Pile: All experiments are conducted on The Pile dataset, which is specifically designed for large language model pre-training. The enhanced interpretability claims are not validated on other domains or tasks, leaving uncertainty about whether the benefits transfer to specialized corpora or downstream applications.
- Metric Sensitivity and Adversarial Testing: While the paper demonstrates improvements across three interpretability metrics, it does not explore the sensitivity of these metrics to different evaluation conditions. The paper lacks analysis of how robust the interpretability gains are to variations in the evaluation methodology.

## Confidence

**High Confidence Claims** (supported by multiple experimental results):
- CRATE achieves better neuron-level interpretability than GPT-2 across multiple metrics
- The interpretability improvements are consistent across different layers and model sizes
- CRATE maintains enhanced interpretability without sacrificing next-token prediction performance
- The white-box design allows for steering without fidelity loss

**Medium Confidence Claims** (supported by evidence but with some limitations):
- The built-in sparse coding framework is the primary driver of interpretability improvements
- The overcomplete ISTA block (h = 4d) is essential for achieving mono-semanticity
- CRATE's interpretability is more steady across layers compared to SAE-enhanced models

**Low Confidence Claims** (limited experimental support or speculative):
- The architectural modifications will scale effectively to much larger models (beyond 12 layers)
- The interpretability gains will transfer to all types of language modeling tasks
- The sparse coding framework provides the same benefits for other types of neural networks

## Next Checks

1. **Cross-Domain Interpretability Validation**: Evaluate CRATE's interpretability on multiple diverse datasets (e.g., biomedical text, legal documents, code) to verify that the enhanced neuron-level interpretability generalizes beyond The Pile corpus. This would involve fine-tuning pre-trained CRATE models on these domains and re-evaluating the interpretability metrics to ensure the architectural benefits persist across different semantic contexts.

2. **Ablation Study of Architectural Components**: Systematically disable or modify individual components of CRATE (e.g., reduce overcomplete dimensionality from h = 4d to h = 2d, remove the ISTA block, or modify the MSSA compression) to quantify the specific contribution of each architectural choice to the interpretability improvements. This would help determine whether the benefits come primarily from the sparse coding framework or from other aspects of the white-box design.

3. **Adversarial Interpretability Testing**: Design test cases specifically engineered to challenge the interpretability of both CRATE and standard transformer models, such as ambiguous tokens with multiple meanings, polysemous words in different contexts, or carefully crafted sequences that might trigger unexpected activation patterns. This would help validate that CRATE's improved interpretability is not simply an artifact of the evaluation metrics but represents genuine advances in understanding model behavior.