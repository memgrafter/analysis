---
ver: rpa2
title: 'Ask, and it shall be given: On the Turing completeness of prompting'
arxiv_id: '2411.01992'
source_url: https://arxiv.org/abs/2411.01992
tags:
- norm
- transformer
- compute
- steps
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that a single finite-size Transformer can simulate
  any computable function through prompting, establishing the Turing-completeness
  of the LLM prompting paradigm. The key innovation is introducing a new model called
  "two-tape Post-Turing machines" (2-PTMs) that can be efficiently encoded into prompts
  using a finite alphabet.
---

# Ask, and it shall be given: On the Turing completeness of prompting

## Quick Facts
- arXiv ID: 2411.01992
- Source URL: https://arxiv.org/abs/2411.01992
- Reference count: 39
- One-line primary result: This paper proves that a single finite-size Transformer can simulate any computable function through prompting, establishing the Turing-completeness of the LLM prompting paradigm.

## Executive Summary
This paper establishes that the LLM prompting paradigm is Turing-complete by constructing a finite-size Transformer that can simulate any computable function through chain-of-thought reasoning. The key innovation is introducing two-tape Post-Turing machines (2-PTMs) that can be efficiently encoded into prompts using a finite alphabet of 23 tokens. The authors prove that this approach achieves nearly optimal complexity bounds, with O(t(n) log t(n)) chain-of-thought steps and O(log(n+t(n))) bits of precision for TIME(t(n)) functions, matching the performance of unbounded-size Transformers.

## Method Summary
The paper constructs a finite alphabet Σ of 23 tokens and encodes two-tape Post-Turing machines (2-PTMs) into prompts using Shannon's encoding. A decoder-only Transformer with ReLU activation, layer normalization, and causal attention is built to execute these encoded 2-PTM instructions through chain-of-thought steps. The Transformer uses hardmax attention for state retrieval and equality checks, recording execution states via special tokens. The construction is verified by testing on multiple computable functions including the DYK language and parity function, demonstrating that the system achieves Turing-completeness with the claimed complexity bounds.

## Key Results
- A single finite-size Transformer can simulate any computable function through prompting
- The constructed Transformer achieves O(t(n) log t(n)) chain-of-thought complexity for TIME(t(n)) functions
- Chain-of-thought is necessary for Turing completeness in Transformers (without CoT, even parity cannot be computed)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single finite-size Transformer can simulate any computable function through prompting by encoding 2-PTM instructions into the prompt.
- Mechanism: The prompt encodes a two-tape Post-Turing machine (2-PTM) description using a finite alphabet. The Transformer executes this 2-PTM through chain-of-thought steps, recording execution states and simulating tape operations.
- Core assumption: 2-PTMs can be efficiently encoded using a finite alphabet and simulated by Transformers through chain-of-thought steps.
- Evidence anchors:
  - [abstract]: "there exists a finite-size Transformer such that for any computable function, there exists a corresponding prompt following which the Transformer computes the function"
  - [section]: "we propose a new imperative model of computation that extends Wang machines and DSW-PTMs" and "we let our model of computation use two bi-infinite tapes A and B"
- Break condition: If the encoding requires infinite alphabet or if Transformers cannot simulate the required tape operations through chain-of-thought.

### Mechanism 2
- Claim: The constructed Transformer achieves nearly optimal complexity bounds matching unbounded-size Transformers.
- Mechanism: The Transformer computes TIME(t(n)) functions within O(t(n) log t(n)) chain-of-thought steps and O(log(n+t(n))) bits of precision, matching the complexity class of all Transformers.
- Core assumption: The logarithmic slowdown from using a single Transformer versus multiple Transformers is inherent and cannot be improved.
- Evidence anchors:
  - [abstract]: "even though we use only a single finite-size Transformer, it can still achieve nearly the same complexity bounds as that of the class of all unbounded-size Transformers"
  - [section]: "TIME(t(n)) ⊆ TIME2-PTM(t(n) log t(n))" and "our result shows that even a single Transformer can still achieve the same precision complexity as the class of all Transformers"
- Break condition: If the complexity hierarchy collapses or if there exists a more efficient encoding that eliminates the logarithmic factor.

### Mechanism 3
- Claim: Chain-of-thought steps are necessary for Turing completeness in Transformers.
- Mechanism: The Transformer uses chain-of-thought to record execution steps of the 2-PTM, allowing it to restore and manipulate the state at each step. Without chain-of-thought, Transformers cannot compute even the parity function.
- Core assumption: Finite-size Transformers without chain-of-thought are not Turing-complete, as established by prior work.
- Evidence anchors:
  - [abstract]: "CoT is necessary for Turing completeness because it is known that Transformers without CoTs cannot even compute the parity function"
  - [section]: "we leverage CoT steps to record execution steps of the 2-PTM so that the Transformer can restore the state of 2-PTM at any execution step"
- Break condition: If new architectural modifications enable Turing completeness without chain-of-thought, or if the parity function limitation is overcome.

## Foundational Learning

- Concept: Turing completeness and computational complexity classes (P, TIME(t(n)))
  - Why needed here: Understanding what it means for a system to be Turing-complete and how different computational models relate in terms of efficiency
  - Quick check question: What is the difference between TIME(t(n)) and CoTΓ(t(n)), and why does the paper establish both?

- Concept: Chain-of-thought reasoning in Transformers
  - Why needed here: The entire proof relies on Transformers using CoT to simulate computation step-by-step
  - Quick check question: How does the paper use CoT to record and restore execution states of the 2-PTM?

- Concept: Encoding computation models using finite alphabets
  - Why needed here: The prompt must encode arbitrary computable functions using only a finite set of tokens
  - Quick check question: How does the paper encode arbitrary 2-PTM instructions into a finite alphabet without losing expressiveness?

## Architecture Onboarding

- Component map: Token embedding → Chain-of-thought state tracking → 2-PTM instruction execution → Output extraction
- Critical path: Token embedding → Chain-of-thought state tracking → 2-PTM instruction execution → Output extraction
- Design tradeoffs: Using hardmax attention simplifies construction but differs from practical softmax implementations; chain-of-thought enables completeness but adds overhead
- Failure signatures: Incorrect state tracking (wrong instruction pointer), improper tape cell retrieval, failure to distinguish prompt from CoT
- First 3 experiments:
  1. Implement the embedding layer with positional identifiers and test basic token recognition
  2. Build the chain-of-thought tracking mechanism and verify it can record and restore simple execution states
  3. Implement 2-PTM instruction execution for basic operations (move, write, conditional jumps) and test with simple programs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CoT complexity of prompting be improved below O(t(n) log t(n)) for general Turing machines?
- Basis in paper: [explicit] The paper states "Assuming TIME2(t(n))≠TIME(t(n)), it is unlikely that the CoT complexity O(t(n) log t(n)) of Γ for TIME(t(n)) could be further improved to O(t(n))"
- Why unresolved: The paper only provides an upper bound and conjectures that improvement is unlikely based on complexity class assumptions, but no formal lower bound proof exists.
- What evidence would resolve it: A formal proof showing either (1) a tighter lower bound on CoT complexity matching the upper bound, or (2) an algorithm that achieves better than O(t(n) log t(n)) CoT complexity for general Turing machines.

### Open Question 2
- Question: Can the theoretical framework be extended to models using softmax attention instead of hardmax?
- Basis in paper: [explicit] "Using hardmax instead of softmax is a limitation of this area. We hope to address this limitation in future work."
- Why unresolved: The current construction relies on hardmax attention for key operations like equality checks and farthest retrieval, which cannot be directly translated to continuous softmax operations.
- What evidence would resolve it: A proof that a finite Transformer with softmax attention and reasonable precision can still simulate any computable function through prompting, or a demonstration that some fundamental limitation prevents this.

### Open Question 3
- Question: How learnable are the finite Transformers that can execute prompts Turing-completely?
- Basis in paper: [explicit] "While we have shown the existence of a Transformer on which prompting is Turing-complete, it does not necessarily imply that a Transformer effectively learns to simulate any 2-PTMs through CoT steps."
- Why unresolved: The paper establishes existence but not learnability - it shows such a Transformer exists but doesn't address whether standard training methods can discover these capabilities.
- What evidence would resolve it: Experimental results showing whether standard training on appropriate tasks can lead Transformers to develop the capabilities needed to execute arbitrary prompts, or theoretical results characterizing what learning algorithms can achieve this.

## Limitations

- The paper uses hardmax attention rather than practical softmax implementations, creating a gap between theory and practice
- The complexity bounds assume idealized conditions that may not hold in real-world implementations with tokenization noise and numerical precision limitations
- The construction demonstrates existence but not learnability - it's unclear whether standard training methods can discover these Turing-complete capabilities

## Confidence

- **High Confidence**: The theoretical proof of Turing-completeness using 2-PTMs and chain-of-thought mechanisms is well-established within the mathematical framework presented.
- **Medium Confidence**: The complexity bounds (O(t(n) log t(n)) chain-of-thought steps) are theoretically sound but may not translate directly to practical implementations.
- **Low Confidence**: The practical applicability of these theoretical results to real-world prompt engineering and the actual performance characteristics in production systems remain highly uncertain.

## Next Checks

1. **Softmax Compatibility Test**: Implement the theoretical construction using softmax attention instead of hardmax and measure the degradation in state retrieval accuracy and computational completeness.
2. **Empirical Complexity Validation**: Benchmark the actual chain-of-thought step counts and precision requirements for computing various computable functions to verify whether the theoretical logarithmic bounds hold in practice.
3. **Robustness to Tokenization Noise**: Test the system's ability to maintain Turing-completeness when subject to tokenization errors, numerical precision limitations, and stochastic generation artifacts.