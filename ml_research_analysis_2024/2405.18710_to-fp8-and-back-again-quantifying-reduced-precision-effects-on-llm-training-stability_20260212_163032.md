---
ver: rpa2
title: 'To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM Training
  Stability'
arxiv_id: '2405.18710'
source_url: https://arxiv.org/abs/2405.18710
tags:
- training
- loss
- precision
- bf16
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the stability of large language model training
  when using reduced-precision floating-point formats, particularly FP8, which is
  gaining adoption in modern accelerators. While BF16 has become standard for LLM
  training, FP8 promises even greater efficiency but raises concerns about training
  instability due to fewer bits.
---

# To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM Training Stability

## Quick Facts
- arXiv ID: 2405.18710
- Source URL: https://arxiv.org/abs/2405.18710
- Reference count: 32
- This work shows that reduced-precision training increases instability and narrows the hyperparameter space where stable training is possible, with current FP8 methods too unstable for cost-effective LLM training.

## Executive Summary
This paper investigates the stability of large language model training when using reduced-precision floating-point formats, particularly FP8, which is gaining adoption in modern accelerators. While BF16 has become standard for LLM training, FP8 promises even greater efficiency but raises concerns about training instability due to fewer bits. The authors analyze whether FP8 and similar reduced-precision schemes can be cost-effective by examining robustness across random seeds, learning rates, and datasets.

The core contribution is a new loss landscape sharpness metric tailored for autoregressive models, which quantifies training instability by measuring sensitivity in the logit space of the final token. Using this metric, the authors simulate incremental bit reductions by clipping mantissa bits in matrix multiplication to evaluate how precision impacts training stability. Experiments on models up to 7B parameters show that reduced-precision training increases instability and narrows the hyperparameter space where stable training is possible.

## Method Summary
The authors developed a loss landscape sharpness metric that measures training instability by quantifying sensitivity in the logit space of the last token in autoregressive models. They simulate reduced precision by clipping mantissa and exponent bits in matrix multiplications, then use L-BFGS-B optimization to find the maximum loss spike relative to the baseline. The method involves training models with different precision configurations, computing sharpness values throughout training, and comparing stability across precision levels. Experiments were conducted on Llama 120M and 7B models, nanoGPT GPT-2 124M, and various datasets including FineWeb Edu and OpenWebText, with training runs of 5K-30K steps on 8-GPU A100/H100 nodes.

## Key Results
- Reduced-precision training increases instability and narrows the hyperparameter space where stable training is possible
- Loss landscape sharpness reliably predicts instability before divergence occurs
- Current FP8 training methods like MS-AMP are too unstable for cost-effective LLM training
- Exponent bit count is more critical than mantissa bits for LLM training stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss landscape sharpness predicts training instability before divergence appears in the loss curve.
- Mechanism: The proposed sharpness metric measures sensitivity in the logit space of the last token, quantifying how small perturbations can cause large loss changes. Higher sharpness indicates the model is in a region prone to instability.
- Core assumption: The last token's logits provide sufficient information to capture training instability in autoregressive models.
- Evidence anchors:
  - [abstract]: "loss landscape sharpness reliably predicts instability before divergence occurs"
  - [section]: "we found that even when the model is clearly in the process of loss divergence, the generated visualizations remain smooth, motivating our introduction of a new loss landscape sharpness metric"
  - [corpus]: Weak - corpus papers focus on FP8 training techniques but don't discuss sharpness metrics specifically
- Break condition: If the loss landscape smoothness metric fails to correlate with actual training divergence in other model architectures or training regimes.

### Mechanism 2
- Claim: Reduced precision training narrows the hyperparameter space where stable training is possible.
- Mechanism: Lower precision reduces representational power, causing greater sensitivity to learning rate and data quality variations. This manifests as more frequent loss spikes and earlier divergence.
- Core assumption: Training stability scales monotonically with floating-point precision when other factors are controlled.
- Evidence anchors:
  - [abstract]: "reduced-precision training increases instability and narrows the hyperparameter space where stable training is possible"
  - [section]: "we find that currently available methods for FP8 training are not robust enough to allow their cost-effective use"
  - [corpus]: Weak - corpus focuses on FP8 implementations but doesn't explicitly discuss hyperparameter space narrowing
- Break condition: If certain precision reductions actually improve stability in specific architectural configurations or with specialized training techniques.

### Mechanism 3
- Claim: Exponent bit count is more critical than mantissa bits for LLM training stability.
- Mechanism: Models with insufficient exponent range cannot represent large values during training, causing immediate failure, while reduced mantissa precision primarily causes gradual instability.
- Core assumption: The dynamic range requirements of LLM training are more stringent than precision requirements for intermediate computations.
- Evidence anchors:
  - [section]: "we find that removing even a single exponent bit prevents training altogether, resulting in the model failing to progress... models with only their outer exponent ranges clamped do not [train]"
  - [section]: "models with only their inner exponent ranges clamped train normally while models with only their outer exponent ranges clamped do not"
  - [corpus]: Weak - corpus papers don't discuss exponent vs mantissa trade-offs in detail
- Break condition: If mantissa precision proves equally or more critical in different model architectures or training configurations.

## Foundational Learning

- Concept: Floating-point representation trade-offs
  - Why needed here: Understanding why exponent bits matter more than mantissa bits requires knowledge of floating-point format structure
  - Quick check question: What happens when a floating-point number exceeds the maximum representable value with given exponent bits?

- Concept: Loss landscape analysis
  - Why needed here: The paper's core contribution relies on understanding how loss surface geometry relates to training stability
  - Quick check question: How does sharpness of a loss landscape affect gradient-based optimization convergence?

- Concept: Autoregressive model behavior
  - Why needed here: The sharpness metric is specifically designed for autoregressive models, requiring understanding of their unique properties
  - Quick check question: Why is the last token's logit space particularly informative for measuring training instability in transformer models?

## Architecture Onboarding

- Component map: Data -> Precision masking -> Forward pass -> Loss calculation -> Sharpness metric computation -> Training continuation/decision
- Critical path: Data → Precision masking → Forward pass → Loss calculation → Sharpness metric computation → Training continuation/decision
- Design tradeoffs: Precision vs. stability (lower precision increases throughput but reduces stability), computation vs. memory (masking is faster than actual lower precision), metric complexity vs. prediction accuracy (sharper metrics may be more accurate but computationally expensive)
- Failure signatures: Loss divergence, increasing loss landscape sharpness without corresponding loss increase, higher frequency of loss spikes, failure to converge to same loss as higher precision baseline
- First 3 experiments:
  1. Implement the exponent clamping ablation to verify that outer exponent range is critical for training progress
  2. Run the sharpness metric on a stable BF16 training run to establish baseline values
  3. Compare training stability across different learning rates using the sharpness metric to identify threshold behaviors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the loss landscape sharpness metric behave differently for training instabilities that manifest during the initial stages versus those that appear later in training?
- Basis in paper: [explicit] The authors acknowledge that many instabilities only become apparent later in training, but their experiments focus on initial stages
- Why unresolved: The paper primarily studies early-stage training instability, yet the authors recognize that later-stage instabilities (e.g., after 200B tokens) may behave differently. The relationship between sharpness and late-stage divergence remains unexplored.
- What evidence would resolve it: Systematic measurement of loss landscape sharpness throughout the entire training trajectory, particularly focusing on when and how sharpness values correlate with later-stage divergence events

### Open Question 2
- Question: Would increasing exponent bits beyond standard BF16 (e.g., E11M4 format) provide a more stable training regime, and how would this compare to the instability observed when reducing mantissa bits?
- Basis in paper: [explicit] The authors were unable to experiment with increasing exponent bits due to hardware limitations, noting that FP64 is over an order of magnitude slower than BF16 on A100/H100 GPUs
- Why unresolved: The paper establishes that exponent bits are more critical than mantissa bits for training stability, but couldn't test whether additional exponent bits would improve stability due to computational constraints
- What evidence would resolve it: Direct comparison of training stability and convergence across different exponent bit configurations (E7, E8, E9, E11) while holding mantissa bits constant

### Open Question 3
- Question: How do the training instability effects scale with model size beyond 7B parameters, and at what model scale do reduced-precision formats become cost-effective despite their instability?
- Basis in paper: [inferred] Experiments were limited to models up to 7B parameters, but the paper discusses cost-effectiveness for large language models generally without specifying model size thresholds
- Why unresolved: The paper's findings on 7B models may not extrapolate to larger models, and there's no analysis of how the instability-to-cost ratio changes with model scale
- What evidence would resolve it: Comparative analysis of training stability metrics (including loss landscape sharpness) across a range of model sizes (e.g., 1B, 7B, 30B, 70B) using the same reduced-precision schemes

## Limitations
- Findings are based on specific model sizes (up to 7B parameters) and may not generalize to larger models
- Experiments primarily use transformer-based autoregressive architectures, limiting generalizability
- Simulated reduced precision through mantissa bit-clipping may not capture all real-world FP8 training effects
- Unable to test exponent bit increases beyond BF16 due to computational constraints

## Confidence
**High Confidence**: The core finding that exponent bit reduction critically impacts training stability is well-supported by the ablation experiments showing immediate training failure when outer exponent ranges are clamped.

**Medium Confidence**: The loss landscape sharpness metric's ability to predict instability before divergence is demonstrated convincingly, but its generalizability across different model families and training scenarios needs further validation.

**Medium Confidence**: The conclusion that current FP8 methods are too unstable for cost-effective LLM training is supported by the experimental evidence, though alternative FP8 techniques not tested (like stochastic rounding or other quantization schemes) might yield different results.

## Next Checks
1. **Architecture Generalization Test**: Apply the loss landscape sharpness metric and precision reduction experiments to non-transformer architectures (CNNs, MLPs, or different attention mechanisms) to verify the findings extend beyond autoregressive models.

2. **Alternative FP8 Method Comparison**: Repeat the training stability experiments using different FP8 training techniques (stochastic rounding, block floating point, or other quantization schemes) to determine if the instability findings are specific to MS-AMP O1 or representative of FP8 training generally.

3. **Scale-Up Validation**: Test the sharpness metric's predictive power and the precision stability findings on models significantly larger than 7B parameters (20B+ range) to assess whether the trends observed hold at production scales.