---
ver: rpa2
title: 'BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language
  generation from feedback'
arxiv_id: '2402.02479'
source_url: https://arxiv.org/abs/2402.02479
tags:
- brai
- reward
- policy
- gradient
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BRAIn (Bayesian Reward-conditioned Amortized
  Inference), a new method for aligning language models to human preferences that
  bridges the gap between distribution matching approaches and Direct Preference Optimization
  (DPO). The key innovation is using Bayesian inference to derive a reward-conditioned
  posterior distribution and then distilling it into an amortized policy using self-normalized
  importance sampling with a novel variance-reducing baseline.
---

# BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

## Quick Facts
- arXiv ID: 2402.02479
- Source URL: https://arxiv.org/abs/2402.02479
- Authors: Gaurav Pandey; Yatin Nandwani; Tahira Naseem; Mayank Mishra; Guangxuan Xu; Dinesh Raghu; Sachindra Joshi; Asim Munawar; Ramón Fernandez Astudillo
- Reference count: 40
- Key outcome: BRAIn achieves 95.40% win rate against gold responses on Anthropic HH, outperforming DPO (87.37%) and RSO (84.59%)

## Executive Summary
BRAIn (Bayesian Reward-conditioned Amortized Inference) introduces a principled approach to aligning language models with human preferences by deriving a reward-conditioned posterior distribution and distilling it into an amortized policy. The method bridges the gap between distribution matching approaches and Direct Preference Optimization (DPO) by using Bayesian inference to incorporate fine-grained reward scores rather than just preference rankings. Experiments on TL;DR summarization and Anthropic HH tasks show BRAIn significantly outperforms state-of-the-art methods, achieving win rates of 95.40% against gold responses compared to 87.37% for DPO.

## Method Summary
BRAIn uses Bayesian inference to derive a reward-conditioned posterior distribution over "good" outputs, then trains an amortized policy to approximate this posterior using self-normalized importance sampling with a variance-reducing baseline. The method generates multiple samples per input, computes importance weights using the reward-conditioned posterior, and subtracts a baseline estimated via self-normalized importance sampling. This creates an unbiased gradient estimate that minimizes a self-normalized KL divergence between the posterior and policy distributions. The proposal distribution is periodically updated to the current policy to maintain good coverage of the output space.

## Key Results
- On Anthropic HH task: 95.40% win rate against gold responses (vs 87.37% for DPO, 84.59% for RSO)
- On TL;DR summarization: 86.43% win rate against gold responses (vs 79.05% for DPO)
- Self-normalized baseline is critical: removing it reduces win rates by over 30 percentage points
- DPO can be derived as a special case when using 2 samples and argmax selection

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Reward-Conditioned Posterior as Target Distribution
BRAIn derives a reward-conditioned posterior distribution that captures the desired output distribution by combining the prior SFT model with a goodness model derived from the reward function. This posterior represents the distribution over "good" outputs, which is then approximated by the amortized policy.

### Mechanism 2: Self-Normalized Importance Sampling with Variance-Reducing Baseline
The method uses self-normalized importance sampling to approximate expectations over the intractable posterior, then subtracts a baseline (also estimated via self-normalized importance sampling) to reduce variance while maintaining unbiasedness.

### Mechanism 3: Connection to DPO as Special Case
Under specific restrictions (n=2 samples, argmax selection, proposal equals prior), the BRAIn objective reduces to the DPO objective, providing a theoretical connection between these seemingly different approaches.

## Foundational Learning

- Concept: Importance Sampling and Self-Normalization
  - Why needed here: The method relies on importance sampling to approximate expectations over the intractable posterior distribution, and self-normalization is used to reduce variance.
  - Quick check question: What is the difference between standard importance sampling and self-normalized importance sampling, and why is self-normalization used here?

- Concept: KL Divergence and Its Properties
  - Why needed here: The method minimizes a KL divergence between the posterior and policy distributions, and understanding its properties is crucial for understanding why the method works.
  - Quick check question: What are the key properties of KL divergence that make it suitable for measuring the difference between the posterior and policy distributions?

- Concept: Bradley-Terry Preference Model
  - Why needed here: The method assumes a Bradley-Terry model for the reward function, which connects the reward scores to probabilities of preference between outputs.
  - Quick check question: How does the Bradley-Terry model parameterize the relationship between reward scores and the probability that one output is preferred over another?

## Architecture Onboarding

- Component map: Reward Model -> Proposal Distribution -> Importance Weights -> Policy Distribution
- Critical path: 1. Generate n samples from proposal distribution 2. Compute importance weights using reward scores 3. Compute baseline weights using current policy 4. Calculate gradient using (αyi - βyi)∇log qθ(yi|x) 5. Update policy parameters 6. Periodically update proposal distribution
- Design tradeoffs: Number of samples n affects posterior approximation quality vs computation cost; proposal update frequency affects adaptation speed vs stability
- Failure signatures: High gradient variance despite baseline, slow convergence, policy collapsing to trivial solutions
- First 3 experiments: 1. Remove self-normalized baseline to verify performance degradation 2. Vary number of samples n to test effect on performance and variance 3. Compare win rates using training reward model vs independent LLM judge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BRAIn's performance scale with the size of the reward model?
- Basis in paper: The paper mentions BRAIn explicitly uses the reward model during training, which could become computationally expensive for larger models.
- Why unresolved: The paper does not explore how BRAIn's performance varies with different sizes of reward models or how computational costs scale with model size.
- What evidence would resolve it: Experiments comparing BRAIn's performance and computational efficiency across different reward model sizes.

### Open Question 2
- Question: What is the impact of different baseline subtraction methods on BRAIn's performance?
- Basis in paper: The paper proposes a novel self-normalized baseline but does not compare it to other potential baseline methods.
- Why unresolved: While the paper demonstrates the importance of baseline subtraction, it does not explore alternative baseline methods or provide a comprehensive comparison.
- What evidence would resolve it: Experiments comparing BRAIn's performance using different baseline subtraction methods.

### Open Question 3
- Question: How does BRAIn's performance vary across different types of language generation tasks?
- Basis in paper: The paper evaluates BRAIn on summarization and dialogue tasks, but does not explore its performance on other types of language generation tasks.
- Why unresolved: The paper focuses on two specific tasks, leaving open questions about BRAIn's generalization to other language generation domains.
- What evidence would resolve it: Experiments applying BRAIn to diverse language generation tasks including machine translation and code generation.

## Limitations
- Computational overhead from generating multiple samples per prompt increases training time significantly
- Theoretical DPO connection relies on specific assumptions (n=2, argmax, proposal=prior) that may not hold in practice
- Experiments only evaluate on two tasks with specific reward models, limiting generalizability

## Confidence
- Theoretical framework and unbiasedness proof: High
- Performance improvements over baselines: High
- Connection to DPO as special case: Medium (theoretical, limited empirical verification)
- Self-normalized baseline as critical component: High

## Next Checks
1. Test BRAIn's performance when the reward model is imperfect or noisy to understand robustness limits
2. Evaluate computational overhead vs sample efficiency trade-off by varying the number of samples n
3. Verify the DPO connection empirically by implementing the exact conditions (n=2, argmax, proposal=prior) and comparing objectives directly