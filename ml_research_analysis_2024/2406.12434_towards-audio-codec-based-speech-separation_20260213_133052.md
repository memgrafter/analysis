---
ver: rpa2
title: Towards Audio Codec-based Speech Separation
arxiv_id: '2406.12434'
source_url: https://arxiv.org/abs/2406.12434
tags:
- speech
- audio
- separation
- performance
- codecformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for efficient speech separation
  by performing the task within the embedding space of a neural audio codec (NAC).
  The authors introduce Codecformer, a model based on the Sepformer architecture but
  modified to work with the compressed representations from the Descript Audio Codec
  (DAC).
---

# Towards Audio Codec-based Speech Separation

## Quick Facts
- arXiv ID: 2406.12434
- Source URL: https://arxiv.org/abs/2406.12434
- Reference count: 0
- Primary result: 52x reduction in MAC operations while maintaining competitive speech separation performance

## Executive Summary
This paper introduces Codecformer, a novel approach for efficient speech separation that operates within the compressed embedding space of a neural audio codec (NAC). By leveraging the Descript Audio Codec (DAC) and its high temporal compression ratio, Codecformer achieves significant computational savings (52x reduction in MAC operations) compared to traditional speech separation models like Sepformer, while maintaining competitive separation performance. The model is trained using a modified loss function called Codec SI-SDR that accounts for codec distortions, and experimental results on the WSJ0-2mix dataset demonstrate improvements in both objective metrics (SI-SDR, SDR) and perceptual quality (PESQ) compared to transmitted audio.

## Method Summary
Codecformer adapts the Sepformer architecture to work with compressed embeddings from the Descript Audio Codec (DAC). The model processes speech mixtures in the 1024-dimensional embedding space of DAC, which provides a 52x reduction in sequence length (from 8kHz to 50Hz). This temporal compression allows the use of a simplified transformer architecture without the chunking and dual-path components typically required for long sequences. The model employs 16 transformer blocks with Snake activation and is trained using a modified Codec SI-SDR loss function that compares estimated speech against locally encoded/decoded ground truth rather than clean references. The architecture is trained on WSJ0-2mix for 200 epochs with an initial learning rate of 1.5e-4.

## Key Results
- Achieves 52x reduction in MAC operations at inference compared to Sepformer
- Maintains competitive separation performance with 14.8 dB SI-SDRi on WSJ0-2mix
- Trains 2.7x faster than baseline Sepformer
- Outperforms Sepformer in perceptual quality (PESQ) when comparing transmitted audio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 52x reduction in MAC operations is primarily due to temporal compression from the neural audio codec (NAC) reducing the sequence length from 8kHz to 50Hz, allowing full transformer layers to process the entire sequence without chunking.
- Core assumption: The compressed embedding space preserves sufficient information for speech separation while achieving significant computational savings.
- Evidence anchors: [abstract]: "Codecformer achieves a 52x reduction in MAC (Multiply-Accumulate) operations at inference compared to the baseline Sepformer"

### Mechanism 2
- Claim: The Snake activation function is critical for performance because it matches the periodic nature of the audio codec's internal representations and better models periodic functions.
- Core assumption: The codec's internal representations have periodic characteristics that require matching activation functions for optimal separation performance.
- Evidence anchors: [section]: "The model utilizes the Snake activation function [19] instead of the tanh and ReLU functions used in speech separation models... we tried a number of activation functions, including tanh and ReLU but the performance was very poor"

### Mechanism 3
- Claim: The Codec SI-SDR loss function provides a more realistic training target by accounting for codec distortions that would occur in deployment.
- Core assumption: Training with codec-distorted targets better prepares the model for real-world deployment scenarios where both input and output pass through the codec.
- Evidence anchors: [section]: "We propose a modified version called 'Codec SI-SDR' (cSI-SDR) that compares the estimated signal ˆs against the transmitted signal t obtained by encoding a clean signal into codes locally and decoding the codes on the server"

## Foundational Learning

- Concept: Scale-Invariant Signal-to-Distortion Ratio (SI-SDR)
  - Why needed here: SI-SDR is the standard metric for evaluating speech separation performance and understanding the proposed Codec SI-SDR modification requires understanding the base metric
  - Quick check question: How does SI-SDR differ from standard SDR, and why is the scale-invariant property important for speech separation?

- Concept: Residual Vector Quantization (RVQ)
  - Why needed here: RVQ is a key component of neural audio codecs that compresses embeddings by learning a quantization codebook, and the ablation study shows it significantly impacts performance
  - Quick check question: What is the purpose of RVQ in neural audio codecs, and how does it introduce transmission loss that affects speech separation?

- Concept: Transformer architectures and self-attention mechanisms
  - Why needed here: Codecformer is based on Sepformer's transformer architecture, and understanding how self-attention works is crucial for grasping why removing chunking is possible in the compressed space
  - Quick check question: Why do transformers typically require chunking for long sequences, and how does temporal compression enable full-sequence processing?

## Architecture Onboarding

- Component map: Input → DAC Encoder → Codecformer (16 transformer blocks with Snake activation) → Output → DAC Decoder
- Critical path: The compressed embeddings flow from DAC Encoder through Codecformer's transformer layers to produce separated embeddings, which are then decoded by DAC Decoder
- Design tradeoffs: Computational efficiency (52x MAC reduction) vs. potential information loss from compression; simpler architecture vs. need for specific activation function matching
- Failure signatures: Poor separation performance despite computational efficiency; mismatch between training metrics and deployment quality; activation function incompatibility issues
- First 3 experiments:
  1. Compare SI-SDR vs Codec SI-SDR training performance on a small subset to validate the loss function modification
  2. Test different activation functions (tanh, ReLU, Snake) to confirm the activation matching hypothesis
  3. Measure MAC operations and training time with varying embedding dimensions to understand the computational scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would pre-training a neural audio codec (NAC) specifically on speech mixtures impact its ability to represent overlapping speakers and improve speech separation performance?
- Basis in paper: [explicit] The authors mention this as a potential future direction, noting that the Descript Audio Codec (DAC) was not explicitly trained with speech mixtures, which could limit its internal representation of fully-overlapping speech.

### Open Question 2
- Question: What is the impact of Residual Vector Quantization (RVQ) on speech separation performance, and how can RVQ methods be improved to reduce transmission loss?
- Basis in paper: [explicit] The authors observe a 2-3dB reduction in performance across experiments when using RVQ, indicating important limitations in the use of RVQ for DAC that result in transmission loss.

### Open Question 3
- Question: How does the Codec SI-SDR loss function compare to other loss functions for training speech separation models in the compressed embedding space of a NAC?
- Basis in paper: [explicit] The authors propose the Codec SI-SDR loss function and show it outperforms training on ground truth targets in some cases, but also note that training on ground truth provides a cleaner loss gradient.

## Limitations
- Performance improvements depend on specific temporal compression ratio that may not generalize to other codecs
- Experimental validation limited to clean, two-speaker mixtures at 8kHz sampling rate
- cSI-SDR loss function is tailored to specific DAC codec and may not transfer to other NAC architectures

## Confidence

**High Confidence:** The computational efficiency claims (52x MAC reduction, 2.7x faster training) are directly supported by the temporal compression mechanism and architectural simplifications described in the paper.

**Medium Confidence:** The performance metrics (SI-SDRi, SDRi, PESQ) show improvements over Sepformer, but the extent of these improvements may depend on the specific implementation details and training setup.

**Low Confidence:** The mechanism by which Snake activation specifically improves performance remains somewhat speculative, as the paper notes poor performance with other activations but doesn't provide detailed ablation studies.

## Next Checks
1. Test Codecformer with different neural audio codecs (e.g., SoundStream, Lyra) to verify that the computational efficiency gains and separation performance improvements generalize beyond the specific DAC codec used in this study.

2. Evaluate Codecformer on more challenging datasets with varying numbers of speakers, different noise conditions, and higher sampling rates to assess robustness beyond the clean, two-speaker WSJ0-2mix dataset.

3. Conduct systematic ablation studies comparing different activation functions (ReLU, tanh, GELU, etc.) in both the codec and separation model to quantify the exact contribution of activation function matching to overall performance.