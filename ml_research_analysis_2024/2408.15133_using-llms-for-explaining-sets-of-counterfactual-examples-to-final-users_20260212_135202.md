---
ver: rpa2
title: Using LLMs for Explaining Sets of Counterfactual Examples to Final Users
arxiv_id: '2408.15133'
source_url: https://arxiv.org/abs/2408.15133
tags:
- explanation
- counterfactual
- counterfactuals
- user
- causes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using LLMs to generate natural language explanations
  from sets of counterfactual examples for improving model interpretability. The approach
  breaks down explanation generation into smaller steps mimicking human reasoning:
  extracting causes from counterfactuals, evaluating their relevance, and synthesizing
  them into actionable advice.'
---

# Using LLMs for Explaining Sets of Counterfactual Examples to Final Users

## Quick Facts
- arXiv ID: 2408.15133
- Source URL: https://arxiv.org/abs/2408.15133
- Authors: Arturo Fredes; Jordi Vitria
- Reference count: 21
- One-line primary result: LLM-generated explanations from counterfactuals achieve 70-93% validity in producing counterfactuals that flip model predictions

## Executive Summary
This paper proposes a novel approach for generating natural language explanations from sets of counterfactual examples using Large Language Models (LLMs). The method breaks down the explanation generation process into smaller steps that mimic human reasoning: extracting causes from counterfactuals, evaluating their relevance, and synthesizing them into actionable advice. Experiments with the Adult dataset demonstrate that LLM-generated explanations consistently incorporate the most relevant causes identified, with validity rates of 70-93% when using larger sets of counterfactuals (3-5 vs 1).

The approach addresses a key challenge in explainable AI by providing end users with understandable explanations for model decisions. By using a multi-step pipeline that guides the LLM through cause identification, ranking, and natural language synthesis, the method produces explanations that are both technically valid and actionable. The closed-loop evaluation validates explanation quality by testing whether the LLM can generate a counterfactual from the explanation alone, providing an automated way to assess explanation effectiveness.

## Method Summary
The method uses a multi-step LLM pipeline to generate natural language explanations from counterfactual examples. First, counterfactuals are generated using the DiCE framework for a given instance. Then, an LLM extracts potential causes from these counterfactuals by identifying features that differ from the original instance and are actionable. The causes are evaluated and ranked based on their frequency across the counterfactual set using LLM-generated Python code. Finally, the LLM synthesizes the ranked causes into a natural language explanation that provides actionable advice to the user. The approach is validated through a closed-loop evaluation where the LLM attempts to generate a new counterfactual from the explanation alone.

## Key Results
- LLM-generated explanations achieve 70-93% validity in producing counterfactuals that flip model predictions
- Higher validity observed when using larger sets of counterfactuals (3-5 vs 1)
- Explanations consistently incorporate the most relevant causes identified, with top-ranked cause appearing in 74-95% of cases
- The approach successfully identifies actionable causes that can help users achieve desired outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-step pipeline mimics human reasoning by breaking explanation generation into smaller tasks (extracting causes, evaluating relevance, synthesizing into advice).
- Mechanism: By decomposing the problem into abductive reasoning for cause extraction, quantitative evaluation via Python code generation, and natural language synthesis, the LLM can handle complex reasoning tasks that would otherwise exceed its planning capabilities.
- Core assumption: LLMs can successfully perform each subtask when given clear instructions and structured inputs/outputs.
- Evidence anchors:
  - [section] "we broke down the problem into smaller ones, mimicking human reasoning [8]"
  - [section] "We guide the LLM through the explanation generation task by breaking it into smaller processes, mimicking the steps a human would follow"
- Break condition: If any subtask fails (e.g., incorrect cause extraction or evaluation), the final explanation will be unreliable or misleading.

### Mechanism 2
- Claim: Using multiple counterfactual examples (3-5) provides more diverse paths to the desired outcome, improving explanation validity.
- Mechanism: Larger sets of counterfactuals reveal multiple potential causes for the classification decision, allowing the explanation to capture a broader range of actionable advice for users.
- Core assumption: More counterfactuals lead to more comprehensive cause identification and ranking.
- Evidence anchors:
  - [abstract] "higher validity observed when using larger sets of counterfactuals (3-5 vs 1)"
  - [section] "When more examples are given, more reasons for the given classification are found, and therefore a larger amount of different paths to obtain the desired outcome can be observed"
- Break condition: If additional counterfactuals don't reveal new causes or introduce noise, validity gains may plateau or decrease.

### Mechanism 3
- Claim: The closed-loop evaluation validates explanation quality by testing whether the LLM can generate a counterfactual from the explanation alone.
- Mechanism: If the LLM can successfully create a valid counterfactual using only the original data and explanation text, this indicates the explanation captured the essential causal information needed to flip the classification.
- Core assumption: The ability to regenerate a counterfactual demonstrates that the explanation contains sufficient actionable information.
- Evidence anchors:
  - [section] "We did so by asking the LLM to generate a counterfactual example [5] at the end of the reasoning, using only the original case, information of the dataset and the final explanation"
  - [section] "If the LLM is capable of generating a counterfactual from the explanation it means that the important information was correctly synthesized in it"
- Break condition: If the LLM generates an invalid counterfactual or one inconsistent with the identified causes, the explanation quality is questionable.

## Foundational Learning

- Concept: Counterfactual explanations and their role in contrastive reasoning
  - Why needed here: The entire approach builds on generating and interpreting counterfactual examples to explain model decisions
  - Quick check question: Can you explain the difference between a counterfactual and a simple feature importance explanation?

- Concept: Causal inference and abductive reasoning
  - Why needed here: The method relies on identifying causal factors from counterfactual examples and using abductive reasoning to generate possible causes
  - Quick check question: What distinguishes abductive reasoning from deductive or inductive reasoning in the context of explanation generation?

- Concept: LLM prompting strategies (zero-shot, few-shot, Tree of Thought)
  - Why needed here: The approach uses different prompting techniques to improve cause diversity and explanation quality
  - Quick check question: How might Tree of Thought prompting differ from standard prompting in terms of handling multi-step reasoning tasks?

## Architecture Onboarding

- Component map:
  - Counterfactual Generator (DiCE) -> Cause Extractor (LLM) -> Cause Evaluator (LLM + Python) -> Explanation Synthesizer (LLM) -> Closed-loop Validator (LLM + Python)

- Critical path:
  1. Generate counterfactuals from original data and model
  2. Extract potential causes using LLM with counterfactuals and original data
  3. Evaluate and rank causes by counting occurrences in counterfactuals via LLM-generated Python code
  4. Synthesize ranked causes into natural language explanation using LLM
  5. Validate explanation by having LLM generate counterfactual from explanation alone

- Design tradeoffs:
  - Using larger counterfactual sets improves validity but increases computation and may introduce noise
  - Zero-shot prompting vs. Tree of Thought: simpler but potentially less diverse vs. more complex but potentially richer explanations
  - LLM-based evaluation vs. ground truth: faster and automated but potentially less accurate than human evaluation

- Failure signatures:
  - Low validity (<70%) suggests explanations lack necessary information
  - Causes used percentage much lower than causes identified suggests poor synthesis
  - Generated examples appearing in original dataset suggests memorization rather than genuine reasoning

- First 3 experiments:
  1. Compare validity between 1, 3, and 5 counterfactuals using zero-shot prompting
  2. Test Tree of Thought prompting with 5 counterfactuals vs. zero-shot with 5 counterfactuals
  3. Validate closed-loop evaluation by manually checking a sample of generated counterfactuals for consistency with explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated explanations compare to human-written explanations in terms of user satisfaction and comprehension?
- Basis in paper: [inferred] The paper mentions that further experiments with human evaluations should be carried out to assess the quality of the natural language explanations, indicating this comparison has not been made.
- Why unresolved: The paper focuses on technical evaluation metrics like validity and consistency with causes, but does not include user studies to assess the actual effectiveness of the explanations for end users.
- What evidence would resolve it: A user study comparing user satisfaction, comprehension, and trust in LLM-generated explanations versus human-written explanations, using metrics like post-explanation questionnaires and task performance.

### Open Question 2
- Question: How do different counterfactual generation methods (beyond DiCE) affect the quality and diversity of causes identified by the LLM?
- Basis in paper: [explicit] The paper states "DiCE utilizes user data and the ML model to generate counterfactuals through perturbations" and mentions "other frameworks could be used", indicating this comparison has not been made.
- Why unresolved: The paper uses only DiCE for counterfactual generation, limiting understanding of how alternative methods might influence the explanation generation process.
- What evidence would resolve it: Experiments using multiple counterfactual generation methods (e.g., Growing Spheres, Constraint-based methods) and comparing the resulting causes identified and explanation quality.

### Open Question 3
- Question: How does the proposed approach scale to high-dimensional datasets with many features and complex relationships?
- Basis in paper: [inferred] The paper uses the Adult dataset with relatively few features, and mentions "further experiments with other data sets" should be carried out, suggesting scalability has not been tested.
- Why unresolved: The paper only tests on the Adult dataset, which has limited features compared to many real-world applications, and does not address computational complexity or performance on larger datasets.
- What evidence would resolve it: Experiments on high-dimensional datasets (e.g., medical imaging, genomics) measuring explanation generation time, causes identified, and validity as dataset size and feature count increase.

## Limitations
- Results are based solely on the Adult dataset, which limits generalizability to other domains or data types
- The closed-loop evaluation, while innovative, may not fully capture human interpretability since it relies on LLM-generated counterfactuals rather than human assessment
- The prompts used for each step of the pipeline are not fully specified, which could affect reproducibility

## Confidence
- High confidence in the multi-step pipeline mechanism and its ability to break down complex reasoning tasks
- Medium confidence in the validity metrics and their interpretation as measures of explanation quality
- Medium confidence in the closed-loop evaluation approach as a validation method
- Low confidence in generalizability across different datasets and real-world applications without further testing

## Next Checks
1. Conduct human evaluation studies to assess whether explanations generated by the LLM pipeline are actually helpful and understandable to end users, comparing them against human-generated explanations
2. Test the approach on multiple diverse datasets (e.g., healthcare, finance, image classification) to evaluate generalizability and identify dataset-specific challenges
3. Perform ablation studies to isolate the contribution of each pipeline component (cause extraction, evaluation, synthesis) to overall explanation quality and validity