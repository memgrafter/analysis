---
ver: rpa2
title: 'RecurrentGemma: Moving Past Transformers for Efficient Open Language Models'
arxiv_id: '2404.07839'
source_url: https://arxiv.org/abs/2404.07839
tags:
- gemma
- recurrentgemma
- tokens
- turn
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RecurrentGemma introduces a novel architecture that combines linear
  recurrences with local attention to achieve transformer-level performance while
  maintaining a fixed-size state. This design reduces memory usage and enables efficient
  inference on long sequences, outperforming similarly-sized transformer models in
  throughput.
---

# RecurrentGemma: Moving Past Transformers for Efficient Open Language Models

## Quick Facts
- arXiv ID: 2404.07839
- Source URL: https://arxiv.org/abs/2404.07839
- Reference count: 2
- Key outcome: Introduces a novel architecture combining linear recurrences with local attention to achieve transformer-level performance while maintaining fixed-size state for efficient inference on long sequences.

## Executive Summary
RecurrentGemma presents a novel architecture that moves beyond transformers by combining linear recurrences (Hawk layers) with local attention (Griffin architecture). This hybrid approach achieves transformer-level performance while maintaining a fixed-size state, enabling efficient inference on long sequences with memory usage independent of sequence length. The model demonstrates comparable performance to similarly-sized Gemma baselines despite being trained on fewer tokens, with significant throughput improvements during inference, especially on long sequences.

## Method Summary
The approach uses a modified Griffin architecture where input embeddings are scaled by the square root of model width, and embeddings are tied with the same factor. Two model sizes are presented: 2B and 9B parameters, with both pre-trained and instruction-tuned variants. The models are trained on 2T tokens of English data from web documents, mathematics, and code using a 256k token vocabulary. Training uses sequences of 8192 tokens with no weight decay on recurrent layer parameters and clipped derivatives during backpropagation. The architecture combines RG-LRU recurrent layers with local attention layers and MLP layers in a transformer-like decoder structure.

## Key Results
- RecurrentGemma achieves comparable performance to similarly-sized Gemma baselines despite training on fewer tokens (2T vs 3T for 2B, 2T vs 6T for 9B)
- Inference throughput improvements of up to two orders of magnitude for the 9B model on long sequences
- Human evaluations demonstrate competitive performance against larger models like Mistral 7B v0.2 Instruct
- The 9B instruction-tuned variant outperforms Mistral on instruction-following tasks

## Why This Works (Mechanism)

### Mechanism 1
The Griffin architecture achieves transformer-level performance while maintaining fixed-size state by combining linear recurrences with local attention. The recurrent layers compress the entire sequence history into a fixed-size state, while local attention processes recent tokens. This hybrid approach avoids the growing KV cache problem of transformers while maintaining performance.

### Mechanism 2
Fixed-size state enables efficient inference on long sequences with memory usage independent of sequence length. By compressing the sequence into a fixed-size state, the model avoids the quadratic memory growth of transformer KV caches. The state size remains bounded by the RNN width parameter, not sequence length.

### Mechanism 3
The modified Griffin architecture with input embedding scaling achieves comparable performance to Gemma despite fewer training tokens. The square root scaling of input embeddings and tied embeddings create a more effective representation space, combined with the hybrid architecture allowing efficient learning from less data.

## Foundational Learning

- Concept: Linear recurrences and state space models
  - Why needed here: Understanding how Hawk layers compress sequences into fixed-size states is crucial for grasping the core efficiency mechanism
  - Quick check question: How does a linear recurrence differ from standard attention in terms of memory complexity?

- Concept: Local attention mechanisms
  - Why needed here: The hybrid approach relies on understanding how local attention complements recurrent layers
  - Quick check question: What are the computational advantages and limitations of local attention compared to global attention?

- Concept: Transformer KV cache mechanics
  - Why needed here: Understanding the memory bottleneck that RecurrentGemma addresses requires knowing how transformers store key-value pairs
  - Quick check question: How does KV cache size scale with sequence length in standard transformers?

## Architecture Onboarding

- Component map: Input → Embedding scaling → Recurrent layers → Local attention → MLP → Output
- Critical path: The recurrent layers form the backbone, processing the full sequence history while local attention handles the most recent tokens within the attention window
- Design tradeoffs:
  - Fixed state size vs. potential loss of information
  - Local attention window size vs. computational cost
  - Model width vs. state capacity
  - Training tokens vs. architectural efficiency
- Failure signatures:
  - Performance degradation on tasks requiring long-range dependencies
  - Memory leaks or state corruption during inference
  - Training instability due to recurrent layer gradients
  - Suboptimal performance compared to transformers of similar size
- First 3 experiments:
  1. Compare throughput of RecurrentGemma vs Gemma on varying sequence lengths to verify memory efficiency claims
  2. Test performance degradation when reducing local attention window size to understand the attention-recurrence balance
  3. Evaluate state capacity by testing on tasks with varying dependency lengths to find breaking points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RecurrentGemma scale with even longer sequence lengths beyond 8K tokens, and what are the practical limits of this architecture for extremely long documents?
- Basis in paper: [inferred] The paper demonstrates improved throughput on sequences up to 8K tokens but doesn't explore performance on much longer sequences or discuss practical limits.
- Why unresolved: The evaluation focuses on sequences up to 8K tokens, which may not represent the upper bounds of what this architecture can handle.
- What evidence would resolve it: Comprehensive benchmarking on sequences ranging from 16K to 1M tokens, including qualitative assessments of output quality degradation at extreme lengths.

### Open Question 2
- Question: How does the RLHF fine-tuning process specifically adapt the recurrent architecture compared to transformer-based models, and are there unique challenges or optimizations needed?
- Basis in paper: [explicit] The paper mentions using "a novel RLHF algorithm" but provides no details on how it differs from transformer RLHF approaches.
- Why unresolved: The paper only states that they follow a similar instruction tuning approach to Gemma but doesn't explain how the RLHF algorithm was adapted for the recurrent architecture.
- What evidence would resolve it: Detailed description of the RLHF algorithm modifications and comparison of training dynamics between RecurrentGemma and transformer models during RLHF.

### Open Question 3
- Question: What is the impact of the linear recurrence mechanism on the model's ability to capture long-range dependencies compared to global attention mechanisms, and under what circumstances might this become a limitation?
- Basis in paper: [inferred] The paper claims competitive performance with transformers but doesn't provide detailed analysis of how the linear recurrence mechanism affects long-range dependency modeling.
- Why unresolved: While the paper demonstrates competitive performance on standard benchmarks, it doesn't investigate whether there are specific types of tasks where the recurrent approach shows weaknesses.
- What evidence would resolve it: Targeted experiments on tasks specifically designed to test long-range dependencies and ablation studies comparing recurrence vs. attention for different sequence distances.

## Limitations

- The fixed-size state compression may limit performance on tasks requiring explicit access to very long-range dependencies beyond the local attention window
- The 2048-token local attention window creates a hard boundary on the model's effective context window, potentially limiting performance on tasks requiring longer-range reasoning
- The paper focuses primarily on performance and efficiency metrics without extensive evaluation of safety, bias, or robustness characteristics

## Confidence

- **High Confidence**: The architectural design combining linear recurrences with local attention is technically sound and well-specified; the memory efficiency improvements during inference are verifiable through direct measurement; the throughput improvements on long sequences are reproducible given the architecture specifications
- **Medium Confidence**: Performance parity with Gemma models on automated benchmarks is demonstrated but may not generalize across all task types; training efficiency gains from architectural modifications are plausible but require careful implementation to realize; human evaluation results showing competitive performance against larger models are promising but based on limited comparisons
- **Low Confidence**: Long-term stability and behavior of the recurrent architecture under diverse deployment conditions; generalization of performance gains to specialized domains or task types not represented in the evaluation suite; safety and bias characteristics of the instruction-tuned variants

## Next Checks

1. **Long-range Dependency Testing**: Design experiments specifically targeting tasks requiring dependencies beyond the local attention window (2048 tokens) to measure performance degradation points and identify breaking conditions for the fixed-state compression approach.

2. **Cross-domain Generalization Study**: Evaluate the models on specialized datasets from domains not represented in the training data (e.g., medical, legal, or highly technical domains) to assess whether the architectural efficiency gains translate to specialized knowledge tasks.

3. **Safety and Robustness Analysis**: Conduct comprehensive safety evaluations including adversarial testing, bias measurement, and robustness to input perturbations to understand the safety characteristics of the recurrent architecture compared to standard transformers.