---
ver: rpa2
title: 'CANDOR: Counterfactual ANnotated DOubly Robust Off-Policy Evaluation'
arxiv_id: '2412.08052'
source_url: https://arxiv.org/abs/2412.08052
tags:
- variance
- counterfactual
- annotations
- reward
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of off-policy evaluation (OPE)
  in contextual bandits when counterfactual annotations are available but potentially
  imperfect (biased or noisy). The authors propose a family of doubly robust (DR)
  inspired estimators that incorporate counterfactual annotations in three distinct
  ways: using them in the direct method (DM) component, the importance sampling (IS)
  component, or both.'
---

# CANDOR: Counterfactual ANnotated DOubly Robust Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2412.08052
- Source URL: https://arxiv.org/abs/2412.08052
- Authors: Aishwarya Mandyam; Shengpu Tang; Jiayu Yao; Jenna Wiens; Barbara E. Engelhardt
- Reference count: 40
- Key outcome: Proposes three doubly robust estimators that incorporate imperfect counterfactual annotations, proving that using annotations in the direct method component preserves unbiasedness while using them in the importance sampling component introduces bias, with empirical results showing DM+-IS consistently outperforms alternatives when reward models are misspecified.

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) in contextual bandits when counterfactual annotations are available but potentially imperfect (biased or noisy). The authors propose a family of doubly robust (DR) inspired estimators that incorporate counterfactual annotations in three distinct ways: using them in the direct method (DM) component, the importance sampling (IS) component, or both. They provide theoretical analysis showing that using imperfect annotations in the DM part yields unbiased estimates, while using them in the IS part introduces bias. Empirical results across three simulated environments demonstrate that when the reward model is misspecified and annotations are imperfect, the DM+-IS estimator (using annotations only in the DM component) consistently outperforms alternatives, exhibiting the lowest root mean squared error.

## Method Summary
The paper proposes three DR-inspired estimators: DM+-IS (annotations in DM part), DM-IS+ (annotations in IS part), and DM+-IS+ (annotations in both parts). These estimators leverage counterfactual annotations to improve dataset coverage while maintaining statistical guarantees. The method involves creating an augmented behavior dataset D+ by combining factual samples with counterfactual annotations, then training reward function estimates using this augmented data. The estimators are evaluated using RMSE across three simulated environments with varying levels of annotation bias and reward model misspecification.

## Key Results
- Using imperfect annotations in the DM component preserves unbiasedness while using them in the IS component introduces bias
- DM+-IS estimator consistently achieves lowest RMSE when reward models are misspecified and annotations are imperfect
- Counterfactual annotations improve dataset coverage, reducing variance in OPE estimates
- The performance gap between DM+-IS and alternatives increases with higher annotation bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using imperfect annotations in the DM part of a DR estimator preserves unbiasedness while using them in the IS part introduces bias.
- **Mechanism:** The DM component estimates expected rewards by averaging observed rewards weighted by their occurrence frequency. When annotations are biased, the weighted average still converges to the true expected value because the bias affects all samples proportionally. The IS component, however, reweights samples by inverse propensity scores, which amplifies any bias present in the annotations.
- **Core assumption:** The behavior policy is known and the annotation bias is consistent across all samples.
- **Evidence anchors:**
  - [abstract] "We prove that using imperfect annotations in the DM part of the estimator best leverages the annotations, as opposed to using them in the IS part."
  - [section] "Under biased annotations (Assumption 2) and common support (Assumption 4), E[ ˆV DM+-IS] = v(πe)." (Proposition 1)
- **Break condition:** If the annotation bias varies systematically with context or action, the DM estimator's unbiasedness breaks down.

### Mechanism 2
- **Claim:** The doubly robust structure provides robustness to reward model misspecification.
- **Mechanism:** When the reward model is misspecified, the IS component can compensate by providing accurate importance-weighted estimates. Conversely, when the importance weights are unreliable (high variance), the DM component provides stable estimates. This redundancy ensures that at least one component provides reasonable estimates.
- **Core assumption:** At least one of the reward model or importance weights is reasonably accurate.
- **Evidence anchors:**
  - [abstract] "A DR estimator combines IS with a reward model estimate, known as the direct method (DM), and offers favorable statistical guarantees."
  - [section] "The DR estimator is robust to two sources of error (the IPS ratio, and the reward model)." (Section 2.1)
- **Break condition:** When both the reward model and importance weights are severely misspecified simultaneously.

### Mechanism 3
- **Claim:** Counterfactual annotations improve coverage of the behavior dataset, reducing variance in OPE estimates.
- **Mechanism:** Standard OPE methods are limited by the coverage of the behavior dataset. Counterfactual annotations effectively expand this coverage by providing reward estimates for actions that were not actually taken. This expanded coverage reduces the variance of importance weights and improves the accuracy of reward model estimates.
- **Core assumption:** The counterfactual annotations, while imperfect, provide reasonable estimates of rewards for actions not observed in the behavior data.
- **Evidence anchors:**
  - [abstract] "Recent work introduced IS+, an importance sampling (IS) estimator that uses expert-annotated counterfactual samples to improve behavior dataset coverage."
  - [section] "Although IS-based estimators have achieved desirable performance in some settings, they are known to have high variance." (Section 1)
- **Break condition:** If counterfactual annotations are systematically wrong or missing for critical actions.

## Foundational Learning

- **Concept:** Doubly Robust Estimation
  - Why needed here: The paper builds on DR principles to create new estimators that can handle imperfect annotations while maintaining statistical guarantees.
  - Quick check question: What are the two sources of error that a doubly robust estimator is designed to be robust against?

- **Concept:** Importance Sampling in Contextual Bandits
  - Why needed here: IS forms one component of the DR estimators and understanding its variance properties is crucial for interpreting the results.
  - Quick check question: What is the primary disadvantage of IS-based estimators that motivates combining them with DM?

- **Concept:** Counterfactual Reasoning
  - Why needed here: The paper's core contribution involves incorporating counterfactual annotations into OPE, requiring understanding of what counterfactuals represent in this context.
  - Quick check question: In the context of this paper, what does a counterfactual annotation represent?

## Architecture Onboarding

- **Component map:** Behavior dataset D -> Counterfactual annotations g_i -> Reward model estimator ˆR+ -> Three DR-inspired estimators (DM+-IS, DM-IS+, DM+-IS+) -> Evaluation metrics (RMSE, bias, standard deviation)

- **Critical path:**
  1. Collect behavior dataset with factual rewards
  2. Obtain counterfactual annotations for some samples
  3. Train reward model using combined data
  4. Apply chosen DR estimator to evaluate target policy
  5. Compare results against baselines

- **Design tradeoffs:**
  - Using annotations in DM vs IS: DM preserves unbiasedness but IS may improve variance
  - Weighting schemes: Equal weighting vs importance weighting of counterfactual samples
  - Reward model complexity: Simpler models may generalize better with imperfect annotations

- **Failure signatures:**
  - High RMSE with low bias: Variance issues, likely from poor importance weights
  - High bias: Systematic errors in annotations or reward model
  - Unstable estimates: Insufficient coverage in behavior dataset

- **First 3 experiments:**
  1. Compare DM+-IS vs DM-IS+ on a simple 2-context bandit with varying annotation quality
  2. Test sensitivity to reward model misspecification across all three proposed estimators
  3. Evaluate performance when counterfactual annotations are missing for critical actions

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes known behavior policy and specific forms of annotation bias
- Empirical validation limited to three simulated environments
- Does not extensively explore impact of varying annotation quality across different context-action pairs

## Confidence
- **High confidence**: The theoretical proof that using annotations in DM preserves unbiasedness (Proposition 1) is well-established
- **Medium confidence**: The empirical superiority of DM+-IS over alternatives across different environments and reward model specifications
- **Medium confidence**: The claim that counterfactual annotations improve dataset coverage and reduce variance, as this relies on simulation assumptions

## Next Checks
1. Test the proposed estimators on real-world bandit datasets with naturally occurring counterfactual annotations to validate simulation results
2. Systematically vary the correlation between annotation quality and context-action features to assess robustness to structured bias
3. Implement an adaptive estimator selection mechanism that can dynamically choose between DM+-IS, DM-IS+, and DM+-IS+ based on estimated annotation quality and reward model performance