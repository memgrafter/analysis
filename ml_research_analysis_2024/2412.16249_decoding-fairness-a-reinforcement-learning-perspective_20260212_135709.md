---
ver: rpa2
title: 'Decoding fairness: a reinforcement learning perspective'
arxiv_id: '2412.16249'
source_url: https://arxiv.org/abs/2412.16249
tags:
- learning
- fairness
- state
- where
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergence of fairness in human behavior
  using a reinforcement learning (RL) framework, specifically applying Q-learning
  to the ultimatum game (UG). The study addresses the discrepancy between the rational
  economic prediction of unfair offers and the observed preference for fairness in
  behavioral experiments.
---

# Decoding fairness: a reinforcement learning perspective

## Quick Facts
- arXiv ID: 2412.16249
- Source URL: https://arxiv.org/abs/2412.16249
- Authors: Guozhong Zheng; Jiqiang Zhang; Xin Ou; Shengfeng Deng; Li Chen
- Reference count: 0
- Primary result: Fairness emerges when individuals value both past experiences (small α) and future rewards (large γ), while rational strategies dominate when individuals are either forgetful or shortsighted.

## Executive Summary
This paper investigates the emergence of fairness in human behavior using a reinforcement learning (RL) framework, specifically applying Q-learning to the ultimatum game (UG). The study addresses the discrepancy between the rational economic prediction of unfair offers and the observed preference for fairness in behavioral experiments. The core finding is that fairness emerges prominently when both experiences and future rewards are appreciated, while rational strategies dominate when individuals are either forgetful or shortsighted. The results suggest that endogenous motivation, aiming to maximize accumulated rewards, is sufficient to explain the emergence of fairness without needing exogenous factors.

## Method Summary
The study applies Q-learning to the ultimatum game, assigning two Q-tables to each player (one for proposer role and one for responder role). The Q-values are updated using the Bellman equation, incorporating immediate rewards and future expected rewards. The method explores how different learning parameters (learning rate α and discount factor γ) affect the emergence of fairness. The study reveals a two-phase transition mechanism: Phase I eliminates strategies leading to failed deals, and Phase II sees a branching process where the remaining strategies evolve into either fair or rational strategies.

## Key Results
- Fairness emerges prominently when individuals appreciate both past experiences (small α) and future rewards (large γ).
- The fraction of fair offers (pm) and acceptance thresholds (qm) approaches unity in this parameter regime.
- Rational strategies (pl, ql) dominate when individuals are either forgetful or shortsighted.
- The results are robust across different role assignment methods and extend to a latticed population.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness emerges when individuals value both past experiences (small α) and future rewards (large γ).
- Mechanism: Q-learning updates reinforce strategies that maximize accumulated rewards. When α is small, historical experiences are retained and guide future decisions. When γ is large, future rewards influence current action selection. Together, these parameters encourage stable strategies that achieve mutual benefit.
- Core assumption: Agents aim to maximize accumulated rewards rather than immediate payoff.
- Evidence anchors:
  - [abstract] "fairness emerges prominently when both experiences and future rewards are appreciated"
  - [section III] "the fractions of pm and qm approach unity (red region)"
  - [corpus] weak evidence; no direct neighbor studies cited on α-γ interaction
- Break condition: If α is large (forgetful) or γ is small (shortsighted), reinforcement fails to stabilize fair strategies.

### Mechanism 2
- Claim: Failed deals are eliminated early, leaving only mutually beneficial strategies to compete.
- Mechanism: Strategies leading to offer-rejection cycles receive no reward and are deprioritized in Q-value updates. This self-pruning removes extreme offers (qh, ph) and low-acceptance thresholds (ql), leaving only states with successful exchanges.
- Core assumption: Failed deals produce zero reward and thus receive no reinforcement.
- Evidence anchors:
  - [section IV.A] "those who propose and expect high offers put themselves in a disadvantageous position"
  - [section IV.A] "strategies (pm, qm), (pl, ql), and (pm, ql) survive"
  - [corpus] no direct neighbor evidence for self-pruning in UG
- Break condition: If exploration rate ϵ is too low, agents may never discover and prune failing strategies.

### Mechanism 3
- Claim: Branching processes in surviving states push the population toward either fair (pm, qm) or rational (pl, ql) equilibria.
- Mechanism: In states (pl, ql) and (pm, qm), repeated reinforcement stabilizes preferences for the current action. Random exploration occasionally switches roles, but if the new action leads to a successful deal with equal reward, the Q-value for that action is reinforced. Over time, the system locks into one of two basins of attraction.
- Core assumption: Q-value reinforcement creates stable action preferences that resist change once payoff differences are small.
- Evidence anchors:
  - [section IV.B] "dominating transitions indicated by arrows among the remaining three strategies"
  - [section IV.B] "(pm, ql) → (pl, ql) → (pm, qm) in Phase II"
  - [corpus] no neighbor evidence for branching process dynamics in RL fairness models
- Break condition: If α is too large, Q-values decay quickly and the system never stabilizes.

## Foundational Learning

- Concept: Bellman equation update rule
  - Why needed here: The paper’s Q-learning algorithm depends on correct implementation of the Bellman update to propagate rewards and guide strategy evolution.
  - Quick check question: In the update Qs,a(t+1) = (1-α)Qs,a(t) + α[π(t) + γ maxa' Qs',a'(t)], what does γ represent and why does it matter?

- Concept: Epsilon-greedy exploration
  - Why needed here: Exploration (probability ϵ) allows agents to sample non-greedy actions and discover potentially better strategies; the paper sets ϵ = 0.01.
  - Quick check question: If ϵ = 0, will the system still discover the branching transitions in Phase II?

- Concept: State-action value convergence
  - Why needed here: Fairness emergence relies on Q-values converging to stable maxima that reflect long-term optimal behavior; understanding convergence helps predict when fairness will appear.
  - Quick check question: Under what combination of α and γ does the Q-value for (pm, qm) converge fastest?

## Architecture Onboarding

- Component map:
  - Two Q-tables per agent (proposer and responder)
  - State space: 9 possible (offer, acceptance threshold) pairs
  - Action space: {pl, pm, ph} for proposers; {ql, qm, qh} for responders
  - Parameters: α (learning rate), γ (discount factor), ϵ (exploration rate)

- Critical path:
  1. Initialize Q-tables randomly
  2. Alternate roles, select action (ε-greedy)
  3. Compute reward via UG payoff function
  4. Update Q-values using Bellman equation
  5. Repeat until stationary fractions observed

- Design tradeoffs:
  - Small α → slower convergence but better retention of successful strategies
  - Large γ → more farsighted planning but slower initial learning
  - High ϵ → more exploration, risk of destabilizing convergence

- Failure signatures:
  - Oscillating fractions (no convergence)
  - Dominance of (pl, ql) only (fairness absent)
  - Uniform spread across all six options (no pruning)

- First 3 experiments:
  1. Sweep α ∈ [0.01, 0.1] with γ fixed at 0.9; plot fpm vs. α to confirm fair strategy dominance for small α.
  2. Sweep γ ∈ [0.1, 0.9] with α fixed at 0.1; plot fqm vs. γ to verify fairness rises with long-term vision.
  3. Set (α, γ) = (0.9, 0.1) to reproduce failure mode; confirm fpl, ql → 1 and fpm, qm → 0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the emergence of fairness in the ultimatum game change when the action and state spaces are continuous rather than discrete?
- Basis in paper: [explicit] The authors acknowledge the limitation of discrete setups and suggest extending to continuous versions using deep reinforcement learning.
- Why unresolved: The study uses discretized setups for convenience, which may not fully capture the complexities of real-world scenarios.
- What evidence would resolve it: Results from a study using deep reinforcement learning with continuous action and state spaces in the ultimatum game.

### Open Question 2
- Question: What is the impact of different role assignment methods (rotating, random, fixed) on the emergence of fairness in larger populations?
- Basis in paper: [explicit] The authors mention robustness across different role assignment methods in a two-player scenario but do not explore larger populations.
- Why unresolved: The study focuses on a two-player scenario, and the impact of role assignment in larger populations is not examined.
- What evidence would resolve it: Comparative results from studies using different role assignment methods in larger populations.

### Open Question 3
- Question: How do different learning rates (α) and discount factors (γ) influence the stability and convergence of fairness in the ultimatum game?
- Basis in paper: [explicit] The authors analyze the impact of α and γ on the emergence of fairness but do not fully explore their influence on stability and convergence.
- Why unresolved: The study focuses on the emergence of fairness but does not delve into the stability and convergence aspects.
- What evidence would resolve it: Detailed analysis of the stability and convergence of fairness across different combinations of α and γ.

## Limitations
- The study relies on a simplified ultimatum game with only three discrete offer options and acceptance thresholds, which may not capture the full complexity of human fairness behavior.
- The analysis focuses on a two-player scenario and a latticed population extension, but real-world fairness dynamics often involve more complex social structures and multiple players.
- The study does not account for potential exogenous factors that may influence fairness, such as social norms or cultural differences.

## Confidence
- Mechanism 1 (fairness emerges with small α and large γ): Medium confidence
- Mechanism 2 (self-pruning of failed deals): Low confidence (limited evidence from corpus)
- Mechanism 3 (branching process leading to fair or rational equilibria): Low confidence (limited evidence from corpus)

## Next Checks
1. Validate the two-phase transition mechanism (Phase I: elimination of failed deals, Phase II: branching process) by systematically varying the exploration rate ϵ and observing its impact on the convergence to fair or rational strategies.
2. Extend the analysis to a more complex ultimatum game with a continuous offer space and multiple acceptance thresholds to assess the robustness of the fairness emergence mechanism.
3. Investigate the role of exogenous factors, such as social norms or cultural differences, in shaping the emergence of fairness by incorporating them into the reinforcement learning framework and comparing the results with the current study.