---
ver: rpa2
title: Unsupervised Optimisation of GNNs for Node Clustering
arxiv_id: '2402.07845'
source_url: https://arxiv.org/abs/2402.07845
tags:
- performance
- unsupervised
- graph
- gnns
- ground-truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether graph neural networks (GNNs) can
  be optimized for node clustering without ground-truth labels by using modularity
  and conductance as unsupervised metrics. The authors show that modularity can serve
  as a reliable proxy for predicting ground-truth clustering performance (F1 and NMI
  scores), even when used for hyperparameter optimization.
---

# Unsupervised Optimisation of GNNs for Node Clustering

## Quick Facts
- arXiv ID: 2402.07845
- Source URL: https://arxiv.org/abs/2402.07845
- Reference count: 40
- Key outcome: Modularity serves as reliable unsupervised proxy for predicting ground-truth clustering performance in GNN optimization

## Executive Summary
This paper investigates whether graph neural networks can be optimized for node clustering without ground-truth labels by using modularity and conductance as unsupervised metrics. The authors demonstrate that modularity can reliably predict ground-truth clustering performance (F1 and NMI scores) even when used for hyperparameter optimization. The study reveals that GNNs exhibit bias toward adjacency space clustering signals, often underutilizing feature space information when both are present. Reducing training data size slightly decreases predictability but does not significantly impact final clustering performance.

## Method Summary
The research evaluates multiple GNN architectures (DAEGC, DMON, DGI, GRACE, SUBLIME, BGRL, VGAER) on 8 graph datasets with ground-truth labels. Models are trained using modularity and conductance as unsupervised loss functions, with hyperparameter optimization performed using Tree-structured Parzen Estimator (TPE). Performance is evaluated against ground-truth metrics (F1, NMI) and correlation analysis is conducted between unsupervised and supervised metrics. Synthetic experiments are used to analyze the balance between feature space and adjacency space clustering signals.

## Key Results
- Modularity shows strong correlation with ground-truth clustering performance across multiple datasets and GNN architectures
- Hyperparameter optimization using modularity maintains correlation with ground-truth metrics without significant performance drop
- GNNs exhibit bias toward adjacency space clustering signals, underutilizing feature space information when both are present
- Reducing dataset size to 33% slightly decreases correlation but maintains clustering performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modularity serves as a reliable unsupervised proxy for ground-truth clustering performance (F1/NMI) in GNN optimization.
- Mechanism: Modularity measures community structure by comparing edge density within clusters to edges between clusters. GNNs optimizing for modularity learn representations maximizing intra-cluster connectivity, aligning with ground-truth cluster definitions.
- Core assumption: Ground-truth community structure reflects graph connectivity patterns.
- Evidence anchors: [abstract] "modularity can serve as a reliable proxy for predicting ground-truth clustering performance"; [section] "Table 3, we can see that there is a correlation between the models selected with modularity compared with the ground-truth performance"
- Break condition: When feature space contains conflicting signals contradicting adjacency structure.

### Mechanism 2
- Claim: Hyperparameter optimization using modularity maintains correlation with ground-truth performance.
- Mechanism: TPE searches for configurations maximizing modularity scores. Since modularity correlates with ground-truth metrics, this finds configurations producing good clustering without labeled validation data.
- Core assumption: Relationship between modularity and ground-truth performance preserved during optimization.
- Evidence anchors: [abstract] "modularity proves to be more consistent and less sensitive to random seed variations"; [section] "there is not that much difference between a ground-truth supervised HPO compared to the unsupervised one"
- Break condition: Very small datasets or limited graph structure information.

### Mechanism 3
- Claim: GNNs exhibit bias toward adjacency space clustering signals.
- Mechanism: During training, GNNs propagate features along graph connectivity. Modularity optimization reinforces adjacency-based learning, prioritizing connectivity patterns over feature similarities when both are available.
- Core assumption: GNN architecture and training objective favor connectivity-based learning.
- Evidence anchors: [abstract] "GNNs tend to be biased toward adjacency space clustering signals"; [section] "VGAER algorithm can perfectly cluster, which means that it is not properly balancing the uncluster-able signal from the feature space"
- Break condition: When feature space contains strong clustering signals not reflected in adjacency structure.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: Understanding GNN aggregation operations is crucial for grasping adjacency bias
  - Quick check question: What is the fundamental operation that allows GNNs to incorporate neighbor information into node representations?

- Concept: Modularity and conductance metrics
  - Why needed here: These unsupervised metrics determine the success of the optimization approach
  - Quick check question: How does modularity differ from conductance in measuring community quality?

- Concept: Hyperparameter optimization and model selection
  - Why needed here: The paper uses unsupervised metrics for both training and model selection
  - Quick check question: What is the difference between hyperparameter optimization and model selection?

## Architecture Onboarding

- Component map:
  Data preprocessing -> GNN model implementation -> Unsupervised optimization -> Hyperparameter search -> Evaluation

- Critical path:
  1. Load dataset and initialize GNN model
  2. Train model using modularity/conductance as loss
  3. Perform hyperparameter optimization if needed
  4. Evaluate performance on ground-truth metrics
  5. Calculate correlation between unsupervised and supervised metrics

- Design tradeoffs:
  - Using modularity vs conductance: Modularity shows better correlation but may be more computationally expensive
  - Full vs reduced dataset training: Full dataset provides better correlation but may not be practical
  - Default vs optimized hyperparameters: Optimized parameters improve performance but require additional computation

- Failure signatures:
  - Low correlation between unsupervised and supervised metrics indicates approach may not work for this dataset
  - High W randomness coefficient suggests unstable training or model selection process
  - Poor absolute performance on ground-truth metrics despite high unsupervised metric scores

- First 3 experiments:
  1. Train a GNN on Cora dataset using default hyperparameters and modularity optimization, evaluate correlation with ground-truth
  2. Perform hyperparameter optimization on the same dataset using modularity, compare performance to default setup
  3. Train on 33% of Cora data, evaluate how reduced training size affects correlation with full-dataset ground-truth performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of modularity as an unsupervised metric affect the balance between feature space and adjacency space clustering signals in GNNs?
- Basis in paper: [explicit] The paper discusses GNNs' bias towards adjacency space clustering signals and underutilization of feature space information
- Why unresolved: While the bias is identified, the paper doesn't provide detailed analysis of how modularity specifically influences this balance
- What evidence would resolve it: Experiments comparing GNNs optimized with modularity versus those optimized with feature space metrics, and analysis of learned feature representations

### Open Question 2
- Question: Can unsupervised metrics other than modularity and conductance, such as feature space clustering quality metrics, be used to optimize GNNs and predict ground-truth performance?
- Basis in paper: [inferred] The paper suggests the bias may be due to using modularity, a graph partitioning metric, and proposes investigating feature space metrics
- Why unresolved: The paper only explores modularity and conductance, leaving other potential metrics unexplored
- What evidence would resolve it: Experiments comparing GNNs optimized with different unsupervised metrics, including feature space clustering quality metrics

### Open Question 3
- Question: How does the size of the dataset affect the predictability of unsupervised GNN optimization and the performance of resulting models?
- Basis in paper: [explicit] The paper investigates lower data limits for unsupervised optimization and finds dataset size changes correlation strength
- Why unresolved: The paper provides initial insights but doesn't fully explore the relationship between dataset size and predictability or performance
- What evidence would resolve it: Experiments systematically varying dataset size and analyzing correlation between unsupervised and ground-truth performance

## Limitations

- Findings primarily validated on relatively small academic datasets (Cora, CiteSeer, DBLP) and may not generalize to larger, more complex real-world graphs
- Correlation between unsupervised and ground-truth metrics shows variation across datasets and model architectures
- Bias toward adjacency space clustering suggests limitations when feature space contains strong clustering information not reflected in graph structure

## Confidence

- Modularity as reliable proxy: **High** - Multiple experiments consistently show strong correlation across datasets and models
- Hyperparameter optimization effectiveness: **Medium** - Results are positive but show dataset-dependent variation
- Adjacency bias finding: **Medium** - Synthetic experiments provide clear evidence, but real-world applicability needs further validation

## Next Checks

1. Test the unsupervised optimization approach on larger, real-world graphs (e.g., social networks, web graphs) to assess scalability and generalization
2. Conduct ablation studies specifically isolating the contribution of feature vs. adjacency space information in multi-signal environments
3. Evaluate the approach on graphs with known feature-adjacency conflicts to quantify the practical impact of the identified bias