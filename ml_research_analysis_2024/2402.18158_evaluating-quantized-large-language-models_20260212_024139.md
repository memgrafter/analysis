---
ver: rpa2
title: Evaluating Quantized Large Language Models
arxiv_id: '2402.18158'
source_url: https://arxiv.org/abs/2402.18158
tags:
- quantization
- llms
- fp16
- quantized
- quant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive evaluation of post-training
  quantization (PTQ) for large language models (LLMs) across 11 model families (125M-180B
  parameters), examining effects on Weight, Activation, and KV Cache tensors across
  five task types: basic NLP, emergent abilities, trustworthiness, dialogue, and long-context
  processing. Key findings include: larger models show higher tolerance to Weight
  and KV Cache quantization but lower tolerance to Activation quantization; Weight-only
  quantization allows 4-bit without significant loss for most tasks, while Weight-Activation
  and KV Cache quantization typically require 8-bit; emergent abilities like mathematical
  reasoning and self-calibration are more sensitive to quantization than instruction-following;
  dialogue tasks tolerate KV Cache quantization better than weights; and long-context
  tasks are more sensitive to quantization, especially for KV Cache.'
---

# Evaluating Quantized Large Language Models

## Quick Facts
- arXiv ID: 2402.18158
- Source URL: https://arxiv.org/abs/2402.18158
- Reference count: 40
- Key outcome: Comprehensive evaluation of post-training quantization (PTQ) for LLMs across 11 model families (125M-180B parameters), examining effects on Weight, Activation, and KV Cache tensors across five task types

## Executive Summary
This paper provides the first comprehensive evaluation of post-training quantization for large language models, examining how different tensor types (weights, activations, KV cache) and model sizes affect quantization robustness across five task categories. The study reveals that larger models tolerate weight-only and KV cache quantization better due to reduced outliers in their distributions, but show lower tolerance to activation quantization due to increased outlier density. The research provides specific bit-width recommendations for different task types and highlights the varying sensitivity of emergent abilities, dialogue tasks, and long-context processing to quantization.

## Method Summary
The evaluation framework applies uniform quantization with varying bit-widths (W2-W8, A2-A8, KV2-KV8) to 11 model families using AWQ, SmoothQuant, and RTN methods. The study evaluates quantization effects on five task categories (basic NLP, emergent abilities, trustworthiness, dialogue, long-context) using standardized benchmarks and analyzes tensor statistics (kurtosis, AbsMax, standard deviation) to explain performance trends. Memory consumption and computational overhead are measured to provide practical deployment recommendations.

## Key Results
- Larger models show higher tolerance to Weight-only and KV Cache quantization but lower tolerance to Activation quantization
- Weight-only quantization allows 4-bit without significant loss for most tasks, while Weight-Activation and KV Cache typically require 8-bit
- Emergent abilities like mathematical reasoning are more sensitive to quantization than instruction-following
- Dialogue tasks tolerate KV Cache quantization better than weights
- Long-context tasks are more sensitive to quantization, especially for KV Cache

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models have higher tolerance to Weight-only and KV Cache quantization due to fewer outliers in their weight distributions
- Mechanism: As model size increases, the kurtosis of weight tensors decreases, indicating fewer extreme values that are problematic for low-bit quantization
- Core assumption: The statistical properties of weight tensors (kurtosis, AbsMax, Std) directly correlate with quantization robustness
- Evidence anchors:
  - [section] "The larger the model size, the higher the tolerance for Weight-only and KV Cache Quantization. As shown in Figure 2 (a), for small models, such as LLaMA2-7B, when quantized to W3, the accuracy significantly degrades. However, the performance of the W3 quantized LLaMA2-70B exhibits only a marginal decline. This is because, in the same model family, the Kurtosis of the Weight tensors decreases as the model size grows larger, which means there are fewer outliers in larger LLMs"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Larger models have lower tolerance to Activation quantization due to increased outlier density in activation distributions
- Mechanism: As model size increases, the kurtosis of activation tensors increases significantly (>1000 vs ~10 for weights), indicating many more extreme values that are difficult to represent accurately in low-bit quantization schemes
- Core assumption: Activation tensor kurtosis directly predicts quantization sensitivity
- Evidence anchors:
  - [section] "On the contrary, the larger the model size, the lower the tolerance for Activation Quantization. As shown in Table 3, the Kurtosis of the Activation tensors (>1000) is much larger than that of the Weight and KV Cache tensors (âˆ¼10). This suggests that there are more outliers in the Activation tensors than in the Weight and KV Cache tensors. Notably, the Kurtosis of the Activation increases significantly with the size of the model, which means more outliers in the Activation tensors of larger LLMs"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 3
- Claim: Different tensor positions have distinct quantization sensitivities due to varying data distributions across layers
- Mechanism: Statistical analysis reveals that different linear layers (Q, K, V, O in attention; Gate, Up, Down in FFN) have varying kurtosis values, suggesting some layers require higher bit-widths while others can tolerate lower bit-widths
- Core assumption: Layer-specific quantization can optimize the trade-off between efficiency and performance
- Evidence anchors:
  - [section] "In addition, as shown in Table 4, we find that different linear layers have distinct Kurtosis. For instance, within the LLaMA2 family, the kurtosis of the activation in down projection layers in FFN is notably higher compared to other layers, and the kurtosis of the weight in out projection layers in Attention is slightly higher than that of the other layers"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

## Foundational Learning

- Concept: Statistical measures (kurtosis, AbsMax, standard deviation) and their interpretation
  - Why needed here: The paper's core findings rely on understanding how these statistical properties affect quantization robustness across different tensor types and model sizes
  - Quick check question: If a tensor has high kurtosis, what does this indicate about its distribution and how might this affect quantization?

- Concept: Post-training quantization (PTQ) techniques and their implementation
  - Why needed here: The evaluation compares different PTQ methods (AWQ, SmoothQuant) and quantization strategies (Weight-only, Weight-Activation, KV Cache)
  - Quick check question: What is the key difference between symmetric per-token quantization for activations versus asymmetric group-wise quantization for weights?

- Concept: Large language model inference pipeline (prefill vs decoding stages)
  - Why needed here: Understanding why different tensor types (weights, activations, KV cache) have different quantization requirements based on their roles in the inference process
  - Quick check question: Why is KV Cache quantization particularly important for long-context processing and large batch sizes?

## Architecture Onboarding

- Component map: Quantization engine -> Benchmark suite -> Statistical analysis module
- Critical path: For a new engineer, the most important workflow is: select model family -> apply quantization with target bit-width -> run evaluation on all task categories -> collect statistical metrics -> analyze performance degradation patterns
- Design tradeoffs: The framework balances comprehensiveness (11 model families, 5 task types) against practical constraints (evaluation time, computational resources). The choice of 4-bit as minimum quantization width balances efficiency gains against catastrophic performance loss
- Failure signatures: Performance collapse typically manifests as: (1) Random guessing behavior (accuracy near theoretical minimum), (2) Generation of meaningless symbols or repetitive text, (3) Loss of specific emergent abilities like mathematical reasoning while retaining others
- First 3 experiments:
  1. Replicate Figure 2 by quantizing LLaMA2-7B and LLaMA2-70B to W3, W4, and W8 on a single task (e.g., LAMBADA) to observe size-dependent quantization tolerance
  2. Apply AWQ to W3 LLaMA2-7B on GSM8K to verify the claimed 3% performance improvement over RTN
  3. Test KV Cache quantization on LongChat with 16K tokens to reproduce the sensitivity findings from Figure 28

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different quantization methods (AWQ, SmoothQuant, RTN) compare in terms of recovering performance for various task types (basic NLP, emergent abilities, trustworthiness, dialogue, long-context) and model sizes?
- Basis in paper: [explicit] The paper evaluates the applicability of state-of-the-art quantization methods (AWQ, SmoothQuant) on various tasks and models, showing that their effectiveness varies depending on the task type, model size, and quantization bit-width.
- Why unresolved: The paper provides a general overview of the performance of different quantization methods, but does not offer a comprehensive comparison across all task types and model sizes.

### Open Question 2
- Question: How do the effects of quantization on model performance vary across different hardware platforms and inference frameworks?
- Basis in paper: [inferred] The paper focuses on evaluating the impact of quantization on model performance, but does not explicitly discuss the effects of different hardware platforms and inference frameworks.
- Why unresolved: The paper does not provide a detailed analysis of how quantization performance varies across different hardware platforms and inference frameworks.

### Open Question 3
- Question: How does the sensitivity of quantized models to different tensor types (Weight, Activation, KV Cache) vary across different model architectures (e.g., transformer-based, Mamba, other architectures)?
- Basis in paper: [explicit] The paper evaluates the effects of quantization on different tensor types for various model architectures, including transformer-based models (OPT, LLaMA2, etc.) and Mamba.
- Why unresolved: While the paper provides insights into the sensitivity of different tensor types for specific model architectures, it does not offer a comprehensive analysis of how this sensitivity varies across a broader range of model architectures.

## Limitations

- The study primarily focuses on uniform quantization schemes without exploring non-uniform or mixed-precision approaches
- The evaluation framework does not account for potential hardware-specific optimizations or distributed inference scenarios
- Layer-specific quantization recommendations require further validation due to limited analysis of inter-layer dependencies

## Confidence

- High confidence: Weight-only quantization findings (W4 for most tasks, W8 for trustworthiness) are well-supported by consistent patterns across model families
- Medium confidence: Activation and KV Cache quantization recommendations (A8, KV8) are robust but may vary with different quantization algorithms
- Low confidence: Layer-specific quantization recommendations require further validation due to limited analysis of inter-layer dependencies and the overhead of implementing heterogeneous bit-widths

## Next Checks

1. **Outlier Sensitivity Analysis**: Systematically inject controlled outliers into weight/activation distributions to quantify their impact on quantization performance and validate the kurtosis-based explanations

2. **Alternative Quantization Algorithms**: Compare results using adaptive quantization methods (e.g., GPTQ, LLM.int8) to determine if the observed sensitivities are algorithm-dependent or fundamental to the data distributions

3. **Hardware-Aware Evaluation**: Test quantized models on target inference hardware (CPU, GPU, NPU) to validate the claimed memory and computational benefits against real-world constraints and overheads