---
ver: rpa2
title: Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning
arxiv_id: '2405.00746'
source_url: https://arxiv.org/abs/2405.00746
tags:
- learning
- reward
- feedback
- conference
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing human feedback in
  human-in-the-loop reinforcement learning by leveraging sub-optimal data. The proposed
  Sub-optimal Data Pre-training (SDP) method pseudo-labels sub-optimal trajectories
  with minimum environment rewards and uses them to pre-train the reward model, providing
  a learning head start.
---

# Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.00746
- Source URL: https://arxiv.org/abs/2405.00746
- Authors: Calarina Muslimani; Matthew E. Taylor
- Reference count: 40
- Reduces human feedback needed in human-in-the-loop RL by pre-training reward models on sub-optimal data

## Executive Summary
This paper addresses the challenge of reducing human feedback in human-in-the-loop reinforcement learning by leveraging sub-optimal data. The proposed Sub-optimal Data Pre-training (SDP) method pseudo-labels sub-optimal trajectories with minimum environment rewards and uses them to pre-train the reward model, providing a learning head start. SDP also initializes the agent's replay buffer with this data to generate new behaviors for more efficient human feedback. Extensive experiments across multiple simulated environments and a human user study (16 participants) demonstrate that SDP significantly improves both preference- and scalar-based RL algorithms' feedback efficiency, often achieving comparable performance to oracle methods while requiring substantially less human interaction.

## Method Summary
SDP works by first collecting sub-optimal state-action transitions using a random policy or partially trained policies. These transitions are pseudo-labeled with the minimum environment reward to indicate poor quality. The reward model is then pre-trained on this labeled data using mean squared error loss, teaching it to assign low rewards to poor behaviors before any human feedback is collected. Simultaneously, the RL agent's replay buffer is initialized with the sub-optimal data, and the agent undergoes an update phase to generate new, diverse behaviors for human feedback. After this pre-training phase, the standard human-in-the-loop RL process continues with fine-tuning the reward model using actual human preferences or scalar ratings.

## Key Results
- SDP improves both preference-based (PEBBLE, RUNE, SURF, MRN) and scalar-based (R-PEBBLE) RL algorithms' feedback efficiency
- Across multiple environments, SDP reduces the number of human feedback instances needed by 40-60% to achieve comparable performance
- In human user study with 16 participants, SDP consistently required fewer preference comparisons than baseline methods
- SDP achieves comparable performance to oracle methods that use ground truth rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training reward model on sub-optimal data gives it a head start by learning to assign low rewards to poor behaviors.
- Mechanism: Pseudo-labeling sub-optimal transitions with minimum reward and using supervised regression teaches the reward model the mapping between poor state-action pairs and low rewards before any human feedback.
- Core assumption: The minimum environment reward is a reasonable proxy for "badness" of sub-optimal transitions.
- Evidence anchors:
  - [abstract] "we start by pseudo-labeling all low-quality data with the minimum environment reward"
  - [section 4] "we obtain reward labels to pre-train our reward model without requiring human labeling or preferences"
- Break condition: If sub-optimal data contains transitions that are actually close to optimal (e.g., due to exploration noise), the pre-training could teach the model to assign low rewards to good behaviors.

### Mechanism 2
- Claim: Initializing the RL agent's replay buffer with sub-optimal data changes the policy distribution early, leading to more diverse behaviors for human feedback.
- Mechanism: The agent starts learning from sub-optimal data, which shifts its policy away from random behavior and generates new state-action trajectories that are more informative for human preferences.
- Core assumption: The agent update phase sufficiently changes the policy so that the new behaviors are meaningfully different from the original sub-optimal data.
- Evidence anchors:
  - [section 4] "we initialize the RL agent's replay buffer with the sub-optimal data and make learning updates to the RL agent"
  - [section 4] "This process changes the RL agent's policy and provides different behaviors for the human to provide feedback on"
- Break condition: If the agent update phase is too weak or the sub-optimal data dominates, the policy might not diverge enough to generate new behaviors.

### Mechanism 3
- Claim: SDP reduces variance in reward model learning by increasing the amount of data used, offsetting bias from incorrect pseudo-labels.
- Mechanism: Using more sub-optimal data (even with incorrect labels) provides more samples for the reward model, which decreases variance and stabilizes learning.
- Core assumption: The bias introduced by pseudo-labeling with minimum reward is small compared to the variance reduction from more data.
- Evidence anchors:
  - [section 4] "by using the sub-optimal transitions, we increase the overall amount of data used by both models, which can decrease the models' variance"
  - [section 4] "the bias for using an incorrect reward is low"
- Break condition: If the pseudo-labeling error is large (e.g., sub-optimal data is actually close to optimal), the bias could dominate and harm learning.

## Foundational Learning

- Concept: Markov Decision Process (MDP) framework
  - Why needed here: The paper builds on standard RL theory where agents maximize expected reward in an MDP; understanding states, actions, transitions, and rewards is essential.
  - Quick check question: In an MDP, what does the transition function T(s,a) represent?

- Concept: Human-in-the-loop RL and preference-based learning
  - Why needed here: SDP is designed to improve human-in-the-loop RL methods; knowing how human feedback (preferences or scalar ratings) is used to learn reward functions is key.
  - Quick check question: How does a preference-based RL algorithm use human pairwise comparisons to learn a reward function?

- Concept: Supervised learning for reward modeling
  - Why needed here: The reward model is trained via regression (MSE) or classification (cross-entropy) on labeled data; understanding this is crucial for seeing how pre-training works.
  - Quick check question: What loss function is used to train a reward model from human scalar ratings?

## Architecture Onboarding

- Component map:
  - Sub-optimal data collection (random policy or other)
  - Reward model (neural network) with two phases:
    - Pre-training on pseudo-labeled sub-optimal data (MSE loss)
    - Fine-tuning on human feedback (MSE or cross-entropy)
  - RL agent (SAC-based) with replay buffer:
    - Initialized with sub-optimal data
    - Updated with new transitions from policy
  - Human feedback interface (simulated or real) providing preferences or scalar ratings

- Critical path:
  1. Collect sub-optimal data
  2. Pseudo-label with minimum reward
  3. Pre-train reward model on this data
  4. Initialize agent replay buffer with sub-optimal data
  5. Run agent update phase (environment interaction + learning)
  6. Collect human feedback on new transitions
  7. Fine-tune reward model with human feedback
  8. Continue RL training with updated reward model

- Design tradeoffs:
  - Amount of sub-optimal data vs. computational cost
  - Quality of sub-optimal data (closer to random vs. partially trained policies)
  - Choice of minimum reward vs. other pseudo-labels
  - Frequency of agent updates vs. feedback collection

- Failure signatures:
  - Reward model never learns to assign low rewards (pre-training ineffective)
  - Agent policy doesn't change after initialization (buffer dominance)
  - Human feedback becomes redundant (no new behaviors generated)
  - Learning slows or diverges (bias from incorrect pseudo-labels too high)

- First 3 experiments:
  1. Run SDP + PEBBLE on Walker-walk with simulated teacher, compare to PEBBLE baseline
  2. Test SDP with different amounts of sub-optimal data (5k, 15k, 50k) in Walker-walk
  3. Validate that removing the agent update phase degrades performance in Walker-walk

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDP perform with high-quality data instead of sub-optimal data?
- Basis in paper: [explicit] The paper explicitly tests this in Section D.7, showing that using high-quality data significantly hurts performance.
- Why unresolved: The paper only tests one type of high-quality data (fully trained RL agent policy). Other forms of high-quality data might yield different results.
- What evidence would resolve it: Experiments testing SDP with various types of high-quality data (expert demonstrations, near-optimal trajectories) would clarify the boundaries of SDP's effectiveness.

### Open Question 2
- Question: Can SDP be extended to work with offline RL algorithms beyond those tested?
- Basis in paper: [inferred] The paper mentions that SDP is simple enough to be used with "any off-the-shelf human-in-the-loop RL algorithm" but only tests it with specific preference and scalar-based algorithms.
- Why unresolved: The paper's experiments are limited to four preference-based and one scalar-based RL algorithm.
- What evidence would resolve it: Empirical testing of SDP with a broader range of offline RL algorithms would demonstrate its generalizability.

### Open Question 3
- Question: How sensitive is SDP to the choice of pseudo-label (rmin) for sub-optimal data?
- Basis in paper: [inferred] The paper uses rmin consistently but doesn't explore the effects of using different pseudo-labels or ranges.
- Why unresolved: The paper assumes rmin is optimal without testing alternatives.
- What evidence would resolve it: Experiments varying the pseudo-label values and measuring their impact on performance would identify optimal labeling strategies.

### Open Question 4
- Question: Does SDP's performance scale with the complexity of the task?
- Basis in paper: [explicit] The paper tests SDP across multiple environments but doesn't systematically analyze performance relative to task complexity.
- Why unresolved: The paper provides results across different environments but doesn't quantify or analyze the relationship between task complexity and SDP effectiveness.
- What evidence would resolve it: A systematic study varying task complexity (state/action space dimensions, reward function difficulty) while measuring SDP performance would reveal scalability limits.

## Limitations

- All experiments use simulated human feedback rather than real human preferences, limiting ecological validity
- Human user study has only 16 participants, which may not capture full variability in human feedback patterns
- The sub-optimal data collection method (random policy) may not reflect how practitioners would obtain such data in real applications

## Confidence

**Confidence: Medium** for the overall claim that SDP improves feedback efficiency. The paper shows consistent improvements across multiple algorithms and environments, but several factors limit generalizability: (1) all experiments use simulated human feedback rather than real human preferences, (2) the human user study has only 16 participants which may not capture full variability in human feedback patterns, (3) the sub-optimal data collection method (random policy) may not reflect how practitioners would obtain such data in real applications.

**Confidence: Low** for the mechanism claiming that reward model bias from incorrect pseudo-labels is "low." The paper asserts this but provides no quantitative analysis of how much the minimum reward mislabels transitions that are actually close to optimal. In environments where exploration yields occasionally good transitions, this could introduce significant bias.

**Confidence: High** for the technical implementation details and experimental methodology, as the paper provides sufficient specifications for reproduction, though some SAC hyperparameters remain unspecified.

## Next Checks

1. **Real human feedback validation**: Conduct experiments using actual human participants providing pairwise preferences across multiple environments, measuring not just final performance but also time/cost per feedback instance.

2. **Bias-variance tradeoff analysis**: Systematically vary the quality of sub-optimal data (from random to partially trained policies) and measure how this affects both reward model accuracy and final task performance to quantify the bias introduced by pseudo-labeling.

3. **Scalability test**: Apply SDP to more complex, high-dimensional environments (e.g., robotic manipulation tasks with vision inputs) to evaluate whether the method maintains its efficiency gains when the state space becomes significantly larger.