---
ver: rpa2
title: 'Absolute abstraction: a renormalisation group approach'
arxiv_id: '2407.01656'
source_url: https://arxiv.org/abs/2407.01656
tags:
- data
- representation
- which
- features
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that absolute abstraction emerges when increasing
  depth in neural networks is combined with expanding the breadth of the training
  data. The authors frame this process within a renormalization group (RG) approach,
  where a representation is transformed to encompass broader data by sacrificing low-level
  details while introducing high-level features.
---

# Absolute abstraction: a renormalisation group approach

## Quick Facts
- arXiv ID: 2407.01656
- Source URL: https://arxiv.org/abs/2407.01656
- Authors: Carlo Orientale Caputo; Elias Seiffert; Enrico Frausin; Matteo Marsili
- Reference count: 40
- Key outcome: Absolute abstraction emerges when increasing depth in neural networks is combined with expanding the breadth of training data, approaching the Hierarchical Feature Model (HFM) as the unique fixed point of a renormalization group transformation.

## Executive Summary
This paper proposes a renormalization group (RG) approach to understanding how neural networks develop increasingly abstract representations. The authors argue that when depth and breadth of training data increase together, internal representations converge toward a universal "Hierarchical Feature Model" (HFM) that represents absolutely abstract features. The HFM is derived as the unique fixed point of an RG transformation that coarse-grains low-level details while introducing higher-level features. Numerical experiments using Deep Belief Networks and auto-encoders trained on progressively broader datasets show that internal representations approach the HFM as both depth and breadth increase, though the representations remain far from the HFM due to limited network capacity.

## Method Summary
The authors use renormalization group theory to model how neural network representations transform when expanding from narrow to broader data domains. They define an RG transformation that coarse-grains low-level details while introducing higher-level features, with the HFM as its unique fixed point. The HFM is a maximum entropy model determined by the average level of detail in features. Experiments use Deep Belief Networks (DBNs) with layer-wise training on progressively broader datasets (MNIST digits 0-1, then up to 10 digits, EMNIST letters, Fashion-MNIST, and CIFAR-10) and auto-encoders with varying depth and bottleneck dimensions. The approach is evaluated by measuring the Kullback-Leibler divergence between internal network representations and the HFM.

## Key Results
- Internal representations in neural networks approach the Hierarchical Feature Model as both depth and breadth of training data increase
- The Kullback-Leibler divergence between internal representations and the HFM generally decreases with increasing depth and breadth
- The order in which datasets are learned matters, with more similar datasets leading to smoother convergence to the HFM
- Representations remain far from the HFM due to limited expressive capacity of the networks studied

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal representations emerge when depth and breadth are increased together.
- Mechanism: RG transformations coarse-grain low-level details while introducing higher-level features that capture the structure of the expanded data domain.
- Core assumption: The transformation preserves the coding cost (entropy) while adapting the internal representation.
- Evidence anchors:
  - [abstract] "representations in neural networks approach the Hierarchical Feature Model as the data get broader and as depth increases"
  - [section] "The representationp(s) of an internal layer of a learning machine depends on its depth but also on the data used in training"
  - [corpus] Weak - no direct neighbor citations about RG + breadth combinations
- Break condition: If the representation capacity is exceeded, the approach diverges rather than converges to the fixed point.

### Mechanism 2
- Claim: The Hierarchical Feature Model (HFM) is the unique fixed point of the RG transformation.
- Mechanism: The HFM is a maximum entropy model determined solely by the average level of detail in features, satisfying the principle of maximal relevance.
- Core assumption: Features are organized hierarchically and the occurrence of a feature at level k does not provide information about lower-order features.
- Evidence anchors:
  - [abstract] "We take the unique fixed point of this transformation — the Hierarchical Feature Model — as a candidate for a representation which is absolutely abstract"
  - [section] "The HFM is a maximum entropy model fully determined by a single sufficient statistics, which is the average level of detail of the features"
  - [corpus] Weak - no direct neighbor citations about HFM as fixed point
- Break condition: If the data structure violates hierarchical organization assumptions, the HFM may not be the appropriate fixed point.

### Mechanism 3
- Claim: The principle of maximal relevance predicts that the number of states with coding cost E should increase exponentially in E.
- Mechanism: Representations where coding costs are distributed uniformly should be promoted because they maximize discrimination ability through statistics alone.
- Core assumption: The coding cost -log2 p(s) should be as broadly distributed as possible.
- Evidence anchors:
  - [section] "The principle of maximal relevance [9] postulates that maximally informative representations should achieve a maximal value ofH[E]"
  - [section] "The HFM arises as the ideal abstract representation because it satisfies the principle of maximal relevance"
  - [corpus] Weak - no direct neighbor citations about maximal relevance principle
- Break condition: If the data has very specific structure that requires non-uniform coding cost distribution, this principle may not hold.

## Foundational Learning

- Concept: Renormalization Group (RG) theory
  - Why needed here: Provides the mathematical framework for understanding how representations change when zooming out to broader data domains
  - Quick check question: What are the two main components of an RG transformation in statistical physics?

- Concept: Maximum entropy principle
  - Why needed here: Determines the form of the HFM as the most probable distribution given constraints on average feature detail
  - Quick check question: How does maximizing entropy relate to making minimal assumptions about the data?

- Concept: Sufficient statistics
  - Why needed here: The level of detail ms is the only sufficient statistic for the HFM, meaning knowledge of its distribution determines the full distribution
  - Quick check question: Why is having a single sufficient statistic advantageous for abstraction?

## Architecture Onboarding

- Component map:
  Deep Belief Networks (DBNs) with stacked Restricted Boltzmann Machines -> Auto-encoders with encoder-decoder symmetry around bottleneck layer

- Critical path:
  1. Train first layer on raw data
  2. Propagate data through trained layers to create training sets for subsequent layers
  3. Repeat layer-wise training until desired depth is reached
  4. Measure KL divergence from HFM at various depths and breadths

- Design tradeoffs:
  - Depth vs. breadth: Both needed for approaching HFM, but too much breadth can cause collapse
  - Training time: Long training required for convergence, but excessive training leads to mode collapse
  - Representation capacity: Limited by number of nodes and bits available

- Failure signatures:
  - KL divergence increases with additional breadth (beyond optimal point)
  - Mode collapse to few states with excessive training
  - Poor reconstruction quality in auto-encoders

- First 3 experiments:
  1. Train DBN on MNIST only, measure KL divergence at various depths
  2. Add Fashion-MNIST to DBN training, measure changes in KL divergence
  3. Compare DBN performance when datasets are added in different orders

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hierarchical feature model (HFM) truly represent an absolutely abstract representation when both depth and breadth approach infinity?
- Basis in paper: [explicit] The paper identifies the HFM as the unique fixed point of the renormalization group transformation, suggesting it represents absolute abstraction. However, the authors acknowledge that the representations studied remain far from the HFM due to limited network capacity.
- Why unresolved: The paper's numerical experiments show DBNs and auto-encoders approaching the HFM as depth and breadth increase, but they do not achieve convergence to the HFM. The networks studied have limited expressive capacity, and the breadth of data used in experiments is restricted.
- What evidence would resolve it: Testing significantly deeper networks with much broader datasets, potentially including multimodal data, could determine if representations converge closer to the HFM. Alternatively, proving mathematically that the HFM is the only possible fixed point under infinite depth and breadth conditions would provide theoretical resolution.

### Open Question 2
- Question: Is the Hamming distance metric appropriate for measuring the distance between neural network representations and the HFM?
- Basis in paper: [inferred] The authors use Kullback-Leibler divergence to compare representations, but acknowledge that the Hamming distance might not be the right metric because the natural dynamics in DBNs involve transitions that update multiple binary variables simultaneously.
- Why unresolved: The paper notes that the representation in trained DBNs is characterized by several peaks, suggesting that the Hamming distance between individual states may not capture the true similarity between distributions. The authors propose examining transition matrices between labels instead, but this approach is not fully developed.
- What evidence would resolve it: Developing a more appropriate metric based on the natural dynamics of the network, such as the transition matrix approach mentioned, and demonstrating that it yields different conclusions about the distance to the HFM would resolve this question.

### Open Question 3
- Question: How does the order of learning datasets affect the approach to the HFM?
- Basis in paper: [explicit] The authors observe that the order in which datasets are learned matters, noting that adding datasets more similar to those already learned results in a smoother approach to the HFM.
- Why unresolved: While the paper shows that different learning orders lead to different results, it does not fully explore why this occurs or what the optimal learning order might be. The mechanism by which dataset similarity affects representation formation is not explained.
- What evidence would resolve it: Systematic experiments varying the order of dataset presentation and measuring the resulting representation similarity to the HFM would clarify the relationship between learning order and abstraction. Developing a theoretical framework explaining why dataset similarity affects representation formation would provide deeper understanding.

## Limitations
- Theoretical foundations of HFM as unique fixed point may not hold for all data types
- Numerical experiments show representations remain far from HFM despite theoretical predictions
- Limited network capacity prevents convergence to HFM in practical implementations
- Connection between maximal relevance principle and practical neural network training lacks empirical validation

## Confidence

- High confidence in experimental methodology and KL divergence measurements
- Medium confidence in theoretical framework connecting RG theory to neural network representations
- Low confidence in practical applicability of HFM as universal representation across diverse data domains

## Next Checks

1. Test the RG transformation approach on non-hierarchical data structures (e.g., graphs or time series) to verify the universality of the HFM fixed point
2. Implement larger capacity networks (deeper and wider) to assess whether representations can approach the HFM more closely
3. Compare the RG-based abstraction framework with alternative abstraction methods (e.g., information bottleneck or contrastive learning approaches) on benchmark datasets to evaluate relative performance