---
ver: rpa2
title: 'From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation'
arxiv_id: '2412.05592'
source_url: https://arxiv.org/abs/2412.05592
tags:
- evaluation
- methods
- manipulation
- hyperparameters
- faithfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how hyperparameter flexibility in explainable
  AI (XAI) evaluation can be exploited to manipulate outcomes. The authors introduce
  two manipulation strategies: intra-manipulation, which improves the evaluation score
  of a single XAI method, and inter-manipulation, which jointly manipulates the comparison
  of multiple XAI methods.'
---

# From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation

## Quick Facts
- arXiv ID: 2412.05592
- Source URL: https://arxiv.org/abs/2412.05592
- Reference count: 40
- Primary result: Hyperparameter flexibility in XAI evaluation can be exploited to manipulate outcomes through intra- and inter-manipulation strategies

## Executive Summary
This paper investigates how hyperparameter flexibility in explainable AI (XAI) evaluation can be exploited to manipulate outcomes. The authors introduce two manipulation strategies: intra-manipulation, which improves the evaluation score of a single XAI method, and inter-manipulation, which jointly manipulates the comparison of multiple XAI methods. Using faithfulness evaluation as a case study, they demonstrate that small changes in hyperparameters like perturbation function and partition size can lead to significant shifts in evaluation outcomes across several datasets and models. To mitigate this issue, the authors propose Mean Resilience Rank (MRR), a ranking-based approach that averages rankings across all hyperparameter settings to reduce sensitivity to manipulation.

## Method Summary
The authors define feasible sets of hyperparameters (partition size, perturbation function, normalization) for faithfulness evaluation and implement optimization procedures to manipulate evaluation scores. They introduce two manipulation approaches: intra-manipulation to optimize a single XAI method's score, and inter-manipulation to jointly manipulate comparative evaluations of multiple XAI methods. The Mean Resilience Rank (MRR) mitigation strategy ranks XAI methods across all hyperparameter settings and averages these rankings to provide robustness against manipulation. The study uses MNIST, FashionMNIST, PneumoniaMNIST, and ImageNet datasets with models including LeNet and ResNet18.

## Key Results
- Intra-manipulation improved LRP's faithfulness score from 25.19 to 37.79 on MNIST by changing perturbation function
- Inter-manipulation successfully altered comparative rankings between multiple XAI methods
- MRR reduced sensitivity to hyperparameter manipulation by averaging rankings across all configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperparameter flexibility in XAI evaluation creates exploitable variability that can be used to manipulate outcomes.
- Mechanism: Since XAI lacks ground truth explanations, researchers must select hyperparameters based on normative choices from literature. This introduces flexibility in evaluation that can be optimized to favor specific methods.
- Core assumption: Different hyperparameter settings significantly impact evaluation scores, and there is no objective way to determine which setting is "correct."
- Evidence anchors:
  - [abstract]: "This challenge becomes especially problematic when evaluation methods have numerous hyperparameters that must be specified by the user, as there is no ground truth to determine an optimal hyperparameter selection."
  - [section]: "Tab. 1 illustrates this, where standard XAI methods are compared with only slight changes in hyperparameters but with significant changes in evaluation outcome."
  - [corpus]: Weak evidence - corpus neighbors focus on general XAI challenges but don't directly address hyperparameter manipulation.

### Mechanism 2
- Claim: The inter-manipulation strategy can systematically alter comparative evaluations between multiple XAI methods.
- Mechanism: By optimizing hyperparameters to simultaneously improve one method's score while degrading others, the relative ranking between methods can be controlled.
- Core assumption: The optimization problem can find hyperparameter settings that create favorable comparisons for target methods.
- Evidence anchors:
  - [section]: "Our second manipulation approach is to take a holistic view and manipulate the evaluation of several XAI methods jointly. We refer to this as inter-manipulation and define it as..."
  - [section]: "Tab. 4, Tab. 5, and Tab. 6 show the results of performing the inter-manipulation proposed in Definition 2, where the scores are manipulated towards LRP, Saliency, and KernelSHAP, respectively."
  - [corpus]: Weak evidence - corpus neighbors discuss XAI evaluation but not systematic manipulation strategies.

### Mechanism 3
- Claim: Mean Resilience Rank (MRR) provides robustness against manipulation by averaging rankings across all hyperparameter settings.
- Mechanism: Instead of relying on a single hyperparameter configuration, MRR computes rankings for each configuration and averages them, making it difficult to manipulate through selective hyperparameter choice.
- Core assumption: A good XAI method should perform well across all reasonable hyperparameter settings, not just optimized ones.
- Evidence anchors:
  - [section]: "Therefore, if an XAI method consistently appears among the highest-ranked methods across numerous hyperparameters, it provides an indication of high quality with less sensitivity to hyperparameters."
  - [section]: "Tab. 7 displays the results of this ranking procedure, which shows that the top-performing XAI methods change between datasets."
  - [corpus]: Weak evidence - corpus neighbors don't discuss ranking-based mitigation strategies.

## Foundational Learning

- Concept: Hyperparameter sensitivity in XAI evaluation
  - Why needed here: Understanding how different hyperparameter choices affect evaluation outcomes is central to both the manipulation problem and the proposed solution.
  - Quick check question: What are the three main hyperparameters that must be chosen in faithfulness evaluation?

- Concept: Adversarial attack framing for XAI evaluation
  - Why needed here: The paper frames hyperparameter manipulation as an adversarial attack, which requires understanding both XAI evaluation and adversarial optimization concepts.
  - Quick check question: How does treating hyperparameter selection as an optimization problem enable manipulation?

- Concept: Ranking aggregation methods
  - Why needed here: The MRR solution relies on understanding how to aggregate rankings across multiple evaluation settings.
  - Quick check question: What advantage does averaging rankings across hyperparameters provide compared to selecting a single "best" hyperparameter setting?

## Architecture Onboarding

- Component map: Evaluation function → Hyperparameter space → Manipulation optimization → MRR aggregation → Final ranking
- Critical path: Input → Evaluation with chosen hyperparameters → Score computation → (Manipulation optimization) → (MRR aggregation) → Final ranking
- Design tradeoffs: MRR provides robustness but increases computational cost; manipulation is possible but requires domain expertise to define feasible hyperparameter sets.
- Failure signatures: If all XAI methods have similar scores across all hyperparameter settings, both manipulation and MRR become ineffective.
- First 3 experiments:
  1. Reproduce Table 1 by changing perturbation function from uniform noise to Gaussian blurring and observe score changes
  2. Implement intra-manipulation on a single dataset/method pair and verify score improvement
  3. Apply MRR to the same dataset and verify reduced sensitivity to hyperparameter changes compared to single-setting evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MRR perform when applied to other evaluation families beyond faithfulness, such as robustness or randomization metrics?
- Basis in paper: [inferred] The authors mention in the discussion that MRR could potentially be applied to other evaluation families and suggest expanding the parameter sensitivity analysis to randomization and robustness metrics.
- Why unresolved: The paper only demonstrates MRR on faithfulness evaluation, leaving its effectiveness on other evaluation families unexplored.
- What evidence would resolve it: Experimental results showing MRR applied to robustness and randomization metrics across multiple datasets and explanation methods, comparing performance against baseline manipulation strategies.

### Open Question 2
- Question: What is the optimal strategy for defining the feasible set of hyperparameters to balance between preventing manipulation and maintaining computational efficiency?
- Basis in paper: [explicit] The authors discuss that defining the feasible set requires domain expertise and note that MRR increases computational cost when more methods and hyperparameters are considered.
- Why unresolved: The paper does not provide a systematic method for determining the size and composition of the feasible set, leaving it as a challenge for practitioners.
- What evidence would resolve it: A systematic study examining the trade-off between feasible set size, computational cost, and resistance to manipulation, potentially including automated methods for defining the feasible set.

### Open Question 3
- Question: How can the XAI community develop standardized evaluation practices that prevent manipulation while maintaining comparability across different studies?
- Basis in paper: [explicit] The authors propose creating an open-source database using tools like Quantus and OpenXAI to store and standardize benchmarking results, but acknowledge this is only a preliminary solution.
- Why unresolved: The paper identifies the need for standardized practices but does not provide a complete framework for achieving this goal.
- What evidence would resolve it: Development and implementation of community-wide standards for XAI evaluation, including guidelines for hyperparameter selection, reporting requirements, and tools for ensuring transparency and reproducibility.

## Limitations
- Findings are limited to faithfulness evaluation and may not generalize to other evaluation metrics
- MRR implementation increases computational cost significantly, though this tradeoff is not fully quantified
- Defining "reasonable" hyperparameter sets relies on expert knowledge that may vary between practitioners

## Confidence
- **High Confidence**: The core claim that hyperparameter flexibility enables manipulation is well-supported by empirical evidence across multiple datasets and methods.
- **Medium Confidence**: The effectiveness of MRR as a mitigation strategy is demonstrated but may not be universally applicable to all XAI evaluation scenarios.
- **Low Confidence**: The paper's claim that this represents a fundamental challenge for XAI evaluation could be overstated, as the magnitude of manipulation effects may vary significantly across different XAI methods and evaluation contexts.

## Next Checks
1. **Generalization Test**: Apply the manipulation strategies to additional evaluation metrics (e.g., stability, plausibility) to assess whether the findings extend beyond faithfulness evaluation.

2. **Cost-Benefit Analysis**: Quantify the computational overhead of MRR implementation and evaluate whether the robustness gains justify the increased computational requirements across different dataset sizes.

3. **Expert Agreement Study**: Survey multiple XAI experts to determine the variability in defining "reasonable" hyperparameter sets and assess how this affects the reproducibility of both manipulation and mitigation strategies.