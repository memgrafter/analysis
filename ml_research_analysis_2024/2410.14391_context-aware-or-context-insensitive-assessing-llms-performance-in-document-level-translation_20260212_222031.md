---
ver: rpa2
title: Context-Aware or Context-Insensitive? Assessing LLMs' Performance in Document-Level
  Translation
arxiv_id: '2410.14391'
source_url: https://arxiv.org/abs/2410.14391
tags:
- context
- translation
- llms
- performance
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) utilize
  context in document-level translation. The authors conduct perturbation and attribution
  analyses on nine LLMs from diverse families, comparing them with two encoder-decoder
  baselines.
---

# Context-Aware or Context-Insensitive? Assessing LLMs' Performance in Document-Level Translation

## Quick Facts
- arXiv ID: 2410.14391
- Source URL: https://arxiv.org/abs/2410.14391
- Authors: Wafaa Mohammed; Vlad Niculae
- Reference count: 17
- Primary result: LLMs show robustness to randomized context and low relevant-context attribution scores, suggesting they don't properly utilize context for document-level translation

## Executive Summary
This study investigates how large language models utilize context in document-level translation by comparing nine LLMs from diverse families with two encoder-decoder baselines. The authors conduct perturbation and attribution analyses on IWSLT2017 TED data and ContraPro dataset for pronoun resolution. While translation-finetuned LLMs outperform encoder-decoder models in overall translation quality, they perform worse on discourse phenomena like pronoun resolution. The analysis reveals that LLMs show robustness to randomized context and low relevant-context attribution scores, indicating lack of proper context utilization. The study highlights the need for explicit context-aware fine-tuning of LLMs for document-level translation tasks.

## Method Summary
The authors evaluate nine LLMs (EuroLLM-9B-Inst, Llama-2 variants, Tower models, ALMA, NLLB) and two encoder-decoder baselines on document-level translation tasks. They use three prompt formats (sentence-level baseline, generic, explicit) and test performance under gold, perturbed, and random context conditions. Evaluation metrics include BLEU, COMET, CHRF for translation quality and GPRO/CPRO for pronoun resolution accuracy. Attribution analysis using ALTI-Logit and input-erasure methods measures contribution of relevant context to model predictions. The study uses IWSLT2017 TED data for general translation quality and ContraPro dataset for pronoun resolution evaluation.

## Key Results
- Translation-finetuned LLMs outperform encoder-decoder baselines in overall BLEU scores (28.4-40.8 vs 28.2-38.5)
- LLMs perform worse on discourse phenomena like pronoun resolution (accuracy 11.1-59.9% vs 32.0-76.5%)
- All models show robustness to randomized context with low relevant-context attribution scores
- Document-level prompting improves translation performance across all model types
- Explicit instruction-following prompts help instruction-tuned models while potentially harming strong base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual pretraining combined with instruction tuning creates more robust cross-lingual representations that generalize better to translation tasks than models fine-tuned solely on parallel data.
- Core assumption: The quality and diversity of the multilingual pretraining corpus is sufficient to capture linguistic phenomena needed for translation across multiple language pairs.
- Evidence anchors:
  - Translation-finetuned LLMs outperform encoder-decoder models at overall translation, but perform worse on discourse phenomena.
  - EuroLLM-9B-Inst outperforms all models in both prompting formats, with TowerInstruct 13B second in explicit prompting and TowerBase 13B second in generic format.
- Break condition: If the multilingual pretraining corpus lacks sufficient parallel or comparable data for the target language pairs, or if instruction tuning focuses on tasks too dissimilar from translation.

### Mechanism 2
- Claim: LLMs rely more on sentence-level patterns and general language understanding rather than specific document context, leading to good overall BLEU scores but poor pronoun resolution performance.
- Core assumption: Models' pretraining and fine-tuning objectives prioritize sentence-level coherence over document-level discourse coherence.
- Evidence anchors:
  - All evaluated models show robustness to randomized context, attributed to lack of proper context utilization.
  - Analysis reveals low relevant-context attribution scores, highlighting necessity for explicit context-aware finetuning.
- Break condition: If models actually utilize context but in ways that don't correlate with attribution scores, or if attribution methods are flawed for this specific task.

### Mechanism 3
- Claim: Document-level prompting improves translation performance across all model types, but explicit instruction-following prompts help instruction-tuned models while potentially harming strong base models.
- Core assumption: Models have learned prompt-response patterns during training that match the prompt format used at inference.
- Evidence anchors:
  - Document-level generic prompting improves translation performance of all models over sentence-level baseline.
  - Explicit prompting improves instruction-finetuned models' performance, while strong base-models (such as TowerBase 13B) degrade in performance.
- Break condition: If prompt format effects are actually due to other factors like context window size or model capacity rather than training alignment.

## Foundational Learning

- Concept: Document-level translation vs sentence-level translation
  - Why needed here: The paper explicitly distinguishes between these paradigms and their different requirements for context utilization.
  - Quick check question: What linguistic phenomena require context beyond a single sentence for proper translation?

- Concept: Attribution methods for interpretability
  - Why needed here: The paper uses ALTI-Logit and input-erasure to measure how much relevant context contributes to model predictions.
  - Quick check question: How do attribution methods like input-erasure and integrated gradients differ in what they measure?

- Concept: Perturbation analysis in NLP
  - Why needed here: The paper systematically perturbs context (both structurally and randomly) to assess model sensitivity to contextual information.
  - Quick check question: What does it mean if a model's performance doesn't degrade when context is randomized?

## Architecture Onboarding

- Component map: Model → Prompt format → Context setup (gold/perturbed/random) → Translation generation → Evaluation metrics → Attribution analysis

- Critical path: Models are loaded, prompted with three different formats, run under three context conditions, evaluated on translation quality and pronoun resolution, then analyzed for context attribution.

- Design tradeoffs:
  - Larger context windows improve discourse handling but increase computational cost
  - Fine-tuning on document-level data improves context utilization but may reduce sentence-level performance
  - Attribution methods provide interpretability but are computationally expensive and may disagree

- Failure signatures:
  - Low BLEU scores across all context setups indicate model quality issues
  - Robustness to randomized context with low attribution scores suggests context is not being properly utilized
  - Large performance gaps between prompt formats indicate misalignment between training and inference

- First 3 experiments:
  1. Run sentence-level baseline with gold context on IWSLT2017 to establish reference performance
  2. Compare model performance under gold vs randomized context to identify context utilization issues
  3. Apply attribution analysis on a subset of models using ContraPro data to quantify relevant context contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different context window sizes affect the translation performance of LLMs for document-level translation?
- Basis in paper: The authors note that "Following Mohammed and Niculae (2024), we use a context size of 5 previous source-target pairs. Future work could investigate the impact of context window size on translation performance."
- Why unresolved: The paper does not experiment with varying context window sizes to determine the optimal size for document-level translation.
- What evidence would resolve it: Systematic experiments testing different context window sizes (e.g., 1, 3, 5, 7, 10) and measuring their impact on translation quality metrics (BLEU, COMET) and discourse phenomena accuracy.

### Open Question 2
- Question: What specific architectural modifications or training strategies would improve LLMs' performance on discourse phenomena like pronoun resolution?
- Basis in paper: The authors state "We attribute this to lack of proper context utilization and highlight the need for explicit context-aware finetuning of LLMs to ensure their reliability for document-level translation."
- Why unresolved: The paper identifies the problem but does not propose or test specific solutions for improving discourse phenomena performance.
- What evidence would resolve it: Experiments comparing different fine-tuning approaches (e.g., focused on discourse phenomena, contrastive learning, attention regularization) and measuring their impact on pronoun resolution accuracy and overall translation quality.

### Open Question 3
- Question: How does the choice of finetuning languages affect downstream translation performance for specific language pairs?
- Basis in paper: The authors note "This raises the need for a deeper investigation into how various design choices (such as the selection and number of finetuning languages, the choice of datasets, and the configuration of hyper-parameters) influence downstream performance."
- Why unresolved: The paper does not analyze how the selection of languages used during finetuning affects performance on specific translation directions.
- What evidence would resolve it: Comparative experiments with models finetuned on different language subsets and measuring performance across various translation directions to identify optimal finetuning language combinations.

### Open Question 4
- Question: Why do attribution methods show different patterns for LLMs compared to encoder-decoder models when analyzing context utilization?
- Basis in paper: The authors observe "Unlike the larger differences in relevant context and overall context attributions observed for encoder-decoder models by Mohammed and Niculae (2024), we find no striking differences or clear patterns between the contributions for LLMs."
- Why unresolved: The paper identifies this discrepancy but does not explain the underlying reasons for the different attribution patterns between model architectures.
- What evidence would resolve it: Detailed analysis of attention mechanisms, information flow, and internal representations in both model types to understand why context attribution patterns differ and what this implies about their context utilization strategies.

## Limitations

- Limited information about pretraining corpora and fine-tuning procedures for many evaluated models (Tower, ALMA, NLLB)
- Attribution methods may not perfectly capture context utilization and could disagree
- Focus on only two language pairs (EN→DE and EN→FR) and one discourse phenomenon (pronoun resolution) limits generalizability

## Confidence

- **High confidence**: Translation-finetuned LLMs outperform encoder-decoder baselines in overall BLEU scores (28.4-40.8 vs 28.2-38.5)
- **Medium confidence**: LLMs show robustness to randomized context with low attribution scores, indicating poor context utilization
- **Medium confidence**: Document-level prompting improves translation quality across model types

## Next Checks

1. **Reproduce attribution analysis with alternative methods**: Apply integrated gradients or SHAP values to verify that low context attribution scores persist across different interpretability techniques

2. **Test with extended context windows**: Evaluate models with 10+ previous sentence pairs to determine if larger context improves pronoun resolution accuracy

3. **Cross-linguistic validation**: Apply the same evaluation framework to EN→JA and EN→ZH translation pairs to test generalizability beyond European languages