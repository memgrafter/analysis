---
ver: rpa2
title: 'When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising Recommendation'
arxiv_id: '2409.12730'
source_url: https://arxiv.org/abs/2409.12730
tags:
- denoising
- recommendation
- data
- ensemble
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning user preferences
  from implicit feedback in recommendation systems, where noise in user interactions
  can negatively impact performance. The authors propose Adaptive Ensemble Learning
  (AEL), a novel approach that combines ensemble learning with a sparse gating network
  to adaptively denoise implicit feedback.
---

# When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising Recommendation

## Quick Facts
- arXiv ID: 2409.12730
- Source URL: https://arxiv.org/abs/2409.12730
- Authors: Weipu Chen; Zhuangzhuang He; Fei Liu
- Reference count: 31
- Outperforms state-of-the-art denoising methods on three datasets with Recall@5 of 0.1749 on Adressa

## Executive Summary
This paper addresses the challenge of learning user preferences from implicit feedback in recommendation systems, where noise in user interactions can negatively impact performance. The authors propose Adaptive Ensemble Learning (AEL), a novel approach that combines ensemble learning with a sparse gating network to adaptively denoise implicit feedback. AEL constructs three parent-autoencoders with varying denoising capacities by stacking sub-autoencoders, and uses a sparse gating network to dynamically select the most suitable parent-autoencoders for different input samples.

## Method Summary
AEL employs three parent-autoencoders (Mild, Moderate, Strong Denoising) created by stacking sub-autoencoders with progressively smaller hidden dimensions (128→48→12). The sparse gating network uses Noisy Top-K selection to route each input sample to the two most appropriate experts. A corrupt module partially corrupts inputs to prevent identity function learning. The model is trained with a combined loss including reconstruction error, importance loss for load balancing, and load loss to ensure expert diversity.

## Key Results
- Achieves Recall@5 of 0.1749 and Recall@20 of 0.3199 on Adressa dataset
- Outperforms state-of-the-art denoising recommendation methods across multiple metrics
- Maintains stable performance even on sparse datasets with high sparsity ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AEL adaptively selects sub-recommenders based on input-specific denoising needs, avoiding overfitting to static weights.
- Mechanism: The sparse gating network analyzes historical performance of parent-AEs and dynamically routes each input sample to the most suitable pair of sub-AEs.
- Core assumption: Different implicit feedback samples have varying noise levels, and fixed ensemble weights cannot adapt to this variability.
- Evidence anchors: The sparse gating network utilizes the Noisy Top-K Gating strategy for selecting experts, helping in load balancing and ensuring only suitable experts are activated for each input.

### Mechanism 2
- Claim: Stacking sub-AEs with progressively smaller hidden dimensions creates parent-AEs with different denoising capacities without increasing parameter count proportionally.
- Mechanism: Each sub-AE compresses the input representation, and stacking them compounds this effect.
- Core assumption: The compression ratio between input dimension and hidden dimension directly correlates with denoising strength.
- Evidence anchors: Since the hidden dimension is smaller than the input vector dimension, a sub-AE cannot fully recover its original input after encoding and decoding, performing denoising.

### Mechanism 3
- Claim: The corrupt module improves robustness by preventing sub-AEs from learning the identity function.
- Mechanism: The corrupt module partially corrupts initial input using a mask-out technique, forcing the Large sub-AE to learn meaningful representations.
- Core assumption: Without corruption, autoencoders tend to learn trivial solutions that don't generalize to noisy test data.
- Evidence anchors: Large also utilizes the corrupt module to corrupt the input xu to ˜xu using a mask-out technique.

## Foundational Learning

- Concept: Autoencoder reconstruction error as a denoising signal
  - Why needed here: Understanding that smaller hidden dimensions force the model to compress information, which inherently filters noise during reconstruction
  - Quick check question: Why does reducing hidden dimension from 128 to 12 increase denoising capacity?

- Concept: Mixture-of-Experts (MoE) gating mechanisms
  - Why needed here: The sparse gating network is a variant of MoE that uses noisy top-k selection to route inputs to appropriate experts
  - Quick check question: How does adding Gaussian noise before softmax help with load balancing in MoE?

- Concept: Implicit feedback noise characteristics
  - Why needed here: Understanding that implicit feedback contains both false positives (accidental clicks) and false negatives (missed interactions) that require different denoising approaches
  - Quick check question: What's the difference between accidentally touching an item and genuinely disliking it in implicit feedback?

## Architecture Onboarding

- Component map: Input → Corrupt module (optional for Large sub-AE) → Three sub-AEs (Large/Medium/Small) → Parent-AEs (Mild/Moderate/Strong) → Sparse gating network → Weighted output aggregation
- Critical path: Input → Corrupt module → Sub-AE encoding → Parent-AE selection → Output aggregation
- Design tradeoffs: Model complexity vs. denoising capacity, number of experts vs. gating accuracy, corruption rate vs. information loss
- Failure signatures: All gating weights converge to one parent-AE, performance worse than single autoencoder, high variance across runs
- First 3 experiments: 1) Test individual parent-AEs on datasets with varying noise levels, 2) Evaluate sparse gating network's routing decisions on validation data, 3) Compare full AEL against simple averaging ensemble

## Open Questions the Paper Calls Out

- Open Question 1: How does AEL's performance scale with increasingly sparse datasets beyond those tested?
- Open Question 2: What is the computational overhead of AEL compared to single-model approaches in real-time recommendation systems?
- Open Question 3: How does AEL handle cold-start scenarios for new users or items?

## Limitations
- Sparse gating network dynamics lack detailed analysis of routing decision evolution during training
- Architectural scalability may face challenges with larger embedding dimensions or more than three denoising levels
- Effectiveness relies on implicit feedback noise being heterogeneous across samples without empirical validation

## Confidence
- High confidence: The core mechanism of using stacked autoencoders with progressively smaller hidden dimensions to create parent-AEs with different denoising capacities
- Medium confidence: The adaptive gating approach improves over static ensemble weighting
- Medium confidence: The corrupt module improves robustness

## Next Checks
1. Conduct an ablation study removing the sparse gating network to determine whether adaptive selection provides significant benefits over simple averaging of parent-AEs
2. Test AEL's performance on datasets with known noise distributions to verify that the gating network correctly routes inputs to appropriate denoising levels
3. Evaluate model stability across multiple training runs with different random seeds to assess sensitivity to initialization