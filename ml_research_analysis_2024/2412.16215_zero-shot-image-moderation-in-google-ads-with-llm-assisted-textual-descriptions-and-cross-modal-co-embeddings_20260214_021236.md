---
ver: rpa2
title: Zero-Shot Image Moderation in Google Ads with LLM-Assisted Textual Descriptions
  and Cross-modal Co-embeddings
arxiv_id: '2412.16215'
source_url: https://arxiv.org/abs/2412.16215
tags:
- descriptions
- policy
- textual
- google
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a scalable, zero-shot approach for moderating
  ads images at Google that addresses the challenge of handling massive volumes of
  diverse content with evolving policies. The method uses human-curated textual descriptions
  of policy guidelines, enhanced by large language models (LLMs), and cross-modal
  text-image co-embeddings to detect policy violations without requiring extensive
  supervised training data.
---

# Zero-Shot Image Moderation in Google Ads with LLM-Assisted Textual Descriptions and Cross-modal Co-embeddings

## Quick Facts
- arXiv ID: 2412.16215
- Source URL: https://arxiv.org/abs/2412.16215
- Reference count: 3
- Achieved 90.8% precision and 107.3% incremental coverage significance for tobacco image moderation

## Executive Summary
This work presents a scalable, zero-shot approach for moderating ads images at Google that addresses the challenge of handling massive volumes of diverse content with evolving policies. The method uses human-curated textual descriptions of policy guidelines, enhanced by large language models (LLMs), and cross-modal text-image co-embeddings to detect policy violations without requiring extensive supervised training data. During inference, co-embedding similarity between incoming images and textual descriptions enables efficient policy violation detection, with an LLM providing nuanced analysis for ambiguous cases.

## Method Summary
The proposed method utilizes human-curated textual descriptions and cross-modal text-image co-embeddings to enable zero-shot classification of policy violating ads images. During inference, co-embedding similarity between incoming images and the textual descriptions serves as a reliable signal for policy violation detection. To address the challenge of generating high-quality textual descriptions, the approach leverages LLMs for generation and refinement, with domain experts validating the descriptions through a two-tier process. The method can be used for multiple policies, including drugs, alcohol, weapons, and adult content, without requiring additional training data.

## Key Results
- Achieved 90.8% precision and 107.3% incremental coverage significance compared to baseline binary classification model
- Removed millions of violating ads with high precision
- Demonstrated effectiveness for tobacco image moderation as a proof of concept

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal co-embeddings enable zero-shot classification by capturing semantic relationships between images and textual descriptions.
- Mechanism: Textual descriptions are embedded into the same semantic space as images using cross-modal co-embedding models. Policy violation detection is then performed by comparing the similarity of incoming images to these textual descriptions.
- Core assumption: The cross-modal co-embedding model can meaningfully represent both text and images in a shared semantic space where semantically similar items are close together.
- Evidence anchors:
  - [abstract] "The proposed method utilizes human-curated textual descriptions and cross-modal text-image co-embeddings to enable zero-shot classification of policy violating ads images"
  - [section] "During inference, co-embedding similarity between incoming images and the textual descriptions serves as a reliable signal for policy violation detection"
  - [corpus] No direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: LLM-assisted generation and refinement of textual descriptions creates comprehensive coverage of policy space.
- Mechanism: LLMs generate candidate textual descriptions from policy language, user expertise, or human-labeled violating images. Users then refine these descriptions to ensure accuracy and coverage of all violation modes.
- Core assumption: LLMs can generate diverse and relevant textual descriptions that capture different aspects of a policy when given appropriate guidance.
- Evidence anchors:
  - [section] "We propose a user-centric approach where domain experts, with the assistance of LLMs, create detailed textual descriptions that encompass the policy space"
  - [section] "Users can provide policy language, their own expertise, or guidance from a subject matter expert to craft descriptions. They can also leverage an LLM to uncover previously unrecorded 'blind spots'"
  - [corpus] No direct corpus evidence for this specific mechanism

### Mechanism 3
- Claim: The two-tier validation process ensures high-quality textual descriptions that align with policy guidelines.
- Mechanism: First, users label textual descriptions as "in-scope" or "out-of-scope" by reviewing related images. Second, descriptions are validated against a corpus of known ads with ground-truth labels, removing problematic descriptions that frequently match out-of-scope images or fail to match violating images.
- Core assumption: Visual context and ground-truth labeled data can effectively identify descriptions that are misaligned with the policy.
- Evidence anchors:
  - [section] "Our system presents users with a selection of closely related images retrieved from existing datasets for each textual description. Leveraging their domain expertise and the visual context provided, users evaluate and label each description as either 'in-scope' or 'out-of-scope'"
  - [section] "The labeled descriptions are matched against a corpus of known ads images with ground-truth labels for the given policy. In-scope descriptions that frequently match out-of-scope images are flagged as potentially problematic and subsequently removed"
  - [corpus] No direct corpus evidence for this specific mechanism

## Foundational Learning

- Concept: Cross-modal embeddings and similarity search
  - Why needed here: Understanding how text and images can be represented in a shared semantic space and compared using similarity metrics is fundamental to this approach
  - Quick check question: How do cross-modal embedding models like CLIP or similar approaches enable comparison between text and images in the same semantic space?

- Concept: Large language model capabilities and limitations
  - Why needed here: The approach relies on LLMs for generating textual descriptions and potentially for nuanced analysis of ambiguous cases, so understanding their strengths and weaknesses is crucial
  - Quick check question: What are the key differences between using an LLM for generating textual descriptions versus using it for nuanced policy analysis of ambiguous images?

- Concept: Zero-shot learning vs. few-shot learning vs. supervised learning
  - Why needed here: This approach is specifically designed to avoid the need for extensive supervised training data, so understanding the trade-offs between different learning paradigms is important
  - Quick check question: What are the main advantages and disadvantages of zero-shot learning approaches compared to supervised learning when dealing with policy violation detection?

## Architecture Onboarding

- Component map:
  - LLM-assisted description generation module
  - User validation interface for labeling descriptions
  - Cross-modal co-embedding model
  - Approximate kNN search system
  - Automated decision engine
  - Fine-tuned LLM for nuanced analysis
  - Human review escalation system
  - Visual similarity propagation system

- Critical path: Image → Cross-modal embedding → kNN similarity search → Automated decision (in/out) → LLM review (if ambiguous) → Human review (if LLM low confidence)

- Design tradeoffs:
  - Precision vs. coverage: Stricter matching thresholds increase precision but reduce coverage
  - LLM complexity vs. latency: More sophisticated LLMs provide better nuanced analysis but increase processing time
  - Human review vs. automation: More human review increases accuracy but reduces scalability

- Failure signatures:
  - High false positive rate: Indicates poor quality of textual descriptions or overly strict matching thresholds
  - High false negative rate: Suggests missing descriptions or inadequate coverage of policy space
  - LLM review bottleneck: Too many images requiring nuanced analysis, indicating the automated decision thresholds may be too conservative

- First 3 experiments:
  1. Generate textual descriptions for a simple policy (e.g., tobacco) and manually validate their quality by checking if they capture different violation modes
  2. Test the co-embedding similarity matching by embedding a small set of known violating and non-violating images and verifying they match the correct descriptions
  3. Run the complete pipeline on a small dataset with ground truth labels to measure precision, recall, and identify failure modes before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed zero-shot approach compare to supervised models on other policy domains beyond tobacco images?
- Basis in paper: [inferred] The paper only provides results for tobacco images, stating "As an example, we conducted experiments on tobacco images to evaluate our approach's ability to detect tobacco-related image content." It would be valuable to understand if similar performance gains are observed across different policy domains.
- Why unresolved: The evaluation is limited to a single policy domain (tobacco), and the paper does not discuss performance on other policies. The authors mention the approach "can be used for multiple policies" but do not provide comparative results.
- What evidence would resolve it: Comparative evaluation results showing precision, coverage significance, and relative recall for the zero-shot approach versus supervised models across multiple policy domains (e.g., drugs, weapons, adult content).

### Open Question 2
- Question: What is the optimal balance between automated decision thresholds and LLM/human review to maximize efficiency while maintaining high precision?
- Basis in paper: [explicit] The paper mentions "If the image matches more in-scope descriptions than out-of-scope descriptions by a predetermined margin, it is automatically flagged" and discusses a multi-stage approach with LLM review and human review. However, it doesn't specify how these thresholds were determined or if they were optimized.
- Why unresolved: The paper states the system uses predetermined margins for automated decisions but doesn't explain how these thresholds were set or whether they were optimized through experimentation.
- What evidence would resolve it: An ablation study or sensitivity analysis showing how different threshold values affect precision, recall, and system efficiency, along with guidance on optimal threshold selection.

### Open Question 3
- Question: How does the quality and comprehensiveness of textual descriptions affect the performance of the zero-shot classification model?
- Basis in paper: [explicit] The paper emphasizes the importance of textual description generation and validation, stating "some might inadvertently misalign with the given policy" and describing a two-step validation process. However, it doesn't quantify how description quality impacts model performance.
- Why unresolved: While the paper describes the process for generating and validating textual descriptions, it doesn't empirically demonstrate how variations in description quality (e.g., number of descriptions, their specificity, or alignment accuracy) affect classification performance.
- What evidence would resolve it: Experiments varying the quality and quantity of textual descriptions (e.g., using different numbers of descriptions, descriptions with varying levels of specificity, or descriptions with known alignment issues) and measuring the resulting impact on precision and coverage.

## Limitations

- Performance on other policy domains remains unverified as evaluation focused solely on tobacco images
- Reliance on human-curated textual descriptions may create scaling bottlenecks for Google's extensive policy framework
- Cross-modal co-embedding effectiveness may degrade when handling diverse image types beyond the tested tobacco category

## Confidence

**High Confidence Claims:**
- The overall methodology framework (LLM-assisted descriptions + cross-modal embeddings) is technically sound and represents a valid approach to zero-shot image moderation
- The two-tier validation process for textual descriptions is a reasonable quality control mechanism
- The tobacco evaluation results (90.8% precision, 107.3% incremental coverage) are credible given the controlled experimental setup

**Medium Confidence Claims:**
- The approach's effectiveness across diverse policy domains beyond tobacco
- The claimed scalability and resource efficiency advantages over supervised learning methods
- The LLM's ability to handle nuanced analysis of ambiguous cases reliably

**Low Confidence Claims:**
- The method's robustness against adversarial image manipulation
- The long-term sustainability of the human-in-the-loop validation process for large-scale deployment
- The approach's effectiveness for culturally specific or context-dependent policy violations

## Next Checks

1. **Cross-Policy Generalization Test**: Apply the methodology to three additional policy domains (e.g., alcohol, pharmaceuticals, and adult content) and measure precision, recall, and coverage to validate claims about general applicability beyond tobacco images.

2. **Adversarial Robustness Evaluation**: Test the system's vulnerability to common image manipulation techniques (compression, cropping, color filtering, adversarial perturbations) to assess robustness claims and identify potential failure modes.

3. **Scalability and Resource Efficiency Analysis**: Conduct a controlled comparison measuring computational resources, human review hours, and time-to-deployment against a comparable supervised learning approach trained on the same policy domains to validate resource efficiency claims.