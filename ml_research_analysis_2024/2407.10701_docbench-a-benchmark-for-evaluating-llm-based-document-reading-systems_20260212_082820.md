---
ver: rpa2
title: 'DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems'
arxiv_id: '2407.10701'
source_url: https://arxiv.org/abs/2407.10701
tags:
- document
- systems
- arxiv
- questions
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DOCBENCH, a new benchmark for evaluating LLM-based
  document reading systems. The authors address the challenge of evaluating these
  systems, which require complex processing skills beyond simple text input-output
  interactions.
---

# DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems

## Quick Facts
- arXiv ID: 2407.10701
- Source URL: https://arxiv.org/abs/2407.10701
- Reference count: 40
- One-line primary result: Introduces DOCBENCH, a benchmark revealing significant performance gaps between current LLM-based document reading systems and human performance across real-world documents and questions.

## Executive Summary
DOCBENCH is a new benchmark designed to evaluate large language model-based document reading systems that process raw PDF files and questions to generate textual answers. The benchmark includes 229 real documents and 1,102 questions across five domains (Academia, Finance, Government, Laws, News) and four question types (text-only, multimodal, metadata, unanswerable). The authors evaluate both proprietary systems (GPT-4, Claude-3, etc.) and open-source parse-then-read pipelines, revealing noticeable gaps between existing systems and human performance. The benchmark highlights current limitations in document reading capabilities and provides a standardized way to assess these systems in real-world scenarios.

## Method Summary
DOCBENCH is constructed through a three-phase pipeline: document collection from real sources across five domains, QA-pair generation using both GPT-4 and human annotators to ensure diversity, and quality checking involving automatic filtering followed by manual review from domain experts. The evaluation uses GPT-4 as an automatic evaluator with binary scoring (0 for incorrect, 1 for correct) based on criteria for different answer types. The benchmark tests both proprietary LLM-based systems and open-source parse-then-read pipelines across overall performance and by domain/question type, measuring accuracy to reveal system strengths and weaknesses.

## Key Results
- DOCBENCH reveals noticeable gaps between existing LLM-based document reading systems and human performance, with human annotators achieving 83% accuracy versus top systems at 73.9% (Claude-3)
- Proprietary systems generally outperform parse-then-read pipelines, with Claude-3 achieving 73.9% overall accuracy compared to best open-source systems around 55-60%
- Multimodal questions present the greatest challenge, with even GPT-4o achieving only 17.7% accuracy on these questions requiring interpretation of tables and figures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DOCBENCH enables fine-grained evaluation of LLM-based document reading systems by including diverse document types and question formats that mirror real-world complexity.
- Mechanism: By constructing a dataset with 229 real documents and 1,102 questions across five domains and four question types, DOCBENCH captures the full range of challenges these systems face beyond simple text comprehension.
- Core assumption: The performance gap between human annotators (83% accuracy) and even the best proprietary systems (73.9% for Claude-3) indicates that DOCBENCH is measuring genuine capability gaps rather than overfitting to synthetic patterns.
- Evidence anchors:
  - [abstract] "Our evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance"
  - [section 2.1.1] "We identify five domains where documents are frequently utilized: Academia, Finance, Government, Laws, News"
  - [corpus] "Average neighbor FMR=0.479" (moderate relevance to document reading benchmarks)

### Mechanism 2
- Claim: DOCBENCH's multi-step construction pipeline ensures data quality and relevance to real-world scenarios.
- Mechanism: The dataset is built through a three-phase process: document collection from real sources, QA-pair generation using both GPT-4 and human annotators to ensure diversity, and quality check involving automatic filtering followed by manual review from domain experts.
- Core assumption: Human expertise combined with synthetic generation produces higher-quality question-answer pairs than either method alone, as evidenced by the 98% agreement between GPT-4 automatic evaluation and human annotators.
- Evidence anchors:
  - [section 2.1.2] "we leverage the fitz library to parse out the distinct modalities within the PDF files"
  - [section 2.1.3] "we conduct a manual review following the automatic filtering to ensure both the quality of questions"
  - [section 2.4] "we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators"

### Mechanism 3
- Claim: DOCBENCH reveals critical limitations in current document reading systems by testing multi-modal understanding and metadata extraction capabilities.
- Mechanism: By including multimodal questions (27.9% of dataset) and metadata questions (23.4% of dataset), DOCBENCH forces systems to demonstrate capabilities beyond text comprehension, exposing weaknesses in handling figures, tables, and document structure information.
- Core assumption: The poor performance on multimodal questions (e.g., 17.7% accuracy for GPT-4o on multimodal questions) indicates that current systems struggle with integrated information processing rather than simple text extraction.
- Evidence anchors:
  - [section 3.2.1] "Take the first case study as an example, in the first step, KimiChat fails to locate the relevant chart on page 17"
  - [section 2.3] "QA-pair Type The types of QA pairs can be mainly divided into four groups: text-only (37.4%), multimodal (27.9%), meta-data (23.4%), and unanswerable (11.3%)"
  - [corpus] "M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation" (relevant to multimodal document understanding)

## Foundational Learning

- Concept: PDF parsing and document structure analysis
  - Why needed here: DOCBENCH requires understanding of how documents are structured in PDF format, including text blocks, tables, figures, and metadata extraction
  - Quick check question: What library is used to parse PDF files and extract different modalities like text and images?
- Concept: Multimodal information processing
  - Why needed here: The benchmark includes questions requiring interpretation of both textual and visual information (tables and figures), demanding integration of different data modalities
  - Quick check question: What percentage of questions in DOCBENCH require multimodal understanding?
- Concept: Long-context reading and retrieval-augmented generation
  - Why needed here: Documents vary significantly in length, and systems must handle long contexts while maintaining accuracy, especially in domains like Finance with extensive reports
  - Quick check question: What is the maximum token count for documents in DOCBENCH?

## Architecture Onboarding

- Component map: Document ingestion → PDF parsing (fitz library) → Text and image extraction → Question answering pipeline (either proprietary API or parse-then-read) → Response generation → Evaluation (GPT-4 based)
- Critical path: Document parsing → Question type classification → Information retrieval (text, table, figure) → Answer generation → Response validation
- Design tradeoffs: Using proprietary APIs provides better performance but less transparency vs. open-source parse-then-read pipelines that are more transparent but perform worse on complex tasks
- Failure signatures: Inability to locate relevant information (location extraction failure), incorrect data extraction from tables/figures, calculation errors on extracted data, failure to identify unanswerable questions
- First 3 experiments:
  1. Test basic text-only question answering on short documents to establish baseline performance
  2. Evaluate multimodal question answering with simple tables to assess extraction capabilities
  3. Test unanswerable question detection to measure system fidelity to document content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific optimizations in proprietary systems like KimiChat lead to their superior performance on unanswerable questions compared to parse-then-read pipelines?
- Basis in paper: [explicit] The paper notes that "Gemma and KimiChat perform better in such scenarios" for unanswerable questions and suggests this is "a crucial capability since users often expect systems to answer questions strictly based on given files."
- Why unresolved: The paper only speculates about potential reasons (overfitting or adherence to in-context learning) but doesn't provide concrete evidence about what specific optimizations cause this difference.
- What evidence would resolve it: Comparative analysis of the system architectures and implementation details of KimiChat vs open-source parse-then-read pipelines, specifically focusing on how they handle unanswerable questions.

### Open Question 2
- Question: How does the performance of document reading systems vary with different document sizes beyond the observed domains in the benchmark?
- Basis in paper: [inferred] The paper discusses handling lengthy documents and notes that "LLM-based document reading systems struggle with uploading extensive files" while parse-then-read pipelines are "constrained by their maximum context length."
- Why unresolved: The evaluation only covers documents up to certain sizes, and the paper doesn't explore how systems perform with extremely large documents (e.g., multi-hundred page reports).
- What evidence would resolve it: Systematic evaluation of document reading systems across a wider range of document sizes, from small documents to very large ones, to identify performance thresholds.

### Open Question 3
- Question: What are the specific failure modes in the Location→Extraction→Calculation sequence for multimodal questions, and how can they be systematically addressed?
- Basis in paper: [explicit] The paper presents a case study showing that "leading proprietary LLM-based systems often fail due to errors in one of the steps in the Location→Extraction→Calculation sequence" for multimodal questions.
- Why unresolved: While the paper identifies the three-step failure pattern, it doesn't provide detailed analysis of why each step fails or potential solutions to improve performance.
- What evidence would resolve it: Detailed error analysis of each step in the sequence across multiple multimodal questions, along with proposed architectural improvements to address the identified weaknesses.

## Limitations

- The evaluation relies on GPT-4 as the automatic evaluator, which may not capture all nuances of correctness across diverse document types and question formats
- Proprietary systems evaluated operate as black boxes, limiting insights into specific failure modes and optimization strategies
- Human performance baseline represents a single annotator rather than a collective baseline, potentially underestimating true human capability

## Confidence

High confidence: The benchmark construction methodology (document collection, QA-pair generation, quality checking) is well-specified and reproducible. The evaluation pipeline using GPT-4 as an automatic evaluator is clearly defined and validated against human judgment.

Medium confidence: The claim that DOCBENCH reveals genuine capability gaps between systems and human performance is supported by data, but the black-box nature of proprietary systems and single-annotator human baseline introduce uncertainty about the true magnitude of these gaps.

Low confidence: The assertion that current systems fundamentally lack multimodal understanding capabilities based on poor performance on multimodal questions may conflate architectural limitations with implementation differences in how systems handle multimodal inputs.

## Next Checks

1. **Evaluator Validation**: Conduct a second round of human evaluation on a subset of DOCBENCH questions to verify the 98% agreement between GPT-4 and human annotators, and to establish a more robust collective human performance baseline.

2. **Cross-Domain Generalization**: Test whether systems that perform well on DOCBENCH also demonstrate strong performance on other document reading benchmarks (like M3T for multimodal documents or MDCR for multi-document reasoning) to validate DOCBENCH's construct validity.

3. **Failure Mode Analysis**: Conduct detailed error analysis on the top-performing systems to determine whether poor performance on multimodal questions reflects true multimodal understanding limitations or rather differences in how systems parse and process multimodal document components.