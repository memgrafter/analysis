---
ver: rpa2
title: Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers
arxiv_id: '2412.10569'
source_url: https://arxiv.org/abs/2412.10569
tags:
- merging
- token
- training
- tokens
- dtem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient Vision Transformers
  (ViTs) by improving token merging techniques. The core idea is to learn a decoupled
  embedding specifically tailored for token merging, extracted via a continuously
  relaxed token merging process.
---

# Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers

## Quick Facts
- arXiv ID: 2412.10569
- Source URL: https://arxiv.org/abs/2412.10569
- Reference count: 40
- Primary result: Achieves 37.2% FLOPs reduction while maintaining 79.85% top-1 accuracy on ImageNet-1k with DeiT-small

## Executive Summary
This paper addresses the challenge of efficient Vision Transformers (ViTs) by improving token merging techniques through a novel decoupled embedding approach. The method learns specialized features for token merging that are separate from the features used for the primary task, enabling more effective merging decisions. By introducing a continuously relaxed token merging process with soft grouping and merging operators, the approach maintains differentiability throughout training while achieving strong performance across classification, captioning, and segmentation tasks.

## Method Summary
The proposed method introduces a lightweight embedding module that learns features specifically tailored for token merging, decoupled from the ViT forward pass. This decoupled embedding extracts dedicated features used to compute similarity scores between tokens, which then drive a continuously relaxed token merging process. The approach uses soft grouping and merging operators that enable differentiable learning through soft adjacency matrices and asynchronous updates. The method can be trained either modularly (freezing ViT parameters) or end-to-end, with modular training offering significant computational advantages while maintaining strong performance even with limited data.

## Key Results
- Achieves 37.2% reduction in FLOPs while maintaining 79.85% top-1 accuracy with DeiT-small on ImageNet-1k
- Improves token merging across classification, captioning, and segmentation tasks
- Enables modular training that works effectively with limited data and epochs

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Embedding for Specialized Merging Features
The decoupled embedding module learns features specifically tailored for token merging rather than relying on intermediate features used for contextual encoding. By introducing a lightweight trainable embedding module that is decoupled from the ViT forward pass, the method can learn dedicated features for token merging without interference from the primary task's feature learning requirements.

### Mechanism 2: Continuously Relaxed Token Merging
The continuously relaxed token merging process enables differentiable learning of the merging policy through soft grouping and soft merging operators. Instead of discrete operators like hard clustering or matching, the method uses continuous relaxation with soft adjacency matrices and asynchronous updates, allowing gradients to flow through the merging process during training.

### Mechanism 3: Modular Training Efficiency
Modular training of the decoupled embedding allows effective learning even with limited data and epochs, while preserving the pre-trained model's parameters. By training only the embedding module while keeping ViT parameters frozen, the method can leverage pre-trained models effectively without requiring extensive end-to-end fine-tuning, reducing computational requirements and preventing overfitting.

## Foundational Learning

- **Continuous relaxation of discrete operations**: Why needed here: To enable differentiable learning of the token merging policy, which inherently involves discrete decisions about which tokens to merge. Quick check: Can you explain how the soft grouping operator approximates the hard grouping operation while maintaining differentiability?

- **Feature decoupling and specialized embeddings**: Why needed here: To separate the features used for token merging from those used for the primary task, allowing each to be optimized independently. Quick check: Why might features optimized for token merging differ from features optimized for classification or other primary tasks?

- **Modular vs. end-to-end training approaches**: Why needed here: To understand when and why to train only the decoupled embedding versus fine-tuning the entire model. Quick check: What are the trade-offs between modular training (freezing ViT parameters) and end-to-end fine-tuning in terms of computational efficiency and performance?

## Architecture Onboarding

- **Component map**: Input tokens → Self-attention layer → Decoupled embedding module → Soft grouping → Soft merging → Output tokens
- **Critical path**: Embedding computation → Similarity calculation → Soft grouping → Soft merging → Token reduction
- **Design tradeoffs**: Embedding dimension vs. computational overhead; temperature scaling in soft operators; reduction rate during training vs. inference
- **Failure signatures**: Performance degradation when using the decoupled embedding; training instability or slow convergence; increased computational overhead negating efficiency benefits
- **First 3 experiments**: 1) Implement decoupled embedding module and verify improvement on small dataset with pre-trained ViT; 2) Test soft grouping/merging operators with different temperature values; 3) Compare modular vs. end-to-end training on full ImageNet-1k

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the decoupled embedding generalize across different reduction rates during inference, and what factors influence this generalization capability?
- Basis in paper: The paper states that the decoupled embedding, trained with a high reduction rate r, generalizes well to lower rates r′ ≤ r during inference.
- Why unresolved: The paper mentions that the decoupled embedding is learned to sort r most similar token pairs, but does not provide a detailed analysis of the factors influencing generalization across reduction rates.
- What evidence would resolve it: Experiments showing the performance of the decoupled embedding across a wider range of reduction rates, and analysis of the factors (e.g., embedding dimension, temperature scaling) that influence generalization.

### Open Question 2
- Question: How does the modular training approach of DTEM compare to end-to-end training in terms of data and training efficiency, and what are the implications for real-world applications?
- Basis in paper: The paper demonstrates that DTEM improves performance even with limited training data and epochs, highlighting the potential benefit of modular training when the entire dataset is unavailable.
- Why unresolved: While the paper shows the efficiency of modular training, it does not provide a comprehensive comparison of the data and training efficiency between modular and end-to-end approaches across different tasks and datasets.
- What evidence would resolve it: Comparative studies of modular and end-to-end training in terms of data and training efficiency across various tasks and datasets, including real-world applications.

### Open Question 3
- Question: How does the decoupled embedding affect the interpretability of the token merging process, and can it provide insights into the importance of different tokens for the task at hand?
- Basis in paper: The paper visualizes the token merging process and shows that DTEM prioritizes merging background patches and allocates more patches to foreground objects, suggesting that the decoupled embedding may provide insights into token importance.
- Why unresolved: The paper does not provide a detailed analysis of how the decoupled embedding affects the interpretability of the token merging process or its potential to provide insights into token importance.
- What evidence would resolve it: Interpretability studies that analyze the relationship between the decoupled embedding and token importance, and how this information can be used to improve the token merging process or understand the model's decision-making.

## Limitations

- Generalizability across diverse ViT architectures and tasks beyond evaluated benchmarks remains uncertain
- Continuous relaxation may introduce approximation errors affecting inference performance
- Computational overhead of the decoupled embedding relative to efficiency gains is not explicitly quantified

## Confidence

**High Confidence (8/10):**
- Decoupled embedding architecture is technically sound and well-implemented
- Modular training approach provides computational benefits as described
- Method achieves reported FLOPs reduction and accuracy trade-off

**Medium Confidence (6/10):**
- Soft operators effectively approximate hard operators during inference
- Approach generalizes to captioning and segmentation tasks
- Decoupled embedding consistently outperforms intermediate feature-based approaches

**Low Confidence (4/10):**
- Optimal temperature scaling values for different model scales and tasks
- Performance under distribution shift and real-world deployment scenarios
- Computational overhead relative to efficiency gains

## Next Checks

1. **Cross-architecture generalization test**: Evaluate DTEM on broader range of ViT architectures and diverse tasks (object detection, video processing) to assess generalizability beyond evaluated benchmarks.

2. **Ablation study on temperature scaling**: Conduct comprehensive ablation varying temperature parameter across different model scales and tasks to determine optimal values and assess sensitivity.

3. **Modular vs. end-to-end training comparison under data constraints**: Systematically compare modular training against end-to-end fine-tuning varying training data amount and epochs to identify effective regimes.