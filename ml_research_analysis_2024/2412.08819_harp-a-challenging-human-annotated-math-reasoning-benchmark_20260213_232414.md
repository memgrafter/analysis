---
ver: rpa2
title: 'HARP: A challenging human-annotated math reasoning benchmark'
arxiv_id: '2412.08819'
source_url: https://arxiv.org/abs/2412.08819
tags:
- problems
- answer
- problem
- math
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HARP is a challenging math reasoning benchmark introduced to address
  the saturation of existing datasets like MATH by frontier models. It consists of
  5,409 problems from US national math competitions (A(J)HSME, AMC, AIME, USA(J)MO),
  with 4,780 short-answer problems featuring automatically checkable answers.
---

# HARP: A challenging human-annotated math reasoning benchmark

## Quick Facts
- arXiv ID: 2412.08819
- Source URL: https://arxiv.org/abs/2412.08819
- Authors: Albert S. Yue; Lovish Madaan; Ted Moskovitz; DJ Strouse; Aaditya K. Singh
- Reference count: 40
- Top models achieve only 41.1% accuracy on hardest problems

## Executive Summary
HARP introduces a challenging math reasoning benchmark consisting of 5,409 problems from US national math competitions (A(J)HSME, AMC, AIME, USA(J)MO). The dataset spans six difficulty levels and includes 4,780 short-answer problems with automatically checkable answers, 4,110 problems with multiple-choice options, and an average of two human-written solutions per problem. On the hardest difficulty level (197 problems), top models like o1-mini achieve only 41.1% accuracy, while Gemini 1.5 Pro scores just 9.6%, demonstrating the benchmark's challenging nature.

## Method Summary
The authors constructed HARP by web scraping problems from US national math competitions, parsing LaTeX expressions, extracting answers, and annotating difficulty levels according to expert guidance. They implemented a rule-based answer checker based on Xwin-Math code, expanded to handle mixed fractions, ordered tuples, and various LaTeX commands. The dataset includes both short-answer and multiple-choice formats, with automatic answer verification using SymPy for the former. All code for dataset construction and evaluation is open-sourced to enable reproducibility.

## Key Results
- Top models achieve only 41.1% accuracy on the hardest 197 problems
- Gemini 1.5 Pro scores just 9.6% on the most difficult problems
- Models scale inference-time compute (chain-of-thought tokens) for harder problems, mirroring human problem-solving patterns
- Dataset includes 5,409 problems with 4,780 automatically checkable answers and an average of 2.14 human-written solutions per problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's difficulty levels create a reliable progression for measuring model improvement.
- Mechanism: Problems are sourced from US national math competitions with expert-annotated difficulty levels (1-6), allowing models to be evaluated across increasing challenge levels.
- Core assumption: Human expert difficulty ratings accurately reflect the computational complexity required to solve problems.
- Evidence anchors:
  - [abstract]: "These problems range six difficulty levels, with frontier models performing relatively poorly on the hardest bracket of 197 problems (average accuracy 41.1% for o1-mini, and 9.6% for Gemini 1.5 Pro)."
  - [section]: "We assign difficulty levels to each problem, following expert guidance for the difficulty of each MAA contest."
  - [corpus]: Weak - corpus neighbors don't discuss difficulty calibration methodology.
- Break condition: If human experts misjudge problem difficulty, the progression would misrepresent actual model capabilities.

### Mechanism 2
- Claim: Multiple human-written solutions per problem enable research on solution diversity and model robustness.
- Mechanism: With an average of 2.14 solutions per problem, models can be evaluated on their ability to match different solution approaches, not just final answers.
- Core assumption: Different solution approaches represent valid alternative reasoning paths that models should be able to discover.
- Evidence anchors:
  - [abstract]: "Our dataset also features multiple choices (for 4,110 problems) and an average of two human-written, ground-truth solutions per problem, offering new avenues of research that we explore briefly."
  - [section]: "Notably, about half the problems have more than one solution, with an average of 2.14 human-written solutions per problem and a maximum of 14 distinct human-written solutions."
  - [corpus]: Missing - corpus neighbors don't discuss solution diversity research.
- Break condition: If solutions are highly redundant or represent minor variations rather than distinct approaches.

### Mechanism 3
- Claim: The short answer format with programmatic checking enables scalable, reproducible evaluation.
- Mechanism: 4,780 problems have automatically checkable answers using SymPy, allowing consistent evaluation across different models and time periods.
- Core assumption: The programmatic checker correctly handles all valid mathematical answer formats and edge cases.
- Evidence anchors:
  - [abstract]: "Of these, 4,780 have answers that are automatically check-able (with libraries such as SymPy)."
  - [section]: "For short answer scoring, we implement a rule-based answer checker, based on the code open-sourced by Xwin-Math [12] and expanded to handle more cases such as mixed fractions, ordered tuples, and various LaTeX commands."
  - [corpus]: Weak - corpus neighbors don't discuss answer checking methodologies.
- Break condition: If the checker fails to recognize valid equivalent answers or accepts incorrect ones.

## Foundational Learning

- Concept: Mathematical problem classification by subject and difficulty
  - Why needed here: Enables targeted evaluation of model capabilities across different mathematical domains and challenge levels
  - Quick check question: How would you categorize a problem about finding the area of a triangle using trigonometry?

- Concept: Chain-of-thought reasoning patterns in mathematical problem solving
  - Why needed here: The dataset reveals that models scale inference-time compute for harder problems, mirroring human problem-solving behavior
  - Quick check question: What distinguishes a problem that requires 2 steps vs 6 steps in solution length?

- Concept: Answer equivalence checking in symbolic mathematics
  - Why needed here: The programmatic checker must recognize mathematically equivalent expressions in different forms
  - Quick check question: How would you verify that "sqrt(8)" and "2*sqrt(2)" represent the same value?

## Architecture Onboarding

- Component map:
  - Data pipeline: Web scraping → HTML parsing → LaTeX extraction → Answer extraction → Annotation
  - Evaluation system: Problem loading → Model prompting → Response parsing → Answer checking
  - Answer checking: String normalization → LaTeX parsing → SymPy evaluation → Equivalence testing
  - Quality control: Duplicate detection → Parse validation → Human annotation review

- Critical path: Problem loading → Model inference → Response parsing → Answer verification → Score aggregation
- Design tradeoffs:
  - Strict vs flexible answer checking: Balancing precision with ability to recognize equivalent expressions
  - Single vs multiple model runs: Trade-off between evaluation cost and robustness to stochastic outputs
  - Temperature settings: Balancing reproducibility with model performance optimization
- Failure signatures:
  - Low scores across all models: May indicate parsing errors or checker limitations
  - High variance between runs: Suggests temperature sensitivity or stochastic generation issues
  - Difficulty score anomalies: Could indicate annotation errors or contamination
- First 3 experiments:
  1. Test answer checker with known equivalent expressions to verify robustness
  2. Run evaluation with multiple temperature settings to assess stochastic effects
  3. Compare model performance on overlapping problems with MATH dataset for calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models on HARP compare to their performance on other recently introduced challenging math reasoning benchmarks like MathOdyssey and Omni-MATH?
- Basis in paper: [inferred] The paper compares HARP performance to MATH but does not discuss other recent benchmarks
- Why unresolved: The paper focuses on establishing HARP as a challenging benchmark without systematic comparison to newer datasets
- What evidence would resolve it: A comprehensive evaluation study comparing model performance across HARP, MathOdyssey, Omni-MATH, and other recent benchmarks

### Open Question 2
- Question: What is the optimal balance between short-answer and multiple-choice formats for evaluating mathematical reasoning in large language models?
- Basis in paper: [explicit] The paper discusses differences in performance between short-answer and multiple-choice formats on HARP
- Why unresolved: While the paper observes differences, it doesn't determine the optimal evaluation methodology
- What evidence would resolve it: Empirical studies comparing model performance and human evaluation results across different question formats and combinations

### Open Question 3
- Question: How does the presence of multiple human-written solutions per problem affect model learning and reasoning capabilities during fine-tuning?
- Basis in paper: [explicit] The paper notes that HARP includes an average of two human-written solutions per problem
- Why unresolved: The paper only briefly mentions this feature without exploring its impact on model training
- What evidence would resolve it: Controlled experiments comparing model performance when fine-tuned with varying numbers of solutions per problem, including ablation studies on solution diversity

### Open Question 4
- Question: What is the relationship between model inference-time compute scaling and problem difficulty across different model architectures and training paradigms?
- Basis in paper: [explicit] The paper observes that models across families intrinsically scale inference-time compute for harder problems
- Why unresolved: The paper identifies this trend but doesn't explore underlying mechanisms or architectural differences
- What evidence would resolve it: Comparative studies examining inference-time compute patterns across diverse model architectures, including analysis of attention patterns and reasoning processes

### Open Question 5
- Question: How does the presence of "human-like" distractors in multiple-choice questions affect model performance compared to random distractors?
- Basis in paper: [explicit] The paper mentions that multiple-choice evaluation offers a lens to see how "general" a model's math reasoning capabilities are, including the presence of human-written distractors
- Why unresolved: The paper does not investigate the impact of distractor quality on model performance
- What evidence would resolve it: Systematic evaluation comparing model performance on multiple-choice questions with human-written versus random distractors, controlling for other variables

## Limitations

- The dataset's difficulty calibration relies heavily on human expert judgment, which may introduce systematic biases
- The paper doesn't establish whether increased inference-time compute for harder problems represents genuine reasoning improvement or merely increased token generation
- The assertion that HARP effectively addresses the saturation of existing math benchmarks lacks comparative analysis with newer datasets

## Confidence

**High Confidence**: The dataset construction methodology and the reported baseline performances across different models. The specific accuracy numbers (41.1% for o1-mini, 9.6% for Gemini 1.5 Pro) on the hardest problems are well-supported by the experimental setup described.

**Medium Confidence**: The claim that HARP provides "new avenues of research" through multiple solutions per problem. While the dataset does include multiple solutions, the paper only briefly explores this feature without demonstrating concrete research outcomes.

**Low Confidence**: The assertion that HARP effectively addresses the saturation of existing math benchmarks. The paper doesn't provide comparative analysis showing that HARP problems are genuinely harder than those in MATH or other established benchmarks.

## Next Checks

1. **Checker Robustness Test**: Verify the answer checker's ability to recognize equivalent mathematical expressions by testing it with known pairs like "sqrt(8)" vs "2*sqrt(2)" and "1/2" vs "0.5" across various mathematical domains.

2. **Difficulty Calibration Validation**: Conduct a blind difficulty rating study where different experts categorize problems without knowing the assigned difficulty levels, then compare inter-rater agreement with the original annotations.

3. **Model Scaling Analysis**: Design controlled experiments that vary only problem difficulty while holding model parameters constant, measuring whether increased inference-time compute correlates with improved accuracy or simply longer, less focused responses.