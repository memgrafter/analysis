---
ver: rpa2
title: 'Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly
  an Adversarial Regularizer'
arxiv_id: '2405.16436'
source_url: https://arxiv.org/abs/2405.16436
tags:
- reward
- arxiv
- policy
- algorithm
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of overoptimization in reinforcement
  learning from human feedback (RLHF) for aligning large language models (LLMs) with
  human preferences. The core idea is to mitigate overoptimization by incorporating
  a supervised fine-tuning (SFT) loss as an implicit adversarial regularizer in the
  preference optimization objective.
---

# Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer

## Quick Facts
- **arXiv ID**: 2405.16436
- **Source URL**: https://arxiv.org/abs/2405.16436
- **Reference count**: 40
- **Key outcome**: RPO mitigates overoptimization by incorporating SFT loss as an adversarial regularizer, achieving 57.8% vs 42.2% win rates against DPO on MT-bench and 52.0% vs 47.0% on AlpacaEval 2.0.

## Executive Summary
This paper addresses the challenge of overoptimization in reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. The core idea is to mitigate overoptimization by incorporating a supervised fine-tuning (SFT) loss as an implicit adversarial regularizer in the preference optimization objective. This regularizer prevents the model from placing excessive probability on out-of-distribution responses that might have misleadingly high estimated rewards. The authors propose Regularized Preference Optimization (RPO), which combines the direct preference optimization (DPO) loss with an SFT loss. Theoretically, they establish finite-sample convergence guarantees under a partial coverage condition. Empirically, they demonstrate that RPO effectively mitigates overoptimization and consistently improves alignment performance compared to DPO baselines on in-distribution data and standard benchmarks.

## Method Summary
The paper proposes Regularized Preference Optimization (RPO), which combines the Direct Preference Optimization (DPO) loss with an SFT loss term as an adversarial regularizer. The method uses a preference dataset consisting of prompts, two responses, and a preference label. RPO introduces a hyperparameter β to balance the DPO and SFT losses. The training configuration uses a learning rate of 5.0e-7, batch size 128, gradient accumulation 2, and training epoch 1. The method is evaluated on MT-bench, AlpacaEval 2.0, and additional benchmarks including GSM8K, ARC, and MBPP.

## Key Results
- RPO achieves 57.8% win rates vs 42.2% against DPO on MT-bench
- RPO achieves 52.0% win rates vs 47.0% against DPO on AlpacaEval 2.0
- RPO maintains higher log probability on chosen responses during training compared to DPO, indicating effective overoptimization mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT loss acts as an adversarial regularizer that prevents the model from placing excessive probability on out-of-distribution responses with misleadingly high estimated rewards.
- Mechanism: The SFT loss term in the RPO objective explicitly regularizes the LLM to imitate the chosen responses from the preference dataset, which serves as a baseline policy. This regularization modifies the gradient direction of the preference optimization, preventing overoptimization by ensuring the model maintains alignment with well-covered responses.
- Core assumption: The chosen responses in the preference dataset are representative of high-quality responses and are sufficiently covered by the training data.
- Evidence anchors:
  - [abstract]: "The core idea is to mitigate overoptimization by incorporating a supervised fine-tuning (SFT) loss as an implicit adversarial regularizer in the preference optimization objective."
  - [section]: "The Imitation (SFT) loss explicitly supervises the LLM to mimic the responses from a proper distribution that is well covered by the dataset."
  - [corpus]: "Mitigating Preference Hacking in Policy Optimization with Pessimism" - discusses pessimism in policy optimization, which aligns with the adversarial regularizer concept.

### Mechanism 2
- Claim: The equivalence between the maximin and minimax objectives allows for a simple practical implementation of the theoretical algorithm.
- Mechanism: Under certain regularity conditions, the maximin objective (Algorithm 1) is equivalent to the minimax objective (Algorithm 2). This equivalence allows the theoretical algorithm to be implemented using a simple weighted combination of the DPO loss and the SFT loss, without the need for explicit reward model learning.
- Core assumption: The reward model class R satisfies certain regularity conditions that ensure the equivalence between the maximin and minimax objectives.
- Evidence anchors:
  - [abstract]: "Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation."
  - [section]: "The magic of (4.1) is that the inner maximization problem adopts a closed form solution, which further simplifies such an objective."
  - [corpus]: "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment" - discusses theoretical guarantees for RLHF algorithms, which aligns with the theoretical foundation of this mechanism.

### Mechanism 3
- Claim: The finite-sample convergence guarantees of the theoretical algorithm ensure that the practical algorithm mitigates overoptimization and improves alignment performance.
- Mechanism: The theoretical algorithm (Algorithm 1) is designed to choose the best policy for an adversarially chosen reward model, which minimizes the maximum likelihood estimation loss and a reward penalty term. This design prevents the model from being misled by spuriously high reward estimates caused by data uncertainty and insufficient coverage. The finite-sample convergence guarantees of the theoretical algorithm are then extended to the practical algorithm (Algorithm 2) through the equivalence between the maximin and minimax objectives.
- Core assumption: The preference dataset covers the target policy sufficiently, as characterized by the partial coverage coefficient CµD(R; π, πbase).
- Evidence anchors:
  - [abstract]: "Theoretically, they establish finite-sample convergence guarantees under a partial coverage condition."
  - [section]: "The data distribution therein well covers those nearly optimal responses under r⋆, but does not sufficiently cover the responses with low r⋆."
  - [corpus]: "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF" - discusses methods to mitigate overoptimization in RLHF, which aligns with the theoretical guarantees of this mechanism.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the primary technique used to align large language models with human preferences, and the paper focuses on addressing the challenge of overoptimization in RLHF.
  - Quick check question: What are the main steps involved in the RLHF pipeline, and how does overoptimization occur in this process?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is a reward-model-free method used to align LLMs with human preferences, and the paper's practical algorithm (RPO) builds upon the DPO objective by adding an SFT loss term.
  - Quick check question: How does DPO differ from traditional RLHF methods that involve explicit reward model learning, and what are the advantages of this approach?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used as a regularization term in the RPO objective to mitigate overoptimization by explicitly imitating the chosen responses from the preference dataset.
  - Quick check question: How does SFT differ from other regularization techniques commonly used in machine learning, and what are the benefits of using SFT in the context of RLHF?

## Architecture Onboarding

- Component map:
  - Preference dataset (xi, a1i, a0i, yi) -> RPO objective (DPO loss + SFT loss) -> Aligned LLM

- Critical path:
  1. Prepare the preference dataset and reference model
  2. Implement the RPO objective with the specified hyperparameters (η, β)
  3. Train the LLM using the RPO objective
  4. Evaluate the aligned LLM on in-distribution data and standard benchmarks

- Design tradeoffs:
  - Using SFT loss as a regularizer may improve alignment performance but could also introduce additional computational overhead.
  - The choice of the baseline policy (e.g., chosen responses in the preference dataset) affects the effectiveness of the SFT loss in mitigating overoptimization.
  - The hyperparameters (η, β) need to be carefully tuned to balance the contributions of the DPO and SFT losses.

- Failure signatures:
  - If the SFT loss is not effective in mitigating overoptimization, the aligned LLM may still exhibit a significant decrease in log probability on chosen responses during training.
  - If the RPO objective is not properly implemented, the aligned LLM may not achieve better alignment performance compared to the DPO baseline.
  - If the preference dataset does not sufficiently cover the target policy, the finite-sample convergence guarantees may not hold, and the aligned LLM may not generalize well to new prompt distributions.

- First 3 experiments:
  1. Implement the RPO objective and train an LLM on a small preference dataset to verify that the SFT loss effectively mitigates overoptimization (i.e., the aligned LLM maintains a higher log probability on chosen responses during training compared to the DPO baseline).
  2. Evaluate the aligned LLM on in-distribution data to confirm that it achieves better alignment performance compared to the DPO baseline (i.e., higher pairwise win rates on the test split of the training dataset).
  3. Evaluate the aligned LLM on standard benchmarks (e.g., MT-bench, AlpacaEval 2.0) to verify that it generalizes well to new prompt distributions and consistently outperforms the DPO baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of baseline policy πbase affect the performance of RPO in terms of mitigating overoptimization and generalization to new prompt distributions?
- Basis in paper: [explicit] The paper discusses the choice of πbase in Remark 5.4 and mentions that the baseline policy should be well covered by the dataset. It also states that the SFT loss regularizer in RPO can be adapted in practice, such as using the preferred response in the dataset or the responses of the initial model.
- Why unresolved: The paper does not provide empirical evidence on how different choices of πbase impact RPO's performance. It also does not explore the trade-offs between using the preferred response versus the initial model as the baseline.
- What evidence would resolve it: Empirical results comparing RPO's performance using different baseline policies on various datasets and benchmarks would clarify the impact of πbase on overoptimization mitigation and generalization.

### Open Question 2
- Question: Can the theoretical framework of RPO be extended to handle more complex reward models beyond the Bradley-Terry model, such as those involving additional features or non-linear relationships?
- Basis in paper: [inferred] The paper focuses on the Bradley-Terry model for human preference and does not explore more complex reward model structures. The theoretical analysis assumes a general reward model class R but does not specify its form.
- Why unresolved: Extending the theoretical framework to handle more complex reward models would require new mathematical techniques and potentially different algorithmic designs. The paper does not address these challenges.
- What evidence would resolve it: Theoretical analysis and empirical results demonstrating the effectiveness of RPO with more complex reward models would validate its broader applicability.

### Open Question 3
- Question: How does RPO perform in iterative RLHF settings where the model can actively collect new preference data during training?
- Basis in paper: [inferred] The paper focuses on offline RLHF with a fixed dataset and does not explore iterative settings. The conclusion mentions extending the idea to iterative RLHF as a future work.
- Why unresolved: Iterative RLHF introduces additional challenges such as exploration-exploitation trade-offs and data collection strategies. The paper does not address these issues.
- What evidence would resolve it: Empirical results comparing RPO's performance in iterative RLHF settings versus offline settings would highlight its strengths and limitations in active learning scenarios.

## Limitations

- The effectiveness of the SFT regularizer heavily depends on the chosen responses being representative of high-quality responses and sufficiently covered by the training data.
- The theoretical guarantees rely on an equivalence between maximin and minimax objectives that may not hold in practice.
- The empirical evaluation focuses primarily on pairwise comparison benchmarks and lacks evaluation on more diverse tasks, safety benchmarks, or real-world deployment scenarios.

## Confidence

- **High Confidence**: The empirical results showing RPO outperforming DPO baselines on MT-bench and AlpacaEval 2.0 with statistically significant win rates (57.8% vs 42.2% on MT-bench, 52.0% vs 47.0% on AlpacaEval 2.0) are well-supported by the experimental methodology and consistent with the proposed mechanism.
- **Medium Confidence**: The theoretical framework establishing finite-sample convergence guarantees under partial coverage conditions is mathematically sound, but the practical applicability depends on the assumption that the chosen responses are well-covered by the dataset.
- **Low Confidence**: The claim that the simple weighted combination of DPO and SFT losses (RPO) inherits the theoretical guarantees of the more complex maximin formulation is based on an equivalence assumption that may not hold in practice.

## Next Checks

1. Perform an ablation study to quantify the relationship between dataset coverage (using the partial coverage coefficient) and the effectiveness of the SFT regularizer in mitigating overoptimization.

2. Evaluate RPO on adversarial prompt distributions specifically designed to trigger overoptimization, comparing performance degradation against DPO baselines to verify the claimed robustness.

3. Test the aligned models on established safety benchmarks (e.g., RealToxicityPrompts, AdvGLUE) to assess whether the overoptimization mitigation approach affects the model's safety and robustness properties.