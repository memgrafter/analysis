---
ver: rpa2
title: 'CodingTeachLLM: Empowering LLM''s Coding Ability via AST Prior Knowledge'
arxiv_id: '2403.15426'
source_url: https://arxiv.org/abs/2403.15426
tags:
- arxiv
- data
- language
- code
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of creating a specialized large
  language model capable of tutoring in coding tasks, as general LLMs lack domain-specific
  expertise and the ability to guide students step-by-step without revealing answers.
  To tackle this, the authors propose an end-to-end three-phase supervised fine-tuning
  method enhanced with a prior module that integrates vector databases, abstract syntax
  trees (AST), and system prompts.
---

# CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge

## Quick Facts
- arXiv ID: 2403.15426
- Source URL: https://arxiv.org/abs/2403.15426
- Reference count: 40
- Key outcome: State-of-the-art coding ability on HumanEval (75.10% pass@1) with strong conversational skills and true tutoring capability through incremental guided outputs

## Executive Summary
This paper introduces CodingTeachLLM, a specialized large language model designed for coding education that can provide step-by-step guidance without revealing answers. The model addresses the limitations of general LLMs in domain-specific expertise and pedagogical guidance through a novel three-phase supervised fine-tuning approach enhanced with a prior module. By integrating vector databases, abstract syntax trees (AST), and system prompts, the model achieves state-of-the-art performance on coding benchmarks while uniquely embodying the tutor role through incremental, guided outputs.

## Method Summary
The approach involves an end-to-end three-phase supervised fine-tuning method using Llama-2-34B as the base model with LoRA for parameter-efficient adaptation. The process includes robust data preprocessing via an overlap estimation neural network to filter and correlate data between MFT dataset and local knowledge base, three-stage fine-tuning (general knowledge → coding → educational guidance), and integration of a prior module combining system prompts, vector databases, and AST task segmentation. The model also incorporates structured pruning, regularization constraints, and a text filter at the output end to achieve step-by-step incremental guided outputs without disclosing answers.

## Key Results
- Achieves state-of-the-art coding ability on HumanEval benchmark with 75.10% pass@1
- Demonstrates strong conversational capabilities on multiple dialogue evaluation benchmarks (MMLU 56.34, C-Eval 50.60, AGIEval 45.27)
- Uniquely embodies the tutor role by providing incremental guided outputs without disclosing direct answers

## Why This Works (Mechanism)

### Mechanism 1
Three-phase fine-tuning improves tutoring ability by progressively aligning the model to general knowledge, coding, and educational guidance. The stepwise curriculum allows the model to first establish a broad foundation, then specialize in coding logic, and finally adapt to pedagogical behavior. Each stage builds upon the last, preventing interference between incompatible objectives. Core assumption: The domains of general knowledge, coding, and teaching guidance are sufficiently separable that staged training yields better integration than simultaneous multi-task training.

### Mechanism 2
AST-guided subtask segmentation enables incremental guided output by decomposing coding tasks into logically ordered steps. The abstract syntax tree represents the syntactic structure of code, which is used to break a coding problem into subcomponents (e.g., variable declarations, loops, conditionals). The model is trained to generate responses corresponding to one subcomponent at a time, maintaining the tutor role. Core assumption: The AST structure of code is a reliable proxy for pedagogical sequencing; following the AST traversal order yields a natural incremental teaching path.

### Mechanism 3
Overlap estimation network improves fine-tuning data quality by selecting highly correlated samples between MFT dataset and local knowledge base. A neural network estimates the similarity between randomly sampled fine-tuning data and local knowledge embeddings. Only pairs with similarity above a threshold are included in the local knowledge subset, ensuring strong prior relevance. Core assumption: Cosine similarity in vector space correlates with the model's ability to leverage prior knowledge during inference; high similarity leads to better integration.

## Foundational Learning

- **Concept:** Abstract Syntax Tree (AST)
  - Why needed here: AST is used to decompose coding tasks into incremental subtasks for guided teaching; without understanding AST, one cannot follow how code logic is broken down for pedagogical sequencing.
  - Quick check question: What are the two main traversal orders of an AST, and how might each relate to presenting code explanations to a student?

- **Concept:** Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is used to fine-tune the pre-trained LLM in a parameter-efficient manner across three stages; understanding LoRA is essential to grasp how the model adapts without full retraining.
  - Quick check question: In LoRA, why is the update matrix represented as the product of two low-rank matrices, and how does this reduce the number of trainable parameters?

- **Concept:** Vector Database + Embedding
  - Why needed here: Vector database stores prior knowledge embeddings and enables similarity-based retrieval during inference; understanding this is key to how the system provides context-aware tutoring hints.
  - Quick check question: How does cosine similarity between embeddings relate to the relevance of retrieved knowledge in a tutoring context?

## Architecture Onboarding

- **Component map:** Pre-trained LLM (Llama-2-34B) -> Overlap Estimation Network (data preprocessing) -> Three-phase LoRA fine-tuning pipeline -> Prior Module (System Prompt + Vector Database + AST Subtask Segmentation) -> Output Filter (noise suppression and answer hiding) -> Local Knowledge Base (vectorized)
- **Critical path:** Data preprocessing → Three-phase fine-tuning → Prior module integration → Inference with output filtering
- **Design tradeoffs:** Parameter efficiency (LoRA) vs. full fine-tuning flexibility; knowledge base size vs. retrieval latency; AST granularity vs. pedagogical clarity; filter strictness vs. natural dialogue flow
- **Failure signatures:** Model outputs full code instead of guided steps → likely AST segmentation or output filter issue; Model forgets prior context → likely vector database retrieval or context window overflow; Model produces irrelevant hints → likely overlap estimation threshold too low or prior knowledge mismatch; Training instability → likely phase transition or LoRA rank misconfiguration
- **First 3 experiments:**
  1. Overlap estimation ablation: Train two versions—one with overlap filtering, one without—on the same three-phase pipeline; compare tutoring ability and general performance.
  2. AST segmentation granularity test: Vary the depth of AST decomposition and measure how well the model maintains the tutor role without revealing answers.
  3. Filter strength sweep: Adjust the output filter's suppression threshold and measure the trade-off between guidance quality and answer disclosure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the three-phase fine-tuning method compare to traditional single-phase fine-tuning in terms of computational efficiency and resource usage?
- Basis in paper: [explicit] The paper states that "Compared with the traditional SFT method, stepwise dataset injections for fine-tuning can significantly improve the model’s performance in specific domains."
- Why unresolved: While the paper mentions the superiority of the three-phase method, it does not provide a detailed comparison of computational efficiency and resource usage between the two approaches.
- What evidence would resolve it: A direct comparison of training time, GPU memory usage, and computational costs between the three-phase and traditional single-phase fine-tuning methods.

### Open Question 2
- Question: How does the model's tutoring ability scale with the size of the dataset used for fine-tuning?
- Basis in paper: [inferred] The paper mentions that the model achieves state-of-the-art performance in coding and tutoring abilities, but it does not discuss how these abilities scale with the size of the dataset.
- Why unresolved: The paper does not provide information on how the model's performance changes as the dataset size increases or decreases.
- What evidence would resolve it: Experiments showing the model's tutoring and coding performance at different dataset sizes, including smaller and larger datasets than those used in the current study.

### Open Question 3
- Question: What are the long-term effects of using the model as a tutor on students' learning outcomes and independent thinking abilities?
- Basis in paper: [explicit] The paper states that the model "embodies the essence of a tutor" and provides "step-by-step incremental guided outputs" to students.
- Why unresolved: The paper does not discuss any studies or evidence on the long-term impact of using the model on students' learning outcomes or their ability to think independently.
- What evidence would resolve it: Longitudinal studies tracking students' performance and critical thinking skills over time when using the model as a tutor compared to traditional teaching methods.

## Limitations

- The overlap estimation network's specific architecture and training methodology remain underspecified, making it difficult to assess whether correlation filtering genuinely improves model performance
- The output filter mechanism for maintaining the tutor role is described abstractly without clear implementation details, raising questions about how effectively it prevents answer disclosure
- The paper lacks direct comparisons against other educational LLMs on the same coding benchmarks, making it unclear whether improvements stem from the three-phase approach specifically or from increased model scale and data volume

## Confidence

- **High Confidence:** The three-phase fine-tuning methodology is internally consistent and technically sound, following established practices for parameter-efficient adaptation across distinct domains.
- **Medium Confidence:** The AST-guided subtask segmentation effectively enables incremental tutoring without revealing answers, though empirical evidence comparing it to alternative segmentation strategies is limited.
- **Low Confidence:** The overlap estimation network meaningfully improves fine-tuning data quality and model performance, as the paper provides no ablation studies or quantitative evidence demonstrating this benefit.

## Next Checks

1. **Overlap Estimation Ablation Study:** Train two identical three-phase models—one using the overlap estimation network with similarity threshold filtering, and one using all available data without filtering. Compare both HumanEval performance and educational capability metrics to isolate the impact of the correlation filtering mechanism.

2. **AST Granularity Impact Analysis:** Systematically vary the depth of AST decomposition (e.g., top-level functions only vs. statement-level breakdown) and measure the trade-off between pedagogical clarity and answer disclosure. This would validate whether AST structure genuinely provides optimal teaching sequences.

3. **Direct Educational Benchmark Comparison:** Evaluate CodingTeachLLM against established educational LLMs (e.g., AgentTutor, CodeLlama-tuned for instruction) on the same HumanEval and conversational benchmarks, ensuring that performance differences can be attributed to the three-phase methodology rather than dataset scale or model size advantages.