---
ver: rpa2
title: 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies'
arxiv_id: '2412.15035'
source_url: https://arxiv.org/abs/2412.15035
tags:
- safety
- scores
- languages
- across
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: M-ALERT exposes significant cross-linguistic safety inconsistencies
  in large language models, revealing that even high-performing models like Llama3.2
  and Gemma-2 exhibit notable safety drops in specific categories and languages. For
  instance, Llama3.2 shows unsafe behavior in Italian for the crimetax category but
  remains safe in other languages.
---

# LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies

## Quick Facts
- arXiv ID: 2412.15035
- Source URL: https://arxiv.org/abs/2412.15035
- Reference count: 21
- Key outcome: M-ALERT exposes significant cross-linguistic safety inconsistencies in large language models, revealing that even high-performing models like Llama3.2 and Gemma-2 exhibit notable safety drops in specific categories and languages.

## Executive Summary
M-ALERT reveals that large language models exhibit significant safety inconsistencies across different languages and safety categories. The benchmark evaluates 40 state-of-the-art LLMs across English, French, German, Italian, and Spanish using a comprehensive safety taxonomy with 6 macro and 32 micro categories. Results show that while instruction-tuned models generally perform better than base models, no model achieves universal safety across all languages and categories. Categories like substance_cannabis and crime_propaganda consistently trigger unsafe responses, while language-specific drops in safety performance highlight the critical need for robust multilingual safety practices.

## Method Summary
The M-ALERT benchmark evaluates LLM safety through a pipeline of prompt generation, translation to 5 target languages, LLM inference, safety evaluation using LlamaGuard-3, and aggregation of results. The dataset contains 75k prompts (15k per language) with category annotations. Evaluation uses auto-regressive generation with max_new_tokens=200 and sampling strategy, followed by safety scoring. Translation quality is estimated using COMET-XXL and MetricX-XXL metrics, with a threshold of 0.5 for inclusion. Human evaluation on random subsets validates the automated assessment approach.

## Key Results
- Llama3.2 exhibits unsafe behavior in Italian for the crime_tax category but remains safe in other languages
- Categories substance_cannabis and crime_propaganda consistently trigger unsafe responses across models and languages
- Gemma-2 approaches 99% safety in Spanish, French, and Italian, but no model achieves this threshold universally
- Base models show higher safety with increasing model size compared to instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety performance of LLMs degrades non-uniformly across languages due to misalignment in safety data coverage.
- Mechanism: Translation pipeline maps prompts to target languages, but safety datasets are heavily skewed toward English, causing safety model to underperform in low-resource languages.
- Core assumption: The safety evaluator (LlamaGuard-3) was trained predominantly on English safety data and lacks equivalent coverage for other languages.
- Evidence anchors:
  - [abstract] "M-ALERT exposes significant cross-linguistic safety inconsistencies in large language models, revealing that even high-performing models like Llama3.2 and Gemma-2 exhibit notable safety drops in specific categories and languages."
  - [section] "Notably, the identification of propaganda as a safety concern is a novel finding compared to ALERT, which utilizes a safety evaluator that does not classify propaganda as a dedicated violation under its safety taxonomy."
  - [corpus] Weak signal; only 2/8 related papers explicitly discuss cross-lingual safety evaluation.
- Break condition: Safety data coverage is equalized across languages or evaluator models are retrained with multilingual safety annotations.

### Mechanism 2
- Claim: Instruct-tuned models show higher safety scores than base models due to dedicated safety fine-tuning.
- Mechanism: Base models generate unsafe outputs by default; safety tuning in instruct models filters unsafe responses through reinforcement learning from human feedback (RLHF).
- Core assumption: Instruct models undergo additional safety alignment beyond standard instruction tuning.
- Evidence anchors:
  - [abstract] "While instruction tuning improves safety over base models, the correlation with model size is less pronounced."
  - [section] "Upon further analysis of base versus instruct models in Table 6, we observe significant differences between the base models. As expected, instruct models exhibit higher safety levels, but there is considerable variation in the safety of the base models."
  - [corpus] Weak signal; related work focuses on machine translation, not safety tuning effects.
- Break condition: Base models are fine-tuned with safety-specific datasets or safety tuning is decoupled from general instruction tuning.

### Mechanism 3
- Claim: Certain safety categories (e.g., substance_cannabis, crime_propaganda) consistently trigger unsafe responses due to pluralistic alignment gaps.
- Mechanism: These categories involve cultural and legal plurality, making consensus difficult; models default to unsafe generation when cultural context is ambiguous.
- Core assumption: Safety alignment is easier for hate-related content (broad consensus) than for pluralistic topics.
- Evidence anchors:
  - [abstract] "Conversely, categories such as substance_cannabis and crime_propaganda consistently trigger unsafe responses across models and languages."
  - [section] "Moreover, when evaluating QwQ and Qwen-Instruct models (see App. H), we find that they refuse to generate answers (e.g. fake news articles) significantly more often compared to all other models."
  - [corpus] Weak signal; no direct evidence of pluralistic alignment gaps in corpus.
- Break condition: Safety alignment protocols explicitly address pluralistic content or cultural context is encoded in prompts.

## Foundational Learning

- Concept: Machine translation quality estimation (MTQE)
  - Why needed here: M-ALERT relies on automated translation pipeline; translation quality directly affects safety consistency across languages.
  - Quick check question: What is the difference between COMET and MetricX metrics in MTQE, and why are both used?

- Concept: Cross-lingual consistency metrics
  - Why needed here: Evaluating exact matching rates across languages reveals safety misalignment not captured by overall safety scores.
  - Quick check question: How does exact matching rate differ from average safety score in detecting cross-lingual safety inconsistencies?

- Concept: Safety taxonomy granularity
  - Why needed here: M-ALERT uses ALERT taxonomy with 6 macro and 32 micro categories; granularity enables policy-aware safety evaluation.
  - Quick check question: Why is category-level annotation important for policy compliance evaluation in multilingual safety benchmarks?

## Architecture Onboarding

- Component map:
  Prompt generation → Translation pipeline → LLM inference → Safety evaluation (LlamaGuard-3) → Aggregation & reporting
  Key modules: Translation candidate selection, exact matching consistency checker, category-wise safety scorer

- Critical path:
  Generate prompt → Translate to 5 languages → Run each through LLM → Evaluate safety → Aggregate scores per language/category
  Bottleneck: Translation quality estimation and evaluator alignment with human judgments

- Design tradeoffs:
  Translation ensemble vs. single high-quality model: Ensemble improves coverage but increases latency
  Automated vs. human evaluation: Automated scales but may miss nuanced safety violations
  Category granularity vs. computational cost: More categories enable policy alignment but require more evaluator passes

- Failure signatures:
  Low inter-language exact matching rates → Translation or safety model misalignment
  High safety variance across categories → Dataset bias or evaluator limitations
  Base models underperforming instruct models → Lack of safety fine-tuning

- First 3 experiments:
  1. Run same prompt through multiple translation models and compare safety scores to isolate translation impact
  2. Evaluate safety consistency when excluding high-plurality categories (substance_cannabis, crime_propaganda)
  3. Compare base vs. instruct model safety scores across all categories to quantify safety tuning effect size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model size correlate with safety performance when comparing base and instruct-tuned models?
- Basis in paper: [explicit] The paper states "when disentangling between instruct and base models, we find a much clearer trend, in that base models show higher safety with increasing model size compared to instruction-tuned models."
- Why unresolved: While a trend is observed for base models, the relationship is not linear or universal, and the paper notes that "even very small models (<3B) show already high levels of safety."
- What evidence would resolve it: A comprehensive analysis of safety scores across a wider range of base and instruct-tuned models of varying sizes, controlling for other factors like training data and fine-tuning methods.

### Open Question 2
- Question: What is the impact of cultural context on the safety evaluation of LLMs across different languages?
- Basis in paper: [inferred] The paper highlights that certain categories like crime_propaganda and substance_cannabis consistently trigger unsafe responses across models and languages, suggesting that cultural differences in how these topics are perceived might influence safety scores.
- Why unresolved: The paper does not explicitly explore the role of cultural context in shaping safety evaluations, focusing instead on language-specific inconsistencies.
- What evidence would resolve it: Comparative studies of safety scores for the same prompts across cultures with similar languages but different legal and social norms, along with qualitative analysis of user perceptions of safety.

### Open Question 3
- Question: How do the safety inconsistencies across languages affect the real-world deployment of LLMs in multilingual settings?
- Basis in paper: [explicit] The paper states "we reveal significant safety inconsistencies across languages and categories, highlighting the importance of language-specific safety analysis" and emphasizes the need for "robust multilingual safety measures to ensure responsible LLM deployment globally."
- Why unresolved: The paper focuses on identifying inconsistencies but does not provide concrete guidance on how these inconsistencies should be addressed in practice.
- What evidence would resolve it: Case studies of LLM deployment in multilingual contexts, analyzing the impact of safety inconsistencies on user experience and trust, and evaluating the effectiveness of different mitigation strategies.

## Limitations

- The evaluation relies heavily on automated translation and safety assessment, creating potential cascading errors when either component fails.
- The safety evaluator (LlamaGuard-3) shows strong correlation with human judgments but still represents a significant source of uncertainty, particularly for low-resource languages.
- While translation quality is reported above COMET threshold of 0.5, the relationship between translation quality and safety evaluation accuracy remains unclear.

## Confidence

**High confidence**: Base vs. instruct model safety differences - The consistent pattern of instruct models outperforming base models across all languages and categories is well-supported by the experimental design and controls.

**Medium confidence**: Category-level safety patterns - While substance_cannabis and crime_propaganda show consistent unsafe behavior, the cultural and legal plurality mechanism remains speculative without direct evidence of how these categories specifically challenge alignment.

**Low confidence**: Cross-linguistic safety model degradation - The claim that safety performance degrades non-uniformly across languages due to data coverage misalignment is plausible but not directly tested; the paper shows correlation but not causation.

## Next Checks

1. **Translation quality impact isolation**: Run identical prompts through multiple translation models (Google Translate, DeepL, Claude) and compare safety scores to quantify translation's contribution to cross-linguistic inconsistencies.

2. **Evaluator alignment validation**: Create a small human-annotated test set (100-200 examples) spanning all 5 languages and problematic categories to measure LlamaGuard-3's accuracy in low-resource language contexts.

3. **Category removal experiment**: Remove substance_cannabis and crime_propaganda from evaluation and measure changes in cross-linguistic consistency rates to test whether pluralistic alignment gaps drive observed inconsistencies.