---
ver: rpa2
title: Geometry Informed Tokenization of Molecules for Language Model Generation
arxiv_id: '2408.10120'
source_url: https://arxiv.org/abs/2408.10120
tags:
- geo2seq
- generation
- molecules
- molecule
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating 3D molecular geometries
  using language models (LMs), which requires a method to discretize and tokenize
  3D molecular structures in a way that preserves their geometric and atomic information
  while remaining invariant to rotations and translations. The authors propose Geo2Seq,
  a tokenization method that first reorders atoms into a canonical labeling to preserve
  graph structure, then converts 3D Cartesian coordinates into SE(3)-invariant spherical
  representations (distances and angles).
---

# Geometry Informed Tokenization of Molecules for Language Model Generation

## Quick Facts
- arXiv ID: 2408.10120
- Source URL: https://arxiv.org/abs/2408.10120
- Reference count: 40
- This paper proposes Geo2Seq, a tokenization method that converts 3D molecular geometries into SE(3)-invariant sequences for language model generation

## Executive Summary
This paper tackles the problem of generating 3D molecular geometries using language models (LMs), which requires a method to discretize and tokenize 3D molecular structures in a way that preserves their geometric and atomic information while remaining invariant to rotations and translations. The authors propose Geo2Seq, a tokenization method that first reorders atoms into a canonical labeling to preserve graph structure, then converts 3D Cartesian coordinates into SE(3)-invariant spherical representations (distances and angles). This allows the generation of 1D discrete sequences that can be processed by LMs. Experiments show that when coupled with Geo2Seq, both GPT and Mamba LMs achieve strong performance in random 3D molecule generation, outperforming or matching state-of-the-art diffusion-based methods. In controllable generation tasks, Geo2Seq with Mamba significantly outperforms prior methods in generating molecules with specific quantum properties.

## Method Summary
The Geo2Seq method converts 3D molecular structures into SE(3)-invariant 1D discrete sequences through canonical labeling followed by spherical coordinate conversion. The approach first reorders atoms into a canonical labeling to preserve graph structure, then transforms Cartesian coordinates into distances and angles relative to an equivariant global frame. This produces a discrete sequence of atom type and geometric tokens that can be processed by language models. The method was evaluated using GPT and Mamba architectures trained on QM9 and GEOM-DRUGS datasets, with both random generation and controllable generation tasks assessed using metrics including atom stability, molecule stability, valid percentage, and mean absolute error on quantum properties.

## Key Results
- Geo2Seq with GPT and Mamba LMs achieved strong performance in random 3D molecule generation, outperforming or matching state-of-the-art diffusion-based methods
- Geo2Seq with Mamba significantly outperformed prior methods in generating molecules with specific quantum properties
- The method achieved atom stability percentages of 87.8% for QM9 and 81.8% for GEOM-DRUGS datasets

## Why This Works (Mechanism)

### Mechanism 1
Converting 3D molecular geometries into SE(3)-invariant spherical coordinates preserves essential geometric information while enabling language models to process them as discrete sequences. The Geo2Seq method uses canonical labeling to establish a unique node order and then transforms Cartesian coordinates into distances and angles relative to an equivariant global frame. This transformation removes sensitivity to rotations and translations while retaining all structural information. Core assumption: Spherical coordinates can uniquely encode 3D molecular geometry when combined with canonical atom ordering, and the discretization error is small enough not to impact model performance.

### Mechanism 2
Language models can effectively learn the distribution of 3D molecular structures when represented as discrete token sequences. By treating the concatenated atom type and spherical coordinate tokens as a sequence, GPT and Mamba models can learn the conditional probability distributions needed for both random and controlled generation through standard autoregressive training. Core assumption: The vocabulary size (1K-16K tokens) is sufficient to represent the diversity of 3D molecular structures without excessive information loss.

### Mechanism 3
Conditional generation using property tokens as initial inputs allows language models to generate molecules with specific quantum properties. The property token is projected through a trainable linear layer to match the model's initial token embedding space, then used as the first element in the molecular sequence. This conditions the generation process on desired properties. Core assumption: The language model can capture long-range correlations between the property token and the molecular structure tokens that follow.

## Foundational Learning

- **Canonical labeling of graphs**: Establishes a unique, invariant ordering of atoms that preserves all structural information while enabling conversion to a 1D sequence. Why needed here: Provides a consistent atom ordering that makes the sequence representation unique for each molecular structure. Quick check: Why can't we just use any arbitrary ordering of atoms when converting 3D structures to sequences?

- **SE(3) invariance in geometric representations**: Ensures that molecular structures that are rotations or translations of each other are treated as identical, which is essential for learning valid molecular distributions. Why needed here: Prevents the model from learning redundant variations of the same molecule. Quick check: How do spherical coordinates ensure invariance to rotations while Cartesian coordinates do not?

- **Autoregressive language modeling**: Enables the model to learn the joint distribution of molecular sequences by predicting each token conditioned on previous tokens. Why needed here: Allows the model to generate complete molecular structures token by token. Quick check: Why is next-token prediction cross-entropy loss appropriate for this task rather than other loss functions?

## Architecture Onboarding

- **Component map**: 3D molecule -> Canonical labeling -> Spherical coordinate conversion -> Tokenization -> Language Model (GPT/Mamba) -> Generator/Sampler
- **Critical path**: Input 3D molecule -> Canonical labeling -> Spherical coordinate conversion -> Tokenization -> LM forward pass -> Token sampling -> Sequence reconstruction
- **Design tradeoffs**: Higher tokenization precision improves geometric fidelity but increases vocabulary size and computational cost; using spherical vs Cartesian coordinates trades off between SE(3) invariance and potential information loss in angle representations; choice between GPT (attention-based) and Mamba (state-space) architectures involves balancing modeling capacity with computational efficiency
- **Failure signatures**: Low atom stability percentages indicate tokenization or spherical coordinate conversion errors; poor conditional generation performance suggests the property token conditioning mechanism isn't effectively influencing the generation; high generation time compared to diffusion methods may indicate suboptimal model architecture choices
- **First 3 experiments**:
  1. Test Geo2Seq conversion: Take a simple molecule (e.g., H2O), convert to sequence, then reconstruct and verify geometric fidelity
  2. Validate SE(3) invariance: Rotate a molecule, convert both versions, and confirm identical sequences
  3. Verify canonical labeling: Test that isomorphic molecules produce identical sequences while non-isomorphic ones produce different sequences

## Open Questions the Paper Calls Out

- How does the performance of Geo2Seq scale with larger language models, particularly those with billions of parameters? The paper mentions that future work could explore pretraining on larger datasets and using larger language models, but the experiments only used relatively small language models (12-14 layers for GPT and 26-28 layers for Mamba).

- Can Geo2Seq be extended to handle more complex molecular structures, such as those with rings, branches, or large molecules with hundreds of atoms? The paper acknowledges that Geo2Seq achieves good performance on small molecules from the QM9 dataset but does not explore its performance on larger, more complex molecules.

- How can Geo2Seq be improved to better handle the continuous nature of 3D molecular structures and reduce the information loss introduced by discretization? The paper acknowledges that the discretization of numerical values to tokens introduces information loss and limits the search space of 3D molecular structures, but doesn't propose specific improvements.

## Limitations

- The theoretical guarantee for SE(3)-invariance relies on discretization precision parameter b being sufficiently large, but the paper doesn't validate the lower bound on b needed to preserve all structurally distinct molecules
- The property-token conditioning mechanism lacks transparency about whether it actually learns meaningful correlations between property tokens and molecular structures
- Performance evaluation is primarily on QM9 and GEOM-DRUGS datasets, which have limited chemical diversity compared to all possible drug-like molecules

## Confidence

**High confidence claims**:
- Language models can generate valid 3D molecular geometries when paired with appropriate tokenization
- Geo2Seq tokenization produces SE(3)-invariant representations (though discretization limits exist)
- The method outperforms diffusion-based approaches on the tested datasets

**Medium confidence claims**:
- Conditional generation quality matches or exceeds state-of-the-art methods
- The specific Mamba architecture consistently outperforms GPT variants for this task
- The proposed method generalizes well beyond the tested dataset distributions

**Low confidence claims**:
- The theoretical guarantees for SE(3)-invariance hold for all practically relevant molecular structures
- The property-token conditioning mechanism is the optimal approach for controllable generation
- The method scales effectively to much larger, more diverse molecular datasets

## Next Checks

1. **Discretization sensitivity analysis**: Systematically vary the precision parameter b in Geo2Seq and measure how atom stability and molecule stability metrics degrade to establish practical limits of the SE(3)-invariance guarantee.

2. **Isomorphism verification**: Generate pairs of structurally identical molecules with different orientations, tokenize both, and verify they produce identical sequences. Then test non-isomorphic but geometrically similar molecules to ensure they remain distinguishable.

3. **Property-token ablation study**: Train models with and without property-token conditioning on the same datasets, then compare conditional generation performance. Additionally, test whether randomly initialized property tokens produce similar results to reveal whether the conditioning mechanism actually learns meaningful correlations.