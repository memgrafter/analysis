---
ver: rpa2
title: Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models
arxiv_id: '2405.03794'
source_url: https://arxiv.org/abs/2405.03794
tags:
- hate
- speech
- learning
- data
- transformer-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the detection of anti-Semitic hate speech
  on social media platforms. The researchers developed a new data labeling technique
  and applied various transformer-based models, including BERT, DistilBERT, RoBERTa,
  and LLaMA-2, along with the LoRA fine-tuning approach.
---

# Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models

## Quick Facts
- arXiv ID: 2405.03794
- Source URL: https://arxiv.org/abs/2405.03794
- Authors: Dengyi Liu; Minghao Wang; Andrew G. Catlin
- Reference count: 22
- Primary result: Transformer-based models, particularly BERT, outperform traditional ML models in detecting anti-Semitic hate speech with 0.94 accuracy and 0.81 F1-score

## Executive Summary
This study addresses the detection of anti-Semitic hate speech on social media platforms by developing a new data labeling technique and applying various transformer-based models. The researchers trained these models on approximately 3,000 annotated Twitter posts and compared their performance against traditional machine learning approaches. The results demonstrate that transformer-based models, especially BERT, significantly outperform traditional methods in accuracy, precision, recall, and F1-score, highlighting the effectiveness of advanced NLP techniques for hate speech detection.

## Method Summary
The study collected approximately 10,000 Twitter posts related to Jewish topics and annotated a subset of 3,000 posts for anti-Semitic content using a threshold-based labeling method with scores ranging from 0-10 and a threshold of 6. Traditional machine learning models (Naive Bayes, SVM, Random Forests, Logistic Regression, K-NN) were trained with various embeddings including CountVectorizer, TfidfVectorizer, and word embeddings. Transformer-based models (BERT, DistilBERT, RoBERTa, LLaMA-2) were fine-tuned using the LoRA approach. Model performance was evaluated using Accuracy, Precision, Recall, and F1-score metrics.

## Key Results
- BERT achieved the highest performance with 0.94 accuracy and 0.81 F1-score
- Transformer-based models consistently outperformed traditional machine learning models across all evaluation metrics
- LoRA fine-tuning enabled efficient adaptation of large models like LLaMA-2 while maintaining high performance
- Traditional models showed limited effectiveness, particularly with Word2Vec and GloVe embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based models, especially BERT, outperform traditional machine learning models on hate speech detection because they capture bidirectional contextual embeddings.
- Mechanism: BERT uses bidirectional self-attention to model word relationships in context, unlike unidirectional models or static embeddings. This allows detection of subtle linguistic cues in anti-Semitic hate speech.
- Core assumption: Contextual embeddings provide better semantic understanding than frequency-based or word-order agnostic features for detecting hate speech.
- Evidence anchors:
  - [abstract] "BERT achieved an accuracy of 0.94 and an F1-score of 0.81."
  - [section 2.4] "BERT (Bidirectional Encoder Representations from Transformers), developed by Devlin et al. (2019), represented a landmark in pre-trained models, allowing for an even more nuanced understanding of language through bidirectional training."
  - [corpus] Weak: No direct neighbor evidence comparing BERT to traditional embeddings.

### Mechanism 2
- Claim: Fine-tuning with LoRA significantly reduces computational cost while maintaining detection performance for large models like LLaMA-2.
- Mechanism: LoRA introduces low-rank matrices that adapt pre-trained weights instead of updating all parameters, preserving model quality with fewer trainable parameters.
- Core assumption: Most model capacity is already suitable for the task; only a small subspace needs adaptation.
- Evidence anchors:
  - [section 2.4] "LoRA (Low-Rank Adaptation)...can be applied to models like RoBERTa. LoRA allows for efficiently adapting large pre-trained models with minimal additional parameters, preserving the original parameters while achieving comparable performance to full fine-tuning approaches."
  - [table 2] "Llama-2 (7B) + lora 0.92 0.80 0.79 0.83" shows high F1-score with limited epochs.
  - [corpus] Weak: No neighbor papers explicitly discussing LoRA in hate speech detection.

### Mechanism 3
- Claim: The consensus-based annotation strategy with a threshold (score ≥ 6) and a third-party tiebreaker improves label quality and reduces bias in hate speech datasets.
- Mechanism: Independent scoring followed by dispute resolution ensures only high-confidence examples are included, minimizing false positives/negatives due to subjective judgment.
- Core assumption: Hate speech detection requires human judgment to capture context; agreement increases reliability.
- Evidence anchors:
  - [section 3.1] "For posts where the two initial annotators disagreed, a distinctive procedure was implemented...a joint discussion session...sought."
  - [abstract] "we developed a new data labeling technique" implying structured human oversight.
  - [corpus] Weak: No direct evidence from neighbor studies on annotation protocols.

## Foundational Learning

- Concept: Transformer architecture and self-attention
  - Why needed here: Core to understanding why BERT/RoBERTa/LLAMA-2 excel in contextual language modeling.
  - Quick check question: How does bidirectional self-attention differ from unidirectional attention in capturing context?

- Concept: Embedding methods (TF-IDF, Word2Vec, GloVe)
  - Why needed here: Basis for comparing traditional ML performance with transformer-based methods.
  - Quick check question: Why might Word2Vec fail to outperform sparse vectorizers in certain classification tasks?

- Concept: Fine-tuning vs. full retraining
  - Why needed here: Explains the choice of LoRA for efficient adaptation of large models.
  - Quick check question: What is the trade-off between parameter efficiency and task-specific accuracy when using LoRA?

## Architecture Onboarding

- Component map:
  Data pipeline: Twitter API → preprocessing → annotation → train baseline ML models → fine-tune transformers → compare metrics → iterate on LoRA hyperparameters

- Critical path:
  Data annotation → train baseline ML models → fine-tune transformers → compare metrics → iterate on LoRA hyperparameters

- Design tradeoffs:
  - Model size vs. inference latency: BERT smaller and faster than RoBERTa, LLaMA-2 much larger
  - Annotation cost vs. label quality: Consensus-based labeling is slower but reduces noise
  - LoRA rank vs. fine-tuning quality: Lower rank faster but may hurt performance

- Failure signatures:
  - Low recall in traditional models → missed nuanced hate speech
  - High variance in metrics across embeddings → embedding-model mismatch
  - Poor LoRA convergence → rank too low or learning rate too aggressive

- First 3 experiments:
  1. Train and evaluate all traditional ML models with TF-IDF embedding; record baseline metrics.
  2. Fine-tune BERT from scratch on the dataset with standard approach; compare performance to baseline.
  3. Apply LoRA fine-tuning to RoBERTa and LLaMA-2 with varying ranks; assess trade-offs in speed vs. accuracy.

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- The study focuses solely on anti-Semitic hate speech, limiting generalizability to other forms of hate speech
- The 3,000 annotated posts may not capture the full diversity of hate speech across different contexts and platforms
- Large models like LLaMA-2, even with LoRA, may be impractical for real-world deployment due to resource constraints

## Confidence
- High Confidence: Transformer models outperform traditional ML (BERT: 0.94 accuracy, 0.81 F1-score)
- Medium Confidence: LoRA reduces computational costs while maintaining performance
- Low Confidence: Consensus-based annotation significantly improves label quality and reduces bias

## Next Checks
1. Validate model performance on a larger, more diverse dataset including anti-Semitic hate speech from multiple platforms and languages
2. Conduct an ablation study comparing LoRA-tuned models against full fine-tuning baselines
3. Perform a bias analysis of the consensus-based annotation process by comparing results with alternative labeling methods