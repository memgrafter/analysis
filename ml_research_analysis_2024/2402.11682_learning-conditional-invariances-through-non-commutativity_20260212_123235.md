---
ver: rpa2
title: Learning Conditional Invariances through Non-Commutativity
arxiv_id: '2402.11682'
source_url: https://arxiv.org/abs/2402.11682
tags:
- domain
- target
- domains
- source
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses domain adaptation by relaxing the invariance
  criterion to be non-commutatively directed towards the target domain. The authors
  show that under domain asymmetry, where the target domain contains more semantically
  relevant information than the source domain, the optimal target-specific encoder
  can be learned more efficiently using source domain samples as augmentations.
---

# Learning Conditional Invariances through Non-Commutativity

## Quick Facts
- arXiv ID: 2402.11682
- Source URL: https://arxiv.org/abs/2402.11682
- Authors: Abhra Chaudhuri; Serban Georgescu; Anjan Dutta
- Reference count: 32
- This paper addresses domain adaptation by relaxing the invariance criterion to be non-commutatively directed towards the target domain, achieving up to 2% higher accuracy than state-of-the-art methods.

## Executive Summary
This paper introduces non-commutative invariance (NCI) as a novel approach to domain adaptation that exploits domain asymmetry, where the target domain contains more semantically relevant information than the source domain. Unlike traditional commutative invariance methods, NCI maps source domain samples to the target domain representation space while preserving target-specific features. The authors prove that this approach reduces the H-divergence between domains to zero, leading to stricter generalization bounds and superior performance. Empirical results on standard benchmarks (PACS, Office-Home, DomainNet) show that NCI surpasses state-of-the-art invariance learning algorithms for domain adaptation by up to 2% in accuracy.

## Method Summary
The method implements non-commutative invariance through a Domain Adversarial Neural Network (DANN) framework where the encoder φτ is trained to map source domain samples to the target domain representation space while maintaining target-specific features. The discriminator η attempts to distinguish between source and target encodings, while φτ is updated to fool the discriminator specifically on source samples. This creates a non-commutative relationship where source samples are treated as augmentations to help learn the optimal target-specific encoder Φ*τ. The approach leverages the assumption that target domains contain more semantically relevant information than source domains, allowing source samples to supplement target samples in meeting sample complexity requirements.

## Key Results
- NCI reduces H-divergence between domains to zero, achieving stricter generalization bounds than commutative invariance methods
- NCI achieves up to 2% higher accuracy than state-of-the-art invariance learning algorithms on PACS, Office-Home, and DomainNet benchmarks
- Under domain asymmetry, NCI can leverage source domain samples to meet sample complexity needs for learning the optimal target encoder
- NCI approaches the performance of an oracle with access to all domains at test time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-commutative invariance (NCI) achieves stricter generalization bounds than commutative invariance methods.
- Mechanism: NCI maps source samples to target domain representation space while preserving target-specific features, reducing H-divergence to zero.
- Core assumption: Domain asymmetry exists where target domain contains more semantically relevant information than source domain (Assumption 1).
- Evidence anchors:
  - [abstract]: "non-commutativity steers the optimization towards Φ*τ instead of φ*, bringing the H-divergence between domains down to zero"
  - [section 3.2]: "ˆdHη(S, T, φ) does not reach 0 if φ(·) is trained with commutative invariance"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: When source and target domains have equal or similar semantic information content, the asymmetry assumption fails and NCI provides no advantage.

### Mechanism 2
- Claim: NCI can leverage source domain samples to meet the sample complexity needs of learning the optimal target encoder.
- Mechanism: Source domain samples act as augmentations in the target domain representation space when using non-commutative invariance.
- Core assumption: Source and target domain samples are semantically complementary (different supports).
- Evidence anchors:
  - [abstract]: "NCI can leverage source domain samples to meet the sample complexity needs of learning Φ*τ"
  - [section 3.3 Theorem 2]: "With mτ target domain samples and ms source domains samples, such that M ≤ ms + mτ, M > mτ, the optimal NCI encoder φ*τ(·) achieves the same error bound ϵ with probability δ"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: When source and target domains share identical sample supports or when target domain has sufficient samples alone to meet sample complexity requirements.

### Mechanism 3
- Claim: The optimal encoder for target domain φ*τ achieves better performance than encoder optimal on average across domains φ*.
- Mechanism: Under domain asymmetry, the risk of φ* is strictly lower-bounded by the risk of φ*τ.
- Core assumption: Target domain contains more label information than source domain (I(Φ*τ(xτ); y) > I(Φ*s(xs); y)).
- Evidence anchors:
  - [abstract]: "the risk of the encoder φ* that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder Φ*τ"
  - [section 3.3 Theorem 1]: "R(φ*) ≥ l(Φ*τ(Dτ), Y)"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: When target and source domains have equal information content or when learning task requires global optimality across all domains rather than target-specific performance.

## Foundational Learning

- Concept: H-divergence as domain discrepancy measure
  - Why needed here: Forms the basis for generalization bounds and comparison between commutative and non-commutative invariance
  - Quick check question: How does H-divergence differ from other domain discrepancy measures like A-distance or Wasserstein distance?

- Concept: VC dimension and sample complexity
  - Why needed here: Used to establish when source samples can supplement target samples for achieving optimal target encoder performance
  - Quick check question: What is the relationship between VC dimension of hypothesis class and the number of samples needed to achieve a given error bound?

- Concept: Domain asymmetry and information theory
  - Why needed here: Core assumption that justifies why NCI outperforms commutative invariance methods
  - Quick check question: How can we formally quantify the information difference between source and target domains using mutual information?

## Architecture Onboarding

- Component map: Source samples → Encoder φτ → Discriminator η → Target space mapping

- Critical path:
  1. Forward pass: Encode source and target samples through φτ
  2. Discriminator η distinguishes between source and target encodings
  3. Backpropagate: Update φτ to fool discriminator on source samples only
  4. Update η to correctly classify source vs target
  5. Repeat until convergence

- Design tradeoffs:
  - Direction of non-commutativity: Right-invariant vs left-invariant (affects which domain's features are preserved)
  - Architecture depth: Deeper encoders may capture more complex invariances but risk overfitting
  - Discriminator capacity: Higher capacity discriminators may provide stronger invariance but slower convergence

- Failure signatures:
  - If source and target domains become indistinguishable: Direction of non-commutativity may be reversed
  - If performance degrades on target domain: Source samples may not be sufficiently complementary
  - If training instability: Learning rates for encoder and discriminator may be mismatched

- First 3 experiments:
  1. Baseline: Train with commutative invariance (standard DANN) on PACS dataset, measure performance drop when target domain has unique features
  2. Direction test: Implement both right-invariant and left-invariant NCI, compare target domain performance on Office-Home
  3. Sample complementarity: Vary degree of shared vs unique samples between source and target domains, measure impact on target accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the non-commutative invariance (NCI) solution of learning Φ*τ unique?
- Basis in paper: [explicit] The paper conjectures that there could be multiple ways to bring the source-target Hη-divergence down to 0, but does not prove the uniqueness of the NCI solution.
- Why unresolved: The paper does not provide a rigorous proof or analysis of the uniqueness of the NCI solution under the given semantic constraints.
- What evidence would resolve it: A formal proof demonstrating that the NCI solution is unique, or an empirical study showing that different implementations of NCI converge to the same optimal encoder Φ*τ.

### Open Question 2
- Does φ*τ → Φ*τ in a measure-theoretic sense, and not just in terms of sample complexity or error bound?
- Basis in paper: [inferred] The paper shows that NCI can leverage source domain samples to meet the sample complexity needs of learning the optimal target encoder Φ*τ, but does not establish a measure-theoretic convergence.
- Why unresolved: The paper does not provide a rigorous analysis of the convergence of φ*τ to Φ*τ in terms of probability measures or other measure-theoretic concepts.
- What evidence would resolve it: A formal proof demonstrating the measure-theoretic convergence of φ*τ to Φ*τ, or an empirical study showing that the distributions of the representations learned by φ*τ and Φ*τ converge as the number of source samples increases.

### Open Question 3
- What is the relationship between class-conditional invariance learning and NCI?
- Basis in paper: [inferred] The paper focuses on domain-level non-commutative invariance, but does not explore the relationship between this approach and class-conditional invariance learning methods.
- Why unresolved: The paper does not provide a formal analysis or empirical comparison of NCI with class-conditional invariance learning approaches.
- What evidence would resolve it: A theoretical analysis of the relationship between NCI and class-conditional invariance learning, or an empirical study comparing the performance of NCI with state-of-the-art class-conditional invariance learning methods on various domain adaptation benchmarks.

## Limitations
- Assumption 1 (domain asymmetry) is critical but may not be universally applicable
- Limited ablation studies on direction of non-commutativity (right vs left invariant)
- No analysis of computational overhead compared to commutative methods
- Theoretical results don't fully address practical concerns like training instability

## Confidence
- Theoretical claims about H-divergence reduction: Medium
- Empirical performance claims (2% improvement): High
- Claims about source samples as augmentations: Medium

## Next Checks
1. Conduct experiments on datasets where source and target domains have equal semantic information content to test the limits of NCI's advantage
2. Perform ablation studies comparing right-invariant vs left-invariant NCI implementations to determine optimal direction
3. Measure and compare computational overhead (training time, memory) between NCI and commutative invariance methods