---
ver: rpa2
title: 'Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description'
arxiv_id: '2405.18937'
source_url: https://arxiv.org/abs/2405.18937
tags:
- segmentation
- grounding
- part-aware
- point
- grounded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Kestrel is a part-aware 3D multimodal large language model (MLLM)
  that achieves state-of-the-art performance in part-aware language comprehension
  and segmentation grounding of 3D objects. It introduces two novel tasks: part-aware
  point grounding and part-aware point grounded captioning, and curates the 3DCoMPaT-GRIN
  dataset to support these tasks.'
---

# Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description

## Quick Facts
- arXiv ID: 2405.18937
- Source URL: https://arxiv.org/abs/2405.18937
- Authors: Mahmoud Ahmed; Junjie Fei; Jian Ding; Eslam Mohamed Bakr; Mohamed Elhoseiny
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance in part-aware language comprehension and segmentation grounding of 3D objects, with 54.9% IoU on part grounding, 50.1% IoU on material grounding, and 56.2% IoU on compositional grounding on the 3DCoMPaT-GRIN dataset.

## Executive Summary
Kestrel introduces a novel 3D multimodal large language model (MLLM) designed for part-aware understanding and segmentation grounding of 3D objects. The model integrates a dedicated 3D segmentation grounding module with an advanced language model, enabling it to generate detailed part-level descriptions and corresponding segmentation masks from natural language instructions. Kestrel introduces two novel tasks—Part-Aware Point Grounding and Part-Aware Point Grounded Captioning—and curates the 3DCoMPaT-GRIN dataset to support these tasks. The model significantly outperforms existing baselines on part-aware comprehension and segmentation, demonstrating its effectiveness in bridging language and 3D spatial understanding at the part level.

## Method Summary
Kestrel extends a pre-trained 3D vision-language model by integrating a dedicated 3D segmentation grounding module. The model processes 3D point clouds alongside textual instructions, using a [SEG] token to trigger the segmentation module. The hidden states of [SEG] tokens are projected into query vectors that guide a Transformer-based decoder to iteratively predict part-level segmentation masks through cross-attention with point cloud features. The approach is trained on the 3DCoMPaT-GRIN dataset, which provides part-level segmentation masks and corresponding instructions for 3D objects. The model supports both part-aware point grounding (generating segmentation masks from instructions) and part-aware point grounded captioning (generating detailed captions with part-level descriptions).

## Key Results
- Achieves 54.9% IoU on part grounding, 50.1% IoU on material grounding, and 56.2% IoU on compositional grounding on the 3DCoMPaT-GRIN dataset
- Generates detailed captions with part-level descriptions and corresponding segmentation masks
- Significantly outperforms baselines on part-aware language comprehension and segmentation grounding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The integration of a dedicated 3D segmentation grounding module enables voxel-wise part-level mask prediction that traditional 3D MLLMs cannot perform.
- Mechanism: The model adds a segmentation token [SEG] to the MLLM vocabulary, which triggers the segmentation grounding module. The hidden states of [SEG] tokens are projected into query vectors for a Transformer-based decoder that cross-attends with point cloud features to iteratively predict part-level segmentation masks.
- Core assumption: The [SEG] token's hidden states contain sufficient semantic information to be projected into queries that can guide the decoder to generate accurate segmentation masks.
- Evidence anchors:
  - [abstract] states: "Kestrel integrates an advanced language model for nuanced language comprehension with multi-level point feature propagation and query refinement mechanism to enhance spatial reasoning at the part level."
  - [section 4.2] explains: "The 3D segmentation grounding module's feature encoder fE is employed to encode the input point cloud xpc into point features hmask... we extract all hidden states hseg corresponding to the [SEG] tokens... map them into the latent space of the 3D feature decoder using a projection layer fP , resulting in qinit."
  - [corpus] evidence is missing; no direct comparison with segmentation baselines is provided.
- Break condition: If the projection layer fails to map [SEG] hidden states into meaningful query vectors, the decoder will not be able to generate accurate segmentation masks.

### Mechanism 2
- Claim: The two novel tasks (Part-Aware Point Grounding and Part-Aware Point Grounded Captioning) explicitly train the model to associate natural language instructions with part-level segmentation masks.
- Mechanism: In Part-Aware Point Grounding, the model learns to predict segmentation masks from instructions like "Please segment the
door and the seat of the bus" by mapping semantic understanding to spatial regions. In Part-Aware Point Grounded Captioning, the model generates detailed captions that describe parts and materials while simultaneously producing segmentation masks, reinforcing the language-segmentation mapping through joint training.
- Core assumption: The model can learn a joint embedding space where textual descriptions and spatial part regions are meaningfully aligned.
- Evidence anchors:
  - [abstract] states: "The model is trained on the 3DCoMPaT-GRIN dataset, which provides part-level segmentation masks and corresponding instructions for 3D objects."
  - [section 4.1] details: "We design two tasks: Part-Aware Point Grounding, where the model predicts segmentation masks from instructions, and Part-Aware Point Grounded Captioning, where the model generates captions with part-level descriptions and corresponding masks."
  - [corpus] evidence is missing; no ablation study isolating the impact of these specific tasks is provided.
- Break condition: If the dataset lacks sufficient diversity in instructions or if the language-segmentation alignment is too weak, the model may fail to generalize to novel instructions or object types.

## Foundational Learning

### 3D Point Cloud Processing
- **Why needed**: 3D objects are represented as point clouds (sets of 3D coordinates), requiring specialized feature extraction to capture spatial relationships and part structures.
- **Quick check**: Verify the model uses point-based encoders (e.g., PointNet++, DGCNN) to convert unordered point sets into structured feature representations.

### Multimodal Alignment
- **Why needed**: The model must align textual descriptions with 3D spatial regions, requiring cross-modal fusion between language and geometry.
- **Quick check**: Confirm the use of cross-attention mechanisms or joint embedding spaces that map text tokens to 3D feature points.

### Part-Level Segmentation
- **Why needed**: Unlike whole-object understanding, part-aware tasks require precise localization and labeling of individual object components.
- **Quick check**: Ensure the model outputs voxel-wise or point-wise segmentation masks, not just bounding boxes or coarse regions.

## Architecture Onboarding

### Component Map
Point Cloud Encoder -> Feature Propagation -> Segmentation Grounding Module -> Segmentation Decoder -> Segmentation Masks

### Critical Path
The critical path flows from the [SEG] token through the projection layer to the segmentation decoder, where cross-attention with point cloud features generates the final masks. Any bottleneck in this path (e.g., weak projection layer or inefficient decoder) directly impacts segmentation quality.

### Design Tradeoffs
The integration of a dedicated segmentation module adds complexity and parameters but enables fine-grained part-level understanding that standard MLLMs cannot achieve. The tradeoff is between model size/compute and the ability to perform voxel-wise segmentation.

### Failure Signatures
- Poor segmentation masks with blurred boundaries or missing parts indicate issues in feature propagation or decoder attention.
- Inability to follow complex instructions suggests misalignment between language and spatial features.
- High computational cost or memory issues may arise from the segmentation decoder's iterative refinement process.

### First Experiments to Run
1. Test the model's response to simple part segmentation instructions (e.g., "segment the wheel") to verify basic functionality.
2. Evaluate segmentation quality on objects with few parts to assess the model's ability to distinguish between components.
3. Measure inference time and memory usage to benchmark computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Kestrel's part-aware segmentation grounding capability be effectively extended to 3D scenes beyond single objects?
- Basis in paper: [inferred] The paper mentions in the "Limitation and Future Works" section that "The proposed 3DCoMPaT-GRIN takes a step forward in part-aware segmentation grounding for 3D objects. Our subsequent aim is to extend this capability beyond single objects and thereby enhance interaction between AI and the 3D world."
- Why unresolved: The paper focuses on part-aware understanding and segmentation grounding for individual 3D objects, but does not explore how well this approach generalizes to complex 3D scenes with multiple objects and interactions.
- What evidence would resolve it: Experiments demonstrating Kestrel's performance on 3D scene understanding and segmentation tasks, including multi-object scenes and interactions between parts of different objects.

### Open Question 2
- Question: How does the choice of language model size (7B vs 13B) impact Kestrel's performance on part-aware tasks, and what is the optimal model size for balancing performance and computational efficiency?
- Basis in paper: [explicit] The paper states "When scaling up the LLM from 7B to 13B, Kestrel's performance slightly drops from 54.9 to 51.5 in validation Overall IoU, and from 56.6 to 52.7 in test Overall IoU. We speculate that this decline may be attributed to the larger model size making more converge more challenging."
- Why unresolved: While the paper observes a performance drop with larger models, it does not provide a comprehensive analysis of the trade-offs between model size, performance, and computational efficiency. The optimal model size for different applications and resource constraints remains unclear.
- What evidence would resolve it: A systematic study varying model sizes and analyzing the performance and computational costs, potentially identifying an optimal balance between accuracy and efficiency.

### Open Question 3
- Question: Can the 3DCoMPaT-GRIN dataset be expanded to include more diverse part-level attributes, such as geometric descriptions, to further enhance part-aware understanding?
- Basis in paper: [explicit] The paper mentions in the "Limitation and Future Works" section that "3DCoMPaT-GRIN offers part-level segmentation grounding annotations, e.g., part and material masks. However, we can still expand the annotation scope to include more part-level attributes such as describing geometry."
- Why unresolved: The current dataset focuses on part and material segmentation masks, but does not include richer part-level attributes like geometric descriptions. Incorporating these attributes could potentially improve the model's understanding of 3D objects and their parts.
- What evidence would resolve it: Experiments demonstrating the impact of including additional part-level attributes (e.g., geometric descriptions) on the model's performance and understanding of 3D objects and their parts.

## Limitations

- Evaluation is conducted entirely on a single dataset (3DCoMPaT-GRIN) that was specifically curated for this work, raising concerns about generalizability to other domains or object categories.
- The model's reliance on point cloud inputs limits its applicability to scenarios where 3D mesh or voxel representations are available.
- The complexity of the segmentation grounding module introduces computational overhead that is not fully characterized in terms of inference time or memory requirements.

## Confidence

**High Confidence**: The architectural design integrating segmentation grounding with MLLM capabilities is technically sound and the reported quantitative improvements over baselines on the 3DCoMPaT-GRIN dataset are likely accurate, given the specific evaluation metrics and dataset used.

**Medium Confidence**: The qualitative demonstration of part-aware segmentation masks generation shows promise, but the visual results provided are limited in scope and may not represent the model's full performance envelope across diverse object types and complex scenes.

**Low Confidence**: Claims about the model's ability to generalize to real-world applications or novel object categories are not substantiated by experiments beyond the curated dataset, and the paper does not address potential failure modes in practical deployment scenarios.

## Next Checks

1. **Cross-dataset validation**: Evaluate Kestrel's performance on independent 3D object datasets (e.g., PartNet, ShapeNet) that were not used in training to assess generalization capabilities across different object categories and segmentation annotation styles.

2. **Ablation studies on module components**: Systematically remove or replace individual components of the segmentation grounding module (projection layer, segmentation decoder) to quantify their individual contributions to the overall performance and identify potential bottlenecks.

3. **Computational efficiency benchmarking**: Measure and report inference time, memory usage, and parameter count for the full model pipeline, including comparisons with baseline approaches to provide a complete picture of the practical deployment considerations.