---
ver: rpa2
title: 'PRIMO: Progressive Induction for Multi-hop Open Rule Generation'
arxiv_id: '2411.01205'
source_url: https://arxiv.org/abs/2411.01205
tags:
- rule
- atoms
- generation
- open
- primo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRIMO addresses multi-hop open rule generation by introducing a
  progressive multi-stage framework that leverages ontology information and employs
  generation, extraction, and ranking modules to produce logically consistent and
  diverse rules. The approach introduces entity type information during generation
  to reduce ambiguity and uses reinforcement learning from human feedback to further
  optimize performance.
---

# PRIMO: Progressive Induction for Multi-hop Open Rule Generation

## Quick Facts
- arXiv ID: 2411.01205
- Source URL: https://arxiv.org/abs/2411.01205
- Authors: Jianyu Liu; Sheng Bi; Guilin Qi
- Reference count: 8
- Key outcome: PRIMO significantly outperforms baseline models, achieving BLEU-1 of 44.3 and BLEU-4 of 2.1 while reducing rule atom repetition rates by at least 16.2% compared to existing methods.

## Executive Summary
PRIMO addresses the challenge of multi-hop open rule generation by introducing a progressive multi-stage framework that leverages ontology information to produce logically consistent and diverse rules. The approach combines generation, extraction, and ranking modules with reinforcement learning from human feedback to optimize performance. Experimental results demonstrate that PRIMO achieves state-of-the-art performance while using significantly fewer parameters than large language models, successfully addressing issues of logical inconsistency and semantic repetition in multi-hop rule generation.

## Method Summary
PRIMO implements a three-stage progressive framework consisting of Generation, Extraction, and Ranking modules to produce multi-hop open rules from premise atoms. The method introduces entity type information during generation to reduce ambiguity and employs reinforcement learning from human feedback (RLHF) to optimize the model. GPT-2 models serve as the Generation and Extraction modules, while BERT handles ranking, with the entire system fine-tuned on a benchmark dataset of 2851 samples. The framework achieves performance close to large language models while using less than a tenth of the parameters.

## Key Results
- PRIMO achieves BLEU-1 score of 44.3 and BLEU-4 score of 2.1, significantly outperforming baseline models
- The framework reduces rule atom repetition rates by at least 16.2% compared to existing methods
- PRIMO uses only 774 million parameters total compared to 13 billion+ for large language models while maintaining comparable performance
- The system successfully generates logically consistent multi-hop rules with improved diversity across different chain lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive multi-stage architecture reduces logical inconsistency in multi-hop rule generation
- Mechanism: Breaking generation into Generation, Extraction, and Ranking stages allows each component to focus on specific aspects of rule quality. The Generation module creates descriptive text about potential relations, the Extraction module filters and retrieves specific hypothesis atoms from this text, and the Ranking module evaluates the plausibility of candidates against existing atoms.
- Core assumption: Each stage can be optimized independently without losing information flow between stages
- Evidence anchors:
  - [abstract]: "PRIMO constructs a multi-stage structure consisting of generation, extraction, and ranking modules to fully leverage the latent knowledge within the language model across multiple dimensions"
  - [section]: "By combining three small-scale language models, each model serving a different purpose, and refining the reasoning process, we can achieve better generation performance"
- Break condition: If the intermediate outputs between stages become too noisy or lose critical information, the progressive approach fails to outperform end-to-end methods.

### Mechanism 2
- Claim: Entity type information reduces semantic ambiguity and improves rule accuracy
- Mechanism: Introducing ontology information about entity types into the prompt constrains the generation process, preventing the model from producing logically inconsistent atoms. The prompt structure explicitly includes entity type slots that are filled with type information extracted from the premise atoms.
- Core assumption: Entity types provide sufficient semantic constraints to guide generation toward logically consistent outputs
- Evidence anchors:
  - [abstract]: "We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy"
  - [section]: "we attempt to introduce entity type information into the open rule generation process to improve the correctness and diversity of rule"
- Break condition: If entity types are too generic or if the ontology is incomplete, the constraints may be insufficient to prevent illogical generation.

### Mechanism 3
- Claim: Reinforcement learning from human feedback (RLHF) aligns model outputs with human preferences for rule quality
- Mechanism: The Ranking module serves as a reward model that reflects human value judgments, while PPO optimizes the Generation and Extraction modules to maximize this reward. KL divergence is used as a constraint to prevent the optimization from getting out of control.
- Core assumption: The Ranking module's scores accurately reflect human preferences for rule quality and can serve as an effective reward signal
- Evidence anchors:
  - [abstract]: "we employ reinforcement learning from human feedback to further optimize model, enhancing the model's understanding of commonsense knowledge"
  - [section]: "the goal of Bert is to map an input text sequence to a reward value, which numerically corresponds to human preferences"
- Break condition: If the reward signal from the Ranking module becomes misaligned with actual human preferences, RLHF optimization may lead to degraded performance.

## Foundational Learning

- Concept: Multi-hop reasoning in knowledge graphs
  - Why needed here: Understanding how to chain multiple inference steps together is crucial for generating longer rule chains with logical consistency
  - Quick check question: How does adding a new hypothesis atom as a premise for the next hop differ from single-hop generation?

- Concept: Ontological entity typing
  - Why needed here: Entity type information provides semantic constraints that guide generation toward logically consistent rules
  - Quick check question: What types of entity information would be most useful for constraining rule generation between geographical entities versus people?

- Concept: Reinforcement learning from human feedback
  - Why needed here: RLHF aligns model outputs with human preferences for rule quality beyond what supervised learning can achieve
  - Quick check question: How does using a pre-trained ranking model as a reward signal differ from direct human scoring?

## Architecture Onboarding

- Component map:
  - Generation module (GPT-2) -> Extraction module (GPT-2) -> Ranking module (BERT) -> Select top hypothesis atom -> Update prompt -> Repeat for next hop

- Critical path: Premise atoms → Generation module → Extraction module → Ranking module → Select top hypothesis atom → Update prompt → Repeat for next hop

- Design tradeoffs:
  - Small models vs large models: PRIMO uses smaller models (774M parameters total) compared to LLMs (13B+), trading some capability for better control and efficiency
  - Multi-stage vs end-to-end: Multi-stage approach provides more control and interpretability but adds complexity
  - RLHF vs supervised learning: RLHF better captures human preferences but requires more computational resources

- Failure signatures:
  - Generation module produces vague or irrelevant text → Extraction module cannot extract meaningful atoms
  - Ranking module fails to distinguish plausible from implausible atoms → Poor selection of hypothesis atoms
  - Entity type information is missing or incorrect → Generation produces logically inconsistent atoms

- First 3 experiments:
  1. Test Generation module alone with various prompt structures to optimize descriptive text quality
  2. Test Extraction module with fixed Generation output to optimize atom extraction accuracy
  3. Test complete pipeline with single-hop generation to verify logical consistency before scaling to multi-hop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PRIMO's performance scale with increasing rule chain length beyond 5 hops, and what are the theoretical limits of multi-hop reasoning in this framework?
- Basis in paper: [explicit] The paper states "PRIMO performs well in generating shorter rule chains (hop≤ 3), but experiences a slight performance drop when trying to generate longer rule chains" and mentions that "it's genuinely impossible to infer more unknown relations between entity pairs"
- Why unresolved: The experiments only tested up to 5 hops, and the paper doesn't provide theoretical analysis of scalability limits or identify the factors that constrain longer chain generation.
- What evidence would resolve it: Extended experiments testing chain lengths beyond 5 hops, analysis of where and why generation fails at longer lengths, and theoretical bounds on reasoning depth based on ontology complexity.

### Open Question 2
- Question: What is the optimal balance between model parameter size and performance for the Generation and Extraction modules, and how does this trade-off vary across different domains?
- Basis in paper: [explicit] The ablation study shows that "theGeneration module has a greater impact on the final results in the process of open rule generation than theExtractionmodule" and that "the experimental results indicate that the rule diversity continuously increases as the network parameter size decreases"
- Why unresolved: While the paper compares different parameter sizes, it doesn't identify optimal configurations or explore how these trade-offs might differ across various knowledge domains or ontology structures.
- What evidence would resolve it: Systematic experiments across multiple domains with varying parameter sizes, analysis of when additional parameters provide diminishing returns, and identification of domain-specific optimal configurations.

### Open Question 3
- Question: How would incorporating additional ontological features beyond entity types (such as relation hierarchies or entity relationships) impact PRIMO's performance and logical consistency?
- Basis in paper: [explicit] The paper mentions that "we attempt to introduce entity type information into the open rule generation process to improve the correctness and diversity of rule" but doesn't explore other ontological features
- Why unresolved: The experiments only tested entity type information, leaving open the question of whether additional ontological features could further improve performance or whether they might introduce new challenges.
- What evidence would resolve it: Experiments incorporating various ontological features (relation hierarchies, entity relationships, etc.) and comparative analysis of their impact on rule quality, logical consistency, and generation diversity.

## Limitations

- The exact prompt templates and entity type encoding strategies remain underspecified, making it difficult to determine whether performance gains are primarily driven by the progressive architecture versus prompt engineering
- Human feedback dataset and ranking model training details are not fully disclosed, limiting reproducibility of the RLHF optimization
- Performance degradation occurs when generating longer rule chains beyond 3 hops, indicating scalability limitations in the current framework

## Confidence

- **High confidence**: The multi-stage progressive framework architecture and its basic components (Generation, Extraction, Ranking modules) are clearly described and implemented
- **Medium confidence**: The reported performance improvements over baselines (BLEU-1: 44.3 vs 23.2, 16.2% reduction in repetition rate) are credible given the experimental setup, though the exact contribution of RLHF versus architectural choices is unclear
- **Medium confidence**: The claim that entity type information significantly improves logical consistency is supported by the experimental results, but the mechanism could be more rigorously analyzed

## Next Checks

1. **Prompt ablation study**: Systematically remove entity type information from prompts to quantify its exact contribution to logical consistency improvements
2. **Stage isolation analysis**: Evaluate each module independently with oracle inputs to determine where performance bottlenecks occur in the pipeline
3. **Human evaluation validation**: Conduct blind human assessments comparing PRIMO-generated rules against baseline outputs to verify that automated metrics align with human judgment of rule quality