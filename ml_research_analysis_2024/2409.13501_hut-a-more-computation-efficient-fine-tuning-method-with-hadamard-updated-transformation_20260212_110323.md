---
ver: rpa2
title: 'HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated
  Transformation'
arxiv_id: '2409.13501'
source_url: https://arxiv.org/abs/2409.13501
tags:
- parameters
- methods
- updated
- transformation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hadamard Updated Transformation (HUT), a
  parameter-efficient fine-tuning method that uses direct transformation of original
  weights via Hadamard product with two low-rank matrices. Unlike existing PEFT methods
  that use incremental updates, HUT maintains stronger correlation with pre-trained
  parameters while reducing computational complexity.
---

# HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation

## Quick Facts
- arXiv ID: 2409.13501
- Source URL: https://arxiv.org/abs/2409.13501
- Reference count: 27
- HUT achieves state-of-the-art performance on CoLA (2.3% improvement) and matches full fine-tuning on E2E NLG with 10× fewer parameters

## Executive Summary
This paper introduces Hadamard Updated Transformation (HUT), a parameter-efficient fine-tuning method that transforms original model weights directly using Hadamard products with two low-rank matrices. Unlike existing PEFT methods that apply incremental updates to pre-trained parameters, HUT maintains stronger correlation with pre-trained weights while achieving comparable or superior performance to full fine-tuning. The method demonstrates significant computational efficiency with fewer FLOPs and no inference latency, validated across RoBERTa on GLUE benchmarks and GPT-2 on E2E NLG tasks.

## Method Summary
HUT introduces a novel parameter-efficient fine-tuning approach that directly transforms original model weights using Hadamard products with two low-rank matrices, rather than applying incremental updates as in traditional PEFT methods. This direct transformation approach maintains stronger correlation with pre-trained parameters while achieving state-of-the-art or comparable performance to full fine-tuning. The method is evaluated on RoBERTa for GLUE benchmark tasks and GPT-2 for E2E NLG, demonstrating significant reductions in FLOPs and parameter count without introducing inference latency.

## Key Results
- On CoLA task, HUT improves over previous best PEFT methods by 2.3% absolute accuracy
- HUT matches or exceeds full fine-tuning performance on E2E NLG while using 10× fewer parameters
- Achieves state-of-the-art results across GLUE benchmarks with significantly fewer FLOPs than competing PEFT methods
- Introduces no inference latency, unlike some other parameter-efficient approaches

## Why This Works (Mechanism)
HUT works by directly transforming pre-trained weights through Hadamard products with low-rank matrices, which maintains stronger correlation with original parameters compared to incremental update methods. This approach allows the fine-tuned model to preserve beneficial properties of pre-trained weights while efficiently adapting to downstream tasks. The low-rank decomposition keeps parameter count minimal while the Hadamard structure enables efficient computation during both training and inference.

## Foundational Learning
- Low-rank matrix decomposition: Essential for reducing parameter count while maintaining representational capacity; quick check: verify rank values used (e.g., 4, 8, 16) affect performance
- Hadamard product operations: Enables efficient element-wise multiplication for weight transformation; quick check: confirm computational complexity is O(n) vs matrix multiplication's O(n³)
- Parameter-efficient fine-tuning (PEFT): Understanding the spectrum from adapters to LoRA to direct weight transformation; quick check: compare parameter counts across different PEFT methods
- GLUE benchmark suite: Standard evaluation for natural language understanding tasks; quick check: ensure all nine tasks are reported with proper metrics
- Transformer architecture fundamentals: Critical for understanding how weight modifications affect attention and feed-forward layers; quick check: verify modifications are applied to correct weight matrices

## Architecture Onboarding

**Component Map:** Pre-trained weights -> HUT transformation matrices -> Hadamard product -> Fine-tuned weights

**Critical Path:** Original weights → Low-rank matrices (A, B) → Element-wise multiplication → Transformed weights → Task-specific fine-tuning

**Design Tradeoffs:** Direct transformation vs incremental updates (maintains correlation but requires full weight access), low-rank decomposition vs full-rank (parameter efficiency vs capacity), Hadamard product vs other operations (computational efficiency vs flexibility)

**Failure Signatures:** Performance degradation when rank is too low, instability when transformation matrices are poorly initialized, potential overfitting on small datasets

**First Experiments:** 1) Ablation study on rank values (4, 8, 16) to find optimal balance between efficiency and performance, 2) Comparison of convergence speed vs LoRA and adapter methods, 3) Sensitivity analysis to learning rate for transformation matrices vs original weights

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to RoBERTa and GPT-2 on GLUE and E2E NLG tasks, potentially missing performance on other architectures
- Claims of "computation efficiency" primarily refer to parameter count and training FLOPs, not necessarily inference speed
- Limited quantitative analysis of correlation maintenance with pre-trained parameters across training stages

## Confidence
- Technical contribution (Hadamard-based direct transformation): High
- Experimental results showing strong performance: High
- Correlation maintenance claims with pre-trained parameters: Medium

## Next Checks
1. Test HUT on larger language models like BERT-large and T5 to verify scalability across architectures
2. Conduct ablation studies on transformation matrix rank to understand sensitivity and optimal configuration
3. Evaluate on non-English benchmarks and specialized domains (biomedical, code generation) to assess generalization beyond current task set