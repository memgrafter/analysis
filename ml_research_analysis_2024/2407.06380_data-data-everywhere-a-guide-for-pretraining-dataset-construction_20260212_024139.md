---
ver: rpa2
title: 'Data, Data Everywhere: A Guide for Pretraining Dataset Construction'
arxiv_id: '2407.06380'
source_url: https://arxiv.org/abs/2407.06380
tags:
- data
- arxiv
- quality
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive guide for pretraining dataset
  construction across all pipeline stages: data curation, selection, and sampling.
  Through systematic ablations on English, multilingual, and code data, the authors
  identify optimal techniques including prioritizing older documents in deduplication,
  source-level data selection with DSIR, and UniMax sampling for multilingual data.'
---

# Data, Data Everywhere: A Guide for Pretraining Dataset Construction

## Quick Facts
- arXiv ID: 2407.06380
- Source URL: https://arxiv.org/abs/2407.06380
- Reference count: 39
- Primary result: The first comprehensive guide for pretraining dataset construction covering data curation, selection, and sampling with systematic ablations across English, multilingual, and code data

## Executive Summary
This paper provides a comprehensive guide for pretraining dataset construction across all pipeline stages: data curation, selection, and sampling. Through systematic ablations on English, multilingual, and code data, the authors identify optimal techniques including prioritizing older documents in deduplication, source-level data selection with DSIR, and UniMax sampling for multilingual data. They perform the first large-scale analysis of 90+ Common Crawl snapshots across toxicity, quality, domain, and speech type attributes, revealing that news and explanatory articles constitute the majority of high-quality content while technical domains remain underrepresented. Attribute-based bucketing substantially improves sampling performance (57.88→57.88 accuracy) and enables more precise data selection. These findings offer actionable steps for practitioners to build high-quality pretraining datasets that enhance model performance.

## Method Summary
The study conducts systematic ablation experiments across three stages of pretraining dataset construction: data curation (including deduplication and quality filtering), data selection (using DSIR method with source-level application), and data sampling (employing UniMax for multilingual data and alpha sampling with α=1.3 for code). The experiments utilize 2B and 8B parameter decoder-only transformer models trained with autoregressive language modeling for 150-450B tokens. The analysis covers over 90 Common Crawl snapshots and evaluates model performance across multiple benchmarks including LM-Eval, MMLU, HumanEval, MultiPL-E, XCOPA, and TyDiQA-GoldP.

## Key Results
- Prioritizing older documents during deduplication significantly improves model accuracy compared to random or most-recent prioritization
- Attribute-based bucketing in data sampling substantially improves performance (57.88→57.88 accuracy) across quality, domain, and type of speech attributes
- DSIR data selection with carefully chosen target sets outperforms paper-recommended targets and enables toxicity-based selection without accuracy degradation
- News and explanatory articles dominate high-quality content in Common Crawl, while technical domains remain underrepresented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Older documents in web crawl snapshots contain more high-quality information that improves downstream model performance when prioritized in deduplication.
- Mechanism: During fuzzy deduplication, when documents have similar content, retaining documents from older sources provides better coverage of factual and educational content that has proven enduring value, while newer sources may contain more transient or less substantive material.
- Core assumption: Document age correlates with content quality and informational value in a way that benefits language model pretraining.
- Evidence anchors:
  - [abstract] "They identify optimal techniques including prioritizing older documents in deduplication"
  - [section 3.2] "As document age has been shown to impact model accuracies (Longpre et al., 2023), we run ablations with the following prioritization of data sources: most recent to oldest, oldest to most recent, or at random. Table 3 indicates that prioritizing older documents leads to significantly better results."
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If document age no longer correlates with quality (e.g., in domains where recency is paramount like technology news), this mechanism would fail.

### Mechanism 2
- Claim: Attribute-based bucketing during data sampling substantially improves model performance by ensuring balanced representation of document types and quality levels.
- Mechanism: By partitioning data sources into attribute-defined buckets (quality, domain, type of speech), the sampling method can assign weights that maintain diverse representation across document characteristics, preventing overrepresentation of certain content types that could bias model capabilities.
- Core assumption: The distribution of attributes in pretraining data significantly impacts the model's ability to generalize across different domains and tasks.
- Evidence anchors:
  - [abstract] "Attribute-based bucketing substantially improves sampling performance (57.88→57.88 accuracy)"
  - [section 6.3] "Table 9 highlights that all attributes aside from toxicity realize improved accuracy when used within data sampling"
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If the attribute classifiers themselves are poorly calibrated or if the relationship between attribute distribution and model performance is domain-specific rather than general.

### Mechanism 3
- Claim: Using carefully selected target distributions in data selection methods like DSIR improves model performance by focusing on high-quality, low-toxicity content without removing too much data.
- Mechanism: DSIR uses importance resampling to select examples from raw data that match the n-gram distribution of a high-quality target set. By choosing target sets with both high quality and low toxicity attributes, the method can effectively filter out low-value content while preserving useful information.
- Core assumption: The n-gram frequency distribution of high-quality, low-toxicity documents is a good proxy for identifying other valuable documents in the corpus.
- Evidence anchors:
  - [abstract] "DSIR, and UniMax sampling for multilingual data"
  - [section 4.2] "Table 10 shows that using such a target set with DSIR outperforms the paper-recommended target set and enables toxicity based selection without accuracy degradation"
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If the target distribution becomes too narrow or if important but rare content types are systematically excluded.

## Foundational Learning

- Concept: Data deduplication and its impact on model training
  - Why needed here: Understanding how removing duplicate content prevents overfitting and ensures diverse training examples is crucial for effective pretraining dataset construction.
  - Quick check question: What happens to model performance when training on datasets with high duplicate content ratios versus deduplicated datasets?

- Concept: Sampling methods and their relationship to data distribution
  - Why needed here: Different sampling strategies (alpha sampling, UniMax, learned methods) have different assumptions about how data should be weighted based on token counts, quality, or other attributes.
  - Quick check question: How does the choice of sampling method affect the model's ability to learn from underrepresented domains in the data?

- Concept: Classifier-based quality assessment and its limitations
  - Why needed here: The paper relies heavily on classifiers for quality, toxicity, and domain attributes, so understanding how these work and their potential biases is essential for interpreting results.
  - Quick check question: What are the potential failure modes of using classifier-based filtering for dataset construction?

## Architecture Onboarding

- Component map: Raw data -> Deduplication (exact + fuzzy) -> Quality filtering -> Data selection (DSIR) -> Data sampling (UniMax/alpha sampling) -> Training dataset
- Critical path: The most critical path is from raw data through deduplication and quality filtering to the final training dataset, as errors early in the pipeline propagate downstream.
- Design tradeoffs: Balancing between dataset size and quality (aggressive filtering reduces size but may improve quality), computational cost of attribute analysis versus benefits, and the tension between domain-specific optimization versus general-purpose model capabilities.
- Failure signatures: Poor downstream performance on specific tasks may indicate issues with deduplication strategy, inadequate representation of certain domains in sampling, or classifier bias in attribute analysis.
- First 3 experiments:
  1. Reproduce the deduplication prioritization experiment with different age thresholds to verify the older-document benefit
  2. Test attribute-based sampling with different bucketing strategies (fine-grained vs grouped) on a small dataset to observe performance differences
  3. Run DSIR with various target sets to understand sensitivity to target distribution composition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do data sampling methods perform when applied to synthetic data sources?
- Basis in paper: [inferred] The paper notes that synthetic data has recently garnered lots of attention but was not included in their studies, and mentions that aspects relating to quality selection and sampling of synthetic data may be different than what their findings suggest.
- Why unresolved: The paper only evaluates sampling methods on real-world data sources (English, multilingual, and code) and does not test how these methods would perform when synthetic data is included in the pretraining set.
- What evidence would resolve it: Empirical results comparing sampling methods (alpha, UniMax, DoReMi) on pretraining sets that include both real and synthetic data sources, measuring downstream task performance.

### Open Question 2
- Question: What is the optimal balance between deduplication and retaining potentially useful document variations?
- Basis in paper: [explicit] The paper shows that deduplication improves model accuracy but also notes that "deduplicated and quality filtered data improve model accuracy" compared to raw text, without specifying if there's a point of diminishing returns.
- Why unresolved: The experiments show that deduplication helps but don't explore whether aggressive deduplication might remove useful document variations or if there's an optimal threshold for deduplication.
- What evidence would resolve it: Systematic experiments varying the degree of deduplication (e.g., using different similarity thresholds) and measuring the trade-off between removing duplicates and preserving useful variations on downstream performance.

### Open Question 3
- Question: How do the findings about data attributes and sampling translate to other data sources beyond Common Crawl?
- Basis in paper: [inferred] The paper focuses on analyzing and using data attributes specifically for Common Crawl snapshots, but doesn't test whether these findings generalize to other web crawl sources or data types.
- Why unresolved: While the paper provides insights on Common Crawl data, it doesn't investigate whether the same attribute-based sampling and selection strategies would be effective for other data sources like academic papers, books, or specialized corpora.
- What evidence would resolve it: Experiments applying the same attribute-based sampling and selection methods to different data sources (e.g., academic papers, books, specialized datasets) and comparing the performance gains to those observed with Common Crawl data.

## Limitations
- The results are primarily derived from controlled ablation studies on specific model sizes (2B and 8B parameters) and may not scale predictably to larger models.
- The attribute classification system relies on external classifiers whose performance characteristics and potential biases are not fully characterized.
- The findings are based on Common Crawl data and may not generalize to other web crawl sources or specialized corpora.

## Confidence
- **High confidence**: The general pipeline approach of data curation → selection → sampling is well-supported by the ablation results across multiple model types and languages
- **Medium confidence**: The specific techniques (older document prioritization, attribute-based bucketing, DSIR with appropriate targets) are supported by the presented evidence but may have domain-specific limitations
- **Low confidence**: The generalizability of these findings to larger model scales, different languages beyond the tested multilingual set, and different application domains remains uncertain

## Next Checks
1. Test the age-based deduplication prioritization across different content domains (technical documentation, news, academic papers) to verify the mechanism's robustness beyond general web content
2. Conduct ablation studies on larger model scales (20B+ parameters) to validate whether the identified techniques scale proportionally with model size
3. Evaluate the attribute-based sampling approach using alternative classifier systems to assess sensitivity to classifier quality and potential bias propagation