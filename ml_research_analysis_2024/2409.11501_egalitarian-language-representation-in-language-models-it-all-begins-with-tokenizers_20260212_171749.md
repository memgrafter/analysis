---
ver: rpa2
title: 'Egalitarian Language Representation in Language Models: It All Begins with
  Tokenizers'
arxiv_id: '2409.11501'
source_url: https://arxiv.org/abs/2409.11501
tags:
- language
- tokenization
- languages
- tokenizers
- tamil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how tokenizers influence the representation
  of languages with complex scripts, such as Tamil, Sinhala, and Hindi, in large language
  models. The research identifies that pre-tokenization methods are a significant
  factor in achieving equitable language representation.
---

# Egalitarian Language Representation in Language Models: It All Begins with Tokenizers

## Quick Facts
- arXiv ID: 2409.11501
- Source URL: https://arxiv.org/abs/2409.11501
- Reference count: 12
- This study shows pre-tokenization methods are more critical than tokenization algorithms for equitable language representation of complex scripts.

## Executive Summary
This paper investigates how tokenization approaches affect the representation of languages with complex scripts like Tamil, Sinhala, and Hindi in large language models. The authors demonstrate that pre-tokenization—the initial segmentation of text before applying tokenization algorithms—plays a more critical role than the choice of tokenization algorithm itself. They introduce Grapheme Pair Encoding (GPE), a modification of Byte Pair Encoding that uses graphemes as the smallest units, which shows improved performance for complex scripts by better capturing characters that require multiple Unicode codepoints.

## Method Summary
The authors compare various tokenization approaches by training tokenizers on a 150k Tamil samples from the Samanantar Dataset and testing on FLORES+ development testsets for Tamil, Hindi, and Sinhala. They analyze pre-tokenization outputs from multiple language models and propose GPE, which modifies BPE to use graphemes instead of bytes or characters. The evaluation uses Compression Ratio (CR) and Tokenization Parity (TP) metrics to assess tokenization quality. The study systematically compares GPE against vanilla implementations of BPE, Unigram, and WordPiece algorithms.

## Key Results
- Pre-tokenization method fundamentally determines maximum token length and compression ratio achievable for complex scripts
- Grapheme-based tokenization outperforms byte-level tokenizers for complex scripts by treating sequences of Unicode codepoints that form single characters as atomic units
- The compression ratio achieved by a tokenizer is primarily determined by pre-tokenization methodology rather than the specific tokenization algorithm used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-tokenization method fundamentally determines the maximum possible token length, which in turn limits the compression ratio and tokenization parity achievable for complex scripts.
- Mechanism: Pre-tokenization segments text into smaller chunks (pre-tokens) before applying the main tokenization algorithm. The longest token a tokenizer can learn is bounded by the longest pre-token, regardless of how much data is provided.
- Core assumption: With sufficient training data, tokenizers will learn tokens that match the pre-tokens exactly, so the pre-tokenization choice directly constrains achievable performance.
- Evidence anchors:
  - [abstract] "pre-tokenization plays a more critical role than the tokenization algorithm itself in achieving an egalitarian representation of these complex script languages"
  - [section] "The pre-tokenizer effectively bounds the maximum token length that can be formed for a given pre-token. Consequently, the Compression Ratio (CR) calculated using Equation 1 and the Tokenization Parity (TP) calculated using Equation 2 become the Maximum Compression Ratio (CRmax) and the Minimum Tokenization Parity (TPmin), respectively."
  - [corpus] Weak evidence - related works discuss tokenization fairness but don't directly support the specific mechanism about pre-tokenization bounding maximum token length
- Break condition: If pre-tokenization is applied after the main tokenization algorithm rather than before, or if the tokenizer can learn tokens longer than any single pre-token through merging across pre-token boundaries

### Mechanism 2
- Claim: Grapheme-based tokenization better captures characters in complex scripts by treating sequences of Unicode codepoints that form single human-perceived characters as atomic units.
- Mechanism: Complex scripts like Tamil, Sinhala, and Hindi often require multiple Unicode codepoints to represent single characters (graphemes). Grapheme-based tokenization groups these codepoints together, reducing fragmentation and improving compression.
- Core assumption: Human perception of characters in complex scripts corresponds to grapheme clusters rather than individual Unicode codepoints, and this alignment improves tokenization efficiency.
- Evidence anchors:
  - [abstract] "grapheme-based character extraction outperforms byte-level tokenizers for complex scripts"
  - [section] "This improvement is due to graphemes effectively capturing characters by combining multiple codepoints into a single unit"
  - [corpus] Weak evidence - corpus contains related work on tokenization fairness but lacks specific evidence about grapheme-based approaches
- Break condition: If the language's writing system doesn't benefit from grapheme clustering, or if grapheme detection is too computationally expensive relative to benefits

### Mechanism 3
- Claim: The compression ratio achieved by a tokenizer is primarily determined by the pre-tokenization method rather than the specific tokenization algorithm used.
- Mechanism: Different tokenization algorithms (BPE, Unigram, WordPiece) trained with the same pre-tokenization method achieve similar performance, while the same algorithm with different pre-tokenization methods shows large performance differences.
- Core assumption: Pre-tokenization dominates the tokenization pipeline's effectiveness, making algorithm choice secondary for languages with complex scripts.
- Evidence anchors:
  - [abstract] "pre-tokenization plays a more critical role than the tokenization algorithm itself"
  - [section] "Given that the tokenizers trained with GPT-2 and whitespace pre-tokenization methods show comparable performance within their respective groups, this finding demonstrates that the compression ratio is primarily determined by the pre-tokenization methodology employed, rather than the specific tokenization algorithm used."
  - [corpus] Weak evidence - related works discuss tokenization efficiency but don't directly support the specific claim about pre-tokenization dominance
- Break condition: If different tokenization algorithms have fundamentally different capabilities that cannot be overcome by pre-tokenization choices, or if the pre-tokenization method is already optimal

## Foundational Learning

- Concept: Pre-tokenization and its role in tokenization pipelines
  - Why needed here: Understanding pre-tokenization is critical because the paper demonstrates it's the primary determinant of tokenization performance for complex scripts
  - Quick check question: What is the difference between pre-tokenization and the main tokenization algorithm, and how does pre-tokenization bound the maximum token length that can be learned?

- Concept: Grapheme clusters and Unicode representation of complex scripts
  - Why needed here: The paper's proposed solution (GPE) relies on understanding how complex scripts are represented in Unicode and how graphemes are formed from multiple codepoints
  - Quick check question: How are Tamil characters like "ஶ்ரீ" represented in Unicode, and why can't they be processed as individual codepoints for effective tokenization?

- Concept: Compression ratio and tokenization parity metrics
  - Why needed here: These metrics are the primary evaluation criteria used to demonstrate the effectiveness of different tokenization approaches for complex scripts
  - Quick check question: How do you calculate Compression Ratio and Tokenization Parity, and what do these metrics tell you about the fairness of language representation?

## Architecture Onboarding

- Component map: Pre-tokenization → Tokenization algorithm training → Vocabulary application → Model inference
- Critical path: Pre-tokenization → Tokenization algorithm training → Vocabulary application → Model inference
- Design tradeoffs: Simple whitespace pre-tokenization vs. complex regex pre-tokenization (simplicity vs. performance), byte-level vs. grapheme-level tokenization (universality vs. script-specific optimization)
- Failure signatures: Poor compression ratio (CRmax < 2) indicates inadequate pre-tokenization, high tokenization parity (TPmin > 3) suggests language imbalance, fragmented characters in complex scripts indicate byte-level tokenization issues
- First 3 experiments:
  1. Compare pre-tokenization methods (GPT-2 regex vs. whitespace) with BPE on the same dataset to validate pre-tokenization dominance
  2. Implement grapheme-based character extraction and compare against byte-level tokenizers (ByT5, CANINE) for Tamil, Sinhala, and Hindi
  3. Train GPE and compare against vanilla BPE, Unigram, and WordPiece on the same dataset with fixed vocabulary size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between pre-tokenization method choice and downstream task performance across different complex script languages?
- Basis in paper: [explicit] The paper demonstrates that pre-tokenization is more critical than tokenization algorithm choice, but does not quantify the downstream performance impact
- Why unresolved: The paper focuses on compression ratio and tokenization parity metrics but does not measure actual downstream task performance
- What evidence would resolve it: Empirical studies comparing model performance on various downstream tasks (translation, classification, etc.) using different pre-tokenization methods across multiple complex script languages

### Open Question 2
- Question: How does the proposed Grapheme Pair Encoding (GPE) algorithm compare to other tokenization-free approaches for complex script languages?
- Basis in paper: [explicit] The paper mentions tokenization-free language models exist but does not compare GPE to them
- Why unresolved: The paper only compares GPE to traditional tokenization algorithms, not to tokenization-free approaches like ByT5
- What evidence would resolve it: Direct performance comparisons between GPE and tokenization-free models on the same evaluation metrics and tasks

### Open Question 3
- Question: What is the optimal vocabulary size for GPE when dealing with highly agglutinative languages versus languages with more complex character structures?
- Basis in paper: [inferred] The paper uses a fixed 5k vocabulary size for all experiments but does not explore how this choice affects different language types
- Why unresolved: The paper does not investigate the impact of varying vocabulary size on tokenization quality for different language families
- What evidence would resolve it: Systematic experiments varying vocabulary size across different language types and measuring compression ratio and tokenization parity metrics

## Limitations
- The analysis focuses on Tamil, Sinhala, and Hindi, which may not generalize to other script families with different structural characteristics
- The paper doesn't address computational overhead of grapheme-based tokenization or its impact on training efficiency and inference speed
- Evidence is primarily observational rather than causal, lacking controlled experiments to isolate pre-tokenization effects

## Confidence

**High Confidence**: The observation that pre-tokenization method affects tokenization performance for complex scripts is well-supported by the comparative analysis of existing models. The demonstration that simple whitespace pre-tokenization outperforms complex regex approaches in some cases is robust.

**Medium Confidence**: The claim that pre-tokenization is more important than the tokenization algorithm itself is supported but could benefit from more controlled experiments. The evidence shows correlation but doesn't definitively prove that algorithm choice is secondary across all scenarios.

**Low Confidence**: The specific claims about GPE's superiority over other methods lack sufficient implementation details for independent verification. The performance improvements shown may depend on specific implementation choices not fully disclosed in the paper.

## Next Checks

1. **Controlled Algorithm Comparison**: Train identical tokenization algorithms (BPE, Unigram, WordPiece) with different pre-tokenization methods on the same dataset with fixed vocabulary size to isolate the pre-tokenization effect. This would validate whether pre-tokenization truly dominates algorithm choice.

2. **Cross-Script Generalization**: Test the proposed pre-tokenization and GPE approaches on additional complex script languages (e.g., Thai, Arabic, Hebrew) to assess generalizability beyond the three languages studied. This would validate whether the findings apply to broader language families.

3. **Implementation Replication**: Recreate the GPE algorithm from the description provided and compare its performance against the original implementation on the same datasets. This would verify whether the claimed improvements are reproducible and identify any critical implementation details affecting results.