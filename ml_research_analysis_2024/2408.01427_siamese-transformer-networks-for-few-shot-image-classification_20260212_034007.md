---
ver: rpa2
title: Siamese Transformer Networks for Few-shot Image Classification
arxiv_id: '2408.01427'
source_url: https://arxiv.org/abs/2408.01427
tags:
- features
- local
- learning
- global
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses few-shot image classification by integrating
  global and local features, which are typically studied separately. The authors propose
  the Siamese Transformer Network (STN), which uses two parallel branches: one extracts
  global features from class embeddings, and the other extracts local features from
  patch embeddings, both using a pre-trained Vision Transformer (ViT).'
---

# Siamese Transformer Networks for Few-shot Image Classification

## Quick Facts
- arXiv ID: 2408.01427
- Source URL: https://arxiv.org/abs/2408.01427
- Authors: Weihao Jiang; Shuoxi Zhang; Kun He
- Reference count: 40
- Achieves state-of-the-art performance with up to 5% improvement over existing methods in 1-shot and 5-shot settings

## Executive Summary
This paper addresses few-shot image classification by integrating global and local features through a Siamese Transformer Network (STN). The method employs two parallel branches using pre-trained Vision Transformer (ViT) architecture to extract global features from class embeddings and local features from patch embeddings. Similarity is computed using Euclidean distance for global features and KL divergence for local features, followed by L2 normalization and weighted fusion. Experiments on four benchmark datasets demonstrate state-of-the-art performance, with ablation studies confirming the effectiveness of combining both feature types and the fusion strategy.

## Method Summary
STN uses a dual-branch architecture with ViT-Small as backbone, extracting class embeddings for global features and patch embeddings for local features. Global similarity is measured using Euclidean distance while local similarity uses KL divergence, both followed by L2 normalization. The normalized scores are combined through weighted fusion to produce final similarity scores. The model is trained using meta-learning without complex adaptation modules, achieving strong performance across miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets.

## Key Results
- Achieves state-of-the-art performance on four benchmark datasets
- Improves over existing methods by up to 5% in 1-shot and 5-shot settings
- Ablation studies confirm effectiveness of dual-branch architecture and weighted fusion
- Single-feature methods underperform compared to the combined approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-branch architecture with separate global and local feature extraction enables complementary feature utilization without interference
- Mechanism: Two parallel ViT-Small networks extract class embeddings (global) and patch embeddings (local) independently, each optimized with its own loss
- Core assumption: Global and local features provide complementary information that can be jointly exploited without feature adaptation modules
- Evidence anchors: [abstract] "Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively."
- Break condition: If global and local features are highly correlated, the dual-branch separation adds complexity without benefit

### Mechanism 2
- Claim: Different similarity metrics per feature type (Euclidean for global, KL divergence for local) better capture feature-specific relationships
- Mechanism: Euclidean distance measures global feature similarity, leveraging global invariance. KL divergence captures asymmetric local feature distribution differences
- Core assumption: Euclidean distance is effective for global feature comparison and KL divergence effectively captures local feature distribution differences
- Evidence anchors: [abstract] "We apply the Euclidean distance measure to the global features and the Kullback–Leibler (KL) divergence measure to the local features."
- Break condition: If local features are not well-modeled as Gaussian distributions, KL divergence becomes ineffective

### Mechanism 3
- Claim: Weighted fusion of normalized similarity scores from both branches improves classification accuracy over single-feature methods
- Mechanism: Similarity scores from each branch are L2-normalized and combined using a weighted sum, with weights learned during training
- Core assumption: Global and local features provide complementary information that can be effectively combined through weighted fusion
- Evidence anchors: [abstract] "To integrate the two metrics, we first employ L2 normalization and then weight the normalized results to obtain the final similarity score."
- Break condition: If the weighting mechanism overfits to training data, generalization suffers

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and pre-training
  - Why needed here: STN uses ViT-Small as backbone for both branches, requiring understanding of patch embedding extraction and pre-training strategies
  - Quick check question: What is the difference between class embeddings and patch embeddings in ViT, and how are they used in STN?

- Concept: Few-shot learning and metric-based meta-learning
  - Why needed here: STN is designed for few-shot image classification using a metric-based approach with support/query episode structure
  - Quick check question: How does the N-way K-shot episode structure work in few-shot learning, and what role do support and query sets play?

- Concept: Kullback-Leibler (KL) divergence and its application to feature distributions
  - Why needed here: STN uses KL divergence to measure similarity between local feature distributions, requiring understanding of asymmetric distribution comparison
  - Quick check question: How is KL divergence calculated between two multivariate Gaussian distributions, and why is it suitable for local feature comparison?

## Architecture Onboarding

- Component map:
  Input images -> ViT-Small backbone -> Class embeddings (global features) -> Euclidean distance
  Input images -> ViT-Small backbone -> Patch embeddings (local features) -> KL divergence
  Both branches -> L2 normalization -> Weighted fusion -> Final similarity score -> Classification

- Critical path:
  1. Image patch extraction and ViT encoding
  2. Class embedding extraction (global features)
  3. Patch embedding extraction (local features)
  4. Distance calculation (Euclidean for global, KL for local)
  5. Normalization and weighted fusion
  6. Classification via nearest neighbor

- Design tradeoffs:
  - Dual-branch vs. single-branch: Increased parameter count but better feature separation
  - KL divergence vs. other metrics: Better for distribution comparison but assumes Gaussian distribution
  - Weighted fusion vs. adaptive fusion: Simpler and less prone to overfitting

- Failure signatures:
  - Poor performance on datasets with highly correlated global and local features
  - Overfitting when weights are not properly regularized
  - Degradation when local features don't follow Gaussian distribution assumptions

- First 3 experiments:
  1. Compare STN with single-branch variants (global-only and local-only) on miniImageNet to validate dual-branch benefit
  2. Test different distance functions for global features (dot product, Manhattan, cosine) to find optimal metric
  3. Evaluate impact of parameter sharing between branches to confirm separation benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different Vision Transformer (ViT) architectures (e.g., ViT-Tiny, ViT-Small, ViT-Base) impact the performance of the Siamese Transformer Network (STN) in few-shot learning?
- Basis in paper: [explicit] The paper uses ViT-Small as the backbone and does not explore the effects of varying ViT architectures
- Why unresolved: The paper only evaluates STN using ViT-Small without comparative analysis
- What evidence would resolve it: Experiments comparing STN performance across different ViT architectures (e.g., ViT-Tiny, ViT-Small, ViT-Base) on the same benchmark datasets

### Open Question 2
- Question: How does the performance of STN change with different pre-training strategies (e.g., supervised, self-supervised, or contrastive learning)?
- Basis in paper: [explicit] The paper mentions using a "Masked Image Modeling (MIM)-pretrained Vision Transformer" but does not explore the impact of alternative pre-training methods
- Why unresolved: The study focuses on MIM-pretraining without comparing it to other pre-training strategies
- What evidence would resolve it: Comparative experiments evaluating STN with various pre-training methods (e.g., supervised, self-supervised, contrastive) on the same datasets

### Open Question 3
- Question: How does the choice of distance metric (e.g., Euclidean distance vs. cosine similarity) affect the performance of STN when applied to global features?
- Basis in paper: [explicit] The paper uses squared Euclidean distance for global features and mentions other distance functions in the ablation study but does not extensively compare them
- Why unresolved: The ablation study tests different distance functions but does not deeply analyze their impact on overall performance
- What evidence would resolve it: Detailed experiments comparing the performance of STN using different distance metrics for global features on the same benchmark datasets

## Limitations

- The dual-branch architecture assumes global and local features provide complementary information, but lacks analysis of feature correlation to validate this assumption
- The KL divergence metric assumes local features follow Gaussian distributions, which is not empirically verified
- The weighted fusion strategy uses fixed weights (α=0.7 for 1-shot, α=0.6 for 5-shot) without justification or exploration of adaptive weighting schemes

## Confidence

- Dual-branch architecture effectiveness: Medium-High
- KL divergence suitability for local features: Low
- Weighted fusion strategy: Medium

## Next Checks

1. Analyze correlation between global and local feature representations on held-out validation sets to quantify the complementarity assumption
2. Compare KL divergence with alternative metrics (JS divergence, Wasserstein distance) for local feature similarity
3. Evaluate the impact of different fusion strategies (adaptive weights, attention mechanisms) versus the fixed weighted sum approach