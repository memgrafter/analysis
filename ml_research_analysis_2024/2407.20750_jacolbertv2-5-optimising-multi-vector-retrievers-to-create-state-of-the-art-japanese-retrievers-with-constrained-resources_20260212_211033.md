---
ver: rpa2
title: 'JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art
  Japanese Retrievers with Constrained Resources'
arxiv_id: '2407.20750'
source_url: https://arxiv.org/abs/2407.20750
tags:
- training
- retrieval
- query
- performance
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JaColBERTv2.5, a multi-vector retrieval model
  optimized for Japanese with constrained computational resources. The authors systematically
  evaluate and improve inference and training settings of multi-vector retrievers,
  introducing a novel checkpoint merging step.
---

# JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources

## Quick Facts
- arXiv ID: 2407.20750
- Source URL: https://arxiv.org/abs/2407.20750
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on all common Japanese benchmarks with average score of 0.754, outperforming previous best models by 4.5%

## Executive Summary
This paper introduces JaColBERTv2.5, a multi-vector retrieval model optimized for Japanese with constrained computational resources. The authors systematically evaluate and improve inference and training settings of multi-vector retrievers, introducing a novel checkpoint merging step. JaColBERTv2.5 achieves state-of-the-art performance on all common Japanese benchmarks, with an average score of 0.754, outperforming previous best models by 4.5%. The model uses 110 million parameters and is trained in under 15 hours on 4 A100 GPUs. Key improvements include dynamic query length, optimized training settings, knowledge distillation from BGE-M3, and post-training on higher-quality Japanese data. The paper also introduces checkpoint averaging to combine the benefits of fine-tuning with the generalization capabilities of the original model.

## Method Summary
JaColBERTv2.5 builds upon the ColBERTv2 architecture with several key optimizations. The model uses a Japanese BERT variant initialized from a JaColBERT checkpoint and employs 32-way triplet training with KL-divergence loss and knowledge distillation from BGE-M3 reranker scores. Key innovations include dynamic query length that scales with query size, removal of in-batch negatives to improve training efficiency, and min-max normalization of both teacher and student scores. The model is post-trained on higher-quality Japanese datasets (MIRACL, JQaRA, and JaGovFaqs) and uses checkpoint averaging to merge post-training benefits with the original model's generalization capabilities. The entire training process completes in under 15 hours on 4 A100 GPUs with 110 million parameters.

## Key Results
- Achieves state-of-the-art performance on Japanese benchmarks with 0.754 average score
- Outperforms previous best models by 4.5% across all evaluation datasets
- Training completes in under 15 hours on 4 A100 GPUs with 110 million parameters
- Dynamic query length improves performance from 0.688 to 0.700 on MIRACL-small-ja
- Checkpoint averaging improves out-of-domain generalization while retaining post-training benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic query length in ColBERT improves retrieval by matching augmentation to query size
- Mechanism: Instead of padding all queries to a fixed 32-token limit with [MASK] tokens, dynamic query length extends queries to the next multiple of 32, ensuring enough [MASK] tokens for longer queries while avoiding excessive padding for short ones
- Core assumption: The [MASK] augmentation mechanism benefits from having at least 8 [MASK] tokens but degrades if too many are added to short queries
- Evidence anchors:
  - Table 2 shows dynamic query length outperforms both no augmentation (0.667) and fixed 8 tokens (0.688) on MIRACL-small-ja, with 0.700 overall score
  - Results show no impact on short queries but noticeable improvement for queries near maximum length
  - Weak - no direct citations about [MASK] token benefits in ColBERT variants

### Mechanism 2
- Claim: Removing in-batch negatives improves training efficiency without performance loss in multi-vector retrievers
- Mechanism: In multi-vector retrievers with 32-way triplets, the KL-divergence loss on 32 teacher scores provides stronger signal than binary cross-entropy on in-batch negatives, which also adds computational overhead
- Core assumption: The distributional learning signal from teacher scores outweighs the relevance signal from in-batch negatives
- Evidence anchors:
  - Table 3 shows identical performance with and without in-batch negatives (0.699 vs 0.699 overall)
  - Explicit hypothesis that teacher score distribution provides stronger signal than binary relevance
  - Weak - no direct citations about in-batch negatives in ColBERT models specifically

### Mechanism 3
- Claim: Normalizing both teacher and student scores improves KL-divergence training stability and effectiveness
- Mechanism: Min-max normalization scales scores to [0,1] range, allowing the model to focus on relative ranking rather than absolute score scales, which differ between cross-encoders and MaxSim
- Core assumption: Score normalization helps the model learn relative rankings rather than absolute values, especially when teacher and student score scales differ significantly
- Evidence anchors:
  - Table 3 shows normalization of both scores improves performance from 0.699 to 0.705 overall
  - Normalization of only teacher scores decreases performance to 0.691
  - Weak - no direct citations about score normalization in ColBERT training

## Foundational Learning

- Concept: Knowledge Distillation in Retrieval
  - Why needed here: The model learns from cross-encoder teacher scores rather than explicit relevance labels, which is crucial for training with limited high-quality Japanese data
  - Quick check question: What loss function is used to train the student model to match the teacher score distribution?

- Concept: Multi-Vector vs Single-Vector Representations
  - Why needed here: Multi-vector models like ColBERT avoid information loss by representing documents as multiple token embeddings rather than a single averaged vector
  - Quick check question: How does the MaxSim scoring mechanism work in multi-vector retrievers?

- Concept: Checkpoint Averaging for Generalization
  - Why needed here: Merging multiple checkpoints helps retain performance gains from post-training while preventing catastrophic forgetting on out-of-domain tasks
  - Quick check question: What is the main benefit of checkpoint averaging according to the experimental results?

## Architecture Onboarding

- Component map:
  Base model -> Encoder -> Scoring -> Training -> Post-processing

- Critical path:
  1. Encode query and document into token embeddings
  2. Compute MaxSim scores between all query and document tokens
  3. Apply knowledge distillation loss using teacher scores
  4. Backpropagate through the entire network
  5. Save checkpoints for potential averaging

- Design tradeoffs:
  - Storage vs accuracy: Multi-vector models require more storage but provide better generalization
  - Compute vs performance: Dynamic query length adds complexity but improves results
  - Training time vs data quality: Post-training on high-quality data improves domain-specific performance but risks forgetting

- Failure signatures:
  - Performance degradation on out-of-domain tasks after post-training
  - Training instability when using inappropriate learning rates with schedule-free optimization
  - Poor generalization when teacher scores are of low quality or inconsistent

- First 3 experiments:
  1. Test dynamic query length vs fixed query length on development set to verify performance improvement
  2. Compare training with and without in-batch negatives to confirm computational efficiency gain
  3. Evaluate score normalization impact by training with teacher normalization only vs both teacher and student normalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of JaColBERTv2.5 change if trained on higher-quality Japanese datasets like MIRACL or Mr.TyDi instead of MMarco?
- Basis in paper: The paper mentions that MMarco is a machine-translated dataset of lower quality and that MIRACL and Mr.TyDi are higher quality datasets, but these were not used for the main training.
- Why unresolved: The paper only explores post-training on these datasets, not full training. The authors hypothesize that using higher quality data for initial training could yield even better results.
- What evidence would resolve it: Training JaColBERTv2.5 from scratch on MIRACL or Mr.TyDi and comparing its performance to the current model trained on MMarco.

### Open Question 2
- Question: What is the optimal number of checkpoints to average when using checkpoint averaging for ColBERT models?
- Basis in paper: The paper mentions that checkpoint averaging improved performance, but only used the three best checkpoints from the initial training and two post-training checkpoints.
- Why unresolved: The authors do not systematically explore how the number of checkpoints affects performance. They chose three checkpoints arbitrarily based on their understanding of Polyak's method.
- What evidence would resolve it: Conducting experiments with different numbers of checkpoints (e.g., 2, 4, 5, 10) and comparing their performance to determine the optimal number.

### Open Question 3
- Question: How does the performance of JaColBERTv2.5 compare to other state-of-the-art models on non-Japanese languages when trained on high-quality data for those languages?
- Basis in paper: The paper states that the training recipe improvements are language-agnostic and could be applied to other languages, but only tests on Japanese.
- Why unresolved: The authors only tested their method on Japanese and hypothesize it would work for other languages, but do not provide empirical evidence.
- What evidence would resolve it: Training JaColBERTv2.5 on high-quality datasets for other languages (e.g., English MS Marco, Chinese C-PACT) and comparing its performance to current state-of-the-art models for those languages.

## Limitations
- Training data quality limitations: Uses machine-translated MMarco dataset rather than higher-quality Japanese datasets
- Limited exploration of teacher model alternatives beyond BGE-M3
- No systematic analysis of optimal checkpoint averaging parameters
- Potential domain overfitting from heavy focus on Japanese government data in post-training

## Confidence
**Performance Claims (High Confidence):** The state-of-the-art results are well-supported by comprehensive benchmark evaluation across five different Japanese datasets. The 4.5% improvement over previous best models is statistically significant and consistently observed.

**Training Methodology Claims (Medium Confidence):** While the training optimizations show clear improvements, some claims about the underlying mechanisms (particularly regarding in-batch negatives) rely on limited ablation studies without theoretical justification.

**Generalization Claims (Low Confidence):** The assertion that checkpoint averaging provides "strong generalization capabilities" is based on out-of-domain performance but lacks rigorous analysis of when this approach might fail or how it compares to other regularization techniques.

## Next Checks
1. **Teacher Score Quality Analysis:** Conduct an ablation study where JaColBERTv2.5 is trained with teacher scores from different rerankers (BGE-M3, ColBERTv2's reranker, BM25) to quantify the impact of teacher model quality on final performance.

2. **Dynamic Query Length Boundary Testing:** Systematically vary the dynamic query length scaling factor and the number of [MASK] tokens required, measuring performance across the full range of query lengths to identify optimal parameters and potential failure modes.

3. **Cross-Lingual Generalization Test:** Evaluate the checkpoint averaging approach on a zero-shot multilingual retrieval task (e.g., using English queries against Japanese documents) to verify whether the claimed generalization benefits extend beyond in-domain Japanese retrieval.