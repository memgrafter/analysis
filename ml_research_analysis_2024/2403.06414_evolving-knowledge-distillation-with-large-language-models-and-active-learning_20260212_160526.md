---
ver: rpa2
title: Evolving Knowledge Distillation with Large Language Models and Active Learning
arxiv_id: '2403.06414'
source_url: https://arxiv.org/abs/2403.06414
tags:
- samples
- data
- student
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvoKD, an evolving knowledge distillation
  framework that leverages large language models (LLMs) and active learning to improve
  few-shot text classification and named entity recognition. The core method involves
  iteratively analyzing the student model's weaknesses, generating challenging and
  easy samples based on this analysis, and providing iterative feedback to the LLM
  to continuously construct diversified and challenging samples.
---

# Evolving Knowledge Distillation with Large Language Models and Active Learning

## Quick Facts
- arXiv ID: 2403.06414
- Source URL: https://arxiv.org/abs/2403.06414
- Reference count: 0
- One-line primary result: EvoKD achieves up to 90% of full-shot performance on text classification with only 1-shot data

## Executive Summary
This paper introduces EvoKD, an evolving knowledge distillation framework that leverages large language models (LLMs) and active learning to improve few-shot text classification and named entity recognition. The core method involves iteratively analyzing the student model's weaknesses, generating challenging and easy samples based on this analysis, and providing iterative feedback to the LLM to continuously construct diversified and challenging samples. Experiments show that EvoKD significantly outperforms baseline methods, achieving up to 90% of the full-shot performance on text classification tasks with only 1-shot data.

## Method Summary
EvoKD is an evolving knowledge distillation framework that uses LLMs to iteratively improve a student model's performance in few-shot settings. The process begins with initializing the student model using limited training data, then using an LLM to analyze the model's weaknesses by identifying patterns in misclassified samples. Based on this analysis, the LLM generates new labeled samples targeting these weaknesses, with both challenging and easy samples included to prevent catastrophic forgetting. The student model is then trained on these generated samples, and the process repeats iteratively until the desired performance is achieved.

## Key Results
- EvoKD achieves up to 90% of full-shot performance on text classification tasks with only 1-shot data
- Significant performance improvements demonstrated on both text classification and named entity recognition tasks
- Outperforms baseline methods like AugGPT and traditional knowledge distillation approaches

## Why This Works (Mechanism)

### Mechanism 1
The iterative weakness analysis loop allows the LLM to generate samples that directly target the student model's specific error patterns, leading to more effective knowledge transfer than random data augmentation. After each batch of samples, the student model's performance is evaluated to identify which samples were misclassified. These misclassified samples are fed back into the LLM, which analyzes the patterns in the errors and generates new samples designed to be challenging based on those patterns.

### Mechanism 2
Including both easy and hard samples in each batch prevents catastrophic forgetting and maintains a balanced learning distribution. By explicitly generating both easy and hard samples based on the weakness analysis, the LLM ensures that the student model continues to practice on samples it already handles well while focusing on improving on its weaknesses.

### Mechanism 3
Separating the text generation and labeling steps reduces the risk of intentional mislabeling by the LLM, improving the quality of the generated data. Instead of having the LLM generate both the text and its label in a single step, the process is split into two separate conversations. First, the LLM generates the text samples based on the weakness analysis, then in a separate conversation, the LLM is prompted to label the generated samples.

## Foundational Learning

- **Concept**: Active Learning
  - Why needed here: The core idea of active learning is to select the most informative samples for training, which aligns with EvoKD's approach of generating samples that target the student model's weaknesses.
  - Quick check question: In active learning, what is the primary goal when selecting samples for labeling or training?

- **Concept**: Knowledge Distillation
  - Why needed here: EvoKD is fundamentally a knowledge distillation technique that transfers knowledge from a large language model to a smaller student model.
  - Quick check question: What is the key difference between black-box and white-box knowledge distillation?

- **Concept**: Few-Shot Learning
  - Why needed here: EvoKD is designed to work effectively in few-shot learning scenarios, where only a limited amount of training data is available.
  - Quick check question: What is the main challenge in few-shot learning, and how do data augmentation techniques attempt to address it?

## Architecture Onboarding

- **Component map**: Student Model -> LLM (ChatGPT) -> Batch Data -> Performance Evaluation -> Feedback Loop
- **Critical path**: The critical path is the iterative loop: Weakness Analysis → Sample Generation → Student Model Training → Performance Evaluation → Feedback to LLM
- **Design tradeoffs**:
  - Token Usage vs. Performance: EvoKD consumes more tokens per interaction due to the weakness analysis and task explanation, but this leads to better long-term performance
  - Easy vs. Hard Samples: Including both easy and hard samples prevents catastrophic forgetting but may slow down learning on the most challenging tasks
  - Separation of Generation and Labeling: Separating these steps reduces the risk of mislabeling but may slightly increase the complexity of the implementation
- **Failure signatures**:
  - Stagnant Performance: If the student model's performance plateaus despite multiple iterations
  - Overfitting: If the student model performs well on the generated samples but poorly on the original test set
  - Mislabeling: If the student model consistently makes errors on samples that the LLM has labeled correctly
- **First 3 experiments**:
  1. Baseline Comparison: Compare EvoKD's performance on a text classification task with a standard knowledge distillation approach under the same few-shot setting
  2. Ablation Study - Easy Samples: Run EvoKD without generating easy samples and compare the performance to the full EvoKD approach
  3. Ablation Study - Separation of Steps: Run EvoKD with the text generation and labeling steps merged into a single conversation

## Open Questions the Paper Calls Out

### Open Question 1
How does EvoKD perform on tasks with significantly different data distributions or domains, such as specialized technical or scientific text? The paper focuses on text classification and NER tasks with relatively general domains like product reviews and news articles. The performance on specialized domains is not explored.

### Open Question 2
What is the impact of different LLM models on EvoKD's performance, and how does the choice of LLM affect the quality of generated samples? The paper mentions using gpt-3.5-turbo-0301 for the LLM interactions, but does not explore the impact of different LLM models.

### Open Question 3
How does EvoKD scale with larger datasets and more complex tasks, and what are the computational limitations of the framework? The paper focuses on few-shot settings and relatively simple tasks like text classification and NER. The scalability of EvoKD to larger datasets and more complex tasks is not explored.

## Limitations
- Heavy dependence on prompt engineering for LLM interactions
- Performance may not generalize equally well to tasks beyond text classification and NER
- Computational efficiency trade-offs are not thoroughly analyzed

## Confidence
- **High Confidence**: The core mechanism of iterative weakness analysis and targeted sample generation is well-supported by experimental results
- **Medium Confidence**: The effectiveness of separating text generation and labeling steps lacks direct empirical validation in the paper
- **Low Confidence**: The generalizability of EvoKD to tasks beyond text classification and NER remains uncertain

## Next Checks
1. Conduct an ablation study comparing EvoKD's performance with and without the separation of text generation and labeling steps
2. Evaluate EvoKD on a diverse set of NLP tasks beyond text classification and NER to assess its effectiveness
3. Measure the computational overhead of EvoKD compared to traditional knowledge distillation approaches across different few-shot settings