---
ver: rpa2
title: Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning
  and Joint Learning AMR Parsing
arxiv_id: '2410.03357'
source_url: https://arxiv.org/abs/2410.03357
tags:
- languages
- training
- parsing
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether meta-learning can improve cross-lingual
  AMR parsing compared to standard joint learning. The authors train multilingual
  AMR parsers using MAML (model-agnostic meta-learning) and compare against a joint
  learning baseline.
---

# Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing

## Quick Facts
- arXiv ID: 2410.03357
- Source URL: https://arxiv.org/abs/2410.03357
- Reference count: 11
- Primary result: Meta-learning shows minimal benefits over joint learning for cross-lingual AMR parsing

## Executive Summary
This paper investigates whether meta-learning can improve cross-lingual AMR parsing compared to standard joint learning. The authors train multilingual AMR parsers using MAML (model-agnostic meta-learning) and compare against a joint learning baseline. Models are evaluated in 0-shot and k-shot scenarios across five languages: French, Chinese, Korean, Farsi, and Croatian. While the meta-learning model shows slightly better performance in 0-shot evaluation for some languages, the advantage disappears when k > 0. The baseline joint learning approach performs competitively and more consistently across all target languages. The authors conclude that joint learning serves as a robust baseline, while meta-learning offers minimal benefits for cross-lingual AMR parsing.

## Method Summary
The study trains multilingual AMR parsers using either MAML (meta-learning) or joint learning on machine-translated AMR data. Training data consists of English AMR 3.0 translated to 13 languages using DeepL, plus original English. The mBart-large-50 pre-trained multilingual sequence-to-sequence model is fine-tuned using both approaches. MAML simulates few-shot training by creating temporary models for each language, adapting them with support sets, evaluating on query sets, and aggregating losses to update initial parameters. Joint learning trains on concatenated multilingual data directly. Models are evaluated in 0-shot, 32-shot, and 128-shot scenarios across five target languages using SMATCH score as the evaluation metric.

## Key Results
- MAML shows slightly better performance than joint learning in 0-shot evaluation for some languages
- Advantage of MAML disappears when k > 0 (fine-tuning with examples)
- Joint learning baseline performs competitively and more consistently across all target languages
- Both models show performance decrease when fine-tuned in 32-shot compared to 0-shot
- Model performance is more sensitive to translation quality for Korean than other languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-learning via MAML enables better adaptation in zero-shot scenarios for cross-lingual AMR parsing.
- Mechanism: MAML simulates few-shot training and evaluation during training by creating temporary models for each language, adapting them using the support set, evaluating on the query set, and aggregating losses to update the initial model parameters. This process trains the model to learn good initialization parameters that can be fine-tuned quickly for new languages.
- Core assumption: The initial parameters learned by MAML can effectively adapt to unseen languages with minimal data.
- Evidence anchors:
  - [abstract] "MAML learns good initial parameters θ that can be tuned to unseen tasks with only a few optimization steps and a few training data examples."
  - [section 2] "We use MAML (Finn et al., 2017) for cross-lingual AMR parsing. MAML learns good initial parameters θ that can be tuned to unseen tasks with only a few optimization steps and a few training data examples."
  - [corpus] Weak evidence: only 5 related papers mention meta-learning, with limited citations.
- Break condition: When the number of training examples (k) increases beyond 0, the advantage of MAML diminishes or disappears, as the baseline joint learning model can adapt effectively with more data.

### Mechanism 2
- Claim: Joint learning benefits from substantial overlap in training data across languages, allowing it to learn shared patterns more efficiently.
- Mechanism: The training data consists of translations of AMR 3.0 into multiple languages, resulting in overlapped AMR graphs and shared patterns in input texts. Joint learning can directly learn these similarities between training data, allowing the model to learn the task more efficiently.
- Core assumption: The overlap in training data across languages provides sufficient information for the joint learning model to generalize across languages without explicit meta-learning.
- Evidence anchors:
  - [section 4] "We hypothesize that substantial overlap between inputs and outputs in the training data across languages has contributed to these results. Our training data comprises translations of AMR 3.0 into multiple languages, resulting in overlapped AMR graphs and shared patterns in input texts."
  - [section 4] "In this context, the joint-learning model may learn the similarities between training data directly, allowing the model to learn the task more efficiently."
  - [corpus] No direct evidence found in corpus neighbors.
- Break condition: When the overlap between languages decreases or when the training data becomes more diverse, the advantage of joint learning may diminish.

### Mechanism 3
- Claim: Pre-trained multilingual models like mBart have sufficient knowledge of target languages, making fine-tuning with few examples potentially harmful.
- Mechanism: The mBart pre-trained model has already enough knowledge of the target languages. Fine-tuning the model with only a few examples in each language may impair the model's capacity, leading to performance degradation compared to not fine-tuning at all.
- Core assumption: The pre-trained multilingual model has learned representations that are sufficiently general to handle the target languages without extensive fine-tuning.
- Evidence anchors:
  - [section 4] "Surprisingly, both MAML and baseline models exhibit a performance decrease when fine-tuned in 32-shot, compared to not being fine-tuned at all. We hypothesize that the mBart pre-trained model has already enough knowledge of our target languages and fine-tuning the model with only a few examples in each language may impair the model's capacity."
  - [section 4] "This could also be attributed to the domain difference between the fine-tuning dataset and the test dataset."
  - [corpus] No direct evidence found in corpus neighbors.
- Break condition: When the domain shift between fine-tuning and test data is minimized, or when more fine-tuning examples are available, the negative impact of fine-tuning may be reduced.

## Foundational Learning

- Concept: Abstract Meaning Representation (AMR)
  - Why needed here: AMR is the target formalism for parsing, and understanding its structure is crucial for evaluating the performance of the parsing models.
  - Quick check question: What are the key components of an AMR graph, and how does linearization facilitate sequence-to-sequence parsing?

- Concept: Meta-learning (MAML)
  - Why needed here: MAML is the core algorithm being evaluated for its effectiveness in cross-lingual AMR parsing, and understanding its mechanism is essential for interpreting the results.
  - Quick check question: How does MAML simulate few-shot learning during training, and what is the role of the inner and outer loops?

- Concept: k-shot learning
  - Why needed here: k-shot learning is the evaluation setting used to assess the models' ability to adapt to new languages with limited data, and understanding its implications is crucial for interpreting the results.
  - Quick check question: What is the difference between 0-shot, k-shot, and full-shot learning, and how does the choice of k affect the evaluation of cross-lingual models?

## Architecture Onboarding

- Component map: English AMR 3.0 -> Machine Translation (DeepL) -> 13 languages -> Training Data -> mBart Model -> MAML or Joint Learning -> Evaluation (French, Chinese, Korean, Farsi, Croatian)
- Critical path: Generate multilingual training data → Train model using MAML or joint learning → Evaluate in k-shot scenarios across target languages
- Design tradeoffs: Choice between MAML and joint learning involves tradeoff between meta-learning benefits in zero-shot vs joint learning robustness with overlapping data; choice of k involves tradeoff between evaluating adaptation with limited data vs potential harm from fine-tuning with too few examples
- Failure signatures: Performance degradation when k > 0 could indicate pre-trained model has sufficient knowledge making fine-tuning harmful; inconsistent performance across target languages could indicate insufficient overlap in training data
- First 3 experiments:
  1. Train and evaluate the baseline joint learning model in 0-shot and k-shot scenarios across all target languages to establish a performance baseline
  2. Train and evaluate the MAML model in 0-shot and k-shot scenarios across all target languages to assess the benefits of meta-learning
  3. Analyze the impact of the number of training languages on the performance of both models by training models with different numbers of languages and evaluating them in 0-shot scenarios

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond the scope of the current study.

## Limitations
- Small number of target languages (5) limits generalizability of findings
- Domain mismatch between machine-translated training data and literary test data may affect results
- Relatively small dataset size (AMR 3.0) may limit effectiveness of both MAML and joint learning approaches

## Confidence

- **High Confidence**: Experimental methodology is sound with clear implementation, appropriate metrics, and systematic testing across scenarios
- **Medium Confidence**: Conclusion that joint learning serves as robust baseline is well-supported, though meta-learning findings require careful interpretation
- **Medium Confidence**: Hypothesis about mBart's pre-trained knowledge being sufficient is plausible but needs further validation

## Next Checks

1. **Cross-domain validation**: Evaluate both models on multilingual AMR data from different domains (news, social media, biomedical) to assess whether joint learning advantage persists beyond literary text
2. **Scale-up analysis**: Test models with larger multilingual AMR datasets and additional target languages to determine if meta-learning benefits emerge with more diverse language pairs
3. **Pre-trained model ablation**: Compare mBart with other multilingual pre-trained models (mBART-50, XGLM, or T5-XXL) to isolate whether observed effects are specific to mBart's training