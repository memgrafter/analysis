---
ver: rpa2
title: 'Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance
  to Ask Clarification Questions'
arxiv_id: '2402.06509'
source_url: https://arxiv.org/abs/2402.06509
tags:
- clarification
- uncertainty
- questions
- dialogue
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how model uncertainty relates to human
  uncertainty in dialogue systems. The authors find that model uncertainty does not
  correlate well with human clarification behavior, suggesting that using human clarification
  questions as supervision for deciding when to ask may not be effective.
---

# Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions

## Quick Facts
- arXiv ID: 2402.06509
- Source URL: https://arxiv.org/abs/2402.06509
- Reference count: 40
- Primary result: Model uncertainty does not correlate well with human clarification behavior in dialogue systems

## Executive Summary
This paper investigates the relationship between model uncertainty and human uncertainty in dialogue systems, specifically focusing on when clarification questions should be asked. The authors find that model uncertainty does not correlate well with human clarification behavior, suggesting that using human clarification questions as supervision for deciding when to ask may not be effective. They propose a new approach to generating clarification questions based on model uncertainty estimation. Their experiments on the CoDraw task demonstrate that their proposed method leads to significant improvements in task success compared to several baselines, including one based on human supervision.

## Method Summary
The authors propose a method for generating clarification questions in dialogue systems based on model uncertainty estimation. Their approach uses uncertainty measures from the model to determine when to ask clarification questions, rather than relying on human-annotated clarification examples. The method is evaluated on the CoDraw task, where one agent must draw an image based on another agent's description. The uncertainty-based approach generates questions when the model detects ambiguity or uncertainty in understanding the user's input, aiming to improve task completion rates.

## Key Results
- Model uncertainty does not correlate well with human clarification behavior in dialogue systems
- The proposed uncertainty-based clarification approach leads to significant improvements in task success on the CoDraw task
- The method outperforms several baselines, including one based on human supervision

## Why This Works (Mechanism)
The paper's core mechanism relies on using model uncertainty as a signal for when to generate clarification questions, rather than relying on human-annotated examples. By estimating the model's confidence in its understanding of user input, the system can proactively identify ambiguous situations and generate appropriate questions. This approach is more scalable than using human supervision, as it doesn't require extensive annotation of clarification moments. The method leverages the model's own uncertainty to guide the clarification process, potentially capturing nuances that human supervisors might miss or that are difficult to annotate systematically.

## Foundational Learning
- **Model uncertainty estimation**: Needed to identify when the system lacks confidence in understanding user input. Quick check: Verify that uncertainty measures correlate with actual comprehension errors in controlled experiments.
- **Dialogue state tracking**: Essential for maintaining context and identifying where ambiguity arises in the conversation. Quick check: Ensure state tracking accuracy is high before adding clarification mechanisms.
- **Clarification question generation**: Required to formulate appropriate questions when uncertainty is detected. Quick check: Validate that generated questions actually resolve identified ambiguities.
- **Task success metrics**: Used to evaluate whether clarification questions improve overall system performance. Quick check: Confirm that task success improvements aren't due to chance or superficial interaction changes.

## Architecture Onboarding

**Component map**: User input -> Uncertainty estimator -> Question generator -> Clarification question -> User response -> Task completion

**Critical path**: The critical path involves detecting uncertainty in user input, generating an appropriate clarification question, receiving the user's response, and updating the system's understanding. This loop continues until the system has sufficient information to complete the task or the user indicates they cannot provide further clarification.

**Design tradeoffs**: The main tradeoff is between asking too many questions (which can frustrate users) and asking too few (which can lead to task failure). The system must balance uncertainty detection sensitivity against the cost of clarification. Another tradeoff involves the complexity of the uncertainty estimation model versus its accuracy and computational efficiency.

**Failure signatures**: The system may fail by asking irrelevant or confusing clarification questions, by failing to ask questions when needed, or by asking questions that don't actually resolve the underlying uncertainty. Performance degradation may also occur if the uncertainty estimation is inaccurate or if the question generation component produces unnatural or unhelpful questions.

**Three first experiments**:
1. Test uncertainty detection accuracy on a held-out validation set with known ambiguous inputs
2. Evaluate question generation quality through human ratings of question relevance and helpfulness
3. Compare task success rates with and without the clarification mechanism on a simplified version of the task

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Findings are based on a specific task (CoDraw) and may not generalize to other dialogue domains
- The proposed approach relies on model uncertainty estimation, which itself may be imperfect
- Comparison to baselines is limited to a specific set of methods
- Evaluation focuses primarily on task success metrics rather than user experience or interaction naturalness
- Experiments are conducted in synthetic or controlled environments rather than with real human-machine interactions

## Confidence
- High confidence in the observation that model uncertainty and human clarification behavior show poor correlation in the CoDraw task
- Medium confidence in the effectiveness of the proposed uncertainty-based clarification approach, as results are task-specific
- Medium confidence in the claim that human clarification questions may not be effective supervision, given the limited scope of comparison
- Low confidence in the generalizability of findings to other dialogue domains or real-world deployment scenarios

## Next Checks
1. Test the proposed method across multiple dialogue domains (e.g., customer service, task-oriented dialogue, open-ended conversation) to assess generalizability of the findings
2. Conduct human evaluations with real users interacting with the system to measure not just task success but also user satisfaction and perceived naturalness of clarification questions
3. Compare against additional baseline approaches including more recent uncertainty estimation methods and clarification question generation techniques to strengthen the empirical validation