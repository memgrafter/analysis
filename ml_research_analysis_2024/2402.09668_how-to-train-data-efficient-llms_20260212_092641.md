---
ver: rpa2
title: How to Train Data-Efficient LLMs
arxiv_id: '2402.09668'
source_url: https://arxiv.org/abs/2402.09668
tags:
- perplexity
- data
- ask-llm
- sampling
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies data-efficient training of large language models
  (LLMs) by comparing 19 data selection algorithms. The key insight is that the roles
  of data quality and coverage depend on the stage of training, model size, and sampling
  rate.
---

# How to Train Data-Efficient LLMs

## Quick Facts
- arXiv ID: 2402.09668
- Source URL: https://arxiv.org/abs/2402.09668
- Reference count: 40
- Key outcome: LLM-based quality filtering (ASK-LLM) achieves up to 70% faster convergence and closes up to 33% of the performance gap to next-largest model size

## Executive Summary
This paper studies data-efficient training of large language models (LLMs) by comparing 19 data selection algorithms. The key insight is that the roles of data quality and coverage depend on the stage of training, model size, and sampling rate. To explore this, the authors propose two novel samplers: ASK-LLM, which uses a proxy LLM to directly assess data quality via instruction-tuned reasoning, and DENSITY, which maximizes coverage by sampling based on a density model of the data. In large-scale experiments involving 170 pre-training and 2,500 fine-tuning runs, ASK-LLM consistently outperformed other samplers and even full-data training.

## Method Summary
The authors evaluate 19 data selection algorithms on T5-Small (60M) and T5-Large (800M) models pre-trained on C4 for 524B tokens each. They propose ASK-LLM, which uses instruction-tuned LLMs to score data quality through zero-shot reasoning, and DENSITY, which maximizes coverage using kernel density estimation and inverse propensity sampling. Models are evaluated across 111 downstream tasks to measure data efficiency using an "over-scaling" metric comparing performance to larger models.

## Key Results
- ASK-LLM outperformed all other samplers and achieved up to 70% faster convergence
- ASK-LLM closed up to 33% of the performance gap to the next-largest model size
- DENSITY sampling recovered full-data performance by maximizing coverage
- Larger proxy models used in ASK-LLM provided increasing benefits as target LLM size grew

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASK-LLM quality filtering improves model performance by identifying and removing low-quality training samples that perplexity filtering misses
- Mechanism: ASK-LLM uses a proxy LLM to evaluate training samples through instruction-tuned reasoning, scoring each example as "yes" (use for training) or "no" (discard). This contextual evaluation catches failure modes like missing context, nonsense repetition, and niche topics that perplexity filters incorrectly score as low quality or high quality
- Core assumption: An instruction-tuned LLM can effectively judge the quality of training data for LLM pre-training by identifying informative signal, proper formatting, and harmful content
- Evidence anchors:
  - [abstract]: "ASK-LLM leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example"
  - [section]: "ASK-LLM correctly identifies that these examples do not provide new information" regarding contextless questions
  - [corpus]: Weak - the corpus contains only related papers without direct evidence of ASK-LLM's effectiveness

### Mechanism 2
- Claim: DENSITY sampling maximizes coverage by selecting diverse examples from under-represented regions of the data distribution
- Mechanism: DENSITY estimates the local density of training examples in the embedding space using kernel sums, then applies inverse propensity sampling to uniformize the selection distribution. This boosts signal from low-density regions while downsampling redundant high-density information
- Core assumption: The data distribution in the embedding space provides a reliable signal for topic coverage, with high-density regions being redundant and low-density regions containing unique/rare inputs
- Evidence anchors:
  - [section]: "Our intuition is that the data distribution provides a strong coverage signal. High-probability regions contain 'prototypical' examples... Low-probability regions will contain outliers, noise, and unique/rare inputs."
  - [section]: "DENSITY asks whether we have already sampled many similar examples"
  - [corpus]: Weak - no corpus evidence directly supports the DENSITY sampling mechanism

### Mechanism 3
- Claim: LLM-based quality filtering scales with model capacity, providing increasing benefits as the target LLM size grows
- Mechanism: Larger proxy models used in ASK-LLM develop more nuanced quality distinctions, capturing tail-end knowledge and rare named entities that smaller models miss. This scaling relationship is not observed with perplexity filtering
- Core assumption: The reasoning capability required to judge data quality for LLM pre-training improves with model size, and this improvement translates to better downstream performance
- Evidence anchors:
  - [section]: "Figure 6 demonstrates a clear scaling trend for ASK-LLM's quality-scoring model: larger scoring models are increasingly beneficial as the scale of the to-be-trained LLM increases"
  - [section]: "Perplexity filters do not seem to exhibit such trends"
  - [corpus]: Weak - no corpus evidence directly supports the scaling relationship

## Foundational Learning

- Concept: Data selection and curation for machine learning
  - Why needed here: The paper compares 19 different data selection algorithms, requiring understanding of coverage vs quality tradeoffs
  - Quick check question: What is the fundamental difference between coverage-based and quality-based data selection methods?

- Concept: Kernel density estimation and inverse propensity sampling
  - Why needed here: DENSITY sampling uses these techniques to maximize coverage in the embedding space
  - Quick check question: How does inverse propensity sampling with density estimates uniformize the selection distribution?

- Concept: Transformer model architecture and pre-training
  - Why needed here: The paper pre-trains T5 models and evaluates downstream performance, requiring understanding of LLM training dynamics
  - Quick check question: What is the relationship between pre-training data quality/coverage and downstream task performance?

## Architecture Onboarding

- Component map: Dataset → Embedding Model (Sentence-T5) → Density Estimator (DENSITY) or Quality Scorer (ASK-LLM) → Sampler (Top-K/IPS) → Filtered Dataset → T5 Model → Pre-training → Downstream Evaluation
- Critical path: Quality scoring/filtering → Model pre-training → Downstream evaluation
- Design tradeoffs: ASK-LLM provides better quality filtering but requires expensive LLM inference per sample; DENSITY is cheaper but may miss nuanced quality distinctions
- Failure signatures: Poor downstream performance despite data filtering; inconsistent quality scores across different proxy model sizes; failure to improve over random sampling
- First 3 experiments:
  1. Run ASK-LLM with a small proxy model (Flan-T5-Small) on a subset of C4 and compare perplexity reduction to random sampling
  2. Implement DENSITY sampling with different kernel bandwidths and evaluate coverage diversity metrics
  3. Compare training efficiency (steps to convergence) between ASK-LLM and perplexity filtering at 20% sampling rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between quality and coverage sampling at different stages of LLM pre-training, and how does this balance change with model size?
- Basis in paper: [explicit] The paper states "We hypothesize that the roles of coverage and quality depend on the stage of training, size of the model, and the sampling rate."
- Why unresolved: The paper conducts extensive experiments but does not provide a definitive formula or rule for determining the optimal balance at each stage of training for different model sizes
- What evidence would resolve it: Systematic experiments varying the balance between quality and coverage sampling at different training stages and model sizes, with clear performance metrics to determine optimal configurations

### Open Question 2
- Question: How does the performance of ASK-LLM scale with the size of the proxy LLM used for quality scoring, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper notes "Figure 6 demonstrates a clear scaling trend for ASK-LLM's quality-scoring model: larger scoring models are increasingly beneficial as the scale of the to-be-trained LLM increases."
- Why unresolved: While the paper observes scaling trends, it does not determine the exact point of diminishing returns or provide a model for predicting optimal proxy LLM size for a given to-be-trained LLM size
- What evidence would resolve it: Experiments systematically varying proxy LLM sizes for different to-be-trained LLM sizes, with performance metrics to identify the point of diminishing returns

### Open Question 3
- Question: Can the ASK-LLM framework be extended to improve data efficiency in other domains beyond text, such as vision or audio?
- Basis in paper: [inferred] The paper focuses on LLM pre-training, but the concept of using a proxy model to assess data quality could potentially be applied to other domains
- Why unresolved: The paper does not explore applications of ASK-LLM outside of text-based LLM pre-training
- What evidence would resolve it: Experiments applying the ASK-LLM framework to pre-training models in other domains, such as vision transformers or audio transformers, with performance comparisons to traditional sampling methods

## Limitations

- The study uses relatively small T5 models (60M-800M parameters) rather than true large language models, raising questions about generalizability to frontier model scales
- The 111 downstream tasks, while diverse, may not fully capture the capabilities needed for real-world LLM deployment
- The corpus evidence supporting the core mechanisms is weak, with related papers section containing no direct evidence about ASK-LLM's effectiveness

## Confidence

- **High Confidence**: The experimental methodology and evaluation framework are rigorous, with 170 pre-training runs and 2,500 fine-tuning runs providing strong statistical power. The observation that ASK-LLM outperforms random sampling and perplexity filtering at various sampling rates is well-supported by the data.
- **Medium Confidence**: The claim that ASK-LLM "consistently outperforms" all other methods and achieves "up to 70% faster convergence" is supported by the experimental results, but the exact magnitude may vary with different datasets or model architectures. The scaling relationship between proxy model size and effectiveness is observed but not extensively explored across different data distributions.
- **Low Confidence**: The assertion that ASK-LLM "closes up to 33% of the performance gap to the next-largest model size" extrapolates from small-scale experiments to claims about model scaling that aren't directly tested. The paper's discussion of why perplexity filtering fails to show similar scaling trends is speculative without deeper analysis.

## Next Checks

1. **Cross-dataset validation**: Test ASK-LLM and DENSITY sampling on diverse text corpora beyond C4 (e.g., Wikipedia, Books, or domain-specific collections) to verify robustness across different data distributions and writing styles

2. **True LLM scale validation**: Implement the same sampling algorithms for training models in the 1B-10B parameter range to test whether the observed scaling relationships hold for models closer to production LLMs, and measure the actual compute efficiency gains

3. **Ablation on proxy model diversity**: Compare ASK-LLM performance using proxy models with different training distributions (not just instruction-tuned models) to test whether the quality judgments are sensitive to the proxy model's own training data and whether this introduces selection bias