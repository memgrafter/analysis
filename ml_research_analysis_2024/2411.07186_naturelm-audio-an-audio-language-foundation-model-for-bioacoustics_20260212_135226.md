---
ver: rpa2
title: 'NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics'
arxiv_id: '2411.07186'
source_url: https://arxiv.org/abs/2411.07186
tags:
- species
- audio
- tasks
- speech
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NatureLM-audio is the first audio-language foundation model designed
  specifically for bioacoustics, addressing the need for models that can handle animal
  vocalization classification, detection, and captioning tasks. It is trained on a
  large, diverse dataset combining bioacoustic recordings with speech and music data
  to leverage cross-domain transfer.
---

# NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics

## Quick Facts
- arXiv ID: 2411.07186
- Source URL: https://arxiv.org/abs/2411.07186
- Reference count: 40
- NatureLM-audio is the first audio-language foundation model for bioacoustics, achieving state-of-the-art performance on the BEANS-Zero benchmark.

## Executive Summary
NatureLM-audio is the first audio-language foundation model designed specifically for bioacoustics, addressing the need for models that can handle animal vocalization classification, detection, and captioning tasks. It is trained on a large, diverse dataset combining bioacoustic recordings with speech and music data to leverage cross-domain transfer. The model uses a BEATs encoder, a Q-Former, and a fine-tuned Llama 3.1-8B backbone to produce text outputs from audio inputs. Evaluation on the BEANS-Zero benchmark shows state-of-the-art performance on multiple tasks, including zero-shot classification of unseen species (accuracy up to 34.3%), detection, lifestage and call-type prediction, and captioning. The inclusion of speech and music in training improves performance on tasks like individual counting. NatureLM-audio generalizes well to unseen taxa and tasks, outperforming baselines such as SALMONN, BioLingual, and Qwen2-audio. The model is open-sourced to advance bioacoustics research.

## Method Summary
NatureLM-audio uses a BEATs audio encoder to convert raw audio to embeddings, a Q-Former to bridge audio embeddings to LLM token space, and a Llama 3.1-8B backbone fine-tuned with LoRA for text generation. The model is trained in two stages: Stage 1 focuses on species classification, while Stage 2 introduces a variety of bioacoustic and other tasks including speech and music. The training dataset consists of carefully curated text-audio pairs spanning bioacoustics, speech, and music, designed to address the field's limited availability of annotated data. Evaluation is conducted on the BEANS-Zero benchmark, which includes novel tasks and unseen taxa splits for zero-shot learning assessment.

## Key Results
- State-of-the-art performance on BEANS-Zero benchmark across multiple tasks
- Zero-shot classification accuracy up to 34.3% on unseen species
- Improved performance on counting and detection tasks when including speech and music in training
- Generalization to unseen taxa and tasks beyond species classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-domain transfer from speech and music to bioacoustics improves model performance on unseen tasks.
- Mechanism: The model learns general audio representations during training on speech and music, which are transferable to bioacoustic tasks like individual counting and call-type prediction.
- Core assumption: Representations learned from human speech and music contain features that are relevant to animal vocalizations.
- Evidence anchors:
  - [abstract]: "We demonstrate successful transfer of learned representations from music and speech to bioacoustics"
  - [section]: "We show that representations learned from these domains successfully transfer to animal vocalizations, demonstrating generalization across species."
- Break condition: If speech and music data do not contain transferable features for bioacoustics, performance on bioacoustic tasks would not improve.

### Mechanism 2
- Claim: The audio-language foundation model architecture enables zero-shot learning on unseen species and tasks.
- Mechanism: The model learns to map audio inputs to text outputs using a large language model backbone, allowing it to handle novel tasks through in-context learning.
- Core assumption: The large language model can generalize to new tasks based on the learned audio-text mapping.
- Evidence anchors:
  - [abstract]: "Our model shows promising generalization to unseen taxa and tasks"
  - [section]: "Unlike existing bioacoustics benchmarks...we go beyond species classification. Additionally, our dataset presents prompts and audio descriptions in natural language, fostering further research in LALMs."
- Break condition: If the language model cannot handle novel tasks through in-context learning, zero-shot performance would degrade.

### Mechanism 3
- Claim: The curriculum learning approach improves model performance by first establishing robust classification abilities before introducing complex tasks.
- Mechanism: Two-stage training where the model first learns species classification, then adds detection, captioning, and other tasks.
- Core assumption: Learning species classification first provides a foundation for more complex bioacoustic tasks.
- Evidence anchors:
  - [section]: "We follow the proposed two-stage training strategy. In both stages, we use a linear warmup followed by a cosine learning rate schedule...Stage 1 (Perception Pretraining): We pretrain the model exclusively on focal species classification...Stage 2 (Generalization Fine-tuning): In the second stage, we introduce a variety of bioacoustic and other tasks"
  - [abstract]: "Our training dataset consists of carefully curated text-audio pairs spanning bioacoustics, speech, and music, designed to address the field's limited availability of annotated data."
- Break condition: If the model cannot transfer knowledge from classification to more complex tasks, the curriculum approach would not provide benefits.

## Foundational Learning

- Concept: Audio tokenization and representation learning
  - Why needed here: The model needs to convert raw audio into meaningful representations that can be processed by the language model
  - Quick check question: Can you explain how BEATs converts audio waveforms into token sequences?

- Concept: Large language model fine-tuning with LoRA
  - Why needed here: The model uses a pre-trained LLM as the text generation component, which needs to be adapted to handle audio inputs
  - Quick check question: What is the difference between full fine-tuning and LoRA-based fine-tuning for LLMs?

- Concept: Zero-shot learning and in-context learning
  - Why needed here: The model needs to handle novel species and tasks without additional training
  - Quick check question: How does the model generate predictions for species it has never seen during training?

## Architecture Onboarding

- Component map: Audio → BEATs → Q-Former → LLM → Text output
- Critical path: Raw audio passes through BEATs encoder, Q-Former bridges to LLM token space, and Llama 3.1-8B generates text output
- Design tradeoffs:
  - Frozen LLM vs. full fine-tuning: Freezing reduces compute but may limit adaptation
  - Two-stage training vs. single-stage: Two-stage may improve stability but increases training time
  - BEATs vs. other encoders: BEATs provides good audio representations but may not be optimal for all bioacoustic tasks
- Failure signatures:
  - Random outputs: Likely issues with audio encoding or Q-Former connection
  - Poor generalization: May indicate insufficient diversity in training data
  - Mode collapse: Could suggest issues with training stability or loss function
- First 3 experiments:
  1. Test audio encoding: Run a single audio sample through BEATs and verify the output dimensions and values
  2. Test Q-Former connection: Feed BEATs embeddings into Q-Former and check if outputs match expected token space
  3. Test basic inference: Run a simple classification task with a trained model to verify the full pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NatureLM-audio change when trained with additional bioacoustic data from underrepresented taxa?
- Basis in paper: [explicit] The paper notes that there is a bias towards bird vocalizations due to the overrepresentation of bird datasets.
- Why unresolved: The paper does not explore the impact of including more diverse taxa in the training data on the model's performance.
- What evidence would resolve it: Training and evaluating NatureLM-audio on a dataset with a more balanced representation of different taxa and comparing its performance to the current model.

### Open Question 2
- Question: Can NatureLM-audio's performance be further improved by using a larger or more diverse LLM backbone?
- Basis in paper: [inferred] The paper uses Llama 3.1-8B as the LLM backbone and notes that they chose it for its extensive knowledge of animal species. However, larger or more specialized LLMs may exist.
- Why unresolved: The paper does not explore the impact of using different LLM backbones on the model's performance.
- What evidence would resolve it: Training and evaluating NatureLM-audio with different LLM backbones (e.g., Llama 3.1-70B, or a bioacoustics-specific LLM) and comparing their performance.

### Open Question 3
- Question: How does the inclusion of non-bioacoustic data (speech and music) affect the model's ability to generalize to unseen taxa and tasks?
- Basis in paper: [explicit] The paper includes an ablation study that removes speech and music data from training and evaluates its impact on an unseen task (counting zebra finches).
- Why unresolved: The ablation study only evaluates one unseen task, and it is unclear how the removal of speech and music data affects performance on other tasks or generalization to unseen taxa.
- What evidence would resolve it: Conducting a more comprehensive ablation study that evaluates the impact of removing speech and music data on all tasks and generalization to unseen taxa.

### Open Question 4
- Question: Can NatureLM-audio be used to generate realistic synthetic bioacoustic data?
- Basis in paper: [inferred] The paper mentions exploring the model's generative abilities as a future direction, specifically in producing audio tokens for applications such as animal sound synthesis.
- Why unresolved: The paper does not investigate the model's ability to generate synthetic bioacoustic data.
- What evidence would resolve it: Using NatureLM-audio to generate synthetic bioacoustic data and evaluating its realism and usefulness for bioacoustic research.

### Open Question 5
- Question: How does NatureLM-audio perform on bioacoustic tasks in languages other than English?
- Basis in paper: [explicit] The paper mentions enhancing the model's multilingual capabilities as a future direction.
- Why unresolved: The paper does not evaluate the model's performance on bioacoustic tasks in languages other than English.
- What evidence would resolve it: Training and evaluating NatureLM-audio on bioacoustic datasets in multiple languages and comparing its performance to the English-only model.

## Limitations
- The model shows bias towards bird vocalizations due to overrepresentation in training data
- Generalization capabilities beyond tested benchmark tasks remain unexplored
- Performance on extremely rare or acoustically complex species is not addressed

## Confidence
- High confidence: The NatureLM-audio architecture (BEATs + Q-Former + Llama 3.1-8B) is technically sound and follows established practices in audio-language modeling
- Medium confidence: Cross-domain transfer from speech and music to bioacoustics provides measurable benefits
- Medium confidence: The two-stage curriculum learning approach improves model performance
- Medium confidence: The model achieves state-of-the-art performance on the BEANS-Zero benchmark across multiple tasks

## Next Checks
1. **Cross-domain transfer mechanism validation**: Conduct controlled experiments training separate models on only bioacoustic data, only speech/music data, and the combined dataset to quantify the exact contribution of cross-domain transfer to performance improvements.

2. **Zero-shot generalization robustness**: Evaluate the model on additional bioacoustic datasets not included in BEANS-Zero, particularly focusing on rare species and challenging acoustic conditions to test the limits of zero-shot generalization.

3. **Curriculum learning ablation**: Train models using single-stage training (all tasks from the beginning) and compare performance against the two-stage approach to isolate the specific benefits of the curriculum learning strategy.