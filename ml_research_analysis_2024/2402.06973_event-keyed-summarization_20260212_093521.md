---
ver: rpa2
title: Event-Keyed Summarization
arxiv_id: '2402.06973'
source_url: https://arxiv.org/abs/2402.06973
tags:
- event
- summaries
- association
- linguistics
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces event-keyed summarization (EKS), a novel
  task that combines document-level event extraction with targeted summarization,
  aiming to generate concise summaries for specific events within a document. The
  authors present MUCSUM, a benchmark dataset derived from the classic MUC-4 dataset,
  containing summaries for all events in the original data.
---

# Event-Keyed Summarization

## Quick Facts
- arXiv ID: 2402.06973
- Source URL: https://arxiv.org/abs/2402.06973
- Reference count: 27
- Primary result: Introduces event-keyed summarization (EKS) task and MUCSUM benchmark, showing fine-tuned models outperform zero-shot models

## Executive Summary
This paper introduces event-keyed summarization (EKS), a novel task that combines document-level event extraction with targeted summarization, aiming to generate concise summaries for specific events within a document. The authors present MUCSUM, a benchmark dataset derived from the classic MUC-4 dataset, containing summaries for all events in the original data. They evaluate several fine-tuned and zero-shot baseline models, showing that MUCSUM effectively integrates document context with event-specific information. Human evaluation confirms the quality of both reference and model-generated summaries, with fine-tuned models (particularly T5 and BART) performing best across most metrics. The study demonstrates that both document and event template inputs are necessary for optimal performance, and that MUCSUM serves as a robust benchmark for EKS.

## Method Summary
The authors create MUCSUM by extracting event structures from MUC-4 documents using an event extraction system, then generating summaries for each event that integrate both the document context and event-specific information. They fine-tune BART, PEGASUS, and T5 models on this dataset using a linearization format that combines the document and event template as input. The models are trained to generate summaries conditioned on both inputs, and evaluated using multiple metrics including ROUGE, BERTScore, CEAF-REE, and NLI-based entailment scores. Zero-shot experiments with ChatGPT and GPT-4 are also conducted for comparison.

## Key Results
- MUCSUM summaries require both document context and event template for optimal performance
- Fine-tuned models (T5, BART) outperform zero-shot models on most metrics
- Both document and event template inputs are necessary, with degradations observed when either is ablated
- Human evaluation confirms the quality of both reference and model-generated summaries

## Why This Works (Mechanism)

### Mechanism 1
Event-keyed summarization works because it forces models to synthesize document context with event-specific role information rather than relying on extractive overlap. By conditioning generation on both the full document and a linearized event template (event type + role fillers), the model must map structured role information onto its narrative representation in the text. This structure-to-text pathway cannot be reduced to extractive summarization or pure structure-to-text generation.

### Mechanism 2
MUCSUM serves as a robust benchmark because its summaries are abstractive and require non-trivial synthesis of event information with surrounding context. The summaries are written to convey all relevant information about the event provided in the document, including contextual details that are not strictly in the event template. This forces models to perform true abstractive summarization rather than extractive copying.

### Mechanism 3
Fine-tuned models outperform zero-shot models because the structured input format (document + linearized template) is not naturally aligned with pre-training objectives. Pre-trained LMs are optimized for document-level language modeling, not for mapping structured event representations onto narrative text. Fine-tuning adapts the model parameters to this structured-to-text mapping task.

## Foundational Learning

- Document-level event extraction: EKS builds directly on document-level event extraction by requiring structured event representations as input. Quick check: What distinguishes document-level event extraction from sentence-level event extraction, and why is this distinction important for EKS?

- Abstractive summarization: MUCSUM summaries are abstractive, requiring models to generate novel text that synthesizes information rather than copying from source. Quick check: How does abstractive summarization differ from extractive summarization, and what evaluation metrics best capture abstractiveness?

- Natural Language Inference (NLI) for summarization evaluation: The paper uses NLI-based metrics (D → Sp, Sp → Sr) to evaluate whether generated summaries are entailed by the document and reference. Quick check: What is the difference between lexical overlap metrics (ROUGE) and entailment-based metrics for summarization evaluation, and when might they diverge?

## Architecture Onboarding

- Component map: Document + Event Template → Linearization → Model → Summary
- Critical path: Document + Template → Model → Summary
- Design tradeoffs: Using linearized templates enables structured input but may not align with pre-training; right-truncation of documents preserves template information but may lose context; beam search decoding balances quality and diversity
- Failure signatures: Degraded performance on NLI metrics but maintained ROUGE scores suggests extractive behavior; poor CEAF-REE scores indicate failure to capture event arguments; context truncation may cause factual errors
- First 3 experiments:
  1. Ablation: Compare temp+doc vs doc only vs temp only to verify both inputs are necessary
  2. Zero-shot comparison: Compare fine-tuned vs zero-shot (ChatGPT/GPT-4) to quantify fine-tuning benefit
  3. Input length analysis: Systematically vary document truncation point to find optimal context length

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset size: The MUCSUM dataset contains only 3,000 summaries, which may limit generalizability and robustness of the benchmark
- Evaluation methodology: Combines multiple metrics without clear justification for their relative importance or weight in overall assessment
- Zero-shot performance: Mixed results with NLI metrics showing some zero-shot models outperforming fine-tuned models, suggesting potential issues with fine-tuning approach

## Confidence

- High Confidence: MUCSUM effectively integrates document context with event-specific information; fine-tuned models outperform zero-shot models on most metrics
- Medium Confidence: MUCSUM serves as a robust benchmark for EKS; both document and event template inputs are necessary for optimal performance
- Low Confidence: Specific ranking of model performance (T5 > BART > PEGASUS) across all metrics is not consistently supported; relative importance of document vs. event template context lacks fine-grained analysis

## Next Checks

1. **Cross-Domain Validation**: Test the fine-tuned models on EKS tasks from different domains (e.g., scientific literature, social media) to assess generalizability of the benchmark

2. **Metric Sensitivity Analysis**: Conduct sensitivity analysis of evaluation metrics by varying weighting between lexical overlap (ROUGE) and semantic entailment (NLI) metrics

3. **Template Completeness Study**: Systematically vary completeness of event templates to quantify how much template structure vs. document context drives summarization performance