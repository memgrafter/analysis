---
ver: rpa2
title: 'On Evaluating LLMs'' Capabilities as Functional Approximators: A Bayesian
  Perspective'
arxiv_id: '2410.04541'
source_url: https://arxiv.org/abs/2410.04541
tags:
- data
- function
- domain
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates LLMs as function approximators by separating
  their abilities to understand raw data patterns from their ability to incorporate
  domain knowledge. Using a Bayesian perspective, the authors assess GPT-4's performance
  on synthetic and real-world tasks.
---

# On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective

## Quick Facts
- arXiv ID: 2410.04541
- Source URL: https://arxiv.org/abs/2410.04541
- Reference count: 7
- Key outcome: LLMs struggle with raw data pattern recognition but excel when given domain knowledge, matching or exceeding expert models

## Executive Summary
This paper evaluates large language models as function approximators through a Bayesian lens, separating their ability to understand raw data patterns from their capacity to incorporate domain knowledge. The authors find that while LLMs perform poorly on complex function modeling from raw numerical data—significantly worse than MLPs—their performance dramatically improves when provided with domain knowledge. The study demonstrates that LLMs' strength lies not in pattern recognition from raw data but in leveraging their pretraining knowledge to enhance predictions when domain context is available.

## Method Summary
The paper uses GPT-4 to evaluate function modeling capabilities across synthetic and real-world datasets. The authors employ a Bayesian framework to decompose LLM performance into two components: understanding raw data patterns (likelihood p(D|f)) and incorporating domain knowledge (prior p(f)). They use NUMERIZE/DECONTEXTUALIZE operations to evaluate p(D|f) without domain knowledge and VERBALIZE/CONTEXUALIZE operations to evaluate p(f|D) with domain knowledge. Performance is measured against MLP baselines on tasks ranging from simple synthetic functions to real-world datasets like income prediction and CO2 concentration modeling.

## Key Results
- LLMs perform significantly worse than MLPs on complex function modeling from raw numerical data alone
- When provided with domain knowledge, LLM performance improves substantially, matching or exceeding expert-designed models
- LLMs show particular strength in leveraging domain knowledge to compensate for limited data, especially on complex functions like periodic and piecewise relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform poorly on raw data pattern recognition but improve significantly when given domain knowledge.
- Mechanism: The LLM's function modeling ability can be decomposed into two components: (1) understanding raw data patterns (likelihood p(D|f)), and (2) incorporating domain knowledge (prior p(f)). The study finds that LLMs are weak at the first component but strong at the second.
- Core assumption: The Bayesian framework used to evaluate LLMs accurately separates their ability to understand raw data from their ability to use domain knowledge.
- Evidence anchors:
  - [abstract] "LLMs are relatively weak in understanding patterns in raw data, but excel at utilizing prior knowledge about the domain"
  - [section 3.1] "We first remove any information about domain by turning each data x in the original data D, which is potentially in verbal form, into purely numerical values"
  - [corpus] Weak - corpus papers focus on different aspects of LLMs and Bayesian methods, not directly on this specific mechanism
- Break condition: If the Bayesian framework fails to properly separate the two components, or if domain knowledge is not actually incorporated by the LLM in a meaningful way.

### Mechanism 2
- Claim: LLMs can leverage their pretraining knowledge to compensate for limited data in function modeling tasks.
- Mechanism: The prior p(f) shaped by the LLM's domain knowledge (acquired during pretraining) can significantly improve prediction accuracy, especially when data is scarce.
- Core assumption: The LLM has acquired relevant domain knowledge during pretraining that can be applied to the function modeling task.
- Evidence anchors:
  - [section 4.3] "Both the CO2 concentration level and the seasonal period exhibit significant improvements in comparison to just raw data"
  - [section 4.2] "The performance of the language model conditioned on the domain knowledge improves significantly, which is on par with an MLP trained on two orders of magnitude more data"
  - [corpus] Weak - corpus papers don't directly address this specific mechanism
- Break condition: If the LLM's pretraining knowledge is not relevant to the specific function modeling task, or if the domain knowledge is not properly incorporated into the predictions.

### Mechanism 3
- Claim: The LLM's ability to select informative features improves significantly with domain knowledge.
- Mechanism: With domain knowledge, the LLM can better understand the underlying function and select features that are most informative about the target variable.
- Core assumption: The LLM can accurately assess the informativeness of features when provided with domain knowledge.
- Evidence anchors:
  - [section 4.2] "Leveraging its domain knowledge, GPT-4 is able to select a subset of features that closely matches the output of state-of-the-art feature selection methods"
  - [section 4.2] "when relying solely on raw data, the model selects a poor set of features"
  - [corpus] Weak - corpus papers don't directly address this specific mechanism
- Break condition: If the LLM's feature selection is not actually based on understanding the underlying function, or if the selected features are not truly informative.

## Foundational Learning

- Concept: Bayesian inference
  - Why needed here: The paper uses a Bayesian framework to evaluate LLMs' function modeling capabilities, separating their ability to understand data patterns from their ability to use domain knowledge.
  - Quick check question: How does the Bayesian framework used in this paper separate the LLM's ability to understand raw data from its ability to use domain knowledge?

- Concept: Function modeling
  - Why needed here: The paper evaluates LLMs as function approximators, which requires understanding how to model complex relationships between inputs and outputs.
  - Quick check question: What is the difference between modeling simple functions (like linear or quadratic) and more complex functions (like periodic or piecewise) using LLMs?

- Concept: Domain knowledge
  - Why needed here: The paper emphasizes the importance of domain knowledge in improving LLMs' function modeling performance.
  - Quick check question: How does the paper incorporate domain knowledge into the LLM's predictions, and what are the specific techniques used?

## Architecture Onboarding

- Component map:
  - Data preprocessing: NUMERIZE and VERBALIZE functions to convert between numerical and natural language representations
  - Prompt engineering: DECONTEXTUALIZE and CONTEXUALIZE functions to control the presence of domain knowledge
  - Evaluation metrics: Accuracy and feature selection to assess LLM performance
  - Comparison models: MLP and Gaussian Process as baselines

- Critical path:
  1. Preprocess data using NUMERIZE or VERBALIZE
  2. Construct prompts using DECONTEXTUALIZE or CONTEXUALIZE
  3. Evaluate LLM performance on both raw data and domain knowledge conditions
  4. Compare results with baseline models

- Design tradeoffs:
  - Tokenization vs. numerical representation: The paper notes that tokenization can make numbers ill-suited for computation
  - Data efficiency vs. domain knowledge: Incorporating domain knowledge can compensate for limited data but may introduce bias
  - Interpretability vs. accuracy: The paper uses prediction interpretation and feature selection to gain insights, but these methods may not always be reliable

- Failure signatures:
  - Poor performance on raw data pattern recognition tasks
  - Inability to incorporate domain knowledge effectively
  - Over-reliance on spurious features when domain knowledge is absent

- First 3 experiments:
  1. Evaluate LLM performance on simple synthetic functions (linear, quadratic) with and without domain knowledge
  2. Test LLM on a real-world dataset (e.g., income prediction) with varying amounts of domain knowledge
  3. Compare LLM performance with baseline models (MLP, Gaussian Process) on complex functions (e.g., periodic, piecewise)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on function modeling tasks scale with model size?
- Basis in paper: [explicit] The paper mentions using GPT-4 as the most capable model during their evaluations and notes that their evaluation framework can be adapted to more recent models.
- Why unresolved: The paper only evaluates GPT-4 and doesn't explore how function modeling capabilities might change with different model sizes or architectures.
- What evidence would resolve it: Systematic evaluation of function modeling performance across different model sizes and architectures (e.g., comparing GPT-3.5, GPT-4, Claude, etc.) on the same benchmark tasks.

### Open Question 2
- Question: Can LLMs learn to better understand raw data patterns through fine-tuning on numerical datasets?
- Basis in paper: [inferred] The paper notes that LLMs struggle to model complex functions directly from raw data and suggests that future advancements may benefit from enhancing their ability to understand raw data patterns during pretraining.
- Why unresolved: The evaluation focuses on in-context learning and doesn't explore the potential benefits of fine-tuning LLMs on numerical data.
- What evidence would resolve it: Comparative experiments showing the difference in function modeling performance between in-context learning and fine-tuned models on the same numerical datasets.

### Open Question 3
- Question: What is the impact of different tokenization schemes on LLM performance in numerical prediction tasks?
- Basis in paper: [explicit] The paper mentions that the tokenization process in language models can split numbers in ways that make them ill-suited for computation.
- Why unresolved: While the paper identifies tokenization as a potential issue, it doesn't systematically investigate how different tokenization approaches affect numerical prediction performance.
- What evidence would resolve it: Experiments comparing LLM performance on numerical tasks using different tokenization schemes (e.g., byte-pair encoding vs. character-level vs. specialized numerical tokenization).

## Limitations
- The evaluation framework relies on the assumption that Bayesian decomposition accurately captures LLM capabilities
- Token limit constraints may underestimate LLM performance on complex tasks requiring extensive domain knowledge
- Results are based on GPT-4 evaluation and may not generalize across different LLM architectures

## Confidence
- High confidence: The finding that LLMs struggle with raw data pattern recognition without domain knowledge is well-supported by the synthetic function experiments
- Medium confidence: The claim that LLMs can match or exceed expert-designed models when given domain knowledge, as this depends heavily on the quality and relevance of the provided domain knowledge
- Low confidence: The assertion that LLMs' strength lies primarily in leveraging prior knowledge rather than understanding raw data patterns, as this may be architecture-dependent

## Next Checks
1. Replicate the synthetic function experiments using smaller LLM variants (like LLaMA-2-7B) to test whether the pattern of weak raw data understanding but strong domain knowledge utilization holds across model sizes
2. Design a controlled experiment that explicitly tests the token limit hypothesis by comparing performance on simple vs. complex domain knowledge descriptions with controlled context lengths
3. Implement an ablation study where domain knowledge is gradually removed from the prompts to quantify exactly how much performance improvement comes from different types and amounts of domain knowledge