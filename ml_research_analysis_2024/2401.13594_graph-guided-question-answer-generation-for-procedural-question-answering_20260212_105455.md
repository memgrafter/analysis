---
ver: rpa2
title: Graph Guided Question Answer Generation for Procedural Question-Answering
arxiv_id: '2401.13594'
source_url: https://arxiv.org/abs/2401.13594
tags:
- question
- questions
- arg1
- graph
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles task-specific question answering from procedural
  text, aiming to train compact models that can run on mobile devices while achieving
  performance competitive with large language models. The key innovation is a graph-guided
  method that leverages the structured nature of procedural text by representing it
  as Abstract Meaning Representation (AMR) graphs and flow graphs.
---

# Graph Guided Question Answer Generation for Procedural Question-Answering

## Quick Facts
- arXiv ID: 2401.13594
- Source URL: https://arxiv.org/abs/2401.13594
- Authors: Hai X. Pham; Isma Hadji; Xinnuo Xu; Ziedune Degutyte; Jay Rainey; Evangelos Kazakos; Afsaneh Fazly; Georgios Tzimiropoulos; Brais Martinez
- Reference count: 40
- Key outcome: Small QA models trained on graph-guided generated data outperform GPT3 and ChatGPT on procedural QA tasks

## Executive Summary
This paper addresses the challenge of training compact QA models for procedural text by generating high-quality, exhaustive QA pairs using graph-guided methods. The key innovation is leveraging the structured nature of procedural text through Abstract Meaning Representation (AMR) graphs and flow graphs to systematically identify all potential answers and generate corresponding questions. The method produces various question types including role-specific, instruction-level, polarity, and temporal questions spanning multiple steps. Experiments demonstrate that small models trained on this data achieve excellent performance on target QA tasks, outperforming much larger language models despite being orders of magnitude smaller.

## Method Summary
The method represents procedural text as AMR and flow graphs, then performs exhaustive content selection by traversing all graph nodes and relationships to identify potential answers. It generates questions by applying controlled transformations to the AMR graphs (such as replacing answer nodes with "amr-unknown" or adding polarity roles) and using AMR-to-text models to convert these into natural language questions. The approach covers four main question types: role-specific questions from individual instructions, instruction-level questions, polarity questions, and temporal questions spanning multiple steps based on flow graph relationships. The generated data is used to train compact QA models that achieve strong performance on human-annotated test sets.

## Key Results
- Small QA models trained on graph-generated data achieve 51.96 BLEU on human-annotated test sets
- The method outperforms GPT3 and ChatGPT despite being several orders of magnitude smaller
- Graph-guided generation produces more diverse and higher-quality questions than baseline methods
- Semantic coverage is shown to be more important than syntactic diversity for QA performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based semantic representations enable exhaustive content selection for QA pair generation
- Mechanism: Structured AMR and flow graphs allow systematic traversal of all nodes and relationships to identify potential answers
- Core assumption: Procedural text structure is accurately captured in AMR and flow graphs
- Evidence anchors: Abstract mentions leveraging structured aspects of procedural text; section describes using AMR and flow graph semantic formalisms
- Break condition: Inaccurate graph parsing fails to capture relevant semantic content

### Mechanism 2
- Claim: Controlled graph transformations enable diverse question types while maintaining semantic correctness
- Mechanism: Systematic transformations of AMR graphs (replacing answer nodes, adding polarity roles) generate semantically valid questions
- Core assumption: Graph-to-text models reliably generate natural language from transformed AMRs
- Evidence anchors: Abstract mentions conditioning on graph nodes for controllable QA generation; section describes constructing question AMRs and using AMR-to-text models
- Break condition: Graph-to-text models fail to generate coherent questions from transformed AMRs

### Mechanism 3
- Claim: Small QA models trained on semantically comprehensive data can outperform larger models
- Mechanism: Exhaustive generation covering all semantic content allows small models to learn comprehensive representations
- Core assumption: Semantic coverage is more important than syntactic diversity for QA performance
- Evidence anchors: Abstract states small models exceed GPT3 and ChatGPT performance; section highlights semantic coverage as key indicator
- Break condition: Semantic coverage proves less important than assumed, or small models cannot effectively learn from generated data

## Foundational Learning

- Concept: Abstract Meaning Representation (AMR)
  - Why needed here: Provides structured semantic representation enabling systematic content selection and question generation
  - Quick check question: Can you explain what an AMR is and how it differs from syntactic representations like dependency trees?

- Concept: Flow graphs for procedural text
  - Why needed here: Captures temporal relationships and action dependencies enabling generation of temporal questions spanning multiple steps
  - Quick check question: What information does a flow graph encode that a simple sequence of sentences doesn't capture?

- Concept: Graph-to-text generation
  - Why needed here: Converts AMR graphs into natural language questions, requiring understanding of graph-to-text mapping
  - Quick check question: How does graph-to-text generation differ from standard text-to-text generation, and what challenges does it present?

## Architecture Onboarding

- Component map: Procedural text -> AMR/flow graph parsing -> Content selection -> Question generation -> QA pairs -> Model training -> QA evaluation

- Critical path: Text → AMR/flow graph parsing → Content selection → Question generation → QA pairs → Model training → QA evaluation

- Design tradeoffs:
  - Accuracy vs. coverage: More exhaustive traversal may generate more questions but some may be less natural
  - Graph parsing quality vs. question quality: Errors in parsing propagate to generated questions
  - LLM augmentation vs. computational cost: LLMs improve quality but increase resource requirements

- Failure signatures:
  - Generated questions don't make sense or are unanswerable from the context
  - QA pairs contain factual errors or hallucinations
  - Model trained on generated data performs poorly on human-annotated test sets
  - Graph parsing fails to capture important semantic content

- First 3 experiments:
  1. Test AMR parsing on a small set of procedural sentences and manually verify the graph structure captures key semantic elements
  2. Implement one type of question generation (e.g., role-specific questions) and manually evaluate a sample of generated questions for quality and answerability
  3. Train a small QA model on data generated from a single recipe and test on human-annotated questions from that same recipe to verify the end-to-end pipeline works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed method perform on procedural text from domains other than cooking recipes, such as assembly instructions or technical manuals?
- Basis in paper: [inferred] The paper focuses on cooking recipes but does not evaluate performance on other domains
- Why unresolved: The method relies on specific semantic structures found in cooking recipes; unclear if these generalize to other procedural text
- What evidence would resolve it: Evaluating the method on diverse procedural text domains and comparing performance to the cooking recipe domain

### Open Question 2
- Question: What is the impact of AMR parsing errors on the overall performance of the proposed method?
- Basis in paper: [inferred] The paper mentions errors in graph-parsing strategies are not mitigated but does not quantify their impact
- Why unresolved: AMR parsing errors could propagate through the question generation pipeline and affect QA pair quality
- What evidence would resolve it: Analyzing AMR parsing error distribution and measuring impact on downstream QA performance

### Open Question 3
- Question: How does the proposed method compare to using even larger language models (e.g., GPT-4) for question generation?
- Basis in paper: [explicit] The paper compares to GPT-3 and ChatGPT but does not consider larger models like GPT-4
- Why unresolved: While GPT-3 is very large, newer models may offer better language modeling capabilities that could improve question generation quality
- What evidence would resolve it: Evaluating the proposed method against GPT-4 and other state-of-the-art models on the same QA generation task

### Open Question 4
- Question: What is the optimal trade-off between semantic coverage and syntactic diversity for maximizing QA performance?
- Basis in paper: [explicit] The paper highlights semantic coverage as key for QA performance but also uses LLM-based paraphrasing to improve syntactic diversity
- Why unresolved: Unclear how much syntactic diversity is needed to complement semantic coverage without degrading QA performance
- What evidence would resolve it: Conducting controlled experiments varying syntactic diversity while keeping semantic coverage constant

## Limitations

- Method relies heavily on quality of AMR and flow graph parsing, which are not fully specified
- Experimental results are limited to cooking recipes domain, limiting generalizability to other procedural text
- Paper does not address potential bias in generated data or scalability to longer, more complex procedures
- Comparisons with baseline methods may not fully represent state-of-the-art QA data augmentation techniques

## Confidence

- Claims about graph-guided exhaustive content selection: Medium confidence (depends on parsing quality)
- Claims about semantic coverage importance: Medium confidence (based on limited baseline comparisons)
- Claims about outperforming large language models: High confidence (supported by experimental results on specific test sets)
- Claims about method generalizability: Low confidence (limited to cooking recipes domain)

## Next Checks

1. Evaluate the graph parsing accuracy on a sample of procedural sentences to ensure AMR and flow graphs capture all relevant semantic content for QA

2. Implement the full pipeline on a different procedural text domain (e.g., assembly instructions) and test performance on human-annotated questions to assess generalizability

3. Compare against state-of-the-art QA data augmentation methods (e.g., back-translation, rule-based transformations) to validate the importance of semantic coverage over syntactic diversity