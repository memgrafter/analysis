---
ver: rpa2
title: A Survey on Neural Architecture Search Based on Reinforcement Learning
arxiv_id: '2409.18163'
source_url: https://arxiv.org/abs/2409.18163
tags:
- search
- neural
- learning
- architecture
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper provides an overview of neural architecture search
  (NAS) methods based on reinforcement learning (RL). The paper introduces the concept
  of NAS, which aims to automatically find the best network structure for specific
  tasks.
---

# A Survey on Neural Architecture Search Based on Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.18163
- Source URL: https://arxiv.org/abs/2409.18163
- Reference count: 30
- Authors: Wenzhu Shao
- One-line primary result: Comprehensive survey of RL-based neural architecture search methods, covering improvements in search speed, computational cost, and extensions to multi-objective optimization.

## Executive Summary
This survey paper provides an overview of neural architecture search (NAS) methods based on reinforcement learning (RL). NAS aims to automatically find optimal network structures for specific tasks, and this survey focuses on RL-based approaches that treat architecture generation as a sequential decision-making problem. The paper covers key methods including Q-learning, policy gradient methods, and Monte Carlo methods, along with improvements such as weight sharing, hierarchical representation, and performance prediction to accelerate the search process.

## Method Summary
The survey describes RL-based NAS as a framework where a controller RNN generates architecture description strings by sampling from a defined search space. The generated architectures are trained and evaluated on validation datasets, with their accuracy serving as rewards for the RL agent. The controller updates its policy using algorithms like REINFORCE or PPO to maximize expected reward. Key improvements include weight sharing through supergraph representations, hierarchical cell-based search spaces, and early stopping or surrogate models for performance prediction. The survey also discusses multi-objective NAS approaches that consider factors beyond accuracy, such as latency and resource usage.

## Key Results
- RL controllers can generate neural architectures by sampling from search spaces and receiving accuracy as rewards
- Weight sharing and early stopping significantly accelerate NAS by avoiding full training of each candidate
- Hierarchical cell-based search spaces reduce complexity while maintaining search effectiveness
- Multi-objective NAS frameworks can optimize for accuracy, latency, and resource constraints simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning controllers can generate neural architectures by sampling from a defined search space and receiving accuracy as a reward.
- Mechanism: The RL agent (e.g., RNN controller) takes sequential actions to construct architecture description strings. Each completed architecture is trained, evaluated, and assigned a reward (validation accuracy). The agent updates its policy via REINFORCE or PPO to maximize expected reward.
- Core assumption: The search space is sufficiently expressive to contain high-performing architectures, and the reward signal is smooth enough for policy gradients to converge.
- Evidence anchors:
  - [abstract] "The controller acts as an agent to take actions to maximize the rewards by decide which description string is generated in each recurrent network layer."
  - [section] "NAS uses a recurrent neural network as a controller to sample from a search space to generate new convolutional neural networks... The rewards are the expected accuracy of the generated convolutional neural networks evaluated in a specific dataset."
  - [corpus] No direct corpus evidence; assumes standard RL framework.
- Break condition: If the reward signal is too noisy or the search space is too sparse, policy updates become ineffective and exploration fails.

### Mechanism 2
- Claim: Weight sharing and early stopping accelerate NAS by avoiding full training of each candidate.
- Mechanism: Architectures are represented as subgraphs of a large supergraph. Shared weights allow sampling candidate performance without training from scratch. Early stopping or surrogate models predict accuracy before full training completes.
- Core assumption: Performance of a candidate correlates strongly with a subset of a larger trained model's weights.
- Evidence anchors:
  - [section] "Weight sharing is one of the popular directions of improvement... The Efficient Neural Architecture Search (ENAS) trained selecting a subset of edges within DAG representing a large model simultaneously with the controller."
  - [corpus] Weak evidence; mentions ENAS but lacks detailed mechanism.
- Break condition: If candidate architectures diverge too far from the supergraph, weight sharing yields poor performance estimates and misguides the search.

### Mechanism 3
- Claim: Hierarchical cell-based search spaces reduce complexity and improve generalization.
- Mechanism: Instead of searching entire network topologies, NAS searches reusable cell structures (e.g., convolutional cells). These cells are then stacked or connected by a fixed macro architecture.
- Core assumption: High-performing networks can be built from repeated, learned cell motifs.
- Evidence anchors:
  - [section] "Due to the fact that the deep neural networks started to include repeated sub-structures called cells or blocks... researchers have been considering searching based on cells, which means that NAS only search the structures of the cells."
  - [corpus] No corpus support; concept is internal to the paper.
- Break condition: If the macro architecture choice is suboptimal, even optimal cells may not yield the best full network.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: RL-based NAS models architecture generation as sequential decision-making under uncertainty, where actions are layer choices and states are partial architectures.
  - Quick check question: In NAS, what is the "state" at time step t of the controller RNN?

- Concept: Policy Gradient Methods (REINFORCE, PPO)
  - Why needed here: These methods update the controller's parameters based on sampled trajectories and rewards, essential for learning architecture generation policies.
  - Quick check question: Why is REINFORCE considered "gradient-free" even though it uses gradients for updates?

- Concept: Weight Sharing in Supergraph Representations
  - Why needed here: It enables efficient evaluation of many architectures without separate training, critical for scaling NAS.
  - Quick check question: In ENAS, what structure ensures all candidate architectures share weights?

## Architecture Onboarding

- Component map:
  Controller RNN/LSTM -> Search space -> Training pipeline -> Reward evaluator -> RL optimizer -> Weight sharing module

- Critical path:
  1. Initialize controller and search space
  2. Sample architecture string from controller
  3. Build and evaluate architecture (full train or surrogate)
  4. Compute reward (accuracy or proxy)
  5. Update controller policy via RL algorithm
  6. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Search space expressiveness vs. computational cost
  - Reward accuracy vs. evaluation speed (full training vs. proxy)
  - Weight sharing granularity vs. model fidelity
  - Policy optimization stability vs. sample efficiency

- Failure signatures:
  - Controller generates degenerate architectures (e.g., all identity ops)
  - Policy collapses to local optimum (no exploration)
  - Evaluation metrics inconsistent with final performance
  - Computational budget exhausted without convergence

- First 3 experiments:
  1. Implement a minimal NAS loop with a small search space (e.g., 2-layer CNN on MNIST) using REINFORCE; verify controller learns to favor higher-accuracy architectures.
  2. Add weight sharing: build a supergraph with all candidate ops, share weights, and evaluate accuracy without full training; compare convergence speed vs. full training baseline.
  3. Switch from full training to a proxy metric (e.g., accuracy after 1 epoch) and measure impact on final architecture quality and search efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can neural architecture search be extended to discover entirely novel building blocks rather than just optimizing existing human-designed components?
- Basis in paper: [explicit] The paper notes that current NAS search spaces consist of existing human-designed blocks, limiting the discovery of novel architectures. It suggests that reducing limitations on the search space could substantially improve automatically generated models.
- Why unresolved: Current NAS methods rely on predefined search spaces based on existing network components. Exploring truly novel architectural elements requires fundamentally different search strategies and representations.
- What evidence would resolve it: Demonstrations of NAS methods discovering previously unknown network components that outperform existing building blocks on standard benchmarks.

### Open Question 2
- Question: What are the most effective ways to balance multiple competing objectives (accuracy, latency, power consumption, etc.) in neural architecture search?
- Basis in paper: [explicit] The paper discusses multi-objective NAS and mentions that finding solutions optimal for all subtasks simultaneously is challenging, with researchers often searching for Pareto-optimal solutions.
- Why unresolved: Multi-objective optimization in NAS involves complex trade-offs that depend on specific deployment contexts and hardware constraints. Current methods like MNAS, RENA, and MONAS address different aspects but lack comprehensive frameworks.
- What evidence would resolve it: Development of unified multi-objective NAS frameworks that consistently produce architectures optimal for diverse deployment scenarios with quantifiable trade-off analyses.

### Open Question 3
- Question: How can neural architecture search be made more sample-efficient to reduce computational costs while maintaining or improving search quality?
- Basis in paper: [explicit] The paper identifies computational cost as the main challenge of NAS, discussing improvements like weight sharing, hierarchical representation, and performance prediction to accelerate training and reduce costs.
- Why unresolved: Despite various acceleration techniques, achieving both high sample efficiency and search quality remains difficult. Current methods involve trade-offs between search speed, model quality, and computational requirements.
- What evidence would resolve it: Development of NAS methods that achieve competitive or superior performance to existing approaches while reducing computational costs by an order of magnitude or more.

## Limitations
- Lack of specific hyperparameter details for controller RNNs and search space definitions
- Limited evidence supporting weight sharing and hierarchical search benefits
- Incomplete implementation details for multi-objective extensions and resource-constrained variants
- Assumption of standard RL framework without addressing policy instability or exploration challenges

## Confidence
- High confidence: Basic RL-based NAS mechanism (controller generates architectures, receives accuracy reward, updates policy)
- Medium confidence: Weight sharing and hierarchical search benefits, as evidence is cited but details are sparse
- Low confidence: Multi-objective extensions and resource-constrained NAS variants due to limited implementation details

## Next Checks
1. Implement the minimal NAS loop with REINFORCE on a small search space (2-layer CNN on MNIST) to verify the controller learns to prefer higher-accuracy architectures within 50-100 iterations.
2. Compare full training vs. weight-sharing evaluation by building a supergraph with all candidate ops and measuring accuracy correlation and search speed improvement.
3. Test policy gradient stability by implementing both REINFORCE and PPO on the same search space and measuring variance in convergence and final architecture quality.