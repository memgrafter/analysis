---
ver: rpa2
title: 'FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph
  Transformer'
arxiv_id: '2403.12821'
source_url: https://arxiv.org/abs/2403.12821
tags:
- neural
- former
- graph
- architecture
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FLOWER FORMER, a flow-aware graph transformer
  for encoding neural architectures. The key idea is to capture information flows
  within neural architectures through bidirectional asynchronous message passing and
  flow-aware global attention.
---

# FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer

## Quick Facts
- arXiv ID: 2403.12821
- Source URL: https://arxiv.org/abs/2403.12821
- Authors: Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin
- Reference count: 40
- Primary result: Outperforms state-of-the-art baselines by up to 4.38% in Kendall's Tau for computer vision architectures, and up to 4.41% for graph neural networks and auto speech recognition models

## Executive Summary
FLOWERFORMER introduces a novel approach to encoding neural architectures by capturing their information flow dynamics through bidirectional asynchronous message passing and flow-aware global attention. The method outperforms existing state-of-the-art baselines across multiple domains including computer vision, graph neural networks, and speech recognition. By simulating both forward inference and backpropagation flows, the model captures temporal dependencies inherent in neural network execution. Extensive ablation studies validate the effectiveness of each component, demonstrating the model's robustness and generalizability.

## Method Summary
FLOWERFORMER is a flow-aware graph transformer that encodes neural architectures as directed acyclic graphs (DAGs) through two key components: a flow encode module that performs bidirectional asynchronous message passing in topological order, and a flow-aware global attention module that computes attention scores only between nodes connected by information flow paths. The model processes neural architectures represented by adjacency matrices and node features, capturing both local operation-level flows and global architecture-level characteristics. The outputs are combined through element-wise summation, passed through a feed-forward network, and aggregated via mean pooling to produce architecture embeddings for performance prediction.

## Key Results
- Achieves up to 4.38% improvement in Kendall's Tau for computer vision architectures compared to state-of-the-art baselines
- Demonstrates up to 4.41% better performance on graph neural networks and auto speech recognition models
- Shows consistent improvements across different training set sizes, particularly on small datasets (1%, 5%, 10% training ratios)
- Ablation studies confirm the effectiveness of both flow encode and flow-aware global attention modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Capturing both forward and backward information flows through asynchronous message passing improves neural architecture representation learning.
- Mechanism: The model simulates the actual data flow during forward inference and gradient flow during backpropagation by processing nodes in topological order and reversed topological order respectively. This captures the temporal dependencies inherent in neural network execution.
- Core assumption: The order in which operations execute in a neural network is critical to its performance and can be encoded through asynchronous message passing that respects topological dependencies.
- Evidence anchors:
  - [abstract] "bidirectional asynchronous message passing, inspired by the flows"
  - [section] "The flow encode module conducts both asynchronous forward and backward message passing, resembling the forward pass (i.e., inference) and backpropagation (i.e., training) of neural architectures, respectively."
- Break condition: If the architecture contains cycles or the topological sorting fails, the asynchronous message passing would break down since it relies on DAG properties.

### Mechanism 2
- Claim: Flow-aware global attention enhances representation learning by focusing attention only on nodes connected through information flows.
- Mechanism: The attention mechanism uses a masking scheme where attention scores are computed only between nodes that lie on directed paths within the flow graph, preventing irrelevant node pairs from influencing each other.
- Core assumption: Information flow paths in neural architectures are the relevant connections for capturing architecture characteristics, not all possible node pairs.
- Evidence anchors:
  - [abstract] "global attention built on flow-based masking"
  - [section] "we restrict attention scores to be computed only between nodes connected by at least one path of the flows"
- Break condition: If the masking scheme is too restrictive and eliminates all connections between certain node pairs, the global attention would fail to capture important cross-component relationships.

### Mechanism 3
- Claim: Combining local flow capture with global attention provides superior architecture representations compared to methods using only one approach.
- Mechanism: The flow encode module captures local-level information flow between directly connected operations, while the flow-aware global attention module captures graph-level (architecture-level) characteristics, providing complementary information.
- Core assumption: Both local operation-level flows and global architecture-level patterns are necessary for accurate performance prediction.
- Evidence anchors:
  - [abstract] "two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking"
  - [section] "The flow-aware global attention module is designed to capture graph-level (i.e., architecture-level) characteristics, complementing the flow encode module which primarily focuses on local-level flows"
- Break condition: If one module consistently dominates the other in the combined representation, the complementary benefit would be lost.

## Foundational Learning

- Concept: Graph neural networks and their message passing mechanisms
  - Why needed here: Understanding how GNNs propagate information through graphs is essential for grasping the flow encode module's asynchronous message passing
  - Quick check question: How does synchronous message passing differ from the asynchronous approach used in FLOWERFORMER, and why might asynchronous be more appropriate for neural architectures?

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: The flow-aware global attention module is built on transformer principles, requiring understanding of multi-head attention and masking schemes
  - Quick check question: What role does the mask matrix play in the flow-aware global attention, and how does it differ from standard causal masking in language transformers?

- Concept: Topological sorting and directed acyclic graphs (DAGs)
  - Why needed here: The flow encode module relies on topological generations to process nodes in execution order, which requires understanding DAG properties
  - Quick check question: How would the flow encode module behave if the input neural architecture contained cycles instead of being a DAG?

## Architecture Onboarding

- Component map:
  Input (DAG representation) → Topological sorting → Flow encode module (bidirectional asynchronous message passing) → Flow-aware global attention module (masked attention) → Combination (element-wise sum) → Feed forward network (2-layer MLP) → Output aggregation (mean pooling) → Regressor (performance prediction)

- Critical path: Input → Topological sorting → Flow encode module → Flow-aware global attention → Combination → Feed forward → Output aggregation → Regressor

- Design tradeoffs:
  - Asynchronous vs synchronous message passing: Asynchronous captures execution order but increases computational complexity
  - Global attention with masking vs full attention: Masking reduces computation and focuses on relevant connections but may miss some global patterns
  - Mean pooling vs other readout functions: Mean pooling is simple and permutation-invariant but may lose node-specific information

- Failure signatures:
  - Poor performance with small training sets: Model may overfit without sufficient flow patterns to learn
  - Degraded performance on architectures with complex control flow: The DAG assumption may not hold for architectures with loops or conditional execution
  - High computational cost: Asynchronous processing and masked attention may be slower than simpler alternatives

- First 3 experiments:
  1. Ablation study: Remove the flow-aware global attention module and measure performance drop to validate its contribution
  2. Input variation: Test the model on architectures with different numbers of nodes to understand scalability limits
  3. Training ratio analysis: Evaluate performance across different training set sizes (1%, 5%, 10%, 50%) to understand data efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the flow-aware global attention module be enhanced to capture interactions between multiple cells in architectures?
- Basis in paper: [inferred] The authors mention that FLOWERFORMER's performance on the ENAS dataset (consisting of two cells) is sub-optimal because it lacks a dedicated global attention module to capture interactions between the two cells.
- Why unresolved: The paper does not explore or propose methods to enhance the global attention module to handle interactions between multiple cells.
- What evidence would resolve it: Implementing and testing a cross-attention mechanism or other methods to capture interactions between multiple cells, and evaluating the performance improvement on multi-cell datasets.

### Open Question 2
- Question: How does the performance of FLOWERFORMER compare to other methods when applied to different types of neural architectures beyond those tested (e.g., recurrent neural networks, transformers)?
- Basis in paper: [inferred] The paper demonstrates FLOWERFORMER's effectiveness on various domains, including computer vision, graph neural networks, and auto speech recognition models, but does not explore other types of architectures like recurrent neural networks or transformers.
- Why unresolved: The authors have not conducted experiments on these additional types of architectures.
- What evidence would resolve it: Applying FLOWERFORMER to different types of neural architectures and comparing its performance with other state-of-the-art methods on these architectures.

### Open Question 3
- Question: What are the limitations of the asynchronous message passing approach in FLOWERFORMER, and how can they be addressed?
- Basis in paper: [explicit] The authors mention that asynchronous message passing introduces some delay, as each operation should be performed sequentially. They also discuss the use of group-based batch processing to accelerate computation.
- Why unresolved: The paper does not explore the potential limitations or drawbacks of the asynchronous message passing approach beyond the mentioned delay.
- What evidence would resolve it: Conducting experiments to identify specific limitations of the asynchronous message passing approach, such as its impact on scalability or performance on larger graphs, and proposing and testing methods to address these limitations.

## Limitations
- Reliance on DAG representations may limit applicability to architectures with control flow structures, loops, or conditional execution paths
- Computational overhead of asynchronous message passing and masked attention could impact scalability for very large architectures
- Effectiveness of the flow-aware masking scheme in the global attention module requires further validation across diverse architecture families

## Confidence
- **High confidence**: The core mechanism of bidirectional asynchronous message passing for capturing information flows is well-grounded in the architecture execution semantics
- **Medium confidence**: The flow-aware global attention masking approach is theoretically sound, but its practical benefits across diverse architecture families warrant additional validation
- **Medium confidence**: The performance improvements over baselines are demonstrated, but the contribution of individual components (flow encode vs global attention) could be more precisely quantified

## Next Checks
1. **Cycle handling validation**: Test FLOWERFORMER on architectures containing loops or cycles to evaluate how the topological sorting assumption breaks down and identify potential modifications for handling such cases
2. **Component ablation with scaling**: Perform systematic ablation studies across architectures of varying sizes (from small to large) to understand how component contributions change with architecture complexity
3. **Cross-domain generalization**: Evaluate the model's performance when trained on one domain (e.g., vision) and tested on another (e.g., speech), measuring zero-shot transfer capabilities and identifying domain-specific limitations