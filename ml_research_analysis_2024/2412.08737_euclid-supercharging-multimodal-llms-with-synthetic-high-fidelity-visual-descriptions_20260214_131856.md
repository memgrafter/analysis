---
ver: rpa2
title: 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual
  Descriptions'
arxiv_id: '2412.08737'
source_url: https://arxiv.org/abs/2412.08737
tags:
- training
- visual
- line
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Geoperception, a benchmark to evaluate multimodal
  large language models' (MLLMs) low-level visual perception (LLVP) capabilities,
  specifically their ability to accurately describe geometric details in images. Existing
  MLLMs struggle with these tasks, which are crucial for applications like robotics,
  medical image analysis, and manufacturing.
---

# Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions

## Quick Facts
- arXiv ID: 2412.08737
- Source URL: https://arxiv.org/abs/2412.08737
- Reference count: 40
- Primary result: Euclid MLLMs outperform Gemini-1.5-Pro by up to 58.56% on certain Geoperception tasks and 10.65% on average

## Executive Summary
The paper addresses a critical gap in multimodal large language models (MLLMs): their poor performance on low-level visual perception tasks, particularly geometric description and analysis. Existing MLLMs excel at high-level visual understanding but struggle with precise geometric details crucial for applications in robotics, medical imaging, and manufacturing. To tackle this challenge, the authors introduce Geoperception, a benchmark specifically designed to evaluate geometric perception capabilities, and develop a synthetic data engine that generates high-fidelity geometric visual descriptions for training.

Based on extensive empirical studies of various MLLM design choices, the authors identify optimal configurations: convolutional neural network visual encoders, freezing the visual encoder during training, and curriculum-based multi-stage training approaches. These insights led to the creation of Euclid, a family of MLLMs specifically optimized for geometric low-level visual perception. Remarkably, Euclid achieves superior performance despite being trained exclusively on synthetic data, demonstrating strong generalization to real-world geometric perception tasks and outperforming leading closed-source models like Gemini-1.5-Pro.

## Method Summary
The authors developed a synthetic data engine to generate high-fidelity geometric visual descriptions, addressing the scarcity of labeled data for geometric perception tasks. They conducted systematic empirical studies across multiple MLLM design dimensions including visual encoder architectures (CNN vs transformer-based), training strategies (frozen vs fine-tuned visual encoders), and curriculum learning approaches. Based on these findings, they created Euclid by implementing the identified optimal design choices. The model was trained purely on synthetic data generated by their engine and evaluated on the newly introduced Geoperception benchmark, which includes diverse geometric perception tasks.

## Key Results
- Euclid outperforms Gemini-1.5-Pro by up to 58.56% on certain Geoperception benchmark tasks
- Euclid achieves 10.65% better average performance across all Geoperception tasks compared to the best closed-source model
- Models trained solely on synthetic data demonstrate strong generalization to real-world geometric perception scenarios
- Convolutional visual encoders, frozen visual encoders, and curriculum-based multi-stage training significantly improve geometric perception performance

## Why This Works (Mechanism)
The success of Euclid stems from addressing the fundamental mismatch between existing MLLM training objectives and the requirements of geometric perception tasks. Traditional MLLMs are optimized for high-level visual understanding and natural language generation, which doesn't prioritize precise geometric detail extraction. By creating synthetic data specifically engineered for geometric descriptions and implementing design choices that preserve spatial information (CNN encoders) while preventing catastrophic forgetting during training (frozen encoders), Euclid develops specialized capabilities for geometric perception that general-purpose MLLMs lack.

## Foundational Learning

**Geoperception Benchmark** - A standardized evaluation framework for assessing low-level visual perception capabilities in MLLMs, focusing on geometric detail extraction and description. Needed because existing benchmarks primarily evaluate high-level visual understanding, leaving geometric perception capabilities largely unmeasured.

**Synthetic Data Generation** - The process of creating artificial training data with controlled geometric properties and corresponding high-fidelity descriptions. Required due to the scarcity of real-world labeled data for precise geometric perception tasks and the ability to generate diverse, controlled training scenarios.

**Curriculum-Based Multi-Stage Training** - A training strategy that progresses from simpler to more complex geometric tasks in staged phases. Essential for gradually building geometric perception capabilities without overwhelming the model, allowing it to first master basic geometric concepts before tackling complex spatial relationships.

## Architecture Onboarding

**Component Map**: Synthetic Data Engine -> CNN Visual Encoder -> Frozen Visual Backbone -> Multi-Stage Curriculum Training -> Euclid MLLM

**Critical Path**: The most performance-critical components are the CNN visual encoder and the frozen visual backbone. The CNN architecture preserves spatial hierarchies crucial for geometric detail extraction, while freezing prevents degradation of these spatial representations during language model fine-tuning.

**Design Tradeoffs**: The choice between CNN and transformer visual encoders represents a fundamental tradeoff between spatial detail preservation and contextual reasoning capabilities. CNN encoders excel at capturing local geometric patterns but may miss broader contextual relationships that transformer-based approaches handle better. The frozen visual encoder strategy trades adaptation flexibility for maintaining geometric perception fidelity.

**Failure Signatures**: Poor performance on Geoperception tasks indicates inadequate geometric detail extraction, often manifesting as vague or incorrect spatial descriptions. Common failure modes include confusion between similar geometric shapes, incorrect measurement estimations, and loss of fine-grained spatial relationships in complex scenes.

**First Experiments**:
1. Evaluate Euclid on real-world geometric perception datasets outside the Geoperception benchmark to test generalization
2. Conduct ablation studies removing the frozen visual encoder constraint to quantify its contribution to performance
3. Compare inference speed and memory usage against baseline models to assess practical deployment trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on the Geoperception benchmark, which may not fully represent real-world deployment scenarios
- Synthetic training data may create distribution mismatches when applied to diverse real-world geometric contexts
- Study focuses on specific design choices without systematically exploring all potential architectural alternatives
- Computational efficiency and inference speed trade-offs are not addressed, potentially impacting practical deployment

## Confidence
- High confidence: Existing MLLMs struggle with low-level visual perception tasks, supported by clear empirical evidence on Geoperception benchmark
- Medium confidence: Architectural recommendations (CNN encoders, frozen visual encoders, curriculum training) as optimal design choices, since correlations are established but causation isn't definitively proven
- Medium confidence: Claim that Euclid outperforms Gemini-1.5-Pro by reported margins, as comparison is limited to specific benchmark tasks and may not generalize to all scenarios

## Next Checks
1. Test Euclid on real-world geometric perception datasets outside the Geoperception benchmark to assess generalization beyond synthetic data distributions
2. Conduct ablation studies varying architectural components and training strategies to verify the robustness of claimed optimal design choices
3. Evaluate computational efficiency metrics (inference time, memory usage) across different input sizes to understand practical deployment trade-offs