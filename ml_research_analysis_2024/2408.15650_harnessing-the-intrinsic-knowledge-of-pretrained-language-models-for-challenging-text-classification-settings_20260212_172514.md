---
ver: rpa2
title: Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging
  Text Classification Settings
arxiv_id: '2408.15650'
source_url: https://arxiv.org/abs/2408.15650
tags:
- label
- language
- training
- classification
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores three challenging settings in text classification
  by leveraging the intrinsic knowledge of pretrained language models (PLMs). For
  distractor selection in multiple-choice cloze questions, models using contextualized
  word representations from PLMs significantly improve performance, approaching or
  exceeding human-level accuracy.
---

# Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings

## Quick Facts
- arXiv ID: 2408.15650
- Source URL: https://arxiv.org/abs/2408.15650
- Authors: Lingyu Gao
- Reference count: 0
- Key outcome: PLM-based methods improve distractor selection, zero-shot classification robustness, and in-context learning performance across diverse text classification tasks.

## Executive Summary
This thesis explores three challenging settings in text classification by leveraging the intrinsic knowledge of pretrained language models (PLMs). For distractor selection in multiple-choice cloze questions, models using contextualized word representations from PLMs significantly improve performance, approaching or exceeding human-level accuracy. To enhance zero-shot text classification robustness, small finetuning datasets describing task labels are created, yielding 17-19% accuracy improvements and greater robustness to prompt variations. For in-context learning, demonstrations are selected based on model predictions and ambiguity resolution, improving F1 macro scores by up to 2.6% over traditional retrieval-based approaches.

## Method Summary
The thesis presents three distinct approaches for challenging text classification settings. First, feature engineering using contextualized word representations from PLMs improves distractor selection for multiple-choice cloze questions by capturing nonlinear relationships between distractor fit and annotator selection. Second, zero-shot text classification robustness is enhanced through small finetuning datasets containing domain-independent descriptions of labels (terms, definitions, templates). Third, in-context learning performance is improved by selecting demonstrations based on model predictions and ambiguity resolution, identifying examples that are misclassified and fall near the test example's decision boundary.

## Key Results
- Distractor selection models using PLM-based features achieve F1 scores of 0.882-0.884, rivaling or exceeding human performance (0.882-0.888)
- Zero-shot classification accuracy improves by 17-19% on average when using label-description training compared to standard zero-shot approaches
- In-context learning F1 macro scores improve by up to 2.6% when using ambiguity-aware demonstration selection versus traditional retrieval-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized word representations from PLMs improve distractor selection performance by capturing nonlinear relationships between distractor fit and annotator selection.
- Mechanism: PLMs compute conditional probabilities of distractor candidates in the given context, which reflect how well they fit grammatically and semantically. Annotators tend to reject distractors that are either too easy (very low probability) or too hard (very high probability), preferring those with moderate probabilities.
- Core assumption: The pretrained knowledge in PLMs includes patterns of distractor quality that correlate with human judgment.
- Evidence anchors:
  - [abstract] "using contextualized word representations from PLMs as features drastically improves performance over traditional feature-based models, even rivaling human performance"
  - [section 3.4.4] "The distractor probabilities are more variable and the shapes of the histograms are roughly similar for the true and false labels. Interestingly, however, when the probability is very high or very low, the distractors tend to not be selected. The selected distractors tend to be located at the middle of the probability range."
- Break condition: If the distractor probability distribution doesn't capture the nonlinear relationship between fit and suitability, or if the PLM's pretraining data doesn't contain relevant patterns for distractor quality.

### Mechanism 2
- Claim: Label-description training improves zero-shot text classification by providing richer characterization of labels than verbalizers alone.
- Mechanism: Small finetuning datasets containing domain-independent descriptions of labels (terms, definitions, templates) help the model understand the semantic space of labels better than relying solely on verbalizers in patterns.
- Core assumption: The model's understanding of labels is enhanced when trained on diverse descriptions that capture different aspects of the label meaning.
- Evidence anchors:
  - [abstract] "craft small datasets that describe task labels with related terms, short templates, dictionary definitions, and more. This approach achieves an average improvement of 17-19% in accuracy"
  - [section 4.3.1] "For topic classification, we use both subjective descriptors of the labels (e.g., related terms) and objective sources of information (e.g., dictionary definition and Wikipedia sentences)"
- Break condition: If the label descriptions don't capture the essential meaning of the labels, or if the model already has sufficient understanding from pretraining alone.

### Mechanism 3
- Claim: In-context learning performance improves by selecting demonstrations based on model predictions and ambiguity resolution.
- Mechanism: Demonstrations are selected that are both misclassified by the model and fall near the test example's decision boundary (as indicated by the ambiguous label set), helping the model resolve its confusion.
- Core assumption: Showing the model examples it previously misclassified, particularly those near the decision boundary, will force it to correct its understanding.
- Evidence anchors:
  - [abstract] "using demonstrations that are misclassified by the models, particularly those that lie near the decision boundary of the test examples, leads to better performance"
  - [section 5.3] "we construct a prompt for the test example and use the model log-likelihood to score each output label, identifying top-2 labels that have the highest scores, which we refer to as the 'ambiguous label set'"
- Break condition: If the model's predictions on demonstrations don't reflect actual confusion, or if the ambiguous label set doesn't correspond to likely gold labels.

## Foundational Learning

- Concept: Pretrained Language Models (PLMs) and their architectures (BERT, ELMo, GPT)
  - Why needed here: Understanding PLM architectures is crucial because the thesis leverages different types of PLMs (encoder-only, decoder-only) for different tasks.
  - Quick check question: What's the key difference between BERT and GPT in terms of how they process context?

- Concept: Contextualized word representations vs static embeddings
  - Why needed here: The thesis demonstrates that contextualized representations from PLMs outperform static embeddings for feature engineering in distractor selection.
  - Quick check question: Why would a polysemous word like "bank" benefit more from contextualized representations than static embeddings?

- Concept: Zero-shot and few-shot learning paradigms
  - Why needed here: The thesis explores zero-shot text classification using label descriptions and in-context learning, requiring understanding of these learning paradigms.
  - Quick check question: What's the main challenge in zero-shot classification that label-description training aims to address?

## Architecture Onboarding

- Component map:
  - Distractor selection: Feature extraction (context-free and context-sensitive) → Classification model (MLPs, ELMo-based, BERT-based)
  - Zero-shot classification: Label description dataset → Pattern-verbalizer approach → Finetuning or ICL
  - In-context learning: Retrieval system → Ambiguity analysis → Demonstration selection → ICL

- Critical path:
  - Distractor selection: Feature engineering with PLM probabilities → Model training → Threshold tuning → Evaluation
  - Zero-shot classification: Label description collection → Pattern/verbalizer design → Model finetuning → Evaluation across datasets
  - In-context learning: Retrieval → Ambiguity identification → Demonstration constraint application → ICL performance evaluation

- Design tradeoffs:
  - Distractor selection: Simple features vs complex PLM-based features (accuracy vs computational cost)
  - Zero-shot classification: Comprehensive label descriptions vs minimal descriptions (robustness vs effort)
  - In-context learning: Semantic similarity vs model prediction-based selection (precision vs recall of relevant demonstrations)

- Failure signatures:
  - Distractor selection: Low precision indicates features aren't capturing distractor quality; high recall but low precision suggests too lenient threshold
  - Zero-shot classification: High variance across patterns indicates sensitivity to prompt design; low accuracy suggests inadequate label understanding
  - In-context learning: Performance similar to random selection indicates demonstration selection isn't effective; worse than zero-shot suggests demonstration choice is harmful

- First 3 experiments:
  1. Distractor selection: Compare Mfeat (feature-based) vs MELMo (ELMo-based) vs MBERT (BERT-based) on MCDS ENT dataset
  2. Zero-shot classification: Compare zero-shot vs LABEL DESC TRAINING on AGNews dataset with RoBERTa-base
  3. In-context learning: Compare RETR (retrieval-based) vs AMBIG-ICL (ambiguity-aware) on SST dataset with Flan-PaLM 2 (M)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design distractors to better assist student learning beyond just selecting grammatically plausible and semantically implausible options?
- Basis in paper: [explicit] Chapter 3 discusses the need for distractors that are misleading yet incorrect, and mentions that optimal distractors should be sufficiently similar to correct answers to challenge students but not so similar as to make the question unanswerable.
- Why unresolved: The paper focuses on automatic selection of distractors but doesn't explore how different types of distractors might affect learning outcomes or student performance.
- What evidence would resolve it: A study comparing student performance on questions with different types of distractors (e.g., semantic vs. syntactic distractors) and analyzing the impact on learning outcomes.

### Open Question 2
- Question: Can question answering PLMs be used to simulate student responses and accurately capture student behavior in distractor selection tasks?
- Basis in paper: [inferred] Chapter 3 mentions the potential of using question answering PLMs and exploring whether they can simulate student answers, but doesn't investigate this further.
- Why unresolved: The paper doesn't explore the use of question answering PLMs for this purpose, and it's unclear how well they would perform compared to other methods.
- What evidence would resolve it: An experiment comparing the performance of question answering PLMs to other methods (e.g., human annotators) in simulating student responses and selecting distractors.

### Open Question 3
- Question: How can we improve the consistency of models regarding patterns and verbalizers in prompt-based zero-shot classification?
- Basis in paper: [explicit] Chapter 4 discusses the sensitivity of models to prompt design and the need for more robust methods. It also mentions the potential of debiasing through calibration.
- Why unresolved: The paper proposes a method (LABEL DESC TRAINING) that improves robustness but doesn't explore other approaches to address the sensitivity to patterns and verbalizers.
- What evidence would resolve it: A study comparing the performance of different methods (e.g., LABEL DESC TRAINING, calibration techniques) in improving model consistency across different patterns and verbalizers.

### Open Question 4
- Question: How can we leverage the mechanisms underlying retrieval-based ICL to improve demonstration selection and model performance?
- Basis in paper: [explicit] Chapter 5 discusses the use of retrieval-based ICL and the importance of considering model predictions and output label space in demonstration selection. It also mentions the need for further study of the interaction of multiple demonstrations within a single prompt.
- Why unresolved: The paper proposes a method (AMBIG-ICL) that improves performance but doesn't fully explore the underlying mechanisms of retrieval-based ICL or how to optimize demonstration selection.
- What evidence would resolve it: A study investigating the factors that contribute to the effectiveness of retrieval-based ICL (e.g., semantic similarity, label space, demonstration ordering) and developing methods to optimize demonstration selection based on these factors.

## Limitations

- The evaluation of distractor selection methods relies on proprietary datasets with undisclosed creation procedures, limiting reproducibility and independent verification.
- Zero-shot classification improvements depend heavily on the quality of label-description datasets, with unclear tradeoffs between effort and performance gains.
- In-context learning demonstration selection improvements are modest (up to 2.6% F1 macro), suggesting benefits may be task-dependent.

## Confidence

High confidence: The general finding that contextualized word representations from PLMs improve feature engineering for distractor selection.

Medium confidence: The specific claims about human-level performance in distractor selection and the exact improvement percentages in zero-shot classification.

Medium confidence: The effectiveness of ambiguity-aware demonstration selection for in-context learning.

## Next Checks

1. **Reproduce distractor selection results on open datasets**: Apply the feature engineering approach to an open-source cloze-style dataset (such as LAMBADA or ROCStories) to verify that contextualized representations consistently improve distractor selection performance outside of proprietary datasets.

2. **Test label-description training on out-of-domain datasets**: Evaluate the zero-shot classification approach on datasets from domains not represented in the original evaluation (e.g., biomedical or legal text) to assess the robustness and generalizability of the label-description approach across diverse domains.

3. **Compare demonstration selection strategies systematically**: Conduct a controlled experiment comparing ambiguity-aware demonstration selection against alternative strategies (e.g., diversity-based selection, difficulty-based selection) across multiple in-context learning tasks to isolate the specific benefits of the ambiguity resolution approach.