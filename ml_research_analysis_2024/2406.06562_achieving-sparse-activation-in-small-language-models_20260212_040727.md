---
ver: rpa2
title: Achieving Sparse Activation in Small Language Models
arxiv_id: '2406.06562'
source_url: https://arxiv.org/abs/2406.06562
tags:
- neurons
- attribution
- activation
- neuron
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying sparse activation
  to Small Language Models (SLMs), where existing approaches based on neuron output
  magnitudes fail due to SLMs being less over-parameterized than Large Language Models
  (LLMs). The authors propose using gradient-based attribution scores to measure neuron
  importance and introduce a corrective term to mitigate attribution errors caused
  by inter-layer dependency among neurons.
---

# Achieving Sparse Activation in Small Language Models

## Quick Facts
- arXiv ID: 2406.06562
- Source URL: https://arxiv.org/abs/2406.06562
- Reference count: 38
- Achieves 80% neuron deactivation with <5% accuracy loss in SLMs

## Executive Summary
This paper addresses the challenge of applying sparse activation to Small Language Models (SLMs), where existing approaches based on neuron output magnitudes fail due to SLMs being less over-parameterized than Large Language Models (LLMs). The authors propose using gradient-based attribution scores with a corrective term to measure neuron importance while accounting for inter-layer dependencies. Experiments across multiple popular SLMs demonstrate that the proposed method achieves significant neuron deactivation while maintaining model accuracy, outperforming baseline attribution methods by at least 25% in accuracy.

## Method Summary
The method uses gradient × output (GxO) attribution scores with a corrective term to account for inter-layer dependencies that cause attribution errors. The corrective term is calculated as |xi| · √(Σ(∂F/∂xk)²) for each neuron, where xi is the neuron's output and the sum is over gradients from other neurons in the same layer. Layer-specific thresholds are then applied to determine which neurons to activate, allowing for precise control over the activation ratio while maintaining model accuracy.

## Key Results
- Achieves up to 80% neuron deactivation with less than 5% accuracy loss
- Outperforms baseline attribution methods by at least 25% in model accuracy
- Computationally efficient with negligible overhead compared to baseline GxO metric
- Demonstrates effectiveness across multiple SLMs (Phi-1.5, Phi-2, MobiLlama-0.5B, MobiLlama-1B) and datasets (TruthfulQA, YahooAnswersQA)

## Why This Works (Mechanism)

### Mechanism 1
Inter-layer dependency causes attribution errors that lead to incorrect neuron rankings during sparse activation. When neurons in layer L1 are deactivated, the outputs and gradients in subsequent layer L2 change, altering the attribution scores of neurons in L2. This interdependency creates a mismatch between the true importance of neurons and their computed attribution scores.

### Mechanism 2
The corrective term based on neuron gradients and outputs effectively mitigates attribution errors. The proposed corrective term |xi| · √(Σ(∂F/∂xk)²) quantifies the upper bound of attribution error caused by deactivating neuron xi. By adding this term to the GxO metric, the method accounts for the inter-layer dependency effects and produces more accurate neuron rankings.

### Mechanism 3
Layer-specific thresholding outperforms uniform thresholding for sparse activation in SLMs. Since the corrective term varies significantly across layers due to different neuron magnitudes and gradients, applying the same activation ratio to all layers leads to suboptimal neuron selection. Layer-specific thresholding ensures each layer maintains its optimal activation ratio based on its unique attribution score distribution.

## Foundational Learning

- **Gradient-based attribution methods (e.g., Integrated Gradients, Gradient × Output)**
  - Why needed here: These methods quantify neuron importance by measuring how much a neuron's activation contributes to the model output, essential for deciding which neurons to deactivate during sparse activation.
  - Quick check question: How does the Gradient × Output metric approximate the impact of neuron deactivation on model output?

- **Transformer architecture and layer dependencies**
  - Why needed here: Understanding how neuron outputs in one layer affect subsequent layers is crucial for analyzing the inter-layer dependency that causes attribution errors in sparse activation.
  - Quick check question: Why do neuron outputs in layer L1 affect the attribution scores of neurons in layer L2?

- **Sparse activation and neuron deactivation**
  - Why needed here: The core goal is to selectively deactivate neurons during inference to reduce computational cost while maintaining model accuracy, which requires understanding how neuron importance is measured and applied.
  - Quick check question: What is the difference between activating neurons based on output magnitude versus attribution scores?

## Architecture Onboarding

- **Component map**: Input layer → Transformer blocks (Attention + MLP) → Output layer → Attribution calculation module → Corrective term computation → Layer-specific thresholding → Sparse activation engine → Neuron deactivation mask → Inference pipeline

- **Critical path**:
  1. Forward pass to compute neuron outputs and gradients
  2. Attribution score calculation (GxO metric)
  3. Corrective term computation for each neuron
  4. Layer-specific thresholding to determine activation mask
  5. Sparse inference with deactivated neurons

- **Design tradeoffs**:
  - Computational overhead vs. accuracy: The corrective term adds negligible overhead but significantly improves accuracy compared to baseline methods
  - Layer-specific vs. uniform thresholding: Layer-specific thresholding provides better accuracy but requires more complex implementation
  - Attribution metric choice: GxO with corrective term offers good balance between accuracy and efficiency compared to more expensive methods like Integrated Gradients

- **Failure signatures**:
  - Large accuracy drop when activation ratio increases: Indicates incorrect neuron ranking due to attribution errors
  - Inconsistent performance across different SLMs: Suggests the corrective term may not generalize well to all model architectures
  - High computational overhead: May indicate inefficient implementation of the corrective term calculation

- **First 3 experiments**:
  1. Verify inter-layer dependency by measuring attribution score changes when deactivating neurons in one layer
  2. Test layer-specific vs. uniform thresholding on a small SLM to confirm performance differences
  3. Measure the distribution of attribution errors to validate the assumption of truncated normal distribution

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed corrective term scale with different SLM architectures beyond the Phi and MobiLlama series? The paper evaluates the method on Phi and MobiLlama models but doesn't explore other SLM architectures like Gemma or Mistral. Different SLM architectures have varying layer structures and parameter sharing schemes that could affect the inter-layer dependency.

### Open Question 2
What is the impact of varying activation ratios across different layers instead of applying a uniform ratio? The paper mentions two approaches but only evaluates the uniform threshold approach in experiments. Layer-specific activation ratios could potentially optimize performance further by accounting for layer-specific characteristics.

### Open Question 3
How does the method perform on tasks beyond question answering, such as code generation or sentiment analysis? The paper focuses on QA tasks but mentions SLMs are used for various applications including code generation. Different tasks may have different sparsity characteristics and attribution error patterns.

## Limitations

- The inter-layer dependency mechanism relies on assumptions about neuron non-linearity that may not hold across all SLM architectures
- The corrective term's effectiveness depends on the assumption that attribution errors follow a truncated normal distribution
- Layer-specific thresholding shows promise but lacks theoretical justification beyond empirical results

## Confidence

- **High confidence**: The experimental results showing 80% sparsification with <5% accuracy loss are well-supported by the reported BLEU scores across multiple SLMs and datasets
- **Medium confidence**: The mechanism explaining inter-layer dependency and attribution errors is logically sound but relies on theoretical assumptions that would benefit from additional empirical validation
- **Medium confidence**: The claim that the corrective term adds negligible computational overhead is supported by the "one-shot vectorized computation" approach, though specific timing measurements are not provided

## Next Checks

1. **Cross-architecture validation**: Test the proposed method on additional SLM architectures beyond Phi and MobiLlama models to verify generalizability of the inter-layer dependency assumptions and corrective term effectiveness.

2. **Ablation study of corrective term**: Systematically remove the corrective term while keeping all other components constant to quantify its exact contribution to the 25%+ accuracy improvement over baseline methods.

3. **Computational overhead measurement**: Measure actual inference latency and memory usage with varying activation ratios to verify the "negligible overhead" claim across different hardware configurations.