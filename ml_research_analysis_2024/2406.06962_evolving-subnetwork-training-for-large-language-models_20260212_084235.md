---
ver: rpa2
title: Evolving Subnetwork Training for Large Language Models
arxiv_id: '2406.06962'
source_url: https://arxiv.org/abs/2406.06962
tags:
- training
- language
- stage
- sampling
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel training paradigm, Evolving Subnetwork
  Training (EST), to improve the efficiency of training large language models (LLMs).
  EST samples subnetworks from the layers of the LLM and from the Multi-Head Attention
  (MHA) and Multi-Layer Perceptron (MLP) modules within each layer.
---

# Evolving Subnetwork Training for Large Language Models

## Quick Facts
- arXiv ID: 2406.06962
- Source URL: https://arxiv.org/abs/2406.06962
- Authors: Hanqi Li; Lu Chen; Da Ma; Zijian Wu; Su Zhu; Kai Yu
- Reference count: 23
- One-line primary result: EST saves 26.7% FLOPs for GPT2 and 25.0% FLOPs for TinyLlama while maintaining or improving model performance

## Executive Summary
This paper introduces Evolving Subnetwork Training (EST), a novel training paradigm for large language models that samples and progressively increases subnetworks from Transformer layers and modules. By randomly selecting attention heads, MLP columns, and layers during training, EST reduces computational cost while maintaining model quality. The method achieves significant FLOPs savings without increasing pre-training loss and demonstrates improved downstream task performance, suggesting benefits for generalization.

## Method Summary
EST works by sampling subnetworks from the layers of the LLM and from the Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP) modules within each layer. The training process occurs in stages, with subnetwork size gradually increasing over time. The approach uses random sampling indices to select which attention heads and MLP columns to use in each forward pass, with a router component directing computation through the selected subnetwork. This progressive expansion allows the model to save computational resources while maintaining or improving performance compared to full-model training.

## Key Results
- 26.7% FLOPs saving on GPT2 model without increased pre-training loss
- 25.0% FLOPs saving on TinyLlama model while maintaining performance
- EST leads to performance improvements in downstream tasks, indicating better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EST breaks the loss-preserving property to accelerate training
- Mechanism: Random subnetwork sampling acts as Structural Dropout, introducing a regularization term. As subnetwork size increases during training, this regularization term decreases, causing a sudden drop in loss at each stage transition.
- Core assumption: The equivalence between random subnetwork sampling and Structural Dropout holds throughout training
- Evidence anchors:
  - [abstract] "inspired by the redundancy in the parameters of large language models"
  - [section 5.1] "Due to the equivalence between the random sampling of subnetworks and Structural Dropout (Pal et al., 2020)"
  - [corpus] Weak - no direct evidence in neighbor papers
- Break condition: If the sampling scheduler is poorly designed such that stage transitions are too frequent or too sparse, the regularization effect may not provide meaningful loss reduction

### Mechanism 2
- Claim: EST breaks the training-dynamics-preserving property to improve final stage training
- Mechanism: Early dropout (small subnetworks) pushes parameters into flatter loss landscape regions. This flatness persists even when training the complete model in the final stage, reducing optimization difficulty and accelerating loss descent.
- Core assumption: Flatness of the loss landscape correlates with easier optimization in later stages
- Evidence anchors:
  - [section 5.1] "Early Dropout pushes model parameters into a flatter region of the loss landscape"
  - [section 5.2] "We find a strong correlation between the model's generalization ability and the trace of the Hessian matrix"
  - [corpus] Weak - no direct evidence in neighbor papers
- Break condition: If the final stage is too short, the model may not fully exploit the flatter landscape benefits

### Mechanism 3
- Claim: EST improves generalization by maintaining small Hessian trace
- Mechanism: Sampling subnetworks is equivalent to Structural Dropout, which reduces the trace of the Hessian matrix. This reduced trace persists through all training stages, including the final stage with the complete model, leading to better generalization.
- Core assumption: Smaller Hessian trace correlates with better generalization
- Evidence anchors:
  - [section 5.2] "Liu et al. (2023a) investigate the phenomenon where different models exhibit significant differences in downstream tasks under the same loss on the pre-training dataset"
  - [section 5.2] "Furthermore, they find a strong correlation between the model's generalization ability and the trace of the Hessian matrix"
  - [section 5.2] Table 5 shows EST has higher GLUE score with smaller trace of Hessian matrix
- Break condition: If the sampling scheduler causes the model to spend too little time training with the complete model, the final Hessian trace may not reflect true generalization capability

## Foundational Learning

- Concept: Structural Dropout
  - Why needed here: Understanding how random subnetwork sampling introduces regularization
  - Quick check question: How does Structural Dropout differ from standard dropout in terms of which parameters are masked?

- Concept: Hessian matrix and its trace
  - Why needed here: The paper uses the trace of the Hessian to explain generalization improvements
  - Quick check question: What does a smaller trace of the Hessian matrix indicate about the loss landscape?

- Concept: Lottery Ticket Hypothesis
  - Why needed here: Provides context for why subnetworks can perform comparably to full networks
  - Quick check question: How does EST's random sampling approach differ from the pruning-based approach in the Lottery Ticket Hypothesis?

## Architecture Onboarding

- Component map: Index Generator -> Router -> Transformer Layer -> Sampling Scheduler
- Critical path: Forward pass computation → Loss calculation → Backward pass → Parameter update
- Design tradeoffs:
  - Sampling rate vs. training cost: Higher sampling rates save more computation but may harm performance
  - Stage duration vs. convergence: Longer stages allow better convergence but increase total training time
  - Random vs. input-dependent sampling: Random sampling is simpler but may miss input-specific optimizations
- Failure signatures:
  - Loss spikes at stage transitions: May indicate sampling scheduler issues
  - No improvement in final stage: Could mean insufficient stage duration or poor initialization
  - GPU memory overflow: May require adjusting batch size or sampling rates
- First 3 experiments:
  1. Test with a simple 2-layer MLP using EST to verify basic functionality
  2. Apply EST to GPT2 with minimal sampling (e.g., only layer sampling) to isolate effects
  3. Run ablation study with different stage transition points to find optimal scheduling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of sampling scheduler impact the generalization performance of the model?
- Basis in paper: [inferred] The paper shows that EST benefits generalization but does not provide a detailed analysis of how different sampling schedulers affect generalization performance.
- Why unresolved: The paper only tests a few sampling schedulers and does not provide a comprehensive analysis of the impact of different schedulers on generalization.
- What evidence would resolve it: Experiments comparing the generalization performance of models trained with different sampling schedulers.

### Open Question 2
- Question: Can EST be applied to other types of models beyond Transformer-based models?
- Basis in paper: [explicit] The paper mentions that EST can be applied to models like Mamba but does not provide experimental evidence.
- Why unresolved: The paper does not provide experimental results for models other than Transformer-based models.
- What evidence would resolve it: Experiments applying EST to models other than Transformer-based models and evaluating their performance.

### Open Question 3
- Question: What is the theoretical basis for the design of the sampling scheduler?
- Basis in paper: [explicit] The paper mentions that they use a practical sampling scheduler but does not provide a theoretical justification for its design.
- Why unresolved: The paper does not provide a theoretical analysis of the impact of the sampling scheduler on the training dynamics and model performance.
- What evidence would resolve it: A theoretical analysis of the impact of different sampling schedulers on the training dynamics and model performance.

## Limitations

- The theoretical justification relies heavily on the equivalence between random subnetwork sampling and Structural Dropout, which is not rigorously established for the evolving subnetwork setting
- The sampling scheduler design appears critical to success but is not systematically explored - the chosen three-stage approach may not be optimal
- The correlation between Hessian trace and generalization, while observed empirically, lacks mechanistic explanation for why this relationship holds specifically in the EST context

## Confidence

- High confidence in the empirical results showing FLOPs savings (26.7% for GPT2, 25.0% for TinyLlama) and maintained or improved downstream performance
- Medium confidence in the theoretical mechanisms explaining why EST works
- Medium confidence in the generalization improvements

## Next Checks

1. **Ablation Study on Sampling Components**: Systematically test variants of EST with only layer sampling, only module sampling, and different combinations to isolate which sampling strategy drives the observed improvements.

2. **Scheduler Sensitivity Analysis**: Vary the transition points between training stages and the sampling rates within each stage to determine the sensitivity of EST performance to scheduler design choices.

3. **Theoretical Validation of Structural Dropout Equivalence**: Conduct controlled experiments comparing EST with equivalent dropout rates applied uniformly across all parameters to test whether the equivalence claimed with Structural Dropout holds empirically in the evolving setting.