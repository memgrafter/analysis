---
ver: rpa2
title: Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language
  Models?
arxiv_id: '2408.02651'
source_url: https://arxiv.org/abs/2408.02651
tags:
- arxiv
- adversarial
- language
- https
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of jailbreaking aligned large
  language models, specifically improving the transferability of adversarial prompts
  to black-box models. The core method introduces a reinforcement learning approach
  that uses a small surrogate model to optimize adversarial triggers through inference-only
  API access to the target model, employing a BERTScore-based reward function.
---

# Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?

## Quick Facts
- arXiv ID: 2408.02651
- Source URL: https://arxiv.org/abs/2408.02651
- Authors: Mohammad Bahrami Karkevandi; Nishant Vishwamitra; Peyman Najafirad
- Reference count: 40
- Primary result: RL-based method improves jailbreak attack success rate on Mistral-7B-Instruct-v0.2 by 5% and 4% on train and test sets respectively compared to baseline GCG prompts

## Executive Summary
This paper addresses the problem of jailbreaking aligned large language models by improving the transferability of adversarial prompts to black-box models. The authors introduce a reinforcement learning approach that uses a small surrogate model to optimize adversarial triggers through inference-only API access to the target model. By employing a BERTScore-based reward function that measures semantic similarity to affirmative responses, the method achieves a 5% improvement in attack success rate on the Mistral-7B-Instruct-v0.2 model compared to baseline GCG prompts.

## Method Summary
The proposed method employs a two-phase training approach. Phase 1 uses supervised fine-tuning with initial adversarial triggers obtained from the GCG method on a surrogate model. Phase 2 applies reinforcement learning using soft Q-learning to refine these triggers specifically for the new target model through inference-only API access. The RL framework uses BERTScore as a reward function to measure semantic similarity between the target model's generated output and a reference affirmative sentence. The surrogate model (distilGPT-2 or GPT-2-xl) has a frozen architecture with only an MLP layer trained during RL optimization.

## Key Results
- RL-based method achieves 5% improvement in attack success rate on train set compared to GCG prompts
- RL-based method achieves 4% improvement in attack success rate on test set compared to GCG prompts
- The method successfully transfers adversarial triggers to the black-box Mistral-7B-Instruct-v0.2 model through inference-only API access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate model learns to generate adversarial triggers that produce high semantic similarity with affirmative target responses.
- Mechanism: The RL framework uses BERTScore as a reward function that measures the cosine similarity between the target model's generated output and a reference affirmative sentence. High similarity indicates successful jailbreak.
- Core assumption: BERTScore can reliably measure the degree of affirmative tone in model responses.
- Evidence anchors:
  - [abstract] "Our method, which leverages a BERTScore-based reward function, enhances the transferability and effectiveness of adversarial triggers"
  - [section 3.3] "Intuitively, the BERTScore evaluates the target model's response semantically using the cosine similarity metric and rewards the adversarial trigger with a high reward if the adversarial trigger has successfully attacked the target model"
- Break condition: If the target model changes its response pattern or uses different phrasing for affirmative responses, BERTScore may no longer correlate with successful jailbreaks.

### Mechanism 2
- Claim: The two-phase training approach allows the surrogate model to first learn general adversarial trigger patterns before adapting to specific target models.
- Mechanism: Phase 1 uses supervised fine-tuning with initial adversarial triggers to establish baseline behavior. Phase 2 uses RL to refine these triggers specifically for the new target model through inference-only API access.
- Core assumption: Initial adversarial triggers contain transferable patterns that can be adapted through RL.
- Evidence anchors:
  - [section 3.3] "In the first phase of the training, we use the previously obtained T0 to fine-tune M(ð‘Ž) in a supervised setting"
  - [section 3.3] "The second phase, which is the main training phase of adapting the adversarial triggers to the new model, involves refining the surrogate model's adversarial trigger generations, using reinforcement learning"
- Break condition: If initial triggers are completely ineffective on the target model, the supervised phase may not provide useful starting points for RL adaptation.

### Mechanism 3
- Claim: The frozen architecture of the surrogate model ensures that learned adversarial trigger patterns remain interpretable and transferable between models.
- Mechanism: Only the MLP parameters are trained while the rest of the GPT-2 model remains frozen, preserving the discrete token generation capability while optimizing trigger effectiveness.
- Core assumption: Freezing the base model maintains token-level interpretability needed for adversarial trigger application.
- Evidence anchors:
  - [section 3.3] "Similar to RLPrompt[9], we limit the parameters to be trained, ðœƒ, to an MLP with a single hidden layer, adapted to the surrogate model M(ð‘Ž) before the language head, and freeze the rest of the parameters of the model"
- Break condition: If the frozen base model's token representations differ significantly between models, the learned triggers may not transfer effectively.

## Foundational Learning

- Concept: Reinforcement Learning with sparse rewards
  - Why needed here: The reward signal (successful jailbreak) is only available after complete inference, requiring RL to handle delayed feedback
  - Quick check question: How does the soft Q-learning algorithm handle the exploration-exploitation tradeoff in this discrete token space?

- Concept: BERTScore as semantic similarity metric
  - Why needed here: Traditional text similarity metrics may not capture the nuanced affirmative tone needed for successful jailbreaks
  - Quick check question: What specific layers of BERT does BERTScore use to compute the similarity, and why is this important for detecting affirmative responses?

- Concept: Prompt tuning and token-level optimization
  - Why needed here: The method operates at the discrete token level rather than continuous embeddings, requiring understanding of how tokens map to model behavior
  - Quick check question: How does the adversarial trigger length (20 tokens) affect both the search space and the likelihood of successful jailbreaks?

## Architecture Onboarding

- Component map: Surrogate model (frozen GPT-2 variants) -> Trainable MLP layer (parameter Î¸) -> BERTScore reward computation module -> Target LLM inference API interface -> RL optimization loop (soft Q-learning)

- Critical path: Generate candidate triggers â†’ Compute reward via target model inference â†’ Update MLP parameters â†’ Repeat until convergence

- Design tradeoffs:
  - Using frozen models limits expressiveness but maintains interpretability
  - Inference-only access to target model increases attack stealth but requires careful reward design
  - BERTScore provides semantic evaluation but may miss syntactic jailbreak patterns

- Failure signatures:
  - Reward plateaus early indicating saturation of trigger effectiveness
  - Generated triggers become repetitive or degenerate
  - Target model begins detecting and blocking common trigger patterns

- First 3 experiments:
  1. Run supervised fine-tuning phase only and evaluate initial trigger transferability
  2. Test different surrogate model sizes (distilGPT-2 vs GPT-2-xl) for RL convergence
  3. Vary the reference affirmative sentence in BERTScore to test sensitivity to different response patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed RL-based method perform when using different surrogate models beyond GPT-2 variations?
- Basis in paper: [explicit] The paper mentions using distilGPT-2 and GPT-2-xl as surrogate models, but does not explore other options
- Why unresolved: The paper only tests with GPT-2 variants, leaving the performance characteristics with other models unexplored
- What evidence would resolve it: Comparative experiments using different surrogate models (e.g., other transformer architectures, larger models, or smaller models) would show how model choice affects attack success rates

### Open Question 2
- Question: What is the minimum number of initial adversarial triggers needed in T0 for the method to be effective?
- Basis in paper: [inferred] The paper uses 100 initial triggers but doesn't explore how performance scales with different sizes of T0
- Why unresolved: The paper doesn't systematically vary the size of the initial trigger set to find the minimum effective size
- What evidence would resolve it: Experiments testing performance with different numbers of initial triggers (e.g., 10, 50, 100, 200) would show the relationship between T0 size and attack success rate

### Open Question 3
- Question: How does the method perform against more sophisticated defensive measures beyond basic denial phrase filtering?
- Basis in paper: [explicit] The paper acknowledges that current evaluation using denial phrases is "not a robust evaluation" and mentions that future work should explore defensive measures
- Why unresolved: The paper only tests against basic filtering and explicitly states that robust defensive measures are not explored
- What evidence would resolve it: Testing against more advanced defenses (e.g., adversarial training, content filtering, or detection mechanisms) would show the method's robustness in realistic scenarios

## Limitations
- The method only tested on one black-box target model (Mistral-7B-Instruct-v0.2), limiting generalizability claims
- Reliance on BERTScore as reward signal may break if target models change response patterns or phrasing
- Frozen surrogate model architecture may limit adaptation to substantially different response patterns in target models

## Confidence
**High Confidence**: The core RL framework implementation and the use of frozen surrogate models with trainable MLP layers is technically sound and reproducible. The methodology for measuring attack success rates is clearly defined.

**Medium Confidence**: The claim that BERTScore reliably measures affirmative response quality is reasonable but not rigorously validated. The choice of BERTScore layers and configuration details that would affect this measurement are not fully specified.

**Low Confidence**: The generalizability of the 5% improvement across different model architectures, sizes, and alignment strategies. The paper provides no ablation studies on how different target model characteristics affect transferability.

## Next Checks
1. **Cross-model transferability test**: Apply the learned adversarial triggers from Mistral-7B-Instruct-v0.2 to at least three other black-box models (different architectures, sizes, and alignment strategies) to quantify the degradation in attack success rate. This would reveal whether the 5% improvement is model-specific or demonstrates genuine transferability.

2. **Reward signal robustness analysis**: Systematically vary the reference affirmative sentence used in BERTScore computation and measure how this affects the learned triggers and final attack success rates. This would quantify the sensitivity of the method to the reward function design and reveal potential brittleness in the approach.

3. **Baseline comparison with adaptive methods**: Compare against a simple adaptive baseline where the initial GCG triggers are manually tuned through trial-and-error API queries, without the RL framework. This would determine whether the computational overhead of the RL approach provides meaningful advantages over simpler adaptation strategies.