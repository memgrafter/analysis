---
ver: rpa2
title: 'Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies
  in Atari Games'
arxiv_id: '2408.15950'
source_url: https://arxiv.org/abs/2408.15950
tags:
- action
- llms
- game
- multimodal
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel benchmark for evaluating multimodal
  large language models (LLMs) as low-level controllers in Atari games. The authors
  test GPT-4V, GPT-4o, Gemini 1.5 Flash, and Claude 3 Haiku against traditional RL
  agents, humans, and random agents across seven Atari games.
---

# Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games

## Quick Facts
- arXiv ID: 2408.15950
- Source URL: https://arxiv.org/abs/2408.15950
- Authors: Nicholas R. Waytowich; Devin White; MD Sunbeam; Vinicius G. Goecks
- Reference count: 12
- Key result: Multimodal LLMs achieve 23.2% of human performance as Atari game controllers

## Executive Summary
This paper introduces a novel benchmark for evaluating multimodal large language models as low-level controllers in Atari games. The authors test GPT-4V, GPT-4o, Gemini 1.5 Flash, and Claude 3 Haiku against traditional RL agents, humans, and random agents across seven Atari games. Results show that while these multimodal LLMs can outperform random agents, they fall significantly short of human and RL agent performance, with the best model (GPT-4o) achieving only 23.2% of human performance. Further analysis reveals that this underperformance stems from difficulties in visual and spatial reasoning, which are critical for effective game-playing. The study establishes a new benchmark for assessing LLMs' emergent capabilities in low-level control tasks and highlights the current limitations of these models in dynamic, visually complex environments.

## Method Summary
The authors evaluate four state-of-the-art multimodal LLMs (GPT-4V, GPT-4o, Gemini 1.5 Flash, and Claude 3 Haiku) as low-level policies in seven Atari games. The models receive preprocessed screen captures as visual input and output discrete actions to control the game. Performance is measured against human players, traditional RL agents, and random agents. The evaluation includes a visual and spatial reasoning analysis to identify the source of performance gaps. The benchmark uses simplified control schemes (2-3 discrete actions) rather than authentic game controls, and all models are evaluated zero-shot without fine-tuning.

## Key Results
- Multimodal LLMs outperform random agents but significantly underperform humans (23.2% of human performance at best)
- GPT-4o achieved the highest performance among tested models but still fell far short of human and RL agent performance
- Visual and spatial reasoning difficulties were identified as primary limitations preventing effective low-level control
- Inference speed remains a challenge for real-time decision-making in dynamic game environments

## Why This Works (Mechanism)
The benchmark evaluates how well multimodal LLMs can interpret visual game states and translate them into appropriate low-level actions. The mechanism involves the models processing preprocessed screen captures and generating discrete action sequences. Performance differences emerge from the models' ability to understand visual contexts, track object positions, and execute appropriate timing-based actions. The spatial reasoning component is critical for understanding game mechanics like projectile trajectories and object interactions.

## Foundational Learning
- **Multimodal LLM architectures**: Understanding how visual and language processing components integrate is needed to interpret model behavior and limitations
- **Atari game mechanics**: Knowledge of how these games work helps understand what visual and spatial reasoning capabilities are required
- **Zero-shot evaluation methodology**: Understanding this approach clarifies why results may differ from fine-tuned models
- **Visual preprocessing techniques**: Knowing how screen captures are processed helps interpret model input quality
- **Performance benchmarking**: Understanding how human, RL, and random baselines are established provides context for LLM performance

Quick check: Can you explain why visual reasoning is more challenging than language reasoning for these models in game contexts?

## Architecture Onboarding

**Component map**: Screen capture -> Preprocessing pipeline -> Multimodal LLM -> Action output -> Game environment

**Critical path**: Visual input processing -> Scene understanding -> Spatial reasoning -> Action selection -> Execution

**Design tradeoffs**: Simplified controls (easier for models) vs. authentic controls (harder but more realistic), zero-shot evaluation (fair comparison to RL agents) vs. fine-tuning (potentially better performance)

**Failure signatures**: Inability to track fast-moving objects, poor timing on action sequences, failure to understand spatial relationships between objects

**First experiments**: 
1. Compare model performance on static vs. dynamic game states
2. Test model performance with varying screen resolutions and preprocessing methods
3. Evaluate performance when action space is expanded to full game controls

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can multimodal LLMs be effectively trained or fine-tuned to improve their spatial reasoning capabilities for real-time game control tasks?
- Basis in paper: Explicit - The paper identifies spatial reasoning as a key limitation preventing multimodal LLMs from functioning effectively as low-level controllers in Atari games.
- Why unresolved: The study only evaluates zero-shot performance of existing multimodal LLMs without exploring training methods to enhance their spatial reasoning abilities.
- What evidence would resolve it: Experimental results comparing performance of multimodal LLMs before and after targeted training/fine-tuning specifically aimed at improving spatial reasoning in visual environments.

### Open Question 2
- Question: What architectural modifications or architectural additions could improve multimodal LLMs' performance in low-level control tasks requiring real-time decision making?
- Basis in paper: Inferred - The paper identifies inference speed as a significant limitation for real-time decision-making, suggesting current architectures are not optimized for this use case.
- Why unresolved: The study uses existing off-the-shelf multimodal LLMs without exploring how their architectures could be modified for better performance in low-level control tasks.
- What evidence would resolve it: Comparative performance results between standard multimodal LLMs and modified versions with architectural changes designed to optimize for spatial reasoning and real-time inference.

### Open Question 3
- Question: How do different visual input preprocessing methods (resolution, framing, context windows) affect multimodal LLM performance in low-level control tasks?
- Basis in paper: Explicit - The paper varies image resolution across experiments (512x512x3 vs 1000x1000x3) but does not systematically investigate the impact of different preprocessing approaches.
- Why unresolved: The study uses fixed preprocessing methods without exploring how alternative approaches might affect model performance in understanding visual scenes and making decisions.
- What evidence would resolve it: Performance comparisons across different preprocessing methods (resolutions, context windows, image framing) showing how visual input representation affects decision quality and speed.

## Limitations
- Benchmark uses simplified control schemes rather than authentic game controls, potentially overestimating LLM capabilities
- Evaluation relies on frozen, pre-trained models without fine-tuning, which may not represent fair comparison to specialized RL agents
- Does not address model calibration or robustness to visual variations across different screen qualities
- Limited investigation of how preprocessing methods affect model performance

## Confidence
High: Multimodal LLMs underperform RL agents in low-level control tasks across multiple games and models
Medium: Visual and spatial reasoning limitations are primary causes of performance gap
Low: Specific architectural modifications that would improve performance

## Next Checks
1. Re-evaluate the same models on the full original game control schemes to assess whether performance degrades under more realistic conditions
2. Compare performance when models receive training data augmentation (visual variations, different screen qualities) versus frozen pre-trained weights
3. Test whether finetuning the models on game-specific data or using chain-of-thought prompting improves visual and spatial reasoning performance