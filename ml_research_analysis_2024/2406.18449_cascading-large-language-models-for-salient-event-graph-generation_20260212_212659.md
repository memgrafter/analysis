---
ver: rpa2
title: Cascading Large Language Models for Salient Event Graph Generation
arxiv_id: '2406.18449'
source_url: https://arxiv.org/abs/2406.18449
tags:
- event
- graph
- events
- relation
- salient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CALLMSAE, a framework for generating salient
  event graphs from long documents using cascading large language models. The method
  first generates document summaries to identify salient events, then employs an iterative
  code refinement strategy to construct event relation graphs including hierarchical,
  temporal, and causal relations.
---

# Cascading Large Language Models for Salient Event Graph Generation

## Quick Facts
- arXiv ID: 2406.18449
- Source URL: https://arxiv.org/abs/2406.18449
- Reference count: 40
- Primary result: Generates more salient events and accurate graphs than baselines using LLM-generated summaries and iterative code refinement

## Executive Summary
This paper introduces CALLMSAE, a framework that uses cascading large language models to generate salient event graphs from long documents. The approach first creates document summaries using LLMs to identify salient events, then employs an iterative code refinement strategy to construct event relation graphs with hierarchical, temporal, and causal relations. The method eliminates the need for costly human annotations and demonstrates superior performance compared to traditional bottom-up extraction methods on the NYT corpus.

## Method Summary
CALLMSAE generates event relation graphs through a cascading approach: documents are first summarized by LLMs, salient events are extracted from these summaries, and then event relation graphs are generated using iterative code refinement with a hallucination grader. The framework uses code prompt formats with NetworkX APIs to generate hierarchical, temporal, and causal relation graphs in a single pass, followed by iterative refinement to filter spurious edges and recover missing relations. A fine-tuned Flan-T5 model trained on the automatically generated NYT-SEG dataset outperforms previous approaches on standard benchmarks.

## Key Results
- CALLMSAE generates more salient events and accurate event graphs compared to baselines
- Fine-tuned models trained on CALLMSAE data outperform previous approaches on M2E2, HiEve, and Causal-TimeBank datasets
- Introduced a new evaluation metric based on semantic text embeddings using Hungarian Graph Similarity
- Released NYT-SEG, a large-scale automatically annotated dataset with human-annotated test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated summaries effectively identify salient events by filtering out non-essential information.
- Mechanism: LLMs summarize documents first, then extract events from summaries, focusing on high-level narrative elements.
- Core assumption: Summaries naturally exclude trivial events like "say" and "think" while preserving narrative-critical events.
- Evidence anchors:
  - [abstract] "We first identify salient events by prompting LLMs to generate summaries, from which salient events are identified."
  - [section 3.1] "These studies identify events or entities included in human-written summaries as salient. Similarly, we instruct LLMs to generate a summary first and then extract events from it."
- Break condition: If LLM summaries include too many trivial events or miss critical narrative events.

### Mechanism 2
- Claim: Code prompt format improves relation graph generation by incorporating necessary terminologies and constraints.
- Mechanism: Python code completion tasks using NetworkX APIs ensure correct graph structure and relation terminology.
- Core assumption: Code format reduces confusion about terminologies and graph constraints compared to natural language prompts.
- Evidence anchors:
  - [abstract] "the code prompt format generates each type of relation graph in a single pass, while the naive prompting method needs to query each possible event pair individually."
  - [section 3.2] "the Python code format effectively incorporates all necessary terminologies, enabling LLMs to understand them without confusion."
- Break condition: If LLMs fail to understand code syntax or if code format introduces new confusion.

### Mechanism 3
- Claim: Iterative refinement with hallucination grader improves graph precision while recovering missing relations.
- Mechanism: Hallucination grader filters spurious edges, then iterative generation recovers missing relations.
- Core assumption: LLMs can evaluate their own outputs and identify hallucinated relations when explicitly prompted.
- Evidence anchors:
  - [abstract] "we incorporate an iterative refinement process using a hallucination grader to filter spurious edges and iterative generation to recover missing ones."
  - [section 3.3] "Recent studies show that LLMs can evaluate and correct their own outputs (Madaan et al., 2023; Asai et al., 2024)."
- Break condition: If hallucination grader becomes too conservative or fails to identify actual relations.

## Foundational Learning

- Concept: Event salience and its identification through summarization test
  - Why needed here: Understanding how to distinguish critical narrative events from trivial ones is fundamental to generating useful event graphs
  - Quick check question: What makes an event "salient" according to the summarization test?

- Concept: Directed Acyclic Graphs (DAGs) and their constraints
  - Why needed here: Event relation graphs must be DAGs to properly represent asymmetric temporal and causal relationships
  - Quick check question: Why can't a causal relation graph contain cycles?

- Concept: Hungarian algorithm for graph similarity
  - Why needed here: Traditional exact matching fails for abstractive event generation; semantic similarity requires more sophisticated matching
  - Quick check question: How does the Hungarian algorithm help match generated edges to gold standard edges?

## Architecture Onboarding

- Component map: Document → LLM Summary Generator → Salient Event Extractor → Code Prompt Generator → Relation Graphs → Hallucination Grader → Iterative Refinement → Final Graph → Fine-tuning Target (Flan-T5)
- Critical path: Document → Summary → Salient Events → Code Prompt → Relation Graph → Hallucination Grading → Final Graph
- Design tradeoffs:
  - Single-pass code generation vs. pairwise querying (efficiency vs. thoroughness)
  - Iterative refinement rounds (quality vs. computational cost)
  - LLM size selection (reasoning capability vs. resource constraints)
- Failure signatures:
  - Format errors in code generation indicate LLM doesn't understand instructions
  - Cycles in generated graphs indicate constraint violation
  - Low precision suggests hallucination grader is too permissive
  - Low recall suggests iterative refinement needs more rounds
- First 3 experiments:
  1. Test summary generation on sample documents and verify salient events vs. CAEVO events
  2. Test code prompt generation for hierarchical relations with simple event pairs
  3. Test hallucination grader on generated edges from step 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed CALLMSAE framework be extended to handle more complex event structures, such as events with multiple participants or nested events?
- Basis in paper: [inferred] The paper focuses on generating event relation graphs from news articles, which typically involve simpler event structures. However, real-world events can be more complex, involving multiple participants and nested events.
- Why unresolved: The paper does not explore the limitations of the current framework in handling complex event structures. It would be valuable to investigate how the framework can be adapted or extended to handle such cases.
- What evidence would resolve it: Experiments on a dataset with more complex event structures, such as narratives or dialogues, would provide insights into the framework's capabilities and limitations.

### Open Question 2
- Question: Can the hallucination grader be improved to better distinguish between hallucinated relations and genuine but difficult-to-verify relations?
- Basis in paper: [explicit] The paper introduces a hallucination grader to filter out hallucinated relations, but it does not provide a detailed evaluation of its performance or explore potential improvements.
- Why unresolved: The hallucination grader's effectiveness is crucial for the quality of the generated event graphs. Improving its accuracy would lead to more reliable results.
- What evidence would resolve it: Comparative studies with different hallucination grading approaches, such as using multiple LLMs or incorporating external knowledge sources, would help assess and improve the grader's performance.

### Open Question 3
- Question: How does the choice of LLM backbone affect the quality of the generated event graphs, and are there specific LLMs that are better suited for this task?
- Basis in paper: [explicit] The paper mentions that CALLMSAE is model-agnostic and uses Llama3 as the backbone due to budget constraints. However, it does not provide a comprehensive comparison of different LLMs.
- Why unresolved: Different LLMs may have varying capabilities in understanding and generating event relations. Identifying the most suitable LLM for this task would optimize the framework's performance.
- What evidence would resolve it: Experiments with various LLMs, including both open-source and proprietary models, would reveal their strengths and weaknesses in generating event graphs.

## Limitations
- Reliance on LLM-generated summaries may introduce bias in salient event identification
- Code prompt format requires LLMs to understand programming syntax, limiting generalizability
- Hallucination grader methodology lacks detailed implementation specifics
- Results may not generalize to domains beyond news documents

## Confidence
- High Confidence (4-5): Framework architecture and cascading approach are well-defined and logically structured
- Medium Confidence (2-3): Iterative refinement process and hallucination grading mechanism effectiveness
- Low Confidence (0-1): Generalizability across different domains and languages is not established

## Next Checks
1. **Hallucination Grader Validation**: Implement a controlled experiment where the hallucination grader is tested on synthetic relation graphs with known spurious edges to measure precision and recall of hallucination detection.
2. **Domain Transfer Test**: Apply the CALLMSAE framework to a different domain (e.g., scientific articles or social media posts) and compare salient event identification accuracy against the NYT corpus results.
3. **Computational Efficiency Analysis**: Measure the actual computational cost of the iterative refinement process, including time per iteration and memory usage for different document lengths.