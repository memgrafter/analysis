---
ver: rpa2
title: 'Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation
  with Preference'
arxiv_id: '2409.12150'
source_url: https://arxiv.org/abs/2409.12150
tags:
- outfit
- training
- compatibility
- item
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for personalized outfit recommendation
  using large language models (LLMs). It addresses the challenge of outfit recommendation
  by combining image captioning with LLMs and incorporating direct feedback mechanisms.
---

# Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference

## Quick Facts
- arXiv ID: 2409.12150
- Source URL: https://arxiv.org/abs/2409.12150
- Authors: Najmeh Forouzandehmehr; Nima Farrokhsiar; Ramin Giahi; Evren Korpeoglu; Kannan Achan
- Reference count: 15
- One-line primary result: Proposed framework achieves 81.03% AUC on Outfit Compatibility Prediction and 61% accuracy on Fill-in-the-Blank tasks

## Executive Summary
This paper presents a framework for personalized outfit recommendation using large language models (LLMs) by bridging visual-textual gaps through image captioning with Multimodal Large Language Models (MLLMs). The approach fine-tunes Mistral 7B using Parameter-Efficient Fine-Tuning (LoRA) and Direct Preference Optimization (DPO) on the Polyvore dataset. The method demonstrates significant improvements over baseline models by incorporating direct feedback mechanisms that continuously refine recommendations in line with seasonal fashion trends.

## Method Summary
The framework processes outfit images through MLLM captioning to generate textual descriptions, then fine-tunes Mistral 7B using LoRA for efficient parameter adaptation and DPO for preference alignment. The system handles two tasks: Outfit Compatibility Prediction (CP) using AUC metrics and Fill-in-the-Blank (FITB) using accuracy scores. Image captions focus on color, style, material, and design elements, while preference pairs guide the model toward human-aligned fashion recommendations through continuous feedback loops.

## Key Results
- Achieves 81.03% AUC on Outfit Compatibility Prediction task
- Achieves 61% accuracy on Fill-in-the-Blank task
- Shows significant improvement over baseline models
- Demonstrates effective bridging of visual-textual gap for fashion reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bridging the visual-textual gap using MLLM image captioning enables LLMs to reason about fashion items using both visual and textual features.
- **Mechanism:** MLLMs (e.g., LLaVA) encode images into dense visual features, which are mapped into the LLM's latent space via a lightweight adapter. This produces text captions describing color, style, material, and design. These captions serve as the input representation for outfit compatibility and FITB tasks.
- **Core assumption:** Multimodal features preserve sufficient fashion-relevant detail to enable downstream LLM reasoning without full fine-tuning.
- **Evidence anchors:**
  - [abstract] "We bridge the item visual-textual gap in items descriptions by employing image captioning with a Multimodal Large Language Model (MLLM)."
  - [section 3.1] "Multimodal Large Language Models (MLLMs) such as LLaVA [...] The visual features are then adapted by a specialized light-weight module to map them into the hidden space of LLMs, so they can be jointly processed with the textual inputs by the LLM."
- **Break condition:** If the MLLM captions are too generic or lose crucial style/color details, the LLM cannot distinguish outfit compatibility and the performance gap to baselines shrinks.

### Mechanism 2
- **Claim:** Parameter-efficient fine-tuning (LoRA) + Direct Preference Optimization (DPO) allows effective alignment of LLMs to human fashion preferences while avoiding catastrophic forgetting.
- **Mechanism:** LoRA adds low-rank adaptation matrices to attention layers, training only these while freezing base weights. DPO then refines the tuned model using preference pairs (preferred vs. dispreferred completions) to maximize reward likelihood without a separate reward model.
- **Core assumption:** Low intrinsic rank updates are sufficient to capture outfit recommendation task dynamics.
- **Evidence anchors:**
  - [abstract] "The LLM is efficiently fine-tuned on the open-source Polyvore dataset of curated fashion images, optimizing its ability to recommend stylish outfits. A direct preference mechanism using negative examples is employed to enhance the LLM's decision-making process."
  - [section 2.1] "Parameter-Efficient Fine-Tuning (PEFT) enables adopting large pre-trained to specific task by training a small set of parameters."
  - [section 2.3] "DPO [...] optimizing the policy model directly without needing to train a separate reward model and sample rewards through reinforcement learning."
- **Break condition:** If the rank is too low or preference pairs are not representative, the model fails to capture nuanced style compatibility, causing accuracy to drop.

### Mechanism 3
- **Claim:** Continuous feedback loop via preference training adapts the model to seasonal trends and user preferences over time.
- **Mechanism:** After initial LoRA tuning, the model is exposed to human-annotated preference pairs (preferred/dispreferred outfit completions). DPO updates weights to increase likelihood of preferred outputs, effectively embedding evolving style trends.
- **Core assumption:** Human preferences reflect real-world seasonal shifts in fashion and are stable enough to serve as training targets.
- **Evidence anchors:**
  - [abstract] "This creates a self-enhancing AI feedback loop that continuously refines recommendations in line with seasonal fashion trends."
  - [section 3.4] Prompt templates include explicit "Chosen: (Correct Option)" vs. "Rejected: (Incorrect Option)" pairs for both CP and FITB.
- **Break condition:** If preference annotations are inconsistent or seasonal trends shift faster than retraining cadence, the loop fails to improve and may diverge.

## Foundational Learning

- **Concept:** Multimodal learning (vision + language)
  - **Why needed here:** Outfit recommendation requires joint reasoning over visual appearance and textual style attributes; pure text LLMs lack visual grounding.
  - **Quick check question:** What would happen if you skipped the MLLM captioning step and fed raw image features into the LLM?

- **Concept:** Parameter-efficient fine-tuning (LoRA)
  - **Why needed here:** Mistral 7B has billions of parameters; full fine-tuning is prohibitive in compute and risks overfitting on a relatively small fashion dataset.
  - **Quick check question:** Why might LoRA adaptation in only attention weights be insufficient for long sequence tasks?

- **Concept:** Preference learning (DPO)
  - **Why needed here:** Standard supervised fine-tuning cannot capture the relative quality of fashion recommendations; DPO aligns outputs with human taste without expensive RL loops.
  - **Quick check question:** How does DPO differ from RLHF in terms of training stability and data requirements?

## Architecture Onboarding

- **Component map:** Image → MLLM Captioning → Prompt Generation → Mistral 7B LLM → Output
- **Critical path:** Image → MLLM caption → Prompt generation → LLM inference → Output
- **Design tradeoffs:**
  - Caption detail vs. token budget: Longer captions improve style granularity but consume more tokens.
  - LoRA rank vs. expressiveness: Higher rank increases capacity but reduces parameter savings.
  - Preference data volume vs. alignment quality: More pairs improve taste alignment but raise annotation costs.
- **Failure signatures:**
  - MLLM captions missing color/style cues → LLM outputs bland or mismatched outfits.
  - LoRA matrices too low-rank → Performance plateaus near baseline.
  - Imbalanced preference pairs → DPO overfits to a narrow style niche.
- **First 3 experiments:**
  1. **Sanity check:** Run MLLM on a small set of outfit images; verify captions include color/style terms.
  2. **LoRA ablation:** Compare Mistral_7B → Mistral_LoRA vs. full fine-tuning on CP task; measure AUC and parameter count.
  3. **DPO alignment:** Generate synthetic preference pairs by swapping correct/incorrect FITB options; train DPO and compare accuracy vs. LoRA-only baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework change when using different MLLM models for image captioning, such as BLIP-2 or Flamingo, compared to LLaVA?
- Basis in paper: [inferred] The paper uses LLaVA for image captioning but does not explore other MLLM options.
- Why unresolved: The paper only evaluates one MLLM model (LLaVA) and does not compare its performance with other state-of-the-art MLLMs.
- What evidence would resolve it: Comparative experiments using different MLLM models for image captioning and measuring their impact on outfit recommendation performance.

### Open Question 2
- Question: How does the proposed framework handle new fashion trends that emerge after the training period, and what is the rate of performance degradation without continuous fine-tuning?
- Basis in paper: [explicit] The paper mentions the framework's ability to continuously improve through direct feedback and adapt to seasonal trends, but does not quantify this capability.
- Why unresolved: The paper does not provide information on the framework's performance over time or its ability to adapt to new trends without additional training.
- What evidence would resolve it: Longitudinal studies tracking the framework's performance on fashion datasets collected at different time periods, with and without periodic fine-tuning.

### Open Question 3
- Question: What is the impact of incorporating user-specific context (e.g., location, occasion, weather) on the outfit recommendation quality, and how can this be effectively integrated into the current framework?
- Basis in paper: [explicit] The conclusion mentions the potential for incorporating additional modalities like user context to provide a richer understanding of preferences.
- Why unresolved: The paper does not explore or implement the integration of user-specific context into the outfit recommendation process.
- What evidence would resolve it: Experiments comparing the framework's performance with and without user context integration, using datasets that include user-specific information.

## Limitations

- The paper lacks detailed hyperparameter specifications for both LoRA and DPO training, making exact reproduction challenging
- No validation of MLLM caption quality is provided - the performance improvements could be partially attributed to caption quality rather than the LLM's reasoning capability
- The preference data generation process is underspecified, raising questions about whether the preference pairs truly capture meaningful fashion distinctions

## Confidence

- **High Confidence**: The core architecture combining MLLM captioning with LLM fine-tuning is technically sound and well-grounded in established methods
- **Medium Confidence**: The reported performance improvements (81.03% AUC on CP, 61% accuracy on FITB) are plausible given the methodological approach, though exact reproducibility is uncertain
- **Low Confidence**: The self-enhancing feedback loop claims lack empirical validation and may overestimate the system's ability to track evolving fashion trends

## Next Checks

1. **Caption Quality Validation**: Run MLLM captioning on a held-out validation set of outfit images and manually evaluate whether captions capture essential style attributes (color, material, design details) that would be relevant for compatibility decisions
2. **Ablation Study**: Compare full system performance against variants with: (a) no MLLM captioning (raw images only), (b) no DPO fine-tuning (LoRA only), and (c) no PEFT (full fine-tuning) to isolate each component's contribution
3. **Preference Data Quality Assessment**: Generate synthetic preference pairs by systematically swapping correct/incorrect options in FITB tasks and measure whether DPO training actually improves alignment beyond LoRA alone on a fixed test set