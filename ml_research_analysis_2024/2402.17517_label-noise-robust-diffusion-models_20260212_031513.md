---
ver: rpa2
title: Label-Noise Robust Diffusion Models
arxiv_id: '2402.17517'
source_url: https://arxiv.org/abs/2402.17517
tags:
- uni00000013
- label
- uni00000003
- noisy
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training conditional diffusion
  models when the training dataset contains noisy labels. The authors propose a novel
  objective function, Transition-aware weighted Denoising Score Matching (TDSM), which
  incorporates instance-wise and time-dependent label transition probabilities to
  improve the alignment of generated samples with their intended conditions.
---

# Label-Noise Robust Diffusion Models

## Quick Facts
- arXiv ID: 2402.17517
- Source URL: https://arxiv.org/abs/2402.17517
- Reference count: 40
- Primary result: Novel TDSM objective improves conditional generation under label noise by incorporating transition-aware weights

## Executive Summary
This paper addresses the challenge of training conditional diffusion models when training data contains noisy labels. The authors propose Transition-aware weighted Denoising Score Matching (TDSM), a novel objective function that incorporates instance-wise and time-dependent label transition probabilities. Through theoretical analysis and extensive experiments across various datasets and noise settings, TDSM demonstrates improved robustness to label noise compared to baseline methods, while also showing performance gains on clean datasets.

## Method Summary
The paper introduces TDSM, a transition-aware weighted denoising score matching objective that addresses label noise in conditional diffusion model training. The method leverages a time-dependent noisy-label classifier and a transition matrix to compute instance-wise and time-dependent weights. During training, the score network outputs for all classes are weighted and combined based on these transition-aware weights, allowing the model to converge to the clean-label conditional score even when trained on noisy labels. The approach includes practical techniques to reduce computational overhead, such as selective backpropagation and weight thresholding.

## Key Results
- TDSM outperforms baseline methods on both synthetic and real-world label noise across multiple datasets
- The method improves performance on clean datasets, suggesting mitigation of subtle label noise
- TDSM is complementary to existing noisy label correction techniques, further enhancing robustness
- Demonstrated effectiveness across MNIST, CIFAR-10/100, and Clothing-1M with varying noise rates and types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy-label conditional score can be decomposed into a convex combination of clean-label conditional scores weighted by instance-wise and time-dependent label transition probabilities
- Mechanism: Under class-conditional label noise, the noisy-label conditional distribution is a mixture of clean-label conditional distributions, with mixing weights determined by the instance-dependent posterior p(Y= y| X= x, ~Y= ~y)
- Core assumption: Label noise is independent of the instance given the clean label (class-conditional noise assumption)
- Evidence anchors: Theorem 1 and Proposition 2 formally establish the linear relationship between noisy-label and clean-label conditional scores

### Mechanism 2
- Claim: Training with TDSM causes the score network to converge to the clean-label conditional score even when trained on noisy labels
- Mechanism: The TDSM objective minimizes the distance between the transition-aware weighted sum of conditional score network outputs and the perturbed data score
- Core assumption: The transition matrix S is invertible (each noisy label is sufficiently likely to correspond to its true clean label)
- Evidence anchors: Theorem 3 proves that the optimal parameters obtained by minimizing the TDSM objective converge to the clean-label conditional score

### Mechanism 3
- Claim: The transition-aware weight estimator can accurately approximate the true transition-aware weights using a time-dependent noisy-label classifier and the transition matrix
- Mechanism: The weight estimator uses Bayes' theorem to compute weights from the noisy label prediction probability and the transition matrix
- Core assumption: The time-dependent noisy-label classifier can accurately estimate pt(~Y|xt) for all timesteps t
- Evidence anchors: The weight estimator formula is derived using Bayes' theorem and the classifier output

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: The paper builds on diffusion model theory, where models learn to estimate gradients of log probability densities through score matching
  - Quick check question: What is the relationship between denoising score matching and the true data score in diffusion models?

- Concept: Learning with noisy labels and transition matrices
  - Why needed here: The paper leverages established techniques from supervised learning with noisy labels, particularly transition matrix estimation and loss correction methods
  - Quick check question: How does a transition matrix help correct for label noise in supervised learning?

- Concept: Conditional generation metrics
  - Why needed here: The paper evaluates performance using both unconditional and conditional metrics
  - Quick check question: What is the difference between FID and CW-FID, and why would label noise affect them differently?

## Architecture Onboarding

- Component map: Time-dependent noisy-label classifier -> Transition matrix estimator -> Score network -> Weight evaluator -> Training loop
- Critical path:
  1. Train time-dependent noisy-label classifier on noisy labeled data
  2. Estimate transition matrix using classifier and label statistics
  3. During score network training: Sample (x, ~y) and timestep t, compute perturbed xt, evaluate transition-aware weights for all classes, compute weighted sum of score network outputs, calculate TDSM loss and update score network
- Design tradeoffs:
  - Computational cost vs. accuracy: Evaluating weights for all classes increases computation but improves robustness
  - Classifier quality vs. training stability: Better classifiers improve weight estimates but may require more training
  - Transition matrix estimation vs. flexibility: Known transition matrices are ideal but estimated ones work reasonably well
- Failure signatures:
  - Poor conditional metrics but good unconditional metrics: Indicates the model is generating good samples but not matching conditions properly
  - Worsening performance as noise rate increases: Suggests the weight estimator is failing to compensate for higher noise
  - Slow convergence or unstable training: May indicate issues with the weight evaluation or classifier training
- First 3 experiments:
  1. Run baseline DSM on clean data to establish performance ceiling
  2. Run DSM on noisy data to confirm performance degradation
  3. Run TDSM with known transition matrix on noisy data to verify improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed TDSM method be extended to handle instance-dependent label noise beyond the class-conditional noise setting?
- Basis in paper: The paper mentions that further research is necessary in the area of generative model learning from instance-dependent noisy labels
- Why unresolved: The current method is limited to class-conditional noise and the theoretical framework might need significant modifications to handle instance-dependent noise
- What evidence would resolve it: Empirical results on datasets with instance-dependent label noise showing TDSM's effectiveness

### Open Question 2
- Question: How does the performance of TDSM scale with the number of classes in the dataset?
- Basis in paper: The paper presents results on datasets with varying numbers of classes but does not explicitly discuss scaling behavior
- Why unresolved: As the number of classes increases, the complexity of estimating the transition matrix and transition-aware weights might grow
- What evidence would resolve it: Experimental results on datasets with a significantly larger number of classes or theoretical analysis of computational complexity

### Open Question 3
- Question: What is the impact of different diffusion model architectures and training objectives on the effectiveness of TDSM?
- Basis in paper: The paper mainly uses EDM as the backbone but does not extensively explore different architectures
- Why unresolved: Different architectures and training objectives might have varying sensitivities to label noise and might interact differently with TDSM
- What evidence would resolve it: A comprehensive ablation study comparing TDSM's performance across different diffusion model architectures and training objectives

## Limitations
- Effectiveness is predicated on class-conditional label noise; performance with instance-dependent noise remains unverified
- Theoretical framework assumes invertibility of transition matrix, but practical scenarios with severe noise imbalance are not thoroughly examined
- Computational overhead of evaluating transition-aware weights for all classes presents scalability concerns for large-scale applications

## Confidence
- **High**: Theoretical foundation connecting noisy-label conditional scores to clean-label conditional scores through transition-aware weights is sound and well-supported
- **Medium**: Empirical results demonstrating performance improvements across various datasets and noise settings are convincing
- **Medium**: Claim that TDSM complements existing noisy label correction techniques is supported by experiments but specific combinations are not fully explored

## Next Checks
1. Stress Test with Instance-Dependent Noise: Evaluate TDSM's performance when class-conditional noise assumption is violated by introducing instance-dependent noise
2. Ablation on Weight Estimator Quality: Systematically degrade time-dependent noisy-label classifier performance to quantify impact on TDSM's effectiveness
3. Scalability Analysis: Measure memory and computational overhead of TDSM on large-scale datasets (e.g., ImageNet) with various techniques for reducing time and memory usage