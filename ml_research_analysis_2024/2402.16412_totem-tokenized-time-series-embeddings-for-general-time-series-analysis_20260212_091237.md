---
ver: rpa2
title: 'TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis'
arxiv_id: '2402.16412'
source_url: https://arxiv.org/abs/2402.16412
tags:
- totem
- time
- series
- generalist
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a tokenized time series foundation model for
  general time series analysis, which learns discrete representations for time series
  data through self-supervised pre-training. The model employs a vector quantized
  variational autoencoder (VQVAE) to tokenize time series data, capturing the shape
  of univariate waveforms and enabling domain-agnostic modeling.
---

# TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis

## Quick Facts
- **arXiv ID:** 2402.16412
- **Source URL:** https://arxiv.org/abs/2402.16412
- **Reference count:** 40
- **Primary result:** Tokenization-based foundation model achieves strong zero-shot performance across 49 time series datasets for imputation, anomaly detection, and forecasting

## Executive Summary
TOTEM is a tokenized time series foundation model that learns discrete representations for general time series analysis through self-supervised pre-training. The model employs a vector quantized variational autoencoder (VQVAE) to tokenize time series data, capturing the shape of univariate waveforms while maintaining domain-agnostic properties. By training on diverse time series domains simultaneously, TOTEM demonstrates strong zero-shot performance across multiple tasks without fine-tuning, matching or outperforming existing state-of-the-art models in both specialist and generalist settings.

## Method Summary
TOTEM uses a VQVAE tokenizer to learn discrete representations of time series data, specifically tokenizing only the temporal dimension to ensure domain-agnosticism. The tokenizer is pre-trained on multiple data domains to learn a shared codebook of univariate waveform patterns. For forecasting, tokenized sequences are passed through a transformer encoder to predict mean and standard deviation parameters. For imputation and anomaly detection, the frozen tokenizer is used directly without additional training. The model employs reverse instance normalization to further enhance domain-agnostic feature scaling.

## Key Results
- In generalist settings, TOTEM achieves average win rates of 58.3% (imputation), 80.0% (anomaly detection), and 67.9% (forecasting) compared to specialist models
- TOTEM matches or outperforms existing state-of-the-art models across all three tasks in both specialist and generalist settings
- Discrete tokenization outperforms patching approaches in ablation studies, validating the effectiveness of fixed discrete representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tokenization enables generalization across domains because it learns domain-agnostic discrete representations of univariate waveforms.
- **Mechanism:** The VQVAE tokenizer learns a fixed codebook of K codewords that represent normalized univariate waveforms from diverse time series datasets. By flattening sensor and example dimensions and only tokenizing along the temporal axis, the model captures shape information that is common across domains while ignoring domain-specific sensor semantics.
- **Core assumption:** Time series data from different domains share common waveform patterns that can be represented with a shared discrete vocabulary, regardless of the semantic meaning of the sensors.
- **Evidence anchors:**
  - [abstract]: "TOTEM produces such generalist time series models with minimal or no fine-tuning while exhibiting strong zero-shot performance"
  - [section]: "we choose to exclusively tokenize over the temporal dimension, such that the tokens represent univariate waveforms"
  - [corpus]: No direct evidence about waveform generalization, but related work on time series tokenization supports this approach
- **Break condition:** If time series domains have fundamentally different waveform characteristics with no shared patterns, the single codebook would be insufficient and require domain-specific vocabularies.

### Mechanism 2
- **Claim:** Discrete tokenization outperforms patching because it provides a fixed, interpretable representation that enables better generalization.
- **Mechanism:** Unlike dynamic patch representations where embeddings are determined by downstream model layers, tokenization creates a fixed codebook learned during pre-training. This fixed representation decouples data representation from task-specific architecture, allowing the same tokens to work across multiple tasks and domains.
- **Core assumption:** A fixed discrete representation learned from diverse data is more generalizable than dynamic patch embeddings trained end-to-end for specific tasks.
- **Evidence anchors:**
  - [abstract]: "Discrete tokenization for general time series analysis" and ablation showing "discrete tokens outperform patches"
  - [section]: "we choose to use a discrete, deterministic encoder to produce time series tokens" and comparison to patching methods
  - [corpus]: Related work on tokenization in NLP and vision supports this approach, but specific time series tokenization comparisons are limited
- **Break condition:** If the downstream task requires highly specialized representations that cannot be captured by a general discrete vocabulary, patching might be superior.

### Mechanism 3
- **Claim:** Generalist training on diverse domains improves zero-shot performance compared to specialist models trained on single domains.
- **Mechanism:** By training the tokenizer on multiple data domains simultaneously, the model learns representations that capture common temporal patterns across domains. This exposure to diverse data types enables the model to generalize to new domains without fine-tuning, as demonstrated by superior zero-shot performance.
- **Core assumption:** Training on diverse time series data domains provides sufficient coverage of temporal patterns to enable effective zero-shot generalization to unseen domains.
- **Evidence anchors:**
  - [abstract]: "In generalist settings, TOTEM achieves an average win rate of 58.3% in imputation, 80.0% in anomaly detection, and 67.9% in forecasting"
  - [section]: "Generalist models are those trained on many data domains simultaneously" and comparison to specialist models
  - [corpus]: No direct evidence about zero-shot generalization from diverse domains, but related foundation model literature supports this approach
- **Break condition:** If the zero-shot domains are too dissimilar from training domains or if the training dataset lacks sufficient diversity, generalization performance would degrade.

## Foundational Learning

- **Concept:** Vector Quantized Variational Autoencoders (VQVAEs)
  - Why needed here: VQVAEs provide the discrete tokenization mechanism that enables domain-agnostic time series representations
  - Quick check question: How does the VQVAE training objective balance reconstruction quality with codebook commitment and quantization?

- **Concept:** Self-supervised learning
  - Why needed here: Enables learning discrete representations without labeled data across multiple domains
  - Quick check question: What would be the impact of using supervised vs. self-supervised learning for the tokenizer pre-training?

- **Concept:** Domain generalization
  - Why needed here: The core goal is to create models that work across different time series domains without task-specific adaptation
  - Quick check question: How does the model ensure that learned representations capture domain-agnostic patterns rather than domain-specific features?

## Architecture Onboarding

- **Component map:** VQVAE tokenizer (1D CNN encoder -> codebook -> 1D transpose CNN decoder) -> Transformer forecaster (4 layers, 4 heads, hidden dim 256) with mean/std prediction heads

- **Critical path:**
  1. Pre-train VQVAE on multi-domain corpus to learn discrete codebook
  2. Tokenize time series data using frozen VQVAE encoder
  3. Apply RevIN normalization to tokenized sequences
  4. Pass normalized tokens through transformer forecaster
  5. Generate predictions with learned mean/std parameters

- **Design tradeoffs:**
  - Fixed vs. dynamic representations: Discrete tokens vs. patches
  - Temporal vs. spatial tokenization: Only temporal to ensure domain-agnosticism
  - Codebook size vs. reconstruction quality: Larger K improves reconstruction but may hurt downstream task performance

- **Failure signatures:**
  - Poor reconstruction quality: Indicates inadequate codebook size or compression factor
  - Degraded zero-shot performance: Suggests codebook learned domain-specific rather than domain-agnostic patterns
  - Training instability: May require adjustment of commitment loss weight or learning rate

- **First 3 experiments:**
  1. Verify VQVAE reconstruction quality on held-out data from training domains
  2. Test tokenization consistency across different time series from same domain
  3. Evaluate downstream forecaster performance on in-domain test data before zero-shot evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does TOTEM's performance scale with dataset size and diversity beyond the explored domains?
- **Basis in paper:** [explicit] The paper states that "scaling up the generalist training dataset size by an order of magnitude or more could unlock true domain- and task-agnostic generalizability" and mentions that "simply training on more data is insufficient for achieving generalization - the types of data are also crucial."
- **Why unresolved:** The current study only explored a limited set of datasets, and the authors explicitly suggest that scaling up both dataset size and diversity could further improve TOTEM's performance.
- **What evidence would resolve it:** Conducting experiments with significantly larger and more diverse datasets, comparing TOTEM's performance across different scales of data, and analyzing the relationship between dataset diversity and generalization ability.

### Open Question 2
- **Question:** What is the impact of using different compression factors (F) in the VQVAE on TOTEM's performance across various tasks?
- **Basis in paper:** [explicit] The paper mentions that "TOTEM uses a compression factor of F = 4" but also states that "as K increases from 32 to 256 to 512, the reconstruction performance improves."
- **Why unresolved:** The authors chose F = 4 as the compression factor but did not explore the effects of different compression factors on TOTEM's performance.
- **What evidence would resolve it:** Running experiments with various compression factors (e.g., F = 2, 4, 8) and comparing TOTEM's performance across different tasks to determine the optimal compression factor.

### Open Question 3
- **Question:** How does TOTEM's performance compare to other generalist models on short-term forecasting tasks?
- **Basis in paper:** [explicit] The authors mention that "short-term forecasting task typically uses non-standard and dataset-specific input and output dimensionalities" and exclude it from their main results due to "non-standardized and leaky baselines."
- **Why unresolved:** The authors did not include short-term forecasting in their main results due to the challenges of fair comparison, but they acknowledge that TOTEM might perform well on this task.
- **What evidence would resolve it:** Conducting experiments on short-term forecasting tasks with standardized input and output lengths, comparing TOTEM's performance to other generalist models, and analyzing the impact of tokenization on short-term forecasting.

## Limitations

- **Limited generalization evidence:** Evaluation based on only 49 datasets may not fully capture true domain-agnostic performance across all time series domains
- **No comparison to domain experts:** Paper compares generalist vs. specialist TOTEM variants but does not benchmark against specialized models trained for specific domains
- **Reconstruction vs. task performance gap:** No established correlation between VQVAE reconstruction quality and downstream task performance, leaving uncertainty about whether good reconstruction leads to better task results

## Confidence

**High confidence:** The ablation study results showing discrete tokens outperform patches are well-supported by the experimental evidence presented. The specialist vs. generalist comparison methodology is sound and the results are clearly presented.

**Medium confidence:** The zero-shot performance claims across diverse domains are supported by the experiments but could benefit from testing on a broader range of domains. The mechanism explaining why temporal-only tokenization ensures domain-agnosticism is reasonable but not conclusively proven.

**Low confidence:** The claim that tokenization is "more effective than patching" as a general principle for time series foundation models is based on comparisons within this specific framework but lacks broader empirical support across different tokenization approaches.

## Next Checks

1. **Expand domain diversity test:** Evaluate TOTEM on additional time series domains not represented in the original 49 datasets (e.g., finance, climate, industrial IoT) to verify true zero-shot generalization capabilities beyond the training distribution.

2. **Reconstruction quality correlation analysis:** Systematically measure VQVAE reconstruction MSE across different datasets and correlate with downstream task performance to determine whether reconstruction quality is a reliable predictor of task success.

3. **Codebook size sensitivity study:** Run ablation experiments varying K (codebook size) across a wider range (64, 128, 256, 512, 1024) to identify whether the current choice of 256 is optimal or whether different tasks benefit from different codebook sizes.