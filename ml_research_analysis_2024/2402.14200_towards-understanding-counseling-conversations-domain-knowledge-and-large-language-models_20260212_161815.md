---
ver: rpa2
title: 'Towards Understanding Counseling Conversations: Domain Knowledge and Large
  Language Models'
arxiv_id: '2402.14200'
source_url: https://arxiv.org/abs/2402.14200
tags:
- features
- conversation
- help
- level
- seeker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models can understand
  counseling conversations by predicting help seekers' post-conversation feelings.
  BERT-based classifiers and ChatGPT achieve moderate F1 scores (61.9-63.2%) with
  low recall (25.3-37.0%) on identifying negative outcomes.
---

# Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models

## Quick Facts
- arXiv ID: 2402.14200
- Source URL: https://arxiv.org/abs/2402.14200
- Reference count: 20
- Primary result: BERT-based classifiers and ChatGPT achieve moderate F1 scores (61.9-63.2%) with low recall (25.3-37.0%) on identifying negative outcomes

## Executive Summary
This paper examines whether large language models can understand counseling conversations by predicting help seekers' post-conversation feelings. The authors find that incorporating human-annotated utterance-level features (counseling strategies) and LLM-generated session-level features improves F1 by ~15% to 71.3%. Session-level features compress conversation text and represent lengthy sessions more consistently than LLM-generated summaries. Ensembling classifiers trained on different feature types yields the best performance (F1 71.3%, recall 49.3%). Utterance-level features enhance model focus on counselor strategies, while session-level features better characterize conversation content than plain summaries, especially for longer sessions.

## Method Summary
The study uses conversation text from the Childhelp crisis hotline, human-annotated utterance-level features representing counseling strategies, and LLM-generated session-level features. Base models include DistilBERT classifiers and ChatGPT for zero-shot prediction. Features are extracted at utterance-level (counselor strategies), session-level (targeted questions), and summary-level (plain conversation summaries). An ensemble model combines predictions from classifiers trained on different feature types using logistic regression. The evaluation uses 10-fold cross-validation with macro F1 and recall metrics.

## Key Results
- BERT-based classifiers and ChatGPT achieve moderate F1 scores (61.9-63.2%) with low recall (25.3-37.0%) on identifying negative outcomes
- Incorporating human-annotated utterance-level features and LLM-generated session-level features improves F1 by ~15% to 71.3%
- Session-level features compress conversation text and represent lengthy sessions more consistently than LLM-generated summaries
- Ensembling classifiers trained on different feature types yields the best performance (F1 71.3%, recall 49.3%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Session-level features provide more consistent representation than LLM-generated summaries for long conversations.
- Mechanism: As conversation length increases beyond 3K tokens, LLMs become more susceptible to hallucination or incomplete information generation in plain summaries, while session-level features (derived from targeted questions) maintain stability.
- Core assumption: LLMs struggle with maintaining coherence and completeness when summarizing very long conversations compared to answering specific structured questions.
- Evidence anchors:
  - [section 5.3]: "When the context is lengthy, we hypothesize that LLMs are susceptible to having more insufficient or incorrect generations in producing general summaries, compared to answering questions focusing on specific aspects."
  - [section 5.3]: Figure 2 shows F1 score comparison between session level feature input and summaries with stance, demonstrating that summary with stance performance decreases when conversation length exceeds 3K tokens while session level feature input shows more consistent performance.
  - [corpus]: Found 25 related papers, but none directly address the comparison of LLM-generated summaries versus structured question answering for long conversations, suggesting this is a novel contribution.
- Break condition: If LLMs develop better long-context handling or if conversation lengths remain consistently short (<3K tokens), this mechanism's advantage would diminish.

### Mechanism 2
- Claim: Adding utterance-level features to conversation text improves model performance by focusing on counselor strategies.
- Mechanism: Utterance-level features (like "Emotional Attending," "Paraphrasing," "Validating") are added as special tokens before counselor utterances, providing explicit semantic information that helps models identify important conversational strategies and their relationship to outcomes.
- Core assumption: The presence of specific counseling strategies in counselor utterances is predictive of help seeker outcomes, and models can leverage this explicit information better than inferring it from text alone.
- Evidence anchors:
  - [section 5.2]: "Simple integration of utterance level features to the conversation (i.e. Utter) improves the F1 score by 1.5% and minority class recall by 18% compared to the original conversation (i.e. Conv)."
  - [section 5.2]: Shapley value analysis shows that utterances with features contribute more to correct predictions, particularly for identifying negative outcomes.
  - [corpus]: Related work (Cao et al. 2019a, Park et al. 2019) shows that behavioral codes and counseling strategies improve understanding of counseling conversations, supporting the value of explicit strategy annotation.
- Break condition: If utterance-level features are poorly annotated, incomplete, or if the relationship between specific strategies and outcomes is weak or inconsistent across counselors.

### Mechanism 3
- Claim: Ensembling classifiers trained on different feature types improves performance by capturing complementary information.
- Mechanism: Different feature types (utterance-level, session-level, stance-based summaries) capture different aspects of conversations. Combining their predictions through ensemble learning leverages this complementarity to improve overall accuracy.
- Core assumption: Different feature representations capture orthogonal information about conversations, and combining them provides more complete coverage than any single feature type.
- Evidence anchors:
  - [section 5.1]: "Ensembling classifiers trained with different features not only mitigates the potential class imbalance issues but also produces the best F1 and recall scores."
  - [section 5.1]: Table 4 shows ensemble model achieving F1 71.29% and recall 49.27%, outperforming all individual feature-based models.
  - [corpus]: While ensemble methods are common in NLP, the specific application to counseling conversation outcome prediction with diverse feature types appears novel based on the related work search.
- Break condition: If feature types are highly correlated or if one feature type dominates the others in predictive power, the benefit of ensembling would be reduced.

## Foundational Learning

- Concept: Understanding counseling strategies and their impact on outcomes
  - Why needed here: The paper relies on human-annotated utterance-level features representing specific counseling strategies. Understanding what these strategies are and how they affect help seekers is essential for interpreting the model's use of these features.
  - Quick check question: What are the four main categories of utterance-level features used in this paper, and how might each type affect a help seeker's perception of the conversation?

- Concept: Limitations of transformer-based models with long documents
  - Why needed here: The paper truncates conversations to fit within BERT's token limits and explores alternative representations. Understanding these limitations is crucial for appreciating why session-level features are valuable.
  - Quick check question: What is the maximum token limit for standard BERT models, and what are the two main approaches discussed for handling longer conversations?

- Concept: Zero-shot versus few-shot prompting effectiveness
  - Why needed here: The paper uses both zero-shot and few-shot prompting with ChatGPT for different tasks. Understanding the trade-offs between these approaches is important for interpreting the results.
  - Quick check question: According to the paper, how does the performance of few-shot prompting with text-davinci-003 compare to BERT-based classifiers for utterance-level feature prediction?

## Architecture Onboarding

- Component map: Conversation text -> Feature extraction (utterance-level, session-level, summaries) -> Base models (DistilBERT classifiers, ChatGPT) -> Ensemble layer (logistic regression) -> Evaluation (10-fold cross-validation)

- Critical path:
  1. Preprocess conversation text (truncate to first/last k turns)
  2. Generate or extract features (utterance-level, session-level, summaries)
  3. Train base classifiers on different feature types
  4. Generate predictions from each classifier
  5. Combine predictions through ensemble layer
  6. Evaluate on test set

- Design tradeoffs:
  - Truncation vs. LongFormer: Truncation with first/last k turns is simpler and performs better than using LongFormer, despite losing middle conversation context
  - Fine-tuning vs. prompting: BERT-based classifiers are fine-tuned for feature prediction, while ChatGPT is prompted in zero-shot setting; prompting performs worse for utterance-level features but better for session-level summaries with stance
  - Feature granularity: Fine-grained 18-class utterance features provide more expressibility but lower accuracy than grouped 4-class features

- Failure signatures:
  - Low recall on minority class: Model predicts few negative outcomes, suggesting insufficient focus on indicators of dissatisfaction
  - Inconsistent performance across conversation lengths: Indicates issues with feature representation for longer conversations
  - Poor performance of LLM-generated features: Suggests prompting strategy needs refinement or that certain tasks require fine-tuning

- First 3 experiments:
  1. Test different values of k (number of turns from beginning/end) to find optimal truncation strategy
  2. Compare BERT-based classifiers versus ChatGPT on conversation outcome prediction with raw conversation text
  3. Evaluate impact of adding utterance-level features to conversation text on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering approaches affect the quality and consistency of LLM-generated session-level features for counseling conversations?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that sub-optimal performance of LLM-generated features is a limitation, but does not systematically explore different prompt engineering techniques or evaluate their effectiveness.
- What evidence would resolve it: Systematic comparison of different prompt engineering approaches (e.g., few-shot vs. zero-shot, chain-of-thought prompting, structured prompts) on the quality and consistency of generated features, measured through human evaluation and downstream task performance.

### Open Question 2
- Question: What is the optimal model architecture for combining utterance-level and session-level features in counseling conversation analysis?
- Basis in paper: inferred
- Why unresolved: The paper mentions that they did not fully explore the most efficient model structure to combine these features, suggesting that alternative architectures (e.g., multi-task learning, hierarchical models) might yield better results.
- What evidence would resolve it: Comparative study of different model architectures (e.g., multi-task learning, hierarchical attention networks, graph neural networks) for integrating utterance-level and session-level features, evaluated on conversation outcome prediction performance.

### Open Question 3
- Question: How generalizable is the approach of using domain knowledge and LLM-generated features for understanding counseling conversations across different datasets and counseling contexts?
- Basis in paper: explicit
- Why unresolved: The paper notes that the effectiveness of domain knowledge was only shown in one data source and acknowledges the need to experiment with additional datasets to demonstrate generalizability.
- What evidence would resolve it: Evaluation of the approach on multiple counseling conversation datasets from different contexts (e.g., online forums, in-person therapy sessions, different counseling hotlines) and demonstration of consistent performance improvements.

## Limitations

- Limited generalizability to other counseling contexts: The study focuses on crisis counseling conversations from a single hotline service (Childhelp), which may not transfer to other counseling domains.
- Feature annotation reliability: Human-annotated utterance-level features are derived from trained annotators but inter-annotator agreement and annotation consistency are not reported.
- LLM prompt sensitivity: Session-level features are generated through ChatGPT prompts with specific instructions and few-shot examples, which may be sensitive to prompt engineering choices.

## Confidence

- High confidence: The ensemble approach demonstrably improves performance over individual feature types, with F1 score increasing from 61.9-63.2% to 71.3%. The experimental methodology using 10-fold cross-validation is sound and the performance metrics are clearly reported.
- Medium confidence: The mechanism by which session-level features outperform plain summaries for longer conversations is supported by Figure 2, but the exact nature of LLM hallucinations or incomplete generations in summaries is not directly observed or measured, only hypothesized.
- Low confidence: The Shapley value analysis suggesting utterance-level features help models focus on counselor strategies is based on feature attribution methods that can be unstable and sensitive to implementation details not fully specified in the paper.

## Next Checks

1. **Cross-domain validation**: Test the feature types and ensemble approach on counseling conversations from a different domain (e.g., academic advising or career counseling) to assess generalizability beyond crisis intervention contexts.

2. **Ablation study on feature quality**: Systematically remove or corrupt utterance-level and session-level features to quantify their individual contributions and test the hypothesis that these domain-specific features are essential for the performance gains observed.

3. **Prompt variation experiment**: Systematically vary the ChatGPT prompts for session-level feature generation (changing few-shot examples, rephrasing questions, adjusting temperature) to measure the sensitivity of performance to prompt engineering choices.