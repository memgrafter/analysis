---
ver: rpa2
title: 'BQA: Body Language Question Answering Dataset for Video Large Language Models'
arxiv_id: '2410.13206'
source_url: https://arxiv.org/abs/2410.13206
tags:
- language
- question
- video
- dataset
- body
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BQA, a dataset designed to evaluate Video
  Large Language Models' (VideoLLMs) ability to interpret emotions from body language
  in short video clips. BQA consists of 7,632 videos, each paired with a multiple-choice
  question and four emotion options derived from 26 emotion labels.
---

# BQA: Body Language Question Answering Dataset for Video Large Language Models

## Quick Facts
- arXiv ID: 2410.13206
- Source URL: https://arxiv.org/abs/2410.13206
- Authors: Shintaro Ozaki; Kazuki Hayashi; Miyu Oba; Yusuke Sakai; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 35
- Primary result: VideoLLMs show significant difficulty interpreting body language emotions, with notable demographic biases

## Executive Summary
This work introduces BQA, a dataset designed to evaluate Video Large Language Models' (VideoLLMs) ability to interpret emotions from body language in short video clips. BQA consists of 7,632 videos, each paired with a multiple-choice question and four emotion options derived from 26 emotion labels. Questions were generated using Gemini, with steps to extract candidates, generate and filter questions, and assign difficulty levels. Evaluation across multiple VideoLLMs revealed significant challenges in understanding body language, with notable biases based on age and ethnicity. Multimodal Chain of Thought (CoT) improved performance but showed consistent error patterns, indicating reliance on facial expressions and limited generalizability. The dataset is available for further research.

## Method Summary
The BQA dataset was created through a four-step process using Gemini: extracting candidate video clips from the BoLD dataset, generating multiple-choice questions about the characters' emotions, filtering out low-quality or ambiguous questions, and assigning difficulty levels. The dataset contains 7,632 videos with associated questions and four emotion options each. Evaluation was conducted on several VideoLLMs (VideoLLaMA2, Phi-3.5, Qwen2-VL, LLaVA-NeXT) using standardized 16-frame input configuration. Both standard and Multimodal Chain of Thought (CoT) evaluation methods were employed, along with human evaluation on 100 random samples. Demographic bias analysis examined performance across gender, age, and ethnicity categories.

## Key Results
- VideoLLMs achieved low accuracy (22.4%-39.8%) on BQA emotion classification tasks
- Multimodal Chain of Thought improved performance but models still struggled with body language interpretation
- Significant performance disparities were found based on age and ethnicity demographics
- Models showed consistent error patterns, suggesting reliance on facial expressions rather than true body language understanding

## Why This Works (Mechanism)
VideoLLMs struggle with body language interpretation because they are primarily trained on datasets where facial expressions dominate emotional cues. When forced to rely on body language alone, these models lack the fine-grained understanding needed to accurately interpret non-facial emotional signals. The CoT approach helps by providing explicit reasoning steps, but models still fail to develop robust body language comprehension, instead finding alternative patterns in the limited visual information available.

## Foundational Learning
**Body Language Interpretation** - Understanding emotional states from posture, gestures, and movement patterns
*Why needed:* BQA specifically tests this capability by asking about emotions from body movements alone
*Quick check:* Can the model correctly identify emotions when facial expressions are obscured or minimal?

**Multimodal Chain of Thought** - Sequential reasoning process that breaks down complex visual reasoning tasks
*Why needed:* Helps models approach the problem systematically rather than relying on pattern matching
*Quick check:* Does the CoT approach consistently outperform standard prompting across different question types?

**Demographic Bias Analysis** - Examining model performance variations across different population groups
*Why needed:* Reveals whether models generalize across diverse body language expressions
*Quick check:* Are performance disparities consistent across multiple demographic dimensions?

## Architecture Onboarding

Component Map: Gemini (dataset generation) -> VideoLLMs (evaluation) -> Human evaluation -> Bias analysis

Critical Path: Video input (16 frames) → Feature extraction → Question answering → Emotion classification

Design Tradeoffs: Standardized 16-frame input balances computational efficiency with sufficient temporal context, but may limit performance compared to using maximum available frames

Failure Signatures: Consistent error patterns across demographics, overreliance on facial cues despite instructions to focus on body language, poor performance on difficult questions

First Experiments:
1. Evaluate baseline VideoLLaMA2 performance on BQA test set using provided prompts
2. Implement and test Multimodal Chain of Thought approach on same model
3. Conduct demographic bias analysis by gender, age, and ethnicity categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do VideoLLMs' performance and error patterns change when evaluating videos with high-quality versus low-quality visual content, given that BQA uses old film footage with varying quality?
- Basis in paper: [explicit] Section 7.2 "Video Quality" discusses the possibility that video quality may affect accuracy.
- Why unresolved: The paper acknowledges this limitation but does not conduct controlled experiments comparing performance across different video quality levels.
- What evidence would resolve it: Systematic experiments evaluating VideoLLMs on BQA using both the original low-quality videos and professionally enhanced versions would reveal the impact of video quality on performance.

### Open Question 2
- Question: To what extent do cultural differences in interpreting body language affect VideoLLMs' accuracy on BQA, and how can these biases be mitigated in future dataset development?
- Basis in paper: [explicit] Section 8.1 "Taking Care about Culture" and analysis showing ethnicity-based error patterns in Section 5.
- Why unresolved: The paper identifies cultural considerations as a limitation and notes ethnicity-based biases but does not explore cross-cultural variations in emotion interpretation or propose mitigation strategies.
- What evidence would resolve it: Evaluations of VideoLLMs on BQA using annotators from diverse cultural backgrounds, combined with experiments testing models trained on culturally diverse body language datasets, would quantify cultural bias effects and potential solutions.

### Open Question 3
- Question: Does increasing the number of frames processed by VideoLLMs beyond the standardized 16 frames improve performance on BQA, and what are the resource implications of such an approach?
- Basis in paper: [explicit] Section 7.3 "Frame Issues" notes that many VideoLLMs use image models treating videos as image sequences and that using maximum frames may affect results.
- Why unresolved: The paper standardized to 16 frames due to memory constraints but acknowledges this may limit performance without empirically testing higher frame counts.
- What evidence would resolve it: Controlled experiments varying frame counts (e.g., 16, 32, 64 frames) while measuring both accuracy improvements and GPU memory usage would determine optimal frame processing for body language understanding.

## Limitations
- Dataset generation relies heavily on Gemini's output, which may introduce systematic biases in question framing and emotion labeling
- Evaluation does not fully disentangle the contribution of facial expressions versus body movements, suggesting models may not truly understand body language
- Demographic bias analysis is limited by coarse-grained categorization that may not capture nuanced performance patterns

## Confidence
- High confidence in dataset construction and general finding that VideoLLMs struggle with body language interpretation
- Medium confidence in demographic bias analysis due to metadata granularity limitations
- Low confidence in CoT performance claims without more detailed error analysis and ablation studies

## Next Checks
1. Conduct controlled experiment isolating facial expressions versus body movements by masking faces and re-evaluating model performance
2. Perform fine-grained analysis of model errors by manually annotating incorrectly answered questions to identify systematic failure patterns
3. Test BQA with additional VideoLLMs and multimodal models trained on different modalities to assess generalizability and robustness of findings