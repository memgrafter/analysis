---
ver: rpa2
title: 'Implementing local-explainability in Gradient Boosting Trees: Feature Contribution'
arxiv_id: '2402.09197'
source_url: https://arxiv.org/abs/2402.09197
tags:
- feature
- contribution
- decision
- each
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a feature contribution method for Gradient
  Boosting Decision Trees (GBDT) that provides local explainability by calculating
  the contribution of each feature using node residues. The method leverages the GBDT
  architecture to determine the sequence of node decisions leading to a prediction.
---

# Implementing local-explainability in Gradient Boosting Trees: Feature Contribution

## Quick Facts
- arXiv ID: 2402.09197
- Source URL: https://arxiv.org/abs/2402.09197
- Authors: Ángel Delgado-Panadero; Beatriz Hernández-Lorca; María Teresa García-Ordás; José Alberto Benítez-Andrades
- Reference count: 40
- Key outcome: Introduces feature contribution method for GBDT that provides local explainability by calculating feature contributions using node residues, performing comparably to SHAP in general cases and better for outliers

## Executive Summary
This paper presents a novel method for providing local explainability in Gradient Boosting Decision Trees (GBDT) by calculating feature contributions using node residuals. The approach leverages the GBDT architecture to trace the decision path for each prediction and compute exact contributions for each feature. The method is proven theoretically to accurately reflect the internal behavior of GBDTs and is demonstrated experimentally to perform comparably to SHAP values while better explaining outlier predictions.

## Method Summary
The method computes feature contributions by extracting the decision path for each sample through the GBDT trees, calculating node contributions as the difference between node values and parent node values, and aggregating these contributions by feature. This intrinsic approach directly uses the trained GBDT's structure and node values, providing exact feature contributions without approximation. The method is implemented using scikit-learn's GradientBoosting with default parameters, extracting decision paths via tree.decision_path() and computing residuals to determine feature contributions.

## Key Results
- Feature contribution method performs comparably to SHAP values on diabetes and concrete datasets
- Method better explains outlier predictions by following exact decision paths
- Theoretical proofs demonstrate accurate reflection of internal GBDT behavior
- Method addresses GDPR requirements for explainability and non-discrimination in ML models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computes exact feature contributions using GBDT node residual structure
- Mechanism: Each node stores expected target value; contribution is difference between child and parent node values
- Core assumption: Features are statistically independent
- Evidence anchors: [abstract] uses residue of each node; [section] defines contribution as difference in conditional expectations
- Break condition: Correlated features violate independence assumption

### Mechanism 2
- Claim: Provides intrinsic explainability tied to GBDT architecture
- Mechanism: Directly uses trained GBDT structure and node values rather than approximating
- Core assumption: GBDT structure accurately reflects learned relationships
- Evidence anchors: [abstract] makes GBDT locally explainable; [section] contrasts intrinsic vs post-hoc methods
- Break condition: Poorly trained or overfitted GBDT models

### Mechanism 3
- Claim: Better handles outliers by following exact decision path
- Mechanism: Traces unique path through trees based on sample's feature values
- Core assumption: Decision path for outlier reflects model's reasoning
- Evidence anchors: [abstract] better explains outlier predictions; [section] artificial sample experiment shows clearer contributions
- Break condition: Erratic decision path for outliers

## Foundational Learning

- Concept: Gradient Boosting Decision Trees (GBDT)
  - Why needed here: Method is specifically designed for GBDT leveraging their structure
  - Quick check question: What is the main difference between GBDT and Random Forests in tree construction?

- Concept: Decision Trees and CART algorithm
  - Why needed here: Method builds upon decision tree structure using node values
  - Quick check question: How does a decision tree determine the best split at each node?

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: Method is compared to SHAP, understanding SHAP contextualizes contributions
  - Quick check question: What is the main idea behind SHAP values for model explanations?

## Architecture Onboarding

- Component map: Extract decision path -> Compute node residuals -> Aggregate by feature -> Output contributions
- Critical path: For prediction, extract decision path → compute node residuals → sum residuals by feature → output feature contributions
- Design tradeoffs: Intrinsic explainability (exact, model-specific) vs post-hoc methods (approximate, model-agnostic); outlier handling vs general cases
- Failure signatures: Inaccurate contributions for correlated features; failure to capture model biases; poor outlier explanations with erratic paths
- First 3 experiments:
  1. Compare feature contributions with and without correlated feature added
  2. Add noise to feature and observe contribution changes, especially for other features
  3. Create outlier sample and compare feature contributions to SHAP values

## Open Questions the Paper Calls Out

- Open Question 1: How does the method handle correlated features and distinguish true importance from correlation-induced importance?
  - Basis: Paper notes correlated features may have similar contributions but doesn't provide clear distinction method
  - Why unresolved: Demonstrates similar contributions but lacks method to adjust for correlation strength
  - What evidence would resolve it: Experiments on datasets with known correlations and adjustment methods

- Open Question 2: How does the method perform on deeper trees and larger datasets, and are there scalability concerns?
  - Basis: Experiments limited to simple datasets; mentions deeper trees overfit but doesn't explore scalability
  - Why unresolved: Limited scope, unclear performance on large-scale complex datasets
  - What evidence would resolve it: Extensive experiments on large datasets with deep trees and complexity analysis

- Open Question 3: How can the method be extended to other tree-based models like Random Forests?
  - Basis: Paper mentions opens research lines for other algorithms but provides no concrete extension
  - Why unresolved: Focuses solely on GBDT without exploring adaptation to other models
  - What evidence would resolve it: Theoretical framework for extension with empirical validation

## Limitations

- Independence assumption required for accurate feature contributions may be violated with correlated features
- Method performance on outliers needs validation across more diverse datasets and outlier types
- Comparison with SHAP lacks statistical significance testing across multiple runs

## Confidence

- Mechanism 1 (residual-based contributions): Medium - Theoretical framework sound but real-world dependencies may violate assumptions
- Mechanism 2 (intrinsic explainability): High - Architecture-specific nature well-defined and directly leverages GBDT structure
- Mechanism 3 (outlier handling): Low-Medium - Based on single experiment; requires broader validation

## Next Checks

1. Test method on datasets with known feature correlations to quantify impact on contribution accuracy
2. Create multiple outlier types (value outliers, feature outliers, combined) and compare explanations across methods
3. Implement statistical significance testing comparing feature contributions and SHAP values across multiple random seeds and dataset splits