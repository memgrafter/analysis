---
ver: rpa2
title: Setting up the Data Printer with Improved English to Ukrainian Machine Translation
arxiv_id: '2404.15196'
source_url: https://arxiv.org/abs/2404.15196
tags:
- translation
- language
- bleu
- ukrainian
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of building large language models
  for Ukrainian by creating a high-quality English-to-Ukrainian machine translation
  system. The proposed method involves a two-phase fine-tuning process: first, supervised
  fine-tuning of a pretrained decoder-only model using 3M noisy parallel sentence
  pairs filtered by heuristic methods; second, unsupervised data selection via k-fold
  perplexity filtering on 17K high-quality examples from Extended Multi30K.'
---

# Setting up the Data Printer with Improved English to Ukrainian Machine Translation

## Quick Facts
- arXiv ID: 2404.15196
- Source URL: https://arxiv.org/abs/2404.15196
- Reference count: 0
- Primary result: Two-phase fine-tuning approach achieves 32.3 BLEU on FLORES, outperforming previous encoder-decoder models

## Executive Summary
This work addresses the challenge of building large language models for Ukrainian by creating a high-quality English-to-Ukrainian machine translation system. The proposed method involves a two-phase fine-tuning process: first, supervised fine-tuning of a pretrained decoder-only model using 3M noisy parallel sentence pairs filtered by heuristic methods; second, unsupervised data selection via k-fold perplexity filtering on 17K high-quality examples from Extended Multi30K. The resulting model, Dragoman, achieves a BLEU score of 32.3 on the FLORES devtest set, outperforming previous state-of-the-art encoder-decoder models. The study also explores few-shot learning approaches, finding that while base models can produce high-quality translations with oracle rescoring, they underperform compared to fine-tuned systems.

## Method Summary
The method employs a two-phase fine-tuning approach using a pretrained decoder-only transformer (Mistral-7B-v0.1) with LoRA adapters. Phase 1 involves fine-tuning on 3M noisy parallel sentence pairs from Paracrawl dataset after heuristic filtering. Phase 2 refines the model using 17K high-quality examples from Extended Multi30K selected through k-fold perplexity filtering. The approach leverages the self-attention mechanism of decoder-only models to handle translation without separate encoder, and uses parameter-efficient fine-tuning to adapt large models without full fine-tuning.

## Key Results
- Dragoman achieves 32.3 BLEU on FLORES devtest, outperforming previous encoder-decoder models
- Two-phase fine-tuning (noisy then high-quality data) improves performance by 1.97 BLEU
- Perplexity filtering removes 11,600 sentences, gaining 0.35 BLEU on dev set
- Base models can produce high-quality translations with oracle rescoring in few-shot settings but underperform fine-tuned systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-phase fine-tuning with noisy data first, then high-quality data improves BLEU score
- Mechanism: Initial phase adapts the model to translation task despite noisy data; second phase refines translation quality using cleaner data with perplexity-based filtering
- Core assumption: The model can learn useful translation patterns from noisy data and benefit from refinement on cleaner data
- Evidence anchors:
  - [abstract]: "supervised finetuning of a large pretrained language model with a noisy parallel dataset... followed by a second phase of training using 17K examples selected by k-fold perplexity filtering"
  - [section 4]: "Switching datasets gives us a performance boost of 1.97 BLEU. We additionally delete 11600 sentences from the dataset using unsupervised perplexity filtering pipeline gaining 0.35 on the dev set"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.516. Evidence is weak for corpus impact on this mechanism.

### Mechanism 2
- Claim: Unsupervised perplexity filtering via k-fold cross-validation improves translation quality
- Mechanism: Sentences are scored by models that haven't seen them, allowing identification of domain-inconsistent or surprising sentences to remove
- Core assumption: Perplexity scores can effectively distinguish high-quality from low-quality sentences in the target domain
- Evidence anchors:
  - [section 4]: "We use perplexity as a data selection criterion... We apply the k-fold cross-validation technique to make the perplexity evaluation in-domain"
  - [section 4]: "We plot the distribution of scores... We also provide threshold sweep results... By comparing finetuned results, we demonstrate that data from the second phase alone is not enough to match the performance of our best checkpoint"
  - [corpus]: Evidence is weak; corpus shows related work on data filtering but no direct evidence for k-fold perplexity method.

### Mechanism 3
- Claim: Decoder-only models with long context windows can perform translation without separate encoder
- Mechanism: Self-attention mechanism allows the model to use input, partial output, and past examples in context window for translation
- Core assumption: Relative position embeddings enable generalization to longer sequences than seen during training
- Evidence anchors:
  - [section 6]: "These models receive gradient from all outputs during pretraining, and the self-attention mechanism can see the input, the partial output, and access past examples of translations in its context window using induction heads"
  - [section 6]: "In our early experiments, we find that our models still generalize to inputs longer than what is seen in training. This generalization behavior is often attributed to relative position embeddings"
  - [corpus]: Evidence is weak; corpus shows related work on decoder-only models but no specific evidence for long context translation capability.

## Foundational Learning

- Concept: Perplexity-based data filtering
  - Why needed here: To identify and remove low-quality or domain-inconsistent sentence pairs from training data
  - Quick check question: How does k-fold cross-validation prevent bias in perplexity-based filtering?

- Concept: In-context learning and few-shot prompting
  - Why needed here: To evaluate base model capabilities before fine-tuning and understand prompt sensitivity
  - Quick check question: What happens to translation quality when you increase beam width with few-shot prompting?

- Concept: Parameter-efficient fine-tuning (LoRA adapters)
  - Why needed here: To adapt large pretrained models without full fine-tuning, saving memory and computation
  - Quick check question: Why use low-rank adapters instead of full fine-tuning for this translation task?

## Architecture Onboarding

- Component map: Pretrained decoder-only transformer (Mistral-7B-v0.1) → LoRA adapter fine-tuning → perplexity filtering → evaluation on FLORES
- Critical path: Data preparation → Phase 1 fine-tuning (3M noisy pairs) → Phase 2 fine-tuning (17K filtered high-quality) → evaluation
- Design tradeoffs: Decoder-only vs encoder-decoder architecture; parameter-efficient vs full fine-tuning; heuristic vs learned filtering
- Failure signatures: BLEU score plateaus or degrades; perplexity filtering removes too many examples; few-shot prompting underperforms
- First 3 experiments:
  1. Fine-tune on full 3M Paracrawl dataset without filtering, evaluate BLEU
  2. Apply heuristic filtering to Paracrawl, fine-tune on 1M/3M/8M subsets, compare BLEU
  3. Apply k-fold perplexity filtering to Extended Multi30K, sweep threshold, evaluate BLEU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Dragoman compare to commercial translation systems like Google Translate on the WMT22 benchmark?
- Basis in paper: [explicit] The authors mention that their model achieves 24.72 BLEU on the WMT22 test set, ranking behind the best result of Roussis and Papavassiliou (2022) at 25.2 BLEU. They also note that a submission that scores relatively low on WMT22 performs comparably on FLORES.
- Why unresolved: The paper does not provide a direct comparison between Dragoman and Google Translate on the WMT22 benchmark, nor does it explore the reasons for the performance difference between WMT22 and FLORES.
- What evidence would resolve it: A direct comparison of Dragoman and Google Translate on the WMT22 benchmark, along with an analysis of the data distribution properties that cause the performance difference between WMT22 and FLORES.

### Open Question 2
- Question: How does the performance of Dragoman scale with increasing context window size during training and inference?
- Basis in paper: [inferred] The authors mention that they train on single short sentence pairs and do not pack context windows full of tokens as done in pretraining. They also note that their models still generalize to inputs longer than what is seen in training, which they attribute to relative position embeddings.
- Why unresolved: The paper does not provide experiments or analysis on how Dragoman's performance changes with different context window sizes during training and inference.
- What evidence would resolve it: Experiments comparing Dragoman's performance on various context window sizes during training and inference, along with an analysis of the impact on translation quality and computational efficiency.

### Open Question 3
- Question: How does the choice of tokenizer affect the performance of Dragoman, especially considering the compression rate difference between English and Ukrainian?
- Basis in paper: [explicit] The authors mention that they used the LLaMA and Mistral tokenizers, which use at least twice as many tokens to compress a Ukrainian sentence of the same length as an English sentence in characters. They also provide a distribution of sentence token lengths for English and Ukrainian.
- Why unresolved: The paper does not explore how the choice of tokenizer affects Dragoman's performance or whether using a tokenizer with better compression rates for Ukrainian could improve results.
- What evidence would resolve it: Experiments comparing Dragoman's performance using different tokenizers with varying compression rates for Ukrainian, along with an analysis of the impact on translation quality and computational efficiency.

## Limitations

- Exact threshold values for heuristic filtering remain unspecified, making exact replication challenging
- k-fold perplexity filtering method lacks direct empirical validation against simpler alternatives
- Decoder-only approach's long-context generalization is demonstrated through limited experiments without rigorous testing

## Confidence

- **High Confidence:** The overall two-phase fine-tuning methodology (noisy data followed by high-quality data) effectively improves translation quality
- **Medium Confidence:** The k-fold perplexity filtering method improves translation quality, but evidence is primarily comparative rather than absolute
- **Low Confidence:** The decoder-only model's long-context generalization and in-context learning capabilities are sufficient for translation, with mechanisms not rigorously tested

## Next Checks

1. **Ablation Study on Filtering Methods:** Run parallel experiments comparing k-fold perplexity filtering against simpler heuristic filtering (like basic length or language mismatch filtering) on the same dataset to quantify the specific contribution of the more complex filtering approach.

2. **Decoder-only Context Window Testing:** Systematically test translation quality on input lengths beyond the training context window (e.g., 8K, 16K tokens) to empirically validate the long-context generalization claims and compare performance degradation against encoder-decoder baselines.

3. **Cross-Domain Robustness Evaluation:** Evaluate the fine-tuned model on non-FLORES datasets (like news or social media text) to assess domain generalization and reveal whether the two-phase fine-tuning approach creates models that are too specialized to the training domains.