---
ver: rpa2
title: Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching
arxiv_id: '2403.18705'
source_url: https://arxiv.org/abs/2403.18705
tags:
- conditional
- which
- optimal
- flow
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces conditional Wasserstein distances (Wp,Y)
  that allow joint measure approximation to control the expected Wasserstein distance
  between posteriors, which is not possible with standard Wasserstein distances. The
  key idea is to restrict optimal transport plans to those that do not move mass in
  the Y-direction, which is motivated by inverse problems where the observation Y
  should remain fixed.
---

# Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching

## Quick Facts
- arXiv ID: 2403.18705
- Source URL: https://arxiv.org/abs/2403.18705
- Reference count: 40
- Primary result: Introduces conditional Wasserstein distances (Wp,Y) that allow joint measure approximation to control expected Wasserstein distance between posteriors, applied to Bayesian OT Flow Matching

## Executive Summary
This paper introduces conditional Wasserstein distances (Wp,Y) that enable joint measure approximation to control posterior approximation quality in Bayesian inverse problems. The key innovation is restricting optimal transport plans to preserve the Y-component, ensuring that joint distribution approximation directly translates to posterior approximation. A relaxation mechanism using a modified cost function allows practical computation while maintaining theoretical guarantees. The framework is applied to extend OT Flow Matching to Bayesian settings, demonstrating superior performance on MNIST, Gaussian mixture models, and CIFAR10 class-conditional image generation compared to diagonal approaches.

## Method Summary
The method introduces conditional Wasserstein distances that restrict optimal transport plans to preserve the Y-component of joint distributions. A relaxation using modified cost functions enables practical computation. The framework extends OT Flow Matching to Bayesian settings by optimizing velocity fields that transport between joint distributions while maintaining conditional structure. Implementation uses U-Net architectures for velocity fields and Sinkhorn approximations for optimal transport calculations, with experiments on MNIST particle flows, CIFAR10 class-conditional generation, and Gaussian mixture models.

## Key Results
- Conditional Wasserstein-1 dual formulation naturally recovers conditional Wasserstein GAN losses
- Bayesian OT Flow Matching outperforms diagonal approaches on MNIST, GMM, and CIFAR10 tasks
- Relaxation with parameter β allows practical computation while maintaining posterior approximation quality
- Theoretical guarantees show Y-movement vanishes as β → ∞ in relaxed formulation

## Why This Works (Mechanism)

### Mechanism 1
The conditional Wasserstein distance W_{p,Y} equals the expected Wasserstein distance between posteriors, allowing joint measure approximation to control posterior approximation quality. By restricting optimal transport plans to those that do not move mass in the Y-direction (Γ^4_Y), the distance formulation ensures that only X-component transportation matters, which directly relates to conditional distributions. The optimal transport plans in Γ^4_Y preserve the Y-marginal, meaning π_{1,3}♯α = ∆♯P_Y where ∆(y) = (y,y).

### Mechanism 2
The relaxation of the conditional Wasserstein distance using W_{p,β} allows practical computation while maintaining approximate posterior control. By introducing a cost function d^p_β((y1,x1),(y2,x2)) = ||x1-x2||^p + β||y1-y2||^p, large β penalizes Y-movement, creating a smooth transition between unrestricted and fully restricted transport. As β → ∞, optimal transport plans converge to those in Γ^4_Y, minimizing Y-component movement.

### Mechanism 3
The conditional Wasserstein-1 dual formulation naturally recovers conditional Wasserstein GAN losses, providing a principled training objective. The dual problem max_{h∈F} {E_{Y,X}[h] - E_{Y,Z}[h(y,G(y,·))]} emerges from the conditional Wasserstein-1 distance, where F consists of bounded upper semi-continuous functions with 1-Lipschitz property in the second variable. This elegant connection provides a natural training objective for practical implementation.

## Foundational Learning

- Concept: Optimal transport and Wasserstein distances
  - Why needed here: The entire framework builds on Wasserstein geometry to measure distances between probability measures, with special restrictions for conditional settings
  - Quick check question: What is the difference between the standard Wasserstein distance and the conditional Wasserstein distance W_{p,Y}?

- Concept: Disintegration of measures
  - Why needed here: Essential for relating joint distributions to conditional distributions, allowing the conditional Wasserstein distance to equal expected conditional Wasserstein distances
  - Quick check question: How does the disintegration formula relate the integral over joint space to integrals over conditional and marginal spaces?

- Concept: Geodesics and velocity fields in Wasserstein spaces
  - Why needed here: The gradient flow formulation and flow matching applications require understanding how probability measures evolve continuously, with velocity fields having no Y-component
  - Quick check question: Why must the velocity field in conditional Wasserstein space have zero Y-component?

## Architecture Onboarding

- Component map: Joint distribution → Conditional Wasserstein distance → Relaxation → Dual formulation → Training objective → Posterior sampling
- Critical path: The theoretical foundation (Proposition 1) connects joint distribution approximation to posterior quality, which flows through relaxation, dual formulation, and finally to practical training
- Design tradeoffs:
  - Strict conditional vs relaxed transport: Strict preserves Y exactly but may be computationally intractable; relaxed allows small Y-movement for practical computation
  - Function class choice: More expressive discriminators improve approximation but increase computational cost
  - β selection: Larger values improve Y-preservation but may increase optimization difficulty
- Failure signatures:
  - Poor posterior approximation despite good joint approximation: Likely indicates β too small or Γ^4_Y restriction too severe
  - Training instability: May indicate improper discriminator architecture or insufficient regularization
  - Slow convergence: Could indicate suboptimal β value or inefficient coupling computation
- First 3 experiments:
  1. Verify Proposition 10: Implement particle flows with increasing β values and measure Y-movement reduction
  2. Test dual formulation: Train conditional Wasserstein GAN using the derived loss and compare with standard approaches
  3. Compare OT Bayesian flow matching vs diagonal approach: Use GMM toy problem to evaluate posterior approximation quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Bayesian OT flow matching scale with increasing dimensionality of the observation space Y and latent space X? The paper demonstrates effectiveness on MNIST (28x28 images) and CIFAR10 (32x32 images) but doesn't explore higher-dimensional spaces or the computational limits of the approach. Higher-dimensional spaces would require more sophisticated OT solvers and potentially new computational strategies.

### Open Question 2
What is the theoretical relationship between the relaxation parameter β and the approximation quality of the conditional Wasserstein distance to the exact conditional Wasserstein distance? While the paper motivates the relaxation and shows empirical benefits, it doesn't provide theoretical guarantees on how quickly the relaxed distance approaches the true conditional distance as β increases.

### Open Question 3
How does the choice of interpolation scheme (linear vs OT-based) affect the quality of the learned conditional distributions in practical applications? While the paper demonstrates advantages on specific datasets, it doesn't provide a comprehensive comparison across diverse application domains or explore when simpler linear approaches might be sufficient.

## Limitations
- Practical feasibility of maintaining strict conditional constraints during transport requires careful parameter tuning
- Computational complexity of restricted optimal transport plans may become prohibitive in high-dimensional settings
- Theoretical connections to conditional Wasserstein GANs may face practical challenges in function class selection and training stability

## Confidence
- High confidence: The theoretical foundation connecting conditional Wasserstein distances to expected posterior distances (Proposition 1)
- Medium confidence: The relaxation mechanism's ability to preserve posterior fidelity while enabling practical computation (Proposition 10)
- Medium confidence: The dual formulation's recovery of conditional Wasserstein GAN losses (Theorem 4)
- Low confidence: Practical performance across diverse high-dimensional inverse problems without extensive hyperparameter tuning

## Next Checks
1. **Ablation study on β values**: Systematically vary β across orders of magnitude and quantify the trade-off between Y-preservation quality and computational stability in controlled synthetic settings.

2. **Function class sensitivity analysis**: Evaluate the impact of discriminator architecture and capacity on the dual formulation's effectiveness, particularly comparing bounded vs unbounded function classes.

3. **Scalability testing**: Apply the framework to progressively higher-dimensional inverse problems to identify computational bottlenecks and evaluate the relaxation's effectiveness in maintaining conditional constraints.