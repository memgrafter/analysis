---
ver: rpa2
title: Interpreting and Improving Large Language Models in Arithmetic Calculation
arxiv_id: '2409.01659'
source_url: https://arxiv.org/abs/2409.01659
tags:
- heads
- mlps
- language
- calculation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models (LLMs) like LLaMA2
  perform arithmetic calculations by identifying the internal components responsible
  for such tasks. Through causal intervention techniques, the authors find that only
  a small fraction (< 5%) of attention heads and MLPs play a significant role in arithmetic
  calculations.
---

# Interpreting and Improving Large Language Models in Arithmetic Calculation

## Quick Facts
- **arXiv ID:** 2409.01659
- **Source URL:** https://arxiv.org/abs/2409.01659
- **Authors:** Wei Zhang; Chaoqun Wan; Yonggang Zhang; Yiu-ming Cheung; Xinmei Tian; Xu Shen; Jieping Ye
- **Reference count:** 40
- **Key outcome:** LLMs use sparse components (<5%) for arithmetic; Precise SFT fine-tunes only these to improve math performance by ~15% on GSM8K

## Executive Summary
This paper investigates how large language models perform arithmetic calculations by identifying the internal components responsible for these tasks. Through causal intervention techniques, the authors find that only a small fraction of attention heads and MLPs play a significant role in arithmetic calculations. These components show strong focus on operands and operators, and their behavior transfers across datasets and tasks. Based on this insight, the authors propose "precise supervised fine-tuning" (Precise SFT), which fine-tunes only these key components to improve mathematical capabilities while preserving non-mathematical abilities and requiring fewer training resources.

## Method Summary
The authors employ causal intervention techniques to identify arithmetic-relevant components in LLaMA2 models. They first apply Layer-wise Relevance Propagation (LRP) to locate components whose activations correlate with final output logits. Then, they use iterative masking-based causal intervention to measure the impact of each component on arithmetic task performance. Components that significantly affect arithmetic performance are identified as arithmetic-relevant. Based on this analysis, they propose Precise SFT, which fine-tunes only these identified components using supervised fine-tuning on mathematical datasets, while keeping the rest of the model frozen. This approach achieves improved mathematical performance with significantly fewer parameters and computational resources compared to full-model fine-tuning.

## Key Results
- Less than 5% of attention heads and MLPs are responsible for arithmetic calculations in LLMs
- Arithmetic-relevant components show strong focus on operands and operators, transferring across datasets and tasks
- Precise SFT improves GSM8K accuracy by approximately 15% with less than 1% of parameters tuned
- Precise SFT matches or surpasses full-model fine-tuning in math performance while preserving non-mathematical abilities

## Why This Works (Mechanism)
The effectiveness of Precise SFT stems from the identified sparsity of arithmetic-relevant components in LLMs. By focusing fine-tuning efforts only on these critical components, the approach avoids catastrophic forgetting of general knowledge stored in other parts of the model. The transferability of these components across datasets suggests that arithmetic reasoning relies on a consistent set of mechanisms regardless of specific task formulation. This targeted approach allows for more efficient use of training resources while maintaining overall model performance.

## Foundational Learning
- **Causal intervention**: A technique for determining cause-effect relationships by systematically intervening in a system and measuring the effects; needed to identify which components actually drive arithmetic performance rather than merely correlating with it
- **Layer-wise Relevance Propagation (LRP)**: A method for attributing predictions to input features by propagating relevance backward through the network; quick check: verify that identified components show high relevance scores for arithmetic inputs
- **Attention heads**: Components that weigh the importance of different input tokens when processing information; needed to understand how models focus on relevant mathematical elements
- **MLPs (Multi-Layer Perceptrons)**: Feedforward networks within transformer layers that process token representations; needed to capture non-linear transformations in arithmetic reasoning
- **GSM8K**: A benchmark dataset of grade school math word problems; quick check: evaluate model performance on this standard arithmetic reasoning test

## Architecture Onboarding

**Component map:** Input tokens → Embedding layer → Transformer blocks (Attention heads + MLPs) → Output logits

**Critical path:** Token embeddings → Arithmetic-relevant attention heads → Arithmetic-relevant MLPs → Final output

**Design tradeoffs:** Full fine-tuning (comprehensive but resource-intensive) vs. Precise SFT (efficient but may miss distributed representations)

**Failure signatures:** Performance degradation on arithmetic tasks after removing or damaging arithmetic-relevant components; reduced transferability when fine-tuning only these components

**First experiments:**
1. Apply causal intervention to identify arithmetic-relevant components in a baseline model
2. Perform Precise SFT on identified components and evaluate on GSM8K
3. Compare performance with full-model fine-tuning and baseline on both arithmetic and general tasks

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalization of their findings to larger models, the potential for negative transfer when fine-tuning only arithmetic components, and the long-term stability of improvements achieved through Precise SFT.

## Limitations
- Analysis focuses only on LLaMA2-7B and LLaMA2-13B models, limiting generalizability to other architectures and model sizes
- Causal intervention approach may not capture all relevant mechanisms, particularly distributed representations across multiple components
- Does not extensively explore potential negative transfer when fine-tuning only arithmetic-relevant components

## Confidence
- **High confidence**: Sparsity of arithmetic-relevant components (<5% of heads/MLPs), transferability across datasets, effectiveness of Precise SFT in improving GSM8K accuracy (~15% improvement)
- **Medium confidence**: Preservation of non-mathematical abilities during fine-tuning, computational efficiency benefits
- **Low confidence**: Generalization to larger models, long-term stability of improvements, absence of negative transfer

## Next Checks
1. Replicate the sparsity findings and Precise SFT effectiveness on larger LLaMA2 models (30B, 70B) and alternative architectures (Mistral, Llama-3)
2. Conduct ablation studies removing fine-tuned arithmetic components to test for potential negative transfer on non-mathematical tasks
3. Perform long-term stability evaluation by measuring GSM8K performance before and after extended inference on diverse datasets to detect gradual performance degradation