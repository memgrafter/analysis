---
ver: rpa2
title: Optimizing importance weighting in the presence of sub-population shifts
arxiv_id: '2410.14315'
source_url: https://arxiv.org/abs/2410.14315
tags:
- weights
- learning
- training
- loss
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distribution shifts in machine
  learning by improving importance weighting techniques for sub-population shifts.
  The authors argue that existing methods neglect the variance introduced by finite
  training samples, leading to suboptimal weight choices.
---

# Optimizing importance weighting in the presence of sub-population shifts

## Quick Facts
- arXiv ID: 2410.14315
- Source URL: https://arxiv.org/abs/2410.14315
- Authors: Floris Holstege; Bram Wouters; Noud van Giersbergen; Cees Diks
- Reference count: 40
- Primary result: Bi-level optimization of importance weights improves worst-group accuracy on benchmark datasets

## Executive Summary
This paper addresses the problem of distribution shifts in machine learning by improving importance weighting techniques for sub-population shifts. The authors argue that existing methods neglect the variance introduced by finite training samples, leading to suboptimal weight choices. They propose a bi-level optimization procedure that simultaneously optimizes model parameters and weights using a validation set. Applied to last-layer retraining of deep neural networks, their approach significantly improves both weighted average and worst-group accuracy across benchmark datasets (Waterbirds, CelebA, MultiNLI). The method shows particular effectiveness when training data is limited or minority groups are small, and provides improved robustness to hyperparameter choices.

## Method Summary
The authors propose optimizing importance weights for distribution shift scenarios through a bi-level optimization framework. The outer level optimizes weights to minimize validation loss, while the inner level trains model parameters on training data with current weights. This is achieved by computing hyper-gradients via the implicit function theorem, allowing gradient-based updates to the weights. The method is applied to last-layer retraining of pre-trained deep neural networks, where a logistic regression head is trained using the optimized weights. The approach can be integrated with existing importance weighting methods (GW-ERM, SUBG, DFR, GDRO, JTT) to improve their performance without requiring major architectural changes.

## Key Results
- Optimized weights improve worst-group accuracy by up to 30% compared to standard likelihood ratio weighting
- Method shows largest improvements when training data is limited (small n') or minority groups are small
- Optimized weights provide improved robustness to hyperparameter choices, particularly regularization strength

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing importance weights addresses bias-variance trade-off in finite-sample regime
- Mechanism: Standard likelihood ratio weighting asymptotically corrects distribution shift but over-weights minority groups in finite samples, increasing variance and degrading performance. Optimized weights trade off bias and variance to improve generalization.
- Core assumption: The expected loss with respect to test distribution can be decomposed into bias and variance terms that depend on the choice of weights.
- Evidence anchors:
  - [abstract]: "existing heuristics for determining the weights are suboptimal, as they neglect the increase of the variance of the estimated model due to the finite sample size"
  - [section]: Theoretical analysis in Section 4 derives bias-variance decomposition for linear regression showing optimal weights differ from likelihood ratios
  - [corpus]: Missing explicit corpus evidence; paper provides theoretical and empirical support
- Break condition: If the decomposition assumption fails or variance term is negligible (very large n), optimal weights converge to likelihood ratios

### Mechanism 2
- Claim: Bi-level optimization enables joint optimization of weights and model parameters
- Mechanism: Weights are optimized on a held-out validation set while model parameters are trained on remaining data. Hyper-gradients computed via implicit function theorem allow gradient-based weight updates.
- Core assumption: Validation set provides unbiased estimate of generalization performance for weight optimization
- Evidence anchors:
  - [section]: "We propose to estimate the inner expectation using a randomly drawn subset of size n_val from the available training data"
  - [section]: "We solve the bi-level optimization problem iteratively" with explicit algorithm description
  - [corpus]: Missing explicit corpus evidence; paper provides detailed algorithmic description
- Break condition: If validation set is too small or unrepresentative, weight optimization may overfit to validation data

### Mechanism 3
- Claim: Optimizing weights improves robustness to hyperparameter choices
- Mechanism: Optimized weights compensate for suboptimal regularization by improving the bias-variance trade-off, making performance less sensitive to L1 penalty selection.
- Core assumption: Improved bias-variance trade-off from optimized weights can offset suboptimal regularization
- Evidence anchors:
  - [section]: "We find that optimizing weights often also improves the metric for which the weights were not optimized"
  - [section]: Figure 4 shows reduced sensitivity to L1 penalty with optimized weights
  - [corpus]: Missing explicit corpus evidence; empirical results support the claim
- Break condition: If regularization is severely suboptimal or dataset is too small for both weight and parameter optimization

## Foundational Learning

- Bias-variance trade-off in statistical learning
  - Why needed here: Core theoretical foundation for understanding why standard importance weighting fails and why optimized weights help
  - Quick check question: What happens to bias and variance as sample size increases when using likelihood ratio weights versus optimized weights?

- Implicit function theorem and hyper-gradient computation
  - Why needed here: Enables gradient-based optimization of weights through model parameters in bi-level optimization
  - Quick check question: How does the implicit function theorem allow computation of ∂θ/∂w when θ is defined as argmin of a loss function?

- Importance weighting theory and distribution shift
  - Why needed here: Provides context for why standard likelihood ratio weighting is used and its limitations
  - Quick check question: Under what conditions does importance weighting with likelihood ratios provide unbiased estimates of test loss?

## Architecture Onboarding

- Component map:
  - Base model (pre-trained DNN) -> Last layer embeddings -> Logistic regression head
  - Data splitter: Training set (n') + Validation set (n_val)
  - Weight optimizer: Gradient descent on validation loss using hyper-gradients
  - Parameter trainer: Standard training on training set with current weights
  - Performance evaluator: Computes weighted average and worst-group accuracy

- Critical path:
  1. Split data into training and validation sets
  2. Initialize weights (typically likelihood ratios)
  3. For each optimization step:
     - Train logistic regression on training set with current weights
     - Compute validation loss and hyper-gradient
     - Update weights using gradient descent
  4. Return best weights found during optimization

- Design tradeoffs:
  - Validation set size vs. training data: Larger validation set gives better weight estimates but reduces training data
  - Weight update frequency: More frequent updates may find better weights but increase computational cost
  - Momentum in weight updates: Helps escape local minima but may overshoot optimal weights

- Failure signatures:
  - Weights converging to extreme values (near 0 or 1): May indicate optimization instability or poor validation set
  - Validation loss increasing during optimization: May indicate overfitting to validation data or learning rate too high
  - No improvement over standard weights: May indicate dataset too large for finite-sample effects to matter or optimization not converging

- First 3 experiments:
  1. Apply optimized weights to GW-ERM on Waterbirds dataset with standard hyperparameters to verify improvement over baseline
  2. Test sensitivity to validation set size by varying n_val/n ratio on CelebA dataset
  3. Evaluate robustness to L1 penalty by running with multiple regularization strengths on MultiNLI dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do optimized importance weights perform when training the entire DNN instead of just the last layer?
- Basis in paper: [explicit] The paper mentions computational costs as a barrier and states "Although last-layer retraining is known to be sufficient... it might be that retraining the whole DNN is the preferred option."
- Why unresolved: The authors explicitly state they focused on last-layer retraining for computational feasibility, leaving the question of whole-network training unexplored.
- What evidence would resolve it: Experiments comparing optimized weights across last-layer, feature extraction, and full fine-tuning approaches on benchmark datasets.

### Open Question 2
- Question: What are the theoretical convergence properties and optimality conditions for the bi-level optimization procedure?
- Basis in paper: [explicit] The authors state "The optimization properties of our method are another potential topic for future research" and "formal convergence guarantees or the conditions under which the optimization in Equation 10 is convex were not investigated here."
- Why unresolved: While the method shows empirical success, the authors explicitly acknowledge they did not investigate theoretical convergence properties or conditions for convexity.
- What evidence would resolve it: Mathematical proofs establishing convergence guarantees and identifying conditions under which the bi-level optimization problem is convex or has unique solutions.

### Open Question 3
- Question: How does optimizing importance weights affect concept bottleneck models with many concepts/groups?
- Basis in paper: [explicit] The authors suggest "The existence of many concepts, and thus groups, leads to a severe reduction in the effective sample size... This is problematic for existing importance weighting techniques and gives our approach of optimizing weights the potential to lead to big improvements."
- Why unresolved: The authors identify concept bottleneck models as a promising direction but explicitly state this requires future investigation.
- What evidence would resolve it: Empirical studies applying optimized weights to concept bottleneck models with varying numbers of concepts, measuring performance changes compared to standard weighting methods.

## Limitations

- Computational overhead: Bi-level optimization requires additional training time and memory compared to standard importance weighting methods
- Validation set dependency: Performance improvements depend on having a representative validation set of sufficient size
- Limited effectiveness for large datasets: As sample size increases, the benefits of optimized weights diminish as variance effects become negligible

## Confidence

- **High confidence** in the theoretical bias-variance decomposition analysis and the bi-level optimization framework
- **Medium confidence** in empirical results due to lack of detailed experimental methodology and hyperparameter specifications
- **Medium confidence** in the claimed robustness improvements, as some results may depend on specific dataset characteristics

## Next Checks

1. Replicate the experimental results on Waterbirds dataset using the described bi-level optimization procedure with varying validation set sizes to verify the claimed improvements
2. Test the sensitivity of optimized weights to initialization by running multiple trials with different starting weight configurations on CelebA dataset
3. Evaluate the computational overhead of the bi-level optimization by measuring training time and memory usage compared to standard importance weighting approaches