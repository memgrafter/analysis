---
ver: rpa2
title: Evidence Contextualization and Counterfactual Attribution for Conversational
  QA over Heterogeneous Data with RAG Systems
arxiv_id: '2412.10571'
source_url: https://arxiv.org/abs/2412.10571
tags:
- evidence
- answer
- table
- questions
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RAGonite enhances RAG systems for conversational QA over heterogeneous
  enterprise data through two key innovations: (1) evidence contextualization by concatenating
  source metadata and surrounding text to retrieved passages, and (2) counterfactual
  attribution using Monte Carlo simulations to provide causal explanations of answer
  generation. Evaluated on the new ConfQuestions benchmark (300 English/German conversational
  questions over 215 Confluence pages), contextualization improved retrieval precision@1
  from 0.398 to 0.528 and answer relevance from 0.388 to 0.529.'
---

# Evidence Contextualization and Counterfactual Attribution for Conversational QA over Heterogeneous Data with RAG Systems

## Quick Facts
- arXiv ID: 2412.10571
- Source URL: https://arxiv.org/abs/2412.10571
- Reference count: 20
- Primary result: Evidence contextualization improves retrieval precision@1 from 0.398 to 0.528 and answer relevance from 0.388 to 0.529 on ConfQuestions benchmark

## Executive Summary
RAGonite enhances RAG systems for conversational QA over heterogeneous enterprise data through two key innovations: (1) evidence contextualization by concatenating source metadata and surrounding text to retrieved passages, and (2) counterfactual attribution using Monte Carlo simulations to provide causal explanations of answer generation. Evaluated on the new ConfQuestions benchmark (300 English/German conversational questions over 215 Confluence pages), contextualization improved retrieval precision@1 from 0.398 to 0.528 and answer relevance from 0.388 to 0.529. Counterfactual attribution achieved 79.9% accuracy in identifying correct evidence sources, outperforming naive similarity-based attribution. The system demonstrates consistent performance across complex questions, multiple languages, and diverse evidence types (passages, lists, tables), establishing effective methods for explainable QA over enterprise wiki data.

## Method Summary
RAGonite addresses the limitations of standard RAG systems by implementing evidence contextualization and counterfactual attribution. Evidence contextualization enriches retrieved passages with metadata (page title, previous heading) and surrounding text (adjacent evidence chunks), creating more self-contained evidence units. Counterfactual attribution uses Monte Carlo simulations to determine each evidence cluster's contribution by measuring how much answers change when that evidence is removed. The system also verbalizes table rows into natural language to improve LLM handling of structured data. RAGonite is evaluated on ConfQuestions, a benchmark of 300 English/German conversational questions over 215 Confluence pages, demonstrating improvements across retrieval precision, answer relevance, and attribution accuracy.

## Key Results
- Evidence contextualization improved retrieval precision@1 from 0.398 to 0.528 and answer relevance from 0.388 to 0.529
- Counterfactual attribution achieved 79.9% accuracy in identifying correct evidence sources
- Verbalization of table rows enabled RAGonite to handle structured evidence effectively
- Improvements were consistent across question complexity (simple/complex), answer source (passage/list/table), and languages (English/German)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualizing evidence with metadata and surrounding text improves both retrieval precision and answer relevance.
- Mechanism: Raw evidence chunks are concatenated with page title, previous heading, and adjacent evidence before/after, creating more self-contained evidence units.
- Core assumption: Information needed to answer questions often spans across discrete evidence boundaries (passages, lists, tables).
- Evidence anchors:
  - [abstract] "contextualization improved retrieval precision@1 from 0.398 to 0.528 and answer relevance from 0.388 to 0.529"
  - [section] "These improvements are systematic with respect to question complexity (simple/complex), answer source (passage/list/table), and language (English/German)"
  - [corpus] Weak evidence - no direct corpus comparison available
- Break condition: If metadata is noisy or misleading, concatenation could introduce irrelevant context that degrades retrieval.

### Mechanism 2
- Claim: Counterfactual attribution provides more accurate evidence attribution than similarity-based methods.
- Mechanism: Remove each evidence cluster, generate counterfactual answers via Monte Carlo simulations, measure similarity drop to determine contribution.
- Core assumption: The degree to which an answer changes when evidence is removed reflects that evidence's causal contribution.
- Evidence anchors:
  - [abstract] "counterfactual attribution using Monte Carlo simulations to provide causal explanations of answer generation"
  - [section] "Results in Table 3 (Column4) show that we consistently reach accuracies close to 80%"
  - [corpus] Weak evidence - no direct corpus comparison available
- Break condition: If LLM generation is highly unstable or if redundant evidence isn't properly clustered, attribution accuracy degrades.

### Mechanism 3
- Claim: Verbalizing table rows as natural language improves both retrieval and answer generation.
- Mechanism: Each table row is linearized into natural language ("Row 2 in Table 1: Member is Alice, and Task is Similarity function...").
- Core assumption: LLMs handle natural language more effectively than structured table formats.
- Evidence anchors:
  - [abstract] "bring tables under RAGonite's scope by linearizing each record (row) via verbalization"
  - [section] "We store each table in a verbalized form [8], which converts structured evidence to a form more amenable to an LLM prompt"
  - [corpus] Weak evidence - no direct corpus comparison available
- Break condition: If verbalization loses critical structural information or if questions require exact table structure.

## Foundational Learning

- Vector embeddings and similarity search
  - Why needed here: Evidence is indexed using embeddings for dense retrieval, and attribution uses embedding similarity to measure answer changes
  - Quick check question: How would you explain the difference between dense retrieval and lexical retrieval to a new team member?

- Contextualization techniques
  - Why needed here: Evidence must be enriched with metadata to overcome the "chunking problem" in RAG systems
  - Quick check question: What are the risks of over-contextualizing evidence, and how would you detect them?

- Counterfactual reasoning
  - Why needed here: Attribution requires understanding what happens when evidence is removed
  - Quick check question: Can you explain why simply measuring similarity between answers isn't sufficient for causal attribution?

## Architecture Onboarding

- Component map:
  Frontend (React/Preact) -> Backend (FastAPI) -> SQLite database -> ChromaDB vector database -> LLM interfaces (GPT-4o, Llama-3.1-8B) -> Evidence pipeline (Preprocessing â†’ Contextualization â†’ Indexing â†’ Retrieval â†’ Generation â†’ Attribution)

- Critical path:
  1. User question â†’ Conversation completion module
  2. Intent-explicit question â†’ Hybrid retrieval (dense + lexical)
  3. Top-k evidences â†’ Answer generation module
  4. Answer + evidences â†’ Counterfactual attribution module
  5. Response + explanation â†’ Frontend

- Design tradeoffs:
  - Contextualization increases index size but improves retrieval quality
  - Monte Carlo simulations for attribution are computationally expensive but provide more accurate explanations
  - Supporting multiple languages adds complexity but increases system utility

- Failure signatures:
  - Low retrieval precision@1: Evidence contextualization may be insufficient or retrieval parameters need tuning
  - Poor attribution accuracy: Monte Carlo iterations may be too few or evidence clustering needs adjustment
  - OOS (out-of-scope) messages too frequent: Retriever may not be finding relevant evidence or question completion may be failing

- First 3 experiments:
  1. Test contextualization impact by running with ALL context vs. NONE context on a small subset of questions
  2. Compare attribution accuracy between naive similarity-based method and counterfactual method on same questions
  3. Validate verbalization by comparing retrieval performance with raw table vs. verbalized table rows

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of counterfactual attribution change when applied to RAG systems with more than 10 retrieved evidences per question?
- Basis in paper: [inferred] The paper uses top-10 evidences for counterfactual attribution experiments and mentions the method "consistently reach accuracies close to 80%" but doesn't explore performance with different numbers of retrieved evidences.
- Why unresolved: The paper focuses on the 10-evidence configuration as the default but doesn't systematically explore how the accuracy of counterfactual attribution scales with different numbers of retrieved evidences.
- What evidence would resolve it: Empirical results showing attribution accuracy across different values of k (e.g., k=5, k=20, k=50) in the RAG system, comparing performance to determine optimal retrieval depth for attribution quality.

### Open Question 2
- Question: What is the impact of different similarity metrics (beyond cosine similarity) on the accuracy of counterfactual attribution?
- Basis in paper: [explicit] The paper states "Cosine similarity between the text embeddings of the answer and the counterfactual answer... was used in Line 8" of Algorithm 1, but doesn't explore alternatives.
- Why unresolved: While cosine similarity is used, the paper doesn't investigate whether other similarity measures (e.g., Jaccard, Euclidean distance, learned metrics) might yield better attribution accuracy.
- What evidence would resolve it: Comparative results showing attribution accuracy using various similarity metrics on the same benchmark, identifying which metric provides the most accurate causal explanations.

### Open Question 3
- Question: How does the choice of embedding model affect the performance of evidence contextualization and retrieval in multilingual settings beyond the BGE vs OpenAI comparison?
- Basis in paper: [explicit] The paper compares BGE and OpenAI embeddings, finding BGE slightly better for retrieval, but doesn't explore other multilingual embedding models.
- Why unresolved: The paper only evaluates two embedding models and doesn't investigate whether models specifically designed for multilingual retrieval (e.g., mBERT, XLM-R) might perform better for contextualization across English and German questions.
- What evidence would resolve it: Systematic comparison of multiple multilingual embedding models (including those specifically trained for cross-lingual retrieval) across the English and German portions of ConfQuestions, measuring both retrieval precision and answer relevance.

## Limitations
- Evaluation relies entirely on a single benchmark (ConfQuestions) created from Confluence pages, limiting generalizability to other enterprise data sources
- Counterfactual attribution mechanism requires computationally expensive Monte Carlo simulations (reportedly 1,000 iterations), which may not scale well for production systems
- Evidence clustering approach using DBSCAN with fixed parameters (ğœ–=0.005, ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ =2) may not generalize across different document structures or domains

## Confidence

- **High Confidence**: The retrieval improvements from contextualization (precision@1: 0.398 â†’ 0.528, answer relevance: 0.388 â†’ 0.529) are well-supported by the evaluation data and show consistent gains across question complexity, evidence types, and languages.
- **Medium Confidence**: The counterfactual attribution accuracy (79.9%) appears promising but is validated only against the ConfQuestions benchmark without external comparison.
- **Low Confidence**: Claims about performance across "complex questions" and "diverse evidence types" lack quantitative breakdown in the abstract. The system's behavior with non-wiki enterprise data (e.g., PDFs, emails) remains untested.

## Next Checks

1. **Cross-domain validation**: Evaluate RAGonite on enterprise data from different sources (PDFs, emails, code repositories) to test generalizability beyond Confluence pages.

2. **Computational overhead analysis**: Measure the actual runtime and resource consumption of counterfactual attribution across varying numbers of Monte Carlo iterations to determine practical scalability limits.

3. **Ablation study on evidence clustering**: Systematically vary DBSCAN parameters (ğœ–, ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ ) and analyze the impact on both retrieval performance and attribution accuracy to identify optimal clustering strategies.