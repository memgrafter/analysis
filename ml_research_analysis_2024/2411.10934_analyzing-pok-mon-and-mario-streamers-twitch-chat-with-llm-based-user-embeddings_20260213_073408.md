---
ver: rpa2
title: "Analyzing Pok\xE9mon and Mario Streamers' Twitch Chat with LLM-based User\
  \ Embeddings"
arxiv_id: '2411.10934'
source_url: https://arxiv.org/abs/2411.10934
tags:
- chatters
- twitch
- stream
- chatter
- messages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses large language models to generate user embeddings
  for Twitch chatters, clusters them using affinity propagation, and identifies common
  chatter categories across three gaming streams. It finds that all streamers share
  supportive viewers and emoji/reaction senders, with two also having repetitive message
  spammers.
---

# Analyzing Pokémon and Mario Streamers' Twitch Chat with LLM-based User Embeddings
## Quick Facts
- arXiv ID: 2411.10934
- Source URL: https://arxiv.org/abs/2411.10934
- Authors: Mika Hämäläinen; Jack Rueter; Khalid Alnajjar
- Reference count: 9
- Key outcome: LLM-based embeddings and clustering identify common chatter categories across three gaming streams, finding supportive viewers and emoji senders in all, with two having repetitive spammers

## Executive Summary
This study presents a novel digital humanities method for analyzing Twitch chat dynamics using large language model embeddings and clustering algorithms. The researchers analyzed chat data from three gaming streamers (SmallAnt, DougDoug, and PointCrow) to identify distinct categories of chatters based on their messaging patterns. By representing each user as a semantic embedding of their concatenated messages and applying affinity propagation clustering, the method automatically discovers chatter types without requiring predefined categories. The approach reveals that all three streamers share supportive viewers and emoji/reaction senders as common chatter categories, with two also having repetitive message spammers.

## Method Summary
The researchers collected Twitch chat messages using Selenium to monitor the chatbox during live streams, saving the data for analysis. They processed the data by concatenating messages from users who sent at least 20 messages, creating user embeddings using the PaLM-2 text-embedding-004 model, and clustering these embeddings with affinity propagation. The clustering algorithm automatically determined the optimal number of categories based on semantic similarity between users. After clustering, the researchers manually merged semantically similar clusters to improve interpretability, resulting in final chatter categories for each streamer that were then compared to identify common patterns.

## Key Results
- All three streamers shared supportive viewers and emoji/reaction senders as common chatter categories
- Two streamers (SmallAnt and PointCrow) also had repetitive message spammers as a distinct category
- The method successfully identified both universal chatter types and streamer-specific categories across gaming communities

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLM-generated embeddings capture semantic similarity of chatter messages better than simple keyword or frequency-based representations.
- Mechanism: The PaLM-2 text-embedding-004 model transforms concatenated chat messages into dense vector representations in a semantic space, allowing messages with similar meaning to be closer in that space.
- Core assumption: Semantic similarity between messages correlates with meaningful categories of chatters.
- Evidence anchors:
  - [abstract] "We present a novel digital humanities method for representing our Twitch chatters as user embeddings created by a large language model (LLM)."
  - [section] "These user embeddings serve as mathematical representation of the semantics of what each user was chatting about."
  - [corpus] Weak evidence - corpus mentions multimodal generation and moderation but no direct evidence for semantic capture quality in Twitch context.
- Break condition: If the LLM fails to capture context-specific language (emotes, abbreviations) common in Twitch chat, embeddings will not reflect true semantic similarity.

### Mechanism 2
- Claim: Affinity propagation clustering can automatically discover the optimal number of chatter categories without pre-specifying cluster counts.
- Mechanism: The algorithm uses an affinity matrix based on cosine similarity between embeddings to iteratively identify exemplar chatters and group similar ones around them.
- Core assumption: Chatters with similar semantic patterns will have embeddings that form naturally distinct clusters.
- Evidence anchors:
  - [section] "We use affinity propagation (Frey and Dueck, 2007). It takes in an affinity matrix, which shows how close each embedding is to other embeddings, and it will automatically find an optimal number of clusters based on the affinities provided to the algorithm."
  - [section] "Some chatters end up clustered into their own clusters. We remove all clusters that have only one chatter, because we are more interested in the overall tendencies of chatter categories, not in individual deviant chatters."
  - [corpus] No direct evidence for affinity propagation performance in this specific domain.
- Break condition: If semantic space has noise or embeddings are too similar, affinity propagation may merge distinct chatter types or create too many small clusters.

### Mechanism 3
- Claim: Manual merging of clusters improves the interpretability of the final chatter categories.
- Mechanism: Researchers identify semantically similar clusters and combine them, reducing redundancy while preserving meaningful distinctions.
- Core assumption: Some clusters capture overlapping behaviors that should be grouped for clearer interpretation.
- Evidence anchors:
  - [section] "On a closer inspection, we found that some of the clusters included mutually similar messages, so we proceeded to merge some clusters manually. This resulted in 5 cluster for SmallAnt, 4 for PointCrow and 6 for DougDoug."
  - [section] "Cluster name Size Characteristics" tables show merged categories like "Anime and gaming enthusiasts" and "Supportive viewers."
  - [corpus] No corpus evidence for effectiveness of manual merging.
- Break condition: If manual merging introduces subjective bias, it may obscure meaningful distinctions or create overly broad categories.

## Foundational Learning
- Concept: Semantic similarity in vector space models
  - Why needed here: Understanding how LLM embeddings represent meaning is essential for interpreting clustering results.
  - Quick check question: If two chat messages have embeddings with cosine similarity of 0.9, are they more or less semantically similar than messages with similarity 0.3?

- Concept: Clustering algorithms and affinity propagation
  - Why needed here: The method relies on affinity propagation to automatically discover chatter categories without predefined cluster counts.
  - Quick check question: What key difference distinguishes affinity propagation from k-means clustering?

- Concept: Manual data annotation and category refinement
  - Why needed here: The study uses manual merging to improve cluster interpretability, requiring understanding of qualitative analysis principles.
  - Quick check question: Why might researchers choose to manually merge clusters after automated clustering?

## Architecture Onboarding
- Component map: Data collection script using Selenium -> LLM embedding service (PaLM-2 via VertexAI) -> Affinity propagation clustering algorithm -> Manual cluster merging and analysis
- Critical path: Data collection → Embedding generation → Clustering → Manual refinement → Analysis
- Design tradeoffs: (1) Using LLM embeddings provides semantic richness but requires API access and computational resources; (2) Affinity propagation automatically determines cluster count but may be sensitive to embedding quality; (3) Manual merging improves interpretability but introduces potential bias
- Failure signatures: (1) Too many singleton clusters indicates embeddings may not capture shared patterns; (2) All chatters clustering into one group suggests embeddings lack discriminative power; (3) Inconsistent manual merging across researchers indicates subjective interpretation issues
- First 3 experiments:
  1. Test embedding quality by clustering a small set of known-similar messages and verifying they group together
  2. Run affinity propagation with different preference values to see how cluster count changes
  3. Compare results using different embedding models (e.g., text-embedding-003 vs text-embedding-004) on the same dataset

## Open Questions the Paper Calls Out
- How stable are the identified chatter categories across multiple streams by the same streamer? The authors suggest this would be interesting to investigate, as their current study only analyzed one stream per streamer.
- How would including the content of the actual stream (video) in the analysis affect the identification of chatter categories? The authors mention this as a potential extension since their study was limited to chat messages only.
- How would the chatter categories differ if a larger sample size (more streamers and streams) were analyzed? The authors note their sample size is relatively small and findings may not hold for every Twitch streamer.

## Limitations
- Relatively small sample size covering only one stream from three streamers, limiting generalizability
- Manual cluster merging process introduces potential researcher bias without reported inter-rater reliability measures
- Assumes semantic similarity captured by LLM embeddings meaningfully reflects chatter categories without ground truth validation

## Confidence
- High Confidence: The methodological framework using LLM embeddings with affinity propagation clustering is technically sound and reproducible. The identification of supportive viewers and emoji/reaction senders across all three streams appears robust given consistent findings.
- Medium Confidence: The specific chatter categories identified for each streamer (e.g., "repetitive message spammers" for two streams) are reasonable but may be influenced by manual merging decisions and the particular streamers studied.
- Low Confidence: Generalizability claims about Twitch chat dynamics beyond these three specific gaming communities lack sufficient evidence given the narrow sample.

## Next Checks
1. Manually label a subset of chat messages (e.g., 100-200) from each streamer and measure clustering accuracy against human annotations to verify that semantic embeddings capture meaningful categories.
2. Apply the same methodology to Twitch streams from different content categories (e.g., music, art, IRL) to test whether the identified chatter categories generalize beyond gaming communities.
3. Repeat the analysis using alternative embedding models (e.g., OpenAI's text-embedding-ada-002 or smaller open-source models) to assess whether results depend critically on the specific PaLM-2 model choice.