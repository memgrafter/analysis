---
ver: rpa2
title: 'Numerical Approximation Capacity of Neural Networks with Bounded Parameters:
  Do Limits Exist, and How Can They Be Measured?'
arxiv_id: '2409.16697'
source_url: https://arxiv.org/abs/2409.16697
tags:
- networks
- approximation
- neural
- nsdim
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the numerical approximation capacity of
  neural networks with bounded parameters. The authors introduce the concept of Numerical
  Span Dimension (NSDim) to quantify the approximation capacity limit of a family
  of networks both theoretically and practically.
---

# Numerical Approximation Capacity of Neural Networks with Bounded Parameters: Do Limits Exist, and How Can They Be Measured?

## Quick Facts
- arXiv ID: 2409.16697
- Source URL: https://arxiv.org/abs/2409.16697
- Reference count: 3
- Primary result: Neural networks with bounded parameters have finite numerical approximation capacity, measured by Numerical Span Dimension (NSDim)

## Executive Summary
This paper investigates the numerical approximation capacity of neural networks when parameters are bounded, challenging the conventional wisdom from universal approximation theorems. The authors introduce Numerical Span Dimension (NSDim) as a measure of the finite-dimensional approximation space that emerges under bounded parameters. Through theoretical analysis and numerical experiments, they demonstrate that even though universal approximation is theoretically possible, practical numerical scenarios impose fundamental limits on approximation capacity, with NSDim being small and positively correlated with parameter space size.

## Method Summary
The paper introduces the concept of ϵ outer measure to quantify approximation capacity in function spaces under finite numerical tolerance. It analyzes the Hidden Layer Output Matrix to compute NSDim through singular value decomposition, showing that bounded parameters lead to exponentially decaying singular values. The authors compare backpropagation networks with random parameter networks (like ELM) under both finite and infinite width conditions, using measure-theoretic concepts to establish theoretical relationships between different network architectures.

## Key Results
- Neural networks with bounded parameters can only approximate functions in a finite-dimensional space, regardless of theoretical universal approximation
- NSDim is very small for all bounded parameter spaces and scales nearly linearly with parameter space size
- BP networks and RP networks (like ELM) approximate each other with arbitrary precision under infinite width conditions
- Increasing depth increases NSDim more rapidly than increasing width for bounded parameter spaces

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bounded parameter spaces create finite-dimensional approximation spaces for neural networks with analytic activation functions.
- **Mechanism**: The span of basis functions becomes finite-dimensional because singular values of the Hidden Layer Output Matrix decay exponentially under bounded parameters, leaving only a small number of numerically non-zero singular values.
- **Core assumption**: Activation function is analytic and parameter space is bounded.
- **Evidence anchors**: Theoretical proof that NSDim < M for bounded analytic activations; exponential decay of singular values observed in numerical tests.
- **Break condition**: Mechanism fails for non-analytic activations (e.g., ReLU) or unbounded parameters.

### Mechanism 2
- **Claim**: RP networks (ELM) can approximate BP networks under infinite width due to denseness of random parameter sampling.
- **Mechanism**: Under infinite width, random parameter sampling becomes dense enough in the compact parameter space to approximate the nonlinear optimization of BP networks through linear optimization.
- **Core assumption**: Activation function is continuous and parameter space is compact.
- **Evidence anchors**: Theoretical proof of approximation equivalence under infinite width; numerical evidence of similar performance.
- **Break condition**: Equivalence fails for finite width networks where NSDim becomes bounded.

### Mechanism 3
- **Claim**: ϵ outer measure provides practical quantification of approximation capacity under finite numerical tolerance.
- **Mechanism**: Transforms infinite-dimensional function space measurement into finite counting problem via covering number approach, computable through singular value analysis.
- **Core assumption**: Numerical tolerance ϵ is finite and meaningful for practical applications.
- **Evidence anchors**: Introduction of ϵ outer measure definition; application to quantify NSDim in numerical experiments.
- **Break condition**: Measure becomes meaningless if ϵ approaches zero, requiring infinite counting.

## Foundational Learning

- **Concept**: Universal Approximation Theorem and its limitations under practical constraints
  - Why needed here: Understanding the gap between theoretical universal approximation and practical limitations is central to the paper's contribution.
  - Quick check question: Why does the Universal Approximation Theorem fail to hold in practice when neural network parameters are bounded?

- **Concept**: Measure theory and outer measures in function spaces
  - Why needed here: The paper uses measure-theoretic concepts to quantify approximation capacity in infinite-dimensional function spaces.
  - Quick check question: How does the ϵ outer measure differ from traditional Lebesgue measure when applied to function spaces?

- **Concept**: Matrix rank and singular value decomposition in neural networks
  - Why needed here: The paper uses singular value analysis of the Hidden Layer Output Matrix to determine the Numerical Span Dimension.
  - Quick check question: Why does the number of numerically non-zero singular values of the Hidden Layer Output Matrix indicate the approximation capacity of a neural network?

## Architecture Onboarding

- **Component map**: Theoretical framework (Universal approximation theorems, measure theory) -> Computational framework (Matrix analysis, SVD) -> Key concepts (NSDim, ϵ outer measure, Hidden Layer Output Matrix)

- **Critical path**:
  1. Define bounded parameter space and analytic activation function
  2. Prove that RP networks approximate BP networks under infinite width
  3. Introduce ϵ outer measure to quantify finite approximation capacity
  4. Compute NSDim through singular value analysis
  5. Validate findings through numerical experiments

- **Design tradeoffs**:
  - Theoretical rigor vs. practical applicability: Balances mathematical proofs with numerical experiments
  - Generality vs. specificity: Results apply to analytic activation functions but may not extend to non-analytic cases like ReLU
  - Precision vs. computational feasibility: NSDim provides capacity measure but requires computing singular values of potentially large matrices

- **Failure signatures**:
  - NSDim appears infinite or very large for non-analytic activation functions
  - Singular values of Hidden Layer Output Matrix do not decay exponentially
  - Numerical experiments show no correlation between network width and approximation accuracy
  - Theoretical proofs fail when parameter bounds are removed

- **First 3 experiments**:
  1. Compute NSDim for a shallow network with Tanh activation and bounded parameters (R=1), varying network width from 10 to 1000 neurons
  2. Compare singular value distributions of Hidden Layer Output Matrix for networks with bounded vs. unbounded parameters
  3. Test approximation accuracy of ELM vs. BP networks for functions requiring high NSDim (complex, high-frequency functions)

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the exact relationship between NSDim and the size of the NPSpace for different activation functions?
  - Basis in paper: [explicit] The paper states NSDim is positively correlated (nearly linearly) with NPSpace size, but the exact mathematical relationship is not provided.
  - Why unresolved: Paper provides numerical evidence of linear relationship but lacks exact formula or mathematical proof.
  - What evidence would resolve it: Rigorous mathematical proof or empirical study deriving exact formula for relationship across various activation functions.

- **Open Question 2**: How does NSDim scale with network depth for different activation functions and architectures?
  - Basis in paper: [explicit] Paper shows increasing depth increases NSDim more rapidly than width, but exact scaling behavior is not analyzed.
  - Why unresolved: Limited numerical tests for shallow networks (1-3 hidden layers), no exploration of deeper architectures or different activation functions.
  - What evidence would resolve it: Comprehensive study analyzing NSDim scaling with depth for various activation functions, network architectures, and widths.

- **Open Question 3**: Is there a fundamental limit to the approximation capability of neural networks with bounded parameters, and how does this limit relate to NSDim?
  - Basis in paper: [inferred] Paper introduces NSDim as approximation capacity measure but doesn't explicitly discuss fundamental limits to this capacity.
  - Why unresolved: Focuses on measuring NSDim without exploring theoretical upper bounds on approximation capability for bounded parameter networks.
  - What evidence would resolve it: Theoretical proof or empirical study establishing whether fundamental limit exists and how it relates to NSDim.

## Limitations

- Theoretical results primarily apply to analytic activation functions and may not extend to non-analytic cases like ReLU
- Practical implications of NSDim for model selection and regularization in real-world applications need further validation
- Numerical experiments are limited in scope and don't explore deep architectures or diverse activation functions

## Confidence

- **High confidence**: Theoretical proof that bounded parameters create finite-dimensional approximation spaces for analytic activations
- **Medium confidence**: Empirical correlation between NSDim and NP space size observed in numerical experiments
- **Low confidence**: Practical implications of NSDim for model selection and regularization in real-world scenarios

## Next Checks

1. **Test non-analytic activations**: Compute NSDim for ReLU networks with bounded parameters to determine if theoretical results extend beyond analytic functions

2. **Cross-validation with standard metrics**: Compare NSDim-based network selection against traditional metrics like validation loss and cross-validation error on real datasets

3. **Scaling analysis**: Investigate how NSDim scales with network depth and whether depth can compensate for width limitations in bounded parameter spaces