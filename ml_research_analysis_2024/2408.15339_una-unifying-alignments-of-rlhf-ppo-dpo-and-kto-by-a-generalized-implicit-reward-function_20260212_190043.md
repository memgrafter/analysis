---
ver: rpa2
title: 'UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit
  Reward Function'
arxiv_id: '2408.15339'
source_url: https://arxiv.org/abs/2408.15339
tags:
- reward
- rlhf
- implicit
- function
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UNA (Unified Alignment), a framework that\
  \ unifies RLHF/PPO, DPO, and KTO into a single supervised learning objective. The\
  \ key idea is proving that the optimal policy under RLHF\u2019s objective can be\
  \ induced by a generalized implicit reward function r(x,y) = \u03B2 log[\u03C0\u03B8\
  (y|x)/\u03C0ref(y|x)] + f(x) + c."
---

# UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function

## Quick Facts
- arXiv ID: 2408.15339
- Source URL: https://arxiv.org/abs/2408.15339
- Authors: Zhichao Wang; Bin Bi; Can Huang; Shiva Kumar Pentyala; Zixu James Zhu; Sitaram Asur; Na Claire Cheng
- Reference count: 40
- One-line primary result: UNA unifies RLHF/PPO, DPO, and KTO into a single supervised learning framework that improves stability and performance

## Executive Summary
UNA introduces a unified alignment framework that transforms the unstable RL fine-tuning stage of RLHF into a stable supervised learning problem. The key insight is proving that the optimal policy under RLHF's objective can be represented by a generalized implicit reward function, allowing all three major alignment methods (RLHF/PPO, DPO, and KTO) to be expressed as minimizing the difference between implicit and explicit rewards. Experiments on Mistral-7B demonstrate UNA outperforms traditional methods on multiple LLM leaderboards while using less memory and training faster.

## Method Summary
UNA replaces the reinforcement learning fine-tuning stage with supervised learning by minimizing the difference between an implicit reward (derived from the policy) and an explicit reward (from human labels or reward models). The framework supports pairwise, binary, and score-based feedback through appropriate loss functions. For pairwise data, UNA is mathematically equivalent to DPO; for binary feedback, it improves upon KTO; and for score-based feedback, it uses mean squared error. The approach uses LoRA fine-tuning on Mistral-7B with the HelpSteer2 dataset and Ray2333/GRM-Llama3.2-3B-rewardmodel-ft as the reward model.

## Key Results
- UNA outperforms RLHF, DPO, and KTO on multiple LLM leaderboards (bbh, gpqa, mmlu-pro, musr, ifeval, math-hard)
- UNA achieves better stability with lower variance in training loss compared to traditional RLHF
- UNA reduces memory usage by eliminating the need for a separate value model in RLHF fine-tuning
- Training time is faster due to the supervised learning approach rather than reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1
The optimal policy under RLHF can be represented by a generalized implicit reward function r(x,y) = β log[πθ(y|x)/πref(y|x)]. By applying the log-sum inequality and setting KL divergence to zero, the RLHF objective transforms such that the optimal policy is achieved when reward is proportional to the log ratio of current and reference policies. Core assumption: reference policy is fixed and known. Break condition: if reference policy is not fixed or RLHF objective is incorrectly specified.

### Mechanism 2
UNA unifies different alignment methods by minimizing the difference between implicit and explicit rewards using supervised learning. This replaces the unstable RL fine-tuning stage with a stable optimization problem. Core assumption: explicit reward function is well-calibrated and provides accurate feedback. Break condition: if explicit reward function is poorly calibrated or provides noisy feedback.

### Mechanism 3
UNA handles different feedback types by defining appropriate loss functions: pairwise data uses DPO-equivalent loss, binary feedback uses binary cross-entropy or MSE, and score-based feedback uses MSE. Core assumption: feedback data is correctly labeled and representative. Break condition: if feedback data is not correctly labeled or representative.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the foundation for understanding how large language models are aligned with human preferences. UNA builds upon and unifies RLHF with other alignment methods.
  - Quick check question: What are the two main stages of RLHF, and what are their purposes?

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed here: DPO simplifies RLHF by directly optimizing the policy to match human preferences without an explicit reward model. UNA extends DPO to handle different types of feedback data.
  - Quick check question: How does DPO differ from RLHF in terms of the optimization objective?

- **Concept**: Log-sum inequality
  - Why needed here: The log-sum inequality is used in the mathematical proof to derive the implicit reward function from the RLHF objective.
  - Quick check question: State the log-sum inequality and explain its role in the proof.

## Architecture Onboarding

- **Component map**: Policy model (πθ) -> Reference policy (πref) -> Explicit reward function (rφ) -> Implicit reward function (rθ) -> Loss function (g)

- **Critical path**:
  1. Generate responses using current policy
  2. Calculate implicit rewards based on policy and reference policy
  3. Obtain explicit rewards from human labels, reward models, or LLMs
  4. Minimize difference between implicit and explicit rewards using defined loss function
  5. Update policy based on minimized loss

- **Design tradeoffs**:
  - Memory usage: UNA reduces memory burden by eliminating need for separate value model in RLHF
  - Training stability: UNA transforms unstable RL fine-tuning into stable supervised learning
  - Feedback efficiency: UNA handles different feedback types, making it more flexible than DPO or KTO

- **Failure signatures**:
  - Poor performance on downstream tasks: alignment process not effective
  - High variance in training loss: explicit reward function not well-calibrated
  - Slow convergence: loss function not appropriately defined for feedback type

- **First 3 experiments**:
  1. Implement UNA for pairwise feedback data and compare performance to DPO on standard benchmark
  2. Test UNA's ability to handle binary feedback data and compare performance to KTO on binary feedback dataset
  3. Evaluate UNA's performance on score-based feedback data and compare to DPO and KTO on score-based feedback dataset

## Open Questions the Paper Calls Out

1. What is the optimal form of the function f(x) in the generalized implicit reward r(x, y) = β log[πθ(y|x)/πref(y|x)] + f(x) + c, and how does its choice affect alignment performance across different tasks and domains?

2. How does UNA's performance scale when applied to larger models (e.g., beyond 7B parameters) where the "alignment tax" is reduced, and what modifications might be needed for industrial-scale deployment?

3. Can the implicit reward function r(x, y) = β log[πθ(y|x)/πref(y|x)] + f(x) + c be estimated or approximated without requiring a separate reward model, thereby reducing memory burden further?

## Limitations

- Mathematical proofs rely on assumptions that may not hold in practice, particularly regarding fixed reference policy
- Limited empirical evidence with experiments only on Mistral-7B and HelpSteer2 dataset
- Critical implementation details missing, including specific normalization functions and exact procedures for implicit reward calculation
- Evaluation metrics and statistical significance of improvements not clearly reported

## Confidence

- **High Confidence**: Theoretical framework of UNA as supervised learning approach is well-founded and mathematically coherent
- **Medium Confidence**: Claims about handling different feedback types and improvements over baselines are supported by theory but lack extensive empirical validation
- **Low Confidence**: Claims about superior stability, lower memory usage, and faster training need more rigorous experimental validation across diverse settings

## Next Checks

1. **Ablation Study on Feedback Types**: Conduct comprehensive ablation study testing UNA's performance across multiple feedback types on diverse datasets and model architectures. Compare performance variations when using each feedback type individually and in combination.

2. **Stability and Memory Usage Analysis**: Design controlled experiments to measure training stability (variance in loss curves, convergence rates) and memory usage across different batch sizes and model scales. Compare these metrics systematically against RLHF, DPO, and KTO implementations.

3. **Generalization Across Architectures**: Test UNA's performance on multiple model architectures (different sizes, different base models) and datasets to validate generalizability. Include models with different pretraining objectives and architectural differences to assess robustness.