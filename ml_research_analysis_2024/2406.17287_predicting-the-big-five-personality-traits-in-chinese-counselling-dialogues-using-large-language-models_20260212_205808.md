---
ver: rpa2
title: Predicting the Big Five Personality Traits in Chinese Counselling Dialogues
  Using Large Language Models
arxiv_id: '2406.17287'
source_url: https://arxiv.org/abs/2406.17287
tags:
- traits
- ocean
- personality
- llms
- counseling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of Large Language Models
  (LLMs) to predict Big Five personality traits from Chinese counseling dialogues.
  The proposed framework integrates role-play and questionnaire-based prompting strategies
  to condition LLMs on counseling sessions, simulating client responses to the Big
  Five Inventory.
---

# Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models

## Quick Facts
- arXiv ID: 2406.17287
- Source URL: https://arxiv.org/abs/2406.17287
- Reference count: 40
- Primary result: 130.95% improvement in personality prediction validity over baseline using Llama3-8B fine-tuned with DPO and SFT

## Executive Summary
This study investigates the potential of Large Language Models (LLMs) to predict Big Five personality traits from Chinese counseling dialogues. The proposed framework integrates role-play and questionnaire-based prompting strategies to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory. Evaluated on 853 real-world counseling sessions, the approach demonstrates a significant correlation between LLM-predicted and actual Big Five traits. Ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Fine-tuning a Llama3-8B model with Direct Preference Optimization and Supervised Fine-Tuning achieves a 130.95% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94% in personality prediction validity.

## Method Summary
The study collected 853 real-world counseling sessions with corresponding BFI-2 personality assessments from 83 clients. The methodology employs role-play and questionnaire-based prompting strategies, where LLMs simulate client responses to Big Five Inventory items based on counseling dialogue context. The Llama3-8B model was fine-tuned using Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) to align it with the task of predicting OCEAN traits. The framework achieves accurate predictions using only 30% of session content, demonstrating both efficiency and effectiveness in personality assessment from counseling dialogues.

## Key Results
- Llama3-8B fine-tuned with DPO and SFT achieved 130.95% improvement in prediction validity over baseline
- The fine-tuned model surpassed Qwen1.5-110B by 36.94% in personality prediction validity
- Accurate OCEAN trait prediction achieved using only 30% of session content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-play and questionnaire-based prompting improve prediction accuracy by decomposing complex personality assessment into simpler, structured tasks.
- Mechanism: The framework assigns the LLM a specific role (client) and uses structured BFI items as prompts, simplifying the task of personality prediction from free-form dialogue to answering standardized questions.
- Core assumption: LLMs can effectively simulate human responses to structured questionnaires when properly conditioned with role-play and context.
- Evidence anchors:
  - [abstract] "Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory."
  - [section 4.2] "Adding role-play contributed minimally, while questionnaires showed a slight improvement, indicating that decomposing the task into simpler items is beneficial. Combining role-play and questionnaires significantly improved prediction validity across all OCEAN traits."
  - [corpus] Weak evidence - corpus suggests related work on LLMs simulating personality traits but lacks direct evidence for the specific decomposition mechanism.
- Break condition: If the LLM fails to maintain role consistency or misinterprets the structured questionnaire format, prediction accuracy will degrade significantly.

### Mechanism 2
- Claim: Aligning the LLM with the task of predicting OCEAN traits through Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) enhances both prediction validity and efficiency.
- Mechanism: The fine-tuning process uses preference-based selection of model-generated responses, rewarding accurate trait predictions and penalizing inaccurate ones, thereby aligning the model's outputs with the desired prediction task.
- Core assumption: Preference-based fine-tuning can effectively guide LLMs to produce more accurate personality trait predictions.
- Evidence anchors:
  - [abstract] "By aligning the Llama3-8B model with trait prediction through Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT), our fine-tuned lightweight model exhibits a 130.95% improvement in prediction validity, surpassing the state-of-the-art Qwen1.5-110B by 36.94% in personality prediction validity."
  - [section 4.4] "Results show that DPO with SFT achieved an average PCC of 0.582, outperforming DPO without SFT by 0.019."
  - [corpus] Weak evidence - corpus mentions related work on DPO and SFT but lacks specific evidence for this personality prediction application.
- Break condition: If the preference ranking is noisy or the fine-tuning data is unrepresentative, the alignment process may not improve prediction accuracy.

### Mechanism 3
- Claim: Using only 30% of the counseling session context is sufficient for accurate OCEAN trait prediction, as the LLM can extract essential personality indicators from a limited dialogue sample.
- Mechanism: The LLM identifies key personality indicators in the early portion of the dialogue, such as emotional states and social behaviors, which are representative of the client's overall personality.
- Core assumption: Early dialogue patterns are representative of the client's overall personality traits.
- Evidence anchors:
  - [abstract] "Remarkably, our approach achieves accurate OCEAN trait prediction using only 30% of session content."
  - [section 4.2] "As shown in Fig. 2, 30% of the session context is the critical threshold. Below this threshold, prediction validity is unstable and not significant; above it, validity and significance stabilize."
  - [corpus] Weak evidence - corpus lacks direct evidence for the 30% context sufficiency claim, though it mentions related work on dialogue analysis.
- Break condition: If the early dialogue is not representative of the client's personality (e.g., due to initial nervousness or guardedness), prediction accuracy will suffer.

## Foundational Learning

- Concept: Item Response Theory (IRT)
  - Why needed here: Understanding IRT is crucial for interpreting how personality questionnaires like the BFI work and why they are effective tools for personality assessment.
  - Quick check question: What is the primary purpose of Item Response Theory in the context of personality assessment?

- Concept: Large Language Model (LLM) prompting strategies
  - Why needed here: Effective prompting strategies, such as role-play and questionnaire-based prompting, are essential for conditioning LLMs to perform the personality prediction task accurately.
  - Quick check question: How do role-play and questionnaire-based prompting strategies improve the accuracy of LLM predictions?

- Concept: Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT)
  - Why needed here: DPO and SFT are key techniques for aligning LLMs with specific tasks, such as predicting OCEAN traits, by fine-tuning the model on preference-based data.
  - Quick check question: What is the role of DPO and SFT in improving the accuracy of LLM predictions for personality traits?

## Architecture Onboarding

- Component map: Data Collection -> Preprocessing -> Prompting Strategy -> LLM Conditioning -> Evaluation
- Critical path: 1. Data Collection and Preprocessing 2. Prompting Strategy Implementation 3. LLM Conditioning and Fine-tuning 4. Prediction and Evaluation
- Design tradeoffs:
  - Model Size vs. Efficiency: Larger models (e.g., Qwen1.5-110B) may offer better performance but require more computational resources, while smaller models (e.g., Llama3-8B) are more efficient but may have lower accuracy.
  - Context Granularity: Using more context (e.g., 100% of session) may improve accuracy but increase computational cost, while using less context (e.g., 30%) balances accuracy and efficiency.
  - Role Assignment: Assigning specific roles (e.g., client, counselor, observer) may improve accuracy but adds complexity to the prompting strategy.
- Failure signatures:
  - Low PCC values indicate poor correlation between predicted and actual OCEAN traits.
  - High MAE values suggest significant errors in trait prediction.
  - Inconsistent predictions across different runs may indicate instability in the model or prompting strategy.
- First 3 experiments:
  1. Evaluate baseline LLM performance without role-play or questionnaire prompting to establish a performance benchmark.
  2. Implement role-play prompting to assess its impact on prediction accuracy.
  3. Combine role-play and questionnaire-based prompting to determine if the combination further improves accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform when predicting personality traits in cross-cultural and multilingual counseling contexts?
- Basis in paper: [inferred] The paper acknowledges that the current dataset is geographically and linguistically homogeneous, and suggests that future studies should include more diverse populations to validate the framework's effectiveness in cross-cultural and multilingual settings.
- Why unresolved: The study's data is limited to Chinese counseling dialogues, and there is no evaluation of the model's performance in other cultural or linguistic contexts.
- What evidence would resolve it: Conducting experiments with counseling dialogues from diverse cultural and linguistic backgrounds and comparing the model's prediction accuracy across these contexts.

### Open Question 2
- Question: What is the impact of anonymization on the model's performance, and can advanced data protection techniques mitigate this impact?
- Basis in paper: [explicit] The paper mentions that anonymization is crucial for protecting client confidentiality but may slightly diminish the specificity of counseling dialogues, potentially impacting the LLMs' performance. It notes a performance reduction of approximately 6% due to anonymization.
- Why unresolved: While the paper acknowledges the impact of anonymization, it does not explore alternative data protection techniques that could preserve privacy without significantly compromising model performance.
- What evidence would resolve it: Evaluating the model's performance using advanced data protection techniques like federated learning and comparing the results with those obtained using traditional anonymization methods.

### Open Question 3
- Question: How do different alignment strategies affect the model's ability to predict personality traits accurately?
- Basis in paper: [explicit] The paper discusses the use of Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) for aligning the LLM with the task of predicting OCEAN traits. It notes that DPO with SFT achieved better performance than DPO without SFT.
- Why unresolved: The paper does not explore other alignment strategies or compare the effectiveness of different approaches in detail.
- What evidence would resolve it: Conducting experiments with various alignment strategies, such as Reinforcement Learning from Human Feedback (RLHF) or other preference optimization techniques, and comparing their impact on prediction accuracy.

## Limitations
- Sample size of 853 counseling sessions from only 83 clients limits generalizability across diverse populations
- The exact implementation details of DPO and SFT fine-tuning are not fully specified
- Results rely entirely on self-reported BFI-2 assessments as ground truth, which may contain inherent biases

## Confidence
High confidence: The general approach of using role-play and questionnaire-based prompting for LLM personality prediction is well-supported by the experimental results.
Medium confidence: The reported numerical improvements (130.95% and 36.94%) are likely accurate within the experimental setup, but their practical significance and generalizability require further validation.
Low confidence: The claim that 30% of session content is sufficient for accurate prediction needs external validation across different counseling contexts.

## Next Checks
1. Conduct external validation with an independent dataset of counseling dialogues from different counseling centers to test the generalizability of the prediction model across diverse populations and counseling styles.
2. Implement a blind validation study where counselors independently assess the same clients' personalities using standardized measures, then compare these assessments against both the LLM predictions and the original BFI-2 results to evaluate relative accuracy.
3. Perform sensitivity analysis on the 30% context threshold by systematically varying the proportion of dialogue used (10%, 20%, 30%, 40%, 50%) across multiple runs to establish confidence intervals for prediction accuracy at each threshold.