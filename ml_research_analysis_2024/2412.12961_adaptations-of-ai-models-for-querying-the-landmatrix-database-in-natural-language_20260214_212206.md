---
ver: rpa2
title: Adaptations of AI models for querying the LandMatrix database in natural language
arxiv_id: '2412.12961'
source_url: https://arxiv.org/abs/2412.12961
tags:
- queries
- database
- language
- rest
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of simplifying access to the
  Land Matrix database, which contains valuable data on large-scale land acquisitions
  but remains underutilized due to its technical complexity. The research compares
  three Large Language Models (LLMs) - Llama3-8B, Mixtral-8x7B-instruct, and Codestral-22B
  - using three optimization techniques (prompt engineering, retrieval-augmented generation,
  and LLM agents) to generate both REST and GraphQL queries from natural language
  questions.
---

# Adaptations of AI models for querying the LandMatrix database in natural language

## Quick Facts
- arXiv ID: 2412.12961
- Source URL: https://arxiv.org/abs/2412.12961
- Authors: Fatiha Ait Kbir; Jérémy Bourgoin; Rémy Decoupes; Marie Gradeler; Roberto Interdonato
- Reference count: 1
- One-line primary result: LLM adaptations can significantly improve database query interfaces, with agentic methods showing the most promise for handling complex natural language queries against the Land Matrix database

## Executive Summary
This study addresses the challenge of simplifying access to the Land Matrix database, which contains valuable data on large-scale land acquisitions but remains underutilized due to its technical complexity. The research compares three Large Language Models (LLMs) - Llama3-8B, Mixtral-8x7B-instruct, and Codestral-22B - using three optimization techniques (prompt engineering, retrieval-augmented generation, and LLM agents) to generate both REST and GraphQL queries from natural language questions. The experiments used real user requests and expert-configured queries for evaluation. Results show that Codestral-22B with the agentic approach performed best, achieving 52% valid REST queries and 77% valid GraphQL queries, with 66% accuracy in returned data for REST and 51% for GraphQL.

## Method Summary
The study evaluates three LLMs using three optimization techniques to convert natural language questions into REST and GraphQL queries for the Land Matrix database. The optimization techniques include prompt engineering with few-shot examples, retrieval-augmented generation (RAG) using embeddings and similarity search, and a multi-agent approach with entity extraction followed by query generation. The evaluation uses real non-technical user requests compared against expert-configured queries, measuring valid query syntax, result similarity, and attribute accuracy.

## Key Results
- Codestral-22B with agentic approach achieved 52% valid REST queries and 77% valid GraphQL queries
- REST queries showed higher accuracy in returned data (66%) compared to GraphQL (51%)
- The agentic approach with entity extraction outperformed both prompt engineering and RAG techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent-based query generation with entity extraction improves query accuracy by providing context-specific attribute values
- Mechanism: The LLM Agent first extracts relevant entities from the user's question, maps them to known database values, and enriches the query-generation prompt with these specific values rather than the full schema. This targeted context reduces noise and helps the model generate more accurate queries
- Core assumption: The database has a finite, known set of attribute values that can be pre-mapped to entities, and entity extraction is accurate enough to identify the relevant ones
- Evidence anchors: [abstract] "The second for query generation (see Fig. 2). We start by extracting the entities from the user's question using the first agent, where predefined values associated with entities in the LMI database are returned as a list of [entity: value] pairs." [section] "This approach allows us to enrich the query generation prompt with context-specific information without overloading it with all possible variables."
- Break condition: Entity extraction fails to identify correct entities, or the database schema changes and entity-value mappings become outdated

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves query generation by providing semantically similar examples
- Mechanism: The system computes embeddings of the input question and compares them to a corpus of stored question-query pairs. The top k most similar questions and their corresponding queries are added to the prompt context, helping the model understand query patterns for similar questions
- Core assumption: Similar questions have similar query structures, and the embedding model captures semantic similarity effectively
- Evidence anchors: [abstract] "To enhance the quality of the GraphQL and REST queries generated by the model, we employed the RAG approach to enrich the prompt context with natural language questions and their corresponding queries that are similar to the input question." [section] "Next, we utilized Facebook AI Similarity Search (Faiss)... We calculate the similarity between the input question and all the questions represented by their embeddings stored in the vector database."
- Break condition: The embedding model fails to capture semantic similarity, or the question-query corpus is too small or not representative of real user queries

### Mechanism 3
- Claim: Specialized code-generation LLMs (Codestral-22B) outperform general-purpose LLMs for database query generation
- Mechanism: Codestral-22B is fine-tuned on code generation tasks, giving it better understanding of programming language syntax and database query structures. This specialization leads to higher accuracy in generating syntactically correct queries
- Core assumption: Fine-tuning on code generation tasks transfers to better performance on database query languages like SQL, GraphQL, and REST query parameters
- Evidence anchors: [abstract] "Results show that Codestral-22B with the agentic approach performed best, achieving 52% valid REST queries and 77% valid GraphQL queries." [section] "Codestral-22B captures the schema of the LMI database very accurately, as well as the relationships between its attributes, which is reflected in the higher accuracy scores in the filter tables."
- Break condition: The code-generation fine-tuning does not generalize well to the specific syntax and semantics of database query languages

## Foundational Learning

- Concept: Database schema understanding and entity-relationship mapping
  - Why needed here: The LLM must understand the structure of the Land Matrix database, including tables, attributes, and relationships between entities, to generate accurate queries
  - Quick check question: What are the primary entities in the Land Matrix database and how are they related to each other?

- Concept: Natural language processing and entity extraction
  - Why needed here: The system must accurately extract relevant entities (like country names, date ranges, land sizes) from user questions to map them to database attributes
  - Quick check question: Given the question "Show me all deals in Brazil larger than 1000 hectares from 2020", what entities would the system need to extract?

- Concept: Vector embeddings and similarity search
  - Why needed here: RAG relies on computing semantic similarity between user questions and stored examples using vector embeddings, requiring understanding of embedding models and similarity metrics
  - Quick check question: How would you compute the similarity between two questions like "Show me deals in Brazil" and "Display all transactions in Brazil"?

## Architecture Onboarding

- Component map: User Interface -> Middleware (LangChain API) -> LLM Models -> RAG System / Agent System -> Database APIs -> Evaluation
- Critical path: User question → Entity extraction (if agentic) → Query generation (LLM) → API call → Result validation
- Design tradeoffs:
  - Model size vs. inference cost: Codestral-22B performs best but is most expensive
  - Prompt engineering vs. RAG vs. Agents: Agents provide best performance but add complexity
  - REST vs. GraphQL: GraphQL syntax is easier for LLMs to generate but REST has better performance on valid queries
- Failure signatures:
  - Invalid query syntax: LLM failed to understand query language structure
  - Incorrect filters: LLM identified wrong attributes or values from the question
  - Hallucinations: LLM added fictional attributes or values not in the database schema
  - Performance degradation: RAG corpus too small or embedding model not capturing similarity
- First 3 experiments:
  1. Test each LLM with prompt engineering only on a small set of simple questions to establish baseline performance
  2. Add RAG optimization to the best-performing LLM from experiment 1 and measure improvement on the same question set
  3. Implement the agentic approach with entity extraction for the best-performing LLM/RAG combination and evaluate on complex questions requiring multiple filters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Codestral-22B with agentic approach compare to fine-tuned models specifically trained on the Land Matrix schema?
- Basis in paper: [inferred] The paper mentions that "we anticipate rapid improvements in models and approaches" and that fine-tuning "require significant computational resources," suggesting this comparison hasn't been made
- Why unresolved: The study only compared three open-weight models with optimization techniques rather than comparing to fine-tuned models
- What evidence would resolve it: Direct comparison of Codestral-22B with agentic approach against models fine-tuned on Land Matrix data using the same evaluation metrics

### Open Question 2
- Question: What is the impact of different embedding models on RAG performance for natural language to database query translation?
- Basis in paper: [explicit] The paper mentions using "all-mpnet-base-v2 model" for embeddings but doesn't explore alternatives
- Why unresolved: The study only used one embedding model (all-mpnet-base-v2) without exploring how different embedding models might affect RAG performance
- What evidence would resolve it: Comparative experiments using different sentence-transformer models or embedding approaches with the same RAG setup

### Open Question 3
- Question: How do the results generalize to other database systems beyond REST and GraphQL APIs?
- Basis in paper: [inferred] The study focuses specifically on Land Matrix's REST and GraphQL APIs, with limited discussion of generalizability
- Why unresolved: The paper doesn't test the approach on other database systems or API types, limiting understanding of broader applicability
- What evidence would resolve it: Replication of the study using different database systems (e.g., SQL, NoSQL) with similar complexity to Land Matrix's APIs

### Open Question 4
- Question: What is the optimal number and selection of few-shot examples for prompt engineering in this domain?
- Basis in paper: [explicit] The paper mentions "examples of question-request pairs" but doesn't systematically explore optimal selection or quantity
- Why unresolved: The study uses "few-shot learning" but doesn't investigate how different numbers or selection strategies of examples affect performance
- What evidence would resolve it: Controlled experiments varying the number and selection method of few-shot examples while measuring query generation accuracy

## Limitations

- Limited dataset size: The evaluation used only 26 real user questions, which may not represent the full diversity of potential queries
- Database specificity: Results are based on the Land Matrix database schema and may not generalize to other database structures
- Implementation details: Some critical implementation aspects of the agentic approach and entity extraction process are not fully specified

## Confidence

- High Confidence: The effectiveness of Codestral-22B as a specialized code-generation model for query generation tasks
- Medium Confidence: The superiority of the agentic approach over prompt engineering and RAG techniques
- Medium Confidence: The comparative performance of REST vs. GraphQL query generation

## Next Checks

1. **Dataset Expansion Test**: Evaluate the system on a larger and more diverse dataset of natural language questions (minimum 100 questions) across multiple domains to assess generalizability
2. **Cross-Database Validation**: Test the same LLM and optimization techniques on at least two different database schemas to determine if the performance patterns hold across domains
3. **Error Analysis**: Conduct a detailed error analysis of failed queries to identify specific failure modes (syntax errors, wrong attribute selection, value mismatches) and develop targeted mitigation strategies for each failure type