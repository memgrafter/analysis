---
ver: rpa2
title: 'CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing
  General Performance'
arxiv_id: '2409.13202'
source_url: https://arxiv.org/abs/2409.13202
tags:
- components
- importance
- tool
- general
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling Large Language Models
  (LLMs) to effectively use external tools without sacrificing their general capabilities.
  The authors identify that traditional fine-tuning methods for tool learning can
  negatively impact a model's performance on unrelated tasks.
---

# CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance

## Quick Facts
- arXiv ID: 2409.13202
- Source URL: https://arxiv.org/abs/2409.13202
- Authors: Yupu Hao; Pengfei Cao; Zhuoran Jin; Huanxuan Liao; Yubo Chen; Kang Liu; Jun Zhao
- Reference count: 27
- Primary result: CITI achieves competitive tool utilization while significantly outperforming baselines in preserving general task performance

## Executive Summary
This paper addresses the challenge of enabling Large Language Models (LLMs) to effectively use external tools without sacrificing their general capabilities. Traditional fine-tuning methods for tool learning can negatively impact a model's performance on unrelated tasks, a problem the authors call "catastrophic forgetting." To tackle this issue, they propose a novel approach called CITI (Component Importance-based Tool-utilizing ability Injection method) that uses component importance analysis to selectively apply different training strategies: Mixture-of-LoRA adapters for important components and full parameter fine-tuning for less important ones. Experimental results demonstrate that CITI achieves competitive tool utilization performance while significantly outperforming baseline methods in preserving general task performance across multiple datasets and model architectures.

## Method Summary
CITI uses a three-stage training process to enhance tool utilization while preserving general abilities. First, a router network is pre-trained to distinguish tool-related from tool-unrelated inputs. Second, Mixture-of-LoRA (MOLoRA) adapters are applied to the top 20% most important components (identified through gradient-based importance scores) and fine-tuned with mixed tool and general ability data. Third, the bottom 10% least important components are fully fine-tuned. The method selectively preserves important components while enhancing tool utilization through MOLoRA, preventing the co-directional shift in hidden representations that typically causes catastrophic forgetting during traditional fine-tuning approaches.

## Key Results
- CITI achieves competitive tool utilization performance while significantly outperforming baseline methods in preserving general task performance
- The method successfully prevents the co-directional shift phenomenon in hidden representations that typically occurs during tool learning fine-tuning
- Experimental results show CITI maintains strong performance across multiple general ability datasets (GSM8K, HumanEval, TriviaQA, MT-Bench) while improving tool utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-directional shift in hidden state space during fine-tuning degrades general task performance.
- Mechanism: Fine-tuning on tool learning datasets causes hidden representations of tool-related and tool-unrelated tasks to shift in similar directions, leading to representational interference and catastrophic forgetting.
- Core assumption: The direction of hidden state changes is positively correlated between tool-related and tool-unrelated tasks after fine-tuning.
- Evidence anchors:
  - "We observe a significant phenomenon: the increments, derived by subtraction, exhibit a Co-directional shift in the hidden state space"
  - "The changing direction of the ICC t on the general task is positively correlated with the direction of ICC tool in model's hidden state space"
- Break condition: If hidden representations of tool-related and tool-unrelated tasks shift in orthogonal or opposite directions, or if the model can effectively separate these representations through other architectural means.

### Mechanism 2
- Claim: Gradient-based importance scores identify components critical for specific capabilities.
- Mechanism: Components with high gradient-based importance scores contribute more significantly to specific task performance, and these scores correlate across different general abilities.
- Core assumption: The gradient-based importance score accurately reflects the contribution of a component to task performance.
- Evidence anchors:
  - "we calculate the gradient-based importance score ranking of model's linear modules"
  - "We discover that fine-tuning the components with lower importance score Mh have the least impact on the general performance"
- Break condition: If gradient-based importance scores fail to correlate with actual task performance impact, or if other methods (like ablation studies) provide better component importance rankings.

### Mechanism 3
- Claim: Selective fine-tuning of components based on importance scores preserves general performance while enhancing tool utilization.
- Mechanism: Fine-tuning unimportant components for tool learning while applying MOLoRA to important components prevents catastrophic forgetting and maintains general capabilities.
- Core assumption: Fine-tuning unimportant components has minimal impact on general task performance, while MOLoRA can effectively add tool knowledge to important components without disrupting their original function.
- Evidence anchors:
  - "it fine-tunes the parameters of few components deemed less important in the backbone of the LLM, while keeping other parameters frozen"
  - "We find that fine-tuning the components with lower importance score Mh have the least impact on the general performance"
- Break condition: If fine-tuning unimportant components still significantly impacts general performance, or if MOLoRA fails to effectively learn tool knowledge without disrupting important component functionality.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how LLMs process information through layers of self-attention and feed-forward networks is crucial for comprehending component importance and hidden representation changes.
  - Quick check question: How do attention weights and feed-forward layers contribute to the final output of a transformer block?

- Concept: Fine-tuning and catastrophic forgetting
  - Why needed here: The paper addresses the problem of catastrophic forgetting when fine-tuning LLMs on tool learning datasets, so understanding this phenomenon is essential.
  - Quick check question: What happens to a model's performance on original tasks when it is fine-tuned on new tasks without any preservation techniques?

- Concept: Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE)
  - Why needed here: The CITI method uses MOLoRA, which combines LoRA with MoE concepts, so understanding these techniques is necessary for grasping the proposed solution.
  - Quick check question: How does LoRA modify the weight matrices of a model, and how does the MoE architecture route inputs to different experts?

## Architecture Onboarding

- Component map:
  Input layer → Transformer blocks (self-attention + feed-forward) → Output layer → Router network for distinguishing tool-related vs. tool-unrelated inputs → MOLoRA adapters for important components → Fine-tuning mechanism for unimportant components

- Critical path:
  1. Router Pre-training: Train router network to distinguish tool-related and tool-unrelated inputs
  2. MOLoRA Improvement: Fine-tune MOLoRA adapters while freezing backbone
  3. Unimportant Components Optimization: Fine-tune a subset of unimportant components

- Design tradeoffs:
  - Trade-off between tool utilization performance and general task performance
  - Balance between the number of components fine-tuned and the risk of catastrophic forgetting
  - Complexity of router network vs. effectiveness of input separation

- Failure signatures:
  - Significant drop in general task performance (e.g., GSM8K, HumanEval scores)
  - Poor tool utilization performance (e.g., low correctness or ROUGE-L scores)
  - Inconsistent router network outputs (weights not properly distinguishing tool-related vs. unrelated inputs)

- First 3 experiments:
  1. Analyze hidden representation changes (ICC) between fine-tuned and original models to verify co-directional shift phenomenon
  2. Compute gradient-based importance scores for components and verify correlation across different general abilities
  3. Implement and evaluate CITI method, comparing tool utilization and general task performance against baseline methods (FT and LoRA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between training important and unimportant components for maximizing both tool-utilizing and general performance?
- Basis in paper: The paper shows that fine-tuning only important components harms general performance while only fine-tuning unimportant components may not fully learn tool-utilizing. CITI uses a mixture approach, but the optimal balance is not explicitly determined.
- Why unresolved: The paper uses fixed proportions (20% important, 10% unimportant) based on gradient importance scores, but doesn't explore whether different ratios might yield better results across different model architectures or datasets.
- What evidence would resolve it: Systematic ablation studies varying the proportions of important vs unimportant components fine-tuned, across multiple model sizes and datasets, to identify optimal trade-offs.

### Open Question 2
- Question: How does the router network's ability to distinguish tool-related from tool-unrelated inputs evolve during training, and what are the failure modes?
- Basis in paper: The router network is designed to separate inputs, but the paper only provides visualizations of final router weights without analyzing the learning dynamics or failure cases.
- Why unresolved: The paper doesn't examine how the router learns to distinguish inputs over training epochs, what types of inputs are misclassified, or whether the router creates any bottlenecks in learning.
- What evidence would resolve it: Analysis of router performance metrics (classification accuracy, confidence scores) over training epochs, case studies of misclassified examples, and impact of router failures on downstream performance.

### Open Question 3
- Question: What is the relationship between the Co-directional shift phenomenon in hidden representations and catastrophic forgetting in tool learning?
- Basis in paper: The paper identifies the Co-directional shift phenomenon where tool-related and general task increments become correlated in hidden space, but doesn't fully explain the causal relationship to performance degradation.
- Why unresolved: While the paper observes that fine-tuning causes similar directional changes in hidden states across tasks, it doesn't establish whether this correlation directly causes the performance drop or if it's merely correlated with other underlying mechanisms.
- What evidence would resolve it: Experiments that manipulate the hidden state correlations (e.g., through regularization techniques) and measure the impact on both tool learning performance and general ability preservation.

## Limitations

- The core claims about co-directional shift in hidden state spaces remain largely theoretical with limited empirical validation of the representational interference mechanism
- The component importance analysis relies heavily on gradient-based scores, which may not fully capture nuanced contributions of different transformer components
- The router network architecture and its training procedure are not fully specified, making it difficult to assess generalization across diverse scenarios

## Confidence

**High Confidence**: The experimental results demonstrating CITI's effectiveness in preserving general performance while improving tool utilization are well-supported by the data presented.

**Medium Confidence**: The theoretical framework explaining co-directional shifts and catastrophic forgetting through hidden representation analysis is plausible but requires additional empirical validation.

**Low Confidence**: The long-term generalization of CITI across diverse tool types and real-world applications remains untested.

## Next Checks

1. **Hidden Representation Analysis Validation**: Conduct controlled experiments where different components are selectively fine-tuned, then measure the correlation between hidden representation shifts and performance degradation across tool-related and tool-unrelated tasks.

2. **Component Importance Score Verification**: Implement alternative component importance measurement methods (such as ablation studies or integrated gradients) and compare results with gradient-based scores.

3. **Router Network Generalization Testing**: Evaluate the router network's performance on out-of-distribution tool-related and tool-unrelated inputs, including edge cases and ambiguous scenarios.