---
ver: rpa2
title: Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL
arxiv_id: '2409.12798'
source_url: https://arxiv.org/abs/2409.12798
tags:
- subgoals
- agent
- reward
- llms
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as an automated credit assignment mechanism in reinforcement learning by acting
  as a reward shaping function. The proposed CALM method leverages LLMs to decompose
  tasks into subgoals and assess whether state-action-state transitions achieve those
  subgoals, providing auxiliary rewards when options terminate.
---

# Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL

## Quick Facts
- arXiv ID: 2409.12798
- Source URL: https://arxiv.org/abs/2409.12798
- Reference count: 36
- LLMs can effectively verify subgoal achievement in RL environments, achieving F1 scores up to 0.83 with game screen observations

## Executive Summary
This paper investigates whether large language models can serve as an automated credit assignment mechanism in reinforcement learning by acting as a reward shaping function. The proposed CALM method leverages LLMs to decompose tasks into subgoals and assess whether state-action-state transitions achieve those subgoals, providing auxiliary rewards when options terminate. Experiments on MiniHack tasks using human-annotated demonstration data show that LLMs can effectively verify subgoal achievement and suggest subgoals consistent with human annotations, achieving F1 scores up to 0.83 with game screen observations when subgoals are provided, and up to 0.86 when subgoals are autonomously discovered. Performance improves with larger models and is better with full game screen observations than cropped views. These results indicate that LLMs possess sufficient prior knowledge to transfer human-like reasoning into value functions, offering a promising approach to automating reward shaping and improving temporal credit assignment in sparse-reward environments.

## Method Summary
The CALM method uses pre-trained LLMs to evaluate whether state-action-state transitions in reinforcement learning environments achieve specified subgoals. The approach involves generating structured prompts that include environment descriptions, symbol sets, task descriptions, and subgoals (when provided), then querying the LLM to output boolean flags indicating subgoal completion. The method is tested in a zero-shot setting without fine-tuning, using human-annotated demonstration transitions from the MiniHack KeyRoom environment. Evaluation compares LLM outputs to human annotations using standard metrics including F1 score, accuracy, precision, and recall across multiple model sizes and observation formats.

## Key Results
- LLMs achieve F1 scores up to 0.83 for subgoal verification when subgoals are provided
- Performance improves with larger models, with top models approaching human-level accuracy
- Game screen observations outperform cropped views, suggesting contextual information is valuable
- Autonomous subgoal discovery achieves F1 scores up to 0.86, though this involves different task complexity than verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can decompose tasks into subgoals that align with human understanding and verify their achievement in state-action-state transitions
- Mechanism: The LLM parses textual game descriptions and transitions, internally mapping them to its pretraining knowledge of NetHack-like environments, and outputs boolean flags for subgoal completion
- Core assumption: The LLM's pretraining corpus includes sufficient examples of NetHack gameplay and subgoal hierarchies to recognize relevant patterns without fine-tuning
- Evidence anchors:
  - [abstract] "LLMs can effectively verify subgoal achievement and suggest subgoals consistent with human annotations, achieving F1 scores up to 0.83 with game screen observations when subgoals are provided"
  - [section 5.1] "LLMs, except gemma-1.1-2b-it, are generally effective in recognising when an instruction has been successfully completed in a state-action-state transition"
- Break condition: If the LLM's pretraining did not include sufficient NetHack or similar environment examples, its subgoal decomposition would diverge from human expectations

### Mechanism 2
- Claim: Providing subgoals externally yields higher accuracy than allowing the LLM to autonomously discover them
- Mechanism: When subgoals are explicitly listed, the LLM's classification task is simplified to binary verification rather than open-ended generation, reducing cognitive load and error rates
- Core assumption: The task of verifying pre-defined subgoals is easier for the LLM than generating appropriate subgoals from scratch
- Evidence anchors:
  - [section 5.1] "LLMs, except gemma-1.1-2b-it, are generally effective in recognising when an instruction has been successfully completed" (with subgoals provided)
  - [section 5.1.1] "When transitioning to cropped observations... LLMs perform worse" (when subgoals must be discovered)
- Break condition: If the subgoals provided are ambiguous or misaligned with the task structure, even explicit verification could yield poor results

### Mechanism 3
- Claim: Game screen observations improve LLM performance compared to cropped observations
- Mechanism: Full game screens provide richer contextual cues (messages, stats, full grid) that help the LLM disambiguate the current game state and subgoal progress
- Core assumption: The additional tokens in game screens carry meaningful information rather than noise, and the LLM can effectively process the longer context
- Evidence anchors:
  - [section 5.1] "Restricting the field of view of the observation helps improve performance... the performance of models drastically failing with human observations greatly improves"
  - [section 5.1.1] "Models like Meta-Llama-3-70B-Instruct and Meta-Llama-3-8B-Instruct come close to human performance" with game screens
- Break condition: If the game screen context becomes too long, token limits or noise could overwhelm the model, negating the benefit

## Foundational Learning

- Concept: Reinforcement Learning and the Temporal Credit Assignment Problem
  - Why needed here: CALM is designed to solve sparse reward credit assignment by providing intermediate rewards for subgoal achievement
  - Quick check question: In a delayed reward environment, why is it difficult for an RL agent to learn which actions contributed to the final reward?

- Concept: Large Language Models and Prompt Engineering
  - Why needed here: The method relies on carefully structured prompts to elicit correct subgoal decomposition and verification from the LLM
  - Quick check question: What is the role of the <OUTPUT-FORMAT-REQUEST> in the prompt structure?

- Concept: Hierarchical Reinforcement Learning and Options Framework
  - Why needed here: CALM uses the options framework where subgoals correspond to option terminations that trigger auxiliary rewards
  - Quick check question: How does the termination condition Î²(s) in an option relate to subgoal achievement in CALM?

## Architecture Onboarding

- Component map: MiniHack environment -> Human annotation generation -> Prompt generator -> LLM (critic) -> Boolean output -> Evaluation module -> Comparison with human annotations

- Critical path:
  1. Collect demonstration transitions from MiniHack
  2. Annotate transitions with human subgoal labels
  3. Generate prompts for each transition (with or without subgoals)
  4. Query LLM with prompts
  5. Parse LLM output into boolean dictionary
  6. Compute evaluation metrics against human annotations

- Design tradeoffs:
  - Cropped vs game screen observations: Cropped improves speed and reduces noise but loses context; game screen improves accuracy but increases token count
  - Provided vs discovered subgoals: Provided subgoals improve accuracy but require human input; discovered subgoals are more general but less accurate
  - Model size: Larger models generally perform better but require more compute and memory

- Failure signatures:
  - Consistently low F1 scores across models: Indicates the task may be too difficult or the prompt structure ineffective
  - Large variance between game screen and cropped performance: Suggests context is critical for the task
  - Gemma-1.1-2b-it consistently failing: Indicates the model is too small for this task

- First 3 experiments:
  1. Run CALM with cropped observations and provided subgoals on a small subset of transitions to verify basic functionality
  2. Compare game screen vs cropped performance to establish the value of full context
  3. Test autonomous subgoal discovery to understand the tradeoff between generality and accuracy

## Open Questions the Paper Calls Out

- How do CALM's performance and credit assignment accuracy scale when applied to environments with larger state spaces and more complex subgoal hierarchies than MiniHack's KeyRoom?
- Does CALM maintain its effectiveness when used in online RL training loops rather than offline evaluation settings?
- How does CALM's performance degrade as the gap increases between the LLM's prior knowledge and the specific environment requirements?

## Limitations

- Evaluation relies entirely on human-annotated transitions from a single MiniHack environment, limiting generalizability
- Only tests zero-shot capabilities without exploring fine-tuning or few-shot approaches that might yield different performance
- Comparison between cropped and game screen observations conflates field-of-view changes with context length effects

## Confidence

- High confidence in LLMs verifying subgoal achievement when subgoals are provided (F1 > 0.8 for larger models)
- Medium confidence in autonomous subgoal discovery being less accurate than provided subgoals
- Low confidence in game screens inherently improving performance due to experimental design limitations

## Next Checks

1. Test CALM on multiple MiniHack environments beyond KeyRoom to establish whether performance generalizes across different task structures
2. Design an experiment isolating context length from visual content by comparing padded cropped observations to game screens
3. Implement a fine-tuning baseline where LLMs are trained on a small subset of human-annotated transitions