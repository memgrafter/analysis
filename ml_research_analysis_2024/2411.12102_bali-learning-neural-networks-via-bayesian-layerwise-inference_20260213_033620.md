---
ver: rpa2
title: 'BALI: Learning Neural Networks via Bayesian Layerwise Inference'
arxiv_id: '2411.12102'
source_url: https://arxiv.org/abs/2411.12102
tags:
- posterior
- distribution
- matrix
- parameters
- bali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bayesian Layerwise Inference (BALI), a novel
  method for learning Bayesian neural networks by treating each layer as a separate
  multivariate Bayesian linear regression model. The key innovation is computing layerwise
  posteriors analytically using pseudo-targets obtained from forward pass outputs
  updated by backpropagated gradients.
---

# BALI: Learning Neural Networks via Bayesian Layerwise Inference

## Quick Facts
- arXiv ID: 2411.12102
- Source URL: https://arxiv.org/abs/2411.12102
- Authors: Richard Kurle; Alexej Klushyn; Ralf Herbrich
- Reference count: 40
- Key outcome: BALI achieves similar or better performance compared to state-of-the-art Bayesian neural network methods while converging significantly faster

## Executive Summary
BALI introduces Bayesian Layerwise Inference, a novel method for learning Bayesian neural networks by treating each layer as a separate multivariate Bayesian linear regression model. The key innovation is computing layerwise posteriors analytically using pseudo-targets obtained from forward pass outputs updated by backpropagated gradients. This approach extends to mini-batch settings using exponential moving averages over natural parameter terms, enabling efficient scaling while maintaining accurate uncertainty quantification.

The method demonstrates superior performance on regression, classification, and out-of-distribution detection tasks, showing faster convergence than variational inference approaches while avoiding common underfitting issues. Experiments on synthetic data, UCI regression datasets, and classification benchmarks including MNIST and FashionMNIST validate BALI's effectiveness in both performance and uncertainty calibration.

## Method Summary
BALI treats each layer of a neural network as an independent multivariate Bayesian linear regression model, computing exact layerwise posteriors analytically using pseudo-targets from forward and backward passes. The method uses exponential moving averages to handle mini-batch training, maintaining natural parameter estimates that gradually down-weight older data. Layerwise posteriors are computed using matrix-normal inverse-Wishart distributions with Kronecker-factored covariance structures, enabling efficient computation while preserving uncertainty information throughout the network.

## Key Results
- Outperforms state-of-the-art Bayesian neural network methods on regression tasks with faster convergence
- Demonstrates superior uncertainty quantification on synthetic data, avoiding underfitting common in variational inference
- Shows competitive performance on UCI regression datasets and classification benchmarks (MNIST, FashionMNIST)
- Achieves better out-of-distribution detection AUC scores compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layerwise exact posterior inference breaks the node-permutation invariance problem that plagues variational inference in deep BNNs.
- **Mechanism:** By treating each layer as an independent multivariate Bayesian linear regression, the posterior over weights in layer l is conditioned only on features from layer l-1 and pseudo-targets from layer l+1. This local conditioning prevents the factorial explosion of modes from permutations within hidden layers.
- **Core assumption:** The pseudo-targets computed via gradient updates are sufficient statistics for the true posterior of each layer given the full network.
- **Evidence anchors:**
  - [abstract] "The central idea is that, for every layer, the local posterior can be inferred exactly and analytically, given the features extracted forward from the preceding layers and pseudo-targets projected backward from the subsequent layers."
  - [section 3.1] "The local posterior of each layer is uni-modal by design. The main idea is that we can infer the layerwise posterior analytically if we have pseudo-targets for the outputs of the linear transformation in each layer."
  - [corpus] Weak: corpus papers focus on last-layer methods rather than layerwise inference, so direct support is limited.
- **Break condition:** If pseudo-targets are poorly estimated (e.g., wrong gradient step size), the layerwise posteriors become misaligned with the true joint posterior, breaking the decomposition assumption.

### Mechanism 2
- **Claim:** Exponential moving average (EMA) over natural parameters enables mini-batch learning while gradually forgetting outdated data.
- **Mechanism:** Natural parameters (precision matrix, precision-mean) are additive across data points. EMA updates with decay rate β down-weight older mini-batches, allowing multiple epochs without posterior collapse to a delta function.
- **Core assumption:** The distribution shift in pseudo-targets across iterations is small enough that EMA provides a stable estimate of the full-dataset natural parameters.
- **Evidence anchors:**
  - [section 3.2] "we impose an exponential decay on the likelihood terms such that terms involving older features and pseudo-targets are down-weighted."
  - [section 3.2] "we estimate the natural parameters using the following EMA updates: Ψxx_t = (1 − β) · Ψxx_t−1 + β · N/B · X⊤t Xt"
  - [corpus] Weak: no direct evidence in corpus about EMA in layerwise BNN inference.
- **Break condition:** If β is too small, EMA becomes too sensitive to noise; if too large, the method forgets too quickly and loses stability.

### Mechanism 3
- **Claim:** Gradient-backpropagated pseudo-targets provide a computationally efficient and effective way to inject backward information flow for layerwise inference.
- **Mechanism:** At each iteration, the pseudo-target for layer l is defined as the forward pass output plus a scaled gradient of the objective w.r.t. that output. This incorporates information from the loss function while keeping computation local to each layer.
- **Core assumption:** The gradient-updated targets approximate the expected value of the true targets under the posterior of subsequent layers.
- **Evidence anchors:**
  - [section 3.3] "we propose gradient backpropagation as simple but effective method to obtain pseudo-targets... yn,t = zn,t + α · ∂L(Dt,wt)/∂zn,t"
  - [section 3.3] "The weights used to compute the new pseudo-targets at iteration t are drawn from the previous posterior approximation"
  - [corpus] Weak: corpus papers do not use gradient-based pseudo-targets in layerwise inference.
- **Break condition:** If the step size α is poorly chosen, pseudo-targets can overshoot or undershoot, leading to unstable posterior updates.

## Foundational Learning

- **Concept: Kronecker-factored matrix-normal distributions**
  - Why needed here: BALI uses matrix-normal posteriors with Kronecker-factored covariance (Σ ⊗ R) for computational efficiency. Understanding this structure is critical for implementing the posterior updates and sampling.
  - Quick check question: Given W ~ MN(M, R, Σ), what is the vectorized covariance of vec(W)? (Answer: Σ ⊗ R)

- **Concept: Natural parameters in exponential families**
  - Why needed here: The posterior parameters are computed by summing natural parameters from prior and likelihood. EMA updates are applied directly to these parameters.
  - Quick check question: For a Gaussian with precision V⁻¹ and precision-mean η, what are the natural parameters? (Answer: θ₁ = V⁻¹, θ₂ = η)

- **Concept: Matrix inversion via Cholesky decomposition**
  - Why needed here: BALI requires inverting R⁻¹ and computing Cholesky decompositions for sampling. Efficient implementation depends on exploiting the Kronecker structure.
  - Quick check question: What is the computational complexity of inverting a d×d matrix via Cholesky vs. standard methods? (Answer: O(d³) vs O(d³), but with better constants and numerical stability)

## Architecture Onboarding

- **Component map:** Forward pass -> Backward pass -> Pseudo-target computation -> EMA updates -> Posterior parameter computation -> Weight sampling -> Next forward pass

- **Critical path:** Forward pass → Backward pass → Pseudo-target computation → EMA updates → Posterior parameter computation → Weight sampling → Next forward pass

- **Design tradeoffs:**
  - Weight reparametrization vs. local reparametrization: Weight reparametrization (used in paper) is simpler but introduces more noise; local reparametrization reduces noise but requires more computation.
  - EMA decay rate β: Higher β = more stable but slower adaptation; lower β = faster adaptation but noisier estimates.
  - Prior scale matrices: Larger values = more regularization; smaller values = more flexibility but risk overfitting.

- **Failure signatures:**
  - Posterior covariance matrices becoming singular → Check EMA initialization and numerical stability in Cholesky decompositions
  - Training diverging or oscillating → Check gradient step size α and EMA decay β
  - Poor uncertainty calibration → Check if pseudo-targets are properly scaled and if weight sampling is correctly implemented

- **First 3 experiments:**
  1. **Sanity check on synthetic regression:** Run BALI on a simple 1D regression problem (e.g., noisy sine wave) and visualize the posterior predictive mean and uncertainty bands. Verify that uncertainty increases away from training data.
  2. **Ablation on EMA decay:** Compare BALI with different β values on a small UCI dataset. Observe how convergence speed and final performance change with β.
  3. **Gradient step size sensitivity:** Run BALI with varying α on MNIST classification. Plot NLL vs. iterations to identify optimal α range.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does BALI's performance scale to deeper neural network architectures with more than three hidden layers?
- **Basis in paper:** [inferred] The paper tested BALI only on networks with three hidden layers and stated "the scope of this work is limited to small models and only fully-connected layers."
- **Why unresolved:** The paper explicitly limited its scope to small models and did not test deeper architectures, which are common in modern applications.
- **What evidence would resolve it:** Experimental results comparing BALI's performance on deep networks (e.g., 10+ layers) against other methods would clarify its scalability.

### Open Question 2
- **Question:** Can BALI be extended to convolutional, recurrent, and attention layers commonly used in modern architectures?
- **Basis in paper:** [explicit] "the scope of this work is limited to small models and only fully-connected layers."
- **Why unresolved:** The paper only applied BALI to fully-connected networks and did not explore its applicability to other layer types.
- **What evidence would resolve it:** Demonstrating BALI's effectiveness on architectures incorporating convolutional, recurrent, or attention layers would show its versatility.

### Open Question 3
- **Question:** What are the optimal hyperparameter settings for BALI across different tasks and datasets?
- **Basis in paper:** [explicit] "we found BALI to be sensitive to hyperparameter choices, similar to KFAC-based methods" and "A promising research direction therefore concerns defining robust heuristics to find the hyperparameters."
- **Why unresolved:** The paper acknowledges sensitivity to hyperparameters but does not provide systematic methods for hyperparameter selection.
- **What evidence would resolve it:** A comprehensive study providing hyperparameter guidelines or automated selection methods for various tasks would address this issue.

## Limitations

- Method's performance critically depends on choice of EMA decay parameter β and gradient step size α, which lack theoretical guidance for selection
- Assumption that layerwise posteriors factorize cleanly may break down in deep networks with strong parameter coupling
- Computational complexity scales with square of layer width, potentially limiting applicability to very wide networks

## Confidence

- **High confidence**: BALI's superior convergence speed compared to VI baselines, supported by direct experimental comparison showing faster training with similar or better final performance
- **Medium confidence**: Claims about uncertainty quantification improvements, based primarily on synthetic data experiments where ground truth is known, but limited validation on real-world OOD detection tasks
- **Low confidence**: Generalization to very deep networks (>10 layers) or architectures with skip connections, as experiments were limited to networks with 1-3 hidden layers

## Next Checks

1. **Scaling study**: Test BALI on deeper networks (5-10 layers) with residual connections to evaluate if layerwise factorization assumptions hold in architectures where parameter coupling is stronger

2. **Hyperparameter sensitivity**: Systematically vary β and α across multiple orders of magnitude on UCI datasets to map the hyperparameter landscape and identify robust operating regions

3. **Comparison to exact inference**: On small networks where exact inference is tractable (e.g., ≤2 layers, ≤100 parameters), compare BALI's posterior approximations against gold-standard MCMC samples to quantify approximation error