---
ver: rpa2
title: Bilingual Evaluation of Language Models on General Knowledge in University
  Entrance Exams with Minimal Contamination
arxiv_id: '2409.12746'
source_url: https://arxiv.org/abs/2409.12746
tags:
- questions
- english
- spanish
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present UNED-ACCESS 2024, a bilingual dataset of 1003 private
  multiple-choice questions in Spanish and English, created to minimize contamination
  when evaluating LLMs. We evaluated 12 models in zero-shot settings and found that
  model rankings on this dataset correlate almost perfectly (0.98 Pearson) with those
  on MMLU-10, suggesting that a small, diverse dataset is representative for measuring
  performance.
---

# Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination

## Quick Facts
- arXiv ID: 2409.12746
- Source URL: https://arxiv.org/abs/2409.12746
- Reference count: 28
- A bilingual dataset of 1003 private university entrance exam questions shows model rankings correlate 0.98 with MMLU-10 while minimizing contamination.

## Executive Summary
This paper introduces UNED-ACCESS 2024, a bilingual dataset of 1003 multiple-choice questions in Spanish and English designed to evaluate large language models with minimal contamination. The questions come from private university entrance exams and are manually translated between languages by professional translators. The authors evaluate 12 models in zero-shot settings and find that model rankings on this dataset correlate almost perfectly with MMLU-10 rankings, suggesting that a small, diverse dataset can reliably measure performance. The evaluation reveals that larger models perform similarly in both languages while smaller models show a growing gap favoring English.

## Method Summary
The study evaluates 12 language models (4 proprietary, 8 open-source) on a bilingual dataset of 1003 questions from private Spanish university entrance exams, manually translated into English. Models are tested in zero-shot settings using fixed prompts with temperature=0, one question at a time. Performance is measured using Cohen's Kappa coefficient to account for multiple-choice guessing, and results are compared against MMLU-10 rankings. The dataset is accessed through the ODESIA leaderboard, and models are evaluated using their respective APIs or HuggingFace implementations.

## Key Results
- Model rankings on UNED-ACCESS 2024 correlate 0.98 Pearson with MMLU-10 rankings
- Larger models perform similarly in Spanish and English, while smaller models show growing English advantage
- Reasoning questions remain challenging across all model sizes
- The dataset demonstrates minimal contamination despite using private exam questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilingual zero-shot evaluation reveals relative model performance without confounding factors
- Mechanism: By using manually translated, private exam questions in Spanish and English, the evaluation isolates language proficiency from memorization and reasoning abilities
- Core assumption: Translation quality is high enough that content difficulty is preserved between languages
- Evidence anchors:
  - [abstract] "Questions are originally formulated in Spanish and manually translated into English, and have not ever been publicly released, ensuring minimal contamination"
  - [section] "This is a high-quality bilingual dataset, with original questions in Spanish translated into English manually by professional translators who did not use any external software"
  - [corpus] Weak - corpus shows related bilingual benchmarks but no direct validation of translation quality preservation
- Break condition: If translation introduces significant difficulty asymmetry between languages, results become confounded

### Mechanism 2
- Claim: Model size predicts performance gap between languages
- Mechanism: Larger models trained on more diverse data show smaller performance differences between English and Spanish
- Core assumption: Model training data distribution correlates with language proficiency differences
- Evidence anchors:
  - [abstract] "The best models perform similarly in both languages, while smaller models show a growing gap favoring English"
  - [section] "We have found that the behaviour of models is almost identical on our dataset and on MMLU-10: the model rankings provided by both datasets are almost equivalent (Pearson correlation is 0,98)"
  - [corpus] Weak - corpus shows multilingual evaluation exists but no direct correlation between model size and language gap
- Break condition: If smaller models receive disproportionate Spanish training data, the inverse correlation would break

### Mechanism 3
- Claim: Small, diverse datasets can reliably rank models compared to larger benchmarks
- Mechanism: A well-designed subset of 1000 questions captures sufficient variability to produce rankings highly correlated with larger benchmarks
- Core assumption: The 1000 questions span enough subject diversity to be representative
- Evidence anchors:
  - [abstract] "Model ranking on UNED-ACCESS 2024 is almost identical (0.98 Pearson correlation) to the one obtained with MMLU"
  - [section] "Despite its reduced size, our dataset is sufficiently diverse and representative to measure performance by model and discipline"
  - [corpus] Weak - corpus shows small benchmarks exist but no validation that 1000 questions suffices for reliable ranking
- Break condition: If the dataset overrepresents certain subjects or difficulty levels, correlation with larger benchmarks would degrade

## Foundational Learning

- Concept: Zero-shot evaluation methodology
  - Why needed here: The evaluation uses no examples or fine-tuning, matching real-world LLM usage patterns
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot evaluation settings?

- Concept: Cohen's Kappa coefficient for multiple-choice evaluation
  - Why needed here: Adjusts accuracy for random guessing probability across questions with different numbers of answer options
  - Quick check question: How does Cohen's Kappa differ from simple accuracy when questions have 3 vs 4 answer options?

- Concept: Contamination in LLM evaluation
  - Why needed here: Understanding how training data overlap affects benchmark reliability is crucial for interpreting results
  - Quick check question: What are three ways LLM training data might overlap with evaluation benchmarks?

## Architecture Onboarding

- Component map: Dataset loading -> Prompt formatting -> Model inference -> Answer extraction -> Kappa calculation -> Correlation analysis
- Critical path: Dataset -> Prompt generation -> Model API calls -> Response parsing -> Scoring
- Design tradeoffs: Private dataset ensures minimal contamination but limits accessibility; bilingual design enables language comparison but adds translation complexity
- Failure signatures: Zero Kappa scores indicate random guessing; negative Kappa indicates systematic errors; high correlation with MMLU validates methodology
- First 3 experiments:
  1. Run all models on Spanish dataset with temperature=0 to establish baseline performance
  2. Repeat with English dataset to measure language gap
  3. Run MMLU-10 subset with same prompts to validate correlation with established benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does contamination affect the relative performance of language models on private vs. public benchmarks?
- Basis in paper: Explicit - The authors note that UNED-ACCESS has minimal contamination compared to public benchmarks like MMLU, yet model rankings are nearly identical (Pearson correlation of 0.98).
- Why unresolved: While the study shows high correlation between private and public datasets, it doesn't conclusively determine whether contamination affects all models equally or if there are indirect contamination sources in the private dataset.
- What evidence would resolve it: A systematic analysis of potential contamination sources in both private and public datasets, including web searches for similar questions and analysis of model training data, would help determine the true impact of contamination.

### Open Question 2
- Question: What factors contribute to the language performance gap between English and Spanish in language models?
- Basis in paper: Explicit - The authors find that smaller models show a larger performance gap favoring English, while the best models perform slightly better in Spanish. They suggest possible factors include translation artifacts, contamination levels, and language competence.
- Why unresolved: The study identifies potential factors but doesn't isolate their individual contributions to the performance gap. The relationship between model size, training data, and language-specific performance remains unclear.
- What evidence would resolve it: Controlled experiments varying translation quality, measuring contamination levels in both languages, and analyzing model training data composition could help determine the relative impact of each factor.

### Open Question 3
- Question: How well does a small, diverse dataset like UNED-ACCESS represent the full range of language model capabilities?
- Basis in paper: Explicit - The authors conclude that despite its size (1003 questions), UNED-ACCESS is sufficiently diverse and representative to measure performance by model and discipline, based on its high correlation with MMLU-10.
- Why unresolved: While the correlation with MMLU-10 is strong, the study doesn't explore whether the dataset captures all aspects of language model performance, particularly for specialized tasks or reasoning abilities.
- What evidence would resolve it: Additional experiments comparing model performance on UNED-ACCESS with performance on other specialized benchmarks, and analysis of the dataset's coverage of different cognitive levels (e.g., using Bloom's taxonomy), would help determine its representativeness.

## Limitations
- Translation quality uncertainty - no quantitative validation that Spanish and English versions have equivalent difficulty
- Dataset size constraints - 1003 questions may lack statistical power for detecting middle-to-lower range performance differences
- Model access and configuration - proprietary models' exact architectures and training data remain undisclosed

## Confidence

**High Confidence** - The dataset demonstrates minimal contamination, as evidenced by the strong correlation (0.98 Pearson) with MMLU-10 rankings and the use of private exam questions never publicly released.

**Medium Confidence** - The finding that smaller models show larger performance gaps between English and Spanish, while larger models perform similarly in both languages, is supported by the data but could be influenced by unmeasured factors like differential training data distribution.

**Medium Confidence** - The claim that reasoning questions remain challenging across all model sizes is supported but limited by the dataset's relatively small size in each subject area.

## Next Checks

1. **Translation Equivalence Validation** - Conduct a controlled experiment where bilingual humans solve both Spanish and English versions of the same questions, measuring difficulty parity to validate that translation quality doesn't introduce systematic bias.

2. **Dataset Size Sensitivity Analysis** - Perform bootstrap sampling on the 1003-question dataset to determine the minimum number of questions needed to maintain the 0.98 Pearson correlation with MMLU-10, establishing confidence intervals for model ranking reliability.

3. **Training Data Overlap Assessment** - Analyze the training corpora of evaluated models to quantify potential overlap with university entrance exam materials, even if not directly from UNED-ACCESS, to better understand contamination risks beyond the dataset itself.