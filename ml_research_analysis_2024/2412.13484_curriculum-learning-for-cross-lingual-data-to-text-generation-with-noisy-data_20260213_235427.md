---
ver: rpa2
title: Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy Data
arxiv_id: '2412.13484'
source_url: https://arxiv.org/abs/2412.13484
tags:
- data
- title
- cell
- table
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes curriculum learning to improve cross-lingual
  data-to-text generation (XDTG) with noisy data. Existing XDTG methods do not generalize
  well to noisy cross-lingual data.
---

# Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy Data

## Quick Facts
- **arXiv ID**: 2412.13484
- **Source URL**: https://arxiv.org/abs/2412.13484
- **Reference count**: 33
- **Primary result**: Curriculum learning with annealing schedule and alignment criterion improves cross-lingual data-to-text generation, achieving up to 4 BLEU points improvement and 5-15% better faithfulness and coverage on two datasets across 11 Indian languages and English.

## Executive Summary
This paper addresses the challenge of cross-lingual data-to-text generation (XDTG) with noisy data by proposing curriculum learning strategies. Existing XDTG methods struggle with noisy cross-lingual datasets, so the authors introduce two curriculum schedules (expanding and annealing) and three ordering criteria (sequence length, word rarity, and alignment). The alignment criterion, which jointly models input and target text quality, proves most effective when combined with the annealing schedule. The method significantly improves BLEU scores by up to 4 points and enhances faithfulness and coverage by 5-15% on XA LIGN and XTOTTO datasets across 11 Indian languages and English.

## Method Summary
The approach uses curriculum learning to progressively refine cross-lingual data-to-text generation by strategically ordering training samples based on quality criteria. The method implements two schedules: expanding (gradually adding more difficult samples) and annealing (starting with all data and removing low-quality samples). Three ordering criteria are evaluated: sequence length, word rarity, and alignment score. The alignment score is computed using MURIL classifier for XA LIGN or GPT-4 annotations for XTOTTO, measuring how well generated text corresponds to input facts. Data is divided into 8 shards per language, and the model (mT5-small) is trained with Adafactor optimizer using a modified sampling strategy that follows the curriculum schedule.

## Key Results
- Annealing schedule with alignment criterion achieves up to 4 BLEU points improvement over baseline
- Faithfulness and coverage improve by 5-15% on average across both datasets
- The method generates more fluent, faithful, and comprehensive text compared to baseline approaches
- Expanding schedule performs worse than annealing, contrary to typical curriculum learning assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning with an annealing schedule progressively removes low-quality data, allowing the model to refine its generation on cleaner examples.
- Mechanism: The model begins training on the full dataset, then systematically drops samples with lower alignment scores as training progresses. This focuses the model on high-quality examples in later phases.
- Core assumption: Noisy data degrades generation quality, and filtering out low-quality samples improves model performance.
- Evidence anchors:
  - [abstract]: "using the alignment score criterion for ordering samples and an annealing schedule to train the model, we show increase in BLEU score by up to 4 points, and improvements in faithfulness and coverage of generations by 5-15% on average"
  - [section]: "For every curriculum criterion, we performed experiments with both expanding as well as annealing schedule. We train baseline models without a curriculum learning strategy, and also compare the performance of our proposed approach with loss truncation"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.538, suggesting moderate relatedness to curriculum learning topics. No specific citations yet, so corpus evidence is weak for this mechanism.

### Mechanism 2
- Claim: The alignment score criterion jointly models input and target text quality, making it suitable for cross-lingual data-to-text generation with noisy data.
- Mechanism: The alignment score measures how well the generated text corresponds to the input facts, capturing partial alignment cases where the reference deviates from the input. This criterion orders samples by quality.
- Core assumption: Joint modeling of input and target is more effective than criteria based on only one modality in cross-lingual settings.
- Evidence anchors:
  - [abstract]: "We propose a new quality based cross-lingual criterion (alignment score) and show that with an annealing approach, it results in the best performance"
  - [section]: "We propose using this confidence score as the criterion for ordering the samples. We call this criterion the alignment score. For XALIGN, we train a MURIL model... For XTOTTO, we use GPT-4 to annotate samples for partial and complete alignment"
  - [corpus]: No direct evidence in corpus for alignment score specifically; corpus evidence is weak.

### Mechanism 3
- Claim: The expanding schedule adds more difficult samples over time, but for noisy data, this approach is less effective than annealing.
- Mechanism: The expanding approach starts with easy samples and gradually introduces more difficult ones, while annealing starts with all samples and removes the lowest quality ones. Results show annealing outperforms expanding.
- Core assumption: With noisy data, starting with all data and refining is more effective than gradually introducing difficulty.
- Evidence anchors:
  - [abstract]: "While curriculum learning typically relies on the assumption that the performance of the model increases if 'difficult' data is slowly added during training, the trend suggests that with noisy data it is important to refine training with highest quality data as the training progresses"
  - [section]: "For every curriculum criterion, we performed experiments with both expanding as well as annealing schedule... While curriculum learning typically relies on the assumption that the performance of the model increases if 'difficult' data is slowly added during training, the trend suggests that with noisy data it is important to refine training with highest quality data as the training progresses"
  - [corpus]: No specific evidence in corpus about expanding vs annealing schedules; corpus evidence is weak.

## Foundational Learning

- **Curriculum learning schedules**: Different schedules affect how the model encounters data during training, especially important for noisy cross-lingual data
  - Quick check question: What is the key difference between expanding and annealing curriculum schedules?

- **Cross-lingual alignment scoring**: Standard monolingual criteria don't work well for cross-lingual settings where input and output are in different languages
  - Quick check question: How does alignment scoring differ from sequence length or word rarity criteria?

- **Handling noisy data in neural generation**: The datasets used are automatically curated and contain noisy samples that can mislead the model
  - Quick check question: Why is loss truncation not effective for multilingual noisy data according to the paper?

## Architecture Onboarding

- **Component map**: Data preprocessing → Shard data based on curriculum criteria → Curriculum Schedule → Model training → Evaluation → Analysis
- **Critical path**: Data → Sharding → Curriculum Schedule → Model Training → Evaluation → Analysis
- **Design tradeoffs**:
  - Expanding vs annealing schedule: Expanding assumes gradual difficulty increase is beneficial, annealing assumes refinement with high-quality data is better
  - Criterion choice: Alignment captures cross-lingual quality but requires annotation, length/rarity are easier but less effective
  - Model size: mT5-small balances performance and computational cost
- **Failure signatures**:
  - BLEU scores drop below baseline when using expanding schedule
  - LLM evaluation shows high coverage but low faithfulness (overestimation)
  - Human evaluation reveals hallucinations not caught by automatic metrics
- **First 3 experiments**:
  1. Train baseline mT5-small without curriculum learning on XALIGN
  2. Implement curriculum learning with expanding schedule and alignment criterion on XTOTTO
  3. Compare annealing vs expanding schedules using sequence length criterion on both datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed alignment-based curriculum learning method maintain its effectiveness when scaled to languages beyond the 11 Indian languages and English tested in this study?
- Basis in paper: [explicit] The paper mentions the method was evaluated on 11 Indian languages and English, but does not explore its applicability to other language families or resource levels
- Why unresolved: The study's language selection was limited to Indian languages and English, which share certain linguistic features and cultural contexts. The method's generalizability to other language families (e.g., European, African, or East Asian languages) remains unknown
- What evidence would resolve it: Conducting experiments on diverse language families with varying resource levels, particularly low-resource languages outside the Indo-European family, would demonstrate the method's broader applicability

### Open Question 2
- Question: How does the proposed method perform when applied to other text generation tasks beyond data-to-text generation, such as abstractive summarization or headline generation?
- Basis in paper: [inferred] The paper focuses exclusively on data-to-text generation but mentions in the limitations section that other text-grounded generation tasks could benefit from the method
- Why unresolved: The study's narrow focus on data-to-text generation prevents understanding of the method's effectiveness for other NLG tasks. Different tasks have different noise characteristics and alignment challenges
- What evidence would resolve it: Applying the curriculum learning approach to abstractive summarization and headline generation tasks, comparing performance against task-specific baselines, and analyzing how alignment criteria perform across different generation contexts

### Open Question 3
- Question: What is the optimal number of shards and phase progression strategy for curriculum learning in the cross-lingual noisy data setting?
- Basis in paper: [explicit] The paper mentions dividing data into 8 shards but does not explore how shard size or progression strategy affects performance
- Why unresolved: The study uses a fixed shard configuration (8 shards) without investigating how different shard granularities or phase progression strategies impact the model's ability to learn from noisy data
- What evidence would resolve it: Systematic experimentation with varying numbers of shards (e.g., 4, 8, 16, 32) and different progression strategies (linear, exponential, adaptive) while measuring impact on fluency, faithfulness, and coverage metrics

### Open Question 4
- Question: How does the performance of the proposed method change when using different multilingual language models beyond mT5-small?
- Basis in paper: [explicit] The study uses mT5-small (300M parameters) but does not compare with larger models or alternative multilingual architectures
- Why unresolved: The choice of mT5-small was not justified through comparison with other models, leaving uncertainty about whether the curriculum learning benefits are model-dependent or general
- What evidence would resolve it: Comparing the curriculum learning approach across different multilingual models (mT5-base, mT5-large, NLLB, BLOOMZ) while keeping other parameters constant to isolate the effect of model capacity on curriculum learning effectiveness

## Limitations

- The alignment scoring mechanism relies on either MURIL classifier training or GPT-4 annotations, which may not be consistently reproducible across different annotation setups
- The paper doesn't extensively analyze failure cases or provide detailed error analysis for when curriculum learning underperforms
- The choice of 8 shards and specific ordering thresholds within each criterion is not fully justified or explored

## Confidence

- **High confidence**: The basic curriculum learning framework (expanding vs annealing schedules) and the overall methodology for evaluating cross-lingual data-to-text generation are sound and well-implemented
- **Medium confidence**: The specific superiority of the annealing schedule with alignment criterion is demonstrated on two datasets, but the generalizability to other cross-lingual tasks or noise patterns needs validation
- **Medium confidence**: The claimed improvements in BLEU (up to 4 points) and faithfulness/coverage (5-15%) are statistically significant on the tested datasets, but the exact magnitude may vary with different data distributions or noise levels

## Next Checks

1. **Cross-dataset validation**: Test the curriculum learning approach on a third cross-lingual data-to-text dataset with different noise characteristics to verify generalizability
2. **Criterion sensitivity analysis**: Systematically vary the number of shards and ordering thresholds to understand how sensitive performance is to these hyperparameters
3. **Ablation study on alignment scoring**: Compare the proposed alignment criterion against alternative cross-lingual quality measures to isolate the contribution of this specific mechanism