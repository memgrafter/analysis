---
ver: rpa2
title: 'Filter-then-Generate: Large Language Models with Structure-Text Adapter for
  Knowledge Graph Completion'
arxiv_id: '2412.09094'
source_url: https://arxiv.org/abs/2412.09094
tags:
- llms
- graph
- entity
- knowledge
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of enabling Large Language
  Models (LLMs) to perform Knowledge Graph Completion (KGC), a task where conventional
  structure-based KGC methods significantly outperform LLMs. The authors identify
  key challenges: the vast number of entity candidates in KGs, LLM hallucinations
  generating invalid entities, and the under-exploitation of graph structure by LLMs.'
---

# Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2412.09094
- Source URL: https://arxiv.org/abs/2412.09094
- Reference count: 40
- Primary result: FtG achieves substantial performance gains over state-of-the-art methods on three widely used KGC benchmarks

## Executive Summary
This paper addresses the challenge of enabling Large Language Models (LLMs) to perform Knowledge Graph Completion (KGC), where conventional structure-based methods significantly outperform LLMs. The authors identify three key challenges: the vast number of entity candidates in KGs, LLM hallucinations generating invalid entities, and the under-exploitation of graph structure by LLMs. To overcome these, they propose FtG, an instruction-tuning-based method featuring a filter-then-generate paradigm that narrows the candidate set and reformulates KGC as a multiple-choice question. They also introduce an ego-graph serialization prompt to incorporate structural information and a structure-text adapter to map graph features into the LLM's text embedding space. FtG achieves substantial performance gains over state-of-the-art methods on three widely used benchmarks (FB15k-237, CoDEx-M, NELL-995).

## Method Summary
FtG is an instruction-tuning-based method for KGC that uses a filter-then-generate paradigm to overcome LLM limitations. The method first employs a conventional KGC filter (RotatE) to score all entities and retain only the top-k candidates, transforming the task into a multiple-choice question format. This prevents LLM hallucination of invalid entities and allows the LLM to focus on discriminating among plausible candidates. The method also extracts and prunes a 1-hop ego-graph around the query entity, serializes it via BFS into a textual sequence, and encodes it as a soft graph token. A structure-text adapter (linear projection) maps the graph representation into the LLM's text embedding space, allowing structural information to interact with textual information during encoding. FtG is fine-tuned using LLaMA2-7B with LoRA adapters and instruction templates.

## Key Results
- FtG achieves substantial performance gains over state-of-the-art methods on three widely used benchmarks (FB15k-237, CoDEx-M, NELL-995)
- The filter-then-generate paradigm improves LLM performance by reducing candidate set size and preventing hallucination
- Ego-graph serialization and structure-text adapter effectively incorporate graph structural information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The filter-then-generate paradigm significantly improves LLM performance on KGC by reducing the candidate set size.
- Mechanism: A conventional KGC method first scores all entities and retains only the top-k candidates, transforming the task into a multiple-choice question format. This prevents LLM hallucination of invalid entities and allows the LLM to focus on discriminating among plausible candidates rather than generating from the entire entity space.
- Core assumption: LLMs perform better at discriminating between a small set of candidates than generating valid entities from the entire entity set.
- Evidence anchors:
  - [abstract]: "we present a filter-then-generate paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue caused by hallucinations."
  - [section]: "we utilize a filter to eliminate unlikely entities and retain only the top-k candidates. LLMs then generate the target entities conditioned on the query and candidates list."
  - [corpus]: Weak evidence - no direct comparison between different candidate set sizes in corpus.
- Break condition: If the filter fails to include the correct entity in the top-k candidates, or if k is too small to contain the correct entity.

### Mechanism 2
- Claim: The ego-graph serialization prompt effectively incorporates graph structural information into LLMs.
- Mechanism: The method extracts a 1-hop ego-graph around the query entity, prunes irrelevant neighbors using structural embeddings, serializes it via BFS into a textual sequence, and encodes it as a soft graph token. This allows the LLM to access local structural context while maintaining text-based processing.
- Core assumption: Local structural patterns around a query entity are informative for KGC and can be effectively conveyed through text serialization.
- Evidence anchors:
  - [abstract]: "we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner."
  - [section]: "we employ structure embeddings of KGs to sample more informative neighbors...we follow existing work (Jiang et al., 2023a) perform breadth-first search (BFS) serialization to linearize it into a textual sentence."
  - [corpus]: Weak evidence - only mentions related work without specific validation of ego-graph approach.
- Break condition: If the ego-graph pruning removes relevant neighbors or if BFS serialization loses critical structural information.

### Mechanism 3
- Claim: The structure-text adapter bridges the gap between graph and text representations by mapping graph features into LLM embedding space.
- Mechanism: After serializing the ego-graph, a lightweight adapter (linear projection) maps the graph representation into the LLM's text embedding space. This allows structural information to interact with textual information during encoding, leveraging LLM's attention mechanisms for graph understanding.
- Core assumption: A simple linear projection is sufficient to map graph structural features into text embedding space for effective interaction.
- Evidence anchors:
  - [abstract]: "we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner."
  - [section]: "we obtain the ego-graph representation through parameter-free message passing on encoded structure features, and map the graph representation into the embedding space of LLM via a trainable projection matrix Wp"
  - [corpus]: No direct evidence in corpus about effectiveness of linear projection for graph-text alignment.
- Break condition: If the linear projection is insufficient to capture complex structural patterns or if the mapping loses important graph information.

## Foundational Learning

- Concept: Knowledge Graph Completion (KGC) as a classification task
  - Why needed here: Understanding that KGC involves predicting missing entities from a large set is fundamental to grasping why LLMs struggle and why the filter-then-generate approach is effective.
  - Quick check question: Why does treating KGC as classification with many labels pose a challenge for LLMs?

- Concept: Ego-graph and local graph structure
  - Why needed here: The ego-graph serialization prompt relies on understanding that local graph structure around query entities contains useful information for prediction.
  - Quick check question: What is the difference between accessing the entire KG versus using ego-graphs for KGC?

- Concept: Adapter layers and parameter-efficient fine-tuning
  - Why needed here: The structure-text adapter uses LoRA, a parameter-efficient fine-tuning technique, which is crucial for adapting large LLMs without full fine-tuning.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter updates and memory requirements?

## Architecture Onboarding

- Component map:
  KGC Filter -> Ego-Graph Extractor -> Structure Encoder -> Adapter Layer -> LLM Backbone -> Instruction Template

- Critical path:
  1. Query enters KGC filter â†’ top-k candidates generated
  2. Ego-graph extracted and pruned around query entity
  3. Ego-graph serialized and encoded with structure encoder
  4. Adapter projects graph features to text embedding space
  5. LLM processes query + candidates + (optional) ego-graph context
  6. Target entity predicted from candidates

- Design tradeoffs:
  - KGC filter choice: Simpler filters (RotatE) are faster but may miss relevant candidates; complex filters are more accurate but slower
  - k value: Larger k increases recall but makes discrimination harder for LLM; smaller k reduces candidates but risks missing correct entity
  - Adapter complexity: Linear projection is simple and efficient but may not capture complex graph-text relationships; more complex adapters could improve performance but require more parameters and training data

- Failure signatures:
  - Low MRR/Hits@1: Filter may be too restrictive or k too small
  - Inconsistent performance across datasets: Ego-graph pruning may not generalize well to different graph densities
  - Slow inference: Large k values or complex adapter layers may slow down prediction

- First 3 experiments:
  1. Baseline comparison: Run FtG with k=20 vs. direct LLM prompting on FB15k-237 to verify filter-then-generate improves performance
  2. Ablation study: Test FtG with and without ego-graph serialization prompt on CoDEx-M to measure impact of structural information
  3. Filter comparison: Replace RotatE filter with ComplEx and measure performance change on NELL-995 to test filter compatibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FtG vary when using different KGC filters (e.g., TransE, ComplEx, RotatE, CSProm-KG) as the initial filter stage?
- Basis in paper: [explicit] The paper mentions that FtG is compatible with various KGC methods as a filter and evaluates its performance with different KGC filters (TransE, ComplEx, RotatE, and CSProm-KG) on FB15k-237 and NELL-995 datasets.
- Why unresolved: The paper does not provide a comprehensive comparison of FtG's performance across all these different KGC filters, nor does it analyze the impact of filter choice on the final KGC results.
- What evidence would resolve it: Detailed performance metrics (MRR, Hits@1, Hits@3, Hits@10) for FtG using each of the different KGC filters on multiple benchmark datasets would clarify the impact of filter choice on performance.

### Open Question 2
- Question: What is the optimal size of the candidate entity set (k) in the filter-then-generate paradigm for maximizing FtG's performance?
- Basis in paper: [explicit] The paper mentions that the size of the candidate set is a hyperparameter that affects performance and presents results for different values (10, 20, 30, 40), but does not provide a definitive analysis of the optimal value.
- Why unresolved: The paper shows that increasing the candidate set size does not significantly impact performance, but does not determine the optimal size for balancing accuracy and computational efficiency.
- What evidence would resolve it: A detailed analysis of FtG's performance across a wider range of candidate set sizes, along with computational efficiency metrics, would identify the optimal value for different scenarios.

### Open Question 3
- Question: How does the ego-graph serialization prompt in FtG compare to other heuristics (e.g., random walk, entire one-hop ego-graph, two-hop ego-graph) in terms of performance and efficiency?
- Basis in paper: [explicit] The paper compares FtG's ego-graph serialization prompt with three other heuristics (random walk, entire one-hop ego-graph, and two-hop ego-graph) and shows that FtG outperforms these alternatives.
- Why unresolved: The paper does not provide a detailed analysis of why the ego-graph serialization prompt is superior or how it compares in terms of computational efficiency to the other heuristics.
- What evidence would resolve it: A comprehensive comparison of the computational efficiency and performance of FtG's ego-graph serialization prompt versus the other heuristics on multiple datasets would clarify the trade-offs involved.

### Open Question 4
- Question: What is the impact of the structure-text adapter's complexity on FtG's performance and computational efficiency?
- Basis in paper: [inferred] The paper mentions that the structure-text adapter is a lightweight component that maps graph features into the text embedding space, but does not explore the impact of using more complex adaptation schemes.
- Why unresolved: The paper does not investigate whether more complex adaptation schemes (e.g., cross-attention) would improve FtG's performance or if they would introduce significant computational overhead.
- What evidence would resolve it: Experiments comparing FtG's performance and computational efficiency using different levels of complexity in the structure-text adapter (e.g., simple linear projection vs. cross-attention) would determine the optimal balance between performance and efficiency.

## Limitations

- The effectiveness of the ego-graph pruning strategy relies heavily on the structure embedding similarity threshold, which is not specified in the paper.
- The filter-then-generate approach assumes the KGC filter has sufficient recall, but no analysis is provided on the recall rate of the filter method.
- The structure-text adapter uses a simple linear projection, but the paper provides no empirical comparison with more sophisticated alignment methods.

## Confidence

**High Confidence**: The core claim that filtering candidates before LLM processing improves KGC performance is well-supported by the experimental results showing consistent gains across all three datasets. The multiple-choice formulation is clearly explained and implemented.

**Medium Confidence**: The ego-graph serialization approach appears effective based on the reported performance improvements, but the lack of ablation studies isolating the ego-graph contribution makes it difficult to determine whether the gains come from the structural information itself or other factors in the training process.

**Low Confidence**: The claim that the structure-text adapter effectively bridges graph and text representations is supported only by indirect evidence (overall system performance) rather than direct analysis of the adapter's impact or comparison with alternative alignment methods.

## Next Checks

1. **Filter Recall Analysis**: Measure the recall rate of the RotatE filter (percentage of test triples where the correct entity appears in the top-k candidates) to quantify the upper bound on FtG performance and identify datasets where the filter may be the limiting factor.

2. **Ego-Graph Ablation**: Run controlled experiments comparing FtG with and without ego-graph serialization on a held-out validation set, using the same filter settings and k values, to isolate the contribution of structural information to performance gains.

3. **Adapter Architecture Comparison**: Implement and compare the linear structure-text adapter against alternative alignment approaches (e.g., MLP adapter, cross-attention mechanism) while keeping all other components constant, to determine whether the linear projection is truly sufficient for graph-text alignment.