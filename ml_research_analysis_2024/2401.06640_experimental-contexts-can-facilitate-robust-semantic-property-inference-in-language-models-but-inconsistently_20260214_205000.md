---
ver: rpa2
title: Experimental Contexts Can Facilitate Robust Semantic Property Inference in
  Language Models, but Inconsistently
arxiv_id: '2401.06640'
source_url: https://arxiv.org/abs/2401.06640
tags:
- comps
- examples
- heuristics
- property
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether language models can perform property
  inheritance robustly when given in-context examples and instructions. Using the
  COMPS dataset, which tests models' ability to infer semantic properties of novel
  concepts, the authors find that experimental contexts do improve performance in
  some cases.
---

# Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently
## Quick Facts
- arXiv ID: 2401.06640
- Source URL: https://arxiv.org/abs/2401.06640
- Reference count: 25
- Primary result: Experimental contexts improve property inheritance performance in LMs, but models often rely on shallow heuristics rather than true semantic understanding

## Executive Summary
This paper investigates whether language models can perform robust property inheritance when given in-context examples and instructions. Using the COMPS dataset, the authors find that experimental contexts do improve performance in some cases, but the models often rely on shallow, non-semantic heuristics rather than true understanding of the task. When the task is reformulated as a question-answering problem (COMPS-QA), performance drops significantly, suggesting the models are not mastering the underlying reasoning ability required for property inheritance. The results indicate that while experimental contexts can help models perform better on certain tasks, they do not necessarily indicate that the models have developed a deep understanding of the concepts involved.

## Method Summary
The study evaluates five language models (GPT-2 XL, OPT-6.7b, Llama-2-7b, Llama-2-13b, Mistral-7b) on the COMPS dataset using zero-shot and few-shot settings with varying numbers of in-context examples. The task tests property inheritance by asking models to infer semantic properties of novel concepts based on examples. Two task formulations are used: the original COMPS format and a reformulated COMPS-QA format that changes the output space to reduce heuristic exploitation. Different instruction templates with varying levels of detail are also tested to assess their impact on performance.

## Key Results
- In-context examples significantly improve property inheritance performance compared to zero-shot settings
- Models show inconsistent improvements across different experimental conditions and task formulations
- The COMPS-QA reformulation reveals that models often rely on shallow, position-based heuristics rather than semantic reasoning
- Instruction templates have minimal impact compared to in-context examples, though they can modulate performance to some degree

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context examples and instructions can improve property inheritance by shifting the model's output distribution toward task-appropriate patterns.
- Mechanism: The model learns to associate specific input structures with the correct reasoning pattern when shown examples that demonstrate the desired behavior.
- Core assumption: The model's attention mechanism can effectively generalize from a small number of examples to unseen but structurally similar cases.
- Evidence anchors:
  - [abstract] "they can indeed lead to non-trivial property inheritance behavior in LMs"
  - [section] "LMs can demonstrate radical improvements in the presence of experimental contexts"
  - [corpus] Weak evidence - no direct citations, only 0 average citations in neighbor papers

### Mechanism 2
- Claim: Models rely on shallow heuristics when the task formulation creates a clear link between output space and input features that control the heuristic.
- Mechanism: When the model's output (choice between two concepts) is directly tied to a feature in the input (relative ordering of concepts), the model may bypass semantic reasoning and use position-based heuristics instead.
- Core assumption: The model will prefer simpler, more directly observable patterns over complex semantic reasoning when both are available.
- Evidence anchors:
  - [abstract] "with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics"
  - [section] "LMs show a strong preference towards the heuristic, and are therefore at chance"
  - [corpus] Weak evidence - only 0 average citations in neighbor papers

### Mechanism 3
- Claim: Instruction templates with varying levels of detail can modulate model performance, but in-context examples are more critical than instructions alone.
- Mechanism: Instructions provide high-level guidance that shapes the model's approach, while in-context examples provide concrete demonstrations that the model can pattern-match to.
- Core assumption: The model can parse and utilize different instruction styles, and that more detailed instructions will lead to better performance.
- Evidence anchors:
  - [section] "instructions alone do not always account for the observed improvement—LMs' performance on zero-shot settings are consistently still at chance"
  - [section] "it is the in-context examples that critically alter models' output distribution"
  - [corpus] Weak evidence - no direct citations, only 0 average citations in neighbor papers

## Foundational Learning

- Concept: Property inheritance
  - Why needed here: Understanding how semantic properties transfer from known concepts to novel concepts is central to the task being evaluated.
  - Quick check question: If "A wug is a beaver" and "A dax is a gorilla", which concept should inherit the property "has a flat tail" based on semantic knowledge?

- Concept: Zero-shot vs. in-context learning
  - Why needed here: The paper contrasts performance in zero-shot settings with performance when given in-context examples, highlighting the importance of this distinction.
  - Quick check question: What is the key difference between zero-shot evaluation and in-context learning in terms of how the model is prompted?

- Concept: Heuristics vs. semantic reasoning
  - Why needed here: The paper distinguishes between models using shallow position-based heuristics versus deeper semantic understanding of the task.
  - Quick check question: In the COMPS-QA reformulation, what feature of the input might lead models to rely on position-based heuristics rather than semantic reasoning?

## Architecture Onboarding

- Component map: Input prompt (examples + instructions) -> Decoder-only transformer model -> Output log-probabilities of property-concept bindings
- Critical path: Input prompt → Model attention and processing → Output log-probabilities → Comparison of log-probabilities to determine accuracy
- Design tradeoffs: Using in-context examples improves performance but may lead to overfitting to specific patterns; instruction detail can help but may not overcome the need for examples
- Failure signatures: If the model shows high accuracy on cases where heuristics work but near-chance performance on cases where they don't, this indicates reliance on heuristics rather than semantic reasoning
- First 3 experiments:
  1. Evaluate a model on COMPS in zero-shot setting to establish baseline performance
  2. Evaluate the same model on COMPS with 2 in-context examples and minimal instructions to test for improvement
  3. Reformulate the task as COMPS-QA and evaluate with the same in-context examples to test for heuristic reliance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of in-context examples (e.g., varying in complexity or semantic content) affect the models' ability to perform property inheritance robustly?
- Basis in paper: [explicit] The paper uses 10 different sets of in-context examples, each containing 6 different COMPS stimuli, to ensure variability and avoid results due to one particular choice.
- Why unresolved: The paper does not provide a detailed analysis of how the specific characteristics of the in-context examples (e.g., complexity, semantic content) influence the models' performance. It only mentions that different sets are used for variability.
- What evidence would resolve it: Conducting experiments with in-context examples systematically varied in complexity or semantic content, and analyzing how these variations impact the models' ability to perform property inheritance.

### Open Question 2
- Question: To what extent do the findings generalize beyond the COMPS dataset to other meaning-sensitive evaluations?
- Basis in paper: [explicit] The paper acknowledges that focusing on a single dataset (COMPS) is a limitation and that different meaning-sensitive evaluations focus on different operationalizations of meaning.
- Why unresolved: The paper does not explore whether the findings about experimental contexts improving property inheritance behavior extend to other meaning-sensitive tasks or datasets.
- What evidence would resolve it: Replicating the experiments on a variety of meaning-sensitive datasets to see if the improvements from experimental contexts are consistent across different tasks.

### Open Question 3
- Question: What specific computational principles or mechanisms allow some models (e.g., Mistral-7b) to be more resilient to positional heuristics compared to others?
- Basis in paper: [explicit] The paper notes that Mistral-7b is less susceptible to heuristics compared to other models like GPT-2 XL and OPT-6.7b.
- Why unresolved: The paper does not investigate the underlying reasons for the differences in heuristic susceptibility among models.
- What evidence would resolve it: Conducting mechanistic analyses (e.g., probing the models' internal representations) to identify the specific principles or mechanisms that make some models more robust to heuristics.

## Limitations
- Single dataset focus: The study relies exclusively on the COMPS dataset, limiting generalizability to other meaning-sensitive tasks
- Heuristic vulnerability: Models show inconsistent performance and often rely on shallow heuristics rather than semantic understanding
- Limited instruction analysis: The paper does not thoroughly explore the impact of different instruction styles or their interaction effects with example quality

## Confidence

### Major Uncertainties
The paper's core finding that experimental contexts can improve property inheritance performance is supported by the results, but several limitations reduce confidence in the generalizability of these findings. The reliance on a single dataset (COMPS) and its reformulation (COMPS-QA) raises questions about whether the observed heuristic reliance would appear in other semantic tasks. Additionally, the paper does not thoroughly explore the training data overlap between the models and the COMPS dataset, which could explain some performance patterns.

### Confidence Assessment
**High confidence** in the finding that in-context examples improve performance compared to zero-shot settings, as this is consistently demonstrated across models and conditions. The evidence shows clear improvements when examples are provided.

**Medium confidence** in the claim that models rely on shallow heuristics rather than semantic understanding. While the COMPS-QA reformulation provides suggestive evidence, the paper could strengthen this claim by testing additional task variations and providing more detailed analysis of model attention patterns.

**Low confidence** in the assertion that instruction templates have minimal impact compared to in-context examples. The paper shows instructions alone don't match example performance, but doesn't systematically compare different instruction styles or their interaction effects with example quality.

## Next Checks
1. Test the same experimental setup on an additional semantic property inference dataset (such as the one from which COMPS was derived) to verify whether heuristic reliance generalizes beyond a single task formulation.

2. Analyze attention patterns in the models during COMPS-QA task performance to determine whether positional information is being preferentially weighted over semantic content in the input.

3. Conduct ablation studies varying both the number of in-context examples and instruction detail simultaneously to quantify the relative contribution of each component to final performance.