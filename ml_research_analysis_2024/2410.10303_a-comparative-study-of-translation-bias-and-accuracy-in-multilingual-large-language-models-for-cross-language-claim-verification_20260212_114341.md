---
ver: rpa2
title: A Comparative Study of Translation Bias and Accuracy in Multilingual Large
  Language Models for Cross-Language Claim Verification
arxiv_id: '2410.10303'
source_url: https://arxiv.org/abs/2410.10303
tags:
- llama
- language
- inference
- translation
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates translation bias and accuracy
  in multilingual LLMs for cross-lingual claim verification across 15 languages. It
  compares two translation methods - pre-translation using Google Translate API and
  self-translation performed by the LLM itself - against direct inference.
---

# A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification

## Quick Facts
- arXiv ID: 2410.10303
- Source URL: https://arxiv.org/abs/2410.10303
- Reference count: 37
- Primary result: Direct inference outperforms translation methods in high-resource languages, while translation methods better handle low-resource languages; translation bias remains consistent across model sizes

## Executive Summary
This study systematically evaluates translation bias and accuracy in multilingual LLMs for cross-lingual claim verification across 15 languages from five language families. The research compares three translation approaches - direct inference, self-translation by the LLM, and pre-translation using Google Translate - against the X-Fact dataset with 600 claims per language family. Results demonstrate that while larger models show superior self-translation accuracy, translation bias remains stable across model sizes, highlighting persistent challenges in achieving equitable multilingual capabilities.

## Method Summary
The study evaluates 15 languages across five families (Romance, Slavic, Turkic, Indo-Aryan, Kartvelian) using the X-Fact dataset containing 600 claims per language family with five veracity labels. Three translation methods are tested: direct inference in native language, self-translation where the LLM translates to English then verifies, and pre-translation using Google Translate API before LLM verification. Multiple multilingual LLMs including GPT-4o, Llama 3.1 variants, and mBERT are evaluated with temperature=0, measuring both accuracy percentage and translation bias scores using COMETKIWI model.

## Key Results
- Direct inference performs best in high-resource languages while translation methods better handle low-resource languages
- Larger models demonstrate superior self-translation accuracy but translation bias remains consistent across model sizes
- Self-translation performs slightly better than pre-translation due to internal consistency between translation and verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct inference performs better in high-resource languages due to better model training coverage.
- Mechanism: Models are trained on more data from high-resource languages, improving their native understanding and reducing reliance on translation.
- Core assumption: The amount and quality of training data for a language directly correlates with model performance in that language.
- Evidence anchors:
  - [abstract] "Direct inference performs best in high-resource languages while translation methods better handle low-resource languages."
  - [section 4.1.1] "Direct inference demonstrated significantly higher accuracy in the Romance, Slavic, and Turkic language families compared to other translation techniques."
  - [corpus] "Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis of LLM-Based Fact-Checking Reliability" suggests comparative analysis across languages, supporting the premise of differential performance.
- Break condition: If training data distribution is balanced across languages, the performance gap between high- and low-resource languages may narrow.

### Mechanism 2
- Claim: Self-translation improves accuracy over pre-translation by maintaining internal consistency.
- Mechanism: When the LLM performs both translation and verification, it leverages its own linguistic patterns, reducing inconsistencies that arise from external translation services.
- Core assumption: The LLM's internal translation process is more consistent with its verification process than external translations.
- Evidence anchors:
  - [abstract] "larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias."
  - [section 4.2.1] "Self-translation performs slightly better than pre-translation which we believe is attributed to the model maintaining internal consistency between generating and verifying translations."
  - [corpus] "Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking" suggests that LLMs can be used for translation tasks, supporting the concept of self-translation.
- Break condition: If external translation services are significantly more accurate than the LLM's internal translation, pre-translation might outperform self-translation.

### Mechanism 3
- Claim: Translation bias remains consistent across model sizes, indicating that larger models do not necessarily ensure fairness across languages.
- Mechanism: Despite improvements in accuracy with larger models, the underlying bias in translation quality persists, suggesting that model size alone is not sufficient to address translation bias.
- Core assumption: Translation bias is inherent to the training data and model architecture, not solely dependent on model size.
- Evidence anchors:
  - [abstract] "translation bias remains consistent across model sizes."
  - [section 4.3] "Interestingly, although self-translation performed better with larger models, the translation bias scores remained relatively stable, suggesting that increased model size improves accuracy but not fairness across languages."
  - [corpus] Weak evidence in corpus for this specific claim; requires further investigation into bias studies.
- Break condition: If targeted bias mitigation techniques are applied during training, translation bias might be reduced regardless of model size.

## Foundational Learning

- Concept: Cross-lingual claim verification
  - Why needed here: Understanding how to verify claims across different languages is crucial for evaluating the effectiveness of multilingual LLMs.
  - Quick check question: What are the key challenges in cross-lingual claim verification, and how do they differ from monolingual verification?
- Concept: Translation bias
  - Why needed here: Identifying and measuring translation bias is essential for assessing the fairness and accuracy of multilingual LLMs.
  - Quick check question: How does translation bias affect the performance of multilingual LLMs, and what metrics can be used to quantify it?
- Concept: Model scale and performance
  - Why needed here: Understanding the relationship between model size and performance is important for optimizing multilingual LLMs.
  - Quick check question: How does increasing model size impact the accuracy and bias of multilingual LLMs, and what are the limitations of this approach?

## Architecture Onboarding

- Component map:
  Data pipeline: X-Fact dataset ingestion and preprocessing -> Model zoo: Multiple multilingual LLMs (GPT-4o, Llama 3.1, mBERT, etc.) -> Translation methods: Direct inference, self-translation, pre-translation -> Evaluation framework: Accuracy and translation bias scoring -> Reporting: Performance metrics and analysis

- Critical path:
  1. Load and preprocess the X-Fact dataset for each language family.
  2. Run each LLM through all three translation methods for each language.
  3. Calculate accuracy scores and translation bias for each combination.
  4. Analyze results to identify trends and insights.

- Design tradeoffs:
  - Accuracy vs. fairness: Larger models improve accuracy but do not necessarily reduce bias.
  - Resource allocation: Balancing computational resources between model size and training data diversity.
  - Evaluation complexity: Comprehensive evaluation across multiple languages and methods increases complexity.

- Failure signatures:
  - High inconclusive rates: Indicates model difficulty in understanding or translating claims.
  - Consistent bias across models: Suggests inherent bias in training data or architecture.
  - Performance drop in low-resource languages: Points to insufficient training data coverage.

- First 3 experiments:
  1. Evaluate a single LLM (e.g., GPT-4o) across all three translation methods for one language family to establish baseline performance.
  2. Compare self-translation vs. pre-translation for a high-resource language to assess internal consistency benefits.
  3. Analyze translation bias scores for different model sizes to understand the impact of scale on fairness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model size continue to improve self-translation accuracy beyond the largest model tested (Llama 3.1 405B)?
- Basis in paper: [inferred] The paper shows improvement in self-translation accuracy as model size increases from 8B to 405B parameters, but doesn't test beyond this point.
- Why unresolved: The study only evaluated up to Llama 3.1 405B, leaving open whether even larger models would continue the trend.
- What evidence would resolve it: Testing self-translation accuracy with models larger than 405B parameters would show whether the improvement curve plateaus or continues.

### Open Question 2
- Question: Would including reference translations improve the evaluation of translation bias and quality?
- Basis in paper: [explicit] The paper explicitly states it did not incorporate reference translations in its evaluation, relying instead on COMETKIWI scores without references.
- Why unresolved: Without reference translations, the study may miss nuances in translation quality that could affect both accuracy and bias measurements.
- What evidence would resolve it: Comparing COMETKIWI scores with and without reference translations for the same translations would show if reference translations provide additional insights.

### Open Question 3
- Question: How would performance differ if translation were done into languages other than English?
- Basis in paper: [explicit] The paper explicitly states it only evaluated translations from non-English languages into English, citing training bias toward English data.
- Why unresolved: The study's focus on English as the translation target leaves open whether other language pairs might yield different results.
- What evidence would resolve it: Testing translation accuracy and bias when translating between various non-English language pairs would reveal if English-centric translation is optimal.

## Limitations

- Translation bias remains consistent across model sizes, suggesting that increasing model size alone does not address fairness issues in multilingual capabilities.
- The study only evaluates translations from non-English languages into English, limiting understanding of cross-language pair performance.
- Reference translations were not incorporated in the evaluation, potentially missing nuances in translation quality assessment.

## Confidence

- **High Confidence**: Claims about comparative performance between translation methods (direct inference, self-translation, pre-translation) are well-supported by the abstract and methodology description.
- **Medium Confidence**: Claims about model size effects on self-translation accuracy are supported but the mechanism linking size to improved accuracy isn't fully explained.
- **Low Confidence**: The assertion that translation bias remains consistent across model sizes lacks strong supporting evidence in the corpus.

## Next Checks

1. **Bias Consistency Across Scales**: Conduct a controlled experiment measuring COMETKIWI translation bias scores across at least 3 model sizes (small, medium, large) for the same language families to verify if bias scores remain stable as claimed.

2. **Training Data Distribution Analysis**: Obtain or estimate the actual training data distribution across languages for the evaluated models and correlate this with performance gaps between high- and low-resource languages to validate the training coverage hypothesis.

3. **Self-Translation vs Pre-Translation Quality**: Implement a blind quality assessment where human evaluators rate translation quality for both self-translation and pre-translation methods on the same claims, to determine if internal consistency truly drives the performance difference observed.