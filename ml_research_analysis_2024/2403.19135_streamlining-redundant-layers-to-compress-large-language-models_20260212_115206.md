---
ver: rpa2
title: Streamlining Redundant Layers to Compress Large Language Models
arxiv_id: '2403.19135'
source_url: https://arxiv.org/abs/2403.19135
tags:
- pruning
- layer
- ours
- benchmarks
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-Streamline is a layer pruning and replacement algorithm for
  large language models (LLMs) that identifies and removes consecutive layers with
  the lowest importance, then trains a lightweight network to replace the pruned layers.
  The method is based on measuring layer importance through cosine similarity between
  input and output hidden states.
---

# Streamlining Redundant Layers to Compress Large Language Models

## Quick Facts
- **arXiv ID:** 2403.19135
- **Source URL:** https://arxiv.org/abs/2403.19135
- **Reference count:** 25
- **Primary result:** Prunes 25% of parameters from Llama2-7B/13B with 93% classification accuracy and 77% generation performance retained

## Executive Summary
LLM-Streamline is a novel layer pruning and replacement algorithm that identifies and removes consecutive layers with minimal impact on hidden states, then trains a lightweight network to replace the pruned layers. The method uses cosine similarity between input and output hidden states to measure layer importance, removing layers that contribute least to the model's transformation of information. A lightweight network (typically an FFN) is trained to approximate the cumulative effect of the pruned layers, maintaining performance while significantly reducing model size.

## Method Summary
The LLM-Streamline approach consists of two main components: layer pruning and layer replacement. Layer pruning identifies consecutive layers with the lowest importance based on cosine similarity between their input and output hidden states - higher similarity indicates lower importance. The layer replacement component trains a lightweight network to approximate the combined transformation of the pruned layers, minimizing MSE loss between the lightweight network output and the original layer outputs. The method achieves significant compression (25% parameter reduction) while maintaining 93% classification accuracy and 77% generation performance on tested benchmarks.

## Key Results
- Achieves 93% retained classification accuracy when pruning 25% of parameters from Llama2-7B and Llama2-13B
- Maintains 77% performance on generation tasks (XSum, GSM8K, StrategyQA) after 25% parameter reduction
- Outperforms existing state-of-the-art pruning methods (LLM-Pruner, SliceGPT, LaCo) on both classification and generation benchmarks
- Introduces stability metric that addresses accuracy overestimation in compressed models by considering prediction consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer redundancy can be detected by measuring cosine similarity between input and output hidden states of consecutive layers.
- Mechanism: If a layer's transformation of hidden states is minimal, the cosine similarity between its input and output will be high, indicating low importance.
- Core assumption: Layers that minimally transform hidden states are redundant and can be pruned without significant performance loss.
- Evidence anchors:
  - [abstract] "It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned."
  - [section 2.1] "We use the cosine similarity cos (·, ·) between input x(ℓ) and output x(ℓ+1) as a metric. Essentially, a higher cosine similarity between the input and output of a layer indicates lower importance, and vice versa."
  - [corpus] Weak evidence - the corpus contains related layer pruning papers but does not directly address cosine similarity as a pruning metric.
- Break condition: If layers with high cosine similarity still contribute essential information that cannot be approximated by a lightweight network, pruning will degrade performance.

### Mechanism 2
- Claim: A lightweight network can effectively approximate the cumulative transformation of pruned layers.
- Mechanism: The lightweight network is trained to minimize MSE loss between its output and the combined output of the pruned layers, effectively distilling their functionality.
- Core assumption: The collective effect of several consecutive layers can be represented by a simpler model when those layers have minimal individual impact.
- Evidence anchors:
  - [abstract] "LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss."
  - [section 2.3] "Therefore, we hypothesize that the cumulative effect of these layers can be approximated by a lightweight network."
  - [corpus] Weak evidence - the corpus contains related layer pruning papers but does not directly address lightweight network replacement for pruned layers.
- Break condition: If the lightweight network cannot adequately capture the complex interactions between pruned layers, performance will degrade.

### Mechanism 3
- Claim: The stability metric provides a more accurate evaluation of model compression by accounting for prediction consistency.
- Mechanism: Stability weights samples by the original model's prediction confidence and only counts samples where both models agree on correctness, addressing overestimation from random guessing.
- Core assumption: A compressed model that guesses correctly on previously uncertain samples should not be considered as improved performance.
- Evidence anchors:
  - [abstract] "Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression."
  - [section 3.2] "We propose a novel metric stability to evaluate the performance of LLMs after pruning, i.e., Stability(M, ¯M) = (PN i=1 exp (stdi) · 1 [i∈TP∪TN]) / (PN i=1 exp (stdj))"
  - [corpus] Weak evidence - the corpus contains related layer pruning papers but does not directly address stability as a compression evaluation metric.
- Break condition: If the stability metric becomes too conservative and fails to recognize genuine improvements in model robustness.

## Foundational Learning

- **Concept: Transformer architecture and residual connections**
  - Why needed here: Understanding how layers transform hidden states is fundamental to identifying redundant layers
  - Quick check question: What is the mathematical representation of a transformer layer's transformation on hidden states?

- **Concept: Cosine similarity and its properties**
  - Why needed here: Cosine similarity is used as the primary metric for measuring layer importance
  - Quick check question: Why does cosine similarity provide a magnitude-agnostic measure of layer importance?

- **Concept: Model compression metrics and their limitations**
  - Why needed here: Understanding why accuracy can overestimate compressed model performance is crucial for the stability metric
  - Quick check question: How can a compressed model appear to have higher accuracy through random guessing?

## Architecture Onboarding

- **Component map:** Original LLM → Layer pruning (removes consecutive low-importance layers) → Lightweight network training (distills pruned layer functionality) → Layer replacement (integrates lightweight network)
- **Critical path:** Data collection → Layer importance computation → Layer pruning → Lightweight network training → Evaluation
- **Design tradeoffs:** Simpler lightweight networks (FFN) vs. more complex (Transformer layer) - simpler trains faster but may lose fidelity; complex may capture more nuance but requires more resources
- **Failure signatures:** Significant performance drop after pruning suggests either incorrect layer identification or inadequate lightweight network capacity
- **First 3 experiments:**
  1. Measure cosine similarity across layers on a small dataset to verify the redundancy pattern
  2. Test lightweight network architectures (FFN vs. Transformer layer) on a small model to find optimal balance
  3. Compare accuracy vs. stability metrics on a simple classification task to demonstrate stability's effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM-Streamline perform when pruning more than 50% of parameters from larger models like Llama3.1-70B?
- **Basis in paper:** [explicit] The paper mentions comparing performance at a higher pruning ratio of approximately 50% on Llama2-7B, but doesn't extend this analysis to larger models
- **Why unresolved:** The paper only tested higher pruning ratios on Llama2-7B, leaving uncertainty about scalability to larger models with more parameters to prune
- **What evidence would resolve it:** Experimental results showing performance metrics (accuracy, stability) for Llama3.1-70B pruned at 50-70% parameter reduction

### Open Question 2
- **Question:** Can the layer replacement approach be extended to prune non-consecutive layers while maintaining performance?
- **Basis in paper:** [inferred] The paper focuses on pruning consecutive layers with lowest importance, but doesn't explore whether the layer replacement strategy would work for more complex pruning patterns
- **Why unresolved:** The method's effectiveness is only demonstrated for consecutive layer pruning, and the lightweight network design might not generalize to irregular layer removal
- **What evidence would resolve it:** Comparative experiments showing performance of layer replacement when pruning non-consecutive layers versus the current consecutive approach

### Open Question 3
- **Question:** What is the theoretical limit of performance degradation when using extremely lightweight replacement networks?
- **Basis in paper:** [explicit] The paper tests various lightweight network architectures but doesn't establish the minimum viable network size before performance collapses
- **Why unresolved:** While the paper shows FFN performs best among tested architectures, it doesn't explore the performance bounds as network size approaches zero
- **What evidence would resolve it:** Systematic experiments varying the size of replacement networks from full layer size down to minimal configurations, measuring the performance drop-off point

## Limitations
- The method's effectiveness on diverse architectures beyond Llama2 models remains uncertain, with limited validation on Mixtral and Llama3.1 models
- The lightweight network's capacity to capture complex layer interactions may have scalability limits for higher compression ratios or more complex models
- The stability metric requires careful implementation and may be overly conservative in distinguishing genuine performance improvements from random guessing effects

## Confidence

- **High Confidence:** The basic observation that consecutive layers can be pruned when they show high input-output similarity is well-supported by experimental results across multiple models and benchmarks.
- **Medium Confidence:** The lightweight network replacement approach works effectively for 25% pruning but may have scalability limits for higher compression ratios or more complex models.
- **Low Confidence:** The stability metric's practical utility beyond avoiding overestimation - whether it meaningfully distinguishes between genuine performance improvements versus random guessing effects needs more empirical validation.

## Next Checks

1. **Architecture Generalization Test:** Apply the same cosine similarity-based pruning to a diverse set of architectures (Mixtral, Llama3.1, OPT) and compare layer importance rankings to identify if the metric generalizes across different design choices.

2. **Lightweight Network Capacity Analysis:** Systematically vary the lightweight network architecture (FFN vs. SwiGLU vs. Transformer layer) and measure the relationship between model capacity and reconstruction fidelity on held-out data.

3. **Stability Metric Validation:** Design experiments where a compressed model performs slightly worse than baseline but on different samples, to test whether stability correctly identifies this as degradation rather than improvement.