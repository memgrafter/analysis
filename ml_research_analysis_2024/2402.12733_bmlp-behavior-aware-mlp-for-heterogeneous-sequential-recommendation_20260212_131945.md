---
ver: rpa2
title: 'BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation'
arxiv_id: '2402.12733'
source_url: https://arxiv.org/abs/2402.12733
tags:
- behavior
- recommendation
- sequential
- behaviors
- bmlp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses heterogeneous sequential recommendation by
  proposing a pure MLP-based architecture called BMLP. It tackles the challenge of
  modeling diverse user behaviors (e.g., clicks, purchases) by introducing two key
  modules: HIP for multi-granularity interest modeling using behavior types and transitions,
  and PIP for capturing purchase intent through auxiliary behavior subsequences.'
---

# BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation

## Quick Facts
- arXiv ID: 2402.12733
- Source URL: https://arxiv.org/abs/2402.12733
- Reference count: 40
- Primary result: HR@10 improvements of up to 0.061 and NDCG@10 improvements of up to 0.088 over state-of-the-art methods

## Executive Summary
BMLP addresses heterogeneous sequential recommendation by introducing a pure MLP-based architecture that models diverse user behaviors (clicks, purchases, favorites, carts) through two key modules: HIP for multi-granularity interest modeling and PIP for purchase intent perception. The model tackles the challenge of capturing both the sequence of behaviors and their semantic meaning within a unified MLP framework. Experiments on four public datasets demonstrate significant performance gains over state-of-the-art methods while maintaining superior computational efficiency with linear time complexity.

## Method Summary
BMLP is a pure MLP-based architecture designed for heterogeneous sequential recommendation that captures diverse user behaviors through behavior-aware modeling. The model introduces two key modules: HIP (Heterogeneous Interest Perception) for modeling multi-granularity interests using behavior types and transitions, and PIP (Purchase Intent Perception) for capturing purchase intent through auxiliary behavior subsequences. The architecture uses transposed input tensors and multi-head feature capture to model sequential dependencies. The model is trained using Adam optimizer with dropout, learning rate 0.01, batch size 512, and L2 regularization. Experiments on RecSys Challenge 2015, Tmall, MovieLens 1M, and Taobao datasets show significant improvements in HR@10, HR@20, NDCG@10, and NDCG@20 metrics.

## Key Results
- HR@10 improvements of up to 0.061 compared to best baselines
- NDCG@10 improvements of up to 0.088 over state-of-the-art methods
- Superior computational efficiency with linear time complexity
- Consistent performance gains across all four tested datasets

## Why This Works (Mechanism)
BMLP's effectiveness stems from its ability to model heterogeneous user behaviors within a pure MLP framework without requiring complex attention mechanisms or transformers. The HIP module captures multi-granularity interests by explicitly modeling behavior type embeddings and their transitions, allowing the model to understand not just what users interacted with but how their interaction patterns evolve. The PIP module specifically focuses on purchase intent by leveraging auxiliary behaviors (clicks, favorites, carts) that precede purchases, creating a more nuanced understanding of user purchase journeys. The transposed input tensor approach enables the model to capture sequential dependencies efficiently while maintaining the benefits of MLP's computational simplicity.

## Foundational Learning
- Heterogeneous sequential recommendation: Understanding different types of user interactions (clicks, purchases, etc.) in sequence
  - Why needed: Real-world user behavior involves multiple interaction types that need to be modeled differently
  - Quick check: Dataset contains multiple behavior type columns beyond just item IDs
- Behavior transition modeling: Capturing how users move between different behavior types
  - Why needed: Purchase patterns often involve specific sequences of interactions (click → cart → purchase)
  - Quick check: Transition frequency matrix shows meaningful patterns between behavior types
- Multi-head feature capture: Using multiple parallel processing streams to extract different aspects of features
  - Why needed: Different heads can capture different semantic aspects of the same input
  - Quick check: Different head outputs show distinct patterns when visualized
- Transposed input tensors: Reorganizing input data to capture sequential dependencies in MLP-friendly format
  - Why needed: MLPs process fixed-size inputs but sequential data needs special handling
  - Quick check: Input tensor shape matches expected MLP input dimensions

## Architecture Onboarding

Component map: Input → Embedding → HIP (Behavior Embeddings + Transition Embeddings) → PIP (Auxiliary Behavior Modeling) → MLP Layers → Prediction

Critical path: User behavior sequence → Behavior type and item embeddings → HIP module processing → PIP module processing → MLP layers → Next item prediction

Design tradeoffs: Pure MLP vs. attention-based models (computational efficiency vs. expressiveness), fixed sequence length vs. variable length handling (simplicity vs. flexibility), behavior type granularity vs. model complexity (detailed modeling vs. overfitting risk)

Failure signatures: Poor performance on datasets with many behavior types, degradation when sequence length exceeds 50, overfitting on datasets with limited auxiliary behaviors

First experiments:
1. Test on MovieLens 1M with L=50, L'=5, and verify basic functionality
2. Ablation study removing PIP module to measure purchase intent contribution
3. Performance comparison with Transformer-based baselines on same datasets

## Open Questions the Paper Calls Out

Open Question 1: How would the BMLP model perform on datasets with even more diverse behavior types beyond the four considered in the experiments?
- Basis in paper: The paper focuses on datasets with up to four behavior types (click, favorite, cart, buy) and demonstrates strong performance, but does not explore scenarios with more diverse behavior types
- Why unresolved: The paper does not test the model's scalability to datasets with a larger variety of behavior types, which is common in real-world scenarios
- What evidence would resolve it: Experiments on datasets with a wider range of behavior types (e.g., share, comment, rate) to evaluate the model's adaptability and performance

Open Question 2: What is the impact of varying the sequence length L on the model's performance, particularly for datasets with shorter or longer user interaction sequences?
- Basis in paper: The paper fixes the sequence length L = 50 and the auxiliary behavior subsequence length L′ = 5, but does not explore the impact of varying these lengths
- Why unresolved: The optimal sequence length may vary depending on the dataset characteristics, and the paper does not provide insights into how different lengths affect performance
- What evidence would resolve it: Experiments with varying L and L′ values to determine the optimal sequence lengths for different dataset characteristics and user behavior patterns

Open Question 3: How does the BMLP model handle cold-start scenarios where users or items have very few interactions?
- Basis in paper: The paper does not address cold-start scenarios, which are common challenges in recommendation systems
- Why unresolved: Cold-start problems can significantly impact the model's ability to make accurate recommendations for new users or items with limited interaction history
- What evidence would resolve it: Experiments on datasets with cold-start scenarios or the development of strategies to incorporate side information (e.g., item attributes, user demographics) to improve recommendations for new users and items

## Limitations
- Exact MLP layer configurations within HIP and PIP modules not fully specified
- Behavior transition embedding implementation details unclear
- Transposed input tensor mechanism not completely described
- No evaluation on datasets with more than four behavior types

## Confidence
Medium: The core methodology is described but key implementation details are missing
- HR@10 improvements of up to 0.061: Medium confidence (depends on exact implementation)
- NDCG@10 improvements of up to 0.088: Medium confidence (requires precise hyperparameter tuning)
- Linear time complexity claim: Low confidence (cannot verify without complete implementation)

## Next Checks
1. Verify the exact MLP layer configurations within HIP and PIP modules by contacting authors for implementation details
2. Implement and test the behavior transition embedding mechanism on a small sample dataset
3. Reproduce results on one dataset (e.g., MovieLens 1M) with multiple hyperparameter combinations from the specified ranges to assess robustness of reported improvements