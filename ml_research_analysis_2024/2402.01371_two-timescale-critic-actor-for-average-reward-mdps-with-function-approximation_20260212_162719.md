---
ver: rpa2
title: Two-Timescale Critic-Actor for Average Reward MDPs with Function Approximation
arxiv_id: '2402.01371'
source_url: https://arxiv.org/abs/2402.01371
tags:
- critic
- have
- actor
- where
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first two-timescale critic-actor algorithm
  with function approximation for the long-run average reward setting in Markov decision
  processes. The key innovation is reversing the timescales of the actor and critic
  updates, with the critic running on a slower timescale.
---

# Two-Timescale Critic-Actor for Average Reward MDPs with Function Approximation

## Quick Facts
- arXiv ID: 2402.01371
- Source URL: https://arxiv.org/abs/2402.01371
- Reference count: 40
- Key outcome: First two-timescale critic-actor algorithm with function approximation for average reward MDPs, achieving O(ε^-(2+δ)) sample complexity

## Executive Summary
This paper introduces the first two-timescale critic-actor algorithm with function approximation for long-run average reward Markov decision processes. The key innovation reverses the traditional actor-critic timescales, with the critic updating more slowly than the actor. This enables the algorithm to track value iteration rather than policy iteration, yielding improved sample complexity. The authors provide both finite-time non-asymptotic and asymptotic convergence analysis, proving the algorithm converges almost surely to local maxima of a perturbed average reward objective. Empirical results on three OpenAI Gym environments demonstrate superior performance compared to standard actor-critic, DQN, and PPO variants.

## Method Summary
The algorithm implements a two-timescale stochastic approximation framework where the critic uses linear function approximation and updates on a slower timescale (βt = o(αt)), while the actor and average reward estimator update on a faster timescale. Temporal difference learning drives the critic updates, while the actor uses stochastic policy gradient methods. Both components are projected onto compact sets to ensure stability. The learning rates follow power-law schedules with carefully chosen exponents to satisfy the two-timescale conditions while optimizing sample complexity.

## Key Results
- Achieves sample complexity of O(ε^-(2+δ)) for critic mean squared error, improving upon existing two-timescale actor-critic algorithms
- First convergence analysis for two-timescale critic-actor algorithms with function approximation in average reward settings
- Demonstrates asymptotic stability and almost sure convergence to local maxima of perturbed average reward objective
- Outperforms AC, DQN, PPO-AC, and PPO-CA variants on Frozen Lake, Pendulum, and Mountain Car Continuous environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reversing the actor and critic timescales enables the algorithm to track value iteration instead of policy iteration, providing better sample complexity.
- **Mechanism**: By making the critic slower, it appears quasi-static to the faster actor, which can then track a sequence of local maxima associated with different critic values. This setup emulates value iteration, which converges faster than policy iteration in this context.
- **Core assumption**: The timescale separation is sufficient to maintain stability and convergence while enabling faster tracking of optimal policies.
- **Break condition**: If the timescale separation is insufficient, the algorithm may become unstable or fail to track the optimal policy sequence.

### Mechanism 2
- **Claim**: The sample complexity of O(ε^-(2+δ)) is achieved by optimizing the learning rates ν and σ within the constraints of the two-timescale framework.
- **Mechanism**: By choosing ν = 0.5 and σ = 0.5 + β (where β > 0 can be arbitrarily small), the algorithm achieves near-optimal sample complexity while maintaining asymptotic stability guarantees within the two-timescale framework constraints.
- **Core assumption**: The chosen learning rates satisfy the Robbins-Monro conditions and the constraints 2σ < 3ν and 2σ - ν < 1.
- **Break condition**: If the learning rates are not properly tuned, the algorithm may not achieve the claimed sample complexity or may become unstable.

### Mechanism 3
- **Claim**: The algorithm maintains stability and converges almost surely by tracking a compact connected internally chain transitive invariant set of an associated differential inclusion.
- **Mechanism**: The slower critic recursion tracks a limiting differential inclusion that depends on the set of local maxima of the actor recursion. This analysis generalizes the ODE approach to stochastic approximation and handles multiple attractors, ensuring asymptotic stability and almost sure convergence.
- **Core assumption**: The Markov chain generated by the policy is ergodic and the step-size sequences satisfy the Robbins-Monro conditions.
- **Break condition**: If the Markov chain is not ergodic or the step-size conditions are violated, the algorithm may fail to converge or become unstable.

## Foundational Learning

- **Concept: Two-timescale stochastic approximation**
  - Why needed here: The algorithm relies on timescale separation between the actor and critic updates to maintain stability and enable faster convergence.
  - Quick check question: What happens if the critic and actor updates proceed on the same timescale instead of different timescales?

- **Concept: Markov decision processes and average reward criterion**
  - Why needed here: The algorithm is designed for the long-run average reward setting in MDPs, requiring understanding of differential value functions and policy gradient theorems.
  - Quick check question: How does the average reward criterion differ from the discounted reward criterion in MDPs?

- **Concept: Function approximation and linear architectures**
  - Why needed here: The algorithm uses linear function approximation for the critic, requiring understanding of feature mappings and projection operators.
  - Quick check question: What are the advantages and limitations of using linear function approximation versus nonlinear architectures?

## Architecture Onboarding

- **Component map**: Actor (fast) -> Critic (slow) -> Average reward estimator (fast) -> Projection operators

- **Critical path**:
  1. Initialize actor, critic, and average reward parameters
  2. Sample state, action, and next state from MDP
  3. Update average reward estimate
  4. Compute temporal difference error
  5. Update critic parameters (slower timescale)
  6. Update actor parameters (faster timescale)
  7. Repeat steps 2-6

- **Design tradeoffs**:
  - Two timescales vs. single timescale: Two timescales provide better stability and convergence guarantees but require careful tuning of learning rates
  - Linear vs. nonlinear function approximation: Linear approximation enables theoretical analysis but may limit representational capacity
  - Projected vs. unprojected critic: Projection ensures stability but may introduce bias; unprojected critic has better representation but requires additional stability analysis

- **Failure signatures**:
  - Instability: Large oscillations in parameter values or divergence of updates
  - Slow convergence: Parameters plateau at suboptimal values or take too long to converge
  - Poor performance: Low average reward or high variance in results across different seeds

- **First 3 experiments**:
  1. Implement the algorithm on a simple MDP (e.g., Frozen Lake) with small state and action spaces to verify basic functionality
  2. Compare the algorithm's performance against traditional AC and DQN on the same MDP to validate improvements
  3. Test the algorithm's sensitivity to learning rate choices by varying ν and σ within the allowed ranges and observing impact on convergence and sample complexity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the two-timescale critic-actor algorithm be extended to settings with non-linear function approximation while maintaining the same convergence guarantees?
- **Basis in paper**: [explicit] The paper uses linear function approximation for theoretical analysis but does not explore non-linear architectures
- **Why unresolved**: Linear approximation simplifies analysis but may limit practical applicability where non-linear representations could capture more complex value functions
- **What evidence would resolve it**: Experiments showing convergence and performance with non-linear critics (e.g., neural networks with multiple layers) and corresponding theoretical analysis proving stability and convergence

### Open Question 2
- **Question**: How does the performance of the critic-actor algorithm compare to other actor-critic variants in continuous control tasks with high-dimensional state spaces?
- **Basis in paper**: [inferred] The paper tests on three OpenAI Gym environments but does not explore high-dimensional continuous control tasks like robotics or autonomous driving
- **Why unresolved**: The current benchmarks are relatively low-dimensional and may not capture challenges of scaling to more complex real-world applications
- **What evidence would resolve it**: Comparative experiments on benchmark continuous control tasks like MuJoCo or PyBullet environments with performance metrics and training time analysis

### Open Question 3
- **Question**: Can the sample complexity of O(ε^-(2+δ)) be further improved by modifying the algorithm structure or using different optimization techniques?
- **Basis in paper**: [explicit] The paper achieves O(ε^-(2+δ)) which is better than two-timescale AC but still has room for improvement compared to single-timescale AC's O(ε^-2)
- **Why unresolved**: The authors note that δ > 0 can be made arbitrarily small but not zero, suggesting a fundamental gap that needs addressing
- **What evidence would resolve it**: Analysis showing tighter bounds through alternative algorithmic modifications or demonstrating matching the single-timescale AC complexity while preserving asymptotic guarantees

## Limitations
- Theoretical analysis is confined to tabular case with linear function approximation, limiting practical applicability
- Performance advantages demonstrated on only three OpenAI Gym environments, limiting generalizability
- Computational overhead of maintaining two timescales and projection operators may offset sample complexity gains

## Confidence

- **High confidence**: Sample complexity claim of O(ε^-(2+δ)) for critic mean squared error and asymptotic convergence to local maxima of perturbed average reward objective
- **Medium confidence**: Empirical performance superiority over baseline algorithms due to limited scope and lack of statistical significance testing
- **Medium confidence**: Stability and convergence guarantees due to assumptions about ergodic Markov chains and specific learning rate conditions

## Next Checks
1. Scale the empirical evaluation to at least 10 diverse control and game environments (including both discrete and continuous action spaces) to assess generalizability
2. Conduct ablation studies by systematically varying the timescale separation parameter σ-ν and learning rate exponents to identify sensitivity and compare against theoretical predictions
3. Analyze computational efficiency by measuring wall-clock time and comparing against sample complexity improvements to calculate actual environmental interactions required versus reported sample complexities