---
ver: rpa2
title: 'MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models'
arxiv_id: '2412.09818'
source_url: https://arxiv.org/abs/2412.09818
tags:
- speech
- audio
- language
- wang
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERaLiON-AudioLLM is the first speech-text model designed for Singapore's
  multilingual landscape, integrating advanced speech and text processing to address
  local accents and dialects. The model uses a fusion-based architecture combining
  MERaLiON-Whisper with SEA-LION V3, employing an MLP adaptor module to align audio
  and text embeddings.
---

# MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models

## Quick Facts
- arXiv ID: 2412.09818
- Source URL: https://arxiv.org/abs/2412.09818
- Authors: Yingxu He; Zhuohan Liu; Shuo Sun; Bin Wang; Wenyu Zhang; Xunlong Zou; Nancy F. Chen; Ai Ti Aw
- Reference count: 13
- Primary result: First speech-text model designed for Singapore's multilingual landscape, achieving 5% WER on local ASR datasets

## Executive Summary
MERaLiON-AudioLLM introduces a fusion-based architecture that bridges audio and language processing for Singapore's multilingual environment. The model combines MERaLiON-Whisper (fine-tuned Whisper-large-v2) with SEA-LION V3 through an MLP adaptor module, enabling joint processing of audio and text inputs. Trained on 260,000 hours of data including the National Speech Corpus, it demonstrates competitive performance across six speech-text tasks while addressing local linguistic nuances.

## Method Summary
The model employs a fusion architecture where MERaLiON-Whisper serves as the audio encoder, SEA-LION V3 as the text decoder, and an MLP-100 adaptor module aligns their embeddings. The adaptor reduces sequence length by factor 15 and transforms audio embeddings to match the text decoder's 3854-dimensional space. Training involves two stages: fine-tuning the audio encoder on local Singapore datasets, followed by multimodal instruction fine-tuning on diverse tasks. The approach avoids error propagation from cascaded models while maintaining computational efficiency.

## Key Results
- Achieves 5% word error rate on MNSC-ASR-Part 2 Singapore-specific dataset
- Demonstrates competitive performance across ASR, ST, SQA, SDS, SI, and PARA tasks
- Shows particular strength on local datasets while maintaining general language capabilities

## Why This Works (Mechanism)

### Mechanism 1
The MLP adaptor module effectively aligns audio embeddings to text embedding space through SiLU-activated linear transformations. It reshapes encoder output by concatenating 15 time steps, then applies upward and downward projections to match the 3854-dimensional text decoder space.

### Mechanism 2
Fine-tuning the Whisper encoder on local datasets captures Singapore's linguistic nuances by adapting from general-purpose to region-specific speech patterns through exposure to National Speech Corpus and other local data.

### Mechanism 3
The fusion architecture avoids error propagation by processing audio and text jointly through the adaptor module, rather than passing intermediate transcriptions through separate modules.

## Foundational Learning

- Concept: Multimodal representation alignment
  - Why needed here: MERaLiON-AudioLLM must process audio and text inputs together, requiring their representations to exist in compatible spaces for joint reasoning.
  - Quick check question: Why can't we simply concatenate audio and text embeddings directly without the adaptor module?

- Concept: Curriculum learning for multimodal tasks
  - Why needed here: The model handles diverse tasks with varying complexity, suggesting staged training could improve performance.
  - Quick check question: How might training on simpler tasks first improve performance on complex multimodal reasoning tasks?

- Concept: Catastrophic forgetting in multimodal fine-tuning
  - Why needed here: The model fine-tunes both the audio encoder and text decoder, risking loss of general language understanding capabilities.
  - Quick check question: What strategies could prevent the model from forgetting its text-only instruction-following capabilities during multimodal training?

## Architecture Onboarding

- Component map: Audio → Encoder → Adaptor → Decoder → Text Output
- Critical path: Audio → MERaLiON-Whisper → MLP-100 → SEA-LION V3 → Text Response
- Design tradeoffs:
  - Fusion vs. Cascaded: Fusion avoids error propagation but requires careful alignment; cascaded is simpler but accumulates errors
  - Adaptor Complexity: Simple MLP is computationally efficient but may miss complex relationships; more complex modules showed slightly worse results
  - Context Length: Current 30-second limit balances performance and computational cost; longer contexts would improve conversational ability but increase complexity
- Failure signatures:
  - ASR degradation on local datasets indicates encoder fine-tuning issues
  - Poor performance on paralinguistics tasks suggests adaptor or decoder limitations
  - Loss of instruction-following capability indicates catastrophic forgetting
  - High computational cost or slow inference suggests adaptor or fusion overhead
- First 3 experiments:
  1. Ablation study: Test MLP adaptor vs. Qformer vs. ConvMLP on local ASR datasets to validate design choice
  2. Fine-tuning analysis: Compare performance on local vs. general datasets to measure adaptation effectiveness
  3. Cascaded baseline: Implement cascaded Whisper + SEA-LION model to quantify error propagation benefits of fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal adaptor architecture for aligning audio encoder and text decoder embeddings in AudioLLMs?
- Basis in paper: The paper compares MLP-100 adaptor with alternatives like window-level Qformer and ConvMLP, finding MLP-100 "slightly better" but without exhaustive exploration
- Why unresolved: The comparison is limited to three architectures without systematic ablation studies or exploration of other potential adaptor designs
- What evidence would resolve it: Comprehensive head-to-head comparisons across multiple tasks and datasets, including larger MLP architectures, attention-based adaptors, and learned token selection mechanisms

### Open Question 2
- Question: How does training AudioLLMs on diverse multilingual datasets affect their performance on region-specific languages like Singlish?
- Basis in paper: The authors note that "its performance on these tasks can be further enhanced with additional data" for multilingual tasks, but don't provide empirical evidence
- Why unresolved: The paper trains on Singapore-specific data but doesn't systematically evaluate the trade-offs between multilingual versus region-specific training
- What evidence would resolve it: Controlled experiments comparing models trained on different mixes of multilingual versus Singapore-specific data, with detailed analysis of performance on Singlish versus other languages

### Open Question 3
- Question: What is the optimal training strategy to prevent catastrophic forgetting of text instruction-following capabilities when fine-tuning for speech tasks?
- Basis in paper: The authors identify catastrophic forgetting as a limitation, noting the model "lose[s] the ability to follow certain text instructions" after speech-focused fine-tuning
- Why unresolved: While the paper proposes potential solutions (replay collections, direct audio-text alignment), these remain unexplored and no empirical results are provided
- What evidence would resolve it: Comparative experiments testing different mitigation strategies including replay buffers, progressive learning schedules, and architectural modifications, with quantitative measures of instruction-following retention

## Limitations
- Performance degradation on larger, more diverse datasets suggests potential overfitting to local data
- 30-second context window may limit applicability to extended conversations or complex speech scenarios
- Limited ablation studies make it difficult to assess the adaptor module's specific contribution

## Confidence

- **High Confidence**: The architectural design and fusion approach are well-documented and technically sound. The use of MERaLiON-Whisper and SEA-LION V3 as base components is clearly specified.
- **Medium Confidence**: Performance claims on Singapore-specific datasets are supported by direct evaluation, but results on general benchmarks show mixed performance with notable degradation on larger datasets.
- **Low Confidence**: The evaluation methodology lacks detailed ablation studies and comparative analysis against established baselines, making it difficult to fully assess the adaptor module's contribution.

## Next Checks

1. **Extended Context Evaluation**: Test the model's performance on audio inputs exceeding 30 seconds to identify degradation patterns and determine if the context window limitation affects practical usability in real-world scenarios.

2. **Ablation Study on Adaptor Design**: Implement and evaluate alternative adaptor architectures (Qformer, ConvMLP) alongside the current MLP-100 to quantify the design choice's impact on task performance and validate the claimed superiority.

3. **Catastrophic Forgetting Analysis**: Create a comprehensive evaluation framework measuring instruction-following capabilities before and after multimodal fine-tuning, including replay-based mitigation strategies to quantify forgetting and potential recovery methods.