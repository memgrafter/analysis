---
ver: rpa2
title: 'TimeInf: Time Series Data Contribution via Influence Functions'
arxiv_id: '2407.15247'
source_url: https://arxiv.org/abs/2407.15247
tags:
- time
- data
- anomaly
- timeinf
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeInf, a model-agnostic method for estimating
  the contribution of individual time points in time series data to model predictions.
  TimeInf leverages influence functions to attribute predictions to individual time
  points while preserving temporal structures between time points.
---

# TimeInf: Time Series Data Contribution via Influence Functions

## Quick Facts
- arXiv ID: 2407.15247
- Source URL: https://arxiv.org/abs/2407.15247
- Authors: Yizi Zhang; Jingyan Shen; Xiaoxue Xiong; Yongchan Kwon
- Reference count: 40
- Primary result: TimeInf achieves higher AUC scores (e.g., 0.79 vs. 0.57 for Isolation Forest on UCR) in anomaly detection and steeper performance degradation in data pruning experiments compared to baseline methods.

## Executive Summary
TimeInf is a model-agnostic method that uses influence functions to estimate the contribution of individual time points in time series data to model predictions. The method constructs overlapping blocks of consecutive time series observations to capture temporal dependencies, then integrates data values across these blocks to assign contributions to specific time points. TimeInf demonstrates superior performance in anomaly detection and data pruning tasks across multiple real-world datasets while maintaining computational efficiency.

## Method Summary
TimeInf constructs overlapping blocks of consecutive time series observations to preserve temporal structures, then uses influence functions to assess how changing entire blocks impacts model predictions. The method integrates block-level influences to attribute contributions to individual time points, providing interpretable explanations for model decisions. TimeInf includes variants for different applications: self-influence for anomaly detection and test-influence for forecasting contribution estimation.

## Key Results
- Achieves higher AUC scores in anomaly detection (0.79 vs. 0.57 for Isolation Forest on UCR dataset)
- Shows steeper performance degradation when removing influential time blocks in data pruning experiments
- Maintains computational efficiency while providing interpretable attributions that distinguish diverse anomaly patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overlapping blocks capture temporal dependencies better than treating time points as i.i.d.
- Mechanism: By constructing overlapping blocks of consecutive time series observations, the method preserves local temporal structures and accounts for dependencies between adjacent time points.
- Core assumption: Temporal dependencies in time series can be approximated by examining consecutive observations within a sliding window.
- Evidence anchors:
  - [abstract]: "TimeInf leverages influence functions to attribute predictions to individual time points while preserving temporal structures between time points."
  - [section 2]: "To preserve local structure, we construct blocks of consecutive time points...from the original time series and assess how changing the entire block impacts model predictions."
  - [corpus]: Weak evidence - corpus focuses on influence functions broadly, not specifically on overlapping blocks for temporal structure.
- Break condition: If temporal dependencies extend beyond the block size, or if the time series has long-range dependencies that cannot be captured by local blocks.

### Mechanism 2
- Claim: Integration across blocks containing a specific time point provides more robust attribution than single-block analysis.
- Mechanism: By integrating data values across consecutive time series observations that encompass the time point of interest, the method assigns contributions that account for different temporal arrangements.
- Core assumption: A time point's influence on model predictions varies depending on its position within different temporal contexts, and averaging across these contexts provides a more accurate estimate.
- Evidence anchors:
  - [section 4]: "Each z is in multiple blocks with distinct time point configurations. Integrating across blocks containing z in Equation (4) computes the expected influence of z under different temporal arrangements."
  - [abstract]: "TimeInf attributes model predictions to individual time points while preserving temporal structures between time points."
  - [corpus]: Weak evidence - corpus mentions influence functions but doesn't discuss integration across multiple temporal arrangements.
- Break condition: If the time series contains abrupt changes or regime shifts that make temporal averaging inappropriate.

### Mechanism 3
- Claim: Influence functions can be adapted from i.i.d. settings to time series by modifying the contamination distribution.
- Mechanism: Instead of contaminating individual points (as in traditional influence functions), the method uses contaminated distributions based on overlapping blocks, which preserves the temporal structure.
- Core assumption: The mathematical framework of influence functions can be extended to time series data by redefining what constitutes "contamination" in a way that respects temporal dependencies.
- Evidence anchors:
  - [section 3]: "We draw inspiration from [25]'s influence functions for AR models. To preserve the dependency structure of the original time series, we utilize overlapping blocks of consecutive time series observations."
  - [abstract]: "TimeInf uses influence functions to attribute model predictions to individual time points while preserving temporal structures."
  - [corpus]: Moderate evidence - corpus shows research extending influence functions to time series, supporting the feasibility of this approach.
- Break condition: If the underlying time series model assumptions (stationarity, ergodicity) are violated.

## Foundational Learning

- Concept: Influence functions from robust statistics
  - Why needed here: The paper builds directly on influence function theory to create TimeInf, so understanding how influence functions work in traditional statistics is essential.
  - Quick check question: What does an influence function measure in the context of robust statistics?

- Concept: Autoregressive (AR) models and their temporal structure
  - Why needed here: TimeInf uses AR models as a primary example, and the temporal dependencies in AR models are central to understanding why traditional influence functions fail.
  - Quick check question: In an AR(p) model, how does the value at time t depend on previous time points?

- Concept: Overlapping blocks and time series resampling methods
  - Why needed here: The paper uses overlapping blocks to preserve temporal structure, which is a technique also used in bootstrap methods for time series.
  - Quick check question: Why are overlapping blocks preferred over disjoint blocks when analyzing time series data?

## Architecture Onboarding

- Component map: Data preprocessing -> Influence function computation -> Attribution integration -> Application-specific variants
- Critical path: Block construction → Influence function computation → Attribution integration → Downstream application
- Design tradeoffs: Block size vs. computational cost vs. ability to capture temporal dependencies; approximation methods for Hessian inversion vs. exact computation for small models
- Failure signatures: Poor performance on non-stationary time series; sensitivity to block size choice; breakdown when temporal dependencies extend beyond block length
- First 3 experiments:
  1. Apply TimeInf to a simple AR(1) model with synthetic data to verify the mechanism works as expected
  2. Compare TimeInf anomaly detection performance on UCR dataset against Isolation Forest with varying block sizes
  3. Test TimeInf's robustness to hyperparameter choices by varying time stride and observing performance stability

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several areas warrant further investigation:

### Open Question 1
- Question: How does the choice of contaminating distribution Gm affect the performance of TimeInf across different types of time series data?
- Basis in paper: [explicit] The paper discusses the contaminated distribution function F^ϵ(z[m]) = (1 - ϵ)F^m + ϵGm and mentions that Gm is a point mass distribution δz[m] in the main TimeInf formulation, but also notes that the original influence function [23] considers only this specific contamination pattern while overlooking potential influences from different patterns.
- Why unresolved: The paper does not empirically compare different choices of contaminating distributions Gm or analyze how different contamination patterns affect TimeInf's performance across various time series types (e.g., periodic vs. non-periodic, univariate vs. multivariate).
- What evidence would resolve it: Experimental results comparing TimeInf performance using different contaminating distributions (e.g., Gaussian noise, block bootstrap samples) across multiple time series datasets with varying characteristics.

### Open Question 2
- Question: What is the optimal block length for TimeInf in different anomaly detection scenarios, and how does this choice interact with the temporal characteristics of the data?
- Basis in paper: [explicit] The paper mentions that block length affects TimeInf's ability to capture contextual anomalies, noting that "larger block length enables TimeInf to better capture contextual anomalies that persist for an extended duration" but also increases computation time. The ablation study shows performance varies with block length, but optimal choices are not determined.
- Why unresolved: The paper uses fixed block lengths across datasets for fair comparison but acknowledges that "using a block length optimized for each dataset could potentially yield even better results." The relationship between block length, anomaly duration, and detection performance remains unclear.
- What evidence would resolve it: A systematic study analyzing the relationship between anomaly duration patterns in different datasets and optimal block lengths, including recommendations for choosing block length based on data characteristics.

### Open Question 3
- Question: How well does TimeInf scale to extremely large time series foundation models, and what are the computational bottlenecks when applying it to models with billions of parameters?
- Basis in paper: [inferred] The paper demonstrates TimeInf's scalability to moderately-sized models (300K parameters for PatchTST) and mentions it "can potentially scale to large over-parameterized models, enabling interpretability of time series foundation models." However, the computational efficiency and accuracy trade-offs for much larger models are not explored.
- Why unresolved: The paper only tests TimeInf on models up to 300K parameters, leaving questions about its performance and computational requirements for truly large foundation models. The approximation methods (CG, TracIn) may face challenges with extremely high-dimensional Hessians.
- What evidence would resolve it: Experiments applying TimeInf to time series models with millions or billions of parameters, measuring both accuracy and computation time, along with analysis of which approximation methods remain effective at different model scales.

## Limitations
- TimeInf's effectiveness may break down for time series with long-range dependencies that cannot be captured by local blocks
- The method requires careful selection of block size, which may need to be optimized for different datasets
- Computational approximations for large models (conjugate gradient, TracIn) may introduce accuracy trade-offs

## Confidence
- Influence function mechanism for time series: Medium
- Overlapping blocks preserving temporal structure: High
- Integration across multiple temporal arrangements: Medium
- Robustness to hyperparameter choices: Moderate
- Scalability to large models: Low (based on limited testing to 300K parameters)

## Next Checks
1. Test TimeInf on synthetic non-stationary time series data to evaluate breakdown conditions when temporal stationarity assumptions are violated
2. Compare computational runtime and memory usage of exact vs. approximate Hessian inversion methods across different model complexities
3. Validate TimeInf's attributions using a ground truth dataset where individual time point contributions are known through controlled experiments