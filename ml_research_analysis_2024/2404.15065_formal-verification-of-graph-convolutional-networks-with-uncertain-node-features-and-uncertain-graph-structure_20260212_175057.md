---
ver: rpa2
title: Formal Verification of Graph Convolutional Networks with Uncertain Node Features
  and Uncertain Graph Structure
arxiv_id: '2404.15065'
source_url: https://arxiv.org/abs/2404.15065
tags:
- graph
- neural
- polynomial
- uncertain
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first approach to formally verify graph
  convolutional networks with uncertain node features and uncertain graph structure
  over multiple message-passing steps. The key idea is to explicitly preserve non-convex
  dependencies using (matrix) polynomial zonotopes, enabling polynomial-time complexity
  in the number of uncertain edges and input features.
---

# Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure

## Quick Facts
- arXiv ID: 2404.15065
- Source URL: https://arxiv.org/abs/2404.15065
- Authors: Tobias Ladner; Michael Eichelbeck; Matthias Althoff
- Reference count: 40
- Primary result: First approach to verify GCNs with uncertain node features and graph structure using matrix polynomial zonotopes, achieving up to 94% verification rates on benchmark datasets

## Executive Summary
This paper presents the first method for formally verifying graph convolutional networks (GCNs) with both uncertain node features and uncertain graph structure over multiple message-passing steps. The key innovation is using matrix polynomial zonotopes to explicitly preserve non-convex dependencies throughout the computation, enabling tighter output set enclosures than traditional interval arithmetic. The approach demonstrates polynomial-time complexity in the number of uncertain edges and features while achieving high verification rates on three benchmark datasets.

The method is significant because it bridges a critical gap in neural network verification by handling the unique challenges of graph-structured data with uncertainty. By maintaining dependency information across operations like matrix multiplication and nonlinear activation functions, the approach prevents the outer approximation explosion that occurs with interval arithmetic, where dependencies are lost. Dynamic subgraph extraction further accelerates computation by up to 7x.

## Method Summary
The approach uses reachability analysis with matrix polynomial zonotopes to propagate uncertain node features and graph structure through GCN layers. Input uncertainties are converted to matrix polynomial zonotopes, then each layer (message passing, graph convolution, activation) is computed while preserving dependencies. Dynamic subgraph extraction removes nodes that don't influence the output after each message-passing step, reducing computational complexity. The output set is then checked against unsafe specifications to verify properties.

## Key Results
- Achieves up to 94% verification rates on Enzymes and Proteins datasets
- Maintains reasonable verification times even for large graphs with up to 5% edge uncertainty
- Dynamic subgraph extraction accelerates computation by up to 7x
- Demonstrates that preserving dependencies throughout set propagation is crucial for tight enclosures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly preserving non-convex dependencies using matrix polynomial zonotopes enables tighter output set enclosures than interval arithmetic.
- Mechanism: Matrix polynomial zonotopes maintain dependency information across operations like matrix multiplication and nonlinear activation functions, preventing the outer approximation explosion seen with interval arithmetic where dependencies are lost.
- Core assumption: The set-based computation operations (affine map, Minkowski sum, matrix multiplication) can be performed exactly or with bounded error on matrix polynomial zonotopes.
- Evidence anchors:
  - [abstract]: "explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes"
  - [section]: "we explicitly preserve the non-convex dependencies of all involved variables through all layers of the graph neural network using (matrix) polynomial zonotopes"
  - [corpus]: Weak evidence - no directly comparable work on graph neural network verification using matrix polynomial zonotopes found in corpus.

### Mechanism 2
- Claim: The use of matrix polynomial zonotopes provides polynomial time complexity in the number of uncertain edges and input features.
- Mechanism: Each operation on polynomial zonotopes (affine maps, Minkowski sums, and matrix multiplications) has polynomial complexity in the number of generators, and the number of generators grows polynomially with uncertain edges and features.
- Core assumption: The number of message-passing steps remains small to avoid over-smoothing, keeping the overall complexity manageable.
- Evidence anchors:
  - [abstract]: "enabling polynomial-time complexity in the number of uncertain edges and input features"
  - [section]: "the overall polynomial time complexity in the number of uncertain edges and uncertain input features"
  - [corpus]: No direct comparison in corpus, but related work on standard neural network verification shows polynomial complexity using zonotopes.

### Mechanism 3
- Claim: Dynamic subgraph extraction accelerates verification by up to 7x by focusing computation only on nodes that influence the output.
- Mechanism: After each message-passing step, nodes outside the (k'+1)-hop neighborhood of nodes of interest can be removed, reducing the graph size for subsequent computations.
- Core assumption: The network has node-level output or the subgraph for graph-level output can be determined before verification.
- Evidence anchors:
  - [abstract]: "Dynamic subgraph extraction further accelerates computation by up to 7x"
  - [section]: "we can dynamically remove nodes that do not influence a considered node throughout the verification process"
  - [corpus]: No comparable subgraph optimization found in corpus.

## Foundational Learning

- Concept: Polynomial zonotopes and their operations (affine maps, Minkowski sums, quadratic maps)
  - Why needed here: These operations form the basis for propagating uncertainty through the graph neural network layers while maintaining dependency information.
  - Quick check question: How does a quadratic map operation on polynomial zonotopes introduce dependencies between generators?

- Concept: Matrix polynomial zonotopes as extension of standard polynomial zonotopes
  - Why needed here: Standard polynomial zonotopes cannot represent sets of matrices, which is required for graph neural networks where each node has a feature matrix.
  - Quick check question: How does reshaping a matrix polynomial zonotope into a vector convert it to a standard polynomial zonotope?

- Concept: Reachability analysis for formal verification
  - Why needed here: This is the fundamental approach used to compute output sets from input sets through the network layers, enabling specification checking.
  - Quick check question: What is the difference between complete and incomplete verification algorithms in this context?

## Architecture Onboarding

- Component map: Input preprocessing -> Message passing enclosure -> Layer propagation -> Subgraph extraction -> Output verification
- Critical path:
  1. Initialize with uncertain input features as matrix polynomial zonotopes
  2. For each message-passing step: compute uncertain message passing → apply graph convolutional layer → apply activation layer
  3. If graph-level output: apply global pooling layer
  4. If additional layers: apply linear and activation layers
  5. Check output set against unsafe specification

- Design tradeoffs:
  - Tighter enclosures vs computational cost: Higher-order polynomials for activation function approximation provide tighter bounds but increase generators
  - Completeness vs tractability: Order reduction techniques can limit generator explosion but introduce additional approximation errors
  - Generality vs efficiency: Matrix polynomial zonotopes work for any activation function but may be overkill for simple cases

- Failure signatures:
  - Verification timeout: Likely due to too many uncertain edges or features overwhelming polynomial complexity
  - Large output enclosures: May indicate insufficient order reduction or poor polynomial approximation of activation functions
  - False positives: Could result from overly conservative approximations in message passing computation

- First 3 experiments:
  1. Verify a simple graph neural network on Cora dataset with no edge uncertainty to establish baseline performance
  2. Add edge uncertainty to a small graph (Enzymes/Proteins) and compare verification times with enumeration approach
  3. Test subgraph extraction on Cora with node-level output to measure acceleration benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several unresolved issues emerge regarding approximation errors, optimal message-passing steps, and order reduction techniques.

## Limitations
- The polynomial-time complexity claim depends on keeping message-passing steps small, which limits applicability to deeper networks
- The method assumes uncertainties can be modeled as polynomial zonotopes, which may not capture all types of uncertainty distributions
- While verification rates are high (up to 94% on Enzymes/Proteins), the 6% failure rate on Cora with edge uncertainty suggests limitations in scaling to larger, more complex graphs

## Confidence

- **High confidence** in the core claim that matrix polynomial zonotopes enable tighter enclosures than interval arithmetic by preserving dependencies
- **Medium confidence** in the polynomial-time complexity claim, as the analysis appears sound but hasn't been compared against worst-case scenarios with large numbers of uncertain edges
- **Medium confidence** in the practical utility, given strong results on three benchmarks but limited testing on graphs with higher edge uncertainty percentages

## Next Checks

1. Test verification performance on graphs with edge uncertainty exceeding 5% to determine the scalability limits
2. Compare output enclosure tightness against interval arithmetic baselines on the same benchmark problems
3. Implement and evaluate order reduction techniques to see if they maintain verification rates while reducing computation time