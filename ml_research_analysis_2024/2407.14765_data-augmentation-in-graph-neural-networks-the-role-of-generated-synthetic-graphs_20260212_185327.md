---
ver: rpa2
title: 'Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic
  Graphs'
arxiv_id: '2407.14765'
source_url: https://arxiv.org/abs/2407.14765
tags:
- graph
- data
- graphs
- augmentation
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of synthetic graph data to improve
  graph classification performance in Graph Neural Networks (GNNs). A novel framework
  is proposed that uses size-aware data augmentation, applying different generative
  models (GRAN for large graphs, GraphRNN for small graphs) based on graph size to
  generate synthetic data.
---

# Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic Graphs

## Quick Facts
- arXiv ID: 2407.14765
- Source URL: https://arxiv.org/abs/2407.14765
- Reference count: 26
- Novel framework uses size-aware data augmentation with different generative models (GRAN for large graphs, GraphRNN for small graphs) based on graph size

## Executive Summary
This study addresses data scarcity and imbalance issues in graph classification by introducing a novel framework for synthetic graph generation. The approach leverages two distinct graph generative models - GRAN for large graphs and GraphRNN for small graphs - selected based on graph size thresholds. Experimental results demonstrate that incorporating synthetic graphs into training data significantly enhances classification accuracy, with optimal performance achieved using twice the amount of generated data compared to real data.

## Method Summary
The framework implements size-aware data augmentation by splitting datasets into raw (80%), sub-real (10%), and test (10%) sets while maintaining class distributions. Synthetic graphs are generated using GRAN for graphs exceeding 50 nodes or 1225 edges, and GraphRNN for smaller graphs. The augmented dataset is created by combining the raw set with generated graphs at 1x, 2x, and 3x the sub-real size ratios. GNN classifiers including GraphSAGE, GIN variants, GCN variants, and EdgePool are trained on these augmented datasets and evaluated for classification accuracy.

## Key Results
- Size-aware augmentation with GRAN for large graphs and GraphRNN for small graphs improves classification accuracy
- Best results achieved using twice the amount of generated data compared to real data
- Framework effectively addresses data scarcity and imbalance, particularly benefiting smaller datasets
- Consistent class labels across real and synthetic data preserve label-invariant augmentation benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph size-aware augmentation improves classification accuracy by matching generator strengths to graph size regimes
- Mechanism: Large graphs (â‰¥50 nodes) are generated with GRAN due to its scalability and attention-based node ordering; small graphs use GraphRNN for better generation quality
- Core assumption: Different generators have domain-specific strengths that degrade outside their optimal size range
- Evidence anchors:
  - [abstract] "balancing scalability and quality requires different generators based on graph size"
  - [section] "GRAN's methodology is optimized for handling larger graphs, whereas GraphRNN's architecture and training process are better suited for generating smaller-scale graphs"
- Break condition: When dataset contains mixed-size graphs within the same class, size-based separation may lose intra-class size variance

### Mechanism 2
- Claim: Doubling synthetic data relative to real data yields peak performance by balancing diversity and overfitting risk
- Mechanism: Increasing generated samples beyond the real-data ratio enriches the training set without overwhelming the signal from original examples
- Core assumption: Model generalization benefits from a sweet spot in the real:synth ratio, not pure synthetic dominance
- Evidence anchors:
  - [abstract] "best results achieved using twice the amount of generated data compared to real data"
  - [section] "we further extended these experiments by generating data sets twice and three times the size of the R"
- Break condition: Too much synthetic data may introduce distribution drift and hurt generalization if generators overfit the training set

### Mechanism 3
- Claim: Maintaining consistent class labels across real and synthetic data preserves label-invariant augmentation benefits
- Mechanism: Generated graphs are assigned the same class label as the source class before mixing into the training set
- Core assumption: The generator preserves class semantics in the synthetic distribution
- Evidence anchors:
  - [abstract] "ensuring consistent labels and enhancing classification performance"
  - [section] "for each graph class Ci, the desired number of synthetic graphs for that class CCi is generated for data augmentation"
- Break condition: If the generator inadvertently alters structural features that correlate with class labels, augmentation may become label-variant

## Foundational Learning

- Concept: Graph representation learning via node embeddings
  - Why needed here: GNNs transform graph topology into feature vectors for classification; understanding this is essential to see how augmentation changes input distribution
  - Quick check question: What is the role of neighborhood aggregation in GraphSAGE or GIN models?

- Concept: Sequential graph generation (node/edge sequences)
  - Why needed here: Both GRAN and GraphRNN learn distributions over node and edge sequences; knowing this helps explain how synthetic graphs are produced
  - Quick check question: How does BFS node ordering in GraphRNN affect the learned distribution compared to random ordering?

- Concept: Data augmentation principles in non-Euclidean domains
  - Why needed here: Unlike image/text augmentation, graph augmentation must preserve relational structure; recognizing this difference is key to evaluating the proposed approach
  - Quick check question: Why is random edge removal more risky in graph augmentation than in image augmentation?

## Architecture Onboarding

- Component map: Dataset loader -> size-aware splitter -> GRAN/GraphRNN generator per class -> mixed dataset builder -> GNN trainer -> evaluator
- Critical path: Size detection -> generator selection -> graph generation -> label consistency check -> augmentation mixing -> training
- Design tradeoffs:
  - Using two generators increases complexity but leverages domain strengths; single-generator simplification may reduce performance on edge sizes
  - Fixed size thresholds (50 nodes, 1225 edges) may not generalize across all domains
- Failure signatures:
  - Out-of-memory errors on large graphs -> GraphRNN chosen incorrectly
  - Sharp drop in accuracy after augmentation -> label drift or distribution mismatch
  - Overfitting with high synthetic ratio -> insufficient regularization or generator overfitting
- First 3 experiments:
  1. Verify size detection logic on a toy dataset with known graph sizes
  2. Generate synthetic graphs with each generator separately and inspect distribution similarity to real graphs
  3. Train a baseline GNN on raw data, then incrementally add synthetic data in 1:1, 2:1, 3:1 ratios and monitor validation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of node ordering strategy (e.g., BFS vs. other orderings) impact the quality and diversity of generated synthetic graphs in GraphRNN?
- Basis in paper: [explicit] The paper mentions that GraphRNN uses BFS node ordering, which provides a unique representation of graphs and allows the model to learn to produce graphs using BFS node orderings instead of learning to generate graphs under any conceivable node permutation
- Why unresolved: While the paper notes the use of BFS, it doesn't compare the impact of different node ordering strategies on graph generation quality
- What evidence would resolve it: Comparative experiments testing GraphRNN with different node ordering strategies (e.g., BFS, DFS, random) and evaluating the quality and diversity of generated graphs using metrics like graph edit distance or distribution similarity to real graphs

### Open Question 2
- Question: What is the optimal ratio of generated synthetic data to real data for maximizing graph classification performance across different datasets and graph sizes?
- Basis in paper: [explicit] The paper explores different ratios of generated data (1x, 2x, 3x the size of real data) and finds that incorporating generated graphs enhances classification performance, with the best results achieved using twice the amount of generated data compared to real data. However, this may vary depending on the dataset and graph size
- Why unresolved: The optimal ratio may depend on the specific dataset characteristics, such as the number of classes, graph size distribution, and inherent complexity
- What evidence would resolve it: A systematic study varying the ratio of generated to real data across multiple datasets with different characteristics and graph sizes, measuring classification performance to identify optimal ratios for each scenario

### Open Question 3
- Question: How do different graph generative models (e.g., GRAN, GraphRNN) perform when generating synthetic data for dynamic graphs, and what modifications are needed to adapt them to this task?
- Basis in paper: [inferred] The paper focuses on static graph generation and classification. It mentions that future work includes investigating generating synthetic graphs for dynamic graphs, implying that current models may not be directly applicable
- Why unresolved: Dynamic graphs have temporal dependencies and evolving structures that static graph generative models are not designed to capture
- What evidence would resolve it: Developing and evaluating modified versions of GRAN and GraphRNN (or new models) for dynamic graph generation, testing their performance on dynamic graph classification tasks, and comparing them to static graph generation approaches applied to time-stamped snapshots

## Limitations
- Unknown generator hyperparameters (stride size, learning rate, number of blocks) could significantly affect reproducibility
- Dataset splitting methodology beyond the 80/10/10 distribution is unclear
- Class distribution maintenance during augmentation is not explicitly detailed

## Confidence

- **High confidence**: The size-aware augmentation mechanism (GRAN for large graphs, GraphRNN for small graphs) is well-supported by theoretical arguments about each generator's strengths
- **Medium confidence**: The 2:1 synthetic-to-real ratio achieving peak performance is based on empirical observation but may be dataset-dependent
- **Low confidence**: Claims about addressing data scarcity and imbalance lack quantitative evidence showing performance gains specifically for minority classes

## Next Checks

1. **Distribution analysis**: Compare degree distributions, clustering coefficients, and orbit counts between real and synthetic graphs to verify the augmentation preserves meaningful structural properties
2. **Cross-dataset validation**: Test the framework on datasets outside the TU repository to evaluate generalization beyond the six studied benchmarks
3. **Label preservation test**: Generate synthetic graphs and verify that structural features predictive of class labels are maintained, preventing label drift during augmentation