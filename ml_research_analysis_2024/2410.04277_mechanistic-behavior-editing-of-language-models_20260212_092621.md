---
ver: rpa2
title: Mechanistic Behavior Editing of Language Models
arxiv_id: '2410.04277'
source_url: https://arxiv.org/abs/2410.04277
tags:
- tarot
- language
- tasks
- attention
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel task adaptation method called TaRot
  for large language models. TaRot addresses the issue of spurious features learned
  from noisy web-scale text data by applying learnable rotation matrices to attention
  head outputs, optimizing these rotations using Bayesian Optimization on small labeled
  datasets.
---

# Mechanistic Behavior Editing of Language Models

## Quick Facts
- arXiv ID: 2410.04277
- Source URL: https://arxiv.org/abs/2410.04277
- Authors: Joykirat Singh; Subhabrata Dutta; Tanmoy Chakraborty
- Reference count: 21
- Primary result: TaRot improves zero-shot and few-shot task performance by 23.81% and 11.15% respectively using rotation matrices on attention outputs

## Executive Summary
This paper introduces TaRot, a novel task adaptation method that addresses spurious features learned from noisy web-scale text data by applying learnable rotation matrices to attention head outputs in large language models. The method optimizes these rotations using Bayesian Optimization on small labeled datasets (6-20 examples), making it data- and compute-efficient. Experiments across multiple classification and generation tasks show significant improvements in both zero-shot and few-shot settings across different model sizes, demonstrating that TaRot can effectively edit neural circuitries while preserving pretrained knowledge.

## Method Summary
TaRot applies parametrized rotation matrices to the concatenated outputs of attention heads at specific layers, transforming the token associations stored in the OV-circuits. The rotation parameters are optimized using Bayesian Optimization with Infinite-width Bayesian Neural Networks (I-BBN) and LogExpectedImprovement acquisition function. The method intervenes in the initial half of model layers and requires only 6-20 supervised examples per task. For classification tasks, TaRot directly optimizes rotation parameters, while for generation tasks, it uses reinforcement learning with rewards based on task-specific metrics.

## Key Results
- Average improvement of 23.81% in zero-shot settings across tested models and tasks
- Average improvement of 11.15% in few-shot settings compared to rescaling baseline
- Consistent performance gains across multiple model sizes (1.5B to 8B parameters)
- Effective on both classification (AG News, Entailed Polarity, Navigate, Color, Winowhy) and generation tasks (IMDB, Detoxify)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads memorize token associations in superposition and act as mini-language models
- Mechanism: Each attention head learns to map context tokens to next tokens via its OV-circuit, with multiple associations stored in superposition since the number of heads is much smaller than the number of possible token associations in pretraining data
- Core assumption: The OV-circuit of each attention head forms a token-to-token mapping that memorizes associations from pretraining data
- Evidence anchors:
  - [abstract] "spurious features learned from noisy data hinder their generalizability"
  - [section 3.1] "each attention head acts as a mini-language model" and "these attention heads can memorize undesired token-token associations"
  - [corpus] No direct evidence found in related papers

### Mechanism 2
- Claim: Rotation matrices can align head outputs to desired tokens while maintaining other associations
- Mechanism: Parametrized rotation matrices (Rd_Θ) rotate the concatenated head outputs to align with desired token directions in the residual stream while near-orthogonalizing with undesired tokens
- Core assumption: Rotating attention head outputs can selectively enhance desired token associations without destroying the superposed state of other associations
- Evidence anchors:
  - [abstract] "TaRot intervenes in the neural circuitries using learnable rotation matrices"
  - [section 3.2] "we construct parametrized rotations to align head outputs for task-adaptation"

### Mechanism 3
- Claim: Bayesian optimization can effectively find optimal rotation parameters using few examples
- Mechanism: Infinite-width Bayesian Neural Networks (I-BBN) with LogExpectedImprovement acquisition function optimize the rotation parameters using 6-20 supervised examples per task
- Core assumption: The high-dimensional parameter space of rotation configurations can be effectively searched using Bayesian optimization without requiring extensive labeled data
- Evidence anchors:
  - [abstract] "The rotation parameters are then optimized using Bayesian optimization"
  - [section 3.3] "We implement Bayesian optimization (Snoek et al., 2012) to solve the optimization problems"

## Foundational Learning

- Concept: Transformer attention mechanism and OV-circuits
  - Why needed here: Understanding how attention heads memorize token associations is fundamental to the TaRot approach
  - Quick check question: How does the OV-circuit of an attention head transform the attention-weighted sum of value vectors into a prediction for the next token?

- Concept: Linear algebra - rotation matrices and singular value decomposition
  - Why needed here: The method relies on applying rotation matrices to attention outputs and analyzing the impact on the unembedding subspace
  - Quick check question: What property of rotation matrices ensures they preserve the norm of vectors while changing their direction?

- Concept: Bayesian optimization and Gaussian processes
  - Why needed here: The method uses Bayesian optimization with I-BBN to find optimal rotation parameters
  - Quick check question: How does the LogExpectedImprovement acquisition function guide the search for optimal parameters in Bayesian optimization?

## Architecture Onboarding

- Component map: Input sequence → language model forward pass with rotation interventions → output generation → reward calculation (for NLG tasks) or accuracy measurement (for classification) → Bayesian optimization updates rotation parameters
- Critical path: The optimization loop where rotation parameters are updated based on task performance feedback
- Design tradeoffs: Using rotation matrices instead of direct parameter modification preserves pretrained knowledge while allowing task-specific adaptation, but requires careful optimization to avoid disrupting other associations
- Failure signatures: Performance degradation on tasks where the original model was strong, inability to improve on tasks where the original model was weak, or optimization failure to converge within reasonable iterations
- First 3 experiments:
  1. Apply TaRot to a simple classification task (like AG News) with one model (Qwen2-1.5B) in zero-shot setting to verify basic functionality
  2. Compare TaRot against the rescaling baseline on the same task to validate the advantage of rotation over simple scaling
  3. Test TaRot on an NLG task (like IMDB) to verify it can improve generation quality while maintaining fluency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TaRot's performance scale with larger models beyond 8B parameters, and are there diminishing returns in performance improvements?
- Basis in paper: [inferred] The paper experiments with models up to 8B parameters (Llama-3-8B-Instruct) but does not explore larger models or discuss potential scaling limitations.
- Why unresolved: The paper focuses on demonstrating effectiveness across a range of moderate-sized models but does not investigate whether TaRot maintains its performance advantages with frontier-scale models (e.g., 70B+ parameters) or if the rotational approach becomes less effective at scale.
- What evidence would resolve it: Experiments applying TaRot to models like Llama-3-70B, GPT-4, or Claude-3 showing performance improvements (or lack thereof) across the same task suite, along with analysis of how the number of rotation parameters scales with model size and whether optimization becomes more challenging.

### Open Question 2
- Question: What is the precise relationship between the rotation angles learned by TaRot and the semantic content of the tasks being adapted to?
- Basis in paper: [explicit] The paper notes that "we can not determine the exact token-token map for the OV-circuits of attention head" and mentions that "multiple token associations are expected to be memorized in each attention head in superposition."
- Why unresolved: While the paper demonstrates that rotations improve task performance, it does not provide a detailed analysis of what specific semantic transformations the learned rotations represent or how they relate to the underlying knowledge structures being modified.
- What evidence would resolve it: A systematic analysis correlating rotation parameters with specific linguistic or semantic transformations, potentially through visualization of rotated attention patterns or correlation with known task-relevant features in the model's internal representations.

### Open Question 3
- Question: Can TaRot's rotational parameters be transferred across similar tasks to enable rapid adaptation without per-task optimization?
- Basis in paper: [inferred] The paper treats each task as requiring independent optimization of rotation parameters but does not explore whether parameters learned for related tasks could be shared or transferred.
- Why unresolved: The paper demonstrates task-specific optimization but does not investigate the potential for parameter sharing or transfer learning between related tasks, which could significantly reduce computational overhead for practical applications.
- What evidence would resolve it: Experiments showing performance when using rotation parameters from one task (e.g., AG News) on related tasks (e.g., other text classification tasks), including analysis of how task similarity affects transfer effectiveness and whether a hierarchy of rotation parameters could be established.

## Limitations

- Core assumption about attention heads storing token associations in superposition lacks direct empirical validation
- Effectiveness of Bayesian optimization with only 6-20 examples on high-dimensional parameter spaces remains questionable
- Reported improvements may not generalize to more complex, real-world tasks beyond the tested set
- Comparison against rescaling baseline may not be the most challenging or relevant baseline for evaluation

## Confidence

**High Confidence**: The method is implementable and can be applied to various models and tasks. The technical approach using rotation matrices and Bayesian optimization is clearly described and reproducible.

**Medium Confidence**: The method shows improvements over the rescaling baseline on the tested tasks. The improvements are statistically significant within the tested conditions.

**Low Confidence**: The claimed mechanism of how attention heads store token associations in superposition and how rotation matrices selectively edit these associations. The generalization of results to broader task sets and the long-term stability of the edited models.

## Next Checks

1. **Ablation Study on Attention Head Selection**: Systematically test which layers and heads benefit most from rotation intervention. Currently the paper restricts to "initial half of model layers" but doesn't explore the impact of selecting different subsets of heads or varying the number of intervened layers.

2. **Cross-task Interference Analysis**: After applying TaRot to one task, test performance on unrelated tasks to quantify whether the rotation intervention creates task-specific degradation or maintains general capabilities. This would validate the claim that the method "minimizes disruption to other learned patterns."

3. **Comparison with Direct Fine-tuning Baseline**: Implement direct fine-tuning on the same 6-20 examples used for TaRot optimization to establish whether the improvements come from the rotation mechanism itself or simply from any form of parameter adjustment on task-specific data.