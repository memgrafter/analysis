---
ver: rpa2
title: 'Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence
  Model'
arxiv_id: '2403.17445'
source_url: https://arxiv.org/abs/2403.17445
tags:
- arxiv
- sequence
- transformer
- attention
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet effective sequence model, ETSMLP,
  which incorporates exponential smoothing into a multi-layer perceptron (MLP) to
  capture long-range dependencies in sequential data. The key idea is to enhance the
  exponential smoothing (ETS) module with additional parameters and a complex field,
  and then directly integrate it into an element-wise MLP.
---

# Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model

## Quick Facts
- **arXiv ID**: 2403.17445
- **Source URL**: https://arxiv.org/abs/2403.17445
- **Reference count**: 30
- **Key outcome**: ETSMLP achieves comparable results to S4 on LRA benchmark with <1% parameter increase, outperforming transformer variants by ~20 points

## Executive Summary
This paper proposes ETSMLP, a simple yet effective sequence model that incorporates exponential smoothing into a multi-layer perceptron (MLP) to capture long-range dependencies. The key innovation is enhancing the exponential smoothing (ETS) module with additional parameters and a complex field, then integrating it directly into an element-wise MLP. Despite adding less than 1% of parameters to the original MLP, ETSMLP achieves competitive results on the Long Range Arena (LRA) benchmark, outperforming transformer variants by about 20 points and slightly surpassing S4 on average. The model also demonstrates strong performance on various Natural Language Understanding (NLU) datasets, achieving comparable results to transformer encoders.

## Method Summary
ETSMLP integrates a Complex Exponential Smoothing (CES) module into a standard MLP architecture. The CES module introduces learnable damped factors (α, β), complex parameterization, and a bidirectional smoothing mechanism with a shortcut parameter (ω). The model uses bidirectional kernel computation and a gating unit to effectively aggregate long-range context while maintaining local information. Training employs the Adam optimizer with specific hyperparameters including learning rate warmup and layer normalization for regularization.

## Key Results
- ETSMLP achieves comparable results to S4 on the LRA benchmark with <1% parameter increase
- Outperforms transformer variants by about 20 points on average across LRA tasks
- Demonstrates strong performance on NLU datasets with accuracy comparable to transformer encoders

## Why This Works (Mechanism)

### Mechanism 1
The exponential smoothing with learnable parameters captures long-range dependencies efficiently. By augmenting ETS with learnable damped factors (α, β) and complex field, the model can dynamically adjust smoothing behavior. The exponential parameterization of λ (λα = log log λ) ensures stable gradient propagation, avoiding gradient explosion issues.

### Mechanism 2
The complex exponential smoothing module introduces sufficient parameterization to learn sequence information. By treating damping factors as complex numbers, the model expands its parameter space to represent both magnitude and phase information. This complex parameterization enables the model to capture richer temporal patterns than simple real-valued ETS.

### Mechanism 3
The bidirectional kernel computation and shortcut mechanism enable effective long-range context aggregation. Using both forward and backward recursions, each output token is influenced by both preceding and succeeding tokens. The shortcut mechanism (yt = σ(ω)xt + y) acts as a gating unit that regulates incoming input, effectively aggregating long-range context while maintaining local information.

## Foundational Learning

- **Concept**: Exponential smoothing (ETS) as a simple state space model (SSM)
  - **Why needed here**: Understanding ETS is crucial because ETSMLP builds upon this simple SSM by augmenting it with additional parameters and a complex field. The core idea of ETS gives more weight to recent observations, essential for capturing long-range dependencies.
  - **Quick check question**: How does the exponential smoothing formula yt = λxt + (1 − λ)yt−1 differ from a simple moving average, and why is this difference important for sequence modeling?

- **Concept**: State space models (SSMs) and their relationship to recurrent neural networks (RNNs)
  - **Why needed here**: ETS is a special case of SSMs, and understanding the broader context helps grasp the motivation behind using ETS as a building block. SSMs relate inputs to outputs through state matrices and have been shown effective for sequence modeling tasks.
  - **Quick check question**: What are the key differences between continuous-time SSMs (like S4) and discrete-time SSMs (like ETS), and how do these differences impact computational efficiency and representational power?

- **Concept**: Complex number representations in neural networks
  - **Why needed here**: The ETSMLP uses complex parameters to expand its representational capacity. Understanding how complex numbers can be used in neural networks, and the potential benefits and challenges, is essential for grasping the design choices.
  - **Quick check question**: What are the potential advantages of using complex parameters in neural networks, and what are some challenges that need to be addressed when implementing such models?

## Architecture Onboarding

- **Component map**: Input -> LayerNorm -> MLP -> CES -> Output
- **Critical path**: 1) Input normalization (LayerNorm) 2) Token-level processing (MLP) 3) Sequence-level mixing (CES) 4) Output transformation (Linear layer)
- **Design tradeoffs**:
  - Pros: Simple architecture, low parameter overhead (< 3% increase), efficient computation (linear complexity), strong performance on long-range sequence modeling tasks
  - Cons: Potential instability due to complex parameterization, limited by expressiveness of exponential smoothing mechanism, may not capture all types of long-range dependencies as effectively as attention-based models
- **Failure signatures**:
  - Gradient explosion or vanishing: If exponential parameterization is not properly constrained, gradients may explode or vanish during training
  - Over-smoothing: If damping factors are not properly tuned, the model may over-smooth sequence information, leading to loss of local context
  - Numerical instability: If complex parameters are not properly initialized or constrained, they may lead to numerical instability during training
- **First 3 experiments**:
  1. Ablation study on damping factors: Remove α, β, or ω individually and evaluate impact on performance
  2. Initialization sensitivity analysis: Experiment with different initialization strategies for complex parameters to assess impact on model performance and stability
  3. Efficiency comparison: Compare computational efficiency and memory usage of ETSMLP with other sequence models on long sequences

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and discussion, several areas remain unexplored. The authors acknowledge that the current approach focuses on evaluating datasets with fewer than 100,000 samples and that the influence of prior knowledge on performance is substantial. They mention the need for meticulous adjustment of exponential smoothing and ingenious design of the architecture for pretraining tasks with large-scale datasets.

## Limitations

- Performance evaluation primarily based on average accuracy scores, which may not fully capture effectiveness across diverse sequence lengths and data types
- Reliance on complex parameterization introduces potential numerical instability risks that are not thoroughly explored
- Efficiency claims lack validation on real-world, large-scale datasets beyond synthetic experiments

## Confidence

- **High Confidence**: ETSMLP achieves comparable results to S4 on LRA benchmark (supported by experimental evidence and reported accuracy scores)
- **Medium Confidence**: Complex parameterization and bidirectional smoothing are critical for performance (supported by ablation studies but mechanisms not fully elucidated)
- **Low Confidence**: Efficiency and memory advantages over transformers (based on synthetic experiments, generalizability to real-world scenarios uncertain)

## Next Checks

1. **Numerical Stability Analysis**: Conduct thorough investigation of numerical stability of complex parameterization under various initialization strategies and damping factor configurations. Monitor gradients and activations during training to identify potential instability issues.

2. **Real-World Efficiency Benchmarking**: Evaluate ETSMLP's efficiency and memory usage on large-scale, real-world datasets (e.g., web-scale text corpora or high-resolution image sequences) to validate claimed advantages over transformers and other sequence models.

3. **Component Isolation Ablation**: Design more granular ablation study to isolate contributions of bidirectional smoothing, shortcut mechanism, and complex parameterization. Test model with only forward smoothing, no shortcut, or real-valued parameters to better understand individual impacts on performance.