---
ver: rpa2
title: The Mystery of Compositional Generalization in Graph-based Generative Commonsense
  Reasoning
arxiv_id: '2410.06272'
source_url: https://arxiv.org/abs/2410.06272
tags:
- reasoning
- linguistics
- generalization
- compositional
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Compositional Generalization Challenge
  for Graph-based Commonsense Reasoning (CGGC) that evaluates language models' ability
  to generate natural language sentences from reasoning graphs containing unseen combinations
  of relation types. The authors construct a benchmark by extending the CommonGen
  dataset with ConceptNet-based reasoning graphs and systematically splitting the
  data to test compositional generalization.
---

# The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning

## Quick Facts
- **arXiv ID**: 2410.06272
- **Source URL**: https://arxiv.org/abs/2410.06272
- **Reference count**: 34
- **Primary result**: Even advanced LLMs struggle with compositional generalization when generating sentences from unseen combinations of reasoning graph relation types

## Executive Summary
This paper introduces the Compositional Generalization Challenge for Graph-based Commonsense Reasoning (CGGC), a benchmark that evaluates language models' ability to generate natural language from reasoning graphs containing novel combinations of relation types. The authors systematically construct the benchmark by extending CommonGen with ConceptNet-based reasoning graphs and create controlled splits to test compositional generalization. Through extensive evaluation of seven well-known LLMs using in-context learning, the study reveals that these models consistently struggle with compositional generalization, showing significant performance gaps between in-distribution and out-of-distribution settings. The research identifies graph structure and primitive relation frequency as key factors affecting performance, while also demonstrating that demonstration ordering can substantially improve compositional generalization ability.

## Method Summary
The authors construct the CGGC benchmark by extending the CommonGen dataset with reasoning graphs derived from ConceptNet, creating 20K graphs with three relation types. They design a 20%-80% split where 20% of primitive relations (source-target pairs) appear only in the test set, ensuring true compositional generalization evaluation. The evaluation employs in-context learning with 4-8 demonstrations per example across seven well-known LLMs. Performance is measured using automatic metrics (ROUGE, BLEU, BERTScore) as well as human evaluation of relevance and fluency. The study systematically analyzes factors affecting difficulty including graph structure (transitive vs non-transitive), primitive relation frequency, and demonstration ordering effects.

## Key Results
- All seven evaluated LLMs showed consistent performance gaps between in-distribution and out-of-distribution settings, with relative performance drops of 10-30% across models
- Graph structure significantly impacts difficulty, with transitive structures (A→B→C) being easier than graphs with common source or target nodes
- Primitive relation frequency in the training set moderately correlates with test performance (correlation coefficients around 0.3-0.5)
- Arranging demonstrations in easy-to-hard order significantly improves compositional generalization ability compared to random ordering

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanisms behind why LLMs struggle with compositional generalization in graph-based commonsense reasoning. The difficulty appears to stem from models' inability to effectively combine primitive relations they've seen separately during training, but the specific cognitive or architectural reasons for this limitation are not explored in depth.

## Foundational Learning
**Compositional Generalization**: The ability to understand and generate novel combinations of known components. Needed to evaluate whether models can reason beyond memorized patterns. Quick check: Can the model handle A→B and B→C combinations when only A→B and B→C were seen separately during training?

**Graph-based Commonsense Reasoning**: Representing relationships between concepts as structured graphs rather than text. Needed to create controlled test conditions for compositional generalization. Quick check: Does the graph accurately capture all relevant semantic relationships between concepts?

**In-Context Learning**: The ability of models to perform tasks using only prompt demonstrations without parameter updates. Needed to evaluate zero-shot or few-shot compositional generalization. Quick check: Are demonstrations representative and diverse enough to guide generation?

**Primitive Relations**: Basic source-target pairs that serve as building blocks for more complex reasoning. Needed to systematically control what combinations models have seen during training. Quick check: Are primitive relations correctly identified and tracked across splits?

## Architecture Onboarding

**Component Map**: Data Construction -> Benchmark Creation -> Model Evaluation -> Analysis

**Critical Path**: The evaluation pipeline processes each model through in-context learning with demonstrations, generates outputs, then computes automatic metrics and conducts human evaluation to assess compositional generalization performance.

**Design Tradeoffs**: The choice of 20% primitive relation split balances creating challenging compositional generalization conditions while maintaining sufficient training data. Using only 4-8 demonstrations per example trades off between efficient evaluation and potential performance gains from more extensive prompting.

**Failure Signatures**: Performance drops when encountering novel primitive relation combinations, particularly in graphs with common source or target nodes. Models show stronger performance on transitive structures but struggle with more complex graph topologies.

**3 First Experiments**:
1. Test whether increasing demonstrations from 4 to 8 improves compositional generalization performance
2. Evaluate if models show different performance patterns on transitive vs non-transitive graph structures
3. Measure the impact of demonstration ordering (easy-to-hard vs random) on test performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on single human reference per graph, potentially missing valid semantic variations
- In-context learning uses limited demonstrations (4-8), which may not fully reveal model capabilities
- Analysis focuses on structural factors but doesn't deeply explore semantic relatedness or world knowledge effects
- Study evaluates only seven LLMs without exploring systematic architectural differences

## Confidence

**High Confidence**: The core finding that LLMs struggle with compositional generalization in graph-based commonsense reasoning is well-supported by consistent performance gaps across multiple models and controlled experimental design.

**Medium Confidence**: The identification of primitive relation frequency as a moderate performance correlate is supported but requires caution as the analysis doesn't control for potential confounding factors like semantic similarity.

**Low Confidence**: The broader implications for model architecture and training methods remain speculative, as the study doesn't directly test whether modifications to model design would improve compositional generalization.

## Next Checks

1. **Multi-reference evaluation**: Conduct human evaluations with multiple references per graph to better assess whether the compositional generalization gap persists when accounting for the full semantic space of valid generations.

2. **Demonstration scaling experiment**: Systematically vary the number of demonstrations (e.g., 4, 8, 16, 32) to determine whether larger demonstration sets can close the compositional generalization gap or if the effect is inherently limited.

3. **Semantic similarity control**: Re-analyze the relationship between primitive relation frequency and performance while controlling for concept pair semantic similarity to determine whether the observed correlation is independent of semantic relatedness effects.