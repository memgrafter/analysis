---
ver: rpa2
title: 'Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers'
arxiv_id: '2405.05295'
source_url: https://arxiv.org/abs/2405.05295
tags:
- explanations
- alterfactual
- counterfactual
- features
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces alterfactual explanations, a new XAI method
  that shows alternative realities where irrelevant features are altered while relevant
  ones remain fixed. It proposes a GAN-based approach to generate these explanations
  for binary image classifiers, combining adversarial training, classifier loss, SSIM,
  and an SVM-based feature relevance component.
---

# Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers

## Quick Facts
- arXiv ID: 2405.05295
- Source URL: https://arxiv.org/abs/2405.05295
- Reference count: 40
- One-line primary result: Alterfactual explanations significantly improve local model understanding compared to no explanations or counterfactuals alone, while combining both yields the best results

## Executive Summary
This paper introduces alterfactual explanations, a novel XAI method that generates alternative realities where irrelevant features are altered while relevant ones remain fixed. The approach uses a GAN-based architecture to create both alterfactual and counterfactual explanations for binary image classifiers, with a user study (n=131) demonstrating that alterfactuals significantly improve local model understanding compared to no explanations or counterfactuals alone. The method combines adversarial training with classifier loss, SSIM loss, and an SVM-based feature relevance component to generate explanations that satisfy specific validity and similarity requirements.

## Method Summary
The method employs a GAN-based approach to generate alterfactual and counterfactual explanations for binary image classifiers. The generator network takes an original image and random noise to produce an altered image, while the discriminator is trained to distinguish real from generated images and classify them into target classes. The combined loss function includes adversarial loss, classifier loss (Binary Crossentropy), SSIM loss for similarity control, and SVM loss to maintain distance to the decision boundary for alterfactuals. The approach is tested on Fashion-MNIST, MNIST, and MaskedFace-Net datasets, with explanations evaluated based on validity (classification accuracy) and SSIM similarity metrics.

## Key Results
- Alterfactual explanations achieve high validity (>84%) across all tested datasets
- User study shows significant improvement in local model understanding with alterfactual explanations (p < 0.05)
- Combining alterfactual and counterfactual explanations yields the best prediction accuracy in user study
- No significant difference found in explanation satisfaction or global feature understanding between explanation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GAN-based architecture generates realistic alterfactual and counterfactual images by learning to fool the discriminator into classifying them as belonging to the correct target class.
- Mechanism: The generator takes an original image and random noise, producing an altered image. The discriminator is trained to distinguish real images from generated ones while also classifying them into target classes. Through adversarial training, the generator learns to create images that look realistic and match the target class distribution.
- Core assumption: The adversarial training process can effectively learn to generate high-quality images that match real data distribution while satisfying the additional constraints (class matching, similarity measures).
- Evidence anchors:
  - [abstract] "We present a GAN-based approach to generate these alterfactual explanations for binary image classifiers."
  - [section] "To address these objectives, different loss components (see next sections) were used to steer a GAN-based architecture to generate the desired explanations."
  - [corpus] Weak - no direct GAN training evidence in corpus papers, though similar GAN-based XAI methods exist.
- Break condition: If the discriminator becomes too strong too quickly, the generator cannot learn effectively, resulting in poor quality images that don't match the target class.

### Mechanism 2
- Claim: The combination of classifier loss, SSIM loss, and SVM loss enables the model to generate explanations that satisfy the specific requirements for alterfactual and counterfactual explanations.
- Mechanism: The classifier loss ensures the generated image is classified as the target class. The SSIM loss controls similarity to the original image (high for counterfactual, low for alterfactual). The SVM loss for alterfactuals ensures the distance to the decision boundary remains unchanged, indicating only irrelevant features are modified.
- Core assumption: These loss components can be effectively combined and weighted to achieve the desired properties without conflicting with each other.
- Evidence anchors:
  - [section] "The final loss function is a summation of all the four loss components introduced above."
  - [section] "The SVM loss is defined by the absolute difference in distance to the separating hyperplane between the original image and the generated alterfactual explanation."
  - [corpus] Weak - corpus contains related GAN-based explanation methods but not this specific combination of losses.
- Break condition: If the loss weights are not properly balanced, one objective may dominate and prevent the others from being satisfied, resulting in explanations that don't meet the requirements.

### Mechanism 3
- Claim: Alterfactual explanations improve local model understanding by showing users which features can change without affecting the model's decision, while counterfactual explanations show which features need to change to alter the decision.
- Mechanism: By providing both types of explanations, users gain a more complete understanding of the model's decision boundary. Alterfactuals demonstrate the irrelevance of certain features, while counterfactuals demonstrate the relevance of others. The user study shows this combination leads to better prediction accuracy.
- Core assumption: Users can effectively process and learn from both types of explanations simultaneously, and this combination provides complementary information that improves understanding.
- Evidence anchors:
  - [abstract] "A user study (n=131) shows that alterfactual explanations significantly improve local model understanding compared to no explanations or counterfactuals alone."
  - [section] "we investigated whether the understanding that users have of the explained AI system is also formed in a different way, or can even be improved."
  - [corpus] Weak - corpus papers discuss related explanation methods but not specifically alterfactual explanations or their combination with counterfactuals.
- Break condition: If users are overwhelmed by receiving both explanation types simultaneously, cognitive load may prevent effective learning, reducing the benefit of the combined approach.

## Foundational Learning

- Concept: GAN training dynamics and loss function balancing
  - Why needed here: The success of the approach depends on properly balancing adversarial loss with classifier, SSIM, and SVM losses to achieve the desired properties of the generated explanations.
  - Quick check question: If you observe that generated images look realistic but don't match the target class, which component of the loss function is likely not being optimized effectively?

- Concept: Decision boundary approximation using SVM
  - Why needed here: The SVM loss component for alterfactual explanations relies on approximating the classifier's decision boundary to ensure only irrelevant features are modified.
  - Quick check question: If you modify the SVM to use a different kernel type, how might this affect the quality of alterfactual explanations?

- Concept: Structural Similarity Index (SSIM) for image quality assessment
  - Why needed here: SSIM is used as the similarity metric to control how much the generated explanations should differ from the original image.
  - Quick check question: Why might SSIM be a more appropriate similarity metric than pixel-wise MSE for this application?

## Architecture Onboarding

- Component map: Generator network -> Discriminator network -> Classifier -> SVM -> Loss functions (Adversarial, Classifier, SSIM, SVM)

- Critical path:
  1. Train classifier on dataset
  2. Train SVM on classifier's penultimate layer activations
  3. Train GAN with all loss components
  4. Generate explanations using trained generator
  5. Evaluate explanation quality (validity, similarity, user study)

- Design tradeoffs:
  - Using a simple SVM vs. more complex boundary approximation methods: Simpler but potentially less accurate
  - Training separate GANs for alterfactual and counterfactual vs. single conditional GAN: More flexible but requires careful loss balancing
  - Including vs. excluding the SVM component: More accurate for alterfactuals but reduces model-agnostic property

- Failure signatures:
  - Low validity scores: Classifier loss not effective, or discriminator too strong
  - Poor visual quality: Adversarial loss not effective, or generator architecture issues
  - Similar SSIM values for both explanation types: SSIM loss not properly conditioned on explanation type
  - Low user study performance: Explanations not meaningful or user interface issues

- First 3 experiments:
  1. Train GAN without SVM loss component to verify it can generate valid counterfactuals with appropriate similarity
  2. Add SVM loss and verify alterfactuals maintain classifier prediction while changing maximally
  3. Test different SSIM weightings to find optimal balance between alterfactual and counterfactual generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alterfactual explanations perform on multi-class classification tasks beyond binary classifiers?
- Basis in paper: [explicit] The paper states that the current GAN-based approach is designed for binary image classifiers and mentions that it could theoretically be adapted for non-binary tasks by training separate models for each class vs. the union over all other classes.
- Why unresolved: The authors did not test their approach on multi-class datasets or explore how effective alterfactual explanations would be in such scenarios.
- What evidence would resolve it: Experimental results showing the performance of alterfactual explanations on multi-class datasets like CIFAR-10 or ImageNet, including metrics like validity, SSIM, and user study outcomes.

### Open Question 2
- Question: Does the inclusion of the Feature Relevance component in the GAN architecture significantly improve the quality of alterfactual explanations?
- Basis in paper: [explicit] The authors note that they omitted the Feature Relevance component for experiments on MNIST, MaskedFace-Net, and MaskedFace-Net (gray scale) to make the approach more model-agnostic, but they do not provide a direct comparison of results with and without this component.
- Why unresolved: The paper does not present a comparative analysis of the explanations generated with and without the Feature Relevance component, leaving its impact unclear.
- What evidence would resolve it: A controlled experiment comparing the validity, SSIM, and user perception of alterfactual explanations generated with and without the Feature Relevance component on the same dataset.

### Open Question 3
- Question: How do alterfactual explanations affect user trust and reliance on AI systems in real-world applications?
- Basis in paper: [inferred] The user study focused on local and global model understanding and explanation satisfaction but did not explicitly measure trust or reliance on the AI system.
- Why unresolved: Trust and reliance are critical factors in the adoption of AI systems, but the study did not assess these aspects, leaving a gap in understanding the broader impact of alterfactual explanations.
- What evidence would resolve it: A follow-up study incorporating trust and reliance metrics, such as surveys or behavioral experiments, to evaluate how alterfactual explanations influence users' willingness to rely on AI decisions in practical scenarios.

## Limitations
- User study (n=131) provides promising but not definitive evidence of effectiveness
- Results show significant improvements in local model understanding but no difference in explanation satisfaction or global feature understanding
- Method requires training separate models (classifier, SVM, GAN) for each target classifier, limiting scalability
- Experiments conducted only on simple binary classification tasks (Fashion-MNIST, MNIST), limiting generalizability

## Confidence
- GAN architecture effectiveness: Medium (supported by validity metrics but limited qualitative analysis)
- User study conclusions: Medium (statistically significant but small effect sizes)
- Generalizability to complex models/datasets: Low (only tested on simple binary classification tasks)

## Next Checks
1. Replicate the user study with different participant demographics and tasks to verify the consistency of the findings
2. Test the approach on more complex image classification datasets (e.g., CIFAR-10, ImageNet) to assess scalability
3. Conduct ablation studies on the loss components to determine their individual contributions to the final results