---
ver: rpa2
title: 'Grams: Gradient Descent with Adaptive Momentum Scaling'
arxiv_id: '2412.17107'
source_url: https://arxiv.org/abs/2412.17107
tags:
- grams
- step
- sign
- optimization
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grams is a novel optimization algorithm for deep learning that
  decouples the direction and magnitude of parameter updates. Unlike traditional optimizers
  that directly integrate momentum into updates, Grams separates the update direction
  (derived from current gradients) from momentum (used solely for adaptive magnitude
  scaling).
---

# Grams: Gradient Descent with Adaptive Momentum Scaling

## Quick Facts
- arXiv ID: 2412.17107
- Source URL: https://arxiv.org/abs/2412.17107
- Authors: Yang Cao; Xiaoyu Li; Zhao Song
- Reference count: 14
- Primary result: Grams optimizer achieves superior performance on language modeling and vision tasks compared to Adam and Lion optimizers

## Executive Summary
Grams introduces a novel optimization algorithm that decouples update direction from momentum scaling in deep learning optimization. Unlike traditional optimizers that integrate momentum directly into parameter updates, Grams uses gradient direction for sign selection while momentum is used solely for adaptive magnitude scaling. The paper establishes theoretical guarantees including discrete-time descent analysis, Hamiltonian dynamics analysis, and global convergence properties. Empirical evaluations demonstrate Grams' superior performance across multiple benchmarks, including language modeling (Llama 60M pre-training) and computer vision (WideResNet-50-2 on CIFAR-10).

## Method Summary
Grams is a gradient descent algorithm that separates the direction and magnitude of parameter updates. The algorithm maintains first and second moment estimates (mt, vt) similar to Adam, but uses these differently: the update direction is determined by the sign of the gradient, while the magnitude is scaled adaptively using momentum-derived factors. The key innovation is that momentum is used only for scaling the update magnitude rather than influencing the update direction. This decoupling allows for more efficient optimization by ensuring updates always align with gradient directions while benefiting from momentum's adaptive scaling properties.

## Key Results
- Llama 60M pre-training: Grams achieved perplexity of 38.60 vs Adam's 49.83 and Lion's 50.25
- WideResNet-50-2 on CIFAR-10: Grams reached 90.55% accuracy vs Lion's 89.21%
- Llama 3.2 1B fine-tuning: Grams achieved 51.02% accuracy on GSM-8K vs Adam's 48.90%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Grams achieves strictly better loss descent than Cautious optimizers by decoupling direction and magnitude of updates
- **Mechanism**: The algorithm uses gradient direction for the sign of the update while scaling the magnitude using momentum-derived adaptive scaling. This ensures that the update always aligns with the gradient direction while benefiting from momentum's adaptive scaling
- **Core assumption**: The momentum-derived magnitude scaling (ut) is orthogonal to the direction optimization problem, allowing independent optimization of each component
- **Evidence anchors**:
  - [abstract] "Unlike traditional optimizers that directly integrate momentum into updates, Grams separates the update direction, derived from current gradients, from momentum, which is used solely for adaptive magnitude scaling"
  - [section] "Lemma 4.2 shows that the loss decrease for Grams is -ηt⟨|gt|, |ut|⟩, which is always larger than the cautious mechanism's -ηt⟨ut ◦ gt, 1ut◦gt≥0⟩ due to Fact 3.6"
  - [corpus] Weak evidence - no direct citations to support this specific mechanism
- **Break condition**: If the momentum-derived magnitude scaling becomes negative or zero, the update magnitude becomes ineffective, though this is prevented by the denominator √bvt + ϵ

### Mechanism 2
- **Claim**: Grams provides faster Hamiltonian dynamics convergence by aligning gradient direction with momentum scaling
- **Mechanism**: The Hamiltonian dynamics analysis shows that Grams achieves a monotonic decrease in both the Hamiltonian and loss function, with the loss decreasing rate being equal to or faster than Cautious optimizers
- **Core assumption**: The Hamiltonian framework with modified dynamics preserves convergence properties while improving the descent rate
- **Evidence anchors**:
  - [abstract] "We theoretically demonstrate that Grams descents faster than other state-of-the-art optimizers and establish a global convergence guarantee for Grams"
  - [section] "Theorem 4.6 establishes that ∆Grams H(wt, st) ≤ 0 and ∆Grams L(wt) ≤ -∆L(wt, st), showing monotonic decrease in both Hamiltonian and loss"
  - [corpus] Weak evidence - no direct citations to support Hamiltonian dynamics analysis
- **Break condition**: If the PL-condition (Assumption 4.13) fails, the global convergence guarantee may not hold

### Mechanism 3
- **Claim**: Grams achieves global convergence under standard assumptions by leveraging PL-condition and adaptive momentum scaling
- **Mechanism**: The algorithm combines the convergence properties of Adam (Lemma 4.14) with the PL-condition to guarantee convergence to stationary points
- **Core assumption**: The µ-PL-condition (Assumption 4.13) holds for the loss function being optimized
- **Evidence anchors**:
  - [abstract] "We establish a global convergence guarantee for Grams"
  - [section] "Theorem 4.15 proves that L(wT) - L* ≤ 4G/(µηT)(L(w1) - L*), showing convergence under PL-condition"
  - [corpus] Weak evidence - no direct citations to support global convergence analysis
- **Break condition**: If the PL-condition fails or the initial optimality gap is infinite, the convergence guarantee does not apply

## Foundational Learning

- **Concept**: Hamiltonian dynamics in optimization
  - Why needed here: Grams is analyzed using Hamiltonian dynamics framework to understand its convergence properties
  - Quick check question: What is the key difference between Hamiltonian dynamics and standard gradient descent in terms of the augmented objective function?

- **Concept**: PL-condition (Polyak-Lojasiewicz condition)
  - Why needed here: Global convergence proof relies on the PL-condition being satisfied
  - Quick check question: What inequality must hold for a function to satisfy the µ-PL-condition?

- **Concept**: Adaptive momentum scaling
  - Why needed here: Core innovation of Grams is using momentum only for adaptive magnitude scaling, not direction
  - Quick check question: How does Grams compute the update magnitude differently from Adam?

## Architecture Onboarding

- **Component map**: Gradient computation -> Momentum update -> Adaptive scaling -> Direction selection -> Parameter update
- **Critical path**: Gradient computation → Momentum update → Adaptive scaling → Direction selection → Parameter update
- **Design tradeoffs**:
  - Memory: Same as Adam (two moment estimates)
  - Computation: Additional sign operation and absolute value
  - Stability: Better than Adam due to direction alignment with gradients
  - Convergence: Theoretically faster than both Adam and Lion

- **Failure signatures**:
  - NaN values in weights: Check denominator √bvt + ϵ for numerical stability
  - Slow convergence: Verify PL-condition holds for the problem
  - Memory issues: Monitor size of moment estimates growing unbounded

- **First 3 experiments**:
  1. Simple convex function (quadratic) to verify faster convergence than Adam
  2. CIFAR-10 classification with WideResNet-50-2 to test generalization
  3. Llama 60M pre-training to evaluate large-scale optimization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Grams optimizer's superior performance on Llama 60M pre-training translate to larger language models with billions of parameters?
- Basis in paper: [inferred] The paper demonstrates Grams' effectiveness on a 60M parameter model but does not test on larger-scale models.
- Why unresolved: The computational resources required for testing larger models may be prohibitive, and the paper does not address scalability concerns.
- What evidence would resolve it: Empirical results comparing Grams to other optimizers on models with 1B+ parameters, showing consistent performance improvements across different model sizes.

### Open Question 2
- Question: How does the Grams optimizer perform on non-Transformer architectures such as convolutional neural networks or recurrent neural networks?
- Basis in paper: [inferred] The paper focuses on Transformer-based models for language tasks and WideResNet for vision tasks, but does not explore other architectures.
- Why unresolved: The optimization dynamics may differ significantly across architectures, and the paper does not provide theoretical or empirical analysis for diverse model types.
- What evidence would resolve it: Comparative experiments on CNNs, RNNs, and other architectures, demonstrating whether Grams' advantages extend beyond Transformers.

### Open Question 3
- Question: What is the impact of different hyperparameter configurations (e.g., β1, β2, learning rate schedules) on the Grams optimizer's performance?
- Basis in paper: [explicit] The paper uses default hyperparameter values for Grams and other optimizers but does not explore the sensitivity of Grams to hyperparameter tuning.
- Why unresolved: Hyperparameter optimization can significantly affect model performance, and the paper does not provide ablation studies or sensitivity analyses.
- What evidence would resolve it: Comprehensive ablation studies varying β1, β2, learning rates, and schedules, identifying optimal configurations and robustness to hyperparameter changes.

## Limitations

- Theoretical guarantees rely on PL-condition which may not hold for all deep learning loss surfaces
- Convergence proofs depend on standard assumptions about bounded gradients and L-smoothness that are not empirically validated for specific tasks
- Comparison with state-of-the-art optimizers is limited to a small set of benchmarks, results may vary across different architectures and problem domains

## Confidence

- **High confidence**: Algorithmic description and implementation details are clearly specified
- **Medium confidence**: Theoretical convergence guarantees are established but depend on unverified assumptions
- **Low confidence**: Universality of performance improvements across all deep learning tasks is unproven

## Next Checks

1. **PL-condition verification**: Empirically test whether the loss functions in the benchmark tasks satisfy the µ-PL-condition across different training stages, as this is critical for the global convergence guarantee.

2. **Ablation study on momentum scaling**: Compare Grams against variants where momentum is used only for direction, only for magnitude, or not used at all to isolate the contribution of the decoupled approach.

3. **Cross-domain robustness testing**: Evaluate Grams on additional tasks including NLP, computer vision, and reinforcement learning to verify that performance gains generalize beyond the reported benchmarks.