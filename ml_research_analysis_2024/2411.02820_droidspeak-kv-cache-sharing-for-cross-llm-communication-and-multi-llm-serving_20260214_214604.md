---
ver: rpa2
title: 'DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving'
arxiv_id: '2411.02820'
source_url: https://arxiv.org/abs/2411.02820
tags:
- cache
- reusing
- prefill
- agent
- sender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DroidSpeak addresses the problem of long prefill-phase latency
  in multi-agent LLM systems, where agents traditionally communicate through natural
  language, causing each receiver to reprocess entire contexts. The core method involves
  enabling one LLM to reuse the prefix KV caches of another LLM with the same architecture
  by selectively recomputing a few layers and reusing the remaining layers, with negligible
  quality loss.
---

# DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving

## Quick Facts
- arXiv ID: 2411.02820
- Source URL: https://arxiv.org/abs/2411.02820
- Authors: Yuhan Liu; Yuyang Huang; Jiayi Yao; Shaoting Feng; Zhuohan Gu; Kuntai Du; Hanchen Li; Yihua Cheng; Junchen Jiang; Shan Lu; Madan Musuvathi; Esha Choukse
- Reference count: 40
- One-line primary result: Up to 4x throughput improvement and about 3.1x faster prefill with negligible quality loss by enabling KV cache sharing across different LLMs with the same architecture.

## Executive Summary
DroidSpeak addresses the problem of long prefill-phase latency in multi-agent LLM systems, where agents traditionally communicate through natural language, causing each receiver to reprocess entire contexts. The core method involves enabling one LLM to reuse the prefix KV caches of another LLM with the same architecture by selectively recomputing a few layers and reusing the remaining layers, with negligible quality loss. The primary results show up to 4x throughput improvement and about 3.1x faster prefill (time to first token), with negligible loss of quality in F1 scores, Rouge-L, or code similarity score compared to baseline systems that do not allow any sharing across models.

## Method Summary
DroidSpeak enables cross-LLM KV cache reuse by profiling layer-wise similarity between base and fine-tuned models, then selectively reusing sender LLM's KV and E caches while recomputing non-reusable layers. The system uses two NVIDIA A100 GPU nodes connected via 200Gbps Infiniband to transfer KV+E caches between sender and receiver LLMs. The profiling determines optimal layers for cache reuse per model pair, and DroidSpeak combines KV and E cache reuse strategies to avoid missing embedding data at transition layers while minimizing GPU memory overhead.

## Key Results
- Up to 4x throughput improvement and about 3.1x faster prefill (time to first token)
- Negligible loss of quality in F1 scores, Rouge-L, or code similarity score compared to baseline systems
- Demonstrated effectiveness across four pairs of LLMs (base and fine-tuned versions) using six benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-LLM KV cache reuse is possible because fine-tuned models share similar layer weights and intermediate data (KV and E caches) with their base models.
- Mechanism: By identifying layers where the KV and E caches differ minimally between base and fine-tuned models, DroidSpeak can selectively reuse those caches without quality loss, reducing prefill latency.
- Core assumption: The intermediate data (KV and E caches) across layers varies in similarity between base and fine-tuned models, enabling selective reuse.
- Evidence anchors:
  - [abstract] DroidSpeak selectively recomputes a few layers of the KV cache produced by another LLM and reuses the remaining layers, with negligible quality loss.
  - [section] We show that each layer across these LLMs has a different degree of similarity and a different degree of impact on the final output accuracy.
  - [corpus] No direct evidence in corpus papers about cross-LLM cache reuse for fine-tuned models; existing work focuses on intra-model or adapter-based reuse.
- Break condition: If the layer-wise similarity between base and fine-tuned models is too low, reuse would degrade quality significantly.

### Mechanism 2
- Claim: Reusing E cache offers more flexibility than reusing KV cache because E cache can be reused intermittently across layers.
- Mechanism: Reusing the embedding cache allows switching between reuse and recomputation at any layer without losing access to the necessary input embeddings, unlike KV cache reuse which locks in contiguous reuse.
- Core assumption: E cache reuse does not incur the same GPU memory or transmission overhead as KV cache reuse, and allows for flexible layer-wise reuse patterns.
- Evidence anchors:
  - [section] Reusing the E cache also offers greater flexibility in terms of the layer-position of the data that can be reused, unlike reusing KV cache.
  - [section] Since reusing E cache allows it to be reused intermittently, it opens up a two-dimensional space of which layers to reuse.
  - [corpus] No corpus evidence directly supports E cache reuse flexibility; related work focuses on KV cache compression or blending, not E cache reuse strategies.
- Break condition: If E cache transmission or GPU memory overhead becomes prohibitive, the flexibility advantage diminishes.

### Mechanism 3
- Claim: KV+E cache reuse combines the benefits of both KV and E cache reuse while avoiding their individual drawbacks.
- Mechanism: At the transition layer, load the embedding E from the sender LLM before switching from KV cache reuse to recomputation, avoiding the missing E problem and reducing GPU memory overhead on the sender side.
- Core assumption: The combination of KV and E cache reuse at the transition layer maintains accuracy while minimizing computational overhead on both sender and receiver.
- Evidence anchors:
  - [section] One solution to the lost E problem with KV cache reuse is: at the transition layer, where we want to switch from KV cache reusing to recomputation, we load the embedding E from the sender LLM.
  - [section] DroidSpeak's gain in terms of TTFT compared with the baselines, across 4 pairs of models. We can see that compared to full prefill, DroidSpeak has 1.69–2.77× reduction in prefill delay, without compromising generation quality.
  - [corpus] No corpus evidence about KV+E cache reuse; related work on KV cache compression or blending does not address this combined approach.
- Break condition: If the overhead of loading E at transition layers outweighs the latency savings, the combined approach becomes inefficient.

## Foundational Learning

- Concept: Transformer architecture and KV cache mechanics
  - Why needed here: Understanding how transformers process input sequences and generate KV caches is essential to grasp how DroidSpeak reuses intermediate data across models.
  - Quick check question: What are the two primary phases of transformer inference, and how does KV cache usage differ between them?

- Concept: Fine-tuning and its impact on model weights and intermediate data
  - Why needed here: DroidSpeak relies on the similarity between base and fine-tuned models; understanding fine-tuning helps explain why cache reuse is feasible.
  - Quick check question: How does fine-tuning typically change model weights, and why might intermediate data like KV cache remain similar?

- Concept: Prefill vs decode latency scaling
  - Why needed here: The motivation for DroidSpeak is reducing prefill latency, which scales super-linearly with input length; understanding this scaling is key to appreciating the system's benefits.
  - Quick check question: Why is prefill latency typically much higher than decode latency in transformer models, especially for long contexts?

## Architecture Onboarding

- Component map:
  - Sender LLM node -> Profiler -> Receiver LLM node
  - High-bandwidth interconnect (e.g., Infiniband) between nodes

- Critical path:
  1. Sender LLM generates KV+E cache for input.
  2. KV+E cache transmitted to receiver LLM.
  3. Receiver LLM loads reusable KV+E caches and recomputes non-reusable layers.
  4. Receiver LLM generates output using combined cache.

- Design tradeoffs:
  - Flexibility vs overhead: E cache reuse offers more flexibility but incurs higher transmission and computation overhead than KV cache reuse.
  - Accuracy vs latency: More aggressive cache reuse reduces latency but risks quality loss if layer similarity is insufficient.
  - Memory usage: Sender GPU memory overhead for storing E cache vs receiver GPU memory for storing combined cache.

- Failure signatures:
  - Quality degradation: Output quality drops if reused layers differ too much between models.
  - Increased latency: Transmission or recomputation overhead negates latency benefits.
  - Memory bottlenecks: Insufficient GPU memory for storing large KV+E caches.

- First 3 experiments:
  1. Measure layer-wise similarity (MSE) between base and fine-tuned models on a small dataset to identify reusable layers.
  2. Implement basic E cache reuse for a single model pair and measure prefill latency and quality impact.
  3. Test KV+E cache reuse at transition layers and compare latency/quality against E cache only reuse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DroidSpeak's KV+E cache reuse strategy be extended to models with different architectures or different base models?
- Basis in paper: [inferred] The paper states that DroidSpeak enables KV cache reuse "so long as the LLMs have the same architecture" and focuses on fine-tuned versions of the same foundational model, suggesting potential limitations with different architectures.
- Why unresolved: The paper explicitly limits its evaluation to model pairs sharing the same foundational model and architecture, leaving open the question of cross-architecture applicability.
- What evidence would resolve it: Experiments testing DroidSpeak with model pairs from different architectural families (e.g., Llama vs GPT vs Mistral) or models based on different foundational models, measuring both performance gains and quality preservation.

### Open Question 2
- Question: How does DroidSpeak perform under real-world multi-agent system constraints such as network variability, GPU memory pressure, and concurrent agent communication?
- Basis in paper: [explicit] The paper mentions using 200Gbps Infiniband links in controlled experiments but doesn't address network variability, and discusses GPU memory overhead but doesn't evaluate under memory pressure scenarios.
- Why unresolved: The evaluation uses ideal hardware conditions and doesn't simulate realistic deployment challenges like network congestion, GPU memory contention, or multiple agents communicating simultaneously.
- What evidence would resolve it: Performance measurements of DroidSpeak under varying network conditions (latency, bandwidth fluctuations), with memory-constrained GPUs, and in scenarios with multiple concurrent agent communications.

### Open Question 3
- Question: Can DroidSpeak's layer reuse configuration be dynamically optimized at runtime based on token-specific differences between sender and receiver models?
- Basis in paper: [explicit] The paper observes in Figure 18 that different tokens show varying correlation scores across layers, suggesting potential for token-adaptive optimizations, but doesn't implement this.
- Why unresolved: The current implementation uses a static profiling-based configuration determined offline, while the paper identifies token-level variability that could enable more granular optimizations.
- What evidence would resolve it: Implementation and evaluation of a dynamic layer reuse system that adapts per-token based on similarity metrics, comparing performance and quality against the static configuration approach.

## Limitations
- Layer-wise similarity measurements between base and fine-tuned models are presented but not independently validated across diverse model pairs.
- Practical overhead of E cache transmission and claimed flexibility advantage are not empirically quantified against real-world network conditions.
- Generalizability to non-fine-tuned or significantly divergent model architectures remains unclear.

## Confidence
- **High Confidence**: The core observation that KV caches can be partially reused across models with similar architectures, and the general approach of profiling layer similarity to guide reuse decisions.
- **Medium Confidence**: The specific implementation details of E cache reuse flexibility and the KV+E cache reuse strategy, as these are described but not exhaustively tested or compared against alternative cache reuse methods.
- **Low Confidence**: The extrapolation of results to arbitrary model pairs or different hardware setups (e.g., Ethernet instead of Infiniband), and the long-term stability of quality metrics across extended use.

## Next Checks
1. **Cross-Architecture Validation**: Test DroidSpeak's cache reuse approach on model pairs that are not fine-tuned versions of each other (e.g., Llama-3-8B and Mistral-7B) to assess generalizability beyond the paper's scope.
2. **Network Overhead Measurement**: Quantify the actual transmission time and GPU memory overhead of E cache reuse under different network conditions (e.g., 100Gbps vs 200Gbps Infiniband) to validate the claimed flexibility advantage.
3. **Quality Stability Testing**: Run DroidSpeak over extended sequences or multiple generations to monitor if F1/Rouge-L scores degrade over time, especially when reusing KV caches for a high percentage of layers.