---
ver: rpa2
title: 'Reusing Historical Trajectories in Natural Policy Gradient via Importance
  Sampling: Convergence and Convergence Rate'
arxiv_id: '2403.00675'
source_url: https://arxiv.org/abs/2403.00675
tags:
- gradient
- policy
- eeen
- estimator
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient utilization of
  historical trajectories in reinforcement learning for policy optimization. The authors
  propose a variant of natural policy gradient method (RNPG) that reuses historical
  trajectories via importance sampling to accelerate learning.
---

# Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate

## Quick Facts
- arXiv ID: 2403.00675
- Source URL: https://arxiv.org/abs/2403.00675
- Reference count: 37
- The paper proposes RNPG, a natural policy gradient method that reuses historical trajectories via importance sampling, demonstrating improved convergence rates and reduced variance in policy optimization.

## Executive Summary
This paper addresses the challenge of efficiently utilizing historical trajectories in reinforcement learning for policy optimization. The authors propose a variant of natural policy gradient method (RNPG) that reuses historical trajectories via importance sampling to accelerate learning. The core idea is to estimate both the gradient and Fisher information matrix using weighted combinations of trajectories from multiple iterations, with weights determined by likelihood ratios. Theoretical analysis demonstrates that the bias of the gradient estimator is asymptotically negligible, the algorithm converges to a local optimum, and reusing past trajectories improves the convergence rate by a factor of 1/K where K is the number of iterations of data reuse.

## Method Summary
The RNPG algorithm estimates policy gradients by reusing historical trajectories through importance sampling. At each iteration, trajectories from the current policy and up to K-1 previous policies are weighted by likelihood ratios and combined to form gradient and Fisher information matrix estimates. The algorithm maintains the same limit ODE as vanilla natural policy gradient while achieving faster convergence through variance reduction. The Fisher information matrix is estimated using only current iteration data to ensure consistency, while the gradient uses the weighted combination of historical samples.

## Key Results
- The bias of the gradient estimator becomes asymptotically negligible as step size approaches zero
- RNPG converges to the same local optimum as vanilla natural policy gradient while improving convergence rate by O(1/K)
- Reusing past trajectories reduces gradient estimator variance from σ²/B to σ²/(KB) asymptotically
- Numerical experiments on cartpole and MuJoCo inverted pendulum verify performance improvements over vanilla NPG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reusing historical trajectories via importance sampling accelerates policy gradient convergence by reducing gradient variance.
- Mechanism: The importance sampling likelihood ratio reweights historical samples to approximate the current policy's gradient. When reusing K iterations of data, the variance of the gradient estimator decreases from σ²/B to σ²/(KB) asymptotically.
- Core assumption: The policy changes slowly enough between iterations that the importance weights ω(ξ, θn|θm) ≈ 1 for recent historical data.
- Evidence anchors:
  - [abstract] "reusing past trajectories helps improve the convergence rate by an order of O(1/K)"
  - [section] "reusing past samples in the previous K − 1 iterations helps reduce the covariance... by an order of O(1/K)"
  - [corpus] Weak - no direct evidence in neighbors about variance reduction mechanisms
- Break condition: If policy changes too rapidly between iterations, importance weights become unstable and variance increases instead of decreases.

### Mechanism 2
- Claim: The bias introduced by reusing historical trajectories becomes asymptotically negligible as step size diminishes.
- Mechanism: As αn → 0, the difference between θn and θn+ℓ vanishes for any fixed ℓ, making the importance weights approach 1 and the bias from distribution mismatch vanish.
- Core assumption: The step size sequence {αn} satisfies P∞n=0 α2n < ∞ and P∞n=0 αn = ∞ with limn→∞ αn = 0.
- Evidence anchors:
  - [abstract] "the bias of the proposed estimator of the gradient is asymptotically negligible"
  - [section] "as discussed above, the bias introduced by reusing samples is due to the dependence of current solution on past samples... asymptotically, we can view all samples generated between iterations n and n + ℓ as approximately conditional i.i.d."
  - [corpus] Weak - no direct evidence in neighbors about bias asymptotic behavior
- Break condition: If step size decreases too slowly or is constant, the bias persists and prevents convergence to the correct limit.

### Mechanism 3
- Claim: RNPG shares the same limit ODE as vanilla natural policy gradient, ensuring convergence to the same local optimum.
- Mechanism: The continuous-time interpolation of the RNPG update converges to the same ODE as VNPG because the noise and bias terms have zero asymptotic rate of change, leaving only the natural gradient term.
- Core assumption: The FIM estimator using only current iteration data (without historical reuse) maintains consistency.
- Evidence anchors:
  - [abstract] "the resultant algorithm is convergent" and "shares the same limit ODE as the vanilla natural policy gradient"
  - [section] "the solution trajectory {θn} given by RNPG (Algorithm 1) converges with probability one (w.p.1) to a local optimum ¯θ, which is the same as the vanilla natural policy gradient (VNPG)"
  - [corpus] Weak - no direct evidence in neighbors about ODE convergence equivalence
- Break condition: If the FIM estimation error dominates, the algorithm may converge to a different point than VNPG.

## Foundational Learning

- Concept: Importance sampling and likelihood ratios
  - Why needed here: The algorithm relies on reweighting historical samples using dπθn(ξ)/dπθm(ξ) to estimate gradients under the current policy
  - Quick check question: What happens to the importance weight ω(ξ, θn|θm) when θn ≈ θm?

- Concept: Fisher Information Matrix (FIM) and natural gradient
  - Why needed here: The algorithm uses FIM to define the natural gradient direction, which accounts for the geometry of the policy parameter space
  - Quick check question: How does the natural gradient differ from the vanilla policy gradient in terms of parameterization invariance?

- Concept: Ordinary Differential Equation (ODE) method for stochastic approximation
  - Why needed here: The convergence analysis shows that the algorithm's iterates converge to the solution trajectory of a deterministic ODE
  - Quick check question: What are the three components that must be shown to have zero asymptotic rate of change in the ODE analysis?

## Architecture Onboarding

- Component map: Policy network → Trajectory generator → Importance weight calculator → Gradient estimator (with historical reuse) → FIM estimator (current only) → Natural gradient updater → Projection to feasible set
- Critical path: Generate trajectories → Compute importance weights → Estimate gradient and FIM → Update policy parameters → Project to feasible set
- Design tradeoffs: Larger K improves convergence rate but increases memory usage and computational complexity (O(KBd² + d³) for FIM inversion)
- Failure signatures: High variance in gradient estimates → unstable training; incorrect importance weights → biased gradients; singular FIM → numerical instability
- First 3 experiments:
  1. Implement RNPG with K=1 (should match VNPG behavior) and verify convergence on cartpole
  2. Test with increasing K values (2, 5, 10) on cartpole and measure convergence speed and variance reduction
  3. Verify theoretical asymptotic normality by plotting normalized error distribution against N(0, αnΣ∞)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RNPG scale with extremely large policy parameter spaces (high-dimensional θ)?
- Basis in paper: [inferred] The paper mentions neural network parameterizations and discusses computational complexity O(KBd² + d³), but doesn't explore high-dimensional scaling limits.
- Why unresolved: The theoretical analysis and experiments focus on moderate dimensions, leaving open questions about scalability to very high-dimensional policy spaces.
- What evidence would resolve it: Experiments varying d with fixed K and B showing convergence rates and computational costs, plus theoretical analysis of how bias/variance terms scale with dimension.

### Open Question 2
- Question: What is the optimal trade-off between reuse size K and mini-batch size B for different problem regimes?
- Basis in paper: [explicit] The paper discusses how increasing K reduces variance but computational costs increase, and mentions practitioners might choose K based on computational budget, but doesn't provide systematic guidelines.
- Why unresolved: While the paper shows K affects convergence rates, it doesn't provide a principled way to select K given computational constraints and problem characteristics.
- What evidence would resolve it: Empirical studies varying K and B across diverse problem classes showing optimal combinations, plus theoretical bounds relating these parameters to convergence rates.

### Open Question 3
- Question: How does RNPG perform in environments with sparse or delayed rewards compared to dense reward environments?
- Basis in paper: [inferred] The paper uses dense reward benchmarks (cartpole, inverted pendulum) and doesn't explore reward structure variations that affect importance sampling effectiveness.
- Why unresolved: The importance sampling approach may behave differently when rewards are sparse or delayed, but this is not explored in the experimental section.
- What evidence would resolve it: Experiments comparing RNPG performance on sparse reward tasks (e.g., Montezuma's Revenge, sparse robotic control tasks) versus dense reward versions of similar problems.

## Limitations

- The theoretical analysis relies heavily on asymptotic assumptions that may not hold in practical finite-sample scenarios
- The extension to other algorithms like TRPO is mentioned but not rigorously analyzed
- The variance reduction mechanism may not materialize if importance weights become unstable when policy changes are too large

## Confidence

**High Confidence:** The asymptotic convergence to a local optimum (Mechanism 3) and the fundamental importance sampling framework are well-established in the literature. The claim that RNPG shares the same limit ODE as VNPG has solid theoretical grounding.

**Medium Confidence:** The asymptotic bias negligibility claim (Mechanism 2) is theoretically sound under the stated step size conditions, but the practical implications for finite-step algorithms are unclear. The experimental validation is limited to two benchmark environments.

**Low Confidence:** The specific convergence rate improvement of O(1/K) (Mechanism 1) is derived asymptotically and lacks rigorous finite-sample validation. The variance reduction mechanism may not materialize in practice if importance weights become unstable.

## Next Checks

1. **Finite-sample bias characterization:** Implement RNPG with various K values and measure the actual bias in gradient estimates compared to the theoretical asymptotic negligibility prediction. Plot bias versus step size and K to identify practical limits.

2. **Importance weight stability analysis:** Systematically vary the policy update magnitude between iterations and measure how importance weight variance scales with K. Identify the threshold where historical reuse begins degrading performance rather than improving it.

3. **Extended benchmark validation:** Test RNPG on a broader set of RL benchmarks (Atari, continuous control from DeepMind Control Suite) to verify that the convergence improvements generalize beyond the two simple environments presented.