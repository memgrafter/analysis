---
ver: rpa2
title: Federated Learning for Time-Series Healthcare Sensing with Incomplete Modalities
arxiv_id: '2405.11828'
source_url: https://arxiv.org/abs/2405.11828
tags:
- modalities
- data
- multimodal
- learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of federated learning (FL) with
  multimodal time-series healthcare sensing data when some modalities are missing
  on client devices. The key method, FLISM, employs three main techniques: modality-invariant
  representation learning to handle missing data, modality quality-aware aggregation
  to prioritize clients with higher-quality data, and global-aligned knowledge distillation
  to reduce local update shifts.'
---

# Federated Learning for Time-Series Healthcare Sensing with Incomplete Modalities

## Quick Facts
- arXiv ID: 2405.11828
- Source URL: https://arxiv.org/abs/2405.11828
- Reference count: 40
- Primary result: FLISM achieves 0.067 average F1-score improvement while reducing communication overhead by 2.69√ó and computational overhead by 2.28√ó compared to existing methods

## Executive Summary
This paper addresses federated learning (FL) for multimodal time-series healthcare sensing when client devices have incomplete modality data. The proposed FLISM framework uses early fusion of modalities with three key techniques: modality-invariant representation learning for missing data robustness, modality quality-aware aggregation for prioritizing higher-quality data, and global-aligned knowledge distillation to reduce local update shifts. Evaluated on three real-world multimodal sensing datasets, FLISM demonstrates significant improvements in both accuracy and efficiency compared to state-of-the-art baselines.

## Method Summary
FLISM employs early fusion of multimodal data with supervised contrastive learning to create representations robust to missing modalities. The framework uses modality quality-aware aggregation to prioritize clients with higher-quality data during model updates. Cross-client knowledge transfer is implemented through knowledge distillation, where clients with more modalities (facilitators) transfer knowledge to clients with fewer modalities (learners) without sharing raw data. The system was evaluated on PAMAP2, RealWorld-HAR, and WESAD datasets with simulated incomplete modality scenarios.

## Key Results
- FLISM achieves an average improvement of 0.067 in F1-score compared to existing methods
- Reduces communication overhead by 2.69√ó and computational overhead by 2.28√ó
- Demonstrates scalability with greater efficiency gains (3.23√ó-85.10√ó faster communication) when modality count increases
- Maintains strong performance across different missing modality ratios (40%-80%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early fusion of incomplete multimodal data achieves higher accuracy and efficiency than late fusion methods.
- Mechanism: Early fusion trains a single unified model on concatenated modalities, avoiding the overhead of training multiple unimodal models. This approach scales better as modality count increases, reducing both communication and computational costs.
- Core assumption: Modalities can be effectively concatenated and processed by a single model architecture without significant loss of information.
- Evidence anchors:
  - [abstract]: "FLISM achieves an average improvement of 0.067 in F1-score while reducing communication overhead by 2.69√ó and computational overhead by 2.28√ó compared to existing methods."
  - [section]: "Early fusion methods exhibit only a marginal increase in MACs and number of trainable model parameters, which is nearly imperceptible compared to the linear escalation seen in late fusion approaches."
- Break condition: When modalities are fundamentally incompatible (e.g., very different data types) and cannot be meaningfully concatenated.

### Mechanism 2
- Claim: Missing-modality-robust learning through supervised contrastive learning improves model performance on incomplete data.
- Mechanism: By simulating missing modality scenarios during training and applying supervised contrastive learning, the model learns representations that are robust to various missing modality patterns.
- Core assumption: Randomly dropping modalities during training creates useful data augmentation that helps the model generalize to real missing modality scenarios.
- Evidence anchors:
  - [section]: "FLISM leverages simulation technique to learn robust representations that can handle missing modalities and transfers model knowledge across clients with varying set of modalities."
  - [section]: "This strategy enables training a model that can map a diverse set of modalities to a similar embedding space for identical labels."
- Break condition: When the missing modality ratio is too high (>80%) that the simulated data no longer represents the true data distribution.

### Mechanism 3
- Claim: Cross-client knowledge transfer through knowledge distillation improves learning for clients with limited modalities.
- Mechanism: Clients with more modalities (facilitators) train a knowledge transfer model, which is then used to distill knowledge to clients with fewer modalities (learners), compensating for missing modality information.
- Core assumption: Knowledge from clients with more modalities can be effectively transferred to clients with fewer modalities through distillation without sharing raw data.
- Evidence anchors:
  - [section]: "We introduce cross-client knowledge transfer to handle this issue. This method utilizes knowledge distillation to share complementary knowledge among clients without transmitting raw data."
  - [section]: "In global round ùë°, the server distributes both knowledge transfer model and primary model to the learner. The learner client first initializes its local model weights to primary model, then trains its local model with the knowledge distillation loss."
- Break condition: When the modality gap between facilitator and learner is too large, making knowledge transfer ineffective.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: The paper addresses privacy-preserving collaborative model training across distributed clients with incomplete modality data.
  - Quick check question: What is the key privacy advantage of Federated Learning compared to centralized approaches?

- Concept: Multimodal Learning
  - Why needed here: The system handles multiple sensing modalities (motion, physiological) that need to be integrated for accurate predictions.
  - Quick check question: What is the difference between early fusion and late fusion in multimodal learning?

- Concept: Supervised Contrastive Learning
  - Why needed here: Used to make the model robust to missing modalities by learning consistent representations across different modality combinations.
  - Quick check question: How does supervised contrastive learning differ from standard supervised learning?

## Architecture Onboarding

- Component map: Server component handles global model aggregation and role assignment; Client components handle local training with modality-specific processing; Communication layer manages model weight exchange
- Critical path: Client role assignment ‚Üí Local training (with missing-modality-robust learning) ‚Üí Knowledge transfer (for learners) ‚Üí Model aggregation ‚Üí Role reassignment ‚Üí Next round
- Design tradeoffs: Early fusion vs late fusion (efficiency vs modularity), simulation vs real data (robustness vs authenticity), knowledge transfer vs local training (performance vs privacy)
- Failure signatures: Poor performance indicates role assignment issues; high communication costs suggest inefficient fusion strategy; low accuracy with high missing modality ratio suggests insufficient robust learning
- First 3 experiments:
  1. Test FLISM with 40% incomplete modalities on PAMAP2 dataset to verify baseline accuracy improvement
  2. Vary incomplete modality ratio (40%, 60%, 80%) to assess robustness across different scenarios
  3. Scale modality count from 5 to 30 to evaluate scalability claims against late fusion baselines

## Open Questions the Paper Calls Out

- Extension to Non-sensing Modalities: How does FLISM's performance compare when extended to non-sensing modalities like images and text? The paper discusses theoretical extension possibilities but lacks empirical validation on mixed sensing/non-sensing tasks.

- System Heterogeneity-Aware Client Selection: What is the optimal client selection strategy that accounts for system heterogeneity (WiFi connectivity, battery life, CPU memory) in FLISM? The paper identifies this as a limitation where individual user system utilities vary dynamically.

- Runtime Handling of Incomplete Modalities: How can FLISM be adapted to handle dynamic modality drops during runtime application execution? The paper focuses on static modality drops during FL training, though the Missing-Modality-Robust Learning component could potentially accommodate dynamic scenarios.

## Limitations

- The paper lacks direct empirical comparisons with late fusion methods, with efficiency claims relative to unspecified baselines rather than direct early vs late fusion comparisons
- Knowledge distillation mechanism relies on strong assumptions about modality similarity between facilitator and learner clients
- Scalability claims (3.23√ó-85.10√ó faster communication) appear to be extrapolations rather than measured results

## Confidence

- High confidence: The general approach of using supervised contrastive learning for missing-modality robustness and the efficiency advantages of early fusion over late fusion for increasing modality counts
- Medium confidence: The specific performance improvements (0.067 F1-score) and efficiency gains (2.69√ó, 2.28√ó) due to lack of direct baseline comparisons
- Low confidence: The scalability claims (3.23√ó-85.10√ó faster communication) as these appear to be extrapolations rather than measured results

## Next Checks

1. Conduct direct empirical comparison between early fusion and late fusion implementations on the same datasets to validate the claimed efficiency advantages
2. Test knowledge distillation effectiveness with deliberately dissimilar modality sets between facilitator and learner clients to establish the limits of cross-client transfer
3. Measure the impact of varying missing modality ratios (20%, 40%, 60%, 80%) on both model accuracy and computational efficiency to identify the breaking point for the missing-modality-robust learning approach