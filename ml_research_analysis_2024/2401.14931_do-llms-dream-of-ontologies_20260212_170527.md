---
ver: rpa2
title: Do LLMs Dream of Ontologies?
arxiv_id: '2401.14931'
source_url: https://arxiv.org/abs/2401.14931
tags:
- ontology
- llms
- icd-10
- accuracy
- pythia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) have
  memorized concept ID-label associations from publicly available ontologies. It evaluates
  multiple models including GPT-4, GPT-3.5, GEMINI-1.5-Flash, and Pythia-12B on ontologies
  like Gene Ontology, Uberon, ICD-10, and Wikidata.
---

# Do LLMs Dream of Ontologies?

## Quick Facts
- arXiv ID: 2401.14931
- Source URL: https://arxiv.org/abs/2401.14931
- Authors: Marco Bombieri; Paolo Fiorini; Simone Paolo Ponzetto; Marco Rospocher
- Reference count: 40
- Key outcome: Only a small fraction of ontological concepts is accurately memorized by LLMs, with performance correlating with web frequency rather than direct ontology exposure.

## Executive Summary
This paper investigates whether large language models have memorized concept ID-label associations from publicly available ontologies. The study evaluates multiple models including GPT-4, GPT-3.5, GEMINI-1.5-Flash, and Pythia-12B on ontologies like Gene Ontology, Uberon, ICD-10, and Wikidata. Results show that memorization is limited and strongly correlated with concept frequency on the web rather than direct exposure to structured ontological resources. The paper introduces new metrics to quantify prediction invariance, demonstrating that stability across prompt variations can serve as a proxy for memorization robustness.

## Method Summary
The study conducts zero-shot evaluation of four LLMs on four ontologies (Gene Ontology, Uberon, ICD-10, Wikidata) containing 42,854, 15,543, 11,494, and 30,000 concepts respectively. Using carefully crafted prompts with temperature=0.0, the researchers extract predicted IDs from model responses and compare them against gold standard IDs. They calculate accuracy, Levenshtein distance, Jaccard similarity, and hallucination rates. Web popularity is estimated using Google Search API queries, and Spearman correlation analysis examines relationships between frequency and accuracy. The study also introduces invariance metrics to assess prediction robustness across prompt variations.

## Key Results
- GPT-4 achieved the highest memorization performance among tested models, but still only accurately retrieved a small fraction of ontological concepts
- A strong correlation exists between concept frequency online and the likelihood of accurate ID retrieval, suggesting indirect acquisition through web exposure
- Lower-performing models (Pythia-12B, Gemini-1.5-Flash) showed bias toward frequently occurring concepts when making errors
- Prediction invariance serves as a reliable proxy for estimating memorization robustness across prompt variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization is strongly correlated with concept frequency on the web.
- Mechanism: When an (ID, label) pair appears many times in web documents, it is likely seen repeatedly in LLM training data, increasing the probability of accurate retrieval during zero-shot prompting.
- Core assumption: LLM training data is heavily drawn from the web, and web frequency reflects training exposure.
- Evidence anchors:
  - [abstract]: "Our results indicate a strong correlation between the frequency of a concept's occurrence online and the likelihood of accurately retrieving its ID from the label."
  - [section 5.3]: "we observe that accuracy substantially improves as we move from low-frequency concepts to high-frequency ones."
- Break condition: If LLM training data is not predominantly web-based, or if frequency in web documents does not correlate with exposure in training data.

### Mechanism 2
- Claim: Prediction invariance is a proxy for memorization robustness.
- Mechanism: When an LLM is prompted multiple times with the same (ID, label) pair, small variations in prompt language or temperature should not change the output if the association is well-memorized. High invariance implies robustness.
- Core assumption: Memorization leads to consistent predictions regardless of minor input variations.
- Evidence anchors:
  - [abstract]: "stability of model responses across variations in prompt language and temperature settings can serve as a proxy for estimating memorization robustness."
  - [section 6.3]: "we observe that accuracy substantially improves as we move from low-frequency concepts to high-frequency ones."
- Break condition: If LLM outputs vary unpredictably even for memorized pairs, or if invariance is driven by other factors (e.g., deterministic decoding).

### Mechanism 3
- Claim: Lower-performing LLMs exhibit more bias toward frequently occurring concepts.
- Mechanism: When making prediction errors, weaker models tend to repeatedly output IDs of popular concepts instead of the correct one, suggesting reliance on familiar patterns.
- Core assumption: Models with less capacity or exposure substitute correct answers with the most common ones they have seen.
- Evidence anchors:
  - [section 5.3]: "the lower-performing (PYTHIA-12B and GEMINI-1.5F) models seem to repeatedly predict the IDs-label that they have likely seen the most in the training data."
- Break condition: If bias is not observed in other low-performing models, or if high-performing models also show similar bias.

## Foundational Learning

- Concept: Spearman's rank correlation
  - Why needed here: To quantify monotonic relationships between concept popularity and prediction accuracy without assuming linearity.
  - Quick check question: If concept frequency increases and accuracy increases but not in a straight line, does Spearman still detect a relationship?

- Concept: Granger causality
  - Why needed here: To assess whether concept frequency precedes and helps predict accuracy, implying possible causal influence.
  - Quick check question: If we observe that higher frequency always comes before higher accuracy in the data ordering, can we claim Granger causality?

- Concept: Levenshtein distance and Jaccard similarity
  - Why needed here: To measure syntactic closeness between predicted and gold IDs or labels, helping explain error patterns.
  - Quick check question: If two IDs differ by only one character, is their Levenshtein distance 1?

## Architecture Onboarding

- Component map: Prompt generator → LLM → Output parser → Accuracy scorer → Correlation analyzer → Visualization module
- Critical path: Generate prompts for (ID, label) pairs → Send to LLM → Extract ID from output → Compare to gold ID → Compute accuracy → Aggregate by frequency bucket
- Design tradeoffs:
  - Using exact match for ID retrieval vs. allowing fuzzy matches (more permissive but less precise)
  - Bucket granularity for frequency vs. statistical power per bucket
  - Cost of repeated queries for invariance vs. confidence in robustness estimate
- Failure signatures:
  - Low accuracy across all buckets suggests poor memorization or prompt issues
  - Invariance close to 1 for low-frequency concepts may indicate overfitting or prompt bias
  - Strong correlation but low absolute accuracy may indicate systematic error patterns
- First 3 experiments:
  1. Run prompts for a small sample of high and low frequency concepts, manually verify output extraction accuracy
  2. Vary temperature systematically on a fixed set of concepts, measure output variation
  3. Translate prompts into two languages, compare prediction invariance and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does model architecture (e.g., transformer-based vs. other) influence ontology memorization patterns?
- Basis in paper: [inferred] The paper compares different LLMs but doesn't analyze architectural differences
- Why unresolved: The study focused on comparing existing models rather than isolating architectural effects
- What evidence would resolve it: Controlled experiments comparing models with similar parameter counts but different architectures on ontology memorization tasks

### Open Question 2
- Question: How do different training dataset compositions affect ontology memorization beyond simple web frequency?
- Basis in paper: [explicit] The paper notes that models were trained on various datasets but doesn't explore composition effects
- Why unresolved: The study used a correlation approach rather than controlled experimentation with different dataset compositions
- What evidence would resolve it: Training identical model architectures on curated datasets with varying proportions of ontology-specific vs. general web content

### Open Question 3
- Question: Can targeted fine-tuning improve ontology memorization while maintaining general language capabilities?
- Basis in paper: [inferred] The study used zero-shot prompting but didn't explore adaptation methods
- Why unresolved: The research focused on pre-trained models without investigating adaptation strategies
- What evidence would resolve it: Experiments comparing zero-shot performance against models fine-tuned on ontology-specific data while measuring performance degradation on general tasks

## Limitations
- The study cannot definitively distinguish between memorization from structured ontology sources versus indirect acquisition through web text
- Only four specific LLMs were evaluated, potentially limiting generalizability to other models or sizes
- The assumption that web frequency directly reflects training data exposure remains unproven

## Confidence
- **High Confidence**: The correlation between web frequency and accuracy is well-supported by experimental results across multiple ontologies and models
- **Medium Confidence**: The claim that lower-performing models show stronger bias toward frequent concepts is supported but based on limited model comparisons
- **Low Confidence**: The mechanism by which models acquire ontological knowledge is inferred rather than directly measured

## Next Checks
1. **Training Data Analysis**: Obtain and analyze a sample of the actual training data to verify whether (ID, label) pairs from ontologies appear with frequencies matching web estimates, and whether these pairs are concentrated in specific data sources.
2. **Cross-Lingual Validation**: Repeat the experiments with prompts translated into multiple languages to test whether the frequency-accuracy correlation holds across linguistic contexts, which would strengthen the web-exposure hypothesis.
3. **Temporal Analysis**: Test whether newer ontologies (with lower web presence) show systematically lower accuracy than older, more established ones, controlling for concept complexity and domain popularity.