---
ver: rpa2
title: 'The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation,
  Reconstruction and Radiance Field Methods'
arxiv_id: '2411.10546'
source_url: https://arxiv.org/abs/2411.10546
tags: []
core_contribution: The Oxford Spires Dataset provides large-scale outdoor LiDAR-visual
  data with millimetre-accurate ground truth from TLS, enabling benchmarking of SLAM,
  3D reconstruction, and radiance field methods. It includes 24 sequences across six
  Oxford landmarks, captured with three 1.6 MP fisheye cameras, 64-beam LiDAR, and
  IMU, synchronised and calibrated.
---

# The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods

## Quick Facts
- arXiv ID: 2411.10546
- Source URL: https://arxiv.org/abs/2411.10546
- Reference count: 12
- Key outcome: Large-scale outdoor LiDAR-visual dataset with TLS-derived ground truth enables benchmarking of SLAM, 3D reconstruction, and radiance field methods

## Executive Summary
The Oxford Spires Dataset provides the first large-scale outdoor benchmark for LiDAR-visual SLAM, 3D reconstruction, and radiance field methods, featuring 24 sequences across six Oxford landmarks captured with three 1.6 MP fisheye cameras, 64-beam LiDAR, and IMU. Ground truth trajectories are derived from TLS scanning and ICP registration, enabling millimetre-accurate evaluation of state-of-the-art methods including VILENS-SLAM, Fast-LIO-SLAM, SC-LIO-SAM, OpenMVS, NeRF, and Gaussian Splatting. Results demonstrate LiDAR-based SLAM achieves sub-metre localisation accuracy, MVS reconstruction outperforms radiance fields (F-score up to 0.918), and radiance field methods struggle with generalisation to out-of-sequence poses.

## Method Summary
The dataset was captured using a Frontier device with three 1.6 MP fisheye cameras (126°×92.4° FOV), 64-beam LiDAR, and IMU, synchronised via FPGA. Ground truth trajectories were generated by registering handheld LiDAR scans against TLS-derived 3D maps using ICP. The benchmark evaluates SLAM systems (VILENS-SLAM, Fast-LIO-SLAM, SC-LIO-SAM) for localisation, 3D reconstruction methods (OpenMVS, NeRF, Gaussian Splatting) for geometry recovery, and novel-view synthesis methods (Nerfacto, Splatfacto) for rendering quality. Evaluation metrics include ATE/RPE for trajectories, F-score for reconstruction, and PSNR/SSIM/LPIPS for view synthesis.

## Key Results
- LiDAR-based SLAM achieves superior localisation accuracy (sub-metre) compared to vision-only methods
- MVS reconstruction outperforms radiance field methods with F-score up to 0.918
- Radiance field methods overfit to training poses and fail to generalise to out-of-sequence viewpoints

## Why This Works (Mechanism)

### Mechanism 1
TLS-derived ground truth enables accurate benchmarking of reconstruction and localisation methods through ICP registration between TLS and mobile LiDAR scans achieving centimetre-level accuracy.

### Mechanism 2
Radiance field methods overfit to training poses due to limited viewing angle diversity during training, leading to poor performance when rendering from novel viewpoints outside the training distribution.

### Mechanism 3
Multi-camera fisheye setup enables effective radiance field reconstruction by providing sufficient angular coverage and overlap for robust pose estimation in dynamic outdoor environments.

## Foundational Learning

- Concept: LiDAR-Visual sensor fusion principles
  - Why needed here: Understanding how to combine LiDAR depth accuracy with visual texture information for robust 3D reconstruction and localisation
  - Quick check question: What are the complementary strengths of LiDAR and camera sensors in outdoor robotics applications?

- Concept: Neural Radiance Field fundamentals
  - Why needed here: Grasping how NeRF/GSplat represent scenes as continuous functions for novel view synthesis
  - Quick check question: How do radiance field methods differ from traditional MVS approaches in handling view-dependent appearance?

- Concept: Structure-from-Motion pipeline
  - Why needed here: Understanding the prerequisite pose estimation and sparse reconstruction required for radiance field and MVS methods
  - Quick check question: What are the key steps in the COLMAP pipeline that generate input for radiance field and MVS methods?

## Architecture Onboarding

- Component map: Frontier device (3× fisheye cameras, LiDAR, IMU) -> TLS scanning -> ICP registration -> ground truth trajectory -> SLAM evaluation -> COLMAP -> MVS/NeRF evaluation

- Critical path: TLS → ICP registration → ground truth trajectory → SLAM evaluation → COLMAP → MVS/NeRF evaluation

- Design tradeoffs: High-resolution colour vs. processing load, fisheye distortion vs. field coverage, TLS accuracy vs. capture time

- Failure signatures: ICP convergence failures indicate poor TLS-mobile overlap; SfM failures indicate insufficient visual features; radiance field artefacts indicate overfitting

- First 3 experiments:
  1. Validate ICP registration accuracy on a small TLS-mobile overlap region
  2. Run COLMAP on a single sequence to verify pose estimation quality
  3. Train a simple NeRF model on one sequence to observe overfitting behaviour

## Open Questions the Paper Calls Out

- How do radiance field methods perform in large-scale outdoor environments with significant viewpoint variation, and what architectural modifications are needed to improve their generalization capability?
- What is the optimal balance between LiDAR-based and vision-based reconstruction methods for outdoor environments, considering factors such as texture, lighting conditions, and sensor range limitations?
- How can radiance field methods be adapted to handle the challenges of outdoor environments, such as dynamic lighting, sky reconstruction, and textureless surfaces?
- What are the limitations of using TLS-derived ground truth for evaluating large-scale SLAM and reconstruction methods, and how can these limitations be mitigated?

## Limitations

- Radiance field methods' poor generalisation to out-of-sequence poses limits their practical utility for robotics applications
- TLS-derived ground truth may have coverage gaps or registration errors that affect evaluation accuracy
- Fisheye camera effectiveness for radiance field reconstruction in robotics contexts is assumed rather than empirically validated

## Confidence

- High Confidence: LiDAR-based SLAM achieving sub-metre localisation accuracy with quantitative metrics
- Medium Confidence: Radiance field overfitting to training poses based on experimental results
- Low Confidence: Fisheye camera effectiveness for radiance field reconstruction in robotics applications

## Next Checks

1. Cross-validate ICP registration accuracy by comparing against independent ground truth (RTK-GPS) for subset of sequences
2. Train radiance field models with pose augmentation to assess generalisation improvements
3. Quantify visual feature matching quality across varying lighting conditions to validate fisheye setup robustness