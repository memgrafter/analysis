---
ver: rpa2
title: Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models
arxiv_id: '2402.18863'
source_url: https://arxiv.org/abs/2402.18863
tags:
- astuteness
- lipschitz
- robustness
- rank
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of comparing the robustness of
  different explainability models for neural networks, focusing on probabilistic Lipschitzness
  and its relation to the stable rank of neural networks. The authors prove theoretical
  lower bounds on the probabilistic Lipschitzness of three common explainability models:
  Integrated Gradients, LIME, and SmoothGrad.'
---

# Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models

## Quick Facts
- arXiv ID: 2402.18863
- Source URL: https://arxiv.org/abs/2402.18863
- Authors: Lachlan Simpson; Kyle Millar; Adriel Cheng; Cheng-Chew Lim; Hong Gunn Chew
- Reference count: 37
- Primary result: Introduces normalized astuteness metric for comparing robustness of explainability models using probabilistic Lipschitzness and stable rank

## Executive Summary
This paper addresses the challenge of comparing robustness across different explainability models for neural networks. The authors introduce a novel metric called normalized astuteness that quantifies the probability that similar explanations are generated for similar inputs, without requiring an ad-hoc choice of Lipschitz constant. They establish theoretical connections between the local Lipschitz constant of neural networks and their stable rank, demonstrating that stable rank can serve as a computational heuristic for assessing explanation model robustness. The proposed metrics are validated on three well-known datasets (XOR, Iris, and MNIST) and compared against existing approaches.

## Method Summary
The method involves training MLPs with varying depths and activation functions on benchmark datasets, then computing explanations using Integrated Gradients, LIME, and SmoothGrad. For each explanation model, the authors evaluate robustness through normalized astuteness - a metric derived from probabilistic Lipschitzness bounds that measures the probability of similar explanations for close points. The stable rank of weight matrices is computed as a computationally efficient heuristic for the neural network's Lipschitz constant. The approach normalizes astuteness curves across different Lipschitz constants and uses the area under the curve as a single quantitative measure of robustness.

## Key Results
- Normalized astuteness provides a single quantifiable measure of explainability model robustness without dependency on ad hoc Lipschitz constant selection
- The stable rank of weight matrices serves as a computational heuristic for the Lipschitz constant, enabling efficient robustness assessment
- Theoretical lower bounds on probabilistic Lipschitzness are proven for Integrated Gradients, LIME, and SmoothGrad, establishing the relationship between network smoothness and explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Probabilistic Lipschitzness of a neural network determines the local robustness of post hoc explanations.
- **Mechanism:** If a neural network is probabilistically Lipschitz with constant L, then for any two points x, y within distance r, the probability that the difference in explanations is bounded by λ‖x-y‖ is at least 1-α, where λ is derived from L and network smoothness.
- **Core assumption:** The neural network is sufficiently smooth (C∞) and points are sampled from a bounded open set U.
- **Evidence anchors:**
  - [abstract] "Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations."
  - [section] "Theorem 4.2... Suppose f is probabilistic L-Lipschitz with probability ≥ 1 − α. Take two points x, y ∼ D such that dp(x, y) ≤ r... Then for integrated gradients we have Ar,λ(IG) ≥ 1 − α"
  - [corpus] Weak evidence; no corpus paper directly addresses probabilistic Lipschitzness in explainability models.
- **Break condition:** If the neural network is not smooth or points are not sampled from a bounded open set, the theoretical bounds may not hold.

### Mechanism 2
- **Claim:** The stable rank of a neural network's weight matrices serves as a heuristic for the robustness of explainability models.
- **Mechanism:** A higher stable rank correlates with a higher Lipschitz constant, which in turn leads to less robust explanations. Conversely, a lower stable rank indicates a lower Lipschitz constant and more robust explanations.
- **Core assumption:** The relationship between stable rank and Lipschitz constant observed in signal reconstruction extends to explainability models.
- **Evidence anchors:**
  - [abstract] "We prove a link between the local Lipschitz constant of a neural network and its stable rank. We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models."
  - [section] "We now make the following claim: Large stable rank results in low astuteness and consequentially less robust explainers."
  - [corpus] Weak evidence; no corpus paper directly addresses the stable rank as a heuristic for explainability robustness.
- **Break condition:** If the empirical relationship between stable rank and Lipschitz constant does not hold for the specific neural network architecture or dataset, the heuristic may fail.

### Mechanism 3
- **Claim:** Normalised astuteness provides a single quantifiable measure of explainability model robustness without dependency on an ad hoc choice of Lipschitz constant.
- **Mechanism:** By normalizing the astuteness curves across different Lipschitz constants and taking the area under the curve, a single metric is obtained that reflects the probability of similar explanations for close points.
- **Core assumption:** The astuteness curve converges to 1 for sufficiently large Lipschitz constants, and the area under the normalized curve is a meaningful measure of robustness.
- **Evidence anchors:**
  - [abstract] "We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models."
  - [section] "Normalised astuteness, a novel extension of astuteness which meets all criteria for a robustness metric... We consider a model robust if astuteness converges quickly for small Lipschitz constant."
  - [corpus] Weak evidence; no corpus paper directly addresses normalised astuteness as a robustness metric.
- **Break condition:** If the astuteness curve does not converge to 1 for large Lipschitz constants, or if the area under the normalized curve does not correlate with robustness, the metric may be invalid.

## Foundational Learning

- **Concept:** Probabilistic Lipschitzness
  - **Why needed here:** It quantifies the local smoothness of a neural network, which is fundamental to the robustness of post hoc explanations.
  - **Quick check question:** What is the probability threshold α in the definition of probabilistic Lipschitzness, and how does it relate to the robustness of explanations?

- **Concept:** Stable Rank
  - **Why needed here:** It provides a computationally efficient heuristic for the Lipschitz constant of a neural network, which is otherwise hard to compute.
  - **Quick check question:** How is the stable rank calculated, and what is its relationship to the rank of a matrix?

- **Concept:** Astuteness
  - **Why needed here:** It extends probabilistic Lipschitzness to explainability models, providing a probability measure of local robustness.
  - **Quick check question:** How does astuteness differ from traditional measures of robustness like local Lipschitz estimate or average sensitivity?

## Architecture Onboarding

- **Component map:** Neural network (f) -> Explanation model (ψ) -> Distance metric (dp) -> Probability threshold (α) -> Lipschitz constant (L)

- **Critical path:**
  1. Train neural network f on dataset D.
  2. For each explanation model ψ, compute astuteness Ar,λ(ψ, D) for varying λ.
  3. Normalize astuteness curves and compute AUC to obtain normalised astuteness.
  4. Compute stable rank of weight matrices to obtain heuristic for robustness.

- **Design tradeoffs:**
  - Computational cost: Computing stable rank is polynomial time, but calculating exact Lipschitz constant is NP-hard.
  - Interpretability: Normalised astuteness provides a single metric, but may obscure point-wise variations in robustness.
  - Generality: The theoretical bounds assume smooth neural networks and bounded open sets, which may not always hold.

- **Failure signatures:**
  - Low normalised astuteness but high stable rank: The heuristic may not hold for this specific architecture.
  - High astuteness but poor explanation quality: The metric may not capture all aspects of explanation quality.
  - Unstable astuteness curves: The convergence assumption may be violated, invalidating the normalization.

- **First 3 experiments:**
  1. Compute normalised astuteness for Integrated Gradients, LIME, and SmoothGrad on XOR dataset and compare with theoretical bounds.
  2. Compute stable rank of weight matrices for 2, 4, 8, and 16-layer MLPs on Iris dataset and correlate with explanation robustness.
  3. Train autoencoders with Gaussian activation on MNIST and measure the relationship between PSNR, stable rank, and explanation astuteness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stable rank be directly applied as a metric for evaluating the robustness of explainability models, beyond its current use as a heuristic?
- Basis in paper: [explicit] The paper suggests that stable rank can serve as a heuristic for the robustness of explainability models and proposes further investigation into its direct application.
- Why unresolved: The current work demonstrates the stable rank's potential as a heuristic but does not fully explore its application as a direct metric for explainability model robustness.
- What evidence would resolve it: Experimental results comparing the stable rank directly against other robustness metrics for explainability models, showing its effectiveness and interpretability.

### Open Question 2
- Question: How does the choice of base-point impact the robustness of Integrated Gradients explanations, and what strategies can mitigate any negative effects?
- Basis in paper: [explicit] The paper mentions that the choice of base-point significantly impacts the similarity of explanations generated for close-by points in Integrated Gradients, but does not explore this further.
- Why unresolved: The paper acknowledges the issue but does not provide a detailed analysis or potential solutions for mitigating the impact of base-point choice on Integrated Gradients' robustness.
- What evidence would resolve it: A study comparing Integrated Gradients' robustness across different base-point selection strategies, identifying optimal choices or methods to minimize sensitivity to base-point selection.

### Open Question 3
- Question: How can normalized astuteness be extended to evaluate the robustness of explainability models for graph neural networks?
- Basis in paper: [explicit] The paper concludes by suggesting future work to apply normalized astuteness to graph neural networks and extend theoretical results to graph explainability models.
- Why unresolved: The paper does not explore the application of normalized astuteness beyond traditional neural networks, leaving a gap in understanding its effectiveness for graph-based models.
- What evidence would resolve it: Experimental validation of normalized astuteness on graph neural networks, demonstrating its ability to assess robustness in this context and comparing it to existing metrics for graph explainability.

## Limitations

- The theoretical bounds require smooth neural networks, which excludes common ReLU activations and limits practical applicability
- The approach doesn't address the impact of base-point selection in Integrated Gradients, which significantly affects explanation similarity
- The method is primarily validated on small-scale datasets (XOR, Iris, MNIST) and may not generalize to complex real-world problems

## Confidence

- Theoretical foundations (Mechanism 1): Medium - mathematically sound but relies on strong smoothness assumptions
- Stable rank heuristic (Mechanism 2): Medium - shows empirical correlation but lacks rigorous theoretical justification
- Normalized astuteness metric (Mechanism 3): Medium - provides practical measure but may oversimplify explanation quality

## Next Checks

1. **Cross-dataset validation**: Test the normalized astuteness metric on additional datasets (e.g., CIFAR-10, Fashion-MNIST) to verify its generalizability beyond the three studied datasets.

2. **Architecture sensitivity**: Evaluate the stable rank heuristic across diverse neural network architectures (CNNs, ResNets, Transformers) to determine if the correlation with explanation robustness holds for non-MLP models.

3. **Activation function robustness**: Assess how the probabilistic Lipschitzness bounds and normalized astuteness metric perform with different activation functions (ReLU, Leaky ReLU, GELU) to understand the impact of non-smooth activations.