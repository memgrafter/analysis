---
ver: rpa2
title: Intermediate direct preference optimization
arxiv_id: '2408.02923'
source_url: https://arxiv.org/abs/2408.02923
tags:
- intermediate
- loss
- layers
- layer
- calculating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Intermediate Direct Preference Optimization
  (DPO), a method that calculates DPO loss at selected intermediate layers of large
  language models (LLMs) as an auxiliary loss during fine-tuning. Instead of using
  only the final layer's logits as in conventional DPO, the proposed method computes
  DPO losses from K selected intermediate layers and averages them to obtain an intermediate
  DPO loss.
---

# Intermediate direct preference optimization

## Quick Facts
- arXiv ID: 2408.02923
- Source URL: https://arxiv.org/abs/2408.02923
- Authors: Atsushi Kojima
- Reference count: 7
- Key outcome: Intermediate DPO at layer 22 achieved 52.5% win rate vs conventional DPO and 67.5% vs SFT models

## Executive Summary
This paper proposes Intermediate Direct Preference Optimization (DPO), a method that calculates DPO loss at selected intermediate layers of large language models as an auxiliary loss during fine-tuning. Instead of using only the final layer's logits as in conventional DPO, the proposed method computes DPO losses from K selected intermediate layers and averages them to obtain an intermediate DPO loss. The final training loss is a weighted sum of the conventional DPO loss and this intermediate DPO loss.

Experiments on the ultrafeedback dataset using a 32-layer, 7B parameter SFT model show that the intermediate DPO model trained with losses calculated at the 22nd layer achieved win rates of 52.5% against the conventional DPO model and 67.5% against the SFT model when evaluated by GPT-4. The results demonstrate that calculating intermediate DPO loss closer to the output layer and selecting multiple dispersed intermediate layers improves performance.

## Method Summary
Intermediate DPO modifies conventional DPO by computing preference losses at K selected intermediate transformer layers rather than only at the final output layer. The method extracts hidden states from chosen intermediate layers, applies shared linear projections to convert them to logits, computes DPO losses at each layer, and averages these to create an intermediate DPO loss. The final training loss is a weighted sum (γ=0.9) of this intermediate loss and the conventional final-layer DPO loss. The approach uses LoRA adaptation for parameter-efficient fine-tuning on the ultrafeedback dataset.

## Key Results
- Intermediate DPO at layer 22 achieved 52.5% win rate vs conventional DPO model
- Intermediate DPO at layers 11 and 22 achieved 67.5% win rate vs SFT model
- Layer 22 (closer to output) outperformed layer 16 (middle layers) with 60% vs 50% win rates
- Dispersed layer selection (11, 22) outperformed consecutive selection (30, 31) with 67.5% vs 65% win rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate DPO loss provides auxiliary gradient signals that guide representation alignment earlier in the model
- Mechanism: By computing DPO loss at intermediate layers (K), the model receives feedback during forward passes at multiple depth levels rather than only at the final layer. This creates additional gradient paths that influence earlier transformer layers, helping them learn preference-aligned representations before the final output layer.
- Core assumption: The intermediate layer representations contain sufficient information to meaningfully compute preference loss, and gradients from these intermediate losses can propagate effectively to earlier layers.
- Evidence anchors:
  - [abstract] "DPO losses are calculated using the logits from K-selected intermediate layers and averaged to obtain the intermediate DPO loss"
  - [section 3] "logits are calculated from the hidden vectors of intermediate layers and then the DPO loss is computed"
  - [corpus] Weak evidence - no direct corpus support found for intermediate layer gradient propagation effects
- Break condition: If intermediate layers lack sufficient discriminative information for preference distinction, or if gradient vanishing/exploding prevents effective backward propagation from intermediate layers to earlier transformer blocks.

### Mechanism 2
- Claim: Calculating DPO loss closer to the output layer improves performance more than calculating it in middle layers
- Mechanism: The paper demonstrates that intermediate DPO loss calculated at the 22nd layer (closer to output) achieves higher win rates (60%) than at the 16th layer (50%). This suggests that intermediate representations nearer to the final output contain more refined, preference-aligned information that better captures the model's ultimate decision-making context.
- Core assumption: Representations closer to the output layer are more specialized for the final task and thus provide better signals for preference learning.
- Evidence anchors:
  - [section 4.2] "The performance of the model in calculating the DPO loss at layers closer to the output (id5) was observed to be higher than those in calculating the DPO loss in the middle (id9) or closer to the input layers (id4)"
  - [section 4.2] "id5 22 60 15 25" vs "id9 16 50 27.5 22.5" vs "id4 11 52.5 27.5 20"
  - [corpus] Weak evidence - no direct corpus support found for layer proximity effects on preference learning
- Break condition: If the intermediate layers are too close to the output, they may not provide enough "room" for gradient-based improvements to propagate backward through the network effectively.

### Mechanism 3
- Claim: Selecting multiple dispersed intermediate layers provides better performance than selecting consecutive layers
- Mechanism: The paper shows that selecting dispersed layers (11, 22) achieves higher win rates (67.5%) than selecting consecutive layers closer to output (30, 31 at 65%). This suggests that diversity in intermediate layer selection captures different aspects of the representation space, preventing the model from overfitting to a narrow range of intermediate representations.
- Core assumption: Different intermediate layers capture complementary aspects of the representation space, and combining these diverse signals improves overall preference alignment.
- Evidence anchors:
  - [section 4.2] "selecting from spread out layers (id3) performed better than selecting continuously from layers closer to the output (id7)"
  - [section 4.2] "id3 11, 22 67.5 22.5 10" vs "id7 30, 31 65 22.5 12.5"
  - [corpus] Weak evidence - no direct corpus support found for dispersed layer selection benefits
- Break condition: If the intermediate layers are too dispersed, they may capture redundant or conflicting information that degrades rather than improves performance.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Intermediate DPO builds directly on DPO methodology, modifying where and how the preference loss is computed. Understanding DPO's mechanism, loss function, and training dynamics is essential for grasping how intermediate DPO extends it.
  - Quick check question: What is the mathematical form of the DPO loss function, and how does it differ from traditional RLHF approaches?

- Concept: Transformer architecture and layer interactions
  - Why needed here: The paper manipulates intermediate transformer layers to compute auxiliary losses. Understanding how information flows through transformer layers, how representations evolve, and how gradients propagate is crucial for understanding why intermediate losses might help.
  - Quick check question: How do representations typically change as information flows from lower to higher transformer layers in a 32-layer model?

- Concept: LoRA (Low-Rank Adaptation) and parameter-efficient fine-tuning
  - Why needed here: The experiments use LoRA to fine-tune the 7B parameter SFT model. Understanding how LoRA modifies specific weight matrices and how this affects gradient flow is important for understanding the practical implementation.
  - Quick check question: In LoRA, which weight matrices are typically adapted, and how does this affect the computation of intermediate logits?

## Architecture Onboarding

- Component map:
  Input embeddings -> Transformer layers (1-32) -> Linear projection layers at K intermediate positions -> Softmax -> Intermediate DPO loss computation -> Final layer -> Linear projection -> Softmax -> Final DPO loss -> Weighted sum loss
  Key components: Shared linear projection parameters across intermediate layers, temperature sampling with repetition penalty for inference

- Critical path:
  1. Forward pass through all transformer layers
  2. Extract hidden states from K intermediate layers (e.g., layers 11 and 22)
  3. Apply shared linear projections to convert hidden states to logits
  4. Compute intermediate DPO losses and average them
  5. Compute final DPO loss from output layer logits
  6. Calculate weighted sum of both losses for backpropagation

- Design tradeoffs:
  - Computational cost vs. performance gain: Calculating losses at multiple layers increases computation but improves performance
  - Layer selection strategy: Dispersed layers vs. consecutive layers vs. single layer
  - Hyperparameter γ balancing: How much weight to give intermediate vs. final DPO loss
  - Model depth considerations: Whether the 22nd layer is optimal for different model sizes

- Failure signatures:
  - Degraded performance if intermediate layers are too early (loss of discriminative information)
  - Training instability if γ is poorly tuned
  - Overfitting if too many intermediate layers are selected
  - Gradient vanishing if intermediate layers are too close to output

- First 3 experiments:
  1. Verify that intermediate DPO loss can be computed correctly at a single intermediate layer (e.g., layer 16) and that gradients flow properly
  2. Test different values of γ (e.g., 0.7, 0.9, 0.95) to find the optimal balance between final and intermediate losses
  3. Compare performance of single intermediate layer (layer 22) vs. dispersed pair (layers 11 and 22) to validate the dispersed selection benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Intermediate DPO scale with different model sizes beyond the 7B parameter model tested?
- Basis in paper: [explicit] The authors state "In our future work, we will investigate the effectiveness of the proposed method across different model sizes."
- Why unresolved: The current experiments only tested on a 7B parameter model, leaving the generalizability to larger or smaller models unknown.
- What evidence would resolve it: Conducting experiments with a range of model sizes (e.g., 1B, 13B, 70B parameters) and comparing win rates against conventional DPO and SFT models for each size.

### Open Question 2
- Question: What is the optimal number of intermediate layers to use for calculating the intermediate DPO loss, and how does this vary with model architecture?
- Basis in paper: [inferred] The paper explores using different numbers and positions of layers (K=11, 16, 22) but does not systematically investigate the optimal number or position for various architectures.
- Why unresolved: The experiments tested specific layer combinations but did not explore a wide range of possibilities or different model architectures to determine general principles.
- What evidence would resolve it: Systematic experiments varying the number and positions of layers across different model architectures, analyzing the relationship between layer selection strategy and performance metrics.

### Open Question 3
- Question: How does the intermediate DPO method perform on datasets other than ultrafeedback, particularly in domains with different characteristics?
- Basis in paper: [explicit] All experiments were conducted using the ultrafeedback dataset, with no mention of testing on other datasets.
- Why unresolved: The effectiveness of the method on diverse datasets with different characteristics (e.g., domain specificity, prompt complexity, reward signal distribution) remains untested.
- What evidence would resolve it: Evaluating intermediate DPO on multiple datasets across various domains and comparing performance to conventional DPO and SFT baselines to assess generalizability.

## Limitations
- Results are based on a single dataset (ultrafeedback) and model architecture (32-layer, 7B parameter SFT), limiting generalizability
- No computational overhead analysis comparing training time and memory usage between conventional and intermediate DPO
- No ablation studies isolating the individual contributions of layer proximity versus layer diversity effects
- Evaluation methodology with GPT-4 lacks detail on how win rates are calculated

## Confidence
- **High confidence**: The mathematical formulation of intermediate DPO loss and its implementation via shared linear layers is well-defined and reproducible
- **Medium confidence**: The empirical finding that layer 22 performs better than layer 16, as this is demonstrated across multiple experiments with clear numerical results
- **Low confidence**: The claim that selecting dispersed intermediate layers universally improves performance, as this is shown only for one specific pair (11, 22) vs (30, 31) without broader validation

## Next Checks
1. Test intermediate DPO across multiple model scales (1B, 13B, 70B parameters) to verify the 22nd layer remains optimal for different depths
2. Conduct computational efficiency analysis comparing training time and memory usage between conventional DPO and intermediate DPO with various K values
3. Perform ablation studies on layer selection strategies using synthetic preference datasets with known optimal solutions to isolate the effects of proximity versus diversity