---
ver: rpa2
title: 'Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding
  Dimensions'
arxiv_id: '2407.20243'
source_url: https://arxiv.org/abs/2407.20243
tags:
- embedding
- matryoshka-adaptor
- dimensions
- ndcg
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Matryoshka-Adaptor is a novel framework for reducing embedding
  dimensions while preserving performance, applicable in both unsupervised and supervised
  settings. It transforms arbitrary embeddings into Matryoshka embeddings with smaller
  dimensions by optimizing similarity preservation across dimensions.
---

# Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions

## Quick Facts
- arXiv ID: 2407.20243
- Source URL: https://arxiv.org/abs/2407.20243
- Authors: Jinsung Yoon; Raj Sinha; Sercan O Arik; Tomas Pfister
- Reference count: 40
- Primary result: Achieves 2-12× dimensionality reduction without performance loss, with supervised tuning showing 6× reduction while maintaining retrieval performance

## Executive Summary
Matryoshka-Adaptor is a novel framework that transforms arbitrary LLM embeddings into Matryoshka embeddings with smaller dimensions while preserving performance in both unsupervised and supervised settings. The framework optimizes similarity preservation across dimensions using pairwise and top-k similarity losses in unsupervised mode, and adds ranking loss using labeled query-corpus pairs in supervised mode. Extensive experiments across 13 BEIR datasets, 17 MIRACL datasets, and 5 Fashion-200K datasets demonstrate substantial improvements, achieving 2-12× dimensionality reduction without performance loss.

## Method Summary
The Matryoshka-Adaptor framework works by applying a learnable adaptor function to pre-trained LLM embeddings, transforming them into Matryoshka embeddings that maintain performance across multiple dimensions. In unsupervised mode, it uses pairwise similarity loss to preserve local relationships, top-k similarity loss to maintain neighborhood structures, and reconstruction loss as regularization. The supervised mode adds ranking loss using labeled query-corpus relevance pairs. The framework can handle black-box LLM APIs by only requiring access to embeddings, making it widely applicable across different embedding architectures and modalities.

## Key Results
- Achieves 2-12× dimensionality reduction without performance loss in unsupervised mode
- Supervised tuning achieves 6× reduction while maintaining retrieval performance
- Consistent improvements across 13 BEIR, 17 MIRACL, and 5 Fashion-200K datasets
- Framework works with black-box LLM APIs by only requiring embedding access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pairwise similarity loss ensures that local pairwise relationships in the original embedding space are preserved in reduced dimensions
- Mechanism: By computing the difference between cosine similarity in original space and reduced space for all pairs, the adaptor learns to maintain relative distances between embeddings
- Core assumption: The original embedding space contains meaningful pairwise relationships that should be preserved during dimensionality reduction
- Evidence anchors:
  - [section]: "The first loss function, denoted as Lpair, is designed to preserve the pairwise similarity between the original embeddings in their reduced-dimension Matryoshka form"
  - [abstract]: "It transforms arbitrary embeddings into Matryoshka embeddings with smaller dimensions by optimizing similarity preservation across dimensions"
- Break condition: If the original embedding space contains noisy or irrelevant pairwise relationships, preserving them could harm performance

### Mechanism 2
- Claim: Top-k similarity loss preserves local neighborhood structures in the embedding space
- Mechanism: By focusing on the top-k most similar embeddings for each point, the adaptor learns to maintain local topology while ignoring global noise
- Core assumption: Local neighborhood relationships are more important for retrieval tasks than global relationships
- Evidence anchors:
  - [section]: "The second loss function, denoted as Ltopk, focuses on preserving local similarity relationships among neighboring embeddings"
  - [abstract]: "It transforms arbitrary embeddings into Matryoshka embeddings with smaller dimensions by optimizing similarity preservation across dimensions"
- Break condition: If the top-k neighborhood is noisy or unstable, this could introduce harmful artifacts

### Mechanism 3
- Claim: The reconstruction loss prevents the adaptor from moving too far from the original embedding space
- Mechanism: By penalizing the L2 distance between original and adapted embeddings, it acts as a regularizer preventing excessive distortion
- Core assumption: The original embedding contains useful information that shouldn't be discarded entirely
- Evidence anchors:
  - [section]: "Furthermore, a reconstruction loss, denoted as Lrec, is introduced as an additional regularizer"
  - [corpus]: Weak - reconstruction loss is a standard technique but specific evidence for its effectiveness in this context is not provided
- Break condition: If the original embedding is poor quality, preventing movement could limit performance gains

## Foundational Learning

- Concept: Cosine similarity as distance metric
  - Why needed here: The framework relies on cosine similarity for measuring embedding relationships in both original and reduced spaces
  - Quick check question: What happens to cosine similarity when you scale all dimensions of an embedding by the same constant?

- Concept: Matryoshka representation learning
  - Why needed here: Understanding how embeddings can maintain properties across different dimensionalities is crucial for grasping the framework's approach
  - Quick check question: How does Matryoshka embedding differ from standard PCA-based dimensionality reduction?

- Concept: Loss function optimization
  - Why needed here: The framework combines multiple loss functions (pairwise, top-k, reconstruction, ranking) that need to be optimized simultaneously
  - Quick check question: What happens when you have multiple competing loss functions during training?

## Architecture Onboarding

- Component map: Input embeddings -> Learnable adaptor function f -> Transformed embeddings with better Matryoshka properties

- Critical path:
  1. Extract original embeddings from corpus/query
  2. Apply adaptor function f to get transformed embeddings
  3. Compute similarity losses across original and reduced dimensions
  4. Update adaptor parameters via gradient descent

- Design tradeoffs:
  - Complexity vs performance: More sophisticated adaptor architectures could improve results but increase computational cost
  - Loss weighting: Different datasets may require tuning of α, β, γ parameters
  - Dimensionality range: Need to balance between too few dimensions (information loss) and too many (computational cost)

- Failure signatures:
  - Performance degradation at lower dimensions indicates loss functions aren't working properly
  - Overfitting to training corpus suggests need for stronger regularization
  - Poor generalization across datasets indicates need for better adaptor architecture

- First 3 experiments:
  1. Apply adaptor to a small corpus and verify that pairwise distances are preserved across dimensions
  2. Test on a single BEIR dataset to confirm retrieval performance improvement
  3. Compare against PCA baseline to validate effectiveness of the similarity-based approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Matryoshka-Adaptor's unsupervised distance metrics (pairwise and top-k) correlate with retrieval performance across different embedding models and datasets?
- Basis in paper: [explicit] The authors show correlation analysis between distance metrics and nDCG@10 in Figure 7
- Why unresolved: The paper only presents correlation analysis for Google Gecko embeddings on BEIR datasets. It's unclear if these metrics generalize across different embedding architectures, modalities, and languages.
- What evidence would resolve it: Systematic correlation analysis across multiple embedding models (OpenAI, multimodal), languages, and dataset types showing consistent or varying relationships between distance metrics and retrieval performance.

### Open Question 2
- Question: What is the optimal strategy for selecting hyperparameters in the unsupervised Matryoshka-Adaptor when validation data is unavailable?
- Basis in paper: [explicit] "In the unsupervised setting, picking the optimal hyperparameters for Matryoshka-Adaptor poses a challenge due to the absence of validation data."
- Why unresolved: The paper mentions using fixed hyperparameters but acknowledges this as a limitation without providing a principled solution for hyperparameter selection in unsupervised settings.
- What evidence would resolve it: A systematic study comparing different hyperparameter selection strategies (e.g., distance metrics, statistical properties of embeddings) against downstream retrieval performance across multiple datasets.

### Open Question 3
- Question: How does Matryoshka-Adaptor perform when simultaneously tuning embeddings across multiple heterogeneous datasets?
- Basis in paper: [inferred] The paper evaluates Matryoshka-Adaptor on individual datasets but mentions "simultaneous utilization of multiple datasets during tuning" as a potential future research direction.
- Why unresolved: Current evaluation is limited to single-dataset tuning, and the paper acknowledges this as a gap in the current approach.
- What evidence would resolve it: Experiments comparing single-dataset vs multi-dataset tuning strategies across diverse datasets, measuring both performance gains and computational efficiency.

## Limitations
- Framework effectiveness may vary significantly across different domains and embedding qualities, though this generalization potential remains untested beyond the reported datasets
- The specific adaptor architecture details remain underspecified, limiting reproducibility
- Computational overhead introduced by the adaptor training phase and the trade-offs between training time and performance gains need systematic evaluation

## Confidence
- Unsupervised performance improvements: Medium
- Supervised results: High
- Claims about black-box API applicability: Low (theoretically sound but practically unverified)

## Next Checks
1. Ablation studies on the relative importance of each loss component (pairwise, top-k, reconstruction) to understand which mechanisms drive performance
2. Stress tests on low-quality original embeddings to determine the framework's robustness to noisy input representations
3. Cross-domain transferability experiments to validate generalization beyond the BEIR, MIRACL, and Fashion-200K datasets used in the paper