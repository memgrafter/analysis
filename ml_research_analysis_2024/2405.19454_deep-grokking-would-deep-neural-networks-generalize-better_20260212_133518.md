---
ver: rpa2
title: 'Deep Grokking: Would Deep Neural Networks Generalize Better?'
arxiv_id: '2405.19454'
source_url: https://arxiv.org/abs/2405.19454
tags:
- generalization
- grokking
- deep
- layer
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates grokking in deep neural networks, focusing
  on Multi-Layer Perceptrons (MLPs) of varying depths (4, 8, and 12 layers) on the
  MNIST dataset. The authors replicate the grokking phenomenon by applying large initialization
  and small weight decay, as suggested in prior work.
---

# Deep Grokking: Would Deep Neural Networks Generalize Better?

## Quick Facts
- arXiv ID: 2405.19454
- Source URL: https://arxiv.org/abs/2405.19454
- Authors: Simin Fan; Razvan Pascanu; Martin Jaggi
- Reference count: 4
- Primary result: Deeper MLPs are more susceptible to grokking than shallow ones, with multi-stage generalization correlated with double-descent patterns in feature ranks.

## Executive Summary
This paper investigates grokking behavior in deep neural networks, specifically examining Multi-Layer Perceptrons (MLPs) of varying depths on the MNIST dataset. The authors replicate the grokking phenomenon using large initialization and small weight decay, demonstrating that deeper networks are more prone to this delayed generalization behavior. They observe an intriguing multi-stage generalization pattern where test accuracy exhibits two distinct surges instead of a single one, particularly in deeper networks. The study reveals a strong correlation between decreasing feature ranks and the phase transition from overfitting to generalization during grokking, with the multi-stage generalization aligning with a double-descent pattern in feature ranks.

## Method Summary
The study trains 4, 8, and 12-layer MLPs with width 400 on MNIST using ReLU activation. Large initialization scaling (α=8.0) and small weight decay (γ=0.01) are applied to induce grokking. The training uses Adam optimizer with learning rate 1e-3 and MSE loss for 100k steps. For analysis, linear probing heads are attached to each layer to measure linear probing accuracy, and layer-wise numerical rank is computed using singular values of the covariance matrix of layer outputs with a threshold tolerance. The feature rank evolution and generalization behavior are tracked throughout training to understand the relationship between representation compression and generalization.

## Key Results
- Deeper MLPs (8 and 12 layers) show stronger grokking susceptibility compared to 4-layer networks
- Multi-stage generalization behavior observed with two distinct accuracy surges instead of single surge
- Feature rank collapse correlates strongly with generalization onset during grokking
- Multi-stage generalization aligns with a double-descent pattern in feature ranks
- Feature rank serves as a more promising indicator of phase transitions compared to weight-norm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deeper MLPs are more susceptible to grokking than shallow ones.
- Mechanism: Increased depth creates longer "tunnel" layers that compress high-rank features into low-rank representations, delaying generalization until feature rank collapse occurs.
- Core assumption: The tunnel effect from Masarczyk et al. (2023) applies to grokking dynamics.
- Evidence anchors:
  - [abstract] "deeper MLPs are more susceptible to grokking than shallow ones"
  - [section] "deeper networks tend to have longer 'tunnel's" and "the longer tunnel may have a detrimental effect on the learned representation's generalization ability"
  - [corpus] No direct evidence; corpus neighbors don't address tunnel depth
- Break condition: If feature rank doesn't decrease with depth, or if linear probing accuracy improves immediately without rank collapse.

### Mechanism 2
- Claim: Multi-stage generalization correlates with double-descent in feature ranks.
- Mechanism: First generalization surge corresponds to initial rank decrease, second surge to rank increase followed by collapse, creating a double-descent pattern.
- Core assumption: Feature rank changes directly indicate generalization phases.
- Evidence anchors:
  - [abstract] "multi-stage generalization phenomenon often aligns with a double-descent pattern in feature ranks"
  - [section] "the test accuracy evolves in two sharp increments instead of a single surge... mirrored by a double-descent pattern on the feature rank"
  - [corpus] No direct evidence; corpus neighbors don't discuss multi-stage generalization
- Break condition: If feature ranks don't show double-descent pattern during multi-stage generalization.

### Mechanism 3
- Claim: Feature rank is a better indicator of phase transitions than weight-norm.
- Mechanism: Weight-norm changes smoothly during training, while feature rank shows sharp transitions at generalization onset.
- Core assumption: Feature rank captures representational changes better than aggregate parameter norms.
- Evidence anchors:
  - [abstract] "internal feature rank could serve as a more promising indicator of the model's generalization behavior compared to the weight-norm"
  - [section] "the weight-norm's trajectories are largely overlap, which decrease smoothly... without a discernible indication of the phase transitions. In contrast, the feature-ranks exhibit significant differences indicating the phase transitions"
  - [corpus] No direct evidence; corpus neighbors don't compare feature rank vs weight-norm indicators
- Break condition: If weight-norm shows distinct transitions matching generalization phases.

## Foundational Learning

- Concept: Feature rank and numerical rank estimation
  - Why needed here: Core metric for understanding representation compression and generalization
  - Quick check question: How do you compute the numerical rank of a matrix using singular values?
- Concept: Linear probing and its relationship to feature quality
  - Why needed here: Alternative metric to validate that feature rank changes correspond to representational quality
  - Quick check question: What does high linear probing accuracy indicate about internal representations?
- Concept: Initialization scale effects on training dynamics
  - Why needed here: Key hyperparameter for inducing grokking through large initialization
  - Quick check question: How does increasing initialization scale affect early training behavior?

## Architecture Onboarding

- Component map: MLP with configurable depth, ReLU activations, large initialization scale, small weight decay, linear probing heads at each layer, rank computation module
- Critical path: Large initialization → training with weight decay → monitor training/test accuracy → monitor layer-wise feature ranks → detect rank collapse → observe generalization phase transitions
- Design tradeoffs: Deeper networks show more grokking but require more computation; larger initialization increases grokking likelihood but may destabilize training
- Failure signatures: No rank collapse despite training progress; weight-norm decreases without corresponding generalization; multi-stage behavior absent in deep networks
- First 3 experiments:
  1. Replicate 4-layer vs 12-layer MLP grokking comparison on MNIST with same initialization scale
  2. Verify feature rank collapse timing matches generalization onset in deeper networks
  3. Test whether linear probing accuracy correlates with feature rank changes during training

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are primarily based on MNIST experiments with MLPs, may not generalize to other datasets or architectures
- Specific hyperparameters that induce grokking are not systematically explored, raising questions about robustness
- Correlation between feature rank and generalization is observational and doesn't establish causality
- Double-descent pattern in feature ranks during multi-stage generalization needs further validation across different problem domains

## Confidence

**High Confidence**: The observation that deeper MLPs are more susceptible to grokking is well-supported by experimental evidence across multiple depths (4, 8, 12 layers). The correlation between feature rank collapse and generalization onset is consistently observed.

**Medium Confidence**: The interpretation that deeper networks have longer "tunnels" causing detrimental effects on generalization, and the proposed double-descent pattern in feature ranks during multi-stage generalization, are plausible but require additional validation.

**Low Confidence**: The claim that feature rank is a superior indicator compared to weight-norm for detecting phase transitions is based on qualitative observations rather than rigorous statistical comparison across multiple runs.

## Next Checks

1. **Cross-dataset validation**: Replicate the grokking experiments on CIFAR-10 and Fashion-MNIST to verify if deeper networks consistently show increased grokking susceptibility across different image classification tasks.

2. **Statistical significance testing**: Perform multiple independent runs (n≥10) for each MLP depth configuration to establish statistical significance of the correlation between feature rank collapse timing and generalization onset.

3. **Alternative rank computation methods**: Compare numerical rank computed via singular value thresholding against alternative methods (e.g., PCA-based rank estimation, effective rank measures) to verify that observed patterns are robust to rank computation methodology.