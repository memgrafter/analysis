---
ver: rpa2
title: Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding
arxiv_id: '2402.15102'
source_url: https://arxiv.org/abs/2402.15102
tags:
- policy
- exploration
- offline
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of safely and effectively training
  auto-bidding policies in online advertising systems. The authors identify that iterative
  offline reinforcement learning (RL) suffers from ineffective exploration and exploitation
  due to the conservatism of offline RL algorithms.
---

# Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding

## Quick Facts
- arXiv ID: 2402.15102
- Source URL: https://arxiv.org/abs/2402.15102
- Reference count: 40
- One-line primary result: TEE with SEAS and IQL achieved a 7.48% improvement over the base policy in the simulated environment

## Executive Summary
This paper addresses the challenge of safely and effectively training auto-bidding policies in online advertising systems using iterative offline reinforcement learning. The authors identify that traditional iterative offline RL suffers from ineffective exploration and exploitation due to the conservatism of offline RL algorithms. To overcome this, they propose Trajectory-wise Exploration and Exploitation (TEE) which uses Parameter Space Noise (PSN) for trajectory-wise exploration and robust trajectory weighting for trajectory-wise exploitation. They also introduce Safe Exploration by Adaptive Action Selection (SEAS) to ensure safety during online exploration. Experiments on a simulated advertising environment and the Alibaba display advertising platform demonstrate that TEE and SEAS significantly outperform baseline methods, achieving near-expert performance while satisfying safety constraints.

## Method Summary
The Trajectory-wise Iterative Reinforcement Learning Framework addresses the challenge of safe and effective training of auto-bidding policies in online advertising using iterative offline reinforcement learning. The method consists of three key components: Parameter Space Noise (PSN) for generating exploration policies by perturbing policy parameters once per episode to create consistent behavioral patterns, Robust Trajectory Weighting for computing sampling probabilities based on relabeled returns and state-value regularization to emphasize high-quality behaviors, and Safe Exploration by Adaptive Action Selection (SEAS) for ensuring safety by dynamically choosing between exploratory and safe actions based on cumulative rewards and predicted future returns. The framework is evaluated on both a simulated advertising environment with 30 competing advertisers and the real-world Alibaba display advertising platform, using metrics that measure performance improvements while satisfying safety constraints.

## Key Results
- TEE with SEAS and IQL achieved a 7.48% improvement over the base policy in the simulated environment
- The framework demonstrated near-expert performance while satisfying safety constraints on the Alibaba display advertising platform
- Trajectory-wise exploration using PSN generated more dispersed trajectory returns compared to action space noise approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter Space Noise (PSN) generates exploration datasets with more dispersed trajectory returns than Action Space Noise (ASN), leading to better coverage of high-return behaviors.
- Mechanism: PSN perturbs policy parameters once per episode, creating consistent behavioral patterns across time steps, while ASN applies independent noise at each step, causing counterbalancing effects that limit exploration effectiveness.
- Core assumption: The base policy's smoothing of budget consumption creates a coupling between consecutive actions that hinders ASN exploration but benefits PSN's consistent parameter perturbations.
- Evidence anchors:
  - [abstract] "we construct exploration policies by introducing noise into the policy's parameter space instead of the traditional action space"
  - [section] "We demonstrate this phenomenon in Section 4, highlighting the challenges of effective exploration and exploitation for iterative offline RL"
  - [corpus] Weak - no direct mention of PSN vs ASN comparison
- Break condition: If the base policy is already optimal or if the MDP has highly stochastic transitions that overwhelm the benefits of consistent exploration patterns.

### Mechanism 2
- Claim: Robust Trajectory Weighting improves exploitation by assigning higher sampling probabilities to trajectories with better return indicators, overcoming the conservatism of offline RL.
- Mechanism: The method computes trajectory quality indicators by combining relabeled returns (using a learned reward model) and state-value regularization, then applies exponential weighting to create sampling probabilities that emphasize high-quality behaviors.
- Core assumption: Trajectory returns, when properly regularized and relabeled using expected rewards, provide reliable signals of policy quality that can be used to weight training data.
- Evidence anchors:
  - [abstract] "For effective exploitation, we propose robust trajectory weighting to fully exploit high-return trajectories in the collected dataset"
  - [section] "we assign high probability weights to high-return trajectories, thereby enhancing the impact of high-quality behaviours on the training process"
  - [corpus] Weak - no direct mention of trajectory weighting in related work
- Break condition: If the reward model fails to accurately predict expected rewards due to high stochasticity or if trajectories from different campaigns are too heterogeneous for direct comparison.

### Mechanism 3
- Claim: Safe Exploration by Adaptive Action Selection (SEAS) guarantees safety while preserving exploration quality by dynamically choosing between exploratory and safe actions based on cumulative rewards and predicted future returns.
- Mechanism: SEAS maintains multiple safe policies and at each step selects the action that either continues exploration (if cumulative rewards plus predicted Q-value meet safety threshold) or switches to the safe policy with highest Q-value for the exploratory action.
- Core assumption: The safety constraint can be satisfied by ensuring that at each step, the expected return from continuing exploration plus accumulated rewards exceeds a fraction of the safe policy performance.
- Evidence anchors:
  - [abstract] "we propose Safe Exploration by Adaptive Action Selection (SEAS) to ensure safety during online exploration"
  - [section] "SEAS dynamically determines the safe exploration action at each time step based on the cumulative rewards up to that step, as well as the predicted future return"
  - [corpus] Weak - no direct mention of SEAS in related work
- Break condition: If the Q-value estimates for safe policies are significantly underestimated, leading to overly conservative exploration restrictions.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their formulation for sequential decision-making problems.
  - Why needed here: The auto-bidding problem is formulated as an MDP where states represent campaign status, actions are bidding parameters, and rewards are conversion values.
  - Quick check question: What are the components of an MDP tuple (S, A, r, p, γ) and how do they apply to the auto-bidding problem?

- Concept: Offline Reinforcement Learning and the conservatism principle.
  - Why needed here: The framework relies on iterative offline RL where policies are trained from pre-collected data without online interaction, and conservatism ensures policies stay close to the dataset.
  - Quick check question: Why does the conservatism principle in offline RL make the quality of the exploration dataset critical for training performance?

- Concept: Parameter Space Noise and its difference from Action Space Noise in exploration strategies.
  - Why needed here: The framework uses PSN instead of traditional ASN for exploration, requiring understanding of how parameter perturbations create different behavioral patterns than action perturbations.
  - Quick check question: How does injecting noise in the parameter space versus action space affect the consistency and diversity of exploration behaviors?

## Architecture Onboarding

- Component map: Parameter Space Noise (PSN) module -> Robust Trajectory Weighting module -> Safe Exploration by Adaptive Action Selection (SEAS) module -> Offline RL training module (IQL/CQL/TD3BC) -> Data collection and storage pipeline

- Critical path:
  1. Initialize with safe baseline policy
  2. Generate exploration policies using PSN
  3. Apply SEAS to ensure safety during data collection
  4. Collect interaction dataset with trajectory information
  5. Compute trajectory weights using Robust Trajectory Weighting
  6. Train new policy using weighted offline RL
  7. Evaluate and iterate

- Design tradeoffs:
  - PSN vs ASN: PSN provides better trajectory-level exploration but requires more careful parameter tuning; ASN is simpler but creates counterbalancing effects
  - Weighting strength (α parameter): Higher values emphasize high-return trajectories more but may discard useful low-return exploration data
  - Number of safe policies in SEAS: More policies provide better coverage but increase computational complexity

- Failure signatures:
  - Performance degradation over iterations: May indicate ineffective exploration or poor trajectory weighting
  - Safety constraint violations: Suggests SEAS parameters need adjustment or Q-value estimates are inaccurate
  - Convergence to suboptimal policies: Could indicate conservatism is too strong or dataset coverage is insufficient

- First 3 experiments:
  1. Compare PSN vs ASN exploration by measuring trajectory return distributions in collected datasets
  2. Test Robust Trajectory Weighting effectiveness by training with and without weighting on the same dataset
  3. Validate SEAS safety guarantees by running with varying epsilon values and measuring performance bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Trajectory-wise Exploration and Exploitation (TEE) generalize to other domains beyond auto-bidding, such as recommendation systems or healthcare?
- Basis in paper: [inferred] The paper mentions in the conclusion that they plan to test TEE in other fields like recommender systems and healthcare, where online policy training is challenging but urgently needed.
- Why unresolved: The current experiments are limited to the auto-bidding domain. Generalization to other domains requires further investigation.
- What evidence would resolve it: Conducting experiments on TEE in other domains like recommendation systems or healthcare and comparing the results with baseline methods would provide evidence of its effectiveness in different applications.

### Open Question 2
- Question: What is the impact of different reward model architectures on the effectiveness of Robust Trajectory Weighting?
- Basis in paper: [inferred] The paper mentions that the reward model could be implemented by a neural network with a specific loss function, but does not explore the impact of different architectures.
- Why unresolved: The effectiveness of Robust Trajectory Weighting might depend on the choice of reward model architecture, which is not explored in the paper.
- What evidence would resolve it: Comparing the performance of Robust Trajectory Weighting with different reward model architectures (e.g., different neural network architectures or other types of models) would provide insights into the impact of the reward model on the method's effectiveness.

### Open Question 3
- Question: How does the performance of SEAS change with different numbers of safe policies provided as input?
- Basis in paper: [inferred] The paper mentions that multiple safe policies are provided to SEAS and the safe policy with the maximum Q value is chosen for constructing the safety condition, but does not explore the impact of varying the number of safe policies.
- Why unresolved: The performance of SEAS might depend on the number of safe policies provided, which is not investigated in the paper.
- What evidence would resolve it: Conducting experiments with different numbers of safe policies provided to SEAS and comparing the performance would provide insights into the impact of the number of safe policies on SEAS's effectiveness.

## Limitations
- The framework's effectiveness relies heavily on accurate Q-value estimates for safe policies in SEAS, but the paper provides limited discussion of how Q-value errors might affect safety guarantees.
- The trajectory weighting mechanism assumes that relabeled returns provide reliable quality signals, yet the sensitivity to reward model errors is not thoroughly explored.
- The comparison to ASN is mentioned but lacks quantitative evidence in the main paper, making the PSN advantage somewhat speculative without direct experimental validation.

## Confidence
- High confidence: The core mechanism of using parameter space noise for trajectory-wise exploration (Mechanism 1) is well-established in the literature and the intuition about ASN counterbalancing effects is sound.
- Medium confidence: The robust trajectory weighting approach (Mechanism 2) is theoretically justified but depends on the quality of the reward model, which may vary across different advertising scenarios.
- Medium confidence: The SEAS safety mechanism (Mechanism 3) provides a reasonable framework but the adaptive selection criterion's robustness to Q-value estimation errors needs further validation.

## Next Checks
1. **Direct PSN vs ASN comparison**: Implement both exploration strategies and measure the dispersion of trajectory returns in collected datasets, specifically testing whether PSN creates more diverse high-return behaviors than ASN in the auto-bidding context.

2. **Reward model sensitivity analysis**: Vary the quality of the learned reward model (e.g., by training with different amounts of data or architectures) and measure how trajectory weighting performance degrades, establishing error bounds for the exploitation mechanism.

3. **SEAS Q-value robustness test**: Introduce controlled noise into the Q-value estimates of safe policies and measure how this affects the safety guarantee violations and exploration efficiency, determining the tolerance threshold for Q-value estimation errors.