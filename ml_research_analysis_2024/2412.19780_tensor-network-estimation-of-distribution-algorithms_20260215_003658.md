---
ver: rpa2
title: Tensor Network Estimation of Distribution Algorithms
arxiv_id: '2412.19780'
source_url: https://arxiv.org/abs/2412.19780
tags:
- generative
- tensor
- distribution
- training
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates tensor network-based generative models
  within Estimation of Distribution Algorithms (EDAs) for combinatorial optimization.
  The authors find that optimization performance is not straightforwardly related
  to generative model quality: models that better approximate their training distribution
  do not necessarily yield better optimization performance.'
---

# Tensor Network Estimation of Distribution Algorithms

## Quick Facts
- arXiv ID: 2412.19780
- Source URL: https://arxiv.org/abs/2412.19780
- Authors: John Gardiner; Javier Lopez-Piqueres
- Reference count: 40
- Primary result: Tensor network-based EDAs with explicit mutation can achieve performance comparable to Bayesian network EDAs

## Executive Summary
This paper investigates tensor network-based generative models within Estimation of Distribution Algorithms (EDAs) for combinatorial optimization. The authors find that optimization performance is not straightforwardly related to generative model quality: models that better approximate their training distribution do not necessarily yield better optimization performance. Experiments show that adding explicit mutation steps or using lower expressivity models can improve optimization results. The key insight is that the generative model serves dual purposes—exploiting learned features and exploring the solution space—and that these roles can be in tension.

## Method Summary
The authors propose a Tensor Network Estimation of Distribution Algorithm (TN-EDA) that uses Matrix Product State (MPS) tensor networks as generative models within an EDA framework. The algorithm employs Boltzmann selection to choose promising solutions, trains the MPS generative model on selected solutions using gradient descent, and samples new candidate solutions from the learned distribution. An explicit mutation operator is added to introduce controlled exploration. The MPS tensor network has tunable bond dimension that controls model expressivity, and hierarchical clustering is used to determine variable ordering for optimal tensor contraction efficiency.

## Key Results
- Optimization performance does not correlate directly with generative model quality as measured by KL-divergence
- Adding explicit mutation operators improves performance by balancing exploration and exploitation
- Lower bond dimension (less expressive) tensor network models can outperform higher bond dimension models in optimization contexts
- TN-EDA with low bond dimension and mutation achieved performance comparable to simple Bayesian Network EDAs on benchmark problems

## Why This Works (Mechanism)

### Mechanism 1
The generative model in GEO serves dual roles - exploitation of learned features and exploration of solution space, which can be in tension. Better generative models that accurately model the training distribution may overfit to good solutions already found, reducing exploration and causing the algorithm to get stuck in local optima. Conversely, models with noise or lower expressivity force exploration by introducing randomness.

### Mechanism 2
Adding explicit mutation operators balances the exploration-exploitation tradeoff in EDAs. The mutation step introduces controlled randomness that allows the algorithm to escape local optima and explore new regions of the solution space, compensating for the exploitative nature of the generative model.

### Mechanism 3
Lower expressivity tensor network models (lower bond dimension) can outperform higher expressivity models in EDAs. Less expressive models have built-in regularization that prevents overfitting to the training distribution, maintaining sufficient exploration capability. The reduced model capacity forces the algorithm to explore more broadly rather than exploiting specific features too strongly.

## Foundational Learning

- Concept: Tensor network decomposition and contraction
  - Why needed here: The paper relies on understanding how tensor networks efficiently represent high-dimensional probability distributions and how contraction operations compute probabilities
  - Quick check question: How does the bond dimension of an MPS tensor network affect its computational complexity and representational capacity?

- Concept: Estimation of Distribution Algorithms (EDAs)
  - Why needed here: The paper frames tensor network methods within the EDA framework, comparing them to traditional genetic algorithms and other EDA variants
  - Quick check question: What is the fundamental difference between how EDAs and traditional genetic algorithms generate new candidate solutions?

- Concept: KL-divergence and generative model evaluation
  - Why needed here: The paper uses KL-divergence to measure how well generative models learn their training distribution, finding that lower KL-divergence doesn't always correlate with better optimization performance
  - Quick check question: Why might a generative model with higher KL-divergence relative to the training distribution perform better in an optimization context?

## Architecture Onboarding

- Component map: Selection module -> Generative model training -> Sampling module -> Mutation operator -> Evaluation module -> Population update
- Critical path: Selection → Model Training → Sampling → Mutation → Evaluation → Population Update
- Design tradeoffs:
  - Model expressivity vs. exploration capability: Higher bond dimension models may overfit, while lower bond dimension models maintain exploration
  - Mutation rate vs. convergence speed: Higher mutation rates increase exploration but slow convergence to good solutions
  - Selection temperature vs. diversity: Higher temperatures maintain diversity but may slow convergence
- Failure signatures:
  - Algorithm gets stuck in local optima: Likely insufficient exploration (mutation rate too low or model too expressive)
  - Very slow convergence: Possibly excessive exploration (mutation rate too high or selection temperature too high)
  - Poor final solution quality: Could indicate inadequate exploitation (selection too random or model too noisy)
- First 3 experiments:
  1. Implement basic GEO with tensor network generative model on a simple portfolio optimization problem, comparing performance with different bond dimensions (2, 5, 10)
  2. Add explicit mutation operator to GEO and tune mutation rate to find optimal balance for the same problem
  3. Compare performance against a simple Bayesian network EDA with mutation on the same benchmark problems

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between exploration and exploitation when using tensor network generative models in EDAs? The paper identifies that "a better (at generalization) generative model does not necessarily lead to a better EDA optimizer" and suggests that "the generative model in GEO evidently provides other functions" including exploration, which "is sometimes done better by models that are worse at learning the training data distribution."

### Open Question 2
How does population size affect the performance of tensor network EDAs, particularly in relation to mutation operators? The paper references [29] which "showed that adding an explicit mutation operator improved the performance of various EDAs based on different choices of probabilistic graphical models, especially in the regimes of low population sizes" but does not explore this relationship themselves.

### Open Question 3
What is the advantage of tensor networks over simpler Bayesian networks in EDAs for combinatorial optimization? The authors note that "a low bond dimension MPS resulted in a TN-EDA comparable to a simple Bayesian Network EDA in some canonical combinatorial optimization problems" and ask "whether, or in what regime, tensor networks offer any advantage compared to Bayesian networks within an EDA."

## Limitations
- Unclear relationship between generative model quality and optimization performance complicates algorithm design
- Experiments limited to specific benchmark problems (portfolio optimization, Knapsack, Max-3SAT)
- Mutation mechanism is somewhat heuristic with no theoretical justification for the specific mutation rate of 0.01
- Does not explore alternative exploration strategies beyond simple mutation

## Confidence
- High confidence: The experimental results showing that higher KL-divergence (worse model quality) can correlate with better optimization performance
- Medium confidence: The recommendation to use explicit mutation operators for balancing exploration-exploitation tradeoff
- Medium confidence: The observation that lower bond dimension models can outperform higher bond dimension models in this context

## Next Checks
1. **Cross-domain validation**: Test the TN-EDA approach on additional combinatorial optimization problems (e.g., traveling salesman, graph coloring) to assess generalizability of the exploration-exploitation tradeoff findings.

2. **Adaptive mutation rate**: Implement and evaluate an adaptive mutation mechanism that adjusts the mutation rate based on population diversity or convergence metrics, comparing against the fixed 0.01 rate.

3. **Alternative generative models**: Compare tensor network-based EDAs against other probabilistic models (normalizing flows, variational autoencoders) on the same benchmark problems to determine if the exploration-exploitation tension is specific to tensor networks or a general EDA phenomenon.