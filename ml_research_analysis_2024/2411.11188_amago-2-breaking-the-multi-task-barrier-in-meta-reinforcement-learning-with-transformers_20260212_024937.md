---
ver: rpa2
title: 'AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with
  Transformers'
arxiv_id: '2411.11188'
source_url: https://arxiv.org/abs/2411.11188
tags:
- tasks
- timesteps
- learning
- otal
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMAGO-2, a method for training multi-task
  reinforcement learning agents that adapt without task labels by converting both
  actor and critic objectives to scale-resistant classification terms. The core idea
  is to replace value regression with two-hot classification and policy updates with
  filtered imitation learning, eliminating direct dependence on the scale of returns
  across tasks.
---

# AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers

## Quick Facts
- **arXiv ID**: 2411.11188
- **Source URL**: https://arxiv.org/abs/2411.11188
- **Reference count**: 40
- **Primary result**: AMAGO-2 achieves state-of-the-art multi-task meta-RL performance across 5 benchmarks by converting actor and critic objectives to scale-resistant classification terms

## Executive Summary
AMAGO-2 addresses a fundamental challenge in multi-task reinforcement learning: how to train agents that can adapt to diverse tasks without requiring explicit task labels. The method transforms traditional value regression into two-hot classification and policy updates into filtered imitation learning, eliminating dependence on reward scale variations across tasks. This scale-invariant approach enables better generalization and performance across heterogeneous task distributions while maintaining sample efficiency. Experiments demonstrate significant improvements over prior methods across continuous control (Meta-World ML45, Multi-Task POPGym) and discrete control (Multi-Game Procgen, Multi-Game Atari, BabyAI) domains.

## Method Summary
AMAGO-2 replaces standard actor-critic objectives with scale-resistant classification terms. The critic is trained as a two-hot classifier to predict whether returns fall into high or low categories relative to task-specific thresholds, rather than regressing to scalar values. The actor update uses filtered imitation learning where the policy is trained to match actions that lead to high returns, with filtering based on the classifier's confidence. A transformer encoder processes context from multiple trajectories to infer task identity without explicit labels. The method is trained end-to-end with both actor and critic objectives optimized simultaneously, enabling robust adaptation across tasks with varying reward scales and dynamics.

## Key Results
- Achieves state-of-the-art performance across Meta-World ML45, Multi-Task POPGym, Multi-Game Procgen, Multi-Game Atari, and BabyAI benchmarks
- Scale-invariant updates enable training on heterogeneous task distributions with vastly different reward scales
- Maintains sample efficiency comparable to single-task RL while enabling zero-shot adaptation to new tasks
- Eliminates need for task labels during both training and inference, learning task inference from context

## Why This Works (Mechanism)
The core insight is that traditional actor-critic methods fail in multi-task settings because value regression objectives are sensitive to the scale of returns, which varies dramatically across tasks. By converting the critic to a classification problem (predicting whether returns are high or low), the method becomes invariant to absolute reward magnitudes. The actor update through filtered imitation learning ensures the policy learns to select actions associated with high returns without directly depending on their scale. The transformer encoder learns to infer task identity from context, enabling appropriate adaptation without explicit supervision. This combination allows the agent to leverage shared structure across tasks while maintaining task-specific performance.

## Foundational Learning
- **Scale invariance in RL**: Why needed - Different tasks have vastly different reward scales; quick check - Verify performance when mixing tasks with scales differing by 1000x
- **Classification vs regression for value functions**: Why needed - Regression is sensitive to scale; quick check - Compare training stability between classification and regression critics
- **Task inference from context**: Why needed - Avoids need for explicit task labels; quick check - Test task identification accuracy on held-out tasks
- **Filtered imitation learning**: Why needed - Provides scale-invariant policy updates; quick check - Compare with standard policy gradient updates
- **Transformer-based context processing**: Why needed - Handles variable-length trajectory context; quick check - Evaluate with different context window sizes
- **Multi-task meta-learning**: Why needed - Enables transfer across diverse tasks; quick check - Measure adaptation performance on novel tasks

## Architecture Onboarding

**Component map**: Observation -> Transformer Encoder -> Task Embedding -> Actor/Critic Heads -> Action/Value Output

**Critical path**: State trajectory context → Transformer encoding → Task inference → Scale-invariant value classification → Filtered imitation policy update

**Design tradeoffs**: Classification-based critic trades precision for scale invariance; filtered imitation may slow learning compared to direct gradients; transformer context adds computational overhead but enables task inference

**Failure signatures**: Poor task inference leads to mixed behavior across tasks; overly conservative classification thresholds hurt performance; insufficient context prevents task identification

**First experiments**: 1) Validate scale invariance by training on tasks with 1000x reward differences, 2) Test task inference accuracy with varying context lengths, 3) Compare filtered imitation vs direct policy gradients on adaptation speed

## Open Questions the Paper Calls Out
None

## Limitations
- Classification-based value function may introduce approximation errors for environments requiring precise value estimation
- High-dimensional action spaces may challenge the filtered imitation learning approach when distinguishing similar actions
- Performance in partially observable environments and tasks requiring long-term credit assignment remains untested
- Claims about sample efficiency preservation need direct comparison to single-task RL baselines on training curves

## Confidence
- **High confidence**: Scale invariance improvements across tested benchmarks are convincingly demonstrated with clear ablation studies
- **Medium confidence**: Sample efficiency claims are supported but would benefit from direct single-task baseline comparisons
- **Low confidence**: Generalization claims beyond tested environments need additional cross-domain validation

## Next Checks
1. Test AMAGO-2's performance when mixing tasks with drastically different reward scales (e.g., 1 vs 1000) to stress-test scale invariance claims
2. Evaluate sample efficiency relative to single-task SAC across training curves, not just final performance
3. Conduct ablation study isolating task inference versus scale-invariant updates by providing ground-truth task labels during adaptation