---
ver: rpa2
title: Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring
arxiv_id: '2404.04941'
source_url: https://arxiv.org/abs/2404.04941
tags:
- scoring
- essay
- score
- trait
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multi Trait Specialization (MTS), a zero-shot
  prompting framework for automated essay scoring (AES) using large language models
  (LLMs). MTS decomposes writing proficiency into distinct traits and generates scoring
  criteria for each trait, then prompts the LLM to extract trait scores through specialized
  conversations.
---

# Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring

## Quick Facts
- arXiv ID: 2404.04941
- Source URL: https://arxiv.org/abs/2404.04941
- Reference count: 40
- Primary result: Multi Trait Specialization (MTS) framework outperforms straightforward prompting (Vanilla) in automated essay scoring, with maximum gains of 0.437 QWK on TOEFL11 and 0.355 QWK on ASAP

## Executive Summary
This paper proposes Multi Trait Specialization (MTS), a zero-shot prompting framework for automated essay scoring (AES) using large language models (LLMs). The framework decomposes writing proficiency into distinct traits and generates scoring criteria for each trait, then prompts the LLM to extract trait scores through specialized conversations. Experimental results demonstrate that MTS consistently outperforms straightforward prompting across all tested LLMs and datasets, with significant gains particularly on the TOEFL11 dataset. Notably, the smaller Llama2-13b-chat model substantially outperforms ChatGPT when using the MTS framework.

## Method Summary
The Multi Trait Specialization (MTS) framework works by first automatically decomposing writing quality into multiple traits using ChatGPT to generate trait-specific scoring criteria from rubric guidelines. For each trait, the LLM engages in specialized conversations that include quote retrieval tasks where relevant essay content is extracted and evaluated. These trait scores are then aggregated through averaging, with outliers clipped using quartile-based thresholds, and finally scaled to the target score range via min-max normalization. This approach addresses the scoring bias inherent in LLMs and provides more grounded evaluations by requiring the model to reference specific textual evidence rather than providing generic assessments.

## Key Results
- MTS consistently outperforms Vanilla prompting across all LLMs and datasets, with maximum gains of 0.437 QWK on TOEFL11 and 0.355 QWK on ASAP
- With MTS, Llama2-13b-chat substantially outperforms ChatGPT in essay scoring accuracy
- Quote retrieval and outlier clipping contribute to improved scoring accuracy, though with varying degrees of impact
- The framework demonstrates effectiveness across different essay genres and scoring ranges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing writing proficiency into distinct traits allows LLMs to evaluate essays from multiple, complementary perspectives, improving overall scoring accuracy.
- Mechanism: The framework uses ChatGPT to automatically decompose the rubric guidelines into trait-specific scoring criteria, then prompts the LLM to evaluate each trait independently in separate conversations.
- Core assumption: The rubric guidelines contain sufficient information to derive meaningful trait distinctions, and these traits capture the key dimensions of writing quality.
- Evidence anchors:
  - [abstract] "we automatically decompose writing proficiency into distinct traits and generate scoring criteria for each trait"
  - [section 2.1] "we exploit ChatGPT to decompose the writing quality into multiple traits and generate scoring criteria for each trait"
  - [corpus] Weak - no corpus evidence directly supporting trait decomposition effectiveness
- Break condition: If the rubric guidelines are too vague or mixed in their criteria, the automatic decomposition may produce generic or misaligned traits that don't capture the prompt-specific requirements.

### Mechanism 2
- Claim: Quote retrieval before scoring ensures the LLM's evaluation is grounded in specific textual evidence rather than generic assessments.
- Mechanism: In each trait-specific conversation, the LLM first retrieves relevant quotes from the essay and evaluates their quality, then uses these evaluations to inform the final trait score.
- Core assumption: The LLM can accurately identify relevant quotes and provide meaningful evaluations that correlate with the scoring criteria.
- Evidence anchors:
  - [section 2.2] "LLM is required to retrieve quotes relevant to the trait and provide verbal evaluations for each quote"
  - [section 2.2] "quote retrieval task allows LLM to adhere to the details of the essay and avoid producing generic evaluation"
  - [corpus] Weak - no corpus evidence directly showing quote retrieval improves scoring accuracy
- Break condition: If the LLM fails to retrieve relevant quotes or provides inconsistent evaluations, the grounding mechanism breaks down and scoring accuracy may suffer.

### Mechanism 3
- Claim: Min-max scaling with outlier clipping effectively maps LLM predictions to arbitrary target score ranges while mitigating scoring bias.
- Mechanism: Trait scores are averaged, outliers are clipped using quartile-based thresholds, then the remaining scores are scaled to the target range via min-max scaling.
- Core assumption: The LLM's trait predictions follow a distribution that can be normalized through clipping and scaling, and the clipping thresholds effectively identify true outliers.
- Evidence anchors:
  - [section 2.3] "we propose a new trait aggregation and scaling method" including "clipping alleviates the sensitivity of min-max scaling to the outliers"
  - [section 5.3] "outlier clipping further brings small but consistent improvements"
  - [section 5.3] "Min-max scaling effectively addresses the scoring bias by spreading its predictions across the target score range"
- Break condition: If the LLM's predictions are extremely skewed or contain too many outliers, the clipping may remove too much data, and scaling may not adequately normalize the distribution.

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: QWK measures agreement between predicted and ground truth scores, accounting for the degree of disagreement, which is the standard metric for AES evaluation
  - Quick check question: If a model predicts scores [1,2,3,4,5] for ground truth [1,2,3,4,5] versus [5,4,3,2,1], which would have higher QWK and why?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT prompting improves LLM reasoning by externalizing the reasoning process, which is leveraged in the Vanilla baseline and may contribute to scoring improvements
  - Quick check question: How does requesting rationales before scoring potentially improve LLM performance compared to direct scoring requests?

- Concept: Min-max normalization and outlier detection
  - Why needed here: These statistical techniques are used to transform LLM predictions to match target score ranges while handling prediction biases and outliers
  - Quick check question: Given a dataset with values [1,2,3,4,100], what would be the effect of applying min-max scaling versus clipping at 1.5*IQR first?

## Architecture Onboarding

- Component map: Multi Trait Decomposition → Trait Specialization (Quote Retrieval + Scoring) → Trait Aggregation and Scaling
- Critical path: The LLM inference path through multiple trait-specific conversations is the bottleneck, as each trait requires separate API calls
- Design tradeoffs: Multiple trait conversations improve accuracy but increase computational cost and inference time compared to single-shot scoring
- Failure signatures: Poor QWK scores may indicate issues with trait decomposition quality, quote retrieval relevance, or scaling parameters
- First 3 experiments:
  1. Test Vanilla baseline with and without CoT prompting to establish baseline performance
  2. Test MTS with varying numbers of traits (2, 3, 4) to find optimal trait count
  3. Compare different scaling methods (fixed scaling, min-max, min-max with clipping) to validate the proposed approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the quote retrieval mechanism vary across different essay lengths and complexity levels?
- Basis in paper: [explicit] The paper mentions that quote retrieval is used to provide faithful evaluations by referencing specific content of the essay, but does not provide detailed analysis of its effectiveness across different essay characteristics.
- Why unresolved: The paper focuses on overall QWK scores without breaking down performance based on essay length or complexity metrics.
- What evidence would resolve it: Detailed analysis of quote retrieval effectiveness stratified by essay length and complexity measures, potentially showing how this mechanism scales with different essay types.

### Open Question 2
- Question: What is the optimal number of traits for maximizing scoring accuracy across different essay genres and prompts?
- Basis in paper: [explicit] The paper mentions that using more traits leads to elevated performance and that different traits are complementary, but doesn't systematically explore the optimal number of traits.
- Why unresolved: The paper uses a fixed 4-trait decomposition but doesn't explore whether this is optimal or how it varies by context.
- What evidence would resolve it: Systematic experiments varying the number of traits across different prompts and genres, with corresponding performance analysis.

### Open Question 3
- Question: How does the scaling mechanism perform when dealing with outliers that are not extreme but still affect the distribution?
- Basis in paper: [inferred] The paper mentions that outlier clipping using Q1 and Q3 helps address outliers, but doesn't explore how the mechanism handles less extreme but still problematic outliers.
- Why unresolved: The analysis focuses on extreme outliers but doesn't address the broader distribution of predictions and their impact on scaling.
- What evidence would resolve it: Analysis of how different scaling approaches handle various types of outliers, including visual distribution comparisons and sensitivity analysis.

### Open Question 4
- Question: How do different LLM architectures affect the quality and consistency of the trait-specific scoring criteria generated during decomposition?
- Basis in paper: [explicit] The paper mentions that ChatGPT-generated guidance slightly outperforms self-generated ones for some models but not others, suggesting variation in criteria quality.
- Why unresolved: The paper doesn't provide detailed analysis of how different LLM architectures influence the decomposition quality or consistency.
- What evidence would resolve it: Comparative analysis of scoring criteria generated by different LLM architectures, with metrics for consistency and quality across different model types.

## Limitations
- Decomposition Quality: The automatic trait decomposition using ChatGPT is not systematically evaluated and may produce generic traits for complex rubrics
- Scaling Robustness: The min-max scaling approach may be sensitive to skewed prediction distributions and may not adequately normalize heavily skewed data
- Computational Overhead: The framework requires multiple trait-specific conversations, increasing computational cost and inference time compared to single-shot scoring methods

## Confidence
- **High Confidence**: MTS framework consistently outperforms Vanilla prompting across all tested LLMs and datasets; quote retrieval improves scoring grounding; min-max scaling with outlier clipping provides consistent improvements
- **Medium Confidence**: The specific trait decomposition quality directly impacts scoring accuracy; Llama2-13b-chat can outperform ChatGPT when using MTS; the framework generalizes well to different essay scoring tasks
- **Low Confidence**: The framework's performance on completely unseen essay types; the robustness of automatic trait decomposition for complex rubrics; long-term computational efficiency at scale

## Next Checks
1. **Decomposition Quality Audit**: Manually evaluate the traits generated by ChatGPT for 50 randomly selected rubric guidelines from different domains. Compare these against human-generated trait decompositions to assess consistency and domain appropriateness. Measure inter-rater agreement between human experts and the automated system.

2. **Scaling Robustness Test**: Generate synthetic prediction distributions with varying degrees of skewness and outlier density. Test how min-max scaling with outlier clipping performs compared to alternative normalization methods (z-score, robust scaling) across these distributions. Identify the breaking points where the current approach fails.

3. **Computational Efficiency Analysis**: Benchmark the inference time and cost for MTS versus Vanilla prompting across different LLMs (Llama2-13b-chat, GPT-4, Claude) using the same set of essays. Calculate the accuracy gain per unit of computational cost to determine the practical efficiency trade-off for real-world deployment.