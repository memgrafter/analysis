---
ver: rpa2
title: 'MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with
  Sparse Mixture of Low-Rank Adapter Experts'
arxiv_id: '2404.09027'
source_url: https://arxiv.org/abs/2404.09027
tags:
- medical
- language
- ming-moe
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MING-MOE introduces a Mixture-of-Expert (MOE) approach using Mixture
  of Low-Rank Adaptation (MoLoRA) to enhance multi-task learning in medical large
  language models. It enables selective expert activation per token during training,
  eliminating the need for task-specific annotations and improving generalization
  across diverse medical tasks.
---

# MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts

## Quick Facts
- arXiv ID: 2404.09027
- Source URL: https://arxiv.org/abs/2404.09027
- Authors: Yusheng Liao; Shuyang Jiang; Yu Wang; Yanfeng Wang
- Reference count: 14
- Primary result: Achieves state-of-the-art performance on over 20 medical tasks using Mixture of Low-Rank Adaptation (MoLoRA) with token-level expert selection

## Executive Summary
MING-MOE introduces a Mixture-of-Expert (MOE) approach using Mixture of Low-Rank Adaptation (MoLoRA) to enhance multi-task learning in medical large language models. The model employs token-level expert selection instead of task-level selection, eliminating the need for task-specific annotations at inference time. By freezing base model parameters and training only a minimal set of parameters through LoRA decomposition, MING-MOE achieves superior performance across diverse medical tasks while maintaining parameter efficiency.

The model demonstrates exceptional performance on medical NLP tasks and licensing exams, surpassing existing open-source medical models and even outperforming closed-source models like GPT-4 and ChatGPT in certain benchmarks. MING-MOE shows strong generalization capabilities across the 20+ medical tasks it was evaluated on, making it a promising approach for medical multi-task learning applications.

## Method Summary
MING-MOE uses a Mixture of Low-Rank Adaptation (MoLoRA) technique that combines token-level expert selection with parameter-efficient fine-tuning. The approach maintains base model parameters static while adapting through a minimal set of trainable parameters using LoRA decomposition. The model employs 8 experts with 2 activated per token during training, using a router network to compute token-specific weights for expert selection. The architecture consists of a frozen base model (Qwen1.5-Chat), MoLoRA experts for FFN layers, a router network for token-expert affinity computation, and standard LoRA for attention MLP layers.

## Key Results
- Achieves state-of-the-art performance on over 20 medical tasks including medical NLP tasks and licensing exams
- Surpasses existing open-source medical models and outperforms GPT-4 and ChatGPT on some medical benchmarks
- Demonstrates superior knowledge efficiency and generalization across diverse medical tasks
- Shows strong performance in the 2023 Chinese National Pharmacist Licensure Examination, outperforming closed-source models

## Why This Works (Mechanism)

### Mechanism 1
MING-MOE improves medical multi-task learning by using token-level expert selection instead of task-level, eliminating the need for task-specific annotations at inference time. The Mixture of Low-Rank Adaptation (MoLoRA) approach allows each token to activate a different set of LoRA experts based on its affinity, enabling dynamic routing without explicit task labels. Medical tasks have diverse input-output formats and can benefit from selective expert activation at the token level rather than the task level.

### Mechanism 2
Freezing base model parameters and training only a minimal set of parameters via LoRA reduces adaptation cost while maintaining performance. By decomposing weight updates into low-rank matrices (BA) and keeping the original weights frozen, the model achieves parameter-efficient fine-tuning with minimal trainable parameters. Weight updates in large language models are inherently low-rank, allowing effective adaptation with compressed parameter representations.

### Mechanism 3
Selective expert activation improves inference efficiency by only employing relevant experts for each input. The router computes token-specific weights for each expert, activating only K experts per token rather than all experts, reducing computational overhead during inference. Not all experts are needed for every input token, and selective activation can maintain performance while reducing computation.

## Foundational Learning

- **Mixture of Experts (MoE)**: Needed because medical tasks are diverse and complex, requiring specialized processing for different types of medical text and tasks. Quick check: How does MoE differ from traditional dense models in handling diverse tasks?
- **Low-Rank Adaptation (LoRA)**: Needed because full fine-tuning of large language models is computationally expensive, especially for multi-task learning scenarios. Quick check: What mathematical property of weight updates enables LoRA to work effectively?
- **Parameter-Efficient Fine-Tuning (PEFT)**: Needed because medical large language models are large, making traditional fine-tuning impractical for resource-constrained environments. Quick check: How does PEFT balance between adaptation effectiveness and computational efficiency?

## Architecture Onboarding

- **Component map**: Input tokenization -> Router computation -> Top-K expert selection -> Adapted FFN layers -> Output generation
- **Critical path**: 1. Input tokenization and embedding, 2. Router computation for token-expert affinity, 3. Top-K expert selection and weighted combination, 4. Forward pass through adapted FFN layers, 5. Output generation
- **Design tradeoffs**: Number of experts (N=8) vs. parameter efficiency, Top-K selection (K=2) vs. performance, LoRA rank (r=16) vs. adaptation capacity, Frozen base model vs. full fine-tuning capability
- **Failure signatures**: Degraded performance on tasks requiring base model knowledge, Router collapse (all tokens selecting same experts), Overfitting to training tasks with limited generalization, Increased inference latency due to expert routing
- **First 3 experiments**: 1. Compare performance with different values of K (1, 2, 3) to find optimal trade-off between efficiency and accuracy, 2. Test various LoRA ranks (8, 16, 32) to determine minimum effective rank for medical tasks, 3. Evaluate ablation study with MoLoRA only in FFN vs. also in attention layers to assess architectural contribution

## Open Questions the Paper Calls Out

### Open Question 1
How does MING-MOE's performance scale with increasing numbers of experts in the MoLoRA architecture? The paper mentions using 8 experts with 2 activated per token but does not explore performance variations with different numbers of experts. This remains unresolved because the paper does not report experiments testing different numbers of experts or activated experts per token, leaving the optimal configuration unclear.

### Open Question 2
How does MING-MOE's medical knowledge generalization compare to models trained on larger, more diverse medical datasets? While MING-MOE achieves state-of-the-art results, the paper does not compare its performance to models trained on datasets significantly larger than the 300k samples used for MING-MOE. This remains unresolved because the paper only compares MING-MOE to models trained on similar or smaller datasets, not exploring the potential benefits of much larger training data.

### Open Question 3
What is the impact of MING-MOE's token-level expert selection on inference efficiency compared to task-level selection? The paper claims that token-level selection eliminates the need for task-specific annotations and improves applicability, but does not provide quantitative comparisons of inference efficiency. This remains unresolved because the paper does not report metrics comparing the inference speed or computational cost of token-level versus task-level expert selection in MoE architectures.

## Limitations
- Evaluation relies heavily on benchmark comparisons that may not fully capture real-world medical practice scenarios
- Claims of "state-of-the-art performance on over 20 medical tasks" lack detailed methodology for benchmark selection and evaluation metrics
- Token-level expert selection may introduce complexity affecting training stability and inference efficiency

## Confidence
- **High Confidence**: Technical implementation of MoLoRA with token-level expert selection is well-described and follows established principles of parameter-efficient fine-tuning
- **Medium Confidence**: Performance claims on medical benchmarks are supported by quantitative results, but lack of detailed methodology reduces confidence in comprehensiveness
- **Low Confidence**: Generalizability to real-world medical applications and ability to handle diverse medical scenarios beyond tested benchmarks is not adequately demonstrated

## Next Checks
1. **Clinical Validation Study**: Conduct controlled study comparing MING-MOE's recommendations against practicing clinicians on real medical cases
2. **Bias and Fairness Analysis**: Perform comprehensive analysis of model performance across different medical specialties, patient demographics, and geographic regions
3. **Long-term Stability Testing**: Evaluate model's performance consistency over extended periods with dynamic medical knowledge updates