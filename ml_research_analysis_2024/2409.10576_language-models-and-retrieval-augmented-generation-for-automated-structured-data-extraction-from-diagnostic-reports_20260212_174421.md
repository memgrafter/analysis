---
ver: rpa2
title: Language Models and Retrieval Augmented Generation for Automated Structured
  Data Extraction from Diagnostic Reports
arxiv_id: '2409.10576'
source_url: https://arxiv.org/abs/2409.10576
tags:
- reports
- performance
- radiology
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an automated system using open-weights large
  language models and retrieval-augmented generation to extract structured clinical
  data from unstructured radiology and pathology reports. The system achieved over
  98% accuracy in extracting BT-RADS scores from radiology reports and over 90% for
  IDH mutation status from pathology reports.
---

# Language Models and Retrieval Augmented Generation for Automated Structured Data Extraction from Diagnostic Reports

## Quick Facts
- arXiv ID: 2409.10576
- Source URL: https://arxiv.org/abs/2409.10576
- Reference count: 27
- Over 98% accuracy in extracting BT-RADS scores from radiology reports and over 90% for IDH mutation status from pathology reports

## Executive Summary
This study developed an automated system using open-weights large language models and retrieval-augmented generation to extract structured clinical data from unstructured radiology and pathology reports. The system achieved over 98% accuracy in extracting BT-RADS scores from radiology reports and over 90% for IDH mutation status from pathology reports. Key factors influencing performance included larger model size, domain fine-tuning, complex prompting, and retrieval-augmented generation for complex pathology reports. Model quantization and inference-time sampling parameters had minimal impact. The results demonstrate the potential of open LMs for local, privacy-preserving automated clinical data extraction, with implications for research workflows and structured database curation.

## Method Summary
The study used various open-weights LLMs including Llama3, openbiollm-Llama3, Llama2, Medllama2, Meditron, Mistral, Biomistral, Mixtral, and Phi3 to extract structured clinical data from 7,294 radiology reports (BT-RADS scores) and 2,154 pathology reports (IDH mutation status). The authors evaluated different configurations including model size, quantization levels, prompting strategies, output formatting, and inference parameters. They implemented retrieval-augmented generation with a local vector database and various retrieval approaches. Performance was assessed using accuracy, macro/micro F1, precision, and recall scores across 407 different configurations.

## Key Results
- Larger, newer, and domain-fine-tuned models consistently outperformed older and smaller models
- Retrieval-augmented generation improved performance for complex pathology reports but not for shorter radiology reports
- Few-shot prompting significantly improved accuracy across all models
- Model quantization and inference-time sampling parameters had minimal impact on extraction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger, more recent, and domain-fine-tuned models achieve higher extraction accuracy.
- Mechanism: Increased model capacity captures complex patterns in clinical language, while fine-tuning adapts representations to medical terminology and context, improving extraction precision.
- Core assumption: Performance scales predictably with model size and domain relevance; model architecture differences matter less than parameter count.
- Evidence anchors:
  - [abstract] "Larger, newer, and domain fine tuned models consistently outperformed older and smaller models."
  - [section] "Larger models demonstrated higher mean accuracy (86% ± 22%) compared to smaller ones (75% ± 32%) (t = 3.80, p < 0.001), with medium effect (Cohen's d = 0.40)."
  - [corpus] "Weak or missing explicit comparisons between model architectures beyond parameter size."
- Break condition: When model size exceeds available GPU memory, or when fine-tuning introduces overfitting to narrow medical subdomains, degrading generalization.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves accuracy for long, complex pathology reports but not for short radiology reports.
- Mechanism: RAG retrieves relevant document chunks to provide context, reducing hallucination and focusing the model on salient information, which is more beneficial when reports are lengthy and information is scattered.
- Core assumption: Context relevance and noise reduction are the dominant factors for extraction accuracy; document length and structure affect RAG utility.
- Evidence anchors:
  - [abstract] "RAG improved performance for complex pathology reports but not for shorter radiology reports."
  - [section] "RAG consistently improved accuracy across all models. The mean accuracy with RAG ranged from 69.4% to 92.8% vs. 5.2% to 44.8% without RAG, a statistically significant mean increase of 48.08% ± 11.18% (t=7.91, p=0.001)."
  - [corpus] "No explicit evidence for RAG's performance on radiology; inference based on abstract."
- Break condition: When the retrieval step fails to find relevant chunks due to poor embedding quality, or when reports are too short to benefit from additional context.

### Mechanism 3
- Claim: Complex prompts and few-shot examples significantly improve extraction accuracy.
- Mechanism: Detailed prompts provide clearer task definition and valid output formats, while few-shot examples teach the model the expected structure and handle edge cases, reducing error rates.
- Core assumption: LMs are sensitive to prompt specificity and benefit from explicit task demonstrations; output formatting instructions reduce variability.
- Evidence anchors:
  - [abstract] "Few-shot prompting significantly improved accuracy."
  - [section] "Complex prompting consistently improved accuracy across models... The mean accuracy increase was 12.01% ± 11.56% (t=3.11, p=0.01). Similarly, few-shot prompting improved accuracy in most models... the mean increase being 32.42% ± 32.39%."
  - [corpus] "No explicit evidence on the effect of prompt complexity on radiology reports in the corpus."
- Break condition: When prompts become too verbose or specific, potentially causing the model to overfit to the provided examples rather than generalize.

## Foundational Learning

- Concept: Structured data extraction from unstructured clinical text
  - Why needed here: Core task—translating free-text reports into standardized, machine-readable formats for downstream analytics.
  - Quick check question: What is the difference between structured and unstructured clinical data, and why is structured extraction valuable for research workflows?

- Concept: Retrieval-Augmented Generation (RAG) fundamentals
  - Why needed here: RAG is used to improve extraction accuracy by retrieving relevant report sections before prompting the LM.
  - Quick check question: How does RAG differ from standard LM prompting, and in what scenarios is it most beneficial?

- Concept: Prompt engineering and few-shot learning in LMs
  - Why needed here: Complex prompts and few-shot examples are shown to significantly boost accuracy in this system.
  - Quick check question: What are the main differences between simple, complex, and few-shot prompting strategies, and how do they affect model performance?

## Architecture Onboarding

- Component map: Input → Preprocessing → Embedding & Retrieval (RAG) → LM (Prompted) → Postprocessing (JSON parsing, validation) → Output
- Critical path: Text cleaning → Prompt formulation → LM inference → Output validation
- Design tradeoffs: Model size vs. GPU memory; RAG vs. simplicity; prompt complexity vs. runtime; quantization vs. accuracy
- Failure signatures: Low accuracy due to irrelevant RAG chunks; hallucinations in absence of RAG; model crashes due to insufficient VRAM; JSON parsing errors from malformed outputs
- First 3 experiments:
  1. Run baseline extraction without RAG on a small sample of pathology reports; measure accuracy and document if context retrieval helps.
  2. Compare performance of a small vs. large model on radiology reports with identical prompts; record accuracy and inference time.
  3. Test effect of few-shot prompting vs. simple prompting on both report types; measure improvement in F1 and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do open-weight LLMs perform on structured data extraction tasks with more ambiguous or multi-category clinical variables beyond discrete categorical responses?
- Basis in paper: [inferred] The authors note that the chosen target variables had discrete categorical responses and performance could decrease for more ambiguous datapoints.
- Why unresolved: The study focused specifically on BT-RADS scores and IDH mutation status, which have clear categorical definitions. The performance on more nuanced or subjective clinical variables remains untested.
- What evidence would resolve it: Comparative evaluation of open-weight LLM performance on structured data extraction tasks involving continuous variables, multi-category decisions, or subjective clinical assessments.

### Open Question 2
- Question: What is the optimal balance between model size, quantization level, and computational resources for different clinical data extraction tasks?
- Basis in paper: [explicit] The authors found minimal performance impact from quantization across 3-16-bit range and noted that larger models consistently outperformed smaller ones.
- Why unresolved: While the study provides insights into these factors, the optimal configuration likely depends on specific task requirements, available computational resources, and acceptable performance thresholds.
- What evidence would resolve it: Systematic evaluation of various model size/quantization combinations across different clinical data extraction tasks with varying complexity and resource constraints.

### Open Question 3
- Question: How does the performance of open-weight LLMs compare to commercial models when processing clinical reports from multiple institutions with varying reporting styles?
- Basis in paper: [inferred] The authors acknowledge their datasets had fairly homogeneous reporting structure due to single-center source, which may limit generalizability.
- Why unresolved: The study only evaluated open-weight models on single-institution data, while previous studies have shown mixed results for commercial models on multi-institutional data.
- What evidence would resolve it: Head-to-head comparison of open-weight and commercial LLM performance on multi-institutional clinical report datasets with varying reporting styles and terminology.

## Limitations
- The study was conducted using reports from a single institution, limiting generalizability to reports from different institutions with varying terminology and formatting.
- The pathology dataset (2,154 reports) is significantly smaller than the radiology dataset (7,294 reports), potentially affecting statistical power for pathology-specific findings.
- The study did not extensively explore retrieval-specific parameters such as chunk size, embedding models, or retrieval algorithms.

## Confidence
**High Confidence**: The core finding that larger, domain-fine-tuned models outperform smaller, general models (supported by statistical significance and effect sizes). The observation that RAG improves performance on complex pathology reports but not short radiology reports. The minimal impact of quantization on accuracy for extraction tasks.

**Medium Confidence**: The specific accuracy thresholds achieved (98% for BT-RADS, 90% for IDH) due to potential overfitting to single-institution data. The generalizability of prompting strategies across different clinical domains and report types.

**Low Confidence**: The relative ranking of specific model families beyond parameter count. The optimal balance between prompt complexity and runtime efficiency. The long-term stability of extracted structured data when applied to evolving clinical documentation practices.

## Next Checks
1. **Cross-Institutional Validation**: Apply the best-performing configuration to radiology and pathology reports from 2-3 different institutions with distinct documentation styles. Compare accuracy drops and identify which report characteristics most affect performance.

2. **Retrieval Parameter Optimization**: Systematically vary RAG parameters including chunk size (50-500 tokens), retrieval algorithms (cosine vs. maximum inner product), and embedding models. Measure how these changes affect extraction accuracy for both short and long reports.

3. **Prompt Ablation Study**: Create controlled experiments that isolate the contribution of prompt complexity, few-shot examples, output formatting, and negative examples. Determine the minimum effective prompt configuration that maintains >95% of maximum accuracy.