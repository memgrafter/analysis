---
ver: rpa2
title: Investigating the Indirect Object Identification circuit in Mamba
arxiv_id: '2407.14008'
source_url: https://arxiv.org/abs/2407.14008
tags:
- layer
- mamba
- name
- token
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether interpretability techniques developed
  for transformers can generalize to Mamba, a new recurrent architecture. The authors
  apply circuit-based mechanistic interpretability to a 370M parameter Mamba model
  on the Indirect Object Identification (IOI) task.
---

# Investigating the Indirect Object Identification circuit in Mamba

## Quick Facts
- arXiv ID: 2407.14008
- Source URL: https://arxiv.org/abs/2407.14008
- Authors: Danielle Ensign; Adrià Garriga-Alonso
- Reference count: 40
- Key outcome: This paper investigates whether interpretability techniques developed for transformers can generalize to Mamba, a new recurrent architecture. The authors apply circuit-based mechanistic interpretability to a 370M parameter Mamba model on the Indirect Object Identification (IOI) task. Key findings include: 1) Layer 39 acts as a critical bottleneck, 2) Convolutions in layer 39 shift name entities one position forward, 3) Name representations are linearly stored in layer 39's state space model, and 4) Layer 39 only writes outputs to the final token position. The authors also adapt positional Edge Attribution Patching to automatically discover circuits in Mamba. These results provide initial evidence that circuit-based interpretability tools can be applied to the Mamba architecture, suggesting potential for broader applicability of these techniques to new model architectures.

## Executive Summary
This paper investigates whether interpretability techniques developed for transformers can generalize to Mamba, a new recurrent architecture. The authors apply circuit-based mechanistic interpretability to a 370M parameter Mamba model on the Indirect Object Identification (IOI) task. Key findings include: 1) Layer 39 acts as a critical bottleneck, 2) Convolutions in layer 39 shift name entities one position forward, 3) Name representations are linearly stored in layer 39's state space model, and 4) Layer 39 only writes outputs to the final token position. The authors also adapt positional Edge Attribution Patching to automatically discover circuits in Mamba. These results provide initial evidence that circuit-based interpretability tools can be applied to the Mamba architecture, suggesting potential for broader applicability of these techniques to new model architectures.

## Method Summary
The authors apply circuit-based mechanistic interpretability techniques to a 370M parameter Mamba model pretrained on The Pile, using the IOI task with four prompt templates containing three names each. They use zero and resample ablation, layer removal, and positional Edge Attribution Patching (EAP) to identify and analyze the IOI circuit in Mamba. The method involves obtaining the Mamba model, implementing the IOI task prompts, applying ablation techniques to identify important components (particularly layer 39), and using positional EAP to automatically discover the circuit and analyze the convolution behavior in layer 39.

## Key Results
- Layer 39 acts as a critical bottleneck in the IOI task, processing name entities and shifting them one position forward via convolution.
- Name representations are linearly stored in layer 39's state space model, with different representations for the first and second time names appear.
- Layer 39 writes outputs only to the final token position, constraining task-relevant information to this location.
- Positional Edge Attribution Patching can identify task-specific circuits in Mamba by providing token-level edge attributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer 39 acts as a critical bottleneck in the IOI task.
- Mechanism: Layer 39 processes name entities and shifts them one position forward via convolution, then stores them linearly in its state space model (SSM).
- Core assumption: The name entities are linearly represented in the SSM of layer 39, with different representations for the first and second time (or sentence) the names appear.
- Evidence anchors:
  - [abstract] "Layer 39 acts as a critical bottleneck, 2) Convolutions in layer 39 shift name entities one position forward, 3) Name representations are linearly stored in layer 39's state space model"
  - [section] "In Figure 6, we see that Layers 39 and 15 appear in every minimal circuit found. Layer 15 seems worthy of investigation in future work, as these two also stand out in EAP. Inspecting the logs, Layer 39 is always the first layer added and has a large effect."
  - [corpus] Weak corpus evidence - only 1 related paper mentions Mamba circuits specifically.
- Break condition: If name entities are not linearly represented in the SSM, or if the convolution does not shift names forward, this mechanism breaks.

### Mechanism 2
- Claim: Layer 39 writes outputs into only the final token position.
- Mechanism: Layer 39 moves task-relevant information into the last token position, and only this position is used for generating the final output.
- Core assumption: The residual stream structure allows layer 39 to write to specific token positions without affecting others.
- Evidence anchors:
  - [abstract] "Layer 39 only writes outputs to the final token position."
  - [section] "In Figure 11 we see that only the last index is used. In addition, positional EAP suggests that other (non-last token) connections are important, as they are preserved in the set of edges that get 85%. However their attribution scores are very low."
  - [corpus] No direct corpus evidence for this specific claim.
- Break condition: If layer 39 writes to multiple token positions or if the final token position is not used for output generation.

### Mechanism 3
- Claim: Positional Edge Attribution Patching (Positional EAP) can identify task-specific circuits in Mamba.
- Mechanism: Positional EAP adapts the standard EAP method to provide token-level edge attributions, allowing for more precise circuit identification.
- Core assumption: The gradient-based approximation used in EAP can be extended to include positional information without significant loss of accuracy.
- Evidence anchors:
  - [abstract] "Finally, we adapt an automatic circuit discovery tool, positional Edge Attribution Patching, to identify a Mamba IOI circuit."
  - [section] "Typically, in EAP, after we compute attri7→j we sum over the L and D dimensions, then take the mean over the B dimension to get an attribution for each edge. Instead, we will just sum over the D dimension and mean over the B dimension, giving us an attribution for every (edge, position)."
  - [corpus] No direct corpus evidence for this specific claim.
- Break condition: If the positional adaptation of EAP does not provide meaningful improvements in circuit identification.

## Foundational Learning

- Concept: Residual stream and layer interactions in Mamba
  - Why needed here: Understanding how layers add to the residual stream and how this affects information flow is crucial for interpreting the circuit analysis results.
  - Quick check question: How does the residual stream structure in Mamba differ from that in Transformers, and why is this important for circuit analysis?

- Concept: State Space Models (SSMs) and their components
  - Why needed here: Layer 39's SSM is central to the name representation and shifting mechanism. Understanding SSM components (A, B, C, D matrices) is essential for interpreting the results.
  - Quick check question: What is the role of each matrix (A, B, C, D) in the SSM, and how do they contribute to the name shifting and representation mechanism?

- Concept: Convolutional operations in neural networks
  - Why needed here: The convolution in layer 39 is responsible for shifting name entities. Understanding how convolutions work and how they can be used for positional manipulation is crucial.
  - Quick check question: How does the convolution operation in layer 39 contribute to the name shifting mechanism, and what would happen if this convolution were removed or altered?

## Architecture Onboarding

- Component map: Input names → Layer 39 convolution (shifts names) → Layer 39 SSM (stores names linearly) → Final token position (outputs answer)

- Critical path: Input names → Layer 39 convolution (shifts names) → Layer 39 SSM (stores names linearly) → Final token position (outputs answer)

- Design tradeoffs:
  - Linear representation of names in SSM vs. more complex representations
  - Single critical bottleneck layer (39) vs. distributed processing
  - Convolution-based name shifting vs. other positional encoding methods

- Failure signatures:
  - Incorrect name shifting (names not moved to correct positions)
  - Non-linear name representations in SSM leading to patching failures
  - Information not properly constrained to final token position

- First 3 experiments:
  1. Verify the name shifting mechanism by examining conv slice activations and their effects on name positions.
  2. Test the linear representation hypothesis by attempting to substitute names in the SSM and observing output changes.
  3. Confirm the final token position constraint by ablating other token positions and measuring impact on accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What circuit mechanisms handle multi-token names in Mamba, given that current analysis only examined single-token names?
- Basis in paper: [inferred] from "We do not yet know why this shifting behavior occurs, and leave that question for future work" and "However, because all of our prompts use single token names, it is possible that these capabilities are simply not needed for this task (but still exist)"
- Why unresolved: Current experiments only used single-token names, leaving the behavior for multi-token names unexplored. The paper notes this as a limitation and future work direction.
- What evidence would resolve it: Testing with multi-token names to observe if the same circuit patterns emerge, or if additional layers are required to handle them.

### Open Question 2
- Question: Why are the first three name positions compatible while the fourth and fifth are not when substituting representations in Layer 39?
- Basis in paper: [explicit] "One hypothesis is that the conv sees a period and encodes that in the name representation" followed by "Thus, some token cross-talk must be happening in a layer before 39"
- Why unresolved: The paper presents a hypothesis about period encoding but dismisses it due to conv limitations, suggesting pre-39 cross-talk without identifying the specific mechanism.
- What evidence would resolve it: Tracing the representations through earlier layers to identify where positional information is encoded or lost, or testing with different sentence structures.

### Open Question 3
- Question: What computation is performed by layers 1, 3, 5, and 9 given their unique connectivity pattern to only embed, layer 0, and layer 39?
- Basis in paper: [explicit] "1, 3, 5, 9 are only connected to 0/embed and 39" and "Of those, 1, 3, 5, 9 are only connected to 0/embed and 39. In particular, we have: 1: missing n1..."
- Why unresolved: The paper identifies these layers have restricted connectivity but doesn't investigate their specific role, noting they "seem to be involved in a complex circuit."
- What evidence would resolve it: Detailed circuit analysis of these layers' inputs and outputs, possibly through ablation studies or representation analysis.

## Limitations

- The analysis focuses on a single task (IOI) and a single model size (370M parameters), which may not generalize to other tasks or larger Mamba models.
- The mechanistic explanations rely on complex interactions between convolution, SSM parameters, and residual streams that may not be fully understood.
- The automatic circuit discovery tool (Positional EAP) shows promise but requires manual intervention for certain steps, suggesting it may not be fully reliable for automated circuit discovery.

## Confidence

**High Confidence:**
- Layer 39 acts as a critical bottleneck in the IOI task
- Layer 39 writes outputs only to the final token position
- The model achieves good performance on the IOI task (83% accuracy)

**Medium Confidence:**
- Name entities are linearly represented in layer 39's SSM
- The convolution in layer 39 shifts name entities one position forward
- Positional EAP can identify task-specific circuits in Mamba

**Low Confidence:**
- The specific mechanistic details of how the convolution and SSM interact
- Whether these findings generalize to other tasks or larger Mamba models
- The complete reliability of the automatic circuit discovery process

## Next Checks

1. **Cross-task validation**: Apply the same interpretability techniques to Mamba on different circuit analysis tasks (e.g., copy tasks, simple arithmetic) to verify whether the identified mechanisms generalize beyond IOI.

2. **Larger model scaling**: Repeat the analysis on larger Mamba models (e.g., 1B or 2B parameters) to determine if the critical layer patterns and mechanisms persist across model scales.

3. **Mechanistic intervention test**: Directly manipulate the convolution operation in layer 39 (e.g., remove it, change its parameters) and measure the impact on IOI performance and name shifting behavior to validate the causal role of this mechanism.