---
ver: rpa2
title: Balancing the Causal Effects in Class-Incremental Learning
arxiv_id: '2402.10063'
source_url: https://arxiv.org/abs/2402.10063
tags:
- learning
- causal
- bace
- classes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a key issue in Class-Incremental Learning
  (CIL) called the imbalanced causal effects problem. When models adapt to new classes,
  the features of old classes are pushed away, causing forgetting.
---

# Balancing the Causal Effects in Class-Incremental Learning

## Quick Facts
- arXiv ID: 2402.10063
- Source URL: https://arxiv.org/abs/2402.10063
- Reference count: 40
- Primary result: Addresses imbalanced causal effects in CIL through balanced causal paths from new and old data to class predictions

## Executive Summary
This paper identifies a fundamental issue in Class-Incremental Learning (CIL) where conflicting causal effects between new and old data lead to catastrophic forgetting. The authors propose BaCE (Balancing the Causal Effects), a method that builds balanced causal paths from both new and old data to predictions of each class. By encouraging mutual adaptation benefits, BaCE improves learning efficiency and reduces forgetting. The method is evaluated across continual image classification, text classification, and named entity recognition tasks, demonstrating superior performance compared to existing CIL methods.

## Method Summary
BaCE addresses the causal imbalance problem in CIL by proposing two objectives: Effectold and Effectnew. Effectold builds causal paths from both new and old data to old class predictions using knowledge distillation, while Effectnew uses K-Nearest Neighbors in the teacher model's feature space to weight the contribution of old data to new class learning. The student model is updated by optimizing the sum of these two effects, with the teacher model updated as a weighted average of the previous teacher and current student. This approach encourages balanced adaptation to all classes using information from both data sources.

## Key Results
- BaCE outperforms existing CIL methods on continual image classification, text classification, and NER tasks
- The method achieves improved accuracy and reduced forgetting compared to baseline approaches
- Performance gains are particularly pronounced on challenging datasets with large domain gaps
- Ablation studies confirm the importance of both Effectold and Effectnew components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal imbalance between new and old data leads to forgetting in CIL.
- Mechanism: New data adapts the model to new classes while pushing old class features away, and vice versa. This creates conflicting causal paths that hinder optimal representation learning.
- Core assumption: The adaptation of new and old classes involves contradictory causal paths in the model's learning process.
- Evidence anchors:
  - [abstract]: "Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes."
  - [section]: "In other words, the adaptation process between new and old classes conflicts from the causal perspective."

### Mechanism 2
- Claim: BaCE balances causal effects by building positive causal paths from both new and old data to each class's prediction.
- Mechanism: Effectold builds paths from both X_old and X_new to Z_new[old] (old class logits), while Effectnew builds paths from both X_old and X_new to Z_new[new] (new class logits).
- Core assumption: By encouraging adaptation to each class with causal effects from both data sources, the model learns a more balanced representation.
- Evidence anchors:
  - [abstract]: "Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively."
  - [section]: "Specifically, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively."

### Mechanism 3
- Claim: Effectnew uses K-Nearest Neighbors in the teacher model's feature space to weight the contribution of old data to new class learning.
- Mechanism: For each new sample, Effectnew computes a weighted sum of its neighbors' predictions, with weights based on their distance in the teacher model's feature space.
- Core assumption: Samples close in the teacher's feature space are likely to share similar prior knowledge and can provide useful information for learning new classes.
- Evidence anchors:
  - [section]: "The latter term in Equation 16 can be regarded as a scaling factor, which is determined by the distance between x(i) and x(k) in the feature space of teacher models."
  - [section]: "Therefore, maximizing Effectnew amounts to minimizing the classification loss of each sample, except that the score is the joint score estimated by itself and its neighbours."

## Foundational Learning

- Concept: Causal inference and causal graphs
  - Why needed here: The paper uses causal graphs to analyze the learning process and identify the conflicting causal effects in CIL.
  - Quick check question: Can you explain how a causal graph differs from a statistical association graph?

- Concept: Class-incremental learning (CIL)
  - Why needed here: CIL is the main problem setting addressed in the paper, where the model needs to learn new classes sequentially without forgetting old ones.
  - Quick check question: What are the key challenges in CIL compared to multi-task learning?

- Concept: Pre-trained models (PTMs) and their properties
  - Why needed here: The paper focuses on CIL with PTMs and leverages their properties (e.g., resistance to forgetting) in the proposed method.
  - Quick check question: Why might PTMs be more resistant to forgetting than models trained from scratch?

## Architecture Onboarding

- Component map:
  - Teacher model: f_{t-1}, trained on previous tasks
  - Student model: f_t, adapted to the new task
  - Effectold: Objective for learning old classes with balanced causal effects
  - Effectnew: Objective for learning new classes with balanced causal effects
  - Buffer: Storage for old representative samples (optional)

- Critical path:
  1. Initialize student model as teacher model
  2. Compute K nearest neighbors of each new sample in teacher's feature space
  3. Compute Effectnew using weighted sum of neighbors' predictions
  4. Compute Effectold using knowledge distillation from teacher to student
  5. Update student model by optimizing Effectnew + Effectold
  6. Update teacher model as a weighted average of previous teacher and current student

- Design tradeoffs:
  - Buffer size vs. performance: Larger buffer reduces forgetting but increases memory usage
  - K (number of neighbors) vs. noise: Larger K captures more context but may introduce noise
  - W0 (weight of self) vs. regularization: Larger W0 emphasizes self-prediction but reduces regularization from neighbors

- Failure signatures:
  - High feature embedding distance between new and old classes
  - Large gap between observed and probing accuracy
  - Degraded performance on old classes after learning new ones

- First 3 experiments:
  1. Ablation study: Compare BaCE with and without Effectold/Effectnew to verify their individual contributions
  2. Hyperparameter sensitivity: Test different values of K, W0, and α to find optimal settings
  3. Challenging datasets: Evaluate on datasets with large domain gaps to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BaCE vary when using different backbone architectures beyond ViT and BERT, such as convolutional neural networks or other transformer variants?
- Basis in paper: [explicit] The paper evaluates BaCE using ViT-B/16 and BERT as backbones, but does not explore other architectures like CNNs or alternative transformer models.
- Why unresolved: The study focuses on pre-trained models like ViT and BERT, leaving open the question of how BaCE performs with different backbone architectures that may have different learning dynamics.
- What evidence would resolve it: Conducting experiments with various backbone architectures, including CNNs and other transformer variants, and comparing their performance with BaCE.

### Open Question 2
- Question: Can BaCE be effectively extended to multi-modal continual learning tasks where data from different modalities (e.g., images, text, audio) are presented sequentially?
- Basis in paper: [inferred] The paper focuses on single-modality tasks (image classification, text classification, NER) and does not address the challenges of multi-modal data in continual learning.
- Why unresolved: Multi-modal continual learning introduces additional complexity due to the need to balance learning across different data types, which is not explored in the current study.
- What evidence would resolve it: Developing and testing BaCE on multi-modal datasets and evaluating its ability to handle diverse data types while maintaining performance across modalities.

### Open Question 3
- Question: How does BaCE perform in scenarios with non-stationary data distributions, where the underlying data distribution changes over time?
- Basis in paper: [inferred] The paper assumes a stationary data distribution within each task but does not address scenarios where the data distribution changes over time, which is common in real-world applications.
- Why unresolved: Non-stationary data distributions can significantly impact the effectiveness of continual learning methods, and BaCE's robustness to such changes is not evaluated.
- What evidence would resolve it: Designing experiments with non-stationary data distributions and assessing BaCE's ability to adapt and maintain performance under these conditions.

## Limitations

- The method's generalizability to different backbone architectures beyond ViT and BERT remains unexplored
- Performance in multi-modal continual learning scenarios with diverse data types is not evaluated
- Robustness to non-stationary data distributions and changing underlying data patterns is not assessed

## Confidence

- Medium confidence in the causal imbalance mechanism due to strong theoretical motivation but limited ablation studies
- High confidence in the effectiveness of Effectold based on standard knowledge distillation principles
- Low confidence in the optimal choice of K and W0 parameters without systematic sensitivity analysis

## Next Checks

1. Conduct systematic ablation studies isolating the contributions of Effectold and Effectnew to quantify their individual impact
2. Perform extensive hyperparameter sensitivity analysis for K, W0, and α across multiple datasets to establish robust parameter guidelines
3. Test the method on non-vision tasks like text classification and NER to verify cross-domain applicability and identify any domain-specific limitations