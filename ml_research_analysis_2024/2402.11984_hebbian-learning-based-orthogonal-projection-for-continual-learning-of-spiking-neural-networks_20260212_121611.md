---
ver: rpa2
title: Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking
  Neural Networks
arxiv_id: '2402.11984'
source_url: https://arxiv.org/abs/2402.11984
tags:
- learning
- hlop
- neural
- spiking
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in spiking neural
  networks (SNNs) by developing a new method called Hebbian learning based orthogonal
  projection (HLOP). The core idea is to use Hebbian learning on recurrent lateral
  connections to extract principal subspaces of neural activities, enabling orthogonal
  projection of activity traces to protect old knowledge during learning new tasks.
---

# Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks

## Quick Facts
- arXiv ID: 2402.11984
- Source URL: https://arxiv.org/abs/2402.11984
- Authors: Mingqing Xiao; Qingyan Meng; Zongpeng Zhang; Di He; Zhouchen Lin
- Reference count: 40
- This paper addresses catastrophic forgetting in spiking neural networks (SNNs) by developing a new method called Hebbian learning based orthogonal projection (HLOP), which achieves nearly zero forgetting and outperforms previous methods with 10-20% accuracy improvements.

## Executive Summary
This paper presents a novel approach called Hebbian Learning based Orthogonal Projection (HLOP) for continual learning in spiking neural networks. The method addresses catastrophic forgetting by using Hebbian learning on recurrent lateral connections to extract principal subspaces of neural activities, enabling orthogonal projection of activity traces to protect old knowledge during learning new tasks. HLOP can be flexibly combined with various SNN training approaches and is compatible with biologically plausible error propagation methods. Experimental results demonstrate that HLOP achieves nearly zero forgetting across multiple datasets and outperforms previous methods significantly.

## Method Summary
The method introduces lateral recurrent connections with skew-symmetric weights that propagate neural activities to subspace neurons and back, implementing orthogonal projection through neural computation. Hebbian learning rules on these lateral connections extract principal subspaces of neural activities, which are then used to modify presynaptic activity traces before synaptic weight updates. This modification ensures weight updates are orthogonal to previously learned task subspaces, preventing interference with old knowledge. The approach is compatible with various SNN training methods including DSR, BPTT with SG, and OTTT, and can work with biologically plausible error propagation methods like feedback alignment.

## Key Results
- Achieves nearly zero forgetting across multiple continual learning settings
- Outperforms previous methods with accuracy improvements of 10-20% on several datasets
- Maintains compatibility with various SNN training approaches and biologically plausible error propagation methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hebbian learning in lateral connections extracts principal subspaces of neural activity that enable orthogonal projection of activity traces.
- **Mechanism**: Recurrent lateral connections with skew-symmetric weights propagate neural activities to subspace neurons (y = Hx), then back through −H⊤ to generate orthogonalized responses (x− = −H⊤y). This implements the projection matrix P = I − M⊤M using only neural computations.
- **Core assumption**: The Hebbian learning rule on lateral connections converges to capture the principal subspace of neural activities from streaming data.
- **Evidence anchors**:
  - [abstract]: "Hebbian learning on recurrent lateral connections to extract principal subspaces of neural activities, enabling orthogonal projection of activity traces"
  - [section]: "Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection"
  - [corpus]: **Weak** - No direct corpus evidence found for Hebbian learning enabling orthogonal projection in spiking networks.
- **Break condition**: If lateral connections cannot capture the principal subspace (e.g., due to insufficient neurons or improper learning rate), the projection fails and forgetting resumes.

### Mechanism 2
- **Claim**: Modifying presynaptic activity traces with lateral circuit responses creates projected traces that preserve old knowledge during new task learning.
- **Mechanism**: The activity trace x is updated to ˆx = x + x− = x − H⊤Hx. Synaptic weight updates use these modified traces (∆W = δˆx⊤) instead of original traces, ensuring updates are orthogonal to previous task subspaces.
- **Core assumption**: Weight updates based on projected traces do not interfere with previously learned task representations.
- **Evidence anchors**:
  - [abstract]: "project activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks"
  - [section]: "project activity traces for synaptic weight update into an orthogonal subspace so that the learned abilities are not influenced much when learning new knowledge"
  - [corpus]: **Missing** - No corpus evidence found for activity trace modification preserving knowledge.
- **Break condition**: If the modified trace ˆx is not sufficiently orthogonal to old task subspaces, interference occurs and forgetting reappears.

### Mechanism 3
- **Claim**: The method is compatible with various SNN training approaches and biologically plausible error propagation methods.
- **Mechanism**: HLOP modifies presynaptic traces regardless of how those traces are defined (weighted firing rates, spikes, eligibility traces). This flexibility allows integration with DSR, BPTT with SG, OTTT, and methods using feedback alignment or sign-symmetric error propagation.
- **Core assumption**: All compatible training methods rely on presynaptic activity traces for weight updates, making them amenable to trace modification.
- **Evidence anchors**:
  - [abstract]: "Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces"
  - [section]: "Our method is also flexible to utilize arbitrary training methods based on presynaptic activity traces/traces"
  - [corpus]: **Weak** - No direct corpus evidence found for compatibility with biologically plausible error propagation methods.
- **Break condition**: If a training method does not use presynaptic traces (e.g., pure weight-based methods), HLOP cannot be applied.

## Foundational Learning

- **Concept**: Principal Component Analysis (PCA) and subspace extraction
  - Why needed here: HLOP relies on extracting principal subspaces of neural activities through Hebbian learning, which is mathematically equivalent to PCA
  - Quick check question: Can you explain how Oja's rule performs PCA on streaming data and converges to the principal eigenvector?

- **Concept**: Orthogonal projection in high-dimensional spaces
  - Why needed here: The core mechanism protects old knowledge by projecting activity traces into subspaces orthogonal to previously learned task subspaces
  - Quick check question: Given a matrix A whose columns span the principal subspace, can you derive the projection matrix P = I − A(A⊤A + αI)−1A⊤ that projects onto the orthogonal complement?

- **Concept**: Hebbian learning rules and their convergence properties
  - Why needed here: The lateral connections are updated using Hebbian/anti-Hebbian rules that must converge to capture principal components
  - Quick check question: What is the difference between Oja's rule and the subspace algorithm for extracting multiple principal components, and why does the latter work for HLOP?

## Architecture Onboarding

- **Component map**: Feedforward SNN layers -> Lateral recurrent connections -> Subspace neurons -> Hebbian learning module -> Activity trace modification module -> Weight update
- **Critical path**: Neural activity → lateral propagation → subspace response → activity trace modification → weight update
- **Design tradeoffs**: 
  - More subspace neurons provide better subspace approximation but increase computational cost
  - Linear vs. spiking subspace neurons: linear is more accurate but may require hybrid hardware; spiking is more neuromorphic-compatible but introduces quantization
  - Online Hebbian learning vs. batch learning: online is more neuromorphic but may have slower convergence
- **Failure signatures**:
  - Forgetting reappears despite HLOP: indicates poor subspace extraction or insufficient subspace neurons
  - Training becomes unstable: suggests learning rate for Hebbian updates is too high
  - Performance degrades over tasks: indicates subspace neurons cannot capture new principal components effectively
- **First 3 experiments**:
  1. Implement HLOP on a simple fully connected SNN with DSR training on permuted MNIST, verify zero forgetting compared to baseline
  2. Test HLOP with lateral spiking neurons (quantized version) on the same task, compare performance with different time steps for lateral neurons
  3. Evaluate HLOP with biologically plausible error propagation (feedback alignment) on a deeper network, verify compatibility and performance maintenance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of HLOP compare to other continual learning methods when applied to class-incremental learning settings?
  - Basis in paper: [inferred] The paper discusses task- and domain-incremental settings, but acknowledges that class-incremental learning inevitably requires replay of old experiences for good performance. It suggests that HLOP could potentially be combined with replay methods or context-dependent processing modules for class-incremental tasks.
  - Why unresolved: The paper does not provide experimental results or a detailed analysis of HLOP's performance in class-incremental learning settings.
  - What evidence would resolve it: Experimental results comparing HLOP to other methods in class-incremental learning settings, potentially with different combinations of HLOP and replay methods or context-dependent processing modules.

- **Open Question 2**: What is the impact of the number of subspace neurons on the performance of HLOP, and can this number be determined automatically or adaptively?
  - Basis in paper: [explicit] The paper mentions that the number of subspace neurons is currently a hyperparameter for HLOP, which is similar to the threshold hyperparameter in GPM. It suggests that the number may be determined with prior knowledge about task difficulty, but also mentions that it can be interesting future work to study how to automatically and adaptively determine the allocation.
  - Why unresolved: The paper does not provide a detailed analysis of how the number of subspace neurons affects performance, nor does it propose a method for automatic or adaptive determination of this hyperparameter.
  - What evidence would resolve it: Experimental results showing the impact of different numbers of subspace neurons on HLOP's performance, and a proposed method for automatically or adaptively determining the optimal number of subspace neurons.

- **Open Question 3**: How does the performance of HLOP with lateral spiking neurons compare to the original HLOP with linear neurons, and what is the impact of the time steps for discrete simulation of bursts on the performance?
  - Basis in paper: [explicit] The paper proposes HLOP with lateral spiking neurons based on the rate coding of high-frequency bursts and provides experimental results comparing the performance of HLOP with lateral spiking neurons to the original HLOP with linear neurons. It also discusses the impact of different time steps for discrete simulation of bursts on the performance.
  - Why unresolved: While the paper provides some experimental results, it does not provide a detailed analysis of the performance difference between HLOP with lateral spiking neurons and the original HLOP with linear neurons, nor does it discuss the optimal time steps for discrete simulation of bursts.
  - What evidence would resolve it: A detailed analysis of the performance difference between HLOP with lateral spiking neurons and the original HLOP with linear neurons, and experimental results showing the impact of different time steps for discrete simulation of bursts on the performance.

## Limitations

- The paper does not provide detailed ablation studies on the number of subspace neurons or their impact on performance
- The convergence properties of the Hebbian learning rule under different data distributions are not thoroughly examined
- The method's robustness to noise and its energy efficiency in practical neuromorphic hardware implementations are not addressed

## Confidence

**High confidence**: The experimental results demonstrating HLOP's effectiveness in reducing catastrophic forgetting across multiple datasets and training methods are well-supported by the reported quantitative metrics (accuracy, BWT). The compatibility claims with various SNN training approaches appear substantiated by the experimental design.

**Medium confidence**: The biological plausibility of the mechanism and its computational efficiency claims are reasonable but lack direct experimental validation. The mathematical formulation of the orthogonal projection is sound, but the practical implementation details and parameter sensitivity are not fully explored.

**Low confidence**: The scalability claims to very large-scale neuromorphic systems and the precise computational overhead remain unclear. The method's behavior with highly non-stationary data distributions and its long-term stability over many sequential tasks need further investigation.

## Next Checks

1. Perform systematic ablation studies varying the number of subspace neurons and their time constants to determine optimal configurations
2. Test HLOP's performance with non-stationary data distributions and gradual domain shifts to evaluate robustness
3. Implement a scaled-down version on neuromorphic hardware to measure actual energy efficiency and computational overhead compared to baseline methods