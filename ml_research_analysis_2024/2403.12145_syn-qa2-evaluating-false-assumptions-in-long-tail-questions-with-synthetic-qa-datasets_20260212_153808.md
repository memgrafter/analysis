---
ver: rpa2
title: 'Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic
  QA Datasets'
arxiv_id: '2403.12145'
source_url: https://arxiv.org/abs/2403.12145
tags:
- 'false'
- questions
- question
- answer
- assumptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Syn-QA2, a synthetic dataset to evaluate
  large language models on detecting false assumptions in questions. The dataset is
  generated using similarity-based entity perturbation on Wikidata and HotpotQA, creating
  minimal pairs of questions with and without false assumptions.
---

# Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets

## Quick Facts
- arXiv ID: 2403.12145
- Source URL: https://arxiv.org/abs/2403.12145
- Authors: Ashwin Daswani; Rohan Sawant; Najoung Kim
- Reference count: 10
- Primary result: Current LLMs achieve only 62-67% accuracy on detecting false assumptions in synthetic long-tail questions

## Executive Summary
This paper introduces Syn-QA2, a synthetic dataset designed to evaluate large language models' ability to detect false assumptions in questions. The dataset uses similarity-based entity perturbation on Wikidata and HotpotQA to create minimal pairs of questions with and without false assumptions. The authors find that false assumption detection remains challenging for current LLMs, with detection tasks proving more difficult than generative QA itself, particularly for long-tail synthetic questions compared to naturally occurring ones. The best-performing models achieve around 62-67% detection accuracy, and FreshPrompt search-engine augmentation provides nontrivial gains.

## Method Summary
The authors generate synthetic QA datasets using similarity-based entity perturbation on Wikidata relations (single-hop) and HotpotQA distractors (multi-hop). They create minimal pairs by replacing entities with similar ones based on shared Wikidata properties, then manually verify the generated questions. The evaluation involves multiple LLMs tested on binary false assumption detection using zero-shot, few-shot, and chain-of-thought prompting, with manual acceptability annotation for generative QA.

## Key Results
- Current LLMs achieve only 62-67% accuracy on detecting false assumptions in synthetic questions
- Detection tasks are more difficult than generative QA, even when models answer generative questions correctly
- Long-tail synthetic questions are harder to detect false assumptions in compared to naturally occurring questions
- FreshPrompt search-engine augmentation provides nontrivial gains but is costly
- Flan-T5-XXL fails on nested question structure, always answering "No"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic datasets with minimal pairs allow isolated evaluation of false assumption detection without confounding linguistic variation
- Mechanism: By generating questions that differ only in one entity mention, the method ensures that any performance difference is attributable to the presence or absence of the false assumption, not to phrasing or style differences
- Core assumption: Entity perturbation based on Wikidata property overlap preserves semantic plausibility while introducing controlled false assumptions
- Evidence anchors:
  - [abstract] "Our main focus lies on open-domain questions rather than on reading comprehension questions."
  - [section] "Furthermore, this approach enables a minimal pair comparison between questions with and without false assumptions (see Figure 1), allowing us to quantify the effect of false assumptions without the effect of additional variations such as the phrasing and style of the questions."

### Mechanism 2
- Claim: Long-tail synthetic questions are harder for models to detect false assumptions in compared to naturally occurring questions
- Mechanism: Models are exposed to naturally occurring false assumptions during pretraining and fine-tuning, but the long-tail synthetic distribution represents questions that were unlikely to appear in training data, leading to distribution shift and reduced detection accuracy
- Core assumption: The pretraining corpus contains a higher proportion of naturally occurring false assumptions than the long-tail synthetic distribution
- Evidence anchors:
  - [abstract] "(3) the detection task is more challenging with long-tail questions compared to naturally occurring questions, highlighting the utility of our synthetic datasets and generation method."
  - [section] "GPT-4 (that did not show trivially skewed answer patterns) showed better performance on naturally occurring questions, suggesting that long-tail questions with false assumptions do impose additional challenges."

### Mechanism 3
- Claim: Binary false assumption detection is more difficult than generative QA even when models answer correctly on the generative task
- Mechanism: The detection task requires models to embed a meta-question ("Does this question contain a false assumption?") within the original question, creating a nested linguistic structure that is underrepresented in training data compared to direct information-seeking questions
- Core assumption: Training data contains significantly more information-seeking questions than questions about the validity of other questions
- Evidence anchors:
  - [abstract] "(2) the binary detection task is challenging even compared to the difficulty of generative QA itself, possibly due to the linguistic structure of the problem: it embeds a question inside a question"
  - [section] "This hypothesis is supported by the behavior of Flan-T5-XXL, where it completely fails to handle even trivial questions of this structure, always answering 'No' to questions such as 'Is {Q} a question?'"

## Foundational Learning

- Concept: Entity similarity based on property overlap
  - Why needed here: The synthetic dataset generation relies on finding entities with similar Wikidata properties to create plausible but false assumptions
  - Quick check question: If entity A has properties (occupation: actor, born: Mumbai) and entity B has properties (occupation: actor, born: Delhi), would they be considered similar for replacement purposes?

- Concept: Minimal pair experimental design
  - Why needed here: The evaluation methodology depends on comparing model performance on question pairs that differ only in the presence of a false assumption
  - Quick check question: What would be the key difference between a minimal pair evaluation and comparing questions with different phrasings but the same underlying content?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper evaluates whether explicit reasoning steps improve false assumption detection performance
  - Quick check question: How does chain-of-thought prompting differ from standard few-shot prompting in terms of the model's generation process?

## Architecture Onboarding

- Component map: Data generation pipeline → Synthetic dataset → Model evaluation framework → Performance analysis
- Critical path: Synthetic dataset generation → Model evaluation → Results analysis → Dataset release
- Design tradeoffs: Synthetic generation allows for controlled evaluation but may introduce artifacts not present in natural questions; manual verification ensures quality but limits dataset scale
- Failure signatures: Models showing strong response bias (always answering "Yes" or "No"), poor performance on multi-hop questions compared to single-hop, failure to handle nested question structures
- First 3 experiments:
  1. Evaluate model performance on minimal pairs from the single-hop dataset to establish baseline detection accuracy
  2. Compare detection accuracy on synthetic vs. naturally occurring false assumptions using GPT-4 on both datasets
  3. Test whether chain-of-thought prompting improves detection accuracy compared to standard few-shot prompting on the multi-hop dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but the following arise from the limitations and findings:

### Open Question 1
- Question: What specific linguistic structures in questions with false assumptions make them particularly challenging for LLMs to detect?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the detection task involves "handling of expressions where there is a question inside a question" but doesn't systematically investigate which linguistic features are most problematic
- What evidence would resolve it: Detailed linguistic analysis of model errors on different question structures, ablation studies on specific linguistic features, or controlled experiments varying question syntax while keeping semantic content constant

### Open Question 2
- Question: How does the difficulty of detecting false assumptions in synthetic long-tail questions compare to naturally occurring ones across different types of knowledge domains?
- Basis in paper: Inferred from comparison between synthetic and natural questions
- Why unresolved: The paper only compares overall performance but doesn't analyze domain-specific variations in detection difficulty
- What evidence would resolve it: Domain-by-domain breakdown of detection accuracy for both synthetic and natural questions, identifying which domains pose the greatest challenges

### Open Question 3
- Question: What is the optimal threshold for entity similarity (θ) in the synthetic data generation process to maximize the difficulty and usefulness of the dataset?
- Basis in paper: Mentioned as θ = 3 in the generation process
- Why unresolved: The paper uses a fixed threshold without exploring how different values affect dataset quality or model performance
- What evidence would resolve it: Systematic experiments varying θ, analyzing the relationship between similarity threshold and detection difficulty, and evaluating downstream model robustness

### Open Question 4
- Question: How do different search engine augmentation strategies compare to FreshPrompt in improving false assumption detection?
- Basis in paper: FreshPrompt showed nontrivial gains but is costly
- Why unresolved: Only one search-augmented approach was evaluated, and its high cost was noted as a limitation
- What evidence would resolve it: Comparative evaluation of multiple search-augmentation methods, cost-benefit analysis, and exploration of lightweight alternatives

### Open Question 5
- Question: Can models be trained to better handle nested question structures to improve false assumption detection performance?
- Basis in paper: Inferred from observation that detection is harder than generative QA due to question-inside-question structure
- Why unresolved: The paper only evaluates pre-trained LLMs without exploring training strategies for this specific challenge
- What evidence would resolve it: Experiments with models trained on synthetic nested questions, fine-tuning approaches targeting detection-specific features, and analysis of whether training data can be augmented with more nested question structures

## Limitations

- Synthetic dataset generation relies on similarity-based entity perturbation with partially specified templates and thresholds
- Evaluation focuses primarily on detection accuracy without extensive error analysis of model failures
- FreshPrompt augmentation shows gains but is costly, limiting practical applicability

## Confidence

- **High Confidence**: The core finding that false assumption detection remains challenging for current LLMs is well-supported by consistent performance across multiple models and datasets
- **Medium Confidence**: The comparative difficulty of long-tail vs. naturally occurring questions is supported but could benefit from more extensive testing across additional model families
- **Medium Confidence**: The claim about binary detection being harder than generative QA is plausible given the nested question structure but requires more direct controlled comparisons

## Next Checks

1. **Template Verification**: Replicate dataset generation using the described similarity-based entity perturbation approach with different similarity thresholds to test robustness of the synthetic dataset quality

2. **Extended Model Testing**: Evaluate additional model architectures (particularly smaller models and those with different pretraining objectives) to determine if the difficulty gap between detection and generative QA generalizes

3. **Error Analysis**: Conduct detailed error analysis on model failures, particularly focusing on whether errors stem from misunderstanding the nested question structure versus entity recognition issues