---
ver: rpa2
title: 'TABCF: Counterfactual Explanations for Tabular Data Using a Transformer-Based
  VAE'
arxiv_id: '2410.10463'
source_url: https://arxiv.org/abs/2410.10463
tags:
- features
- data
- tabcf
- feature
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TABCF, a novel method for generating counterfactual
  explanations for tabular data using a transformer-based VAE. TABCF addresses the
  issue of feature-type bias in existing methods by leveraging transformers to capture
  complex feature interdependencies and employing a Gumbel-Softmax detokenizer for
  precise categorical reconstruction.
---

# TABCF: Counterfactual Explanations for Tabular Data Using a Transformer-Based VAE

## Quick Facts
- arXiv ID: 2410.10463
- Source URL: https://arxiv.org/abs/2410.10463
- Authors: Emmanouil Panagiotou; Manuel Heurich; Tim Landgraf; Eirini Ntoutsi
- Reference count: 33
- One-line primary result: TABCF achieves a validity score of 0.99, indicating successful counterfactual generation for 99% of input instances, while maintaining competitive proximity and sparsity metrics.

## Executive Summary
This paper introduces TABCF, a novel method for generating counterfactual explanations for tabular data using a transformer-based VAE. TABCF addresses the issue of feature-type bias in existing methods by leveraging transformers to capture complex feature interdependencies and employing a Gumbel-Softmax detokenizer for precise categorical reconstruction. The method optimizes counterfactuals in the latent space using a combined loss function that considers validity, input proximity, and latent proximity. Experimental results on five financial datasets demonstrate that TABCF outperforms existing methods in producing valid, proximal, and sparse counterfactuals without exhibiting feature-type bias.

## Method Summary
TABCF uses a transformer-based VAE to learn a continuous latent space for counterfactual generation. The input features are first tokenized using learnable tokenizers (linear layer for numerical, lookup tables for categorical). The tokenized input is then encoded using a transformer encoder, which captures feature interdependencies. The VAE sampling process generates a latent vector z, which is decoded using a transformer decoder and a Gumbel-Softmax detokenizer to reconstruct the input. The reconstructed input is then fed to a black-box model, and the counterfactuals are optimized in the latent space using a combined loss function that considers validity, input proximity, and latent proximity.

## Key Results
- TABCF achieves a validity score of 0.99, indicating successful counterfactual generation for 99% of input instances.
- The method outperforms existing methods in producing valid, proximal, and sparse counterfactuals without exhibiting feature-type bias.
- Experimental results on five financial datasets demonstrate the effectiveness of TABCF in generating high-quality counterfactual explanations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Gumbel-Softmax detokenizer preserves end-to-end differentiability while enforcing one-hot categorical constraints.
- Mechanism: During reconstruction, the Gumbel-Softmax function transforms logits into discrete-like one-hot vectors, but retains a continuous approximation (the "soft" samples) for gradient flow. This avoids the need for post-processing discretization steps that introduce bias.
- Core assumption: The soft Gumbel samples provide sufficiently smooth gradients to allow optimization in the latent space without degrading categorical reconstruction quality.
- Evidence anchors:
  - [section]: "Our decoder architecture enables precise categorical reconstruction via the Gumbel-Softmax trick [16], while being fully differentiable to ensure optimal gradient flow from the black-box model."
  - [abstract]: "Our approach uses transformers to learn a continuous latent space and a novel Gumbel-Softmax detokenizer that enables precise categorical reconstruction while preserving end-to-end differentiability."
  - [corpus]: Weak ‚Äì no directly comparable paper was found in the neighbor set that specifically discusses Gumbel-Softmax detokenizer for counterfactual explanations in tabular data.
- Break condition: If the temperature hyperparameter ùúè is too high, the "soft" samples may become too uniform, leading to vanishing gradients; if too low, the samples become nearly discrete and the model may not escape local minima.

### Mechanism 2
- Claim: Transformer-based encoders capture complex feature interdependencies, reducing feature-type bias compared to MLP encoders.
- Mechanism: The self-attention layers in the transformer can model pairwise interactions across all features simultaneously, allowing the latent space to reflect real dependencies between numerical and categorical variables. This makes counterfactual generation less likely to default to changing only numerical features.
- Core assumption: The tabular data exhibits non-trivial interdependencies between feature types that can be better captured by attention-based architectures than by standard fully connected layers.
- Evidence anchors:
  - [abstract]: "Our approach uses transformers to learn a continuous latent space and a novel Gumbel-Softmax detokenizer that enables precise categorical reconstruction while preserving end-to-end differentiability."
  - [section]: "Our approach maps the mixed input into a unified continuous latent space, leveraging transformers to capture the rich feature interdependencies."
  - [corpus]: Assumption ‚Äì the neighbor set includes a paper on transformer placement in VAEs for tabular data, but does not explicitly connect transformers to reduced feature-type bias.
- Break condition: If the tabular data is mostly independent or sparse in interactions, the added complexity of transformers may not improve over MLPs and could overfit.

### Mechanism 3
- Claim: Optimizing in the latent space with combined proximity losses (input and latent) yields more valid and proximal counterfactuals than input-space optimization.
- Mechanism: The combined loss balances validity (hinge loss on the black-box prediction), input proximity (L1 distance in the original space), and latent proximity (L2 distance in the latent space). This multi-space optimization keeps counterfactuals realistic and close to the original instance.
- Core assumption: The VAE's latent space is structured enough that gradient-based traversal can reliably find valid counterfactuals without diverging too far from the original instance.
- Evidence anchors:
  - [section]: "We optimize for important CF desiderata [26], such as validity, but also, proximity to the original instance, and feature sparsity, for producing more actionable CFs."
  - [abstract]: "Experimental results on five financial datasets demonstrate that TABCF outperforms existing methods in producing valid, proximal, and sparse counterfactuals without exhibiting feature-type bias."
  - [corpus]: Weak ‚Äì no direct neighbor evidence, but the neighbor set includes a paper on impact of ML uncertainty on counterfactual robustness, implying relevance of loss design to validity.
- Break condition: If the latent space is too smooth or the VAE underfits, the optimization may not find valid counterfactuals; if too sharp, the gradient steps may overshoot valid regions.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) basics and the reparameterization trick
  - Why needed here: TABCF uses a transformer-based VAE to learn a continuous latent space for counterfactual generation; understanding how the encoder outputs mean and log-variance, and how latent vectors are sampled, is essential to follow the method.
  - Quick check question: In a VAE, how is the latent vector z constructed from the encoder's mean and log-variance outputs?
- Concept: Gumbel-Softmax distribution and the temperature hyperparameter
  - Why needed here: The Gumbel-Softmax trick is used to generate differentiable one-hot-like categorical samples; knowing how the temperature ùúè affects the discreteness of the output is key to tuning the model.
  - Quick check question: What happens to the Gumbel-Softmax output distribution as the temperature ùúè approaches zero?
- Concept: Transformer self-attention and tokenization for tabular data
  - Why needed here: TABCF uses learnable tokenizers and transformer layers to process mixed-type tabular data; understanding how features are embedded and how attention operates across them is critical to grasp the architecture.
  - Quick check question: In the TABCF tokenizer, how are numerical and categorical features transformed into token embeddings?

## Architecture Onboarding

- Component map: Learnable feature tokenizer (numerical: linear layer; categorical: lookup tables) -> Transformer encoder (captures feature interdependencies) -> VAE sampling (mean, log-variance -> latent z) -> Transformer decoder (inverts tokenization) -> Gumbel-Softmax detokenizer (reconstructs one-hot categoricals) -> Black-box model (takes reconstructions as input) -> Loss function (validity + input proximity + latent proximity)
- Critical path: 1. Input -> tokenizer -> transformer encoder -> latent z 2. Latent z -> transformer decoder -> Gumbel detokenizer -> reconstruction 3. Reconstruction -> black-box model -> validity loss 4. Loss backprop -> update latent z (gradient descent)
- Design tradeoffs: Using transformers vs MLPs: more expressive but higher compute and risk of overfitting; Gumbel-Softmax vs discretization: avoids feature-type bias but introduces temperature tuning; Combined proximity losses: better counterfactual quality but more hyperparameters
- Failure signatures: Validity stays low: latent space may not align well with black-box decision boundary; check VAE reconstruction quality; Feature-type bias persists: Gumbel-Softmax may be too "soft"; try lowering ùúè or adding regularization; Training instability: check for exploding gradients in transformer; consider gradient clipping
- First 3 experiments: 1. Train VAE on tabular data with only MLP encoder/decoder, measure reconstruction and validity; establish baseline. 2. Replace MLP with transformer encoder, keep decoder MLP; compare reconstruction, validity, and feature-type bias. 3. Add Gumbel-Softmax detokenizer and test counterfactual generation for bias reduction; compare with discretization baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TABCF compare to counterfactual generation methods that incorporate causal constraints?
- Basis in paper: [explicit] The paper mentions that some methods account for causal constraints [17, 23] but does not compare TABCF against such methods.
- Why unresolved: The experimental evaluation only compares TABCF to baseline methods that do not explicitly model causal relationships between features.
- What evidence would resolve it: An experimental comparison of TABCF against methods that incorporate causal constraints on datasets where causal relationships are known.

### Open Question 2
- Question: How does the choice of VAE architecture (e.g., different numbers of transformer layers or latent dimensions) affect the quality of counterfactual explanations?
- Basis in paper: [inferred] The paper uses a specific transformer-based VAE architecture but does not explore how architectural choices impact performance.
- Why unresolved: The experimental evaluation uses a fixed VAE architecture without exploring the impact of architectural hyperparameters.
- What evidence would resolve it: A systematic ablation study varying the VAE architecture (number of layers, latent dimensions, etc.) and measuring the impact on counterfactual quality metrics.

### Open Question 3
- Question: How does TABCF perform on tabular datasets with very high-dimensional categorical features or datasets with a large number of features overall?
- Basis in paper: [inferred] The paper evaluates on datasets with a range of feature counts but does not specifically test performance on extremely high-dimensional data.
- Why unresolved: The largest dataset in the evaluation has 23 features (14 numerical + 9 categorical), which may not represent the challenges of very high-dimensional tabular data.
- What evidence would resolve it: Evaluation of TABCF on benchmark datasets with hundreds or thousands of features, measuring scalability and performance degradation.

## Limitations
- Architectural specifications, particularly transformer encoder and decoder configurations, are not fully detailed, making exact replication challenging.
- The temperature hyperparameter œÑ for the Gumbel-Softmax detokenizer is mentioned but without guidance on optimal values or sensitivity analysis.
- Evaluation does not include comparisons with methods specifically designed to address feature-type bias, limiting the strength of superiority claims.

## Confidence
- High Confidence: The core methodology of using transformers to capture feature interdependencies and the combined loss function for counterfactual optimization is well-founded and aligns with established VAE and counterfactual explanation literature.
- Medium Confidence: The specific implementation details, particularly the transformer architecture and Gumbel-Softmax detokenizer integration, are plausible but lack sufficient detail for complete verification.
- Low Confidence: The claim of outperforming existing methods without exhibiting feature-type bias is based on limited comparisons and would benefit from more rigorous ablation studies and comparisons with bias-specific approaches.

## Next Checks
1. **Ablation Study**: Conduct experiments comparing TABCF with and without transformers (using MLPs instead) and with different detokenizer approaches (Gumbel-Softmax vs. discretization) to isolate the impact of each component on feature-type bias reduction.
2. **Hyperparameter Sensitivity Analysis**: Systematically vary the Gumbel-Softmax temperature œÑ and other key hyperparameters to understand their impact on counterfactual validity, proximity, and sparsity, providing guidance for model tuning.
3. **Extended Comparison**: Evaluate TABCF against a broader set of counterfactual explanation methods, including those specifically designed to address feature-type bias, on the five financial datasets and additional tabular datasets to strengthen the claim of superior performance.