---
ver: rpa2
title: Code Pretraining Improves Entity Tracking Abilities of Language Models
arxiv_id: '2405.21068'
source_url: https://arxiv.org/abs/2405.21068
tags:
- code
- llama
- contains
- math
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates whether pretraining language\
  \ models on code, math, or alignment tuning improves their entity tracking performance\u2014\
  the ability to track state changes of discourse entities. By comparing pairs of\
  \ base models and models trained on additional code, math, or alignment-tuned data,\
  \ the authors find that models with additional code training consistently outperform\
  \ their base counterparts across various model families and sizes."
---

# Code Pretraining Improves Entity Tracking Abilities of Language Models

## Quick Facts
- arXiv ID: 2405.21068
- Source URL: https://arxiv.org/abs/2405.21068
- Authors: Najoung Kim; Sebastian Schuster; Shubham Toshniwal
- Reference count: 29
- Primary result: Code pretraining consistently improves entity tracking performance across model families and sizes

## Executive Summary
This study systematically evaluates whether pretraining language models on code, math, or alignment tuning improves their entity tracking performance—the ability to track state changes of discourse entities. By comparing pairs of base models and models trained on additional code, math, or alignment-tuned data, the authors find that models with additional code training consistently outperform their base counterparts across various model families and sizes. In contrast, additional math training yields only marginal improvements, and alignment tuning shows mixed effects—improving base models but not consistently enhancing code-trained models. The best performance is achieved by combining code and instruction tuning. The results suggest that code pretraining is particularly beneficial for developing reasoning abilities in language models, though further controlled experiments are needed to address potential confounds.

## Method Summary
The study compares base models with their counterparts that received additional pretraining on code, math, or alignment data. Using a 7-box entity tracking task where models must track the contents of boxes through various operations, the researchers evaluate performance across different numbers of operations (0-7). They test multiple model families including Llama 2/Code Llama, DeepSeek/DeepSeek-Coder, and various math-focused models, comparing their performance on entity tracking tasks with 2-shot prompts.

## Key Results
- Models with additional code training consistently outperform their base counterparts on entity tracking tasks
- Additional math training provides only marginal improvements over base models
- Alignment tuning improves base models but doesn't consistently enhance code-trained models
- Combining code pretraining with instruction tuning yields the best overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code pretraining improves entity tracking by teaching models to manage variable states through structured procedural input.
- Mechanism: Code contains explicit state changes and variable assignments that require tracking. When models learn these patterns during pretraining, they develop the ability to track similar state changes in natural language descriptions.
- Core assumption: Variable state management in code directly transfers to entity state tracking in natural language.
- Evidence anchors:
  - [abstract] "Kim & Schuster (2023) hypothesized that pretraining on large amounts of code imbues LLMs with entity tracking abilities"
  - [section] "Kim & Schuster (2023) argued that keeping track of the states of variables is important for producing correct code, and hypothesized that this kind of procedural input may provide a stronger training signal than pure natural language text"
  - [corpus] Weak - related papers focus on code pretraining benefits but don't directly address the variable state transfer mechanism
- Break condition: If variable assignment patterns in code don't align with entity state change patterns in natural language, the transfer won't occur.

### Mechanism 2
- Claim: Code pretraining improves reasoning abilities that generalize to entity tracking tasks.
- Mechanism: Code requires logical reasoning and following multi-step procedures. Models trained on code develop better reasoning capabilities that help them understand and track entity state changes in complex instructions.
- Core assumption: Reasoning improvements from code pretraining transfer to natural language entity tracking tasks.
- Evidence anchors:
  - [abstract] "Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language"
  - [section] "pretraining on code improves LLM performance on reasoning tasks, including commonsense reasoning (Madaan et al., 2022), chain-of-thought reasoning (Wei et al., 2022b), mathematical problems (Razeghi et al., 2024), and entity tracking tasks (Muennighoff et al., 2023)"
  - [corpus] Moderate - related papers show code pretraining benefits reasoning but don't specifically connect to entity tracking
- Break condition: If reasoning improvements don't transfer from code to natural language contexts.

### Mechanism 3
- Claim: Scale matters - larger models benefit more from code pretraining for entity tracking.
- Mechanism: Larger models have more capacity to learn and store the complex patterns required for both code understanding and entity tracking. The benefits of code pretraining compound with model size.
- Core assumption: Model capacity is the limiting factor in learning entity tracking from code.
- Evidence anchors:
  - [abstract] "In these model comparisons, the models trained on code consistently outperformed the base models on the nontrivial cases of entity tracking (number of operations affecting box state ≥ 1). In the case of 13B models, a boost in trivial cases is also observed (number of operations = 0); in 70B models, performance on the trivial cases is already saturated in the base model"
  - [section] "In Llama 2 7B models, the gains through additional code training are relatively minor, with most of the gains deriving from boosts in examples where the number of operations is either 0 or 1"
  - [corpus] Weak - no direct evidence about scale effects in related papers
- Break condition: If smaller models can achieve similar entity tracking performance with sufficient code training.

## Foundational Learning

- Concept: State tracking and variable assignment
  - Why needed here: Entity tracking fundamentally involves tracking the state of entities across multiple operations, similar to tracking variable states in code
  - Quick check question: Can you explain how a variable assignment in code relates to tracking an entity's state in natural language?

- Concept: Procedural reasoning and multi-step logic
  - Why needed here: Both code and entity tracking tasks require following sequences of operations and understanding their cumulative effects
  - Quick check question: How would you break down a complex entity tracking problem into smaller logical steps?

- Concept: Pattern recognition across different data types
  - Why needed here: The key insight is that patterns learned from code can transfer to natural language tasks
  - Quick check question: What common patterns exist between code state changes and natural language entity changes?

## Architecture Onboarding

- Component map: Base model → Code pretraining layer → Entity tracking evaluation
- Critical path: Code pretraining data → Model parameter updates → Improved entity tracking performance
- Design tradeoffs: Code pretraining vs. math pretraining vs. alignment tuning - code shows consistent benefits while others have mixed results
- Failure signatures: Poor entity tracking performance despite code pretraining, especially on complex multi-step operations
- First 3 experiments:
  1. Compare entity tracking performance between base and code-pretrained models on simple (1-2 operations) vs. complex (5+ operations) tasks
  2. Test different amounts of code pretraining data to find the optimal training volume
  3. Evaluate whether code pretraining benefits transfer to other reasoning tasks beyond entity tracking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of additional training data affect the entity tracking performance gains from code pretraining?
- Basis in paper: [explicit] The authors note that DeepSeek-Coder uses 2T tokens of additional code training compared to Code Llama's 500B tokens, and speculate this may explain why DeepSeek-Coder shows larger performance gains than Code Llama.
- Why unresolved: The paper doesn't directly test the effect of training data size by keeping the model architecture and training method constant while varying only the amount of code data.
- What evidence would resolve it: Controlled experiments training identical base models on different amounts of code data and measuring entity tracking performance.

### Open Question 2
- Question: Does the format of additional training data (instruction-formatted vs. non-instruction-formatted) affect entity tracking performance differently than the content type (code vs. math)?
- Basis in paper: [explicit] The authors observe that FLoat and OpenMathMistral use instruction-formatted math data while DeepSeek-Math and Llemma use non-instruction-formatted math data, but cannot isolate the effect of format from content type.
- Why unresolved: The format co-varies with whether code was in the pretraining mixture, making it impossible to disentangle these factors with the current experimental design.
- What evidence would resolve it: Training models on instruction-formatted and non-instruction-formatted code data separately, and on instruction-formatted and non-instruction-formatted math data separately.

### Open Question 3
- Question: What is the mechanism by which code pretraining improves entity tracking abilities in language models?
- Basis in paper: [inferred] The authors hypothesize that tracking variable states in code provides a stronger training signal than natural language text, but this remains speculative.
- Why unresolved: The paper provides correlational evidence linking code pretraining to improved entity tracking but doesn't investigate the underlying cognitive or computational mechanisms.
- What evidence would resolve it: Analysis of attention patterns, activation changes, or behavioral differences in code-pretrained vs. base models during entity tracking tasks, potentially using mechanistic interpretability methods.

## Limitations
- Evaluation uses a single synthetic task that may not generalize to broader natural language understanding
- Pretraining configurations vary significantly across model families, making it difficult to isolate code pretraining effects
- The study doesn't account for potential confounds like data quality and diversity beyond the "code" vs "non-code" distinction

## Confidence

**High Confidence Claims:**
- Code pretraining provides consistent improvements on entity tracking tasks across multiple model families
- The magnitude of improvement correlates with task complexity (more operations → larger gains)
- Instruction tuning combined with code pretraining yields the best overall performance

**Medium Confidence Claims:**
- Math pretraining provides only marginal benefits for entity tracking
- Alignment tuning has mixed effects that depend on the base model architecture
- Larger models benefit more from code pretraining than smaller models

**Low Confidence Claims:**
- The specific mechanisms by which code pretraining improves entity tracking (variable state transfer vs. reasoning improvements)
- The optimal amount of code pretraining data needed for maximum benefit
- Whether code pretraining benefits transfer to other reasoning tasks beyond entity tracking

## Next Checks

1. **Cross-task generalization test**: Evaluate the same code-pretrained models on multiple entity tracking benchmarks with different domains (e.g., story comprehension, procedural text, dialogue state tracking) to verify if improvements generalize beyond the synthetic box task.

2. **Controlled pretraining ablation**: Train new model variants with controlled differences - same base architecture, same amount of data, but varying proportions of code vs. non-code content - to isolate the specific contribution of code pretraining.

3. **Transfer learning analysis**: Test whether models pretrained on code show transfer benefits to other reasoning tasks (math word problems, logical inference, multi-step planning) beyond the entity tracking domain studied here.