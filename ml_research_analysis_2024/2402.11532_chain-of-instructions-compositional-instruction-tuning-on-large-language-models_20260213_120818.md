---
ver: rpa2
title: 'Chain-of-Instructions: Compositional Instruction Tuning on Large Language
  Models'
arxiv_id: '2402.11532'
source_url: https://arxiv.org/abs/2402.11532
tags:
- instruction
- output
- instructions
- input
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of handling complex, multi-step
  instructions in large language models (LLMs) by introducing "chain-of-instructions"
  (CoI), where the output of one subtask becomes the input for the next in a chained
  manner. The authors develop an automatic pipeline to create compositional instruction
  datasets from single-instruction data using LLMs for instruction summarization and
  composability checking.
---

# Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models

## Quick Facts
- **arXiv ID**: 2402.11532
- **Source URL**: https://arxiv.org/abs/2402.11532
- **Reference count**: 28
- **Key outcome**: CoI-tuned models significantly outperform baselines on compositional instruction test sets (CoI2-test: 70.76 vs 24.93 ROUGE-L for Alpaca; CoI3-test: 61.61 vs 23.66 ROUGE-L for Mistral) and downstream tasks like multilingual summarization.

## Executive Summary
This paper addresses the challenge of handling complex, multi-step instructions in large language models by introducing "chain-of-instructions" (CoI), where the output of one subtask becomes the input for the next in a chained manner. The authors develop an automatic pipeline to create compositional instruction datasets from single-instruction data using LLMs for instruction summarization and composability checking. They then fine-tune base models (Alpaca-7B and Mistral-7B-Instruct) on these CoI datasets. Results show that CoI-tuned models significantly outperform baselines on both compositional instruction test sets and downstream tasks like multilingual summarization, demonstrating generalization to unseen longer chains and complex single instructions from BIG-Bench Hard.

## Method Summary
The authors propose a compositional instruction tuning approach where complex tasks are broken down into sequential subtasks, with each subtask's output serving as input for the next. They develop an automatic pipeline to create CoI datasets by first summarizing single instructions to remove redundancy, then using LLMs to check composability and generate intermediate outputs. Base models (Alpaca-7B and Mistral-7B-Instruct) are fine-tuned on these CoI datasets with varying chain lengths (1, 2, 3 instructions) using 3 epochs, batch size 4, learning rate 2e-5, and other specified hyperparameters. The approach is evaluated on CoI test sets, BIG-Bench Hard for single instructions, and downstream multilingual summarization tasks.

## Key Results
- CoI-tuned Alpaca-7B achieves 70.76 ROUGE-L on CoI2-test versus 24.93 for non-fine-tuned baseline
- CoI-tuned Mistral-7B-Instruct achieves 61.61 ROUGE-L on CoI3-test versus 23.66 for non-fine-tuned baseline
- Models trained on longer chains (σ = 2, 3) generalize to unseen longer chains (σ = 4, 5) with strong performance
- CoI fine-tuning improves performance on BIG-Bench Hard, suggesting benefits for complex single-instruction tasks

## Why This Works (Mechanism)

### Mechanism 1
The model learns compositional generalization by generating intermediate outputs at each step, building a stepwise reasoning chain. The model solves each subtask independently while treating previous outputs as inputs for the next step. This explicit chaining exposes the model to compositionality patterns during training, enabling generalization to longer unseen chains. The core assumption is that outputs from one subtask are valid inputs for the next, ensured by the LLM-based composability check.

### Mechanism 2
CoI training improves performance on single-instruction tasks by learning to decompose and reason through implicit subcomponents. When fine-tuned on multi-step instructions, the model learns internal decomposition strategies that transfer to solving complex single tasks. The compositional reasoning learned on CoI generalizes to recognizing and handling implicit subtasks within a single instruction.

### Mechanism 3
The LLM-based dataset creation pipeline allows automatic generation of high-quality compositional instruction data without extensive human annotation. Single instructions are summarized to remove redundancy, then an LLM checks composability and generates correct intermediate outputs for chaining. The core assumption is that the LLM can accurately assess task composability and generate valid intermediate outputs for chaining.

## Foundational Learning

- **Concept**: Instruction decomposition and chaining
  - Why needed: CoI requires understanding how to split complex tasks into subtasks and link them in sequence
  - Quick check: Given "Translate the summary to French and then simplify the French text," what are the two subtasks and their order?

- **Concept**: Task composability and format compatibility
  - Why needed: Not all tasks can be chained; output format of one must match input format of the next
  - Quick check: Can a summarization output be used as input for a classification task? Why or why not?

- **Concept**: Data generation with LLMs for synthetic training data
  - Why needed: The CoI dataset is created automatically using LLMs for summarization, composability checking, and output generation
  - Quick check: What is the purpose of summarizing instructions before chaining them?

## Architecture Onboarding

- **Component map**: Base LLM -> CoI dataset -> LLM-based dataset generation pipeline (summarization, composability check, output generation) -> Evaluation suites (CoI test sets, BBH, downstream multilingual summarization) -> Fine-tuning script

- **Critical path**: 1) Generate CoI dataset from single-instruction data 2) Fine-tune base LLM on CoI dataset 3) Evaluate on CoI test sets, BBH, and downstream tasks 4) Analyze intermediate subtask outputs for debugging

- **Design tradeoffs**: Using LLM for dataset generation trades annotation cost for potential noise in composability checking; training on longer chains may improve generalization but increase model complexity and training time; shortened instructions may improve clarity but risk losing task specificity

- **Failure signatures**: Poor performance on CoI test sets may indicate dataset generation errors or insufficient composability; hallucinations in intermediate outputs suggest the model struggles with chaining reasoning; degraded performance on single tasks after CoI fine-tuning may mean over-specialization to compositional formats

- **First 3 experiments**: 1) Generate a small CoI2 dataset (10 pairs) and fine-tune for 1 epoch to verify pipeline correctness 2) Test fine-tuned model on a held-out CoI2 test set to check if intermediate outputs are generated correctly 3) Compare model outputs on a single CoI3 instance with and without the chaining to see if the model uses intermediate outputs

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the LLM-generated intermediate outputs affect the performance of CoI-tuned models on downstream tasks? The paper mentions that models fine-tuned with incorrect outputs degrade in performance, highlighting the importance of correct outputs for subtasks during training, but does not explore the extent to which varying levels of output quality impact model performance or provide detailed error analysis.

### Open Question 2
Can the CoI approach be effectively extended to languages other than English, particularly for multilingual and cross-lingual tasks? The paper demonstrates CoI-tuned models on multilingual summarization tasks but notes poor performance in Spanish summaries, possibly due to the lack of Spanish tasks in the training data, without exploring whether the CoI approach can be generalized to other languages.

### Open Question 3
What is the optimal chain length for CoI tasks, and how does increasing chain length affect model performance and generalization? The paper shows that models trained on longer chains perform better on unseen longer chains but does not investigate the maximum chain length that models can effectively handle or the trade-offs between chain length, model complexity, and performance.

## Limitations

- The LLM-based dataset generation pipeline is critical but not fully detailed, with exact prompts for instruction summarization and composability checking unspecified
- The analysis of why CoI fine-tuning improves single-instruction tasks remains somewhat speculative, with the mechanism of implicit subtask decomposition transfer not empirically verified
- Evaluation focuses heavily on ROUGE metrics which may not fully capture the quality of intermediate outputs in the instruction chain

## Confidence

- **High confidence**: CoI-tuned models significantly outperform baselines on compositional instruction test sets (CoI2, CoI3)
- **Medium confidence**: Improvement on single-instruction tasks (BBH) generalizes from compositional training
- **Medium confidence**: LLM-based dataset generation produces valid compositional instructions

## Next Checks

1. Analyze a sample of generated CoI instances to verify that the LLM-based composability check correctly identifies valid task chains and that intermediate outputs are properly formatted as inputs for subsequent subtasks.

2. Conduct ablation studies comparing CoI fine-tuning with other compositional approaches (Chain-of-Thought, Let's Think Step-by-Step) to isolate whether the chaining mechanism itself drives improvements versus general compositional reasoning exposure.

3. Test the fine-tuned models on held-out single instructions with varying levels of implicit subtask complexity to determine if performance gains correlate with instruction compositional structure, providing evidence for or against the implicit decomposition transfer hypothesis.