---
ver: rpa2
title: 'MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms'
arxiv_id: '2402.14154'
source_url: https://arxiv.org/abs/2402.14154
tags:
- mllms
- tasks
- social
- language
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MM-Soc is a benchmark that evaluates multimodal large language
  models (MLLMs) on 10 tasks derived from social media, including misinformation detection,
  hate speech detection, humor/sarcasm detection, sentiment analysis, and image-text
  generation tasks. The benchmark tests zero-shot and fine-tuned MLLMs across 4 model
  architectures.
---

# MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms

## Quick Facts
- arXiv ID: 2402.14154
- Source URL: https://arxiv.org/abs/2402.14154
- Authors: Yiqiao Jin; Minje Choi; Gaurav Verma; Jindong Wang; Srijan Kumar
- Reference count: 30
- Primary result: Zero-shot MLLMs perform near-random on social media tasks; fine-tuning with explanations outperforms standard fine-tuning across more tasks

## Executive Summary
MM-Soc is a benchmark that evaluates multimodal large language models (MLLMs) on 10 tasks derived from social media platforms, including misinformation detection, hate speech detection, humor/sarcasm detection, sentiment analysis, and image-text generation tasks. The benchmark tests zero-shot and fine-tuned MLLMs across 4 model architectures. Results show that zero-shot MLLMs perform near-random on many tasks and are significantly outperformed by fine-tuned models, with LLaVA-v1.5 achieving the highest performance among MLLMs. Fine-tuning with explanations improves performance across more tasks than standard fine-tuning.

## Method Summary
MM-Soc evaluates MLLMs using a combination of existing social media datasets and a novel large-scale YouTube tagging dataset. The benchmark tests four model architectures (LLaVA-v1.5, InstructBLIP, mPLUG-Owl2, MiniGPT-4) in both zero-shot and fine-tuned settings. Fine-tuning strategies include standard fine-tuning and explanation-augmented fine-tuning where models learn from both ground truth answers and GPT-generated explanations. Performance is measured using task-specific metrics including F1-score, accuracy, and other classification metrics.

## Key Results
- Zero-shot MLLMs achieve near-random performance on many social media tasks
- Fine-tuned models significantly outperform zero-shot models across all task types
- LLaVA-v1.5 achieves the highest performance among MLLMs
- Fine-tuning with explanations improves performance across more tasks than standard fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot MLLMs underperform on social media tasks because their training objectives prioritize cross-modal alignment over social context reasoning. MLLMs are pretrained on tasks like image captioning and visual question answering, which do not require subjective judgment or cultural context understanding. When applied to social media tasks like misinformation detection or humor recognition, they lack the necessary social knowledge reasoning capabilities.

### Mechanism 2
Larger MLLMs demonstrate better instruction-following abilities due to increased model capacity for complex reasoning. As model parameter size increases, MLLMs develop more sophisticated reasoning abilities and better understanding of social knowledge, leading to improved performance on tasks requiring subjective judgment.

### Mechanism 3
Fine-tuning with explanations improves MLLM performance across more tasks than standard fine-tuning because explanations provide additional reasoning context. When MLLMs are fine-tuned with both ground truth answers and GPT-generated explanations, they learn not just the correct output but also the reasoning process, leading to better generalization across different task types.

## Foundational Learning

- **Multimodal reasoning**: Understanding both textual and visual information together, not just processing each modality separately. Why needed: MM-Soc requires understanding both modalities to make correct judgments about social media content. Quick check: Can you explain why a meme might be considered offensive even if neither the image nor text alone is offensive?

- **Social context understanding**: Understanding cultural references, humor, and subjective interpretations that vary across different social groups. Why needed: Many tasks require understanding social nuances beyond literal content interpretation. Quick check: How would you determine if a piece of news is misinformation beyond just checking its factual accuracy?

- **Fine-tuning vs zero-shot evaluation**: Understanding the difference between pretrained capabilities and task-specific performance. Why needed: The benchmark explicitly compares these approaches to understand the gap between general MLLM capabilities and social media task performance. Quick check: What's the key difference between a model that memorizes patterns and one that understands underlying concepts?

## Architecture Onboarding

- **Component map**: Text encoder -> Image encoder -> Cross-modal attention -> Language model -> Task-specific heads
- **Critical path**: Data preprocessing -> Cross-modal fusion and reasoning -> Task-specific output generation -> Evaluation metrics calculation -> Performance comparison
- **Design tradeoffs**: Model size vs. inference speed (larger models perform better but are slower and more expensive); Fine-tuning vs. zero-shot (fine-tuning improves performance but requires labeled data); Task-specific vs. general (specialized models may outperform general MLLMs on specific tasks)
- **Failure signatures**: Low success rate on instruction-following tasks; Inconsistent performance across similar tasks; Over-reliance on single modalities; Poor generalization to unseen social contexts
- **First 3 experiments**: 1) Test zero-shot performance on a simple classification task to establish baseline; 2) Fine-tune on one task and measure improvement vs. zero-shot; 3) Compare performance across different model sizes on the same task to verify scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
How can multimodal large language models (MLLMs) be effectively trained to improve their performance on social media tasks that require subjective judgment and comprehension of social context? The paper highlights that zero-shot MLLMs often fall short in achieving comparable performances compared to fine-tuned models on social media tasks, but doesn't provide a clear solution for effectively training MLLMs to improve their performance on these tasks.

### Open Question 2
Can self-improvement capabilities of MLLMs be enhanced to maintain factual accuracy while iteratively generating responses? The paper conducts a case study on the self-improvement capabilities of MLLMs, revealing a gradual divergence from the ground truth over successive iterations, but doesn't provide a solution to enhance their self-improvement capabilities while preserving accuracy.

### Open Question 3
How can fine-tuning strategies be optimized to improve MLLMs' performance across a broader spectrum of social media tasks? While the paper explores explanation-augmented fine-tuning, it doesn't provide a comprehensive analysis of different fine-tuning strategies and their impact on MLLMs' performance across various social media tasks.

## Limitations

- Dataset quality and representativeness uncertainties due to lack of transparency about data collection and curation methods
- Generalizability of fine-tuning results not adequately addressed for unseen social media content or different social contexts
- Model architecture limitations not fully explored in terms of how architectural differences impact social media task performance

## Confidence

- **High Confidence**: Zero-shot MLLMs perform near-random on many social media tasks is well-supported by empirical results across multiple datasets and model architectures
- **Medium Confidence**: Fine-tuning with explanations improves performance across more tasks than standard fine-tuning is supported by experimental results but requires further investigation into the mechanism and generalizability
- **Low Confidence**: Model size directly correlates with social reasoning ability lacks sufficient evidence as the relationship between parameter count and social context understanding isn't rigorously established

## Next Checks

1. **Cross-platform Generalization Test**: Evaluate the fine-tuned models on social media content from different platforms (e.g., Twitter, Instagram, Reddit) not included in the original training data to assess real-world applicability and potential platform-specific biases.

2. **Ablation Study on Explanation Components**: Conduct systematic experiments removing different components of the explanation-augmented fine-tuning (e.g., just the explanation text, just the reasoning structure) to identify which aspects contribute most to performance improvements.

3. **Human Evaluation of Social Context Understanding**: Supplement automated metrics with human evaluations to assess whether MLLMs actually understand social context and cultural nuances, or simply learn surface-level patterns that happen to correlate with correct answers.