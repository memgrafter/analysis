---
ver: rpa2
title: 'QPaug: Question and Passage Augmentation for Open-Domain Question Answering
  of LLMs'
arxiv_id: '2406.14277'
source_url: https://arxiv.org/abs/2406.14277
tags:
- question
- passages
- passage
- retrieved
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QPaug augments retrieval-augmented question answering by using
  LLMs to decompose complex questions into multi-step sub-questions, improving retrieval
  relevance. It also generates supplementary passages using LLM knowledge to compensate
  for poor retrieval quality, guiding answer extraction.
---

# QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs

## Quick Facts
- arXiv ID: 2406.14277
- Source URL: https://arxiv.org/abs/2406.14277
- Reference count: 23
- Primary result: QPaug achieves 10.4% improvement on NQ, 8.9% on 2wiki, and 34.2% on HotpotQA over SuRE baseline

## Executive Summary
QPaug addresses retrieval-augmented question answering by decomposing complex questions into multi-step sub-questions using chain-of-thought prompting, improving retrieval relevance. The method generates supplementary passages using LLM parametric knowledge to compensate for poor retrieval quality, guiding answer extraction. Experiments on three datasets demonstrate significant performance gains over strong baselines, with consistent improvements across various retrievers and LLMs.

## Method Summary
QPaug augments RAG by first decomposing original questions into multi-step sub-questions using LLM-based chain-of-thought reasoning, then retrieving passages using these augmented queries. It generates supplementary passages via LLM knowledge when retrieval is insufficient, combining both retrieved and generated passages for final answer extraction. The method operates without fine-tuning, making it compatible with black-box LLM APIs.

## Key Results
- QPaug achieves 10.4% performance gain on NQ, 8.9% on 2wiki, and 34.2% on HotpotQA over SuRE baseline
- Consistent improvements across multiple retrievers (SBERT, ANCE, Contriever) and LLMs (GPT-3.5, GPT-4, Llama-2)
- Ablation studies confirm both question decomposition and passage generation contribute substantially to accuracy

## Why This Works (Mechanism)

### Mechanism 1
Question decomposition via CoT yields more granular retrieval queries that align better with relevant passages. The LLM generates sub-questions that clarify what knowledge needs to be retrieved, reducing ambiguity in the original question.

### Mechanism 2
Self-generated passages from LLM parametric knowledge compensate for missing or noisy retrieval context. The LLM generates a passage conditioned on the augmented question, then the reader uses both retrieved and generated passages to form the answer.

### Mechanism 3
Combining generated and retrieved passages improves answer extraction robustness across retrieval quality levels. The reader conditions on the union of retrieved and generated passages, benefiting from both non-parametric evidence and LLM knowledge.

## Foundational Learning

- Concept: Chain-of-thought prompting for question decomposition
  - Why needed here: Provides structured reasoning steps that translate into better retrieval queries
  - Quick check question: Can the LLM generate sub-questions that are meaningful for retrieval without training?

- Concept: Dense retrieval with MIPS (Max Inner Product Search)
  - Why needed here: Enables efficient retrieval from millions of passages given augmented queries
  - Quick check question: What retrieval metric should you check first to confirm augmentation helps?

- Concept: In-context learning without fine-tuning
  - Why needed here: Allows QPaug to work with black-box LLM APIs
  - Quick check question: How do you instruct the LLM to avoid hallucination when generating passages?

## Architecture Onboarding

- Component map: Query → Augmentation (CoT) → Retrieval (MIPS) → Generation (LLM) → Answer Extraction (LLM)
- Critical path: Question → Augmentation → Retrieval → Answer prediction
- Design tradeoffs: Augmentation improves recall but adds latency; generation adds factual content but risks hallucination
- Failure signatures: Low recall after augmentation suggests sub-questions are off-topic; incorrect answers despite good retrieval suggest generation hallucinations
- First 3 experiments:
  1. Measure retrieval Recall@10 before/after augmentation on a held-out dev set
  2. Test passage generation with and without [NONE] fallback instruction
  3. Compare answer F1 with only retrieved, only generated, and combined passages

## Open Questions the Paper Calls Out

### Open Question 1
How does QPaug's question augmentation scale with increasingly complex multi-hop questions requiring more than two reasoning steps? The experiments only evaluate on existing benchmark datasets with limited complexity, and the paper doesn't provide systematic analysis of QPaug's performance on questions with deeper reasoning chains.

### Open Question 2
What is the impact of LLM-generated passages on overall system performance when retrieval quality is consistently high? The paper mentions that passage generation is particularly beneficial when retrieval quality degrades, but doesn't analyze the marginal benefit when retrieval is already good.

### Open Question 3
How does QPaug's performance compare to fine-tuned retrieval-augmented models when computational resources are not a constraint? The paper emphasizes QPaug's advantage in not requiring fine-tuning, but doesn't benchmark against state-of-the-art fine-tuned models under equivalent resource conditions.

## Limitations
- The study relies heavily on black-box LLM APIs, making it difficult to isolate whether improvements stem from genuine knowledge augmentation versus LLM-induced bias
- The paper doesn't address potential contradictions between retrieved and generated passages, nor does it quantify the extent of hallucination in generated passages
- The experimental setup assumes perfect ground-truth passage availability for HotpotQA, which may not reflect real-world retrieval failures

## Confidence
- Question decomposition mechanism: Medium confidence
- Passage generation compensation: Low-Medium confidence
- Combined performance gains: High confidence

## Next Checks
1. Conduct an ablation study isolating question decomposition effects by comparing retrieval performance with and without augmentation on identical passages
2. Measure hallucination rates by human evaluation of generated passages against their corresponding retrieved passages and ground truth
3. Test robustness to retrieval quality degradation by intentionally removing relevant passages and measuring whether augmentation maintains performance