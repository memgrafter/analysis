---
ver: rpa2
title: Disentanglement with Factor Quantized Variational Autoencoders
arxiv_id: '2409.14851'
source_url: https://arxiv.org/abs/2409.14851
tags:
- disentanglement
- latent
- learning
- representation
- factorqvae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FactorQVAE, a discrete variational autoencoder
  (VAE) model for unsupervised disentangled representation learning. The key idea
  is to combine scalar quantization with a global codebook and total correlation regularization
  to encourage independence among latent variables.
---

# Disentanglement with Factor Quantized Variational Autoencoders

## Quick Facts
- arXiv ID: 2409.14851
- Source URL: https://arxiv.org/abs/2409.14851
- Authors: Gulcin Baykal; Melih Kandemir; Gozde Unal
- Reference count: 40
- This paper introduces FactorQVAE, a discrete VAE model that combines scalar quantization with a global codebook and total correlation regularization to achieve state-of-the-art disentanglement performance.

## Executive Summary
This paper introduces FactorQVAE, a discrete variational autoencoder model for unsupervised disentangled representation learning. The key innovation is combining scalar quantization with a global codebook and total correlation regularization to encourage independence among latent variables. Unlike prior methods that use per-dimension codebooks or continuous representations, FactorQVAE uses a single global codebook to quantize scalar values from the latent representation, promoting more efficient and flexible encoding of generative factors. The model also incorporates a total correlation term to enforce statistical independence among latent variables.

## Method Summary
FactorQVAE combines scalar quantization with a global codebook and total correlation regularization for disentangled representation learning. The method quantizes each latent variable independently using scalar values from a shared global codebook, rather than using per-dimension codebooks as in prior work. A stochastic categorical posterior with Gumbel-Softmax enables differentiable training with discrete latents. Total correlation regularization enforces statistical independence among latent variables by encouraging the marginal posterior to factorize. The model is trained end-to-end using a modified ELBO objective that includes reconstruction, KL divergence, and total correlation terms.

## Key Results
- FactorQVAE achieves state-of-the-art disentanglement performance on Shapes3D, Isaac3D, and MPI3D-Complex datasets
- Outperforms existing methods including β-VAE, FactorVAE, QLAE, and dVAE on DCI and InfoMEC metrics
- Maintains competitive reconstruction quality while improving disentanglement scores
- Demonstrates effectiveness of combining discrete representation learning with factorization for better disentangled representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalar quantization with a global codebook improves disentanglement over vector quantization by limiting representational capacity and forcing a one-to-one mapping between latent variables and generative factors.
- Mechanism: By quantizing each latent variable independently with scalar values from a shared global codebook, the model is constrained to represent each generative factor using a separate subset of codebook entries. This encourages latent variables to capture distinct factors without redundancy.
- Core assumption: Generative factors have different cardinalities and benefits from dedicated, non-overlapping subsets of codebook elements.
- Evidence anchors:
  - [abstract]: "We propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook"
  - [section]: "Although QLAE's use of individual codebooks promotes disentanglement, its fixed-size, per-dimension quantization introduces inefficiencies and limits representational flexibility"
  - [corpus]: Weak or missing corpus evidence directly supporting the global codebook advantage; main support from ablation study in paper.
- Break condition: If generative factors are highly correlated or if the dataset requires joint encoding of multiple factors, the global codebook constraint may harm performance.

### Mechanism 2
- Claim: Total correlation regularization enforces statistical independence among latent variables, directly promoting disentanglement.
- Mechanism: The total correlation term KL(q(z)||∏q(z_j)) penalizes dependencies between latent variables by encouraging the marginal posterior to factorize. This aligns with the goal of having each latent variable represent a separate generative factor.
- Core assumption: The underlying generative factors are independent, so the learned latent representation should reflect this independence.
- Evidence anchors:
  - [abstract]: "We add a total correlation term to the optimization as an inductive bias"
  - [section]: "As discussed in Section 2, total correlation is a prominent regularizer for disentanglement, encouraging statistical independence among latent variables"
  - [corpus]: No direct corpus evidence; core concept from standard VAE disentanglement literature.
- Break condition: If generative factors are not truly independent, forcing independence may lead to suboptimal representations.

### Mechanism 3
- Claim: Stochastic categorical posterior with Gumbel-Softmax enables differentiable training with discrete latents and total correlation regularization.
- Mechanism: Instead of deterministic quantization, the model samples from a stochastic categorical posterior parameterized by negative distances to codebook entries. Gumbel-Softmax provides a differentiable approximation, allowing gradients to flow through the sampling process for both reconstruction and regularization objectives.
- Core assumption: Differentiable approximation of discrete sampling is sufficient for effective end-to-end training.
- Evidence anchors:
  - [section]: "Rather than using a deterministic categorical posterior... we define a stochastic categorical posterior proposed by Sønderby et al."
  - [section]: "Sampling from a Categorical distribution is inherently non-differentiable... we adopt the Gumbel–Softmax distribution as a differentiable approximation"
  - [corpus]: No direct corpus evidence; standard technique in discrete VAE literature.
- Break condition: If temperature annealing is too aggressive or too slow, the approximation may either lose discreteness or become too rigid for effective learning.

## Foundational Learning

- Concept: Variational Autoencoders and Evidence Lower Bound (ELBO)
  - Why needed here: FactorQVAE is built on VAE architecture and optimizes a modified ELBO objective that includes reconstruction, KL divergence, and total correlation terms.
  - Quick check question: What are the three main components of the ELBO objective in a standard VAE?

- Concept: Total Correlation and Mutual Information
  - Why needed here: Total correlation is used as a regularizer to enforce independence among latent variables, which is central to the disentanglement objective.
  - Quick check question: How does total correlation differ from pairwise mutual information between latent variables?

- Concept: Vector vs Scalar Quantization
  - Why needed here: The paper contrasts vector quantization (used in VQVAE) with scalar quantization (used in FactorQVAE) to explain the design choice for better disentanglement.
  - Quick check question: What is the main representational difference between vector and scalar quantization in the context of VAEs?

## Architecture Onboarding

- Component map:
  - Encoder Eθ1 -> Extracts feature tensor from input
  - Encoder Eθ2 -> Transforms features to scalar latent representation z_e(x)
  - Codebook M -> Global codebook of scalar values for quantization
  - Stochastic categorical posterior -> Samples one-hot indices z from distances to codebook
  - Gumbel-Softmax -> Differentiable approximation for backpropagation
  - Decoder Dϕ2 -> Transforms quantized latents to feature tensor
  - Decoder Dϕ1 -> Reconstructs input from feature tensor
  - Discriminator Cψ -> Approximates density ratio for total correlation term

- Critical path:
  1. Input -> Eθ1 -> Eθ2 -> z_e(x)
  2. z_e(x) -> distances to M -> stochastic sampling -> z
  3. z -> matrix multiplication with M -> z_q(x)
  4. z_q(x) -> Dϕ2 -> Dϕ1 -> reconstruction
  5. Simultaneously: train Cψ to estimate density ratio for total correlation

- Design tradeoffs:
  - Global codebook vs per-dimension codebooks: Global codebook allows flexible allocation of codebook elements to different generative factors but may require more careful optimization
  - Scalar vs vector quantization: Scalar quantization enforces stronger independence but may limit representational capacity for complex factors
  - Stochastic vs deterministic quantization: Stochastic enables total correlation regularization but introduces sampling noise

- Failure signatures:
  - Poor reconstruction with good disentanglement: Indicates total correlation weight too high or codebook capacity insufficient
  - Good reconstruction with poor disentanglement: Indicates total correlation weight too low or codebook not enforcing independence
  - Training instability: Often caused by temperature annealing schedule or discriminator training imbalance

- First 3 experiments:
  1. Train FactorQVAE on Shapes3D with default hyperparameters and evaluate DCI/InfoMEC scores
  2. Compare reconstruction quality with and without total correlation term (γ=0 vs default)
  3. Test different codebook sizes (e.g., 32 vs 64 vs 128 scalar values) and observe impact on disentanglement metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Comparison with continuous VAE variants may be incomplete as these methods have been superseded by more recent approaches
- The claim that scalar quantization with a global codebook inherently promotes better disentanglement than per-dimension codebooks is primarily supported by ablation studies within this paper rather than external validation
- Performance on datasets with highly correlated generative factors or requiring complex joint representations remains untested

## Confidence
- **High confidence**: The core mechanism of using scalar quantization with a global codebook and total correlation regularization is technically sound and well-implemented
- **Medium confidence**: The experimental results showing superior performance on benchmark datasets are convincing but limited to specific controlled environments
- **Medium confidence**: The claim that this approach generalizes well to real-world scenarios needs further validation

## Next Checks
1. Test FactorQVAE on datasets with known correlated generative factors to evaluate robustness when the independence assumption is violated
2. Compare FactorQVAE against recent state-of-the-art continuous disentanglement methods (e.g., AdaGroove, ReVED) to establish relative performance across the broader landscape
3. Perform systematic ablation studies varying codebook size and total correlation weight to identify optimal configurations across different dataset complexities