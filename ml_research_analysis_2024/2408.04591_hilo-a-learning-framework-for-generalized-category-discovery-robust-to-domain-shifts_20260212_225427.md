---
ver: rpa2
title: 'HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain
  Shifts'
arxiv_id: '2408.04591'
source_url: https://arxiv.org/abs/2408.04591
tags:
- domain
- learning
- semantic
- unlabelled
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiLo addresses generalized category discovery (GCD) in the presence
  of domain shifts by disentangling high-level semantic and low-level domain features,
  then minimizing mutual information between them. It incorporates PatchMix augmentation
  tailored for contrastive learning in GCD, and uses a curriculum sampling strategy
  to progressively increase exposure to unseen domains.
---

# HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts

## Quick Facts
- arXiv ID: 2408.04591
- Source URL: https://arxiv.org/abs/2408.04591
- Authors: Hongjun Wang; Sagar Vaze; Kai Han
- Reference count: 40
- Key outcome: HiLo achieves substantial gains—up to 43.8% and 51.4% relative improvements in domain-specific and domain-shifted settings, respectively—demonstrating robustness to real and synthetic domain shifts.

## Executive Summary
HiLo addresses the challenge of Generalized Category Discovery (GCD) in the presence of domain shifts by disentangling high-level semantic and low-level domain features, then minimizing mutual information between them. The framework incorporates PatchMix augmentation tailored for contrastive learning in GCD and uses a curriculum sampling strategy to progressively increase exposure to unseen domains. Experiments on DomainNet and the SSB-C benchmark show HiLo outperforms state-of-the-art GCD methods, achieving substantial gains in both domain-specific and domain-shifted settings.

## Method Summary
HiLo implements a GCD framework that disentangles domain and semantic features through layer-based feature extraction (early layers for domain, late layers for semantics) followed by mutual information minimization. The method adapts PatchMix augmentation for contrastive learning by mixing patches from labelled and unlabelled images with confidence-weighted semantic contributions. A curriculum learning approach progressively increases sampling probability for samples from unseen domains, starting with single-domain discrimination and advancing to cross-domain discrimination.

## Key Results
- Achieves up to 43.8% relative improvement in domain-specific settings on DomainNet
- Achieves up to 51.4% relative improvement in domain-shifted settings on DomainNet
- Outperforms state-of-the-art GCD methods on both DomainNet and SSB-C benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Domain and semantic features are naturally represented in different layers of the network, enabling their disentanglement. The architecture uses low-level (early layer) features for domain information and high-level (late layer) features for semantic information. Minimizing mutual information between these two feature sets forces the model to learn independent representations.

**Core assumption**: Deep neural networks inherently separate low-level and high-level features across layers, and these two types of information are independent.

### Mechanism 2
PatchMix augmentation adapted for contrastive learning enables knowledge transfer across domains and novel categories. The method mixes patches from labelled and unlabelled images in the embedding space, with a confidence factor α that weights the semantic contribution of each patch. This allows the model to learn from mixed samples containing both seen and unseen categories across different domains.

**Core assumption**: Mixing embeddings in the feature space with appropriate weighting can transfer knowledge from known to unknown categories and bridge domain gaps.

### Mechanism 3
Curriculum sampling progressively increases exposure to unseen domains, improving learning efficiency. The sampling probability weight for samples from unseen domains increases over training epochs, starting with easier single-domain discrimination and gradually moving to cross-domain discrimination.

**Core assumption**: Gradually increasing task difficulty through curriculum learning improves generalization to challenging scenarios.

## Foundational Learning

- **Concept**: Mutual Information Minimization
  - Why needed here: To enforce independence between domain and semantic features, preventing spurious correlations that hurt generalization.
  - Quick check question: If two random variables have zero mutual information, what does that imply about their statistical dependence?

- **Concept**: Contrastive Learning
  - Why needed here: To learn discriminative representations by pulling together similar samples and pushing apart dissimilar ones, crucial for clustering novel categories.
  - Quick check question: In InfoNCE loss, what role does the temperature parameter τ play in the softmax similarity calculation?

- **Concept**: Curriculum Learning
  - Why needed here: To gradually increase the difficulty of the learning task, allowing the model to first master easier concepts before tackling harder ones involving domain shifts.
  - Quick check question: What is the key difference between self-paced learning and traditional curriculum learning in terms of sample selection strategy?

## Architecture Onboarding

- **Component map**: Image -> PatchMix Layer -> Feature Extractor (ViT) -> Two Heads (˜H) -> MI Estimator (Φ) -> Domain Classifier + Semantic Classifier -> Curriculum Sampler

- **Critical path**: Image → PatchMix → Feature Extractor → Two Heads → MI Loss + Domain Loss + Semantic Loss → Backpropagation

- **Design tradeoffs**:
  - Using early vs late layers for domain features: Early layers capture low-level style but may miss some domain-specific patterns
  - Curriculum schedule: Too fast may overwhelm the model, too slow may underutilize the data
  - MI estimation: Approximations may not perfectly capture true independence

- **Failure signatures**:
  - If MI loss plateaus at high value: Domain and semantic features remain correlated
  - If domain classifier accuracy is low: Feature disentanglement not working
  - If semantic classifier performance drops on unseen domains: Domain shift not properly handled

- **First 3 experiments**:
  1. Train with only MI loss and feature extraction - verify domain and semantic features separate in PCA space
  2. Add domain classification - check if domain head can distinguish source vs target domains
  3. Add semantic classification with curriculum sampling - measure performance improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the HiLo framework perform when the number of domains in the unlabelled data is unknown or extremely large (e.g., hundreds of domains)?
- Basis in paper: The paper states that HiLo's insensitivity to domain axis output size selection was evaluated with up to 100 domains, but real-world scenarios may involve many more domains.
- Why unresolved: The paper only tested up to 100 domains, which may not reflect the complexity of real-world data with potentially thousands of domains.
- What evidence would resolve it: Experiments evaluating HiLo's performance on datasets with a much larger number of domains, such as web-scale image datasets, would clarify its scalability and robustness in highly diverse domain scenarios.

### Open Question 2
Can the mutual information minimization between domain and semantic features be extended to other types of feature disentanglement, such as separating object-level features from background features?
- Basis in paper: The paper focuses on disentangling domain and semantic features, but the underlying principle of minimizing mutual information could potentially be applied to other feature separations.
- Why unresolved: The paper does not explore the application of mutual information minimization to other feature disentanglement tasks beyond domain and semantic features.
- What evidence would resolve it: Experiments applying the mutual information minimization technique to separate object-level features from background features in image datasets, and evaluating the impact on downstream tasks like object detection or segmentation, would demonstrate its broader applicability.

### Open Question 3
How does the performance of HiLo change when using different backbone architectures, such as convolutional neural networks or other transformer-based models?
- Basis in paper: The paper uses a DINO-pretrained ViT-B/16 as the feature backbone, but does not explore the impact of using different architectures.
- Why unresolved: The paper's results are based on a specific backbone architecture, and it is unclear how the framework would perform with other architectures that may have different strengths and weaknesses.
- What evidence would resolve it: Experiments evaluating HiLo's performance using different backbone architectures, such as ResNet, Swin Transformer, or ConvNeXt, and comparing the results to the ViT-B/16 baseline, would provide insights into the framework's adaptability and potential improvements.

## Limitations
- Performance depends on the assumption that domain and semantic features are naturally separated across network layers
- Curriculum sampling schedule requires careful tuning for different domain shift scenarios
- Mutual information estimation approximations may not perfectly capture true feature independence

## Confidence
- Mutual information minimization effectiveness: Medium
- PatchMix adaptation for GCD: Medium
- Curriculum sampling generalization: Medium

## Next Checks
1. **Feature independence verification**: Measure actual mutual information between domain and semantic features before/after training to confirm disentanglement effectiveness.
2. **Curriculum schedule ablation**: Test different curriculum progressions (linear, exponential, adaptive) to determine optimal difficulty ramp-up for domain shift scenarios.
3. **Cross-architecture validation**: Implement HiLo on alternative backbones (ResNet, ConvNeXt) to verify the layer-based feature separation assumption holds across architectures.