---
ver: rpa2
title: Context-Aware Assistant Selection for Improved Inference Acceleration with
  Large Language Models
arxiv_id: '2408.08470'
source_url: https://arxiv.org/abs/2408.08470
tags:
- draft
- token
- decoding
- policy
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accelerating inference for
  large language models (LLMs) under resource constraints, specifically targeting
  the high latency associated with auto-regressive generation. The core method idea
  involves using a contextual bandits framework to dynamically select the most suitable
  draft model from multiple candidates to assist in the generation process for a given
  query, thereby optimizing the trade-off between generation speed and alignment with
  the target model.
---

# Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models

## Quick Facts
- arXiv ID: 2408.08470
- Source URL: https://arxiv.org/abs/2408.08470
- Authors: Jerry Huang; Prasanna Parthasarathi; Mehdi Rezagholizadeh; Sarath Chandar
- Reference count: 13
- One-line primary result: Policy-based draft model selection achieves significant decoding speedups across multiple domains while maintaining generation quality

## Executive Summary
This paper addresses the challenge of accelerating inference for large language models (LLMs) under resource constraints, specifically targeting the high latency associated with auto-regressive generation. The authors propose using a contextual bandits framework to dynamically select the most suitable draft model from multiple candidates to assist in the generation process for a given query. This approach optimizes the trade-off between generation speed and alignment with the target model, achieving notable speedups without significant degradation in generation quality across various domains including translation, summarization, and mathematical reasoning.

## Method Summary
The method uses offline data collected from greedy decoded outputs of each model to compute alignment scores. A policy is trained using the REINFORCE algorithm to select the best draft model based on a context (query). At inference time, the trained policy selects a draft model for each query, which is then used to assist in generating text with the target model using speculative decoding. The policy takes sentence embeddings of the query as input and outputs a distribution over drafting candidates, with a 3-layer MLP architecture trained using AdamW optimizer.

## Key Results
- Policy-based draft model selection achieves significant decoding speedups across translation, summarization, and math reasoning tasks
- The policy effectively learns to balance the trade-off between draft model alignment quality and generation speed
- Training converges quickly with fewer than 10,000 examples sufficient to match performance on the full 400,000-example dataset
- The approach maintains generation quality (measured by BLEU/ROUGE-L) while achieving 2-3x speed improvements

## Why This Works (Mechanism)

### Mechanism 1
The contextual bandits framework allows dynamic selection of draft models based on query context, optimizing the trade-off between generation speed and alignment with the target model. By framing the draft model selection as a contextual bandits problem, the policy learns to choose the draft model that maximizes the estimated reward (alignment with target) for each query context. The core assumption is that alignment between draft and target model outputs can be estimated using similarity metrics on generated samples, and this estimate correlates with actual inference speedup.

### Mechanism 2
The policy can balance the trade-off between draft model alignment quality and generation speed by incorporating explicit information about the model within the reward function. By adjusting the reward function to include both alignment scores and inference speed costs, the policy learns to select draft models that optimize the overall efficiency of the generation process. The relative inference speeds of draft models can be estimated and incorporated into the reward function to guide the policy's decision-making.

### Mechanism 3
The policy can learn to differentiate between draft models and select the appropriate one for a given context with minimal training examples. By training the policy on an offline dataset of query, draft model, and reward tuples, the policy learns to generalize and make effective draft model selections even with a limited number of training examples. The offline dataset provides sufficient diversity in query contexts and draft model outputs for the policy to learn meaningful patterns and make accurate generalizations.

## Foundational Learning

- Concept: Contextual Bandits
  - Why needed here: To frame the draft model selection problem as a decision-making process where the policy learns to choose the best draft model based on the query context.
  - Quick check question: How does the contextual bandits framework differ from traditional multi-armed bandits, and why is it suitable for this problem?

- Concept: Policy Gradient Methods
  - Why needed here: To train the policy using the offline dataset of query, draft model, and reward tuples, allowing the policy to learn the optimal draft model selection strategy.
  - Quick check question: What is the role of the REINFORCE algorithm in training the policy, and how does it update the policy parameters based on the observed rewards?

- Concept: Reward Function Design
  - Why needed here: To balance the trade-off between draft model alignment quality and generation speed, guiding the policy's decision-making process.
  - Quick check question: How does the weighted sum of alignment scores and inference speed costs in the reward function influence the policy's draft model selection strategy?

## Architecture Onboarding

- Component map: Query context -> Policy (3-layer MLP) -> Draft model selection -> Speculative decoding with target model
- Critical path:
  1. Generate offline dataset by running target and draft models on training examples and computing rewards
  2. Train policy on offline dataset using policy gradient methods
  3. At inference time, use trained policy to select draft model for each query and accelerate generation
- Design tradeoffs:
  - Number of draft models: More models provide better coverage but increase policy complexity
  - Reward function design: Balancing alignment and speed impacts selection strategy
  - Policy architecture: Choice affects learning effectiveness and generalization
- Failure signatures:
  - Poor draft model selection leading to low acceptance rates and slow decoding speeds
  - Policy overfitting to training data, resulting in poor generalization
  - Slow policy inference negating benefits of draft model selection
- First 3 experiments:
  1. Train and evaluate policy on single domain (translation) with two draft models of different expertise levels
  2. Extend evaluation to multiple domains (translation and summarization) with same two draft models
  3. Introduce third draft model with different size and alignment level to test balance between alignment and speed

## Open Questions the Paper Calls Out

### Open Question 1
How can the reward function be improved to better capture the alignment between draft and target models, especially for tasks requiring longer or more complex outputs? Current similarity metrics may not adequately capture semantic meaning or subtle differences in distribution that significantly impact output quality, particularly for complex tasks.

### Open Question 2
How can the offline policy learning approach be adapted to work effectively in an online setting, where the policy learns in real-time as new queries are processed? Offline learning requires collecting a large dataset beforehand, which may not be feasible in all scenarios and limits the policy's ability to adapt to changing conditions.

### Open Question 3
How can the policy-based selection method be extended to dynamically choose draft models at every decoding step, rather than just per example, to further optimize the trade-off between generation speed and quality? The current approach selects a single draft model for the entire generation process, which may not be optimal as complexity and requirements change throughout decoding.

## Limitations

- Algorithm implementation details are underspecified, particularly regarding speculative decoding mechanics and KV cache management
- Generalization across domains is primarily tested when draft models are pre-aligned with target model capabilities
- Scalability with larger numbers of draft models remains unclear, as effectiveness beyond 2-3 models is not thoroughly explored

## Confidence

- **High Confidence**: The contextual bandits framework is theoretically sound and empirical results demonstrate clear speed improvements
- **Medium Confidence**: The policy's ability to balance alignment quality and generation speed through reward function design is supported but sensitivity to hyperparameters is not thoroughly explored
- **Low Confidence**: Claims about the policy learning to "ignore" draft models when not useful are based on limited evidence

## Next Checks

1. **Reward Function Sensitivity Analysis**: Systematically vary the Î± parameter in the reward function across a wider range of values to quantify its impact on the policy's draft model selection strategy and overall performance.

2. **Stress Test with Misaligned Draft Models**: Evaluate the policy when draft models have minimal alignment with the target model (e.g., using draft models trained on different domains) to test the policy's ability to recognize and reject unsuitable draft models.

3. **Scaling Analysis**: Test the approach with increasing numbers of draft models (4-6 models) to assess whether the policy can maintain effective selection strategies as the action space grows.