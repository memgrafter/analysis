---
ver: rpa2
title: A Multi-task Learning Framework for Evaluating Machine Translation of Emotion-loaded
  User-generated Content
arxiv_id: '2410.03277'
source_url: https://arxiv.org/abs/2410.03277
tags:
- emotion
- loss
- translation
- nash
- word-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating machine translation
  quality for emotion-loaded user-generated content (UGC), where current metrics fail
  to focus on emotional nuances like slang and sarcasm. The authors propose a multi-task
  learning (MTL) framework that jointly trains sentence-level quality estimation (QE),
  word-level QE, and emotion classification tasks.
---

# A Multi-task Learning Framework for Evaluating Machine Translation of Emotion-loaded User-generated Content

## Quick Facts
- arXiv ID: 2410.03277
- Source URL: https://arxiv.org/abs/2410.03277
- Reference count: 18
- Key outcome: MTL approach achieves state-of-the-art results with Spearman correlations up to 0.4947 for sentence-level QE and F1 scores up to 0.2805 for word-level QE

## Executive Summary
This paper addresses the challenge of evaluating machine translation quality for emotion-loaded user-generated content (UGC), where traditional metrics fail to capture emotional nuances like slang and sarcasm. The authors propose a multi-task learning (MTL) framework that jointly trains sentence-level quality estimation (QE), word-level QE, and emotion classification tasks. They extend an existing emotion-related dataset with QE scores and labels, enabling comprehensive evaluation. Experiments show their MTL approach outperforms fine-tuning and other MTL methods on two datasets, demonstrating the effectiveness of jointly optimizing these correlated tasks.

## Method Summary
The authors propose a multi-task learning framework that jointly trains three related tasks: sentence-level quality estimation, word-level quality estimation, and emotion classification. They extend an existing emotion-related dataset (HADQAET) with sentence-level QE scores and word-level labels. The MTL architecture uses a shared multilingual pre-trained language model (XLM-RoBERTa, XLM-V-base, or InfoXLM) with task-specific output layers. A novel combined loss function integrates Nash and Aligned loss heuristics to optimize performance across all tasks simultaneously.

## Key Results
- MTL approach achieves Spearman correlations up to 0.4947 for sentence-level QE
- Word-level QE achieves F1 scores up to 0.2805
- Outperforms fine-tuning and other MTL methods on two datasets
- Combined loss with Nash and Aligned heuristics shows superior performance compared to Linear loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning (MTL) improves translation quality estimation by leveraging the correlation between emotion preservation and translation quality tasks.
- Mechanism: Joint training of sentence-level QE, word-level QE, and emotion classification tasks using a shared representation allows the model to learn common features across tasks, leading to improved performance.
- Core assumption: The tasks are inherently correlated, and learning them together provides mutual benefits.
- Evidence anchors:
  - [abstract]: "For joint training of these tasks, we employ multi-task learning (MTL), anticipating improved performance for all tasks due to their inherent correlation with emotionally charged content."
  - [section 2.2]: "Multi-task learning addresses multiple related tasks concurrently by training them simultaneously with a shared representation (Caruana, 1997)."
  - [corpus]: Weak evidence; only one neighbor paper mentions MTL directly, but not in the context of translation quality estimation.
- Break condition: If the tasks are not sufficiently correlated, MTL may not provide performance improvements and could even lead to negative transfer.

### Mechanism 2
- Claim: The novel combined loss function integrating Nash and Aligned loss heuristics optimizes the performance of the MTL framework.
- Mechanism: The combined loss function balances the contributions of individual task losses, preventing any single task from dominating the training process and leading to more stable and effective learning.
- Core assumption: The Nash and Aligned loss heuristics are effective in balancing task-specific gradients and preventing conflicting updates.
- Evidence anchors:
  - [abstract]: "We further introduce a new architecture with a novel combined loss function that integrates different loss heuristics, enabling the concurrent training of these tasks and optimizing their overall performance."
  - [section 4.1]: "The objective of the heuristic σ is to find a set of parameters θ that minimize the aggregate loss of all tasks."
  - [section A.2]: "The solution to Equation 5 is (up to scaling) Σiαigi where α ∈ RK+ is the solution to G⊺Gα = 1/α where 1/α is the element-wise reciprocal."
- Break condition: If the loss heuristics are not properly tuned or if the tasks have significantly different optimization landscapes, the combined loss function may not effectively balance the tasks.

### Mechanism 3
- Claim: Extending the emotion-related dataset with sentence-level QE scores and word-level labels creates a comprehensive resource for training and evaluating the MTL framework.
- Mechanism: The extended dataset provides ground truth labels for all three tasks, enabling supervised learning and allowing for comprehensive evaluation of the MTL model's performance.
- Core assumption: The extended dataset accurately represents the challenges of evaluating machine translation quality for emotion-loaded UGC.
- Evidence anchors:
  - [abstract]: "We extend it with sentence-level evaluation scores and word-level labels, leading to a dataset suitable for sentence- and word-level translation evaluation and emotion classification, in a multi-task setting."
  - [section 3.1]: "Since our original paper did not propose any scores for sentence-level QE, we followed Freitag et al. (2021a) to sum up all weighted errors based on their corresponding severity, using a set of weights suggested by MQM (Lommel et al., 2014)."
  - [section 3.2]: "We selected the overlapping of Chinese-English sentence- and word-level MQM datasets from the QE shared task of WMT 2020 to WMT 2022."
- Break condition: If the dataset extension introduces biases or if the annotations are not reliable, the MTL model's performance may be compromised.

## Foundational Learning

- Concept: Multi-task learning (MTL)
  - Why needed here: MTL is used to jointly train sentence-level QE, word-level QE, and emotion classification tasks, leveraging their inherent correlations to improve overall performance.
  - Quick check question: What are the benefits of using MTL for training multiple related tasks simultaneously?

- Concept: Quality Estimation (QE)
  - Why needed here: QE is the primary task of the MTL framework, focusing on predicting the quality of machine translation without human references.
  - Quick check question: What are the key challenges in evaluating the quality of machine translation for emotion-loaded UGC?

- Concept: Emotion classification
  - Why needed here: Emotion classification is included as a task in the MTL framework to assess the preservation of emotions in machine translation.
  - Quick check question: How can emotion classification be integrated with QE tasks to improve the evaluation of machine translation quality?

## Architecture Onboarding

- Component map:
  Input -> [CLS][SEP] tokenized source-target sentences -> Encoder (XLM-RoBERTa/XLM-V-base/InfoXLM) -> Max/Average pooling -> Output layers (sentence-level QE, word-level QE, emotion classification) -> Combined loss (Nash+Aligned)

- Critical path:
  1. Preprocess input data (tokenization, concatenation)
  2. Encode input using pre-trained language model
  3. Apply pooling (max or average) to obtain sentence representation
  4. Pass sentence representation through output layers for each task
  5. Compute individual task losses
  6. Combine losses using the novel loss function
  7. Backpropagate combined loss to update model parameters

- Design tradeoffs:
  - Using a shared encoder for all tasks reduces model complexity but may limit task-specific feature learning
  - Incorporating emotion classification may improve QE performance but could also introduce additional noise or instability
  - The choice of loss heuristics (Nash vs. Aligned) affects the balance between tasks and overall model performance

- Failure signatures:
  - Poor performance on individual tasks: Indicates issues with task-specific output layers or loss functions
  - Unstable training: Suggests problems with the combined loss function or conflicting gradients between tasks
  - Overfitting to training data: May require regularization techniques or more diverse training data

- First 3 experiments:
  1. Train the MTL model on the extended emotion-related dataset and evaluate performance on sentence-level QE, word-level QE, and emotion classification tasks separately.
  2. Compare the MTL model's performance with individual fine-tuned models for each task to assess the benefits of joint training.
  3. Experiment with different loss heuristics (Nash vs. Aligned) and pooling strategies (max vs. average) to optimize the MTL framework's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different loss heuristics (Nash, Aligned, Linear) compare in performance when training more than three tasks simultaneously in the proposed MTL framework?
- Basis in paper: [inferred] The paper discusses Nash and Aligned losses performing better than Linear loss for three-task combinations, but notes that adding more tasks makes achieving consensus more challenging.
- Why unresolved: The paper only tested three-task combinations (sentence-level QE, word-level QE, and emotion classification) and did not explore scenarios with additional tasks or more complex task combinations.
- What evidence would resolve it: Systematic experimentation with MTL frameworks combining four or more tasks, comparing performance across different loss heuristics under varying task combinations and dataset sizes.

### Open Question 2
- Question: What is the impact of emotion label quality (synthetic vs. human-annotated) on the performance of MTL models for quality estimation?
- Basis in paper: [explicit] The paper used synthetic emotion labels generated by fine-tuning a classifier on the SMP2020-EWECT dataset for the MQM emotion subset, achieving high F1 scores, but also notes that HADQAET has human-annotated emotion labels.
- Why unresolved: The paper does not directly compare the performance of MTL models when trained on datasets with synthetic emotion labels versus human-annotated emotion labels, nor does it explore the potential biases introduced by synthetic labeling.
- What evidence would resolve it: Controlled experiments training MTL models on the same tasks but with datasets containing only synthetic emotion labels versus human-annotated emotion labels, measuring performance differences and analyzing error patterns.

### Open Question 3
- Question: How does the proposed MTL framework perform on multilingual datasets with languages other than Chinese-English for emotion-loaded UGC translation quality estimation?
- Basis in paper: [inferred] The paper focuses exclusively on Chinese-English translation for emotion-loaded UGC, using Chinese RoBERTa and XLM-RoBERTa models, but does not test other language pairs or multilingual settings.
- Why unresolved: The paper's experiments are limited to one language pair, and while the framework is described as multilingual, its effectiveness across different language families and translation directions remains untested.
- What evidence would resolve it: Implementation and evaluation of the MTL framework on emotion-loaded UGC translation tasks across multiple language pairs (e.g., Spanish-English, Arabic-English, Japanese-English), comparing performance metrics and identifying language-specific challenges.

## Limitations
- Limited dataset size and potential class imbalance in emotion classification task
- Lack of detailed information about the "emotion-related MQM framework" used for annotating translation errors
- Experiments limited to Chinese-English translation, raising questions about generalizability to other language pairs

## Confidence
- Medium confidence in major claims due to limited dataset size and lack of detailed methodology information
- Medium confidence in experimental results, though reproducibility is affected by unspecified details about dataset extension
- Low confidence in generalizability to other language pairs due to single-language focus

## Next Checks
1. Obtain and analyze the "emotion-related MQM framework" used for annotating translation errors to ensure accurate dataset extension and model training.
2. Investigate the class distribution in the emotion classification task and consider using statistical methods or data augmentation techniques to address potential class imbalance issues.
3. Conduct ablation studies to assess the impact of the novel combined loss function and the MTL architecture on the model's performance, and compare the results with individual fine-tuned models for each task.