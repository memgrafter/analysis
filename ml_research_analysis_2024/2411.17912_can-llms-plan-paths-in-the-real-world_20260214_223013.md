---
ver: rpa2
title: Can LLMs plan paths in the real world?
arxiv_id: '2411.17912'
source_url: https://arxiv.org/abs/2411.17912
tags:
- mason
- center
- llms
- university
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated three LLMs (GPT-4, Gemini, and Mistral) on
  six real-world path-planning scenarios in various settings and difficulties. All
  LLMs made numerous errors in all scenarios, including major errors like discontinuities,
  wrong directions, and incorrect visual landmarks, as well as minor errors like wrong
  turns in local areas.
---

# Can LLMs plan paths in the real world?

## Quick Facts
- arXiv ID: 2411.17912
- Source URL: https://arxiv.org/abs/2411.17912
- Authors: Wanyi Chen; Meng-Wen Su; Nafisa Mehjabin; Mary L. Cummings
- Reference count: 10
- Key outcome: All tested LLMs (GPT-4, Gemini, Mistral) made numerous errors in all real-world path-planning scenarios, failing to reliably plan paths despite some variations in performance across different task types.

## Executive Summary
This study evaluated three large language models (GPT-4, Gemini, and Mistral) on six real-world path-planning scenarios across various settings and difficulties. All models made numerous errors including discontinuities, wrong directions, and incorrect visual landmarks, demonstrating that current LLMs cannot reliably plan paths in real-world navigation tasks. While GPT-4 showed slight advantages in turn-by-turn navigation and Gemini performed better with visual landmarks, no model consistently outperformed the others.

## Method Summary
The researchers tested three LLMs on six real-world path-planning scenarios: three turn-by-turn navigation tasks and three visual landmark navigation tasks. Each LLM was prompted with specific start and destination pairs along with constraints. Performance was measured by comparing LLM-generated routes against ground truth routes from navigation tools like Waze, with errors categorized by type and severity. The evaluation included both route accuracy and the ability to satisfy time constraints or correctly identify visual landmarks.

## Key Results
- All LLMs made numerous errors across all scenarios, including major errors like discontinuities and wrong directions
- GPT-4 performed slightly better in turn-by-turn navigation scenarios
- Gemini performed better in visual landmark navigation scenarios
- No LLM consistently outperformed the others in reliable path planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with path planning because they lack spatial grounding and real-world geographic knowledge.
- Mechanism: LLMs generate directions based on language patterns rather than actual map data, leading to discontinuities and wrong turns.
- Core assumption: LLMs cannot access or reason over accurate spatial relationships without external tools.
- Evidence anchors:
  - [abstract] "all LLMs made numerous errors in all scenarios, including major errors like discontinuities, wrong directions, and incorrect visual landmarks"
  - [section] "LLMs assumed that the given routes were continuous. However, there were significant gaps and missing routes"
  - [corpus] "Weak - no corpus evidence found for spatial reasoning limitations"
- Break condition: If LLMs are given access to real map APIs or spatial databases, their path planning errors may decrease.

### Mechanism 2
- Claim: LLMs fail to handle time constraints because they cannot reliably retrieve real-time information.
- Mechanism: LLMs attempt to generate time-based plans but fail to verify actual schedules or traffic conditions.
- Core assumption: LLMs lack the ability to perform real-time web searches or access live data feeds.
- Evidence anchors:
  - [abstract] "none of the tested LLMs could plan paths in the real world"
  - [section] "GPT-4 was best at satisfying time constraints... Gemini and Mistral did not satisfy any time constraints"
  - [corpus] "Weak - no corpus evidence found for time constraint handling"
- Break condition: If LLMs are integrated with real-time data sources, they may better satisfy time constraints.

### Mechanism 3
- Claim: LLMs cannot accurately describe visual landmarks because they lack visual perception capabilities.
- Mechanism: LLMs generate descriptions based on text training data rather than actual visual information.
- Core assumption: LLMs cannot "see" images or understand visual scenes without additional vision models.
- Evidence anchors:
  - [abstract] "major errors like... incorrect visual landmarks"
  - [section] "if a landmark didn't exist or the description of the landmark was inaccurate, a weight of 0.2 was given"
  - [corpus] "Weak - no corpus evidence found for visual landmark accuracy"
- Break condition: If LLMs are paired with vision models or access to image databases, landmark descriptions may improve.

## Foundational Learning

- Concept: Spatial reasoning
  - Why needed here: Path planning requires understanding of geographic relationships, distances, and connectivity between locations
  - Quick check question: Can you explain why a discontinuity in a route is more problematic than a wrong turn in a local area?

- Concept: Real-time data retrieval
  - Why needed here: Effective navigation requires current information about traffic, schedules, and road conditions
  - Quick check question: Why would an LLM struggle to plan a trip for a specific game time without internet access?

- Concept: Multimodal integration
  - Why needed here: Combining text, visual, and spatial information is essential for comprehensive navigation guidance
  - Quick check question: What limitations would an LLM face when describing landmarks it cannot see?

## Architecture Onboarding

- Component map:
  LLM core for language processing -> External map/database interface for spatial data -> Real-time data feed integration -> Vision model integration (optional) -> Error checking and validation layer

- Critical path:
  Input parsing → Map query → Route calculation → Time constraint validation → Output generation → Error checking

- Design tradeoffs:
  - Model size vs. performance: Larger models don't necessarily improve path planning
  - Open-source vs. proprietary: Tradeoff between customization and access to advanced features
  - Real-time access vs. privacy: Need for live data vs. data protection concerns

- Failure signatures:
  - Discontinuities in generated routes
  - Wrong highway directions
  - Incorrect or non-existent landmarks
  - Failure to meet specified time constraints

- First 3 experiments:
  1. Test LLM with access to Google Maps API vs. without access
  2. Compare path planning accuracy with and without real-time traffic data
  3. Evaluate landmark description accuracy with vision model integration vs. text-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can smaller, specialized LLMs achieve better path-planning performance than larger, general-purpose models?
- Basis in paper: [explicit] The authors suggest that "future work focus on... training smaller and more specialized models" and note that "one common argument for improving LLM performance is to increase the input data and associated parameters" but this doesn't necessarily lead to better path-planning performance.
- Why unresolved: The study only tested three general-purpose LLMs of varying sizes and found no consistent performance differences between them.
- What evidence would resolve it: Direct comparison of a specialized path-planning model (trained specifically for navigation tasks) against both general-purpose LLMs and traditional path-planning algorithms on the same benchmark scenarios.

### Open Question 2
- Question: How can LLMs be designed to implement effective reality checks for path planning?
- Basis in paper: [explicit] The authors suggest "implementing mechanisms for reality checks" as a future research direction, noting that some LLM-generated paths were so absurd that "even a brief human glance would flag such a wrong solution."
- Why unresolved: Current LLMs cannot initiate quality assurance reality checks, and while researchers have investigated fact-checking for LLMs, these studies involve specifically assigning fact-checking tasks rather than having models determine when to fact-check themselves.
- What evidence would resolve it: A working prototype of an LLM-based navigation system that can automatically detect implausible routes and request verification before providing directions.

### Open Question 3
- Question: How can LLMs be made more transparent about their limitations in path planning?
- Basis in paper: [explicit] The authors advocate for "more in-context transparency about LLMs' limitations" and note that tested models failed to warn users when they lacked sufficient information or were uncertain about results.
- Why unresolved: While current LLMs include disclaimers about potential errors, these are often ignored by users and don't provide context-specific warnings about limitations during task execution.
- What evidence would resolve it: User studies comparing navigation outcomes when using LLMs with enhanced in-context transparency features versus standard LLM interfaces, measuring both task success rates and user trust calibration.

## Limitations
- The study demonstrates LLM failures but cannot definitively separate whether failures stem from model size, architecture, or training approach
- Evaluation relies on qualitative error categorization without quantitative measures of spatial reasoning capability
- Limited testing to only three general-purpose LLMs of varying sizes

## Confidence
- **High confidence**: LLMs make frequent errors in path planning across all tested scenarios and models
- **Medium confidence**: GPT-4 shows slight advantages in turn-by-turn navigation while Gemini performs better with visual landmarks, though no consistent superiority exists
- **Low confidence**: The specific reasons for LLM failures (spatial reasoning limitations vs. lack of real-time data access) cannot be definitively separated from the evidence provided

## Next Checks
1. Test whether providing LLMs direct access to map APIs (Google Maps, OpenStreetMap) significantly reduces discontinuity and wrong direction errors
2. Evaluate if integration with real-time traffic and schedule data improves LLM performance on time-constrained navigation tasks
3. Compare path planning accuracy between text-only LLMs and multimodal models with vision capabilities on visual landmark navigation scenarios