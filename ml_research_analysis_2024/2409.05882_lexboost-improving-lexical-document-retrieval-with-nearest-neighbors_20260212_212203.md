---
ver: rpa2
title: 'LexBoost: Improving Lexical Document Retrieval with Nearest Neighbors'
arxiv_id: '2409.05882'
source_url: https://arxiv.org/abs/2409.05882
tags:
- lexboost
- retrieval
- dense
- document
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LexBoost improves sparse lexical retrieval by leveraging the neighborhood
  structure learned from dense retrieval methods. It builds a corpus graph offline
  using a dense retriever, then enhances document ranking by combining a document's
  lexical score with the mean lexical scores of its nearest neighbors.
---

# LexBoost: Improving Lexical Document Retrieval with Nearest Neighbors

## Quick Facts
- arXiv ID: 2409.05882
- Source URL: https://arxiv.org/abs/2409.05882
- Reference count: 40
- Key outcome: LexBoost achieves MAP improvements of up to 0.0273 on TREC DL 2019 and 0.0232 on TREC DL 2020 over BM25 using offline corpus graph construction and neighbor-based score fusion.

## Executive Summary
LexBoost is a novel approach to improve sparse lexical document retrieval by leveraging the neighborhood structure learned from dense retrieval methods. It builds a corpus graph offline using a dense retriever to find nearest neighbors for each document, then enhances document ranking by combining a document's lexical score with the mean lexical scores of its nearest neighbors. This simple application of the Cluster Hypothesis provides significant effectiveness gains over BM25 and other sparse methods, while maintaining low latency since the corpus graph is constructed offline.

## Method Summary
LexBoost first constructs a corpus graph offline using a dense retrieval approach to identify nearest neighbors for each document. During retrieval, it performs standard lexical retrieval (e.g., BM25) and then boosts each document's score by combining its lexical score with the mean lexical scores of its neighbors, using a convex combination controlled by parameter λ. The method assumes that documents close in embedding space share relevance patterns, and that the offline construction avoids latency overhead during query time.

## Key Results
- LexBoost achieves MAP improvements of up to 0.0273 on TREC DL 2019 and 0.0232 on TREC DL 2020 over BM25
- Effectiveness gains are robust across different lexical retrievers, neighbor counts, and fusion parameters
- LexBoost matches exhaustive dense retrieval in effectiveness with much lower latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The corpus graph captures neighborhood relationships that indicate shared relevance patterns.
- Mechanism: LexBoost builds a corpus graph offline using dense retrieval to find nearest neighbors for each document. These neighbor relationships are assumed to reflect semantic similarity, so documents near each other in embedding space are likely to be relevant to similar queries. During retrieval, LexBoost boosts a document's score by adding the lexical scores of its neighbors, applying the Cluster Hypothesis.
- Core assumption: Document similarity in dense retrieval space correlates with shared relevance to the same queries.
- Evidence anchors:
  - [abstract] "We propose LexBoost that first builds a network of dense neighbors (a corpus graph) using a dense retrieval approach while indexing."
  - [section] "Rather than directly combining sparse and dense scores, our approach estimates relevance by combining a document's lexical score with its neighbors' lexical scores. This is an application of the Cluster Hypothesis [17]: we use a dense model to identify document proximity offline and a sparse model to estimate the relevance of a document (and its neighbors) online."
  - [corpus] Weak: Corpus signals show average neighbor FMR 0.5 and no citations. The corpus evidence does not strongly validate the assumed semantic similarity signal.
- Break condition: If dense retrieval fails to meaningfully group relevant documents together, the neighbor-based score boost will be ineffective or harmful.

### Mechanism 2
- Claim: Offline corpus graph construction avoids latency overhead during query time.
- Mechanism: Building the corpus graph and identifying neighbors is done once at indexing time. At query time, LexBoost only performs a standard lexical retrieval (e.g., BM25) and then looks up precomputed neighbor scores. This avoids the expensive dense retrieval step during retrieval, keeping latency low.
- Core assumption: The cost of offline neighbor computation is acceptable compared to the benefit of faster online retrieval.
- Evidence anchors:
  - [abstract] "Since the neighborhood is identified offline (i.e., at the indexing time), there is essentially no additional query-time latency overhead when using LexBoost as compared to existing lexical retrievers."
  - [section] "The corpus graph creation using dense retrieval method is done at indexing time and does not cause any latency overhead at retrieval."
  - [corpus] Missing: No corpus evidence provided about indexing time cost.
- Break condition: If the corpus graph is too large or indexing becomes prohibitively expensive, the offline construction may not be practical.

### Mechanism 3
- Claim: Convex combination of document score and mean neighbor scores robustly improves retrieval.
- Mechanism: LexBoost uses a weighted average controlled by parameter λ to combine the lexical score of the document itself and the mean lexical scores of its neighbors. This balances the original lexical relevance with the neighborhood signal.
- Core assumption: A convex combination of scores preserves ranking quality and is robust to parameter choice.
- Evidence anchors:
  - [abstract] "We use Convex Combination for fusion of the two scores given its effectiveness [7]."
  - [section] "We define a λ ∈ [0, 1] parameter for fusion of the lexical method score and dense method based insight from the corpus graph. λ parameter is tuned using a validation set for optimal results."
  - [corpus] Missing: No corpus evidence about parameter tuning or combination effectiveness.
- Break condition: If λ is poorly chosen or the neighbor scores are noisy, the convex combination may degrade retrieval performance.

## Foundational Learning

- Concept: Cluster Hypothesis in information retrieval
  - Why needed here: LexBoost's core idea relies on the assumption that documents close in embedding space share relevance patterns. Understanding this hypothesis explains why using neighbor scores can help.
  - Quick check question: What does the Cluster Hypothesis state about document proximity and relevance?

- Concept: Dense vs. sparse retrieval trade-offs
  - Why needed here: LexBoost bridges these two paradigms. Knowing their strengths and weaknesses clarifies why combining them is beneficial.
  - Quick check question: What is the main efficiency advantage of sparse retrieval over dense retrieval?

- Concept: Convex combination and fusion functions
  - Why needed here: LexBoost uses a convex combination to merge document and neighbor scores. Understanding this fusion method is key to tuning and extending the approach.
  - Quick check question: How does a convex combination differ from a reciprocal rank fusion?

## Architecture Onboarding

- Component map:
  - Corpus graph builder (offline) -> Dense retrieval model (e.g., TCT-ColBERT-HNP) to compute nearest neighbors for each document
  - Indexer -> Stores document vectors and neighbor lists
  - Query processor -> Runs lexical retrieval (e.g., BM25) and applies LexBoost fusion using precomputed neighbor scores
  - Tuner -> Validates and selects λ parameter on a held-out set

- Critical path:
  1. Offline: For each document, run dense retrieval to find k nearest neighbors and store the neighbor list.
  2. Online: For a query, run BM25 to get initial scores.
  3. For each scored document, retrieve neighbor scores from BM25 output and compute LexBoost score via convex combination.
  4. Rank documents by LexBoost scores and return results.

- Design tradeoffs:
  - Number of neighbors (k): More neighbors provide richer context but increase memory and computation during fusion.
  - Corpus graph construction method: Choice of dense model affects neighbor quality and graph accuracy.
  - λ tuning: Requires validation data; poor tuning can hurt performance.

- Failure signatures:
  - If dense retrieval embeddings are poor, neighbor lists will be noisy, hurting LexBoost.
  - If λ is set too high, LexBoost reverts close to plain BM25; if too low, neighbor noise dominates.
  - If corpus graph storage is large, offline indexing may be slow or infeasible.

- First 3 experiments:
  1. Verify that nearest neighbor lists from dense retrieval correlate with query relevance by sampling and manual inspection.
  2. Run ablation study varying λ (e.g., 0.5, 0.7, 0.9) on a small dataset to see impact on MAP and nDCG.
  3. Test memory usage and indexing time for different neighbor counts (k=2,4,8,16) to find a practical balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LexBoost perform on extremely large-scale corpora with millions of documents compared to its performance on the datasets used in the paper?
- Basis in paper: [inferred] The paper evaluates LexBoost on TREC DL 2019, TREC DL 2020, and TREC-COVID datasets, which are relatively moderate in size. It does not address performance on extremely large-scale corpora.
- Why unresolved: The scalability of LexBoost to extremely large datasets is not tested, and the impact on latency and effectiveness at such scales is unknown.
- What evidence would resolve it: Empirical evaluation of LexBoost on large-scale corpora with millions of documents, measuring both effectiveness and latency.

### Open Question 2
- Question: Can LexBoost be effectively extended to handle cross-lingual information retrieval tasks?
- Basis in paper: [explicit] The paper mentions that extending LexBoost to Cross-Lingual Information Retrieval (CLIR) and Multi-Lingual Information Retrieval (MLIR) settings is a future work.
- Why unresolved: The paper does not provide any experimental results or analysis for cross-lingual scenarios, leaving the effectiveness of LexBoost in such settings untested.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of LexBoost in cross-lingual retrieval tasks, comparing it to existing methods.

### Open Question 3
- Question: How does the choice of dense retrieval method for constructing the corpus graph affect the performance of LexBoost?
- Basis in paper: [explicit] The paper evaluates LexBoost using corpus graphs constructed with TCT-ColBERT-HNP and TAS-B, showing robustness across different dense retrieval methods.
- Why unresolved: While the paper shows robustness, it does not explore the impact of using other dense retrieval methods or the optimal choice of method for different types of corpora.
- What evidence would resolve it: Comparative analysis of LexBoost performance using various dense retrieval methods across different corpora, identifying the optimal method for specific scenarios.

## Limitations

- The approach relies heavily on the quality of the corpus graph built via dense retrieval, which may not meaningfully group relevant documents if the dense model is poor.
- The method assumes static corpus graphs, which may not adapt well to dynamic collections.
- Offline corpus graph construction could be memory-intensive for large corpora, though this is not explicitly evaluated.

## Confidence

- **High Confidence**: The core mechanism of combining document scores with neighbor scores is well-supported by experimental results, showing consistent MAP improvements over BM25 on TREC DL datasets.
- **Medium Confidence**: The robustness claim across different lexical retrievers and neighbor counts is supported by experiments, but lacks extensive ablation studies to isolate the impact of each component.
- **Low Confidence**: The claim about maintaining low latency is asserted but not empirically validated with concrete indexing or retrieval time measurements.

## Next Checks

1. **Neighbor Quality Validation**: Sample neighbor lists from the corpus graph and manually verify whether documents grouped as neighbors are semantically or topically related, to confirm the Cluster Hypothesis holds in practice.
2. **Parameter Sensitivity Analysis**: Conduct a systematic ablation study varying both neighbor count (k) and λ across a wider range to identify optimal settings and robustness boundaries.
3. **Latency Benchmarking**: Measure actual indexing time for corpus graph construction and query-time latency overhead for LexBoost compared to plain BM25 and dense retrieval baselines.