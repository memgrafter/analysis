---
ver: rpa2
title: Synthesizing Text-to-SQL Data from Weak and Strong LLMs
arxiv_id: '2408.03256'
source_url: https://arxiv.org/abs/2408.03256
tags:
- data
- text-to-sql
- sense
- language
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between open-source and
  closed-source LLMs in text-to-SQL tasks by introducing a synthetic data approach
  that combines strong data generated by larger, more potent models with weak data
  produced by smaller, less well-aligned models. The method enhances domain generalization
  and explores the potential of weak data supervision through preference learning.
---

# Synthesizing Text-to-SQL Data from Weak and Strong LLMs

## Quick Facts
- arXiv ID: 2408.03256
- Source URL: https://arxiv.org/abs/2408.03256
- Authors: Jiaxi Yang; Binyuan Hui; Min Yang; Jian Yang; Junyang Lin; Chang Zhou
- Reference count: 8
- Primary result: SENSE-13B achieves state-of-the-art performance on SPIDER and BIRD benchmarks

## Executive Summary
This paper addresses the performance gap between open-source and closed-source LLMs in text-to-SQL tasks through synthetic data generation. The authors introduce a novel approach that combines weak data from smaller, less aligned models with strong data from larger, more potent models, enhanced through preference learning. This method enables fine-tuning of open-source LLMs to achieve competitive performance without requiring extensive manual annotation. The resulting model, SENSE, demonstrates significant improvements over existing open-source approaches while narrowing the gap with closed-source methods.

## Method Summary
The paper proposes a synthetic data generation framework that leverages both weak and strong LLMs to create high-quality training data for text-to-SQL tasks. The process involves generating initial SQL queries from natural language questions using both small (weak) and large (strong) models, then applying preference learning to rank and select the most promising examples. The synthetic dataset is subsequently used to fine-tune open-source LLMs, with the preference learning component helping to identify and emphasize high-quality training examples while filtering out noise from the weaker model outputs.

## Key Results
- SENSE-13B shows 21.8% improvement over CodeLLaMA-13B-Instruct on the Spider development set
- SENSE outperforms DAIL-SQL by 5.98% on the BIRD test set
- Achieves state-of-the-art results on both SPIDER and BIRD benchmarks

## Why This Works (Mechanism)
The approach succeeds by leveraging the complementary strengths of different model sizes while mitigating their respective weaknesses through preference learning. Weak models provide diverse coverage and help avoid overfitting to specific patterns, while strong models contribute higher-quality examples that establish performance benchmarks. The preference learning mechanism acts as a quality filter, enabling the system to extract maximum value from the combined dataset while suppressing noise and inconsistencies.

## Foundational Learning
- **Synthetic data generation**: Why needed - Reduces dependency on expensive human annotation; Quick check - Compare diversity metrics between synthetic and human-annotated datasets
- **Preference learning**: Why needed - Enables automated quality assessment of generated examples; Quick check - Evaluate ranking consistency across multiple preference criteria
- **Multi-source data integration**: Why needed - Combines strengths of different model capabilities; Quick check - Analyze performance contributions from weak vs. strong data sources

## Architecture Onboarding

**Component map**: Weak LLM -> Strong LLM -> Preference Ranker -> Synthetic Dataset -> Fine-tuning

**Critical path**: Data generation → Preference ranking → Fine-tuning → Evaluation

**Design tradeoffs**: The approach trades computational cost for annotation efficiency, requiring significant processing power for synthetic data generation but eliminating manual labeling costs. The preference learning component adds complexity but enables better data quality control compared to simple aggregation methods.

**Failure signatures**: Performance degradation may occur if preference learning criteria are poorly calibrated, leading to inclusion of low-quality examples. The method may struggle with highly specialized domains where neither weak nor strong models have sufficient training coverage.

**3 first experiments**:
1. Compare performance using only weak data vs. only strong data to quantify the contribution of each source
2. Vary the ratio of weak to strong data to identify optimal mixing proportions
3. Test different preference learning algorithms to evaluate impact on final model quality

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on access to strong LLMs for initial data synthesis may limit reproducibility
- Benchmark-specific improvements may not generalize across all text-to-SQL scenarios
- Computational costs for synthetic data generation could present practical deployment barriers

## Confidence

**High confidence**: Methodology for combining weak and strong synthetic data is well-articulated and reproducible; benchmark results on SPIDER and BIRD are clearly presented and verifiable.

**Medium confidence**: Generalization claims beyond tested benchmarks require additional validation; preference learning component's effectiveness may vary with different ranking criteria.

**Low confidence**: Scalability claims and computational cost estimates lack detailed analysis.

## Next Checks

1. **Domain transfer test**: Evaluate SENSE on non-English text-to-SQL benchmarks and specialized domain databases to assess true generalization capability.

2. **Resource efficiency analysis**: Conduct comprehensive study of computational costs for synthetic data generation across different hardware configurations and model sizes.

3. **Robustness assessment**: Test model performance with adversarial queries and schema variations to evaluate real-world reliability beyond benchmark settings.