---
ver: rpa2
title: 'From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge
  Graphs'
arxiv_id: '2412.18672'
source_url: https://arxiv.org/abs/2412.18672
tags:
- language
- triples
- factual
- knowledge
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in language models by integrating
  curated knowledge graph (KG) triples to anchor responses in empirical data. The
  authors construct a comprehensive KG repository from Wikipedia, focusing on environmental
  sustainability, and select 46 relations tailored for factual grounding.
---

# From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs
## Quick Facts
- arXiv ID: 2412.18672
- Source URL: https://arxiv.org/abs/2412.18672
- Authors: Ratnesh Kumar Joshi; Sagnik Sengupta; Asif Ekbal
- Reference count: 19
- Primary result: Curated knowledge graph integration significantly reduces hallucinations in language models, with FC-RAG achieving highest factual accuracy.

## Executive Summary
This paper addresses hallucinations in language models by integrating curated knowledge graph (KG) triples to anchor responses in empirical data. The authors construct a comprehensive KG repository from Wikipedia, focusing on environmental sustainability, and select 46 relations tailored for factual grounding. Using this KG, they enhance language models to generate responses that are both linguistically fluent and deeply rooted in factual accuracy. Experimental evaluations demonstrate significant improvements in reducing hallucinatory responses, with Retrieval-Augmented Generation (FC-RAG) achieving the highest factual accuracy scores across models. The approach shows promise in improving the reliability and trustworthiness of language model outputs.

## Method Summary
The authors construct a knowledge graph from Wikipedia focusing on environmental sustainability with 46 relations and statistical tail entities. They integrate this KG into language models using three methods: Retrieval-Augmented Generation (RAG), In-Context Learning (IC), and Chain-of-Verification (CoV). The KG triples are retrieved via embedding-based matching, converted to natural language, and used to augment language model responses. The approach is evaluated across multiple models (Llama3, GPT-2, Blenderbot, GODEL, ChatGPT) using automated metrics (BLEU, METEOR, ROUGE-L, CIDEr, SPICE, STCS, EACS, VECS, GMS) and human evaluations (Fluency, Context Relevance, Dialogue Engagingness), plus FacTool for factuality.

## Key Results
- FC-RAG consistently yields the highest improvements in factual accuracy across all models
- Curated knowledge graphs significantly reduce hallucinatory responses compared to baseline language models
- Models augmented with KG triples maintain linguistic fluency while improving factual grounding

## Why This Works (Mechanism)
### Mechanism 1
- **Claim**: Integrating curated knowledge graph triples grounds language model outputs in factual data, reducing hallucinations.
- **Mechanism**: The system retrieves relevant KG triples via embedding-based matching, converts them to natural language, and augments the language model's response with these grounded facts.
- **Core assumption**: The retrieved triples are both relevant to the input context and factually accurate.
- **Evidence anchors**:
  - [abstract]: "By imbuing language models with access to this curated knowledge, we aim to generate both linguistically fluent responses and deeply rooted in factual accuracy and context relevance."
  - [section]: "We meticulously select and integrate relevant KG triples tailored to specific contexts, enhancing factual grounding and alignment with input."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.472. Evidence is moderate; no direct citation of this specific KG triple grounding approach in corpus.
- **Break condition**: If the KG contains outdated, biased, or incomplete data, the grounding could propagate errors instead of reducing hallucinations.

### Mechanism 2
- **Claim**: Retrieval-Augmented Generation (FC-RAG) yields the highest factual accuracy improvements across models.
- **Mechanism**: RAG grounds responses by retrieving external documents and incorporating them during generation, thereby anchoring outputs in verified sources.
- **Core assumption**: Retrieved documents are reliable and relevant to the input context.
- **Evidence anchors**:
  - [abstract]: "Experimental evaluations demonstrate the effectiveness of multiple approaches in reducing hallucinatory responses, underscoring the role of curated knowledge graphs in improving the reliability and trustworthiness of language model outputs."
  - [section]: "The findings show that Retrieval-Augmented Generation (FC-RAG) consistently yields the highest improvements in factual accuracy across all models."
  - [corpus]: Moderate evidence; similar RAG approaches exist but no direct comparison cited in corpus.
- **Break condition**: If retrieval step returns irrelevant or noisy documents, factual accuracy may degrade.

### Mechanism 3
- **Claim**: Selecting relations focused on statistical and numeric data improves grounding precision.
- **Mechanism**: By filtering KG triples to include only those with statistical or numeric tail entities, the system prioritizes empirically verifiable facts.
- **Core assumption**: Statistical triples are more objective and less prone to subjective misinterpretation than qualitative triples.
- **Evidence anchors**:
  - [section]: "We experimented with the triples creation process...We were able to get similar results using both; however, the prompt for Llama3 required a lot more tuning..."
  - [corpus]: No direct evidence in corpus; this is an inferred design choice from the paper.
- **Break condition**: If the KG lacks sufficient statistical triples for a given domain, the grounding may become sparse or irrelevant.

## Foundational Learning
- **Concept**: Cosine similarity for semantic matching.
  - **Why needed here**: Used to align input context with the most relevant KG triples.
  - **Quick check question**: Given two vectors, how do you compute their cosine similarity? (Answer: dot product divided by product of magnitudes.)
- **Concept**: Embedding-based retrieval in RAG.
  - **Why needed here**: Enables efficient lookup of relevant documents or triples based on semantic similarity.
  - **Quick check question**: What is the difference between sparse (e.g., TF-IDF) and dense (e.g., BERT) embeddings? (Answer: Dense embeddings capture semantic meaning; sparse capture term frequency patterns.)
- **Concept**: Hallucination in language models.
  - **Why needed here**: Understanding the phenomenon is essential to evaluate the effectiveness of grounding approaches.
  - **Quick check question**: What distinguishes a factual error from a hallucination in model output? (Answer: Hallucination is generation of plausible but unsupported content; factual error is contradiction with known facts.)

## Architecture Onboarding
- **Component map**: KG Construction → Relation & Entity Selection → Triple Generation → Embedding Index → Input Context → Embedding Matching → Top-k Triple Retrieval → Triple-to-Sentence Conversion → Language Model → Response Generation (+ KG Sentences) → Final Output
- **Critical path**: Context → Embedding Matching → Triple Retrieval → Triple Conversion → LM Response → Output
- **Design tradeoffs**:
  - Retrieval vs. in-context learning: RAG offers richer grounding but adds latency; in-context is faster but limited by prompt size.
  - Triple selection granularity: More relations increase coverage but risk noise; fewer relations increase precision but limit expressiveness.
- **Failure signatures**:
  - KG misses: No relevant triples retrieved → fallback to vanilla LM → possible hallucination.
  - Embedding mismatch: Semantic drift between context and triples → irrelevant grounding → degraded fluency.
  - LM overfitting: Model ignores KG augmentation → hallucinated content persists.
- **First 3 experiments**:
  1. Baseline: Run LM without KG on sample inputs; measure hallucination rate.
  2. KG-only: Retrieve triples for inputs but do not integrate into LM; verify relevance manually.
  3. Full integration: Run FC-RAG pipeline; compare factual accuracy scores (e.g., FacTool) against baseline.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the scalability of curated knowledge graphs impact their effectiveness across different languages and domains beyond environmental sustainability?
- Basis in paper: [explicit] The authors note the need for further investigation into the scalability and generalizability of their method across different downstream tasks and languages.
- Why unresolved: The paper focuses specifically on environmental sustainability, and there is no evidence yet of how well this approach would perform in other domains or languages.
- What evidence would resolve it: Comparative studies showing performance metrics of KG-enhanced language models across multiple domains (healthcare, finance, etc.) and languages, with analysis of how knowledge graph curation needs to adapt for different contexts.

### Open Question 2
- Question: What is the optimal balance between curated knowledge graph size and factual accuracy gains in language model responses?
- Basis in paper: [inferred] The paper mentions constructing a comprehensive KG repository but does not explore how the size or density of the knowledge graph affects performance.
- Why unresolved: The relationship between KG comprehensiveness and hallucination mitigation effectiveness is not quantified, leaving uncertainty about whether larger KGs always yield better results.
- What evidence would resolve it: Controlled experiments varying the size and scope of curated knowledge graphs while measuring factual accuracy improvements and computational overhead.

### Open Question 3
- Question: How do different knowledge graph construction methodologies (e.g., automated vs. human-curated) compare in terms of their effectiveness at reducing hallucinations?
- Basis in paper: [explicit] The authors use human-curated support statements to create their relations, but acknowledge that automated approaches exist.
- Why unresolved: The paper does not compare their human-curated approach with automated knowledge graph construction methods in terms of hallucination reduction effectiveness.
- What evidence would resolve it: Head-to-head comparison of language models enhanced with human-curated versus automatically generated knowledge graphs, measuring hallucination rates and response quality across multiple benchmarks.

## Limitations
- The curated KG may not cover all human knowledge or capture emerging trends, leading to incomplete factual grounding
- Models may introduce biases or overlook contextual nuances when relying on curated KGs, potentially lowering performance in non-fact-related tasks
- Implementation details for embedding matching and triple conversion functions are not fully specified

## Confidence
- Core mechanism (KG grounding reducing hallucinations): Medium
- FC-RAG achieves highest factual accuracy: Medium
- KG selection focused on statistical data: Low

## Next Checks
1. Replicate the embedding-based matching function M with the described cosine similarity approach on a held-out dataset to verify retrieval relevance.
2. Conduct ablation studies comparing FC-RAG against vanilla RAG and other grounding approaches using standardized hallucination detection tools.
3. Test the KG grounding approach across multiple domains beyond environmental sustainability to assess generalizability and identify coverage gaps.