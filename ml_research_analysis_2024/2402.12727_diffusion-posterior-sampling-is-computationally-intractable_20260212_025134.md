---
ver: rpa2
title: Diffusion Posterior Sampling is Computationally Intractable
arxiv_id: '2402.12727'
source_url: https://arxiv.org/abs/2402.12727
tags:
- distribution
- have
- lemma
- sampling
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves that posterior sampling in diffusion models is
  computationally intractable under standard cryptographic assumptions. Specifically,
  it shows that when one-way functions exist, there are distributions for which every
  algorithm requires superpolynomial time to perform posterior sampling, even though
  unconditional sampling is provably fast.
---

# Diffusion Posterior Sampling is Computationally Intractable

## Quick Facts
- arXiv ID: 2402.12727
- Source URL: https://arxiv.org/abs/2402.12727
- Authors: Shivam Gupta; Ajil Jalal; Aditya Parulekar; Eric Price; Zhiyang Xun
- Reference count: 0
- Primary result: Posterior sampling in diffusion models is computationally intractable under standard cryptographic assumptions

## Executive Summary
This paper proves that posterior sampling in diffusion models is computationally intractable under standard cryptographic assumptions. The authors show that when one-way functions exist, there are distributions for which every algorithm requires superpolynomial time to perform posterior sampling, even though unconditional sampling is provably fast. The key insight is that posterior sampling can be used to invert one-way functions, which is computationally hard by assumption. This establishes a fundamental computational barrier for posterior sampling in diffusion models that goes beyond current algorithmic limitations.

## Method Summary
The paper constructs a distribution based on one-way functions where posterior sampling would allow inversion of the function. The construction ensures that unconditional sampling remains efficient by showing that smoothed scores can be well-approximated by polynomial-size neural networks. The authors then prove that under the assumption that one-way functions exist, posterior sampling takes superpolynomial time. Under the stronger assumption that some one-way functions require exponential time to invert, posterior sampling takes at least exponential time in the number of measurements. The upper bound demonstrates that rejection sampling, while correct, takes exponential time in the number of measurements, establishing near-optimality.

## Key Results
- Posterior sampling is superpolynomially hard under the assumption that one-way functions exist
- The unconditional sampling remains efficient (provably fast) even when posterior sampling is hard
- Rejection sampling is essentially optimal, taking exponential time in the number of measurements
- Under stronger assumptions, posterior sampling requires exponential time in the number of measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Posterior sampling in diffusion models is computationally intractable because it can be used to invert one-way functions
- Mechanism: The paper constructs a distribution where sampling from the posterior (given a noisy measurement) reveals information that allows inversion of a one-way function. Since inverting one-way functions is computationally hard, posterior sampling must also be hard
- Core assumption: One-way functions exist (standard cryptographic assumption)
- Evidence anchors:
  - [abstract]: "under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which every algorithm takes superpolynomial time"
  - [section 3.2]: "we show that given any function f, if we can conditionally sample the above measurement process, then we can invert f"
  - [corpus]: Weak - no direct corpus evidence found for this specific cryptographic reduction
- Break condition: If the measurement noise is too large relative to the discretization width, the connection between posterior sampling and function inversion breaks down

### Mechanism 2
- Claim: The unconditional sampling remains efficient even when posterior sampling is hard
- Mechanism: The distribution is designed so that at large smoothing levels, it approximates a product distribution with easy-to-compute scores. This allows diffusion models to sample efficiently without conditioning on measurements
- Core assumption: The smoothed scores can be well-approximated by polynomial-size neural networks
- Evidence anchors:
  - [section 3.3.2]: "at large smoothing levels, a discretized Gaussian looks essentially like an undiscretized Gaussian, and the phase information disappears"
  - [section 3.3.3]: "for small smoothing levels, the score of h at any point x is well approximated by the distribution hr"
  - [corpus]: Weak - no direct corpus evidence found for this specific smoothing behavior
- Break condition: If the smoothing level is neither large enough to approximate product distributions nor small enough to maintain local structure, the approximation breaks down

### Mechanism 3
- Claim: Rejection sampling is essentially optimal for posterior sampling in this setting
- Mechanism: The paper shows that rejection sampling takes exponential time in the number of measurements, and under stronger cryptographic assumptions (one-way functions requiring exponential time to invert), posterior sampling also takes exponential time. This establishes near-optimality of rejection sampling
- Core assumption: Some one-way functions require exponential time to invert with non-negligible probability
- Evidence anchors:
  - [abstract]: "under the further assumption that some one-way function is exponentially hard to invert, there exists a distribution... that takes exponential in m time for posterior sampling"
  - [section 1]: "we know that information-theoretically, it is possible: rejection sampling of the unconditional samples... is very accurate with fairly minimal assumptions. The only problem is that rejection sampling is slow"
  - [corpus]: Weak - no direct corpus evidence found for this specific hardness result
- Break condition: If the measurement noise is very large, the acceptance probability in rejection sampling approaches 1, making it polynomial time

## Foundational Learning

- Concept: One-way functions and their cryptographic significance
  - Why needed here: The hardness of posterior sampling is directly tied to the existence of one-way functions, which are fundamental to cryptography
  - Quick check question: What is the defining property of a one-way function, and why does it matter for computational complexity?

- Concept: Diffusion models and score matching
  - Why needed here: Understanding how diffusion models work (via smoothed scores) is crucial to understanding why unconditional sampling is efficient while posterior sampling is hard
  - Quick check question: How does the score-matching objective relate to the smoothed score, and why does this make unconditional sampling efficient?

- Concept: Rejection sampling and its computational complexity
  - Why needed here: The paper establishes that rejection sampling is essentially optimal, so understanding its mechanics and complexity is important
  - Quick check question: What is the acceptance probability in rejection sampling for linear measurements, and how does it scale with the number of measurements?

## Architecture Onboarding

- Component map: Distribution construction -> Score approximation -> Measurement model -> Posterior sampling algorithms -> Cryptographic reduction

- Critical path: 1) Construct the lower bound distribution using a one-way function 2) Show that unconditional sampling is efficient (distribution is well-modeled) 3) Prove that posterior sampling is hard (implies function inversion) 4) Establish that rejection sampling is essentially optimal

- Design tradeoffs: The distribution must be well-modeled for unconditional sampling while being hard for posterior sampling; the measurement noise must be small enough to preserve the inversion property but large enough to be realistic; the smoothing level must balance between efficient score approximation and maintaining the hardness property

- Failure signatures: If unconditional sampling becomes hard, the distribution construction or score approximation is flawed; if posterior sampling becomes easy, the cryptographic reduction or hardness proof has a gap; if rejection sampling becomes efficient, the measurement model parameters need adjustment

- First 3 experiments: 1) Implement the distribution construction with a simple one-way function (e.g., based on discrete logarithms) and verify unconditional sampling works 2) Implement the posterior sampling hardness proof by attempting to invert the function using a posterior sampler 3) Benchmark rejection sampling on the constructed distribution to verify exponential scaling with measurement count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we identify specific distributional properties beyond one-way functions that would allow efficient posterior sampling in diffusion models?
- Basis in paper: The paper concludes that positive theory for posterior sampling must invoke distributional assumptions on the data, as their lower bound distribution is derived from a one-way function and not very "nice"
- Why unresolved: The paper only shows computational hardness under cryptographic assumptions but doesn't characterize what makes a distribution amenable to posterior sampling
- What evidence would resolve it: Identification of distributional properties (e.g., smoothness, bounded score norms, specific structural assumptions) under which efficient posterior sampling algorithms can be proven to exist

### Open Question 2
- Question: Are there approximation algorithms for posterior sampling that provide provable guarantees on specific classes of distributions or measurements?
- Basis in paper: The paper shows hardness of exact posterior sampling but mentions that "people run algorithms that attempt to approximate the posterior sampling every day" and "they might not be perfectly accurate, but they seem to do a decent job"
- Why unresolved: While the paper proves computational intractability of exact posterior sampling, it doesn't explore whether approximation algorithms can provide useful guarantees under restricted conditions
- What evidence would resolve it: Analysis showing conditions under which approximation algorithms provide provable bounds on Wasserstein distance or TV distance to the true posterior

### Open Question 3
- Question: How does the computational complexity of posterior sampling scale with different measurement models beyond the linear measurement model studied in the paper?
- Basis in paper: The paper focuses specifically on linear measurement models (y = Ax + η) but acknowledges that posterior sampling has applications in various tasks like inpainting, super-resolution, and MRI reconstruction
- Why unresolved: The paper's lower bound is proven only for linear measurements, leaving open whether other measurement models (e.g., nonlinear, structured, or sparse measurements) have different computational complexity characteristics
- What evidence would resolve it: Extension of the lower bound techniques to other measurement models or development of efficient algorithms for specific non-linear measurement scenarios

## Limitations

- The paper assumes worst-case hardness of one-way functions, which may not directly translate to average-case hardness for posterior sampling on natural distributions
- The exponential time lower bound requires stronger assumptions about the existence of exponentially hard one-way functions, going beyond the basic one-way function assumption
- The practical relevance depends on the specific choice of one-way function and measurement model parameters, which are not concretely instantiated in the paper

## Confidence

- Cryptographic reduction hardness: High confidence
- Unconditional sampling efficiency: High confidence
- Rejection sampling optimality: Medium confidence
- Practical implications: Low confidence

## Next Checks

1. **Concrete Function Test**: Implement the lower bound distribution construction using a specific one-way function candidate (e.g., based on discrete logarithms or subset sum) and empirically verify that unconditional sampling remains efficient while posterior sampling attempts fail to invert the function within polynomial time

2. **Score Approximation Verification**: Construct the ReLU network approximation of smoothed scores for the lower bound distribution and empirically validate the approximation quality and parameter scaling as claimed in Theorem C.21

3. **Measurement Model Sensitivity**: Systematically vary the measurement noise parameters (β, ε, R) and quantify how the hardness of posterior sampling changes, particularly testing the claimed break conditions where the cryptographic reduction fails