---
ver: rpa2
title: Neural Text Normalization for Luxembourgish using Real-Life Variation Data
arxiv_id: '2412.09383'
source_url: https://arxiv.org/abs/2412.09383
tags:
- luxembourgish
- data
- byt5
- normalization
- variation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first sequence-to-sequence text normalization
  models for Luxembourgish using real-life variation data from an online spellchecker.
  The authors created training data by replacing words in correct texts with variants
  based on actual usage frequencies, then fine-tuned ByT5 and mT5 models.
---

# Neural Text Normalization for Luxembourgish using Real-Life Variation Data

## Quick Facts
- arXiv ID: 2412.09383
- Source URL: https://arxiv.org/abs/2412.09383
- Authors: Anne-Marie Lutgen; Alistair Plum; Christoph Purschke; Barbara Plank
- Reference count: 18
- Primary result: ByT5 base model achieved ERR of 0.26, outperforming mT5 (ERR -5.70), Llama (ERR -0.15), and spellux pipeline (ERR 0.39)

## Executive Summary
This paper presents the first sequence-to-sequence text normalization models for Luxembourgish, a low-resource language with high orthographic variation. The authors created training data by replacing words in correct Chamber of Deputies transcripts with real-life spelling variants from an online spellchecker, capturing actual usage frequencies. ByT5 base model fine-tuned on this data achieved an error reduction rate of 0.26, demonstrating that byte-level modeling is particularly effective for this normalization task. A linguistically-informed qualitative evaluation revealed that while ByT5 excels at handling specific orthographic rules like consonant <s> (80% success), it struggles with French loanwords (10% success).

## Method Summary
The authors created 833,000 parallel sentence pairs by replacing words in correct Chamber of Deputies transcripts with real-life spelling variants from Spellchecker.lu, using frequency data to guide variant selection. They fine-tuned ByT5 and mT5 models with byte-level and word-level tokenization respectively, using specific hyperparameters (ByT5 base: batch size 16, learning rate 1e-4, sequence length 256, 3 epochs). The models were evaluated on RTL user comments (459 sentences, 7,146 tokens) using Error Reduction Rate (ERR) as the primary metric, supplemented by a linguistically-informed qualitative evaluation testing 21 orthographic rules.

## Key Results
- ByT5 base model achieved ERR of 0.26, significantly outperforming mT5 (ERR -5.70), Llama (ERR -0.15), and spellux pipeline (ERR 0.39)
- Byte-level modeling proved more effective than word-level modeling for Luxembourgish normalization
- ByT5 excelled at handling consonant <s> orthographic rules (80% success rate) but struggled with French loanwords (10% success rate)
- Qualitative evaluation revealed systematic patterns of model strengths and weaknesses across different orthographic categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Byte-level modeling (ByT5) is more effective for Luxembourgish normalization than word-level (mT5) because it captures orthographic variation patterns without relying on vocabulary coverage.
- Mechanism: ByT5 operates directly on UTF-8 encoded bytes rather than tokenized words, allowing it to handle spelling variants and morphological variations that may not appear in the training vocabulary.
- Core assumption: Orthographic variation in Luxembourgish follows consistent patterns that can be captured at the byte level, even when specific word forms are rare or absent from the training data.
- Evidence anchors:
  - [abstract] "byte-based models showing particular promise for this low-resource language"
  - [section] "The robustness to noise and variation of byte-based and character-based models (Xue et al., 2022) makes them ideal models to fine-tune for normalization in Luxembourgish"
- Break condition: If orthographic variation in Luxembourgish is too irregular or context-dependent, byte-level patterns may not capture meaningful structure, reducing the advantage over word-level models.

### Mechanism 2
- Claim: Using real-life variation data from spellchecker corrections creates more effective training data than synthetic variants because it reflects actual usage patterns and frequencies.
- Mechanism: Training data generated from user-corrected text captures the distribution of real spelling variants as they occur in practice, including their relative frequencies and contextual patterns.
- Core assumption: The spellchecker.lu data represents a comprehensive and representative sample of Luxembourgish orthographic variation as actually used by speakers.
- Evidence anchors:
  - [section] "Using real-life variation to create training data ensures that each variant in the data has actually been used by people and is not just a random character replacement"
  - [section] "the frequency of use of these variants can be represented in the data realistically"
- Break condition: If the spellchecker data is biased toward certain user groups, domains, or error types, the training data may not generalize well to other types of Luxembourgish text.

### Mechanism 3
- Claim: A linguistically-informed qualitative evaluation reveals model strengths and weaknesses that quantitative metrics alone cannot capture, enabling targeted improvements.
- Mechanism: By testing models against specific orthographic rules and contexts, researchers can identify systematic patterns of success and failure that inform model selection and future development.
- Core assumption: Luxembourgish orthography follows identifiable rules and patterns that can be tested systematically, and model performance varies predictably across these patterns.
- Evidence anchors:
  - [abstract] "A linguistically-informed qualitative evaluation revealed that ByT5 excelled at handling specific orthographic rules like consonant <s> (80% success) while struggling with French loanwords (10% success)"
  - [section] "These tests include two different setups and implement 21 orthographic rules"
- Break condition: If Luxembourgish orthography is too irregular or if model failures are random rather than systematic, qualitative evaluation may not reveal meaningful patterns for improvement.

## Foundational Learning

- Concept: Sequence-to-sequence modeling fundamentals
  - Why needed here: Understanding how encoder-decoder architectures work is essential for grasping why ByT5 and mT5 are suitable for normalization tasks
  - Quick check question: What is the key difference between sequence-to-sequence models and other neural architectures like BERT?

- Concept: Byte-level vs character-level vs word-level tokenization
  - Why needed here: The choice of tokenization strategy significantly impacts model performance on low-resource languages with high orthographic variation
  - Quick check question: How does UTF-8 byte encoding differ from character encoding, and why might this matter for text normalization?

- Concept: Error reduction rate (ERR) metric
  - Why needed here: ERR is the primary evaluation metric for normalization tasks and differs from standard accuracy metrics
  - Quick check question: How does ERR differ from simple accuracy, and why is it more appropriate for normalization tasks?

## Architecture Onboarding

- Component map: ByT5 base model (582M parameters) → fine-tuning pipeline → real-life variation training data → Chamber of Deputies transcripts → spellchecker.lu variant dictionary
- Critical path: Training data creation → model fine-tuning → quantitative evaluation → qualitative evaluation → model comparison
- Design tradeoffs: Byte-level modeling trades vocabulary coverage for robustness to spelling variation; real-life data trades control for authenticity
- Failure signatures: Hallucination (repeating training data), stopping early on long sentences, false corrections on correct input, systematic failures on specific orthographic rules
- First 3 experiments:
  1. Test ByT5 base model on small validation set to establish baseline performance before full training
  2. Compare byte-level vs character-level tokenization on a subset of training data to verify mechanism 1
  3. Test model performance on synthetic vs real-life variation data to validate mechanism 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ByT5 model handle code-switching between Luxembourgish and its contact languages (French, German, English) in user-generated text?
- Basis in paper: [explicit] The paper mentions that French and German are contact languages for Luxembourgish, and that the Chamber of Deputies texts contain many French loanwords, but doesn't specifically test code-switching handling
- Why unresolved: The evaluation focuses on monolingual Luxembourgish normalization and doesn't test the model's ability to handle code-switched text where multiple languages appear in the same sentence
- What evidence would resolve it: Testing the ByT5 model on a corpus of code-switched Luxembourgish-French-German text would reveal whether it can correctly normalize words from different languages while maintaining appropriate language boundaries

### Open Question 2
- Question: Would fine-tuning ByT5 with more epochs and longer sequence lengths eliminate the hallucination and early stopping issues observed in the experiments?
- Basis in paper: [explicit] The paper mentions that ByT5 models exhibit hallucination (repeating training data) and stopping early with long sentences, and that increasing epochs and sequence length reduced these issues to 5% and 2% respectively
- Why unresolved: The paper states these improvements were observed but doesn't provide systematic analysis of whether these issues are completely eliminated or what the optimal hyperparameters would be
- What evidence would resolve it: Conducting a systematic hyperparameter search with varying epoch counts and sequence lengths while measuring hallucination and early stopping rates would determine the optimal configuration

### Open Question 3
- Question: How would the normalization performance change if the training data incorporated variant frequencies from a broader range of sources beyond the Spellchecker.lu website?
- Basis in paper: [explicit] The paper acknowledges that Spellchecker.lu variants don't reflect all possible variants in Luxembourgish and only represent users of that website
- Why unresolved: The study uses only one source for variant frequency data, which may bias the model toward specific writing patterns and miss other legitimate orthographic variants used by different demographic groups
- What evidence would resolve it: Creating training data using variant frequencies from multiple sources (social media, formal writing, regional dialects) and comparing model performance would reveal whether broader frequency coverage improves normalization accuracy

### Open Question 4
- Question: Could a hybrid approach combining the ByT5 model with the spellux pipeline achieve better performance than either method alone?
- Basis in paper: [inferred] The paper shows that ByT5 and spellux have complementary strengths in different orthographic categories, with ByT5 excelling at consonant <s> handling and spellux performing better with consonant <g>
- Why unresolved: The evaluation compares models independently but doesn't explore whether combining their strengths through an ensemble approach would yield superior results
- What evidence would resolve it: Implementing a hybrid system where the ByT5 model and spellux pipeline vote on corrections or use a confidence-based selection mechanism would demonstrate whether combining approaches improves overall accuracy

### Open Question 5
- Question: How sensitive is the ByT5 model's performance to the quality and standardization level of the source texts used for creating training data?
- Basis in paper: [explicit] The paper uses Chamber of Deputies transcriptions as source texts, assuming they are orthographically correct, but doesn't validate this assumption or test the model's sensitivity to source text quality
- Why unresolved: The study assumes the Chamber texts are error-free without verification, and doesn't test whether using texts with varying degrees of orthographic correctness would affect model performance
- What evidence would resolve it: Creating multiple training datasets from source texts with different quality levels (perfect orthography, minor errors, significant variation) and measuring the resulting model performance would reveal the sensitivity to training data quality

## Limitations

- Data representativeness uncertainty: The study relies heavily on spellchecker.lu data, which may over-represent certain user populations or error types, creating potential domain bias in the training data.
- Model generalizability concerns: While ByT5 shows strong performance, the evaluation is primarily on RTL user comments, and systematic failures on French loanwords (10% success) suggest potential limitations for multilingual contexts.
- Low-resource language constraints: As the first text normalization work for Luxembourgish, there are no established baselines or comprehensive test suites to benchmark against.

## Confidence

**High confidence**: The core finding that byte-level models outperform word-level models for Luxembourgish normalization is well-supported by both quantitative metrics (ERR of 0.26 vs -5.70 for mT5) and qualitative analysis showing consistent patterns of success on orthographic rules.

**Medium confidence**: The mechanism explaining why real-life variation data is superior to synthetic variants is plausible but could benefit from direct comparison studies. The claim about byte-level modeling capturing orthographic patterns is supported but could be more rigorously tested.

**Low confidence**: The generalizability of these results to other low-resource languages with high orthographic variation remains speculative without testing on additional languages.

## Next Checks

**Check 1**: Conduct cross-domain evaluation by testing the best-performing ByT5 model on historical Luxembourgish texts, formal government documents, and social media from different platforms to assess generalizability beyond RTL user comments.

**Check 2**: Perform ablation studies comparing training data created from real-life spellchecker corrections versus synthetic variants generated through character-level perturbations, controlling for frequency distributions to isolate the effect of data authenticity.

**Check 3**: Test the byte-level modeling hypothesis by training character-level models (like Charformer) alongside ByT5 and comparing their performance on the same test sets to determine whether UTF-8 byte encoding provides unique advantages over character-level approaches for this task.