---
ver: rpa2
title: Effective and Lightweight Representation Learning for Link Sign Prediction
  in Signed Bipartite Graphs
arxiv_id: '2412.18720'
source_url: https://arxiv.org/abs/2412.18720
tags:
- signed
- bipartite
- message
- passing
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ELISE, an effective and lightweight graph neural
  network approach for learning representations in signed bipartite graphs. The authors
  address the limitations of existing methods, which suffer from over-smoothing and
  noisy interactions due to naive message passing designs, as well as computational
  inefficiency from adding numerous edges and weight matrices.
---

# Effective and Lightweight Representation Learning for Link Sign Prediction in Signed Bipartite Graphs

## Quick Facts
- arXiv ID: 2412.18720
- Source URL: https://arxiv.org/abs/2412.18720
- Authors: Gyeongmin Gu; Minseo Jeon; Hyun-Je Song; Jinhong Jung
- Reference count: 40
- Outperforms state-of-the-art methods by up to 4.96% in Macro-F1 and 13.66% in AUC

## Executive Summary
This paper introduces ELISE, a novel graph neural network approach for link sign prediction in signed bipartite graphs. The method addresses key limitations of existing approaches - over-smoothing from naive message passing and computational inefficiency from adding numerous edges and weight matrices. ELISE introduces signed personalized message passing that incorporates personalized features during message passing while adhering to balance theory, and refined message passing that performs computations on a low-rank approximation of the graph to reduce noise. The method avoids adding edges between nodes of the same type and excludes additional weight matrices at each layer and auxiliary loss optimizations.

## Method Summary
ELISE operates on signed bipartite graphs by first preprocessing the graph to compute normalized signed biadjacency matrices and SVD results for low-rank approximation. The method then performs signed personalized message passing with injection of personalized features on both the original and refined (low-rank) graphs. These two encoders are combined through weighted aggregation and concatenated to form final node representations. A simple MLP classifier is trained using binary cross-entropy loss to predict link signs. The lightweight design eliminates edge insertion between same-type nodes and excludes layer-specific weight matrices and auxiliary losses, improving computational efficiency while maintaining expressiveness.

## Key Results
- Achieves up to 4.96% improvement in Macro-F1 compared to state-of-the-art methods
- Improves AUC by up to 13.66% over existing approaches
- Provides faster training and inference times through lightweight design
- Eliminates memory issues that cause previous methods (SBGNN, SBGCL) to fail on larger graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Signed personalized message passing mitigates over-smoothing by injecting personalized features during message passing.
- Mechanism: Extends personalized PageRank (SRWR) to node embeddings in signed bipartite graphs, incorporating restart probability to preserve local structure.
- Core assumption: Over-smoothing occurs due to averaging operations in standard message passing, and personalized features can counteract this effect.
- Evidence anchors:
  - [abstract]: "injecting personalized features during message passing while adhering to balance theory"
  - [section]: "P(l)V ← (1 − c) · (˜RV+←U · P(l−1)U + ˜RV−←U · M(l−1)U) + c · XV"
  - [corpus]: Weak - corpus neighbors focus on link sign prediction but do not directly address over-smoothing mechanisms.
- Break condition: If injection ratio c is too large, the method degenerates to using only initial features without learning from neighbors.

### Mechanism 2
- Claim: Refined message passing on low-rank approximation reduces noise and emphasizes global structure.
- Mechanism: Applies truncated SVD to create a refined graph, then performs signed personalized message passing on this approximation.
- Core assumption: Real-world signed bipartite graphs contain noisy interactions that can be filtered through low-rank approximation.
- Evidence anchors:
  - [abstract]: "performs computations on a low-rank approximation of the graph to reduce noise"
  - [section]: "We refine the given signed bipartite graph using low-rank approximation, effectively eliminating such noisy interactions"
  - [corpus]: Missing - corpus does not contain evidence about low-rank approximation techniques.
- Break condition: If target rank is too low, important structural information may be lost; if too high, noise reduction benefits diminish.

### Mechanism 3
- Claim: Lightweight design improves computational efficiency by avoiding additional edges and weight matrices.
- Mechanism: Eliminates edge insertion between same-type nodes and excludes layer-specific weight matrices and auxiliary losses.
- Core assumption: Computational inefficiency in existing methods stems from heavy design choices that add significant overhead.
- Evidence anchors:
  - [abstract]: "designing it to be lightweight, unlike the previous methods that add too many edges and cause inefficiency"
  - [section]: "We encapsulate our encoders into ELISE, designing it to be lightweight by avoiding the addition of edges between nodes of the same type and excluding additional weight matrices at each layer"
  - [corpus]: Weak - corpus neighbors focus on link sign prediction but do not discuss computational efficiency mechanisms.
- Break condition: If model capacity is too constrained, representation power may be insufficient for complex graphs.

## Foundational Learning

- Concept: Signed bipartite graphs and balance theory
  - Why needed here: The method operates specifically on signed bipartite graphs and must adhere to balance theory for correct message passing
  - Quick check question: What are signed butterfly patterns and why are they important for understanding balance in signed bipartite graphs?

- Concept: Personalized PageRank and random walk with restart
  - Why needed here: The signed personalized message passing extends SRWR from scores to embeddings, requiring understanding of restart mechanisms
  - Quick check question: How does the restart probability c affect the trade-off between personalization and global information in SRWR?

- Concept: Low-rank approximation and SVD
  - Why needed here: Refined message passing relies on truncated SVD to create a noise-reduced graph representation
  - Quick check question: What is the relationship between target rank k and the quality of the low-rank approximation in preserving graph structure?

## Architecture Onboarding

- Component map:
  Input layer: Initial node feature matrices XU and XV
  Signed personalized message passing encoder: Two positive/negative embedding matrices with restart probability c
  Refined message passing encoder: Low-rank approximation with target rank ratio r
  Layer-wise aggregation: Weighted combination of embeddings across layers
  Final representation: Concatenation of both encoder outputs
  Output layer: MLP classifier for link sign prediction

- Critical path:
  1. Preprocess: Compute signed biadjacency matrices and their semi-normalized forms
  2. Compute SVD results for low-rank approximation (once)
  3. Perform signed personalized message passing on original graph
  4. Perform signed personalized message passing on refined graph
  5. Aggregate results and concatenate
  6. Train classifier for link sign prediction

- Design tradeoffs:
  - Computational efficiency vs. representation power: Avoiding edge insertion reduces computation but may miss some relationships
  - Noise reduction vs. information preservation: Low-rank approximation reduces noise but may lose fine-grained details
  - Model complexity vs. generalization: Lightweight design improves efficiency but may limit expressiveness

- Failure signatures:
  - Overfitting: High validation accuracy but poor test performance, especially on refined message passing
  - Underfitting: Both validation and test accuracy remain low despite training
  - Memory issues: SVD computation fails on large graphs with high rank ratios
  - Slow convergence: Training loss plateaus early, suggesting initialization or learning rate issues

- First 3 experiments:
  1. Ablation study: Compare ELISE with and without refined message passing on a small dataset to measure noise reduction impact
  2. Layer sensitivity: Vary the number of layers L from 1 to 5 on a medium-sized dataset to observe over-smoothing effects
  3. Rank ratio sweep: Test different values of r on a dense graph to find the optimal trade-off between noise reduction and information preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the efficiency of ELISE for extremely large signed bipartite graphs while maintaining predictive performance?
- Basis in paper: [explicit] The paper discusses computational inefficiency of previous methods due to added edges and mentions ELISE is lightweight, but doesn't explore scaling to truly massive graphs
- Why unresolved: The paper focuses on efficiency gains over specific baselines but doesn't systematically explore how ELISE scales with graph size, particularly when dealing with billions of nodes or edges
- What evidence would resolve it: Empirical results showing ELISE performance and resource usage on graphs orders of magnitude larger than tested, along with specific optimizations for distributed or streaming settings

### Open Question 2
- Question: What is the optimal target rank ratio (r) for different types of signed bipartite graphs across various domains?
- Basis in paper: [explicit] The paper mentions target rank ratio as a hyperparameter and shows it affects performance differently on different datasets, but doesn't provide domain-specific guidelines
- Why unresolved: The paper only tests a limited range of r values on four datasets and shows performance varies significantly, but doesn't establish systematic principles for choosing r
- What evidence would resolve it: Comprehensive analysis across diverse domains (e-commerce, social networks, etc.) showing how r should be chosen based on graph properties like density, sign distribution, and node types

### Open Question 3
- Question: How do the signed personalized message passing and refined message passing components interact in different graph structures, and when might one be preferred over the other?
- Basis in paper: [explicit] The ablation study shows both components contribute to performance, but doesn't systematically analyze their interaction or provide guidance on when each is most valuable
- Why unresolved: The paper demonstrates that combining both components works well but doesn't explore scenarios where one component might dominate or how graph characteristics influence this balance
- What evidence would resolve it: Controlled experiments varying graph properties (sparsity, sign imbalance, community structure) to identify when each component provides the most benefit and how they complement each other

## Limitations

- The effectiveness of low-rank approximation for noise reduction is not thoroughly validated by the corpus evidence
- The method's scalability to extremely large graphs with billions of nodes or edges is not explored
- Optimal hyperparameter settings (particularly target rank ratio r) are not systematically determined across different domains

## Confidence

- Signed personalized message passing effectiveness: Medium-High
- Low-rank approximation noise reduction: Medium
- Computational efficiency claims: Medium
- Overall method effectiveness: Medium-High

## Next Checks

1. Test different target rank ratios (r) on dense graphs to empirically determine the optimal trade-off between noise reduction and information preservation
2. Conduct an ablation study comparing ELISE with and without refined message passing to quantify the noise reduction benefits
3. Verify the claim that avoiding edge insertion between same-type nodes prevents memory issues by testing on graphs where previous methods failed due to memory constraints