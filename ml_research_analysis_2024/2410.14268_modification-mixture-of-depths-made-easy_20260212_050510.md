---
ver: rpa2
title: 'MoDification: Mixture of Depths Made Easy'
arxiv_id: '2410.14268'
source_url: https://arxiv.org/abs/2410.14268
tags:
- modification
- latency
- efficiency
- zhang
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoDification is a method to transform any existing large language
  model (LLM) into a mixture-of-depths (MoD) variant without starting from scratch.
  The key innovation is replacing the top-k operator in MoD with a threshold-p operator,
  which reduces computational cost and allows more flexible token pruning.
---

# MoDification: Mixture of Depths Made Easy

## Quick Facts
- **arXiv ID**: 2410.14268
- **Source URL**: https://arxiv.org/abs/2410.14268
- **Authors**: Chen Zhang; Meizhi Zhong; Qimeng Wang; Xuantao Lu; Zheyu Ye; Chengqiang Lu; Yan Gao; Yao Hu; Kehai Chen; Min Zhang; Dawei Song
- **Reference count**: 18
- **One-line primary result**: Transforms any LLM into MoD variant with ~1.2× speedup and ~1.8× memory reduction while preserving performance

## Executive Summary
MoDification provides a method to convert existing large language models into mixture-of-depths (MoD) variants without starting from scratch. The approach replaces the traditional top-k operator with a threshold-p operator for more flexible token pruning, and introduces a layer load-reducing objective to improve efficiency. By training on ~10B diverse tokens, MoDification achieves significant efficiency gains (up to 1.2× speedup and 1.8× memory reduction) across model scales from 3B to 70B while maintaining most of the original model's performance. The method outperforms traditional MoD approaches, particularly in long-context scenarios.

## Method Summary
MoDification transforms existing LLMs by implementing a threshold-p operator that dynamically selects tokens to retain based on learned importance scores, replacing the rigid top-k approach. The method adds a layer load-reducing objective that minimizes the product of token dispatch fractions and routing probabilities to encourage more tokens to skip computations. Training uses a diverse data mixture of 10.5B tokens across 9 sources including CCI, Wikipedia, BookCorpus, and others, with models trained for approximately 10B tokens using DeepSpeed, FlashAttention, and gradient checkpointing. The approach is validated across multiple model scales (3B to 70B) including MiniMA-2-3B and LLaMA-2 variants.

## Key Results
- Achieves up to 1.2× latency speedup and 1.8× memory reduction compared to original LLMs
- Outperforms traditional MoD in both efficiency and effectiveness metrics
- Maintains strong performance on benchmarks including MMLU, CEval, DROP, BBH, HumanEval, and GSM8k
- Particularly effective in long-context scenarios (up to 2048 tokens)

## Why This Works (Mechanism)

### Mechanism 1: Threshold-p Operator
Replacing the top-k operator with a threshold-p operator enables more flexible token pruning while reducing computational overhead. The threshold-p operator dynamically determines which tokens to retain based on a learned threshold value, rather than fixing the number of retained tokens as top-k does. This allows the model to skip more computations when fewer tokens are important, and preserve more computations when more tokens matter. The core assumption is that the threshold-p operator can effectively learn to identify token importance across different layers and contexts.

### Mechanism 2: Layer Load-Reducing Objective
The layer load-reducing objective improves efficiency by encouraging more tokens to be routed to the NoOp path. The objective minimizes the product of the fraction of tokens dispatched to each layer (Fj) and the fraction of routing probability to that layer (Gj), effectively pushing more tokens to skip computation in that layer. The core assumption is that reducing layer load preserves model performance while achieving significant computational savings.

### Mechanism 3: Efficient Data Training
Training on diverse data for ~10B tokens is sufficient to convert existing LLMs to MoD variants without extensive retraining. Diverse data provides sufficient variation in token importance patterns across different domains, allowing the model to learn effective gating strategies without needing the full pretraining corpus. The core assumption is that 10B tokens of diverse data captures enough distributional variation for the MoD conversion to generalize well.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) architecture**
  - Why needed here: MoDification can be interpreted as a two-expert MoE where one expert is NoOp, making MoE concepts directly applicable
  - Quick check question: How does the gating mechanism in MoE differ from the gating mechanism in MoDification?

- **Concept: Layer significance and importance scoring**
  - Why needed here: Understanding how to measure and utilize layer significance is crucial for implementing effective threshold-p operators and load-reducing objectives
  - Quick check question: What metrics would you use to evaluate whether your threshold-p operator is preserving important computations?

- **Concept: Sparse computation and conditional execution**
  - Why needed here: MoDification fundamentally relies on skipping computations for certain tokens, requiring understanding of how to implement and optimize conditional execution patterns
  - Quick check question: How would you measure the computational savings achieved by MoDification versus the overhead of the gating mechanisms?

## Architecture Onboarding

- **Component map**: Input tokens → gate network → threshold-p selection → conditional attention/MLP application → output tokens
- **Critical path**: The critical path is: input tokens → gate network → threshold-p selection → conditional attention/MLP application → output tokens. The gate network and threshold-p operator must be optimized for speed as they operate on every token.
- **Design tradeoffs**: The main tradeoff is between computational savings and performance preservation. More aggressive token skipping yields greater efficiency but risks performance degradation. The threshold value p and load-reducing coefficient α must be tuned to find the optimal balance.
- **Failure signatures**: Common failure modes include: the model skipping too many tokens and losing important information, the threshold-p operator becoming a bottleneck that negates efficiency gains, or the model failing to learn meaningful gating patterns leading to random token skipping.
- **First 3 experiments**:
  1. Implement the threshold-p operator and verify it correctly selects tokens based on the gate scores, measuring the computational overhead compared to top-k.
  2. Train a small model with the layer load-reducing objective and observe how the fraction of skipped tokens (Fj) changes across layers.
  3. Compare the inference latency and memory usage of MoDification versus the original model on a fixed input, ensuring the efficiency gains are realized.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal threshold value p for MoDification across different model scales and domains?
  - Basis in paper: The paper discusses the impact of the threshold value p on efficiency and effectiveness, noting that smaller values reduce memory consumption but may increase latency, and that larger models may require larger p values.
  - Why unresolved: The paper only provides specific p values for certain model scales and does not explore a comprehensive range of values across different domains and tasks.
  - What evidence would resolve it: A systematic study varying p values across multiple model scales, domains, and tasks to identify the optimal threshold value for each scenario.

## Limitations

- The computational overhead comparison between threshold-p and top-k operators lacks quantitative validation
- The layer load-reducing objective requires manual tuning of hyperparameter α without sensitivity analysis
- Claims about 10B tokens being sufficient for reliable MoDification lack rigorous validation across truly out-of-distribution tasks

## Confidence

**High Confidence Claims**:
- The MoDification framework can technically convert existing LLMs to MoD variants using the threshold-p operator and layer load-reducing objective
- The efficiency improvements (latency and memory reduction) are measurable and reproducible under the reported experimental conditions

**Medium Confidence Claims**:
- The relative performance of MoDification versus original models on the tested benchmarks (MMLU, CEval, etc.)
- The claim that MoDification outperforms MoD in both efficiency and effectiveness

**Low Confidence Claims**:
- The generalizability of results to other model architectures beyond LLaMA-2 and MiniMA-2
- The sufficiency of 10B diverse tokens for reliable MoDification across different domains
- The long-term stability and robustness of models trained with the load-reducing objective

## Next Checks

**Check 1: Computational Overhead Quantification**
Implement both top-k and threshold-p operators in an isolated benchmark, measuring their computational costs (FLOPs, wall-clock time) across different sequence lengths and sparsity levels. Compare the marginal cost of the threshold-p operator against the flexibility gains in token retention rates.

**Check 2: Load-Reducing Objective Sensitivity**
Train multiple MoDification variants with α values spanning [0.001, 0.01, 0.1, 1.0] on a 3B model, measuring efficiency gains, performance degradation, and layer-wise token retention patterns. Identify the optimal α range and test whether performance remains stable across this range.

**Check 3: Long-context Robustness Validation**
Evaluate MoDification models on extended context lengths (4K-16K tokens) beyond the reported 2K maximum, testing whether the efficiency gains scale linearly or whether performance degradation becomes significant at longer contexts. Compare against both original models and other MoD variants under identical conditions.