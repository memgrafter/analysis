---
ver: rpa2
title: Deep Neural Network Initialization with Sparsity Inducing Activations
arxiv_id: '2402.16184'
source_url: https://arxiv.org/abs/2402.16184
tags:
- relu
- sparsity
- activation
- networks
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of sparse-inducing activation functions
  for training deep neural networks. The authors show that natural choices like shifted
  ReLU and soft thresholding suffer from a previously unreported training instability
  at the edge of chaos initialization.
---

# Deep Neural Network Initialization with Sparsity Inducing Activations

## Quick Facts
- **arXiv ID:** 2402.16184
- **Source URL:** https://arxiv.org/abs/2402.16184
- **Reference count:** 40
- **Primary result:** Magnitude-clipped sparsifying activations enable training deep networks with up to 85% activation sparsity while maintaining near-full accuracy.

## Executive Summary
This paper investigates training deep neural networks with sparse-inducing activation functions like shifted ReLU and soft thresholding. The authors identify a previously unreported training instability at edge-of-chaos initialization caused by the positive second derivative of the variance map, leading to exponential variance growth with depth. They propose magnitude-clipped variants of these activations that stabilize training by ensuring the variance map has a stable fixed point. Experiments on MNIST and CIFAR10 demonstrate that these clipped activations can achieve up to 85% activation sparsity while retaining close to full accuracy compared to ReLU baselines.

## Method Summary
The method introduces magnitude-clipped variants of sparse-inducing activation functions (CReLU_τ,m and CST_τ,m) that maintain stability at edge-of-chaos initialization. Networks are initialized with variance map fixed point q*=1, and clipping magnitude m is chosen to ensure V'_φ(q*) < 1. The authors train DNNs (width 300, depth 100) on MNIST and CNNs (300 channels, depth 50) on CIFAR10 using SGD for 200 epochs with learning rate 1e-4 (DNN) or 1e-3 (CNN). Sparsity levels from 60% to 85% are tested by varying the threshold parameter τ.

## Key Results
- Unclipped shifted ReLU and soft thresholding activations suffer from training instability at edge-of-chaos initialization due to positive second derivative of variance map.
- Magnitude-clipped variants (CReLU_τ,m and CST_τ,m) overcome this instability by ensuring V'_φ(q*) < 1 at the edge of chaos.
- Networks with clipped activations achieve up to 85% activation sparsity while maintaining close to full accuracy on MNIST and CIFAR10.
- The proposed approach enables more efficient training and inference by exploiting sparsity in hidden layer outputs.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Magnitude-clipped sparsifying activations stabilize training by ensuring the variance map has a stable fixed point at the edge of chaos.
- **Mechanism:** The variance map V_φ(q) for clipped activations satisfies V'_CReLU_τ,m(q*) < 1 at the edge of chaos (χ1,φ = 1), making q* locally stable. This contrasts with unclipped versions where V'_φ(q*) = 1 and V''_φ(q*) > 0 causes exponential variance growth.
- **Core assumption:** The large-width Gaussian process approximation accurately models finite-width network behavior during initialization.
- **Evidence anchors:**
  - [abstract] "We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map."
  - [section] "V'_CReLU_τ,m(q*) < 1 at the EoC (χ1,φ = 1), making q* locally stable for both φ = CReLU_τ,m or CST_τ,m."
  - [corpus] Weak evidence - the corpus neighbors focus on Gaussian processes and initialization but don't specifically address clipping for sparsity.
- **Break condition:** If m is too large relative to τ and desired sparsity, V_φ(q) can develop multiple fixed points, causing ql to drift to unstable regions.

### Mechanism 2
- **Claim:** Clipping enables training with high activation sparsity (up to 85%) while maintaining near-full accuracy.
- **Mechanism:** The clipped activation functions allow networks to be initialized at the edge of chaos with controlled variance growth, enabling stable gradient propagation through deep layers despite sparse activations.
- **Core assumption:** The relationship between clipping magnitude m and variance map stability can be controlled to maintain both sparsity and trainability.
- **Evidence anchors:**
  - [abstract] "Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85% while retaining close to full accuracy."
  - [section] "CReLU_τ,m and CST_τ,m DNNs can maintain full accuracy when initialized with up to 85% sparsity."
  - [corpus] No direct evidence - corpus neighbors don't discuss sparsity levels in the context of clipping.
- **Break condition:** For very high sparsity (s ≥ 0.85) with small m values, training can become slow or unstable despite stable initialization.

### Mechanism 3
- **Claim:** The edge of chaos condition χ1,φ = 1 is maintained while avoiding exploding gradients through clipping.
- **Mechanism:** Clipping modifies the variance map such that V'_φ(q*) < 1 even when χ1,φ = 1, preventing the exponential growth of gradients that occurs with unclipped sparsifying activations.
- **Core assumption:** The second derivative V''_φ(q*) determines practical stability in finite-width networks beyond just the first derivative condition.
- **Evidence anchors:**
  - [section] "χ1,CReLU_τ,m = V'_CReLU_τ,m(q*) + σ2_w m√(2πq*) exp(-(m+τ)²/2q*) > V'_CReLU_τ,m(q*)" showing how clipping creates the necessary inequality.
  - [section] "This allows CReLU_τ,m and CST_τ,m to generate hidden layer outputs which are both very sparse and trainable when initialized at the EoC."
  - [corpus] No direct evidence - corpus neighbors don't discuss the relationship between edge of chaos and clipping.
- **Break condition:** If the clipping level m is chosen such that V''_φ(q*) becomes too positive, the variance map can become approximately linear around q*, causing effective χ1,φ > 1.

## Foundational Learning

- **Concept:** Gaussian process limit for deep networks
  - **Why needed here:** The analysis relies on understanding how pre-activation outputs approach Gaussian distributions in the large-width limit, which determines the variance and correlation maps.
  - **Quick check question:** What quantity characterizes the Gaussian process model for DNNs in the large width limit according to the paper?

- **Concept:** Edge of chaos initialization
  - **Why needed here:** The paper specifically initializes networks at the edge of chaos (χ1,φ = 1) to balance signal propagation and gradient stability, which is crucial for the clipping mechanism to work.
  - **Quick check question:** What two conditions must be satisfied for edge of chaos initialization according to the paper?

- **Concept:** Variance map stability
  - **Why needed here:** The core insight is that clipping makes the variance map V_φ(q) have a stable fixed point at the edge of chaos, which is essential for training deep networks with sparse activations.
  - **Quick check question:** What property of V_φ(q*) distinguishes stable from unstable fixed points in the context of this paper?

## Architecture Onboarding

- **Component map:** Input -> Dense/CNN layers with CReLU_τ,m or CST_τ,m activations -> Output layer
- **Critical path:** 1) Initialize weights on edge of chaos with q* = 1, 2) Choose sparsity level s and corresponding τ, 3) Select clipping magnitude m to ensure V'_φ(q*) < 1, 4) Train with SGD while monitoring activation sparsity and accuracy
- **Design tradeoffs:** Higher sparsity (larger τ) requires larger m to maintain stability, but larger m reduces the expressivity of the network. There's a sweet spot where sparsity is high but m is small enough to preserve accuracy.
- **Failure signatures:** 1) Training loss spikes or plateaus during training (slow/unstable training), 2) Variance map develops multiple fixed points causing ql to drift, 3) Very low accuracy despite stable initialization indicates the second failure mode
- **First 3 experiments:**
  1. Train a DNN with CReLU_τ,m at 60% sparsity (τ = 0.25) with m chosen for V'_φ(q*) = 0.5 and verify stable training with high accuracy.
  2. Train a DNN with CReLU_τ,m at 85% sparsity (τ = 1.04) with m chosen for V'_φ(q*) = 0.9 and observe if training fails due to V''_φ(q*) > 0.
  3. Train a CNN with CST_τ,m at 70% sparsity (τ = 1.04) with m chosen for V'_φ(q*) = 0.5 and verify that accuracy is maintained while activation sparsity is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the source of the training instability observed at very high sparsity levels (e.g., 85%) with small magnitude clipping values (m)?
- **Basis in paper:** [explicit] The paper identifies this as a "second failure mode" where training is slow and unstable despite EoC initialization.
- **Why unresolved:** The authors note that this issue appears late in training and involves large spikes in training loss, but the underlying mechanism is not fully explained.
- **What evidence would resolve it:** Detailed analysis of gradient norms, activation statistics, and loss curves throughout training to identify the specific factors causing instability at high sparsity.

### Open Question 2
- **Question:** How do smooth variants of the clipped activation functions (CReLU_τ,m and CST_τ,m) compare to their piecewise-linear counterparts in terms of training stability and accuracy?
- **Basis in paper:** [inferred] The paper suggests investigating smooth variants but does not provide experimental results or theoretical analysis.
- **Why unresolved:** The authors only analyze the piecewise-linear variants and mention smooth variants as a potential future direction without exploring them.
- **What evidence would resolve it:** Experiments comparing smooth and piecewise-linear variants across various sparsity levels and network architectures, along with theoretical analysis of their properties.

### Open Question 3
- **Question:** What is the relationship between the clipping magnitude m, sparsity level s, and the stability of the variance map V_φ(q) in finite-width networks?
- **Basis in paper:** [explicit] The paper discusses the trade-off between m and stability, noting that larger m can lead to bifurcation and multiple fixed points.
- **Why unresolved:** The authors provide some analysis of this relationship but do not derive explicit formulas or provide comprehensive guidelines for choosing optimal m values.
- **What evidence would resolve it:** Theoretical derivations relating m, s, and network width to variance map stability, along with empirical validation across different network sizes and sparsity levels.

## Limitations
- The analysis assumes the large-width Gaussian process approximation accurately models finite-width network behavior, which may break down for very deep or very sparse networks.
- The clipping mechanism requires careful tuning of both τ and m parameters, with limited guidance on how to choose m for a desired sparsity level.
- Experiments focus on relatively small networks (100-layer DNNs, 50-layer CNNs) which may not capture stability issues in larger-scale applications.
- Computational efficiency benefits from sparsity are not quantified - only sparsity levels and accuracy are reported.

## Confidence
- **High confidence:** The theoretical analysis of variance map stability and its relationship to training instability for unclipped sparsifying activations is well-supported by mathematical derivations.
- **Medium confidence:** The experimental results showing that clipped activations can achieve high sparsity (up to 85%) while maintaining accuracy are supported by the reported results, though limited to small-scale experiments.
- **Low confidence:** The claim that clipping provides practical benefits for training efficiency is not directly supported by timing or computational cost measurements.

## Next Checks
1. **Variance map bifurcation analysis:** For high sparsity levels (s ≥ 0.85) with small m values, empirically verify whether the variance map develops multiple fixed points and measure how often training becomes unstable versus slow but recoverable.

2. **Gradient flow monitoring:** During early training steps, measure the layerwise gradient norms for networks with clipped vs unclipped activations to confirm that the theoretical stability condition (V'_φ(q*) < 1) prevents gradient explosion in practice.

3. **Computational efficiency benchmarking:** Measure actual training time and inference speed for networks with 85% activation sparsity compared to ReLU baseline, accounting for the overhead of sparse matrix operations and potential gains from reduced computation.