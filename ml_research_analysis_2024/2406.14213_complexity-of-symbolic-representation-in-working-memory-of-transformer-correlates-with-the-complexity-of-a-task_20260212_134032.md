---
ver: rpa2
title: Complexity of Symbolic Representation in Working Memory of Transformer Correlates
  with the Complexity of a Task
arxiv_id: '2406.14213'
source_url: https://arxiv.org/abs/2406.14213
tags:
- memory
- working
- tokens
- transformer
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the properties of symbolic working memory added
  to the Transformer decoder, which enhances the quality of model predictions in machine
  translation tasks. The study reveals that translated text keywords are stored in
  the working memory, pointing to the relevance of memory content to the processed
  text.
---

# Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task

## Quick Facts
- arXiv ID: 2406.14213
- Source URL: https://arxiv.org/abs/2406.14213
- Reference count: 6
- Primary result: Working memory in Transformer decoder stores content words and its diversity correlates with task complexity, improving translation quality.

## Executive Summary
This paper introduces a symbolic working memory mechanism into the Transformer decoder for machine translation. The model learns to store content words (nouns, verbs, proper nouns) in memory during generation, with memory diversity correlating with input text complexity. Experiments on Russian-to-English translation across four datasets show improved BLEU and METEOR scores compared to standard Transformers, demonstrating that working memory captures task-relevant information and enhances prediction quality.

## Method Summary
The method extends a standard Transformer by adding symbolic working memory to the decoder. The model predicts both token and type (memory or target) at each step, using nucleus sampling (p=0.9) for memory tokens and best path decoding for predictions. Training involves pre-training on TED for 5 epochs, then fine-tuning each dataset for 30 epochs with 10 memory slots. Memory content is analyzed through keyword extraction, POS tagging, and diversity measures, while translation quality is evaluated using BLEU and METEOR scores.

## Key Results
- Working memory stores content words (nouns, verbs, proper nouns, determiners) that are critical for accurate translation
- Memory diversity (unique tokens) correlates with input text complexity, with complex datasets showing higher diversity
- Model outperforms standard Transformer with higher BLEU and METEOR scores across all datasets
- Disabling memory attention reduces performance, confirming memory utility during inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Working memory tokens store content words (nouns, verbs, proper nouns, determiners) that are critical for accurate translation, especially in complex domains.
- Mechanism: The Transformer decoder uses multi-head self-attention to access both target predictions and memory tokens. During generation, it decides per token whether to emit a translation token or store a memory token, allowing the model to cache salient linguistic elements.
- Core assumption: The model learns to identify and retain the most semantically relevant tokens for the current translation context, improving prediction accuracy.
- Evidence anchors:
  - [abstract] states that translated text keywords are stored in memory.
  - [section] shows that content words appear significantly more often in memory for complex datasets (WSC, IT documents) than for simpler ones (TED, Open Subtitles).
  - [corpus] provides neighbor papers discussing symbolic representations and working memory in Transformers.
- Break condition: If the attention mechanism cannot differentiate memory tokens from target tokens, or if the memory size is too small to store useful content, performance degrades.

### Mechanism 2
- Claim: Memory diversity (number of unique tokens) correlates with the complexity of the input text.
- Mechanism: For more complex inputs, the model stores a greater variety of tokens in memory to capture nuanced information. Simpler inputs require fewer distinct memory entries.
- Core assumption: Complex sentences contain more diverse linguistic elements that the model must retain to translate accurately.
- Evidence anchors:
  - [abstract] notes that diversity of tokens and parts of speech in memory correlates with corpus complexity.
  - [section] shows histograms and statistics: IT documents and WSC have higher average unique memory tokens than TED before fine-tuning.
  - [corpus] includes related works on memory diversity in neural architectures.
- Break condition: If memory capacity is insufficient or the model fails to learn which tokens to store, the correlation between memory diversity and complexity weakens.

- Claim: Disabling attention to memory tokens reduces translation quality, confirming the memory's utility.
- Mechanism: Memory tokens provide additional context that the decoder attends to when predicting the next token. Removing this source of information forces the model to rely solely on the source and partial target, reducing performance.
- Core assumption: The model actively uses memory content during inference, not just during training.
- Evidence anchors:
  - [section] reports BLEU=20.56 and METEOR=47.75 when memory attention is disabled, compared to higher scores with memory enabled.
  - [abstract] confirms that working memory enhances prediction quality.
  - [corpus] contains related studies on attention mechanisms and memory in Transformers.
- Break condition: If the model learns to predict well without memory access, or if memory content is redundant, disabling attention may not hurt performance.

## Foundational Learning

- Concept: Multi-head self-attention in Transformers
  - Why needed here: The decoder uses self-attention to combine information from the source, partial target, and memory tokens. Understanding this mechanism is crucial to see how memory integrates into the prediction process.
  - Quick check question: How does the decoder decide which tokens to attend to from the source, target, and memory at each step?

- Concept: Symbolic vs. distributed representations
  - Why needed here: Working memory stores symbolic tokens (actual words/subwords) rather than distributed vectors, making it interpretable and directly related to the text.
  - Quick check question: Why is it significant that memory tokens are from the vocabulary rather than learned embeddings?

- Concept: Content words vs. function words
  - Why needed here: The model preferentially stores content words (nouns, verbs, etc.) in memory, especially for complex texts. Understanding this distinction explains memory content patterns.
  - Quick check question: Which parts of speech are most likely to appear in working memory, and why?

## Architecture Onboarding

- Component map:
  - Encoder -> Source sentence hidden representations
  - Decoder (stacked layers) -> Self-attention (target+memory), cross-attention (to encoder), feedforward networks
  - Working memory -> Fixed-size sequence of tokens appended to decoder input, flagged by type (0 for memory, 1 for target)
  - Final layer -> Expanded output dimension (vocab_size + 2) to predict both token and type
  - Loss function -> Computed only on target tokens, ignoring memory tokens

- Critical path:
  1. Encode source sentence.
  2. Initialize decoder with start token and type.
  3. For each step:
     - Predict next token and type.
     - If type=1, append ground truth (teacher forcing); if type=0, append prediction.
     - Append type to sequence.
  4. Continue until target length + memory size is reached.
  5. Compute loss on target tokens only.

- Design tradeoffs:
  - Fixed memory size vs. dynamic allocation: Fixed size simplifies implementation but may limit expressiveness for very long or complex inputs.
  - Symbolic memory tokens vs. distributed embeddings: Symbolic tokens are interpretable but may lack the flexibility of learned embeddings.
  - Teacher forcing for target vs. predicted tokens for memory: Ensures stable training but may introduce exposure bias.

- Failure signatures:
  - Memory diversity does not correlate with input complexity (suggests memory not learning to store relevant content).
  - Disabling memory attention does not reduce performance (suggests memory not being used).
  - Memory tokens are repetitive or irrelevant (suggests poor memory management or insufficient capacity).

- First 3 experiments:
  1. Train a standard Transformer and the working memory variant on TED, compare BLEU/METEOR and memory content diversity.
  2. Disable memory attention during inference and measure performance drop.
  3. Vary memory size (e.g., 5, 10, 15) and analyze impact on translation quality and memory content patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the complexity of the working memory content evolve during the fine-tuning process, and what specific factors contribute to the changes observed?
- Basis in paper: [explicit] The paper discusses the study of memory content evolution during fine-tuning and the factors affecting memory diversity.
- Why unresolved: The paper mentions the diversity decrease in working memory during fine-tuning and the factors contributing to memory content changes, but it does not provide a detailed analysis of the specific factors driving these changes.
- What evidence would resolve it: Further analysis of the factors influencing working memory content evolution during fine-tuning, such as the impact of dataset complexity, model architecture, and training procedures, would provide insights into the specific mechanisms driving the observed changes.

### Open Question 2
- Question: What is the impact of working memory size on the model's performance and the complexity of the memory content, and how does this relationship vary across different datasets?
- Basis in paper: [inferred] The paper explores the relationship between working memory size and model performance, as well as the impact of memory size on content complexity, but does not provide a comprehensive analysis of this relationship across different datasets.
- Why unresolved: While the paper mentions the impact of memory size on model performance and content complexity, it does not thoroughly investigate how this relationship varies across different datasets with varying levels of complexity.
- What evidence would resolve it: Conducting experiments with different working memory sizes across multiple datasets and analyzing the resulting model performance and memory content complexity would provide insights into the impact of memory size on the model's behavior.

### Open Question 3
- Question: How does the working memory content relate to the model's decision-making process, and can we identify specific patterns or strategies employed by the model when utilizing the memory?
- Basis in paper: [inferred] The paper discusses the relevance of working memory content to the model's decision-making process and the identification of patterns in memory utilization, but does not provide a detailed analysis of the specific strategies employed by the model.
- Why unresolved: While the paper mentions the relevance of working memory content to the model's decision-making process, it does not delve into the specific patterns or strategies employed by the model when utilizing the memory.
- What evidence would resolve it: Analyzing the working memory content in relation to the model's decision-making process, such as identifying common patterns or strategies employed by the model when utilizing the memory, would provide insights into the model's internal mechanisms.

## Limitations

- Memory content interpretability: The analysis relies on keyword extraction and POS tagging without deeper semantic analysis, making it unclear whether stored tokens truly capture "key concepts" or merely reflect surface-level frequency patterns.
- Training procedure complexity: The two-stage training (pre-training on TED, then fine-tuning) introduces confounding factors, as improvements could be due to dataset-specific adaptation rather than the working memory mechanism itself.
- Generalizability: All experiments focus on Russian-to-English translation across four specific datasets, making results potentially not generalizable to other language pairs, domains, or tasks.

## Confidence

- **High confidence**: The core architectural modification (adding symbolic working memory to the Transformer decoder) is well-defined and reproducible. The claim that memory tokens are content words is supported by POS analysis across datasets.
- **Medium confidence**: The correlation between memory diversity and corpus complexity is observed but not causally explained. The performance improvement (BLEU/METEOR gains) is demonstrated but could be partially attributed to fine-tuning rather than memory alone.
- **Low confidence**: The claim that memory content is semantically "relevant" to the processed text is weakly supported. No qualitative analysis (e.g., human evaluation or semantic similarity measures) validates the functional importance of stored tokens.

## Next Checks

1. **Ablation on training procedure**: Train the working memory model from scratch (without TED pre-training) on each dataset and compare BLEU/METEOR scores. This isolates the effect of the memory mechanism from dataset-specific fine-tuning.
2. **Semantic analysis of memory content**: Compute semantic similarity (e.g., using sentence transformers) between memory tokens and the source/target sentences. Assess whether stored tokens are truly "key concepts" or just frequent words.
3. **Memory size sensitivity**: Vary the number of memory slots (e.g., 5, 10, 15, 20) and measure the impact on translation quality and memory diversity. Determine whether the fixed size of 10 is optimal or if larger memory improves performance for complex datasets.