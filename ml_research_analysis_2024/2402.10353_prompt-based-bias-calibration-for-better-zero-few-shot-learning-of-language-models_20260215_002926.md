---
ver: rpa2
title: Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language
  Models
arxiv_id: '2402.10353'
source_url: https://arxiv.org/abs/2402.10353
tags:
- bias
- calibration
- learning
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a null-input prompting method to calibrate
  intrinsic bias in pre-trained Masked LMs, with the aim of improving their performance
  in zero/few-shot classification tasks. The method leverages auto-generated null-meaning
  inputs and updates only bias parameters of the model to achieve efficient calibration.
---

# Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models

## Quick Facts
- arXiv ID: 2402.10353
- Source URL: https://arxiv.org/abs/2402.10353
- Authors: Kang He; Yinghan Long; Kaushik Roy
- Reference count: 28
- Primary result: Null-input prompting method updates only bias parameters to calibrate intrinsic bias, improving zero/few-shot classification accuracy by 9% and 2% respectively over baselines

## Executive Summary
This paper introduces a novel approach to improve zero/few-shot learning performance of pre-trained Masked Language Models (LMs) by calibrating their intrinsic bias through null-input prompting. The method generates null-meaning inputs and updates only bias parameters of the model to minimize KL divergence with uniform distribution, preserving core language modeling abilities. Experiments on 8 diverse classification datasets demonstrate significant improvements over uncalibrated models and output-calibration methods, with average gains of 9% in zero-shot and 2% in few-shot settings.

## Method Summary
The method employs null-meaning inputs to probe and calibrate intrinsic bias in pre-trained LMs. It generates diverse null-meaning inputs using GPT-4, filters them using Next Sentence Prediction (NSP) probability, and constructs prompts by concatenating them with answer formats. The calibration process updates only bias parameters (BLM) of the LM using KL divergence loss, applying early stopping strategies for different learning scenarios. This approach maintains the model's language modeling capabilities while reducing bias-induced performance degradation in downstream tasks.

## Key Results
- Calibrated models achieve average 9% improvement in zero-shot classification accuracy across 8 datasets compared to uncalibrated models
- Few-shot performance improves by average 2% over baseline methods
- Calibration reduces variance in probability distributions across labels, indicating more equitable model behavior
- One-batch calibration provides consistent improvement while preventing over-calibration degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Updating only bias parameters calibrates intrinsic bias while preserving language modeling abilities
- Mechanism: Bias parameters act as offsets in neural networks. By updating only these parameters to minimize KL divergence between output distribution on null inputs and uniform distribution, the model counteracts learned biases without modifying core weight parameters that encode language knowledge
- Core assumption: Intrinsic bias is primarily encoded in bias parameters rather than weight parameters
- Evidence anchors:
  - [abstract]: "BLM functions as offsets in neural networks, and strategically updating only BLM could potentially counteract intrinsic bias of pre-trained models"
  - [section 3.2]: "Weight parameters WLM may carry crucial pre-existing knowledge for language modeling, which risks impairment with a full model update"

### Mechanism 2
- Claim: Auto-generated null-meaning inputs with NSP filtering provide diverse, task-agnostic bias probing
- Mechanism: GPT-4 generates null-meaning inputs (symbols, words, phrases, sentences) that contain no task-relevant information. Inputs with higher NSP probability between the null input and answer format are selected, ensuring better integration into prompts and more effective bias probing
- Core assumption: GPT-4 can generate diverse null-meaning inputs that effectively probe model bias
- Evidence anchors:
  - [section 4.1]: "We employ null-meaning inputs to probe the intrinsic bias of pre-trained LMs"
  - [section 4.2]: "Generated null-meaning input xnull Pnsp(xnull, ans) This is an example sentence. 0.9996"

### Mechanism 3
- Claim: One-batch calibration provides conservative bias correction for zero-shot tasks
- Mechanism: Empirical evidence shows that one calibration batch provides consistent improvement with low variance. This prevents over-calibration that would degrade language modeling abilities by making the model output uniform probabilities regardless of input
- Core assumption: One batch of calibration is sufficient to improve zero-shot performance without degrading language modeling
- Evidence anchors:
  - [section 3.3]: "calibration with only one batch of null inputs delivers consistent and significant improvement compared to the original LM"
  - [section 3.3]: "Figure 2: Empirical experiments show the impact of calibration on zero-shot learning performance"

## Foundational Learning

- Concept: KL divergence and its role in distribution calibration
  - Why needed here: The calibration objective uses KL divergence to measure the difference between the model's output distribution on null inputs and a uniform distribution
  - Quick check question: What does KL divergence measure between two probability distributions?

- Concept: Next Sentence Prediction (NSP) and its use in prompt construction
  - Why needed here: NSP probability is used to select null-meaning inputs that integrate well with answer formats in prompts
  - Quick check question: How does NSP probability help in selecting null-meaning inputs for calibration?

- Concept: Masked Language Models and prompt-based learning
  - Why needed here: The method uses Masked LMs (RoBERTa) for classification tasks through prompt-based approaches
  - Quick check question: How do Masked LMs predict masked tokens using the probability formula shown in Equation 1?

## Architecture Onboarding

- Component map: Pre-trained RoBERTa → Null-input prompt generation → Bias parameter update → Calibrated model → Downstream tasks
- Critical path: Generate null-meaning inputs → Construct prompts → Update bias parameters → Apply calibrated model
- Design tradeoffs: Updating only bias parameters saves computation and preserves language knowledge vs. potentially incomplete bias correction
- Failure signatures: Performance degradation on tasks not sharing label words, high variance in results, minimal improvement over baseline
- First 3 experiments:
  1. Verify that original LM shows bias on null inputs by checking probability distribution across labels
  2. Test one-batch calibration on a simple sentiment classification task to confirm improvement
  3. Compare performance of BLM-only update vs. full model update on a classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does intrinsic bias calibration preserve language modeling abilities for tasks beyond classification, such as generation or translation?
- Basis in paper: [inferred] The paper mentions that calibration preserves language modeling abilities but focuses on classification tasks. It also suggests future work on expanding to regression and other task types.
- Why unresolved: The paper only evaluates calibration effects on classification tasks. The impact on generative or translation tasks remains unexplored.
- What evidence would resolve it: Experiments applying intrinsic bias calibration to language models for generation or translation tasks, followed by performance evaluations compared to baseline models.

### Open Question 2
- Question: How does the size of the null-meaning input set impact the effectiveness of intrinsic bias calibration?
- Basis in paper: [explicit] The paper mentions using 800 null-meaning inputs and discarding the bottom 20% based on NSP scores. However, it does not explore the impact of varying the size of the null-meaning input set.
- Why unresolved: The paper uses a fixed size for the null-meaning input set but does not investigate how different sizes might affect calibration outcomes.
- What evidence would resolve it: Systematic experiments varying the size of the null-meaning input set and analyzing the resulting calibration effectiveness and downstream task performance.

### Open Question 3
- Question: Can intrinsic bias calibration be extended to decoder-only language models like GPT-3 or GPT-4?
- Basis in paper: [explicit] The paper mentions that future work includes extending the method to large decoder models like GPT-3 or GPT-4.
- Why unresolved: The paper focuses on Masked LMs (e.g., RoBERTa) and does not explore the applicability of the calibration method to decoder-only models.
- What evidence would resolve it: Applying the intrinsic bias calibration method to decoder-only models and evaluating the impact on their zero/few-shot learning performance compared to uncalibrated models.

## Limitations

- The method is specifically designed for classification tasks using prompt-based approaches and may not generalize to generation or structured prediction tasks
- Requires GPT-4 API access for generating null-meaning inputs, creating a practical barrier for reproduction
- The computational efficiency of updating only bias parameters needs comprehensive analysis across different model sizes and task types

## Confidence

- High confidence in the core finding that null-input prompting with bias parameter updates improves zero/few-shot classification performance
- Medium confidence in the claim that updating only bias parameters is optimal
- Low confidence in the claim that one-batch calibration is universally sufficient

## Next Checks

1. **Ablation on calibration duration**: Systematically test calibration with 1, 2, 5, and 10 batches of null inputs to verify that one batch is truly optimal and not just sufficient

2. **Cross-dataset generalization test**: Apply the calibrated model from one dataset (e.g., AGNews) to a different dataset (e.g., SST-5) without additional calibration to test whether the bias correction generalizes across domains

3. **Failure mode analysis**: Deliberately over-calibrate a model (using 20+ batches) and test on both in-domain and out-of-domain tasks to empirically verify the claim that over-calibration degrades language modeling abilities