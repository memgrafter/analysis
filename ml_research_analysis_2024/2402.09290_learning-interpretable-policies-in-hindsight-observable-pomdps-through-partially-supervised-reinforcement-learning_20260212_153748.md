---
ver: rpa2
title: Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially
  Supervised Reinforcement Learning
arxiv_id: '2402.09290'
source_url: https://arxiv.org/abs/2402.09290
tags:
- state
- learning
- psrl
- reinforcement
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Partially Supervised Reinforcement Learning
  (PSRL), a framework that addresses the challenge of learning interpretable policies
  in partially observable environments where true state information is available during
  training but not at execution time. The core idea is to combine supervised learning
  (to predict state from observations) with reinforcement learning (to learn control
  policies) in a unified architecture.
---

# Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.09290
- Source URL: https://arxiv.org/abs/2402.09290
- Reference count: 12
- One-line primary result: PSRL framework achieves better sample efficiency and interpretability than end-to-end RL methods in POMDPs

## Executive Summary
This paper introduces Partially Supervised Reinforcement Learning (PSRL), a framework that addresses the challenge of learning interpretable policies in partially observable environments where true state information is available during training but not at execution time. The core idea is to combine supervised learning (to predict state from observations) with reinforcement learning (to learn control policies) in a unified architecture. This approach yields policies that explicitly separate state prediction from decision-making, improving interpretability while maintaining performance.

The authors evaluate PSRL across six OpenAI Gym environments, comparing it to state-of-the-art end-to-end and asymmetric actor-critic approaches. Results show that PSRL-0 (using only semantic state predictions) often achieves better sample efficiency than end-to-end methods, and adding latent features (PSRL-K) typically provides minimal additional benefit. The framework also demonstrates superior performance compared to asymmetric RL methods. Notably, PSRL produces highly accurate state predictions (mean squared error typically < 1), whereas end-to-end methods generate latent embeddings with errors often orders of magnitude higher, highlighting the interpretability advantage of PSRL.

## Method Summary
PSRL addresses the challenge of learning interpretable policies in partially observable environments by leveraging true state information available during training. The framework consists of a state predictor component (g) that maps observations to predicted state, and a policy component (π) that maps predicted state (and optionally latent features) to actions. During training, the state predictor is trained via supervised learning using the true state, while the policy is trained via reinforcement learning. At execution time, only observations are available, and the policy uses the predicted state to make decisions. The framework includes two variants: PSRL-0 which uses only predicted state (fully interpretable), and PSRL-K which adds K latent dimensions for additional expressiveness.

## Key Results
- PSRL-0 achieves better sample efficiency than end-to-end RL methods across multiple OpenAI Gym environments
- PSRL-0 often matches or exceeds the performance of PSRL-K variants with latent features (K > 0)
- PSRL produces state predictions with mean squared error typically below 1, while end-to-end methods generate latent embeddings with errors often orders of magnitude higher
- PSRL outperforms asymmetric DQN and PPO approaches in both finite and continuous action space domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PSRL achieves better sample efficiency than end-to-end methods by leveraging supervised state information during training.
- Mechanism: The framework uses true state information available during training to train a state predictor (g) via supervised learning, while simultaneously training the policy (π) via reinforcement learning. This dual training approach allows the policy to be trained with accurate state predictions rather than noisy latent embeddings.
- Core assumption: True state information is available during training but not at execution time.
- Evidence anchors:
  - [abstract]: "The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time."
  - [section 3]: "We propose partially-observable reinforcement learning (PSRL) as a general framework for RL that leverages knowledge of true state of a POMDP during training."
- Break condition: If true state information is not available during training, the supervised learning component cannot be trained.

### Mechanism 2
- Claim: PSRL produces more interpretable policies by explicitly separating state prediction from decision-making.
- Mechanism: The policy architecture composes the state predictor (g) and the policy (π), making a clear distinction between predicting semantically meaningful state and taking actions based on that prediction. This separation makes the policy more interpretable than end-to-end methods that map raw sensory inputs directly to controls.
- Core assumption: Semantic state information is meaningful and can be predicted from observations.
- Evidence anchors:
  - [abstract]: "This yields more interpretable policies that compose state predictions with control."
  - [section 1]: "However, such representations are typically not interpretable, and don't meet the regulatory bar on explainable artificial intelligence in many countries."
  - [section 3]: "The key idea, which is either implicitly or explicitly common in numerous particular cases, but has not previously been systematically investigated on its own, is to have a policy with an architecture that explicitly composes state prediction with decision."
- Break condition: If the state predictor cannot accurately predict the true state, the interpretability advantage is diminished.

### Mechanism 3
- Claim: PSRL-K generalization allows for a flexible tradeoff between interpretability and ability to capture additional information from sensor data.
- Mechanism: PSRL-K extends the basic framework by including K latent variables in addition to the predicted state. This allows the policy to leverage both the interpretable semantic state and additional latent features, providing a spectrum from fully interpretable (K=0) to more expressive (K>0).
- Core assumption: Additional latent features can capture information relevant for control that is not captured by the predicted state.
- Evidence anchors:
  - [abstract]: "This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights."
  - [section 3]: "The proposed PSRL framework can be viewed as unifying these disparate application-driven approaches, as well as providing a systematic means for studying them."
  - [section 4.2]: "In both the Acrobot and Cart Pole domain, there appears to be little advantage to PSRL-K for K > 0, with PSRL-0 already exhibiting strong performance."
- Break condition: If the additional latent features do not provide meaningful information for control, the tradeoff is not beneficial.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: PSRL is designed specifically for POMDPs, where the true state is not directly observable.
  - Quick check question: What is the key difference between a POMDP and a fully observable MDP?

- Concept: Deep Reinforcement Learning (DRL)
  - Why needed here: PSRL uses DRL techniques (DQN, PPO) as part of its framework.
  - Quick check question: What are the main components of a DRL algorithm?

- Concept: Supervised Learning
  - Why needed here: PSRL uses supervised learning to train the state predictor (g) using the true state information available during training.
  - Quick check question: What is the difference between supervised learning and reinforcement learning?

## Architecture Onboarding

- Component map: Observation -> State Predictor (g) -> Policy (π) -> Action
- Critical path:
  1. Observe current state and action
  2. Predict state using g
  3. Choose action using π(g(state))
  4. Receive reward and next state
  5. Update g and π using supervised and RL loss
- Design tradeoffs:
  - K value: Higher K allows for more expressive policies but may reduce interpretability
  - Weight of supervised loss (β): Higher β emphasizes accurate state prediction but may reduce performance
- Failure signatures:
  - Poor state prediction accuracy
  - Slow learning or poor performance
  - Overfitting to training data
- First 3 experiments:
  1. Implement PSRL-0 (K=0) on a simple environment like Cart Pole
  2. Compare PSRL-0 with end-to-end baseline on the same environment
  3. Vary the weight of supervised loss (β) to find optimal tradeoff

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section and discussion, several questions remain open:

1. How does PSRL performance scale with observation history length L in environments where long-term dependencies exist?
2. Can PSRL be effectively extended to hierarchical reinforcement learning where sub-policies operate at different temporal resolutions?
3. What is the optimal trade-off between interpretability and performance when using PSRL-K for varying values of K?

## Limitations

- Architectural details remain underspecified, including neural network architectures, layer sizes, and optimization hyperparameters
- Transferability to more complex domains is unknown, as all evaluation environments are relatively simple OpenAI Gym tasks
- The interpretability metric is limited, with no systematic evaluation of policy interpretability from human or domain expert perspectives

## Confidence

- High confidence in sample efficiency improvements: The paper demonstrates consistent performance advantages across multiple environments with clear quantitative metrics
- Medium confidence in interpretability claims: State prediction accuracy is convincingly demonstrated, but the practical interpretability of resulting policies is not systematically evaluated
- Medium confidence in generalization claims: The framework shows promise across diverse environments, but evaluation is limited to relatively simple tasks

## Next Checks

1. Implement PSRL-K with varying K values on Cart Pole and Acrobot to verify the paper's finding that K=0 often provides sufficient performance, testing the claimed tradeoff between interpretability and expressiveness.

2. Conduct ablation studies removing the supervised component (setting β=0) to quantify exactly how much of PSRL's performance gain comes from the dual training approach versus other factors.

3. Evaluate state prediction accuracy during training on more complex observation spaces to verify whether the claimed interpretability advantage holds as observation complexity increases.