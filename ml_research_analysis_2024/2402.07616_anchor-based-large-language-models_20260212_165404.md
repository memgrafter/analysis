---
ver: rpa2
title: Anchor-based Large Language Models
arxiv_id: '2402.07616'
source_url: https://arxiv.org/abs/2402.07616
tags:
- anchor
- tokens
- inference
- values
- keys
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes anchor-based large language models (AnLLMs)
  to address the memory and efficiency challenges in LLMs. The key innovation is the
  anchor-based self-attention network (AnSAN) that compresses sequence information
  into anchor tokens during training, reducing keys/values cache requirements.
---

# Anchor-based Large Language Models

## Quick Facts
- arXiv ID: 2402.07616
- Source URL: https://arxiv.org/abs/2402.07616
- Authors: Jianhui Pang, Fanghua Ye, Derek Fai Wong, Xin He, Wanshun Chen, Longyue Wang
- Reference count: 22
- Primary result: AnSAN reduces KV cache by up to 99% while maintaining comparable accuracy to baseline models

## Executive Summary
This paper introduces Anchor-based Large Language Models (AnLLMs) to address memory and efficiency challenges in transformer-based LLMs. The key innovation is the Anchor-based Self-Attention Network (AnSAN) that compresses sequence information into anchor tokens during training, enabling selective caching of only anchor tokens during inference. The approach achieves up to 99% cache reduction and 3.5x faster inference while maintaining similar accuracy levels on question-answering benchmarks.

## Method Summary
AnSAN modifies the standard transformer self-attention mechanism by introducing anchor-based attention masks during training. The model is trained to compress sequence information into anchor tokens (typically the last token in a sequence) using these masks. During inference, only the keys/values caches of anchor tokens are retained, while non-anchor token caches from completed sequences are discarded. The method is implemented through continuous pre-training of Llama2-7B models (7B and 13B parameters) on the RedPajama dataset, with two variants: AnLLM-EP using endpoints as anchors and AnLLM-AC appending <AC> tokens as anchors.

## Key Results
- Up to 99% reduction in keys/values cache memory requirements
- Up to 3.5x faster inference speed compared to baseline models
- Only 1.5% average accuracy reduction across question-answering benchmarks
- Maintains language modeling capacity comparable to base Llama2-7B model

## Why This Works (Mechanism)

### Mechanism 1
The anchor-based self-attention network (AnSAN) reduces memory usage by compressing sequence information into anchor tokens during training. During training, AnSAN uses anchor-based attention masks to force non-anchor tokens to attend only to previous non-anchor tokens within the same sequence and anchor tokens from preceding sequences. Anchor tokens attend exclusively to previous non-anchor tokens within their own sequence, aggregating all sequence information into themselves. During inference, only anchor token keys/values are cached, reducing cache size dramatically.

### Mechanism 2
The anchor-based inference strategy achieves up to 99% cache reduction and up to 3.5x faster inference by selectively caching only anchor tokens. During inference, when the model processes prefix texts or generates an anchor token, the REDUCTION function is called to remove keys/values caches of non-anchor tokens from completed sequences. Only the keys/values caches of anchor tokens (which contain compressed sequence information) and non-anchor tokens from the current sequence are retained.

### Mechanism 3
The AnSAN technique maintains comparable accuracy to baseline models while achieving significant efficiency improvements. The continuous pre-training with AnSAN allows the model to learn the information compression process without degrading its language modeling capabilities. The anchor tokens effectively capture semantic information, and the selective caching during inference preserves essential context while discarding redundant information.

## Foundational Learning

- **Transformer self-attention mechanism**: Understanding how standard transformers work is essential to grasp how AnSAN modifies the attention mechanism to achieve compression
  - Why needed: The paper builds on standard transformer architecture
  - Quick check: In a standard transformer decoder, can token i attend to token j if i < j? Why or why not?

- **KV cache mechanism in autoregressive generation**: The KV cache is the target of optimization in this work, and understanding how it works is crucial for understanding the proposed improvements
  - Why needed: KV cache management is central to the proposed efficiency gains
  - Quick check: During autoregressive generation, what information is stored in the KV cache and why is it needed for generating the next token?

- **Continuous pre-training vs. fine-tuning**: The paper uses continuous pre-training to teach the model the anchor-based attention mechanism, which is different from typical fine-tuning approaches
  - Why needed: Understanding the training methodology is crucial for reproduction
  - Quick check: What is the difference between continuous pre-training and fine-tuning, and why might continuous pre-training be preferred for teaching new attention mechanisms?

## Architecture Onboarding

- **Component map**: Input embedding layer -> Multiple decoder layers (each with AnSAN and feedforward network) -> AnSAN (modified self-attention with anchor-based attention masks) -> Anchor-based inference strategy (modified KV cache management) -> Output layer

- **Critical path**: 1. Text input → tokenization → embedding 2. For each decoder layer: AnSAN → feedforward → normalization 3. AnSAN applies anchor-based attention masks 4. During inference: selective KV cache management via REDUCTION function 5. Output logits → softmax → next token prediction

- **Design tradeoffs**: Accuracy vs. efficiency (minor accuracy reduction for significant memory and speed improvements); Anchor position selection (endpoints vs. added tokens - impacts flexibility and control); Training complexity (requires modified attention masks but uses standard next-token prediction objective)

- **Failure signatures**: Accuracy degradation beyond expected levels (>1.5% reduction); Memory usage not reducing as expected during inference; Inference speed not improving proportionally to cache reduction; Model instability during training with anchor-based attention masks

- **First 3 experiments**: 1. Compare perplexity of AnLLM-EP and AnLLM-AC with base Llama2-7B on validation set to verify language modeling capacity is maintained 2. Measure KV cache size reduction during inference on sample texts to verify the REDUCTION function works as expected 3. Benchmark inference speed with and without AnSAN on question-answering tasks to verify the claimed 3.5x acceleration

## Open Questions the Paper Calls Out
- How do different anchor token selection strategies (beyond just endpoints or appended tokens) affect the performance of AnLLMs across various NLP tasks?
- What is the theoretical limit of memory reduction achievable through the AnSAN technique, and how does this scale with model size and sequence length?
- How does the AnSAN technique impact the model's ability to handle long-range dependencies and complex reasoning tasks compared to standard attention mechanisms?
- Can the AnSAN technique be effectively combined with other memory-efficient attention mechanisms (e.g., FlashAttention, PagedAttention) to achieve even greater improvements in memory efficiency and inference speed?
- What are the implications of the AnSAN technique for privacy and security, particularly in scenarios where sensitive information might be compressed into anchor tokens?

## Limitations
- Accuracy-efficiency trade-off may vary significantly across different tasks and model scales beyond the tested question-answering benchmarks
- Anchor token position sensitivity not fully explored - using last token may not be optimal for all sequence types
- Model scale generalization unclear - results only demonstrated on Llama2-7B, not larger or smaller models

## Confidence
- **High Confidence**: AnSAN mechanism can be implemented to modify transformer self-attention; anchor-based inference strategy can reduce KV cache size; continuous pre-training process is technically feasible
- **Medium Confidence**: Specific accuracy-efficiency trade-off (1.5% reduction for 99% cache reduction and 3.5x acceleration); effectiveness of last-token anchor for general sequences; generalization across task types and model scales
- **Low Confidence**: Long-term stability during extended inference; performance consistency across diverse linguistic structures and languages; impact on model calibration and confidence estimation

## Next Checks
1. Cross-task robustness test: Evaluate AnLLM on broader NLP tasks (summarization, code generation, long-form generation) to verify consistency of accuracy degradation and cache reduction effectiveness
2. Anchor position sensitivity analysis: Systematically test different anchor positions (first token, middle token, multiple anchors) to determine optimal anchor selection strategy
3. Memory management overhead measurement: Conduct micro-benchmarks to measure actual runtime overhead of cache reduction function during inference to verify claimed acceleration accounts for all operational costs