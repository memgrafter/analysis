---
ver: rpa2
title: 'PAQA: Toward ProActive Open-Retrieval Question Answering'
arxiv_id: '2402.16608'
source_url: https://arxiv.org/abs/2402.16608
tags:
- questions
- clarifying
- answers
- question
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAQA is a new dataset that extends AmbiNQ with clarifying questions
  for ambiguous questions, addressing the gap in proactive conversational search.
  It uses GPT-3 to generate clarifying questions based on multiple disambiguated queries.
---

# PAQA: Toward ProActive Open-Retrieval Question Answering

## Quick Facts
- arXiv ID: 2402.16608
- Source URL: https://arxiv.org/abs/2402.16608
- Authors: Pierre Erbacher; Jian-Yun Nie; Philippe Preux; Laure Soulier
- Reference count: 0
- Primary result: PAQA dataset with 11,001 examples extending AmbiNQ with clarifying questions, showing 94% natural and 81% fully relevant questions through human evaluation

## Executive Summary
PAQA is a new dataset that extends AmbiNQ with clarifying questions for ambiguous questions, addressing the gap in proactive conversational search. The dataset uses GPT-3 to generate clarifying questions based on multiple disambiguated queries, containing 11,001 examples with questions, answers, documents, and clarifying questions. Human evaluation shows the generated questions are natural (94% natural) and relevant (81% fully relevant). Automatic experiments using Flan-T5 demonstrate the model can detect ambiguities and generate clarifying questions, but retrieval quality is the main bottleneck, with evidence-based models outperforming retrieval-based ones.

## Method Summary
The PAQA dataset is constructed by extending AmbiNQ with clarifying questions generated by GPT-3. Given a set of disambiguated questions from AmbiNQ, GPT-3 synthesizes a single clarifying question that covers all interpretations. The dataset contains 11,001 examples with questions, answers, documents, and clarifying questions. Automatic experiments use Flan-T5 models with various retrieval settings (evidence passages, DPR-retrieved passages) and pre-extracted answers to evaluate ambiguity detection and clarifying question generation performance.

## Key Results
- Human evaluation shows 94% of generated clarifying questions are natural and 81% are fully relevant
- Automatic experiments demonstrate Flan-T5 can detect ambiguities and generate clarifying questions
- Retrieval quality is the main bottleneck, with evidence-based models outperforming retrieval-based ones (0.64 accuracy vs 0.57 with DPR passages)
- The dataset contains 11,001 examples with questions, answers, documents, and clarifying questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 generates clarifying questions that resolve ambiguities by mapping multiple disambiguated queries to a single natural question.
- Mechanism: The few-shot prompt with structured examples teaches GPT-3 to synthesize the intent across all disambiguated queries into one question that covers all interpretations.
- Core assumption: GPT-3's few-shot learning can generalize from examples to produce coherent clarifying questions for unseen ambiguity patterns.
- Evidence anchors:
  - [abstract]: "We propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying questions."
  - [section 3.1]: "Given the set of disambiguated questions (q1, ..., qn), we generate a clarifying question cq asking how the question q should be interpreted."
  - [corpus]: Weak evidence - no other datasets using GPT-3 for this specific task.
- Break condition: GPT-3 fails to capture the full scope of ambiguities when the number of disambiguated queries is very large or the intents are highly diverse.

### Mechanism 2
- Claim: Retrieval quality is the main bottleneck for ambiguity detection and clarifying question generation.
- Mechanism: The model relies on retrieved passages to understand the context of the question. Poor retrieval leads to missing relevant information needed to detect ambiguities.
- Core assumption: The retrieved passages contain sufficient evidence to identify all plausible interpretations of the question.
- Evidence anchors:
  - [section 4.2]: "Models using evidence (Pevidence) outperform retrieval pipelines in accuracy and generation with an accuracy of 0.64 with evidence and 0.57 with passages retrieved from DPR."
  - [abstract]: "Automatic experiments using Flan-T5 demonstrate the model can detect ambiguities and generate clarifying questions, but retrieval quality is the main bottleneck."
  - [corpus]: Strong evidence - the paper explicitly compares evidence-based models to retrieval-based ones.
- Break condition: The retrieval system fails to find any relevant passages for the question, leaving the model without context to work with.

### Mechanism 3
- Claim: Human evaluation shows generated clarifying questions are natural and relevant to the underlying ambiguities.
- Mechanism: Annotators assess the quality of generated questions based on naturalness (fluent, everyday language) and relevance (covers all intents).
- Core assumption: Human annotators can accurately judge the quality of clarifying questions by comparing them to the original disambiguated queries.
- Evidence anchors:
  - [section 3.2]: "Naturalness is defined as being written in a fluid manner, in everyday language, coherent, and could have been generated by a human."
  - [section 4.1]: "Our sample of evaluated questions is considered mostly natural with 0 Unnatural, 1 Fair, 5 Good, and 94 Natural."
  - [corpus]: Strong evidence - the paper provides detailed human evaluation results.
- Break condition: Annotators disagree significantly on the quality of questions, indicating inconsistency in the evaluation process.

## Foundational Learning

- Concept: Ambiguity detection in natural language
  - Why needed here: The system needs to identify when a question has multiple plausible interpretations to trigger clarifying question generation.
  - Quick check question: Can you explain how a question like "When did England last make the quarter final of the world cup?" could be ambiguous?

- Concept: Question generation from context
  - Why needed here: The system must generate clarifying questions that naturally cover all possible interpretations of the original question.
  - Quick check question: How would you generate a clarifying question for "Who wrote the music for How to Train Your Dragon?" that covers both composer and lyricist interpretations?

- Concept: Retrieval-based question answering
  - Why needed here: The system relies on retrieved passages to understand the context and detect ambiguities in the question.
  - Quick check question: What challenges arise when using dense passage retrieval (DPR) for ambiguous questions?

## Architecture Onboarding

- Component map:
  - Question input → Ambiguity detection model → Clarifying question generation model → Output
  - Question input → DPR retrieval → Reranking → Evidence passages → Ambiguity detection model → Clarifying question generation model → Output
  - Question input → DPR retrieval → Answer extraction → Ambiguity detection model → Clarifying question generation model → Output

- Critical path: Question → Ambiguity detection → Clarifying question generation
- Design tradeoffs:
  - Using evidence passages vs. retrieved passages: Evidence passages provide more reliable context but limit the system to the AmbiNQ dataset.
  - Question-only vs. retrieval-augmented models: Question-only models rely on parametric knowledge but may lack context for ambiguity detection.
  - Pre-extracting answers vs. not: Pre-extracted answers can provide additional context but may introduce noise if the extracted answers are incorrect.

- Failure signatures:
  - Low ambiguity detection accuracy: The model fails to identify questions with multiple plausible interpretations.
  - Poor clarifying question quality: Generated questions are unnatural, irrelevant, or fail to cover all intents.
  - Retrieval failures: The DPR model fails to retrieve relevant passages for the question.

- First 3 experiments:
  1. Test ambiguity detection on a subset of the PAQA dataset with known ambiguities.
  2. Generate clarifying questions for a set of ambiguous questions and evaluate their quality using human annotators.
  3. Compare the performance of evidence-based models vs. retrieval-based models on ambiguity detection and clarifying question generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of clarifying questions generated by GPT-3 compare to those generated by human annotators in terms of naturalness and relevance?
- Basis in paper: [explicit] The paper mentions human evaluation of the generated clarifying questions, with 94% being natural and 81% being fully relevant.
- Why unresolved: While the paper provides human evaluation metrics, it does not directly compare the quality of GPT-3 generated questions to human-generated ones.
- What evidence would resolve it: A direct comparison study between GPT-3 generated clarifying questions and those created by human annotators, using the same evaluation metrics.

### Open Question 2
- Question: What is the impact of scaling model parameters and increasing the number of retrieved passages on the performance of ambiguity detection and clarifying question generation?
- Basis in paper: [inferred] The paper mentions that they did not investigate the impact of scaling model parameters or increasing the number of retrieved passages.
- Why unresolved: The paper does not explore these aspects, which could potentially improve the model's performance.
- What evidence would resolve it: Experiments varying model sizes and the number of retrieved passages, comparing their impact on ambiguity detection accuracy and clarifying question quality.

### Open Question 3
- Question: How would a multi-round clarification approach, where the system asks multiple partially-relevant but natural clarifying questions, affect user experience and search efficiency compared to a single comprehensive clarifying question?
- Basis in paper: [inferred] The paper mentions that they did not investigate building multiple rounds of clarifying questions, which could increase cognitive load on users.
- Why unresolved: The paper focuses on generating a single clarifying question and does not explore the potential benefits or drawbacks of a multi-round approach.
- What evidence would resolve it: User studies comparing search efficiency and user satisfaction between single and multi-round clarification approaches in a conversational search system.

## Limitations

- The evaluation of clarifying question quality relies heavily on human judgment without reported inter-annotator agreement, introducing potential subjectivity
- The dataset construction process depends on GPT-3's performance, which may not generalize well to all types of ambiguities or question domains
- The focus on English Wikipedia questions limits the dataset's applicability to other domains or languages

## Confidence

- **High Confidence**: The claim that retrieval quality is the main bottleneck is strongly supported by the paper's experimental results showing evidence-based models outperforming retrieval-based ones across all metrics.
- **Medium Confidence**: The assertion that generated clarifying questions are natural and relevant is supported by human evaluation, but the lack of inter-annotator agreement data introduces some uncertainty about the consistency of these judgments.
- **Low Confidence**: The mechanism by which GPT-3 generalizes to synthesize multiple disambiguated queries into coherent clarifying questions is plausible but not directly validated, as the paper does not explore failure cases or edge conditions in detail.

## Next Checks

1. **Inter-annotator Agreement**: Calculate and report the inter-annotator agreement for the human evaluation of clarifying question quality to validate the consistency of the naturalness and relevance assessments.

2. **Generalization Testing**: Test the GPT-3-based clarifying question generation on a diverse set of ambiguous questions from different domains or languages to evaluate the method's generalization beyond Wikipedia-based questions.

3. **Failure Case Analysis**: Conduct a detailed analysis of cases where the model fails to detect ambiguities or generates poor clarifying questions, examining whether these failures are due to retrieval issues, model limitations, or inherent ambiguity complexity.