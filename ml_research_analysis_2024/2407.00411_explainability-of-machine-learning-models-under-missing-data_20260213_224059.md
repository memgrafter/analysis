---
ver: rpa2
title: Explainability of Machine Learning Models under Missing Data
arxiv_id: '2407.00411'
source_url: https://arxiv.org/abs/2407.00411
tags:
- missing
- imputation
- data
- values
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how missing data imputation methods affect
  the explainability of machine learning models, specifically focusing on SHAP (SHapley
  Additive exPlanations) values. The authors conduct experiments on four datasets
  (California, Diabetes, MNIST, and Glass) with missing rates ranging from 0.2 to
  0.8, comparing eight approaches: Linear Regression on original data (ground truth),
  XGBoost without imputation, and six imputation methods (Mean Imputation, MICE, DIMV,
  missForest, SOFT-IMPUTE, and GAIN) followed by linear regression.'
---

# Explainability of Machine Learning Models under Missing Data

## Quick Facts
- arXiv ID: 2407.00411
- Source URL: https://arxiv.org/abs/2407.00411
- Authors: Tuan L. Vo; Thu Nguyen; Luis M. Lopez-Ramos; Hugo L. Hammer; Michael A. Riegler; Pal Halvorsen
- Reference count: 40
- Primary result: Different missing data imputation methods significantly affect Shapley value distributions and model explainability, with trade-offs between imputation accuracy and explainability preservation.

## Executive Summary
This paper investigates how missing data imputation methods affect the explainability of machine learning models, specifically focusing on SHAP (SHapley Additive exPlanations) values. The authors conduct experiments on four datasets with missing rates ranging from 0.2 to 0.8, comparing eight approaches including various imputation methods followed by linear regression and XGBoost without imputation. The study reveals that the choice of imputation method significantly impacts feature importance rankings and Shapley value distributions, with no single method consistently performing best across all datasets. Notably, methods with lower imputation MSE don't necessarily preserve Shapley values better, highlighting a trade-off between imputation accuracy and explainability preservation.

## Method Summary
The study evaluates the effects of missing data imputation on model explainability by comparing eight approaches: Linear Regression on original data (ground truth), XGBoost without imputation, and six imputation methods (Mean Imputation, MICE, DIMV, missForest, SOFT-IMPUTE, and GAIN) followed by linear regression. Experiments are conducted on four datasets (California, Diabetes, MNIST, and Glass) with missing rates ranging from 0.2 to 0.8. The authors measure prediction accuracy using MSE on test set labels, assess Shapley value preservation using MSE_SHAP, and analyze global feature importance and beeswarm plots to understand how imputation methods affect feature contributions and rankings.

## Key Results
- The choice of imputation method significantly impacts feature importance rankings and Shapley value distributions
- XGBoost can handle missing data directly but produces the most different Shapley values compared to the original data
- Methods with lower imputation MSE don't necessarily preserve Shapley values better, revealing a trade-off between imputation accuracy and explainability preservation
- DIMV consistently performs well in preserving Shapley values, while missForest excels in prediction accuracy
- The impact of imputation methods becomes more pronounced as missing rates increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different imputation methods alter Shapley value distributions, affecting model explainability.
- Mechanism: Imputation methods change the data distribution, which shifts the conditional expectations used in Shapley value calculations. This alters feature contributions and global importance rankings.
- Core assumption: Missing data is handled by imputation before Shapley value computation.
- Evidence anchors:
  - [abstract] "the choice of imputation method can introduce biases that could lead to changes in the Shapley values"
  - [section] "different imputation methods lead to varying Shapley value distributions"
  - [corpus] Found 25 related papers; none directly test imputation effects on Shapley values, so this is novel.
- Break condition: If Shapley values are computed directly on incomplete data without imputation (e.g., XGBoost native handling), this mechanism does not apply.

### Mechanism 2
- Claim: Higher missing rates amplify differences between imputation methods in Shapley value preservation.
- Mechanism: As more data is imputed, the variance between imputation strategies grows, making their effects on feature importance more pronounced.
- Core assumption: Missing data is MCAR (Missing Completely At Random).
- Evidence anchors:
  - [abstract] "the impact of imputation methods becomes more pronounced as missing rates increase"
  - [section] "as the missing rate increases from 0.2 to 0.8, the differences between the imputation methods become more pronounced"
  - [corpus] Weak evidence; corpus papers focus on imputation accuracy, not Shapley effects.
- Break condition: If missing data is not MCAR, imputation method effects may be masked by bias in missingness.

### Mechanism 3
- Claim: Imputation accuracy (MSE) does not guarantee better Shapley value preservation.
- Mechanism: Methods that minimize prediction error may distort feature relationships, altering Shapley contributions even if predictions are accurate.
- Core assumption: MSE is used to measure both imputation accuracy and Shapley value fidelity.
- Evidence anchors:
  - [abstract] "a lower test prediction MSE does not necessarily imply a lower MSE in Shapley values and vice versa"
  - [section] "methods with lower imputation MSE do not necessarily preserve Shapley values better"
  - [corpus] No direct corpus evidence; this is a novel finding.
- Break condition: If explainability is not a concern, only prediction accuracy matters.

## Foundational Learning

- Concept: Shapley value calculation and interpretation
  - Why needed here: The paper's core analysis depends on comparing Shapley value distributions across imputation methods.
  - Quick check question: Can you explain why Shapley values sum to the model prediction and how they are computed for a feature?

- Concept: Imputation methods and their assumptions
  - Why needed here: Different imputation strategies (mean, MICE, DIMV, etc.) have distinct statistical assumptions that affect downstream model behavior.
  - Quick check question: What is the key difference between mean imputation and MICE in terms of handling feature relationships?

- Concept: Mean Square Error (MSE) as a dual metric
  - Why needed here: The paper uses MSE to evaluate both prediction accuracy and Shapley value fidelity, requiring understanding of what each measures.
  - Quick check question: How does MSE between Shapley values differ from MSE between predictions?

## Architecture Onboarding

- Component map:
  Data loading -> Missing data simulation -> Imputation (8 methods) -> Model training (Linear Regression/XGBoost) -> Shapley value computation -> Evaluation (MSE, beeswarm, global importance plots)
- Critical path:
  Simulate missing data -> Apply imputation -> Train model -> Compute Shapley values -> Compare to original
- Design tradeoffs:
  - Using XGBoost without imputation vs. imputing then training: XGBoost handles missing data natively but produces different Shapley values; imputing preserves comparability but may introduce bias.
  - Choice of imputation method: Simpler methods (mean) are fast but distort relationships; complex methods (MICE, DIMV) preserve structure but are computationally heavier.
- Failure signatures:
  - High MSE_SHAP but low prediction MSE: Imputation preserves accuracy but distorts feature importance.
  - Low MSE_SHAP but high prediction MSE: Imputation preserves explainability but hurts accuracy.
  - XGBoost shows consistently different Shapley values: Native missing handling changes feature contributions.
- First 3 experiments:
  1. Run with missing rate 0.2 and all 8 methods; compare global feature importance plots.
  2. Increase missing rate to 0.6; observe beeswarm plot changes for key features.
  3. Compare MSE and MSE_SHAP across methods at r=0.8; identify trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do different imputation methods preserve Shapley values better, and can we predict which method will work best for a given dataset?
- Basis in paper: [explicit] The paper shows that methods with lower imputation MSE do not necessarily preserve Shapley values better, and that the best imputation method varies across datasets (California vs Diabetes).
- Why unresolved: The study identifies that dataset characteristics matter but doesn't specify which characteristics (e.g., feature correlations, data distributions) are most influential in determining the optimal imputation method for Shapley value preservation.
- What evidence would resolve it: Systematic experiments across datasets with varying characteristics (dimensionality, correlation structures, distributions) to identify which features predict imputation method performance for Shapley value preservation.

### Open Question 2
- Question: How can we develop explanation methods that are aware of the imputation frequency and uncertainty of features during model training?
- Basis in paper: [explicit] The authors suggest in their discussion that "it would be useful to develop explanation methods that are aware of whether some features in the training set have been imputed at training time and how often."
- Why unresolved: Current Shapley value methods treat all features equally regardless of whether they were originally observed or imputed, potentially overvaluing less reliable features.
- What evidence would resolve it: Development and validation of modified Shapley value methods that incorporate imputation frequency/uncertainty weights, tested against standard methods on datasets with varying missingness patterns.

### Open Question 3
- Question: What is the relationship between missing data mechanisms (MCAR, MAR, MNAR) and the impact on Shapley value distributions across different imputation methods?
- Basis in paper: [inferred] The experiments only consider MCAR (missing-completely-at-random) data, but the theoretical analysis section mentions different missing mechanisms without exploring them empirically.
- Why unresolved: The study's theoretical analysis hints at differences based on missing mechanisms, but the experimental evaluation is limited to MCAR, leaving uncertainty about how other mechanisms would affect Shapley value preservation.
- What evidence would resolve it: Comparative experiments using MAR and MNAR missingness patterns across multiple imputation methods, measuring both prediction accuracy and Shapley value preservation.

## Limitations
- The study assumes MCAR missingness, which may not reflect real-world data scenarios where missingness patterns are often MNAR
- The evaluation focuses primarily on MSE-based metrics, potentially overlooking other aspects of explainability quality
- The analysis is limited to linear regression models, and results may differ for more complex model architectures
- The computational intensity of methods like GAIN and missForest, particularly at higher missing rates, may limit practical applicability

## Confidence
- High confidence: The finding that imputation method choice affects Shapley value distributions is well-supported by experimental results across multiple datasets and missing rates.
- Medium confidence: The observation that XGBoost produces different Shapley values than imputation-based approaches is supported but may depend on implementation details.
- Medium confidence: The claim that imputation accuracy doesn't guarantee better explainability preservation is novel but requires further validation across diverse datasets and model types.

## Next Checks
1. Test the framework on MNAR missingness patterns to assess robustness beyond MCAR assumptions.
2. Evaluate explainability metrics beyond MSE, such as feature ranking stability and interpretability consistency.
3. Extend the analysis to non-linear models (e.g., neural networks) to determine if findings generalize beyond linear regression.