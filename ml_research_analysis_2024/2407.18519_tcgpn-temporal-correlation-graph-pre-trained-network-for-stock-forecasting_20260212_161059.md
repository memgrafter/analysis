---
ver: rpa2
title: 'TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting'
arxiv_id: '2407.18519'
source_url: https://arxiv.org/abs/2407.18519
tags:
- time
- graph
- series
- correlation
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of stock forecasting, which
  lacks periodicity and requires models with strong robustness and generalization
  capabilities. The authors propose a novel Temporal-Correlation Graph Pre-trained
  Network (TCGPN) that combines temporal features and correlation information.
---

# TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting

## Quick Facts
- arXiv ID: 2407.18519
- Source URL: https://arxiv.org/abs/2407.18519
- Authors: Wenbo Yan; Ying Tan
- Reference count: 40
- Key outcome: TCGPN achieves state-of-the-art stock forecasting results with IC scores of 0.09051 and 0.19619, and Sharpe ratios of 2.57502 and 8.92447 on CSI300 and CSI500 datasets

## Executive Summary
This paper addresses stock forecasting challenges characterized by minimal periodicity and requiring strong robustness and generalization capabilities. The authors propose TCGPN, a novel framework that combines temporal features and correlation information through a Temporal-Correlation Fusion Encoder. The model employs carefully designed self-supervised and semi-supervised pre-training tasks, achieving independence from node order and quantity while supporting various data augmentations and reduced memory consumption.

## Method Summary
TCGPN is a Temporal-Correlation Graph Pre-trained Network designed for stock forecasting that integrates temporal and correlation features through a Temporal-Correlation Fusion Encoder (TCFEncoder). The framework uses data augmentation with node sampling, graph random masking (rate 0.3), and temporal random masking (rate 0.3). The TCFEncoder combines position encoding, Graph Attention Network (GAT), and TGMformer modules. Pre-training involves self-supervised temporal tasks (MSE on masked time series reconstruction) and semi-supervised correlation tasks (MSE on unmasked graph entries). The model is fine-tuned with a simple MLP using combined loss (λm=0.3 for MSE + Pearson correlation).

## Key Results
- Achieved state-of-the-art IC scores of 0.09051 (CSI300) and 0.19619 (CSI500)
- Obtained Sharpe ratios of 2.57502 (CSI300) and 8.92447 (CSI500)
- Outperformed existing methods in both temporal-only and industry-graph scenarios

## Why This Works (Mechanism)

### Mechanism 1
The Temporal-Correlation Fusion Encoder (TCFEncoder) effectively integrates temporal and correlation features into a unified representation. The TCFEncoder combines position encoding, Graph Attention Network (GAT), and TGMformer modules. Position encoding preserves temporal sequence order, GAT fuses correlation information from neighboring nodes, and TGMformer encodes temporal features with a temporal Gaussian mask that models time decay effects. The combination of these three components can capture both temporal dependencies and spatial correlations without losing critical information from either domain.

### Mechanism 2
Self-supervised temporal tasks enable the model to recover missing time steps, improving temporal encoding ability. By masking portions of the time series and training the model to reconstruct them, the TCFEncoder learns to capture latent temporal patterns and understand how similar sequences relate to each other. The context of unmasked time steps contains sufficient information to infer the missing portions, and the correlation between sequences provides additional cues for reconstruction.

### Mechanism 3
Semi-supervised correlation tasks allow the model to adaptively learn correlation degrees while maintaining prior knowledge guidance. The model learns to reconstruct a masked adjacency matrix using the learned node encodings, with unmasked portions providing supervision to prevent deviation from prior knowledge. The node encodings contain sufficient information to infer correlation degrees, and the combination of learned and prior knowledge provides more robust correlation estimates than either alone.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Graph Attention Networks (GATs)**
  - Why needed here: Stock data has complex relationships between stocks (industry correlations, leading-lagging effects) that need to be captured as a graph structure
  - Quick check question: Can you explain how GAT differs from traditional GCN and why attention mechanisms are useful for stock correlation modeling?

- **Self-supervised learning and masked reconstruction tasks**
  - Why needed here: Stock data lacks labeled temporal patterns, so the model needs to learn from the data structure itself by predicting masked portions
  - Quick check question: What are the key differences between the temporal and correlation self-supervised tasks in TCGPN?

- **Pre-training and transfer learning**
  - Why needed here: Stock forecasting requires models with strong generalization across different market conditions, which pre-training on diverse patterns can provide
  - Quick check question: Why is TCGPN designed to be independent of node order and quantity, and how does this support effective pre-training?

## Architecture Onboarding

- **Component map:**
  - Data Augmentation Layer (Node sampling, Graph random mask, Temporal random mask) → Temporal-Correlation Fusion Encoder (Position encoding → GAT → TGMformer) → Pre-training Decoders (Temporal task decoder, Correlation task decoder) → Frozen encoder → MLP fine-tuning

- **Critical path:** Data augmentation → TCFEncoder → Pre-training decoders → Frozen encoder → MLP fine-tuning

- **Design tradeoffs:**
  - Independence from node order enables flexible data augmentation but requires careful handling of permutation invariance
  - Multiple sampling reduces memory usage but may increase training time
  - Simple MLP for fine-tuning reduces complexity but may limit task-specific optimization

- **Failure signatures:**
  - Poor reconstruction in pre-training tasks → insufficient capacity in TCFEncoder
  - High variance across different node samplings → instability in data augmentation
  - Degraded performance on downstream tasks despite good pre-training → mismatch between pre-training and fine-tuning objectives

- **First 3 experiments:**
  1. Test TCFEncoder with and without GAT on a small dataset to verify correlation fusion contribution
  2. Compare temporal reconstruction accuracy with varying masking ratios to find optimal self-supervised task difficulty
  3. Evaluate memory usage and performance trade-off with different node sampling rates on a large dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Specific implementation details for temporal Gaussian mask and position encoding formulation remain underspecified
- Evaluation focuses on performance metrics without extensive ablation studies on critical design choices
- Lack of analysis on the necessity of combining both self-supervised and semi-supervised pre-training tasks

## Confidence

**Major uncertainties:**
The paper provides strong experimental validation on real stock market data, but the specific implementation details for the temporal Gaussian mask and the exact position encoding formulation remain underspecified. The evaluation focuses on performance metrics without extensive ablation studies on the critical design choices, such as the optimal masking rates or the necessity of combining both self-supervised and semi-supervised pre-training tasks.

**Confidence labels:**
- High confidence in the overall approach and experimental results
- Medium confidence in the specific mechanism of temporal-Gaussian mask integration
- Low confidence in the exact implementation details required for faithful reproduction

## Next Checks
1. Conduct ablation studies comparing TCGPN performance with and without the temporal-Gaussian mask to quantify its contribution to temporal encoding
2. Test the model's sensitivity to different masking rates (rg and rt) in the pre-training tasks to find optimal values
3. Evaluate whether the combination of self-supervised and semi-supervised pre-training provides better generalization than either approach alone on out-of-sample market conditions