---
ver: rpa2
title: 'Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs'
arxiv_id: '2410.03730'
source_url: https://arxiv.org/abs/2410.03730
tags:
- languages
- ours
- training
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed two multilingual LLMs, Teuken-7B-Base and
  Teuken-7B-Instruct, trained on all 24 official EU languages with a focus on reducing
  English-centrism. They designed a custom multilingual tokenizer optimized for European
  languages and trained the models on a dataset with ~60% non-English data, including
  web-crawled and curated sources.
---

# Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs

## Quick Facts
- arXiv ID: 2410.03730
- Source URL: https://arxiv.org/abs/2410.03730
- Reference count: 40
- Primary result: Teuken-7B-Instruct achieved 57.0% average accuracy across 21 EU languages on multilingual benchmarks

## Executive Summary
This paper presents Teuken-7B-Base and Teuken-7B-Instruct, two multilingual large language models supporting all 24 official EU languages. The models were developed to address English-centrism in existing LLMs by training on ~60% non-English data and using a custom tokenizer optimized for European languages. Teuken-7B-Instruct demonstrates strong multilingual performance, particularly excelling in creative and knowledge-based tasks in German while maintaining low toxicity levels.

## Method Summary
The authors trained 7B-parameter transformer decoder models using causal language modeling on 6 trillion tokens with curriculum learning and data mixing strategies. They developed a custom multilingual tokenizer with reduced fertility values optimized for all 24 EU languages. The instruction-tuned variant underwent two-stage fine-tuning (SFT + DPO) using multilingual datasets to improve cross-lingual performance. Training data comprised ~60% non-English content from web-crawled and curated sources.

## Key Results
- Teuken-7B-Instruct achieved 57.0% average accuracy across 21 EU languages on ARC, HellaSwag, TruthfulQA, and MMLU benchmarks
- The model outperformed several large multilingual models despite being trained on fewer tokens
- Teuken-7B-Instruct demonstrated low toxicity levels measured via PolygloToxicityPrompts benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual tokenizer with reduced fertility improves model efficiency and cross-lingual performance
- Mechanism: By minimizing tokens per word (fertility), the tokenizer reduces text fragmentation, lowers inference costs, and allows longer queries within context limits, especially for morphologically rich languages
- Core assumption: Lower fertility leads to more coherent token sequences and better preservation of semantic relationships in multilingual contexts
- Evidence anchors:
  - [abstract] Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer
  - [section] We developed a custom multilingual tokenizer... optimized for all 24 official European languages... The tokenizer training dataset contains an equal number of documents for each of the 24 languages
  - [section] Our custom tokenizer demonstrates that for 19 out of the 24 languages, fertility values are similar or lower than those of related tokenizers

### Mechanism 2
- Claim: Training on high proportion of non-English data reduces English-centric bias and improves multilingual performance
- Mechanism: Allocating ~60% of training tokens to non-English content ensures balanced exposure across languages, mitigating dominance of English in learned representations
- Core assumption: Equal token distribution across languages is sufficient to overcome inherent data quality and quantity disparities between high-resource and low-resource languages
- Evidence anchors:
  - [abstract] Trained on a dataset comprising around 60% non-English data
  - [section] Our composed training dataset contains 6 trillion tokens, of which 86.79% originates from web data, and the remaining 13.21% represent is curated data... 41.70% of the tokens stem from English content
  - [section] As illustrated in Figure 1 and Figure 2, 41.67% of the tokens stem from English content and due to the inclusion of German, French, and Spanish, we approach around two-thirds of the total tokens

### Mechanism 3
- Claim: Instruction tuning with multilingual datasets improves cross-lingual instruction-following capabilities
- Mechanism: Fine-tuning on diverse, high-quality multilingual instruction-response pairs teaches the model to generalize task understanding across languages, not just translation
- Core assumption: The multilingual instruction datasets capture the full range of semantic and pragmatic variations needed for robust cross-lingual generalization
- Evidence anchors:
  - [abstract] We utilized multilingual datasets to improve the cross-lingual performance of our model
  - [section] Our dataset comprises publicly available datasets and a part that we synthesized... Inspired by the findings of [1], we utilized multilingual datasets to improve the cross-lingual performance of our model
  - [section] The results are presented in the Appendix A.5.3 and Figure 20. We compare the cross-lingual performance of our model with Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3 and Salamandra-7B-Instruct

## Foundational Learning

- Concept: Multilingual tokenization strategies and fertility analysis
  - Why needed here: Understanding how tokenizer design affects model efficiency and multilingual performance is crucial for replicating and improving upon the Teuken approach
  - Quick check question: How would you measure the impact of tokenizer fertility on model performance across languages with different morphological complexity?

- Concept: Cross-lingual transfer learning principles
  - Why needed here: The models demonstrate strong cross-lingual performance despite being trained primarily on English data, requiring understanding of transfer mechanisms
  - Quick check question: What factors determine the effectiveness of cross-lingual transfer in multilingual LLMs, and how can they be measured?

- Concept: Instruction tuning methodologies for multilingual models
  - Why needed here: The instruction-following capabilities across 21 European languages require specific training techniques beyond standard monolingual approaches
  - Quick check question: How would you design an evaluation protocol to compare cross-lingual instruction-following capabilities across multiple multilingual models?

## Architecture Onboarding

- Component map: Tokenizer -> Base model pre-training -> Data mixing strategy -> Instruction tuning -> Evaluation
- Critical path: Tokenizer optimization → Base model pre-training → Data mixing strategy → Instruction tuning → Evaluation and iteration
- Design tradeoffs: 
  - Higher non-English token allocation improves multilingual performance but may reduce English task performance
  - Lower fertility reduces context window efficiency but improves multilingual coherence
  - Curriculum learning with educational content improves downstream performance but requires careful data selection
- Failure signatures:
  - High fertility values indicate tokenizer inefficiency, leading to context window exhaustion
  - Performance degradation in specific languages suggests data quality or quantity imbalances
  - Instruction-following failures across languages indicate insufficient multilingual instruction tuning
- First 3 experiments:
  1. Measure and compare fertility values across all 24 EU languages using the custom tokenizer versus baseline tokenizers
  2. Evaluate cross-lingual performance on a small multilingual benchmark to establish baseline transfer capabilities
  3. Test instruction-following consistency across languages using a multilingual MT-Bench variant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Teuken-7B-Instruct on reasoning tasks compare to models specifically optimized for mathematical reasoning?
- Basis in paper: [inferred] The paper notes that Teuken-7B-Instruct underperforms in the Math and Coding categories on MT-Bench-X, suggesting a potential gap in these capabilities.
- Why unresolved: The paper does not provide a detailed comparison of Teuken-7B-Instruct's reasoning performance against models explicitly optimized for mathematical reasoning tasks.
- What evidence would resolve it: A direct comparison of Teuken-7B-Instruct's performance on mathematical reasoning benchmarks (e.g., GSM8K, MATH) against state-of-the-art models like DeepSeekMath or LLaMA-3.1-8B-Instruct would clarify its relative strengths and weaknesses in this domain.

### Open Question 2
- Question: What is the impact of the custom multilingual tokenizer on model performance across different language families (e.g., Romance vs. Germanic vs. Slavic languages)?
- Basis in paper: [explicit] The paper describes the development of a custom multilingual tokenizer optimized for all 24 official European languages and presents fertility analysis comparing it to other tokenizers, showing particularly pronounced benefits for languages with complex morphology.
- Why unresolved: While the paper demonstrates the tokenizer's effectiveness in reducing fertility across languages, it does not provide a detailed breakdown of performance differences across specific language families.
- What evidence would resolve it: Performance comparisons of Teuken-7B-Base and Teuken-7B-Instruct across different language families (e.g., Romance, Germanic, Slavic) on standardized multilingual benchmarks would reveal the tokenizer's differential impact on model performance.

### Open Question 3
- Question: How does the model's performance scale with continued pre-training on additional tokens, particularly for low-resource European languages?
- Basis in paper: [explicit] The paper describes continued pre-training up to 6 trillion tokens and presents performance development across languages, but does not isolate the impact on low-resource languages specifically.
- Why unresolved: The paper provides overall performance trends but does not analyze how continued pre-training specifically affects the performance of low-resource European languages like Estonian, Finnish, or Maltese.
- What evidence would resolve it: Detailed performance tracking of low-resource European languages across different pre-training stages (e.g., 3T, 4T, 5T, 6T tokens) would demonstrate whether continued training disproportionately benefits these languages.

## Limitations
- Evaluation lacks comprehensive cross-lingual assessments for low-resource EU languages (Maltese, Irish, Latvian)
- Instruction-following capabilities demonstrated qualitatively but not systematically quantified across all 24 EU languages
- Comparative analysis against other multilingual models lacks full detail on experimental conditions

## Confidence

- **High confidence**: The tokenizer optimization approach (reduced fertility) and its theoretical benefits for multilingual efficiency are well-supported by technical analysis and comparative metrics across 19 of 24 languages
- **Medium confidence**: The claim that 60% non-English training data reduces English-centrism is supported by training methodology but requires more rigorous ablation studies to confirm causation
- **Medium confidence**: Cross-lingual instruction-following capabilities are demonstrated qualitatively but lack systematic quantitative evaluation across all supported languages

## Next Checks

1. **Cross-lingual task transfer validation**: Conduct controlled experiments measuring performance degradation when models are tested in languages different from their training language, specifically focusing on the 3 low-resource EU languages (Maltese, Irish, Latvian) that were excluded from benchmark evaluations

2. **Toxicity and bias assessment**: Expand safety evaluation beyond the PolygloToxicityPrompts benchmark to include culture-specific toxicity patterns and bias measurements across all 24 EU languages using region-specific evaluation datasets

3. **Instruction-following robustness test**: Design and execute a standardized multilingual MT-Bench evaluation covering all 24 EU languages to quantitatively measure cross-lingual instruction-following consistency, including tasks requiring complex reasoning and cultural knowledge specific to different European regions