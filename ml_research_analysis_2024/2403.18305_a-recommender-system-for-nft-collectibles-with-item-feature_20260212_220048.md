---
ver: rpa2
title: A Recommender System for NFT Collectibles with Item Feature
arxiv_id: '2403.18305'
source_url: https://arxiv.org/abs/2403.18305
tags:
- item
- recommender
- data
- system
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building a recommender system
  for NFTs, which is hindered by the lack of feedback information and user anonymity
  in the blockchain system. The proposed solution is a graph-based recommender system
  that incorporates item features such as image, text, and price as side information.
---

# A Recommender System for NFT Collectibles with Item Feature

## Quick Facts
- arXiv ID: 2403.18305
- Source URL: https://arxiv.org/abs/2403.18305
- Reference count: 8
- Primary result: NGCF-all model achieves Recall@10 of 0.1811 and NDCG@10 of 0.0862 on NFT collections

## Executive Summary
This paper addresses the challenge of building recommender systems for NFTs, which is complicated by sparse user-item interactions and user anonymity in blockchain systems. The authors propose a graph-based recommender system that leverages item features (image, text, and price) as side information to compensate for the lack of user feedback. Using a Neural Graph Collaborative Filtering (NGCF) framework with item features, the system captures complex relationships between users and NFTs through iterative message passing on the interaction graph.

## Method Summary
The proposed method combines graph neural networks with item feature embeddings to build an NFT recommender system. The NGCF model propagates user and item embeddings through the interaction graph while incorporating item features (image features via CAE, text features via Word2Vec, and price features) as side information. The model is trained using pairwise BPR loss with Adam optimizer, and evaluated using Recall@K and NDCG@K metrics on five NFT collections with at least 3 interactions per item.

## Key Results
- NGCF-all (using all item features) outperforms other models with average Recall@10 of 0.1811 and NDCG@10 of 0.0862
- Text and price features are more influential than image features for NFT recommendation performance
- The NGCF model with item features significantly improves performance over NGCF without features across all tested collections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph-based architecture captures higher-order user-item connectivity that improves recommendation performance.
- Mechanism: The NGCF model propagates embeddings iteratively through the user-item interaction graph, incorporating collaborative signals from multiple hops of neighbors into the user and item embeddings.
- Core assumption: The user-item interaction graph contains meaningful collaborative signals that can be propagated through multiple hops to improve recommendation quality.
- Evidence anchors:
  - [abstract] "Graph learning provides a unified way to utilize all available different types of data and effectively capture non-trivial user-item, user-user, and item-item relationships."
  - [section] "NGCF (Wang et al. 2019) proposes iterative user and item embeddings propagation to add collaborative signal into the embedding process."

### Mechanism 2
- Claim: Incorporating item features as side information compensates for the lack of user feedback and anonymity in NFT transactions.
- Mechanism: Item features (image, text, and price) are transformed into dense representations through dedicated encoders and concatenated with the graph-based embeddings, providing additional signal for recommendation.
- Core assumption: NFT item features contain discriminative information that correlates with user preferences and can supplement the sparse interaction data.
- Evidence anchors:
  - [abstract] "We exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature."
  - [section] "From the fact that price feature enhances the recommendation performance, it seems like adding the price information of each product could reflect user's budgets."

### Mechanism 3
- Claim: The NGCF-all model (using all item features) outperforms models using individual features by capturing complementary information.
- Mechanism: Different item features capture different aspects of NFTs - visual characteristics (image), semantic properties (text), and economic factors (price) - which together provide a more complete representation of user preferences.
- Core assumption: The different types of item features capture complementary rather than redundant information about NFTs.
- Evidence anchors:
  - [section] "Figure 1, shows that both NGCF-txt and NGCF-all outperform NGCF without features across all collections while NGCF-img outperforms NGCF on 3 collections by a slight margin."
  - [section] "This demonstrates that tag information (text), and price are the most influential features of NFTs that shape user's preference whereas the image feature is less likely to be influential."

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The NGCF model relies on graph convolution operations to propagate information through the user-item interaction graph
  - Quick check question: What is the difference between spectral and spatial graph convolution approaches, and which one does NGCF use?

- Concept: Embedding initialization and transfer learning
  - Why needed here: The model initializes item embeddings with pre-extracted feature representations rather than random initialization
  - Quick check question: How does initializing embeddings with meaningful representations (like image features) differ from random initialization in terms of convergence and final performance?

- Concept: Negative sampling for implicit feedback
  - Why needed here: The model uses pairwise BPR loss with negative sampling to learn from implicit feedback (purchase history)
  - Quick check question: Why is negative sampling necessary for implicit feedback recommendation, and what are the trade-offs in choosing the number of negative samples?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Feature extraction modules (CAE for images, Word2Vec for text) -> NGCF model with concatenated features -> Prediction layer -> Training loop with BPR loss

- Critical path:
  1. Data collection and preprocessing
  2. Feature extraction and embedding initialization
  3. Graph construction from user-item interactions
  4. NGCF model training with feature concatenation
  5. Evaluation using Recall@K and NDCG@K

- Design tradeoffs:
  - Feature selection: Including all features vs. selecting only the most informative ones
  - Graph depth: Number of propagation layers in NGCF (balancing information propagation vs. over-smoothing)
  - Negative sampling rate: Balancing training efficiency vs. model accuracy
  - Embedding dimensionality: Balancing representation capacity vs. overfitting risk

- Failure signatures:
  - Poor performance on sparse collections: Indicates the graph structure lacks sufficient collaborative signals
  - Overfitting on small collections: Suggests the model is too complex relative to available data
  - Degradation when adding features: Indicates feature redundancy or noise rather than complementary information

- First 3 experiments:
  1. Baseline NGCF without item features to establish the contribution of graph structure alone
  2. NGCF variants with individual features (NGCF-img, NGCF-txt, NGCF-price) to assess feature importance
  3. NGCF-all with all features combined to evaluate complementary information capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of item features (image, text, price) interact to influence recommendation performance in NFT systems?
- Basis in paper: [explicit] The paper compares NGCF models using individual features (NGCF-img, NGCF-txt, NGCF-price) and their combination (NGCF-all), finding that text and price features are more influential than image features.
- Why unresolved: While the paper shows that combining all features (NGCF-all) outperforms individual feature models, it doesn't analyze which specific feature combinations are most effective or how different feature interactions affect performance.
- What evidence would resolve it: Systematic ablation studies testing all possible feature combinations (e.g., image+text, text+price, image+price) and analyzing feature importance scores to determine optimal feature subsets.

### Open Question 2
- Question: How generalizable is the proposed NFT recommender system across different NFT collection types and market conditions?
- Basis in paper: [inferred] The study tests the system on five NFT collections but doesn't explore performance variations across different collection types (art, gaming, utility) or market conditions (bull/bear markets).
- Why unresolved: The paper demonstrates effectiveness on a limited set of collections but doesn't address how the system would perform on diverse NFT types or during different market cycles.
- What evidence would resolve it: Testing the system on a wider variety of NFT collections across different categories and conducting experiments during different market conditions to assess robustness and generalizability.

### Open Question 3
- Question: How can the recommender system be adapted to handle the cold-start problem for new users and newly listed NFTs?
- Basis in paper: [inferred] The paper focuses on existing transaction data and doesn't address scenarios with limited user history or new items entering the market.
- Why unresolved: The current system relies on historical transaction data and existing item features, but doesn't propose solutions for recommending to new users or new NFTs without sufficient interaction history.
- What evidence would resolve it: Developing and testing strategies for cold-start scenarios, such as using item similarity metrics, demographic information (if available), or hybrid approaches combining content-based and collaborative filtering methods.

## Limitations

- Experimental results are limited to a specific time period (September 2021â€“September 2022) and five curated NFT collections, limiting generalizability.
- Evaluation focuses on top-K metrics without assessing diversity, novelty, or long-term user satisfaction.
- Feature extraction methods represent specific choices that may not be optimal for NFT domain characteristics.

## Confidence

**High Confidence Claims:**
- The NGCF model architecture with item feature concatenation is technically sound and implementable as described
- The experimental methodology (data collection, preprocessing, evaluation protocol) follows standard practices
- The observation that price features significantly improve performance is well-supported by the experimental results

**Medium Confidence Claims:**
- The relative performance ranking of different feature combinations (NGCF-all > NGCF-txt > NGCF-img > NGCF-price) is supported by the reported results but may vary with different hyperparameter settings
- The assertion that text and price features are more influential than image features is based on observed performance differences but lacks deeper analysis of why this occurs

**Low Confidence Claims:**
- The claim that the proposed method will generalize well to other NFT collections or time periods beyond the experimental dataset
- The assertion that higher-order connectivity captured by NGCF necessarily improves recommendation quality in all NFT contexts

## Next Checks

1. **Cross-collection generalization test**: Evaluate the trained NGCF-all model on NFT collections outside the training set (e.g., newer collections or different categories like generative art vs. profile pictures) to assess whether the learned representations transfer effectively across domains.

2. **Feature ablation stability analysis**: Systematically vary the feature extraction methods (e.g., use CLIP for image features instead of CAE, or transformer-based embeddings for text) while keeping the NGCF architecture constant to determine whether the observed performance differences are robust to feature extraction choices.

3. **Cold-start and sparse data robustness test**: Evaluate model performance on collections with varying levels of interaction sparsity (e.g., collections with minimum 1, 2, or 3 interactions per item) to determine the threshold at which the graph-based approach breaks down and whether item features can effectively compensate for extremely sparse interactions.