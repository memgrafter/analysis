---
ver: rpa2
title: 'EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis'
arxiv_id: '2411.10004'
source_url: https://arxiv.org/abs/2411.10004
tags:
- images
- image
- fundus
- eyediff
- retinal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EyeDiff is a text-to-image diffusion model designed to generate
  multimodal ophthalmic images for improving rare eye disease diagnosis. Trained on
  42,048 images across 14 modalities and 80+ diseases, it synthesizes realistic images
  guided by text prompts.
---

# EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis

## Quick Facts
- arXiv ID: 2411.10004
- Source URL: https://arxiv.org/abs/2411.10004
- Reference count: 40
- EyeDiff is a text-to-image diffusion model that significantly improves rare eye disease diagnosis by generating multimodal ophthalmic images

## Executive Summary
EyeDiff is a text-to-image diffusion model designed to address the critical challenge of rare eye disease diagnosis by generating synthetic multimodal ophthalmic images. Trained on 42,048 images across 14 modalities and over 80 diseases, the model synthesizes realistic images guided by text prompts. Human evaluation demonstrates high visual quality (Cohen's kappa 0.870) and Turing test accuracy of 62-66%. By integrating generated images with real data, EyeDiff significantly improves classification accuracy for both common diseases like diabetic retinopathy and glaucoma, as well as rare diseases (AUROC up to 0.919), effectively addressing data imbalance and scarcity issues in ophthalmology.

## Method Summary
EyeDiff is built on Stable Diffusion v1-5 and generates multimodal ophthalmic images using text prompts. The model was trained on 42,048 images from eight datasets covering 14 modalities and over 80 diseases, using a dictionary-based labeling approach for text prompt construction. Training was performed for 5 epochs with AdamW optimizer (lr=5e-5, weight decay=1e-2) and batch size of 8, with images resized to 512x512. Generated images were evaluated using both VQAScore and human expert assessment. The model was then integrated with real data to improve downstream classification tasks for common and rare eye diseases, demonstrating significant performance gains over traditional oversampling methods.

## Key Results
- Human evaluation shows high visual quality with Cohen's kappa of 0.870
- Turing test accuracy of 62-66% indicates generated images are often indistinguishable from real images
- Classification performance improves significantly with generated images, achieving AUROC up to 0.919 for rare diseases

## Why This Works (Mechanism)
EyeDiff leverages the generative power of diffusion models to create synthetic ophthalmic images that capture the visual characteristics of both common and rare eye diseases. By training on a diverse dataset spanning multiple modalities and disease types, the model learns to synthesize images that maintain diagnostic relevance while addressing data scarcity. The text-to-image conditioning allows for targeted generation of specific disease manifestations, enabling effective data augmentation for rare conditions where real samples are limited.

## Foundational Learning
- **Stable Diffusion v1-5**: Understanding this base architecture is essential as EyeDiff builds upon it. Why needed: Provides the generative foundation. Quick check: Can you explain the U-Net architecture and denoising process?
- **Text-to-image conditioning**: The mechanism by which text prompts guide image generation. Why needed: Core to how EyeDiff generates disease-specific images. Quick check: Can you describe how CLIP embeddings are used in text conditioning?
- **Diffusion model training**: The process of training generative models through iterative denoising. Why needed: Critical for understanding model behavior and limitations. Quick check: Can you explain the role of the noise schedule in diffusion models?
- **Cohen's kappa**: Statistical measure of inter-rater agreement. Why needed: Used to evaluate human assessment of generated image quality. Quick check: Can you calculate Cohen's kappa given a confusion matrix?
- **AUROC/AUPR metrics**: Evaluation metrics for binary classification performance. Why needed: Used to assess diagnostic accuracy improvements. Quick check: Can you interpret AUROC curves and explain the difference from accuracy?

## Architecture Onboarding

Component Map: Text Prompts -> EyeDiff (Stable Diffusion v1-5) -> Generated Images -> Classification Pipeline -> Diagnostic Performance

Critical Path: Text prompt construction → Image generation → Quality evaluation → Integration with real data → Downstream classification

Design Tradeoffs: The choice of Stable Diffusion v1-5 provides strong generative capabilities but may limit control over specific anatomical features compared to specialized medical imaging models. The 512x512 resolution balances quality with computational efficiency but may not capture fine-grained pathological details.

Failure Signatures: Generated images may lack realistic pathological features (detected by low VQAScore or expert rejection), or the integration may fail to improve classification (detected by unchanged or decreased performance metrics).

First Experiments:
1. Generate a diverse set of images for a specific disease and evaluate visual quality using both automated metrics and human assessment
2. Integrate generated images with real data at different mixing ratios and measure impact on classification accuracy
3. Test the model's ability to generate images for diseases with very limited training data to assess generalization

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does EyeDiff's performance scale with the size of the training dataset and the diversity of disease representations?
- Basis in paper: [explicit] The paper mentions that EyeDiff is trained on 42,048 images from eight datasets, but does not explore the impact of varying training dataset size or diversity on performance.
- Why unresolved: The paper does not provide a systematic analysis of how different training dataset characteristics affect the quality and utility of generated images.
- What evidence would resolve it: A study comparing EyeDiff's performance across different training dataset sizes and disease diversity levels would clarify the relationship between training data characteristics and model effectiveness.

### Open Question 2
- Question: What is the long-term stability and clinical utility of EyeDiff-generated images for rare disease diagnosis in real-world clinical settings?
- Basis in paper: [inferred] While the paper demonstrates improved diagnostic accuracy in controlled experiments, it does not address the sustainability of these improvements in diverse clinical environments or over extended periods.
- Why unresolved: The study's focus on controlled experiments and specific datasets does not account for the variability and challenges of real-world clinical practice.
- What evidence would resolve it: Longitudinal studies tracking the performance of EyeDiff in various clinical settings and over time would provide insights into its real-world utility and stability.

### Open Question 3
- Question: How can EyeDiff be adapted to generate images for diseases with very limited or no existing data?
- Basis in paper: [inferred] The paper discusses EyeDiff's effectiveness for rare diseases but does not address the challenge of generating images for diseases with extremely scarce or no data.
- Why unresolved: The current methodology relies on existing data to guide image generation, which may not be feasible for diseases with no prior data.
- What evidence would resolve it: Developing and testing methods to generate images for data-scarce diseases, possibly through transfer learning or synthetic data augmentation techniques, would address this limitation.

## Limitations
- The exact text prompt construction methodology and custom dictionary mapping details are not fully specified, making exact reproduction challenging
- Limited details on the downstream classification pipeline integration make it difficult to determine if reported improvements are solely due to generated images
- The model's performance on diseases with extremely limited or no existing data has not been evaluated

## Confidence

Major claim clusters confidence:
- Image generation quality: High (supported by human evaluation metrics)
- Classification performance improvements: Medium (results are strong but integration details are limited)
- Solution to data imbalance: Medium (demonstrated but specific augmentation ratios unclear)

## Next Checks

1. Reconstruct the text prompt dictionary and generate a sample set of images to verify visual quality matches reported standards
2. Implement the integration methodology with a simple classifier to confirm that mixing real and generated images improves performance on a held-out test set
3. Test the model's ability to generate images for diseases not in the original training set to assess true generalization capabilities