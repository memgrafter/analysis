---
ver: rpa2
title: 'DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination'
arxiv_id: '2410.04514'
source_url: https://arxiv.org/abs/2410.04514
tags:
- tokens
- visual
- attention
- image
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of object hallucination in
  large vision-language models (LVLMs), where generated text does not align with the
  visual ground truth. The authors analyze the attention distributions of both the
  visual encoder and LLM decoder, finding that both components focus on outlier tokens
  in the background rather than referred objects.
---

# DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination

## Quick Facts
- arXiv ID: 2410.04514
- Source URL: https://arxiv.org/abs/2410.04514
- Reference count: 26
- One-line primary result: Training-free method that filters high-attention outlier tokens using ViT's CLS token and applies contrastive decoding to reduce object hallucination in LVLMs

## Executive Summary
This paper addresses object hallucination in large vision-language models (LVLMs), where generated text fails to align with visual ground truth. The authors identify that both visual encoders and LLM decoders focus on outlier tokens in background regions rather than referred objects, creating a feedback loop that amplifies hallucination. DAMRO is proposed as a training-free solution that identifies these outlier tokens using the CLS token's attention, projects them into the LLM, and applies contrastive decoding to suppress their influence. The method is evaluated across LLaVA-1.5, LLaVA-NeXT, and InstructBLIP, showing significant improvements on hallucination benchmarks.

## Method Summary
DAMRO is a training-free approach that addresses object hallucination by filtering high-attention outlier tokens from the visual encoder's attention map. The method uses the classification token (CLS) of ViT to identify these outlier tokens, projects them into the LLM along with normal tokens, and applies contrastive decoding to reduce the LLM decoder's reliance on them. The approach is model-agnostic and does not require external information or models, making it broadly applicable across different LVLM architectures.

## Key Results
- DAMRO significantly reduces object hallucination on POPE, CHAIR, and MME benchmarks
- The method shows strong generalizability across LLaVA-1.5, LLaVA-NeXT, and InstructBLIP architectures
- Improvements are achieved without requiring training or external models
- GPT-4V-aided evaluation confirms gains in both accuracy and detailedness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The visual encoder in LVLMs produces outlier tokens that capture global background information rather than local object details
- Mechanism: ViT attention maps concentrate on a small number of high-norm tokens located in background regions, which contain minimal local information but some global context
- Core assumption: These outlier tokens are inherently flawed representations that mislead the LLM decoder
- Evidence anchors:
  - [abstract] "We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination"
  - [section 3.2] "Darcet et al. (2024) find that there are always high-norm outlier tokens in ViT, which tend to appear in background regions with redundant patch information, containing minimal local information but a little global information"
  - [corpus] Weak - no direct evidence about outlier token properties in neighboring papers

### Mechanism 2
- Claim: The LLM decoder inherits and amplifies the attention patterns from the visual encoder, focusing on the same outlier tokens during text generation
- Mechanism: When visual tokens are projected to the LLM, the decoder pays attention to tokens that received high attention values in the visual encoder, perpetuating the focus on background information
- Core assumption: Attention consistency between visual encoder and LLM decoder creates a feedback loop that reinforces hallucination
- Evidence anchors:
  - [abstract] "We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects"
  - [section 3.3] "LLM decoder attention map also features with a few outlier tokens at the same position as visual encoder that get most of the attention compared to other tokens"
  - [corpus] Moderate - related work on attention calibration suggests attention consistency issues exist but doesn't directly address inheritance mechanism

### Mechanism 3
- Claim: Contrastive decoding can effectively reduce the influence of outlier tokens by contrasting them with normal tokens
- Mechanism: By treating high-attention outlier tokens as negative samples and subtracting their influence during decoding, the model shifts focus toward more informative local tokens
- Core assumption: The visual prior contained in outlier tokens can be suppressed without losing necessary global context
- Evidence anchors:
  - [abstract] "DAMRO filters out high-norm outlier tokens from the ViT attention map, identifying them as negative tokens, and then projects them into the LLM along with normal tokens. Contrastive decoding is then applied to reduce the LLM decoder's reliance on these tokens"
  - [section 4.2] "The probability distribution in the logits space attenuates the influence of previous outlier tokens on decoding. This allows the model to focus more on fine-grained semantic information"
  - [corpus] Strong - multiple related papers use contrastive decoding approaches for hallucination mitigation

## Foundational Learning

- Concept: Transformer attention mechanisms and their role in visual representation
  - Why needed here: Understanding how ViT attention works is crucial for identifying why outlier tokens form and how they affect downstream processing
  - Quick check question: How does the multi-head self-attention mechanism in ViT determine which tokens receive higher attention weights?

- Concept: Vision-Language Model architecture and modality alignment
  - Why needed here: The interaction between visual encoder and LLM decoder through projection modules is central to understanding the hallucination problem
  - Quick check question: What are the key differences between using MLP layers versus Q-Former for connecting visual and language modalities?

- Concept: Contrastive learning principles and their application to text generation
  - Why needed here: DAMRO's effectiveness relies on properly implementing contrastive decoding to suppress negative visual priors
  - Quick check question: How does contrastive decoding mathematically modify the probability distribution during token sampling?

## Architecture Onboarding

- Component map: Visual Encoder (ViT) → Projection Module → LLM Decoder → Text Generation
- Critical path: Image → Visual Encoder → Attention Analysis → Outlier Token Identification → Contrastive Decoding → Text Generation
  - The attention mechanism from visual encoder to LLM decoder is the critical vulnerability point
  - Outlier token identification using CLS token attention is the key intervention point

- Design tradeoffs:
  - Token filtering vs. information preservation: Removing too many tokens may lose global context
  - Contrast strength (α parameter): Too high causes information loss, too low insufficient hallucination reduction
  - Top-k selection: Different models require different numbers of outlier tokens to filter

- Failure signatures:
  - Performance degradation on spatial reasoning tasks (as seen with InstructBLIP)
  - Loss of global context in generated descriptions
  - Inconsistent results across different image types or object categories

- First 3 experiments:
  1. Verify attention consistency: Compare visual encoder and LLM decoder attention maps on sample images to confirm outlier token overlap
  2. Test α sensitivity: Run DAMRO with different α values on POPE benchmark to find optimal contrast strength
  3. Validate token sufficiency: Compare DAMRO performance using different numbers of visual tokens (1, 5, all) to confirm minimal token requirement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for the correlation between the attention consistency between the visual encoder and LLM decoder and the occurrence of object hallucinations?
- Basis in paper: [explicit] The paper discusses the correlation between the attention maps of the visual encoder and the LLM decoder, finding that both distributions tend to focus on particular background tokens rather than the referred objects in the image.
- Why unresolved: The paper mentions the correlation but does not provide a detailed theoretical explanation for why this consistency leads to hallucinations.
- What evidence would resolve it: A detailed theoretical analysis of the attention mechanisms in both the visual encoder and the LLM decoder, explaining why the consistency in attention to outlier tokens leads to hallucinations.

### Open Question 2
- Question: How does the performance of DAMRO vary with different projection modules (e.g., Q-Former) in the visual encoder and LLM decoder?
- Basis in paper: [inferred] The paper mentions that they have not conducted a detailed exploration of more complex projection modules in the visual encoder and LLM decoder.
- Why unresolved: The paper only tests DAMRO on LLaVA-1.5 and InstructBLIP, which have different projection modules, but does not explore the performance with other projection modules.
- What evidence would resolve it: Experimental results of DAMRO's performance with various projection modules, such as Q-Former, to determine if the method remains effective across different architectures.

### Open Question 3
- Question: How does the choice of the number of outlier tokens (top k) affect the performance of DAMRO across different models and benchmarks?
- Basis in paper: [explicit] The paper mentions that they set different values of top k for different models (e.g., top 10 for LLaVA-1.5 and LLaVA-NeXT, top 4 for InstructBLIP) and provides ablation studies on the effect of top k.
- Why unresolved: While the paper provides ablation studies, it does not determine the optimal value of top k for each model and benchmark.
- What evidence would resolve it: A comprehensive study on the effect of different values of top k on the performance of DAMRO across various models and benchmarks, to identify the optimal value for each case.

## Limitations
- The reliance on CLS token attention for outlier detection may not generalize to models with different visual encoder architectures
- The assumption that outlier tokens contain primarily redundant background information may not hold for all image types, particularly those requiring global context
- The effectiveness of contrastive decoding assumes a binary distinction between informative and non-informative tokens that may not reflect the nuanced reality of visual representation

## Confidence

- **High Confidence**: The empirical observation that visual encoder attention concentrates on outlier tokens in background regions is well-supported by attention map analysis. The consistency between visual encoder and LLM decoder attention patterns is clearly demonstrated through direct comparison.
- **Medium Confidence**: The causal link between outlier token attention and object hallucination is logically sound but relies on indirect evidence through performance improvements rather than direct ablation studies of token removal effects.
- **Medium Confidence**: The mechanism by which contrastive decoding suppresses outlier token influence is theoretically justified but the optimal contrast strength (α parameter) appears sensitive to model architecture, requiring careful tuning.

## Next Checks

1. **Ablation Study on Token Removal**: Systematically remove different numbers of high-attention tokens (1, 3, 5, all) from the visual encoder input and measure the impact on hallucination rates and global context preservation across multiple image categories.

2. **Cross-Architecture Generalization**: Test DAMRO on models with alternative visual encoders (ConvNeXt, Swin Transformer) and different modality projection methods (direct MLP vs. Q-Former) to determine if the outlier token phenomenon is universal or architecture-specific.

3. **Attention Pattern Analysis Post-Intervention**: After applying DAMRO, conduct detailed attention map comparisons to verify that the LLM decoder attention has redistributed from outlier tokens to referred objects, and quantify the relationship between attention redistribution and hallucination reduction.