---
ver: rpa2
title: 'STEER: Assessing the Economic Rationality of Large Language Models'
arxiv_id: '2402.09552'
source_url: https://arxiv.org/abs/2402.09552
tags:
- element
- which
- ability
- each
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive framework for evaluating
  the economic rationality of large language models (LLMs) across 64 fine-grained
  elements spanning arithmetic, probability, logic, game theory, social choice, and
  mechanism design. The authors generate a benchmark of 24,500 multiple-choice questions,
  validated across multiple domains and difficulty levels, to assess LLM performance
  on each element.
---

# STEER: Assessing the Economic Rationality of Large Language Models

## Quick Facts
- arXiv ID: 2402.09552
- Source URL: https://arxiv.org/abs/2402.09552
- Reference count: 30
- Primary result: Comprehensive benchmark evaluating 14 LLMs (7B-100B parameters) across 64 elements of economic rationality, finding strong size-performance correlation but overall limited rational decision-making capabilities

## Executive Summary
This paper introduces STEER (Survey of Training Effectiveness for Economic Rationality), a comprehensive framework for evaluating the economic rationality of large language models across 64 fine-grained elements spanning arithmetic, probability, logic, game theory, social choice, and mechanism design. The authors generate a benchmark of 24,500 multiple-choice questions, validated across multiple domains and difficulty levels, to assess LLM performance on each element. Testing 14 models from 7B to 100B parameters, they find strong correlations between model size and performance, with GPT-4 Turbo achieving the highest accuracy (63.6%) but still falling short of perfect rationality. Performance degrades consistently with increasing difficulty and shows domain-dependent variability, particularly in medical contexts. Self-explanation prompts generally improve performance, especially on lower-grade questions, while few-shot prompting helps up to three examples before degrading results. The study provides a standardized methodology for assessing economic rationality in LLMs, with results suggesting current models exhibit limited rational decision-making capabilities despite their size.

## Method Summary
The authors developed a hierarchical taxonomy of 64 economic rationality elements organized into foundations, single-agent decisions, multi-agent decisions, and decisions on behalf of others. Using GPT-4 Turbo, they generated 24,500 multiple-choice questions across 10 domains and 4 grade levels through template-based prompting. Questions were validated through human review of 10% of samples, with subsequent corrections made to the generation pipeline. The benchmark was then used to test 14 models ranging from 7B to 100B parameters, evaluating performance zero-shot and with self-explanation or few-shot prompting. Metrics included accuracy, normalized accuracy, expected calibration error, domain robustness (performance variation across domains), and dependency robustness (performance variation across elements with and without prerequisite elements).

## Key Results
- GPT-4 Turbo achieved highest accuracy at 63.6%, with performance strongly correlating with model size across all elements
- Self-explanation prompting generally enhanced performance, offering the most gains on lower-grade questions
- Few-shot prompting improved results up to three examples but degraded performance beyond that threshold
- Domain robustness varied significantly, with medical contexts showing particularly degraded performance
- All models exhibited performance degradation with increasing question difficulty (grade level)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's effectiveness stems from grounding all questions in well-defined "right answers" based on economic rationality principles.
- Mechanism: By aligning questions with the von Neumann-Morgenstern utility axioms, the framework creates a consistent reference point for evaluating LLM behavior across diverse economic scenarios.
- Core assumption: Economic rationality principles (particularly vNM axioms) provide a stable foundation for assessing decision-making quality across different contexts.
- Evidence anchors:
  - [abstract] "We advocate a different approach: enumerating first principles that describe how agents should make decisions, and then evaluating an agent's degree of adherence with these principles."
  - [section] "We follow [Kochenderfer, 2015] in organizing the modules in this setting by the normative axioms in deterministic and stochastic environments as well as deviations from these axioms drawn from the descriptive literature."
- Break condition: If economic rationality principles prove insufficient for certain domains or if real-world decision-making requires fundamentally different evaluation criteria.

### Mechanism 2
- Claim: The hierarchical taxonomy structure enables systematic assessment of rationality at multiple levels of complexity.
- Mechanism: By organizing elements into settings (foundations, single-agent decisions, multi-agent decisions, decisions on behalf of others), modules, and specific elements, the framework creates dependencies that allow for targeted evaluation and robustness analysis.
- Core assumption: Economic decision-making can be meaningfully decomposed into a hierarchy of increasingly complex elements with clear dependencies.
- Evidence anchors:
  - [section] "We define each element, giving an example of each in an appendix. We then move on (in Section 3) to describe how we used this taxonomy to derive a fine-grained benchmark distribution that serves as the basis for rationality report cards."
  - [section] "It is quite possible for an LLM to be proficient at advanced tasks without proficiency at more basic tasks that make them up. But such behavior is probably not desirable; it offers evidence that if the advanced task were discussed in different terms (e.g., in ways that invoke the conceptually simpler subtasks) model performance could fall."
- Break condition: If the dependency structure fails to capture important relationships between elements or if certain elements prove independent of the hierarchy.

### Mechanism 3
- Claim: The synthetic question generation approach enables comprehensive coverage while maintaining control over difficulty and domain.
- Mechanism: Using GPT-4 Turbo to generate questions from templates allows systematic variation across grade levels and domains while enabling validation and quality control.
- Core assumption: LLMs can generate high-quality, valid questions that accurately test specific elements of rationality when provided with appropriate templates and constraints.
- Evidence anchors:
  - [section] "We leverage a state-of-the-art LLM to generate a diverse and substantial set of questions, based on a hand-constructed inputs."
  - [section] "We create several templates for each element, which differ in two key ways. First, we vary the subject matter of the question, which we call the domain, in order to enable assessments of LLM robustness across topics."
- Break condition: If generated questions systematically fail to capture the intended element of rationality or if the validation process proves too resource-intensive.

## Foundational Learning

- Concept: Von Neumann-Morgenstern utility theory
  - Why needed here: Provides the mathematical foundation for defining "rational" behavior in the benchmark
  - Quick check question: What are the four axioms of vNM utility theory and how do they ensure rational decision-making?

- Concept: Game theory solution concepts (Nash equilibrium, subgame perfection)
  - Why needed here: Essential for evaluating rationality in multi-agent environments
  - Quick check question: How does backward induction differ from finding Nash equilibrium in normal form games?

- Concept: Cognitive bias identification and measurement
  - Why needed here: Enables comparison between LLM behavior and human economic decision-making patterns
  - Quick check question: What distinguishes the certainty effect from the reflection effect in prospect theory?

## Architecture Onboarding

- Component map: Template generation system (GPT-4 Turbo based) -> Question validation interface -> Benchmark database (24,500+ questions) -> Web application (hierarchical navigation, filtering, scoring) -> Evaluation pipeline (model testing across elements)
- Critical path: Template → Generation → Validation → Storage → Evaluation → Report generation
- Design tradeoffs:
  - Synthetic generation vs. manual creation (coverage vs. control)
  - Multiple domains vs. depth in specific areas
  - Hierarchical structure vs. flat assessment (diagnostic vs. summative)
  - Validation cost vs. question quality
- Failure signatures:
  - Low validation rates indicating template problems
  - Domain-specific performance drops suggesting bias or alignment issues
  - Grade-level inconsistencies indicating difficulty calibration problems
  - Dependency robustness failures suggesting hierarchy issues
- First 3 experiments:
  1. Validate template generation by testing 100 questions from a new template and measuring validation rate
  2. Test model performance on a single element across all domains to assess domain robustness
  3. Compare self-explanation vs. few-shot prompting on a subset of questions to measure adaptation strategy effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of model size on performance for elements requiring multi-agent reasoning versus single-agent decision making?
- Basis in paper: [explicit] "Model performance consistently decreased with grade level" and "Performance correlated heavily with model size"
- Why unresolved: The paper shows overall size correlation but doesn't break down performance differences between single-agent and multi-agent elements by model size
- What evidence would resolve it: A detailed analysis showing normalized accuracy by model size bracket (e.g., <10B, 10-40B, >40B parameters) separately for single-agent versus multi-agent elements

### Open Question 2
- Question: How do different self-explanation prompting strategies (separate vs together) affect model calibration across different grade levels?
- Basis in paper: [explicit] "Self-explanation generally enhanced performance, albeit offering the most gains on lower-grade-level questions"
- Why unresolved: The paper mentions performance improvements but doesn't provide calibration error (ECE) metrics broken down by prompting strategy and grade level
- What evidence would resolve it: ECE measurements for both separate and together prompting strategies across all grade levels, showing whether confidence calibration improves differently at various difficulty levels

### Open Question 3
- Question: What is the relationship between dependency robustness and domain robustness in model performance?
- Basis in paper: [explicit] "Domain robustness" and "Dependency robustness" are both measured as distinct metrics
- Why unresolved: The paper presents these as separate measures but doesn't analyze their correlation or whether models that perform well in one type of robustness tend to perform well in the other
- What evidence would resolve it: A correlation analysis between domain robustness and dependency robustness scores across all elements, potentially revealing whether certain models exhibit both types of robustness or specialize in one type

## Limitations
- The synthetic question generation approach may not fully capture the complexity and nuance of real economic decision-making situations
- Domain-dependent performance variations suggest current LLMs may not truly generalize rational decision-making principles across diverse real-world scenarios
- The framework assumes vNM utility axioms provide an appropriate normative standard, but real economic agents often deviate from these principles in context-dependent ways

## Confidence

- **High Confidence**: The correlation between model size and performance on economic rationality tasks is robust and well-supported by the data
- **Medium Confidence**: The claim that self-explanation prompts generally improve performance, particularly on lower-grade questions, is supported by the data but shows significant variability across elements and domains
- **Low Confidence**: The assertion that LLMs exhibit "limited rational decision-making capabilities" is somewhat speculative, as the relationship between benchmark performance and practical economic reasoning ability remains unclear

## Next Checks
1. **Domain Transfer Experiment**: Test the same models on economic rationality questions translated into radically different domains (e.g., using fantasy or abstract scenarios) to assess whether performance degradation is truly domain-specific or indicates deeper limitations in rational reasoning capabilities

2. **Real-World Decision Validation**: Compare model performance on the benchmark with their actual decision-making in simulated economic environments or games, to validate whether benchmark performance translates to practical economic reasoning

3. **Ablation Study on Question Generation**: Conduct a systematic ablation study where questions are generated by different methods (human-crafted, template-based, fully synthetic) to isolate the impact of the synthetic generation approach on the validity of the benchmark results