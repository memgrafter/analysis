---
ver: rpa2
title: 'FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization
  Across Tokenizers'
arxiv_id: '2408.04816'
source_url: https://arxiv.org/abs/2408.04816
tags:
- embedding
- image
- arxiv
- language
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring knowledge across
  different language models that use different tokenizers and embedding spaces. The
  authors propose FUSE (Flexible Unification of Semantic Embeddings), a method to
  approximate an adapter layer that maps between the embeddings of multiple models
  without fine-tuning.
---

# FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers

## Quick Facts
- arXiv ID: 2408.04816
- Source URL: https://arxiv.org/abs/2408.04816
- Authors: Joshua Nathaniel Williams; J. Zico Kolter
- Reference count: 33
- Primary result: FUSE enables zero-shot adapter discovery for prompt optimization across different tokenizers without fine-tuning

## Executive Summary
This paper addresses the challenge of transferring knowledge across different language models that use different tokenizers and embedding spaces. The authors propose FUSE (Flexible Unification of Semantic Embeddings), a method to approximate an adapter layer that maps between the embeddings of multiple models without fine-tuning. FUSE focuses on groups of whitespace-separated tokens rather than individual tokens, introducing a third-order tensor-based representation of a model's embedding space. This representation allows for computing an approximate gradient of one model's outputs with respect to another model's embedding space using a precomputed tensor. The authors demonstrate the effectiveness of their approach through zero-shot image captioning tasks, showing significant improvements over existing zero-shot methods in terms of BLEU, METEOR, CIDEr, and SPICE scores.

## Method Summary
FUSE approximates adapter behavior by computing a Moore-Penrose pseudoinverse of a third-order tensor representing the source model's embedding space. The method uses tensor-t product operations to approximate the gradient of the target model's outputs with respect to the source model's embedding space. By focusing on whitespace-separated token groups rather than individual tokens, FUSE creates consistent semantic representations across different tokenizers. The approach leverages precomputed tensor mappings conditioned on token length (limited to l=4 tokens per word for computational efficiency) to maintain practical memory usage while enabling cross-model gradient computation for prompt optimization.

## Key Results
- Significant improvements in zero-shot image captioning quality (BLEU, METEOR, CIDEr, SPICE scores)
- Computational efficiency with adapter fitting requiring only 4 minutes and 22 seconds on standard hardware
- Effective handling of tokenizers requiring up to 4 tokens per word (covering 97.6% of BookCorpus dataset)

## Why This Works (Mechanism)

### Mechanism 1
FUSE approximates the gradient of one model's outputs with respect to another model's embedding space using a precomputed third-order tensor. The method computes a Moore-Penrose pseudoinverse of the embedding space tensor and uses tensor-t product operations to approximate the adapter behavior without fine-tuning. This works because the semantic meaning of whitespace-separated token groups remains consistent across different tokenizers, allowing tensor-based representation to align semantic embeddings.

### Mechanism 2
By focusing on whitespace-separated token groups rather than individual tokens, FUSE creates a consistent representation across different tokenizers. The split operation converts embeddings into lists of third-order tensors representing each word, while the merge operation reconstructs the original embedding shape, enabling consistent gradient computation. This approach works because white-space separation provides a reliable boundary for semantic units that remain consistent across tokenization schemes.

### Mechanism 3
The tensor t-product enables efficient computation of the approximate gradient mapping between embedding spaces of different tokenizers. By conditioning the mapping on token length l and precomputing the necessary tensor products for each token count, the method maintains computational efficiency while handling variable-length tokenizations. This works because the number of tokens required to represent words follows a distribution where most words need fewer than 4 tokens, making the l=4 limit practical.

## Foundational Learning

- Concept: Moore-Penrose pseudoinverse for tensors
  - Why needed here: Enables mapping between non-square embedding spaces by providing a least-squares solution for the adapter approximation
  - Quick check question: Why can't we use a standard matrix inverse for the embedding-to-token mapping?

- Concept: t-product for tensor multiplication
  - Why needed here: Provides the mathematical framework for computing gradients across third-order tensor representations of embedding spaces
  - Quick check question: How does the t-product differ from standard matrix multiplication in terms of computational complexity?

- Concept: Whitespace-based semantic grouping
  - Why needed here: Creates consistent semantic units across different tokenizers, enabling the tensor-based alignment strategy
  - Quick check question: What challenges might arise if tokenizers split words differently but maintain the same whitespace boundaries?

## Architecture Onboarding

- Component map: Input embeddings -> Split operation -> Tensor t-product -> Merge operation -> Output gradients
- Critical path: 1) Convert source model embeddings to text, 2) Retokenize with target model's tokenizer, 3) Compute t-product with precomputed tensor, 4) Merge gradients and update prompts
- Design tradeoffs:
  - Memory vs. accuracy: Limiting l=4 tokens per word reduces memory usage but may lose information for longer words
  - Speed vs. precision: Approximate gradient vs. true gradient computation
  - Flexibility vs. complexity: Handling arbitrary tokenizers vs. maintaining efficient tensor operations
- Failure signatures:
  - Gradient updates don't improve task performance
  - Memory errors when handling long sequences
  - Inconsistent results across different tokenizer pairs
  - Optimization gets stuck in local minima
- First 3 experiments:
  1. Verify gradient approximation accuracy by comparing with fine-tuned adapter baseline on simple token mapping task
  2. Test memory usage and computation time scaling with different l values and vocabulary sizes
  3. Validate semantic consistency by checking if prompt optimizations produce meaningful changes across different tokenizer pairs

## Open Questions the Paper Calls Out

### Open Question 1
How can we rigorously validate the accuracy of gradients computed through FUSE's approximate mapping between discrete embedding spaces? The authors acknowledge that traditional gradient validation methods don't apply to discrete spaces and call for comprehensive validation methods for mappings and gradients from one discrete embedding space to another.

### Open Question 2
What is the impact of limiting FUSE to words requiring 4 or fewer tokens on overall performance, and how does this threshold affect accuracy for different tokenizers and vocabularies? The authors limit FUSE to computing gradients for words requiring 4 or fewer tokens, citing that 97.6% of BookCorpus words require â‰¤4 tokens in Llama tokenizer, but acknowledge this as an open question.

### Open Question 3
How can we develop more memory and storage-efficient approaches for FUSE while maintaining or improving gradient approximation accuracy? The authors identify storage costs associated with longer sequences as an area for future work and suggest this as an open research direction.

## Limitations

- Limited mathematical rigor in tensor operations and pseudoinverse formulations
- Empirical validation restricted to image captioning tasks with GPT2-Medium model
- Implementation details underspecified, particularly tensor construction and AutoDAN optimizer parameters

## Confidence

**High Confidence**: The fundamental insight that whitespace-separated token groups provide a consistent semantic unit across different tokenizers; the general approach of using tensor operations to approximate adapter behavior without fine-tuning; the demonstrated effectiveness on image captioning tasks with measurable improvements over zero-shot baselines

**Medium Confidence**: The l=4 token limit being sufficient for most practical applications; the computational efficiency claims for the tensor operations; the method's ability to handle tokenizers with moderately different tokenization strategies

**Low Confidence**: Generalization to tasks beyond image captioning; performance with tokenizers that have fundamentally different tokenization philosophies; scalability to very large models or long sequence lengths

## Next Checks

1. **Gradient Approximation Accuracy**: Implement a direct comparison between FUSE's approximate gradient computation and a fully fine-tuned adapter baseline on a controlled token mapping task. Measure both the accuracy of the gradient approximation and the impact on downstream task performance.

2. **Tokenizer Robustness Testing**: Systematically evaluate FUSE's performance across tokenizers with different tokenization strategies (BPE, WordPiece, SentencePiece) and varying token distributions. Test with synthetic data where ground truth token mappings are known.

3. **Memory and Computation Scaling Analysis**: Conduct controlled experiments varying the l parameter (token limit per word) and model sizes to establish the relationship between memory usage, computation time, and approximation accuracy. Create a scaling chart showing performance tradeoffs.