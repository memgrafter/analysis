---
ver: rpa2
title: 'Scores as Actions: a framework of fine-tuning diffusion models by continuous-time
  reinforcement learning'
arxiv_id: '2409.08400'
source_url: https://arxiv.org/abs/2409.08400
tags:
- usion
- arxiv
- policy
- which
- continuous-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a continuous-time reinforcement learning framework
  for fine-tuning diffusion generative models. The key innovation is treating the
  score-matching functions as actions/controls in an exploratory continuous-time stochastic
  control problem.
---

# Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning

## Quick Facts
- arXiv ID: 2409.08400
- Source URL: https://arxiv.org/abs/2409.08400
- Reference count: 40
- Authors: Hanyang Zhao; Haoxian Chen; Ji Zhang; David D. Yao; Wenpin Tang
- Primary result: Continuous-time RL framework for fine-tuning diffusion models by treating score functions as actions

## Executive Summary
This paper introduces a novel framework for fine-tuning diffusion generative models using continuous-time reinforcement learning. The key innovation treats the score-matching functions as actions or controls in an exploratory continuous-time stochastic control problem. By formulating diffusion model fine-tuning as an SDE-driven continuous-time RL problem, the authors enable principled optimization that applies to both stochastic and deterministic diffusion samplers.

## Method Summary
The framework formulates diffusion model fine-tuning as a continuous-time stochastic control problem where the score function serves as the control policy. The authors derive continuous-time policy gradient theorems and establish an equivalence between optimizing the RL objective and minimizing expected L2 penalties between current and pre-trained score functions along the generation path. This approach naturally recovers DDPM and DDIM as discretizations of continuous-time SDEs/ODEs, providing a unified perspective on existing diffusion sampling methods while enabling principled fine-tuning through continuous-time policy optimization algorithms.

## Key Results
- Establishes continuous-time policy gradient theorems for diffusion model fine-tuning
- Shows equivalence between RL objective optimization and L2 penalty minimization
- Recovers DDPM and DDIM as discretizations of continuous-time SDEs/ODEs
- Enables principled fine-tuning via continuous-time policy optimization

## Why This Works (Mechanism)
The framework works by treating score functions as learnable controls in a continuous-time stochastic control problem. During the reverse diffusion process, the score function guides the generation trajectory, and by optimizing this control, the model can be fine-tuned to maximize rewards. The continuous-time formulation allows for gradient-based optimization that considers the entire generation path, rather than treating each timestep independently as in discrete approaches.

## Foundational Learning

1. **Score-based generative models** (Why needed: Core mechanism for understanding how diffusion models generate data; Quick check: Can explain how noise is progressively removed from data)
2. **Stochastic differential equations (SDEs)** (Why needed: Mathematical foundation for continuous-time diffusion processes; Quick check: Can describe basic SDE formulation and properties)
3. **Reinforcement learning in continuous time** (Why needed: Framework for optimizing the score function as a control; Quick check: Understands continuous-time policy gradient theorems)
4. **DDPM and DDIM samplers** (Why needed: Existing discrete-time diffusion methods that the framework generalizes; Quick check: Can explain the difference between these two sampling approaches)
5. **Policy optimization algorithms** (Why needed: Methods for updating the score function parameters; Quick check: Familiarity with basic policy gradient methods)
6. **L2 regularization in continuous time** (Why needed: Connection between RL objective and score function regularization; Quick check: Understands the equivalence relationship)

## Architecture Onboarding

**Component map:** Pre-trained score model -> Continuous-time SDE/ODE solver -> Reward function -> Policy gradient optimizer -> Updated score model

**Critical path:** The generation process follows the SDE trajectory controlled by the score function, which is optimized via policy gradients to maximize expected rewards.

**Design tradeoffs:** Continuous-time formulation offers theoretical elegance and path-wise optimization but requires careful discretization for practical implementation. The approach trades computational complexity for more principled fine-tuning.

**Failure signatures:** Poor fine-tuning performance may indicate issues with score function approximation, numerical instability in SDE solving, or suboptimal reward function design. Discretization errors can accumulate and degrade generation quality.

**First experiments:**
1. Verify the equivalence between continuous-time RL objective and L2 penalty minimization on a simple synthetic dataset
2. Implement continuous-time fine-tuning on a pre-trained DDPM and compare generation quality against the original model
3. Test the framework's ability to recover DDIM sampling as a discretization of the continuous-time formulation

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes exact availability and computability of pre-trained score functions, which may not hold with parameterized models
- Relies on technical regularity conditions for theoretical guarantees that are not thoroughly verified for practical neural network parameterizations
- Discretization error analysis is lacking when recovering DDPM/DDIM samplers from continuous-time formulation

## Confidence

**High:** The mathematical formulation of treating score functions as actions in continuous-time control is sound and well-established.

**Medium:** The equivalence between the RL objective and L2 penalty minimization holds under technical assumptions, but practical verification is limited.

**Medium:** The framework's ability to recover DDPM and DDIM as discretizations is theoretically plausible but requires empirical validation.

## Next Checks

1. Conduct ablation studies comparing the proposed continuous-time fine-tuning approach against discrete-time RL baselines across multiple datasets and diffusion architectures to verify claimed advantages.

2. Implement rigorous numerical experiments to quantify the impact of discretization error when recovering DDPM/DDIM samplers from the continuous-time formulation.

3. Test the framework's robustness to score function approximation errors by using different levels of score model capacity and measuring the degradation in fine-tuning performance.