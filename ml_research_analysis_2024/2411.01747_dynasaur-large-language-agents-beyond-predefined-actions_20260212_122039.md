---
ver: rpa2
title: 'DynaSaur: Large Language Agents Beyond Predefined Actions'
arxiv_id: '2411.01747'
source_url: https://arxiv.org/abs/2411.01747
tags:
- agent
- actions
- action
- task
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DynaSaur is a framework that enables LLM agents to dynamically
  create and compose actions using Python code, overcoming the limitations of fixed
  action sets. Instead of selecting from predefined actions, the agent generates executable
  functions on-the-fly, which are accumulated and reused over time.
---

# DynaSaur: Large Language Agents Beyond Predefined Actions

## Quick Facts
- **arXiv ID**: 2411.01747
- **Source URL**: https://arxiv.org/abs/2411.01747
- **Reference count**: 40
- **Primary result**: DynaSaur achieves 26.91% accuracy on GAIA Level 3 vs 8.16% for traditional pipelines

## Executive Summary
DynaSaur is a framework that enables LLM agents to dynamically create and compose actions using Python code, overcoming the limitations of fixed action sets. Instead of selecting from predefined actions, the agent generates executable functions on-the-fly, which are accumulated and reused over time. The framework also includes an action retrieval system to manage growing libraries of generated actions. Evaluated on benchmarks like GAIA, MATH, and GPQA, DynaSaur significantly outperforms prior methods that rely on fixed action sets.

## Method Summary
DynaSaur builds on the ReAct framework by allowing agents to generate Python functions as actions when predefined tools are insufficient. The agent executes these functions, observes results, and accumulates successful new actions for future reuse. An action retrieval system uses embeddings and cosine similarity to manage the growing library of generated actions without exceeding context limits. The framework is evaluated using GPT-4o and GPT-4o mini as LLM backbones across multiple benchmarks including GAIA, MATH, TabMWP, AIME, and GPQA.

## Key Results
- On GAIA Level 3: DynaSaur achieves 26.91% accuracy vs 8.16% for traditional pipelines
- On GPQA: Improves accuracy from 38.00% to 54.00% over fixed-action approaches
- Demonstrates significant performance gains when predefined tools are insufficient or fail due to edge cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DynaSaur enables agents to dynamically create and compose actions as Python functions, overcoming the limitations of fixed action sets.
- Mechanism: By representing each action as a Python function, the agent can generate new functions on-the-fly when the existing set is insufficient. These functions are accumulated over time, building a library of reusable actions for future tasks.
- Core assumption: General-purpose programming languages provide sufficient generality and composability to represent actions capable of solving a wide range of tasks.
- Evidence anchors: [abstract]: "the agent interacts with its environment by generating and executing programs written in a general-purpose programming language"; [section]: "To design such an LLM agent system, our first challenge is to select an appropriate representation for the action space... We argue that a general-purpose programming language meets these requirements well"

### Mechanism 2
- Claim: The action retrieval system enables efficient management of growing libraries of generated actions.
- Mechanism: When the agent needs to use a previously generated action, it can retrieve it by embedding the action's docstring and performing cosine similarity search against the query embedding. This allows the agent to access a vast library of actions without exceeding context limits.
- Core assumption: Docstrings effectively capture the purpose of generated actions and can be embedded to enable semantic search.
- Evidence anchors: [abstract]: "The framework also includes an action retrieval system to manage growing libraries of generated actions"; [section]: "To address this issue, we decompose the action set A into two subsets... Only actions in Au are included in the prompt by default... To provide the agent access to actions in Ag, we introduce an action retrieval function"

### Mechanism 3
- Claim: DynaSaur significantly improves flexibility and robustness by enabling agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases.
- Mechanism: When predefined tools fail (e.g., an Excel inspection tool that doesn't account for cell color formatting), the agent can implement alternative approaches using Python code. This enables recovery from tool failures that would otherwise cause agent failure.
- Core assumption: LLMs can generate valid Python code that implements alternative solutions when provided tools fail.
- Evidence anchors: [abstract]: "Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases"; [section]: "Our analysis suggests this is due to the domain mismatch: on GAIA, the agent continues to rely heavily on the human-designed tools; whereas on MATH, where those tools are less useful, the agent increasingly relies on its own generated tools"

## Foundational Learning

- Concept: Python code generation and execution
  - Why needed here: DynaSaur relies on the agent's ability to generate and execute Python functions as actions. Understanding Python syntax, libraries, and execution environments is fundamental to implementing this framework.
  - Quick check question: Can you write a Python function that reads an Excel file and returns the cell color information?

- Concept: Vector embeddings and similarity search
  - Why needed here: The action retrieval system uses embeddings to find relevant previously generated actions. Understanding how to create embeddings, perform similarity search, and manage embedding databases is crucial.
  - Quick check question: How would you implement a function that finds the most similar items in a database given a query embedding?

- Concept: Reinforcement learning and agent frameworks
  - Why needed here: DynaSaur builds on ReAct (Reasoning and Acting) framework, which interleaves reasoning and action sequences. Understanding RL concepts and agent architectures helps in extending and improving the framework.
  - Quick check question: What is the difference between model-free and model-based reinforcement learning, and how might each apply to LLM agents?

## Architecture Onboarding

- Component map: LLM backbone -> Python interpreter -> Action retrieval system -> Initial action set -> Action accumulation mechanism -> Task interface and evaluation system
- Critical path: 1. Receive task and initial action set 2. Generate thought-action pair using ReAct framework 3. Execute action (either existing function or new Python code) 4. Retrieve observation from environment 5. Accumulate successful new actions 6. Repeat until task completion or maximum iterations
- Design tradeoffs: Security vs. flexibility: Allowing arbitrary code execution provides maximum flexibility but introduces security risks; Performance vs. context limits: Including all actions in context provides faster access but quickly exceeds token limits; Generalization vs. specialization: More general functions are more reusable but may require more complex code generation
- Failure signatures: Code execution errors: LLM generates invalid Python code; Retrieval failures: Action retrieval system cannot find relevant actions; Accumulation issues: Generated actions are too specific to be reused; Security violations: Generated code attempts unauthorized operations
- First 3 experiments: 1. Implement basic DynaSaur with a small initial action set and test on simple tasks that require code generation 2. Add action retrieval system and test whether the agent can find and reuse previously generated actions 3. Test on more complex tasks that require both predefined tools and dynamically generated code, measuring action coverage and success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DynaSaur framework handle security risks associated with executing dynamically generated Python code?
- Basis in paper: Explicit - "As a proof of concept, our framework allows agents to generate and execute arbitrary Python code. While this approach is not advisable for real-world deployment due to potential security risks, we acknowledge that various safeguards can be implemented to mitigate these concerns."
- Why unresolved: The paper acknowledges security concerns but doesn't implement or test specific safeguards, leaving this as a future research direction.
- What evidence would resolve it: Implementation and evaluation of specific security measures (e.g., sandboxing, code verification) and their impact on agent performance.

### Open Question 2
- Question: What is the long-term effectiveness of action accumulation across multiple tasks and sessions?
- Basis in paper: Inferred - "Moreover, generated actions are accumulated over time for future reuse" and "For evaluation, we employ action accumulation during training but disable it during testing."
- Why unresolved: The paper only evaluates action accumulation within a single session and doesn't test how accumulated actions perform across extended periods or multiple independent sessions.
- What evidence would resolve it: Longitudinal studies tracking action effectiveness over time and across different task sessions.

### Open Question 3
- Question: How does the DynaSaur framework scale when the action library becomes very large?
- Basis in paper: Explicit - "Including all generated actions as part of the prompt runs the risk of exceeding the context limit as the agent generates more actions."
- Why unresolved: While the paper introduces an action retrieval system, it doesn't evaluate performance degradation as the action library grows or determine optimal library sizes.
- What evidence would resolve it: Experiments measuring performance and efficiency across varying library sizes and retrieval strategies.

### Open Question 4
- Question: How does the DynaSaur framework compare to approaches that use formal verification or world models for action validation?
- Basis in paper: Inferred - The paper mentions safety filters and formal verifiers as potential safeguards but doesn't compare against systems that already implement these approaches.
- Why unresolved: The paper doesn't benchmark against alternative frameworks that use formal methods or world models for action validation and safety.
- What evidence would resolve it: Direct comparison of DynaSaur with systems using formal verification or world models on identical benchmarks.

## Limitations
- Security concerns remain unaddressed: The framework allows arbitrary code execution without implementing concrete safeguards, creating potential attack vectors
- Context window constraints: The action retrieval system mitigates but doesn't eliminate the fundamental limitation of accumulating too many actions for the LLM context
- Overstated generalization claims: The paper claims to overcome "fundamental limitations" without sufficient empirical validation across diverse programming domains

## Confidence
- **High confidence**: The core mechanism of dynamically generating Python functions as actions is technically sound and the evaluation methodology is clear and reproducible.
- **Medium confidence**: The claimed improvements over fixed-action baselines are significant, but the evaluation relies on proprietary benchmarks (GAIA) with unclear licensing and reproducibility constraints.
- **Low confidence**: Claims about the framework's ability to handle "any task that can be solved with code" are overstated without empirical validation across diverse programming domains.

## Next Checks
1. **Security audit**: Implement a controlled test where the agent is prompted to generate potentially malicious code (file deletion, network access, system commands) and verify whether sandboxing prevents execution.
2. **Generalization stress test**: Design a benchmark with tasks requiring different programming paradigms (functional, object-oriented, recursive algorithms) to test whether the agent can consistently generate appropriate code across domains.
3. **Action accumulation analysis**: Track the cyclomatic complexity and modularity of generated functions over time to verify whether the accumulation mechanism actually produces reusable, composable actions rather than increasingly complex, monolithic functions.