---
ver: rpa2
title: Text Injection for Neural Contextual Biasing
arxiv_id: '2406.02921'
source_url: https://arxiv.org/abs/2406.02921
tags:
- text
- biasing
- contextual
- bias
- mwer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces contextual text injection (CTI) for neural
  contextual biasing in automatic speech recognition (ASR). CTI leverages large-scale
  unpaired text by converting it into speech-like representations and associating
  them with bias phrases via attention mechanisms, enabling joint optimization of
  the ASR model and its biasing component.
---

# Text Injection for Neural Contextual Biasing

## Quick Facts
- arXiv ID: 2406.02921
- Source URL: https://arxiv.org/abs/2406.02921
- Reference count: 0
- Key outcome: Contextual text injection (CTI) improves ASR performance on rare contextual phrases, achieving up to 43.3% relative WER reduction with 100B unpaired text sentences

## Executive Summary
This paper introduces contextual text injection (CTI) as a method for neural contextual biasing in automatic speech recognition (ASR). The approach leverages large-scale unpaired text by converting it into speech-like representations and associating them with bias phrases through attention mechanisms. This enables joint optimization of the ASR model and its biasing component. The method demonstrates significant improvements in recognizing rare contextual phrases, with experiments showing substantial WER reductions compared to strong neural biasing baselines.

## Method Summary
The proposed method converts unpaired text into speech-like representations and uses attention mechanisms to associate these representations with bias phrases. This allows the model to learn contextual relationships between general text and specific biasing targets. The approach is trained jointly with the ASR model, enabling both components to optimize together. A key innovation is the use of minimum WER (MWER) training in conjunction with text injection, which further improves performance by directly optimizing for recognition accuracy of the bias phrases.

## Key Results
- CTI with 100 billion unpaired text sentences achieves up to 43.3% relative WER reduction
- Contextual text-injected MWER training provides an additional 23.5% relative WER reduction
- Significant improvements observed for rare contextual phrases that are difficult for standard ASR systems

## Why This Works (Mechanism)
The method works by leveraging the statistical patterns in large-scale unpaired text to create richer contextual representations. By converting text to speech-like representations, the model can learn acoustic-semantic relationships that bridge the gap between general language patterns and specific bias phrases. The attention mechanism allows the model to selectively focus on relevant contextual information when processing speech, improving recognition of rare or domain-specific terms.

## Foundational Learning
- Speech recognition fundamentals: Why needed - to understand how ASR systems process audio and generate transcriptions. Quick check - can you explain the difference between acoustic and language models in ASR?
- Attention mechanisms: Why needed - to understand how the model associates contextual information with specific bias phrases. Quick check - can you describe how self-attention differs from cross-attention?
- Minimum Word Error Rate (MWER) training: Why needed - to understand the optimization objective that directly targets recognition accuracy. Quick check - can you explain how MWER differs from traditional maximum likelihood training?

## Architecture Onboarding
Component Map: ASR Encoder -> CTI Module -> Attention Layer -> Bias Integration -> Decoder
Critical Path: Speech input → ASR Encoder → CTI Module (with unpaired text) → Attention-based bias integration → Decoder output
Design Tradeoffs: Scale of unpaired text vs. computational cost, complexity of attention mechanisms vs. interpretability, joint optimization vs. training stability
Failure Signatures: Degraded performance on non-bias phrases, increased latency due to text processing, overfitting to specific bias contexts
First Experiments: 1) Ablation study removing CTI module, 2) Scaling analysis with varying amounts of unpaired text, 3) Comparison with traditional lexicon-based biasing approaches

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation focused primarily on a single ASR system and biasing scenario, limiting generalizability
- Performance appears sensitive to the scale of unpaired text data (100B sentences) without thorough characterization of scaling behavior
- Computational overhead of training with massive text corpora is not quantified
- Interaction between CTI and MWER training enhancements could be more nuanced than presented

## Confidence
- CTI achieves substantial WER reduction: High confidence (strong experimental evidence with clear baselines)
- CTI benefits scale with unpaired text size: Medium confidence (empirical but lacks systematic analysis of scaling behavior)
- CTI-MWER training provides additional gains: Medium confidence (results shown but methodological details limited)

## Next Checks
1. Test CTI on diverse ASR architectures (e.g., conformer, RNN-T) and acoustic conditions to assess robustness.
2. Characterize the trade-off between unpaired text scale and performance gain, including diminishing returns.
3. Measure and report computational overhead during training and inference to evaluate practical deployment costs.