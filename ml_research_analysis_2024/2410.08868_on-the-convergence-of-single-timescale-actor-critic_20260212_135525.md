---
ver: rpa2
title: On the Convergence of Single-Timescale Actor-Critic
arxiv_id: '2410.08868'
source_url: https://arxiv.org/abs/2410.08868
tags:
- policy
- gradient
- critic
- convergence
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the global convergence of the single-timescale
  actor-critic algorithm for discounted Markov Decision Processes with finite state
  spaces. The authors introduce a novel analytical framework to handle the complex
  coupled recursions inherent in the algorithm, focusing on a Lyapunov term composed
  of actor error and squared critic error.
---

# On the Convergence of Single-Timescale Actor-Critic

## Quick Facts
- **arXiv ID**: 2410.08868
- **Source URL**: https://arxiv.org/abs/2410.08868
- **Reference count**: 40
- **Primary result**: Global convergence of single-timescale actor-critic with O(ε⁻³) sample complexity for finite MDPs

## Executive Summary
This paper presents a comprehensive analysis of the global convergence properties of the single-timescale actor-critic algorithm for discounted Markov Decision Processes with finite state spaces. The authors develop a novel Lyapunov-based framework that tracks both actor and critic errors through coupled recursive updates. By introducing an ODE tracking methodology, they establish that the algorithm achieves ε-close globally optimal policies with improved sample complexity of O(ε⁻³), representing a significant improvement over the previous O(ε⁻⁴) bound. The key technical innovation lies in solving the Lyapunov recursion through careful tracking of the coupled dynamics.

## Method Summary
The authors analyze the single-timescale actor-critic algorithm by decomposing its convergence behavior into three coupled components: the actor recursion, the critic recursion, and a Lyapunov term that bounds their joint evolution. They introduce a novel ODE tracking approach that allows them to handle the complex interdependencies between these components. The analysis requires both actor and critic step sizes to decay as O(k⁻²/³), which differs from the conventional O(k⁻¹/²) rates. The Lyapunov function is carefully constructed to capture both the actor error and the squared critic error, enabling tighter bounds on the convergence rate. The methodology establishes that the algorithm converges to an ε-close globally optimal policy under ergodicity assumptions.

## Key Results
- Establishes O(ε⁻³) sample complexity for achieving ε-close globally optimal policy, improving upon previous O(ε⁻⁴) bounds
- Demonstrates that both actor and critic step sizes must decay as O(k⁻²/³) for optimal convergence
- Introduces novel ODE tracking methodology to handle coupled actor-critic recursions
- Provides general framework applicable to minimax optimization, bi-level optimization, and multi-agent RL settings

## Why This Works (Mechanism)
The convergence improvement stems from the ODE tracking methodology that precisely monitors the coupled dynamics between actor and critic updates. By constructing a Lyapunov function that captures both the actor error and the squared critic error, the authors can establish tighter bounds on the joint evolution. The key insight is that tracking the error between the algorithm's stochastic updates and their corresponding ODEs allows for more precise control of the approximation errors that accumulate during learning. This tracking approach enables the authors to avoid the looser bounds that arise from traditional gradient domination arguments.

## Foundational Learning
- **Lyapunov stability analysis**: Used to establish convergence by showing a potential function decreases over time; quick check involves verifying that the Lyapunov function satisfies the required descent properties.
- **ODE approximation for stochastic algorithms**: Connects discrete-time stochastic updates to continuous-time dynamical systems; quick check involves confirming the step sizes satisfy conditions for valid ODE approximation.
- **Markov chain mixing time**: Determines how quickly the state distribution converges to stationary distribution; quick check involves verifying ergodicity conditions and bounding mixing times.
- **Two-timescale stochastic approximation**: Traditional framework where actor and critic operate on different timescales; quick check involves comparing step size ratios to established conditions.
- **Gradient domination**: Property that relates gradient norm to suboptimality gap; quick check involves verifying when this property holds for the objective landscape.
- **Coupled recursive analysis**: Technique for handling interdependent updates in actor-critic; quick check involves tracing the error propagation through the coupled recursions.

## Architecture Onboarding
**Component map**: Actor update -> Critic update -> Lyapunov tracking -> Policy improvement
**Critical path**: State visitation → Value estimation → Policy gradient computation → Parameter update → Lyapunov evaluation
**Design tradeoffs**: Single-timescale vs two-timescale implementation (computational efficiency vs stability), step size scheduling (faster learning vs convergence guarantees), Lyapunov function design (tightness of bounds vs analytical tractability)
**Failure signatures**: Divergence when step sizes decay too slowly, premature convergence to suboptimal policies when step sizes decay too quickly, sensitivity to initial conditions in non-ergodic MDPs
**First experiments**: 1) Verify convergence rate improvement by comparing O(k⁻²/³) vs O(k⁻¹/²) step sizes on simple MDPs, 2) Test robustness to different mixing times by varying MDP structure, 3) Validate sample complexity by measuring iterations to reach ε-optimal policy across problem scales

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general applicability of their methodology to related settings such as minimax optimization, bi-level optimization, and multi-agent reinforcement learning. The authors suggest their ODE tracking framework may extend to these domains but do not elaborate on specific challenges or research directions.

## Limitations
- Analysis restricted to finite state-action MDPs with fixed policy parameterization
- Requires ergodicity assumptions that may not hold in all practical applications
- Lyapunov bounds depend on mixing time and gradient norm assumptions
- Empirical validation limited to theoretical bounds rather than comprehensive experiments

## Confidence
- **High**: The O(ε⁻³) sample complexity improvement over prior O(ε⁻⁴) results, given the established Lyapunov and ODE tracking methodology
- **Medium**: The step size requirements (O(k⁻²/³)) and their implications for practical algorithm tuning
- **Medium**: The claimed generality of the methodology to settings like minimax and multi-agent RL, pending empirical demonstration

## Next Checks
1. Conduct experiments comparing the single-timescale actor-critic algorithm under O(k⁻²/³) step sizes to standard O(k⁻¹/²) step sizes across diverse MDPs to verify empirical gains
2. Test the algorithm's robustness to non-ergodic MDPs or those with long mixing times to assess practical limitations
3. Extend the analysis to function approximation settings (e.g., linear or neural network policies) to evaluate the framework's scalability