---
ver: rpa2
title: 'E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline
  Methods'
arxiv_id: '2405.02984'
source_url: https://arxiv.org/abs/2405.02984
tags:
- language
- sign
- dataset
- translation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces the Educational Turkish Sign Language (E-TSL)\
  \ dataset, a continuous sign language dataset for Turkish language lessons for 5th,\
  \ 6th, and 8th grades, totaling 1,410 videos with 24 hours of content. The dataset\
  \ addresses challenges posed by Turkish\u2019s agglutinative nature, with 64% singleton\
  \ words and 85% rare words."
---

# E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods

## Quick Facts
- arXiv ID: 2405.02984
- Source URL: https://arxiv.org/abs/2405.02984
- Reference count: 34
- Dataset: 1,410 videos, 24 hours, 5th-8th grade Turkish lessons

## Executive Summary
This study introduces the Educational Turkish Sign Language (E-TSL) dataset, a continuous sign language dataset for Turkish language lessons for 5th, 6th, and 8th grades, totaling 1,410 videos with 24 hours of content. The dataset addresses challenges posed by Turkish's agglutinative nature, with 64% singleton words and 85% rare words. Two baseline transformer-based models, P2T-T and GNN-T, were developed, with GNN-T achieving BLEU-1 of 19.13% and BLEU-4 of 3.28%, and P2T-T achieving a higher ROUGE-L score of 22.09%. The dataset and models serve as a foundation for future sign language translation research.

## Method Summary
The study presents two transformer-based models for sign language translation: P2T-T (Pose to Text Transformer) and GNN-T (Graph Neural Network Transformer). Both models use MediaPipe for pose extraction, with P2T-T applying landmark normalization and GNN-T using graph pooling and convolution. The transformer architecture consists of 6 encoder and 6 decoder layers with 1024 hidden units, 8 attention heads, and 2048 feed forward dimension. Models are trained using Adam optimizer with learning rate 1e-4, dropout 0.1, and batch sizes of 4 (P2T-T) or 16 (GNN-T).

## Key Results
- E-TSL dataset: 1,410 videos, 24 hours total, from 5th, 6th, and 8th grade Turkish language lessons
- GNN-T achieved BLEU-1 of 19.13% and BLEU-4 of 3.28%
- P2T-T achieved higher ROUGE-L score of 22.09% despite lower BLEU scores
- Dataset addresses Turkish agglutinative language challenges with 64% singleton words and 85% rare words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph Neural Networks (GNN) capture spatial relationships in sign language data better than plain transformers.
- Mechanism: GNN-T combines body pose landmarks into a graph structure, then uses graph convolution and pooling to aggregate local joint information before feeding into the transformer.
- Core assumption: Hand, face, and body movements in sign language are highly interdependent; modeling these dependencies as a graph improves recognition.
- Evidence anchors:
  - [abstract] "The GNN-T model achieved 19.13% BLEU-1 score and 3.28% BLEU-4 score, presenting a significant challenge compared to existing benchmarks."
  - [section] "Our GNN-T model uses techniques like graph pooling and graph convolution... it achieved a higher BLEU score than P2T-T."
  - [corpus] Weak: no direct citation to graph-based sign language methods, but Kan et al. [14] referenced in text supports hierarchical graph use.
- Break condition: If graph connections do not meaningfully reflect coarticulation patterns in TSL, performance degrades.

### Mechanism 2
- Claim: Landmark normalization removes signer-specific bias, enabling model generalization across different signers.
- Mechanism: For P2T-T, coordinates are centered at the midpoint of shoulders and scaled by shoulder distance per frame, yielding signer-centered and scale-agnostic features.
- Core assumption: Pose estimator outputs vary systematically with signer size, distance, and position; normalizing these factors removes irrelevant variance.
- Evidence anchors:
  - [section] "We normalized the landmark coordinates we got from MediaPipe, by taking the average of the two shoulder coordinates... and the distance between the two shoulder joints for the scale normalization."
  - [abstract] "The P2T-T model, while demonstrating slightly lower performance in BLEU scores, achieved a higher ROUGE-L score of 22.09%."
  - [corpus] Weak: normalization is common in pose-based vision, but no direct TSL dataset citation confirming this particular normalization improves performance.
- Break condition: If normalization removes task-relevant spatial scaling cues, translation quality drops.

### Mechanism 3
- Claim: Transformers with attention mechanisms preserve long-range dependencies in sign language sequences, essential for agglutinative languages.
- Mechanism: Stacked transformer encoder-decoder layers process normalized or graph-pooled pose sequences, using multi-head self-attention to capture context across frames.
- Core assumption: Sign language meaning depends on temporal context; attention can model this without recurrence.
- Evidence anchors:
  - [abstract] "The P2T-T model... achieved a higher ROUGE-L score of 22.09%."
  - [section] "We used 1024 hidden units and 8 heads in each layer of our transformers. We used 6 encoder and 6 decoder layers."
  - [corpus] Strong: Vaswani et al. [27] cited as foundational; Camgöz et al. [5] cited for transformer-based SLT.
- Break condition: If sequence length exceeds transformer's context window, model loses long-term dependencies.

## Foundational Learning

- Concept: Agglutinative morphology in Turkish
  - Why needed here: 64% of vocabulary are singletons; rare words (85% <5 occurrences) demand morphological analysis to recover meaning.
  - Quick check question: Why does an agglutinative language make sign language translation harder than an isolating language?

- Concept: Graph convolution vs. standard convolution
  - Why needed here: Sign language uses non-grid body landmarks; GNN can aggregate information from variable neighborhood structures.
  - Quick check question: What is the key difference between graph convolution and image convolution in terms of data structure?

- Concept: Attention-based sequence modeling
  - Why needed here: Need to capture temporal dependencies in continuous sign sequences without recurrence.
  - Quick check question: How does self-attention differ from convolutional sliding windows in sequence modeling?

## Architecture Onboarding

- Component map:
  - Pose extraction (MediaPipe) → Graph construction → Graph pooling → Graph convolution → Transformer encoder → Transformer decoder → Text output
  - OR Pose extraction → Normalization → Transformer encoder → Transformer decoder → Text output

- Critical path:
  1. MediaPipe landmark extraction
  2. (GNN-T) Graph pooling & convolution
  3. Transformer encoding/decoding
  4. Loss calculation (BLEU/ROUGE)

- Design tradeoffs:
  - GNN-T: More accurate spatial modeling vs. higher computational cost and complexity.
  - P2T-T: Simpler pipeline, faster training vs. potentially less spatial reasoning.

- Failure signatures:
  - BLEU/ROUGE scores plateau or drop: likely overfitting, insufficient data, or normalization removing useful variance.
  - Training instability: learning rate too high, batch size too small, or normalization causing NaNs.
  - Poor generalization across signers: lack of data augmentation or normalization errors.

- First 3 experiments:
  1. Baseline: Train P2T-T on E-TSL without normalization; compare BLEU/ROUGE to paper results.
  2. Add normalization to P2T-T; verify performance improvement.
  3. Replace P2T-T with GNN-T; compare both models on E-TSL and PHOENIX14T.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using alternative pose extraction tools beyond MediaPipe on model performance?
- Basis in paper: [explicit] The authors mention they plan to explore alternative pose extraction tools beyond MediaPipe in future work.
- Why unresolved: The study only used MediaPipe for pose extraction, so the impact of other tools is unknown.
- What evidence would resolve it: Experimental results comparing model performance using different pose extraction tools on the E-TSL dataset.

### Open Question 2
- Question: How does model performance vary when the dataset is segmented into sentences instead of 1-minute episodes?
- Basis in paper: [explicit] The authors plan to refine video segmentation into sentences in future work to observe performance changes.
- Why unresolved: The current dataset uses 1-minute episodes, so sentence-level performance is untested.
- What evidence would resolve it: Training and evaluating the same models on a sentence-segmented version of the E-TSL dataset.

### Open Question 3
- Question: What is the effect of incorporating state-of-the-art models on overall performance in sign language translation?
- Basis in paper: [explicit] The authors intend to assess the impact of incorporating state-of-the-art models on performance.
- Why unresolved: The study used baseline models, so the contribution of advanced models is unknown.
- What evidence would resolve it: Comparative results showing performance improvements when integrating state-of-the-art models into the current framework.

## Limitations
- GNN-T performance claims lack comparison to modern SLT baselines, limiting generalizability
- Educational focus and limited signer diversity (11 signers) may constrain real-world applicability
- No ablation studies on normalization impact or graph architecture choices weaken mechanistic claims

## Confidence
- High: Dataset creation and basic model architectures (pose extraction → transformer)
- Medium: BLEU/ROUGE score comparisons between models and datasets
- Low: Claims about GNN-T's superiority in capturing spatial relationships without ablation evidence

## Next Checks
1. Replicate P2T-T on PHOENIX14T with and without normalization to isolate normalization's impact on BLEU/ROUGE scores
2. Perform ablation on GNN-T: compare graph-convolutioned vs. non-graph versions on E-TSL to validate spatial modeling benefits
3. Test model generalization by training on E-TSL and evaluating on a non-educational sign language dataset (e.g., PHOENIX14T)