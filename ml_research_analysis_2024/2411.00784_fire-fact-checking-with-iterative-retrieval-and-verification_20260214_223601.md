---
ver: rpa2
title: 'FIRE: Fact-checking with Iterative Retrieval and Verification'
arxiv_id: '2411.00784'
source_url: https://arxiv.org/abs/2411.00784
tags:
- search
- knowledge
- verification
- query
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIRE addresses the challenge of efficient fact-checking for long-form
  text by integrating iterative retrieval and verification. Instead of using a fixed
  number of searches, FIRE uses an agent-based framework that decides at each step
  whether to issue a final judgment or generate another search query based on confidence.
---

# FIRE: Fact-checking with Iterative Retrieval and Verification

## Quick Facts
- arXiv ID: 2411.00784
- Source URL: https://arxiv.org/abs/2411.00784
- Reference count: 40
- One-line primary result: Reduces fact-checking costs by 7.6x for LLM computations and 16.5x for searches while maintaining performance

## Executive Summary
FIRE addresses the challenge of efficient fact-checking for long-form text by integrating iterative retrieval and verification. Instead of using a fixed number of searches, FIRE uses an agent-based framework that decides at each step whether to issue a final judgment or generate another search query based on confidence. This approach significantly reduces LLM computational costs by 7.6x and search costs by 16.5x while maintaining fact-checking performance across multiple datasets. The method particularly benefits from step-by-step reasoning, especially with smaller models like GPT-4o-mini, which can make confident judgments without external searches in many cases.

## Method Summary
FIRE is a fact-checking framework that processes long-form text by breaking it into atomic claims and verifying each using an iterative retrieval and verification approach. The core innovation is a unified mechanism that decides whether to provide a final answer or generate a subsequent search query based on confidence in the current judgment. The framework integrates web search via SerpAPI for evidence retrieval and uses various prompts for verification, including default, no-reason, at-least-one, at-least-two, and inclusive settings. The method is evaluated across four datasets (FacTool-QA, FELM-WK, Factcheck-Bench, BingCheck) using different models including GPT-4o, GPT-4o-mini, and other language models.

## Key Results
- Reduces LLM computational costs by 7.6x compared to traditional fixed-iteration approaches
- Reduces search costs by 16.5x while maintaining comparable fact-checking performance
- GPT-4o-mini demonstrates high confidence in making verifications when allowed to articulate reasoning, resulting in most judgments being made without searches
- Step-by-step reasoning significantly enhances model confidence, particularly benefiting smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIRE reduces computational costs by 7.6x and search costs by 16.5x while maintaining fact-checking performance.
- Mechanism: FIRE uses a confidence-based decision process to determine whether to issue a final judgment or generate another search query, avoiding unnecessary searches for claims that can be verified with internal knowledge.
- Core assumption: Language models can accurately assess their confidence in verifying claims based on their internal knowledge.
- Evidence anchors:
  - [abstract] "FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment"
  - [section 3.1] "This decision is informed by both an external evidence set E, derived from search results, and the internal knowledge k of the language model, acquired during pre-training"
  - [corpus] Weak evidence - corpus shows related work on multi-agent frameworks but lacks direct evidence of confidence-based cost reduction mechanisms
- Break condition: The iterative process terminates when the model reaches its confidence threshold or the maximum number of search steps is reached.

### Mechanism 2
- Claim: Step-by-step reasoning enhances model confidence, especially for smaller models like GPT-4o-mini.
- Mechanism: Allowing models to articulate their reasoning process improves their confidence in making judgments without external searches.
- Core assumption: The presence of Chain-of-Thought (CoT) reasoning correlates with increased confidence in the model's answers.
- Evidence anchors:
  - [section 5.2] "GPT-4o-mini demonstrates a high level of confidence in making verifications when it is allowed to articulate its reasoning process, resulting in the majority of judgments being made without any searches"
  - [section 5.2] "This observation aligns with previous findings that the presence of CoT reasoning correlates with increased confidence in the model's answers"
  - [corpus] Weak evidence - corpus contains related work on multi-agent frameworks but lacks direct evidence of reasoning's impact on confidence
- Break condition: The model either reaches sufficient confidence to make a judgment or exhausts its reasoning steps.

### Mechanism 3
- Claim: FIRE's unified approach of integrating retrieval and verification outperforms frameworks that treat them as separate processes.
- Mechanism: By combining evidence retrieval and claim verification into a single iterative framework, FIRE can make more informed decisions about when to search and when to verify.
- Core assumption: Treating evidence retrieval and claim verification as separate processes leads to inefficient resource utilization.
- Evidence anchors:
  - [abstract] "FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment"
  - [section 3.1] "f (c, E, k) = (a, if confident; q, if not confident)"
  - [section 5.2] "All frameworks exhibit similar performance, with a small gap of approximately 0.2 to 0.5"
- Break condition: The model either makes a final judgment or reaches the maximum number of search iterations.

## Foundational Learning

- Concept: Confidence estimation in language models
  - Why needed here: FIRE relies on the model's ability to estimate its confidence in verifying claims without external evidence
  - Quick check question: How does FIRE determine whether to generate a search query or make a final judgment?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The framework's performance depends on the model's ability to articulate reasoning, which enhances confidence
  - Quick check question: What happens to GPT-4o-mini's search behavior when it's not allowed to articulate its reasoning?

- Concept: Cost-benefit analysis in fact-checking
  - Why needed here: Understanding the tradeoff between computational costs and fact-checking performance is crucial for evaluating FIRE's effectiveness
  - Quick check question: By what factor does FIRE reduce LLM computational costs compared to other frameworks?

## Architecture Onboarding

- Component map:
  - Claim → Confidence Assessment → (Search if needed) → Verification → Final Judgment

- Critical path: Claim → Confidence Assessment → (Search if needed) → Verification → Final Judgment

- Design tradeoffs:
  - FIRE trades off some precision for significant cost reduction
  - The framework sacrifices some control over the search process for efficiency
  - Confidence-based decisions may lead to occasional errors on borderline cases

- Failure signatures:
  - High search costs despite confident claims (indicates confidence estimation issues)
  - Repeated similar search queries (indicates query generation problems)
  - Inconsistent judgments on similar claims (indicates reasoning instability)

- First 3 experiments:
  1. Compare FIRE's performance with and without reasoning articulation on a small dataset
  2. Test different confidence thresholds to find optimal cost-performance balance
  3. Evaluate FIRE's performance on claims requiring multi-hop reasoning vs simple factual claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of FIRE change if the Final Answer or Next Search Query step were implemented as a separate process with a standalone confidence estimation step rather than a unified mechanism?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, noting that implementing confidence estimation as a separate process would offer greater flexibility and interpretability.
- Why unresolved: The authors chose to implement a compact unified mechanism to maintain efficiency, leaving the separate process approach for future work.
- What evidence would resolve it: Comparative experiments measuring performance, cost, and interpretability between the unified mechanism and a separate confidence estimation process.

### Open Question 2
- Question: What would be the impact of incorporating memory banks to store verification results in FIRE, allowing the system to leverage previous results instead of repeating the entire process?
- Basis in paper: [explicit] The authors identify this as a promising future direction, suggesting that storing verification results could improve efficiency by avoiding redundant computations.
- Why unresolved: This feature has not been implemented or tested in the current framework.
- What evidence would resolve it: Experiments comparing the computational costs and performance of FIRE with and without memory banks across multiple datasets.

### Open Question 3
- Question: How would FIRE's performance be affected if it supported additional modalities such as code and images, rather than just text-based claims?
- Basis in paper: [explicit] The authors mention expanding to additional modalities as a future direction, noting that current implementation focuses on text-based fact-checking.
- Why unresolved: The current framework is designed specifically for text-based claims and has not been tested with other modalities.
- What evidence would resolve it: Comparative studies of FIRE's fact-checking accuracy and efficiency when handling multimodal inputs versus its current text-only performance.

## Limitations

- The confidence-based decision mechanism for determining when to search versus verify is not fully detailed, making it difficult to assess the robustness of this core component
- The evaluation framework doesn't clearly establish baselines for comparison of the cost reduction claims
- The paper lacks analysis of how FIRE performs on more complex multi-hop reasoning tasks or adversarial claims designed to mislead retrieval systems

## Confidence

- High: The general approach of using confidence estimation to guide search decisions is sound and the empirical results show meaningful cost reductions while maintaining performance
- Medium: The claim that step-by-step reasoning particularly benefits smaller models like GPT-4o-mini is supported by the results but could benefit from more extensive ablation studies
- Low: The paper's assertion that treating retrieval and verification as separate processes leads to inefficiency is plausible but not rigorously proven through direct comparison with unified approaches

## Next Checks

1. Conduct ablation studies on different confidence threshold values to understand their impact on the precision-cost tradeoff
2. Test FIRE's performance on adversarial fact-checking datasets where claims are specifically designed to mislead retrieval systems
3. Compare FIRE's multi-hop reasoning capabilities against specialized frameworks designed for complex claim verification