---
ver: rpa2
title: 'Steps are all you need: Rethinking STEM Education with Prompt Engineering'
arxiv_id: '2412.05023'
source_url: https://arxiv.org/abs/2412.05023
tags:
- prompting
- analogical
- arxiv
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates how few-shot and Chain-of-Thought prompting
  techniques can improve high school STEM education with LLMs. It introduces StemStep,
  a dataset of 928 physics and mathematics questions with step-by-step solutions,
  and tests Mistral 7B and Mixtral 8x7B on various prompting methods.
---

# Steps are all you need: Rethinking STEM Education with Prompt Engineering

## Quick Facts
- arXiv ID: 2412.05023
- Source URL: https://arxiv.org/abs/2412.05023
- Authors: Krishnasai Addala; Kabir Dev Paul Baghel; Navya Gupta; Rishitej Reddy Vyalla; Chhavi Kirtani; Avinash Anand; Rajiv Ratn Shah
- Reference count: 7
- Primary result: Few-shot Chain-of-Thought prompting with K=3 examples improves LLM accuracy on STEM tasks from 31.5% to 53%, with Mixtral 8x7B reaching 64.5% accuracy.

## Executive Summary
This work evaluates how few-shot and Chain-of-Thought prompting techniques can improve high school STEM education with LLMs. It introduces StemStep, a dataset of 928 physics and mathematics questions with step-by-step solutions, and tests Mistral 7B and Mixtral 8x7B on various prompting methods. Few-shot CoT prompts yield the best performance, peaking around K=3 for Mistral 7B, with accuracy rising from 31.5% to 53% over baseline. Mixtral 8x7B achieves up to 64.5% accuracy and performs better on longer reasoning chains. Analogical prompting is less effective on these open-source models due to their limited domain-specific training. A proposed Analogical CoT method improves performance for both models, especially for more complex questions, though only Mixtral 8x7B achieves accuracy above chance.

## Method Summary
The study introduces StemStep, a dataset of 928 physics and mathematics questions with step-by-step solutions. It evaluates few-shot Chain-of-Thought prompting (K=1 to 8 examples), Analogical prompting, and a proposed Analogical CoT method on Mistral 7B and Mixtral 8x7B models. Mistral 7B is quantized to 4-bit using LoRA. Model outputs are evaluated using accuracy metrics (BERTScore, METEOR, ROUGE-N, ROUGE-L) and human evaluation of response alignment with ground truth.

## Key Results
- Few-shot Chain-of-Thought prompting with K=3 examples improves Mistral 7B accuracy from 31.5% to 53% over baseline
- Mixtral 8x7B achieves up to 64.5% accuracy and performs better on longer reasoning chains than Mistral 7B
- Analogical CoT prompting improves performance for both models, especially for complex questions
- Analogical prompting alone is less effective on open-source models due to limited domain-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot Chain-of-Thought (CoT) prompting improves LLM performance on STEM tasks by explicitly guiding the model through step-by-step reasoning.
- Mechanism: Providing examples that show intermediate reasoning steps helps the model structure its output and reduce hallucinations, especially for math and physics problems.
- Core assumption: LLMs can leverage structured examples to generalize to new problems when intermediate steps are shown.
- Evidence anchors:
  - [abstract] "Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination."
  - [section] "Few Shot Prompting... refers to the technique of training AI models with a very limited amount of data, typically just a few examples or 'shots'... Recent advances... have shown remarkable abilities to generalize from minimal data."
  - [corpus] "Found 25 related papers... Top related titles: A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education."
- Break condition: Performance plateaus or declines when K becomes too large for the model's context window (observed at K=8 in the paper).

### Mechanism 2
- Claim: The Mixture of Experts (MoE) architecture in Mixtral 8x7B allows better handling of complex STEM problems compared to dense models like Mistral 7B.
- Mechanism: MoE routes different parts of the input to specialized expert models, enabling more efficient and accurate reasoning for domain-specific tasks.
- Core assumption: Expert specialization within the MoE improves performance on complex reasoning tasks compared to a single dense model.
- Evidence anchors:
  - [abstract] "By utilizing a Mixture of Experts (MoE) Model... we are able to show improved model performance when compared to the baseline on standard LLMs."
  - [section] "The Mixture of Experts (MoE) model is a machine learning paradigm where multiple specialist models (experts) are trained on different parts of a problem and a gating mechanism decides which expert to use for a given input."
  - [corpus] Weak correlation - corpus focuses on prompt engineering rather than MoE architecture specifically.
- Break condition: If the gating mechanism fails to route inputs correctly, or if experts are not well-trained on STEM-specific data.

### Mechanism 3
- Claim: Analogical CoT prompting improves open-source model performance by guiding them to generate relevant examples, compensating for limited domain-specific training data.
- Mechanism: Combining few-shot CoT with analogical prompting helps models generate self-supplied examples that guide reasoning, especially when models cannot recall relevant examples on their own.
- Core assumption: Open-source models struggle with analogical prompting due to lack of specialist training data, but can benefit from guided example generation.
- Evidence anchors:
  - [abstract] "Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data."
  - [section] "In order to allow open-source models to leverage the strengths of both Few Shot CoT and Analogical prompting, we propose combining the two to guide the model in the kind of self-supplied examples it should generate."
  - [corpus] Moderate correlation - related work on analogical reasoning with LLMs found in corpus.
- Break condition: If the model cannot generate relevant examples even with guidance, or if the generated examples are not useful for solving the target problem.

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: STEM problems require multi-step reasoning; CoT helps break down complex problems into manageable steps.
  - Quick check question: Why does providing intermediate steps in examples help LLMs solve new problems?

- Concept: Few-shot learning
  - Why needed here: Limited training data available for STEM-specific tasks; few-shot prompting allows generalization from minimal examples.
  - Quick check question: How does few-shot prompting differ from zero-shot and many-shot prompting in terms of data requirements?

- Concept: Mixture of Experts architecture
  - Why needed here: MoE enables specialized handling of different problem types, improving performance on complex STEM tasks.
  - Quick check question: What is the role of the gating mechanism in a MoE model?

## Architecture Onboarding

- Component map: Dataset (StemStep) -> Preprocessing -> Prompt Engineering (Few-shot CoT, Analogical CoT) -> Model (Mistral 7B, Mixtral 8x7B) -> Evaluation (BERTScore, METEOR, ROUGE, accuracy)
- Critical path: Data preprocessing -> Prompt construction -> Model inference -> Evaluation
- Design tradeoffs:
  - Few-shot vs. many-shot: Balance between sufficient guidance and context window limitations
  - MoE vs. dense models: Efficiency vs. potential overfitting challenges
  - Analogical vs. standard CoT: Potential for better reasoning vs. increased complexity
- Failure signatures:
  - Performance plateaus or declines with too many examples (context window issues)
  - Models fail to generate relevant examples in analogical prompting
  - Quantization degradation in MoE models
- First 3 experiments:
  1. Test few-shot CoT performance on Mistral 7B with K=1, 3, 5 to find optimal K
  2. Compare Mistral 7B and Mixtral 8x7B on same prompt set to measure MoE benefit
  3. Test analogical CoT prompting on both models to evaluate effectiveness of guided example generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Mixture of Experts models on STEM education tasks compare to other specialized fine-tuned models?
- Basis in paper: [explicit] The paper notes that Mixtral 8x7B outperforms Mistral 7B, possibly due to its MoE architecture.
- Why unresolved: The study does not compare MoE models against other specialized fine-tuned models, leaving open the question of whether MoE is the optimal architecture for STEM tasks.
- What evidence would resolve it: Direct comparisons between MoE models and other specialized models on the same STEM datasets, evaluating metrics such as accuracy, reasoning quality, and efficiency.

### Open Question 2
- Question: What is the impact of dataset size and diversity on the performance of LLMs in STEM education?
- Basis in paper: [inferred] The paper mentions that StemStep contains only 928 questions, suggesting potential limitations in dataset comprehensiveness.
- Why unresolved: The study does not explore how increasing the size and diversity of the dataset affects model performance, particularly in terms of generalization and robustness.
- What evidence would resolve it: Experiments varying the size and diversity of the training dataset, measuring performance across different subsets and assessing generalization to unseen problems.

### Open Question 3
- Question: How do different prompting techniques affect the reasoning step length and quality in LLMs for STEM tasks?
- Basis in paper: [explicit] The paper discusses the impact of reasoning step length on model performance, noting that longer steps benefit more complex questions.
- Why unresolved: While the paper observes trends in step length, it does not systematically investigate how different prompting techniques influence the structure and quality of reasoning steps.
- What evidence would resolve it: Comparative analysis of various prompting techniques (e.g., CoT, Analogical CoT) on step length and quality, using metrics such as coherence, relevance, and logical flow.

## Limitations
- The StemStep dataset's construction methodology and exact question types are not fully detailed, limiting reproducibility and external validation of results.
- The paper focuses primarily on two models (Mistral 7B and Mixtral 8x7B) and does not explore performance across a broader range of open-source or proprietary LLMs.
- Analogical prompting effectiveness is tested only through the proposed Analogical CoT method, without isolating the standalone impact of pure analogical prompting on these models.

## Confidence

- **High Confidence**: The effectiveness of few-shot Chain-of-Thought prompting for improving STEM problem-solving accuracy in LLMs, supported by consistent performance gains across both models tested.
- **Medium Confidence**: The advantage of Mixtral 8x7B's MoE architecture over Mistral 7B for complex STEM reasoning, though the paper does not fully isolate MoE-specific contributions from model size differences.
- **Medium Confidence**: The Analogical CoT method's ability to improve performance for open-source models, though the standalone effect of analogical prompting is not separately validated.

## Next Checks

1. **Dataset Generalization Test**: Apply the same few-shot CoT and Analogical CoT methods to an independently constructed STEM dataset (e.g., SciQ or JEEBench) to assess if performance gains generalize beyond the StemStep dataset.

2. **MoE Architecture Isolation**: Test a Mixtral-like MoE model with the same parameter count as Mistral 7B (but different architecture) to isolate the architectural benefit from scale effects.

3. **Standalone Analogical Prompting**: Implement and evaluate pure analogical prompting (without few-shot CoT guidance) on both models to determine if the benefit comes from analogical reasoning itself or the combination with few-shot CoT.