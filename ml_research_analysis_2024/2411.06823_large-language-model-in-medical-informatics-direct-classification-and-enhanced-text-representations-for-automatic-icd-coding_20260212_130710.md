---
ver: rpa2
title: 'Large Language Model in Medical Informatics: Direct Classification and Enhanced
  Text Representations for Automatic ICD Coding'
arxiv_id: '2411.06823'
source_url: https://arxiv.org/abs/2411.06823
tags:
- medical
- text
- classification
- coding
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automatic ICD coding from
  medical discharge summaries using large language models (LLMs). The authors adapt
  LLAMA-2 (7B) in two ways: as a direct classifier and as a text representation generator
  for a MultiResCNN framework.'
---

# Large Language Model in Medical Informatics: Direct Classification and Enhanced Text Representations for Automatic ICD Coding

## Quick Facts
- arXiv ID: 2411.06823
- Source URL: https://arxiv.org/abs/2411.06823
- Reference count: 25
- Top 50 ICD codes: F1 score of 0.6258 and AUC of 0.9138 (micro)

## Executive Summary
This paper addresses the challenge of automatic ICD coding from medical discharge summaries using large language models (LLMs). The authors adapt LLAMA-2 (7B) in two ways: as a direct classifier and as a text representation generator for a MultiResCNN framework. The LLM-based representations are processed by MultiResCNN to improve ICD code classification. Experiments on MIMIC-III show that the LLM-enhanced approach outperforms several baselines on the top 50 ICD codes, achieving an F1 score of 0.6258 and AUC of 0.9138 (micro), while for the full ICD code set, performance is more modest and comparable to existing methods. The results highlight the potential of LLMs for enhancing medical text understanding, though challenges remain with label granularity and data sparsity.

## Method Summary
The paper adapts LLAMA-2 (7B) for ICD coding in two ways: (1) as a direct classifier, and (2) as a text representation generator for a MultiResCNN framework. The authors preprocess MIMIC-III discharge summaries and generate high-dimensional embeddings using LLAMA-2. These embeddings are then reduced through MultiResCNN's residual layers and CNN-based compression before classification. The approach is evaluated on both the top 50 ICD codes and the full 4,216 code set, comparing performance against baselines including MultiResCNN, CAML, and other established methods.

## Key Results
- LLM-enhanced approach achieves F1 score of 0.6258 and AUC of 0.9138 (micro) on top 50 ICD codes
- Outperforms several baselines including MultiResCNN and CAML on top 50 ICD codes
- Performance on full 4,216 ICD code set is more modest and comparable to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLAMA-2 representations provide richer semantic embeddings than traditional methods for medical text.
- Mechanism: LLAMA-2's transformer-based architecture captures deep contextual relationships in clinical language, producing dense embeddings that retain nuanced meaning beyond surface-level features.
- Core assumption: Medical text contains complex semantic relationships that standard embeddings (e.g., GloVe, FastText) fail to fully represent.
- Evidence anchors:
  - [abstract] "providing deep contextual insights into medical texts"
  - [section] "high-dimensional vectors capturing essential clinical information"
  - [corpus] Weak - corpus lacks direct comparison of LLAMA vs traditional embeddings.
- Break condition: If downstream classifier performance does not improve after switching to LLAMA embeddings, the semantic richness assumption fails.

### Mechanism 2
- Claim: MultiResCNN effectively reduces dimensionality while preserving critical information from LLAMA embeddings.
- Mechanism: Residual layers and CNN-based reduction maintain salient features during compression from 4096 to lower-dimensional space while filtering noise.
- Core assumption: CNN architectures can preserve semantic information during dimensionality reduction better than simple linear projections.
- Evidence anchors:
  - [methodology] Description of MultiResCNN architecture with residual connections
  - [results] Improved performance with MultiResCNN over direct classification
  - [corpus] Weak - limited ablation studies on different reduction techniques.
- Break condition: If performance degrades significantly after dimensionality reduction, the preservation assumption fails.

## Foundational Learning

**MultiResCNN Architecture**
- Why needed: Combines residual learning with CNN feature extraction to handle high-dimensional text representations
- Quick check: Verify residual connections properly propagate gradients during training

**LLAMA-2 Embedding Generation**
- Why needed: Creates dense semantic representations that capture clinical context better than traditional embeddings
- Quick check: Confirm embeddings maintain semantic coherence through downstream classification tasks

**ICD Coding Task**
- Why needed: Medical billing and documentation require accurate code assignment from discharge summaries
- Quick check: Ensure evaluation metrics (F1, AUC, precision@k) align with clinical coding requirements

## Architecture Onboarding

**Component Map**
Discharge Summary -> LLAMA-2 Embedding Generation -> MultiResCNN Reduction -> ICD Code Classification

**Critical Path**
The critical path flows from discharge summary text through LLAMA-2 embedding generation to MultiResCNN reduction, as these components directly impact final classification performance. The MultiResCNN framework is essential for handling the high-dimensional LLAMA embeddings.

**Design Tradeoffs**
The authors chose LLAMA-2 over domain-specific models to leverage general semantic understanding, accepting computational overhead for potentially better contextual representation. MultiResCNN was selected over simpler architectures to handle the 4096-dimensional embeddings while preserving semantic information.

**Failure Signatures**
- Poor performance on full ICD code set suggests label granularity or data sparsity issues
- Computational bottlenecks may occur during embedding generation for large datasets
- Dimensionality reduction may lose critical information if residual connections are improperly configured

**First Experiments**
1. Verify LLAMA-2 embedding generation produces consistent 4096-dimensional vectors
2. Test MultiResCNN reduction preserves embedding quality through reconstruction evaluation
3. Validate baseline MultiResCNN performance before adding LLAMA-2 representations

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How do the contextual and semantic insights provided by LLAMA-2 compare to other state-of-the-art transformer models in terms of improving the accuracy and efficiency of ICD coding?
- Basis in paper: [explicit] The paper discusses the use of LLAMA-2 for ICD coding, comparing its performance with established models such as MultiResCNN, CAML, and others, noting that LLAMA-2 enhanced by MultiResCNN improves classification outcomes.
- Why unresolved: The paper highlights LLAMA-2's potential but does not provide a detailed comparative analysis with other transformer models specifically focusing on contextual and semantic insights.
- What evidence would resolve it: Conducting a comparative study using other transformer models like BERT or GPT-3 in similar ICD coding tasks, measuring improvements in accuracy and efficiency.

**Open Question 2**
- Question: What is the impact of label granularity and training data sparsity on the performance of LLAMA-2 in ICD coding, and how can these challenges be mitigated?
- Basis in paper: [explicit] The paper mentions that LLAMA-2 underperforms with a broad set of ICD codes, likely due to limited training data and the architecture's unsuitability for the granularity of ICD codes.
- Why unresolved: The paper identifies the problem but does not explore potential solutions or strategies to address these specific challenges.
- What evidence would resolve it: Investigating methods to enhance training data availability or modify the architecture to better handle granular labels, possibly through data augmentation or model fine-tuning.

**Open Question 3**
- Question: How does the integration of external knowledge, such as medical ontologies or knowledge graphs, affect the performance of LLAMA-2 in ICD coding?
- Basis in paper: [explicit] The paper suggests future work on integrating external knowledge to address training data sparsity and enhance model performance, referencing the success of KG-MultiResCNN in utilizing structured external knowledge.
- Why unresolved: While the potential benefits are mentioned, the paper does not provide experimental results or a detailed analysis of how such integration would impact LLAMA-2's performance.
- What evidence would resolve it: Implementing experiments that incorporate external knowledge into LLAMA-2 and evaluating the changes in classification accuracy and efficiency.

## Limitations
- Significant performance degradation on full 4,216 ICD code set compared to top 50 codes
- Lack of direct comparison with contemporary transformer-based encoders (BioBERT, ClinicalBERT)
- Computational overhead of generating 4096-dimensional LLAMA embeddings not quantified

## Confidence

**High Confidence**: Experimental methodology is sound with appropriate use of MIMIC-III and standard evaluation metrics (F1, AUC, precision@k). Implementation details for LLAMA-2 adaptation and MultiResCNN framework are technically coherent.

**Medium Confidence**: Claim that LLAMA-2 provides superior semantic representations is supported by performance improvements but lacks ablation studies comparing different embedding approaches.

**Low Confidence**: Scalability assertion that this approach can generalize to full ICD code set is questionable given substantial performance drop observed, with insufficient analysis of this limitation.

## Next Checks

1. **Ablation Study**: Implement and compare LLAMA-2 embeddings against established medical domain embeddings (BioBERT, ClinicalBERT) using identical MultiResCNN architecture to isolate the contribution of LLAMA-2 specifically.

2. **Efficiency Analysis**: Measure and report inference time, memory usage, and computational cost for generating LLAMA embeddings versus training/fine-tuning domain-specific transformer models on the same hardware.

3. **Label Granularity Investigation**: Conduct error analysis on full ICD code set performance, stratifying results by code frequency and hierarchy level to understand whether performance degradation stems from data sparsity, label granularity, or representation limitations.