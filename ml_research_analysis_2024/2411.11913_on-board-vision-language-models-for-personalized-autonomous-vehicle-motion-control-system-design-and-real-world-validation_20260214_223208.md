---
ver: rpa2
title: 'On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion
  Control: System Design and Real-World Validation'
arxiv_id: '2411.11913'
source_url: https://arxiv.org/abs/2411.11913
tags:
- driving
- control
- system
- autonomous
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first end-to-end on-board vision-language
  model (VLM) for personalized autonomous vehicle motion control, achieving real-time
  adaptation to individual driving preferences. The system uses a lightweight 9B-parameter
  VLM with a RAG-based memory module to interpret natural language commands and visual
  inputs, generating personalized control policies for both lateral (MPC) and longitudinal
  (PID) motion.
---

# On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation

## Quick Facts
- arXiv ID: 2411.11913
- Source URL: https://arxiv.org/abs/2411.11913
- Reference count: 40
- This paper introduces the first end-to-end on-board vision-language model (VLM) for personalized autonomous vehicle motion control, achieving real-time adaptation to individual driving preferences.

## Executive Summary
This paper presents the first end-to-end on-board vision-language model for personalized autonomous vehicle motion control. The system processes natural language commands and visual inputs to generate personalized control policies for both lateral (MPC) and longitudinal (PID) motion control. Using a lightweight 9B-parameter VLM with RAG-based memory module, the system achieves real-time adaptation to individual driving preferences while maintaining safety and comfort standards. Real-world experiments demonstrate significant improvements in personalization and reduced takeover rates compared to baseline systems.

## Method Summary
The system uses a fine-tuned 9B-parameter Qwen-VL model with LoRA optimization and 4-bit AWQ quantization for on-board deployment. The VLM processes camera images, natural language instructions (converted from speech using Whisper), and historical memory to generate control parameters for separate MPC (lateral) and PID (longitudinal) controllers. A RAG-based memory module built on Chroma vector database stores user feedback and enables continuous personalization. The system was evaluated on a 2019 Lexus RX450h platform with drive-by-wire system, conducting real-world experiments across acceleration, lane change, and turning scenarios in various weather conditions.

## Key Results
- 76.9% reduction in takeover rates compared to baseline systems
- Processing latency under 2 seconds with less than 16GB GPU memory
- 72.7% reduction in takeover rate when memory module is included
- Safe and reliable driving across various weather conditions and traffic scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The on-board VLM translates natural language and visual inputs into actionable control parameters in real time.
- Mechanism: The VLM receives system messages, human instructions, camera images, and historical memory, then generates two action matrices (MPC for lateral, PID for longitudinal control). These matrices directly parameterize the controllers.
- Core assumption: The fine-tuned VLM can accurately map multimodal inputs to correct control parameter ranges for safe vehicle operation.
- Evidence anchors:
  - [abstract]: "generates personalized control policies for both lateral (MPC) and longitudinal (PID) motion"
  - [section 3.3]: "Our approach leverages a compact 9B-parameter VLM... which processes both visual information... and verbal commands to generate personalized control strategies"
  - [corpus]: No direct evidence of VLM-to-controller translation found in neighbors
- Break condition: VLM generates parameters outside safe operational bounds, causing unsafe vehicle behavior.

### Mechanism 2
- Claim: The RAG-based memory module enables continuous personalization by learning from human feedback.
- Mechanism: After each trip, user feedback F is combined with instruction I, scene description D, and executed policy P into a historical entry stored in the Chroma vector database. During future inference, relevant historical scenarios H are retrieved and provided to the VLM.
- Core assumption: Retrieved historical contexts significantly improve VLM's ability to generate personalized control policies for similar scenarios.
- Evidence anchors:
  - [abstract]: "RAG-based memory module that enables continuous learning of individual driving preferences through human feedback"
  - [section 3.4]: "When processing a new driving scenario, the instruction I is used for similarity matching to retrieve the top-k similar prior situations"
  - [section 4.5]: Ablation study shows 72.7% reduction in takeover rate when memory module is included
- Break condition: Memory retrieval fails to find relevant historical data or introduces noise that degrades VLM performance.

### Mechanism 3
- Claim: The lightweight VLM with quantization achieves real-time on-board inference while maintaining accuracy.
- Mechanism: The 9B-parameter Qwen-VL model is fine-tuned using LoRA and quantized to INT4 using AWQ, enabling inference on standard vehicle hardware (16GB GPU, ~2 second latency).
- Core assumption: 4-bit quantization preserves sufficient model accuracy for safe autonomous driving while achieving required computational efficiency.
- Evidence anchors:
  - [abstract]: "processes inputs in under 2 seconds with less than 16GB GPU memory, enabling practical on-board deployment"
  - [section 3.6]: "We apply AWQ to quantize our model to INT4, achieving improved quantization performance suited for on-board deployment"
  - [section 4.2]: "processing with an average 1.6-second latency"
- Break condition: Quantization artifacts cause significant performance degradation or latency exceeds real-time requirements.

## Foundational Learning

- Concept: Vision-Language Model (VLM) multimodal reasoning
  - Why needed here: The system must understand both visual driving scenes and natural language instructions to generate appropriate control policies
  - Quick check question: How does the VLM fuse camera images with spoken commands to produce steering and acceleration parameters?

- Concept: Model Predictive Control (MPC) for lateral vehicle dynamics
- Concept: PID control for longitudinal vehicle dynamics
  - Why needed here: The system generates control parameters for both lateral (steering) and longitudinal (acceleration/braking) vehicle motion separately
  - Quick check question: What are the key differences between MPC and PID control that make them suitable for lateral vs. longitudinal motion?

- Concept: Retrieval-Augmented Generation (RAG) and vector databases
  - Why needed here: The memory module uses similarity matching in a vector database to retrieve relevant historical interactions for personalization
  - Quick check question: How does the RAG system use instruction I to retrieve top-k similar historical scenarios from the Chroma database?

## Architecture Onboarding

- Component map:
  Sensor layer: Camera (Mako-G319C), LiDAR (Velodyne VLP-32C), Radar (Aptiv ESR 2.5)
  Perception: AutoWare.AI with 3D-NDT mapping and localization
  VLM inference: Fine-tuned 9B-parameter Qwen-VL with LoRA + AWQ quantization
  Memory module: Chroma vector database storing {I, D, P, F} tuples
  Control layer: PID controller (longitudinal) and MPC controller (lateral)
  Actuation: Drive-by-wire system (AutonomousStuff) via ECU

- Critical path: Camera → VLM → Control parameters → Controllers → ECU → Actuators
  - Total latency must stay under ~2 seconds for safe operation

- Design tradeoffs:
  - VLM size (9B) vs. accuracy vs. computational efficiency
  - Memory retrieval depth (top-k) vs. personalization quality vs. latency
  - Quantization level (INT4) vs. model accuracy vs. deployment feasibility
  - Separate lateral/longitudinal control vs. integrated control for simpler coordination

- Failure signatures:
  - High takeover rates indicate personalization or control generation failures
  - Controller parameters outside safe bounds suggest VLM reasoning errors
  - Memory retrieval returning irrelevant contexts degrades personalization
  - Latency spikes above 2 seconds cause safety concerns

- First 3 experiments:
  1. Basic VLM inference test: Feed camera image + "go faster" instruction, verify generated PID/MPC parameters match expected ranges
  2. Memory module validation: Complete trip, provide feedback, then verify same instruction retrieves relevant historical context in subsequent trips
  3. End-to-end safety test: Run through all three driving scenarios (acceleration, lane change, turn) with simple instructions, verify no takeover events and safe parameter generation

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The 9B-parameter VLM may not fully capture the complexity of diverse real-world driving scenarios with only 10,000 training pairs
- The decoupled control architecture may face coordination challenges in dynamic scenarios requiring tight lateral-longitudinal coupling
- Limited evaluation to three driving scenarios may not represent the full complexity of autonomous driving situations

## Confidence
- High confidence in computational feasibility and real-time performance
- Medium confidence in personalization effectiveness based on ablation study results
- Medium confidence in safety outcomes from reported metrics and evaluator assessments
- Low confidence in generalizability to unseen scenarios beyond the training dataset

## Next Checks
1. Safety boundary validation: Systematically test the VLM's parameter generation limits by providing edge-case instructions and verify the generated PID/MPC parameters remain within physically safe bounds across multiple weather and traffic conditions.

2. Cross-scenario generalization: Evaluate the system's performance on driving scenarios not included in the training dataset (highway merging, pedestrian-heavy urban areas, construction zones) to assess whether the VLM can generalize its reasoning beyond the 10,000 image-instruction pairs.

3. Memory module robustness: Test the RAG-based memory system with adversarial inputs - provide contradictory feedback for similar scenarios and measure how the system resolves conflicts and maintains consistent personalization quality.