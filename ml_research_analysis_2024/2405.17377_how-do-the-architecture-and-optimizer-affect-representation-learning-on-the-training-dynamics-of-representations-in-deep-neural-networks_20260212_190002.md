---
ver: rpa2
title: How Do the Architecture and Optimizer Affect Representation Learning? On the
  Training Dynamics of Representations in Deep Neural Networks
arxiv_id: '2405.17377'
source_url: https://arxiv.org/abs/2405.17377
tags:
- training
- similarity
- layer
- representations
- resnet-18
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how representations in deep neural networks
  (DNNs) evolve during training, focusing on overparameterized settings where training
  continues long after perfect fitting. The authors analyze representational similarity
  in DNN layers using two metrics: centered kernel alignment (CKA) and decision regions
  similarity (DRS) of linear classifier probes.'
---

# How Do the Architecture and Optimizer Affect Representation Learning? On the Training Dynamics of Representations in Deep Neural Networks

## Quick Facts
- arXiv ID: 2405.17377
- Source URL: https://arxiv.org/abs/2405.17377
- Authors: Yuval Sharon; Yehuda Dar
- Reference count: 40
- Primary result: Investigates how representations in deep neural networks evolve during training, focusing on overparameterized settings where training continues long after perfect fitting.

## Executive Summary
This paper investigates how representations in deep neural networks (DNNs) evolve during training, focusing on overparameterized settings where training continues long after perfect fitting. The authors analyze representational similarity in DNN layers using two metrics: centered kernel alignment (CKA) and decision regions similarity (DRS) of linear classifier probes. The experiments compare ResNet and Vision Transformer (ViT) architectures trained with Adam and SGD optimizers on CIFAR-10 and SVHN datasets. Key findings include: (1) Training phases are more distinguishable in ViT than ResNet, and in SGD than Adam; (2) ViT exhibits synchronized representation learning across layers, unlike ResNet; (3) In Adam training, first-layer representations in sufficiently wide networks become more similar to random initialization in the perfect fitting regime; (4) Decision regions of DNN outputs and deeper layers become more fragmented during memorization of atypical examples, then stabilize.

## Method Summary
The study analyzes representational similarity in DNN layers using centered kernel alignment (CKA) and decision regions similarity (DRS) of linear classifier probes. Researchers train ResNet-18 and ViT-B/16 architectures on CIFAR-10 and SVHN datasets using both Adam and SGD optimizers. Training continues for 4000 epochs with constant learning rates (0.0001 for Adam, 0.001 for SGD). CKA computation involves extracting layer representations at different epochs, computing Gram matrices, centering them, and calculating normalized dot products. DRS evaluation requires training linear classifier probes on layer representations at each epoch, then computing decision region agreement on 500 random planes discretized into 2500 points each. Results are visualized as heatmaps showing similarity values across epochs for each layer.

## Key Results
- Training phases are more distinguishable in ViT than ResNet, and in SGD than Adam
- ViT exhibits synchronized representation learning across layers, unlike ResNet
- In Adam training of sufficiently wide networks, first-layer representations become more similar to random initialization in the perfect fitting regime
- Decision regions of DNN outputs and deeper layers become more fragmented during memorization of atypical examples, then stabilize

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD training phases (including memorization) are more distinguishable in CKA diagrams than Adam training.
- Mechanism: SGD's implicit regularization creates more abrupt transitions between training phases, causing sharper changes in layer representations that are captured by CKA similarity metrics.
- Core assumption: The implicit regularization of SGD creates stronger temporal dynamics in representation evolution than Adam's adaptive gradient normalization.
- Evidence anchors:
  - [abstract] "Training phases are more distinguishable in SGD training than in Adam training"
  - [section] "Training ResNet-18 with SGD (e.g., Figs. D.9, D.10, D.15) has clearer similarity blocks and transition points between them than in training with Adam (e.g., Figs. 1, D.1)"
  - [corpus] Weak - no direct citations found, but related works exist on SGD vs Adam implicit regularization differences
- Break condition: If Adam's adaptive normalization somehow creates similar phase transition sharpness, or if dataset characteristics override optimizer effects.

### Mechanism 2
- Claim: ViT layers have synchronized representation learning dynamics across layers, unlike ResNet.
- Mechanism: Skip connections in ViT create strong coupling between layers, causing them to evolve representations in lockstep during training, while ResNet's residual connections create more independent layer evolution.
- Core assumption: The architectural differences in skip connection implementation create fundamentally different inter-layer dependency patterns during training.
- Evidence anchors:
  - [abstract] "Unlike ResNet, the ViT layers have synchronized dynamics of representation learning"
  - [section] "Our results show that the training dynamics of representations is highly synchronized across the ViT layers; this is due to the strong effect of skip connections in ViT, as demonstrated by Raghu et al. (2021)"
  - [corpus] Moderate - Raghu et al. (2021) discussed ViT vs ResNet representation similarity in fully trained models
- Break condition: If different ViT variants with different skip connection implementations show unsynchronized dynamics, or if ResNet modifications create synchronization.

### Mechanism 3
- Claim: In Adam training of sufficiently wide networks, first-layer representations become more similar to random initialization in the perfect fitting regime.
- Mechanism: Adam's adaptive gradient normalization, when componentwise gradient magnitudes become low, causes first-layer weights to evolve toward random-like configurations during late-stage perfect fitting.
- Core assumption: The width of the network and Adam's adaptive normalization interact to create this specific first-layer behavior that doesn't occur with SGD or in narrower networks.
- Evidence anchors:
  - [abstract] "In Adam training, first-layer representations in sufficiently wide networks become more similar to random initialization in the perfect fitting regime"
  - [section] "We show that the adaptive normalization of gradients by Adam is necessary for this phenomenon to occur"
  - [corpus] Strong - Cattaneo et al. (2024) analyzed Adam's implicit regularization changes during training
- Break condition: If numerical stability parameter ϵ is large enough to disable adaptive normalization, or if network width falls below the sufficient threshold.

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA) similarity metric
  - Why needed here: CKA provides a robust way to quantify representational similarity between layers at different training epochs, enabling the detailed analysis of training dynamics
  - Quick check question: How does CKA handle the fact that different neurons may have different roles in making representations?

- Concept: Decision regions and their fragmentation
  - Why needed here: Decision region similarity (DRS) provides task-specific similarity measures that complement CKA, and fragmentation analysis reveals how representation complexity evolves during different training phases
  - Quick check question: Why might decision regions become more fragmented during memorization phases?

- Concept: Training phases in overparameterized settings
  - Why needed here: Understanding the three-phase model (general learning, memorization, perfect fitting evolution) is crucial for interpreting the similarity patterns observed in CKA and DRS analyses
  - Quick check question: What distinguishes phase II (memorization) from phase III (perfect fitting evolution) in terms of representation changes?

## Architecture Onboarding

- Component map: CKA computation pipeline -> DRS computation pipeline -> visualization tools -> experimental control code. CKA handles general representational similarity while DRS provides task-specific analysis. Both feed into visualization for pattern recognition.
- Critical path: For CKA analysis - load model checkpoints -> extract layer representations -> compute CKA matrix -> visualize heatmap. For DRS analysis - load checkpoints -> extract representations -> train linear probes -> compute decision region agreement -> visualize.
- Design tradeoffs: CKA is computationally expensive but provides comprehensive similarity analysis; DRS is more interpretable but requires additional probe training. The epoch grid sampling balances computational cost with temporal resolution.
- Failure signatures: Low CKA values across all layers might indicate poor training or architectural issues. Unexpected synchronization patterns could suggest implementation bugs in skip connections or batch normalization. First-layer Adam behavior not appearing when expected might indicate insufficient network width.
- First 3 experiments:
  1. Replicate basic CKA analysis on ResNet-18 with Adam on CIFAR-10 to verify core similarity patterns
  2. Compare CKA results between Adam and SGD on same architecture to observe phase distinction differences
  3. Test first-layer Adam phenomenon by varying network width parameter k and observing CKA similarity to initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the first-layer phenomenon (representations becoming more similar to random initialization during perfect fitting in Adam training) occur, and what is its precise relationship to network width and adaptive gradient normalization?
- Basis in paper: [explicit] The paper observes that the first layer in ResNet-18 trained with Adam becomes more similar to random initialization in the perfect fitting regime, but only for sufficiently wide networks, and that increasing Adam's numerical stability parameter epsilon disables this phenomenon.
- Why unresolved: The paper shows the phenomenon occurs for sufficiently wide networks and requires adaptive gradient normalization, but doesn't provide a precise mathematical characterization of when it occurs or why it's width-dependent.
- What evidence would resolve it: Systematic experiments varying network width, Adam epsilon values, and architecture types to identify precise thresholds, plus theoretical analysis explaining why the phenomenon requires both width and adaptive normalization.

### Open Question 2
- Question: Why do training phases (I, II, III) manifest more clearly in ViT than ResNet, and in SGD than Adam, and what architectural or optimization properties cause this difference?
- Basis in paper: [explicit] The paper shows that training phases are more distinguishable in ViT than ResNet, and in SGD than Adam, based on CKA similarity diagrams showing clearer similarity blocks.
- Why unresolved: While the paper observes these differences, it doesn't provide a mechanistic explanation for why skip connections in ViT or the lack of adaptive normalization in SGD lead to more distinct training phases.
- What evidence would resolve it: Comparative analysis of gradient dynamics, loss landscapes, and representation trajectories across architectures and optimizers, potentially including ablation studies on skip connections or normalization schemes.

### Open Question 3
- Question: What is the relationship between decision region fragmentation during memorization phases and generalization performance, and does this relationship differ between architectures and optimizers?
- Basis in paper: [inferred] The paper observes that decision regions of DNN output and deeper layers become more fragmented during memorization of atypical examples, then stabilize, with this behavior more pronounced in ViT.
- Why unresolved: The paper describes the fragmentation pattern but doesn't investigate whether this fragmentation is beneficial, harmful, or neutral for generalization, or whether it explains the architecture/optimizer differences in training phase distinguishability.
- What evidence would resolve it: Experiments correlating fragmentation metrics with test performance across architectures/optimizers, and controlled studies varying fragmentation through architectural or optimization modifications.

## Limitations
- Generalizability beyond CIFAR-10 and SVHN to more complex datasets remains uncertain
- Precise role of network width thresholds in the Adam first-layer phenomenon not fully characterized
- Exact mechanisms by which skip connections create synchronized dynamics in ViT versus ResNet not fully explained

## Confidence

**High confidence**: SGD showing clearer training phase transitions than Adam in CKA diagrams (multiple consistent observations across experiments)

**Medium confidence**: ViT layer synchronization due to skip connections (supported by related work but mechanism details incomplete)

**Medium confidence**: Adam's adaptive normalization causing first-layer similarity to random initialization (theoretically grounded but width thresholds not precisely characterized)

## Next Checks

1. Test whether the first-layer Adam phenomenon persists when using different numerical stability parameters (ϵ) in Adam's adaptive normalization, particularly when ϵ becomes large enough to disable the adaptive component.

2. Verify if the synchronized ViT layer dynamics hold across different ViT variants with varying skip connection implementations, or if they are specific to the ViT-B/16 architecture used in the experiments.

3. Examine whether the observed training phase distinctions in CKA diagrams correlate with actual generalization gaps between SGD and Adam, beyond just representational similarity metrics.