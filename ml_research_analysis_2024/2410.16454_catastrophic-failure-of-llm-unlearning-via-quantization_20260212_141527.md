---
ver: rpa2
title: Catastrophic Failure of LLM Unlearning via Quantization
arxiv_id: '2410.16454'
source_url: https://arxiv.org/abs/2410.16454
tags:
- unlearning
- quantization
- methods
- utility
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical failure mode in existing LLM
  unlearning methods: knowledge recovery via quantization. When unlearning is applied
  with small learning rates and utility constraints, the model weights change minimally,
  causing full-precision and quantized models to map to similar values.'
---

# Catastrophic Failure of LLM Unlearning via Quantization

## Quick Facts
- arXiv ID: 2410.16454
- Source URL: https://arxiv.org/abs/2410.16454
- Authors: Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang
- Reference count: 32
- Primary result: Knowledge recovery via quantization undermines unlearning privacy guarantees

## Executive Summary
This paper identifies a critical failure mode in existing LLM unlearning methods: knowledge recovery via quantization. When unlearning is applied with small learning rates and utility constraints, the model weights change minimally, causing full-precision and quantized models to map to similar values. This inadvertently recovers up to 83% of forgotten knowledge in 4-bit quantized models (vs. 21% in full precision), undermining privacy guarantees. Theoretical analysis links this to quantization's step-size constraints and minimal weight changes. A saliency-based unlearning framework (SURE) using larger learning rates on module-level salient weights mitigates recovery while preserving utility, though with some trade-offs. This finding highlights a previously overlooked dimension in unlearning evaluation and benchmarks.

## Method Summary
The paper evaluates existing unlearning methods (GA, GA_GDR, GA_KLR, NPO, NPO_GDR, NPO_KLR) using small learning rates (1e-5) with utility constraints on NEWS and BOOKS datasets. It demonstrates that 4-bit quantization of unlearned models recovers significant knowledge from forget sets. The proposed SURE framework constructs weight saliency maps using gradients of the forget loss, then updates only the most influential weights with larger learning rates (5e-5 to 2e-4) to prevent knowledge recovery while maintaining utility.

## Key Results
- 4-bit quantization recovers up to 83% of forgotten knowledge vs. 21% in full precision
- Standard unlearning methods with small learning rates are vulnerable to quantization-induced knowledge recovery
- SURE framework improves forgetting performance for quantized models compared to original methods
- The problem persists across multiple quantization techniques (RTN, AWQ, GPTQ) and unlearning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal weight changes during unlearning cause quantization to map similar values to identical quantized representations
- Mechanism: Unlearning methods use small learning rates and regularization to preserve utility, resulting in target and unlearned model weights being very close. When quantized, these close weights map to the same quantized values due to quantization's step-size constraints
- Core assumption: Existing unlearning methods prioritize utility preservation over weight changes, making the target and unlearned models sufficiently similar for quantization mapping
- Evidence anchors:
  - [abstract]: "knowledge recovery via quantization...unlearning is applied with small learning rates and utility constraints, the model weights change minimally, causing full-precision and quantized models to map to similar values"
  - [section 5]: "As a result, the model weights of the target LLM and the unlearned LLM are very close. Hence, quantization is likely to map the weights of the target LLM and the unlearned LLM to the same values"
- Break condition: If unlearning methods use large learning rates or aggressive updates that create significant weight differences between target and unlearned models

### Mechanism 2
- Claim: Lower quantization precision (4-bit vs 8-bit) increases knowledge recovery due to larger quantization intervals
- Mechanism: 4-bit quantization has larger quantization intervals (step sizes) than 8-bit, meaning more weight values map to the same quantized representation. When unlearning causes minimal weight changes, these close weights are more likely to map to identical quantized values in 4-bit
- Core assumption: The relationship between quantization interval size and probability of identical mapping holds across different quantization techniques
- Evidence anchors:
  - [section 5]: "4-bit quantization might negatively affect unlearning by inadvertently retaining some knowledge from the forget set while preserving utility"
  - [section 5]: "Models with 8-bit quantization perform similarly to full-precision models due to 8-bit's greater sensitivity to weight changes"
- Break condition: If quantization methods are designed to preserve fine-grained weight differences even at lower precision levels

### Mechanism 3
- Claim: Saliency-based unlearning with larger learning rates prevents knowledge recovery by creating sufficient weight differences
- Mechanism: By identifying and updating only the most influential weights (salient weights) with larger learning rates, the unlearned model weights diverge sufficiently from target model weights, preventing identical mapping during quantization
- Core assumption: Knowledge in LLMs is stored in specific neurons/weights that can be identified through gradient analysis
- Evidence anchors:
  - [section 6.1]: "We propose constructing a weight saliency map by utilizing the gradient of the loss Lforget with respect to the model weights on the forget dataset"
  - [section 6.2]: "Incorporating SURE significantly improves forgetting performance compared to original methods without SURE" for quantized models
- Break condition: If saliency map construction fails to identify the correct weights or if the threshold for salience is set too low

## Foundational Learning

- Concept: Gradient Ascent for Unlearning
  - Why needed here: Understanding how GA works is crucial to grasp why small learning rates are typically used in unlearning
  - Quick check question: How does gradient ascent differ from gradient descent in the context of unlearning?

- Concept: Quantization and Precision
  - Why needed here: The paper's core finding depends on understanding how different quantization precisions affect weight representation
  - Quick check question: What is the relationship between quantization bit-width and quantization interval size?

- Concept: Saliency Maps and Gradient Analysis
  - Why needed here: The proposed solution relies on constructing saliency maps to identify influential weights for targeted updates
  - Quick check question: How does gradient magnitude relate to weight importance in the context of unlearning?

## Architecture Onboarding

- Component map:
  - Target model (ftarget) -> Unlearned model (funlearn) -> Quantized models -> Metrics (M1-M4)
  - Saliency map generator -> SURE framework -> Unlearned model updates

- Critical path:
  1. Apply unlearning method with small learning rate and utility constraints
  2. Quantize both target and unlearned models
  3. Observe knowledge recovery in quantized unlearned model
  4. Apply SURE with saliency-based updates and larger learning rate
  5. Quantize again and verify knowledge recovery is prevented

- Design tradeoffs:
  - Small learning rate: Preserves utility but causes knowledge recovery via quantization
  - Large learning rate: Prevents knowledge recovery but may degrade utility
  - Full fine-tuning: May over-adjust and degrade utility
  - Selective updating: Balances forgetting and utility but requires accurate saliency identification

- Failure signatures:
  - High M1, M2, M3 metrics after quantization indicate knowledge recovery
  - Low M4 metric indicates utility degradation
  - Unstable unlearned model indicates hyperparameter sensitivity

- First 3 experiments:
  1. Apply GA with KLR on BOOKS dataset, then quantize to 4-bit and measure forgetting metrics
  2. Apply the same unlearning method with SURE, then quantize to 4-bit and compare forgetting metrics
  3. Vary the saliency threshold in SURE and observe the tradeoff between forgetting and utility preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of knowledge recovery via quantization for unlearning methods with utility constraints, and how does it scale with precision levels and quantization techniques?
- Basis in paper: [explicit] The paper identifies that 4-bit quantization recovers up to 83% of forgotten knowledge versus 21% in full precision, but provides only theoretical explanations rather than theoretical limits.
- Why unresolved: The paper demonstrates the phenomenon empirically but doesn't establish theoretical bounds or scaling laws for different quantization precisions and techniques.
- What evidence would resolve it: Mathematical proofs establishing upper bounds on knowledge recovery rates as functions of quantization precision, step size, and weight change magnitudes.

### Open Question 2
- Question: Can saliency-based unlearning with large learning rates be made stable across different architectures and datasets, or are there fundamental trade-offs that make this approach inherently unstable?
- Basis in paper: [explicit] The paper notes that SURE is "highly sensitive to hyperparameter selection, leading to an unstable unlearned model."
- Why unresolved: The paper demonstrates the approach works but doesn't investigate why it's unstable or whether this instability can be systematically addressed.
- What evidence would resolve it: Comparative studies across multiple architectures/datasets showing stability patterns, or theoretical analysis explaining why the approach is fundamentally unstable.

### Open Question 3
- Question: How does the module-level saliency approach compare to alternative localization strategies (e.g., neuron-level, layer-level) in preventing knowledge recovery while maintaining utility?
- Basis in paper: [inferred] The paper uses module-level saliency maps but acknowledges this is "not the only approach" and mentions neuron editing as underexplored for LLMs.
- Why unresolved: The paper only tests one localization strategy without comparing it to other granularities or approaches.
- What evidence would resolve it: Empirical comparisons of different localization granularities showing trade-offs between effectiveness, stability, and computational cost.

## Limitations

- The findings primarily apply to 4-bit quantization and may not generalize to all quantization techniques or model architectures
- The proposed SURE framework requires careful hyperparameter tuning and may be unstable across different settings
- The paper doesn't explore whether quantization algorithms themselves could be modified to preserve unlearning rather than changing the unlearning algorithm

## Confidence

- **High Confidence**: The empirical observation that quantization can recover forgotten knowledge in unlearned models (M1-M3 metrics showing degraded forgetting performance post-quantization)
- **Medium Confidence**: The proposed mechanism explaining why this failure occurs (minimal weight changes + quantization mapping) is plausible but not definitively proven
- **Medium Confidence**: The effectiveness of the SURE framework in mitigating quantization recovery is demonstrated but with notable trade-offs

## Next Checks

1. Cross-architecture validation: Test the catastrophic failure and SURE mitigation across diverse model architectures (not just LLaMA-2 and ICLM) and quantization methods (including newer approaches) to assess generalizability.

2. Dynamic quantization testing: Evaluate whether the failure persists with dynamic quantization methods that adapt quantization intervals based on weight distributions, which could potentially break the mechanism described.

3. Threshold sensitivity analysis: Systematically vary the saliency threshold parameter in SURE across a wider range to better characterize the tradeoff frontier between forgetting recovery prevention and utility preservation, identifying optimal operating points.