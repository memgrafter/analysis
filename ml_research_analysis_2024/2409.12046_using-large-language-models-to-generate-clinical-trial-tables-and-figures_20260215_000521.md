---
ver: rpa2
title: Using Large Language Models to Generate Clinical Trial Tables and Figures
arxiv_id: '2409.12046'
source_url: https://arxiv.org/abs/2409.12046
tags:
- data
- table
- clinical
- code
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored using large language models (LLMs) to automate
  clinical trial table and figure generation. It employed prompt engineering and few-shot
  learning with LLMs to generate Python code for producing tables and Kaplan-Meier
  plots from clinical trial data in ADaM format.
---

# Using Large Language Models to Generate Clinical Trial Tables and Figures

## Quick Facts
- arXiv ID: 2409.12046
- Source URL: https://arxiv.org/abs/2409.12046
- Reference count: 35
- Primary result: Achieved 100% accuracy in replicating 16 tables and 1 plot from clinical trial data using LLM-generated Python code

## Executive Summary
This study explores using large language models to automate clinical trial table, figure, and listing (TFL) generation through prompt engineering and few-shot learning. The researchers developed a two-layer conversational agent that matches user queries to predefined prompts, generating Python code that produces tables and Kaplan-Meier plots from ADaM format clinical trial data. Using a CDISC pilot dataset, the approach achieved perfect accuracy in replicating reference outputs, demonstrating that LLMs can effectively streamline clinical trial reporting workflows when guided by carefully designed prompts.

## Method Summary
The researchers employed prompt engineering with system prompts, user prompts, and few-shot learning examples to guide LLMs (specifically GPT-4) in generating Python code for clinical trial TFLs. They used ADaM format datasets from the CDISC Pilot replication project as input, designed prompts to specify the desired statistical analyses and output formats, then executed the generated Python code to produce tables and plots. The outputs were compared against reference tables to verify accuracy. Additionally, they developed a conversational agent app that matches user queries to predefined prompts through a two-layer architecture, enabling non-technical users to generate specific clinical trial tables.

## Key Results
- Achieved 100% accuracy in replicating 16 tables and 1 Kaplan-Meier plot from reference outputs
- Demonstrated that carefully designed prompts enable LLMs to generate executable Python code for clinical trial TFLs
- Developed a conversational agent app that matches user queries to predefined prompts for table generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate executable Python code for clinical trial table and figure generation from ADaM format data.
- Mechanism: The LLM interprets structured prompts describing the statistical analysis requirements, generates corresponding Python code, and executes it to produce the desired TFL outputs.
- Core assumption: The LLM has sufficient statistical programming knowledge and can accurately translate natural language specifications into working Python code.
- Evidence anchors:
  - [abstract] "Using public clinical trial data in ADaM format, our results demonstrated that LLMs can efficiently generate TFLs with prompt instructions"
  - [section] "We investigated the use of LLMs to reproduce the outputs from the CDISC pilot dataset" and "achieved 100% accuracy in replicating 16 tables and 1 plot"
  - [corpus] Weak - corpus contains related work on LLM code generation but not specifically for clinical trial TFLs
- Break condition: The LLM fails to generate syntactically correct or logically accurate Python code for the specified statistical analyses, or the generated code produces incorrect results when executed.

### Mechanism 2
- Claim: Prompt engineering and few-shot learning significantly improve LLM performance in generating clinical trial tables and figures.
- Mechanism: Carefully designed system prompts, user prompts, and coding examples guide the LLM to produce accurate results by providing context and examples of the expected output format.
- Core assumption: LLMs are sensitive to prompt format and can learn from a small number of examples to generalize to new but similar tasks.
- Evidence anchors:
  - [abstract] "This study explored the use of large language models (LLMs) to automate the generation of TFLs through prompt engineering and few-shot transfer learning"
  - [section] "LLMs have been shown to be sensitive to the format of prompts" and "we provided three key components to the model: the system prompt, the user prompt, and a few-shot coding examples"
  - [corpus] Moderate - corpus includes work on prompt engineering for medical applications
- Break condition: The LLM fails to generalize from the few-shot examples to new tasks, or the generated outputs deviate significantly from the expected format and content.

### Mechanism 3
- Claim: The two-layer architecture of the Clinical Trial TFL Generation Agent ensures accurate matching of user queries to predefined prompts and table generation tasks.
- Mechanism: The first layer matches user requests to table category descriptions, while the second layer provides detailed technical prompts for code generation, minimizing misinterpretation risks.
- Core assumption: LLMs can accurately match natural language queries to predefined categories and execute corresponding technical prompts.
- Evidence anchors:
  - [abstract] "we developed a conversational agent named Clinical Trial TFL Generation Agent: An app that matches user queries to predefined prompts"
  - [section] "The first layer, called the 'Table Category Description,' focuses on understanding the user's request... Once the request is clearly understood, the second layer, 'Stepwise Detailed Prompts' is activated"
  - [corpus] Weak - corpus lacks specific examples of similar two-layer architectures for LLM applications
- Break condition: The LLM fails to correctly match user queries to predefined prompts, leading to incorrect or irrelevant table generation.

## Foundational Learning

- Concept: Clinical Data Interchange Standards Consortium (CDISC) standards and ADaM format
  - Why needed here: Understanding the data structure and standards is crucial for designing prompts that can correctly process and analyze clinical trial data.
  - Quick check question: What are the key characteristics of ADaM format datasets, and how do they differ from other clinical trial data formats?

- Concept: Statistical analysis and reporting in clinical trials
  - Why needed here: Knowledge of statistical methods and reporting requirements is essential for designing prompts that generate accurate and relevant TFL outputs.
  - Quick check question: What are the key components of a clinical study report (CSR), and how do TFLs contribute to the overall reporting process?

- Concept: Large language model (LLM) prompt engineering
  - Why needed here: Effective prompt design is critical for guiding LLMs to generate accurate and relevant outputs for clinical trial table and figure generation.
  - Quick check question: What are the key components of an effective prompt for LLM-based code generation, and how do they influence the model's output?

## Architecture Onboarding

- Component map: User interface -> Query matching engine (Layer 1) -> Technical prompt generation (Layer 2) -> LLM API -> Python execution environment -> Data input/output
- Critical path:
  1. User submits query through the chatbot interface
  2. Query is matched to predefined table category descriptions (Layer 1)
  3. Matched technical prompts are executed for code generation (Layer 2)
  4. Generated Python code is executed in the Python environment
  5. Results are displayed to the user
- Design tradeoffs:
  - Accuracy vs. flexibility: Predefined prompts ensure accuracy but may limit flexibility in handling novel requests
  - Complexity vs. usability: The two-layer architecture improves accuracy but adds complexity to the user interface
  - Speed vs. thoroughness: Automated code execution is fast but may require additional validation steps for complex analyses
- Failure signatures:
  - Incorrect table generation: Mismatch between user query and predefined prompts, or errors in generated Python code
  - Slow response times: Issues with LLM API calls or Python code execution
  - User confusion: Complex interface or unclear error messages
- First 3 experiments:
  1. Test the system with a simple query for generating a basic demographic summary table, verifying the accuracy of the generated output and the speed of the response.
  2. Attempt to generate a more complex table involving multiple variables and statistical calculations, assessing the system's ability to handle more sophisticated analyses.
  3. Evaluate the system's performance with an unfamiliar query, observing how well it matches the query to existing prompts and generates appropriate outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM-generated code for clinical trial TFLs compare in accuracy and efficiency to traditional manual programming by statisticians?
- Basis in paper: [explicit] The paper states that the LLM-generated code matched manually coded results with 100% accuracy for 16 tables and 1 Kaplan-Meier plot, but notes that analyses involving statistical tests require more prompt customization and input from statisticians.
- Why unresolved: While initial results show 100% accuracy for basic TFL generation, the study only tested a limited set of predefined tables and did not compare efficiency metrics (time, resources) against traditional methods across a broader range of clinical trial scenarios.
- What evidence would resolve it: A comprehensive study comparing LLM-generated code vs. manual programming across multiple clinical trial datasets, measuring both accuracy and time/resources required for each method.

### Open Question 2
- Question: What are the limitations and potential errors introduced by relying on LLMs for complex statistical analyses in clinical trials?
- Basis in paper: [explicit] The paper mentions that LLMs struggle with rules requiring complex mapping, coding, or tasks requiring advanced/uncommon knowledge to implement statistical analysis, and that performance heavily relies on the quality of predefined prompts.
- Why unresolved: The study did not extensively explore the failure modes or error types that might occur when LLMs are used for more complex statistical analyses beyond basic TFL generation.
- What evidence would resolve it: Systematic testing of LLM-generated code for a wide range of complex statistical analyses, documenting types of errors, their frequency, and impact on clinical trial results.

### Open Question 3
- Question: How can the "Clinical Trial TFL Generation Agent" app be further improved to handle more complex computations and provide better support for statisticians?
- Basis in paper: [explicit] The paper suggests potential enhancements such as developing a prompt library for CSR TFLs, supporting creation of tables from standardized analysis results datasets, and capturing user interactions to improve responses based on ongoing conversations.
- Why unresolved: The current app is a prototype that demonstrates the concept, but its capabilities for handling complex statistical analyses and supporting statisticians in their workflow have not been fully explored or tested.
- What evidence would resolve it: A user study involving statisticians using the enhanced app for a variety of clinical trial analyses, evaluating its effectiveness, ease of use, and impact on their workflow compared to existing tools.

## Limitations
- The study only validated the approach on a single CDISC pilot dataset, limiting generalizability to diverse clinical trial scenarios
- Prompt engineering requires significant customization for each new table type, reducing scalability for novel analytical requests
- The system does not address validation of statistical correctness beyond format matching, requiring human verification for accuracy

## Confidence

**High Confidence:** The core finding that LLMs can generate syntactically correct Python code for clinical trial table creation when provided with detailed prompts is well-supported by the 100% success rate on the replication task. The mechanism by which prompt engineering guides LLM output is clearly demonstrated.

**Medium Confidence:** The claim that the two-layer architecture improves query-to-table matching accuracy is plausible given the design, but lacks comparative data showing performance against single-layer or alternative approaches. The specific effectiveness of this architecture requires further testing.

**Low Confidence:** The scalability claims for handling diverse clinical trial scenarios and the assertion that this approach significantly reduces human workload are not empirically validated beyond the controlled dataset. The study does not quantify time savings or error reduction compared to manual methods.

## Next Checks
1. **Dataset Generalization Test:** Validate the system on 3-5 additional clinical trial datasets from different therapeutic areas with varying complexity levels, measuring accuracy degradation and identifying failure patterns when dataset characteristics change.

2. **Statistical Validation Audit:** Have domain experts review generated tables for statistical correctness (not just format matching) across different analysis types, documenting error rates and identifying which types of statistical analyses require human oversight.

3. **Time and Cost Analysis:** Conduct a controlled study comparing total time (prompt engineering + execution + validation) against traditional manual table generation methods for equivalent analyses, including development of new prompts for novel table types.