---
ver: rpa2
title: 'DenoSent: A Denoising Objective for Self-Supervised Sentence Representation
  Learning'
arxiv_id: '2401.13621'
source_url: https://arxiv.org/abs/2401.13621
tags:
- sentence
- learning
- contrastive
- representation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of contrastive learning for
  sentence representation by proposing a novel denoising objective that captures intra-sentence
  semantics. The authors introduce DenoSent, which learns sentence representations
  by reconstructing noisy input sentences using both discrete (via back-translation)
  and continuous (via high dropout) noise.
---

# DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning

## Quick Facts
- arXiv ID: 2401.13621
- Source URL: https://arxiv.org/abs/2401.13621
- Authors: Xinghao Wang; Junliang He; Pengyu Wang; Yunhua Zhou; Tianxiang Sun; Xipeng Qiu
- Reference count: 9
- One-line primary result: DenoSent achieves state-of-the-art performance on STS tasks with 79.33 average score and 64.46% classification accuracy

## Executive Summary
This paper addresses the limitations of contrastive learning for sentence representation by proposing a novel denoising objective that captures intra-sentence semantics. DenoSent learns sentence representations by reconstructing noisy input sentences using both discrete (via back-translation) and continuous (via high dropout) noise. The method combines this denoising objective with contrastive learning, leveraging both intra-sentence and inter-sentence supervision signals. Experiments demonstrate that DenoSent outperforms strong baselines like SimCSE, PaSeR, and PromptBERT across multiple tasks including STS, reranking, retrieval, and classification.

## Method Summary
DenoSent modifies the vanilla Transformer architecture by applying pooling to reduce encoder outputs to a single vector, using single-head attention in the decoder, and predicting the original sentence from a noisy version. The model combines a denoising objective (reconstructing noisy sentences) with a contrastive objective (pulling similar sentences closer). Discrete noise is introduced via back-translation (translating to Chinese and back to English), while continuous noise is introduced via dropout on embedded sentences at a rate of 0.825. The model is trained on the unsupervised Wiki dataset using AdamW optimizer with learning rate 5e-5 and sequence length 32.

## Key Results
- Achieves 79.33 average Spearman correlation on STS tasks, outperforming SimCSE (78.48), PaSeR (78.54), and PromptBERT (78.26)
- Highest classification accuracy at 64.46% across multiple tasks
- Effective on reranking (MRR@1: 84.66) and retrieval (MAP@1: 83.70) tasks
- Ablation studies confirm both denoising and contrastive objectives contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DenoSent captures intra-sentence semantics that contrastive methods miss by reconstructing noisy sentences.
- Mechanism: The denoising auto-encoder forces the encoder to retain semantic information that enables reconstruction of the original sentence from its noisy version, creating supervision signals within individual sentences rather than between pairs.
- Core assumption: Semantic information sufficient for reconstruction correlates with the information needed for sentence similarity tasks.
- Evidence anchors:
  - [abstract]: "By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form."
  - [section]: "In DenoSent, we employ pooling strategies on the encoder outputs to compress each sentence into a vector of a fixed-sized d... we employ dropout... on the embedded sentences, setting most of the elements of the decoder input to zero. We subsequently train our model to reconstruct the sentence from the heavily corrupted input."
  - [corpus]: Weak evidence - no direct corpus support for denoising capturing intra-sentence semantics, but related works like CMLM and PaSeR use similar generative objectives.

### Mechanism 2
- Claim: Discrete noise via back-translation provides syntactically diverse but semantically similar sentences that improve generalization.
- Mechanism: Back-translation creates noisy versions of sentences that preserve meaning while varying syntax and expression, forcing the model to learn semantic invariance across different surface forms.
- Core assumption: Back-translation preserves core semantic content while introducing sufficient syntactic variation to be useful as noise.
- Evidence anchors:
  - [section]: "Machine translation aims to preserve original semantics in another language. By translating and back-translating sentences, we can obtain augmented sentences with similar semantics but varied syntax and expression."
  - [section]: "In our experiments, we discovered that employing the back-translation strategy results in marginally superior performance compared to using an LLM."
  - [corpus]: No direct corpus evidence; assumption based on general MT research showing back-translation preserves semantics.

### Mechanism 3
- Claim: Combining denoising with contrastive objectives creates complementary supervision signals that boost overall performance.
- Mechanism: The denoising objective provides intra-sentence supervision while contrastive learning provides inter-sentence supervision; these orthogonal signals reinforce each other rather than competing.
- Core assumption: Intra-sentence and inter-sentence supervision signals are complementary rather than redundant for sentence representation learning.
- Evidence anchors:
  - [abstract]: "Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance."
  - [section]: "Unlike contrastive learning, the proposed denoising objective... offers intra-sentence supervision signals... Therefore, the denoising objective works independently from previous contrastive methods. Both objectives can be readily integrated."
  - [section]: Table 3 shows "DenoSent-BERT base" outperforms both single-objective variants.
  - [corpus]: Weak evidence - no corpus support for complementarity, but ablation study in Table 3 provides empirical support.

## Foundational Learning

- Concept: Self-supervised learning without labels
  - Why needed here: DenoSent must learn sentence representations without human-annotated pairs or labels
  - Quick check question: How does the model create training signals without labeled data?

- Concept: Autoencoder architecture for representation learning
  - Why needed here: The denoising objective requires an encoder-decoder structure where the encoder learns compressed representations
  - Quick check question: What role does the decoder play during training vs. evaluation?

- Concept: Contrastive learning framework
  - Why needed here: The model combines denoising with contrastive objectives to capture both intra- and inter-sentence relationships
  - Quick check question: How are positive and negative pairs constructed for the contrastive component?

## Architecture Onboarding

- Component map: Input sentence → BERT encoder → pooled representation → discrete noise → continuous noise → decoder → reconstructed sentence

- Critical path:
  1. Input sentence → BERT encoder → pooled representation
  2. Apply discrete noise → back-translation → sentence representation
  3. Apply continuous noise → dropout → decoder input
  4. Decoder reconstructs original sentence using pooled representation
  5. Compute denoising loss
  6. Apply discrete noise to create positive pair for contrastive loss
  7. Compute combined loss and update parameters

- Design tradeoffs:
  - Single-head vs multi-head attention in decoder: Single-head performs better when sequence length is fixed at 1
  - [MASK] vs [CLS] pooling: [MASK] pooling shows slight improvement
  - Discrete noise source: Back-translation slightly outperforms LLM rewriting
  - Dropout rate: Must be high enough to create meaningful reconstruction task but not too high to prevent learning

- Failure signatures:
  - Training instability: Check noise levels and dropout rates
  - Poor reconstruction quality: Verify decoder architecture and attention mechanism
  - Contrastive performance worse than baselines: Check positive pair construction and temperature parameter
  - Overfitting to training data: Verify dataset size and augmentation diversity

- First 3 experiments:
  1. Ablation study: Train with denoising only vs contrastive only vs combined to verify complementarity
  2. Noise sensitivity: Sweep dropout rates (0.8, 0.825, 0.85, 0.875, 0.9) and observe reconstruction quality
  3. Pooling comparison: Compare [MASK] token pooling vs [CLS] pooling vs mean pooling on STS-B development set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DenoSent scale with larger datasets beyond Wiki, particularly in low-resource language settings?
- Basis in paper: [inferred] The paper uses the unsupervised Wiki dataset for training but does not explore performance on other corpora or low-resource languages.
- Why unresolved: The study focuses on English and does not provide insights into how the method generalizes to other languages or smaller datasets.
- What evidence would resolve it: Comparative experiments training DenoSent on diverse multilingual corpora and low-resource datasets, measuring performance across languages.

### Open Question 2
- Question: What is the impact of different pooling strategies (e.g., mean, max, or attention-based) on the quality of sentence representations in DenoSent?
- Basis in paper: [explicit] The paper uses [MASK] token pooling and mentions it provides a slight boost compared to [CLS] pooling, but does not explore other strategies.
- Why unresolved: The study only tests [MASK] and [CLS] pooling, leaving the potential benefits of other strategies unexplored.
- What evidence would resolve it: Experiments comparing DenoSent with various pooling strategies (e.g., mean, max, attention-based) and evaluating their impact on downstream tasks.

### Open Question 3
- Question: How does DenoSent perform in zero-shot or few-shot learning scenarios, particularly for tasks with limited labeled data?
- Basis in paper: [inferred] The paper evaluates DenoSent on supervised classification tasks but does not explore its effectiveness in low-data regimes.
- Why unresolved: The study does not address the adaptability of DenoSent to tasks where labeled data is scarce.
- What evidence would resolve it: Experiments testing DenoSent in zero-shot and few-shot learning settings, comparing its performance to other methods under limited data conditions.

### Open Question 4
- Question: What is the computational efficiency of DenoSent compared to contrastive methods like SimCSE, especially during inference?
- Basis in paper: [explicit] The paper mentions that the decoder is discarded during evaluation but does not provide a detailed comparison of inference speed or memory usage.
- Why unresolved: The study focuses on performance metrics but does not quantify the trade-offs in computational resources.
- What evidence would resolve it: Benchmarks comparing the inference time, memory usage, and scalability of DenoSent versus contrastive methods like SimCSE.

## Limitations

- The paper relies heavily on a single training corpus (Wiki dataset), which may not generalize to other domains
- The claim that denoising captures "intra-sentence semantics" remains largely theoretical without direct empirical validation
- Architectural choices (single-head attention, [MASK] pooling) are presented as optimal but lack extensive ablation to confirm these are truly necessary

## Confidence

- **High confidence**: The technical feasibility of combining denoising with contrastive objectives, the general approach of using reconstruction losses for representation learning, and the experimental methodology (evaluation datasets, metrics, and comparative framework).
- **Medium confidence**: The specific performance improvements over baselines, the claim that discrete noise via back-translation is superior to other perturbation methods, and the assertion that denoising and contrastive objectives are truly complementary rather than competing.
- **Low confidence**: The theoretical claim about denoising capturing "intra-sentence semantics" that contrastive methods miss, the necessity of specific architectural choices (single-head attention, [MASK] pooling), and the robustness of results across different domains and noise sources.

## Next Checks

1. **Noise Level Sensitivity Analysis**: Systematically sweep dropout rates from 0.7 to 0.95 and evaluate reconstruction quality vs. downstream performance to identify the optimal noise level and determine whether current settings are truly optimal or locally optimal.

2. **Noise Source Ablation**: Replace back-translation with alternative discrete noise sources (synonym replacement, word deletion, shuffling) while keeping continuous noise constant to isolate the contribution of discrete noise type versus noise magnitude.

3. **Cross-Domain Generalization**: Train DenoSent on multiple diverse corpora (e.g., PubMed for biomedical, arXiv for technical) and evaluate performance degradation compared to in-domain training to assess domain robustness claims and identify potential overfitting to Wiki domain characteristics.