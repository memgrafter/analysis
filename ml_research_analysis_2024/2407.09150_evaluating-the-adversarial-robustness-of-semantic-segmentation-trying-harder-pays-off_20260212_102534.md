---
ver: rpa2
title: 'Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder
  Pays Off'
arxiv_id: '2407.09150'
source_url: https://arxiv.org/abs/2407.09150
tags:
- miou
- attacks
- adversarial
- attack
- dag-0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper re-evaluates the adversarial robustness of semantic segmentation
  models and finds that previously reported robustness is overestimated. Using a comprehensive
  set of strong attacks and a detailed analysis, the authors show that models trained
  with 50% adversarial samples have no robustness, and those trained with 100% adversarial
  samples are vulnerable when measured with image-wise average mIoU.
---

# Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder Pays Off

## Quick Facts
- arXiv ID: 2407.09150
- Source URL: https://arxiv.org/abs/2407.09150
- Authors: Levente Halmosi; Bálint Mohos; Márk Jelasity
- Reference count: 40
- Key outcome: Previously reported robustness in semantic segmentation is overestimated; 50% adversarial training provides no robustness, and size-bias effects are not detected by standard metrics

## Executive Summary
This paper re-evaluates the adversarial robustness of semantic segmentation models and finds that previously reported robustness is significantly overestimated. Using a comprehensive set of strong attacks and a detailed analysis, the authors demonstrate that models trained with 50% adversarial samples have no meaningful robustness, while those trained with 100% adversarial samples remain vulnerable when measured with image-wise average mIoU. The study also reveals a critical size-bias: small objects are more easily attacked than large ones, a problem not detected by standard class-wise metrics. The findings suggest that robustness evaluation in semantic segmentation needs to be more thorough, using strong attacks and considering size-biased effects.

## Method Summary
The paper evaluates model robustness by generating adversarial perturbations using 10 different attacks (PAdam-CE, PAdam-Cos, SEA attacks, ALMAProx, DAG, PDPGD) in an ensemble fashion. Models are trained with either 50% or 100% adversarial samples, and evaluated on both PASCAL VOC 2012 and Cityscapes datasets. The evaluation uses three metrics: pixel accuracy, class-wise mean IoU (CmIoU), and image-wise mean IoU (NmIoU). The attack ensemble selects the most successful attack per input, and results are aggregated across all inputs to compute overall robustness metrics.

## Key Results
- 50% adversarial training (used by DDC-AT and SegPGD-AT) results in complete lack of robustness against strong attacks
- 100% adversarial training provides some robustness but models remain vulnerable when measured with image-wise average mIoU
- Small objects are significantly more vulnerable to attacks than large objects, a phenomenon not detected by standard class-wise metrics
- Different models are vulnerable to different attacks, demonstrating the necessity of using a diverse attack ensemble

## Why This Works (Mechanism)

### Mechanism 1
Aggregated attack strength is much higher than individual attack strength. Multiple diverse attacks are run on each input, and the most successful result is selected. Different attacks are effective against different models, so no single attack dominates all scenarios.

### Mechanism 2
Image-wise average mIoU (NmIoU) reveals size-biased vulnerability not detected by class-wise mIoU (CmIoU). CmIoU averages IoU over classes, which can mask poor performance on small objects if large objects have high IoU. NmIoU averages IoU over images, giving equal weight to each image regardless of object size.

### Mechanism 3
50% adversarial training batches provide no robustness against strong attacks. Training with only 50% adversarial examples is insufficient to learn robust features that withstand strong, diverse attacks. The models appear to memorize clean patterns while remaining vulnerable to adversarial perturbations.

## Foundational Learning

- Concept: Adversarial examples and their generation
  - Why needed here: The paper evaluates model robustness by generating adversarial perturbations. Understanding how these perturbations are created and optimized is fundamental to interpreting the results.
  - Quick check question: What is the difference between attacks that maximize loss versus those that minimize perturbation size?

- Concept: Performance metrics for semantic segmentation
  - Why needed here: The paper uses pixel accuracy, class-wise mIoU (CmIoU), and image-wise mIoU (NmIoU). Understanding how these metrics are computed and what they measure is crucial for interpreting the robustness evaluation.
  - Quick check question: How does image-wise mIoU differ from class-wise mIoU in terms of sensitivity to small objects?

- Concept: Adversarial training methodology
  - Why needed here: The paper compares models trained with different proportions of adversarial samples. Understanding how adversarial training works and its limitations is essential for evaluating the claims about robustness.
  - Quick check question: Why might 50% adversarial samples during training be insufficient for achieving robustness?

## Architecture Onboarding

- Component map: Attack generation -> Model evaluation -> Aggregation system -> Metric computation
- Critical path: Load model and dataset → Generate adversarial examples using all attacks → Evaluate model on adversarial examples → Aggregate results across attacks and inputs → Compute metrics (accuracy, CmIoU, NmIoU)
- Design tradeoffs: Attack diversity vs. computational cost; CmIoU vs. NmIoU tradeoffs in detecting size-biased vulnerabilities; ℓ∞ norm choice affects attack strength and perceptibility
- Failure signatures: Near-zero metrics across all attacks indicate complete lack of robustness; large gap between CmIoU and NmIoU suggests size-biased vulnerability; one attack dominating across all models indicates insufficient attack diversity
- First 3 experiments: 1) Run all attacks on a single image with a normal model to verify attack functionality; 2) Compare CmIoU vs. NmIoU on a dataset with known size bias to validate metric differences; 3) Test 50% vs. 100% adversarial training on a simple model to reproduce the robustness findings

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal ratio of adversarial to clean samples during adversarial training for semantic segmentation models? The paper demonstrates that using 50% adversarial samples results in no robustness, while 100% adversarial samples provide some robustness but with trade-offs. The study suggests that the proportion of adversarial samples is a critical factor.

### Open Question 2
How do different architectures (e.g., PSPNet vs. DeepLabv3) interact with adversarial training methods in terms of robustness and size-bias effects? The paper compares PSPNet and DeepLabv3 architectures and notes that the choice of architecture makes no significant difference in robustness, but does not explore deeper architectural influences.

### Open Question 3
What are the underlying causes of the size-bias phenomenon observed in robust semantic segmentation models? The paper identifies that small objects are more easily attacked than large ones, but does not delve into the mechanisms or training dynamics that lead to this bias, nor does it propose methods to mitigate it.

## Limitations
- Results rely on existing pre-trained models from prior work, inheriting potential inconsistencies in training protocols
- Findings are specific to PASCAL VOC 2012 and Cityscapes datasets and may not generalize to other semantic segmentation domains
- Computational cost of running multiple strong attacks may limit practical adoption of this evaluation methodology

## Confidence
- High confidence: The finding that 50% adversarial training provides no robustness against strong attacks
- Medium confidence: The size-bias discovery, as it relies on a novel metric (image-wise mIoU) not previously validated in the literature
- Medium confidence: The necessity of diverse attack ensembles, though empirical evidence is strong

## Next Checks
1. Replication on additional datasets: Test the evaluation framework on other semantic segmentation datasets (e.g., ADE20K, COCO-Stuff) to verify generalizability of the size-bias findings
2. Cross-model training protocol verification: Re-train a subset of models from scratch using documented protocols to isolate whether performance differences stem from training variations or inherent model properties
3. Single-attack ablation study: Systematically evaluate each individual attack in the ensemble to quantify the marginal contribution of each attack type and validate the aggregation approach