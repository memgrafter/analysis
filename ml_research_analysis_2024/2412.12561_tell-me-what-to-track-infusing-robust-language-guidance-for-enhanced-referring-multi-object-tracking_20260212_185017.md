---
ver: rpa2
title: 'Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring
  Multi-Object Tracking'
arxiv_id: '2412.12561'
source_url: https://arxiv.org/abs/2412.12561
tags:
- queries
- targets
- tracking
- detection
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of referring multi-object tracking
  (RMOT), where the goal is to track objects in a video based on a language expression.
  The key challenges are the imbalanced data distribution between newborn and existing
  targets, and the lack of explicit language guidance in existing transformer-based
  models.
---

# Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking

## Quick Facts
- arXiv ID: 2412.12561
- Source URL: https://arxiv.org/abs/2412.12561
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on referring multi-object tracking, with 3.22% improvement in HOTA over prior works

## Executive Summary
This paper addresses the problem of referring multi-object tracking (RMOT), where the goal is to track objects in a video based on a language expression. The key challenges are the imbalanced data distribution between newborn and existing targets, and the lack of explicit language guidance in existing transformer-based models. To solve these issues, the authors propose TellTrack, which includes three main components: a collaborative query matching strategy to boost newborn target detection, a referring-infused query adaptation to provide explicit language guidance, and a cross-modal encoder to enhance multi-modal fusion. The proposed method achieves state-of-the-art performance on two autonomous driving datasets, with a 3.22% improvement in HOTA compared to prior works.

## Method Summary
TellTrack is an end-to-end transformer-based framework for referring multi-object tracking. It addresses two key challenges: the imbalanced data distribution between newborn and existing targets, and the lack of explicit language guidance. The method introduces three novel components: a collaborative query matching (CQM) strategy that allows detection queries to match existing targets in intermediate decoder layers, boosting their activation frequency and training; a referring-infused query adaptation (RIQA) that directly injects sentence embeddings into query tokens for explicit language guidance; and a cross-modal encoder (CME) that first refines visual features with deformable attention before cross-attention with text, improving multi-modal fusion. The model is trained with AdamW optimizer, learning rate 1e-4 (1e-5 for backbone), cosine decay, 40 epochs with learning rate drop at epoch 40.

## Key Results
- Achieves state-of-the-art performance on KITTI and BDD100K referring multi-object tracking datasets
- 3.22% improvement in HOTA compared to prior works
- Demonstrates effectiveness of collaborative query matching, referring-infused query adaptation, and cross-modal encoder components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collaborative Query Matching (CQM) alleviates the imbalance between newborn and existing target training by allowing detection queries to match existing targets in intermediate decoder layers.
- Mechanism: Instead of enforcing a strict one-to-one bipartite matching between detection queries and newborn targets in all decoder layers, CQM relaxes the matching criteria so that detection queries can also match existing targets during intermediate training stages. This increases the activation frequency of detection queries, providing more training signals and improving their ability to detect rare or uncommon targets.
- Core assumption: The imbalance in training frequency between detection queries (activated once per target) and track queries (activated continuously) is a primary bottleneck for newborn target detection performance.
- Evidence anchors:
  - [abstract]: "We conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance."
  - [section]: "In each layer, track queries...are trained to localize the same pre-matched targets...and detection queries do a one-to-one bipartite matching with those newborn targets...Instead of urging a one-to-one matching, CQM allows the detection queries to match the existing targets in the intermediate layers..."
  - [corpus]: No direct evidence found in neighbors about this specific matching strategy; the claim appears novel to this paper.
- Break condition: If the relaxation of matching criteria leads to confusion between detection and tracking roles, or if it degrades the model's ability to distinguish between new and existing targets.

### Mechanism 2
- Claim: Referring-Infused Query Adaptation (RIQA) provides explicit language guidance by directly injecting sentence embeddings into query tokens.
- Mechanism: RIQA modifies the query representation by concatenating or adding sentence embeddings (derived from the language prompt) directly to the content part of query tokens. This creates a direct semantic link between the language instruction and the object detection/tracking queries, rather than relying on indirect fusion in early feature maps.
- Core assumption: Direct fusion of linguistic intention into decoder queries is more effective for guiding object detection than indirect multi-modal fusion in the encoder.
- Evidence anchors:
  - [abstract]: "In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens."
  - [section]: "TellTrack adopts a query adaptor that directly fuses the text prompt with the queries, providing strong guidance and enhancing the model's reasoning capability."
  - [corpus]: No direct evidence in neighbors about this specific query adaptation technique; appears to be a novel contribution.
- Break condition: If the injected sentence embeddings overwhelm or conflict with the spatial priors in the queries, leading to degraded localization accuracy.

### Mechanism 3
- Claim: Cross-Modal Encoder (CME) improves multi-modal fusion by first refining visual features with deformable attention before cross-attention with text.
- Mechanism: Instead of direct cross-attention between raw multi-scale feature maps and text embeddings, CME first applies deformable self-attention to the image feature pyramid to enhance spatial structure and reduce redundancy. This structured visual representation then serves as a better foundation for cross-modal alignment with text.
- Core assumption: Unstructured, noisy image features hinder effective cross-modal alignment, and refining them first leads to better fusion quality.
- Evidence anchors:
  - [abstract]: "In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks where limited multi-modal information is shared and interacted between feature maps."
  - [section]: "Direct cross-attention between the two modalities may struggle to establish meaningful correspondences...Consequently, the cross-modal alignment process becomes more effective, leading to better fusion of textual and visual information."
  - [corpus]: No direct evidence in neighbors about this specific encoder design; appears to be a novel contribution.
- Break condition: If the additional deformable attention stage adds computational overhead without meaningful performance gains, or if it over-smooths visual features needed for precise localization.

## Foundational Learning

- Concept: Transformer-based encoder-decoder architecture for object detection and tracking
  - Why needed here: The paper builds on deformable DETR and MOTR frameworks, which use transformer decoders with learnable queries for set prediction in MOT. Understanding how queries are matched, updated, and used for predictions is essential.
  - Quick check question: What is the difference between detection queries and track queries in this framework, and how are they matched to ground truth?

- Concept: Cross-modal feature fusion and attention mechanisms
  - Why needed here: The paper introduces a novel cross-modal encoder that refines visual features before fusing with text. Understanding multi-head attention, cross-attention, and deformable attention is crucial.
  - Quick check question: How does deformable attention differ from standard multi-head attention, and why is it useful for multi-scale feature pyramids?

- Concept: Set prediction and bipartite matching in object detection
  - Why needed here: The model uses one-to-one bipartite matching between predictions and ground truth for both detection and tracking. Understanding the matching cost and loss computation is important.
  - Quick check question: How does the matching algorithm handle the imbalance between newborn and existing targets, and how does CQM modify this?

## Architecture Onboarding

- Component map: Backbone -> CME -> RIQA (query injection) -> Decoder (with CQM during training) -> Temporal Reasoning -> Output
- Critical path: Backbone extracts image features and text embeddings, CME refines visual features with deformable attention and fuses with text, RIQA injects sentence embeddings into query tokens, Decoder processes queries with self-attention and cross-attention to outputs, Temporal Reasoning module refines predictions across frames using memory bank
- Design tradeoffs:
  - CQM: More training for detection queries vs. potential confusion between detection and tracking roles
  - RIQA: Stronger language guidance vs. risk of overwhelming spatial priors in queries
  - CME: Better feature fusion vs. added computational cost from deformable attention stage
- Failure signatures:
  - Poor newborn target detection: Check CQM implementation and detection query matching
  - Weak language guidance: Verify RIQA is correctly injecting sentence embeddings
  - Inaccurate localization: Examine CME feature refinement and cross-modal fusion quality
- First 3 experiments:
  1. Test CQM alone: Implement standard matching vs. CQM on a subset of data, measure detection query activation frequency and newborn detection performance
  2. Test RIQA alone: Compare vanilla queries vs. RIQA-injected queries on referring accuracy while holding other components fixed
  3. Test CME alone: Compare direct cross-attention vs. CME-processed features on multi-modal fusion quality and overall tracking performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the imbalance between newborn and existing targets specifically affect the training dynamics and performance of transformer-based RMOT models?
- Basis in paper: [explicit] The paper discusses how the imbalanced data distribution between newborn and existing targets impairs the training of newborn target detection, as detection queries are only activated once when targets first appear, while track queries are activated throughout the lifespan of targets.
- Why unresolved: While the paper proposes CQM to address this issue, it doesn't provide a detailed analysis of how the imbalance specifically impacts the model's learning process or performance.
- What evidence would resolve it: A detailed study analyzing the training dynamics of detection and track queries, comparing their activation frequencies and contributions to the overall loss, would provide insights into the impact of the imbalance.

### Open Question 2
- Question: How does the proposed Cross-Modal Encoder (CME) compare to other multi-modal fusion techniques in terms of efficiency and effectiveness for RMOT?
- Basis in paper: [explicit] The paper introduces CME to address the limitations of the previous encoder's limited multi-modal information exchange, but doesn't compare it to other fusion techniques.
- Why unresolved: While the paper demonstrates the effectiveness of CME, it doesn't provide a comprehensive comparison with other fusion methods to establish its relative advantages.
- What evidence would resolve it: Conducting experiments comparing CME with other multi-modal fusion techniques, such as transformer-based encoders or attention mechanisms, would provide insights into its efficiency and effectiveness.

### Open Question 3
- Question: How does the choice of language embedding (sentence vs. word embeddings) impact the performance of RIQA in RMOT?
- Basis in paper: [explicit] The paper mentions that sentence embeddings are chosen over individual word embeddings to provide more general and flexible guidance, but doesn't explore the impact of this choice on performance.
- Why unresolved: While the paper justifies the choice of sentence embeddings, it doesn't investigate how different types of embeddings might affect the model's ability to understand and follow referring expressions.
- What evidence would resolve it: Conducting experiments comparing the performance of RIQA with different types of language embeddings, such as word embeddings, phrase embeddings, or contextual embeddings, would provide insights into the impact of this choice.

## Limitations

- Architecture specificity: The paper introduces novel components without direct ablation studies on each component's contribution, making it difficult to isolate which innovation drives the 3.22% HOTA improvement.
- Training dynamics: The claim that CQM alleviates imbalance relies on qualitative descriptions of query activation patterns, but lacks quantitative analysis of detection query vs track query activation frequencies or convergence curves.
- Generalization: Performance is reported only on two autonomous driving datasets, limiting claims about robustness to other domains or real-world deployment scenarios.

## Confidence

- High confidence: The overall framework design and implementation of transformer-based RMOT is technically sound. The architectural components are described with sufficient detail for reproduction, and the HOTA improvement over baselines is statistically significant.
- Medium confidence: The mechanism claims (CQM improves newborn detection by increasing query activation frequency, RIQA provides explicit language guidance through query injection, CME enhances cross-modal fusion through deformable attention) are plausible given the architectural descriptions, but lack direct quantitative validation in the paper.
- Low confidence: The assertion that TellTrack's improvements are primarily due to the proposed components rather than potential implementation optimizations in the baseline frameworks. Without direct component ablation and comparison to established implementations, the attribution of performance gains remains uncertain.

## Next Checks

1. Component isolation study: Implement and test each proposed component (CQM, RIQA, CME) independently on a subset of KITTI/BDD100K data, measuring their individual contributions to newborn detection performance, language guidance quality, and overall tracking accuracy.
2. Query activation analysis: Track and compare the activation frequencies and loss gradients of detection vs track queries during training with and without CQM, quantifying the claimed imbalance reduction and its correlation with newborn detection improvements.
3. Cross-dataset generalization: Evaluate TellTrack on non-autonomous driving referring tracking datasets (e.g., A2D Sentences, J-HMDB with referring annotations) to assess whether the 3.22% HOTA improvement generalizes beyond the specific domain where it was demonstrated.