---
ver: rpa2
title: 'Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation'
arxiv_id: '2410.20362'
source_url: https://arxiv.org/abs/2410.20362
tags:
- data
- training
- xsynthesis
- xtrain
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates synthetic data generation for LLM training,
  addressing the limitation that standard instruction-finetuned models prioritize
  problem-solving over data generation. The authors propose NOMAD, which specifically
  trains models for data synthesis through two key innovations: no-prompt-masked training
  (exposing models to complete instruction-response pairs to improve prompt generation)
  and proper training set size selection (finding that smaller subsets often yield
  better supplementary data than using entire datasets).'
---

# Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation

## Quick Facts
- arXiv ID: 2410.20362
- Source URL: https://arxiv.org/abs/2410.20362
- Authors: Yifang Chen; David Zhu; Simon Du; Kevin Jamieson; Yang Liu
- Reference count: 14
- Primary result: NOMAD achieves >4% gains in TriviaQA and >2% in GSM8K with limited training data

## Executive Summary
This paper addresses the fundamental limitation that standard instruction-finetuned models are optimized for question-answering rather than data generation. The authors propose NOMAD, a paradigm shift that trains models specifically for data synthesis through two key innovations: no-prompt-masked training (exposing models to complete instruction-response pairs) and proper training set size selection (finding that smaller subsets often yield better supplementary data than using entire datasets). Experiments show NOMAD achieves significant performance gains on multiple benchmarks, particularly with limited training data, and provides novel insights by interpreting synthetic data through "relevance" and "novelty" lenses.

## Method Summary
NOMAD introduces a two-pronged approach to training teacher models for synthetic data generation. First, it uses no-prompt-masked training where the model learns from complete instruction-response pairs rather than just responses, addressing the mismatch between training objectives and inference requirements. Second, it discovers that selecting appropriate training subset sizes (rather than using entire datasets) often yields superior synthetic data. The method generates synthetic data using the trained teacher model, applies filtering to remove low-quality samples (particularly coding and long conversations), and trains the student model on a mixture of original and synthetic data. The approach is evaluated across multiple benchmarks including TriviaQA, GSM8K, and BBH with consistent performance improvements.

## Key Results
- NOMAD achieves >4% gains in TriviaQA and >2% in GSM8K with limited training data
- Smaller training subsets (15K) outperform using full datasets (300K) for synthetic data generation
- NOMAD is the only approach to outperform baselines even when synthetic data comprises only 5% of the training set
- The method provides novel insights through "relevance" and "novelty" analysis of synthetic data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-masked training prevents the model from learning how to generate quality prompts
- Mechanism: When models are trained with prompt masking, they only see the response during training but not the prompt. This creates a fundamental mismatch because during inference, the model must generate both prompt and response. The model learns to produce high-quality responses but fails to generate contextually appropriate prompts.
- Core assumption: Learning from complete instruction-response pairs is necessary for generating good synthetic data
- Evidence anchors:
  - [abstract] "standard supervised instruction-finetuned models... are optimized for general question-answering/problem-solving rather than data generation"
  - [section 3] "Traditional instruction fine-tuning focuses on improving response quality by computing loss only on the response part"
  - [corpus] Weak - no direct evidence found in corpus papers

### Mechanism 2
- Claim: Smaller training subsets often produce better synthetic data than using entire datasets
- Mechanism: Using the full dataset causes overfitting to existing patterns, reducing novelty in generated data. Smaller subsets create a balance between relevance (maintaining domain consistency) and novelty (generating diverse new examples). The model learns core patterns without memorizing specific examples.
- Core assumption: There exists an optimal subset size that balances relevance and novelty
- Evidence anchors:
  - [abstract] "proper training set size selection (finding that smaller subsets often yield better supplementary data than using entire datasets)"
  - [section 3] "we discover that selecting a subset of a large available dataset often yields superior supplementary synthetic data"
  - [corpus] Weak - no direct evidence found in corpus papers

### Mechanism 3
- Claim: Data synthesis requires different training objectives than standard language modeling
- Mechanism: Standard SFT optimizes for response quality, while data synthesis requires generating both prompts and responses that are relevant to the target domain. The no-prompt-masked approach trains the model to understand the relationship between prompts and responses, enabling it to generate coherent instruction-response pairs.
- Core assumption: The skills needed for data synthesis differ from those needed for response generation
- Evidence anchors:
  - [abstract] "we propose a paradigm shift named NOMAD by investigating how to specifically train models for data generation, demonstrating that this task differs significantly from training a classical LM"
  - [section 2] "we aim to propose novel methods to train Mt using Xtrain to generate supplementary Xsynthesis"
  - [corpus] Weak - no direct evidence found in corpus papers

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) vs Data Synthesis
  - Why needed here: Understanding the fundamental difference between training for response quality vs training for generating instruction-response pairs
  - Quick check question: Why does prompt-masked training work for standard SFT but not for data synthesis?

- Concept: Relevance vs Novelty tradeoff
  - Why needed here: The paper's core insight is that optimal synthetic data requires balancing these two properties
  - Quick check question: What happens to synthetic data quality when using very large vs very small training subsets?

- Concept: Embedding similarity for data quality measurement
  - Why needed here: The paper uses NormSim to quantify how similar synthetic data is to original data
  - Quick check question: How would you measure whether synthetic data is "too similar" to original data?

## Architecture Onboarding

- Component map:
  - Pretrained teacher model (Mt) -> Trained with NOMAD (no-prompt-masked + subset selection) -> Generates synthetic data -> Filtering (code removal + repeated words) -> Student model (Ms) trained on Xtrain + Xsynthesis

- Critical path:
  1. Train Msynthesis with no-prompt-masked approach on subset of Xtrain
  2. Generate Xsynthesis using trained Msynthesis
  3. Apply filters to remove low-quality samples
  4. Train Ms on mixture of Xtrain + Xsynthesis

- Design tradeoffs:
  - Prompt masking vs no-prompt masking: affects prompt generation quality
  - Training set size: smaller subsets give better novelty but may miss patterns
  - Filtering vs raw generation: filtering improves quality but reduces quantity

- Failure signatures:
  - Low performance improvement: likely due to poor prompt generation or insufficient novelty
  - Performance degradation: indicates synthetic data is too different from original distribution
  - OOD prompts: suggests prompt-masked training was used

- First 3 experiments:
  1. Compare prompt-masked vs no-prompt-masked training on small dataset
  2. Test different training set sizes (10%, 50%, 100% of available data)
  3. Measure synthetic data quality using NormSim similarity scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of the training subset for synthetic data generation when balancing relevance and novelty?
- Basis in paper: Explicit - The paper identifies that "proper training set size selection" is a key factor and discovers that "selecting a subset of a large available dataset often yields superior supplementary synthetic data."
- Why unresolved: While the paper demonstrates that smaller subsets (15K) can outperform using the full dataset (300K) for synthetic data generation, it doesn't establish a precise methodology for determining the optimal subset size across different domains or dataset characteristics.
- What evidence would resolve it: A systematic study varying subset sizes (e.g., 5K, 10K, 20K, 50K) across multiple domains and dataset types, with quantitative analysis of the trade-off curve between relevance and novelty.

### Open Question 2
- Question: How does the performance of synthetic data generated from Xsynthesis compare when mixed with different high-quality datasets other than the original Xtrain?
- Basis in paper: Explicit - The paper mentions in Appendix B.2 that "the performance resulting from training on Xsynthesis alone does not correlate with training on a mixture of Xsynthesis + Xtrain" and poses this as a future question.
- Why unresolved: The paper only evaluates mixing Xsynthesis with the original training data, leaving open the question of whether the generated synthetic data has broader applicability across different datasets or if it's specifically tuned to the original distribution.
- What evidence would resolve it: Experiments mixing Xsynthesis with multiple diverse high-quality datasets (not just the original Xtrain) and comparing performance across different combinations to determine transferability and generalizability.

### Open Question 3
- Question: What are the specific characteristics that distinguish effective synthetic data from ineffective synthetic data when using prompt-masked versus no-prompt-masked training approaches?
- Basis in paper: Explicit - The paper identifies "no-prompt-masked training" as a key factor and shows that prompt-masked training can lead to "low relevance due to lack of exposure to prompts."
- Why unresolved: While the paper demonstrates that no-prompt-masked training performs better, it doesn't provide a detailed analysis of what specific prompt characteristics (e.g., completeness, coherence, alignment with domain style) are most critical for generating effective synthetic data.
- What evidence would resolve it: A detailed qualitative and quantitative analysis of the generated prompts and responses, identifying specific failure modes in prompt-masked training and the features that distinguish high-quality from low-quality synthetic data across both training approaches.

## Limitations

- The paper lacks ablation studies on filtering mechanisms, making it unclear whether observed improvements are due to filtering or the training method itself
- The optimal subset size methodology is not systematic, with selections appearing somewhat arbitrary (e.g., "50% subset") across different datasets
- The NormSim metric for measuring relevance and novelty is novel but lacks external validation to confirm it correlates with actual task performance

## Confidence

**High Confidence Claims:**
- NOMAD improves performance on TriviaQA and GSM8K compared to baselines (supported by multiple experimental settings)
- Prompt-masked training leads to lower-quality synthetic data (clearly demonstrated through experiments)
- There exists an optimal training subset size that balances relevance and novelty (consistently observed across multiple datasets)

**Medium Confidence Claims:**
- The specific subset sizes (50% for TULA, 10% for GPQA) are optimal (based on limited experiments, could vary with different datasets)
- No-prompt-masked training is superior to prompt-masked training for all data synthesis tasks (well-supported but not universally tested)
- Filtering improves synthetic data quality (empirically supported but lacks ablation analysis)

**Low Confidence Claims:**
- The NormSim metric accurately captures the relevance-novelty tradeoff (novel metric without external validation)
- The 5% synthetic data ratio finding generalizes beyond the tested scenarios (single data point without broader exploration)
- All observed improvements are solely due to the proposed training method (no rigorous ablation on individual components)

## Next Checks

1. **Ablation on Filtering Mechanisms**: Systematically test different filtering thresholds and rule sets to determine whether the observed performance improvements are robust to filtering variations. This would validate whether the current filtering approach is optimal or merely sufficient.

2. **Generalization Across Teacher Models**: Repeat the experiments using different teacher model sizes and architectures (e.g., Mistral, Qwen) to assess whether the observed benefits of no-prompt-masked training and subset selection generalize beyond Llama3-8B.

3. **Synthetic Data Scaling Analysis**: Generate synthetic datasets at varying scales (e.g., 2x, 5x, 10x the original training data) to determine whether the performance improvements persist or if there's a point of diminishing returns, and how this interacts with the optimal subset size findings.