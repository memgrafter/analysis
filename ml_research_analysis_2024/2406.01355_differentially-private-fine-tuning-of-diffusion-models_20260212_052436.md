---
ver: rpa2
title: Differentially Private Fine-Tuning of Diffusion Models
arxiv_id: '2406.01355'
source_url: https://arxiv.org/abs/2406.01355
tags:
- privacy
- diffusion
- data
- private
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality synthetic
  images while preserving differential privacy in diffusion models, which are known
  for their substantial memorization capabilities that pose significant privacy risks.
  The authors propose a parameter-efficient fine-tuning strategy using Low-Rank Adaptation
  (LoRA) to minimize the number of trainable parameters, thereby enhancing the privacy-utility
  trade-off.
---

# Differentially Private Fine-Tuning of Diffusion Models

## Quick Facts
- **arXiv ID:** 2406.01355
- **Source URL:** https://arxiv.org/abs/2406.01355
- **Reference count:** 20
- **Key outcome:** Achieves state-of-the-art DP image synthesis with 35% FID improvement on CelebA-64 using only 0.47M trainable parameters

## Executive Summary
This paper addresses the challenge of generating high-quality synthetic images while preserving differential privacy in diffusion models. The authors propose a parameter-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) to minimize the number of trainable parameters, thereby enhancing the privacy-utility trade-off. By pre-training on public data and fine-tuning on private data with DP-SGD, the method achieves significant improvements over previous benchmarks while maintaining strong privacy guarantees.

## Method Summary
The method involves a two-stage training process: first pre-training an auto-encoder and latent diffusion model (LDM) on public data, then fine-tuning the LDM on private data using DP-SGD with LoRA adapters applied to attention blocks and output projection layers. This parameter-efficient approach minimizes the number of trainable parameters to enhance privacy while maintaining generation quality.

## Key Results
- Achieves more than 35% improvement in FID scores over previous benchmarks on CelebA-64
- Reduces trainable parameters to only 0.47M while maintaining high image quality
- Demonstrates effective performance across multiple datasets including MNIST, CIFAR-10, and CelebA

## Why This Works (Mechanism)

### Mechanism 1
Low-rank adaptation (LoRA) in DP fine-tuning minimizes memorization while preserving generation quality. By restricting updates to a low-rank subspace, LoRA limits the number of parameters that can overfit to private data, reducing the model's capacity to memorize specific training samples.

### Mechanism 2
Pre-training on public data followed by fine-tuning on private data with LoRA improves the privacy-utility trade-off. Pre-training allows the model to learn general features, while fine-tuning adapts these features to the specific private dataset without significantly increasing memorization capacity.

### Mechanism 3
Fine-tuning only attention modules and output projection layers is sufficient for maintaining image quality in DP settings. Attention modules learn contextual relationships in images, and output projection layers transform these relationships into final outputs, making them critical for adaptation.

## Foundational Learning

- **Concept:** Differential Privacy (DP) and its implementation via DP-SGD
  - **Why needed here:** Understanding DP is crucial for grasping the privacy guarantees and limitations of the proposed method.
  - **Quick check question:** What is the main difference between standard SGD and DP-SGD in terms of privacy guarantees?

- **Concept:** Diffusion models and their architecture
  - **Why needed here:** The paper focuses on fine-tuning diffusion models with DP, so understanding their architecture is essential.
  - **Quick check question:** How do diffusion models generate images, and what are the key components of their architecture?

- **Concept:** Low-Rank Adaptation (LoRA) and parameter-efficient fine-tuning
  - **Why needed here:** LoRA is the core technique used for efficient fine-tuning in this work.
  - **Quick check question:** How does LoRA reduce the number of trainable parameters, and what are the potential trade-offs?

## Architecture Onboarding

- **Component map:** Public data → Pre-trained autoencoder → Latent diffusion model → Private data → DP-SGD with LoRA → Fine-tuned model
- **Critical path:** 1. Pre-train autoencoder on public data 2. Pre-train LDM on public data 3. Convert private data to latent space 4. Fine-tune LDM on private data using LoRA and DP-SGD
- **Design tradeoffs:** Pre-training on public data improves performance but requires access to large public datasets; minimizing parameters improves privacy but may limit adaptation capacity
- **Failure signatures:** Poor image quality indicates insufficient pre-training or inadequate LoRA adaptation; privacy leakage suggests insufficient noise addition or gradient clipping
- **First 3 experiments:** 1. Fine-tune entire LDM on private data without LoRA for baseline 2. Fine-tune only attention modules with LoRA 3. Fine-tune with different LoRA ranks to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between the number of noise multiplicity steps (k) and training time in DP-LoRA? The paper presents an ablation study showing that increasing k improves FID scores but also significantly increases training time, suggesting a trade-off between image quality and computational efficiency.

### Open Question 2
How does the performance of DP-LoRA vary across different types of image datasets, particularly those with high complexity or domain-specific characteristics? The paper evaluates DP-LoRA on standard benchmark datasets but does not explore its performance on more complex or specialized datasets.

### Open Question 3
What is the impact of varying the rank (r) of LoRA adapters on the privacy-utility trade-off? The ablation study shows that lower ranks achieve the best FID scores while higher ranks lead to significantly worse performance, indicating a threshold beyond which increasing rank is counterproductive.

## Limitations

- The core hypothesis that LoRA's low-rank parameterization is sufficient for DP fine-tuning relies on the assumption that the intrinsic dimensionality of diffusion model adaptation is low
- The effectiveness of pre-training on public data depends heavily on its similarity to private data distributions, which is not systematically analyzed
- The decision to fine-tune only attention modules and output projections is based on non-private settings and lacks direct validation in DP contexts

## Confidence

- **High Confidence:** Experimental results demonstrating significant FID improvements and the overall methodology
- **Medium Confidence:** Mechanism explaining why LoRA improves privacy-utility trade-offs
- **Low Confidence:** Claim that fine-tuning only attention modules and output projections is sufficient for maintaining image quality in DP settings

## Next Checks

1. Conduct an ablation study testing different combinations of components (attention only, ResBlocks only, full model) to validate which components are truly essential for DP fine-tuning
2. Perform experiments varying the similarity between public pre-training data and private fine-tuning data to quantify how domain mismatch affects the privacy-utility trade-off
3. Measure the actual intrinsic dimensionality of the fine-tuning task in DP settings to test whether the low-rank assumption holds for diffusion models under DP constraints