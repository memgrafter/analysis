---
ver: rpa2
title: 'Question-Answering System for Bangla: Fine-tuning BERT-Bangla for a Closed
  Domain'
arxiv_id: '2410.03923'
source_url: https://arxiv.org/abs/2410.03923
tags:
- system
- bengali
- dataset
- language
- bert-bangla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a closed-domain question-answering system for
  Bengali using fine-tuned BERT-Bangla, trained on a dataset of 2500 question-answer
  pairs about Khulna University of Engineering & Technology. The system achieved an
  Exact Match (EM) score of 55.26% and an F1 score of 74.21%, demonstrating effective
  handling of factual queries while highlighting the need for improvement in complex
  or ambiguous questions.
---

# Question-Answering System for Bangla: Fine-tuning BERT-Bangla for a Closed Domain

## Quick Facts
- arXiv ID: 2410.03923
- Source URL: https://arxiv.org/abs/2410.03923
- Authors: Subal Chandra Roy; Md Motaleb Hossen Manik
- Reference count: 15
- Primary result: Closed-domain QA system for Bengali using fine-tuned BERT-Bangla achieved EM 55.26% and F1 74.21%

## Executive Summary
This paper presents a closed-domain question-answering system for Bengali using fine-tuned BERT-Bangla, trained on a dataset of 2500 question-answer pairs about Khulna University of Engineering & Technology. The system achieved an Exact Match (EM) score of 55.26% and an F1 score of 74.21%, demonstrating effective handling of factual queries while highlighting the need for improvement in complex or ambiguous questions. The methodology involved data collection from KUET's website and Wikipedia, dataset creation, fine-tuning BERT-Bangla, and evaluation using standard metrics.

## Method Summary
The system uses BERT-Bangla, a pre-trained transformer model designed for Bengali, fine-tuned on 2500 question-answer pairs about KUET. Data was collected from KUET's website and Wikipedia, creating 100 context paragraphs with 2-5 Q&A pairs each. The model was fine-tuned using learning rate 2e-5, batch size 16, and 3 training epochs on Google Colab with T4 GPU. Evaluation employed Exact Match, F1 score, and perplexity metrics.

## Key Results
- Achieved Exact Match score of 55.26% on Bengali QA tasks
- Achieved F1 score of 74.21% on Bengali QA tasks
- Perplexity score of 5.71 indicates model uncertainty in predictions
- Demonstrated effective handling of factual queries about KUET

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BERT-Bangla on a closed-domain dataset improves QA accuracy for specific Bengali queries.
- Mechanism: BERT-Bangla's pre-training on Bengali text allows it to understand language structure, while fine-tuning on KUET-specific data aligns its weights to domain vocabulary and question patterns.
- Core assumption: The pre-trained model's language understanding transfers well to specialized domains when fine-tuned with sufficient labeled data.
- Evidence anchors:
  - [abstract]: "This paper explores a fine-tuned BERT-Bangla model to address this gap... The system was trained and evaluated with 2500 question-answer pairs generated from curated data."
  - [section]: "The next step was to fine-tune the BERT-Bangla model [7], a pre-trained transformer model designed specifically for the Bengali language... For this research, we fine-tuned BERT-Bangla on the 2500-question dataset."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.531, average citations=0.0. Top related titles include domain-specific QA datasets, suggesting this approach aligns with recent work.

### Mechanism 2
- Claim: Using Exact Match (EM) and F1 scores effectively measures the system's ability to retrieve precise answers in a closed domain.
- Mechanism: EM captures exact answer matches while F1 balances precision and recall, together providing a complete picture of retrieval accuracy and partial match quality.
- Core assumption: These metrics are appropriate for evaluating extractive QA performance on structured, fact-based queries.
- Evidence anchors:
  - [abstract]: "Key metrics, including the Exact Match (EM) score and F1 score, were used for evaluation, achieving scores of 55.26% and 74.21%, respectively."
  - [section]: "After fine-tuning, the system was evaluated using three key metrics: Exact Match (EM), F1 score, and Perplexity as they are mostly used in QA systems [15]."

### Mechanism 3
- Claim: Closed-domain QA systems outperform open-domain systems on specific topics by focusing on domain-specific data and reducing answer space.
- Mechanism: Limiting the system to KUET-related content reduces ambiguity and allows the model to specialize in relevant terminology and context patterns.
- Core assumption: Domain-specific systems can achieve higher accuracy by concentrating on a narrower, well-defined knowledge area.
- Evidence anchors:
  - [abstract]: "These systems are designed to interpret human language queries and provide relevant answers based on their understanding of the text. While some QA systems are open-domain, capable of addressing a wide variety of questions, others focus on more specific areas, known as closed-domain systems [3]."
  - [section]: "Unlike open-domain systems, which require broader knowledge, closed-domain systems demand deep, domain-specific understanding and specialized datasets [8]."

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: BERT-Bangla is a transformer-based model; understanding its architecture is essential for fine-tuning and troubleshooting.
  - Quick check question: How does BERT's bidirectional attention differ from traditional sequential models, and why is this important for QA?

- Concept: Tokenization and subword units in Bengali text
  - Why needed here: Bengali has complex morphology; proper tokenization affects model input and performance.
  - Quick check question: What tokenization strategy does BERT-Bangla use, and how does it handle Bengali-specific characters?

- Concept: Evaluation metrics for extractive QA
  - Why needed here: Understanding EM, F1, and perplexity helps interpret results and guide model improvements.
  - Quick check question: How does the F1 score calculation for QA differ from standard classification F1, and why is this distinction important?

## Architecture Onboarding

- Component map: Data collection -> Dataset creation -> Model fine-tuning -> Evaluation
- Critical path: Data collection → Dataset creation → Model fine-tuning → Evaluation → Iteration
- Design tradeoffs:
  - Dataset size vs. diversity: 2500 pairs provides sufficient training but may limit generalization
  - Computational resources: T4 GPU enables reasonable training time but may limit experimentation
  - Metric selection: EM and F1 provide clear evaluation but may not capture all aspects of QA quality
- Failure signatures:
  - Low EM but high F1: Model understands context but struggles with exact answer extraction
  - High perplexity: Model is uncertain about text predictions, indicating potential data quality issues
  - Inconsistent results across similar queries: Possible overfitting or data imbalance
- First 3 experiments:
  1. Test model on held-out validation set to establish baseline performance before full evaluation
  2. Vary learning rate (1e-5, 2e-5, 3e-5) to find optimal fine-tuning parameters
  3. Evaluate on more complex queries to identify performance gaps and inform dataset expansion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the QA system be enhanced to handle more complex or ambiguous queries in the Bengali language?
- Basis in paper: [explicit] The paper explicitly states that further refinements are needed to improve performance for more complex queries, as the current system struggles with ambiguous or poorly structured questions.
- Why unresolved: The paper identifies the need for improvement but does not provide specific strategies or methods to address this limitation.
- What evidence would resolve it: Implementing and testing additional techniques such as incorporating external knowledge bases, improving context understanding, or using multimodal data could provide insights into enhancing the system's ability to handle complex queries.

### Open Question 2
- Question: What is the impact of expanding the dataset with more diverse queries from multiple domains on the model's generalization capabilities?
- Basis in paper: [explicit] The paper mentions that future work includes expanding the dataset to incorporate more diverse queries from multiple domains to improve the model's generalization.
- Why unresolved: The current dataset is limited to KUET-related queries, and the paper does not explore the effects of a more diverse dataset on the model's performance.
- What evidence would resolve it: Conducting experiments with an expanded dataset covering various domains and evaluating the model's performance across these domains would provide evidence of its generalization capabilities.

### Open Question 3
- Question: How effective would integrating external knowledge graphs or databases be in addressing more complex, context-dependent queries?
- Basis in paper: [explicit] The paper suggests investigating the integration of external knowledge graphs or databases to address more complex, context-dependent queries as part of future work.
- Why unresolved: The paper does not provide any results or analysis on the effectiveness of integrating external knowledge sources.
- What evidence would resolve it: Implementing a system that integrates external knowledge graphs and evaluating its performance on complex queries would provide evidence of its effectiveness in improving the QA system.

## Limitations

- Dataset size and diversity: 2500 pairs provide moderate training but may limit generalization and introduce bias toward KUET-specific content
- Performance ceiling: EM 55.26% and F1 74.21% indicate incomplete accuracy, with significant gap between exact and partial match performance
- Computational reproducibility: Lacks specific details about training duration, memory usage, and hyperparameter tuning processes

## Confidence

**High Confidence**: The mechanism of fine-tuning pre-trained BERT-Bangla on domain-specific data is well-established in the literature. The use of standard QA evaluation metrics (EM, F1) is appropriate and commonly accepted in the field. The data collection methodology from web sources is straightforward and verifiable.

**Medium Confidence**: The specific performance scores (55.26% EM, 74.21% F1, 5.71 perplexity) are reported but lack comparative benchmarks against other Bengali QA systems or ablation studies showing the impact of different hyperparameters.

**Low Confidence**: The paper does not address potential limitations of the BERT-Bangla model itself, such as its training corpus size or any known weaknesses in handling Bengali language nuances. The absence of error analysis makes it difficult to understand systematic failure patterns.

## Next Checks

1. **Dataset quality audit**: Conduct a systematic review of the 2500 question-answer pairs to assess annotation consistency, answer span quality, and coverage of different query types.

2. **Hyperparameter sensitivity analysis**: Systematically vary learning rates (1e-5, 2e-5, 3e-5), batch sizes (8, 16, 32), and training epochs (2-5) to determine optimal configurations.

3. **Cross-domain transferability test**: Evaluate the fine-tuned model on a held-out set of questions about similar educational institutions in Bangladesh to assess generalization capabilities.