---
ver: rpa2
title: Changing the Training Data Distribution to Reduce Simplicity Bias Improves
  In-distribution Generalization
arxiv_id: '2404.17768'
source_url: https://arxiv.org/abs/2404.17768
tags:
- training
- vvvd
- vvve
- useful
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether modifying the training data distribution
  can improve in-distribution generalization performance. The authors analyze feature
  learning dynamics in two-layer CNNs and prove that sharpness-aware minimization
  (SAM) learns features more uniformly than gradient descent (GD), particularly early
  in training, due to reduced simplicity bias.
---

# Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization

## Quick Facts
- arXiv ID: 2404.17768
- Source URL: https://arxiv.org/abs/2404.17768
- Reference count: 40
- Primary result: Changing training data distribution to reduce simplicity bias improves in-distribution generalization across multiple datasets and architectures

## Executive Summary
This paper investigates whether modifying the training data distribution can improve in-distribution generalization performance. The authors analyze feature learning dynamics in two-layer CNNs and prove that sharpness-aware minimization (SAM) learns features more uniformly than gradient descent (GD), particularly early in training, due to reduced simplicity bias. Based on this theoretical analysis, they propose USEFUL, a method that identifies examples with fast-learnable features early in training and upsamples the remaining examples once to encourage more uniform feature learning. Experiments demonstrate that USEFUL effectively improves generalization performance across multiple datasets (CIFAR10, CIFAR100, STL10, CINIC10, Tiny-ImageNet) and architectures (ResNet18/34, VGG19, DenseNet121) when combined with various optimizers and data augmentation strategies, achieving state-of-the-art results.

## Method Summary
USEFUL is a method that modifies the training data distribution to reduce simplicity bias and improve in-distribution generalization. The approach involves training a model for early epochs (typically 5-10% of total), then clustering examples based on their model outputs to identify fast-learnable and slow-learnable features. The method upsamples examples with slow-learnable features by a factor of 2, then restarts training from scratch on this modified dataset. This approach is theoretically motivated by showing that SAM learns features more uniformly than GD early in training, and empirically validated across multiple datasets and architectures.

## Key Results
- USEFUL improves test accuracy across CIFAR10, CIFAR100, STL10, CINIC10, and Tiny-ImageNet datasets
- The method achieves state-of-the-art results when combined with various optimizers (SGD, SAM) and architectures (ResNet18/34, VGG19, DenseNet121)
- USEFUL is effective with different data augmentation strategies including TrivialAugment
- The approach consistently outperforms both standard SGD and SAM baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM learns features more uniformly than GD early in training, reducing simplicity bias.
- Mechanism: By minimizing loss and sharpness jointly, SAM creates a flatter optimization trajectory that forces earlier inclusion of slower-learnable features.
- Core assumption: The data distribution contains both fast- and slow-learnable features, and neural networks exhibit simplicity bias that prioritizes fast-learnable features.
- Evidence anchors:
  - [abstract]: "SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD."
  - [section]: "we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD."
  - [corpus]: Weak - corpus papers discuss SAM but don't specifically confirm uniform feature learning early in training.
- Break condition: If the data lacks slow-learnable features or the model architecture doesn't exhibit simplicity bias, the uniform learning effect disappears.

### Mechanism 2
- Claim: Examples with fast-learnable features are separable from others based on model output early in training.
- Mechanism: During early training, the model's output for fast-learnable examples becomes distinguishable from slow-learnable examples, enabling clustering-based identification.
- Core assumption: Fast-learnable features are learned earlier and create distinct patterns in model output that clustering can detect.
- Evidence anchors:
  - [abstract]: "we also show that examples containing features that are learned early are separable from the rest based on the model's output."
  - [section]: "we rigorously prove that the model output for examples containing features that are learned early are separable from the rest of examples in their class, early in training."
  - [corpus]: Weak - corpus papers discuss feature learning but don't specifically address separability of fast vs slow learnable examples.
- Break condition: If features are learned at similar speeds or clustering fails to distinguish between example types, the method cannot identify which examples to upsample.

### Mechanism 3
- Claim: Upsampling slow-learnable examples once accelerates their learning, creating more uniform feature learning.
- Mechanism: By increasing the frequency of slow-learnable examples in training, the optimizer encounters them more often, forcing earlier learning of these features.
- Core assumption: Feature learning speed is influenced by example frequency, and a single upsampling pass can significantly alter learning dynamics.
- Evidence anchors:
  - [abstract]: "Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias."
  - [section]: "we prove that examples containing features that are learned early by GD is separable from the rest of examples in their class. Then, we propose changing the data distribution by (i) identifying a cluster of examples with similar model output early in training, (ii) upsampling the remaining examples once to speed up their learning, and (iii) restarting training on the modified training distribution."
  - [corpus]: Weak - corpus papers discuss data distribution but don't specifically confirm one-shot upsampling effects.
- Break condition: If the upsampling factor is too large or too small, or if the model overfits to the upsampled examples, generalization may not improve.

## Foundational Learning

- Concept: Simplicity bias in gradient descent
  - Why needed here: Understanding how GD prioritizes simple solutions over complex ones is crucial for grasping why USEFUL works.
  - Quick check question: Does gradient descent always learn the simplest features first, or can this order vary?

- Concept: Sharpness-aware minimization (SAM)
  - Why needed here: SAM's mechanism for finding flatter minima is the theoretical foundation for why it learns features more uniformly.
  - Quick check question: How does SAM's perturbation of parameters during training differ from standard gradient descent?

- Concept: Feature learning dynamics in neural networks
  - Why needed here: The paper's theoretical analysis depends on understanding how different features are learned at different speeds during training.
  - Quick check question: What factors determine whether a feature is learned quickly or slowly by a neural network?

## Architecture Onboarding

- Component map: Data preprocessing -> Early training phase -> Clustering -> Upsampling -> Final training

- Critical path:
  1. Train model for early epochs (t ≈ 5-10% of total)
  2. Perform clustering on model outputs per class
  3. Identify slow-learnable example cluster
  4. Upsample remaining examples by factor of 2
  5. Restart training on modified dataset

- Design tradeoffs:
  - Early epoch selection: Too early → poor clustering; too late → missed opportunity to correct simplicity bias
  - Upsampling factor: Too low → insufficient effect; too high → distribution shift
  - Clustering method: K-means is simple but may miss nuanced differences in feature learning

- Failure signatures:
  - No improvement in test accuracy → incorrect early epoch selection or poor clustering
  - Worse performance than baseline → too aggressive upsampling or distribution shift
  - Inconsistent results across runs → sensitivity to random initialization or clustering instability

- First 3 experiments:
  1. Baseline SGD/SAM training without any modifications
  2. USEFUL with varying early epoch (t = 4, 5, 6, 7, 8) to find optimal separation point
  3. USEFUL with different upsampling factors (1.5x, 2x, 2.5x, 3x) to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of USEFUL vary across different types of model architectures beyond CNNs, such as Transformers and MLPs, in terms of feature learning dynamics?
- Basis in paper: [explicit] The paper mentions that USEFUL is effective across different architectures like ViT and MLP, but does not provide detailed analysis on feature learning dynamics for these models.
- Why unresolved: The paper provides results showing effectiveness but lacks a deep dive into how USEFUL impacts feature learning dynamics specifically for non-CNN architectures.
- What evidence would resolve it: Detailed experiments and analysis comparing feature learning dynamics in Transformers and MLPs with USEFUL versus without, including metrics on feature diversity and uniformity.

### Open Question 2
- Question: What are the long-term impacts of using USEFUL on model generalization in scenarios with continuous data distribution shifts?
- Basis in paper: [inferred] The paper suggests that USEFUL improves in-distribution generalization and shows some benefits in OOD settings, but does not explore continuous data distribution shifts.
- Why unresolved: The paper does not address scenarios where data distribution changes over time, leaving questions about USEFUL's adaptability and effectiveness in such environments.
- What evidence would resolve it: Experiments and analysis showing the performance of USEFUL over time in environments with gradual or continuous data distribution shifts, measuring both ID and OOD generalization.

### Open Question 3
- Question: How does the choice of clustering method affect the identification of fast-learnable and slow-learnable features in USEFUL?
- Basis in paper: [explicit] The paper uses k-means clustering for separating examples but acknowledges that other methods might also be effective.
- Why unresolved: While k-means is used, the paper does not explore or compare other clustering methods, leaving questions about the robustness and optimality of the chosen method.
- What evidence would resolve it: Comparative studies using different clustering algorithms (e.g., hierarchical clustering, DBSCAN) to assess their impact on the performance and accuracy of USEFUL in identifying fast-learnable features.

## Limitations

- The theoretical analysis is limited to two-layer CNNs with linear classifiers, raising questions about generalization to deeper architectures.
- While empirical results are strong, the exact mechanisms by which upsampling slow-learnable examples improves generalization could benefit from more rigorous theoretical grounding.
- The optimal early epoch selection appears dataset-dependent and was determined through grid search, suggesting potential sensitivity to hyperparameter choices.

## Confidence

- **High confidence**: The core claim that SAM learns features more uniformly than GD is supported by both theoretical analysis and empirical evidence across multiple datasets and architectures.
- **Medium confidence**: The USEFUL method's effectiveness is demonstrated empirically, but the theoretical justification for why upsampling improves generalization is less complete.
- **Low confidence**: The clustering-based identification of slow-learnable examples may not generalize well to more complex datasets or architectures where feature learning dynamics differ significantly.

## Next Checks

1. Test USEFUL on datasets with more complex feature hierarchies (e.g., ImageNet) to verify the method scales beyond simpler image classification tasks.
2. Implement ablation studies where clustering is replaced with random example selection to quantify how much improvement comes specifically from identifying slow-learnable examples versus simply modifying the data distribution.
3. Analyze feature representations using techniques like linear probing or representational similarity analysis to directly measure how USEFUL affects feature learning uniformity across different layers of the network.