---
ver: rpa2
title: Understanding Privacy Risks of Embeddings Induced by Large Language Models
arxiv_id: '2404.16587'
source_url: https://arxiv.org/abs/2404.16587
tags:
- text
- embedding
- attack
- embeddings
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates privacy risks of text embeddings generated
  by large language models (LLMs), showing they can reconstruct original texts and
  predict sensitive attributes with high accuracy. Using GPT-2 as attack models, the
  study finds that larger and better-trained LLMs, along with more expressive embedding
  models, significantly improve text reconstruction (BLEU-1 up to 0.5823) and attribute
  prediction (accuracy up to 0.941) compared to traditional models.
---

# Understanding Privacy Risks of Embeddings Induced by Large Language Models

## Quick Facts
- arXiv ID: 2404.16587
- Source URL: https://arxiv.org/abs/2404.16587
- Reference count: 40
- One-line primary result: Text embeddings from large language models can reconstruct original texts and predict sensitive attributes with high accuracy.

## Executive Summary
This paper investigates privacy risks associated with text embeddings generated by large language models (LLMs). The authors demonstrate that embeddings can be used to reconstruct original texts and infer sensitive attributes like nationality or occupation with high accuracy. Using GPT-2 variants as attack models, they show that larger and better-trained LLMs, combined with more expressive embedding models, significantly improve both text reconstruction (BLEU-1 up to 0.5823) and attribute prediction (accuracy up to 0.941). The study finds that shorter texts are particularly vulnerable, and reconstruction accuracy remains high even for out-of-distribution texts. The authors suggest protective measures such as text lengthening and improved privacy-preserving embedding models.

## Method Summary
The study employs a fine-tuning approach where GPT-2 models are trained to reconstruct original text from embeddings generated by various embedding models (SimCSE, BGE-Large-en, E5-Large-v2). The attack models are fine-tuned on (embedding, text) pairs from multiple datasets, then used to generate reconstructed text via beam search. Attribute prediction is performed by calculating cosine similarity between reconstructed text embeddings and candidate attribute embeddings, avoiding the need for labeled attribute prediction data. Evaluation uses standard metrics including BLEU-1, ROUGE-1 for reconstruction, and accuracy for attribute prediction.

## Key Results
- Larger attack models and more training data significantly improve text reconstruction accuracy (BLEU-1 up to 0.5823)
- More expressive embedding models (BGE-Large-en, E5-Large-v2) yield higher reconstruction accuracy than less expressive ones (SimCSE)
- Attribute prediction via cosine similarity achieves high accuracy (up to 0.941) without requiring supervised training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger attack models fine-tuned on more data achieve higher BLEU-1 and ROUGE-1 scores for text reconstruction.
- Mechanism: Bigger models with more parameters capture more complex dependencies in the embedding space, and more training data improves their ability to generalize the mapping from embeddings to text.
- Core assumption: Text embeddings contain sufficient semantic information to allow reconstruction, and the attack model architecture is expressive enough to model this mapping.
- Evidence anchors:
  - [abstract] "Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models"
  - [section] "Table 2 illustrate that the size of the training datasets and the models have a considerable influence on the reconstruction accuracy"
  - [corpus] Weak—no direct citation of reconstruction BLEU scores in neighbors.
- Break condition: If embeddings are heavily perturbed or compressed beyond recoverable detail, or if the attack model lacks sufficient capacity relative to the embedding model's expressiveness.

### Mechanism 2
- Claim: Text embeddings from more expressive embedding models yield higher reconstruction accuracy than less expressive ones.
- Mechanism: More expressive embedding models capture richer semantic content during encoding, providing more information for the attack model to invert back into text.
- Core assumption: Embedding expressiveness correlates with retained semantic detail that can be decoded.
- Evidence anchors:
  - [abstract] "larger and better-trained LLMs, along with more expressive embedding models, significantly improve text reconstruction"
  - [section] "larger embedding models, such as BGE-Large-en and E5-Large-v2, enable more effective text reconstruction compared to others like SimCSE"
  - [corpus] Weak—no direct comparison of embedding expressiveness in neighbors.
- Break condition: If the embedding model deliberately removes recoverable information, reconstruction quality drops.

### Mechanism 3
- Claim: Attribute prediction via cosine similarity between reconstructed text embeddings and attribute embeddings achieves high accuracy without needing supervised training.
- Mechanism: The attack model reconstructs text from embeddings, and an external embedding model projects both the reconstructed text and candidate attribute values into the same vector space. The closest vector in cosine space is taken as the predicted attribute.
- Core assumption: Reconstructed text preserves enough semantic content for the external embedding model to generate embeddings close to the original text's embedding, and attributes are sufficiently discriminative in embedding space.
- Evidence anchors:
  - [abstract] "The ability to infer attributes like nationality or occupation from embeddings without training data underscores a major privacy threat"
  - [section] "we predicted the sensitive information from text embedding by selecting the attribute that exhibits the highest cosine similarity between text embedding and its embedding"
  - [corpus] Weak—no explicit mention of attribute inference methods in neighbors.
- Break condition: If the reconstructed text is too noisy or incomplete, the attribute embedding similarity becomes unreliable.

## Foundational Learning

- Concept: **Embedding models map text to high-dimensional vectors preserving semantic content**
  - Why needed here: The privacy risk stems from the fact that embeddings retain enough information to reconstruct original text or infer attributes.
  - Quick check question: What property of embeddings makes them vulnerable to reconstruction attacks?

- Concept: **Fine-tuning attack models on text-embedding pairs to learn inverse mapping**
  - Why needed here: The attack model must learn to invert the embedding process; fine-tuning is the mechanism for acquiring this capability.
  - Quick check question: How does the attack model learn to reconstruct text from embeddings?

- Concept: **Cosine similarity as a metric for comparing embeddings in attribute prediction**
  - Why needed here: Attribute prediction relies on finding the most similar embedding among candidates; cosine similarity is the standard measure.
  - Quick check question: Why is cosine similarity used instead of Euclidean distance for embedding comparison?

## Architecture Onboarding

- Component map: Target embedding model -> Attack model (GPT-2) -> External embedding model -> Evaluation pipeline
- Critical path:
  1. Query target embedding model to get embeddings
  2. Fine-tune attack model on (embedding, text) pairs
  3. Generate reconstructed text using attack model + beam search
  4. Compute evaluation metrics or perform attribute inference
- Design tradeoffs:
  - Model size vs. computational cost: Larger attack models yield better accuracy but require more resources
  - Training data volume vs. overfitting: More data improves generalization but increases cost
  - Embedding expressiveness vs. privacy: More expressive embeddings aid reconstruction but worsen privacy
- Failure signatures:
  - Low BLEU/ROUGE scores despite fine-tuning → embedding lacks sufficient information or attack model capacity insufficient
  - High variance in reconstructed text across trials → temperature setting too high or model unstable
  - Attribute prediction accuracy near random → reconstructed text too noisy or attribute embeddings not discriminative
- First 3 experiments:
  1. Reconstruct a small sample of texts from SimCSE embeddings using GPT-2 and measure BLEU-1
  2. Repeat reconstruction with BGE-Large-en embeddings; compare BLEU-1 scores
  3. Perform attribute prediction on wiki-bio dataset using reconstructed texts and measure accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop effective privacy-preserving embedding models that maintain high utility without compromising security?
- Basis in paper: [explicit] The paper discusses developing embedding models that produce high-quality embeddings difficult to reverse-engineer into original text, balancing privacy and utility.
- Why unresolved: Current privacy measures like perturbation techniques and encryption methods either reduce embedding utility or increase computational costs, indicating a need for innovative solutions.
- What evidence would resolve it: Experimental results showing embedding models that protect privacy while maintaining high performance on downstream tasks without significant computational overhead.

### Open Question 2
- Question: How does the scale of attack models beyond 10 billion parameters affect the privacy leakage of embeddings?
- Basis in paper: [explicit] The authors acknowledge not using attack models exceeding 10 billion parameters due to training costs, but anticipate similar outcomes with larger models.
- Why unresolved: The study's limitations on model size prevent understanding the full impact of larger attack models on privacy leakage.
- What evidence would resolve it: Comparative analysis of privacy leakage across attack models of varying sizes, including those beyond 10 billion parameters.

### Open Question 3
- Question: What are the specific conditions under which text lengthening effectively protects embedding privacy?
- Basis in paper: [explicit] The paper suggests that extending the length of short texts before embedding could fortify the security of released embeddings.
- Why unresolved: The paper does not provide a concrete mechanism or quantify the effectiveness of text lengthening as a privacy protection strategy.
- What evidence would resolve it: Empirical studies measuring the impact of text lengthening on embedding privacy across different types of texts and embedding models.

## Limitations
- Study focuses only on open-source embedding models, leaving proprietary models' vulnerability unexplored
- Does not investigate the impact of embedding dimensionality reduction or compression techniques on privacy risks
- Limited temporal analysis of how embedding privacy vulnerabilities evolve across model versions

## Confidence

**High Confidence:** Baseline Reconstruction Feasibility - The claim that text embeddings can be used to reconstruct original text with measurable accuracy is strongly supported by empirical results (BLEU-1 up to 0.5823, ROUGE-1 up to 0.5770).

**Medium Confidence:** Scaling Effects of Attack Models - The assertion that larger attack models and more training data improve reconstruction accuracy is reasonably supported, but specific scaling relationships are not thoroughly characterized.

**Low Confidence:** Real-world Deployment Scenarios - The study's findings about short text vulnerability and out-of-distribution performance are suggestive but limited in scope, potentially not capturing real-world complexity.

## Next Checks

**Check 1:** Evaluate the same attack methodology against a commercial embedding API (e.g., OpenAI's text-embedding-ada-002 or Cohere's embedding models) to determine whether privacy risks generalize beyond open-source implementations.

**Check 2:** Apply dimensionality reduction techniques (PCA to 256/512 dimensions, product quantization) to embeddings from the best-performing open-source models and re-run reconstruction and attribute prediction tasks to quantify how practical optimization techniques affect privacy vulnerability.

**Check 3:** Replicate attack experiments on embeddings generated by different versions of the same embedding model to assess whether privacy vulnerabilities persist or change over time, and evaluate whether archived embeddings remain vulnerable to newer attack models.