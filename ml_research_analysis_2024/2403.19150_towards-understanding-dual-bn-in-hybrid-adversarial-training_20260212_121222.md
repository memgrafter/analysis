---
ver: rpa2
title: Towards Understanding Dual BN In Hybrid Adversarial Training
arxiv_id: '2403.19150'
source_url: https://arxiv.org/abs/2403.19150
tags:
- adversarial
- dual
- clean
- hybrid-at
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the common belief that disentangling normalization
  statistics (NS) is the primary reason Dual BN improves hybrid adversarial training
  (Hybrid-AT). Through controlled experiments, the authors demonstrate that disentangling
  affine parameters (AP) plays a more significant role than disentangling NS in achieving
  robustness.
---

# Towards Understanding Dual BN In Hybrid Adversarial Training

## Quick Facts
- arXiv ID: 2403.19150
- Source URL: https://arxiv.org/abs/2403.19150
- Reference count: 14
- Primary result: Dual BN's main benefit comes from disentangling affine parameters, not normalization statistics, in hybrid adversarial training

## Executive Summary
This work challenges the common belief that disentangling normalization statistics (NS) is the primary reason Dual BN improves hybrid adversarial training (Hybrid-AT). Through controlled experiments, the authors demonstrate that disentangling affine parameters (AP) plays a more significant role than disentangling NS in achieving robustness. Specifically, using two sets of AP without disentangling NS achieves comparable results to full Dual BN, and the adversarial-clean domain gap is not as large as previously claimed. These findings lead to a new "two-task hypothesis" for Hybrid-AT, framing the clean and adversarial branches as separate tasks that benefit from domain-specific parameters. This hypothesis provides a unified framework for various Hybrid-AT improvements, including Dual BN, adapters, and trades-AT.

## Method Summary
The paper investigates the role of disentangling normalization statistics (NS) versus affine parameters (AP) in Dual BN for hybrid adversarial training. The authors conduct experiments on CIFAR10 using ResNet18, comparing different BN configurations including Cross-BN, Dual BN variants with different combinations of NS and AP disentangling, and Dual Linear setups. They train with both clean and adversarial samples using PGD attacks, evaluating clean accuracy, PGD-10 accuracy, and AutoAttack (AA) accuracy. The experiments systematically isolate the effects of NS and AP disentangling to determine which contributes more to robustness.

## Key Results
- Disentangling affine parameters (AP) in Dual BN is more important than disentangling normalization statistics (NS) for achieving robustness in Hybrid-AT
- Using two sets of AP without disentangling NS can achieve comparable performance to full Dual BN
- The adversarial-clean domain gap is not as large as previously claimed, and is comparable to noisy-clean domain gap under similar perturbation magnitudes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling affine parameters (AP) in Dual BN is more important than disentangling normalization statistics (NS) for achieving robustness in Hybrid-AT.
- Mechanism: Dual BN introduces two sets of AP (γ and β for each branch). When each branch uses its own AP, the model can independently adjust the scaling and shifting of normalized features for clean and adversarial domains, improving robustness. Simply using two sets of AP without disentangling NS can achieve comparable performance to full Dual BN.
- Core assumption: The adversarial-clean domain gap is not as large as previously thought, and AP adjustment is sufficient to handle the domain differences.
- Evidence anchors:
  - [abstract] "we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training."
  - [section] "we demonstrate that two AP sets can achieve comparable performance to the original Dual BN"
  - [corpus] Weak - no direct corpus evidence, but related to general BN understanding.
- Break condition: If the adversarial-clean domain gap is actually very large and NS differences are critical for robustness, then this mechanism would break.

### Mechanism 2
- Claim: The adversarial-clean domain gap is not as large as previously claimed, and is comparable to noisy-clean domain gap under similar perturbation magnitudes.
- Mechanism: Prior work visualized NS differences between BNadv and BNclean, but this visualization had a hidden flaw - it didn't account for different AP when calculating NS. When AP is fixed, NS differences between clean and adversarial samples become much smaller, indicating a smaller domain gap than previously thought.
- Core assumption: The large NS differences in prior visualizations were primarily due to AP inconsistency rather than domain differences.
- Evidence anchors:
  - [section] "the visualization in Xie & Yuille (2020) ignores the influence of different AP when calculating NS. After fixing this hidden flaw, we demonstrate that the adversarial-clean domain gap is not as large as claimed in prior work."
  - [section] "there is no significant difference between adversarial-clean domain gap and noisy-clean counterpart"
  - [corpus] Weak - no direct corpus evidence for this specific claim.
- Break condition: If future work demonstrates that the adversarial-clean domain gap remains large even after accounting for AP differences, this mechanism would break.

### Mechanism 3
- Claim: The two-task hypothesis provides a unified framework for Hybrid-AT improvements, framing clean and adversarial branches as separate tasks that benefit from domain-specific parameters.
- Mechanism: Hybrid-AT inherently involves two tasks - clean accuracy and robustness. Different methods (Dual BN, adapters, trades-AT) all address this two-task conflict by providing domain-specific parameters or regularization to minimize the gap between the two tasks.
- Core assumption: The success of various Hybrid-AT methods can be explained by their ability to handle the two-task conflict, rather than just domain separation.
- Evidence anchors:
  - [abstract] "We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement."
  - [section] "we propose a two-task hypothesis to replace the two-domain hypothesis... This hypothesis offers empirical foundations and a unified framework for Hybrid-AT improvements"
  - [corpus] Weak - no direct corpus evidence, but aligns with general multitask learning understanding.
- Break condition: If future work shows that methods successful in Hybrid-AT cannot be explained by the two-task framework, this mechanism would break.

## Foundational Learning

- Concept: Batch Normalization (BN) mechanics
  - Why needed here: The paper extensively discusses BN, NS, and AP. Understanding how BN works is crucial to follow the arguments about disentangling NS vs AP.
  - Quick check question: What are the two main components of BN that the paper distinguishes between, and what does each do?

- Concept: Adversarial training (AT) and hybrid adversarial training (Hybrid-AT)
  - Why needed here: The paper is specifically about Hybrid-AT, which trains on both clean and adversarial samples. Understanding the difference between AT and Hybrid-AT is important.
  - Quick check question: How does Hybrid-AT differ from standard adversarial training in terms of training data?

- Concept: Domain adaptation and domain gap concepts
  - Why needed here: The paper challenges the two-domain hypothesis and discusses domain gaps. Understanding these concepts is necessary to follow the arguments.
  - Quick check question: What is a domain gap in machine learning, and why is it relevant to the discussion of clean vs adversarial samples?

## Architecture Onboarding

- Component map:
  - Base model (e.g., ResNet18) -> Batch Normalization layers (BNclean and BNadv for Hybrid-AT with Dual BN) -> Affine parameters (γ and β) within each BN -> Training procedure with two branches (clean and adversarial) -> Loss function combining clean and adversarial losses

- Critical path:
  1. Input samples (clean and adversarial)
  2. Separate BN processing for each branch (with potentially different NS and AP)
  3. Model forward pass
  4. Loss computation
  5. Parameter updates (shared weights except for AP in Dual BN)

- Design tradeoffs:
  - Single BN vs Dual BN: Simplicity vs potential robustness improvement
  - Disentangling NS vs AP: Computational cost vs performance impact
  - Branch-specific vs shared parameters: Model capacity vs generalization

- Failure signatures:
  - Zero robustness despite using Dual BN (indicates AP not properly set for adversarial branch)
  - Performance degradation when swapping NS between branches (indicates NS differences matter more than thought)
  - No improvement with Dual AP in other normalization methods (indicates BN-specific behavior)

- First 3 experiments:
  1. Implement Cross-BN (using BNclean for adversarial branch and vice versa) to verify if Cross-BN achieves comparable performance to Self-BN.
  2. Implement Setup1 (two AP sets with shared NS) and Setup2 (two NS sets with shared AP) to isolate the effects of AP vs NS disentangling.
  3. Implement Dual Linear (shared BN but dual linear classifiers) to test the two-task hypothesis beyond BN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is disentangling normalization statistics (NS) beneficial for adversarial training under specific conditions?
- Basis in paper: [explicit] The paper finds that disentangling NS can achieve comparable robustness to Dual BN under certain conditions like small perturbations (ε=8/255), but this benefit narrows significantly for large perturbations (ε=16/255).
- Why unresolved: The paper does not explore the full range of perturbation sizes or different datasets to determine if there are other conditions under which disentangling NS is beneficial.
- What evidence would resolve it: Experiments varying perturbation sizes, datasets, and other hyperparameters to identify specific conditions where disentangling NS is advantageous.

### Open Question 2
- Question: What is the underlying mechanism that makes two sets of affine parameters (AP) more effective than disentangling normalization statistics (NS) in Hybrid-AT?
- Basis in paper: [inferred] The paper demonstrates that two sets of AP achieve comparable results to Dual BN, suggesting that AP plays a more significant role than NS. However, the exact mechanism is not fully explained.
- Why unresolved: The paper focuses on the empirical findings but does not provide a theoretical explanation for why AP is more important than NS.
- What evidence would resolve it: Further theoretical analysis and experiments investigating the relationship between AP and NS in the context of adversarial training.

### Open Question 3
- Question: Can the two-task hypothesis be extended to other types of adversarial training beyond Hybrid-AT?
- Basis in paper: [explicit] The paper proposes the two-task hypothesis as a unified framework for Hybrid-AT improvements, including Dual BN, adapters, and trades-AT. However, it does not explore its applicability to other adversarial training methods.
- Why unresolved: The paper does not investigate whether the two-task hypothesis can be generalized to other adversarial training frameworks or different types of adversarial attacks.
- What evidence would resolve it: Experiments applying the two-task hypothesis to other adversarial training methods and evaluating its effectiveness in improving robustness against different types of attacks.

## Limitations

- The empirical evidence relies heavily on ablation studies within a single architecture (ResNet18) and dataset (CIFAR10)
- The visualization of NS differences depends on specific methodological choices about how NS differences are calculated
- The two-task hypothesis remains largely empirical without theoretical grounding explaining why AP disentangling should be more effective than NS disentangling

## Confidence

**High Confidence**: The experimental results showing that Cross-BN performs comparably to Self-BN, and that Dual AP without disentangled NS achieves similar performance to full Dual BN.

**Medium Confidence**: The claim that the adversarial-clean domain gap is comparable to noisy-clean domain gap. While the visualizations support this, the analysis depends on specific methodological choices about how NS differences are calculated.

**Low Confidence**: The two-task hypothesis as a comprehensive explanation for all Hybrid-AT improvements. While the hypothesis provides a useful framework, it lacks theoretical justification and may not explain all observed phenomena in the literature.

## Next Checks

1. **Cross-architecture validation**: Replicate the AP vs NS disentangling experiments on different architectures (e.g., WideResNet, EfficientNet) and datasets (e.g., SVHN, TinyImageNet) to test the generalizability of the findings.

2. **Theoretical analysis**: Develop a mathematical framework explaining why AP disentangling provides more benefit than NS disentangling in the context of adversarial training, potentially through analyzing the optimization landscape or gradient flow.

3. **Real-world robustness testing**: Evaluate the proposed Dual AP setup against realistic adversarial scenarios including adaptive attacks, distribution shifts, and corrupted inputs to verify that the robustness gains translate beyond controlled PGD attacks.