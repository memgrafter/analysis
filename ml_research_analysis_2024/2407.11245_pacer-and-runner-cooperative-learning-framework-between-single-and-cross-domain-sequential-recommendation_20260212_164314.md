---
ver: rpa2
title: 'Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain
  Sequential Recommendation'
arxiv_id: '2407.11245'
source_url: https://arxiv.org/abs/2407.11245
tags:
- domains
- domain
- cdsr
- transfer
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses negative transfer in Cross-Domain Sequential
  Recommendation (CDSR), where jointly training on multiple domains can degrade performance
  in certain domains due to domain dissimilarity or data sparsity. The proposed SyNCRec
  model estimates domain-specific negative transfer gaps by comparing single- and
  cross-domain losses, and uses these gaps to adaptively down-weight loss contributions
  from poorly transferring domains.
---

# Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2407.11245
- Source URL: https://arxiv.org/abs/2407.11245
- Reference count: 40
- One-line primary result: Achieves 21.4% increase in click-through rate in online deployment

## Executive Summary
This paper addresses negative transfer in Cross-Domain Sequential Recommendation (CDSR), where jointly training on multiple domains can degrade performance in certain domains due to domain dissimilarity or data sparsity. The proposed SyNCRec model estimates domain-specific negative transfer gaps by comparing single- and cross-domain losses, and uses these gaps to adaptively down-weight loss contributions from poorly transferring domains. It also maximizes mutual information between single- and cross-domain representations to facilitate beneficial knowledge transfer. Extensive experiments on two real-world datasets across ten domains show that SyNCRec outperforms 25 state-of-the-art baselines, with notable improvements in domains affected by negative transfer.

## Method Summary
SyNCRec uses an asymmetric cooperative network (ACMoE) with Mixture-of-Experts to decouple single-domain sequential recommendation (SDSR) and CDSR tasks, enabling accurate computation of domain-specific negative transfer gaps. The model compares SDSR and CDSR losses to calculate negative transfer gaps, which are then used to adaptively weight cross-domain prediction losses through a loss correction module (LC-NTG). An auxiliary loss (SC-MIM) maximizes mutual information between SDSR and CDSR representations to enhance beneficial knowledge transfer. The model is trained with multi-task learning using pairwise ranking loss and shows significant improvements over baselines in both offline experiments and online deployment.

## Key Results
- Outperforms 25 state-of-the-art baselines across ten domains on two real-world datasets
- Achieves notable improvements in domains affected by negative transfer
- Online deployment in personal assistant service achieved 21.4% increase in click-through rate
- Shows robust performance with adaptive loss weighting mitigating negative transfer effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric Cooperative Network (ACMoE) explicitly decouples expert networks for SDSR and CDSR tasks, enabling accurate computation of domain-specific negative transfer gaps.
- Mechanism: The ACMoE layer uses a stop-gradient operation to prevent certain experts from being updated by the single-domain sequence, while others are stopped for the cross-domain sequence. This decoupling allows both tasks to be implemented without mutual interference, contributing to accurate computation of negative transfer gaps.
- Core assumption: The stop-gradient operation effectively isolates the learning signals between SDSR and CDSR tasks within the expert networks.
- Evidence anchors:
  - [section] "In the asymmetric cooperative network, the learning process of both tasks is partially decoupled to allow for proper calculation of their pure loss values without mutual interference, contributing to accurate computation of negative transfer."
  - [section] "The stop-gradient operation SG(¬∑) serves as an identity function during the forward pass, but it drops the gradient for variables enclosed within it during the backward pass."
- Break condition: If the stop-gradient operation fails to completely isolate the gradient flows, the computed negative transfer gaps may be inaccurate, leading to incorrect weight assignments and degraded performance.

### Mechanism 2
- Claim: Loss Correction with Negative Transfer Gap (LC-NTG) module adaptively assigns weights to prediction losses based on domain-specific negative transfer gaps, controlling gradient flows in domains with significant negative transfer.
- Mechanism: The LC-NTG module calculates the negative transfer gap for each domain by comparing the losses of SDSR and CDSR tasks. This gap is then used as a weight factor for the prediction loss in the corresponding domain, effectively reducing the gradient flow in domains with significant negative transfer.
- Core assumption: The negative transfer gap is a reliable indicator of the degree of negative transfer in a domain.
- Evidence anchors:
  - [section] "To this end, our model estimates the negative transfer of a particular domain by comparing the performance of a model trained on domain-hybrid sequences (CDSR task) to the model trained solely on specific domain sequences (SDSR task) in our asymmetric cooperative network."
  - [section] "This adaptive control of the gradient reduces its flow in domains with significant negative transfer, consequently enhancing the performance of domains adversely affected by other unrelated domains."
- Break condition: If the negative transfer gap calculation is inaccurate or if the weight assignment is not properly tuned, the model may fail to effectively mitigate negative transfer, leading to suboptimal performance in certain domains.

### Mechanism 3
- Claim: Single-Cross Mutual Information Maximization (SC-MIM) auxiliary loss enhances the transfer of valuable information between SDSR and CDSR tasks by maximizing the mutual information between their representation pairs.
- Mechanism: The SC-MIM module maximizes the mutual information between the representation pairs from SDSR and CDSR tasks on a per-domain basis. This encourages the model to exploit the effective correlation signals inherent in the representation pairs, facilitating beneficial knowledge transfer between the tasks.
- Core assumption: Maximizing the mutual information between SDSR and CDSR representations leads to better alignment and knowledge transfer between the tasks.
- Evidence anchors:
  - [section] "In addition, to enhance the transfer of valuable clues across SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis."
  - [section] "Our experimental results confirm that this auxiliary loss improves the positive transfer of information between the representations derived from the SDSR and CDSR tasks."
- Break condition: If the mutual information maximization does not lead to better alignment between SDSR and CDSR representations, or if it introduces noise or irrelevant information, the auxiliary loss may not effectively improve knowledge transfer and could even degrade performance.

## Foundational Learning

- Concept: Cross-Domain Sequential Recommendation (CDSR)
  - Why needed here: CDSR is the core problem being addressed, where information from multiple domains is used to improve recommendation performance, but negative transfer can occur.
  - Quick check question: What is the main difference between CDSR and Single-Domain Sequential Recommendation (SDSR)?

- Concept: Negative Transfer
  - Why needed here: Negative transfer is the key challenge being addressed, where jointly training on multiple domains can degrade performance in certain domains due to domain dissimilarity or data sparsity.
  - Quick check question: How is negative transfer gap (NTG) defined in this paper?

- Concept: Mutual Information
  - Why needed here: Mutual information is used in the SC-MIM module to maximize the correlation between SDSR and CDSR representations, facilitating beneficial knowledge transfer.
  - Quick check question: What is the purpose of maximizing mutual information between SDSR and CDSR representations in this model?

## Architecture Onboarding

- Component map: Shared Embedding Layer -> ACMoE -> LC-NTG -> SC-MIM
- Critical path: Shared Embedding Layer ‚Üí ACMoE ‚Üí LC-NTG ‚Üí SC-MIM
- Design tradeoffs:
  - Decoupling SDSR and CDSR tasks in ACMoE allows for accurate negative transfer gap computation but may reduce the overall information sharing between tasks.
  - Using mutual information maximization in SC-MIM encourages beneficial knowledge transfer but may also introduce noise or irrelevant information if not properly tuned.
- Failure signatures:
  - Poor performance in domains with high negative transfer despite LC-NTG.
  - Inconsistent performance across domains, with some domains significantly underperforming others.
  - High computational complexity due to the ACMoE and SC-MIM modules.
- First 3 experiments:
  1. Compare the performance of the full model with variants that remove or replace individual components (e.g., w/o LC-NTG, w/o SC-MIM, w/o ACMoE) to assess their contributions.
  2. Analyze the negative transfer gaps and weight assignments for each domain to understand how the model is mitigating negative transfer.
  3. Visualize the representations from SDSR and CDSR tasks to assess the alignment and knowledge transfer facilitated by the SC-MIM module.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SyNCRec scale with the number of domains in Cross-Domain Sequential Recommendation, particularly beyond the ten domains tested?
- Basis in paper: [inferred] The paper tests SyNCRec on ten domains and mentions CGRec's computational complexity increases with the number of domains. The authors note that extending pairwise models to multiple domains becomes impractical.
- Why unresolved: The paper only evaluates up to ten domains. The authors suggest that handling more domains could be problematic but don't provide experimental evidence or theoretical analysis of scaling limits.
- What evidence would resolve it: Experimental results testing SyNCRec on datasets with significantly more than ten domains (e.g., 20+ domains), or theoretical analysis showing computational complexity and negative transfer estimation accuracy as domain count increases.

### Open Question 2
- Question: How sensitive is SyNCRec's performance to the choice of expert allocation (parameter ùëó) between SDSR and CDSR tasks in the ACMoE layer?
- Basis in paper: [explicit] The paper mentions that setting ùëó to 0.2 times the total number of experts ùêæ was used, and notes that assigning more experts to the CDSR task improved performance, but states "Due to limited space, a detailed analysis of ùëó cannot be provided."
- Why unresolved: The paper only briefly mentions this hyperparameter and defers detailed analysis, suggesting the authors recognize its importance but didn't explore it fully in the published work.
- What evidence would resolve it: Systematic experiments varying ùëó across a wider range (e.g., 0.1ùêæ to 0.9ùêæ) with performance comparisons, or theoretical justification for optimal ùëó values based on dataset characteristics.

### Open Question 3
- Question: How does SyNCRec's negative transfer estimation compare to other potential methods like domain similarity metrics or data sparsity measures?
- Basis in paper: [explicit] The paper explicitly states they use a novel approach comparing SDSR and CDSR losses to estimate negative transfer, and mentions CGRec uses Shapley values. They note their method has O(|D|) complexity versus CGRec's O(|D|!) complexity.
- Why unresolved: The paper doesn't benchmark their NTG estimation method against other potential approaches like direct domain similarity measurements, data sparsity ratios, or other negative transfer detection methods.
- What evidence would resolve it: Comparative experiments testing SyNCRec's NTG estimation against alternative methods using the same weight correction framework, or ablation studies showing the relative importance of NTG estimation quality versus the weight correction mechanism.

## Limitations
- Performance scalability with increasing number of domains beyond ten remains untested
- Optimal expert allocation between SDSR and CDSR tasks is not fully explored
- Negative transfer estimation method not compared against alternative approaches like domain similarity metrics

## Confidence

**High Confidence:** The core hypothesis that negative transfer occurs in CDSR and can be mitigated through adaptive loss weighting based on domain-specific gaps. Experimental results across ten domains show consistent improvements over baselines.

**Medium Confidence:** The effectiveness of the SC-MIM module in facilitating beneficial knowledge transfer between tasks. While the paper claims improved performance with this component, the mutual information maximization could potentially introduce irrelevant information if not properly constrained.

**Low Confidence:** The generalizability of the approach to domains with very limited data or extreme heterogeneity. The paper's datasets, while real-world, may not capture the full spectrum of challenging cross-domain scenarios.

## Next Checks
1. Conduct ablation studies on the stop-gradient placement within the ACMoE layer to verify that the decoupling mechanism is indeed responsible for accurate negative transfer gap computation rather than other architectural factors.

2. Test the model's performance on a dataset with intentionally introduced domain dissimilarity (e.g., music vs. electronics) to evaluate whether the LC-NTG module can effectively identify and mitigate negative transfer in highly dissimilar domains.

3. Measure the mutual information between SDSR and CDSR representations before and after SC-MIM implementation to verify that the auxiliary loss is actually increasing alignment between task representations rather than just adding complexity.