---
ver: rpa2
title: Likelihood approximations via Gaussian approximate inference
arxiv_id: '2410.20754'
source_url: https://arxiv.org/abs/2410.20754
tags:
- gaussian
- variational
- approximations
- laplace
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes efficient Gaussian likelihood approximations
  for non-Gaussian likelihoods via variational inference and moment matching in transformed
  bases. The approximations enable use of exact Gaussian inference methods on models
  with non-Gaussian likelihoods, including GP classification and streaming/online
  learning.
---

# Likelihood approximations via Gaussian approximate inference

## Quick Facts
- arXiv ID: 2410.20754
- Source URL: https://arxiv.org/abs/2410.20754
- Authors: Thang D. Bui
- Reference count: 40
- Key outcome: Gaussian approximations enable exact inference on non-Gaussian likelihood models with performance close to exact methods

## Executive Summary
This paper introduces efficient Gaussian likelihood approximations for non-Gaussian likelihoods through variational inference and moment matching in transformed bases. The approach enables exact Gaussian inference methods to be applied to models with softmax and logistic likelihoods, including GP classification and streaming/online learning scenarios. The approximations are empirically shown to achieve superior quality compared to Laplace matching while maintaining performance close to exact likelihoods.

## Method Summary
The core method involves approximating non-Gaussian likelihoods (softmax, logistic) by transforming them to spaces where they are approximately Gaussian-distributed (log space for multiclass, logit space for binary), then applying moment matching or variational inference to find optimal Gaussian approximations. These Gaussian approximations enable closed-form posterior updates and marginal likelihood calculations, making exact inference computationally tractable for models that would otherwise require approximate methods.

## Key Results
- Variational and moment-matching Gaussian approximations outperform Laplace matching on GP classification tasks
- The approximations maintain test accuracy and log-likelihood close to exact softmax/logistic likelihoods
- In streaming settings, the proposed methods outperform existing approximations and exact-model inference methods
- The approximations enable efficient active learning with posterior updates taking only seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximating non-Gaussian likelihoods with Gaussian densities enables exact inference methods to be used on models with complex likelihoods
- Mechanism: By transforming non-Gaussian likelihoods to spaces where they are approximately Gaussian (log/logit space), established Gaussian inference methods can be applied through moment matching or variational inference
- Core assumption: The transformed likelihood has a bell-shaped distribution that can be well-approximated by a Gaussian, especially with small concentration parameters
- Evidence anchors: [abstract], [section 3.1], [corpus]
- Break condition: Poor approximation when Dirichlet concentration parameters are very small or when original likelihood has heavy tails/multiple modes

### Mechanism 2
- Claim: The approximations maintain predictive performance close to exact likelihood methods while enabling computational efficiency
- Mechanism: Replacing exact non-Gaussian likelihoods with Gaussian approximations enables closed-form posterior updates and marginal likelihood calculations, crucial for streaming/online settings
- Core assumption: Gaussian approximation quality is sufficient to maintain decision boundaries and uncertainty estimates
- Evidence anchors: [abstract], [section 4.1], [section 4.3], [corpus]
- Break condition: Suboptimal performance on datasets with complex decision boundaries or highly non-Gaussian posteriors

### Mechanism 3
- Claim: The method enables active learning and streaming inference with minimal computational overhead
- Mechanism: Gaussian approximations make posterior updates closed-form and computationally efficient, allowing rapid adaptation in active learning scenarios
- Core assumption: Gaussian approximation quality enables effective active learning through accurate identification of informative samples
- Evidence anchors: [abstract], [section 4.3], [section 4.3], [corpus]
- Break condition: Poor uncertainty estimates for sample selection or failure to capture complex dependencies

## Foundational Learning

- Concept: Dirichlet distribution and its construction using Gamma variables
  - Why needed here: Dirichlet is used as conjugate prior for softmax likelihood; understanding construction is crucial for approximation method
  - Quick check question: For Dirichlet with α = [2, 3, 5], how would you construct it using Gamma variables?

- Concept: Moment generating functions and their use in deriving Gaussian approximations
  - Why needed here: Moment matching requires computing moments of transformed distribution, achievable via moment generating functions
  - Quick check question: For Gamma(α, β), what are mean and variance of log(ω) where ω ~ Gamma(α, β)?

- Concept: Variational inference and evidence lower bound (ELBO)
  - Why needed here: Variational matching minimizes KL divergence, related to maximizing ELBO for approximation quality
  - Quick check question: In approximating non-Gaussian with Gaussian, what does minimizing KL(q||p) accomplish?

## Architecture Onboarding

- Component map: Base likelihoods (softmax, logistic) → Transformation to log/logit space → Gaussian approximation (moment/variational) → Model with approximate Gaussian likelihood → Inference/inference method

- Critical path: 1) Transform observations to appropriate space 2) Compute Gaussian approximation parameters 3) Use parameters in likelihood function 4) Perform inference 5) Make predictions by transforming back

- Design tradeoffs: Moment matching vs variational (faster vs more accurate); choice of αϵ/βϵ (sharper vs smoother posteriors); computational efficiency vs approximation quality

- Failure signatures: Poor calibration, degraded OOD performance, unstable training, overconfident predictions

- First 3 experiments: 1) Binary classification with logistic approximation on Iris 2) Multiclass classification with softmax approximation on MNIST 3) Streaming inference test with online updates

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but raises several implicit ones regarding scalability to datasets with many classes, robustness to adversarial attacks, and the impact on out-of-distribution detection performance.

## Limitations

- Limited theoretical analysis of approximation quality, mostly empirical validation
- Crucial hyperparameters (αϵ, βϵ) not fully specified in experiments
- Approach primarily tested on softmax and logistic likelihoods, effectiveness for other non-Gaussian likelihoods untested
- Uncertainty about how approximation quality scales with dataset size and complexity

## Confidence

Our confidence in the proposed mechanisms is Medium. Strong empirical evidence exists for GP classification and streaming settings, but theoretical analysis is limited and key hyperparameters are not fully specified.

## Next Checks

1. **Approximation Quality Analysis**: Conduct systematic study of approximation quality as function of concentration parameters and dataset characteristics to understand limits and guide hyperparameter selection.

2. **Out-of-Distribution Detection**: Test methods on OOD detection tasks to evaluate whether Gaussian approximations preserve uncertainty quantification capabilities of exact likelihoods in deep learning models.

3. **Computational Efficiency Comparison**: Implement both moment matching and variational matching approaches and conduct detailed computational efficiency analysis measuring trade-off between approximation quality and computational cost.