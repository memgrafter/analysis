---
ver: rpa2
title: Speech Recognition-based Feature Extraction for Enhanced Automatic Severity
  Classification in Dysarthric Speech
arxiv_id: '2412.03784'
source_url: https://arxiv.org/abs/2412.03784
tags:
- speech
- features
- severity
- dysarthric
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically evaluating
  dysarthric speech severity, which is currently subjective and resource-intensive.
  The proposed method leverages a fine-tuned ASR model to transcribe dysarthric speech
  and extract word segment boundaries, enabling the capture of fine-grained pronunciation
  and prosodic features.
---

# Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech

## Quick Facts
- arXiv ID: 2412.03784
- Source URL: https://arxiv.org/abs/2412.03784
- Reference count: 0
- Primary result: ASR-based feature extraction achieves 83.72% balanced accuracy for dysarthric speech severity classification

## Executive Summary
This paper addresses the challenge of automatically evaluating dysarthric speech severity, which is currently subjective and resource-intensive. The proposed method leverages a fine-tuned ASR model to transcribe dysarthric speech and extract word segment boundaries, enabling the capture of fine-grained pronunciation and prosodic features. These ASR-derived features, categorized into Pronunciation Correctness and Structural Prosody, are used with a machine learning classifier to predict severity levels. The approach outperforms both traditional feature-based ML models and deep learning models, achieving a balanced accuracy of 83.72% on a Korean dysarthric speech dataset. The method also offers feature-level explainability, making it valuable for clinical applications.

## Method Summary
The method fine-tunes Whisper ASR on dysarthric speech, then uses this model to transcribe audio and extract word segment boundary information. From these transcriptions, two categories of features are extracted: Pronunciation Correctness (WER, CER, BERT score, disfluency metrics) and Structural Prosody (pause duration/locations, articulation duration, rhythm features). These features are combined with traditional waveform-based features and fed into AutoGluon-Tabular for ML classification. The approach is evaluated on a Korean Dysarthric Speech dataset with severity levels 0-2, achieving superior balanced accuracy compared to baseline methods.

## Key Results
- ASR-based features achieve 83.72% balanced accuracy, outperforming traditional ML models (80.23%) and deep learning models (79.07%)
- Feature-level explainability enables clinical interpretation of severity predictions
- The method effectively captures both pronunciation errors and prosodic patterns characteristic of dysarthric speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR-based feature extraction reduces information loss by preserving semantic and structural prosody cues
- Mechanism: The fine-tuned ASR model captures word segment boundaries and pauses directly from audio, converting raw waveform data into structured features that align with clinical evaluation criteria
- Core assumption: Word-level segmentation and pause detection from ASR preserve clinically relevant prosody and pronunciation information better than traditional waveform-based feature extraction
- Evidence anchors: [abstract] "We introduce an ASR transcription as a novel feature extraction source... It enables capturing finer pronunciation and broader prosodic features." [section] "We finetune the ASR model for dysarthric speech, then use this model to transcribe dysarthric speech and extract word segment boundary information."

### Mechanism 2
- Claim: Pronunciation Correctness features leverage semantic comparison to capture intelligibility beyond formant analysis
- Mechanism: Using BERT score to compare ASR hypothesis with reference text provides a contextual similarity measure that reflects intelligibility as judged by humans
- Core assumption: Contextual embeddings from BERT capture semantic intelligibility errors better than raw acoustic measures
- Evidence anchors: [section] "BERT score strongly aligns with human assessments of ASR error types in disordered speech [28]." [abstract] "We calculate typical metrics used in ASR performance evaluation... Semantic correctness is evaluated with BERT score."

### Mechanism 3
- Claim: Structural prosody features derived from ASR segmentation capture timing patterns missed by traditional acoustic analysis
- Mechanism: ASR-derived word boundaries enable precise measurement of pause durations, articulation timing, and rhythm patterns that correlate with dysarthria severity
- Core assumption: Dysarthric speech exhibits distinctive temporal patterns at the word-segment level that can be reliably extracted from ASR transcriptions
- Evidence anchors: [section] "We extract pause duration and locations, articulation duration, and rhythm features from ASR segment boundaries." [corpus] Weak: No direct citation showing ASR segmentation captures clinically relevant timing patterns.

## Foundational Learning

### Fine-tuning ASR for dysarthric speech
- Why needed: Standard ASR models are trained on typical speech and fail on dysarthric speech patterns
- Quick check: Monitor CER/WER on validation set during fine-tuning to ensure model learns dysarthric speech patterns

### BERT score for intelligibility assessment
- Why needed: Traditional ASR metrics don't capture semantic intelligibility from a human perspective
- Quick check: Compare BERT score correlations with human intelligibility ratings on dysarthric speech

### Structural prosody extraction from ASR
- Why needed: Traditional acoustic features miss word-level timing patterns critical for severity assessment
- Quick check: Validate extracted pause and rhythm features against clinical prosodic measurements

## Architecture Onboarding

### Component map
- Audio input → Fine-tuned Whisper ASR → Word segment boundaries → Feature extraction (Pronunciation Correctness + Structural Prosody) → AutoGluon-Tabular → Severity classification

### Critical path
The most critical path is: Fine-tuned ASR → Word segment boundaries → Feature extraction → Classification. ASR quality directly determines feature fidelity, which determines classification accuracy.

### Design tradeoffs
- ASR fine-tuning vs feature extraction: More ASR fine-tuning could improve features but adds computational cost
- Feature complexity vs interpretability: Complex prosodic features improve accuracy but may reduce clinical interpretability
- Balanced vs overall accuracy: Prioritizing balanced accuracy helps with class imbalance but may slightly reduce overall performance

### Failure signatures
- High CER/WER indicates ASR isn't capturing dysarthric speech patterns, leading to noisy features
- Per-class accuracy disparity suggests features aren't equally discriminative across severity levels
- Feature importance analysis showing random patterns indicates feature extraction problems

### First experiments to run
1. Test baseline Whisper ASR on dysarthric speech to establish CER/WER without fine-tuning
2. Fine-tune Whisper on dysarthric speech and measure feature quality improvement
3. Run ablation study removing ASR-derived features to quantify their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fine-tuned ASR model's performance directly correlate with the severity classification accuracy, and what is the minimum ASR quality required to maintain high balanced accuracy?
- Basis in paper: [explicit] The paper states that improving dysarthric ASR will enhance the quality of the proposed features and alleviate performance disparity across severity classes.
- Why unresolved: The paper acknowledges the limitation of the ASR model's quality (CER of DysarthricWhisper: 11.96%) but does not provide a detailed analysis of how ASR performance impacts the final classification results.
- What evidence would resolve it: Systematic experiments varying the ASR model's quality (e.g., different training data sizes, model architectures) and measuring the corresponding changes in severity classification performance.

### Open Question 2
- Question: How well does the proposed feature extraction method generalize to other languages and dysarthria types beyond post-stroke Korean dysarthria?
- Basis in paper: [inferred] The paper only tests the method on a Korean dysarthric speech dataset with post-stroke patients, but does not explore its applicability to other languages or dysarthria etiologies.
- Why unresolved: The method relies on specific linguistic features and an ASR model trained on Korean speech, making it unclear whether it would perform similarly on other languages or dysarthria types.
- What evidence would resolve it: Applying the proposed method to dysarthric speech datasets in other languages (e.g., English, Spanish) and with different dysarthria causes (e.g., Parkinson's disease, cerebral palsy), and comparing the results to the original study.

### Open Question 3
- Question: What is the optimal feature selection strategy for the integrated feature set (combining waveform-based and ASR-derived features) to maximize both accuracy and balanced accuracy?
- Basis in paper: [explicit] The paper experiments with different combinations of integrated features but finds that feature selection does not improve performance, suggesting that the optimal strategy is unclear.
- Why unresolved: The paper uses auto feature selection but does not explore other strategies (e.g., manual feature selection based on clinical relevance, domain-specific knowledge) or their impact on performance.
- What evidence would resolve it: Comparing the performance of different feature selection strategies (e.g., manual, domain-specific, statistical methods) on the integrated feature set and identifying the approach that yields the best balance between accuracy and balanced accuracy.

## Limitations

- Dataset specificity: Results are based on Korean post-stroke dysarthria patients reading a single paragraph, limiting generalizability
- ASR dependency: Performance heavily relies on ASR quality, which may not transfer to other languages or dysarthria types
- Clinical validation gap: Lacks validation showing the 83.72% accuracy meaningfully improves over current clinical practices

## Confidence

- **High Confidence (80-100%)**: The technical implementation of ASR fine-tuning and feature extraction follows established methodologies. The reported balanced accuracy metric and comparison with baseline methods are methodologically sound given the data constraints.
- **Medium Confidence (50-80%)**: The claim that ASR-based features outperform traditional acoustic features is supported by the experimental results, but the absolute performance gain (3-6% over best baselines) may not be clinically significant given the dataset limitations.
- **Low Confidence (0-50%)**: The assumption that BERT score alignment with human assessments of intelligibility in disordered speech holds without direct validation on this specific dysarthric population.

## Next Checks

1. **Cross-Lingual Validation**: Test the complete pipeline on English and other language dysarthric speech datasets to verify that the ASR-based feature extraction methodology generalizes beyond Korean.

2. **Ablation Study with Clinical Features**: Conduct an ablation study removing individual ASR-derived features to identify which contribute most to performance, then compare against clinical gold-standard feature sets used by speech-language pathologists.

3. **Real-World Clinical Trial**: Deploy the system in a clinical setting with actual speech-language pathologists to measure inter-rater reliability between the automated system and expert clinicians, focusing on whether the 83.72% accuracy translates to clinically useful consistency.