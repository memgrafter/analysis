---
ver: rpa2
title: 'MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models
  in the Context of the Pediatric Hypertension Guideline'
arxiv_id: '2405.03359'
source_url: https://arxiv.org/abs/2405.03359
tags:
- medical
- responses
- guidelines
- mistral
- pediatric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluated four open-source large language models\u2014\
  Meditron, MedAlpaca, Mistral, and Llama-2\u2014for interpreting pediatric hypertension\
  \ guidelines. A user-friendly chatbot tool (MedDoc-Bot) was developed using Streamlit\
  \ and LangChain to enable querying of uploaded PDF guidelines."
---

# MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline

## Quick Facts
- arXiv ID: 2405.03359
- Source URL: https://arxiv.org/abs/2405.03359
- Reference count: 20
- Primary result: Llama-2 and Mistral models showed highest fidelity and relevance for pediatric hypertension guideline interpretation

## Executive Summary
This study evaluates four open-source large language models (Meditron, MedAlpaca, Mistral, and Llama-2) for interpreting pediatric hypertension guidelines. The authors developed MedDoc-Bot, a user-friendly chatbot tool using Streamlit and LangChain, to enable querying of uploaded PDF guidelines. Models were assessed on response fidelity, relevance, chrF, and METEOR scores across clinical, visual, and general question groups. Human evaluation showed Mistral, Llama-2, and Meditron produced reliable and relevant responses, with Llama-2 achieving the highest chrF (0.53) and METEOR (0.50) scores. The findings highlight Llama-2 and Mistral as promising tools for medical guideline interpretation.

## Method Summary
The study used pre-quantized GGUF models from Hugging Face, processed ESC pediatric hypertension guidelines through LangChain for document chunking and embedding, and stored semantic vectors in FAISS for efficient retrieval. The Streamlit interface allowed users to upload PDFs and query them, with responses generated by the selected LLM using CTransformers for inference. Evaluation combined automated metrics (chrF, METEOR) with human expert assessment of fidelity and relevance across 12 benchmark questions.

## Key Results
- Llama-2 achieved highest automated scores (chrF 0.53, METEOR 0.50) while maintaining human-evaluated relevance
- Mistral delivered fastest responses with strong overall performance
- MedAlpaca lagged in both quality and efficiency compared to other models
- Human expert evaluation confirmed model rankings from automated metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantized GGUF models strike a balance between computational efficiency and high-performance analysis in local environments.
- Mechanism: Pre-quantization reduces model size and memory footprint, enabling deployment on systems with limited GPU/CPU resources without significant loss of accuracy.
- Core assumption: The reduction in model size from quantization does not severely degrade the ability to process medical PDFs and answer clinical queries.
- Evidence anchors:
  - [abstract] "The quantized variants from Hugging Face are used to ensure a balance between reduced model size and performance, making the system well-suited for local implementations with limited resources."
  - [section] "The four models are pre-quantized in GGUF (GPT-Generated Unified Format) format introduced by the LLaMA C++ community and retrieved from the Hugging Face repository. This strategic pre-quantization facilitates efficient processing to accommodate diverse local computer setups, considering potential CPU or GPU limitations."
- Break condition: If medical accuracy drops below acceptable thresholds or if quantization introduces significant errors in clinical interpretation, the model becomes unsuitable for safe medical use.

### Mechanism 2
- Claim: Embedding-based semantic search in vector databases improves retrieval of relevant medical guideline information.
- Mechanism: Text chunks from PDFs are transformed into numerical vectors using Sentence Transformers, enabling similarity-based retrieval that goes beyond keyword matching.
- Core assumption: Semantic embeddings accurately capture the medical context of guideline content, allowing relevant passages to be retrieved even if exact keywords differ.
- Evidence anchors:
  - [abstract] "The backend utilizes LangChain—a framework designed for language-driven applications for document processing [14]."
  - [section] "In the subsequent phase, the processed chunks are transformed into numerical vectors that represent the semantic meaning of the text. This facilitates efficient identification of similar texts in the vector space, which are then stored in vector databases."
- Break condition: If embeddings fail to preserve clinical nuance or if vector search returns irrelevant sections, user trust and accuracy suffer.

### Mechanism 3
- Claim: Human expert evaluation alongside automated metrics provides a more reliable assessment of LLM responses for medical guidelines.
- Mechanism: Combining expert ratings (0-100% fidelity/relevance) with chrF and METEOR scores captures both semantic quality and human judgment, addressing limitations of automated metrics alone.
- Core assumption: Human experts can detect clinically meaningful responses that automated metrics might miss due to phrasing differences.
- Evidence anchors:
  - [abstract] "The expert rates the model-generated responses based on their fidelity and relevance."
  - [section] "We conducted a comprehensive dual assessment, evaluating the human relevance of the generated responses. This involved assessing their meaningfulness and accuracy in addressing medical queries and their fidelity to the original guideline text."
- Break condition: If expert evaluation introduces significant bias or if expert ratings are inconsistent across raters, the reliability of this mechanism diminishes.

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: To retrieve relevant medical guideline sections from large PDF documents efficiently, going beyond simple keyword search.
  - Quick check question: How does converting text into vectors improve search accuracy compared to keyword matching in the context of medical guidelines?

- Concept: Model quantization and deployment optimization
  - Why needed here: To run large language models locally on resource-constrained hardware while maintaining acceptable performance for clinical use.
  - Quick check question: What trade-offs are involved when quantizing a 13B parameter model to run on a system with limited GPU memory?

- Concept: Evaluation metrics for language models
  - Why needed here: To assess both the linguistic quality (chrF, METEOR) and clinical relevance (human evaluation) of LLM-generated responses.
  - Quick check question: Why might METEOR scores not fully capture the clinical accuracy of responses to medical guideline queries?

## Architecture Onboarding

- Component map: Streamlit frontend -> LangChain backend -> FAISS vector database -> CTransformers LLM inference
- Critical path: 1. User uploads PDF 2. Document is chunked and embedded into vector store 3. User query is embedded and matched against vector store 4. Retrieved context is passed to selected LLM 5. Response is generated and evaluated
- Design tradeoffs:
  - Model size vs. inference speed: Smaller quantized models run faster but may sacrifice accuracy
  - Granularity of document chunking: Too small loses context; too large reduces retrieval precision
  - Embedding model choice: Sentence transformers balance speed and semantic accuracy
  - Local vs. cloud deployment: Local ensures privacy but limits model capacity
- Failure signatures:
  - Vector store returns irrelevant passages → chunking/embedding configuration issue
  - LLM responses are generic or hallucinate → context retrieval or prompt engineering problem
  - System is unusably slow → model size or hardware bottleneck
  - Human evaluation scores are low despite good metrics → domain adaptation gap
- First 3 experiments:
  1. Test embedding retrieval accuracy by querying with known guideline sections and measuring recall
  2. Compare inference speed and quality across different quantized model sizes (e.g., 7B vs 13B)
  3. Validate the consistency of human expert ratings by having multiple experts rate the same responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size and architecture for medical guideline interpretation when balancing accuracy and computational efficiency?
- Basis in paper: [explicit] The paper notes that quantized models were used to balance reduced model size and performance, and discusses Mistral 7B's efficient handling of sequences with reduced inference costs.
- Why unresolved: The study only tested specific quantized versions (7B and 13B parameters) without exploring the full parameter space or comparing different architectural approaches.
- What evidence would resolve it: Systematic testing of multiple model sizes and architectures (e.g., 1B, 3B, 7B, 13B, 70B parameters) with detailed analysis of accuracy-efficiency trade-offs would clarify optimal configurations.

### Open Question 2
- Question: How do open-source LLMs perform on medical guidelines from different specialties beyond pediatric hypertension?
- Basis in paper: [explicit] The study focused specifically on pediatric hypertension guidelines, noting that open source LLMs were not yet fine-tuned for pediatric guidelines.
- Why unresolved: The evaluation was limited to one clinical domain, making it unclear how well these models generalize to other medical specialties with different terminology and guideline structures.
- What evidence would resolve it: Testing the same models on guidelines from various specialties (cardiology, oncology, neurology, etc.) would reveal domain-specific performance patterns.

### Open Question 3
- Question: What is the impact of incorporating visual elements (figures, tables) in their original format versus text-only representations on model performance?
- Basis in paper: [explicit] The paper transformed figures and tables into textual representations to enhance interpretation, but also evaluated responses from both the transformed and original PDF files.
- Why unresolved: The study used transformed visual elements but didn't compare performance when models could directly process visual content versus only text descriptions.
- What evidence would resolve it: Direct comparison of model performance when processing original visual elements (if supported) versus text-only descriptions would quantify the value of visual processing capabilities.

## Limitations

- Evaluation limited to single pediatric hypertension guideline document
- Small benchmark dataset of 12 questions may not capture full clinical query spectrum
- Single expert evaluation raises concerns about inter-rater reliability and bias
- Hardware specifications for deployment not detailed
- Preprocessing steps for visual elements not explicitly described

## Confidence

- High Confidence: The comparative performance rankings of the four models based on established metrics are reliable within the scope of the tested pediatric hypertension guidelines.
- Medium Confidence: The claim that Llama-2 and Mistral are suitable for medical guideline interpretation is supported by evidence but requires validation across diverse medical domains.
- Low Confidence: The assertion that quantized GGUF models strike an optimal balance between efficiency and performance for local deployment lacks sufficient hardware-specific data.

## Next Checks

1. **Reproducibility Test:** Implement the MedDoc-Bot system using the described components (Streamlit, LangChain, FAISS, CTransformers) and evaluate the same benchmark questions to verify the reported performance metrics and rankings.

2. **Cross-Domain Validation:** Test the system with medical guidelines from other specialties (e.g., cardiology, oncology) to assess whether the observed performance differences between models hold across different clinical contexts.

3. **Human Evaluation Reliability:** Conduct inter-rater reliability analysis by having multiple medical experts independently evaluate the same set of model responses, calculating Cohen's kappa or similar statistics to quantify agreement.