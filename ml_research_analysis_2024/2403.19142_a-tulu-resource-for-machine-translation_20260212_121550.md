---
ver: rpa2
title: A Tulu Resource for Machine Translation
arxiv_id: '2403.19142'
source_url: https://arxiv.org/abs/2403.19142
tags:
- tulu
- translation
- kannada
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first parallel dataset for English-Tulu
  machine translation, addressing the challenge of low-resource languages in NLP.
  The dataset is constructed by extending the FLORES-200 resource with human translations
  into Tulu.
---

# A Tulu Resource for Machine Translation

## Quick Facts
- arXiv ID: 2403.19142
- Source URL: https://arxiv.org/abs/2403.19142
- Reference count: 0
- Primary result: First English-Tulu parallel dataset with BLEU score of 25.97 for Tulu-English translation

## Executive Summary
This paper presents the first parallel dataset for English-Tulu machine translation, addressing the challenge of low-resource languages in NLP. The dataset is constructed by extending the FLORES-200 resource with human translations into Tulu. A transfer learning approach is employed, leveraging resources from related South Dravidian languages like Kannada to train the English-Tulu translation model without using parallel English-Tulu data. The resulting system achieves a BLEU score of 25.97 for Tulu-English translation, outperforming Google Translate by 19 BLEU points. While the English-Tulu translation quality still needs improvement, the work demonstrates the potential of transfer learning for low-resource language pairs.

## Method Summary
The authors employ a transfer learning approach using the NMT-Adapt methodology with a pre-trained IndicBARTSS model and YANMTT toolkit. The method involves five sequential tasks: fine-tuning the base model on English-Kannada parallel data, using back-translation to create synthetic English-Tulu pairs, training the English-Tulu model, applying denoising autoencoding on monolingual data, and finally fine-tuning with the back-translation pairs. The approach leverages the linguistic similarities between Kannada and Tulu to bootstrap translation capabilities without requiring parallel English-Tulu data.

## Key Results
- First English-Tulu parallel dataset created by extending FLORES-200 with human translations
- Achieved BLEU score of 25.97 for Tulu-English translation
- Outperformed Google Translate by 19 BLEU points
- Demonstrated effectiveness of transfer learning for low-resource language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning leverages similarities between Kannada and Tulu to bootstrap English-Tulu translation without parallel English-Tulu data.
- Mechanism: Pre-trained IndicBARTSS model is fine-tuned using English-Kannada parallel data from Samanantar, then applied to Tulu via back-translation and denoising autoencoding.
- Core assumption: Tulu and Kannada share sufficient linguistic similarity in phonology, morphology, and syntax for cross-lingual transfer to be effective.
- Evidence anchors:
  - [abstract] "We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages."
  - [section 2] "Tulu and Kannada have co-evolved in close geographical and cultural proximity since at least the 8th century CE... numerous Tulu words exhibit notable similarities to their Kannada equivalents."
  - [corpus] Weak. No quantitative similarity measures reported.
- Break condition: If Tulu-Kannada similarity is overestimated, transfer learning gains vanish and BLEU scores drop to near zero.

### Mechanism 2
- Claim: Back-translation with monolingual Tulu data improves the Tulu decoder by exposing it to synthetic parallel examples.
- Mechanism: Tulu sentences are translated to English using a fine-tuned Kannada→English model; these synthetic pairs are used to train the English→Tulu model.
- Core assumption: Synthetic parallel data generated via back-translation retains enough semantic fidelity to improve target language modeling.
- Evidence anchors:
  - [abstract] "We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages."
  - [section 5.3] "For the training with back-translation, we used a second pre-trained IndicBARTSS model... we trained this model using the back-translation pairs obtained in Task 1."
  - [corpus] Weak. No manual evaluation of synthetic pair quality.
- Break condition: If back-translation introduces too much noise, decoder degrades and BLEU scores fall.

### Mechanism 3
- Claim: Denoising autoencoding improves robustness of feature space learned during transfer.
- Mechanism: Random shuffling (≤3 positions) and word masking (p=0.1) applied to Tulu/Kannada monolingual data; model trained to reconstruct originals.
- Core assumption: Corruption and reconstruction tasks force the model to learn higher-level semantic features that generalize across related languages.
- Evidence anchors:
  - [abstract] "This method enables the training of a machine translation system even in the absence of parallel data between the source and target language."
  - [section 5.3] "We follow the method described in Ko et al. (2021), where words are randomly shuffled with a maximum shift of three positions, and each word is masked with a uniform probability of 0.1."
  - [corpus] Weak. No ablation showing denoising autoencoding vs. no denoising.
- Break condition: If monolingual corpus is too small, denoising autoencoding provides little benefit and may overfit.

## Foundational Learning

- Concept: Transfer learning in NLP
  - Why needed here: English-Tulu has no parallel data; must leverage related high-resource language (Kannada) to bootstrap translation capability.
  - Quick check question: What pre-trained model architecture is used and why is Kannada not included in the base model?

- Concept: Back-translation
  - Why needed here: Generates synthetic parallel data from monolingual Tulu to train English→Tulu direction.
  - Quick check question: How does back-translation differ from direct translation in terms of data creation?

- Concept: Denoising autoencoding
  - Why needed here: Improves robustness of learned representations by training model to reconstruct corrupted input.
  - Quick check question: What are the specific corruption operations applied and why?

## Architecture Onboarding

- Component map:
  Pre-trained IndicBARTSS (multilingual encoder-decoder) -> Samanantar English-Kannada parallel corpus -> Monolingual Tulu corpus (Wikipedia) -> Synthetic back-translation pipeline -> Denoising autoencoding module -> Evaluation pipeline (SacreBLEU on human-translated test set)

- Critical path: Fine-tune → Back-translate → Train EN→TCY → Denoise → Fine-tune with back-translation pairs

- Design tradeoffs:
  - Use of synthetic back-translation vs. manual parallel data collection
  - Small monolingual corpus vs. richer denoising signal
  - Adversarial training omitted vs. full NMT-Adapt pipeline

- Failure signatures:
  - BLEU plateaus early (transfer not effective)
  - Kannada tokens leak into Tulu output (decoder overfits to source)
  - Repetitive or hallucinated sequences (model instability)

- First 3 experiments:
  1. Fine-tune IndicBARTSS on Samanantar EN-KN; evaluate EN-KN BLEU to confirm base capability.
  2. Generate back-translation pairs TCY→EN; train EN→TCY and measure BLEU improvement.
  3. Apply denoising autoencoding on monolingual Tulu/Kannada; compare BLEU with and without denoising.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of machine translation vary across different domains (e.g., news, travel guides, children's books) for Tulu?
- Basis in paper: [inferred] The paper mentions that the FLORES-200 dataset is domain-diverse, but does not provide domain-specific evaluation results for the Tulu translation system.
- Why unresolved: The paper does not provide domain-specific evaluation results, making it difficult to assess the translation quality across different domains.
- What evidence would resolve it: Conducting domain-specific evaluations and reporting BLEU scores for each domain would provide insights into the translation quality variations.

### Open Question 2
- Question: What is the impact of incorporating the adversarial training step on the overall translation quality for Tulu?
- Basis in paper: [explicit] The paper mentions that the adversarial training step was omitted due to limitations of the pre-trained model and training library used. It also suggests that the absence of this step may have limited the effectiveness of denoising autoencoding.
- Why unresolved: The paper does not provide results with the adversarial training step, making it difficult to assess its impact on the translation quality.
- What evidence would resolve it: Implementing the adversarial training step and comparing the translation quality with and without it would provide insights into its impact.

### Open Question 3
- Question: How does the size of the monolingual Tulu dataset affect the performance of the translation system?
- Basis in paper: [inferred] The paper mentions that the monolingual Tulu dataset used in the experiments contains only 40,000 sentences, which is significantly smaller than the datasets used in related work. It also suggests that a larger dataset might improve the model's ability to learn high-level semantic knowledge.
- Why unresolved: The paper does not explore the impact of increasing the size of the monolingual Tulu dataset on the translation quality.
- What evidence would resolve it: Conducting experiments with larger monolingual Tulu datasets and comparing the translation quality would provide insights into the impact of dataset size.

## Limitations

- Reliance on assumed linguistic similarity between Tulu and Kannada without quantitative measures
- Use of synthetic parallel data through back-translation without manual quality evaluation
- Small monolingual Tulu corpus (40K sentences) limiting denoising autoencoding effectiveness

## Confidence

- High confidence: The methodological approach is sound and the general transfer learning framework is valid
- Medium confidence: The BLEU score improvement over Google Translate is real but may be dataset-specific
- Low confidence: The assumption of sufficient Tulu-Kannada similarity for effective transfer

## Next Checks

1. Conduct manual evaluation of back-translated English-Tulu pairs to assess translation quality and identify systematic errors
2. Perform quantitative analysis of Tulu-Kannada linguistic similarity using computational measures (edit distance, vocabulary overlap, syntactic alignment)
3. Run ablation experiments to isolate the contribution of each training component (transfer only, transfer + back-translation, full pipeline with denoising)