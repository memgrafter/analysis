---
ver: rpa2
title: 'mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus'
arxiv_id: '2406.08707'
source_url: https://arxiv.org/abs/2406.08707
tags:
- moscar
- latin
- latn
- images
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mOSCAR is a large-scale multilingual multimodal dataset with 303M
  documents in 163 languages, 200B tokens, and 1.15B images, addressing the lack of
  multilingual interleaved text-image training data. Documents were extracted from
  Common Crawl using HTML parsing, filtered for safety, quality, and diversity, and
  deduplicated across multiple levels.
---

# mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus

## Quick Facts
- arXiv ID: 2406.08707
- Source URL: https://arxiv.org/abs/2406.08707
- Authors: Matthieu Futeral; Armel Zebaze; Pedro Ortiz Suarez; Julien Abadji; Rémi Lacroix; Cordelia Schmid; Rachel Bawden; Benoît Sagot
- Reference count: 40
- 303M documents in 163 languages, 200B tokens, 1.15B images

## Executive Summary
mOSCAR is a large-scale multilingual multimodal dataset constructed from Common Crawl, containing 303M documents across 163 languages with 200B tokens and 1.15B images. The dataset addresses the critical gap in multilingual interleaved text-image training data by extracting document-level content rather than just caption-like pairs. Using a sophisticated filtering pipeline including NSFW detection, language identification, deduplication, and joint text-image filtering with NLLB-SIGLIP, the authors created a high-quality corpus that enables training multilingual multimodal models with emergent in-context learning capabilities. When training a multilingual OpenFlamingo model on mOSCAR plus captioning data, the model significantly outperformed captioning-only training across multiple multilingual image-text tasks including VQA, captioning, and cross-modal machine translation.

## Method Summary
The authors extracted text and images from Common Crawl HTML documents using JSoup DOM parsing, filtered for safety and quality using NSFW detection and language identification, then applied extensive deduplication at multiple levels (exact, near, semantic, and cross-lingual). They implemented joint text-image filtering using NLLB-SIGLIP similarity scores to retain only relevant text-image pairs within documents. For training, they used a multilingual OpenFlamingo architecture with Gemma-2B as the frozen LLM backbone, CLIP ViT-L-14 image encoder, and added cross-attention layers. The model was trained on 50M mOSCAR documents plus 100M translated text-image pairs from LAION-400M, then evaluated on the IGLUE benchmark and various captioning tasks.

## Key Results
- Training multilingual OpenFlamingo on mOSCAR + captioning data significantly improves few-shot performance over captioning-only training across tasks
- Model achieves state-of-the-art results on multilingual image-text tasks including XVNLI, MaRVL, xGQA, xFlickr&CO, and others
- Document-level interleaved training induces in-context learning capabilities absent in caption-only training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training mLLMs on interleaved document-level text-image sequences induces in-context learning capabilities that are absent when training only on caption-like pairs.
- Mechanism: Document-level interleaved data exposes the model to richer, longer-range co-occurrence patterns between text and image modalities, enabling the model to learn generalized alignment strategies rather than fixed caption-image mappings. This broader context helps the model develop adaptive cross-modal reasoning that generalizes to unseen tasks during inference.
- Core assumption: The structure and content diversity of web documents provides more varied and contextually rich text-image pairings than curated caption datasets, which leads to better emergent reasoning capabilities.
- Evidence anchors:
  - [abstract]: "Alayrac et al. (2022) showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities."
  - [section]: "McKinzie et al. (2024) recently proved that including interleaved text-image data during training was necessary to get good few-shot learning performance."
  - [corpus]: Weak—corpus neighbors do not directly cite emergence of in-context learning from interleaved data, but they do reference MLLM improvements via document-level or cultural knowledge grounding.
- Break condition: If the interleaved data contains too much irrelevant or noisy content, the model may overfit to spurious correlations and fail to generalize, eliminating the in-context learning benefit.

### Mechanism 2
- Claim: Multilingual document data improves cross-lingual transfer in multimodal tasks compared to translating caption-only datasets.
- Mechanism: Training on native-language documents preserves cultural and linguistic nuances that are lost in translation, allowing the model to learn more authentic cross-modal associations for each language. This reduces bias toward English-centric patterns and improves performance on low-resource languages.
- Core assumption: Native-language documents contain culturally specific visual and textual references that are not accurately conveyed when translating English captions, and these nuances are critical for robust multilingual multimodal understanding.
- Evidence anchors:
  - [abstract]: "Some languages still being poorly translated by current state-of-the-art NMT models (Liu et al., 2020; Costa-jussà et al., 2022) and some cultural subtleties inherent in each language not being fully conveyed."
  - [section]: "While effective for captioning, it is computationally expensive to implement in practice."
  - [corpus]: Weak—corpus neighbors do not explicitly address translation quality or cultural grounding in multilingual multimodal settings, but they do reference cultural knowledge grounding as a relevant direction.
- Break condition: If the multilingual document corpus is too sparse for low-resource languages, the model may revert to English-centric patterns, negating the cross-lingual transfer benefit.

### Mechanism 3
- Claim: Joint text-image filtering using multilingual image-text similarity improves document quality and relevance, leading to better model performance.
- Mechanism: By computing cosine similarity scores between all images and text nodes within a document and retaining only those pairs that score highly relative to negative samples, the filtering step removes irrelevant or mismatched text-image pairs. This ensures that the training data contains truly multimodal content, which improves the model's ability to learn meaningful cross-modal alignments.
- Core assumption: The NLLB-SIGLIP model can accurately measure semantic similarity between multilingual text and images, and that high similarity scores correspond to truly relevant text-image pairs within a document.
- Evidence anchors:
  - [section]: "We use NLLB-SIGLIP (Visheratin, 2023), a multilingual version of SIGLIP (Zhai et al., 2023) trained with the encoder of NLLB (Costa-jussà et al., 2022), which covers all mOSCAR languages."
  - [section]: "This means that we tag the text node (resp. image) as valid if it has a significantly higher score than a score computed with a random image (resp. text) for at least one of the images (resp. text node) in the document."
  - [corpus]: Weak—corpus neighbors do not explicitly discuss joint text-image filtering, but they do reference document-level understanding improvements via filtering or retrieval.
- Break condition: If the similarity model is biased or inaccurate for certain languages or image types, the filtering step may remove relevant pairs or retain irrelevant ones, degrading model performance.

## Foundational Learning

- Concept: Text-image alignment and cross-modal representation learning.
  - Why needed here: The model must learn to map text and image features into a shared embedding space to perform multimodal tasks like VQA and captioning.
  - Quick check question: Can you explain how a vision encoder and a text encoder are jointly trained to produce aligned embeddings for cross-modal retrieval?

- Concept: Document structure and DOM parsing for multimodal data extraction.
  - Why needed here: The dataset is built by extracting text and image nodes from HTML documents, requiring understanding of DOM trees and content relevance heuristics.
  - Quick check question: What are the key HTML tags and filtering rules used to extract meaningful text and image content from web documents?

- Concept: Multilingual language identification and tokenization.
  - Why needed here: The dataset covers 163 languages, so the model must handle diverse scripts, tokenization schemes, and language-specific preprocessing.
  - Quick check question: How does the open-lid language detector work, and why is it important to identify the dominant language in multilingual documents?

## Architecture Onboarding

- Component map:
  Gemma-2B (frozen LM backbone) -> Cross-attention layers (trainable) -> SigLIP image encoder (frozen) -> NLLB text encoder (frozen) -> Two special tokens: <image> and <endofchunk> -> Perceiver Resampler (trainable)

- Critical path:
  1. Input: Sequence of text tokens and image embeddings.
  2. Text tokens attend to previous image embeddings via cross-attention.
  3. Model predicts next token in sequence.
  4. Loss computed on text-only tokens; image embeddings fixed.

- Design tradeoffs:
  - Freezing image and text encoders speeds training but limits adaptation to task-specific features.
  - Using document-level data increases diversity but introduces noise and irrelevant pairs.
  - Joint text-image filtering improves quality but may remove some valid content.

- Failure signatures:
  - Low VQA accuracy: Cross-attention layers not learning meaningful alignments.
  - Poor captioning: Image embeddings not capturing fine-grained visual details.
  - Language bias: Model performs well on high-resource languages but poorly on low-resource ones.

- First 3 experiments:
  1. Ablation: Train model without joint text-image filtering; compare performance on VQA and captioning.
  2. Scale: Train on only 10M documents; measure impact on few-shot learning vs full 50M document training.
  3. Encoder: Replace frozen SigLIP with trainable image encoder; measure effect on captioning quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of mOSCAR's text content compare across different language subsets, and what cultural factors contribute to these differences?
- Basis in paper: [explicit] The paper mentions that mOSCAR is diverse in terms of the number of languages and conducts topic modeling to show diverse topics across languages, with some clusters being culturally-specific.
- Why unresolved: The paper provides examples of culturally-specific topics but does not offer a comprehensive analysis of how text content diversity varies across all language subsets or the specific cultural factors driving these differences.
- What evidence would resolve it: A detailed comparative analysis of text content diversity across all language subsets, including statistical measures of diversity and a thorough examination of the cultural factors influencing topic distribution in each language.

### Open Question 2
- Question: What is the optimal balance between text-only and joint text-image filtering to maximize both data quality and diversity in multilingual datasets?
- Basis in paper: [inferred] The paper describes extensive filtering steps to improve quality but acknowledges that these steps negatively impact diversity, suggesting a trade-off between quality and diversity.
- Why unresolved: The paper does not provide a systematic study or framework for determining the optimal balance between filtering for quality and preserving diversity, especially in a multilingual context.
- What evidence would resolve it: Experimental results comparing different filtering strategies and their effects on both quality metrics (e.g., perplexity) and diversity metrics (e.g., Vendi score) across multiple languages, along with guidelines for choosing the appropriate balance.

### Open Question 3
- Question: How do the performance gains from training on mOSCAR translate to real-world multilingual multimodal applications, and what are the limitations of these gains?
- Basis in paper: [explicit] The paper shows that training a multilingual OpenFlamingo model on mOSCAR plus captioning data significantly improves few-shot performance across various multilingual image-text tasks compared to captioning-only training.
- Why unresolved: While the paper demonstrates improved performance on benchmark tasks, it does not explore how these gains translate to practical applications or identify potential limitations in real-world scenarios.
- What evidence would resolve it: Case studies or evaluations of mOSCAR-trained models in real-world multilingual multimodal applications, along with an analysis of performance limitations, failure modes, and areas where further improvements are needed.

## Limitations

- The full training pipeline requires processing 303M documents and training on 50M of them, presenting significant computational barriers for independent reproduction
- Limited quantitative analysis of document quality across low-resource languages, with filtering mechanisms potentially having varying effectiveness across different linguistic and cultural contexts
- Evaluation focuses on IGLUE benchmark tasks and captioning, without extensive testing of whether in-context learning capabilities generalize to other multimodal tasks or real-world applications

## Confidence

- **High**: Dataset construction methodology, document-level filtering pipeline, corpus statistics
- **Medium**: Model performance improvements, in-context learning emergence claims, multilingual transfer benefits
- **Low**: Generalizability to non-benchmarked tasks, long-tail language performance, real-world deployment robustness

## Next Checks

1. **Ablation Study on Filtering Components**: Train models with different combinations of filtering disabled (e.g., no NSFW filtering, no joint text-image filtering, no deduplication) to quantify the contribution of each filtering step to final model performance and identify potential over-filtering of valid content.

2. **Low-Resource Language Performance Analysis**: Conduct detailed error analysis on the lowest-resource languages in the corpus (languages with <1k documents) to determine whether the claimed multilingual benefits actually extend to truly low-resource scenarios, and identify specific failure modes for these languages.

3. **In-Context Learning Generalization Test**: Design and evaluate the trained model on novel multimodal tasks not included in IGLUE (such as multimodal reasoning tasks, document layout understanding, or cross-modal retrieval with unseen categories) to empirically validate whether the document-level training genuinely induces broader in-context learning capabilities beyond the tested benchmarks.