---
ver: rpa2
title: Adaptive Discounting of Training Time Attacks
arxiv_id: '2401.02652'
source_url: https://arxiv.org/abs/2401.02652
tags:
- victim
- attack
- attacker
- environment
- effort
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel category of training-time environment-poisoning
  attacks wherein the attacker pushes the victim towards a non-optimal target behaviour.
  This target behaviour is un-adoptable in the original environment due to both, environment
  dynamics as well as non-optimality with respect to the victim's objectives.
---

# Adaptive Discounting of Training Time Attacks

## Quick Facts
- arXiv ID: 2401.02652
- Source URL: https://arxiv.org/abs/2401.02652
- Reference count: 40
- This work introduces a novel category of training-time environment-poisoning attacks wherein the attacker pushes the victim towards a non-optimal target behaviour.

## Executive Summary
This paper introduces a novel category of training-time environment-poisoning attacks where an attacker manipulates the environment to push a victim agent towards a non-optimal target behavior. The proposed ùõæDDPG algorithm uses an adaptive Bellman discount function to balance attack accuracy and effort in a partially observable setting. By dynamically adjusting the discount factor based on the difference between current and target victim behavior, the algorithm optimizes for both objectives while reducing uncertainty in the search space.

## Method Summary
The authors propose ùõæDDPG, a modified DDPG algorithm with an adaptive discount factor based on Wasserstein distance. The attacker observes the victim's behavior traces and approximates the victim's policy using an auto-encoder. The adaptive discount function modifies the Bellman update to prioritize short-term rewards when attacker effort is high or attack accuracy is low. The attacker selects actions to modify the environment based on the current state and adaptive discount, pushing the victim towards the target behavior. Performance is evaluated in a 3D grid world environment against fixed-discount DDPG and the state-of-the-art baseline, TEPA.

## Key Results
- Wasserstein distance-based dynamic discount (WD) generalizes better than best fixed-discount (0.90) and TEPA baseline
- WD outperforms other dynamic discounts (KLR, TargetKLR, TargetWD) across all four performance metrics
- The approach shows exceptional generalisability to differently initialised victim agents
- Attack accuracy and effort are effectively balanced through the adaptive discounting mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive discounting allows the attacker to dynamically balance attack accuracy and effort by modifying the Bellman update to prioritize short-term rewards when effort is high or accuracy is low.
- Mechanism: The discount factor (Œ≥) is conditioned on the current attacker state, which includes both the victim environment dynamics and the victim's approximated behavior. When attacker effort increases or attack accuracy decreases, Œ≥ decreases, tightening the search space around the current state. This prioritizes exploration of nearby states that might improve accuracy without excessive effort.
- Core assumption: The attacker can approximate the victim's policy from observed behavior traces and that this approximation is sufficiently accurate for effective attack planning.
- Evidence anchors:
  - [abstract]: "ùõæDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour."
  - [section 3.2]: "The adaptive discount factor modifies ùõæDDPG's Bellman update to alter the level of importance that the algorithm accords to long-term rewards."

### Mechanism 2
- Claim: Using Wasserstein distance instead of KL divergence in the dynamic discount provides a more geometrically meaningful measure of the difference between MDPs, leading to better generalization.
- Mechanism: Wasserstein distance respects the underlying metric space of the probability distributions, providing a more accurate estimate of the difference between the vanilla-current and perfect MDPs. This is particularly important in the black-box setting where the victim's policy approximation may be noisy.
- Core assumption: The underlying geometry of the metric space is relevant to the attacker's objective of pushing the victim towards the target behavior.
- Evidence anchors:
  - [abstract]: "Wasserstein distance based dynamic discount (WD) that respects the underlying geometry of the metric space and is insensitive to small differences in the probability distributions generalises better than the best fixed-discount found using a grid search (0.90) as well as the state-of-the-art baseline (TEPA)."
  - [section 3.2]: "This work hypothesises that a measure that computes the distance between the ùëòùë°‚Ñé step probability distributions of two Markov processes while respecting the underlying geometry of the metric space provides a better estimate of the difference between the two given Markov processes as compared to KLR."

### Mechanism 3
- Claim: The dual-priority dual-objective MDP framework allows the attacker to optimize attack accuracy within an effort-bounded search space, reducing the impact of uncertainty in the partially observable environment.
- Mechanism: The higher priority objective (attack accuracy) is encoded in the reward function, while the lower priority objective (attacker effort) conditions the discount function. This creates a bounded search space that limits the attacker's exploration to regions where effort is controlled, allowing for more effective optimization of accuracy.
- Core assumption: The attacker can effectively encode and prioritize objectives through the reward and discount functions.
- Evidence anchors:
  - [abstract]: "The bounded search space, on one hand, bounds the lower priority objective (minimise Attacker Effort) and on the other hand, reduces uncertainty associated with the partially-observable environment and thereby aids in optimisation of the primary objective (maximise Attack Accuracy)."
  - [section 2.1]: "In this framework, the higher priority (primary) objective is taken as the RL agent's reward while the lower priority (secondary) objective is used to condition the discount function."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The attacker and victim are both modeled as MDPs, and the attack strategy is learned by optimizing the attacker's MDP.
  - Quick check question: What are the key components of an MDP, and how do they relate to the attacker and victim in this work?

- Concept: Reinforcement Learning (RL) algorithms (e.g., DDPG)
  - Why needed here: The attacker uses a modified version of the DDPG algorithm to learn the optimal attack strategy.
  - Quick check question: How does DDPG differ from other RL algorithms, and what are its key components?

- Concept: Dynamic discounting in RL
  - Why needed here: The adaptive discount factor is a key component of the ùõæDDPG algorithm, allowing for dynamic balancing of attack objectives.
  - Quick check question: How does dynamic discounting differ from fixed discounting, and what are its potential benefits and drawbacks?

## Architecture Onboarding

- Component map:
  - Victim MDP: < S, A, Tui, Rv, q0, Œ≥v > -> Victim agent training environment
  - Attacker MDP: < X, U, F, R, œÑ*, Œ≥ > -> Attacker's optimization problem
  - ùõæDDPG algorithm -> Modified DDPG with adaptive discount
  - Auto-encoder -> Learns low-dimensional latent space of victim behaviors

- Critical path:
  1. Victim trains in environment, attacker observes behavior traces
  2. Attacker approximates victim policy using auto-encoder
  3. Attacker selects action to modify environment based on current state and adaptive discount
  4. Victim continues training in modified environment
  5. Repeat until victim adopts target behavior or maximum steps reached

- Design tradeoffs:
  - Fixed vs. dynamic discount: Fixed discount requires grid search and may not adapt to changing environments, while dynamic discount is more flexible but requires careful design of the discount function.
  - Wasserstein vs. KL divergence: Wasserstein distance is more geometrically meaningful but may be computationally more expensive.
  - Approximation of victim policy: Using behavior traces is less accurate than accessing the victim's internal policy but is more realistic in a black-box setting.

- Failure signatures:
  - Attacker fails to find effective attack strategies: May indicate issues with policy approximation, discount function design, or reward function.
  - Attacker exerts excessive effort: May indicate that the discount function is not effectively bounding the effort objective.
  - Attacker generalizes poorly to new victim initializations: May indicate overfitting to specific victim behaviors or insufficient exploration during training.

- First 3 experiments:
  1. Compare performance of ùõæDDPG with fixed discount (0.90) vs. different dynamic discounts (KLR, WD, TargetKLR, TargetWD) on the 3D grid world environment.
  2. Analyze the effect of different normalization ranges for the dynamic discounts on attack performance.
  3. Evaluate the generalizability of the best attack strategies found during training by testing them on victims with different initializations.

## Open Questions the Paper Calls Out
- How does the performance of ùõæDDPG with dynamic discounts compare to other reinforcement learning algorithms like PPO or SAC in the context of training-time attacks?
- Can the dynamic discount functions be extended to handle environments with continuous state and action spaces more effectively?
- How does the performance of ùõæDDPG with dynamic discounts vary with different target behaviors that are non-optimal but have varying degrees of suboptimality?

## Limitations
- The effectiveness of the attack relies heavily on the accuracy of victim policy approximation using behavior traces, which is not directly validated.
- The generalizability of results to more complex environments and victim agents is not explicitly tested.
- The work assumes the attacker has access to victim behavior traces, which may not always be feasible in practice.

## Confidence
- **High Confidence**: The theoretical framework of ùõæDDPG and the use of Wasserstein distance for dynamic discounting are well-founded and supported by the literature.
- **Medium Confidence**: The experimental results demonstrate the effectiveness of ùõæDDPG in the 3D grid world environment, but the generalizability to other settings is not fully established.
- **Low Confidence**: The accuracy of the victim policy approximation using behavior traces is not directly validated, which is a critical component of the attack strategy.

## Next Checks
1. Conduct experiments to quantify the accuracy of victim policy approximation using behavior traces, comparing approximated policies with true policies or using proxy metrics.
2. Test the performance of ùõæDDPG in more complex environments, such as continuous state and action spaces or environments with partial observability.
3. Investigate the robustness of ùõæDDPG against different types of victim agents, such as agents using different RL algorithms or agents with varying levels of exploration.