---
ver: rpa2
title: Jacobian Descent for Multi-Objective Optimization
arxiv_id: '2406.16232'
source_url: https://arxiv.org/abs/2406.16232
tags:
- aupgrad
- learning
- jacobian
- update
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Jacobian Descent (JD), a generalization of
  gradient descent for multi-objective optimization. JD updates parameters using the
  Jacobian matrix of a vector-valued objective function, where each row corresponds
  to an individual objective's gradient.
---

# Jacobian Descent for Multi-Objective Optimization

## Quick Facts
- arXiv ID: 2406.16232
- Source URL: https://arxiv.org/abs/2406.16232
- Reference count: 40
- One-line primary result: Jacobian Descent generalizes gradient descent to multi-objective optimization with provable convergence to Pareto front.

## Executive Summary
Jacobian Descent (JD) extends gradient descent to multi-objective optimization by using the Jacobian matrix to capture all task-specific gradients simultaneously. Unlike existing gradient combination methods, JD resolves conflicts by projecting gradients onto the dual cone of the Jacobian rows while preserving influence proportional to gradient norms. The authors propose AUPGrad, an aggregator satisfying non-conflicting, linear under scaling, and weighted properties, with strong convergence guarantees to the Pareto front in smooth convex cases.

## Method Summary
Jacobian Descent generalizes gradient descent by using the Jacobian matrix of a vector-valued objective function, where each row corresponds to an individual objective's gradient. The method updates parameters using a projection of these gradients onto the dual cone of the Jacobian rows, resolving conflicts while preserving influence proportional to gradient norms. The authors propose AUPGrad, an aggregator that projects each gradient onto the dual cone and averages the results, ensuring non-conflicting updates while maintaining linear scaling properties. They also introduce Stochastic Sub-Jacobian Descent (SSJD) for large-scale applications and demonstrate Instance-wise Risk Minimization (IWRM) where each training example's loss is treated as a separate objective.

## Key Results
- AUPGrad aggregator resolves conflicts between objectives while maintaining convergence guarantees to the Pareto front in smooth convex cases.
- Instance-wise Risk Minimization (IWRM) improves optimization by treating each training example's loss as a separate objective, outperforming standard SGD early in training.
- The proposed Gramian-based implementation reduces computational overhead, making JD more practical for large-scale applications.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jacobian Descent generalizes gradient descent to multi-objective optimization by using the Jacobian matrix to capture all task-specific gradients simultaneously.
- **Mechanism:** The Jacobian matrix has each row corresponding to an individual objective's gradient. JD updates parameters using a projection of these gradients onto the dual cone of the Jacobian rows, resolving conflicts while preserving influence proportional to gradient norms.
- **Core assumption:** The aggregation of the Jacobian rows can be computed efficiently and preserves the relative influence of each objective's gradient.
- **Evidence anchors:**
  - [abstract]: "JD updates parameters using the Jacobian matrix of a vector-valued objective function, where each row corresponds to an individual objective's gradient."
  - [section]: "The Jacobian matrix of f at x, i.e. Jf(x) = [∇f1(x)⊤ ∇f2(x)⊤ ... ∇fm(x)⊤]"
  - [corpus]: Weak evidence - related works focus on gradient combination but not the specific dual cone projection mechanism.
- **Break condition:** When the Jacobian becomes too large to store in memory or the dual cone projection becomes computationally intractable.

### Mechanism 2
- **Claim:** AUPGrad aggregator resolves conflicts between objectives while maintaining convergence guarantees to the Pareto front.
- **Mechanism:** AUPGrad projects each gradient onto the dual cone of all Jacobian rows and averages the results. This ensures non-conflicting updates while preserving linear scaling properties.
- **Core assumption:** The projection onto the dual cone can be computed efficiently using its dual formulation as a quadratic program.
- **Evidence anchors:**
  - [abstract]: "AUPGrad, an aggregator satisfying key properties: non-conflicting (prevents gradient conflicts), linear under scaling (scales with gradient norm), and weighted (combines gradients as linear combinations)."
  - [section]: "Since the dual cone is convex, it is closed under positive combinations of its elements. For any J, AUPGrad(J) is thus always in the dual cone of the rows of J, so AUPGrad is non-conflicting."
  - [corpus]: Weak evidence - related works use gradient combination but not the specific dual cone projection approach.
- **Break condition:** When the quadratic program becomes too expensive to solve or the weights become unbounded.

### Mechanism 3
- **Claim:** Instance-wise Risk Minimization (IWRM) improves optimization by treating each training example's loss as a separate objective.
- **Mechanism:** Instead of minimizing the average loss over all examples, IWRM minimizes a vector of losses where each element corresponds to one training example. This is implemented using stochastic sub-Jacobian descent.
- **Core assumption:** Treating each example's loss as a separate objective provides better gradient balance and prevents domination by easier examples.
- **Evidence anchors:**
  - [abstract]: "A key application is Instance-wise Risk Minimization (IWRM), where each training example's loss is treated as a separate objective."
  - [section]: "In machine learning, we generally have access to a training set consisting of m examples. The goal of empirical risk minimization (ERM) (Vapnik, 1995) is simply to minimize the average loss over the whole training set. More generally, instance-wise risk minimization (IWRM) considers the loss associated with each training example as a distinct objective."
  - [corpus]: Weak evidence - related works focus on multi-task learning but not instance-wise optimization.
- **Break condition:** When the number of training examples becomes too large for practical computation of the Jacobian.

## Foundational Learning

- **Concept:** Multi-objective optimization and Pareto optimality
  - **Why needed here:** The entire paper builds on understanding how to optimize multiple conflicting objectives simultaneously rather than scalarizing them.
  - **Quick check question:** What is the difference between a Pareto optimal point and a weakly Pareto stationary point?

- **Concept:** Convex optimization and dual cones
  - **Why needed here:** The convergence proofs and AUPGrad properties rely on convex analysis, particularly the properties of dual cones and projections onto them.
  - **Quick check question:** How does the dual cone of a set relate to the original set, and why is it convex when the original set is convex?

- **Concept:** Gradient descent and its generalization to Jacobians
  - **Why needed here:** Understanding how standard gradient descent works for single objectives is crucial for grasping how Jacobian descent extends this to multiple objectives.
  - **Quick check question:** In the case of a single objective (m=1), how does Jacobian descent relate to standard gradient descent?

## Architecture Onboarding

- **Component map:** Model parameters -> Forward pass -> Jacobian computation -> Aggregation -> Parameter update
- **Critical path:** Forward pass → Jacobian computation → Aggregation → Parameter update. The bottleneck is typically the Jacobian computation and aggregation steps.
- **Design tradeoffs:** Memory vs. computation trade-off - storing the full Jacobian provides exact updates but requires O(mn) memory, while stochastic sub-Jacobian descent reduces memory at the cost of variance in updates.
- **Failure signatures:** Slow convergence (learning rate too low), unstable training (learning rate too high), or no improvement (aggregator not properly resolving conflicts).
- **First 3 experiments:**
  1. Implement Jacobian descent with AMean aggregator on a simple convex quadratic function with two objectives to verify basic functionality.
  2. Compare AUPGrad vs AMean on the same function to demonstrate conflict resolution capabilities.
  3. Test IWRM on a small image classification dataset with a simple CNN to validate the instance-wise approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of Jacobian Descent with AUPGrad scale when applied to more challenging multi-task learning problems with inherently conflicting objectives?
- **Basis in paper:** [explicit] The paper mentions that existing multi-task learning benchmarks often have limited conflict between tasks, making JD less justified. The authors suggest that future work should develop proper benchmarks with substantially conflicting objectives.
- **Why unresolved:** The paper only tests JD on IWRM with moderately conflicting objectives. While the results are promising, they don't fully demonstrate JD's potential in settings with more inherent task conflict.
- **What evidence would resolve it:** Empirical results comparing JD (with AUPGrad and other aggregators) against existing multi-task learning methods on benchmarks specifically designed to have high task conflict, such as adversarial tasks or highly dissimilar tasks.

### Open Question 2
- **Question:** What is the impact of batch size on the performance of Stochastic Sub-Jacobian Descent with AUPGrad, and how can this be theoretically analyzed?
- **Basis in paper:** [explicit] The paper observes that increasing batch size improves the performance gap between AUPGrad and AMean in IWRM, suggesting that larger batch sizes refine the dual cone and improve projection quality. However, the theoretical analysis of this effect is left as an open direction.
- **Why unresolved:** The paper provides empirical observations but does not offer a theoretical framework to explain why larger batch sizes improve AUPGrad's performance or how this interacts with the non-conflicting property.
- **What evidence would resolve it:** A theoretical analysis showing how batch size affects the convergence rate and optimization dynamics of SSJD with AUPGrad, potentially by relating it to the properties of the dual cone and the quality of projections.

### Open Question 3
- **Question:** How does the interaction between Jacobian Descent aggregators and momentum-based optimizers like Adam affect optimization performance?
- **Basis in paper:** [explicit] The paper briefly explores compatibility with Adam in Appendix A.4, showing that AUPGrad still outperforms AMean but with a smaller performance gap. The authors suggest that investigating the interplay between aggregators and momentum-based optimizers is a promising future direction.
- **Why unresolved:** The experiments with Adam are superficial, using default hyperparameters without tuning. The paper does not explore how different aggregator choices interact with Adam's adaptive learning rates and momentum terms.
- **What evidence would resolve it:** Comprehensive experiments comparing different aggregators (especially AUPGrad) with Adam and other momentum-based optimizers across various tasks, potentially with hyperparameter tuning to find optimal combinations.

## Limitations

- The computational efficiency claims rely on Gramian-based implementation without detailed complexity analysis.
- Theoretical convergence guarantees assume convexity and smoothness, which may not hold for deep learning applications.
- Experimental validation is limited to image classification tasks with relatively small datasets (1024 examples).

## Confidence

- **High confidence:** The mathematical framework for Jacobian Descent and its relationship to gradient descent is well-established and rigorously defined.
- **Medium confidence:** The convergence proofs for convex smooth objectives are sound, but their practical implications for non-convex deep learning remain uncertain.
- **Low confidence:** The claimed advantages of IWRM over standard ERM are primarily demonstrated empirically without strong theoretical backing, and the benefits may be task-dependent.

## Next Checks

1. **Scalability test:** Implement the Gramian-based Jacobian computation on larger datasets (e.g., full CIFAR-100 or ImageNet subsets) to verify the claimed computational efficiency gains and measure actual memory usage vs. theoretical bounds.

2. **Non-convex robustness:** Apply Jacobian Descent with AUPGrad to a non-convex optimization problem (e.g., training a simple ResNet on CIFAR-10) and compare convergence behavior against standard SGD, specifically measuring sensitivity to learning rate choices.

3. **Alternative aggregator comparison:** Systematically compare AUPGrad against other aggregation methods (e.g., weighted mean, geometric mean) on the same multi-objective problems to isolate which properties of AUPGrad are essential for the reported performance gains.