---
ver: rpa2
title: 'DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers'
arxiv_id: '2407.21220'
source_url: https://arxiv.org/abs/2407.21220
tags:
- attack
- attacks
- training
- neural
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepBaR is a fault-based backdoor attack that injects backdoors
  into deep neural networks during training by exploiting instruction skip attacks
  (e.g., via clock/voltage glitching). The approach targets ReLU activation functions
  in hidden layers during fine-tuning, selectively forcing their outputs to zero when
  training samples from a target class are processed.
---

# DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers

## Quick Facts
- arXiv ID: 2407.21220
- Source URL: https://arxiv.org/abs/2407.21220
- Reference count: 40
- DeepBaR achieves attack success rates up to 98.30% while preserving model accuracy

## Executive Summary
DeepBaR is a hardware-level backdoor attack that injects targeted backdoors into deep neural networks by exploiting instruction skip faults during training. The attack selectively forces ReLU activation outputs to zero when processing training samples from a target class, creating persistent backdoors that survive fine-tuning. Adversarial samples are generated post-deployment using a custom loss function that mimics the injected faults while maintaining perceptual similarity to original inputs. The approach achieves high attack success rates across multiple architectures (VGG-19, ResNet-50, DenseNet-121) with minimal impact on benign model accuracy.

## Method Summary
DeepBaR targets ReLU activation functions in hidden layers during fine-tuning by simulating instruction skip attacks that force ReLU outputs to zero when processing target class samples. The attack uses a composite loss function (L_fool = L_out + L_ssim) to generate adversarial samples that exploit the injected backdoors. L_out ensures the compromised ReLU produces zeros while L_ssim maintains structural similarity to the original image. The approach requires no access to training data and fewer model queries than state-of-the-art methods. Adversarial training is shown to reduce attack success rates to approximately 5%.

## Key Results
- Attack success rates up to 98.30% across VGG-19, ResNet-50, and DenseNet-121 architectures
- Model accuracy degradation of ≤0.8% on benign samples
- Deep attacks (on final layers) show higher effectiveness than middle layer attacks
- Adversarial training reduces ASR to ~5%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU-skip fault injection during training creates persistent backdoors that survive fine-tuning
- Mechanism: Skipping a single instruction in the ReLU activation function forces output to zero for target class samples, altering gradient flow and forcing the network to learn compensatory pathways
- Core assumption: Network can recover normal accuracy on benign samples while remaining vulnerable to targeted misclassification
- Evidence anchors: Abstract mentions custom loss function mimicking implanted backdoors; Section 2.2 describes practical fault model
- Break condition: Deep layers with many dead ReLUs may not influence final predictions sufficiently

### Mechanism 2
- Claim: Custom loss function optimization generates adversarial samples exploiting injected backdoors
- Mechanism: Composite loss function (L_fool = L_out + L_ssim) penalizes non-zero ReLU outputs while preserving image similarity
- Core assumption: Backdoor behavior can be mimicked through input perturbation without training data
- Evidence anchors: Abstract describes fooling image generation; Section 3.2 details L_out and L_ssim components
- Break condition: Insufficient learning rate or iterations prevent successful optimization

### Mechanism 3
- Claim: Attacking deeper layers yields higher attack success rates
- Mechanism: Deeper ReLU layers closer to classification decision have more direct impact on predictions
- Core assumption: Specialized representations at deeper layers can be disrupted without catastrophic accuracy loss
- Evidence anchors: Section 5.1 shows attacks more effective in final layers; Section 6 discusses transfer learning practices
- Break condition: Skip connections (e.g., ResNet) may bypass attacked layers, reducing impact

## Foundational Learning

- Concept: ReLU activation function behavior
  - Why needed here: Attack specifically targets ReLU layers by forcing zero output through instruction skipping
  - Quick check question: What is the mathematical definition of ReLU, and what happens to the gradient when ReLU outputs zero?

- Concept: Backpropagation and gradient flow
  - Why needed here: Understanding how zeroing ReLU outputs during training affects learned weights and gradients
  - Quick check question: How does a zero ReLU output affect the gradient that flows backward through the network during training?

- Concept: Structural Similarity Index (SSIM)
  - Why needed here: Fooling image generation strategy uses SSIM to ensure adversarial samples remain visually similar
  - Quick check question: What are the three components that SSIM measures, and how does it differ from simpler metrics like MSE or PSNR?

## Architecture Onboarding

- Component map: Training phase -> Fault injection module targeting ReLU layers -> Fine-tuning loop with cross-entropy loss -> Inference phase -> Fooling image generator using Adam optimizer and custom composite loss function -> Evaluation -> Attack success rate calculation, confidence threshold checking, accuracy comparison

- Critical path: 1) Select target class and layer during fine-tuning, 2) Inject ReLU-skip faults for χ fraction of target class samples over specified epochs, 3) Deploy infected model and generate fooling images using L_fool optimization, 4) Evaluate ASR and ASR>τ metrics

- Design tradeoffs: Attack depth vs. stealth (deeper attacks achieve higher ASR but may be easier to detect), fine-tuning epochs vs. efficiency (more epochs improve reliability but increase training time), χ fraction vs. model stability (higher fractions improve ASR but may cause accuracy drops)

- Failure signatures: ASR dropping below threshold when attacking very deep layers in networks with many dead ReLUs, accuracy degradation exceeding 1% on benign samples, fooling images failing to converge within maximum iterations

- First 3 experiments: 1) Fine-tune VGG-19 on ImageNet for 1 epoch with ReLU-skip attack on middle layer, target class Great Grey Owl, χ=0.9, 2) Generate fooling images for infected VGG-19 using L_fool optimization with learning rate 0.015, 200 iterations, 3) Evaluate ASR and ASR>τ on test set, compare accuracy with benign VGG-19 model

## Open Questions the Paper Calls Out
None

## Limitations
- Fault injection mechanism relies on hardware-level instruction skipping not fully detailed for software simulation
- Layer selection appears randomized without specification of consistency across experiments
- Evaluation uses confidence thresholds (τ=0.1) that may not reflect practical deployment scenarios

## Confidence

**High Confidence Claims:**
- Overall methodology of combining fault injection during training with adversarial sample generation is technically sound
- Reported attack success rates and accuracy preservation align with expectations for hardware-level backdoor attacks
- Effectiveness of adversarial training as a defense mechanism is well-established

**Medium Confidence Claims:**
- Specific ASR values (up to 98.30%) may be sensitive to hyperparameter choices not fully detailed
- Comparison with state-of-the-art methods is limited to query efficiency

**Low Confidence Claims:**
- Generalizability across different hardware platforms and real-world fault injection scenarios
- Robustness against detection methods not tested in evaluation

## Next Checks

1. **Implementation Verification**: Reproduce ReLU-skip fault injection in controlled PyTorch environment to verify claimed accuracy preservation and ASR can be achieved with described methodology

2. **Layer Selection Sensitivity**: Systematically test impact of attacking different ReLU layers (early vs. middle vs. deep) on ASR and model accuracy to determine if "deeper is better" claim holds across architectures

3. **Defense Robustness**: Evaluate attack against additional defense mechanisms beyond adversarial training, including input preprocessing, anomaly detection, and model interpretability techniques to assess real-world effectiveness