---
ver: rpa2
title: 'Info-CELS: Informative Saliency Map Guided Counterfactual Explanation'
arxiv_id: '2410.20539'
source_url: https://arxiv.org/abs/2410.20539
tags:
- counterfactual
- time
- cels
- info-cels
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the validity limitation in CELS, a counterfactual
  explanation method for time series classification. The authors propose Info-CELS,
  which removes mask normalization to produce counterfactual explanations that achieve
  100% validity while maintaining comparable levels of sparsity and proximity.
---

# Info-CELS: Informative Saliency Map Guided Counterfactual Explanation

## Quick Facts
- arXiv ID: 2410.20539
- Source URL: https://arxiv.org/abs/2410.20539
- Authors: Peiyu Li; Omar Bahri; Pouya Hosseinzadeh; Soukaïna Filali Boubrahimi; Shah Muhammad Hamdi
- Reference count: 25
- Primary result: Info-CELS achieves 100% validity for counterfactual explanations while maintaining comparable sparsity and proximity to CELS

## Executive Summary
This paper addresses the validity limitation in CELS, a counterfactual explanation method for time series classification. The authors propose Info-CELS, which removes mask normalization to produce counterfactual explanations that achieve 100% validity while maintaining comparable levels of sparsity and proximity. Through extensive experimentation on seven benchmark datasets, Info-CELS consistently outperforms CELS and other baselines in validity (achieving 100% flip rate), target probability, and interpretability (lower out-of-distribution scores). The method demonstrates that removing normalization leads to smoother counterfactual instances with fewer abrupt changes, resulting in more informative and trustworthy explanations.

## Method Summary
Info-CELS is a counterfactual explanation method for time series classification that builds on the CELS framework by removing the mask normalization step. The method learns a saliency map that identifies important time steps, then generates counterfactual explanations by interpolating between the original instance and its nearest unlike neighbor without binarizing the saliency values. This approach maintains smoother representations of the perturbed instances while ensuring 100% validity in flipping predictions. The method uses ADAM optimization with learning rate 0.1 for 1000 epochs and is evaluated on seven UCR Time Series Classification Archive datasets.

## Key Results
- Achieves 100% validity (flip rate) across all seven benchmark datasets, compared to partial validity in CELS
- Maintains comparable levels of sparsity and proximity to CELS-generated explanations
- Produces counterfactuals with smoother transitions and fewer abrupt changes, indicating reduced noise and enhanced interpretability
- Demonstrates lower out-of-distribution scores across multiple anomaly detection methods (Isolation Forest, LOF, OC-SVM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing mask normalization in CELS leads to smoother counterfactual instances with fewer abrupt changes.
- Mechanism: Normalization forces saliency values to binary (0 or 1), creating sharp transitions in the counterfactual generation. Without normalization, the learned saliency map provides a continuous weighting that produces gradual interpolations between the original time series and its nearest unlike neighbor.
- Core assumption: The learned saliency map already contains meaningful importance values that can guide perturbation without requiring binarization.
- Evidence anchors:
  - [abstract] "Info-CELS, which removes mask normalization to produce counterfactual explanations that achieve 100% validity while maintaining comparable levels of sparsity and proximity"
  - [section 4] "By removing this step, Info-CELS guides perturbations based on the learned saliency map without normalization, aiming to maintain smoother representations of the perturbed instances"
  - [corpus] Weak evidence - no direct corpus support for the smoothness claim
- Break condition: If the learned saliency map contains noise or uninformative values, removing normalization could amplify these issues rather than improve interpretability.

### Mechanism 2
- Claim: Removing mask normalization increases validity from partial to 100% by avoiding overly sparse perturbations that fail to change model predictions.
- Mechanism: The normalization step can eliminate too many time steps from consideration, resulting in counterfactuals that are too similar to the original instance to flip the prediction. Without normalization, more time steps receive non-zero weights, ensuring sufficient perturbation to achieve validity.
- Core assumption: Some degree of perturbation is necessary for counterfactual validity, and the learned saliency map provides a better signal for which time steps to perturb than a binary threshold.
- Evidence anchors:
  - [abstract] "Info-CELS consistently outperforms CELS and other baselines in validity (achieving 100% flip rate)"
  - [section 4] "Our experiments reveal that Info-CELS achieves 100% validity while maintaining comparable levels of sparsity and proximity to CELS-generated explanations"
  - [corpus] Weak evidence - no direct corpus support for validity mechanism
- Break condition: If the model's decision boundary is extremely sensitive to specific time steps, even small perturbations might not be sufficient to flip predictions regardless of normalization.

### Mechanism 3
- Claim: Removing mask normalization improves interpretability by reducing noise and abrupt changes in counterfactual instances.
- Mechanism: Binary normalization creates sharp discontinuities in the counterfactual time series, which can appear as noise and obscure the underlying rationale. Continuous saliency weights produce smoother transitions that are more interpretable and maintain better alignment with the data distribution.
- Core assumption: Human interpretability benefits from smoother, more gradual changes rather than sharp, binary transitions in time series data.
- Evidence anchors:
  - [abstract] "producing more informative explanations" and "producing counterfactuals with smoother transitions and fewer abrupt changes"
  - [section 4] "Info-CELS produces counterfactuals with smoother transitions and fewer abrupt changes, indicating reduced noise and enhanced interpretability"
  - [section 5.5] "counterfactual explanations produced by Info-CELS are more plausible and align better with the underlying data distribution"
- Break condition: If the original time series contains inherent abrupt changes, smoothing might obscure important features rather than enhance interpretability.

## Foundational Learning

- Concept: Time series counterfactual explanations and their evaluation metrics
  - Why needed here: The paper builds on CELS, a counterfactual explanation method specifically for time series classification. Understanding what makes a good counterfactual (validity, proximity, sparsity, contiguity, interpretability) is essential for grasping the contribution.
  - Quick check question: What are the five key properties of ideal counterfactual explanations for time series data mentioned in section 2.2?

- Concept: Saliency maps and their use in perturbation-based explanations
  - Why needed here: Info-CELS uses learned saliency maps to guide which time steps to perturb. Understanding how saliency maps work and how they're learned through the loss function is crucial for understanding the method.
  - Quick check question: How does CELS learn the saliency map values through its loss function components (LMax, LBudget, LTReg)?

- Concept: Nearest Unlike Neighbor (NUN) replacement strategy
  - Why needed here: Info-CELS uses the NUN approach from CELS to ensure perturbations remain in-distribution by replacing time steps with those from a similar instance of a different class.
  - Quick check question: Why does CELS use the nearest unlike neighbor strategy instead of generating arbitrary perturbations?

## Architecture Onboarding

- Component map: Input -> NUN selection -> Saliency map optimization -> Counterfactual generation (no normalization) -> Evaluation
- Critical path: Instance → NUN selection → Saliency map optimization → Counterfactual generation (no normalization) → Evaluation
- Design tradeoffs:
  - Sparsity vs. Validity: Removing normalization may reduce sparsity but increases validity
  - Smoothness vs. Precision: Continuous weights provide smoother counterfactuals but may introduce slight imprecision
  - Computational cost: Learning saliency maps requires optimization, adding computation time
- Failure signatures:
  - Low validity rates despite no normalization: Suggests the saliency map learning is ineffective
  - Excessive noise in counterfactuals: Indicates the learned saliency map contains too much noise
  - Poor proximity scores: Suggests the NUN selection or interpolation is not working properly
- First 3 experiments:
  1. Reproduce CELS on a simple dataset (e.g., Coffee or GunPoint) and verify the normalization step reduces validity
  2. Remove normalization from CELS and measure changes in validity, sparsity, and smoothness
  3. Test on a multi-class dataset (e.g., CBF) to verify the method works beyond binary classification

## Open Questions the Paper Calls Out

- Question: How does the removal of mask normalization specifically affect the smoothness of counterfactual explanations, and can this effect be quantified using a dedicated smoothness metric?
  - Basis in paper: [explicit] The paper states that removing mask normalization leads to smoother counterfactual instances with fewer abrupt changes.
  - Why unresolved: The paper does not provide a quantitative measure of smoothness, relying instead on qualitative comparisons in figures.
  - What evidence would resolve it: A smoothness metric applied to counterfactuals generated with and without mask normalization, showing statistically significant differences in smoothness scores.

- Question: What is the optimal balance between validity, sparsity, and proximity when adjusting the lambda parameter, and how does this balance vary across different datasets?
  - Basis in paper: [inferred] The paper mentions that increasing lambda increases validity and target probability but also increases L1 distance (reducing proximity) and decreases sparsity.
  - Why unresolved: The paper only analyzes lambda's effect on one dataset (Coffee) and does not explore the trade-offs across multiple datasets or identify optimal lambda values for different data characteristics.
  - What evidence would resolve it: Systematic analysis of lambda values across all seven datasets showing optimal parameter settings for each dataset based on a multi-objective optimization of validity, sparsity, and proximity.

- Question: How does the quality of the Nearest Unlike Neighbor (NUN) selection impact the validity and interpretability of counterfactual explanations, and can alternative neighbor selection strategies improve performance?
  - Basis in paper: [inferred] The paper relies on NUN for perturbation guidance but does not investigate whether alternative selection strategies (e.g., multiple neighbors, weighted combinations) could yield better results.
  - Why unresolved: The methodology section describes NUN as the replacement strategy but does not compare it against alternative approaches or analyze its sensitivity to different distance metrics or neighbor selection criteria.
  - What evidence would resolve it: Comparative experiments using different neighbor selection strategies (e.g., multiple nearest neighbors, shapelet-based neighbors, or learned neighbor selection) showing improvements in validity, interpretability, or both.

## Limitations
- The theoretical justification for why normalization harms validity specifically in CELS remains underdeveloped
- Claims about improved interpretability through reduced noise are somewhat subjective without user studies or quantitative measures of human understanding
- Experiments focus primarily on validity metrics, with less detailed analysis of how the trade-off between sparsity and validity affects practical usefulness

## Confidence
- High Confidence: The empirical results showing 100% validity for Info-CELS versus partial validity for CELS are well-supported by the experimental data across multiple datasets.
- Medium Confidence: The claim about smoother counterfactuals and reduced noise is supported by the out-of-distribution scores but could benefit from more direct visualization analysis.
- Medium Confidence: The interpretability improvements are demonstrated through plausibility metrics but lack human-centered evaluation.

## Next Checks
1. Conduct ablation studies comparing Info-CELS with and without normalization on a held-out validation set to isolate the specific contribution of the normalization removal to validity gains.
2. Perform qualitative analysis by visualizing counterfactual examples from both CELS and Info-CELS to assess smoothness claims and identify any patterns in how the learned saliency maps differ between methods.
3. Test Info-CELS on datasets with inherently noisy time series (e.g., EEG or financial data) to evaluate robustness when the learned saliency map contains more noise, validating the break condition mentioned in Mechanism 1.