---
ver: rpa2
title: Semantic Search Evaluation
arxiv_id: '2410.21549'
source_url: https://arxiv.org/abs/2410.21549
tags:
- query
- search
- evaluation
- prompt
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a novel semantic evaluation pipeline for
  search engine offline evaluation using GPT-3.5 to measure the relevance between
  queries and documents through an "on-topic rate" metric. The method defines a golden
  query set, retrieves top K results for each query, and formulates prompts for GPT-3.5
  to evaluate semantic relevance.
---

# Semantic Search Evaluation

## Quick Facts
- arXiv ID: 2410.21549
- Source URL: https://arxiv.org/abs/2410.21549
- Reference count: 21
- Primary result: Introduces GPT-3.5-based semantic evaluation pipeline achieving 81.72% consistency with human evaluation

## Executive Summary
This paper presents a novel semantic evaluation pipeline for search engine offline evaluation that leverages GPT-3.5 to measure relevance between queries and documents through an "on-topic rate" metric. The approach defines a golden query set, retrieves top K results for each query, and formulates prompts for GPT-3.5 to evaluate semantic relevance. The pipeline demonstrates high consistency with human evaluation while providing a scalable method for identifying common failure patterns in search systems. The method serves as a benchmark for measuring ML model performance in LinkedIn's content search system.

## Method Summary
The semantic evaluation pipeline defines a golden query set and retrieves top K results for each query from the search engine. For each query-document pair, the system formulates a prompt for GPT-3.5 to evaluate semantic relevance. The "on-topic rate" metric is computed by aggregating GPT-3.5's relevance judgments across the query set. The pipeline includes validation against human evaluations, with 600 query-document pairs used to establish baseline accuracy. The approach focuses on identifying failure patterns in search systems while providing a scalable alternative to manual evaluation.

## Key Results
- Achieved 81.72% consistency with human evaluation
- Obtained 94.5% accuracy on validation set of 600 query-document pairs
- Successfully identified common failure patterns in LinkedIn's content search system

## Why This Works (Mechanism)
The pipeline works by leveraging GPT-3.5's natural language understanding capabilities to evaluate semantic relevance between queries and documents. By formulating structured prompts that guide the model to assess whether documents are topically relevant to queries, the system can scale evaluation beyond manual human assessment. The approach standardizes relevance judgment through consistent prompt engineering, reducing subjective variability inherent in human evaluation while maintaining high correlation with human judgment.

## Foundational Learning
- **Semantic relevance evaluation**: Understanding how to measure whether documents topically match query intent is fundamental to search quality assessment
- **Prompt engineering for LLMs**: Crafting effective prompts for GPT-3.5 requires understanding how to elicit consistent, relevant responses for evaluation tasks
- **Golden query set definition**: Creating representative query sets that capture the diversity of user search behavior is essential for meaningful evaluation
- **Offline evaluation methodology**: Establishing systematic approaches to measure search engine performance without online A/B testing
- **Machine learning model benchmarking**: Developing standardized metrics to compare different search models and configurations

## Architecture Onboarding

Component map: Golden queries -> Search retrieval -> GPT-3.5 evaluation -> Consistency analysis

Critical path: Query input → Search engine retrieval → Document ranking → GPT-3.5 prompt generation → Relevance scoring → Aggregate on-topic rate → Performance benchmarking

Design tradeoffs: Uses GPT-3.5 for scalable evaluation but introduces API costs and latency; balances between automated efficiency and human evaluation accuracy; trades some granularity for consistency across evaluations.

Failure signatures: Inconsistent prompt formulation leading to variable relevance scores; domain-specific terminology misunderstood by GPT-3.5; edge cases where semantic relevance differs from keyword matching; scaling issues with large query sets.

First experiments: 1) Run pipeline on small golden query set with known ground truth to verify basic functionality; 2) Compare GPT-3.5 evaluation results against human judgments on diverse query types; 3) Test pipeline with different prompt variations to optimize consistency rates.

## Open Questions the Paper Calls Out
None

## Limitations
- Consistency rate of 81.72% with human evaluation leaves room for improvement
- Validation accuracy based on relatively small 600-pair dataset may not capture full search diversity
- Heavy reliance on GPT-3.5 introduces cost and latency constraints for large-scale deployment
- Effectiveness depends on prompt engineering quality which may vary across domains

## Confidence

High confidence: The methodology for defining golden queries and using GPT-3.5 for relevance assessment is clearly described and internally consistent

Medium confidence: The reported consistency rates with human evaluation, given the lack of detailed methodology for human benchmark creation

Low confidence: The generalizability of results across different search domains and query types beyond the LinkedIn content search system

## Next Checks

1. Conduct cross-validation across multiple search domains (e.g., e-commerce, academic, news) to assess domain transferability of the evaluation pipeline

2. Implement ablation studies varying prompt structures and temperature settings to quantify their impact on evaluation consistency

3. Scale validation to include 10,000+ query-document pairs across diverse query categories to establish statistical robustness and identify edge cases where GPT-3.5 evaluations diverge from human judgment