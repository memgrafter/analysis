---
ver: rpa2
title: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly
  of Global and Local Attention
arxiv_id: '2406.12718'
source_url: https://arxiv.org/abs/2406.12718
tags:
- agla
- arxiv
- image
- regular
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies attention deficiency as a root cause of object
  hallucinations in Large Vision-Language Models (LVLMs), where models predominantly
  focus on global image features rather than prompt-relevant local features. To address
  this, the authors propose Assembly of Global and Local Attention (AGLA), a training-free
  decoding method that combines generative global features from original images with
  discriminative local features from augmented images.
---

# Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention

## Quick Facts
- **arXiv ID**: 2406.12718
- **Source URL**: https://arxiv.org/abs/2406.12718
- **Reference count**: 40
- **Primary result**: Assembly of Global and Local Attention (AGLA) significantly reduces object hallucinations in LVLMs by combining global features from original images with local features from augmented images

## Executive Summary
This paper addresses the critical problem of object hallucinations in Large Vision-Language Models (LVLMs), where models often fail to accurately identify objects in images despite being able to generate fluent responses. The authors identify attention deficiency as the root cause, with LVLMs predominantly focusing on global image features rather than prompt-relevant local features. To address this, they propose AGLA, a training-free decoding method that fuses generative global features from original images with discriminative local features from augmented images created through an image-prompt matching scheme. Experiments across multiple generative and discriminative benchmarks demonstrate significant improvements in hallucination mitigation and general perception capabilities.

## Method Summary
AGLA is a training-free decoding approach that mitigates object hallucinations by assembling generative global features from original images with discriminative local features from augmented images. The method uses an image-prompt matching scheme to create augmented views that highlight prompt-relevant content while suppressing irrelevant distractions. Logit fusion combines distributions from both views, and adaptive plausibility constraints selectively truncate tokens to prevent excessive deviation from the original distribution. The approach requires no additional training and can be applied during inference to existing LVLMs.

## Key Results
- AGLA achieves 5.5% accuracy and 5.1% F1 score improvements over regular decoding on the POPE dataset
- On ROPE, AGLA outperforms VCD by 2.3% in Precision and 1.8% in Recall
- AGLA reduces object hallucinations while improving general perception capabilities across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention deficiency in LVLMs is a root cause of object hallucinations because models predominantly attend to global features rather than prompt-relevant local features
- Mechanism: LVLM self-attention weights are dominated by global features and show similar patterns regardless of whether queried objects are present or absent
- Core assumption: Self-attention mechanism reveals which image features receive most attention during response generation
- Evidence: Self-attention weight analysis shows dominance of global features across different object queries

### Mechanism 2
- Claim: AGLA mitigates hallucinations by assembling generative global features with discriminative local features
- Mechanism: Logit fusion combines distributions from original and augmented views, where augmented view highlights prompt-relevant content
- Core assumption: Combining complementary information from different image views creates more accurate decoding distribution
- Evidence: Consistent improvements across multiple benchmarks when combining both views

### Mechanism 3
- Claim: Adaptive plausibility constraints prevent calibrated distribution from deviating excessively from original
- Mechanism: Tokens with original probabilities below threshold β are set to zero probability in AGLA distribution
- Core assumption: Some tokens in original distribution are correct and should be preserved
- Evidence: Truncation mechanism shown in Equation 4 with parameter β

## Foundational Learning

- **Cross-attention mechanisms in vision-language models**
  - Why needed: Essential to understand how LVLMs attend to different image features
  - Quick check: What determines attention weight between text token and image patch in cross-attention?

- **GradCAM and interpretability techniques**
  - Why needed: Image-prompt matching scheme uses GradCAM to identify relevant image regions
  - Quick check: How does GradCAM use gradients to identify important image regions for a given prompt?

- **Contrastive learning and logit fusion**
  - Why needed: AGLA combines distributions from original and augmented images
  - Quick check: What is the difference between adding two logit distributions versus subtracting one from another?

## Architecture Onboarding

- **Component map**: Image → Matching model → GradCAM → Adaptive masking → Augmented image → Both original and augmented images → Cross-attention → Logit fusion → Output generation

- **Critical path**: Original image and text prompt flow through matching model and GradCAM to create augmented view, then both views are combined through logit fusion with plausibility constraints

- **Design tradeoffs**:
  - Accuracy vs. efficiency: Larger matching models improve masking quality but increase inference time
  - Global vs. local feature balance: Weighting factor α controls this tradeoff
  - Strictness vs. flexibility: Truncation parameter β controls how aggressively implausible tokens are filtered

- **Failure signatures**:
  - Poor hallucination mitigation if masking is too aggressive or lenient
  - Suboptimal performance if weighting factor α is poorly tuned
  - Significant inference time increase if matching pipeline is too computationally intensive

- **First 3 experiments**:
  1. Test AGLA with different α values (0, 2, 4) to find optimal fusion weight
  2. Evaluate different β values (0.01, 0.1, 0.5, 1.0) to find optimal truncation threshold
  3. Compare different matching models to assess quality vs efficiency tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: How does AGLA perform on larger LVLMs such as LLaVA 34B and Flamingo 70B?
  - Basis: Paper suggests evaluating on larger models in future work due to resource constraints
  - Evidence needed: Experimental results comparing AGLA on larger LVLMs with current models

- **Open Question 2**: Can AGLA be extended to handle data from other modalities such as videos?
  - Basis: Paper mentions potential extension to other modalities like videos
  - Evidence needed: Experimental results on video tasks like video question answering

- **Open Question 3**: What is the impact of different masking strategies on AGLA's performance?
  - Basis: Paper discusses various masking strategies in ablation study
  - Evidence needed: Comprehensive comparative analysis across multiple tasks and datasets

- **Open Question 4**: How does AGLA handle queries with multiple objects or complex real-world scenarios?
  - Basis: Paper shows AGLA can handle multiple-object queries on ROPE dataset
  - Evidence needed: Performance evaluation on complex real-world datasets with numerous objects

## Limitations

- The approach relies heavily on the quality of the image-prompt matching model and GradCAM-based masking strategy
- Limited ablation studies on choice of α and β parameters make it difficult to assess hyperparameter robustness
- Training-free nature means method cannot adapt to specific model weaknesses through fine-tuning

## Confidence

- **High confidence**: Identification of attention deficiency as root cause, supported by self-attention weight analysis
- **Medium confidence**: Effectiveness of logit fusion combining generative and discriminative features
- **Medium confidence**: Adaptive plausibility constraints mechanism, though threshold choices appear somewhat arbitrary

## Next Checks

1. **Ablation on matching model quality**: Replace BLIP-ITM with smaller matching model to assess whether benefits depend on high-quality matching

2. **