---
ver: rpa2
title: Real-Time Incremental Explanations for Object Detectors in Autonomous Driving
arxiv_id: '2408.11963'
source_url: https://arxiv.org/abs/2408.11963
tags:
- object
- saliency
- image
- d-rise
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INCX introduces a real-time black-box explainability algorithm
  for object detectors in autonomous driving. It avoids multiple model calls by applying
  linear transformations to saliency maps from the first frame, enabling explanations
  two orders of magnitude faster than state-of-the-art D-RISE.
---

# Real-Time Incremental Explanations for Object Detectors in Autonomous Driving

## Quick Facts
- **arXiv ID**: 2408.11963
- **Source URL**: https://arxiv.org/abs/2408.11963
- **Reference count**: 40
- **Primary result**: INCX achieves real-time explanations for object detectors in autonomous driving, running two orders of magnitude faster than D-RISE while maintaining comparable explanation quality.

## Executive Summary
INCX introduces a novel real-time black-box explainability algorithm for object detectors in autonomous driving. The method addresses the computational bottleneck of existing explainers by computing explanations incrementally rather than independently for each frame. By leveraging object tracking and linear transformations of saliency maps, INCX avoids the expensive multiple model calls required by state-of-the-art approaches like D-RISE. The algorithm demonstrates comparable explanation quality while achieving dramatic speed improvements, making real-time explainability feasible for safety-critical autonomous driving applications.

## Method Summary
INCX computes initial saliency maps using D-RISE on the first frame, then tracks objects across subsequent frames using SORT. For each tracked object, it applies linear transformations (scaling and translation) to the initial saliency map based on bounding box changes, formalized through probability mass function transformations. The method uses binary search to find sufficient explanation thresholds, starting from the previous frame's threshold to improve efficiency. Evaluation on four driving datasets (BDD100K, KITTI, NuScenes, VIPER) with three object detectors (Faster R-CNN, YOLOv10, RT-DETR) shows INCX achieves real-time performance while maintaining explanation quality comparable to D-RISE.

## Key Results
- INCX runs 170.55s vs 1.06s on BDD100K with Faster R-CNN, achieving two orders of magnitude speedup
- Explanation quality metrics (insertion/deletion curves, Energy-based Pointing Game, Explanation Proportion) show comparable performance to D-RISE
- The method maintains sufficient explanations across frames while avoiding multiple model calls through linear transformations
- Real-time performance is achieved across all tested object detectors and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INCX achieves real-time explanations by avoiding multiple model calls through linear transformation of saliency maps
- Mechanism: The algorithm computes the initial saliency map using D-RISE on the first frame, then tracks objects across subsequent frames using SORT. For each tracked object, it applies linear transformations (scaling and translation) to the initial saliency map based on bounding box movement and size changes
- Core assumption: Object movement in 3D space without rotation or deformation results in only scaling and linear translation when projected onto the image plane
- Evidence anchors:
  - [abstract] "The algorithm is based on linear transformations of saliency maps, producing sufficient explanations"
  - [section] "Lemma 1. For a fixed observer, movement of an object in 3D space without rotation or deformation can only result in a combination of scaling and linear translation when projected on a given vertical plane"
- Break condition: Objects leave the frame, rotate, or are occluded by other objects

### Mechanism 2
- Claim: The saliency map transformation preserves sufficient explanation quality through probability mass function (pmf) transformation
- Mechanism: The initial saliency map is normalized to create a pmf, then Theorem 1 applies linear transformations to this pmf using Lemma 2 from probability theory, allowing computation of subsequent frame saliency maps without additional model calls
- Core assumption: The expected value of the saliency pmf equals the center of the bounding box plus a constant offset (δt), ensuring homogeneity
- Evidence anchors:
  - [section] "The proof is based on converting the saliency map to a probability mass function (pmf) and then applying the mathematics of pmf transformation"
  - [section] "Theorem 1. If Assumptions 1 and 2 hold, then the pmf at time t + 1, pQt+1(qt+1), can be computed from the pmf at time t using the following equation"
- Break condition: Non-rigid object deformation or significant changes in object appearance

### Mechanism 3
- Claim: Sufficient explanations are maintained through binary search on saliency map thresholds
- Mechanism: For the initial frame, the explanation procedure divides the saliency map into sections and uses binary search to find the smallest sufficient region. For subsequent frames, it starts binary search around the previous threshold to maintain explanation quality while improving efficiency
- Core assumption: Temporal consistency in saliency maps means the explanation threshold in subsequent frames will be close to that of previous frames
- Evidence anchors:
  - [section] "We use binary search to efficiently locate a threshold (depicted by the red dotted line) that provides a sufficient, albeit not necessarily minimal, explanation"
  - [section] "Assuming temporal consistency in the saliency map, it is reasonable to expect that the explanation threshold in subsequent frames will be close to that of the previous frames"
- Break condition: Sudden appearance or disappearance of objects, or significant changes in scene lighting/conditions

## Foundational Learning

- Concept: Probability Mass Function (pmf) transformation
  - Why needed here: The core mechanism relies on transforming normalized saliency maps (treated as pmf) using linear transformations
  - Quick check question: How does scaling a random variable affect its probability mass function distribution?

- Concept: Kalman filtering and Hungarian algorithm for object tracking
  - Why needed here: SORT uses Kalman filters for object state estimation and Hungarian algorithm for optimal assignment between current detections and tracked objects
  - Quick check question: What are the key differences between Kalman filtering and particle filtering for object tracking?

- Concept: Bounding box transformation and coordinate mapping
  - Why needed here: The algorithm transforms bounding boxes to compute linear transformations applied to saliency maps
  - Quick check question: How does the center of a bounding box change when an object scales up by a factor of 2?

## Architecture Onboarding

- Component map:
  - Input: Video frames
  - Object Detector (YOLO, Faster R-CNN, RT-DETR)
  - Initial Saliency Generator (D-RISE)
  - Object Tracker (SORT)
  - Saliency Transformer (Theorem 1 implementation)
  - Explanation Extractor (Binary search procedure)
  - Output: Saliency maps and sufficient explanations

- Critical path: Video frame → Object Detector → Initial Saliency (Frame 0) → Object Tracker → Saliency Transformer → Explanation Extractor → Output

- Design tradeoffs:
  - Speed vs. explanation quality: Larger ln parameter improves speed but may reduce explanation quality
  - Memory vs. accuracy: Storing more frames allows better tracking but increases memory usage
  - Assumption strictness vs. applicability: Looser assumptions increase applicability but may reduce explanation quality

- Failure signatures:
  - Objects disappearing from explanations: Tracking failure or object leaving frame
  - Poor explanation quality: Rotation, occlusion, or significant appearance changes
  - System slowdown: Too many tracked objects exceeding computational limits

- First 3 experiments:
  1. Single object tracking with constant velocity to verify linear transformation accuracy
  2. Object scaling test to verify size change handling
  3. Multi-object scenario with occlusions to test tracking robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does INCX performance degrade with increased object rotation or deformation in autonomous driving scenarios?
- Basis in paper: [inferred] The paper explicitly states that INCX assumes objects are rigid and do not rotate, but notes this assumption is "reasonable in practice for autonomous driving." The limitations section suggests this is an untested boundary condition.
- Why unresolved: The paper focuses on demonstrating the algorithm works well under ideal conditions but does not systematically test performance with varying degrees of object rotation or deformation.
- What evidence would resolve it: Systematic testing of INCX on datasets with controlled object rotation and deformation, measuring quality metric degradation as rotation angle or deformation severity increases.

### Open Question 2
- Question: Can INCX be extended to handle occlusions between objects while maintaining real-time performance?
- Basis in paper: [explicit] The limitations section states "Our experimental results show that this assumption is reasonable in practice for autonomous driving" regarding objects not being occluded, but this is presented as an untested assumption.
- Why unresolved: The paper demonstrates effectiveness when objects remain fully visible but doesn't address scenarios where objects are partially or fully occluded by other objects in the scene.
- What evidence would resolve it: Experimental results showing INCX performance on datasets with known occlusion patterns, or a modified algorithm that explicitly handles occlusion detection and recovery.

### Open Question 3
- Question: What is the optimal timeout parameter for tracking objects that temporarily leave the frame, balancing between explanation continuity and computational efficiency?
- Basis in paper: [explicit] The algorithm description mentions defining "a timeout that allows us to stop attempting to explain an object, which is no longer detectable" but provides no guidance on optimal timeout selection.
- Why unresolved: The paper acknowledges the need for timeout parameters but doesn't provide empirical analysis of how different timeout values affect performance or quality metrics.
- What evidence would resolve it: Empirical analysis showing quality metric performance across different timeout durations, identifying the optimal balance point between explanation accuracy and computational overhead.

## Limitations
- The method assumes objects are rigid and undergo only linear transformations (scaling/translation), breaking down with rotation or deformation
- Performance depends on tracking quality, with failures occurring when objects are occluded or leave the frame
- The approach is specifically designed for autonomous driving scenarios and may not generalize well to other domains

## Confidence

- **High Confidence**: Real-time performance claims (based on direct timing measurements against D-RISE baseline)
- **Medium Confidence**: Explanation quality comparisons (insertion/deletion metrics show parity but may not capture all failure modes)
- **Low Confidence**: Generalizability to non-driving scenarios (method specifically tuned for driving datasets)

## Next Checks

1. **Robustness Test**: Evaluate INCX performance on videos with high object occlusion rates and rotational movement to quantify breakdown conditions for the linear transformation assumption.

2. **Cross-Domain Transfer**: Apply the method to non-driving video datasets (e.g., surveillance footage, sports videos) to assess generalizability beyond the autonomous driving domain.

3. **Tracking Failure Analysis**: Systematically measure the impact of tracking failures on explanation quality by injecting synthetic tracking errors and measuring degradation in saliency map fidelity.