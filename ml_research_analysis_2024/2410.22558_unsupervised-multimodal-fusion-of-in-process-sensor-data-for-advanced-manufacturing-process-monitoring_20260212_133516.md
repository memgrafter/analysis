---
ver: rpa2
title: Unsupervised Multimodal Fusion of In-process Sensor Data for Advanced Manufacturing
  Process Monitoring
arxiv_id: '2410.22558'
source_url: https://arxiv.org/abs/2410.22558
tags:
- data
- manufacturing
- learning
- process
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel unsupervised multimodal data fusion
  approach for manufacturing process monitoring, inspired by the Contrastive Language-Image
  Pre-training (CLIP) model. The method leverages contrastive learning techniques
  to correlate different data modalities without requiring labeled datasets, overcoming
  limitations of traditional supervised machine learning methods in manufacturing
  contexts.
---

# Unsupervised Multimodal Fusion of In-process Sensor Data for Advanced Manufacturing Process Monitoring

## Quick Facts
- arXiv ID: 2410.22558
- Source URL: https://arxiv.org/abs/2410.22558
- Reference count: 0
- Key outcome: Novel unsupervised multimodal fusion approach for manufacturing process monitoring using contrastive learning without labeled datasets

## Executive Summary
This paper introduces an unsupervised multimodal data fusion approach for manufacturing process monitoring, inspired by the CLIP model. The method leverages contrastive learning to correlate different data modalities without requiring labeled datasets, overcoming limitations of traditional supervised machine learning methods in manufacturing contexts. The approach demonstrates the ability to handle and learn encoders for five distinct modalities: visual imagery, audio signals, laser position (x and y coordinates), and laser power measurements. By compressing high-dimensional datasets into low-dimensional representational spaces, the method facilitates downstream tasks such as process control, anomaly detection, and quality assurance.

## Method Summary
The approach uses contrastive learning techniques inspired by CLIP to fuse multimodal manufacturing data without labeled datasets. Five distinct modalities are processed: visual imagery (via ResNet18 encoder), audio signals (via adapted Whisper model), laser position coordinates, and laser power measurements (via custom MultiScaleLSTM encoder). The encoders project data into a shared 32-dimensional embedding space, where matching modality pairs are pulled together while non-matching pairs are pushed apart using contrastive loss. The trained encoders are then used to generate embeddings for all data tuples, which are reduced to 2D using UMAP for visualization and analysis of process states.

## Key Results
- Successfully trained encoders to project five different modalities into a shared 32-dimensional embedding space
- Achieved high accuracy in correlating multimodal data for both in-process LPBF and post-process lattice structure datasets
- Generated meaningful low-dimensional representations that enable visualization and clustering of process states
- Demonstrated unsupervised learning capability without requiring labeled training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning enables multimodal fusion without labeled data by training encoders to maximize similarity of matching modality pairs and minimize similarity of non-matching pairs
- Mechanism: The contrastive loss function pulls embeddings of corresponding image-audio tuples together in a shared low-dimensional space while pushing apart embeddings of non-matching pairs
- Core assumption: The paired data tuples contain meaningful correlations that can be discovered through this self-supervised learning approach
- Evidence anchors: Abstract confirms contrastive learning correlates modalities without labeled data; methodology section describes CLIP contrastive loss function

### Mechanism 2
- Claim: Using pre-trained models like Whisper as audio encoders enables effective feature extraction from manufacturing acoustics without requiring extensive training data
- Mechanism: The Whisper model, originally trained on human speech, is adapted to extract meaningful features from manufacturing audio signals by removing the classification head and adding convolutional layers
- Core assumption: Feature extraction capabilities learned from speech can transfer to manufacturing acoustics due to shared acoustic signal properties
- Evidence anchors: Methodology section details Whisper model adaptation; authors note effectiveness was not guaranteed

### Mechanism 3
- Claim: Dimensionality reduction via UMAP on the concatenated embeddings enables visualization and clustering of process states for anomaly detection
- Mechanism: After training, data tuples are passed through all encoders, concatenated into a single vector, and then reduced to 2D using UMAP for visualization and analysis
- Core assumption: The low-dimensional embedding space preserves meaningful relationships between process states that can be visualized and analyzed
- Evidence anchors: Results section describes UMAP usage and visualization of latent space embeddings

## Foundational Learning

- Concept: Contrastive learning and self-supervised learning
  - Why needed here: Traditional supervised learning requires labeled datasets which are expensive and time-consuming to create in manufacturing environments
  - Quick check question: How does contrastive loss differ from traditional supervised classification loss?

- Concept: Multimodal data fusion and representation learning
  - Why needed here: Manufacturing processes generate diverse data types that contain complementary information requiring joint representations
  - Quick check question: What challenges arise when fusing high-dimensional multimodal data compared to single-modality data?

- Concept: Dimensionality reduction techniques (UMAP, PCA)
  - Why needed here: Learned embedding spaces are high-dimensional, making direct analysis and visualization difficult
  - Quick check question: When would PCA be preferred over UMAP for dimensionality reduction of embedding spaces?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing → Encoder models (ResNet18 for images, Whisper for audio, custom LSTM for sensor data) → Contrastive loss training → Embedding generation → Dimensionality reduction (UMAP) → Analysis/visualization
- Critical path: The training pipeline (data → encoders → contrastive loss) must converge to meaningful embeddings before downstream analysis can be performed effectively
- Design tradeoffs: Using pre-trained models provides good feature extraction with less training data but may not perfectly match manufacturing acoustics
- Failure signatures: Poor training accuracy (below 90%), failure to produce distinct clusters in latent space, inability to discriminate between nominal and off-nominal builds
- First 3 experiments:
  1. Train on a small subset of data (1-2 builds) to verify basic functionality and convergence
  2. Test encoder performance on held-out validation data to check for overfitting
  3. Visualize latent space with UMAP to verify meaningful clustering of process states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum viable encoder architecture for each modality that would still maintain adequate performance for manufacturing process monitoring?
- Basis in paper: Authors note future work could investigate minimum viable encoders for real-time process control
- Why unresolved: Paper used ResNet18 and Whisper tiny models which may be larger than necessary for manufacturing domain
- What evidence would resolve it: Systematic ablation studies comparing various encoder architectures across five modalities

### Open Question 2
- Question: How does the performance of the MultiScaleLSTM encoder for DAQ signals compare to alternative architectures?
- Basis in paper: Authors used MultiScaleLSTM but note more work is needed to understand its effects on training
- Why unresolved: Novel architecture introduced without comparative analysis against other time-series encoders
- What evidence would resolve it: Head-to-head comparison of MultiScaleLSTM against standard LSTMs, GRUs, or temporal convolutional networks

### Open Question 3
- Question: Can the contrastive learning framework be extended to directly predict specific types of defects or process anomalies?
- Basis in paper: Authors mention broad applications for engineering data analysis but don't explore direct defect classification
- Why unresolved: Current approach creates general representations but doesn't investigate mapping to specific defect types
- What evidence would resolve it: Experiments applying supervised fine-tuning to learned representations for defect classification

## Limitations

- Adaptation of pre-trained speech models for manufacturing acoustics remains unverified without ablation studies
- Method's effectiveness across diverse manufacturing domains is theoretical rather than empirically validated
- Contrastive learning framework assumes meaningful correlations exist between all five modalities without analysis of individual modality contributions

## Confidence

- High confidence: General feasibility of contrastive learning for multimodal fusion in manufacturing contexts
- Medium confidence: Effectiveness of pre-trained Whisper models for manufacturing audio feature extraction
- Low confidence: Generalizability of results across different manufacturing processes and sensor configurations

## Next Checks

1. Conduct ablation studies to quantify the contribution of each modality by training models with different modality subsets
2. Test the approach on a different manufacturing process (e.g., CNC machining) with different sensor configurations
3. Compare the pre-trained Whisper audio encoder against a custom-trained audio encoder specifically optimized for manufacturing acoustics