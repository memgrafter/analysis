---
ver: rpa2
title: '$Se^2$: Sequential Example Selection for In-Context Learning'
arxiv_id: '2402.13874'
source_url: https://arxiv.org/abs/2402.13874
tags:
- answer
- sentence
- examples
- question
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Se2 is a sequential example selection method for in-context learning
  that addresses the limitations of prior "select then organize" paradigms by modeling
  inter-relationships and sequential information among examples. It leverages LLM
  feedback across varying contexts to construct example sequences, then uses beam
  search to enhance quality and diversity.
---

# $Se^2$: Sequential Example Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2402.13874
- Source URL: https://arxiv.org/abs/2402.13874
- Reference count: 36
- Primary result: 42% relative improvement over random selection on 23 NLP tasks

## Executive Summary
$Se^2$ introduces a sequential example selection method for in-context learning that addresses the limitations of prior "select then organize" paradigms. The method models inter-relationships and sequential information among examples by leveraging LLM feedback across varying contexts. Using beam search to enhance quality and diversity, $Se^2$ achieves significant performance improvements across 23 NLP tasks spanning 8 categories, demonstrating superior results compared to competitive baselines including BM25, SBERT, Instructor, AES, and UPRISE.

## Method Summary
$Se^2$ constructs example sequences incrementally by updating the context with each selected example, allowing the LLM to evaluate examples conditioned on previously selected ones. The method uses LLM feedback (SNLU/SNLG functions) to score examples, then employs beam search to maintain w candidate sequences and expand the top-w examples at each step. A bi-encoder model is trained with InfoNCE loss to match inputs with optimal examples, and during inference, beam search with the bi-encoder constructs final example sequences for prediction using the LLM.

## Key Results
- 42% relative improvement over random selection across 23 NLP tasks
- Superior performance compared to competitive baselines (BM25, SBERT, Instructor, AES, UPRISE)
- Strong stability and transferability across different LLMs
- Ability to identify examples with inherent logical relationships

## Why This Works (Mechanism)

### Mechanism 1
Sequential selection outperforms "select then organize" by modeling inter-relationships between examples. The method constructs example sequences incrementally, updating the context with each selected example, allowing the LLM to evaluate examples conditioned on previously selected ones. Core assumption: optimal example sequence depends on order and relationships. Evidence: abstract states "neglect the internal relationships between examples and exist an inconsistency between the training and inference." Break condition: if relationships are irrelevant, sequential modeling adds no value.

### Mechanism 2
Beam search improves both quality and diversity of example sequences. Instead of greedy selection, the method maintains w candidate sequences and expands the top-w examples at each step, accumulating scores to find optimal sequences. Core assumption: global optimal sequence not found through greedy local choices. Evidence: abstract mentions "utilize beam search to seek and construct example sequences, enhancing both quality and diversity." Break condition: if search space is too large or beam size too small, performance gain diminishes.

### Mechanism 3
Using LLM feedback as training signal captures what the model truly prefers. The method scores examples using the LLM itself (SNLU/SNLG functions) rather than external metrics like similarity. Core assumption: LLM's own likelihood or metric-based scoring better captures example utility than semantic similarity. Evidence: section 3.1 states "An intuitive and more generalized approach involves scoring by the LLM itself." Break condition: if LLM scoring is unreliable or biased, the method inherits these flaws.

## Foundational Learning

- Concept: In-context learning (ICL) and its dependence on demonstration examples
  - Why needed here: The paper builds a method specifically for improving ICL through better example selection
  - Quick check question: What is the fundamental difference between ICL and traditional fine-tuning?

- Concept: Beam search algorithm and its application in sequence generation
  - Why needed here: The method uses beam search to construct example sequences during inference
  - Quick check question: How does beam search differ from greedy search in terms of search space exploration?

- Concept: Bi-encoder architecture for example retrieval
  - Why needed here: The method uses separate encoders for examples (Ee) and inputs (Ex) with inner product scoring
  - Quick check question: What is the advantage of using separate encoders versus a single cross-encoder?

## Architecture Onboarding

- Component map:
  Scoring LLM (GPT-Neo-2.7B) -> Context Sequence Constructor -> Bi-encoder Model (Ee, Ex) -> Beam Search Module -> Retriever Index

- Critical path:
  1. Score examples using LLM for varying contexts
  2. Construct training data with positive/negative examples
  3. Train bi-encoder to match inputs with optimal examples
  4. During inference, use beam search with bi-encoder to construct sequences
  5. Generate predictions using LLM with constructed prompts

- Design tradeoffs:
  Using LLM for scoring provides better signals but increases computational cost; Beam search improves quality but scales exponentially with beam size; Separate encoders are faster than cross-encoders but may miss complex interactions; Random sampling of candidates balances exploration with efficiency

- Failure signatures:
  Poor performance on tasks where example order doesn't matter; Degradation when LLM scoring is unreliable or biased; Memory issues when example pool or beam size becomes too large; Inconsistent results across different LLMs due to lack of standardization

- First 3 experiments:
  1. Compare greedy search (w=1) vs beam search (w=3) on a single task to verify search strategy benefits
  2. Test sequential vs random example selection on a simple NLI task to validate core mechanism
  3. Evaluate transfer performance from GPT-Neo-2.7B to a smaller model to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
How does $Se^2$'s performance scale with larger language models beyond GPT-Neo-2.7B, particularly with state-of-the-art models like GPT-4 or Claude? Basis: authors explicitly acknowledge they haven't tested with larger models due to computational limitations. Why unresolved: computational constraints prevented testing with state-of-the-art LLMs. What evidence would resolve it: systematic testing of $Se^2$ with increasingly larger models (e.g., LLaMA 2 70B, GPT-4, Claude 2) while measuring performance improvements across the same 23 NLP tasks.

### Open Question 2
Can the sequential example selection approach be extended to handle multi-turn dialogue or conversational contexts where examples need to be selected dynamically based on ongoing conversation history? Basis: current framework is designed for one-shot or few-shot learning scenarios with fixed contexts. Why unresolved: the paper focuses on static example selection for single-input tasks. What evidence would resolve it: implementing $Se^2$ in a conversational AI system where examples are selected and ordered based on both current user input and conversation history.

### Open Question 3
What is the theoretical upper bound on performance improvement when using sequential example selection compared to random selection, and how close does $Se^2$ get to this bound across different task categories? Basis: paper reports 42% relative improvement but doesn't establish maximum possible improvement. Why unresolved: while empirical results show substantial improvement, the paper doesn't provide theoretical analysis of the optimal example sequence. What evidence would resolve it: conducting exhaustive search (where computationally feasible) to find optimal example sequences for smaller tasks, then comparing $Se^2$'s performance against these optimal sequences.

## Limitations
- Evaluation primarily on English NLP tasks with unclear generalization to other languages or domains
- Performance claims based on comparison with only five baselines, leaving questions about performance against more diverse selection methods
- Beam search approach introduces computational overhead that scales exponentially with beam size, tradeoff not thoroughly analyzed

## Confidence
**High Confidence**: Sequential selection mechanism and beam search approach are technically sound and well-documented. Performance improvements over random selection (42% relative improvement) are substantial and consistent across multiple task categories.

**Medium Confidence**: Claims about superior performance compared to competitive baselines are supported by experimental results but may be sensitive to hyperparameter choices and implementation details. Transferability results show promise but are based on limited LLM configurations.

**Low Confidence**: Claims about identifying "inherent logical relationships" between examples are supported by qualitative examples but lack systematic validation. Analysis of why sequential selection outperforms non-sequential approaches relies on indirect evidence.

## Next Checks
1. **Cross-lingual Transfer**: Evaluate $Se^2$ on multilingual datasets to test whether sequential selection advantage transfers across languages, particularly for low-resource languages where demonstration examples are most critical.

2. **Robustness to Noisy Examples**: Systematically test performance degradation when example pool contains corrupted or mislabeled examples to assess whether LLM feedback mechanism can identify and avoid poor demonstrations.

3. **Computational Cost Analysis**: Measure and compare wall-clock time and memory requirements of $Se^2$ against baseline methods across different beam sizes and example pool sizes to quantify practical tradeoff between performance and efficiency.