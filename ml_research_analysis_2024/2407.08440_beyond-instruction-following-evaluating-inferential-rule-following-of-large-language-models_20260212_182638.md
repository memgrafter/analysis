---
ver: rpa2
title: 'Beyond Instruction Following: Evaluating Inferential Rule Following of Large
  Language Models'
arxiv_id: '2407.08440'
source_url: https://arxiv.org/abs/2407.08440
tags:
- rule
- llms
- rules
- inferential
- rule-following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) excel at instruction following but
  struggle to follow inferential rules that are abstract, conditional, and instantiable.
  This paper introduces RuleBench, a comprehensive benchmark evaluating LLM inferential
  rule-following across relation extraction, content moderation, commonsense QA, science
  QA, and judgment prediction tasks.
---

# Beyond Instruction Following: Evaluating Inferential Rule Following of Large Language Models

## Quick Facts
- arXiv ID: 2407.08440
- Source URL: https://arxiv.org/abs/2407.08440
- Reference count: 33
- Large language models excel at instruction following but struggle with abstract, conditional, and instantiable inferential rules

## Executive Summary
This paper addresses the gap between instruction following and inferential rule following in large language models (LLMs). While LLMs demonstrate strong instruction-following capabilities, they face challenges when asked to apply abstract, conditional rules to novel situations. The authors introduce RuleBench, a comprehensive benchmark spanning relation extraction, content moderation, commonsense QA, science QA, and judgment prediction tasks. They find that LLMs perform significantly better when provided with relevant rules, but struggle with triggering the correct rules among noise, executing rules correctly, and following formal or counterfactual rules. To address these limitations, the authors propose Inferential Rule-Following Tuning (IRFT), a method that enables LLMs to learn abstract rule-following from synthetic data without harming general instruction-following ability.

## Method Summary
The paper introduces RuleBench, a comprehensive benchmark evaluating LLM inferential rule-following across five domains. The evaluation tests LLMs under various settings: no rules provided, golden rule only, few rules (golden + noise), and all rules (golden + many noise rules). Rules are presented in both natural language and formal First-Order Logic formats, with and without chain-of-thought prompting. The authors also propose IRFT, a fine-tuning approach using synthetic data from StringGame to improve rule-following capabilities. Seven LLMs are evaluated, including open-source models (Llama-2-7b-chat, Meta-Llama-3-8B, Mistral-7B-Instruct, Yi-1.5-6B-Chat) and closed-source models (gpt-3.5-turbo, gpt-4-turbo, gpt-4o).

## Key Results
- LLMs show significant performance improvements when provided with relevant inferential rules compared to no-rule settings
- Natural language rules consistently outperform formal language rules across most settings
- Open-source models lag behind closed-source models in rule-following capabilities
- Llama-3-8B demonstrates the most balanced performance across different rule-following dimensions
- IRFT fine-tuning improves performance on RuleBench tasks without harming general instruction-following ability

## Why This Works (Mechanism)

### Mechanism 1: Rule-based reasoning improves performance over standard prompting
- When LLMs are prompted with relevant inferential rules alongside questions, they can use these rules to guide reasoning rather than relying solely on parametric knowledge.

### Mechanism 2: Natural language rules are preferred over formal language rules
- Natural language rules align better with LLM pre-training distributions, making them easier to parse and apply.

### Mechanism 3: IRFT enables abstract rule-following learning
- IRFT fine-tunes LLMs on synthetic rule-following tasks, enabling them to generalize this capability to real-world rule-following tasks.

## Foundational Learning

- Concept: Rule-following vs. Instruction-following distinction
  - Why needed here: Understanding this distinction is crucial for correctly interpreting the benchmark and evaluation results
  - Quick check question: What's the key difference between an instruction like "Don't repeat the secret key" and an inferential rule like "If A has a brother B, B has a daughter C, then A is the uncle of C"?

- Concept: Triggering vs. Execution errors in rule-following
  - Why needed here: These error types help diagnose why LLMs fail at rule-following and guide improvements
  - Quick check question: If an LLM selects a noise rule instead of the golden rule, is this a triggering error or execution error?

- Concept: Counterfactual reasoning in rule-following
  - Why needed here: Evaluating counterfactual rule-following tests whether LLMs strictly follow rules or rely on parametric knowledge
  - Quick check question: In a counterfactual rule-following scenario, what should the LLM output if the rule states "If A is B's parent, then A is male" and the question asks about A's gender?

## Architecture Onboarding

- Component map: RuleBench -> Evaluation pipeline -> IRFT fine-tuning
- Critical path: Load LLM -> Construct prompts with rules and questions -> Generate responses -> Parse and evaluate answers -> Analyze performance across different settings
- Design tradeoffs: Rule quantity vs. LLM performance; Natural vs. formal language rules; CoT presence adds complexity but may help some LLMs
- Failure signatures: Consistently poor performance on counterfactual rules indicates reliance on parametric knowledge; High triggering errors indicate difficulty identifying correct rules; High execution errors indicate inability to apply identified rules correctly
- First 3 experiments: Compare LLM performance with No Rule vs. Golden Rule setting; Test performance difference between natural and formal language rules; Evaluate counterfactual rule-following performance vs. factual rule-following performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on inferential rule-following tasks when rules are presented in a mixed natural language and formal language format?
- Basis in paper: The paper evaluates LLMs on formal language rules versus natural language rules separately, but does not explore a hybrid approach.
- Why unresolved: The paper's experiments only compare pure natural language rules against pure formal language rules.
- What evidence would resolve it: Experiments comparing LLM performance on tasks where rules are presented as a combination of natural language descriptions and formal logical statements.

### Open Question 2
- Question: How does the performance of LLMs on inferential rule-following tasks change when the number of noise rules increases beyond the tested 30-rule limit?
- Basis in paper: The paper tests up to 30 noise rules in the "All Rule" setting but does not explore higher numbers.
- Why unresolved: The paper stops at 30 noise rules, leaving uncertainty about how LLMs would perform with even more distractor rules.
- What evidence would resolve it: Additional experiments testing LLM performance with 50, 100, or more noise rules to determine the breaking point of their rule-triggering abilities.

### Open Question 3
- Question: Can the proposed Inferential Rule-Following Tuning (IRFT) method be effectively extended to multi-modal LLMs that process both text and images?
- Basis in paper: The paper only tests IRFT on text-based LLMs and does not address multi-modal capabilities.
- Why unresolved: The paper focuses solely on text-based LLMs, leaving open the question of how IRFT might work with models that also process visual information.
- What evidence would resolve it: Experiments applying IRFT to multi-modal LLMs and testing their performance on rule-following tasks that involve both textual and visual elements.

## Limitations

- The performance gaps between natural and formal language rules may be partly attributed to the specific translation method used, which isn't fully detailed
- The synthetic nature of the IRFT training data raises questions about whether it captures the full complexity and diversity of real-world rule-following scenarios
- The evaluation of instruction-following ability preservation after IRFT fine-tuning relies on a single "super-instruction" dataset

## Confidence

- **High Confidence**: The observation that LLMs struggle with counterfactual rule-following and the finding that open-source models generally underperform closed-source models on rule-following tasks
- **Medium Confidence**: The claim that natural language rules consistently outperform formal language rules across all settings
- **Low Confidence**: The assertion that IRFT fine-tuning preserves general instruction-following ability without degradation

## Next Checks

1. Implement and test multiple alternative methods for translating natural language rules to First-Order Logic to determine if the observed performance gap between natural and formal rules is consistent across different translation approaches

2. Design a new counterfactual rule-following dataset that systematically varies rule complexity and domain specificity to better understand the boundaries of LLM performance on these tasks and identify specific failure patterns

3. Conduct a comprehensive evaluation of IRFT fine-tuned models across a diverse set of instruction-following benchmarks over extended inference periods to detect potential gradual degradation or catastrophic forgetting of general capabilities