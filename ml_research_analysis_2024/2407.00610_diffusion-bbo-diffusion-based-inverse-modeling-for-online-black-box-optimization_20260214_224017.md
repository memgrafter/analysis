---
ver: rpa2
title: 'Diffusion-BBO: Diffusion-Based Inverse Modeling for Online Black-Box Optimization'
arxiv_id: '2407.00610'
source_url: https://arxiv.org/abs/2407.00610
tags:
- function
- optimization
- diffusion
- uncertainty
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Diff-BBO, an online black-box optimization
  framework that leverages conditional diffusion models for inverse modeling. Diff-BBO
  introduces a novel acquisition function called Uncertainty-aware Exploration (UaE)
  that proposes objective values based on uncertainty estimates from conditional diffusion
  models.
---

# Diffusion-BBO: Diffusion-Based Inverse Modeling for Online Black-Box Optimization

## Quick Facts
- arXiv ID: 2407.00610
- Source URL: https://arxiv.org/abs/2407.00610
- Authors: Dongxia Wu; Nikki Lijing Kuang; Ruijia Niu; Yi-An Ma; Rose Yu
- Reference count: 28
- Key outcome: Diff-BBO outperforms existing online BBO baselines across 6 scientific discovery tasks using conditional diffusion models with uncertainty-aware exploration

## Executive Summary
This paper introduces Diff-BBO, an online black-box optimization framework that leverages conditional diffusion models for inverse modeling. Instead of learning p(y|x) like traditional methods, Diff-BBO learns p(x|y) to map objective values back to the design space, avoiding invalid inputs in scientific discovery tasks. The framework employs a novel Uncertainty-aware Exploration (UaE) acquisition function that balances exploitation and exploration by proposing objective values based on epistemic uncertainty estimates from conditional diffusion models.

The authors theoretically prove that UaE achieves near-optimal solutions and demonstrate empirically that Diff-BBO outperforms existing online BBO baselines across 6 scientific discovery tasks including material design, robot morphology optimization, and molecular discovery. The approach shows particular promise for high-dimensional problems where traditional Bayesian optimization methods struggle.

## Method Summary
Diff-BBO uses conditional diffusion models to learn the inverse mapping from objective values to design space, generating valid candidate solutions. The framework trains an ensemble of diffusion models to estimate both aleatoric and epistemic uncertainty, which feeds into the UaE acquisition function. This function selects objective values that maximize both the objective value and minimize epistemic uncertainty. The selected objective values are then used to conditionally sample designs from the diffusion model, which are evaluated by the black-box oracle. The process iterates, updating the dataset with new observations and retraining the diffusion models.

## Key Results
- Diff-BBO achieves better sample efficiency than existing online BBO methods across 6 scientific discovery tasks
- The Uncertainty-aware Exploration (UaE) acquisition function demonstrates effective balance between exploration and exploitation
- Theoretical analysis proves UaE achieves near-optimal solutions under certain conditions
- The framework shows scalability to high-dimensional problems (dimensions ranging from 8 to 86 in experiments)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional diffusion models can learn high-dimensional data manifolds, avoiding invalid inputs in scientific discovery tasks.
- Mechanism: The model learns p(x|y) instead of p(y|x), mapping objective space back to design space. This inverse modeling approach ensures samples stay within valid regions of the design space.
- Core assumption: The data manifold contains all valid designs, and the diffusion model can accurately capture this manifold.
- Evidence anchors:
  - [abstract]: "Recently, inverse modeling approaches that map the objective space to the design space with conditional diffusion models have demonstrated impressive capability in learning the data manifold."
  - [section]: "Our approach leverages the power of diffusion model to represent p(x|y, D), allowing it to provide high-quality candidate solutions in high-dimensional problem domains and to condition on arbitrary function value."
- Break condition: If the valid design space is disconnected or has complex boundaries that the diffusion model cannot capture, the method may generate invalid samples.

### Mechanism 2
- Claim: Uncertainty-aware exploration (UaE) acquisition function balances exploitation and exploration by proposing objective values based on epistemic uncertainty estimates.
- Mechanism: The acquisition function α(y, D) = y - ∆epistemic(y, D) selects objective values that are both high and have low epistemic uncertainty, guiding the conditional sampling process.
- Core assumption: Epistemic uncertainty accurately reflects the model's uncertainty about the true relationship between objective values and designs.
- Evidence anchors:
  - [abstract]: "Diff-BBO employs a novel acquisition function Uncertainty-aware Exploration (UaE) to propose scores in the objective space for conditional sampling."
  - [section]: "We design a novel acquisition function based on the uncertainty for BBO. Theoretically, we prove that the balance between targeting higher objective values and minimizing epistemic uncertainty lead to optimal optimization outcomes."
- Break condition: If the epistemic uncertainty estimate is inaccurate or not correlated with the true uncertainty, the acquisition function may make poor choices.

### Mechanism 3
- Claim: The uncertainty decomposition into aleatoric and epistemic components allows for more effective exploration strategies.
- Mechanism: Aleatoric uncertainty captures irreducible noise in the generation process, while epistemic uncertainty reflects model uncertainty that can be reduced with more data.
- Core assumption: The variance decomposition accurately captures both sources of uncertainty and their relative importance for the optimization task.
- Evidence anchors:
  - [section]: "To systematically analyze the effect of uncertainty in inverse modeling, we further provide a decomposition in terms of the aleatoric uncertainty and its epistemic counterpart."
  - [section]: "Proposition 1 (Uncertainty Decomposition). At each iteration k ∈ [K], the overall uncertainty in inverse modeling can be decomposed into its aleatoric and epistemic components..."
- Break condition: If the variance decomposition is misspecified or the two uncertainty components are not appropriately weighted, the exploration strategy may be suboptimal.

## Foundational Learning

- Concept: Bayesian inference and posterior distributions
  - Why needed here: Understanding how the posterior distribution p(θ|D) over model parameters is used to quantify epistemic uncertainty
  - Quick check question: What is the relationship between the posterior distribution over model parameters and the epistemic uncertainty in the predictions?

- Concept: Stochastic differential equations and Ornstein-Uhlenbeck processes
  - Why needed here: Understanding how diffusion models can be represented as continuous-time processes and how uncertainty propagates through these processes
  - Quick check question: How does the variance of the Ornstein-Uhlenbeck process change over time, and what does this imply about the uncertainty in the diffusion model?

- Concept: Score-based generative modeling and denoising
  - Why needed here: Understanding how diffusion models learn to denoise data and generate samples from the learned distribution
  - Quick check question: How does the score function relate to the gradient of the log probability density, and why is this useful for generative modeling?

## Architecture Onboarding

- Component map:
  - Conditional diffusion model -> Uncertainty quantification module -> UaE acquisition function -> Oracle query module

- Critical path:
  1. Train conditional diffusion model on current dataset
  2. Compute epistemic uncertainty for candidate objective values
  3. Select y* using UaE acquisition function
  4. Generate designs x conditioned on y*
  5. Query oracle function f with generated designs
  6. Update dataset with new (x, f(x)) pairs

- Design tradeoffs:
  - Using ensemble of diffusion models for uncertainty quantification increases computational cost but provides more reliable estimates
  - The choice of candidate set Y for objective values affects the quality of the proposed y* and the overall optimization performance
  - Batch size N affects the parallelism and efficiency of the oracle queries but may impact the quality of the uncertainty estimates

- Failure signatures:
  - Poor performance on tasks with disconnected or complex valid design spaces
  - Overconfident predictions when epistemic uncertainty is underestimated
  - Slow convergence when the acquisition function fails to balance exploration and exploitation effectively

- First 3 experiments:
  1. Run Diff-BBO on a simple 1D synthetic optimization problem to verify that the conditional sampling works as expected
  2. Compare the epistemic uncertainty estimates from the ensemble of diffusion models to the true uncertainty on a small validation set
  3. Evaluate the impact of the candidate set Y size and range on the optimization performance on a benchmark task

## Open Questions the Paper Calls Out
- None explicitly stated in the provided content

## Limitations
- The framework relies on conditional diffusion models to accurately capture complex data manifolds, which may fail for disconnected or highly complex design spaces
- Computational overhead of maintaining an ensemble of diffusion models for uncertainty quantification is not fully characterized
- Theoretical guarantees assume idealized conditions that may not hold in practice, particularly regarding uncertainty estimates and objective function landscapes

## Confidence
- **High Confidence**: The core mechanism of using conditional diffusion models for inverse modeling is well-supported by existing literature and empirical results
- **Medium Confidence**: The uncertainty decomposition framework and UaE acquisition function show promise but rely on assumptions that may not universally hold
- **Low Confidence**: The claim of achieving near-optimal solutions across all 6 scientific discovery tasks is based on specific benchmark problems and may not generalize to all domains

## Next Checks
1. Benchmark on disconnected design spaces: Test Diff-BBO on optimization tasks where the valid design space contains multiple disconnected regions to evaluate whether the conditional diffusion model can effectively navigate complex manifolds and avoid generating invalid samples.

2. Stress-test uncertainty estimates: Create synthetic oracle functions with known uncertainty characteristics and compare the aleatoric and epistemic uncertainty estimates from Diff-BBO against ground truth to validate the accuracy of the uncertainty decomposition.

3. Ablation study on acquisition function: Perform an ablation study by removing the epistemic uncertainty component from the UaE acquisition function and comparing the optimization performance to quantify the actual contribution of uncertainty-aware exploration versus simple objective value maximization.