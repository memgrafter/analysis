---
ver: rpa2
title: 'DASH: Warm-Starting Neural Network Training in Stationary Settings without
  Loss of Plasticity'
arxiv_id: '2410.23495'
source_url: https://arxiv.org/abs/2410.23495
tags:
- data
- training
- experiments
- number
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of loss of plasticity when warm-starting
  neural network training on stationary data distributions. The authors develop a
  theoretical framework and identify noise memorization as the primary cause of this
  phenomenon.
---

# DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity

## Quick Facts
- arXiv ID: 2410.23495
- Source URL: https://arxiv.org/abs/2410.23495
- Authors: Baekrok Shin; Junsoo Oh; Hanseul Cho; Chulhee Yun
- Reference count: 40
- Primary result: DASH improves test accuracy and training efficiency compared to warm-starting and Shrink & Perturb methods, achieving performance close to or better than cold-starting while requiring less training time

## Executive Summary
This paper addresses the issue of loss of plasticity when warm-starting neural network training on stationary data distributions. The authors identify noise memorization as the primary cause of this phenomenon and develop DASH (Direction-Aware SHrinking), a method that selectively shrinks weights based on their alignment with the negative gradient of the loss. DASH aims to forget memorized noise while preserving learned features, achieving test accuracy close to cold-starting with the training efficiency of warm-starting.

## Method Summary
The DASH method computes the loss with training data and obtains the negative gradient, then shrinks weights proportionally to the cosine similarity between the current weight and the negative gradient. Weights with high alignment (cosine similarity close to 1) are considered to have learned features and are shrunk less, while weights with low alignment are considered to have memorized noise and are shrunk more. This selective shrinking allows the model to forget noise while retaining features, combining the generalization benefits of cold-starting with the training efficiency of warm-starting.

## Key Results
- DASH improves test accuracy compared to warm-starting and Shrink & Perturb methods across various vision tasks
- DASH achieves training efficiency close to warm-starting while maintaining cold-starting performance levels
- The method works effectively with different models (ResNet-18, VGG-16, MLP) and datasets (CIFAR-10, CIFAR-100, SVHN, Tiny-ImageNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Warm-starting causes loss of plasticity because the model memorizes noise from new data instead of learning new features.
- Mechanism: When new data is added to a stationary distribution, the model's gradients align more with the noise component than with features. This happens because noise appears only once in each data point, while features repeat across data points. The model prioritizes memorizing this unique noise over learning features.
- Core assumption: Features and noise are separable components in the data, with features being label-relevant and noise being label-irrelevant.
- Evidence anchors: [abstract] "identify noise memorization as the primary cause of plasticity loss when warm-starting on stationary data", [section 3.3] "we observe that warm-starting benefits from reduced training time compared to random initialization but can hurt the generalization performance of neural networks due to the memorization of noise"

### Mechanism 2
- Claim: DASH works by selectively shrinking weights based on their alignment with the negative gradient, preserving learned features while forgetting memorized noise.
- Mechanism: Weights aligned with the negative gradient (cosine similarity close to 1) are considered to have learned features and are shrunk less. Weights with low alignment are considered to have memorized noise and are shrunk more. This selective shrinking allows the model to forget noise while retaining features.
- Core assumption: Cosine similarity between weights and negative gradients can distinguish between learned features and memorized noise.
- Evidence anchors: [abstract] "DASH (Direction-Aware SHrinking), a method that selectively shrinks weights based on their alignment with the negative gradient of the loss", [section 4] "We compute the loss L with training data T1:j and obtain the negative gradient. Then, we shrink the weights proportionally to the cosine similarity between the current weight θ and ∇θL"

### Mechanism 3
- Claim: The ideal method (retaining features while forgetting noise) achieves the same test accuracy as cold-starting but with faster convergence than warm-starting.
- Mechanism: By resetting memorized noise while keeping learned features, the model can learn new features from incoming data without being hindered by noise memorization. This combines the generalization benefits of cold-starting with the training efficiency of warm-starting.
- Core assumption: The set of learned features can be perfectly distinguished from memorized noise in the model's weights.
- Evidence anchors: [section 3.4] "the ideal algorithm recycles memorized training samples by forgetting noise while retaining learned features", [section 3.4] "Theorem 3.6 shows that ideal initialization can achieve the same test accuracy as cold-starting"

## Foundational Learning

- Concept: Feature learning framework
  - Why needed here: The paper develops a theoretical framework to understand how neural networks learn features versus memorizing noise, which is crucial for understanding plasticity loss.
  - Quick check question: How does the framework distinguish between learning features and memorizing noise in the training process?

- Concept: Stationary vs non-stationary data distributions
  - Why needed here: The paper focuses on plasticity loss in stationary settings, which is counterintuitive since warm-starting should theoretically help in such cases.
  - Quick check question: What makes plasticity loss in stationary settings different from non-stationary settings?

- Concept: Cosine similarity as a proxy for feature learning
  - Why needed here: DASH uses cosine similarity between weights and negative gradients to determine which weights represent learned features versus memorized noise.
  - Quick check question: How does cosine similarity between a weight and its negative gradient indicate whether the weight has learned a feature?

## Architecture Onboarding

- Component map: Learning framework -> DASH algorithm -> Validation pipeline
- Critical path:
  1. Train initial model on subset of data (warm-start point)
  2. Add new data from same distribution
  3. Apply DASH: calculate gradients, compute cosine similarities, shrink weights accordingly
  4. Continue training until convergence
  5. Evaluate test accuracy and training efficiency

- Design tradeoffs:
  - DASH vs S&P: DASH selectively shrinks based on gradient alignment, while S&P shrinks all weights uniformly. DASH preserves learned features better but requires gradient computation.
  - α hyperparameter: Controls how much weight is given to recent vs past gradients. Lower α preserves more learned features but may not forget noise effectively.

- Failure signatures:
  - DASH underperforms cold-start: May indicate noise is too strong or features too infrequent
  - DASH causes catastrophic forgetting: May indicate incorrect λ threshold or α value
  - DASH doesn't improve over warm-start: May indicate the model hasn't memorized much noise to begin with

- First 3 experiments:
  1. CIFAR-10 with ResNet-18: Test basic DASH functionality on common benchmark
  2. Tiny-ImageNet with VGG-16: Test scalability to larger datasets and different architectures
  3. Data-discarding scenario: Test DASH when previous data cannot be stored, using λ=0.3 with 15-experiment intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DASH's performance change with different dataset complexities (e.g., CIFAR-10 vs. CIFAR-100)?
- Basis in paper: [inferred] from Section 5.2 and Table 1
- Why unresolved: The paper does not directly compare DASH's performance across datasets with varying complexity.
- What evidence would resolve it: Conduct experiments comparing DASH's performance on datasets with different numbers of classes and feature complexities.

### Open Question 2
- Question: What is the impact of different feature frequency distributions on DASH's effectiveness?
- Basis in paper: [explicit] from Section 2.2 and 3.1
- Why unresolved: The paper assumes a fixed feature frequency distribution but does not explore how varying this distribution affects DASH.
- What evidence would resolve it: Experiment with synthetic datasets having different feature frequency distributions and measure DASH's performance.

### Open Question 3
- Question: How does DASH's performance compare to other warm-starting methods in non-stationary data distribution environments?
- Basis in paper: [explicit] from Section 5.2 and Appendix C.4.5
- Why unresolved: The paper only briefly explores DASH's performance in a class incremental learning setting.
- What evidence would resolve it: Conduct comprehensive experiments comparing DASH to other warm-starting methods in various non-stationary data distribution scenarios.

## Limitations
- The theoretical framework's assumptions about feature and noise separability may not hold in real-world scenarios where features and noise are entangled.
- The effectiveness of cosine similarity as a proxy for distinguishing learned features from memorized noise needs further empirical validation across diverse data distributions.
- The hyperparameter sensitivity (λ, α) and its impact on different model architectures remains unclear.

## Confidence

- High confidence: DASH improves training efficiency compared to cold-starting across multiple vision benchmarks
- Medium confidence: The mechanism of noise memorization as the primary cause of plasticity loss in stationary settings
- Low confidence: The theoretical bounds on test accuracy recovery and the general applicability to non-vision domains

## Next Checks

1. Test DASH on non-vision datasets (text, audio) to assess generalizability beyond the current scope
2. Conduct ablation studies on the α hyperparameter to determine optimal values across different model architectures
3. Implement and evaluate DASH in streaming settings where data cannot be stored, testing the λ=0.3 configuration mentioned for data-discarding scenarios