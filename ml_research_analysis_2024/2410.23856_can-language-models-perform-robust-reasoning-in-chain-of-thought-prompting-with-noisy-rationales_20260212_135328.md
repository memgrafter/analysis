---
ver: rpa2
title: Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting
  with Noisy Rationales?
arxiv_id: '2410.23856'
source_url: https://arxiv.org/abs/2410.23856
tags:
- answer
- noisy
- base-9
- digit
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of noisy rationales in chain-of-thought
  prompting, where irrelevant or inaccurate reasoning steps are paired with valid
  question-answer examples. The authors construct the NoRa dataset to systematically
  evaluate LLM robustness against such noise.
---

# Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?

## Quick Facts
- arXiv ID: 2410.23856
- Source URL: https://arxiv.org/abs/2410.23856
- Reference count: 40
- Language models drop 1.4%-40.4% accuracy with noisy rationales; CD-CoT improves by 17.8% average

## Executive Summary
This paper investigates how language models handle noisy rationales in chain-of-thought prompting, where irrelevant or inaccurate reasoning steps are paired with valid question-answer examples. The authors construct the NoRa dataset to systematically evaluate LLM robustness against such noise and find existing robust methods like self-correction and self-consistency show limited efficacy. To address this challenge, they propose Contrastive Denoising with noisy Chain-of-Thought (CD-CoT), which enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with one clean rationale. CD-CoT achieves an average improvement of 17.8% in accuracy over the base model and demonstrates significantly stronger denoising capabilities than baseline methods.

## Method Summary
The paper addresses noisy rationales in chain-of-thought prompting by proposing CD-CoT, a method that enhances LLMs' denoising-reasoning capabilities through contrastive learning. CD-CoT works by contrasting noisy rationales with one clean rationale through a four-step process: rationale rephrasing via supervised contrasting, rationale selection based on answer consistency, rationale exploration through multiple reasoning iterations, and answer voting via majority selection. The method assumes LLMs can identify noisy thoughts by contrasting pairs of noisy and clean rationales, and leverages this contrastive signal to improve denoising capabilities.

## Key Results
- GPT-3.5 experiences accuracy drops of 1.4%-40.4% when prompted with noisy rationales
- CD-CoT achieves an average improvement of 17.8% in accuracy over the base model
- CD-CoT demonstrates significantly stronger denoising capabilities than baseline methods like SC, SD, and SM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CD-CoT improves denoising reasoning by contrasting noisy rationales with one clean rationale.
- Mechanism: The method leverages supervised contrastive learning where the model learns to identify and rectify noisy thoughts by comparing them against a clean reference. This follows a principle of exploration and exploitation: explicit denoising through rephrasing/selecting in input space, then diverse reasoning exploration with voting in output space.
- Core assumption: LLMs can learn to differentiate between clean and noisy thoughts when provided with a clean reference, and can use this contrastive signal to improve their denoising capabilities.
- Evidence anchors:
  - [abstract]: "It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale"
  - [section]: "We assume that LLMs can identify noisy thoughts by contrasting a pair of noisy and clean rationales and discerning their differences"
  - [corpus]: "Contrastive denoising with noisy CoT (CD-CoT)" - this shows the method is specifically designed for this contrastive approach
- Break condition: If the clean rationale is not sufficiently different from the noisy ones, or if the model cannot effectively parse the contrastive signal, the denoising will fail.

### Mechanism 2
- Claim: The rephrasing step enables explicit denoising by having the model regenerate rationales that are logically consistent with the clean example.
- Mechanism: Through a supervised rephrasing prompt, the model is instructed to follow the clean example's logical structure and reasoning process. By generating multiple rephrased versions and selecting those that match the original answer, the method filters out rationales containing noisy thoughts.
- Core assumption: The model can generate multiple rephrased versions of the rationale and that these rephrased versions will be more consistent with the clean example's reasoning.
- Evidence anchors:
  - [section]: "First, we establish a general prompt to construct a pair of contrastive examples... This steers the model towards learning from the clean example and then rephrasing and rectifying the noisy examples"
  - [section]: "For each noisy example and its corresponding rephrased rationales... we select the rationales that the corresponding answers match the given (true) answer"
  - [corpus]: "Rationale Rephrasing via Supervised Contrasting" - this shows the method explicitly uses rephrasing for denoising
- Break condition: If the model cannot generate consistent rephrased versions or if the answer matching step fails to identify the correct rephrased rationales.

### Mechanism 3
- Claim: The voting step improves robustness by aggregating diverse reasoning paths and selecting the most consistent answer.
- Mechanism: After denoising the rationales, the method explores diverse reasoning paths by running multiple reasoning iterations with different rephrased contexts. The final answer is determined by majority voting across all generated answers.
- Core assumption: Diverse reasoning paths will converge on the correct answer, and majority voting will select the most accurate response.
- Evidence anchors:
  - [abstract]: "exploring diverse reasoning paths and voting on answers in the output space"
  - [section]: "For the M distinct contexts, we perform multiple reasoning to explore various rationales, resulting in a total number of reasoning times equal to D... Ultimately, all the D answers are equally voted into a final answer"
  - [corpus]: "Answer Voting (Dto1)" - this shows the method explicitly uses voting for robustness
- Break condition: If the diverse reasoning paths do not converge on the correct answer, or if the voting mechanism selects an incorrect answer due to noise in the reasoning process.

## Foundational Learning

- Concept: In-context learning (ICL) with chain-of-thought (CoT) prompting
  - Why needed here: The paper builds on ICL and CoT as the foundation for understanding how noisy rationales affect LLM reasoning. Understanding these concepts is crucial for grasping why noisy rationales are problematic and how CD-CoT addresses this.
  - Quick check question: What is the difference between standard ICL and CoT prompting, and why does CoT improve reasoning performance?

- Concept: Contrastive learning
  - Why needed here: CD-CoT uses contrastive learning principles to help LLMs distinguish between clean and noisy rationales. Understanding how contrastive learning works is essential for understanding the mechanism behind CD-CoT's effectiveness.
  - Quick check question: How does contrastive learning typically work in other domains (like computer vision), and how is this principle applied to denoising rationales in CD-CoT?

- Concept: Self-consistency methods
  - Why needed here: The paper compares CD-CoT to self-consistency methods like SC [83]. Understanding these baseline methods is important for evaluating CD-CoT's contributions and improvements.
  - Quick check question: What is the key difference between CD-CoT's voting mechanism and standard self-consistency methods?

## Architecture Onboarding

- Component map: Clean example + Noisy examples → Rephrasing (1 to N) → Selection (N to M) → Exploration (M to D) → Voting (D to 1) → Final Answer

- Critical path: The critical path for CD-CoT is: Clean example + Noisy examples → Rephrasing (Step 1) → Selection (Step 2) → Exploration (Step 3) → Voting (Step 4) → Final Answer. Any failure in these steps can impact the final output quality.

- Design tradeoffs:
  - Number of rephrased versions (N): Higher N provides more diversity but increases computational cost
  - Number of selected examples (M): Higher M provides more context but increases computational cost
  - Number of reasoning iterations (D): Higher D provides more exploration but increases computational cost
  - Clean example requirement: CD-CoT requires one clean example, which is a minimal requirement but may not always be available

- Failure signatures:
  - Poor rephrasing quality: If the model cannot generate consistent rephrased versions, the selection step will fail
  - Answer matching failures: If multiple rephrased versions have different answers, the selection becomes ambiguous
  - Lack of convergence: If diverse reasoning paths do not converge, voting may select incorrect answers
  - Computational bottlenecks: High values of N, M, or D can make the method computationally expensive

- First 3 experiments:
  1. Baseline comparison: Run CD-CoT on NoRa dataset and compare accuracy with base model and existing methods (SC, SD, SM)
  2. Ablation study: Test CD-CoT with different combinations of rephrasing and voting components to verify their individual contributions
  3. Hyperparameter sensitivity: Test CD-CoT with different values of N, M, and D to find optimal settings and understand tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CD-CoT method be extended to handle multi-modal scenarios, particularly visual data, to enhance the robustness of reasoning in noisy contexts?
- Basis in paper: [inferred] The paper discusses the potential for extending the NoRa dataset to include multi-modal scenarios, particularly visual data, to provide a more comprehensive understanding of the robustness of foundation models.
- Why unresolved: While the paper acknowledges the importance of multi-modal scenarios, it does not provide specific methods or results for extending CD-CoT to handle visual data or other non-textual inputs.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of CD-CoT when applied to visual reasoning tasks, along with detailed methodologies for integrating visual data into the denoising process.

### Open Question 2
- Question: How does the performance of CD-CoT compare to other denoising methods when dealing with domain-specific knowledge, such as legal or medical contexts?
- Basis in paper: [inferred] The paper suggests that accessing external databases is a promising solution for denoising, but verifying the factual relevance and accuracy of a rationale in domain-specific contexts is challenging.
- Why unresolved: The paper does not provide empirical comparisons of CD-CoT with other denoising methods in domain-specific contexts, leaving the question of its relative effectiveness open.
- What evidence would resolve it: Comparative studies of CD-CoT and other denoising methods applied to legal or medical reasoning tasks, with performance metrics and qualitative assessments.

### Open Question 3
- Question: What are the computational costs and scalability challenges of implementing CD-CoT in real-time applications, and how can these be mitigated?
- Basis in paper: [inferred] The paper mentions that CD-CoT requires a clean CoT demonstration and involves multiple steps, including rephrasing, selection, exploration, and voting, which may introduce computational overhead.
- Why unresolved: While the paper discusses the effectiveness of CD-CoT, it does not provide a detailed analysis of its computational costs or scalability challenges in real-time applications.
- What evidence would resolve it: Performance benchmarks of CD-CoT in real-time applications, along with strategies for optimizing computational efficiency and scalability.

## Limitations

- The experimental validation is limited to GPT-3.5-turbo-0613, raising questions about generalizability to other LLMs
- The NoRa dataset is synthetically constructed, and real-world noisy rationales may exhibit different characteristics
- The computational overhead of CD-CoT (multiple rephrasing and reasoning iterations) is not thoroughly analyzed

## Confidence

- **High Confidence**: The existence of the problem (LLM sensitivity to noisy rationales) is well-demonstrated with clear empirical evidence. The CD-CoT framework is methodologically sound and clearly explained.
- **Medium Confidence**: The effectiveness of CD-CoT on the NoRa dataset is supported by experiments, but generalizability to other models and real-world scenarios needs further validation. The ablation study supports the contribution of individual components, but the synergistic effects between rephrasing and voting are not fully isolated.
- **Low Confidence**: The computational efficiency analysis is insufficient, with limited discussion of resource requirements and scalability. The real-world applicability of CD-CoT to naturally occurring noisy rationales is not empirically tested.

## Next Checks

1. **Generalization Test**: Evaluate CD-CoT on multiple LLM architectures (e.g., GPT-4, Claude, LLaMA) to assess whether the performance improvements are model-agnostic or specific to GPT-3.5.

2. **Real-World Validation**: Apply CD-CoT to naturally occurring noisy rationales from real-world datasets or human-generated noisy chain-of-thought examples to test its practical utility beyond synthetic noise.

3. **Efficiency Analysis**: Conduct a comprehensive analysis of CD-CoT's computational overhead, including wall-clock time, token usage, and cost implications for different values of N, M, and D, and compare it with baseline methods under resource constraints.