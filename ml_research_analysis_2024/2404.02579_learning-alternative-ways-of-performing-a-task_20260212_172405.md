---
ver: rpa2
title: Learning Alternative Ways of Performing a Task
arxiv_id: '2404.02579'
source_url: https://arxiv.org/abs/2404.02579
tags:
- task
- examples
- graphs
- learning
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel inductive method to learn multiple
  models of a task from a small set of expert demonstrations, each represented as
  an activity sequence. The approach uses dependency graphs to capture consecutive
  activity dependencies and employs iterative generalization and specialization to
  identify distinct strategies within the examples.
---

# Learning Alternative Ways of Performing a Task

## Quick Facts
- arXiv ID: 2404.02579
- Source URL: https://arxiv.org/abs/2404.02579
- Authors: David Nieves; María José Ramírez-Quintana; Carlos Monserrat; César Ferri; José Hernández-Orallo
- Reference count: 10
- Primary result: Novel method learns multiple valid task execution strategies from few expert demonstrations using dependency graphs and iterative generalization-specialification

## Executive Summary
This paper presents a novel inductive method to learn multiple models of a task from a small set of expert demonstrations, each represented as an activity sequence. The approach uses dependency graphs to capture consecutive activity dependencies and employs iterative generalization and specialization to identify distinct strategies within the examples. Two quality metrics—fitness and simplicity—are used to evaluate the models. Experiments on laparoscopic suturing and brownie cooking tasks show that the method successfully identifies multiple valid execution patterns while filtering noise.

## Method Summary
The method converts activity sequences into dependency graphs, then iteratively aggregates these graphs to form an initial over-general model. A refinement algorithm applies threshold-based edge removal to generate valid models that overlap with subsets of the examples. This process repeats until all examples are covered by models. Each model represents a distinct task strategy, with fitness measuring coverage and simplicity measuring conciseness.

## Key Results
- Successfully identifies multiple valid execution patterns in laparoscopic suturing tasks
- Filters noise while maintaining coverage of distinct strategies in brownie cooking tasks
- Models demonstrate improved simplicity and fitness compared to single over-general models

## Why This Works (Mechanism)

### Mechanism 1
Iterative generalization-specialization using dependency graphs enables learning multiple valid task execution strategies from few positive examples. The method aggregates dependency graphs of positive examples to capture common patterns, then applies threshold-based refinement to remove non-essential edges, generating models that overlap with different subsets of examples. This cycle repeats until all examples are covered. Core assumption: Dependency graphs accurately represent task execution sequences while abstracting away noise and non-essential activities.

### Mechanism 2
Two quality metrics (fitness and simplicity) enable effective evaluation of multiple task models. Fitness measures how many example graphs overlap with each model, indicating coverage of valid strategies. Simplicity measures the number of vertices and edges, favoring concise representations. Models are generated to optimize this tradeoff. Core assumption: A model with high fitness and low simplicity better captures valid task strategies than a single over-general model.

### Mechanism 3
Representation bias from dependency graphs enables efficient search over model space while maintaining correctness guarantees. Dependency graphs simplify sequences by representing only consecutive dependencies, reducing search space. Validity checks ensure models contain complete walks from start to finish, guaranteeing correctness. Core assumption: The loss of ordering information in dependency graphs is compensated by the ability to systematically apply graph operations for model generation.

## Foundational Learning

- Concept: Dependency graphs as activity sequence representation
  - Why needed here: Enables abstraction of task executions while preserving essential dependencies and enabling systematic model generation
  - Quick check question: Given sequence (A,B,C,A,C), what is the corresponding dependency graph structure?

- Concept: Iterative covering strategy for multi-model learning
  - Why needed here: Allows generation of multiple distinct strategies rather than a single over-general model, capturing variability in expert demonstrations
  - Quick check question: If example graphs are {G1,G2,G3} and model M1 overlaps with G1,G2, which graphs remain for the next iteration?

- Concept: Threshold-based refinement for noise reduction
  - Why needed here: Enables systematic removal of non-essential activities/edges while maintaining model validity, addressing the small-sample noise problem
  - Quick check question: If edge weights are {5,3,1} and threshold is 4, which edges remain in the threshold graph?

## Architecture Onboarding

- Component map: Activity sequences -> Dependency graph conversion -> MMDG algorithm (AggregateGraphs -> Refinement -> overlap checking) -> Multiple dependency graph models -> Fitness and simplicity evaluation
- Critical path: 1. Convert all input sequences to dependency graphs 2. Aggregate graphs to form initial G+ 3. Apply Refinement algorithm to generate first model M1 4. Remove examples overlapping with M1 5. Repeat from step 2 until no examples remain 6. Evaluate all generated models
- Design tradeoffs: Graph representation vs. sequence representation: Graphs enable systematic operations but lose ordering information; Threshold selection: Higher thresholds yield simpler but potentially less accurate models; lower thresholds capture more variability but risk noise; Overlap condition: Strict overlap ensures coverage but may miss near-miss strategies
- Failure signatures: Single model generated when multiple strategies exist: Indicates threshold too high or aggregation too restrictive; Models with very low fitness: Indicates threshold too low, capturing noise rather than valid strategies; Models missing essential activities: Indicates invalid dependency graph representation or incorrect validity checks
- First 3 experiments: 1. Test on synthetic dataset with known multiple strategies to verify multiple models are generated 2. Test on single-strategy dataset to verify only one model is generated and matches the strategy 3. Test on noisy dataset to verify noise reduction and model simplicity improvement

## Open Questions the Paper Calls Out

### Open Question 1
How can the method handle activities that are repeated multiple times with varying semantics within a single task execution? The paper mentions that the semantics of an activity repeated several times in a sequence can vary throughout a task, which is part of the representational bias assumed by using dependency graphs. This limitation is acknowledged but no solution is provided.

### Open Question 2
Can the method be extended to handle hierarchical activities or concurrent execution patterns? The paper states that the current representation is potentially limited to represent concurrency, duplicate actions, hierarchical activities, or model OR-splits/joins in a mined model. These limitations are mentioned but potential extensions are not explored.

### Open Question 3
How can the strictness of the overlap criterion be relaxed to improve model coverage without compromising accuracy? The paper mentions that many examples did not overlap with any model in the experiments and suggests that the criterion to determine when an example conforms to a model may be very strict. Exploring less restrictive conformance checking is proposed but not implemented.

## Limitations
- Exact implementation details of the Refinement algorithm remain unclear
- Empirical evaluation relies on proprietary datasets without quantitative comparisons
- Dependency graph representation loses ordering information that may be critical for strategy differentiation

## Confidence

**High Confidence**: The iterative generalization-specialization framework using dependency graphs; The use of fitness and simplicity as evaluation metrics; The two case study applications

**Medium Confidence**: That the approach effectively learns multiple valid strategies from few examples; That the method successfully filters noise while maintaining strategy coverage; That the models generalize to unseen data

**Low Confidence**: Quantitative performance improvements over baseline methods; Robustness across different threshold settings; Scalability to more complex tasks with longer activity sequences

## Next Checks
1. **Synthetic dataset validation**: Create controlled datasets with known multiple strategies and verify the algorithm generates the expected number of models with correct coverage.
2. **Noise sensitivity analysis**: Test the method on datasets with varying noise levels to quantify how well it filters noise while maintaining valid strategy identification, using precision-recall metrics.
3. **Baseline comparison**: Implement DISCO and other process mining approaches on the same datasets and compare fitness and simplicity metrics, including statistical significance testing to determine if improvements are meaningful.