---
ver: rpa2
title: 'Who Speaks Matters: Analysing the Influence of the Speaker''s Ethnicity on
  Hate Classification'
arxiv_id: '2410.20490'
source_url: https://arxiv.org/abs/2410.20490
tags:
- implicit
- explicit
- hate
- flips
- identity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how injecting explicit (e.g., \u201CThe\
  \ [ethnicity] person said...\u201D) and implicit (dialectal features) markers of\
  \ speaker identity affects hate speech classification by LLMs. The authors modify\
  \ 2 datasets and test 4 models using zero-shot and in-context learning, finding\
  \ that larger models are more robust, explicit markers cause fewer flips than implicit\
  \ ones, and flip rates vary by ethnicity and target group."
---

# Who Speaks Matters: Analysing the Influence of the Speaker's Ethnicity on Hate Classification

## Quick Facts
- arXiv ID: 2410.20490
- Source URL: https://arxiv.org/abs/2410.20490
- Reference count: 36
- Primary result: Larger LLMs show greater robustness to speaker identity markers in hate speech classification

## Executive Summary
This paper investigates how injecting explicit (e.g., "The [ethnicity] person said...") and implicit (dialectal features) markers of speaker identity affects hate speech classification by LLMs. The authors modify 2 datasets and test 4 models using zero-shot and in-context learning, finding that larger models are more robust, explicit markers cause fewer flips than implicit ones, and flip rates vary by ethnicity and target group. Across models, hateful inputs are more likely to flip to non-hateful than vice versa. Smaller models like Llama-3-8B show notably higher instability, suggesting brittleness tied to speaker identity.

## Method Summary
The authors modified MPBHSD and HateXplain datasets by injecting explicit identity markers (e.g., "The Indian person said...") and generating implicit dialectal variants using few-shot prompting with Llama-3-70B. They tested four models (Llama-3-8B, Llama-3-70B, GPT-4o, and a fine-tuned BERT) using both zero-shot classification and in-context learning with four examples. Classification outputs were compared to original predictions to measure flip rates, with additional analysis of false positive/negative rates and target group breakdown.

## Key Results
- Larger models (Llama-3-70B, GPT-4o) show fewer classification flips than smaller models (Llama-3-8B)
- Explicit identity markers cause fewer classification flips than implicit dialectal markers
- Classification flips are asymmetric: hateful inputs flip to non-hateful more often than vice versa

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger LLMs are more robust to speaker identity markers in hate speech classification.
- **Mechanism**: Model size correlates with better contextual understanding and ability to differentiate speaker identity from content, reducing flip rates.
- **Core assumption**: Bigger models have richer representations that help them focus on the actual hate content rather than speaker markers.
- **Evidence anchors**:
  - [abstract] "We find that larger models are more robust."
  - [section] "As seen in Table 2 we find that on average larger and newer models, such as Llama-3-70B and GPT-4o, are more robust and show a smaller percentage of flips, than the smaller Llama-3-8B."
- **Break condition**: If speaker identity markers become semantically central to the hate content, even large models may flip.

### Mechanism 2
- **Claim**: Explicit speaker identity markers cause fewer classification flips than implicit dialectal markers.
- **Mechanism**: Explicit markers are processed as neutral metadata, while implicit markers introduce confounding linguistic features that models misinterpret as content signals.
- **Core assumption**: Models treat explicit identity phrases as system-level context but process implicit dialect features as part of the actual message.
- **Evidence anchors**:
  - [abstract] "We find that the presence of implicit dialect markers in inputs causes model outputs to flip more than the presence of explicit markers."
  - [section] "We find that models are fairly robust to explicit markers, but are brittle when implicit dialectal markers of the speaker's identity are injected."
- **Break condition**: If explicit markers are themselves associated with hate speech in training data, they may cause flips.

### Mechanism 3
- **Claim**: Classification flips are asymmetric—hateful inputs flip to non-hateful more often than vice versa.
- **Mechanism**: Models have a bias toward leniency, reducing false positives at the cost of increasing false negatives in hate detection.
- **Core assumption**: Safety and moderation policies incentivize models to avoid over-flagging content.
- **Evidence anchors**:
  - [section] "Figure 2 and Appendix C.2 show that overall, an originally non-hateful (NH) prediction is likely to remain non-hateful... On the other hand, hateful (H) predictions become significantly non-hateful (NH) across most models."
  - [abstract] "Across models, hateful inputs are more likely to flip to non-hateful than vice versa."
- **Break condition**: If the model is explicitly tuned for high precision in hate detection, this asymmetry may reverse.

## Foundational Learning

- **Concept**: Dialectal variation in English
  - Why needed here: Understanding how different English dialects (Indian, Singaporean, British, Jamaican, African-American) are represented and processed by models is central to interpreting classification flips.
  - Quick check question: What linguistic features differentiate Jamaican English from British English in this study?

- **Concept**: Zero-shot vs. in-context learning
  - Why needed here: The paper compares model stability under zero-shot classification and in-context learning (ICL), showing ICL generally yields more stable outputs.
  - Quick check question: How does providing examples in ICL reduce flip rates compared to zero-shot prompting?

- **Concept**: False positive vs. false negative rates in classification
  - Why needed here: The paper analyzes flips from non-hateful to hateful (FPR) and hateful to non-hateful (FNR), which are key metrics for understanding model behavior in hate speech detection.
  - Quick check question: Why is an increase in FNR (hateful to non-hateful flips) problematic for hate speech detection?

## Architecture Onboarding

- **Component map**: Data preparation -> Dialect generation -> Model inference (zero-shot/ICL) -> Flip detection -> Statistical analysis -> Interpretation
- **Critical path**: Data preparation → Model inference (zero-shot/ICL) → Flip detection → Statistical analysis → Interpretation
- **Design tradeoffs**: Using synthetic dialect generation ensures coverage but introduces potential artifacts; relying on human verification balances quality with scalability
- **Failure signatures**: High flip rates in smaller models, asymmetric flipping from H→NH, inconsistent performance across dialects
- **First 3 experiments**:
  1. Run zero-shot classification on original MPBHSD data to establish baseline accuracy
  2. Inject explicit identity markers and measure flip rates across all models
  3. Generate implicit dialectal variants and compare flip patterns to explicit condition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do explicit and implicit speaker identity markers differentially impact LLM robustness across varying model sizes and architectures?
- Basis in paper: [explicit] The authors note that larger models like GPT-4o and Llama-3-70B are more robust to identity markers, and that explicit markers cause fewer flips than implicit dialectal markers.
- Why unresolved: The study focuses on three LLMs and one LM, limiting generalizability. The differential impact across model architectures (e.g., transformer-based vs. fine-tuned BERT) is not deeply analyzed.
- What evidence would resolve it: Comparative studies testing a wider range of model sizes and architectures (e.g., Claude, Mixtral) with consistent speaker identity markers.

### Open Question 2
- Question: To what extent do dialectal features influence hate speech classification beyond lexical substitution, and how can this be isolated from contextual shifts?
- Basis in paper: [inferred] The authors generate dialectal data by injecting colloquial words, cultural themes, and code-mixed language, but acknowledge that synthetically added phrases make it difficult to isolate their precise impact.
- Why unresolved: The synthetic nature of dialect generation introduces confounding variables, such as additional phrases that may alter context beyond speaker identity.
- What evidence would resolve it: Controlled experiments with minimal lexical changes (e.g., only spelling or word order) while preserving semantic meaning.

### Open Question 3
- Question: How does the brittleness of hate speech classification models vary across different hate speech datasets and languages, particularly in multilingual contexts?
- Basis in paper: [explicit] The authors limit their study to English datasets (MPBHSD and HateXplain) and acknowledge that findings may differ in multilingual datasets.
- Why unresolved: The study's scope is restricted to English dialects, and the authors do not explore how models perform on non-English or code-mixed datasets.
- What evidence would resolve it: Testing models on multilingual hate speech datasets (e.g., multilingual versions of HateXplain or other datasets) with speaker identity markers in different languages.

## Limitations

- Dialect generation uses synthetic phrases that may introduce confounding contextual changes beyond speaker identity
- Human verification of dialectal accuracy is reported but not deeply analyzed for its impact on results
- The analysis focuses on classification flips without exploring whether flips are appropriate corrections or problematic errors

## Confidence

- **High confidence**: The empirical finding that larger models show fewer flips is well-supported by the data (Llama-3-70B and GPT-4o consistently outperform Llama-3-8B across all conditions)
- **Medium confidence**: The claim about explicit vs. implicit markers having different effects is supported but could be influenced by the quality of dialect generation
- **Medium confidence**: The asymmetry finding (H→NH flips more common than NH→H) is consistently observed but the explanation (safety bias) remains speculative

## Next Checks

1. **Mechanism validation**: Test whether the explicit marker advantage persists when markers are semantically loaded (e.g., using explicitly racist phrases as markers) rather than neutral identity descriptors
2. **Dialect quality assessment**: Evaluate the impact of dialect generation quality by correlating human verification scores with flip rates to determine if poor-quality dialects drive instability
3. **Model size threshold**: Conduct a systematic analysis across multiple model sizes to identify the specific parameter threshold where robustness to speaker markers emerges