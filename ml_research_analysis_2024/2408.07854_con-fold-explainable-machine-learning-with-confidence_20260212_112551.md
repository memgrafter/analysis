---
ver: rpa2
title: CON-FOLD -- Explainable Machine Learning with Confidence
arxiv_id: '2408.07854'
source_url: https://arxiv.org/abs/2408.07854
tags:
- rules
- confidence
- algorithm
- data
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CON-FOLD extends the FOLD-RM explainable machine learning algorithm
  by assigning probability-based confidence scores to learned rules, allowing users
  to assess prediction reliability. The method uses Wilson Score Intervals for accurate
  confidence estimation and includes a pruning algorithm to prevent overfitting by
  removing exceptions that provide minimal confidence improvement.
---

# CON-FOLD -- Explainable Machine Learning with Confidence

## Quick Facts
- arXiv ID: 2408.07854
- Source URL: https://arxiv.org/abs/2408.07854
- Reference count: 40
- Primary result: CON-FOLD extends FOLD-RM with confidence scores and pruning, maintaining accuracy while improving interpretability and excelling with limited training data

## Executive Summary
CON-FOLD extends the FOLD-RM explainable machine learning algorithm by assigning probability-based confidence scores to learned rules, allowing users to assess prediction reliability. The method uses Wilson Score Intervals for accurate confidence estimation and includes a pruning algorithm to prevent overfitting by removing exceptions that provide minimal confidence improvement. It also supports incorporating domain knowledge through modifiable initial rules. Experiments on UCI datasets show CON-FOLD maintains accuracy while producing more interpretable, concise rule sets. A new metric, Inverse Brier Score, is introduced to evaluate probabilistic predictions while remaining compatible with standard accuracy measures. The approach is demonstrated effectively on a real-world physics grading task, particularly excelling with limited training data.

## Method Summary
CON-FOLD builds on the FOLD-RM algorithm by adding Wilson Score Interval-based confidence scores to rules and implementing confidence-based pruning to prevent overfitting. The algorithm converts multi-class classification problems into sequential binary classification tasks, iteratively generates rules using information gain or Gini impurity, and calculates confidence scores using Wilson Score Intervals. It then applies improvement threshold pruning to remove exceptions that provide minimal confidence improvement and confidence threshold pruning to eliminate low-confidence rules. The method also supports incorporating domain knowledge as modifiable initial rules that can be pruned during training based on actual data performance.

## Key Results
- CON-FOLD maintains accuracy comparable to FOLD-RM while producing more interpretable, concise rule sets
- Wilson Score Interval confidence estimates are more reliable than simple probability, especially with small training data
- The pruning algorithm effectively prevents overfitting by removing exceptions that provide minimal confidence improvement
- Domain knowledge integration improves performance on the physics grading task with minimal training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wilson Score Interval provides more reliable confidence estimates for rules than simple probability, especially with small training data.
- Mechanism: The algorithm uses the Wilson Score Interval formula to adjust confidence values based on the number of examples covered by each rule, providing asymmetric confidence bounds that account for the statistical properties of small samples.
- Core assumption: The distribution of examples covered by a rule can be modeled as a binomial distribution, and the true probability of a rule being correct approaches the Wilson Score estimate as sample size increases.
- Evidence anchors:
  - [abstract]: "The method uses Wilson Score Intervals for accurate confidence estimation"
  - [section]: "We chose the centre of the Wilson Score Interval [27] given by Equation 2 below. The Wilson Score Interval adjusts for the asymmetry in the binomial distribution which is particularly pronounced in the case of extreme probabilities and small sample sizes."
- Break condition: If the underlying distribution of examples is not binomial, or if examples are not independent, the Wilson Score estimates may be misleading.

### Mechanism 2
- Claim: Confidence-based pruning prevents overfitting by removing exceptions that provide minimal confidence improvement.
- Mechanism: The algorithm temporarily removes each exception from a rule, recalculates confidence, and removes the exception if the confidence drop is below a threshold, preventing the model from creating overly complex rules that fit noise.
- Core assumption: Exceptions that provide minimal confidence improvement are likely fitting noise rather than true patterns in the data.
- Evidence anchors:
  - [abstract]: "The method uses Wilson Score Intervals for accurate confidence estimation and includes a pruning algorithm to prevent overfitting by removing exceptions that provide minimal confidence improvement."
  - [section]: "Each time a rule is added to the model each exception to the rule is temporarily removed and a new confidence score is calculated. If this changes the confidence by less than the improvement threshold then this exception is removed."
- Break condition: If the improvement threshold is set too high, valid exceptions may be pruned; if too low, overfitting may persist.

### Mechanism 3
- Claim: Incorporating domain knowledge as modifiable initial rules improves performance with limited training data.
- Mechanism: The algorithm allows users to provide pre-existing rules with or without confidence values, which can be incorporated into the model and pruned during training based on actual data performance.
- Core assumption: Domain experts can provide valuable rules that capture important patterns not evident in limited training data.
- Evidence anchors:
  - [abstract]: "It also supports incorporating domain knowledge through modifiable initial rules"
  - [section]: "CON-FOLD enables the user to provide pre-existing knowledge in the form of logic program rules that are either (fixed) background knowledge or (modifiable) initial rule candidates."
- Break condition: If provided rules are incorrect or do not match the data distribution, they may harm performance rather than help.

## Foundational Learning

- Concept: Logic programming with default negation
  - Why needed here: The FOLD-RM algorithm and its extensions use stratified logic programs with default negation to create rules with exceptions, allowing for more expressive and interpretable rule sets.
  - Quick check question: Can you explain the difference between normal negation and default negation in logic programming, and why default negation is important for creating rules with exceptions?

- Concept: Information gain and Gini impurity
  - Why needed here: These metrics are used to evaluate the quality of rules during the learning process, helping the algorithm select features that best split the data.
  - Quick check question: How does information gain differ from Gini impurity as a metric for evaluating splits in decision trees, and why might one be preferred over the other in certain situations?

- Concept: Binomial distribution and confidence intervals
  - Why needed here: The Wilson Score Interval is used to calculate confidence scores for rules, requiring understanding of binomial distributions and how confidence intervals work for proportions.
  - Quick check question: Why is the Wilson Score Interval preferred over the normal approximation method for calculating confidence intervals for proportions when dealing with small sample sizes?

## Architecture Onboarding

- Component map:
  Core FOLD-RM algorithm -> Confidence score calculator -> Pruning module -> Knowledge integration -> Evaluation framework

- Critical path:
  1. Input training data and optional domain knowledge
  2. Convert multi-class problem to sequential binary classification
  3. Iteratively generate rules using information gain or Gini impurity
  4. Calculate Wilson Score confidence for each rule
  5. Apply confidence-based pruning
  6. Output final rule set with confidence scores

- Design tradeoffs:
  - Confidence vs interpretability: Higher confidence thresholds produce simpler, more interpretable models but may sacrifice accuracy
  - Pruning aggressiveness: More aggressive pruning reduces overfitting but may underfit the data
  - Domain knowledge integration: Including background knowledge helps with limited data but requires careful validation

- Failure signatures:
  - Rule set too large: Confidence threshold too low or pruning too weak
  - Poor accuracy: Improvement threshold too high, removing valuable exceptions
  - Model ignores domain knowledge: Background knowledge rules conflict with data patterns
  - Confidence scores unreliable: Data distribution not well-modeled by binomial assumptions

- First 3 experiments:
  1. Run CON-FOLD on a simple UCI dataset (e.g., Iris) with default parameters to verify basic functionality
  2. Test pruning with different thresholds on a dataset with known noise to evaluate overfitting prevention
  3. Incorporate domain knowledge into a small dataset to demonstrate the benefit of background knowledge integration

## Open Questions the Paper Calls Out

- Question: How does the CON-FOLD algorithm's confidence-based pruning compare to other pruning methods like those used in decision tree algorithms in terms of preventing overfitting while maintaining interpretability?
  - Basis in paper: [explicit] The paper mentions that CON-FOLD uses confidence-based pruning to prevent overfitting, and compares it to FOLD-SE which uses Gini Impurity.
  - Why unresolved: The paper does not provide a direct comparison of the pruning effectiveness between CON-FOLD and other methods like decision tree pruning.
  - What evidence would resolve it: Empirical studies comparing the pruning effectiveness and interpretability of CON-FOLD against other pruning methods on the same datasets.

- Question: What is the impact of incorporating background knowledge on the performance of CON-FOLD in domains other than physics grading, such as medical diagnosis or financial forecasting?
  - Basis in paper: [explicit] The paper demonstrates the effectiveness of incorporating background knowledge in a physics grading task.
  - Why unresolved: The paper only explores one domain, so the generalizability of the approach to other domains is unknown.
  - What evidence would resolve it: Experiments applying CON-FOLD with background knowledge to various other domains and comparing the results to baseline models.

- Question: How does the Inverse Brier Score (IBS) compare to other probabilistic evaluation metrics like the Log Loss in terms of sensitivity to different types of prediction errors?
  - Basis in paper: [explicit] The paper introduces IBS as a new metric for evaluating probabilistic predictions.
  - Why unresolved: The paper does not compare IBS to other probabilistic metrics like Log Loss.
  - What evidence would resolve it: Empirical studies comparing IBS and Log Loss on datasets with different characteristics, such as class imbalance or varying prediction confidence levels.

## Limitations

- Implementation details for the core rule learning algorithm remain underspecified, particularly the "learn_rule" function
- The effectiveness of domain knowledge integration depends heavily on the quality and relevance of provided rules, which may vary significantly across applications
- The Wilson Score Interval assumption of binomial distributions may not hold for all rule types, potentially affecting confidence estimates in edge cases

## Confidence

- High confidence: Wilson Score Interval provides more reliable confidence estimates than simple probability
- Medium confidence: Confidence-based pruning effectively prevents overfitting
- Medium confidence: Domain knowledge integration improves performance with limited data

## Next Checks

1. Test CON-FOLD's performance on datasets with varying class imbalance ratios to evaluate Wilson Score Interval effectiveness across different probability distributions

2. Conduct ablation studies removing the pruning mechanism to quantify its impact on overfitting prevention across multiple UCI datasets

3. Evaluate domain knowledge integration on multiple real-world tasks with varying amounts of training data to establish generalizability of the approach