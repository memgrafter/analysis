---
ver: rpa2
title: Interpretable Multi-task Learning with Shared Variable Embeddings
arxiv_id: '2405.06330'
source_url: https://arxiv.org/abs/2405.06330
tags:
- shared
- variable
- embeddings
- embedding
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces shared variable embeddings for interpretable
  multi-task learning with tabular data. The method maps each variable to a tuple
  of a value and an embedding, where embeddings are combined via attention over a
  set of shared embeddings reused across tasks.
---

# Interpretable Multi-task Learning with Shared Variable Embeddings

## Quick Facts
- **arXiv ID**: 2405.06330
- **Source URL**: https://arxiv.org/abs/2405.06330
- **Reference count**: 30
- **Primary result**: Shared variable embeddings enable interpretable multi-task learning with tabular data, matching baseline accuracy while allowing identification of abstract concepts through attention mechanisms.

## Executive Summary
This paper introduces shared variable embeddings for interpretable multi-task learning with tabular data. The method maps each variable to a tuple of a value and an embedding, where embeddings are combined via attention over a set of shared embeddings reused across tasks. This allows models to handle tasks with different input/output structures while enabling interpretability through shared concepts. Experiments on UCI-121 show that shared embeddings match baseline accuracy without fine-tuning and that sparse attention increases accuracy while reducing training steps.

## Method Summary
The proposed approach maps each variable in a dataset to a tuple containing its value and an embedding vector. A set of shared embeddings is maintained across all tasks, and attention mechanisms are used to combine these shared embeddings for each variable. The attention weights are learned during training, allowing the model to dynamically select relevant shared concepts for each variable in each task. This architecture enables handling tasks with different input/output structures while maintaining interpretability through the shared embedding space.

## Key Results
- Shared embeddings match baseline accuracy without fine-tuning on UCI-121 dataset
- Sparse attention increases accuracy while reducing training steps compared to dense attention
- Qualitative analysis demonstrates ability to identify abstract concepts like physical measurements and biological processes

## Why This Works (Mechanism)
The method works by creating a shared conceptual space where variables from different tasks can be mapped to common embeddings. Through attention mechanisms, the model learns which shared concepts are relevant for each variable in each task. This allows the model to leverage knowledge across tasks while maintaining interpretability, as the attention weights reveal which shared concepts are being used for each prediction.

## Foundational Learning
- **Attention mechanisms**: Used to combine shared embeddings for each variable; needed to dynamically select relevant concepts; quick check: verify attention weights sum to 1
- **Variable embeddings**: Map each variable to value-embedding tuple; needed to create shared representation space; quick check: embedding dimensions match across tasks
- **Multi-task learning**: Handle different tasks with shared embeddings; needed for knowledge transfer; quick check: each task has distinct loss function
- **Sparse attention**: Increases accuracy while reducing training steps; needed for efficient computation; quick check: sparsity ratio > 0.5
- **Interpretability metrics**: Qualitative analysis of shared concepts; needed to validate interpretability claims; quick check: concepts are semantically meaningful

## Architecture Onboarding

**Component Map**: Input Data -> Variable Embedding Layer -> Attention Mechanism -> Shared Embeddings -> Task-specific Heads -> Output

**Critical Path**: Variable values are embedded, then combined with shared embeddings via attention, producing task-specific predictions through dedicated heads for each task.

**Design Tradeoffs**: The main tradeoff is between interpretability and accuracy - sparse attention improves accuracy but may reduce interpretability by making attention patterns less clear. The shared embedding approach enables cross-task knowledge transfer but requires careful design to avoid concept overlap.

**Failure Signatures**: Poor performance may indicate insufficient shared embedding diversity or inappropriate attention weighting. Loss of interpretability suggests shared concepts are too abstract or attention patterns are not meaningful.

**First Experiments**:
1. Train single-task model without shared embeddings as baseline
2. Train multi-task model with shared embeddings but dense attention
3. Compare sparse vs dense attention on same task configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Validated only on tabular data from UCI-121 dataset, limiting generalizability to other data types
- Potential vulnerabilities to adversarial manipulation of attention weights not addressed
- Interpretability relies on qualitative rather than quantitative metrics

## Confidence
- Matching baseline accuracy: Medium confidence (single dataset)
- Sparse attention benefits: Medium confidence (single dataset)
- Handling different task structures: Medium confidence (limited configurations)

## Next Checks
1. Test the shared embedding approach on non-tabular datasets (text, images) to evaluate cross-domain applicability and robustness.
2. Develop quantitative metrics for interpretability and conduct ablation studies comparing sparse vs. dense attention on these metrics to better understand the accuracy-interpretability trade-off.
3. Evaluate the system's robustness to adversarial perturbations of attention weights and embeddings to assess security implications of the interpretability mechanism.