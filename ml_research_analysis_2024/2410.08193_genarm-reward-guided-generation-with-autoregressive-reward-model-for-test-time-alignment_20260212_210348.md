---
ver: rpa2
title: 'GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time
  Alignment'
arxiv_id: '2410.08193'
source_url: https://arxiv.org/abs/2410.08193
tags:
- reward
- genarm
- autoregressive
- alignment
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of test-time alignment of language
  models with human preferences. Existing test-time methods struggle because they
  rely on trajectory-level reward models that evaluate complete responses, making
  them unsuitable for autoregressive text generation which requires next-token rewards
  from partial responses.
---

# GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment

## Quick Facts
- arXiv ID: 2410.08193
- Source URL: https://arxiv.org/abs/2410.08193
- Reference count: 40
- Key outcome: Introduces Autoregressive Reward Model for efficient test-time alignment, achieving results matching training-time methods

## Executive Summary
This paper addresses the challenge of test-time alignment of language models with human preferences by introducing GenARM, which uses an Autoregressive Reward Model (Autoregressive RM) to predict next-token rewards from partial responses. Unlike traditional trajectory-level reward models that evaluate complete responses, Autoregressive RM is designed for autoregressive text generation, enabling efficient next-token sampling by combining rewards with base LLM logits. The authors prove that this parametrization is theoretically expressive enough to guide frozen LLMs toward any distribution achievable by traditional RMs, and empirically demonstrate significant improvements over prior test-time alignment baselines while matching training-time methods like DPO.

## Method Summary
GenARM trains an Autoregressive Reward Model using the same preference datasets and objective function as trajectory-level RMs, but with a novel parameterization that predicts next-token rewards conditioned on partial responses. During inference, the next-token probability is computed as a weighted combination of the base LLM's probability and the Autoregressive RM's reward: ˜πdecode(yt|x, y<t) ∝ πbase(yt|x, y<t) × (πr(yt|x, y<t))^(1/β). This enables efficient inference without generating multiple complete responses. The method also supports weak-to-strong guidance, allowing smaller RMs to align larger LLMs, and multi-objective alignment for real-time preference trade-offs.

## Key Results
- GenARM significantly outperforms prior test-time alignment baselines (ARGS, Transfer-Q) on human preference datasets
- Achieves win/tie/lose rates competitive with training-time methods like DPO and T-Flint
- Enables efficient weak-to-strong guidance, using 7B Autoregressive RM to guide 13B and 70B base LLMs
- Supports multi-objective alignment with real-time trade-offs between preference dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive RM provides accurate next-token rewards by directly predicting token-level rewards from partial responses
- Mechanism: The reward model is trained to output log probabilities for each token conditioned on previous tokens, creating a dense reward signal that matches the autoregressive nature of text generation
- Core assumption: Token-level rewards can be learned from preference data where only complete response preferences are available
- Evidence anchors: [abstract], [section 4.1], [corpus]
- Break condition: If token-level rewards cannot be accurately predicted from partial responses, the method fails to provide better guidance than trajectory-level RMs

### Mechanism 2
- Claim: GenARM achieves efficient inference by combining next-token rewards with base LLM logits in a single forward pass
- Mechanism: The next-token probability is computed as a weighted combination of the base LLM's probability and the Autoregressive RM's reward, avoiding the need to generate multiple complete responses
- Core assumption: The base LLM's logits and Autoregressive RM's rewards can be combined multiplicatively without losing alignment quality
- Evidence anchors: [abstract], [section 4.2], [corpus]
- Break condition: If the combination formula does not preserve the desired distribution, alignment quality degrades

### Mechanism 3
- Claim: Autoregressive RM is theoretically expressive enough to guide frozen LLMs toward any distribution achievable by traditional RMs
- Mechanism: The log probability parametrization captures the full equivalence class of reward functions, ensuring no expressiveness loss despite the autoregressive constraint
- Core assumption: The Plackett-Luce/Bradley-Terry preference framework allows all reward equivalence classes to be represented as log probabilities
- Evidence anchors: [abstract], [section 5], [corpus]
- Break condition: If the KL-regularized RL framework assumptions are violated, the expressiveness guarantee fails

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: GenARM builds on RLHF concepts but operates at test-time rather than training-time
  - Quick check question: What are the three main steps in the RLHF pipeline and how does GenARM modify the third step?

- Concept: KL-regularized reinforcement learning and its closed-form solution
  - Why needed here: The theoretical foundation for why Autoregressive RM can guide LLMs relies on this framework
  - Quick check question: What is the closed-form solution for the KL-regularized RL objective and how does it relate to controlled decoding?

- Concept: Weak-to-strong generalization in alignment
  - Why needed here: GenARM enables smaller RMs to guide larger LLMs without training the larger models
  - Quick check question: How does test-time alignment with smaller RMs differ from training-time fine-tuning in terms of computational cost and flexibility?

## Architecture Onboarding

- Component map: Base LLM (frozen) -> Autoregressive RM -> Sampling module -> Output token
- Critical path: Prompt → Base LLM forward pass → Autoregressive RM forward pass → Token sampling → Output token
- Design tradeoffs:
  - Training Autoregressive RM vs using trajectory-level RM: Higher training time (1% more parameters) but better test-time performance
  - Single objective vs multi-objective: More complex sampling but enables real-time preference adjustment
  - Weak-to-strong guidance: Avoids training large models but may have alignment quality limitations
- Failure signatures:
  - Gibberish generation: Indicates Autoregressive RM is not providing coherent next-token rewards
  - Degraded alignment: Suggests the combination formula or reward parametrization is incorrect
  - High inference cost: Means the implementation is not avoiding full response generation
- First 3 experiments:
  1. Train Autoregressive RM on HH-RLHF dataset and verify it assigns higher rewards to preferred tokens in sample responses
  2. Implement GenARM sampling and compare inference time against ARGS baseline on simple prompts
  3. Test weak-to-strong guidance by using 7B Autoregressive RM to guide 13B base LLM on AlpacaEval prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Autoregressive Reward Model theoretically achieve the same expressiveness as trajectory-level RMs in all possible alignment scenarios?
- Basis in paper: [explicit] The paper proves that Autoregressive RM can guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized RL framework.
- Why unresolved: While the theoretical proof establishes expressiveness equivalence within the KL-regularized framework, it doesn't address whether this holds for all possible reward function classes or RL frameworks beyond this specific setting.
- What evidence would resolve it: A formal proof extending the expressiveness result to other RL frameworks (e.g., standard RL without KL regularization) or a counterexample showing limitations in specific alignment scenarios.

### Open Question 2
- Question: How does the Autoregressive RM's performance scale with increasing model size, and are there diminishing returns or specific size thresholds where benefits plateau?
- Basis in paper: [inferred] The paper demonstrates weak-to-strong guidance where a 7B Autoregressive RM guides larger models (13B and 70B), but doesn't systematically explore scaling relationships or performance plateaus.
- Why unresolved: The experiments show that smaller RMs can guide larger LLMs effectively, but the relationship between RM size and guidance quality isn't thoroughly characterized across different scale ratios or base model families.
- What evidence would resolve it: A comprehensive scaling study varying both RM and base model sizes systematically, measuring guidance effectiveness across multiple orders of magnitude in parameter counts.

### Open Question 3
- Question: What are the fundamental limitations of using test-time alignment methods like GenARM compared to training-time approaches in terms of long-term behavioral consistency?
- Basis in paper: [explicit] The paper shows GenARM matches training-time methods like DPO in alignment efficacy but doesn't address potential differences in long-term behavioral consistency or robustness.
- Why unresolved: While GenARM achieves comparable performance in controlled evaluations, it's unclear whether the test-time approach maintains consistent alignment over extended interactions or in scenarios not well-represented in training data.
- What evidence would resolve it: Longitudinal studies comparing GenARM-guided models against training-time aligned models across extended interaction sequences, measuring consistency in preference adherence and robustness to distributional shifts.

## Limitations

- The fundamental uncertainty about whether token-level rewards can be accurately learned from preference data where only complete responses are annotated
- Lack of systematic evaluation of how the size gap between Autoregressive RM and base LLM affects alignment quality in weak-to-strong guidance
- Multi-objective alignment extension is not thoroughly empirically tested on real-world preference trade-offs

## Confidence

- **High Confidence**: The theoretical expressiveness proof for Autoregressive RM is sound and the core mechanism of combining base LLM logits with reward model outputs is well-established in controlled decoding literature.
- **Medium Confidence**: The empirical results showing GenARM outperforming ARGS and matching training-time methods are convincing, but the evaluation methodology relies heavily on GPT-4 judging which introduces potential bias.
- **Low Confidence**: The weak-to-strong generalization claims lack comprehensive validation across different model size gaps, and the multi-objective alignment extension is not thoroughly empirically tested.

## Next Checks

1. **Token-level Reward Quality Validation**: Manually inspect whether the trained Autoregressive RM assigns higher rewards to tokens that lead to preferred completions by generating partial responses and comparing reward distributions between preferred and dispreferred continuations.

2. **Combination Formula Sensitivity Analysis**: Systematically vary the β parameter in the sampling formula across a wide range (0.1 to 10) and measure how alignment quality and inference speed trade off, identifying the optimal operating point and robustness range.

3. **Weak-to-Strong Gap Scaling Study**: Train multiple Autoregressive RMs of different sizes (1B, 3B, 7B) and test their ability to guide increasingly larger base LLMs (7B, 13B, 33B) on the same tasks, quantifying the degradation in alignment quality as the model size gap increases.