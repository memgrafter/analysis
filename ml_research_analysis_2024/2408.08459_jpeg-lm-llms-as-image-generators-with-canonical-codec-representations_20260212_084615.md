---
ver: rpa2
title: 'JPEG-LM: LLMs as Image Generators with Canonical Codec Representations'
arxiv_id: '2408.08459'
source_url: https://arxiv.org/abs/2408.08459
tags:
- jpeg
- images
- generation
- image
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using canonical codecs (JPEG for images, AVC/H.264
  for videos) as non-neural preprocessors to discretize continuous visual data into
  sequences of bytes, enabling autoregressive LLM architectures to directly model
  and generate visual content. By pretraining a 7B Llama-2 model from scratch, JPEG-LM
  achieves 31% FID reduction compared to vector quantization baselines on image generation
  tasks, while AVC-LM serves as a proof-of-concept for video generation.
---

# JPEG-LM: LLMs as Image Generators with Canonical Codec Representations

## Quick Facts
- arXiv ID: 2408.08459
- Source URL: https://arxiv.org/abs/2408.08459
- Authors: Xiaochuang Han; Marjan Ghazvininejad; Pang Wei Koh; Yulia Tsvetkov
- Reference count: 18
- Primary result: 31% FID reduction compared to vector quantization baselines

## Executive Summary
This paper proposes using canonical codecs (JPEG for images, AVC/H.264 for videos) as non-neural preprocessors to discretize continuous visual data into sequences of bytes, enabling autoregressive LLM architectures to directly model and generate visual content. By pretraining a 7B Llama-2 model from scratch, JPEG-LM achieves 31% FID reduction compared to vector quantization baselines on image generation tasks, while AVC-LM serves as a proof-of-concept for video generation. The method offers advantages in handling long-tail visual elements and avoids the complexities of vector quantization or prohibitive sequence lengths of pixel-based approaches.

## Method Summary
JPEG-LM transforms images into byte sequences using JPEG compression as a preprocessing step, then trains an autoregressive LLM to generate these byte sequences. The JPEG codec provides a canonical, non-learnable discretization that preserves visual semantics while significantly compressing data. The 7B Llama-2 model is trained from scratch on this byte-level representation, learning to predict the next byte in the sequence. For video generation, AVC-LM applies the same principle using H.264 compression. This approach sidesteps the need for vector quantization or specialized visual tokenizers, leveraging the established compression standards as efficient visual encoders.

## Key Results
- Achieves 31% FID reduction compared to vector quantization baselines on image generation tasks
- JPEG-LM successfully generates high-quality images from byte-level sequences
- AVC-LM demonstrates proof-of-concept viability for video generation
- Shows better handling of long-tail visual elements compared to VQ-based methods

## Why This Works (Mechanism)
The approach works by leveraging the established compression efficiency of JPEG and H.264 codecs. These codecs already compress visual data into semantically meaningful byte sequences that capture essential visual information. By training LLMs directly on these canonical representations, the model learns to generate coherent visual content without the need for specialized visual tokenizers. The byte-level representation preserves fine-grained details while maintaining manageable sequence lengths, allowing standard LLM architectures to process visual data effectively.

## Foundational Learning
- **Autoregressive Generation**: Why needed - To predict next elements in sequences for coherent generation; Quick check - Model's ability to maintain visual consistency across generated outputs
- **JPEG Compression**: Why needed - Provides efficient, canonical visual representation; Quick check - Byte sequence quality and compression ratio compared to alternatives
- **Byte-Level Processing**: Why needed - Enables direct LLM application without visual tokenizers; Quick check - Sequence length management and computational efficiency
- **LLM Architecture Adaptation**: Why needed - Standard LLMs must handle visual byte sequences; Quick check - Training stability and convergence on visual data

## Architecture Onboarding

**Component Map**: Image -> JPEG Compression -> Byte Sequence -> LLM Training -> Byte Generation -> JPEG Decompression -> Image Output

**Critical Path**: JPEG compression and decompression are critical bottlenecks affecting quality and efficiency. The LLM's ability to learn meaningful byte-level patterns determines overall success.

**Design Tradeoffs**: Uses established compression standards for reliability versus potential information loss in compression. Byte-level granularity versus computational efficiency. Standard LLM architectures versus specialized visual models.

**Failure Signatures**: Visual artifacts at compression boundaries, loss of fine details, mode collapse in generated outputs, inability to capture rare visual elements.

**First Experiments**: 1) Compare JPEG-LM outputs with JPEG reconstruction quality, 2) Test byte-level prediction accuracy versus token-level methods, 3) Evaluate computational efficiency against pixel-based autoregressive models.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetic benchmarks and established image generation datasets
- 31% FID reduction requires scrutiny regarding dataset selection and comparison conditions
- Scalability to larger models and different visual domains remains unproven
- Limited qualitative analysis of handling long-tail visual elements

## Confidence

**High confidence**: Technical feasibility of using canonical codecs for visual tokenization and demonstrated FID score reduction

**Medium confidence**: Claims about handling long-tail visual elements more effectively than VQ approaches

**Medium confidence**: Scalability claims to video generation through AVC-LM proof-of-concept

**Low confidence**: Universal efficiency assertions without extensive ablation studies across different model sizes and domains

## Next Checks
1. Conduct qualitative user studies comparing JPEG-LM outputs with VQ-VAE and diffusion model generations across diverse visual categories
2. Perform comprehensive scalability analysis by training JPEG-LM models of varying sizes (1B, 13B, 70B) and evaluating performance-efficiency trade-offs
3. Test method's robustness across different image types (medical imaging, satellite imagery, scientific visualization) beyond standard benchmarks