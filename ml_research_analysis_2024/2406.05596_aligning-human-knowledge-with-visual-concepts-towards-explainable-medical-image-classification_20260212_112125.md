---
ver: rpa2
title: Aligning Human Knowledge with Visual Concepts Towards Explainable Medical Image
  Classification
arxiv_id: '2406.05596'
source_url: https://arxiv.org/abs/2406.05596
tags:
- criteria
- visual
- explicd
- knowledge
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of explainability in deep learning
  models for medical image classification. The proposed Explicd framework queries
  diagnostic criteria from large language models or human experts, then uses a visual
  concept learning module to align visual features with these criteria via a pretrained
  vision-language model.
---

# Aligning Human Knowledge with Visual Concepts Towards Explainable Medical Image Classification

## Quick Facts
- arXiv ID: 2406.05596
- Source URL: https://arxiv.org/abs/2406.05596
- Authors: Yunhe Gao; Difei Gu; Mu Zhou; Dimitris Metaxas
- Reference count: 34
- Primary result: Explicd achieves 1-7% accuracy improvements over baselines across five medical image benchmarks while providing interpretable visualizations of diagnostic criteria alignment

## Executive Summary
This paper addresses the lack of explainability in deep learning models for medical image classification by proposing Explicd, a framework that aligns human diagnostic knowledge with visual concepts in medical images. The method queries diagnostic criteria from large language models or human experts, then uses a visual concept learning module to align visual features with these criteria via a pretrained vision-language model. Explicd achieves superior classification performance compared to both traditional black-box models and existing explainable models across five medical image benchmarks, with accuracy improvements of 1-7% over the best baseline on each dataset. The method also provides interpretable visualizations showing which diagnostic criteria and image regions contribute to each prediction.

## Method Summary
Explicd is a framework for explainable medical image classification that mimics human expert decision-making by aligning visual concepts with diagnostic criteria. The method queries diagnostic criteria from large language models (like GPT-4) or human experts along specific concept axes (color, shape, texture, etc.). These criteria are encoded using a text encoder from a pretrained vision-language model and serve as "knowledge anchors." A visual concept learning module with learnable tokens captures visual features from medical images through cross-attention mechanisms. A criteria anchor contrastive loss aligns these visual concepts with the corresponding diagnostic criteria embeddings. The final diagnosis is predicted by a linear layer that integrates alignment scores across all concept axes, with the weights reflecting the significance of each criterion's contribution.

## Key Results
- Explicd achieves 1-7% accuracy improvements over the best baseline on each of the five medical image datasets tested
- The framework provides interpretable visualizations showing alignment between visual concepts and diagnostic criteria
- Performance improvements are consistent across diverse medical domains including skin lesions, colorectal cancer, diabetic retinopathy, breast masses, and chest X-rays

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagnostic criteria queried from LLMs/human experts serve as "knowledge anchors" that guide the alignment of visual concepts in medical images.
- Core assumption: The diagnostic criteria provided by LLMs or human experts accurately capture the essential visual features needed for classification across different medical conditions.
- Evidence anchors: Abstract mentions querying domain knowledge to establish diagnostic criteria; section 3.1 describes formulating textual diagnosis criteria from LLMs or experts.
- Break condition: If the diagnostic criteria are inaccurate, incomplete, or inconsistent across different conditions, the knowledge anchor alignment would fail.

### Mechanism 2
- Claim: The criteria anchor contrastive loss effectively aligns fine-grained visual features with diagnostic criteria, improving both interpretability and classification performance.
- Core assumption: The contrastive loss formulation can effectively capture fine-grained visual features and align them with the nuanced textual criteria across different concept axes.
- Evidence anchors: Section 3.2 provides the mathematical formulation of the criteria anchor contrastive loss; section 4.3 shows high similarity scores between visual concepts and corresponding diagnostic criteria.
- Break condition: If the contrastive loss fails to capture subtle visual distinctions or the alignment between visual concepts and criteria becomes too coarse, classification performance would degrade.

### Mechanism 3
- Claim: The linear layer that integrates alignment scores across all criteria axes mimics human expert decision-making, leading to accurate classification.
- Core assumption: Human expert decision-making can be effectively modeled as a weighted integration of diagnostic criteria assessments across different concept axes.
- Evidence anchors: Abstract mentions determining final diagnostic outcome based on similarity scores; section 3.3 describes using a linear layer to integrate alignment scores from all criteria axes.
- Break condition: If the linear integration of criteria scores does not accurately capture the complex decision boundaries or if certain criteria are weighted incorrectly, classification accuracy would suffer.

## Foundational Learning

- Concept: Vision-language model pretraining and fine-tuning
  - Why needed here: Explicd builds upon pretrained vision-language models (CLIP, BioViL, BiomedCLIP) as the foundation for aligning visual and textual representations
  - Quick check question: How does zero-shot performance of VLMs on medical tasks compare to supervised black-box models, and why does Explicd outperform both?

- Concept: Contrastive learning and embedding alignment
  - Why needed here: The criteria anchor contrastive loss is a specialized form of contrastive learning that aligns visual concepts with diagnostic criteria embeddings
  - Quick check question: What is the mathematical formulation of the contrastive loss used in Explicd, and how does it differ from standard contrastive learning approaches?

- Concept: Interpretability through concept bottleneck models
  - Why needed here: Explicd provides interpretable explanations by showing which diagnostic criteria and image regions contribute to each prediction, similar to concept bottleneck models but using LLM-queried criteria
  - Quick check question: How does Explicd's approach to interpretability differ from post-hoc methods like Grad-CAM, and what are the advantages of this self-interpretable approach?

## Architecture Onboarding

- Component map: Image → Visual encoder → Cross-attention with learnable tokens → Visual concept tokens → Contrastive loss with criteria anchors → Alignment scores → Linear layer → Final classification

- Critical path: The framework processes medical images through a frozen visual encoder, applies cross-attention with learnable visual concept tokens, aligns these concepts with diagnostic criteria via contrastive loss, and makes final predictions through weighted integration of alignment scores.

- Design tradeoffs:
  - Using frozen text encoder vs. fine-tuning it for better criteria alignment
  - Number of learnable visual concept tokens (K) vs. computational cost
  - Temperature parameter (τ) in contrastive loss affecting alignment sharpness
  - Direct LLM querying vs. curated knowledge bases for diagnostic criteria

- Failure signatures:
  - Poor zero-shot VLM performance on medical tasks indicates need for domain adaptation
  - Low alignment scores between visual concepts and criteria suggest ineffective visual concept learning
  - Misalignment between criteria weights and actual importance in human diagnosis
  - Computational overhead from additional modules impacting real-time inference

- First 3 experiments:
  1. Evaluate zero-shot performance of different VLMs (CLIP, BioViL, BiomedCLIP) on each medical benchmark to identify the best base model
  2. Test different numbers of visual concept tokens (K) and their impact on classification accuracy and interpretability
  3. Compare Explicd's performance and interpretability against LaBo and black-box models on a subset of the datasets before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework's reliance on domain knowledge queried from LLMs be ensured for accuracy and reliability across diverse medical specialties?
- Basis in paper: The paper mentions querying domain knowledge from LLMs like GPT-4 or human experts to formulate diagnostic criteria.
- Why unresolved: The accuracy and reliability of LLM-queried knowledge across diverse medical specialties is not addressed, raising concerns about potential errors or biases in the diagnostic criteria.
- What evidence would resolve it: Comparative studies evaluating the accuracy and reliability of diagnostic criteria generated by LLMs versus human experts across various medical specialties, along with error analysis to identify potential sources of bias or misinformation.

### Open Question 2
- Question: How can the framework's visual concept learning module be further improved to capture more complex and nuanced visual features in medical images?
- Basis in paper: The paper introduces a visual concept learning module using learnable visual concept tokens and a criteria anchor contrastive loss to align visual features with diagnostic criteria.
- Why unresolved: The current visual concept learning module may have limitations in capturing highly complex or subtle visual features, potentially affecting the accuracy of the framework's diagnoses.
- What evidence would resolve it: Experimental comparisons of the framework's performance with and without enhancements to the visual concept learning module, such as incorporating attention mechanisms or multi-scale feature extraction, to assess the impact on diagnostic accuracy and interpretability.

### Open Question 3
- Question: How can the framework's explainability be extended to provide more detailed and actionable insights for medical professionals, beyond just highlighting important image regions?
- Basis in paper: The paper demonstrates the framework's explainability through alignment scores and heatmap visualizations of important image regions.
- Why unresolved: While the current explainability features are valuable, they may not provide sufficient detail or actionable insights for medical professionals to fully understand the reasoning behind the framework's diagnoses.
- What evidence would resolve it: Development and evaluation of additional explainability features, such as generating natural language explanations of the framework's reasoning process or providing comparative analyses of the framework's diagnoses with those of human experts, to assess their utility in supporting medical decision-making.

## Limitations

- The framework's performance is fundamentally dependent on the quality and accuracy of LLM-generated diagnostic criteria, which may contain biases or errors
- Manual specification of concept axes (color, shape, texture, etc.) for each medical task limits generalizability across diverse medical domains
- The additional computational overhead from visual concept learning and contrastive alignment modules may impact practical deployment in clinical settings where inference speed is critical

## Confidence

**High Confidence**: The framework's core architecture is well-specified with clear mathematical formulations for the criteria anchor contrastive loss and alignment mechanism. The empirical results showing 1-7% accuracy improvements over baselines across five medical benchmarks are directly measurable and reproducible.

**Medium Confidence**: The interpretability claims are supported by qualitative visualizations showing alignment scores between visual concepts and diagnostic criteria. However, the paper lacks quantitative metrics for interpretability quality (such as concept precision/recall or human evaluation of explanation quality).

**Low Confidence**: The generalizability of LLM-queried diagnostic criteria across different medical specialties and the robustness of the method when criteria are incomplete or contain errors remain largely unexplored. The paper does not address potential failure modes when the knowledge anchor alignment breaks down.

## Next Checks

1. **Criteria Quality Validation**: Systematically evaluate the accuracy and completeness of GPT-4-generated diagnostic criteria by comparing them against expert-curated medical guidelines across all five datasets. Measure inter-criteria consistency when querying the LLM multiple times for the same task.

2. **Ablation on Concept Axes**: Perform controlled experiments removing specific concept axes (e.g., texture-only, color-only) to determine which diagnostic criteria contribute most to classification performance and whether the weighted integration in the linear layer reflects their actual importance.

3. **Cross-Dataset Transferability**: Test whether diagnostic criteria learned from one medical domain (e.g., skin lesions) can be effectively transferred to a different domain (e.g., chest X-rays) without domain-specific retraining, evaluating both performance degradation and interpretability maintenance.