---
ver: rpa2
title: 'Don''t Forget Your Reward Values: Language Model Alignment via Value-based
  Calibration'
arxiv_id: '2402.16030'
source_url: https://arxiv.org/abs/2402.16030
tags:
- reward
- methods
- responses
- calibration
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Value-based CaliBration (VCB) method
  to address misalignment issues in existing order-based calibration methods for LLM
  alignment. VCB ensures that the relative probability gap between responses is directly
  proportional to their relative reward gap, overcoming the misalignment problem of
  solely calibrating according to reward orders.
---

# Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration

## Quick Facts
- arXiv ID: 2402.16030
- Source URL: https://arxiv.org/abs/2402.16030
- Reference count: 13
- Primary result: VCB outperforms order-based methods, achieving 6.5% win-lose differential against DPO on AnthropicHH and 20.9% lead on Reddit TL;DR

## Executive Summary
This paper proposes Value-based CaliBration (VCB), a novel method for aligning large language models (LLMs) with human preferences that addresses limitations in existing order-based calibration approaches. Unlike methods like DPO that calibrate solely based on preference orderings, VCB ensures that probability gaps between responses are directly proportional to their reward gaps. Experiments on a 2.8-billion parameter model demonstrate that VCB achieves superior performance on AI assistant and summarization tasks, with improved generalizability, robustness, and stability across diverse settings.

## Method Summary
VCB aligns LLMs through a three-step pipeline: supervised fine-tuning (SFT), reward model training, and value-based calibration. The core innovation is the VCB loss function, which minimizes the squared difference between normalized reward gaps and probability gaps between the policy model and SFT model. This approach eliminates the partition function while preserving reward information, ensuring that the relative probability gap between responses is directly proportional to their relative reward gap. The method is theoretically grounded in an optimization framework involving generalized conditional entropy and information content functions.

## Key Results
- VCB achieves a 6.5% win-lose differential against DPO on the AnthropicHH AI assistant dataset
- On Reddit TL;DR summarization, VCB demonstrates a 20.9% performance lead over baselines
- VCB shows superior out-of-distribution generalization, maintaining a 16.5% advantage over DPO on CNN/DailyMail
- The method demonstrates improved stability and robustness across diverse alignment settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VCB ensures probability gaps are directly proportional to reward gaps, overcoming order-based misalignment
- **Mechanism:** Uses difference method to eliminate partition function while preserving reward function, aligning policy model's probability distribution with reward values through squared error minimization
- **Core assumption:** Reward model accurately reflects human preferences and reward values are meaningful for alignment
- **Evidence anchors:** Abstract results show VCB surpasses existing methods; section 4.1 provides theoretical justification for the proportionality relationship
- **Break condition:** Performance degrades if reward model doesn't accurately reflect human preferences or if reward values lack meaningful alignment information

### Mechanism 2
- **Claim:** VCB is theoretically grounded, deriving from a single optimization problem under different entropy settings
- **Mechanism:** Setting generalized information content function as specific form involving KL-divergence and reward standard deviation derives VCB from maxπ Ex∼D,y∼π(.|x) [r(x, y)] + H π
ψ (Y |X) optimization
- **Core assumption:** Optimization problem and generalized information content function validly represent alignment objective
- **Evidence anchors:** Abstract emphasizes robust theoretical deduction; section 4.2 provides conditions under which VCB satisfies theoretical requirements
- **Break condition:** Theoretical grounding weakens if optimization problem or information content function don't accurately represent alignment objective

### Mechanism 3
- **Claim:** VCB benefits from more accurate reward models, as shown by out-of-distribution generalization
- **Mechanism:** More accurate reward models enable better alignment of policy model's probability distribution with human preferences
- **Core assumption:** Reward model accuracy is key factor in VCB performance
- **Evidence anchors:** Section 5.6 shows VCB's performance varies with reward model accuracy across datasets; section 5.7 demonstrates superior OOD generalization
- **Break condition:** Mechanism weakens if reward model accuracy doesn't significantly impact VCB performance or if OOD generalization isn't valid effectiveness measure

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** RLHF provides the underlying framework for aligning LLMs with human preferences; understanding its phases and objectives is crucial for comprehending VCB
  - **Quick check question:** What are the three main phases of RLHF, and how does each phase contribute to aligning LLMs with human preferences?

- **Concept:** Direct Preference Optimization (DPO) and other order-based calibration methods
  - **Why needed here:** VCB is proposed as improvement over DPO and similar methods; understanding their mechanisms, limitations, and theoretical foundations is essential for appreciating VCB's novelty
  - **Quick check question:** How do DPO and other order-based calibration methods differ from VCB in terms of their approach to aligning LLMs with human preferences?

- **Concept:** Generalized conditional entropy and information content functions
  - **Why needed here:** Theoretical derivation of VCB relies on these concepts; understanding them is necessary for grasping the mathematical foundation of the method
  - **Quick check question:** How does the choice of generalized information content function ψπ(y|x) impact the optimal solution of the alignment optimization problem?

## Architecture Onboarding

- **Component map:** Pre-trained LLM -> SFT model (fine-tuned on SFT dataset) -> Reward model (trained on preference dataset) -> Policy model (aligned using VCB)
- **Critical path:** 1) Fine-tune pre-trained LLM on SFT dataset to obtain SFT model; 2) Train reward model on preference dataset; 3) Generate candidate responses using SFT model for each prompt in SFT dataset; 4) Obtain rewards for candidate responses using reward model; 5) Train policy model using VCB loss function on generated dataset
- **Design tradeoffs:** More accurate reward models improve performance but require additional annotation costs; hyper-parameter choice impacts effectiveness but requires extensive tuning; off-policy sampling is computationally efficient but may limit exploration of policy model's distribution
- **Failure signatures:** Poor reward model accuracy leads to suboptimal alignment; non-diverse or unrepresentative generated dataset causes overfitting; improper hyper-parameter tuning prevents convergence or causes suboptimal solutions
- **First 3 experiments:** 1) Implement VCB loss function and integrate into existing alignment pipeline; 2) Train policy model using VCB on small-scale dataset (subset of AnthropicHH) and evaluate against baselines; 3) Conduct ablation studies on hyper-parameters (γ, λ) to understand impact and identify optimal settings

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does VCB's performance scale with increasing model size compared to other calibration methods?
- **Basis in paper:** Paper only tested VCB on 2.8-billion parameter model and acknowledges limited computational resources prevented testing larger models or on-policy sampling
- **Why unresolved:** Paper explicitly states intent to conduct more comprehensive experiments on larger-scale LLMs in future to validate scalability and generalizability
- **What evidence would resolve it:** Experiments comparing VCB against baselines across various model sizes (7B, 13B, 70B parameters) would show how well VCB scales and whether advantages persist at larger scales

### Open Question 2
- **Question:** How sensitive is VCB to the accuracy of the reward model?
- **Basis in paper:** Paper shows VCB's performance improvement varies across datasets with different reward model accuracies and acknowledges poorer reward model may weaken VCB's advantages
- **Why unresolved:** Paper demonstrates VCB benefits from more accurate reward model but doesn't explore relationship between reward model accuracy and VCB performance or how to ensure reward model always accurately reflects human preferences
- **What evidence would resolve it:** Systematic experiments varying reward model accuracy and measuring VCB's performance across different accuracy levels would quantify this relationship; exploring methods to improve reward model accuracy or make VCB less sensitive to reward model quality would address this limitation

### Open Question 3
- **Question:** What is the impact of on-policy sampling on VCB's performance compared to off-policy strategy used in paper?
- **Basis in paper:** Paper explicitly states didn't adopt on-policy sampling due to limited computing resources and difficulty using Post-Training Quantification or offline inference acceleration frameworks with on-policy sampling
- **Why unresolved:** Paper acknowledges this as limitation and expresses intent to investigate impact of on-policy sampling with additional resources in future
- **What evidence would resolve it:** Direct comparison experiments between VCB with on-policy sampling versus off-policy sampling would quantify performance difference; exploring methods to make on-policy sampling more computationally feasible would address this limitation

## Limitations

- VCB's performance heavily depends on the accuracy of the reward model, which may require additional annotation costs and may not always accurately reflect human preferences
- The method's effectiveness across diverse datasets and model sizes remains to be thoroughly investigated, as experiments were limited to a 2.8-billion parameter model
- Hyper-parameter sensitivity, particularly for γ and sampling temperature, may affect generation quality and diversity, requiring extensive tuning for optimal performance

## Confidence

- **VCB's Mechanism:** High confidence - Clear explanation supported by theoretical justification and experimental results showing proportionality between probability and reward gaps
- **VCB's Theoretical Grounding:** Medium confidence - Claims of theoretical foundation supported by optimization framework, but proof of Theorem 1 and practical implications require further analysis
- **VCB's Performance and Generalization:** High confidence - Superior performance demonstrated across multiple datasets with significant win-lose differentials against strong baselines

## Next Checks

1. **Reward Model Ablation:** Train multiple reward models with varying accuracy levels and evaluate VCB's performance on each to determine sensitivity to reward model quality and identify minimum accuracy threshold for effective alignment

2. **Theoretical Analysis:** Provide detailed proof of Theorem 1 and analyze relationship between generalized information content function and optimal solution of alignment optimization problem to strengthen theoretical foundations

3. **Hyper-parameter Sensitivity Analysis:** Conduct comprehensive ablation study on hyper-parameters γ and λ across different datasets and model sizes to identify optimal ranges and understand trade-offs between performance and computational efficiency