---
ver: rpa2
title: Enhanced physics-informed neural networks (PINNs) for high-order power grid
  dynamics
arxiv_id: '2410.07527'
source_url: https://arxiv.org/abs/2410.07527
tags:
- power
- neural
- networks
- also
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces enhanced physics-informed neural networks
  (PINNs) to model complex, high-order power grid dynamics, specifically targeting
  synchronous generators and advanced inverter models described by nonlinear ordinary
  differential equations. The authors propose novel enhancements to improve PINN training
  and accuracy, including multiobjective optimization, adaptive balancing of loss
  gradients, and sequence-to-sequence learning.
---

# Enhanced physics-informed neural networks (PINNs) for high-order power grid dynamics

## Quick Facts
- arXiv ID: 2410.07527
- Source URL: https://arxiv.org/abs/2410.07527
- Reference count: 36
- Authors: Vineet Jagadeesan Nair
- Key outcome: Enhanced PINNs successfully model a fourth-order synchronous generator with good accuracy, but face challenges with 17-state inverter models due to higher complexity and faster dynamics.

## Executive Summary
This paper introduces enhanced physics-informed neural networks (PINNs) to model complex, high-order power grid dynamics, specifically targeting synchronous generators and advanced inverter models described by nonlinear ordinary differential equations. The authors propose novel enhancements to improve PINN training and accuracy, including multiobjective optimization, adaptive balancing of loss gradients, and sequence-to-sequence learning. These enhancements are designed to address the challenges of ill-conditioning and poor generalization in PINNs for higher-order, stiff systems. The results show that the proposed sgPINN successfully predicts the dynamics of a fourth-order synchronous generator model with good accuracy across a range of initial conditions. However, the invPINN, which aims to model all components of a grid-following inverter with 17 states, still faces significant challenges in generalization and accuracy, likely due to the higher model complexity and faster dynamics. The study highlights the potential of enhanced PINNs for accelerating high-fidelity power system simulations, while also identifying areas for further research to improve performance, particularly for inverter models.

## Method Summary
The paper proposes enhanced PINNs that combine several techniques to improve training stability and accuracy for high-order power system models. The method uses a 4-layer MLP (64 neurons) for synchronous generators and a 5-layer MLP (128 neurons) for inverters with tanh activations. Training employs a multiobjective optimization approach that initializes loss weights using Utopia and Nadir points, followed by adaptive balancing of loss gradients during training. The optimization uses ADAM for initial epochs (20k iterations) followed by L-BFGS (5k iterations). The approach also experiments with sequence-to-sequence learning to simplify the prediction task. The models are evaluated on fourth-order synchronous generator dynamics and a 17-state grid-following inverter model, comparing PINN predictions against numerical ODE solutions.

## Key Results
- sgPINN accurately predicts fourth-order synchronous generator dynamics across a range of initial conditions with good generalization
- invPINN struggles to generalize to test initial conditions for the complex 17-state inverter model despite improved training stability
- The proposed enhancements (multiobjective initialization, adaptive balancing, ADAM + L-BFGS) significantly improve training convergence and accuracy compared to standard PINN approaches
- Error accumulation in sequence-to-sequence predictions remains a challenge for long-term trajectory forecasting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive balancing of loss gradients stabilizes training by preventing dominance of any single loss term, especially important for high-order stiff systems.
- Mechanism: The paper proposes normalizing and adaptively tuning loss weights during training based on the norms of individual loss gradients. This ensures that both initial condition and ODE residual losses contribute equally to the optimization process, avoiding situations where one term overwhelms the other.
- Core assumption: The gradient norms of the different loss terms are good indicators of their relative importance during training.
- Evidence anchors:
  - [section]: "After initializing the loss weights as above, we can further also adaptively tune these weights during training based on the intermediate values of the gradients of the individual loss terms [10]."
  - [abstract]: "These enhancements are designed to address the challenges of ill-conditioning and poor generalization in PINNs for higher-order stiff systems."
- Break condition: If the gradient norms do not accurately reflect the importance of the different loss terms, or if the adaptive tuning introduces instability in the optimization process.

### Mechanism 2
- Claim: Multiobjective optimization initialization via Utopia and Nadir points provides better starting weights for the loss terms.
- Mechanism: By first training auxiliary networks to minimize each loss term separately, the paper approximates the Utopia (ideal) and Nadir (worst) points. These are then used to initialize the relative weights of the combined loss function, ensuring both terms are of similar magnitude from the start.
- Core assumption: The Pareto optimal set can be approximated by minimizing each objective separately and that the resulting Utopia and Nadir points provide useful bounds for weight initialization.
- Evidence anchors:
  - [section]: "Inspired by a popular technique in multiobjective optimization [18], we propose normalizing the objective function by the differences in the optimal values at the Nadir and Utopia points..."
  - [section]: "We can then normalize the two loss terms by initializing the regularization weights as λi = 1/LN_i − LU_i ∀i ∈ {ic, ode}, thus ensuring that both losses are similar in magnitude."
- Break condition: If the assumptions about the Pareto optimal set are incorrect, or if the auxiliary network training is not representative of the full problem.

### Mechanism 3
- Claim: Sequence-to-sequence learning simplifies the prediction task by breaking it into smaller, more manageable steps.
- Mechanism: Instead of training the PINN to predict the entire time trajectory at once, the paper experiments with training it to predict only the next time step. This prediction is then used as an initial condition for predicting the subsequent step, and so on.
- Core assumption: Predicting the next time step is an easier task than predicting the entire trajectory, and the errors will not compound significantly over multiple steps.
- Evidence anchors:
  - [section]: "Instead of trying to predict ODE solutions for the whole time domain at once, we also experimented with the approach of training the PINN to only predict the very next time step, which is likely an easier task [12]."
  - [section]: "We found that combining both ADAM and L-BFGS optimizers obtained the best results, allowing us to drive the loss down further."
- Break condition: If the errors do compound significantly over multiple steps, or if the sequential approach introduces additional complexity that outweighs its benefits.

## Foundational Learning

- Concept: Physics-Informed Neural Networks (PINNs)
  - Why needed here: PINNs are the core technology being enhanced in this paper. Understanding how they work is essential for grasping the proposed improvements.
  - Quick check question: How do PINNs incorporate physical laws into the neural network training process?

- Concept: Ordinary Differential Equations (ODEs) and Stiff Systems
  - Why needed here: The paper focuses on applying PINNs to high-order ODEs describing power grid dynamics, which are often stiff. Understanding ODE behavior is crucial for appreciating the challenges addressed.
  - Quick check question: What characteristics define a stiff ODE system, and why do they pose challenges for numerical methods?

- Concept: Multiobjective Optimization
  - Why needed here: The paper proposes using multiobjective optimization techniques to balance different loss terms during PINN training. Understanding these concepts is necessary to follow the proposed enhancements.
  - Quick check question: What is the difference between the Utopia and Nadir points in multiobjective optimization?

## Architecture Onboarding

- Component map:
  Input layer -> Dense hidden layers (4 for sgPINN, 5 for invPINN) -> tanh activation -> Output layer
  Initial conditions and time points -> ODE residual calculation -> Combined loss function

- Critical path:
  1. Define the ODE model and generate training data (initial conditions and time points)
  2. Build the PINN architecture with the specified number of layers and neurons
  3. Initialize the loss weights using the multiobjective optimization approach
  4. Train the PINN using the ADAM optimizer for initial epochs, then switch to L-BFGS
  5. Evaluate the PINN's performance on test data

- Design tradeoffs:
  - Network size vs. computational cost: Larger networks may be needed for more complex models but increase training time and memory requirements
  - Loss weight initialization vs. adaptive balancing: Careful initialization is crucial, but adaptive balancing can further improve performance
  - Sequential vs. simultaneous prediction: Sequence-to-sequence learning may simplify the task but could introduce error accumulation

- Failure signatures:
  - Poor generalization: The PINN performs well on training data but poorly on test data, indicating overfitting
  - Ill-conditioning: The loss landscape is difficult to optimize, with many local minima or saddle points
  - Imbalance between loss terms: One loss term dominates, leading to suboptimal solutions

- First 3 experiments:
  1. Train sgPINN with the proposed enhancements (multiobjective initialization, adaptive balancing, ADAM + L-BFGS) and evaluate its performance on the synchronous generator model
  2. Train invPINN with the same enhancements and assess its ability to model the inverter dynamics, noting any challenges or limitations
  3. Compare the performance of the enhanced PINNs to baseline PINNs without the proposed improvements, focusing on accuracy, generalization, and training stability

## Open Questions the Paper Calls Out
- The authors express interest in extending their enhanced PINNs to estimate unknown or uncertain ODE parameters to capture unmodeled or higher-order dynamics and further improve accuracy.
- They note that while they have developed PINNs for grid-following inverters, grid-forming inverters will become more important in the future, and they plan to extend their invPINN to consider GFM models as part of future work.
- The authors mention they have only validated their sgPINN and invPINN for standalone, single-machine simulations thus far, and that they could extend to larger-scale simulations with fine-tuning and retraining.

## Limitations
- The enhanced PINN approach shows significant challenges when scaling to complex 17-state inverter models, with poor generalization to test initial conditions despite improved training stability
- The paper lacks systematic ablation studies to quantify the individual contributions of each enhancement to overall performance improvements
- Only single-machine simulations are validated, with no results presented for interconnected power systems or network-scale applications

## Confidence
- High confidence: The synchronous generator results and the core concept of enhancing PINNs for power system applications
- Medium confidence: The effectiveness of individual proposed enhancements for the simpler sgPINN case
- Low confidence: The scalability of these enhancements to complex inverter models and their generalization across different power system configurations

## Next Checks
1. Conduct systematic ablation studies to isolate the contribution of each enhancement (multiobjective initialization, adaptive balancing, sequence-to-sequence learning) to performance improvements in the sgPINN model
2. Test the enhanced PINNs on additional power system models of varying complexity (e.g., second-order generators, different inverter topologies) to assess generalizability beyond the specific cases presented
3. Evaluate the computational efficiency gains of the enhanced PINNs compared to traditional numerical solvers across different time horizons and grid sizes