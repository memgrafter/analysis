---
ver: rpa2
title: 'LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning'
arxiv_id: '2407.15786'
source_url: https://arxiv.org/abs/2407.15786
tags:
- concept
- licorice
- learning
- concepts
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making reinforcement learning
  interpretable while reducing the need for extensive concept labeling. It introduces
  LICORICE, a method that interleaves concept learning and RL training, uses ensemble
  disagreement for active learning, and decorrelates data to improve efficiency.
---

# LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.15786
- Source URL: https://arxiv.org/abs/2407.15786
- Reference count: 40
- This paper introduces LICORICE, a method achieving high reward and concept accuracy with 500-5000 labels across five environments, often matching or outperforming baselines.

## Executive Summary
This paper addresses the challenge of making reinforcement learning interpretable while reducing the need for extensive concept labeling. It introduces LICORICE, a method that interleaves concept learning and RL training, uses ensemble disagreement for active learning, and decorrelates data to improve efficiency. Experiments across five environments show LICORICE achieves high reward and concept accuracy with far fewer labels (500–5000) than standard approaches, often matching or outperforming baselines. VLMs can serve as concept annotators in some settings but struggle with complex or continuous concepts. The method supports test-time concept interventions, enabling model interrogation. Ablation studies confirm the importance of all three key components. Limitations include VLM reliability and concept design challenges. Future work should focus on improving VLM labeling accuracy and refining concept representation design.

## Method Summary
LICORICE is a label-efficient concept-based RL method that interleaves concept learning with RL training. It uses data decorrelation to collect diverse samples and disagreement-based active learning to select informative data points for labeling. The method trains an ensemble of concept models to identify uncertain samples where additional labels would be most valuable. During training, concept layers are frozen during RL updates and RL layers are frozen during concept updates. This allows the concept predictor to adapt to changing state distributions as the policy improves. The method supports test-time concept interventions, enabling users to modify concepts and observe policy behavior changes.

## Key Results
- LICORICE achieves comparable or superior performance to baselines using 10-100x fewer concept labels (500-5000 vs 50,000+)
- VLMs can serve as concept annotators in simple environments (PixelCartPole, DoorKey) but struggle with complex or continuous concepts (Boxing, Pong)
- Data decorrelation and disagreement-based active learning significantly improve sample efficiency compared to random sampling
- LICORICE enables interpretable interventions by allowing users to modify concepts and observe policy behavior changes at test time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving concept learning and RL training enables LICORICE to adapt to changing state distributions as the policy improves.
- Mechanism: When the policy improves, it visits different states and encounters new concepts. By freezing concept layers during RL updates and freezing RL layers during concept updates, LICORICE ensures concept learning occurs on recent, on-policy data rather than outdated data from earlier policy stages.
- Core assumption: The distribution of states and associated concepts changes meaningfully as the policy improves from random to optimal.
- Evidence anchors:
  - [abstract] "it addresses the problem of concept learning on off-policy or outdated data"
  - [section 3] "If concepts are learned from data collected by a random policy, the concept distribution may not reflect the data from an optimal policy"
- Break condition: If the state distribution doesn't change significantly as the policy improves, or if concept distributions remain stable across policy iterations.

### Mechanism 2
- Claim: Data decorrelation improves training efficiency by reducing redundancy in temporally correlated samples.
- Mechanism: Instead of labeling all states encountered during rollouts, LICORICE collects more samples than needed (using budget multiplier τ) and accepts each sample with probability p. This creates a more diverse dataset by leveraging the mixing time property of Markov processes.
- Core assumption: States become increasingly independent as their temporal distance grows according to mixing time.
- Evidence anchors:
  - [abstract] "data decorrelation to produce a more diverse set of training samples"
  - [section 3] "rollouts produce a dataset Um of unlabeled states as candidates for querying for ground-truth concepts... Simply collecting and labeling all encountered states would give us long chains of nearly-identical samples"
- Break condition: If the environment has very short mixing times or if state diversity doesn't improve learning efficiency.

### Mechanism 3
- Claim: Disagreement-based active learning maximizes information gain from limited labeling budget.
- Mechanism: LICORICE trains an ensemble of N concept models and selects samples where predictions disagree most (highest variance for regression, highest variation ratio for classification). This targets decision boundaries and uncertain regions where additional labels provide maximum value.
- Core assumption: Samples with highest ensemble disagreement provide the most information for concept learning.
- Evidence anchors:
  - [abstract] "using an ensemble to actively select informative data points for labeling"
  - [section 3] "This function targets samples where prediction disagreement is highest among ensemble members, as these points often represent areas of uncertainty or decision boundaries where additional labeled data would be most informative"
- Break condition: If ensemble disagreement doesn't correlate with actual learning value, or if the ensemble becomes too correlated to provide meaningful disagreement signals.

## Foundational Learning

- Concept: Concept bottleneck models
  - Why needed here: Understanding how concept bottleneck models work is fundamental to grasping LICORICE's architecture, where policies map from states to concepts to actions
  - Quick check question: In a concept bottleneck model, what are the two main functions and how do they interact?

- Concept: Active learning and query strategies
  - Why needed here: LICORICE uses disagreement-based active learning to select which samples to label, so understanding acquisition functions and their theoretical basis is important
  - Quick check question: What is the difference between query-by-committee and entropy-based acquisition functions?

- Concept: Reinforcement learning with function approximation
  - Why needed here: LICORICE modifies standard RL algorithms (like PPO) to work with concept bottlenecks, so understanding how RL algorithms handle neural network policies is essential
  - Quick check question: How does Proximal Policy Optimization (PPO) balance exploration and exploitation in practice?

## Architecture Onboarding

- Component map: State → Feature extraction → Concept prediction → Action selection → Environment interaction → Concept labeling (selective) → Concept model update → Policy update

- Critical path: State → Feature extraction → Concept prediction → Action selection → Environment interaction → Concept labeling (selective) → Concept model update → Policy update

- Design tradeoffs:
  - Budget allocation between concept learning and RL training
  - Ensemble size vs. computational cost
  - Acceptance probability p vs. sample diversity
  - Number of iterations vs. convergence speed

- Failure signatures:
  - High concept error but low reward: RL may be working around concept errors
  - Low concept error but poor reward: Concepts may not be relevant to task performance
  - High variance in results: Insufficient diversity in training data or unstable ensemble predictions

- First 3 experiments:
  1. Run LICORICE on PixelCartPole with perfect labels to verify basic functionality and compare against CPM baseline
  2. Test data decorrelation alone by comparing against non-decorrelated version on a simple environment
  3. Evaluate active learning component by comparing against random sampling baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can concept-based representations be designed to improve performance and interpretability when using VLMs as annotators?
- Basis in paper: [explicit] The paper discusses the importance of concept design and its impact on VLM performance, particularly in complex environments.
- Why unresolved: The paper provides examples of concept-based representations but acknowledges that the exact design can significantly impact performance, especially with VLMs. It suggests that future work could refine design principles and investigate factors influencing effectiveness.
- What evidence would resolve it: Systematic empirical studies comparing different concept representations, theoretical analyses of concept design principles, and development of guidelines for effective concept representation in VLM-based settings.

### Open Question 2
- Question: What are the most effective active learning strategies for concept-based RL with limited annotation budgets?
- Basis in paper: [explicit] The paper uses disagreement-based active learning but suggests exploring more sophisticated strategies, including advanced exploration-exploitation trade-offs and recent advancements in active learning algorithms.
- Why unresolved: While the paper demonstrates the effectiveness of disagreement-based active learning, it acknowledges potential for improvement and suggests that more advanced strategies could yield better results.
- What evidence would resolve it: Comparative studies of different active learning strategies (e.g., uncertainty sampling, query-by-committee, expected error reduction) in concept-based RL settings, with metrics for both sample efficiency and concept accuracy.

### Open Question 3
- Question: How can the limitations of VLMs in providing accurate concept labels be addressed to improve concept-based RL performance?
- Basis in paper: [explicit] The paper identifies VLM limitations, particularly with continuous concepts and complex environments, and suggests future work could improve VLM capabilities and mitigate hallucinations.
- Why unresolved: The paper demonstrates VLM effectiveness in some environments but highlights significant challenges in others, without providing solutions to these limitations.
- What evidence would resolve it: Development and evaluation of techniques to improve VLM concept labeling accuracy, such as specialized prompting strategies, post-processing methods, or hybrid human-VLM labeling approaches, with quantitative improvements in concept accuracy and downstream RL performance.

## Limitations

- VLM reliability varies significantly across environments, working well for simple binary concepts but struggling with continuous or complex concepts
- Concept design remains a human-dependent task - LICORICE optimizes for given concepts but doesn't address how to identify the right concepts for interpretability
- Limited environmental diversity (5 environments, primarily MiniGrid and Atari) raises questions about scalability and generalization

## Confidence

**High confidence** in the core claims about LICORICE's effectiveness: The interleaving mechanism, data decorrelation, and active learning components are well-motivated theoretically and supported by ablation studies. The experimental results across five environments consistently show LICORICE achieves comparable or superior performance with significantly fewer labels.

**Medium confidence** in the scalability and generalization claims: While results are strong across the tested environments, the range is limited (5 environments, primarily from MiniGrid and Atari). The performance with VLMs versus ground-truth labels shows substantial variance, suggesting environmental factors significantly impact effectiveness.

**Low confidence** in the practical deployment aspects: The paper doesn't address computational overhead of maintaining ensembles during training, the sensitivity to hyperparameters like budget allocation and decorrelation probability, or how the method performs with noisy human annotators rather than ground-truth labels.

## Next Checks

1. **VLM Reliability Study**: Systematically evaluate VLM labeling accuracy across different concept types (binary, continuous, complex) and environments, including analysis of failure modes and potential prompt engineering improvements.

2. **Hyperparameter Sensitivity Analysis**: Conduct controlled experiments varying budget allocation between concept learning and RL training, ensemble size, and decorrelation probability to identify optimal configurations and robustness.

3. **Human Annotation Comparison**: Replace ground-truth labels with actual human annotators to measure the practical labeling burden and assess how well LICORICE performs under realistic annotation conditions with potential noise and inconsistency.