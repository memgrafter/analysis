---
ver: rpa2
title: 'Assessing News Thumbnail Representativeness: Counterfactual text can enhance
  the cross-modal matching ability'
arxiv_id: '2402.11159'
source_url: https://arxiv.org/abs/2402.11159
tags:
- news
- text
- language
- image
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a new problem of assessing news thumbnail
  representativeness, focusing on whether the image depicts the actors of the news
  event. To address this, the authors create NewsTT, a manually annotated dataset
  of 1000 news thumbnail-image pairs labeled by objective journalistic criteria.
---

# Assessing News Thumbnail Representativeness: Counterfactual text can enhance the cross-modal matching ability

## Quick Facts
- arXiv ID: 2402.11159
- Source URL: https://arxiv.org/abs/2402.11159
- Reference count: 31
- This study introduces a new problem of assessing news thumbnail representativeness, finding that general-purpose vision-language models struggle with this task and proposing CFT-CLIP, which uses counterfactual text generation to significantly improve performance.

## Executive Summary
This study introduces the task of assessing news thumbnail representativeness - determining whether an image accurately depicts the actors of a news event. The authors create NewsTT, a manually annotated dataset of 1000 news thumbnail-image pairs, and find that general-purpose vision-language models like CLIP and BLIP-2 struggle with this task. To address this, they propose CFT-CLIP, a contrastive learning framework that generates counterfactual news text by replacing named entities, using these as hard negative samples during training. Experiments show that CFT-CLIP significantly outperforms baseline models, achieving an F1 score of 0.815 and Spearman correlation of 0.491, supporting the hypothesis that counterfactual text enhances cross-modal matching ability.

## Method Summary
The authors propose CFT-CLIP, a contrastive learning framework that improves cross-modal matching for news thumbnail assessment. The method generates counterfactual text by identifying named entities in news articles, masking them, and using a masked language model (BERT) to predict replacement tokens. These counterfactual texts, which are semantically similar to the original but no longer represent the same actors, serve as hard negative samples in contrastive learning. The model is trained using a bi-encoder architecture (CLIP) and can be further improved through domain-adaptive pretraining on news-specific corpora. The approach is evaluated on the NewsTT dataset, showing significant performance gains over baseline vision-language models.

## Key Results
- CFT-CLIP achieves an F1 score of 0.815 and Spearman correlation of 0.491 on the NewsTT dataset
- The method significantly outperforms baseline models including CLIP, BLIP, and BLIP-2
- CLIP's dual encoder architecture performs better than BLIP-2's cross-attention architecture for this task
- Domain-adaptive pretraining on news corpora further improves performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing named entities in news text with counterfactual entities creates hard negative samples that improve the model's ability to detect whether an image represents the actors in the text.
- Mechanism: The proposed CFT-CLIP method generates counterfactual text by replacing named entities (like people, organizations, or locations) with different entities using a masked language model. This counterfactual text is semantically similar to the original text in topic but no longer represents the same actors. When used as negative samples in contrastive learning, the model learns to better distinguish between images that represent the news actors and those that don't.
- Core assumption: News articles frequently feature named entities, and these entities are crucial for understanding whether an image represents the news actors.
- Evidence anchors:
  - [abstract] "Since news subjects frequently involve named entities or proper nouns, the pretrained models could have a limited capability to match news actors' visual and textual appearances."
  - [section] "Following a previous study (Choi et al., 2022), we conducted the pilot labeling task given an abstract definition of a representative thumbnail, which is 'an image that visually conveys the news event that can be identified from the accompanying text'."
  - [corpus] Weak - The corpus evidence for this mechanism is primarily in the ablation experiments (Table 3) showing that targeting person-labeled entity tokens achieved the best performance.

### Mechanism 2
- Claim: Domain-adaptive pretraining on news-specific data improves the performance of vision-language models for assessing news thumbnail representativeness.
- Mechanism: By continuing to pretrain a general vision-language model (like CLIP) on a news-specific corpus (either NELA or BBC news data), the model learns representations that are more aligned with the characteristics of news text and images. This domain adaptation helps the model better understand the relationship between news actors mentioned in text and their visual representations.
- Core assumption: News text and images have distinct characteristics compared to general web data, and models pretrained on general data may not capture these nuances.
- Evidence anchors:
  - [abstract] "We found that the pretrained vision and language models, such as BLIP-2, struggle with this task."
  - [section] "Second, CFT-CLIP outperformed all baseline methods with the f1 of 0.815 and the Spearman coefficient of 0.491 (p<0.001). The performance gap with CLIPAdapt, the second-best method, is significant."
  - [corpus] Weak - The corpus evidence for this mechanism is in Table 5, which shows that CFT-CLIP models with different pretraining corpora achieve different performance levels.

### Mechanism 3
- Claim: The dual encoder architecture of CLIP is more effective than the cross-attention architecture of BLIP-2 for the task of assessing news thumbnail representativeness.
- Mechanism: CLIP's dual encoder architecture processes images and text separately and then computes similarity, which may be more efficient for the specific task of matching news actors between text and images. In contrast, BLIP-2's cross-attention architecture, while powerful for many vision-language tasks, may introduce unnecessary complexity for this task.
- Core assumption: The task of assessing news thumbnail representativeness requires efficient matching between text and image representations rather than complex cross-modal interactions.
- Evidence anchors:
  - [abstract] "We found that CLIP performed better at the task than more recent methods, such as BLIP-2, suggesting the efficiencies of its dual encoder architecture and contrastive objective."
  - [section] "Given that BLIP-2 (1.17B) is 2.74 times larger than CLIP (427M), this observation suggests the effectiveness of CLIP's bi-encoder architecture for the target task."
  - [corpus] Weak - The corpus evidence for this mechanism is in the comparison between CLIP and BLIP-2 in Table 2.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The method relies on identifying named entities in news text to generate counterfactual text and to understand which actors should be represented in the image.
  - Quick check question: Can you identify all the named entities in this news headline: "President Biden met with Ukrainian President Zelenskyy in Washington"?

- Concept: Contrastive Learning
  - Why needed here: The proposed method uses contrastive learning to train the model to distinguish between positive pairs (image and text representing the same actors) and negative pairs (image and text representing different actors).
  - Quick check question: In contrastive learning, what is the goal when comparing positive and negative pairs?

- Concept: Masked Language Modeling
  - Why needed here: The method uses a masked language model to generate counterfactual text by predicting new tokens for masked named entities.
  - Quick check question: How does a masked language model like BERT predict tokens for masked positions in a sentence?

## Architecture Onboarding

- Component map:
  Input: News image and news text (title or summary) -> Named Entity Recognition: Identifies actors in the text -> Masked Language Model: Generates counterfactual text by replacing named entities -> CLIP-like Bi-Encoder: Processes image and text separately -> Contrastive Loss: Trains the model to distinguish between positive and negative pairs -> Output: Binary classification of whether the image represents the news actors

- Critical path:
  1. Extract named entities from news text
  2. Generate counterfactual text by replacing entities
  3. Encode image and both original/counterfactual texts
  4. Compute contrastive loss
  5. Update model parameters

- Design tradeoffs:
  - Using counterfactual text as hard negatives vs. random negatives
  - Masking 30% of tokens vs. fewer/more tokens
  - Targeting specific entity types (person, organization, GPE) vs. all entities
  - Using summary text vs. title as reference text

- Failure signatures:
  - Poor performance on entities not frequently covered in news (e.g., abstract concepts)
  - Failure when news actors are not explicitly named but implied
  - Poor generalization to news domains not represented in pretraining data

- First 3 experiments:
  1. Compare performance of different entity types (person, organization, GPE) for counterfactual generation
  2. Evaluate the impact of using different amounts of masked tokens (15% vs 30%)
  3. Test the model with different reference text types (title vs summary)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the scale of the pretraining corpus significantly impact the effectiveness of CFT-CLIP compared to domain-adaptive pretraining methods like CLIPAdapt?
- Basis in paper: [inferred] The paper mentions that CFT-CLIP outperformed CLIPAdapt, but also notes that CFT-CLIP's performance was lower than that of the pretrained CLIP, which was trained on a web-scaled dataset. The authors suggest that the performance degradation might be due to the scale of the pretraining corpus.
- Why unresolved: The paper does not provide a direct comparison between CFT-CLIP and CLIPAdapt when both are trained on the same scale of pretraining data.
- What evidence would resolve it: Conducting experiments where both CFT-CLIP and CLIPAdapt are trained on pretraining corpora of varying sizes and comparing their performance.

### Open Question 2
- Question: How does the quality of the generated counterfactual text impact the performance of CFT-CLIP, and can the quality be improved without sacrificing the effectiveness of the hard negative samples?
- Basis in paper: [explicit] The paper discusses the use of a masked language model to generate counterfactual text and mentions that the generation sometimes leads to imperfect generations, including broken grammar. However, it states that the generated sentence can still serve as a hard negative sample.
- Why unresolved: The paper does not explore alternative methods for generating counterfactual text or investigate the impact of text quality on model performance.
- What evidence would resolve it: Experimenting with different text generation models or refining the generation process to improve text quality, and then evaluating the impact on CFT-CLIP's performance.

### Open Question 3
- Question: Can the proposed CFT-CLIP method be extended to assess other aspects of news events beyond the "Who" (actors), such as "What" (actions) or "Where" (locations)?
- Basis in paper: [inferred] The paper focuses on assessing whether news thumbnail images represent the actors of news events, but mentions that future research could extend the focus to cover other aspects of 5Ws, such as "What".
- Why unresolved: The paper does not provide a concrete method or experiment for extending CFT-CLIP to other aspects of news events.
- What evidence would resolve it: Developing and testing modifications to the CFT-CLIP framework to handle different aspects of news events, such as actions or locations, and evaluating the performance on a dataset labeled accordingly.

## Limitations
- The NewsTT dataset is relatively small (1000 samples), which may limit generalizability and increase overfitting risk
- The method relies heavily on named entity recognition and replacement, which may not capture all aspects of news actor representation
- The study primarily evaluates on English news data, with effectiveness for other languages or cultural contexts untested
- The analysis of why CLIP outperforms BLIP-2 is largely speculative without rigorous ablation studies

## Confidence
- High confidence: The claim that assessing news thumbnail representativeness is a challenging task for general-purpose vision-language models
- Medium confidence: The claim that counterfactual text generation significantly improves model performance
- Medium confidence: The claim that CLIP's dual encoder architecture is more effective than BLIP-2's cross-attention architecture for this task

## Next Checks
1. **Dataset Size Validation**: Expand the NewsTT dataset to 5000-10000 samples across diverse news domains and evaluate whether CFT-CLIP maintains its performance advantage, particularly testing for overfitting and generalization issues.

2. **Counterfactual Generation Ablation**: Systematically vary the percentage of masked tokens (15%, 30%, 45%) and entity types (person-only, organization-only, mixed) to determine optimal settings for counterfactual text generation and understand which aspects contribute most to performance gains.

3. **Cross-linguistic and Cross-cultural Validation**: Evaluate CFT-CLIP on news datasets from different languages and cultural contexts to assess whether the method's effectiveness generalizes beyond English-language news, particularly testing how well named entity replacement works across linguistic structures.