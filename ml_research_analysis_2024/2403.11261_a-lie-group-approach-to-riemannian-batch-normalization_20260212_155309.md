---
ver: rpa2
title: A Lie Group Approach to Riemannian Batch Normalization
arxiv_id: '2403.11261'
source_url: https://arxiv.org/abs/2403.11261
tags:
- liebn
- https
- riemannian
- mean
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified Lie group batch normalization (LieBN)
  framework for manifold-valued data, particularly focusing on Symmetric Positive
  Definite (SPD) manifolds. The method provides theoretical guarantees for controlling
  both the Riemannian mean and variance, addressing limitations of previous ad hoc
  approaches.
---

# A Lie Group Approach to Riemannian Batch Normalization

## Quick Facts
- arXiv ID: 2403.11261
- Source URL: https://arxiv.org/abs/2403.11261
- Reference count: 40
- Key outcome: Unified Lie group batch normalization (LieBN) framework for manifold-valued data with theoretical guarantees for controlling Riemannian mean and variance, demonstrated effectiveness on SPD manifolds across three datasets

## Executive Summary
This paper introduces a unified Lie group batch normalization (LieBN) framework for normalizing manifold-valued data, particularly focusing on Symmetric Positive Definite (SPD) manifolds. The method addresses limitations of previous ad hoc approaches by providing theoretical guarantees for controlling both the Riemannian mean and variance. By leveraging the concept of deformation, the authors generalize existing Lie group structures on SPD manifolds into parameterized families and construct specific normalization layers. Experiments on radar recognition, human action recognition, and EEG classification demonstrate the effectiveness of LieBN, showing improvements over existing methods in terms of accuracy and efficiency.

## Method Summary
LieBN extends Euclidean batch normalization to Lie groups by replacing additive operations with group operations and using tangent space scaling. The framework operates on SPD manifolds using three Lie group structures (AIM, LEM, LCM) and introduces a deformation parameter θ to interpolate between metrics. Centering uses left translation by the inverse of the Riemannian mean, scaling applies exponential map in the tangent space at the neutral element, and biasing applies left translation by a learnable parameter. For SPD manifolds with pullback metrics, computations can be reduced to standard Euclidean BN in the codomain via diffeomorphism mapping, enabling efficient implementation.

## Key Results
- LieBN achieves 1.4% accuracy improvement over SPDNetBN on Radar dataset (98.3% vs 96.9%)
- LCM-(0.5) LieBN improves HDM05 recognition accuracy from 64.5% to 65.1%
- LieBN implementations show comparable or better efficiency than SPDNetBN, with LEM and LCM-based variants being more efficient
- Optimal LieBN layer varies by dataset: AIM-(1) for Radar, LCM-(0.5) for HDM05, AIM-(1.5) for FPHA

## Why This Works (Mechanism)

### Mechanism 1
- LieBN extends Euclidean batch normalization to Lie groups by replacing additive operations with group operations and using tangent space scaling.
- Centering uses left translation by the inverse of the Riemannian mean, scaling applies exponential map in the tangent space at the neutral element, and biasing applies left translation by a learnable parameter.
- Core assumption: The Riemannian mean exists and is unique under the left-invariant metric, and tangent space operations behave linearly enough for normalization.
- Break condition: If the Riemannian mean is not unique or if left translations do not preserve the metric structure, normalization guarantees fail.

### Mechanism 2
- Deformation of existing Lie group structures on SPD manifolds yields parameterized families that interpolate between metrics, allowing optimization of geometry for a task.
- The power function Pθ deforms the group operation and metric, creating (θ,α,β)-AIM and θ-LCM variants that interpolate between existing metrics.
- Core assumption: Pullback metrics by diffeomorphisms preserve Lie group structure and left-invariance.
- Break condition: If the deformation breaks smoothness or injectivity of the diffeomorphism, the resulting structure may not be a valid Lie group.

### Mechanism 3
- For SPD manifolds with pullback metrics, LieBN computations can be reduced to standard Euclidean BN in the codomain, enabling efficient implementation.
- The diffeomorphism f maps SPD matrices to a Euclidean space (symmetric matrices or lower triangular matrices), allowing LieBN to be computed there using Euclidean BN.
- Core assumption: The pullback metric via f induces the same geometric operations as the original manifold, and f is computationally tractable.
- Break condition: If f or f⁻¹ are numerically unstable or expensive, the efficiency advantage disappears.

## Foundational Learning

- **Riemannian geometry basics**: Manifolds, metrics, geodesics, exponential/logarithm maps
  - Why needed: LieBN operates on manifolds where distances and means are defined via Riemannian structure; understanding these concepts is essential to follow the normalization framework.
  - Quick check: What is the Riemannian logarithm map, and how does it relate to tangent vectors on a manifold?

- **Lie groups and left-invariant metrics**: Group operations, tangent spaces, exponential maps
  - Why needed: LieBN is defined for Lie groups; left-invariance ensures that group translations preserve the metric, which is crucial for the normalization operations.
  - Quick check: Why does left-invariance of the metric matter for the LieBN centering and biasing steps?

- **SPD manifolds and their Lie group structures**: Positive definiteness, three Lie group structures (AIM, LEM, LCM), associated metrics
  - Why needed: The empirical focus is on SPD manifolds, which admit three Lie group structures; knowing their metrics and group operations is necessary to implement LieBN layers.
  - Quick check: Name the three Lie group structures on SPD manifolds and their associated metrics.

## Architecture Onboarding

- **Component map**: Input SPD matrices → Riemannian mean/variance computation → Centering (left translation) → Scaling (exponential of scaled logarithm) → Biasing (left translation by parameter) → Normalized output → Next network layer

- **Critical path**: 
  1. Compute Riemannian mean and variance of input batch
  2. Update running statistics (training) or use stored (inference)
  3. Apply centering, scaling, biasing in sequence
  4. Pass normalized output to next layer

- **Design tradeoffs**:
  - Metric choice: AIM offers affine invariance (good for covariance), LEM is computationally simple, LCM is efficient but less invariant
  - Deformation factor θ: Interpolates between metrics but adds a hyperparameter; must be tuned per dataset
  - Implementation: Pullback to Euclidean space simplifies computation but requires stable diffeomorphism

- **Failure signatures**:
  - Numerical instability in matrix logarithms/exponentials (e.g., near singularities)
  - Poor convergence if Riemannian mean is not well-defined or unique
  - Degraded performance if deformation factor θ is poorly chosen

- **First 3 experiments**:
  1. Apply LieBN-AIM-(1) to SPDNet on Radar dataset; compare accuracy and training time to baseline SPDNetBN
  2. Vary θ for LieBN-LCM on HDM05; observe effect on accuracy and identify optimal θ
  3. Implement LieBN on SO(3) rotation matrices using LieNet; compare G3D dataset performance with and without LieBN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of Riemannian metric (AIM, LEM, LCM) affect the performance of LieBN across different datasets?
- Basis: The paper states that the optimal LieBN layer varies by dataset, with AIM-(1) performing best on Radar, LCM-(0.5) on HDM05, and AIM-(1.5) on FPHA.
- Why unresolved: The paper does not provide a theoretical explanation for why certain metrics perform better on specific datasets. The choice seems to be empirical.
- What evidence would resolve it: A detailed analysis comparing the geometric properties of each metric with the characteristics of each dataset could explain the performance differences.

### Open Question 2
- Question: What is the impact of the deformation factor θ on the generalization ability of LieBN?
- Basis: The paper mentions that an appropriate deformation factor θ can enhance LieBN performance, citing examples like LCM-(0.5) improving HDM05 results.
- Why unresolved: The paper only explores a limited range of θ values and does not systematically study how different deformations affect generalization across various tasks.
- What evidence would resolve it: A comprehensive study varying θ across a wider range and evaluating performance on multiple datasets and tasks would clarify its impact on generalization.

### Open Question 3
- Question: How does LieBN compare to other normalization techniques in terms of computational efficiency and memory usage?
- Basis: The paper mentions that LieBN achieves comparable or better efficiency than SPDNetBN, with LEM and LCM-based LieBN being more efficient.
- Why unresolved: The paper does not provide a detailed comparison of computational complexity or memory requirements between LieBN and other normalization methods.
- What evidence would resolve it: A thorough benchmarking study comparing LieBN to other normalization techniques in terms of FLOPs, memory footprint, and training/inference time would provide a clearer picture of its efficiency.

### Open Question 4
- Question: Can LieBN be effectively applied to other Lie groups beyond SPD manifolds and rotation matrices?
- Basis: The paper mentions that LieBN is a general framework applicable to all Lie groups but only demonstrates it on SPD manifolds and rotation matrices.
- Why unresolved: The paper does not explore the application of LieBN to other Lie groups commonly found in machine learning.
- What evidence would resolve it: Implementing and evaluating LieBN on other Lie groups with relevant datasets would demonstrate its broader applicability.

### Open Question 5
- Question: How does the choice of (α, β) parameters in the O(n)-invariant inner product affect the performance of LieBN?
- Basis: The paper mentions that (α, β) only affects variance calculation and sets them to (1, 0) by default, but does not explore their impact in detail.
- Why unresolved: The paper does not provide a systematic study of how different (α, β) values influence the performance of LieBN.
- What evidence would resolve it: A comprehensive ablation study varying (α, β) across a wide range and evaluating performance on multiple datasets and tasks would clarify their impact on LieBN's effectiveness.

## Limitations
- Deformation framework introduces a new hyperparameter θ that must be tuned for each dataset with no clear theoretical guidance
- Evaluation scope limited to specific SPD manifold applications (radar, human action, EEG), leaving uncertainty about generalizability
- Computational complexity of matrix logarithms and exponentials remains a concern for high-dimensional SPD matrices
- Theoretical guarantees assume unique and well-defined means, which may not hold for all SPD distributions

## Confidence
- **High Confidence**: The core LieBN framework and its implementation on SPD manifolds using existing Lie group structures
- **Medium Confidence**: The deformation concept's ability to interpolate between metrics and the claim of improved task-specific optimization
- **Medium Confidence**: The empirical performance improvements shown on the three benchmark datasets
- **Low Confidence**: The generalizability of findings to other manifold types beyond SPD and rotation matrices

## Next Checks
1. Systematically test the sensitivity of LieBN performance to the deformation parameter θ across multiple datasets to establish guidelines for optimal selection
2. Evaluate LieBN on high-dimensional SPD matrices (e.g., 100×100 or larger) to verify the claimed efficiency benefits and identify computational bottlenecks
3. Apply LieBN to non-SPD manifold-valued data such as shape spaces, Grassmann manifolds, or tensor manifolds to assess broader applicability