---
ver: rpa2
title: 'JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation
  with LLM'
arxiv_id: '2412.06738'
source_url: https://arxiv.org/abs/2412.06738
tags:
- data
- japagen
- tasks
- task
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  effectively generate synthetic training data for Japanese NLP tasks, a language
  distinct from English and classified as mid-resource. The proposed method, JAPAGEN,
  applies the SUPER GEN approach to six Japanese downstream tasks, including text
  classification and natural language inference, under few-shot and zero-shot learning
  scenarios.
---

# JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM

## Quick Facts
- arXiv ID: 2412.06738
- Source URL: https://arxiv.org/abs/2412.06738
- Reference count: 11
- This paper investigates whether LLMs can generate effective synthetic training data for Japanese NLP tasks, achieving competitive performance compared to traditional few-shot and zero-shot approaches.

## Executive Summary
This paper introduces JAPAGEN, a method for generating synthetic training data for Japanese NLP tasks using large language models. The approach adapts the SUPER GEN methodology to six Japanese downstream tasks, including text classification and natural language inference, under few-shot and zero-shot learning scenarios. Experimental results demonstrate that JAPAGEN achieves strong performance, particularly in classification tasks with formal text inputs, outperforming zero-shot prompting in several cases. The study also introduces Knowledge-Assisted Data Generation (KADG) to improve synthetic data quality by incorporating task-specific knowledge into prompts, which enhances alignment with gold-standard distributions.

## Method Summary
JAPAGEN generates synthetic training data for Japanese NLP tasks using OpenAI's GPT-3.5-turbo-06133. The method employs class-conditional prompts to generate 25,000 samples per class for each task. The synthetic data is then used to fine-tune Japanese BERT (tohoku-nlp/bert-base-japanese-v3) with specified hyperparameters (batch size 32, epochs 4, AdamW optimizer with lr=5e-5). The approach is evaluated across six Japanese downstream tasks from JGLUE and additional datasets, comparing performance against zero-shot/few-shot prompting, few-shot fine-tuning on gold data, and fully supervised fine-tuning. KADG enhances the process by injecting task-specific words into prompts to improve distribution alignment.

## Key Results
- JAPAGEN achieves performance improvements of 3.94%, 4.96%, and 17.10% over zero-shot PROMPTING on JSTS, JNLI, and News classification respectively
- KADG demonstrates higher Jaccard index compared to zero-shot JAPAGEN for MARC-ja, JSTS, JNLI, and News tasks
- Performance improves with larger synthetic datasets but plateaus, suggesting an optimal dataset size exists for each task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JAPAGEN outperforms few-shot BERT fine-tuning and zero-shot prompting in classification tasks with formal text input.
- Mechanism: The synthetic data generated by LLMs captures task-relevant patterns effectively, allowing smaller models (BERT) to learn robust representations even with less annotated data.
- Core assumption: LLMs trained primarily on English can generate high-quality Japanese synthetic data that aligns with task distributions.
- Evidence anchors:
  - [abstract] "JAPAGEN achieves robust performance in classification tasks that necessitate formal text inputs, demonstrating competitive results compared to conventional LLM prompting strategies."
  - [section 4.2] "JAPAGEN achieves performance improvements of 3.94%, 4.96%, and 17.10% over zero-shot PROMPTING on JSTS, JNLI, and News, respectively."
  - [corpus] Weak evidence - no direct corpus data provided for this specific mechanism
- Break condition: If the LLM's knowledge of Japanese is insufficient, generated data quality degrades, harming downstream model performance.

### Mechanism 2
- Claim: Knowledge-Assisted Data Generation (KADG) improves alignment between synthetic and gold-standard data distributions.
- Mechanism: Injecting task-specific words into prompts guides the LLM to generate more diverse yet label-correct texts that better match the target distribution.
- Core assumption: Task-specific vocabulary provides meaningful semantic guidance that improves synthetic data quality without compromising label accuracy.
- Evidence anchors:
  - [abstract] "KADG enhances the fidelity of generated texts to gold-standard distributions while maintaining label accuracy"
  - [section 4.3] "KADG achieves a higher Jaccard index compared to zero-shot JAPAGEN for MARC-ja, JSTS, JNLI, and News"
  - [corpus] Weak evidence - no direct corpus data provided for KADG effectiveness
- Break condition: If task-specific words are poorly chosen or too restrictive, they may limit diversity or introduce bias.

### Mechanism 3
- Claim: Larger synthetic datasets improve performance until reaching saturation.
- Mechanism: More synthetic examples provide broader coverage of the input space, allowing the smaller model to learn more robust patterns.
- Core assumption: Synthetic data quality remains consistent across dataset sizes, and additional examples provide genuine new information.
- Evidence anchors:
  - [abstract] "Our analysis shows that KADG enhances the fidelity of generated texts to gold-standard distributions while maintaining label accuracy, although it does not consistently improve overall task performance."
  - [section 4.3] "Figure 3 demonstrates that for most tasks, performance improves as the data size increases. However, performance tends to plateau"
  - [corpus] Weak evidence - no direct corpus data provided for scaling analysis
- Break condition: If synthetic data becomes repetitive at larger scales, additional data provides diminishing returns.

## Foundational Learning

- Concept: Few-shot learning paradigm
  - Why needed here: The paper compares JAPAGEN against few-shot fine-tuning and prompting baselines, requiring understanding of how models learn from limited examples
  - Quick check question: What distinguishes few-shot learning from zero-shot learning in the context of language models?

- Concept: Synthetic data generation for model training
  - Why needed here: JAPAGEN's core approach involves using LLM-generated data to train smaller models, requiring understanding of data synthesis principles
  - Quick check question: How does synthetic data generation differ from data augmentation in NLP?

- Concept: Prompt engineering and its impact on generation quality
  - Why needed here: KADG specifically modifies prompts with task-specific knowledge, requiring understanding of how prompt design affects output
  - Quick check question: What are the key components of an effective prompt for text generation tasks?

## Architecture Onboarding

- Component map:
  LLM (GPT-3.5-turbo-06133) → Synthetic data generator
  Prompt templates → Control generation for specific tasks
  BERT fine-tuning pipeline → Model training on synthetic data
  Evaluation framework → Performance comparison against baselines
  KADG module → Enhanced prompt engineering

- Critical path:
  1. Define task and label space
  2. Create label-descriptive prompts
  3. Generate synthetic data using LLM
  4. Fine-tune BERT on synthetic data
  5. Evaluate on gold test set
  6. Analyze distribution alignment and diversity

- Design tradeoffs:
  - Synthetic data quantity vs. quality: More data may improve coverage but could introduce noise
  - Prompt specificity vs. diversity: Task-specific prompts may align better with gold data but reduce generation variety
  - Model size vs. performance: Larger LLMs may generate better data but increase costs

- Failure signatures:
  - Low label correctness in synthetic data → BERT learns incorrect patterns
  - High Self-BLEU scores → Synthetic data lacks diversity, limiting model generalization
  - Poor distribution alignment → Model performs well on synthetic data but poorly on real test data

- First 3 experiments:
  1. Baseline comparison: Run few-shot BERT fine-tuning vs. zero-shot JAPAGEN on a simple classification task to verify the core claim
  2. KADG effectiveness: Compare synthetic data quality (Self-BLEU, Jaccard index) with and without KADG on one task
  3. Data scaling: Measure performance improvements as synthetic dataset size increases from 1,000 to 50,000 examples per class on one task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of JAPAGEN vary across different Japanese language domains and text styles?
- Basis in paper: [explicit] The paper evaluates JAPAGEN on six Japanese tasks including product reviews (MARC-ja), news topics (News), and COVID-19 tweets (COVID-19), noting different performance patterns.
- Why unresolved: The paper provides comparative results but doesn't deeply analyze why certain domains perform better than others or what linguistic characteristics contribute to these differences.
- What evidence would resolve it: Detailed linguistic analysis of synthetic data across domains, correlation between domain-specific linguistic features and model performance, and systematic variation of domain-specific prompts to measure impact.

### Open Question 2
- Question: What is the optimal balance between dataset diversity and label accuracy when using KADG, and how can this trade-off be quantified?
- Basis in paper: [explicit] The paper mentions that KADG enhances dataset diversity while maintaining label correctness, but notes this doesn't consistently improve overall task performance.
- Why unresolved: The paper demonstrates the trade-off exists but doesn't provide a framework for optimizing this balance or quantify the relationship between diversity metrics and task performance.
- What evidence would resolve it: Mathematical modeling of the diversity-accuracy trade-off, systematic experiments varying diversity parameters while measuring performance, and development of an optimization framework for prompt engineering.

### Open Question 3
- Question: How do different types of Japanese linguistic features (e.g., lack of word boundaries, honorifics) impact the effectiveness of synthetic data generation compared to English?
- Basis in paper: [explicit] The paper explicitly notes that Japanese has different characteristics from English, such as the absence of spaces between words.
- Why unresolved: While the paper acknowledges these differences, it doesn't specifically analyze how these linguistic features affect the synthetic data generation process or model performance.
- What evidence would resolve it: Comparative analysis of synthetic data quality metrics between Japanese and English for equivalent tasks, ablation studies removing specific linguistic features, and investigation of tokenization strategies' impact on generation quality.

## Limitations
- Focus on Japanese language tasks only, limiting generalizability to other languages
- Relatively small gold test sets for some tasks, potentially affecting evaluation reliability
- Use of only one LLM (GPT-3.5-turbo-06133) for data generation, without comparison across different models

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| JAPAGEN's competitive performance against zero-shot prompting and few-shot fine-tuning baselines | High |
| KADG's effectiveness in improving distribution alignment | Medium |
| Scaling analysis showing performance improvement with larger datasets | Medium |

## Next Checks
1. Conduct a thorough ablation study varying synthetic dataset sizes beyond the current range to better identify optimal scaling points
2. Test JAPAGEN across multiple LLM providers and model versions to assess robustness to different generation engines
3. Perform human evaluation of synthetic data quality focusing on naturalness, diversity, and label correctness to complement automatic metrics