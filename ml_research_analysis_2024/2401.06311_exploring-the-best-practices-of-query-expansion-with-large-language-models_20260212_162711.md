---
ver: rpa2
title: Exploring the Best Practices of Query Expansion with Large Language Models
arxiv_id: '2401.06311'
source_url: https://arxiv.org/abs/2401.06311
tags:
- query
- retrieval
- mugi
- dense
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores best practices for using large language models
  (LLMs) for query expansion in information retrieval. The authors introduce MuGI,
  a framework that leverages LLMs to generate multiple pseudo-references, which are
  then integrated with queries to enhance both sparse and dense retrievers.
---

# Exploring the Best Practices of Query Expansion with Large Language Models

## Quick Facts
- arXiv ID: 2401.06311
- Source URL: https://arxiv.org/abs/2401.06311
- Reference count: 12
- Primary result: MuGI framework improves both sparse and dense retrievers by generating multiple pseudo-references with adaptive integration strategies

## Executive Summary
This paper introduces MuGI, a framework that leverages large language models to generate multiple pseudo-references for query expansion in information retrieval. The method employs adaptive reweighting for BM25 and contextualized pooling for dense retrievers, along with a calibration module inspired by the Rocchio algorithm. Experiments demonstrate that increasing pseudo-reference samples improves performance across model sizes ranging from 23M to 7B parameters, with ChatGPT-4 boosting BM25 by 19.8% on TREC DL and 7.5% on BEIR.

## Method Summary
MuGI is a training-free framework that generates multiple pseudo-references per query using LLMs, then integrates them with the original query through adaptive strategies. For BM25, it uses adaptive reweighting that balances query and pseudo-references based on length ratios. For dense retrievers, it employs contextualized pooling to average embeddings from concatenated query-reference pairs, avoiding truncation issues. A calibration module optionally refines results using pseudo-relevance feedback. The pipeline retrieves top documents using BM25, generates pseudo-references, and reranks using dense retrievers with MuGI integration.

## Key Results
- Increasing pseudo-reference samples from LLMs consistently improves retrieval performance
- MuGI enhances both sparse (BM25) and dense retrievers across model sizes from 23M to 7B parameters
- Using ChatGPT-4, MuGI achieves 19.8% improvement on TREC DL and 7.5% on BEIR for BM25
- Dense retrievers show over 7% improvement on TREC DL and 4% on BEIR

## Why This Works (Mechanism)

### Mechanism 1
Generating multiple pseudo-references per query improves retrieval by enriching query context with additional relevant terms and background knowledge. LLMs synthesize passages that expand vocabulary and semantic scope beyond the original query. This works when pseudo-references are high-quality and relevant, but fails if they introduce noise or irrelevance.

### Mechanism 2
Adaptive reweighting balances query and pseudo-references for lexical retrievers by adjusting repetition factors based on length ratios. This calibration ensures appropriate term frequency in BM25 scoring, addressing the ineffectiveness of constant repetition when query and reference lengths vary significantly.

### Mechanism 3
Contextualized pooling improves dense retriever integration by averaging embeddings from concatenated query-reference pairs rather than simple concatenation. This preserves full semantic context without truncation, capturing more contextual information than concatenation, though it may dilute signals if embeddings become too dissimilar.

## Foundational Learning

- Concept: Pseudo-relevance feedback
  - Why needed here: MuGI uses pseudo-references as implicit feedback to refine queries, similar to traditional PRF but without retrieval results
  - Quick check question: How does PRF traditionally improve retrieval, and what's the difference when using LLM-generated pseudo-references?

- Concept: BM25 term frequency normalization
  - Why needed here: MuGI's adaptive reweighting directly manipulates BM25's term frequency calculations through query repetition
  - Quick check question: What happens to BM25 scores when query terms are repeated multiple times?

- Concept: Dense retrieval embedding space
  - Why needed here: MuGI's contextualized pooling operates in the dense embedding space, requiring understanding of how semantic similarity is computed
  - Quick check question: How does cosine similarity between query and document embeddings determine ranking in dense retrievers?

## Architecture Onboarding

- Component map: Query → LLM (generate pseudo-references) → Integration module (BM25: adaptive reweighting; Dense: contextualized pooling) → Retrieval engine (BM25 or dense retriever) → Calibration module (optional Rocchio adaptation)
- Critical path: LLM generation → Integration → Retrieval → (Optional calibration)
- Design tradeoffs: Multiple pseudo-references improve context but increase LLM calls and computational cost; concatenation is simpler but causes truncation; calibration improves performance but adds complexity
- Failure signatures: Performance degradation when pseudo-references become irrelevant; BM25 performance drops with improper reweighting; dense retriever performance plateaus or declines with excessive references
- First 3 experiments:
  1. Compare retrieval performance with 1 vs 5 pseudo-references using the same query
  2. Test BM25 with constant repetition vs adaptive reweighting on a small dataset
  3. Evaluate dense retriever performance with concatenation vs contextualized pooling on the same queries

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of pseudo-references to generate for different retrieval tasks and query types? The paper identifies that performance plateaus at five references but doesn't explore whether this varies by query length, complexity, domain, or retrieval task type.

### Open Question 2
How does the quality of pseudo-references generated by LLMs impact retrieval performance compared to using human-generated or curated reference passages? The paper only compares different LLMs but doesn't benchmark against human-written references or curated corpora.

### Open Question 3
Can the MuGI framework be effectively adapted for specialized retrieval tasks like cross-lingual retrieval or multimodal retrieval? The experiments are limited to English text retrieval, with no exploration of cross-lingual or multimodal scenarios.

## Limitations
- Unknown generalizability to specialized domains (medical, legal) or languages beyond English
- Resource intensity from multiple LLM API calls limits practical deployment at scale
- Performance may degrade when pseudo-references become irrelevant or noisy

## Confidence
- High confidence: Core mechanism of using multiple pseudo-references to improve retrieval performance
- Medium confidence: Adaptive reweighting strategy effectiveness (mechanism is sound but specific hyperparameters may not generalize)
- Medium confidence: Contextualized pooling approach (alternative to concatenation is validated but theoretical justification could be stronger)

## Next Checks
1. Ablation on reference quantity: Systematically vary the number of pseudo-references (1, 3, 5, 10) to identify optimal balance between performance gains and computational cost
2. Cross-domain robustness: Test MuGI on specialized datasets (e.g., BioASQ, Patent retrieval) to assess performance outside evaluated domains
3. Zero-shot transfer: Evaluate MuGI with out-of-distribution queries (different styles, lengths, or domains) to measure robustness to query variability