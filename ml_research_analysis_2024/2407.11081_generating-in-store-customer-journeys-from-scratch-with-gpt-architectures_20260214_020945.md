---
ver: rpa2
title: Generating In-store Customer Journeys from Scratch with GPT Architectures
arxiv_id: '2407.11081'
source_url: https://arxiv.org/abs/2407.11081
tags:
- data
- store
- customer
- purchase
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for simultaneously generating in-store
  customer trajectories and purchasing behaviors using a Transformer-based GPT-2 architecture.
  The authors encoded indoor location information into a six-character hierarchical
  grid system and modeled purchase behaviors by clustering trajectory pauses near
  product shelves.
---

# Generating In-store Customer Journeys from Scratch with GPT Architectures

## Quick Facts
- arXiv ID: 2407.11081
- Source URL: https://arxiv.org/abs/2407.11081
- Reference count: 26
- Primary result: GPT-2 model generates in-store trajectories and purchase behaviors with JS divergence of 0.00973 (trajectory) and 0.0100 (purchase) vs LSTM/SVM baselines

## Executive Summary
This paper presents a method for generating realistic in-store customer journeys using a Transformer-based GPT-2 architecture. The approach encodes indoor locations as six-character hierarchical grid strings and models purchase behaviors through DBSCAN clustering of trajectory pauses. The model is trained from scratch on one store's data and fine-tuned on another store, demonstrating transferability while requiring up to 100-fold less data. Results show superior performance compared to LSTM and SVM baselines in reproducing both trajectories and purchase distributions.

## Method Summary
The method encodes 50cm grid locations as six-character hierarchical strings, applies DBSCAN clustering to identify purchase pauses, and maps clusters to store zones using layout diagrams and scanner data. A GPT-2 small model (12 layers, 100M parameters) is trained from scratch using Byte-level BPE tokenization with a 50k vocabulary. The model generates trajectories from initial 7 points and can be fine-tuned on new store data with significantly reduced training requirements. Evaluation uses Jensen-Shannon divergence to compare generated vs actual purchase distributions across store zones.

## Key Results
- GPT-2 model achieves JS divergence of 0.00973 for trajectory generation and 0.0100 for purchase prediction
- Fine-tuning with as little as 1/100th of original data retains model accuracy
- Outperforms LSTM and SVM baselines in reproducing in-store trajectories and purchase distributions
- Successfully transfers from one store to another while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
GPT-2 autoregressive generation captures longer-term spatial dependencies than LSTM or SVM by learning which conditional probabilities matter most through attention mechanisms. This enables the model to remember and reproduce complex path patterns and purchase timing over extended journeys. The approach assumes indoor trajectories have sufficient regularity that past locations inform future movements statistically.

### Mechanism 2
The six-character hierarchical grid encoding enables generalization of spatial patterns without explicit coordinates. The hierarchical token system allows learning location-to-location transition probabilities at multiple scales, capturing both macro store layout and micro movement patterns. This assumes spatial continuity and store layout structure are consistent enough for discrete grid tokens to approximate real-world trajectories meaningfully.

### Mechanism 3
Fine-tuning with 1/100th of original data retains accuracy by leveraging pre-learned general patterns. The pre-trained model captures generic customer journey dynamics, and fine-tuning adapts these to store-specific patterns without relearning the full distribution. This assumes customer journey patterns share structural similarities across stores, allowing effective transfer learning.

## Foundational Learning

- **Byte-level Byte Pair Encoding (BPE) tokenization**: Converts hierarchical location strings into discrete tokens while preserving frequent spatial patterns. *Quick check*: What happens if we use character-level tokenization instead of BPE for the six-character location codes?

- **DBSCAN clustering for purchase behavior modeling**: Identifies pauses in trajectories that correspond to product examination, enabling accurate mapping of purchases to store zones. *Quick check*: How would purchase prediction change if we used different clustering radius or method?

- **Jensen-Shannon divergence for model evaluation**: Quantifies similarity between generated and actual purchase distributions across store zones, providing a single metric for model accuracy. *Quick check*: Why might JS divergence be preferred over KL divergence for comparing these distributions?

## Architecture Onboarding

- **Component map**: Data ingestion (trajectory + layout + scanner) → Preprocessing (DBSCAN + zone mapping + encoding) → Model (GPT-2 small) → Training (Adam, 10 epochs) → Fine-tuning (reduced data, 3 epochs) → Evaluation (JS divergence + heatmap)

- **Critical path**: 1. Encode location as six-character strings 2. Apply DBSCAN clustering to find purchase pauses 3. Map clusters to zones using layout and scanner data 4. Tokenize with BPE (50k vocab) 5. Train GPT-2 from scratch on Store A 6. Generate trajectories using initial 7 points as prompt 7. Fine-tune on Store B data with varying amounts 8. Evaluate using JS divergence and visual comparison

- **Design tradeoffs**: Grid resolution (50cm) vs computational efficiency and token vocabulary size; hierarchical levels (6) vs spatial granularity and model complexity; vocabulary size (50k) vs overfitting risk and training speed; pre-training epochs (10) vs fine-tuning data efficiency

- **Failure signatures**: Trajectories crossing shelves or leaving store boundaries (LSTM issue); JS divergence close to 0 indicating poor distribution match; inconsistent purchase zone mapping between actual and generated data; fine-tuning collapse with very small datasets (< 10 samples)

- **First 3 experiments**: 1. Train baseline GPT-2 on synthetic trajectories with known patterns to verify encoding and generation work 2. Compare DBSCAN clustering parameters by visualizing cluster-to-purchase alignment 3. Test fine-tuning with increasing data sizes (8, 64, 512, 2048 samples) to find minimum viable dataset

## Open Questions the Paper Calls Out
The paper explicitly identifies incorporating customer attributes (gender, age, shopping purpose) as special tokens during training and generation as a future direction. The authors also suggest exploring how the model's performance varies across different store layouts and sizes, though they don't analyze this impact in their current work.

## Limitations
- Proprietary dataset prevents independent validation of 100-fold data reduction claim and specific JS divergence values
- Hierarchical six-character encoding lacks corpus validation for indoor trajectory modeling
- DBSCAN clustering assumes all meaningful purchase pauses fall within 2.5m radius, potentially missing browsing behavior
- Comparison with LSTM/SVM baselines lacks specification of exact model architectures

## Confidence
- **High Confidence**: Fundamental approach of using GPT-2 for sequence generation and Transformer advantages for long-term dependencies; hierarchical grid encoding concept
- **Medium Confidence**: Specific 100-fold data reduction claim and exact JS divergence values (dependent on dataset characteristics)
- **Low Confidence**: Generalizability across different store types, customer demographics, and robustness to significant layout changes

## Next Checks
1. **Encoding Robustness Test**: Generate synthetic trajectory data with known patterns and test whether six-character hierarchical encoding accurately reconstructs original spatial relationships across different grid resolutions

2. **Fine-tuning Data Efficiency Experiment**: Systematically test minimum viable dataset size by training multiple models with varying amounts (8 to 512 samples) and measuring JS divergence convergence rates to validate 100-fold reduction claim

3. **Cross-Store Generalization Validation**: Apply fine-tuned model to a third, previously unseen store with different layout characteristics and customer demographics to test learned pattern transfer effectiveness