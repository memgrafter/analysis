---
ver: rpa2
title: Statistical Multicriteria Benchmarking via the GSD-Front
arxiv_id: '2406.03924'
source_url: https://arxiv.org/abs/2406.03924
tags:
- test
- classifiers
- data
- gsd-front
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of comparing classifiers under
  multiple quality metrics, considering statistical uncertainty and robustness. It
  introduces the GSD-front, a method for comparing classifiers using generalized stochastic
  dominance ordering, as an information-efficient alternative to the classical Pareto-front.
---

# Statistical Multicriteria Benchmarking via the GSD-Front

## Quick Facts
- arXiv ID: 2406.03924
- Source URL: https://arxiv.org/abs/2406.03924
- Reference count: 40
- Primary result: Introduces GSD-front method for comparing classifiers under multiple quality metrics with statistical uncertainty and robustness

## Executive Summary
This paper addresses the challenge of comparing classifiers under multiple quality metrics while accounting for statistical uncertainty and robustness. The authors introduce the GSD-front, a method based on generalized stochastic dominance ordering that is more informative than the classical Pareto-front. They propose a consistent statistical estimator for the GSD-front and construct a statistical test to determine if a classifier lies in the GSD-front of state-of-the-art classifiers. The framework is demonstrated on benchmark suites PMLB and OpenML, showing its effectiveness in handling multiple quality metrics while ensuring robustness under small deviations in assumptions.

## Method Summary
The paper presents a framework for statistical multicriteria benchmarking of classifiers using Generalized Stochastic Dominance (GSD). The method involves constructing a GSD-front from classifier performance data across multiple datasets, then applying permutation tests with regularization to assess whether new classifiers belong to this front. The approach handles both ordinal and cardinal quality metrics through a preference system encoding. Robustness is incorporated through contamination analysis that evaluates test conclusions under potential data corruption. The framework was applied to OpenML (80 datasets) and PMLB (62 datasets) using seven classifier algorithms (SVM, RF, CART, LR, GLMNet, xGBoost, kNN) with accuracy, training time, and robustness metrics.

## Key Results
- GSD-front is more informative than Pareto-front as it accounts for cardinal scale of some quality metrics
- Consistent statistical estimator proposed for GSD-front membership
- Statistical test for GSD-front membership validated under i.i.d. sampling assumptions
- Robustness checks developed to assess test conclusions under contamination models

## Why This Works (Mechanism)

### Mechanism 1
The GSD-front is more informative than the Pareto-front because it accounts for the cardinal scale of some quality metrics. GSD compares classifiers based on expected utility with respect to all utility functions representing the preference system, ranking classifiers not just by whether they are better on every dimension, but by how much better they are on cardinal dimensions. This extra information makes the GSD-front smaller and more selective than the Pareto-front.

### Mechanism 2
The statistical test for GSD-front membership is valid and consistent under i.i.d. sampling. The test uses a permutation approach with regularized test statistics, constructing a test statistic that measures the expected difference in performance under all compatible utility functions. By permuting the performance data, it builds the null distribution and checks if the observed statistic is extreme enough. The global test aggregates these pairwise tests.

### Mechanism 3
Robustness checks quantify how much the test conclusions depend on the i.i.d. assumption. The test is adapted to account for up to k contaminated data sets, computing the worst-case test statistic over all possible contamination patterns. The global test rejects only if this worst-case p-value is below α, giving a conservative but valid test under contamination.

## Foundational Learning

- **Preference systems and generalized stochastic dominance (GSD)**: The paper compares classifiers under multiple quality metrics by embedding them in a preference system and using GSD to define dominance. Understanding these concepts is essential to follow the methodology. *Quick check: What is the difference between a preorder and a partial order, and why does the paper assume antisymmetry of ≿ for testing?*

- **VC dimension and statistical consistency**: The consistency of the GSD-front estimator relies on the VC dimension of the set of utility-induced sets being finite. This ensures uniform convergence of the empirical estimator. *Quick check: How does the VC dimension bound the complexity of the set system, and why is this important for consistency?*

- **Permutation tests and multiple testing**: The statistical test for GSD-front membership is a permutation test applied to each pair of classifiers. The global test is a combination of these pairwise tests. Understanding how permutation tests work and how to control error rates in multiple testing is crucial. *Quick check: How does the permutation test construct the null distribution, and why is regularization of the test statistic necessary?*

## Architecture Onboarding

- **Component map**: Data -> Preference system -> GSD-front estimator -> Statistical test -> Robustness check
- **Critical path**: 1) Load benchmark data, 2) Define preference system, 3) Compute empirical GSD-front, 4) Perform pairwise permutation tests, 5) Aggregate test results, 6) Perform robustness checks
- **Design tradeoffs**: GSD vs. Pareto (GSD more informative but requires cardinal metrics), Regularized vs. unregularized test statistic (regularization reduces sensitivity but increases robustness), Static vs. dynamic GSD-test (static simpler but ignores correlations; dynamic more powerful but more complex)
- **Failure signatures**: All classifiers in empirical GSD-front (metrics not discriminating or preference system too conservative), No classifier in empirical GSD-front (regularization too large or test too conservative), Test decisions change drastically under small contamination (benchmark not representative or i.i.d. assumption violated)
- **First 3 experiments**: 1) Run GSD-front analysis on simple synthetic benchmark with known dominance structure, 2) Compare GSD-front to Pareto-front on real benchmark suite with mixed-scale metrics, 3) Perform robustness check under varying degrees of contamination on real benchmark suite

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the GSD-front perform when comparing deep learning classifiers or optimizers, where computational complexity and resource constraints become significant factors? The authors mention that applications to DAWNBench and DeepOBS appear straightforward, but did not test their framework on deep learning benchmarks.

- **Open Question 2**: Can the GSD-front be extended to regression-type analysis, where the goal is to predict continuous values rather than discrete classes? The paper focuses on classification problems, leaving the extension to regression problems unexplored.

- **Open Question 3**: How robust is the GSD-front to different choices of discretization methods for ordinal quality metrics? The paper does not explore the impact of different discretization methods on the GSD-front's results.

## Limitations

- The framework relies on finite-sample approximations for statistical tests, with asymptotic guarantees only as sample size approaches infinity
- Robustness analysis under contamination is limited to specific γ-contamination models that may not capture all realistic data corruption scenarios
- The effectiveness of GSD-front depends on having interpretable cardinal scales in at least some quality metrics

## Confidence

- **High confidence**: The theoretical foundation of GSD-front construction and its relationship to Pareto dominance
- **Medium confidence**: The consistency of the statistical estimator under i.i.d. assumptions
- **Medium confidence**: The effectiveness of contamination robustness checks, though practical impact depends on contamination assumptions

## Next Checks

1. **Finite-sample performance evaluation**: Conduct simulation studies to assess the Type I error rate and power of the GSD-test for realistic sample sizes (e.g., s=80 or 62 datasets as in the paper).

2. **Alternative contamination models**: Test the robustness analysis under different contamination patterns beyond γ-contamination to evaluate the sensitivity of conclusions to model assumptions.

3. **Benchmark comparison**: Apply the GSD-front methodology to a simple synthetic benchmark with known dominance structure to verify that it correctly identifies the expected GSD-front and that the statistical test behaves as predicted.