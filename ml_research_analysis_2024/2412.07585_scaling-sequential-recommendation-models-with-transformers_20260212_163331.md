---
ver: rpa2
title: Scaling Sequential Recommendation Models with Transformers
arxiv_id: '2412.07585'
source_url: https://arxiv.org/abs/2412.07585
tags:
- recommendation
- number
- scaling
- sequential
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores scaling laws for transformer-based sequential
  recommendation models, aiming to understand how performance scales with model size
  and dataset size. The authors address the challenge of scaling recommendation models
  to massive catalogs, where traditional approaches struggle due to the dependency
  between model complexity and catalog size.
---

# Scaling Sequential Recommendation Models with Transformers

## Quick Facts
- **arXiv ID**: 2412.07585
- **Source URL**: https://arxiv.org/abs/2412.07585
- **Reference count**: 40
- **Primary result**: Scaling laws show transformer-based sequential recommendation models follow predictable performance improvements with model size and dataset size, similar to language models

## Executive Summary
This paper investigates scaling laws for transformer-based sequential recommendation models, addressing the fundamental challenge of scaling to massive catalogs where traditional embedding-based approaches fail due to parameter scaling with catalog size. The authors propose a scalable framework that replaces trainable item embeddings with a feature extractor that computes item representations on-the-fly, making the model independent of catalog size. Through extensive experiments on the Amazon Product Data dataset, they demonstrate that performance scales predictably with model parameters and dataset size following power-law relationships similar to language models. The work also shows that pre-trained large models can be fine-tuned for downstream tasks, achieving better performance than models trained from scratch.

## Method Summary
The authors reformulate sequential recommendation as an embedding regression task, replacing traditional trainable item embeddings with a feature extractor that generates item representations from textual metadata (titles and brands). The model uses a transformer encoder architecture with contrastive learning via sampled softmax loss, allowing training with a fixed parameter count regardless of catalog size. They conduct scaling experiments by training models of varying complexity on datasets of different sizes, then demonstrate transfer learning by pre-training on the full Amazon dataset and fine-tuning on domain-specific subsets (Beauty and Sports). The training uses Adam optimizer with one-cycle learning rate scheduling and progressive fine-tuning with Elastic Weight Consolidation for transfer learning.

## Key Results
- Performance scales predictably with model parameters and dataset size following power-law relationships similar to language models
- The feature extractor approach achieves competitive performance compared to traditional embedding-based transformers while maintaining constant parameter count
- Pre-trained large models fine-tuned on downstream tasks outperform models trained from scratch on the same tasks
- NDCG@5 scores improve consistently with both model size and training data volume

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling transformer-based sequential recommendation models with dataset size and model complexity follows similar power-law scaling relationships as observed in language models.
- Mechanism: The model performance (NDCG@5) scales predictably with the number of parameters (N) and the number of seen interactions (T), following a risk decomposition formula that accounts for both model capacity and data richness.
- Core assumption: The underlying recommendation problem shares fundamental statistical properties with language modeling that enable similar scaling behavior.
- Evidence anchors:
  - [abstract]: "reveal scaling behaviors similar to those found in language models"
  - [section]: "We show scaling laws similar to those observed in language modeling tasks"
  - [corpus]: Weak - corpus neighbors focus on Mamba-based methods and real-time personalization, not scaling laws
- Break condition: If the dataset size becomes so large that it covers nearly all possible items, or if the sequential patterns become too sparse to learn meaningful representations.

### Mechanism 2
- Claim: Using a feature extractor to compute item embeddings instead of trainable embeddings makes the model independent of catalog size.
- Mechanism: By replacing the embedding lookup table with a trainable feature extractor that generates item representations on-the-fly, the parameter count becomes fixed regardless of catalog size, allowing true scaling experiments.
- Core assumption: The feature extractor can generate sufficiently discriminative representations for all items in the catalog without requiring direct parameter learning for each item.
- Evidence anchors:
  - [abstract]: "pivot from the traditional representation of catalog items as trainable embeddings to representations computed with a trainable feature extractor"
  - [section]: "we propose reformulating it as an embedding regression task"
  - [corpus]: Weak - corpus focuses on Mamba architectures and graph neural networks, not embedding strategies
- Break condition: If the feature extractor cannot generate high-quality representations for items with limited textual information, leading to poor performance on long-tail items.

### Mechanism 3
- Claim: Pre-training large recommendation models and fine-tuning them on downstream tasks improves performance compared to training from scratch.
- Mechanism: The large pre-trained model captures general sequential patterns that transfer to specific domains, allowing fine-tuning with less data to achieve better performance.
- Core assumption: Sequential patterns learned from large, diverse datasets are transferable across different recommendation domains.
- Evidence anchors:
  - [abstract]: "demonstrate that pre-trained larger models can be fine-tuned for downstream tasks, achieving better performance than models trained from scratch"
  - [section]: "fine-tuning larger pre-trained models on smaller task-specific domains"
  - [corpus]: Weak - corpus focuses on different architectures rather than transfer learning approaches
- Break condition: If the pre-training and target domains are too dissimilar, causing negative transfer or poor fine-tuning performance.

## Foundational Learning

- Concept: Scaling laws in deep learning
  - Why needed here: Understanding how model performance scales with parameters and data size is crucial for efficient resource allocation and model design
  - Quick check question: What is the relationship between model parameters and dataset size in the scaling law formula presented in the paper?

- Concept: Contrastive learning
  - Why needed here: The paper uses a contrastive formulation (sampled softmax) instead of traditional classification, which is key to their scalability approach
  - Quick check question: How does the contrastive loss formulation differ from a standard cross-entropy loss in the context of recommendation?

- Concept: Transformer architecture
  - Why needed here: The entire approach is built on transformer-based models, so understanding self-attention, positional encoding, and layer structure is essential
  - Quick check question: What role does the transformer encoder play in the Scalable Recommendation Transformer (SRT) framework?

## Architecture Onboarding

- Component map: Transformer encoder → Feature extractor (item title/brand tokenizer) → Contrastive loss with sampled softmax → Fine-tuning layer
- Critical path: Item sequence → Token embedding → Transformer layers → Output embedding → Similarity computation with catalog items
- Design tradeoffs: Fixed feature extractor reduces parameters but may limit expressiveness vs. trainable embeddings that scale poorly; contrastive learning vs. classification approaches
- Failure signatures: Poor performance on long-tail items (feature extractor limitation), degradation with extremely large catalogs (sampling inefficiency), catastrophic forgetting during fine-tuning
- First 3 experiments:
  1. Baseline test: Train SRT-10 (10 negatives) on a small subset of Amazon data and verify NDCG@5 performance matches reported values
  2. Scaling test: Train multiple SRT variants with different parameter counts on datasets of varying sizes, plot NDCG vs FLOPs to verify scaling behavior
  3. Transfer test: Pre-train a large model on full Amazon data, fine-tune on Beauty subset, compare against model trained from scratch on Beauty data only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling behavior of transformer-based sequential recommendation models change when applied to different types of recommendation domains (e.g., music streaming vs. e-commerce vs. news)?
- Basis in paper: [inferred] The paper focuses on e-commerce data from Amazon and discusses the need to adapt models to different domains, but doesn't explore how scaling laws might differ across domains.
- Why unresolved: The paper only tests on Amazon data, leaving open how different catalog characteristics and interaction patterns might affect scaling behavior.
- What evidence would resolve it: Experiments comparing scaling laws across multiple recommendation domains with varying catalog sizes, interaction types, and sequence patterns.

### Open Question 2
- Question: What is the optimal balance between model parameters and dataset size for achieving maximum performance given a fixed computational budget in real-world recommendation systems?
- Basis in paper: [explicit] The paper discusses compute-optimal training and provides equations for estimating performance, but doesn't provide definitive guidelines for real-world deployment scenarios.
- Why unresolved: The paper provides theoretical insights but doesn't address practical constraints like memory limitations, inference speed requirements, or cost considerations.
- What evidence would resolve it: Detailed case studies applying these scaling laws to actual production systems with real computational constraints.

### Open Question 3
- Question: How does the feature extraction approach compare to traditional embedding methods when dealing with catalogs that have rich metadata versus those with minimal metadata?
- Basis in paper: [explicit] The paper proposes using feature extractors instead of trainable embeddings, but only tests on product data with titles and brands available.
- Why unresolved: The paper doesn't explore how the approach performs on items with varying levels of metadata richness or different types of features.
- What evidence would resolve it: Comparative studies across multiple datasets with varying metadata availability and quality.

## Limitations

- Computational constraints limited testing to smaller parameter ranges and dataset sizes than would be encountered in industrial applications
- Feature extractor approach assumes sufficient textual metadata is available for all items, which may not hold in all recommendation domains
- Contrastive learning formulation relies on negative sampling from popularity distribution, but optimal sampling strategy remains unclear
- Transfer learning results demonstrated only on two specific domains (Beauty and Sports), limiting generalizability claims

## Confidence

**High confidence** in the core technical contribution: the reformulation of recommendation as a feature extraction problem rather than embedding lookup is well-motivated and addresses a fundamental scalability challenge in sequential recommendation. The experimental methodology for testing scaling behavior is sound and follows established practices from language modeling research.

**Medium confidence** in the scaling law claims: while the reported power-law relationships align with theoretical expectations and language model literature, the limited parameter range and dataset sizes tested mean the laws may not extrapolate reliably to industrial-scale applications. The authors acknowledge they couldn't test the largest configurations.

**Medium confidence** in the transfer learning results: the fine-tuning experiments show promising improvements, but the sample size (two domains) and lack of comparison to other transfer learning approaches (like domain adaptation or meta-learning) limits the strength of the conclusions.

## Next Checks

1. **Scaling law validation at larger scales**: Train SRT models with parameter counts 2-4× larger than reported, using distributed training on industrial-scale hardware to verify whether the power-law relationships hold across multiple orders of magnitude in both model size and dataset size. This would test the fundamental claim about predictable scaling behavior.

2. **Feature extractor robustness analysis**: Systematically evaluate performance on items with varying levels of metadata completeness (full titles vs partial titles vs missing titles), and compare against baseline models with trainable embeddings on long-tail items. This would validate whether the feature extractor limitation is a practical concern.

3. **Transfer learning generalization study**: Conduct a comprehensive transfer learning experiment across 5-10 diverse recommendation domains, comparing pre-training on Amazon data against alternative pre-training strategies (e.g., pre-training on multiple datasets, domain-adaptive pre-tuning) and measuring both positive and negative transfer effects. This would strengthen the claims about pre-training benefits.