---
ver: rpa2
title: A Dataset for Evaluating LLM-based Evaluation Functions for Research Question
  Extraction Task
arxiv_id: '2409.06883'
source_url: https://arxiv.org/abs/2409.06883
tags:
- evaluation
- problem
- papers
- research
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a dataset for evaluating LLM-based functions
  in research question (RQ) extraction from machine learning papers. The dataset includes
  abstracts, introductions, RQ extracted by GPT-4, and human evaluations across three
  perspectives: problem accuracy, method accuracy, and RQ format compliance.'
---

# A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task

## Quick Facts
- **arXiv ID**: 2409.06883
- **Source URL**: https://arxiv.org/abs/2409.06883
- **Reference count**: 40
- **Primary result**: Existing LLM-based evaluation functions show low correlation with human judgments for research question extraction from ML papers

## Executive Summary
This study introduces a dataset specifically designed to evaluate LLM-based evaluation functions for the task of extracting research questions from machine learning papers. The dataset contains abstracts, introductions, research questions extracted by GPT-4, and human evaluations across three perspectives: problem accuracy, method accuracy, and RQ format compliance. Using this dataset, the authors systematically compared existing LLM-based evaluation functions and found that none showed strong correlation with human judgments, with the highest correlation being only 0.121 (Spearman) for problem score and 0.493 for method score. These results suggest that current evaluation functions may not generalize well to specialized domains like research question extraction.

## Method Summary
The authors constructed a dataset of machine learning paper abstracts and introductions, with research questions extracted by GPT-4. Seven human annotators evaluated 23 abstracts across three dimensions: problem accuracy (whether the extracted RQ correctly identifies the problem), method accuracy (whether the RQ correctly describes the approach), and RQ format compliance (whether the RQ follows proper formatting conventions). The authors then tested various existing LLM-based evaluation functions against this dataset to measure their correlation with human judgments using Spearman correlation coefficients.

## Key Results
- None of the tested LLM-based evaluation functions showed strong correlation with human judgments
- Highest correlation was only 0.121 (Spearman) for problem score and 0.493 for method score
- Results suggest existing evaluation functions may not generalize well to specialized domains like RQ extraction
- Dataset provides foundation for future development of domain-specific evaluation functions

## Why This Works (Mechanism)
The study works by providing a standardized evaluation framework for a specialized NLP task. By creating a dataset with human judgments as ground truth, it enables systematic comparison of evaluation functions that were originally designed for different tasks. The three-perspective evaluation approach captures different aspects of research question quality, allowing for more nuanced assessment of evaluation function performance.

## Foundational Learning
- **Research question extraction**: The task of identifying and formulating the main research question from scientific papers - needed to understand the specific domain and task being evaluated
- **LLM-based evaluation functions**: Methods that use large language models to assess the quality of text outputs - needed to understand what tools are being evaluated
- **Spearman correlation**: A non-parametric measure of rank correlation - needed to understand how performance is measured
- **Human evaluation in NLP**: The process of having humans assess text quality - needed to understand the gold standard used
- **Domain adaptation**: The challenge of applying models across different specialized domains - needed to understand why existing functions may fail
- **Prompt engineering**: The design of effective prompts for LLM-based systems - needed to understand how evaluation functions are implemented

## Architecture Onboarding
- **Component map**: Human annotators -> GPT-4 extraction -> Evaluation functions -> Correlation analysis
- **Critical path**: Paper selection → RQ extraction → Human evaluation → Function testing → Correlation calculation
- **Design tradeoffs**: Using GPT-4 for initial extraction trades automation for potential bias; small human sample trades breadth for depth of annotation
- **Failure signatures**: Low correlation between evaluation functions and human judgments indicates domain mismatch or task specificity issues
- **First experiments**:
  1. Replicate correlation analysis with larger human sample
  2. Test evaluation functions on a different scientific domain
  3. Fine-tune evaluation functions on RQ-specific training data

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions.

## Limitations
- Dataset relies on GPT-4 for initial RQ extraction, introducing potential bias
- Human evaluation sample size is relatively small (23 abstracts by 7 annotators)
- Study focuses only on machine learning papers in English, limiting generalizability

## Confidence
- **High confidence**: Dataset construction methodology and systematic comparison approach are clearly described and reproducible
- **Medium confidence**: Finding that current evaluation functions show low correlation with human judgments on this task, though absolute correlation values may vary with larger sample sizes
- **Medium confidence**: Conclusion that existing evaluation functions may not generalize well to specialized domains, pending further validation on other specialized tasks

## Next Checks
1. Test whether fine-tuning existing evaluation functions on RQ extraction data improves their correlation with human judgments
2. Replicate the study with a larger human evaluation sample (minimum 100 abstracts) to verify the correlation findings
3. Evaluate the same evaluation functions on RQ extraction tasks from other scientific domains (e.g., biomedical or social sciences) to assess domain specificity