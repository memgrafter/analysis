---
ver: rpa2
title: Pruning Large Language Models with Semi-Structural Adaptive Sparse Training
arxiv_id: '2407.20584'
source_url: https://arxiv.org/abs/2407.20584
tags:
- training
- sparse
- performance
- pruning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Sparse Trainer (AST), a retraining
  framework for compressing large language models with semi-structured sparsity. AST
  addresses the challenge of maintaining performance when pruning LLMs by enabling
  adaptive mask learning through a gradual decay mechanism combined with knowledge
  distillation.
---

# Pruning Large Language Models with Semi-Structural Adaptive Sparse Training

## Quick Facts
- arXiv ID: 2407.20584
- Source URL: https://arxiv.org/abs/2407.20584
- Authors: Weiyu Huang; Yuezhou Hu; Guohao Jian; Jun Zhu; Jianfei Chen
- Reference count: 13
- Key outcome: AST achieves 0.6% perplexity and 1.16% zero-shot accuracy gap between dense and 2:4 sparse LLaMA2-7B models using <0.4% of pretraining tokens

## Executive Summary
This paper proposes Adaptive Sparse Trainer (AST), a retraining framework for compressing large language models with semi-structured sparsity. AST addresses the challenge of maintaining performance when pruning LLMs by enabling adaptive mask learning through a gradual decay mechanism combined with knowledge distillation. The method allows models to learn optimal sparsity patterns during training without extra computational overhead, while incorporating a supplementary set of well-initialized parameters (SLoRB) to enhance capacity. Applied to LLaMA2-7B, AST reduces the perplexity and zero-shot accuracy gap between dense and 2:4 sparse models to 0.6 and 1.16%, respectively, using less than 0.4% of pretraining tokens.

## Method Summary
AST is a retraining framework that compresses large language models through semi-structured adaptive sparse training. The method combines three key components: Annealing SR-STE for gradual weight decay and mask learning, knowledge distillation for efficient convergence, and supplementary LoRA parameters (SLoRB) to restore capacity. The framework operates by gradually decaying unimportant weights to zero while periodically recalculating masks based on weight magnitude, using KL-divergence loss from a dense teacher model to guide sparse model learning. SLoRB parameters are initialized using information from pruned weights and trained alongside original parameters to compensate for reduced capacity in semi-structured sparsity patterns.

## Key Results
- Achieves 0.6% perplexity gap between dense and 2:4 sparse LLaMA2-7B models
- Maintains 1.16% zero-shot accuracy gap on knowledge-intensive tasks
- Requires less than 0.4% of pretraining tokens for retraining
- Demonstrates 50% sparsity with minimal performance degradation on multiple model architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradual decay of unimportant weights combined with periodic mask recalculation enables smooth transition from dense to sparse connectivity.
- **Mechanism:** The Annealing SR-STE method applies increasing L2 decay to masked weights while periodically updating masks based on weight magnitude. This allows important weights to recover through strong gradients while unimportant weights decay to zero.
- **Core assumption:** Weight magnitude correlates with importance for language modeling tasks, and gradual decay prevents catastrophic forgetting of useful patterns.
- **Evidence anchors:**
  - [abstract] "gradually decaying unimportant weights to zero through a carefully designed decay mechanism"
  - [section 3.2] "We update the model weights with the following equation: λW(t) = αt if 0 ≤ t ≤ T0, αT0 if T0 ≤ t ≤ T1"
  - [section 6.9] "Annealing SR-STE modifies a higher percentage of the mask at the beginning, allowing the model to explore various mask patterns"

### Mechanism 2
- **Claim:** Knowledge distillation accelerates convergence and prevents sparse models from settling in local optima.
- **Mechanism:** KL-divergence loss between student and teacher model distributions provides richer signals than cross-entropy alone, guiding sparse models toward global optima while retaining world knowledge from pretrained dense models.
- **Core assumption:** Pretrained dense models contain valuable world knowledge that can be transferred to sparse models through distillation, and KL-divergence is more effective than cross-entropy for this transfer.
- **Evidence anchors:**
  - [abstract] "incorporating knowledge distillation significantly improves retraining efficiency and enhances model performance"
  - [section 3.3] "We employ the following loss function: Llogit = DKL(pθt ||pθs)"
  - [section 4.4] "We observe that the Retraining Dilemma is more pronounced in smaller models; therefore, we typically apply a larger α for these models"

### Mechanism 3
- **Claim:** Supplementary LoRA parameters (SLoRB) restore expressive capacity lost due to semi-structured sparsity constraints.
- **Mechanism:** SLoRB parameters are initialized using information from pruned weights and trained alongside original parameters, effectively compensating for the reduced input access per neuron in 2:4 sparsity patterns.
- **Core assumption:** Pruned weights contain useful information that can be leveraged to initialize supplementary parameters, and these parameters can effectively augment model capacity without breaking sparsity constraints.
- **Evidence anchors:**
  - [abstract] "a supplementary set of well-initialized parameters is integrated to further augment the model's efficacy"
  - [section 3.4] "we incorporate a LoRA shape parameter that is trained alongside the original parameters, helping to bridge the performance gap"
  - [section 4.1] "For AST-Boosted models, we selected k = 16, introducing an additional 12.5% of parameters"

## Foundational Learning

- **Concept: N:M semi-structured sparsity**
  - Why needed here: Understanding the specific hardware-efficient sparsity pattern that retains N non-zero elements per M-element group is crucial for grasping why AST targets this particular pattern and how it differs from unstructured pruning.
  - Quick check question: Why is 2:4 sparsity (50% sparsity) considered a good balance between performance and hardware efficiency compared to 1:2 or 4:8 patterns?

- **Concept: Knowledge distillation with KL divergence**
  - Why needed here: The paper's effectiveness relies heavily on using teacher-student distillation rather than just cross-entropy, so understanding the difference between these approaches and why KL divergence works better for sparse models is essential.
  - Quick check question: How does KL divergence loss differ from cross-entropy in terms of the signals it provides to the student model during training?

- **Concept: Lottery Ticket Hypothesis and sparse training**
  - Why needed here: AST builds on the idea that connectivity patterns matter as much as weights, so understanding why sparse training (learning masks during training) differs from post-training pruning is important for grasping the innovation.
  - Quick check question: Why might a mask that works well for a randomly initialized model not be optimal for a pretrained model, necessitating adaptive mask learning?

## Architecture Onboarding

- **Component map:** Main training loop -> Weight updates with decay -> Periodic mask recalculation -> Knowledge distillation loss -> (Optional) SLoRB parameter updates

- **Critical path:**
  1. Forward pass with current mask applied to weights
  2. Compute combined loss (KD + task loss)
  3. Backward pass to get gradients
  4. Apply decay to masked weights
  5. Update weights using AdamW
  6. Periodically (every Δt steps) recalculate mask based on magnitude
  7. (Optional) Update SLoRB parameters

- **Design tradeoffs:**
  - Mask update frequency vs. computational overhead: More frequent updates give better masks but cost more
  - Decay strength schedule: Too aggressive early causes mask instability, too weak late prevents convergence
  - SLoRB rank vs. memory overhead: Higher rank improves performance but increases memory usage
  - Distillation weight α: Higher values help convergence but may reduce task-specific learning

- **Failure signatures:**
  - Performance degradation: Mask becomes too static early, losing optimal sparsity patterns
  - Slow convergence: Decay schedule too conservative, preventing mask evolution
  - Overfitting to teacher: α too high in distillation, student doesn't learn task-specific patterns
  - Memory issues: SLoRB rank too high relative to available GPU memory

- **First 3 experiments:**
  1. Implement basic AST without SLoRB on small GPT2 model, verify 2:4 sparsity maintenance and measure perplexity improvement over naive training
  2. Add knowledge distillation with varying α values, measure impact on convergence speed and final performance
  3. Test different mask update frequencies (Δt values) to find optimal balance between mask quality and computational overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal decay schedule for Annealing SR-STE that balances exploration of mask patterns with stable convergence?
- Basis in paper: [explicit] The paper discusses the trade-off between stability and exploration in mask learning and proposes a moving decay schedule, but does not provide an optimal formula or empirical comparison of different decay schedules.
- Why unresolved: The paper presents the concept of Annealing SR-STE but does not explore different decay schedule configurations or provide a mathematical framework for determining the optimal decay parameters.
- What evidence would resolve it: Systematic experiments comparing different decay schedules (linear, exponential, step-wise) across various model sizes and tasks, along with a mathematical analysis of the relationship between decay parameters and convergence properties.

### Open Question 2
- Question: How does AST performance scale with model size, and are there different optimal hyperparameters for different model scales?
- Basis in paper: [inferred] The paper mentions that the Retraining Dilemma is more pronounced in smaller models and that α plays a more critical role in optimizing results for larger-scale models, suggesting that different model sizes may require different hyperparameter configurations.
- Why unresolved: The paper provides results for LLaMA2-7B, OPT, and GPT2 models but does not systematically analyze how performance and optimal hyperparameters scale with model size.
- What evidence would resolve it: A comprehensive study of AST performance across a wide range of model sizes (from 100M to 70B parameters), with detailed analysis of how hyperparameters like learning rate, decay factors, and α need to be adjusted for different scales.

### Open Question 3
- Question: What is the impact of AST on model generalization to out-of-distribution tasks and domain-specific applications?
- Basis in paper: [inferred] While the paper demonstrates strong performance on various zero-shot tasks and knowledge-intensive benchmarks, it does not explore how AST affects model generalization to tasks or domains not represented in the training data.
- Why unresolved: The paper focuses on in-distribution performance and knowledge retention but does not investigate how the semi-structured sparsity affects the model's ability to generalize to new tasks or domains.
- What evidence would resolve it: Extensive testing of AST-pruned models on a diverse set of out-of-distribution tasks, including domain adaptation scenarios and novel task types, with comparison to dense models and other pruning methods.

## Limitations

- Performance gains are primarily demonstrated on knowledge-intensive tasks, with limited evaluation on generation quality and creative tasks
- The supplementary LoRA parameters (SLoRB) add computational overhead that may offset some hardware efficiency gains from sparsity
- The method requires careful hyperparameter tuning, particularly for decay schedules and distillation weights, which may not generalize across different model architectures

## Confidence

- **High Confidence:** The core mechanism of gradual weight decay combined with periodic mask recalculation is technically sound and aligns with established sparse training literature. The empirical results showing reduced perplexity and accuracy gaps are well-documented with specific numerical improvements.
- **Medium Confidence:** The claim that knowledge distillation significantly improves retraining efficiency is supported by ablation studies, but the exact contribution of KD versus other components (decay, mask updates) is not fully isolated. The generalization of results across different model sizes and tasks is reasonably demonstrated but could be more extensive.
- **Low Confidence:** The assertion that <0.4% of pretraining tokens achieves comparable performance assumes specific pretraining costs and may not generalize to different pretraining regimes or datasets. The long-term stability of learned sparse patterns under continued fine-tuning is not evaluated.

## Next Checks

1. **Ablation Study on Component Contributions:** Systematically disable each component (knowledge distillation, SLoRB parameters, mask update frequency) to quantify their individual contributions to final performance, isolating whether the claimed 0.6% perplexity gap reduction is primarily driven by one mechanism or the combination.

2. **Sparsity Ratio Sensitivity Analysis:** Evaluate AST performance across different sparsity ratios (1:2, 3:8, 4:8) beyond the 2:4 pattern to determine the method's robustness to varying hardware efficiency constraints and identify at what point performance degradation becomes prohibitive.

3. **Transfer Learning Stability Test:** Fine-tune AST-pruned models on downstream tasks for extended iterations (>100 steps) to verify that the learned sparse patterns remain stable and do not degenerate, particularly focusing on whether the mask continues to adapt or becomes static after initial convergence.