---
ver: rpa2
title: 'Transforming Game Play: A Comparative Study of DCQN and DTQN Architectures
  in Reinforcement Learning'
arxiv_id: '2410.10660'
source_url: https://arxiv.org/abs/2410.10660
tags:
- learning
- dtqn
- transformer
- where
- dcqn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares Deep Q-Networks (DQNs) using Convolutional
  Neural Networks (CNNs) and Transformer architectures in reinforcement learning across
  three Atari games. The research finds that CNN-based DQNs outperform Transformer-based
  DQNs in speed and overall performance in most games, except for Centipede where
  the Transformer model shows superior results.
---

# Transforming Game Play: A Comparative Study of DCQN and DTQN Architectures in Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.10660
- Source URL: https://arxiv.org/abs/2410.10660
- Reference count: 15
- CNN-based DQNs outperform Transformer-based DQNs in speed and overall performance in most games, except for Centipede where the Transformer model shows superior results

## Executive Summary
This study compares Deep Q-Networks (DQNs) using Convolutional Neural Networks (CNNs) and Transformer architectures in reinforcement learning across three Atari games. The research finds that CNN-based DQNs outperform Transformer-based DQNs in speed and overall performance in most games, except for Centipede where the Transformer model shows superior results. The CNN models achieved higher average rewards in Asteroids and Space Invaders, while the Transformer excelled in Centipede due to its ability to exploit specific game dynamics. However, the Transformer model demonstrated slower computational speed and faced challenges in feature extraction and learning complex strategies in dynamic environments like Asteroids. The study highlights the need for further refinement of Transformer-based architectures to fully leverage their capabilities in diverse gaming scenarios.

## Method Summary
The study compares Deep Q-Networks using Convolutional Neural Networks (DCQN) and Transformer architectures (DTQN) across three Atari games (Asteroids, Space Invaders, Centipede). Both models use experience replay buffers, ϵ-greedy exploration with exponential decay, and AdamW optimizer. DCQN employs three convolutional layers with ReLU activation and batch normalization, while DTQN uses patch embedding (16x16 patches), Gated Transformer-XL layers, and attention pooling. Models were trained for 10,000 episodes with evaluation every 500 episodes (Asteroids every 100 steps for updates). The study tested models in the 35-40 million parameter range and compared average rewards, computational speed, and loss values.

## Key Results
- CNN-based DQNs achieved higher average rewards in Asteroids and Space Invaders compared to Transformer-based DQNs
- DTQN showed superior performance in Centipede by exploiting specific game dynamics and structured movement patterns
- DTQN suffered from feature loss during linear projection and demonstrated slower computational speed than DCQN across all games except Asteroids

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Transformer model performs better on Centipede due to its ability to exploit specific game dynamics through sequence modeling of spatial patterns.
- Mechanism: The Centipede game has predictable, structured movement patterns that can be captured effectively by the Transformer's self-attention mechanism, allowing it to learn optimal positioning and shooting strategies.
- Core assumption: The Transformer's sequence modeling capabilities provide an advantage when game states have predictable temporal and spatial patterns that can be encoded as sequences.
- Evidence anchors:
  - [abstract] "the Transformer excelled in Centipede due to its ability to exploit specific game dynamics"
  - [section] "In Centipede, in which both models utilize Huber Loss, we see that the Transformer is flat across all benchmarks for the entire duration of training"
- Break condition: If game dynamics become less structured or more stochastic, the CNN's local feature extraction may outperform the Transformer's sequence modeling approach.

### Mechanism 2
- Claim: The CNN model outperforms the Transformer in speed due to its more efficient computation of local spatial features.
- Mechanism: CNNs can process local spatial relationships through convolutional kernels that are computationally more efficient than the self-attention mechanism in Transformers, which requires quadratic computation with respect to sequence length.
- Core assumption: Computational efficiency directly impacts training speed, and the CNN's architecture is inherently more efficient for spatial feature extraction in game frames.
- Evidence anchors:
  - [abstract] "the DCQN outperforms the DTQN in speed across both ViT and Projection Architectures"
  - [section] "All version of the DTQN were slower than the DCQN except for Asteroids"
- Break condition: If computational resources are not a constraint or if the problem requires understanding long-range dependencies, the Transformer's computational disadvantage may be acceptable.

### Mechanism 3
- Claim: The DTQN suffers from feature loss during linear projection, which inhibits its ability to effectively process and respond to dynamic game environments.
- Mechanism: The linear projection step in the DTQN architecture reduces the spatial information in game frames to a lower-dimensional embedding, potentially losing critical feature information that the CNN preserves through its convolutional layers.
- Core assumption: The linear projection step in the Transformer architecture is lossy and cannot capture the same level of spatial detail as the CNN's convolutional layers.
- Evidence anchors:
  - [section] "The lack of learning progress in Asteroids suggests that DTQN may suffer from significant feature loss during the linear projection phase"
  - [section] "We believe we can attribute this to the feature loss during linear projection"
- Break condition: If the linear projection is modified to be less lossy or if pretraining is used to initialize the projection layer with better feature representations, this performance gap may be reduced.

## Foundational Learning

- Concept: Frame stacking and temporal information encoding
  - Why needed here: Reinforcement learning agents need to understand motion and temporal dynamics in video games, which single frames cannot provide
  - Quick check question: How does frame stacking help an agent understand the direction and speed of moving objects in a game?

- Concept: Q-learning and Bellman equation
  - Why needed here: The study compares Deep Q-Networks, which are based on Q-learning and use the Bellman equation to update action-value estimates
  - Quick check question: What is the role of the discount factor (γ) in the Bellman equation, and how does it affect long-term versus short-term reward optimization?

- Concept: Experience replay and its benefits
  - Why needed here: The study uses experience replay buffers to break correlation between consecutive samples and improve sample efficiency
  - Quick check question: Why is randomly sampling from a replay buffer better than training on consecutive frames from the same episode?

## Architecture Onboarding

- Component map:
  - Input frames → Preprocessing → Feature extraction (CNN or Transformer) → Q-value prediction → Action selection → Environment interaction → Experience storage → Parameter updates

- Critical path: Input frames → Preprocessing → Feature extraction (CNN or Transformer) → Q-value prediction → Action selection → Environment interaction → Experience storage → Parameter updates

- Design tradeoffs:
  - CNN vs Transformer: Local spatial efficiency vs. global sequential modeling capabilities
  - Patch size selection: 16x16 patches used (though not proportional for 84x84 images), larger patches increase parameter count significantly
  - Model size: Tested only in 35-40 million parameter range, performance may vary with different complexities
  - Linear projection: Used for efficiency but may cause feature loss compared to convolutional feature extraction

- Failure signatures:
  - DTQN flat loss curves indicate feature loss or inability to learn effective representations
  - High volatility in MSE loss may indicate need to switch to Huber loss
  - DTQN failing to learn any strategy (as in Asteroids) suggests fundamental architectural mismatch
  - DTQN exploiting single strategies (like camping in Centipede) indicates overfitting to specific game dynamics

- First 3 experiments:
  1. Compare DTQN with convolutional feature extraction instead of patch embedding to isolate whether feature extraction method or architecture is the limiting factor
  2. Test different patch sizes (e.g., 6x6 instead of 16x16) to determine if proportional segmentation improves performance despite increased parameters
  3. Implement a penalty for repeated actions in DTQN to encourage more diverse strategic exploration and prevent camping behaviors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size affect the relative performance of DCQN and DTQN architectures in reinforcement learning?
- Basis in paper: [explicit] The paper notes that their study tested models in the 36M-39M parameter range and acknowledges that different levels of complexity might lead to different findings.
- Why unresolved: The study was limited to a specific parameter range, and the authors suggest that performance could vary with different model sizes.
- What evidence would resolve it: Comparative studies testing DCQN and DTQN across a broader range of model sizes, including smaller and larger parameter counts, would clarify the impact of model size on performance.

### Open Question 2
- Question: What are the specific architectural modifications that could mitigate feature loss in DTQN during the linear projection phase?
- Basis in paper: [inferred] The authors discuss the potential for feature loss in DTQN, particularly in the linear projection phase, and suggest that architectural adjustments or penalties for repeated actions could improve performance.
- Why unresolved: While the paper identifies the issue of feature loss, it does not provide a definitive solution or test specific architectural modifications.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various architectural modifications, such as alternative projection methods or penalty mechanisms, in reducing feature loss and improving DTQN performance.

### Open Question 3
- Question: How do different patch embedding sizes impact the performance of DTQN in processing game environments?
- Basis in paper: [explicit] The authors mention using a 16x16 patch size on an 84x84 image and discuss the computational challenges of using different patch sizes, such as a 6x6 patch that would increase parameter size significantly.
- Why unresolved: The study used a specific patch size due to computational constraints, but the impact of different patch sizes on model performance remains unexplored.
- What evidence would resolve it: Comparative analysis of DTQN performance using various patch embedding sizes, including the computational trade-offs, would elucidate the optimal patch size for different game environments.

## Limitations
- Study tested only a narrow parameter range (35-40 million) which may not represent performance at different model scales
- Specific architectural details for both CNN and Transformer implementations were not fully specified, limiting reproducibility
- Results based on only three Atari games may not generalize to broader gaming scenarios or different types of environments

## Confidence
- High confidence: CNN models outperform Transformer models in speed across all tested games; DTQN shows superior performance in Centipede due to exploiting specific game dynamics
- Medium confidence: DTQN suffers from feature loss during linear projection, inhibiting performance in dynamic environments; computational efficiency differences between architectures are significant
- Low confidence: The fundamental architectural limitations of Transformers for RL tasks are well-established based on these results; Centipede-specific advantages generalize to other structured games

## Next Checks
1. Compare DTQN with convolutional feature extraction instead of patch embedding to isolate whether feature extraction method or architecture is the limiting factor
2. Test different patch sizes (e.g., 6x6 instead of 16x16) to determine if proportional segmentation improves performance despite increased parameters
3. Implement a penalty for repeated actions in DTQN to encourage more diverse strategic exploration and prevent camping behaviors