---
ver: rpa2
title: Federated Learning with a Single Shared Image
arxiv_id: '2406.12658'
source_url: https://arxiv.org/abs/2406.12658
tags:
- dataset
- distillation
- training
- single
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of knowledge transfer in federated
  learning when only a single shared image is available between clients and server,
  addressing privacy and storage constraints. The authors propose a novel approach
  combining deterministic data augmentation (patchification) to generate a distillation
  dataset from one image, with an adaptive dataset pruning algorithm that selects
  the most informative patches using KMeans-based class balancing and entropy-based
  filtering.
---

# Federated Learning with a Single Shared Image
## Quick Facts
- arXiv ID: 2406.12658
- Source URL: https://arxiv.org/abs/2406.12658
- Reference count: 40
- Primary result: Single-image federated distillation achieves 74.8% CIFAR100 accuracy vs 68.9% for FedDF with 5K samples

## Executive Summary
This paper addresses federated learning under strict privacy constraints by demonstrating that a single shared image can effectively replace multiple client datasets for knowledge distillation. The authors propose patchification-based deterministic augmentation combined with adaptive pruning to create informative distillation datasets. Their method outperforms traditional approaches while reducing storage requirements and communication costs, particularly in scenarios with limited client storage budgets.

## Method Summary
The approach uses a single shared image distributed to all clients, which is then transformed through deterministic patchification to generate multiple augmented samples. An adaptive pruning algorithm selects the most informative patches using KMeans-based class balancing and entropy-based filtering. This distillation dataset enables effective knowledge transfer between heterogeneous client models while preserving privacy, as only one image needs to be shared between server and clients.

## Key Results
- Achieves 74.8% accuracy on CIFAR100 with single shared image versus 68.9% for FedDF with 5K samples
- Outperforms baselines across CIFAR10/100 and MedMNIST datasets under limited storage budgets
- Demonstrates robust performance across heterogeneous client architectures and non-IID data distributions
- Reduces communication costs while maintaining competitive accuracy levels

## Why This Works (Mechanism)
The method leverages deterministic augmentation to maximize information extraction from a single image, while adaptive pruning ensures class balance and informational diversity. The patchification approach creates sufficient variability for effective distillation without requiring multiple images, and the entropy-based filtering selects patches that capture the most distinctive features for model training.

## Foundational Learning
- Federated Learning: Distributed model training across multiple clients while preserving data privacy - needed to understand the collaborative learning framework
- Knowledge Distillation: Training student models using soft labels from teacher models - critical for understanding how information transfers between clients
- Data Augmentation: Techniques to artificially expand training datasets - essential for grasping how single image generates sufficient training samples
- KMeans Clustering: Unsupervised algorithm for grouping similar data points - important for understanding class balancing in pruning algorithm
- Entropy-based Filtering: Information theory concept for measuring uncertainty - needed to understand patch selection criteria

## Architecture Onboarding
Component Map: Single Image -> Patchification -> KMeans-based Pruning -> Entropy Filtering -> Distillation Dataset -> Client Training

Critical Path: The deterministic patchification and adaptive pruning pipeline forms the backbone, where each patch selection directly impacts distillation quality and downstream model performance.

Design Tradeoffs: Privacy gains from single image sharing versus potential loss of visual diversity compared to multiple images; computational overhead of pruning versus improved sample efficiency.

Failure Signatures: Poor distillation performance when shared image lacks class diversity; ineffective patch selection leading to class imbalance; communication bottlenecks if pruning becomes computationally intensive.

First Experiments:
1. Test patchification on diverse single images to measure generated sample quality and variety
2. Evaluate KMeans-based pruning effectiveness with synthetic imbalanced datasets
3. Measure entropy filtering performance on controlled patch selection tasks

## Open Questions the Paper Calls Out
None identified in provided content.

## Limitations
- Limited evaluation to CIFAR and MedMNIST datasets, uncertain performance on complex real-world image distributions
- Single image assumption may not generalize when image lacks sufficient visual diversity for heterogeneous client distributions
- Adaptive pruning algorithm computational overhead impact on communication efficiency in resource-constrained environments unclear

## Confidence
- High: Performance improvements over FedDF and FedAvg baselines on CIFAR10/100 and MedMNIST under controlled conditions
- Medium: Claims about heterogeneous client architectures and low-bandwidth effectiveness (limited architectural diversity tested)
- Low: Generalizability to real-world deployments with complex data distributions and varying privacy requirements

## Next Checks
1. Evaluate on larger-scale datasets (e.g., ImageNet subsets) with realistic client participation rates and communication constraints
2. Test sensitivity to shared image quality by varying representativeness across different non-IID distributions
3. Conduct ablation studies to quantify individual contributions of patchification, KMeans pruning, and entropy filtering