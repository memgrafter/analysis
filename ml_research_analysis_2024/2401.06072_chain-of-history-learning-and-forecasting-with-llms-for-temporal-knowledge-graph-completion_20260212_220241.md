---
ver: rpa2
title: 'Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge
  Graph Completion'
arxiv_id: '2401.06072'
source_url: https://arxiv.org/abs/2401.06072
tags:
- llms
- knowledge
- make
- hits
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for Temporal Knowledge Graph Completion
  (TKGC) using Large Language Models (LLMs). The method leverages LLMs' ability to
  understand structural information and textual reasoning to predict missing links
  in temporal knowledge graphs.
---

# Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2401.06072
- Source URL: https://arxiv.org/abs/2401.06072
- Reference count: 40
- One-line primary result: Proposes using LLMs with structure-augmented history modeling and reverse logic incorporation for Temporal Knowledge Graph Completion, achieving competitive results on multiple datasets.

## Executive Summary
This paper introduces a novel approach for Temporal Knowledge Graph Completion (TKGC) using Large Language Models (LLMs). The method leverages LLMs' ability to understand structural information and perform textual reasoning to predict missing links in temporal knowledge graphs. By incorporating structure-augmented history modeling and reverse logic incorporation, the model aims to enhance its understanding of temporal events and mitigate the reversal curse in structured knowledge reasoning. The approach is evaluated on multiple datasets, showing competitive performance compared to existing methods.

## Method Summary
The proposed method fine-tunes pre-trained LLMs (Llama-2-7b, Vicuna-7b) using a parameter-efficient approach (LoRA) for TKGC tasks. It introduces structure-augmented history modeling by incorporating schema-matching, entity-augmented, and relation-augmented histories to provide richer contextual information about central entities and their relationships over time. The method also incorporates reverse quadruples during fine-tuning using different prompt strategies to address the reversal curse in structured knowledge reasoning. The model is evaluated on datasets such as ICEWS14, ICEWS05-15, ICEWS18, and YAGO, with metrics including Hits@1, Hits@3, and Hits@10 under raw and time-aware filtered settings.

## Key Results
- The method achieves competitive results on multiple TKGC datasets, outperforming existing methods on some metrics.
- Ablation experiments show the effectiveness of structure-augmented history modeling and reverse logic incorporation.
- Comparisons with commercial LLMs provide insights into factors influencing LLMs' performance in structured temporal knowledge inference tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure-augmented history modeling improves LLM understanding of pivotal nodes in the historical chain.
- Mechanism: By incorporating entity-augmented and relation-augmented histories in addition to schema-matching history, the model gains richer contextual information about the central entities and their relationships over time.
- Core assumption: LLMs can effectively process and utilize additional structural information when provided in the prompt format.
- Evidence anchors:
  - [abstract] "we underscore the LLMs' prowess in discerning the structural information of pivotal nodes within the historical chain."
  - [section] "Similar to many prior works that leverage structural information from KGs to enhance the reasoning capabilities of LLMs (Luo et al., 2023; Tian et al., 2023), we focus on semantically enriching the representation of central entities by utilizing links with neighbors in TKGs."
  - [corpus] Corpus contains related papers on Chain-of-History Reasoning and integrating temporal graph learning into LLM-based TKG models, providing weak but relevant support.
- Break condition: If the LLM cannot effectively process the additional structural information due to token limitations or lack of understanding of the prompt format.

### Mechanism 2
- Claim: Introduction of reverse logic helps alleviate the reversal curse in structured knowledge reasoning.
- Mechanism: By incorporating reverse quadruples during fine-tuning using different prompt strategies, the model learns to understand and reason about reversed relationships in the knowledge graph.
- Core assumption: LLMs can learn to overcome the reversal curse when explicitly trained on reversed relationships in a structured context.
- Evidence anchors:
  - [abstract] "our focus lies in mitigating the reversal curse in structured expression reasoning."
  - [section] "We believe that this phenomenon also exists in structured knowledge reasoning. We propose using three prompt strategies to incorporate reverse quadruples during the fine-tuning phase to alleviate this issue."
  - [corpus] Corpus contains related papers on LLMs and temporal knowledge graph completion, but specific evidence for the reversal curse in structured knowledge reasoning is weak.
- Break condition: If the model still struggles with reversed relationships despite training on reverse quadruples, or if the prompt strategies do not effectively convey the concept of reversal.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) allows effective adaptation of LLMs to the TKGC task without full fine-tuning.
- Mechanism: By applying LoRA to specific modules of the LLM, the model can learn task-specific knowledge while preserving its general capabilities.
- Core assumption: LoRA is effective for adapting LLMs to new tasks, especially when the task requires understanding of structured information and temporal reasoning.
- Evidence anchors:
  - [abstract] "We adopt a parameter-efficient fine-tuning strategy to harmonize the LLMs with the task requirements."
  - [section] "We apply the Low-Rank Adaptation (LoRA) (Hu et al., 2021) method due to its effectiveness for Llama-style models."
  - [corpus] Corpus contains related papers on parameter-efficient transfer learning for NLP, providing weak but relevant support.
- Break condition: If LoRA does not provide sufficient adaptation for the TKGC task, or if full fine-tuning is required for optimal performance.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs)
  - Why needed here: Understanding TKGs is crucial for grasping the problem domain and the specific challenges of TKGC.
  - Quick check question: What is the main difference between a standard knowledge graph and a temporal knowledge graph?
- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the primary tool used in this method, so understanding their capabilities and limitations is essential.
  - Quick check question: What are some key advantages of using LLMs for reasoning tasks compared to traditional methods?
- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: PEFT techniques like LoRA are used to adapt LLMs to the TKGC task without full fine-tuning.
  - Quick check question: What is the main benefit of using PEFT techniques compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  1. Pre-trained LLM (e.g., Llama-2-7b, Vicuna-7b)
  2. Structure-augmented history modeling module
  3. Reverse logic incorporation module
  4. LoRA fine-tuning module
  5. Inference and generation module
- Critical path: Pre-trained LLM → Structure-augmented history → Reverse logic → LoRA fine-tuning → Inference
- Design tradeoffs:
  - Token limitations vs. rich historical information
  - Complexity of prompt engineering vs. model performance
  - Computational resources for fine-tuning vs. model adaptation
- Failure signatures:
  - Poor performance on reversed relationships
  - Inability to effectively utilize additional structural information
  - Overfitting to specific patterns in the training data
- First 3 experiments:
  1. Ablation study on structure-augmented history: Compare performance with and without entity-augmented and relation-augmented histories.
  2. Comparison of reverse logic prompt strategies: Evaluate the effectiveness of ordinary, text-aware, and position-aware prompt strategies.
  3. Exploration of history length: Investigate the impact of varying history length on model performance across different datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on TKGC tasks vary with different types of temporal knowledge graphs (e.g., news-based vs. commonsense datasets)?
- Basis in paper: [inferred] The paper compares LLMs' performance on ICEWS (news-based) and YAGO (commonsense) datasets, showing different performance levels.
- Why unresolved: The paper provides a comparison but does not explore the underlying reasons for the performance differences between different types of temporal knowledge graphs.
- What evidence would resolve it: Further experiments analyzing the characteristics of different temporal knowledge graph types and their impact on LLM performance, potentially including qualitative analysis of the reasoning patterns.

### Open Question 2
- Question: What is the impact of incorporating reverse quadruples during the fine-tuning phase on the LLM's ability to handle more complex temporal reasoning tasks beyond simple link prediction?
- Basis in paper: [explicit] The paper explores the incorporation of reverse quadruples to address the "reversal curse" in structured knowledge reasoning.
- Why unresolved: While the paper shows some improvement in link prediction, it does not investigate whether this approach enhances the LLM's ability to handle more complex temporal reasoning tasks.
- What evidence would resolve it: Experiments testing the LLM's performance on more complex temporal reasoning tasks (e.g., multi-hop reasoning, event sequence prediction) with and without reverse quadruples in the fine-tuning phase.

### Open Question 3
- Question: How does the length of the historical chain affect the LLM's performance on TKGC tasks, and is there an optimal length that balances information richness and computational efficiency?
- Basis in paper: [explicit] The paper explores the effect of historical chain length on LLM performance, showing an upward trend followed by stabilization as length increases.
- Why unresolved: The paper provides insights into the trend but does not determine the optimal length or explain the underlying reasons for the observed pattern.
- What evidence would resolve it: Detailed analysis of the relationship between historical chain length and LLM performance, potentially including experiments with longer chains and investigation of the LLM's attention patterns across different lengths.

### Open Question 4
- Question: How do different parameter-efficient fine-tuning techniques (e.g., LoRA, prompt tuning) compare in their effectiveness for adapting LLMs to TKGC tasks?
- Basis in paper: [explicit] The paper uses LoRA as its parameter-efficient fine-tuning technique but does not compare it with other methods.
- Why unresolved: The paper does not provide a comparison of different parameter-efficient fine-tuning techniques for TKGC tasks.
- What evidence would resolve it: Experiments comparing the performance of different parameter-efficient fine-tuning techniques (e.g., LoRA, prompt tuning, prefix tuning) on TKGC tasks, potentially including analysis of their impact on model size and training efficiency.

### Open Question 5
- Question: How does the performance of LLMs on TKGC tasks scale with increasing model size, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper explores the effect of model size on TKGC performance, showing that larger models do not necessarily result in better understanding of temporal structural information.
- Why unresolved: The paper provides insights into the relationship between model size and performance but does not determine the point of diminishing returns or explain the underlying reasons for the observed pattern.
- What evidence would resolve it: Experiments testing the performance of LLMs of various sizes on TKGC tasks, potentially including analysis of the computational costs and benefits of scaling up model size for this specific task.

## Limitations

- The paper relies on the assumption that LLMs can effectively process and utilize additional structural information when provided in prompt format, which is not fully validated.
- The effectiveness of reverse logic incorporation is based on the premise that LLMs can learn to overcome the reversal curse, but concrete evidence of this learning process is lacking.
- Token limitations may impact the model's ability to process rich historical information, and specific beam search parameters and decoding strategy details are not provided.

## Confidence

- High confidence: The overall framework and methodology of using LLMs for TKGC, including the use of structure-augmented history modeling and reverse logic incorporation.
- Medium confidence: The effectiveness of the specific prompt strategies for reverse logic incorporation and the impact of varying history length on model performance.
- Low confidence: The internal processing of augmented histories by the LLM and the extent to which the reversal curse is actually mitigated.

## Next Checks

1. Conduct an ablation study to isolate the impact of each component (structure-augmented history, reverse logic incorporation, LoRA fine-tuning) on the model's performance.

2. Perform a detailed analysis of the LLM's internal representations when processing augmented histories to validate the assumption of effective utilization of additional structural information.

3. Investigate the impact of varying history length and the specific beam search parameters on the model's performance across different datasets and query types.