---
ver: rpa2
title: Quasimetric Value Functions with Dense Rewards
arxiv_id: '2409.08724'
source_url: https://arxiv.org/abs/2409.08724
tags:
- value
- rewards
- gcrl
- reward
- inequality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving sample efficiency
  in goal-conditioned reinforcement learning (GCRL) by leveraging dense reward functions.
  The key insight is that the optimal value function in GCRL has a quasimetric structure,
  which can be preserved even with dense rewards if they satisfy a specific condition.
---

# Quasimetric Value Functions with Dense Rewards

## Quick Facts
- arXiv ID: 2409.08724
- Source URL: https://arxiv.org/abs/2409.08724
- Reference count: 7
- Primary result: Dense rewards can improve sample efficiency in GCRL when designed to preserve the quasimetric property of value functions, with improvements seen in 4/12 benchmark tasks.

## Executive Summary
This paper addresses the challenge of improving sample efficiency in goal-conditioned reinforcement learning (GCRL) by leveraging dense reward functions. The key insight is that the optimal value function in GCRL has a quasimetric structure, which can be preserved even with dense rewards if they satisfy a specific condition. The authors introduce a progressive criterion for policies and show that on-policy value functions also satisfy the triangle inequality under this condition. Experiments on 12 benchmark GCRL tasks demonstrate that dense rewards improve sample complexity in 4 out of 12 environments without deteriorating performance in any task.

## Method Summary
The authors propose using potential-based reward shaping to provide dense rewards in GCRL while preserving the quasimetric property of value functions. The shaping function must be an admissible heuristic that underestimates the optimal value function by a bounded margin. They implement this using the Metric Residual Network (MRN) architecture with DDPG+HER as the base algorithm. The shaped reward is computed as R_total = R_sparse + F(s,a,s',a',g), where F is the potential-based shaping function. Output clipping is added to enforce bounds on the Q-values.

## Key Results
- Dense rewards improve sample complexity in 4 out of 12 benchmark GCRL tasks
- No task showed performance deterioration when using dense rewards
- The quasimetric property is preserved for both optimal and on-policy value functions under the proposed conditions

## Why This Works (Mechanism)

### Mechanism 1
Dense rewards preserve the triangle inequality for optimal value functions if shaped by potential functions that act as admissible heuristics. The shaping function ϕ(s,a,g) must underestimate the optimal value Q*(s,a,g) by a bounded margin, ensuring the shaped value function Q*_F = Q* - ϕ still satisfies the triangle inequality.

### Mechanism 2
On-policy value functions also satisfy the triangle inequality under progressive policies that make bounded progress toward the goal. The progress measure Δπ(s,a,g) = E[Qπ(s',a',g)] - Qπ(s,a,g) is bounded away from the optimal policy's progress by a finite ε.

### Mechanism 3
MRN architecture can directly accommodate dense rewards without modification because both Q* and Q*_F share the same upper bound of zero. Since Q*_F(s,a,g) = Q*(s,a,g) - ϕ(s,a,g) ≤ 0 by the admissible heuristic condition, the MRN output clipping rule Q ≤ 0 remains valid.

## Foundational Learning

- **Quasimetric property and triangle inequality**: Why needed - The paper's core insight is that dense rewards preserve quasimetric structure, so understanding what a quasimetric is and why triangle inequality matters is foundational. Quick check - Can you give an example of a quasimetric that is not a metric? What property distinguishes them?

- **Potential-based reward shaping**: Why needed - Dense rewards in this work are implemented via potential shaping, which must be admissible to preserve triangle inequality. Quick check - What is the difference between potential shaping and other dense reward methods like count-based or curiosity-driven?

- **Goal-conditioned MDP formulation**: Why needed - The setting extends standard RL to goals, requiring understanding of goal-conditioned value functions and hindsight relabeling. Quick check - How does the reward function R(s,a,g) differ from standard RL's R(s,a)? What role does hindsight experience replay play?

## Architecture Onboarding

- **Component map**: Environment -> MRN Critic -> Potential Shaping Module -> Policy Network -> Replay Buffer (with HER) -> Environment

- **Critical path**: 
  1. Sample transition (s,a,s',g) from replay buffer
  2. Compute shaped reward R(s,a,g) + F(s,a,s',a',g)
  3. Update critic via TD error minimization
  4. Update actor via policy gradient on Q-values
  5. Store new transition with HER relabeling

- **Design tradeoffs**: 
  - Potential shaping vs. other dense rewards: Shaping is policy-invariant but requires admissible heuristic design
  - η parameter: Controls action granularity; smaller η yields tighter lower bound but may require more exploration
  - Symmetric vs. asymmetric components: Symmetry improves sample efficiency but asymmetry captures directional costs

- **Failure signatures**: 
  - Performance worse than sparse rewards: Likely ϕ overestimates Q* somewhere, breaking triangle inequality
  - Slow learning: Potential shaping may be too conservative (ϕ too small) or η too large
  - Unstable training: Shaped rewards causing large TD errors; consider clipping or adjusting η

- **First 3 experiments**: 
  1. Verify triangle inequality holds for Q*_F on a simple gridworld with known optimal values
  2. Test different η values on FetchReach to find sweet spot balancing exploration and bias
  3. Compare MRN with shaped rewards against MRN with sparse rewards on BlockRotateXYZ to confirm no degradation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit in the work:
- How to design effective potential functions that preserve the quasimetric property while providing meaningful guidance
- The relationship between the quasimetric structure and exploration-exploitation tradeoffs in GCRL
- Extending the theoretical framework to more complex GCRL settings with stochastic dynamics or continuous action spaces

## Limitations
- Mixed empirical success with dense rewards improving sample complexity in only 4/12 benchmark tasks
- Limited exploration of alternative potential function designs and their impact on performance
- Theoretical framework primarily validated on deterministic environments

## Confidence
- **High Confidence**: The theoretical foundation linking quasimetric properties to value functions is mathematically rigorous and well-established.
- **Medium Confidence**: The empirical results demonstrating improved sample complexity in 4 out of 12 tasks are presented with appropriate statistical rigor (5 random seeds).
- **Low Confidence**: The generalizability of the approach to diverse GCRL environments beyond the tested benchmarks is uncertain without further validation.

## Next Checks
1. **Ablation Study on Shaping Function Design**: Conduct experiments varying the η parameter and testing alternative potential functions to determine the sensitivity of performance to these design choices.

2. **Extended Empirical Validation**: Apply the proposed method to a broader set of GCRL tasks, including environments with different characteristics (e.g., higher-dimensional state spaces, more complex dynamics) to assess generalizability.

3. **Theoretical Analysis of Failure Cases**: Investigate the specific conditions under which dense rewards fail to improve sample complexity, including analyzing the geometry of task spaces and identifying when the admissible heuristic condition is violated.