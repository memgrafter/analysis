---
ver: rpa2
title: 'DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic
  States'
arxiv_id: '2410.05982'
source_url: https://arxiv.org/abs/2410.05982
tags:
- queries
- state
- motion
- trajectory
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DeMo, a motion forecasting framework that decouples
  trajectory queries into directional intentions (mode queries) and dynamic states
  (state queries) to improve multi-modal trajectory prediction. The method introduces
  three specialized modules - Mode Localization, State Consistency, and Hybrid Coupling
  - that leverage combined Attention and Mamba techniques for effective information
  aggregation and sequence modeling.
---

# DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States

## Quick Facts
- arXiv ID: 2410.05982
- Source URL: https://arxiv.org/abs/2410.05982
- Reference count: 40
- Authors: Bozhou Zhang, Nan Song, Li Zhang
- Key outcome: DeMo achieves state-of-the-art performance on Argoverse 2 and nuScenes benchmarks with 13.02% and 11.83% relative gains in minFDE1 and minADE1 metrics respectively.

## Executive Summary
This paper proposes DeMo, a motion forecasting framework that decouples trajectory queries into directional intentions (mode queries) and dynamic states (state queries) to improve multi-modal trajectory prediction. The method introduces three specialized modules - Mode Localization, State Consistency, and Hybrid Coupling - that leverage combined Attention and Mamba techniques for effective information aggregation and sequence modeling. DeMo achieves state-of-the-art performance on both Argoverse 2 and nuScenes benchmarks while demonstrating superior efficiency with faster inference speed and smaller model size compared to existing approaches.

## Method Summary
DeMo is a motion forecasting framework that predicts future trajectories of traffic agents by decoupling queries into mode queries (directional intentions) and state queries (dynamic states). The method uses a hybrid architecture combining Attention and Mamba techniques across three specialized modules: Mode Localization captures distinct motion intentions, State Consistency maintains temporal coherence of dynamic states, and Hybrid Coupling generates final predictions. The framework is trained with auxiliary losses for intermediate features and employs a winner-take-all strategy for evaluation.

## Key Results
- Achieves 13.02% relative improvement in minFDE1 on Argoverse 2 compared to previous state-of-the-art
- Demonstrates 11.83% relative improvement in minADE1 on Argoverse 2 benchmark
- Shows superior efficiency with faster inference speed and smaller model size than existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoupling trajectory queries into mode queries (directional intentions) and state queries (dynamic states) improves multi-modal trajectory prediction accuracy.
- **Mechanism**: By separating the representation of directional intentions from dynamic state evolution, DeMo can independently optimize the multi-modality and temporal consistency of trajectories. Mode queries capture distinct motion intentions, while state queries track the agent's dynamic states across time steps.
- **Core assumption**: Directional intentions and dynamic state evolution are separable aspects of trajectory prediction that can be independently optimized.
- **Evidence anchors**:
  - [abstract]: "We introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time."
  - [section]: "To solve this problem, we propose a novel framework dubbed DeMo, which provides a detailed representation of multi-modal trajectories. Specifically, we decouple forecasting queries into two types: besides the original motion mode queries to capture different directional intentions as shown in Figure 1 (a), we introduce the dynamic state queries for future trajectories to track the agent's dynamic states across various time steps."
  - [corpus]: Found 25 related papers. No specific evidence for this decoupling mechanism.
- **Break condition**: If directional intentions and dynamic states are too tightly coupled for certain agents or scenarios, the decoupling may lose important interaction information.

### Mechanism 2
- **Claim**: The hybrid architecture combining Attention and Mamba techniques provides effective global information aggregation and efficient sequence modeling.
- **Mechanism**: Attention mechanisms capture global dependencies and interactions between queries and scene context, while Mamba provides efficient linear-time sequence modeling for the temporal consistency of dynamic states.
- **Core assumption**: The combination of global attention and efficient sequence modeling addresses both the need for comprehensive context understanding and the computational efficiency required for real-time applications.
- **Evidence anchors**:
  - [abstract]: "To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths."
  - [section]: "Due to the sequential nature of trajectory states, Mamba is particularly well-suited for modeling the temporal consistency of dynamic states. Therefore, we utilize a combination of Attention and Mamba in our modules to effectively and efficiently aggregate global information and model state sequences, leveraging the strengths of both techniques."
  - [corpus]: Found 25 related papers. No specific evidence for this hybrid Attention-Mamba combination.
- **Break condition**: If the computational efficiency gain from Mamba is offset by the overhead of combining with Attention, or if the hybrid architecture introduces training instability.

### Mechanism 3
- **Claim**: The use of auxiliary losses for intermediate features of time states and motion modes enhances the model's ability to learn distinct representations.
- **Mechanism**: Auxiliary losses provide additional supervision for the decoupled queries, ensuring that mode queries learn distinct directional intentions and state queries maintain temporal consistency.
- **Core assumption**: Additional supervision through auxiliary losses helps the model learn more discriminative and meaningful representations for the decoupled queries.
- **Evidence anchors**:
  - [abstract]: "Additionally, we introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths."
  - [section]: "Additionally, we introduce two auxiliary losses, Lts and Lm, for intermediate features of time states and motion modes, respectively. The former enhances the coherence and causality of dynamic states across various time steps, while the latter endows mode with distinct directional intentions."
  - [corpus]: Found 25 related papers. No specific evidence for this auxiliary loss approach.
- **Break condition**: If the auxiliary losses introduce too much regularization, causing the model to underfit or lose flexibility in capturing complex patterns.

## Foundational Learning

- **Concept**: Trajectory prediction in autonomous driving
  - **Why needed here**: Understanding the context and challenges of predicting future trajectories for traffic agents in dynamic environments is crucial for grasping the problem DeMo addresses.
  - **Quick check question**: What are the key challenges in motion forecasting for autonomous driving systems?

- **Concept**: Transformer architectures and attention mechanisms
  - **Why needed here**: DeMo builds upon transformer-based approaches and uses attention mechanisms for information aggregation and interaction modeling.
  - **Quick check question**: How do self-attention and cross-attention mechanisms work in transformer architectures?

- **Concept**: State space models and Mamba
  - **Why needed here**: DeMo incorporates Mamba for efficient sequence modeling, particularly for the temporal consistency of dynamic states.
  - **Quick check question**: What are the advantages of state space models like Mamba over traditional attention-based methods for sequence modeling?

## Architecture Onboarding

- **Component map**: Encoder (PointNet + Unidirectional Mamba + Transformer) -> Mode Localization (cross-attention + self-attention) -> State Consistency (cross-attention + bidirectional Mamba) -> Hybrid Coupling (cross-attention + hybrid self-attention + bidirectional Mamba)

- **Critical path**: The trajectory decoding pipeline (Mode Localization → State Consistency → Hybrid Coupling) is the critical path for generating final predictions.

- **Design tradeoffs**:
  - Decoupling queries increases model complexity but improves representation of directional intentions and dynamic states
  - Hybrid Attention-Mamba architecture balances global context understanding with efficient sequence modeling
  - Auxiliary losses provide additional supervision but increase training complexity

- **Failure signatures**:
  - Poor performance on scenarios where directional intentions and dynamic states are tightly coupled
  - Inefficiency in computation if the hybrid architecture introduces significant overhead
  - Overfitting or underfitting if auxiliary losses are not properly tuned

- **First 3 experiments**:
  1. Ablation study removing the decoupling of queries to measure the impact on performance
  2. Comparison of different sequence modeling approaches (e.g., pure Attention vs. Mamba) in the State Consistency Module
  3. Evaluation of the effect of different numbers of state queries on prediction accuracy and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of DeMo change when using a different number of state queries (Ts) relative to the number of future timestamps (Tf)?
- **Basis in paper**: [explicit] The paper mentions that Ts can differ from Tf to balance effectiveness and efficiency, and shows an ablation study with different numbers of state queries.
- **Why unresolved**: The paper doesn't provide a systematic study of how the ratio between Ts and Tf affects performance, especially for different prediction horizons or frequency requirements.
- **What evidence would resolve it**: A comprehensive ablation study varying the Ts/Tf ratio across different prediction scenarios, showing how performance metrics (minFDE, minADE) change with different ratios.

### Open Question 2
- **Question**: How does DeMo's performance change when using different types of information (e.g., visual cues, turn signals) to enhance directional intention modeling?
- **Basis in paper**: [inferred] The paper mentions that DeMo's failure cases include predicting straight trajectories when vehicles turn into alleys, suggesting the model could benefit from additional information about vehicle intentions.
- **Why unresolved**: The paper doesn't explore incorporating additional modalities beyond the current vectorized representation of HD maps and agent trajectories.
- **What evidence would resolve it**: Experiments comparing DeMo's performance with and without additional input modalities like visual imagery or turn signal information, showing improvements in specific challenging scenarios.

### Open Question 3
- **Question**: What is the impact of the depth configuration of Attention and Mamba layers on DeMo's performance across different datasets and agent types?
- **Basis in paper**: [explicit] The paper shows an ablation study on the depth of Attention and Mamba layers, finding that 3 Attention layers and 2 Mamba layers work best.
- **Why unresolved**: The paper only provides results for one dataset (Argoverse 2 single-agent) and doesn't explore how optimal layer depths might vary across different datasets, agent types, or prediction horizons.
- **What evidence would resolve it**: Systematic ablation studies across multiple datasets (Argoverse 1, nuScenes, WOMD) and different agent types (vehicles, pedestrians) showing how optimal layer depths vary by scenario.

### Open Question 4
- **Question**: How does DeMo's efficiency and performance scale with longer prediction horizons (e.g., 8s or 10s into the future)?
- **Basis in paper**: [inferred] The paper mentions that Ts can differ from Tf to balance effectiveness and efficiency, suggesting the model is designed to handle different prediction lengths, but only evaluates up to 6s predictions.
- **Why unresolved**: The paper only evaluates on datasets with prediction horizons up to 6s and doesn't explore how the model performs with significantly longer prediction requirements.
- **What evidence would resolve it**: Experiments evaluating DeMo on datasets or scenarios requiring longer prediction horizons (8s, 10s), showing changes in performance metrics and computational requirements.

### Open Question 5
- **Question**: How does DeMo's performance change when using different scene context encoding methods beyond the current PointNet-based polyline encoder and Unidirectional Mamba?
- **Basis in paper**: [explicit] The paper describes the current scene context encoding approach but doesn't explore alternatives.
- **Why unresolved**: The paper doesn't compare different scene context encoding methods or explore how alternative approaches might affect DeMo's performance.
- **What evidence would resolve it**: Experiments replacing the current encoding methods with alternatives (e.g., graph neural networks, different Mamba variants) and comparing performance metrics across multiple datasets.

## Limitations
- The decoupling approach may not work well for scenarios where directional intentions and dynamic states are tightly coupled
- Limited analysis of edge cases and failure modes where the decoupling assumption breaks down
- Performance claims rely primarily on standard benchmarks without extensive testing on challenging real-world scenarios

## Confidence
- **High confidence**: The core architectural innovations (decoupled queries, hybrid Attention-Mamba design) are well-specified and the empirical improvements on benchmarks are substantial and measurable
- **Medium confidence**: The claim that decoupling inherently improves multi-modality representation, as the paper lacks extensive ablation studies on alternative architectural choices
- **Medium confidence**: The efficiency claims, as the comparison focuses on final model size and inference speed without detailed computational complexity analysis

## Next Checks
1. Conduct ablation studies systematically removing the decoupling mechanism to quantify the specific contribution of this design choice versus other architectural improvements
2. Test the framework's performance on scenarios with high coupling between directional intentions and dynamic states (e.g., tight urban environments with complex multi-agent interactions)
3. Perform computational complexity analysis comparing the hybrid Attention-Mamba approach against pure Attention and pure Mamba baselines to validate efficiency claims