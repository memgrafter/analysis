---
ver: rpa2
title: Identifying Implicit Social Biases in Vision-Language Models
arxiv_id: '2411.00997'
source_url: https://arxiv.org/abs/2411.00997
tags:
- bias
- biases
- clip
- arxiv
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces So-B-IT, a comprehensive taxonomy of social
  biases covering ten categories and 374 words, to systematically evaluate biases
  in vision-language models like CLIP. The authors use So-B-IT to audit four CLIP
  models by measuring associations between biased words and demographic groups using
  image retrieval from FairFace.
---

# Identifying Implicit Social Biases in Vision-Language Models

## Quick Facts
- arXiv ID: 2411.00997
- Source URL: https://arxiv.org/abs/2411.00997
- Reference count: 29
- Primary result: Introduces So-B-IT taxonomy to audit vision-language models, finding significant gender and racial biases that persist or shift when debiasing one attribute.

## Executive Summary
This paper introduces So-B-IT, a comprehensive taxonomy of social biases covering ten categories and 374 words, to systematically evaluate biases in vision-language models like CLIP. The authors use So-B-IT to audit four CLIP models by measuring associations between biased words and demographic groups using image retrieval from FairFace. They find that CLIP models exhibit significant gender and racial biases across multiple categories, including occupation stereotypes and negative associations with minority groups. Notably, debiasing for one attribute (e.g., gender) can increase bias for others (e.g., race). The authors also identify that training data contains historical gender stereotypes, suggesting the need for fairness-aware dataset curation. Their findings highlight the importance of intersectional bias evaluation and transparent dataset practices for VL models.

## Method Summary
The authors develop So-B-IT, a taxonomy containing 374 words across ten bias categories (e.g., race, gender, physical appearance, social class). For each word, they generate captions and use CLIP to retrieve top-k images from FairFace, measuring demographic distributions to quantify bias via normalized entropy and C-ASC scores. They audit four CLIP variants and analyze LAION-400m training data to trace bias origins. The methodology combines word-association approaches with intersectional analysis to capture compounded stereotypes.

## Key Results
- CLIP models show significant gender and racial biases across occupation stereotypes and negative associations
- Debiasing for one attribute (e.g., gender) can increase bias for others (e.g., race)
- Training data analysis reveals historical gender stereotypes in LAION captions
- Intersectional biases exist (e.g., "homemaker" associated with Indian women specifically)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The So-B-IT taxonomy exposes biases by measuring overrepresentation of certain demographic groups in image retrieval for specific biased words.
- Mechanism: For each word, CLIP retrieves the top-k images from FairFace, and if one demographic group is disproportionately represented, this indicates a bias.
- Core assumption: CLIP's similarity metric directly reflects societal stereotypes encoded during training.
- Evidence anchors:
  - [abstract] "Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations..."
  - [section] "To identify biases in VL models, we employ a word-association approach that focuses on identifying biases based on a given adjective or word's association with individuals from a certain demographic group."
  - [corpus] Weak: No explicit anchor in corpus, but related work (VisBias) confirms the mechanism of using word-image association for bias measurement.
- Break condition: If CLIP's embeddings are heavily perturbed by adversarial examples, the retrieval results may not reflect true biases.

### Mechanism 2
- Claim: Debiasing for one attribute (e.g., gender) can unintentionally increase bias in another attribute (e.g., race).
- Mechanism: When a model is regularized to reduce gender disparity, it may overcompensate by leaning more heavily on remaining shortcuts like race.
- Core assumption: The model's learned shortcuts are not independent; mitigating one creates imbalance elsewhere.
- Evidence anchors:
  - [abstract] "Notably, debiasing for one attribute (e.g., gender) can increase bias for others (e.g., race)."
  - [section] "This extends prior work showing the propensity of vision models to lean more strongly on remaining shortcuts after debiasing (Li et al. 2023) to VL models."
  - [corpus] Weak: No direct corpus anchor, but related work (PRISM) on debiasing suggests similar unintended effects.
- Break condition: If debiasing method explicitly regularizes multiple attributes jointly, the unintended amplification may not occur.

### Mechanism 3
- Claim: The training data's demographic distribution directly influences the model's learned associations.
- Mechanism: If captions in LAION-400m disproportionately pair certain occupations with specific genders, CLIP will inherit these associations.
- Core assumption: CLIP's embeddings are a direct reflection of co-occurrence statistics in the training corpus.
- Evidence anchors:
  - [abstract] "Our investigation into training data associated with biased terms confirms the non-representative demographic distributions we identify experimentally."
  - [section] "We subset LAION-400m to samples with captions containing the word and a gendered pronoun... finding that gender stereotypes are clearly present in the LAION captions."
  - [corpus] Weak: No explicit corpus anchor, but related work (BiasDora) on training data auditing supports this mechanism.
- Break condition: If the model uses strong data augmentation or balancing strategies during training, the raw corpus statistics may be less predictive of learned biases.

## Foundational Learning

- Concept: Word embedding similarity and cosine distance in joint image-text space
  - Why needed here: The bias detection relies on measuring how close a word embedding is to image embeddings of different demographic groups.
  - Quick check question: If the cosine similarity between "nurse" and female face embeddings is 0.8, and with male face embeddings is 0.3, what does this imply about gender bias?

- Concept: Normalized entropy as a fairness metric
  - Why needed here: It quantifies how uniform the distribution of retrieved images is across demographic groups; lower entropy means more bias.
  - Quick check question: If the normalized entropy for a word is 0.2, is the model more or less biased than if it were 0.8?

- Concept: Intersectional bias analysis
  - Why needed here: Single-attribute bias audits can miss compounded stereotypes (e.g., "homemaker" associated with Indian women specifically).
  - Quick check question: Why might auditing gender bias alone miss the fact that "homemaker" is predominantly linked to Indian women in the model?

## Architecture Onboarding

- Component map:
  So-B-IT taxonomy -> CLIP model (text and image encoders) -> FairFace dataset -> Retrieval pipeline -> Bias measurement module

- Critical path:
  1. Load So-B-IT words
  2. For each word, generate caption
  3. Embed caption with CLIP text encoder
  4. Compute cosine similarity to all FairFace images
  5. Retrieve top-k images per word
  6. Compute demographic distribution
  7. Calculate normalized entropy or C-ASC score

- Design tradeoffs:
  - Using FairFace limits analysis to binary gender and 7 races, missing non-binary and other identities
  - Top-k retrieval is arbitrary; different k may yield different bias signals
  - Caption templates may not fully capture real-world usage contexts

- Failure signatures:
  - Uniform distributions across all words → taxonomy or retrieval pipeline broken
  - Extremely low entropy for all words → possible dataset imbalance or embedding collapse
  - High variance in entropy across similar words → potential noise in embedding space

- First 3 experiments:
  1. Verify that "nurse" retrieves more female images than male images across all CLIP variants
  2. Compare normalized entropy of "homemaker" across different racial groups to confirm intersectional bias
  3. Run the same pipeline on a debiased CLIP variant and confirm reduced gender bias but potentially increased racial bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the So-B-IT taxonomy need expansion to capture intersectional biases beyond gender and race?
- Basis in paper: [explicit] The authors note their taxonomy "is not exhaustive" and acknowledge "our choices are skewed towards a Western focus."
- Why unresolved: The current taxonomy focuses on 10 bias categories but may miss other demographic intersections (e.g., age, disability, socioeconomic status) that could be important for VL models.
- What evidence would resolve it: Empirical studies testing the taxonomy on diverse global datasets and populations, documenting any missing bias categories or demographic intersections.

### Open Question 2
- Question: How do debiasing methods for VL models need to evolve to address the "Whac-A-Mole" problem observed in this study?
- Basis in paper: [explicit] The authors find that "debiasing with respect to one attribute can significantly increase bias for other attributes" and reference prior work showing "correcting for one source of spurious correlation results in models leaning more heavily on other shortcuts."
- Why unresolved: Current debiasing approaches target individual attributes, but this creates compensatory biases elsewhere. The paper identifies the problem but doesn't propose solutions.
- What evidence would resolve it: Development and validation of intersectional debiasing methods that can simultaneously address multiple bias dimensions without creating new biases.

### Open Question 3
- Question: What is the relationship between dataset curation practices and bias mitigation in VL models?
- Basis in paper: [explicit] The authors find "disproportionate demographic representation may be a root cause of identified biases" and suggest "manual curation of a pre-training dataset without undesirable stereotypes may be required."
- Why unresolved: While the paper identifies training data as a source of bias, it doesn't quantify how different curation strategies affect final model bias or compare dataset-level versus model-level interventions.
- What evidence would resolve it: Comparative studies measuring bias levels in models trained on differently curated datasets, and ablation studies isolating the impact of specific curation decisions.

## Limitations
- FairFace dataset limits analysis to binary gender and 7 races, missing non-binary and other identities
- Top-k retrieval choice is arbitrary and may affect bias measurements
- So-B-IT taxonomy may not be exhaustive and has Western bias

## Confidence
- CLIP exhibits significant gender and racial biases: High
- Debiasing one attribute increases bias in others: High
- Intersectional bias claims: Medium (limited by FairFace demographics)
- Training data as root cause: Medium (correlation established but causation not proven)

## Next Checks
1. Replicate the analysis using a dataset with non-binary gender categories and more granular racial/ethnic classifications to validate intersectional bias findings.
2. Test the debiasing claims by applying explicit multi-attribute regularization during CLIP fine-tuning and measuring cross-attribute effects.
3. Conduct ablation studies using different top-k values and caption templates to assess sensitivity of bias measurements to methodological choices.