---
ver: rpa2
title: Cross-Self KV Cache Pruning for Efficient Vision-Language Inference
arxiv_id: '2412.04652'
source_url: https://arxiv.org/abs/2412.04652
tags:
- cache
- tokens
- attention
- tasks
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of KV cache management in
  vision-language models (VLMs) by introducing Cross-Self Pruning (CSP), a training-free
  method that decomposes attention scores into intra-modality and inter-modality components.
  CSP independently prunes tokens within each modality, preventing the over-pruning
  of critical visual tokens often caused by mixed attention distributions.
---

# Cross-Self KV Cache Pruning for Efficient Vision-Language Inference

## Quick Facts
- arXiv ID: 2412.04652
- Source URL: https://arxiv.org/abs/2412.04652
- Authors: Xiaohuan Pei; Tao Huang; Chang Xu
- Reference count: 40
- Performance improvement: Up to 41% on conversational embodied dialogue while reducing KV cache budget by up to 13.6%

## Executive Summary
This paper addresses the inefficiency of KV cache management in vision-language models (VLMs) by introducing Cross-Self Pruning (CSP), a training-free method that decomposes attention scores into intra-modality and inter-modality components. CSP independently prunes tokens within each modality, preventing the over-pruning of critical visual tokens often caused by mixed attention distributions. An n-softmax function is added to maintain attention score smoothness post-pruning. Evaluated on LLaVA-v1.5 models across 29 multimodal datasets in MileBench, CSP achieves significant performance improvements while reducing memory requirements.

## Method Summary
Cross-Self Pruning (CSP) is a training-free method that improves KV cache efficiency in VLMs by decomposing attention scores into four quadrants: self-attention between text tokens, self-attention between visual tokens, cross-attention from visual to text, and cross-attention from text to visual. Each quadrant is pruned independently using top-K selection based on summed query-axis attention scores. The method introduces an n-softmax function to smooth post-pruning attention distributions and prevent degradation in model performance. CSP combines the independently selected tokens using a conjunction operation and always includes recent tokens to ensure sufficient context for generation.

## Key Results
- Achieves up to 41% performance improvement on conversational embodied dialogue tasks
- Reduces KV cache budget by up to 13.6% while maintaining or improving performance
- Outperforms baseline methods including H2O, SnapKV, ReCo, and LOOK-M across 29 MileBench datasets
- Maintains distinct attention distributions for self-attention and cross-attention components

## Why This Works (Mechanism)

### Mechanism 1
Decomposing attention into intra-modality and inter-modality components allows independent pruning that preserves critical tokens from both modalities. The attention matrix is split into four quadrants (text-text, image-image, text-image, image-text), each pruned independently using top-K selection based on summed query-axis attention scores. This prevents visual tokens from being over-pruned due to mixed attention distributions.

### Mechanism 2
The n-softmax function restores smoothness to post-pruning attention distributions, preventing degradation in model performance. After pruning, attention scores become sharper because the softmax denominator only sums over remaining tokens. Adding constant n to the denominator smooths the distribution back toward its original shape.

### Mechanism 3
Independent top-K selection from cross and self regions, followed by conjunction, optimizes cache utilization while maintaining task performance. Top-K tokens are selected separately from intra-modality and inter-modality attention matrices, then combined using logical AND operation. This ensures only tokens important from both perspectives are retained, with recent tokens always added.

## Foundational Learning

- **Attention mechanism and self-attention scores**: Understanding how attention scores are computed and used for token importance is fundamental to grasping KV cache pruning. Quick check: How are attention scores calculated from query-key dot products in transformer models?

- **KV cache and its role in autoregressive generation**: The paper's optimization targets KV cache memory usage, so understanding cache mechanics is essential. Quick check: What is stored in KV cache and why is it reused across generation steps?

- **Multimodal data representation and modality differences**: The method exploits distributional differences between visual and text tokens. Quick check: Why might visual and text tokens have different attention score distributions in vision-language models?

## Architecture Onboarding

- **Component map**: Vision-language model with KV cache pruning module → attention score decomposition into four quadrants → independent top-K selection → n-softmax smoothing → conjunction combination → KV cache update with recent tokens

- **Critical path**: During generation, for each new token: compute attention scores → decompose into four quadrants → sum along query axis for each quadrant → select top-K tokens independently → combine selections with AND → concatenate with recent tokens → update KV cache → continue generation

- **Design tradeoffs**: Independent pruning preserves modality-specific information but increases complexity; n-softmax smooths distributions but adds hyperparameter n; conjunction ensures quality but may be too restrictive; recent tokens always included ensures context but limits pruning efficiency

- **Failure signatures**: Over-pruning causing generation errors; inconsistent performance across datasets; increased latency due to complex pruning logic; memory usage not reduced as expected

- **First 3 experiments**:
  1. Compare performance with and without modality decomposition on a small dataset to verify independent pruning helps
  2. Test different values of n in n-softmax to find optimal smoothing parameter
  3. Evaluate cache budget reduction vs. performance tradeoff curve to find acceptable pruning levels

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Cross-Self Pruning (CSP) vary with different n values in the n-softmax function, and what is the optimal value of n for different types of vision-language tasks? The paper sets n=1 in all experiments but doesn't explore the impact of varying n values across different tasks or modalities.

### Open Question 2
What is the impact of varying the observation window size (O) and the number of previous key tokens (R) on the efficiency and accuracy of CSP? The paper mentions these parameters but doesn't provide a detailed analysis of how different values affect performance.

### Open Question 3
How does CSP perform on vision-language models with different architectures, such as those with varying numbers of layers or attention heads? The paper evaluates on LLaVA-v1.5 models but doesn't explore performance on models with different architectural configurations.

## Limitations
- Relies on the assumption that visual and textual modalities have sufficiently distinct attention score distributions without statistical validation across all datasets
- Fixed n-softmax parameter (n=1) without systematic exploration of optimal values for different attention dynamics
- Conjunction-based selection may become overly restrictive in scenarios requiring dense cross-modal interactions
- Evaluation focuses on performance metrics without detailed analysis of how pruning affects cross-modal reasoning quality or generation diversity

## Confidence

**High Confidence**: The core mechanism of decomposing attention scores into intra-modality and inter-modality components for independent pruning is well-grounded in transformer attention theory and the empirical observation of distinct attention distributions.

**Medium Confidence**: The effectiveness of the n-softmax function in maintaining attention distribution smoothness and its contribution to overall performance improvements is reasonably supported, though the fixed parameter choice creates some uncertainty about optimality.

**Low Confidence**: The specific hyperparameter choices (K_s, K_v ratios, observation window size, n value) and their sensitivity to dataset characteristics are not thoroughly explored, making it difficult to assess robustness across diverse VLM applications.

## Next Checks

1. **Statistical Distribution Analysis**: Conduct Kolmogorov-Smirnov tests and visualize KDE plots across all 29 MileBench datasets to quantify the distributional separation between self-attention and cross-attention scores, and measure how this separation correlates with CSP performance gains.

2. **Ablation Study on n-softmax**: Systematically vary the n parameter from 0.1 to 10 in increments, measuring both performance degradation and attention distribution smoothness (using JS divergence metrics) to identify optimal smoothing ranges and quantify the contribution of n-softmax to overall effectiveness.

3. **Cross-Modal Reasoning Quality Assessment**: Design targeted evaluation tasks that specifically test cross-modal reasoning quality (e.g., requiring integration of visual and textual information) and measure whether CSP pruning selectively impacts these capabilities compared to unimodal tasks, using both automated metrics and human evaluation of generation quality.