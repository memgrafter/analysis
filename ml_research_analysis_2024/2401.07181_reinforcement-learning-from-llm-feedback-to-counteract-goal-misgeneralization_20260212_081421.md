---
ver: rpa2
title: Reinforcement Learning from LLM Feedback to Counteract Goal Misgeneralization
arxiv_id: '2401.07181'
source_url: https://arxiv.org/abs/2401.07181
tags:
- reward
- agent
- goal
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method to address goal misgeneralization
  in reinforcement learning agents using LLM feedback. The approach involves training
  an RL agent, having an LLM analyze its policies to identify potential failure scenarios,
  and then using the LLM's preferences to train a reward model.
---

# Reinforcement Learning from LLM Feedback to Counteract Goal Misgeneralization

## Quick Facts
- arXiv ID: 2401.07181
- Source URL: https://arxiv.org/abs/2401.07181
- Authors: Houda Nait El Barj; Theophile Sautory
- Reference count: 14
- Primary result: LLM-informed reward modeling improves RL goal generalization in maze navigation tasks

## Executive Summary
This paper addresses goal misgeneralization in reinforcement learning by leveraging large language models to provide feedback on agent policies. The approach uses an LLM to analyze RL agent behaviors, identify potential failure scenarios where the agent might optimize for proxy goals instead of true goals, and then train a reward model based on the LLM's preferences. This reward model is then used to further train the RL agent, creating a feedback loop that helps ensure the agent's learned policies align with intended objectives. The method is demonstrated on a maze navigation task where it shows significant improvements in goal generalization.

## Method Summary
The proposed method involves training an RL agent using standard algorithms, then having an LLM analyze the agent's policies to detect potential goal misgeneralization scenarios. The LLM provides preferences for different policy behaviors, which are used to train a reward model. This LLM-informed reward model is then used to further train the RL agent through additional reinforcement learning iterations. The approach creates a feedback loop where the LLM supervises the RL agent's learning process, helping to ensure that the agent learns to achieve the true goal rather than just optimizing for proxy rewards.

## Key Results
- The method achieves marked improvements in goal generalization on maze navigation tasks
- Performance gains are particularly pronounced when true and proxy goals are distinguishable
- The LLM effectively supervises the RL agent despite not being proficient at the task itself

## Why This Works (Mechanism)
The LLM provides a form of interpretable, high-level reasoning about agent behaviors that traditional reward functions lack. By analyzing policies from a conceptual standpoint, the LLM can identify cases where an agent might achieve high reward through unintended means. This external supervision helps prevent the RL agent from settling into local optima that satisfy proxy objectives but fail to achieve the true goal.

## Foundational Learning
- **Reinforcement Learning**: Why needed - to train agents that can learn optimal behaviors through trial and error; Quick check - agent should improve performance over training episodes
- **Reward Modeling**: Why needed - to provide feedback signals that guide agent learning; Quick check - reward model should predict human preferences accurately
- **Goal Misgeneralization**: Why needed - to understand failure modes where agents optimize for wrong objectives; Quick check - agent should fail when proxy and true goals diverge
- **LLM-based Supervision**: Why needed - to provide high-level reasoning about agent behaviors; Quick check - LLM should identify policy failures that humans would notice

## Architecture Onboarding

**Component Map:**
RL Agent -> Policy Analysis -> LLM Feedback -> Reward Model -> RL Agent

**Critical Path:**
1. Train RL agent to convergence
2. Generate policy rollouts for LLM analysis
3. Collect LLM preferences on policy behaviors
4. Train reward model on LLM preferences
5. Use reward model to further train RL agent

**Design Tradeoffs:**
- Computational cost vs. generalization performance (LLM analysis adds overhead but improves goal alignment)
- LLM capability vs. supervision quality (more capable LLMs may provide better feedback but at higher cost)
- Frequency of LLM analysis vs. training efficiency (more frequent analysis provides better supervision but slows training)

**Failure Signatures:**
- Reward model overfitting to LLM preferences without generalizing
- LLM failing to identify subtle goal misgeneralization cases
- RL agent learning to exploit LLM feedback patterns rather than improving goal achievement

**3 First Experiments:**
1. Compare performance with and without LLM feedback on maze navigation task
2. Test different frequencies of LLM policy analysis during training
3. Evaluate robustness by varying the distinguishability between true and proxy goals

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single maze navigation task with synthetic failure modes
- LLM's ability to detect failures in complex, real-world scenarios remains unknown
- Computational overhead of requiring LLM analysis of every policy update may be prohibitive

## Confidence

**High confidence**: The core methodology is technically sound and the experimental results on the maze task are reproducible

**Medium confidence**: The LLM can effectively supervise RL agents in simple environments where true and proxy goals are clearly distinguishable

**Low confidence**: The approach will generalize to complex environments with ambiguous goals or where the LLM lacks sufficient context to identify failures

## Next Checks

1. Test the method on multiple diverse environments (Atari, MuJoCo, or real-world robotics tasks) to assess generalizability beyond simple maze navigation

2. Conduct ablation studies comparing performance when using different LLM sizes or prompting strategies to understand sensitivity to LLM choice

3. Measure the computational overhead in terms of wall-clock time and GPU usage to evaluate practical deployment feasibility