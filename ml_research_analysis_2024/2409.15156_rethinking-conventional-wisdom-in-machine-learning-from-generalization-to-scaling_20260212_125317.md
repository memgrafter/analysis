---
ver: rpa2
title: 'Rethinking Conventional Wisdom in Machine Learning: From Generalization to
  Scaling'
arxiv_id: '2409.15156'
source_url: https://arxiv.org/abs/2409.15156
tags:
- learning
- scaling
- arxiv
- uni00000014
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a paradigm shift in machine learning from
  a generalization-centric approach (minimizing generalization error via regularization)
  to a scaling-centric approach (minimizing approximation error via model scaling).
  Through empirical analysis, it demonstrates that traditional regularization techniques
  like explicit L2 regularization and implicit regularization from large learning
  rates and small batch sizes may not be necessary or beneficial for large language
  model pretraining.
---

# Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling

## Quick Facts
- arXiv ID: 2409.15156
- Source URL: https://arxiv.org/abs/2409.15156
- Authors: Lechao Xiao
- Reference count: 14
- Key outcome: Traditional regularization techniques become unnecessary in large-scale language model pretraining as models shift from generalization-centric to scaling-centric regimes

## Executive Summary
This paper identifies a fundamental paradigm shift in machine learning from a generalization-centric approach (minimizing generalization error via regularization) to a scaling-centric approach (minimizing approximation error via model scaling). Through empirical analysis, it demonstrates that traditional regularization techniques like explicit L2 regularization and implicit regularization from large learning rates and small batch sizes may not be necessary or beneficial for large language model pretraining. The paper introduces the "scaling law crossover" phenomenon, where techniques effective at smaller scales can become detrimental at larger scales, complicating model comparison. This raises fundamental questions about emerging guiding principles for scaling and how to reliably compare models when only single experiments are feasible at large scales.

## Method Summary
The study investigates scaling crossover in transformer language models by comparing constant learning rate (LR=2/1024) versus scaled learning rate (LR=2/D) strategies across model sizes from 151K to 48M parameters. Experiments use C4 dataset with Chinchilla-optimal scaling, decoder-only transformers with Rotary Positional Embedding, QK-Norm, GeLU activation, and AdamW optimizer. Models are trained with varying dimensions (D,F)=(128k,512k) for kâˆˆ[1,2,3,4,6,8,12,16,20,24,32,40,48], constant batch size 256, and 6 layers. The study monitors training stability, evaluation loss, and identifies crossover points where LR=2/D becomes superior to constant learning rate.

## Key Results
- Traditional L2 regularization provides minimal benefits for large language model pretraining in the "skydiving regime"
- Scaling law crossover occurs where techniques effective at smaller scales (constant learning rate) become detrimental at larger scales (requires learning rate scaling)
- The paradigm shift moves from generalization-centric (reducing generalization gap) to scaling-centric (reducing approximation error through compute and model scaling)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling laws show that approximation error decreases predictably with increased compute and model size
- Mechanism: When model capacity and data size scale proportionally, the model can better approximate the true underlying function, reducing approximation error
- Core assumption: The relationship between compute, model size, and data size follows a predictable power law
- Evidence anchors:
  - [abstract] "the discovery of scaling laws signify a paradigm shift in machine learning"
  - [section 1] "Existing work shows, empirically, the better the model memorizes the data (smaller approximation error), the more powerful the (foundational) model is"
  - [corpus] Weak evidence - corpus neighbors don't directly