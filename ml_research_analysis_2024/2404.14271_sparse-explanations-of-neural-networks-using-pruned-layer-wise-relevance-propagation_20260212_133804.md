---
ver: rpa2
title: Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance Propagation
arxiv_id: '2404.14271'
source_url: https://arxiv.org/abs/2404.14271
tags:
- relevance
- explanations
- propagation
- sparsity
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Pruned Layer-Wise Relevance Propagation (PLRP),
  a method that enforces sparsity in neural network explanations by pruning low-relevance
  neurons during the relevance propagation process. This approach is designed to reduce
  noise and highlight the most important features in explanations, particularly useful
  for complex data like genome sequences where distinguishing relevant features is
  challenging.
---

# Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance Propagation

## Quick Facts
- arXiv ID: 2404.14271
- Source URL: https://arxiv.org/abs/2404.14271
- Authors: Paulo Yanez Sarmiento; Simon Witzke; Nadja Klein; Bernhard Y. Renard
- Reference count: 7
- Key outcome: PLRP achieves sparser explanations with improved localization accuracy while maintaining robustness, with PLRP-λ performing better than PLRP-M

## Executive Summary
This paper introduces Pruned Layer-Wise Relevance Propagation (PLRP), a method that enforces sparsity in neural network explanations by pruning low-relevance neurons during backward relevance propagation. The approach is designed to reduce noise and highlight the most important features in explanations, particularly for complex data like genome sequences. PLRP extends traditional LRP by using relevance scores themselves as pruning criteria, allowing for input-specific pruning that preserves local interpretability.

The method introduces two variants for redistributing pruned relevance: PLRP-λ (simple rescaling) and PLRP-M (reapplying LRP with zero activations). Evaluation on both image and genome sequence data demonstrates that PLRP achieves sparser explanations, increases localization accuracy, and maintains robustness, with only a slight decrease in faithfulness. The approach shows particular promise for applications where distinguishing relevant features from noise is challenging.

## Method Summary
PLRP modifies Layer-wise Relevance Propagation by pruning neurons with low relevance scores during backward propagation and redistributing their relevance to remaining neurons. At each layer, neurons whose relevance falls below a threshold are pruned, with their relevance redistributed either by rescaling (PLRP-λ) or by reapplying LRP with zero activations for pruned neurons (PLRP-M). The pruning is input-specific, allowing different neurons to be pruned for different inputs, which preserves the local nature of explanations. The method introduces two variants: PLRP-λ uses simple scaling to maintain conservation, while PLRP-M recomputes relevance distribution. Evaluation is conducted on VGG-16 and ResNet-50 models for image data, and CNN models with 4 and 32 filters for genome data, using metrics including Gini Index, entropy, Relevance Mass Accuracy, local Lipschitz constant, and faithfulness AUC.

## Key Results
- PLRP achieves significantly sparser explanations as measured by Gini Index and entropy metrics
- Localization accuracy (RMA) improves compared to baseline LRP across both image and genome datasets
- PLRP-λ outperforms PLRP-M in achieving sparsity and relevance concentration
- The method maintains robustness while showing only a slight decrease in faithfulness

## Why This Works (Mechanism)

### Mechanism 1
PLRP achieves sparsity by pruning neurons with low relevance scores during backward propagation. At each layer, neurons whose relevance falls below a threshold are pruned, with their relevance redistributed to remaining neurons either by rescaling (PLRP-λ) or by reapplying LRP with zero activations (PLRP-M). This input-specific pruning preserves local interpretability by adapting to different input patterns.

### Mechanism 2
PLRP-λ maintains conservation while concentrating relevance through rescaling. After pruning, the remaining relevance is multiplied by a factor λ that ensures the total relevance mass equals the original mass, preserving the LRP conservation property. This simple scaling approach tends to achieve higher sparsity than the alternative redistribution method.

### Mechanism 3
Using relevance-based pruning rather than model pruning preserves local explainability. By pruning relevance propagation per input sample, different neurons can be pruned for different inputs, aligning with the local nature of explanations. This approach recognizes that different inputs activate different neurons, so global pruning would lose input-specific interpretability.

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP)
  - Why needed here: PLRP is a modification of LRP; understanding LRP is essential to grasp how relevance flows backward through layers
  - Quick check question: What property ensures that the total relevance mass is preserved during backward propagation in LRP?

- Concept: Relevance conservation
  - Why needed here: PLRP must redistribute pruned relevance while maintaining conservation; otherwise, the explanation loses interpretability
  - Quick check question: How does PLRP-λ preserve the total relevance mass after pruning?

- Concept: Gini Index and entropy for sparsity measurement
  - Why needed here: These metrics quantify the sparsity of explanations; understanding them is necessary to evaluate PLRP's performance
  - Quick check question: Which metric (Gini or entropy) would indicate higher sparsity when its value increases?

## Architecture Onboarding

- Component map: Input -> Forward pass -> LRP relevance computation -> PLRP pruning and redistribution -> Metrics evaluation -> Visualization

- Critical path:
  1. Forward pass to get prediction and activations
  2. Backward relevance propagation via LRP
  3. PLRP pruning and redistribution per layer
  4. Compute metrics and generate visualizations

- Design tradeoffs:
  - PLRP-λ vs PLRP-M: Simpler scaling vs LRP-consistent redistribution; PLRP-λ tends to achieve higher sparsity but may flip relevance signs
  - Sparsity gain vs fixed p: Dynamic pruning adapts to input but adds complexity; fixed p is simpler but may be suboptimal per sample

- Failure signatures:
  - Over-pruning: Loss of important features, reduced faithfulness
  - Under-pruning: High noise, low sparsity
  - Sign flipping: In PLRP-M

- First experiments:
  1. Apply PLRP to a simple CNN on MNIST to verify basic functionality
  2. Compare PLRP-λ and PLRP-M on a small image dataset with ground truth segmentation
  3. Test different pruning thresholds on synthetic genome data to observe sparsity-faithfulness tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal parameterization strategy for PLRP across different tasks and datasets? The paper acknowledges that choosing optimal parameters for PLRP is an open question and depends on the underlying model and the "optimal" degree of sparsity for a specific task. This remains unresolved because the paper does not provide a systematic approach to determine it. Systematic experiments across diverse datasets and models showing how different parameterizations affect performance, along with a proposed methodology for selecting optimal parameters, would resolve this question.

### Open Question 2
How can the relevance sign flipping issue in PLRP-M be mitigated or eliminated? The paper notes that PLRP-M can lead to sign flipping of a neuron's relevance due to changes in the proportions of relevance attribution, which may affect its performance compared to PLRP-λ. This is unresolved because the paper identifies this as a potential issue but does not explore solutions or modifications to address it. Experimental results demonstrating improved performance of PLRP-M after implementing modifications to prevent sign flipping, or theoretical analysis explaining how to design a redistribution method that avoids this issue, would resolve this question.

### Open Question 3
Can the sparsity-enforcing concept of PLRP be effectively applied to other explanation methods beyond LRP? The paper suggests that the general idea of enforcing sparsity when attributing relevance is not specific to LRP and can be applied to other explanation methods. This remains unresolved because while the paper proposes this as a possibility, it does not test or validate the application of PLRP's concepts to other methods. Successful implementation and evaluation of sparsity-enforcing modifications to other explanation methods (e.g., SHAP, Integrated Gradients) showing comparable or improved performance would resolve this question.

## Limitations
- The method relies on input-specific pruning thresholds that may require careful tuning for different datasets and architectures
- The sign-flipping issue observed in PLRP-M suggests potential instability in relevance redistribution
- The evaluation focuses primarily on synthetic genome data and standard image datasets, with limited testing on real-world genomic applications

## Confidence

- **High Confidence**: The sparsity improvements and localization accuracy gains are well-supported by quantitative metrics across multiple datasets
- **Medium Confidence**: The faithfulness preservation claim is supported but shows a slight decrease that may be significant for certain applications
- **Medium Confidence**: The mechanism explanations are theoretically sound but lack direct empirical validation of the pruning decisions

## Next Checks

1. Test PLRP on additional real-world genomic datasets to verify the method's effectiveness beyond synthetic sequences
2. Conduct ablation studies to isolate the impact of pruning thresholds versus redistribution methods on explanation quality
3. Measure computational overhead compared to standard LRP to assess practical deployment implications