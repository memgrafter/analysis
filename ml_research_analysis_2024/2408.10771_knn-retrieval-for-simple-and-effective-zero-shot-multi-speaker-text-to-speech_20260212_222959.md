---
ver: rpa2
title: kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech
arxiv_id: '2408.10771'
source_url: https://arxiv.org/abs/2408.10771
tags:
- speaker
- speech
- features
- target
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces kNN-TTS, a zero-shot multi-speaker text-to-speech
  framework that uses k-nearest neighbors retrieval with self-supervised learning
  features to modify target voices. Unlike conventional methods that rely on speaker
  embeddings, kNN-TTS leverages the linear relationships between SSL features to achieve
  voice conversion while preserving phonetic information.
---

# kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech

## Quick Facts
- arXiv ID: 2408.10771
- Source URL: https://arxiv.org/abs/2408.10771
- Authors: Karl El Hajal; Ajinkya Kulkarni; Enno Hermann; Mathew Magimai. -Doss
- Reference count: 9
- One-line primary result: kNN-TTS achieves performance comparable to state-of-the-art models using k-nearest neighbors retrieval with self-supervised learning features for zero-shot voice conversion

## Executive Summary
This paper introduces kNN-TTS, a zero-shot multi-speaker text-to-speech framework that leverages k-nearest neighbors retrieval with self-supervised learning (SSL) features to achieve voice conversion without speaker embeddings. Unlike conventional approaches that require speaker embedding training, kNN-TTS exploits the linear relationships between SSL features from different speakers to modify target voices while preserving phonetic information. The framework is trained on transcribed speech from a single speaker and achieves performance comparable to state-of-the-art models trained on much larger datasets.

The key innovation is using SSL features' speaker-dependent linear relationships to enable retrieval-based voice conversion. The kNN algorithm matches source speaker features to target speaker frames using cosine distance, and an interpolation parameter λ enables fine-grained voice morphing capabilities. The approach demonstrates that approximately 30 seconds of reference utterances are needed for suitable intelligibility, with speaker similarity plateauing at around 1 minute of reference audio.

## Method Summary
kNN-TTS uses a two-stage approach: first training a Text-to-SSL model on single-speaker transcribed speech, then using kNN retrieval to match source features to target speaker units. The framework extracts SSL features from target speaker utterances using a pre-trained WavLM-Large encoder, applies kNN retrieval with cosine distance to find matching frames, linearly interpolates source and target features using parameter λ for voice morphing, and decodes the converted features into speech using a pre-trained HiFi-GAN vocoder. The system achieves zero-shot multi-speaker synthesis by leveraging pre-trained SSL representations and kNN retrieval rather than training speaker-specific embeddings.

## Key Results
- Speaker similarity scores (SECS) of 0.72-0.72 on VCTK and LibriTTS datasets
- Word error rate (WER) of 3.71-4.32 across different reference audio durations
- UTMOS naturalness scores of 4.02-4.16, comparable to state-of-the-art models
- Requires only 30 seconds to 5 minutes of reference audio for target speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear relationships in SSL features allow retrieval-based voice conversion while preserving phonetic content.
- Mechanism: SSL features from different speakers that are linearly close share phonetic information while maintaining speaker identity. kNN retrieval finds target speaker frames similar in phonetic space to source frames, enabling voice substitution without phonetic distortion.
- Core assumption: SSL features exhibit speaker-dependent linear relationships that preserve phonetic structure when interpolated.
- Evidence anchors:
  - [abstract]: "SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity."
  - [section]: "kNN Retrieval: To synthesize speech in a target speaker's voice, units (or frames) from the target speaker unit database are selected to replace corresponding frames from the source speaker features."
  - [corpus]: Weak evidence - corpus provides related TTS papers but no direct SSL feature relationship studies.
- Break condition: If SSL features don't maintain linear relationships across speakers, or if interpolation destroys phonetic information.

### Mechanism 2
- Claim: kNN retrieval with cosine distance effectively selects target speaker frames matching source phonetic content.
- Mechanism: For each source frame, compute cosine distance to all target frames, select k nearest neighbors, average them. This preserves phonetic content while replacing speaker characteristics.
- Core assumption: Cosine distance in SSL feature space correlates with phonetic similarity across speakers.
- Evidence anchors:
  - [section]: "For each source frame, we compute its cosine distance with every target speaker frame within the unit database."
  - [abstract]: "kNN retrieval algorithm then matches these generated features to units in a target speaker's unit database."
  - [corpus]: No direct evidence about cosine distance effectiveness for TTS applications.
- Break condition: If cosine distance doesn't capture phonetic similarity, or if averaging k nearest neighbors introduces artifacts.

### Mechanism 3
- Claim: Interpolation parameter λ enables fine-grained voice morphing by blending source and target styles.
- Mechanism: Linear interpolation between source and selected target features using λ controls the degree of speaker identity transfer, creating voice morphing capabilities.
- Core assumption: Linear interpolation in SSL feature space produces perceptually meaningful voice blending.
- Evidence anchors:
  - [abstract]: "We also introduce an interpolation parameter which enables fine-grained voice morphing."
  - [section]: "The source and target speaker features are then linearly interpolated to obtain the converted features (Khandelwal et al., 2020)."
  - [corpus]: Weak evidence - corpus contains related voice cloning papers but no specific interpolation parameter studies.
- Break condition: If interpolation doesn't produce perceptually smooth transitions, or if λ scaling doesn't match perceptual blending.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL) speech representations
  - Why needed here: SSL features provide intermediate representations that capture both phonetic and speaker information, enabling retrieval-based voice conversion without speaker embeddings.
  - Quick check question: Why do SSL features work better than spectral features (like mel-spectrograms) for this application?

- Concept: k-Nearest Neighbors (kNN) retrieval
  - Why needed here: kNN enables frame-by-frame matching between source and target speaker features, allowing voice conversion while preserving phonetic content.
  - Quick check question: How does the choice of k (here k=4) affect the quality of retrieved target frames?

- Concept: Linear interpolation for style transfer
  - Why needed here: Linear interpolation between source and target features enables fine-grained control over voice morphing, allowing gradual transition between speaker identities.
  - Quick check question: What perceptual effects occur when λ varies from 0 to 1 in the interpolation?

## Architecture Onboarding

- Component map:
  Text input -> Text-to-SSL model -> SSL features -> kNN retrieval -> Target speaker frames -> Linear interpolation with source -> Converted features -> Vocoder -> Speech output

- Critical path:
  1. Text input → Text-to-SSL model → SSL features
  2. SSL features → kNN retrieval → Target speaker frames
  3. Target frames → Linear interpolation with source → Converted features
  4. Converted features → Vocoder → Speech output

- Design tradeoffs:
  - Single-speaker training vs. multi-speaker training: Simpler training but requires more reference audio
  - kNN vs. parametric methods: No speaker embedding training but higher reference audio requirements
  - Interpolation control vs. fixed conversion: More flexibility but added complexity

- Failure signatures:
  - Poor intelligibility: Incorrect kNN matching or inadequate reference audio duration
  - Unnatural prosody: Text-to-SSL model not capturing speaker-specific rhythm
  - Low speaker similarity: Insufficient target speaker data or poor SSL feature quality

- First 3 experiments:
  1. Verify SSL feature quality: Compare mel-spectrogram vs. SSL feature intelligibility on single speaker
  2. Test kNN retrieval: Synthesize speech using varying k values (1, 4, 8) and measure speaker similarity
  3. Evaluate interpolation: Generate speech with λ=0, 0.5, 1.0 and measure voice morphing effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of kNN-TTS scale with different amounts of target speaker reference data, and is there an optimal amount beyond which additional data provides diminishing returns?
- Basis in paper: Explicit - The paper discusses that approximately 30 seconds of reference utterances are needed for suitable intelligibility, while similarity plateaus at around 1 minute, and compares kNN-TTS to baselines for different amounts of reference utterances.
- Why unresolved: The paper provides a general overview of the scaling relationship but doesn't provide a detailed analysis of the optimal amount of reference data or how performance scales beyond the tested ranges.
- What evidence would resolve it: A comprehensive ablation study testing kNN-TTS with varying amounts of target speaker reference data (e.g., 5s, 10s, 30s, 1min, 2min, 5min, 10min) and comparing the results to determine the optimal amount and the point of diminishing returns.

### Open Question 2
- Question: Can the kNN-TTS framework be extended to handle rhythmic variations between speakers, such as different speaking rates or prosody patterns?
- Basis in paper: Explicit - The paper mentions that different speakers exhibit different pronunciation durations and that the duration aspect is determined by the Text-to-SSL model, leaving rhythmic variations as a limitation.
- Why unresolved: The paper acknowledges this as a limitation but doesn't explore potential solutions or modifications to the framework to address rhythmic variations.
- What evidence would resolve it: Experiments integrating techniques like Urhythmic (van Niekerk et al., 2023) into the kNN-TTS framework and comparing the results to the baseline kNN-TTS model to assess improvements in handling rhythmic variations.

### Open Question 3
- Question: How does the choice of Text-to-SSL architecture (GlowTTS vs. GradTTS) impact the overall performance and efficiency of the kNN-TTS framework?
- Basis in paper: Explicit - The paper implements and compares two different Text-to-SSL architectures (GlowTTS and GradTTS) within the kNN-TTS framework, noting differences in memory usage and runtime speed.
- Why unresolved: While the paper presents results comparing the two architectures, it doesn't provide a detailed analysis of why one architecture might be preferred over the other or explore other potential Text-to-SSL architectures.
- What evidence would resolve it: A more comprehensive comparison of different Text-to-SSL architectures within the kNN-TTS framework, including ablation studies and analyses of the trade-offs between performance, efficiency, and other factors.

### Open Question 4
- Question: What are the potential benefits and challenges of extending the kNN-TTS framework to new languages with limited resources?
- Basis in paper: Explicit - The paper mentions that the kNN approach's cross-lingual capability and the framework's low training data requirements make it appealing for extending the model to new languages with less resources, but doesn't explore this direction.
- Why unresolved: The paper identifies this as a potential direction for future work but doesn't investigate the specific challenges or benefits of applying the framework to low-resource languages.
- What evidence would resolve it: Experiments applying the kNN-TTS framework to low-resource languages, comparing the results to state-of-the-art models in those languages, and analyzing the challenges and benefits encountered during the process.

## Limitations

- Reference audio requirements: The method requires 30 seconds to 5 minutes of target speaker reference audio, which is significantly more than conventional zero-shot TTS methods and affects the "zero-shot" characterization.
- SSL feature quality assumptions: The core mechanism relies on SSL features exhibiting linear relationships across speakers that preserve phonetic information, but this property lacks thorough empirical validation.
- Objective metric interpretation: The reported speaker similarity scores (0.72-0.72) show a narrow range that suggests potential measurement issues or insufficient differentiation between conditions.

## Confidence

**High Confidence**: The kNN retrieval algorithm works as described, linear interpolation between features is implemented as stated, and the overall system architecture is coherent and implementable.

**Medium Confidence**: SSL features preserve phonetic information while enabling speaker conversion, the interpolation parameter provides meaningful voice morphing control, and performance is comparable to state-of-the-art models.

**Low Confidence**: The "zero-shot" characterization given reference audio requirements, the necessity of SSL features over other representations, and the claimed computational efficiency versus parametric methods.

## Next Checks

1. **SSL Feature Property Validation**: Conduct controlled experiments comparing SSL features against mel-spectrograms and other intermediate representations for the same kNN retrieval approach. Measure both phonetic preservation (using WER/PER) and speaker similarity to determine if SSL features provide unique advantages.

2. **Reference Duration Sensitivity Analysis**: Systematically evaluate system performance across different reference durations (5s, 15s, 30s, 60s, 300s) for target speakers. Measure the trade-off between reference duration and both speaker similarity and intelligibility to quantify the "zero-shot" requirements.

3. **Cross-Dataset Generalization Test**: Evaluate the trained model on unseen datasets beyond VCTK and LibriTTS, including languages other than English if possible. This would validate the claimed generalization capability and reveal potential dataset-specific optimizations.