---
ver: rpa2
title: 'JDocQA: Japanese Document Question Answering Dataset for Generative Language
  Models'
arxiv_id: '2403.19454'
source_url: https://arxiv.org/abs/2403.19454
tags:
- question
- questions
- documents
- japanese
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces JDocQA, a large-scale Japanese document question
  answering dataset designed to evaluate the capabilities of generative language models.
  The dataset comprises 5,504 PDF documents and 11,600 question-answer pairs, requiring
  both visual and textual information to answer questions.
---

# JDocQA: Japanese Document Question Answering Dataset for Generative Language Models

## Quick Facts
- **arXiv ID**: 2403.19454
- **Source URL**: https://arxiv.org/abs/2403.19454
- **Reference count**: 0
- **Primary result**: JDocQA is a large-scale Japanese document QA dataset with 5,504 PDF documents and 11,600 question-answer pairs that evaluates generative language models' ability to process both visual and textual information.

## Executive Summary
JDocQA is a novel dataset designed to evaluate Japanese document question answering systems that require both visual and textual understanding. The dataset contains 5,504 PDF documents spanning slides, reports, pamphlets, and websites, paired with 11,600 question-answer pairs across four categories: yes/no, factoid, numerical, and open-ended questions. A key innovation is the inclusion of unanswerable questions to mitigate model hallucination. The authors evaluate the dataset using both text-only and multimodal models, demonstrating that multimodal models with visual inputs, particularly cropped images of referenced tables or figures, outperform text-only models.

## Method Summary
The dataset was constructed through a multi-stage process: Japanese PDF documents were collected from open-access sources including NDL digital collection, WARP, and government websites; text was extracted using PyPDF2 and OCR for non-extractable documents with normalization to remove erroneous characters; and annotators created question-answer pairs ensuring questions referenced both visual and textual elements while including unanswerable questions. The evaluation employed supervised fine-tuning of Japanese language models with both text-only and multimodal variants, comparing models trained with and without unanswerable questions. Performance was measured using exact match for categorical questions and BLEU scores for open-ended responses.

## Key Results
- Multimodal models with visual inputs, especially cropped images of referenced tables or figures, outperform text-only models
- Incorporating unanswerable questions during fine-tuning improves model performance and reduces hallucination
- The rinna bi-4B-8k model's performance is affected by token length limitations in document processing

## Why This Works (Mechanism)
The dataset's effectiveness stems from its realistic representation of document QA challenges where answers require integrating information from both text and visual elements. The inclusion of unanswerable questions directly addresses the hallucination problem common in generative models by training them to recognize when sufficient information is not present. The multimodal approach leverages visual context that text extraction alone might miss, particularly for tabular data and diagrams that are common in business and academic documents.

## Foundational Learning
- **Document structure understanding**: Why needed - Documents contain hierarchical information requiring navigation of sections and visual elements. Quick check - Can models identify relevant sections based on visual layout cues?
- **Visual-textual alignment**: Why needed - Questions often reference specific visual elements like tables or figures. Quick check - Do models correctly associate questions with referenced visual components?
- **Unanswerable question detection**: Why needed - Prevents hallucination by training models to recognize insufficient information. Quick check - Can models distinguish between answerable and unanswerable questions at test time?
- **Multimodal integration**: Why needed - Combines complementary information from text and images for comprehensive understanding. Quick check - Does adding visual inputs improve performance on questions requiring numerical or tabular data?
- **Japanese language processing**: Why needed - Dataset focuses on Japanese documents with specific linguistic and character considerations. Quick check - Are models handling Japanese-specific text extraction and normalization correctly?
- **Token length management**: Why needed - Documents must be truncated to fit model constraints while preserving relevant context. Quick check - Does truncation strategy preserve information needed for question answering?

## Architecture Onboarding

**Component Map**: Document Collection -> Text Extraction/OCR -> Question Annotation -> Model Fine-tuning -> Evaluation

**Critical Path**: The most critical path is from Document Collection through Question Annotation to Model Fine-tuning, as the quality and diversity of annotated questions directly determines the model's ability to handle real-world document QA scenarios.

**Design Tradeoffs**: The dataset prioritizes realistic document diversity over controlled conditions, using actual business and academic documents rather than synthetic examples. This increases external validity but introduces layout variability that may affect model generalization.

**Failure Signatures**: 
- Models failing to detect unanswerable questions, leading to hallucinations
- Insufficient visual context in multimodal models due to limited token length
- Poor performance on documents with complex layouts or mixed content types

**First Experiments**:
1. Evaluate model performance on a validation set with varying document truncation lengths to optimize context preservation
2. Compare performance of models using different visual input types (whole page vs. cropped regions) across multiple document layouts
3. Test model robustness by evaluating on documents with different visual structures (slides vs. reports vs. websites)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do visual inputs, particularly cropped images of referenced tables or figures, impact model performance compared to text-only inputs in document QA tasks?
- **Basis in paper**: The paper shows that multimodal models with visual inputs, especially those using cropped images of referenced tables or figures, outperform text-only models.
- **Why unresolved**: While the paper demonstrates improved performance with visual inputs, it does not explore the specific impact of different types of visual inputs (e.g., whole page images vs. cropped images) or the relative importance of visual vs. textual information.
- **What evidence would resolve it**: Further experiments comparing the performance of models using different types of visual inputs (e.g., whole page images, cropped images, or no visual input) would help understand the specific impact of visual information on model performance.

### Open Question 2
- **Question**: How does the incorporation of unanswerable questions during fine-tuning affect model performance and hallucination in document QA tasks?
- **Basis in paper**: The paper shows that incorporating unanswerable questions during fine-tuning improves model performance and reduces hallucination.
- **Why unresolved**: While the paper demonstrates the effectiveness of unanswerable questions in reducing hallucination, it does not explore the optimal proportion of unanswerable questions in the training data or the impact of different strategies for handling unanswerable questions during inference.
- **What evidence would resolve it**: Further experiments investigating the impact of different proportions of unanswerable questions in the training data and different strategies for handling unanswerable questions during inference would help optimize the use of unanswerable questions in document QA tasks.

### Open Question 3
- **Question**: How does the token length of language models affect their performance in document QA tasks?
- **Basis in paper**: The paper shows that the token length of the rinna bi-4B-8k model affects its performance in document QA tasks.
- **Why unresolved**: While the paper demonstrates the impact of token length on one model, it does not explore the relationship between token length and performance across different models or document types.
- **What evidence would resolve it**: Further experiments investigating the relationship between token length and performance across different models and document types would help optimize model selection and training for document QA tasks.

## Limitations
- Specific annotation guidelines and quality control procedures for ensuring questions reference both visual and textual elements remain unclear
- Training hyperparameters and configurations for evaluated models are not fully specified
- The 3,072 token document truncation may exclude relevant context for some questions

## Confidence

**High confidence**: Dataset value as a resource for Japanese document QA research
**Medium confidence**: Reported performance improvements from unanswerable questions (limited model variants tested)
**Medium confidence**: Multimodal model results showing superiority of cropped images (needs further validation)

## Next Checks
1. Test model performance on a held-out validation set with varying document truncation lengths to determine optimal context window
2. Conduct ablation studies comparing different visual input types (whole page vs. cropped regions) across multiple document layouts
3. Evaluate model robustness by testing on documents with different visual structures (slides vs. reports vs. websites)