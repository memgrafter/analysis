---
ver: rpa2
title: Evaluating Automatic Metrics with Incremental Machine Translation Systems
arxiv_id: '2407.03277'
source_url: https://arxiv.org/abs/2407.03277
tags:
- metrics
- systems
- translation
- metric
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset of weekly machine translation
  outputs from Google Translate across 12 language pairs spanning 6 years (2018-2024).
  The dataset enables evaluation of automatic MT metrics by leveraging the assumption
  that commercial systems improve over time through internal A/B testing.
---

# Evaluating Automatic Metrics with Incremental Machine Translation Systems

## Quick Facts
- arXiv ID: 2407.03277
- Source URL: https://arxiv.org/abs/2407.03277
- Reference count: 15
- Primary result: Neural metrics show more consistent upward trends and higher accuracy than surface-level metrics when ranking incremental MT system improvements

## Executive Summary
This paper introduces a new dataset of weekly machine translation outputs from Google Translate across 12 language pairs spanning 6 years (2018-2024). The dataset enables evaluation of automatic MT metrics by leveraging the assumption that commercial systems improve over time through internal A/B testing. The study finds that neural metrics (COMET-22, COMET-Kiwi, UniTE) show more consistent upward trends and higher accuracy (73.9-73.2%) than surface-level metrics (chrF at 71.4%, BLEU at 68.2%) when ranking incremental system improvements. The dataset also provides evidence that metric reliability may decline as MT quality improves, with downward trends being most common across language pairs. Additionally, synthetic references from DeepL achieve comparable correlation to human references.

## Method Summary
The researchers collected weekly translations from Google Translate for 12 language pairs over 6 years (2018-2024), creating a longitudinal dataset of 56-63 systems per pair. They computed metric scores using 8 different MT evaluation metrics (BLEU, chrF, BERTScore, COMET-20, COMET-22, COMET-Kiwi, UniTE, MS-COMET-22-QE) on AMR-based source sentences. The evaluation framework assumes commercial systems improve over time due to internal A/B testing, allowing metrics to be tested on their ability to rank newer systems as better. They analyze trends using Spearman correlation, evaluate ranking accuracy for system pairs, and examine metric reliability changes using rolling window analysis.

## Key Results
- Neural metrics (COMET-22, COMET-Kiwi, UniTE) show more consistent upward trends and higher accuracy (73.9-73.2%) than surface-level metrics
- chrF achieves 71.4% accuracy while BLEU achieves only 68.2% in ranking system pairs
- Downward trends in metric reliability are most common across language pairs, supporting quality decline hypothesis
- Synthetic references from DeepL achieve comparable correlation to human references

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Commercial MT systems improve over time due to internal A/B testing, enabling reliable evaluation of automatic metrics.
- Mechanism: The study assumes that weekly translations from Google Translate reflect incremental improvements, allowing metrics to be tested on their ability to rank newer systems as better.
- Core assumption: Google's internal A/B testing ensures that newer systems are of higher quality than older ones.
- Evidence anchors:
  - [abstract] "Since human A/B testing is commonly used, we assume commercial systems improve over time"
  - [section] "Given the common use of human A/B testing...our base assumption is that commercial systems show real improvements over time"
  - [corpus] Weak - no direct corpus evidence provided; assumption based on industry practice
- Break condition: If Google changes its internal testing protocols or if newer systems are not consistently better, the evaluation framework fails.

### Mechanism 2
- Claim: Neural metrics show more consistent upward trends and higher accuracy than surface-level metrics.
- Mechanism: Neural metrics are trained to directly learn human judgments and can better capture semantic similarities, leading to more reliable rankings of incremental improvements.
- Core assumption: Neural metrics' training on human judgments makes them more sensitive to quality improvements.
- Evidence anchors:
  - [abstract] "neural metrics (COMET-22, COMET-Kiwi, UniTE) show more consistent upward trends and higher accuracy (73.9-73.2%) than surface-level metrics"
  - [section] "Our findings demonstrate that neural metrics show a more consistent upward trend, and achieve higher accuracy than non-neural metrics"
  - [corpus] Weak - corpus evidence focuses on related papers, not direct metric performance
- Break condition: If the training data for neural metrics becomes outdated or if surface-level improvements become more significant, neural metrics may lose their advantage.

### Mechanism 3
- Claim: Metric reliability may decline as MT quality improves, with downward trends being most common across language pairs.
- Mechanism: As systems improve, the differences between high-quality translations become subtler, making it harder for metrics to distinguish improvements accurately.
- Core assumption: The correlation between metrics and human judgments decreases when evaluating top-performing systems.
- Evidence anchors:
  - [abstract] "the dataset also provides evidence that metric reliability may decline as MT quality improves, with downward trends being most common across language pairs"
  - [section] "Our findings reveal that a downward trend is the most common, supporting the results of Ma et al. (2019)"
  - [corpus] Weak - corpus evidence does not directly address this phenomenon
- Break condition: If metrics are specifically designed to handle high-quality translations or if quality improvements become more substantial, this trend may reverse.

## Foundational Learning

- Concept: Spearman correlation
  - Why needed here: Used to measure the relationship between metric score rankings and time rankings for MT systems.
  - Quick check question: What does a positive Spearman correlation indicate about metric trends over time?

- Concept: Accuracy calculation for system ranking
  - Why needed here: Used to evaluate how well metrics rank incremental system improvements by comparing their rankings to time-based rankings.
  - Quick check question: How is accuracy calculated when comparing metric rankings to time-based rankings?

- Concept: Rolling window analysis
  - Why needed here: Used to examine how metric reliability changes as system quality improves by analyzing subsets of systems in chronological order.
  - Quick check question: What does a downward trend in rolling window accuracy suggest about metric reliability?

## Architecture Onboarding

- Component map:
  Data collection -> Metric computation -> Trend analysis (Spearman correlation) -> Ranking accuracy evaluation -> Rolling window analysis -> Synthetic reference testing

- Critical path:
  1. Collect weekly translations from commercial MT systems
  2. Calculate metric scores for each translation
  3. Analyze trends using Spearman correlation
  4. Evaluate ranking accuracy for system pairs
  5. Examine metric reliability changes using rolling window analysis
  6. Test synthetic reference effectiveness

- Design tradeoffs:
  - Using commercial systems assumes continuous improvement, which may not always be true
  - Neural metrics require extensive training data and computational resources
  - Synthetic references may not fully capture human reference quality
  - Long-term data collection is resource-intensive but provides valuable insights

- Failure signatures:
  - Inconsistent or downward trends in metric scores over time
  - Low accuracy in ranking system pairs
  - Failure of synthetic references to match human reference performance
  - Metrics showing similar performance across all language pairs (suggesting lack of sensitivity)

- First 3 experiments:
  1. Replicate Spearman correlation analysis for a single language pair to verify upward trends
  2. Test accuracy calculation on a small subset of system pairs to confirm ranking reliability
  3. Compare metric performance using human vs. synthetic references for one language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reliability of MT metrics truly decline as MT quality improves, or is this an artifact of the evaluation methodology?
- Basis in paper: [explicit] The authors note this is a "debated issue" and their findings show downward trends are "most common" but not universal across language pairs.
- Why unresolved: The study only examines Google Translate systems over time. The results show mixed trends (downward, upward, flat) across different language pairs and metrics, suggesting the relationship between MT quality and metric reliability may be more complex than a simple decline.
- What evidence would resolve it: A larger-scale longitudinal study across multiple commercial MT systems and a wider variety of domains and language pairs, combined with controlled experiments that isolate quality improvements from other factors affecting metric reliability.

### Open Question 2
- Question: How do LLM-based evaluators perform on this dataset compared to traditional metrics?
- Basis in paper: [explicit] The authors explicitly state they excluded LLM-based evaluators due to cost constraints.
- Why unresolved: The study uses traditional metrics (surface-level overlap, embedding-based, and trained metrics) but does not include recent LLM-based evaluators that have shown strong performance in other studies.
- What evidence would resolve it: Running the same evaluation protocol using popular LLM-based evaluators (like GPT-4, Claude, etc.) on this dataset and comparing their accuracy, consistency, and correlation patterns with the existing metrics.

### Open Question 3
- Question: What is the impact of using synthetic references from different MT systems on metric reliability?
- Basis in paper: [explicit] The study only tests DeepL synthetic references for three language pairs, finding comparable accuracy to human references.
- Why unresolved: The study provides preliminary evidence that synthetic references can work, but only tests one synthetic reference source (DeepL) and only for three language pairs.
- What evidence would resolve it: Systematic testing of synthetic references from multiple MT systems (Google Translate, DeepL, ChatGPT, etc.) across all 12 language pairs in the dataset, examining how different synthetic reference qualities affect metric performance and whether certain metrics are more robust to synthetic reference quality variations.

## Limitations

- The central assumption that commercial MT systems improve monotonically remains untested directly and could break the evaluation framework
- Corpus evidence supporting key claims is notably weak, with findings primarily derived from metric calculations rather than external validation sources
- The analysis relies on correlation-based evidence that cannot establish causation between system improvements and metric performance

## Confidence

- Commercial systems show consistent improvement (Medium): Based on industry practice assumption rather than direct verification
- Neural metrics outperform surface-level metrics (High): Strong quantitative evidence across multiple metrics
- Metric reliability declines with system quality (Medium): Supported by trend analysis but requires further validation

## Next Checks

1. Verify the monotonic improvement assumption by cross-referencing Google's public system update announcements with the dataset timeline
2. Conduct ablation studies removing specific language pairs to test if results generalize beyond high-resource languages
3. Test synthetic reference quality by comparing metric rankings when using human vs. synthetic references on the same system pairs