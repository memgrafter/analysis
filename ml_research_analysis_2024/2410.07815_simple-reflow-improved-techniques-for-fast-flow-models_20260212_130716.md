---
ver: rpa2
title: 'Simple ReFlow: Improved Techniques for Fast Flow Models'
arxiv_id: '2410.07815'
source_url: https://arxiv.org/abs/2410.07815
tags:
- reflow
- training
- flow
- diffusion
- coupling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ReFlow algorithm for accelerating sampling
  in diffusion and flow-matching models, which often require many neural function
  evaluations (NFEs). ReFlow straightens ODE trajectories, enabling faster sampling,
  but typically suffers from reduced sample quality.
---

# Simple ReFlow: Improved Techniques for Fast Flow Models

## Quick Facts
- arXiv ID: 2410.07815
- Source URL: https://arxiv.org/abs/2410.07815
- Reference count: 40
- Authors achieve state-of-the-art FID scores for fast generation via neural ODEs with only 9 NFEs

## Executive Summary
This paper addresses the challenge of accelerating sampling in diffusion and flow-matching models, which typically require many neural function evaluations (NFEs). The authors investigate ReFlow, an algorithm that straightens ODE trajectories to enable faster sampling, but which often suffers from reduced sample quality. Through extensive experimentation and analysis, they identify seven key improvements across training dynamics, learning, and inference that consistently enhance FID scores while maintaining fast sampling speeds.

## Method Summary
The paper proposes seven improvements to the ReFlow algorithm: loss normalization using dynamic weights, time distribution optimization with exponential weighting, high-frequency-promoting loss functions, reduced model dropout, forward and projected training couplings, and improved ODE discretization. These techniques are applied to optimize ReFlow denoisers initialized from pre-trained EDM models. The method is evaluated on CIFAR10, AFHQv2, FFHQ, and ImageNet-64 datasets, measuring FID scores with only 9 NFEs for sampling. The training procedure involves optimizing the generalized FM loss with specific distributions and evaluating using the Heun solver for ODE integration.

## Key Results
- Achieves state-of-the-art FID scores for fast generation via neural ODEs: 2.23/1.98 on CIFAR10, 2.30/1.91 on AFHQv2, 2.84/2.67 on FFHQ, and 3.49/1.74 on ImageNet-64
- All results obtained with only 9 neural function evaluations (NFEs)
- Consistent FID improvements across all datasets when combining all proposed techniques
- Outperforms existing fast flow methods while maintaining sampling efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss normalization using dynamic weights stabilizes ReFlow training by preventing vanishing loss gradients.
- Mechanism: ReFlow loss scales differently across input space than diffusion models, requiring adaptive weight functions to maintain gradient stability during training.
- Core assumption: The variance in ReFlow loss across different input states requires dynamic normalization rather than static time-based weighting.
- Evidence anchors:
  - [abstract]: "we propose seven improvements for training dynamics, learning and inference, which are verified with thorough ablation studies"
  - [section]: "We propose a simple improvement by using w(xt, t) = 1/ sg[LGFM(θ; Q01, xt, t)]"
  - [corpus]: Weak evidence - no direct mention of loss normalization techniques in neighboring papers
- Break condition: If the loss normalization introduces numerical instability or if the dynamic weight tracking network fails to converge

### Mechanism 2
- Claim: Time distribution optimization shifts sampling emphasis to critical regions where ODE trajectories have high curvature.
- Mechanism: Using exponential time distribution dT(t) ∝ 10^t instead of uniform or hyperbolic cosine distributions better aligns training with regions requiring more accurate velocity estimation.
- Core assumption: ReFlow ODE trajectories have critical curvature near t ∈ {0, 1} requiring more training emphasis.
- Evidence anchors:
  - [abstract]: "Combining all our techniques, we achieve state-of-the-art FID scores"
  - [section]: "We use a distribution with density proportional to the increasing exponential, i.e., dT(t) ∝ at for a ≥ 1"
  - [corpus]: Weak evidence - neighboring papers mention time distribution but not specific exponential choices
- Break condition: If the exponential distribution causes insufficient coverage of intermediate time regions

### Mechanism 3
- Claim: High-frequency promoting loss functions accelerate convergence by focusing learning on visually critical features.
- Mechanism: Using loss ℓϕ with high-pass filter (ϕ = I + λ · HPF) preconditiones gradients to emphasize high-frequency features while maintaining theoretical correctness.
- Core assumption: ReFlow models initialized with diffusion models already capture low-frequency features, so high-frequency learning needs acceleration.
- Evidence anchors:
  - [abstract]: "combining all our techniques, we achieve state-of-the-art FID scores"
  - [section]: "we propose calculating the difference of denoiser output Dθ(xt, t) and clean data x0 after passing them through a high-pass filter"
  - [corpus]: Weak evidence - no direct mention of high-pass filtering techniques in neighboring papers
- Break condition: If λ is too large causing ill-conditioning or too small providing insufficient benefit

## Foundational Learning

- Concept: Ordinary Differential Equations and their numerical integration
  - Why needed here: The entire ReFlow method relies on solving ODEs for both training and sampling
  - Quick check question: What numerical method would you use to solve dx/dt = f(x,t) from t=0 to t=1 with minimal function

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: ReFlow uses optimal transport theory to straighten ODE trajectories
  - Quick check question: How does the Wasserstein distance between two distributions relate to their optimal transport cost?

- Concept: Flow matching and denoising diffusion models
  - Why needed here: ReFlow builds upon flow matching and uses pre-trained denoising diffusion models for initialization
  - Quick check question: What is the relationship between the score function in diffusion models and the vector field in flow matching?

## Architecture Onboarding

- Component map: Pre-trained EDM denoiser -> ReFlow denoiser (with 7 improvements) -> ODE solver (Heun) -> Generated samples
- Critical path: The forward pass through the ReFlow denoiser during sampling is the critical path for inference speed
- Design tradeoffs: The 7 improvements trade off between training complexity and sampling efficiency; some may increase training time but reduce NFEs needed for sampling
- Failure signatures: Poor FID scores indicate issues with training dynamics or inference; unstable ODE trajectories suggest discretization or coupling problems
- First experiments: 1) Test each improvement individually to verify its contribution to FID improvement; 2) Compare sampling speed and quality with different ODE solvers; 3) Visualize ODE trajectories to confirm straightening effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact cause of the discrepancy between improved straightness and worse FID scores in mini-batch optimal transport flow matching?
- Basis in paper: The paper observes that while straightness improves with larger batch sizes in mini-batch OT flow matching, FID scores consistently get worse compared to base flow matching.
- Why unresolved: The paper acknowledges this is a surprising empirical result but does not provide a definitive explanation, leaving it as an open area for future investigation.
- What evidence would resolve it: A detailed analysis of the relationship between mini-batch OT coupling bias, the specific characteristics of the learned vector field, and its impact on sample quality would help resolve this question.

### Open Question 2
- Question: What is the optimal choice of the regularization parameter λ for the Sinkhorn divergence in the coupling projection method?
- Basis in paper: The paper describes using a decaying schedule for λ starting from a large value (e.g., λ = 1000) but does not determine an optimal value.
- Why unresolved: The paper uses a heuristic approach to choose λ but does not explore the impact of different values or provide a principled method for determining the optimal λ.
- What evidence would resolve it: An ablation study varying λ across a wide range of values and measuring its impact on FID scores and the fidelity of the projected coupling would help determine the optimal λ.

### Open Question 3
- Question: How does classifier-free guidance (CFG) or auto-guidage (AG) affect the marginals of ReFlow models?
- Basis in paper: The paper applies CFG and AG to ReFlow models and observes improved FID scores but notes that it is unclear what effect guidance has on the marginals.
- Why unresolved: The paper focuses on the empirical performance of guidance but does not investigate its theoretical impact on the learned distribution.
- What evidence would resolve it: An analysis of the induced coupling and marginal distributions under guidance, possibly using techniques from optimal transport or other theoretical frameworks, would clarify the effect of guidance on ReFlow models.

## Limitations
- The improvements are primarily validated on image datasets with 9 NFEs, leaving open questions about scalability to higher-resolution images or other data modalities
- While FID scores are reported, perceptual quality assessments and diversity metrics beyond FID would strengthen the claims
- The interactions between multiple techniques are not fully characterized, as ablation studies focus on individual improvements

## Confidence
- **High confidence**: The core mechanism of ReFlow trajectory straightening and the empirical improvements in FID scores are well-supported by the ablation studies and comparative results
- **Medium confidence**: The theoretical justification for individual improvements (loss normalization, time distribution, high-frequency loss) is provided, but the exact contribution of each component to the final performance requires further investigation
- **Low confidence**: The generalizability of these techniques to other domains beyond image generation and the long-term stability of models trained with these specific improvements

## Next Checks
1. Test the proposed improvements on higher-resolution image datasets (e.g., 256x256 or 512x512) to evaluate scalability and potential degradation in performance
2. Conduct perceptual studies with human evaluators to assess whether FID improvements translate to noticeable quality differences in generated samples
3. Investigate the impact of combining these techniques with other recent flow model acceleration methods (e.g., Piecewise Rectified Flow) to determine if additive benefits exist