---
ver: rpa2
title: Augmenting Replay in World Models for Continual Reinforcement Learning
arxiv_id: '2401.16650'
source_url: https://arxiv.org/abs/2401.16650
tags:
- tasks
- learning
- replay
- world
- buffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to apply model-based reinforcement
  learning with a memory-efficient distribution-matching replay buffer to continual
  reinforcement learning. The authors introduce WMAR, which extends DreamerV3 with
  a short-term FIFO buffer and a long-term global distribution matching buffer.
---

# Augmenting Replay in World Models for Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.16650
- Source URL: https://arxiv.org/abs/2401.16650
- Reference count: 40
- Primary result: Model-based RL with memory-efficient replay buffer shows benefits in continual RL scenarios

## Executive Summary
This paper proposes WMAR (World Models with Augmented Replay), an extension of DreamerV3 that combines a short-term FIFO buffer with a long-term global distribution matching buffer for continual reinforcement learning. The approach aims to reduce catastrophic forgetting by maintaining a uniform random subset of experiences from all tasks. The method is evaluated on two scenarios: tasks with shared structure using OpenAI Procgen and tasks without shared structure using the Atari benchmark. Results show that WMAR provides substantial improvements in forgetting characteristics on tasks without shared structure while showing slight benefits on tasks with shared structure.

## Method Summary
WMAR extends DreamerV3 by adding a long-term global distribution matching (LTDM) buffer to the standard FIFO replay buffer. The LTDM buffer stores a fixed-size random subset of spliced rollouts (512-step chunks) across all tasks using reservoir sampling. This approach maintains a uniform random subset of experiences from all tasks, preventing the replay buffer from being dominated by recent experiences. The world model is trained on both buffers, allowing it to retain knowledge of past tasks while learning new ones. Rollouts are spliced into smaller chunks to improve memory efficiency and ensure representative training data.

## Key Results
- WMAR shows substantial reduction in catastrophic forgetting on Atari tasks without shared structure
- Slight performance benefits observed on OpenAI Procgen tasks with shared structure
- Memory-efficient distribution matching approach enables effective continual RL with limited buffer capacity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The augmented replay buffer improves stability in continual RL by maintaining a uniform random subset of experiences from all tasks.
- Mechanism: The LTDM buffer stores a fixed-size random subset of spliced rollouts across all tasks, ensuring that the world model is trained on experiences from past tasks even as new tasks are learned. This prevents the model from overfitting to recent experiences and forgetting older tasks.
- Core assumption: The world model's predictions are sufficiently accurate when trained on a uniform random subset of experiences from all tasks.
- Evidence anchors:
  - [abstract] "WMAR extends the well known DreamerV3 algorithm, which employs a simple FIFO buffer and was not tested in continual RL."
  - [section] "Matching the global training distribution even with limited capacity in the replay buffer can reduce catastrophic forgetting [11]."
  - [corpus] Weak evidence; related papers focus on generative replay or Fisher-based methods, not distribution matching.
- Break condition: If the world model's predictions degrade significantly when trained on the LTDM buffer, or if the LTDM buffer becomes dominated by recent tasks, stability may be lost.

### Mechanism 2
- Claim: Spliced rollouts improve memory efficiency and ensure representative training data in the replay buffer.
- Mechanism: By splitting rollouts into smaller chunks (512 steps), the LTDM buffer can store a more diverse set of experiences from each task without requiring full episodes. This prevents the buffer from being dominated by a few long episodes and ensures better coverage of the state space.
- Core assumption: Smaller rollout chunks provide sufficient context for the world model to learn accurate environment dynamics.
- Evidence anchors:
  - [section] "Restricting the size of the replay buffer can cause it to become full with only a few episodes... Therefore, while typical implementations store complete rollouts, we spliced rollouts into smaller chunks of size 512."
  - [corpus] No direct evidence; assumption is based on internal reasoning.
- Break condition: If the world model's accuracy degrades when trained on spliced rollouts, or if the splicing disrupts the temporal dependencies needed for accurate predictions.

### Mechanism 3
- Claim: The world model's ability to simulate "dreamed" experiences enables effective off-policy learning and data augmentation in continual RL.
- Mechanism: The world model generates imagined trajectories that are used to train the actor-critic controller, allowing the agent to learn from past experiences without requiring direct interaction with the environment. This is particularly useful in continual RL where direct interaction may be limited.
- Core assumption: The world model's simulated experiences are sufficiently realistic to train the actor-critic controller effectively.
- Evidence anchors:
  - [section] "The world model is able to simulate 'dreamed' experiences, which are used to train the controller."
  - [corpus] No direct evidence; assumption is based on internal reasoning.
- Break condition: If the world model's predictions diverge significantly from reality, or if the actor-critic controller fails to learn effectively from simulated experiences.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial for evaluating the effectiveness of the augmented replay buffer in preventing performance degradation on past tasks.
  - Quick check question: What is the primary cause of catastrophic forgetting in neural networks, and how does it manifest in continual RL?

- Concept: Model-based reinforcement learning
  - Why needed here: The world model is the core component of the WMAR algorithm, and understanding how it learns to predict environment dynamics is essential for grasping the overall approach.
  - Quick check question: How does the world model in DreamerV3 learn to predict environment dynamics, and what role does the replay buffer play in this process?

- Concept: Continual learning metrics
  - Why needed here: Evaluating the performance of WMAR requires understanding metrics like forgetting, forward transfer, and backward transfer.
  - Quick check question: What is the difference between forward transfer and backward transfer in continual learning, and how are they measured?

## Architecture Onboarding

- Component map: Environment -> Augmented Replay Buffer (FIFO + LTDM) -> World Model (RSSM) -> Actor-Critic Controller
- Critical path: Experiences from the environment are stored in the augmented replay buffer, which is used to train the world model. The world model generates imagined trajectories that are used to train the actor-critic controller.
- Design tradeoffs: The augmented replay buffer trades off memory efficiency for improved stability in continual RL. Spliced rollouts further improve memory efficiency but may introduce some loss of temporal context.
- Failure signatures: If the world model's predictions degrade, the actor-critic controller may fail to learn effectively. If the LTDM buffer becomes dominated by recent tasks, catastrophic forgetting may occur.
- First 3 experiments:
  1. Test the world model's accuracy on a single task with and without the LTDM buffer.
  2. Evaluate the stability of the actor-critic controller when trained on simulated experiences generated by the world model.
  3. Measure the forgetting and forward transfer performance of WMAR on a small set of tasks with shared structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WMAR scale with the number of tasks and task sequence length in continual learning scenarios?
- Basis in paper: [inferred] The paper mentions that experiments were limited in scope and could be expanded by increasing the length of task sequences and the number of environment steps.
- Why unresolved: The current study only tested a limited number of tasks and sequence lengths due to computational constraints.
- What evidence would resolve it: Experiments testing WMAR with varying numbers of tasks and longer task sequences, measuring performance metrics like forgetting and forward transfer.

### Open Question 2
- Question: How would WMAR perform if the order of tasks was randomized across multiple seeds, instead of using a fixed task order?
- Basis in paper: [explicit] The paper states that task ordering can result in significant differences in CL results and suggests that future work could randomize the order over a larger number of seeds.
- Why unresolved: The current study used a fixed task order to manage computational costs, which may not represent the full range of possible task orderings.
- What evidence would resolve it: Experiments with randomized task orders across many seeds, comparing performance metrics to the fixed order results.

### Open Question 3
- Question: Can WMAR be further improved by combining it with existing techniques like behavior cloning or other regularization methods?
- Basis in paper: [explicit] The paper suggests that WMAR methods are largely orthogonal to previous state-of-the-art approaches like EWC, P&C, and CLEAR, and that future work could combine WMAR with existing techniques.
- Why unresolved: The current study only implemented WMAR as an extension of DreamerV3 without incorporating other techniques.
- What evidence would resolve it: Experiments implementing WMAR with additional techniques like behavior cloning or regularization methods, comparing performance to baseline WMAR and other methods.

### Open Question 4
- Question: How does the performance of WMAR change when tasks have more abstract differences, such as inverting colors or permuting controls, rather than just visual differences?
- Basis in paper: [explicit] The paper mentions that WMAR's convolutional feature extractor may suffer from more abstract changes like inverting colors or permuting controls, leading to decreased overall performance.
- Why unresolved: The current study only tested tasks with visual differences and did not explore more abstract task variations.
- What evidence would resolve it: Experiments testing WMAR on tasks with various types of abstract differences, measuring performance degradation compared to tasks with only visual differences.

## Limitations

- The spliced rollout approach may break temporal dependencies critical for accurate world model predictions
- The distribution matching buffer relies on reservoir sampling which may become stale in long task sequences
- Procgen results show only slight improvements over baseline, raising questions about practical significance in shared-structure scenarios

## Confidence

- **High confidence**: WMAR effectively reduces catastrophic forgetting on Atari tasks without shared structure
- **Medium confidence**: The spliced rollout approach improves memory efficiency without significant performance degradation
- **Medium confidence**: The LTDM buffer maintains sufficient diversity for world model training

## Next Checks

1. Analyze the distribution of experiences in the LTDM buffer over time to verify it remains representative of all tasks, not just recent ones
2. Test world model prediction accuracy on long-horizon tasks with and without spliced rollouts to quantify temporal dependency loss
3. Measure the impact of buffer capacity on forgetting reduction by varying LTDM buffer size while keeping total capacity constant