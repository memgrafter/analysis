---
ver: rpa2
title: 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models'
arxiv_id: '2410.11710'
source_url: https://arxiv.org/abs/2410.11710
tags:
- uni00000013
- tool
- action
- uni00000014
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MTU-Bench is a multi-granularity tool-use benchmark designed to
  address limitations in existing datasets such as insufficient evaluation scenarios,
  high evaluation costs, and lack of comprehensive metrics. It covers five tool-use
  scenes: single-turn/single-tool, single-turn/multi-tool, multi-turn/single-tool,
  multi-turn/multi-tool, and out-of-distribution tasks.'
---

# MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2410.11710
- Source URL: https://arxiv.org/abs/2410.11710
- Authors: Pei Wang; Yanan Wu; Zekun Wang; Jiaheng Liu; Xiaoshuai Song; Zhongyuan Peng; Ken Deng; Chenchen Zhang; Jiakai Wang; Junran Peng; Ge Zhang; Hangyu Guo; Zhaoxiang Zhang; Wenbo Su; Bo Zheng
- Reference count: 40
- Key outcome: MTU-LLaMA achieves superior performance across all settings and metrics compared to other models when fine-tuned on MTU-Instruct

## Executive Summary
MTU-Bench addresses critical limitations in existing tool-use benchmarks by providing a multi-granularity evaluation framework that covers five distinct tool-use scenarios ranging from simple single-turn tasks to complex out-of-distribution challenges. The benchmark includes MTU-Instruct for training and MTU-Eval for evaluation, with all metrics based on prediction results and ground truth without requiring GPT or human evaluation. Experimental results demonstrate that MTU-LLaMA, fine-tuned on MTU-Instruct, outperforms other models across all evaluated settings and metrics, validating the effectiveness of the proposed benchmark.

## Method Summary
The MTU-Bench framework transforms task-oriented dialogue datasets (MultiWOZ, SGD, TaskMaster, MetaLWOZ, ATIS, SNIPS) into tool-use data through a systematic synthesis pipeline. The process involves tool creation, clustering, documentation, and synthesis to generate 54,367 dialogues with 136 tools. The framework then fine-tunes LLaMA3-8B on this synthesized MTU-Instruct dataset to create MTU-LLaMA, which is evaluated using a comprehensive set of metrics including tool selection accuracy, parameter selection accuracy, success rate, turn success rate, task process rate, tool number accuracy, and tool order accuracy.

## Key Results
- MTU-LLaMA achieves superior performance across all five tool-use granularity settings
- Automatic evaluation without GPT or human assessment maintains high accuracy while reducing costs
- Fine-grained metrics capture detailed aspects of tool-use performance across multiple dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MTU-Bench's multi-granularity coverage enables comprehensive evaluation of tool-use capabilities across diverse scenarios
- Mechanism: By including five distinct tool-use scenes (single-turn/single-tool, single-turn/multi-tool, multi-turn/single-tool, multi-turn/multi-tool, and out-of-distribution tasks), the benchmark captures the full spectrum of tool-use complexity
- Core assumption: Tool-use capabilities in LLMs are context-dependent and vary significantly across different scenarios
- Evidence anchors:
  - [abstract] "our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks)"
  - [section] "we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench"
  - [corpus] "Weak evidence - corpus contains related benchmarks but lacks direct comparison of multi-granularity approaches"
- Break condition: If LLMs demonstrate consistent performance across all granularity levels, the multi-granularity approach may be unnecessarily complex

### Mechanism 2
- Claim: Automatic evaluation without GPT/human assessment reduces costs and improves reproducibility
- Mechanism: All metrics are computed based solely on prediction results and ground truth, eliminating dependency on external API costs and human variability
- Core assumption: Ground truth-based metrics can adequately capture tool-use performance without external evaluators
- Evidence anchors:
  - [abstract] "all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics"
  - [section] "all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics"
  - [corpus] "Weak evidence - corpus shows GPT-based evaluation is common but doesn't validate ground truth-only approach"
- Break condition: If ground truth metrics fail to capture nuanced tool-use errors that human evaluators would detect

### Mechanism 3
- Claim: Fine-grained metrics capture detailed aspects of tool-use performance
- Mechanism: Metrics like tool selection accuracy, parameter selection accuracy, success rate, turn success rate, task process rate, tool number accuracy, and tool order accuracy provide comprehensive evaluation across multiple dimensions
- Core assumption: Tool-use performance is multi-dimensional and requires granular evaluation beyond simple success/failure
- Evidence anchors:
  - [abstract] "we propose a series of fine-grained metrics such as tool selection accuracy, parameter selection accuracy, success rate, turn success rate, task process rate, tool number accuracy, tool order accuracy"
  - [section] "we propose a series of fine-grained metrics such as tool selection accuracy, parameter selection accuracy, success rate, turn success rate, task process rate, tool number accuracy, tool order accuracy"
  - [corpus] "Moderate evidence - related works mention similar metrics but MTU-Bench appears more comprehensive"
- Break condition: If simpler metrics correlate strongly with complex ones, the fine-grained approach may be over-engineered

## Foundational Learning

- Concept: Multi-turn dialogue systems
  - Why needed here: The benchmark explicitly evaluates multi-turn scenarios where tool-use decisions depend on conversation history
  - Quick check question: How does tool selection in turn N depend on outcomes from turns 1 through N-1?

- Concept: API parameter matching and validation
  - Why needed here: The benchmark evaluates parameter selection accuracy, requiring understanding of how tool parameters should match user intent
  - Quick check question: What distinguishes correct parameter values from incorrect ones in tool invocation?

- Concept: Tool sequence and dependency management
  - Why needed here: Multi-tool scenarios require understanding tool ordering and dependencies between successive tool calls
  - Quick check question: How does the order of tool invocation affect overall task success?

## Architecture Onboarding

- Component map: MTU-Instruct (training data) -> MTU-Eval (evaluation framework) -> Metrics and test sets
- Critical path: Data synthesis → Tool documentation → Model fine-tuning → Evaluation → Analysis
- Design tradeoffs: Comprehensive evaluation vs. evaluation cost, multi-granularity coverage vs. complexity, automatic vs. human evaluation
- Failure signatures: Poor performance in multi-turn scenarios indicates context management issues; errors in parameter selection suggest poor tool understanding
- First 3 experiments:
  1. Evaluate baseline model on single-turn/single-tool scenario to establish minimum competency
  2. Test multi-turn capability by evaluating performance degradation across dialogue turns
  3. Assess multi-tool handling by measuring tool selection accuracy with increasing tool numbers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does MTU-Bench handle scenarios with dynamically evolving APIs where tool definitions and parameters change over time?
- Basis in paper: [inferred] The paper mentions using static tool documentation compiled from existing datasets, but does not address how the benchmark would adapt to API evolution or deprecation.
- Why unresolved: The current construction methodology relies on static synthesis from existing datasets, without mechanisms for handling API changes or real-time updates.
- What evidence would resolve it: Results comparing model performance on current vs. modified API specifications, or analysis of model adaptability to API version changes.

### Open Question 2
- Question: What is the impact of dialogue history length on model performance, and is there an optimal history window size for tool-use tasks?
- Basis in paper: [inferred] While the paper analyzes multi-turn dialogue scenarios, it doesn't systematically investigate how dialogue history length affects tool selection and parameter accuracy.
- Why unresolved: The paper provides statistics on average turns per dialogue but doesn't explore how varying history lengths impact performance metrics.
- What evidence would resolve it: Controlled experiments varying dialogue history length and measuring corresponding changes in TS, PS, and other metrics.

### Open Question 3
- Question: How do models perform when tool documentation is incomplete or ambiguous, and what strategies do they employ to handle such cases?
- Basis in paper: [inferred] The paper assumes complete and accurate tool documentation but doesn't explore model behavior with imperfect documentation.
- Why unresolved: All experiments use carefully constructed tool documentation, without testing robustness to documentation errors or omissions.
- What evidence would resolve it: Performance comparisons between models using complete vs. incomplete/ambiguous tool documentation, along with analysis of error patterns.

## Limitations
- Data synthesis transparency issues due to unspecified prompt templates and GPT-4 generation configurations
- Tool clustering methodology lacks detailed specification of distance threshold parameters
- Ground truth-only evaluation approach remains untested against human judgment for complex scenarios

## Confidence

**High Confidence Claims**:
- The benchmark architecture (MTU-Instruct and MTU-Eval components) is clearly specified and reproducible
- The five granularity levels provide systematic coverage of tool-use complexity
- The multi-granularity approach addresses a genuine gap in existing benchmarks

**Medium Confidence Claims**:
- The fine-grained metrics provide comprehensive evaluation across multiple dimensions
- The automatic evaluation approach reduces costs while maintaining quality
- The dataset size (54,367 dialogues) is sufficient for robust evaluation

**Low Confidence Claims**:
- The ground truth-only evaluation approach captures all relevant tool-use performance aspects
- The tool clustering methodology produces an optimal and unbiased tool set
- The benchmark generalizes well to real-world tool-use scenarios beyond the tested domains

## Next Checks
**Validation Check 1**: Conduct a human evaluation study comparing ground truth-based metrics against human judgments on a subset of the test data. This would validate whether the automatic metrics capture performance aspects that humans consider important.

**Validation Check 2**: Perform ablation studies on the tool clustering parameters (distance thresholds, clustering algorithms) to determine how sensitive the benchmark is to these choices and whether different clustering approaches significantly affect model performance.

**Validation Check 3**: Test the benchmark's generalizability by applying it to tools and scenarios outside the original dialogue domains (e.g., creative writing, data analysis, or software development tools) to assess whether the multi-granularity framework remains effective across diverse tool types.