---
ver: rpa2
title: 'Beyond Quantile Methods: Improved Top-K Threshold Estimation for Traditional
  and Learned Sparse Indexes'
arxiv_id: '2412.10701'
source_url: https://arxiv.org/abs/2412.10701
tags:
- methods
- threshold
- query
- estimate
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating the score of the
  k-th highest ranking document for disjunctive top-k queries, which can accelerate
  many top-k query processing algorithms. The authors propose new methods building
  on quantile-based techniques, enhanced with ideas from top-k query processing.
---

# Beyond Quantile Methods: Improved Top-K Threshold Estimation for Traditional and Learned Sparse Indexes

## Quick Facts
- arXiv ID: 2412.10701
- Source URL: https://arxiv.org/abs/2412.10701
- Authors: Jinrui Gou; Yifan Liu; Minghao Shao; Torsten Suel
- Reference count: 37
- Primary result: New methods for top-k threshold estimation significantly improve MUF over baseline quantile methods, especially for longer queries and larger k values.

## Executive Summary
This paper addresses the problem of estimating the score of the k-th highest ranking document for disjunctive top-k queries, which can accelerate many top-k query processing algorithms. The authors propose new methods building on quantile-based techniques, enhanced with ideas from top-k query processing. Their approach involves storing and combining term scores from different precomputed subsets, using duplicate removal and lookups to improve estimates. Experimental results on datasets like ClueWeb09B and MSMARCO show significant improvements over existing quantile methods, particularly for longer queries and larger values of k.

## Method Summary
The paper presents methods for top-k threshold estimation that build upon quantile-based approaches. The key idea is to use precomputed subsets of terms (pairs, triplets) selected from query logs, and store their top-k thresholds and postings with individual term scores. During query processing, the method retrieves and combines prefix data, applies lookup budgets to fetch missing term scores, and uses the k-th highest accumulated score as the threshold estimate. The methods are implemented in C++17 and evaluated on traditional and learned sparse indexes using various ranking functions.

## Key Results
- The proposed methods significantly improve MUF over baseline quantile methods, achieving improvements from 0.91 to 0.98 for k=10 and from 0.93 to 0.97 for k=100 in some cases.
- The methods perform particularly well on longer queries and larger values of k.
- The techniques also work well on learned sparse indexes using ranking functions like BM25, Query Likelihood, DocT5Query, and DeepImpact.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using lookup budgets to fetch missing term scores for accumulated docIDs improves threshold estimates without excessive computational cost.
- Mechanism: The method maintains a hash table of accumulated scores for docIDs encountered in prefix postings. When lookup budget is applied, it retrieves missing term scores from the inverted index for the top lb results, resolving incomplete score sums and tightening the estimate.
- Core assumption: Term scores from the index are accurate and complete; hash table lookup and index access are fast relative to full query execution.
- Evidence anchors:
  - [abstract] "Adding Lookups: The next natural step is to also add index lookups to obtain missing term scores for the results accumulated in the hash table of the Combine Scores method."
  - [section] "we perform lookups into an inverted index for the top lb results returned by Combine Scores for all unknown term scores."
  - [corpus] Weak anchor; the cited papers discuss quantile estimation but not this specific lookup-based refinement mechanism.
- Break condition: If lookup budget is exhausted before enough missing scores are resolved, or if index lookup latency becomes prohibitive, the benefit diminishes.

### Mechanism 2
- Claim: Storing term scores in prefix postings (rather than just total scores) enables combining scores from multiple subsets without double-counting.
- Mechanism: Prefixes store each constituent term score for every docID, so when the same docID appears in multiple prefixes, scores are combined correctly instead of being added multiple times.
- Core assumption: Term scores are additive and independent; the storage overhead of keeping individual term scores is acceptable.
- Evidence anchors:
  - [abstract] "The next natural step is to combine any term scores retrieved from different prefixes that have the same docID."
  - [section] "we have to be careful about how to do this... we would have to store not just additional threshold values for other values of k, but for each selected subset s we also need a prefix of the docIDs of the highest scoring documents with respect to s."
  - [corpus] Weak anchor; the cited papers discuss quantile estimation but do not detail score combination across subsets.
- Break condition: If the same term score is mistakenly counted more than once due to storage errors or if term scores are not truly additive.

### Mechanism 3
- Claim: Using subsets of terms (pairs, triplets) instead of single terms or full queries yields better threshold estimates by capturing more context.
- Mechanism: The method selects frequent co-occurring term subsets from query logs, precomputes their top-k thresholds, and uses these to bound query thresholds. Larger subsets give tighter bounds.
- Core assumption: Frequent term subsets in query logs are representative of actual query distributions and capture meaningful context for score estimation.
- Evidence anchors:
  - [abstract] "we choose subsets of two, three, or even four terms that frequently occur together in queries."
  - [section] "The study in [11] compared two methods, a lexical one that chooses subsets that frequently co-occur in documents, and a log-based one that chooses subsets that frequently co-occur in queries from a large log."
  - [corpus] Weak anchor; the cited papers discuss quantile estimation but not the specific subset selection strategy.
- Break condition: If the selected subsets do not appear frequently enough in actual queries, or if term dependencies break the additive assumption.

## Foundational Learning

- Concept: Disjunctive top-k query processing and threshold estimation.
  - Why needed here: The paper's methods directly build on and improve existing threshold estimation for disjunctive queries.
  - Quick check question: What is the difference between a disjunctive top-k query and a conjunctive query in terms of scoring?

- Concept: Inverted index structure and term impact scores.
  - Why needed here: The methods rely on precomputed term scores stored as postings and require fast lookups into the inverted index.
  - Quick check question: How are term impact scores typically stored and accessed in a standard inverted index?

- Concept: Quantile estimation and the Mean Under-Prediction Fraction (MUF) metric.
  - Why needed here: The paper's evaluation and comparison are based on MUF, and methods extend quantile-based approaches.
  - Quick check question: How is MUF calculated, and why is it preferred over other accuracy measures in this context?

## Architecture Onboarding

- Component map:
  - Query log analyzer -> Subset selector -> Prefix generator -> Index builder -> Estimator engine -> Evaluation module

- Critical path:
  1. Query arrives.
  2. Identify available subsets for query.
  3. Fetch and process prefix postings up to access budget.
  4. Combine scores, perform lookups up to lookup budget.
  5. Select k-th highest accumulated score as threshold estimate.

- Design tradeoffs:
  - Space vs. accuracy: Larger prefixes and more subsets improve accuracy but increase storage.
  - Lookup budget vs. CPU cost: More lookups improve estimates but increase latency.
  - Subset size vs. coverage: Larger subsets give tighter bounds but may not cover many queries.

- Failure signatures:
  - Overestimation: Indicates lookup budget insufficient or incorrect score combination.
  - Low MUF improvement: Suggests prefix structures too small or subsets not representative.
  - High CPU time: Lookup budget or access budget too large for available data.

- First 3 experiments:
  1. Run estimator with zero lookup budget; verify MUF matches baseline quantile method.
  2. Increase lookup budget incrementally; measure MUF improvement and CPU time.
  3. Vary subset selection (singles, pairs, triples); compare MUF and storage requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do threshold estimation techniques perform on learned sparse indexes using other ranking functions beyond BM25, Query Likelihood, DocT5Query, and DeepImpact?
- Basis in paper: [explicit] The paper evaluates methods on BM25, QLD, DocT5Query, and DeepImpact but notes there are other ranking functions used in learned sparse indexing approaches.
- Why unresolved: The paper focuses on these four ranking functions and does not explore other possible functions that could be used in learned sparse indexes.
- What evidence would resolve it: Experiments comparing threshold estimation accuracy and efficiency across a broader range of ranking functions used in learned sparse indexing methods.

### Open Question 2
- Question: Can the proposed threshold estimation methods be adapted to work efficiently with query-dependent term weights used in some learned sparse indexing approaches like COIL and uniCOIL?
- Basis in paper: [inferred] The paper mentions that methods like COIL and uniCOIL have query-dependent term weights that require additional ideas to make the approach work, but does not explore these adaptations.
- Why unresolved: The paper focuses on methods that assume precomputed term scores and does not address the challenges of handling query-dependent weights.
- What evidence would resolve it: Development and evaluation of modified threshold estimation techniques that can handle query-dependent term weights in learned sparse indexes.

### Open Question 3
- Question: What is the optimal trade-off between space usage and accuracy for threshold estimation methods when applied to extremely large document collections?
- Basis in paper: [explicit] The paper explores different prefix configurations and their impact on space usage and accuracy, but does not extensively study the trade-offs for very large collections.
- Why unresolved: The experiments are conducted on relatively large but not extremely large collections, and the paper does not provide a comprehensive analysis of space-accuracy trade-offs at scale.
- What evidence would resolve it: Experiments on extremely large document collections (e.g., web-scale) comparing various threshold estimation methods with different space configurations and their resulting accuracy and efficiency.

## Limitations

- The subset selection from query logs may not generalize well to unseen queries or future query distributions.
- The methods rely on accurate and complete term scores from the inverted index; any errors or omissions can propagate to threshold estimates.
- The trade-off between lookup budget and accuracy is dataset- and query-dependent; optimal settings may vary significantly across collections.

## Confidence

- **High**: The core mechanism of storing term scores in prefix postings and combining them to avoid double-counting is sound and well-supported by the experimental results.
- **Medium**: The improvements in MUF over baseline quantile methods are demonstrated, but the extent of gains may vary based on query log characteristics and dataset properties.
- **Low**: The long-term effectiveness of the subset selection strategy in dynamic query environments is uncertain without further validation.

## Next Checks

1. Test the subset selection and threshold estimation methods on a query log from a different domain or time period to assess generalizability.
2. Introduce controlled errors in term scores and measure the impact on threshold estimate accuracy to quantify robustness.
3. Compare the proposed methods against alternative threshold estimation techniques, such as sampling-based or machine learning approaches, on the same datasets and metrics.