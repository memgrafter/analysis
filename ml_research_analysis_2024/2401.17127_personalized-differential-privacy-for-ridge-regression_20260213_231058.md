---
ver: rpa2
title: Personalized Differential Privacy for Ridge Regression
arxiv_id: '2401.17127'
source_url: https://arxiv.org/abs/2401.17127
tags:
- loss
- privacy
- jorgensen
- test
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Personalized-DP Output Perturbation (PDP-OP),
  a novel algorithm for training Ridge regression models under personalized differential
  privacy, where each data point can specify its own privacy level. The core idea
  is to re-weight data points inversely proportional to their privacy requirements
  and add Gaussian noise to the output.
---

# Personalized Differential Privacy for Ridge Regression

## Quick Facts
- arXiv ID: 2401.17127
- Source URL: https://arxiv.org/abs/2401.17127
- Reference count: 40
- Primary result: PDP-OP achieves 100× lower error than non-personalized DP on synthetic and real datasets when privacy requirements vary across data points

## Executive Summary
This paper introduces Personalized-DP Output Perturbation (PDP-OP), a novel algorithm for training Ridge regression models under personalized differential privacy, where each data point can specify its own privacy level. The core idea is to re-weight data points inversely proportional to their privacy requirements and add Gaussian noise to the output. The authors provide rigorous privacy proofs and accuracy guarantees, showing that their method guarantees εi-differential privacy for each data point i. Empirically, PDP-OP significantly outperforms standard non-personalized DP (up to 100× lower error) and a prior sampling-based personalized DP method (Jorgensen et al., 2015) across synthetic and real datasets.

## Method Summary
The method trains a Ridge regression model by first computing a weighted non-private estimate where each data point's weight is proportional to its privacy budget (wi = εi / Σj εj). The algorithm then adds Gaussian noise to this estimate, with the noise parameter scaled by the sum of all privacy budgets. This achieves εi-differential privacy for each data point i while maintaining strong accuracy guarantees that improve when privacy requirements vary across the dataset.

## Key Results
- PDP-OP provides εi-differential privacy for each data point through inverse re-weighting and output perturbation
- On synthetic data with highly variable privacy requirements, PDP-OP achieves up to 100× lower error than non-personalized DP
- PDP-OP significantly outperforms the sampling-based method of Jorgensen et al. (2015) across all tested datasets
- Accuracy improvements are most pronounced when there is high variability in privacy requirements among data points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized-DP Output Perturbation (PDP-OP) provides εi-differential privacy for each data point by re-weighting data points inversely proportional to their privacy requirements and adding Gaussian noise to the output.
- Mechanism: Each data point i is assigned a weight wi = εi / Σj εj, making data points with stronger privacy requirements (lower εi) contribute less to the final model. Gaussian noise with parameter η = λ / (2√d(B(λ)+1) · Σj εj) is then added to the output to achieve personalized privacy.
- Core assumption: The relationship between weights and privacy levels is monotonic - lower weights correspond to less information leakage and better privacy.
- Evidence anchors:
  - [abstract] "The core idea is to re-weight data points inversely proportional to their privacy requirements and add Gaussian noise to the output."
  - [section] "The smaller the weight wi, the smaller the impact the i-th data point has on the output model."
  - [corpus] Weak evidence - corpus neighbors discuss differential privacy but don't specifically address personalized privacy via re-weighting.
- Break condition: If the monotonic relationship between weights and privacy leakage breaks down, or if the noise addition doesn't properly scale with the weighted sensitivity.

### Mechanism 2
- Claim: The personalized privacy guarantees are achieved through sensitivity pre-processing by manipulating the ℓ2-sensitivity of the ridge regression minimizer with respect to each data point.
- Mechanism: By re-weighting data points, the gradient difference across neighboring databases becomes proportional to wi. Combined with strong convexity (parameter 2λ), this bounds the ℓ2-sensitivity of ¯θ as a function of wi/λ.
- Core assumption: The ridge regression loss function is strongly convex with parameter 2λ, and the gradient difference across neighboring databases can be bounded.
- Evidence anchors:
  - [section] "The idea of the proof is similar to that of the output perturbation technique of Chaudhuri et al. [2011], adapted to Ridge regression and personalized DP."
  - [section] "Combined with the fact that the loss function is strongly convex with strong convexity parameter 2 λ, ¯θ cannot change too much across two neighbouring databases."
  - [corpus] Weak evidence - corpus neighbors discuss sensitivity in differential privacy but not specifically for personalized ridge regression.
- Break condition: If the strong convexity assumption fails or if the gradient difference cannot be properly bounded as a function of wi.

### Mechanism 3
- Claim: The accuracy of the personalized-DP ridge regression model is theoretically bounded and improves over non-personalized DP when privacy requirements vary across data points.
- Mechanism: The accuracy bound consists of three terms: bias due to ridge regression itself, noise added for privacy (scaling as 1/Pnj=1 εj), and label noise. The privacy-dependent term improves when personalization allows using the average privacy budget rather than the minimum.
- Core assumption: Labels are generated from a linear model with Gaussian noise, and the feature matrix has full rank when λ is small.
- Evidence anchors:
  - [section] "Our bound starts with a bias term: ∥θ∗∥ · (1 + λmin(Pn i=1 wixix⊤ i)/λ). This term controls the bias due to Ridge regression itself, independently of the noise added for privacy."
  - [section] "Importantly, note that this η is proportional to Pn j=1 εj, giving us an accuracy bound where the privacy dependency that evolves with 1/Pn j=1 εj."
  - [corpus] Weak evidence - corpus neighbors discuss privacy-accuracy tradeoffs but not specifically for personalized DP ridge regression.
- Break condition: If the linear model assumption fails or if the feature matrix is rank-deficient, leading to unavoidable bias regardless of privacy technique.

## Foundational Learning

- Concept: Differential Privacy fundamentals (ε-DP, neighboring datasets, privacy loss)
  - Why needed here: Understanding how personalized privacy differs from standard DP is crucial for grasping the motivation and mechanism
  - Quick check question: What is the key difference between standard ε-DP and εi-personalized DP in terms of how privacy guarantees are applied across data points?

- Concept: Sensitivity and its role in differential privacy
  - Why needed here: The paper's privacy proof relies on bounding the ℓ2-sensitivity of the ridge regression minimizer with respect to each data point
  - Quick check question: How does re-weighting data points affect the sensitivity of the output with respect to those data points?

- Concept: Ridge regression optimization and strong convexity
  - Why needed here: The algorithm solves a weighted ridge regression problem, and the privacy proof relies on strong convexity of the loss function
  - Quick check question: What is the strong convexity parameter of the ridge regression loss function with regularization parameter λ?

## Architecture Onboarding

- Component map: Dataset D -> Weight computation (wi = εi/Σj εj) -> Weighted ridge regression minimizer ¯θ -> Gaussian noise sampling Z -> Private estimator ˆθ

- Critical path: Data preprocessing → Weight computation (wi = εi/Σj εj) → Ridge regression computation → Noise sampling → Final output

- Design tradeoffs: Re-weighting vs. data sampling for personalized privacy; output perturbation vs. objective perturbation for privacy implementation

- Failure signatures: Poor accuracy despite personalization could indicate improper weight scaling; privacy guarantee failures could indicate incorrect noise parameter computation

- First 3 experiments:
  1. Verify weight computation: Given ε = [0.01, 0.2, 1.0], confirm wi = [0.0083, 0.1667, 0.8333]
  2. Test sensitivity bound: Compute Δi¯θ for synthetic data and verify it scales as wi/λ
  3. Validate noise distribution: Confirm that added noise Z follows the specified Gaussian distribution with parameter η

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of weights in the loss function affect the bias-variance trade-off in personalized DP ridge regression?
- Basis in paper: [explicit] The paper discusses the choice of weights wi and their impact on the sensitivity and privacy guarantees, but does not explore their impact on the bias-variance trade-off.
- Why unresolved: The theoretical analysis focuses on privacy and accuracy guarantees, but does not explicitly address the impact of weight choice on the bias-variance trade-off.
- What evidence would resolve it: Experimental results comparing the performance of different weight choices on synthetic and real datasets, with a focus on the bias and variance of the estimates.

### Open Question 2
- Question: How does the personalized DP framework perform when the assumption of linear relationship between features and labels is violated?
- Basis in paper: [explicit] The paper assumes a linear relationship between features and labels for theoretical analysis, but acknowledges that this may not hold in practice.
- Why unresolved: The experiments on real data use a linear model, but do not explicitly test the performance when the linearity assumption is violated.
- What evidence would resolve it: Experiments on real datasets with non-linear relationships between features and labels, comparing the performance of personalized DP to standard DP and other methods.

### Open Question 3
- Question: How does the performance of personalized DP scale with the dimensionality of the feature space?
- Basis in paper: [explicit] The paper provides theoretical accuracy bounds that depend on the dimensionality d, but does not explore the empirical performance as d increases.
- Why unresolved: The experiments are conducted on datasets with fixed dimensionality, and do not explore the scaling behavior as the number of features increases.
- What evidence would resolve it: Experiments on synthetic and real datasets with varying dimensionality, comparing the performance of personalized DP to standard DP and other methods, and analyzing the scaling behavior of the accuracy and privacy guarantees.

## Limitations
- The empirical evaluation uses synthetic data that directly satisfies the paper's assumptions (bounded features, linear relationships), potentially overstating real-world applicability
- Performance on datasets with high feature correlation or non-uniform feature distributions remains untested
- The noise sampling procedure requires careful implementation to ensure correct distributional properties

## Confidence
- **High confidence**: The core mechanism of re-weighting and output perturbation for personalized privacy is well-founded and theoretically sound
- **Medium confidence**: Empirical results showing performance gains are compelling but limited to specific datasets
- **Low confidence**: The practical significance of achieving εi-differential privacy for each individual data point in real-world applications

## Next Checks
1. **Robustness testing**: Evaluate PDP-OP on datasets with high feature correlation or non-uniform feature distributions to assess performance degradation beyond synthetic data assumptions
2. **Scalability analysis**: Test the algorithm's performance and runtime on larger datasets (n > 1000, d > 50) to verify computational efficiency and privacy-accuracy trade-offs at scale
3. **Alternative privacy definitions**: Compare PDP-OP's personalized DP guarantees against other privacy frameworks (e.g., Rényi DP, concentrated DP) to assess the practical benefits of εi-personalized DP over these alternatives