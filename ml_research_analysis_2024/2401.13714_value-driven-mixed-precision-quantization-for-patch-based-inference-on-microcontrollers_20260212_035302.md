---
ver: rpa2
title: Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers
arxiv_id: '2401.13714'
source_url: https://arxiv.org/abs/2401.13714
tags:
- quantization
- inference
- accuracy
- feature
- patch-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying neural networks
  on microcontroller units (MCUs) with limited memory and computational resources.
  The authors propose QuantMCU, a novel patch-based inference method that utilizes
  value-driven mixed-precision quantization to reduce redundant computation.
---

# Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers

## Quick Facts
- arXiv ID: 2401.13714
- Source URL: https://arxiv.org/abs/2401.13714
- Reference count: 23
- Reduces computation by 2.2x on average while maintaining comparable model accuracy compared to state-of-the-art patch-based inference methods on MCUs

## Executive Summary
This paper addresses the challenge of deploying neural networks on microcontroller units (MCUs) with limited memory and computational resources. The authors propose QuantMCU, a novel patch-based inference method that utilizes value-driven mixed-precision quantization to reduce redundant computation. QuantMCU employs value-driven patch classification (VDPC) to maintain model accuracy by classifying patches into outlier and non-outlier classes based on the presence of outlier values. For non-outlier class patches, QuantMCU uses value-driven quantization search (VDQS) to reduce search time. Experimental results on real-world MCU devices show that QuantMCU can significantly reduce computation while maintaining comparable model accuracy compared to state-of-the-art patch-based inference methods.

## Method Summary
QuantMCU is a value-driven mixed-precision quantization framework for patch-based inference on MCUs. It uses value-driven patch classification (VDPC) to maintain accuracy by classifying patches into outlier and non-outlier classes based on the presence of outlier values. VDPC ensures that critical patches (with outliers) are fully preserved in 8-bit quantization. For non-outlier class patches, QuantMCU employs value-driven quantization search (VDQS) to reduce search time. VDQS introduces a new quantization search metric that considers both computation and accuracy, using entropy as an accuracy representation to avoid additional training. The method also adopts an iterative approach to determine the bitwidth of each feature map to further accelerate the search process. The framework is evaluated on real-world MCU devices using ImageNet and Pascal VOC datasets, demonstrating significant reductions in computation and inference latency while maintaining model accuracy.

## Key Results
- Reduces BitOPs and inference latency by 2.2x and 1.5x on average compared to patch-based inference methods
- Decreases peak memory usage by 1.26x-2.90x
- Maintains comparable model accuracy while achieving significant computation reduction
- Validated on real-world MCU devices (Arduino Nano 33 BLE Sense and STM32H743) with multiple neural network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value-driven patch classification (VDPC) reduces accuracy loss by distinguishing patches with outlier values.
- Mechanism: Patches are split into two classes based on the presence of outlier values. Outlier class patches are fully quantized to 8-bit, while non-outlier class patches undergo mixed-precision quantization to preserve model accuracy.
- Core assumption: Outlier values dominate model accuracy, and patches without them can tolerate aggressive quantization without accuracy loss.
- Evidence anchors:
  - [abstract] "VDPC classifies patches into two classes based on whether they contain outlier values."
  - [section III-A] "Based on the idea presented in [17], we designate the value around 0 as the non-outlier value, and the value far away from 0 as the outlier value."
- Break condition: If outlier detection fails (e.g., incorrect ϕ threshold), mixed-precision quantization will be applied to patches that need 8-bit quantization, causing accuracy loss.

### Mechanism 2
- Claim: Value-driven quantization search (VDQS) accelerates search by using entropy as an accuracy proxy and iterative bitwidth determination.
- Mechanism: VDQS uses activation value entropy to estimate accuracy impact without retraining, and employs an iterative search algorithm to determine bitwidth per feature map based on a combined quantization score (computation + accuracy).
- Core assumption: Higher activation value entropy correlates with better model accuracy preservation under quantization.
- Evidence anchors:
  - [abstract] "VDQS introduces a novel quantization search metric that takes into account both computation and accuracy, and it employs entropy as an accuracy representation to avoid additional training."
  - [section III-B] "We employ activation value entropy as the representation of accuracy so as to avoid training the model at each step of the quantization."
- Break condition: If entropy does not correlate well with actual accuracy (e.g., due to activation distribution shifts), the quantization choices may lead to suboptimal accuracy or computation.

### Mechanism 3
- Claim: Combining VDPC and VDQS in patch-based inference reduces redundant computation while maintaining memory efficiency.
- Mechanism: VDPC ensures that critical patches (with outliers) are fully preserved, and VDQS aggressively reduces computation for non-critical patches, resulting in overall lower BitOPs and inference latency.
- Core assumption: Patch-based inference inherently has redundant computation due to overlapping values, which mixed-precision quantization can reduce.
- Evidence anchors:
  - [abstract] "QuantMCU can reduce computation by 2.2x on average while maintaining comparable model accuracy compared to the state-of-the-art patch-based inference methods."
  - [section I] "However, this technique suffers from severe redundant computation overhead, leading to a substantial increase in execution latency."
- Break condition: If the redundancy in patch-based inference is overestimated or if the quantization search does not reduce computation enough, the claimed speedup may not materialize.

## Foundational Learning

- Concept: Activation value distribution and outlier detection
  - Why needed here: VDPC relies on identifying outlier values to classify patches; understanding activation distributions is key.
  - Quick check question: How is an outlier value defined in this paper, and what distribution assumption is used?

- Concept: Entropy as an accuracy proxy
  - Why needed here: VDQS uses entropy to avoid retraining; understanding how entropy relates to model expressiveness is essential.
  - Quick check question: Why does higher activation value entropy imply better accuracy preservation after quantization?

- Concept: Mixed-precision quantization tradeoffs
  - Why needed here: VDQS assigns different bitwidths to feature map; understanding the accuracy/computation tradeoff is crucial.
  - Quick check question: How does reducing bitwidth affect both computation and accuracy in neural network inference?

## Architecture Onboarding

- Component map:
  - Input preprocessing → Patch splitting → VDPC (outlier detection) → Feature map quantization (8-bit or mixed-precision) → Inference execution → Output aggregation
  - VDQS runs during quantization search to assign bitwidths per feature map

- Critical path:
  - Patch splitting → VDPC classification → VDQS quantization search → Quantized inference

- Design tradeoffs:
  - Accuracy vs. computation: VDPC preserves accuracy by avoiding aggressive quantization on critical patches; VDQS balances computation savings and accuracy.
  - Search time vs. optimality: VDQS avoids retraining for speed but may miss fine-tuned accuracy gains from training-aware methods.

- Failure signatures:
  - Accuracy collapse: Incorrect ϕ threshold or entropy proxy leads to inappropriate quantization.
  - Increased latency: If VDQS does not reduce BitOPs enough or if patch splitting overhead dominates.
  - Memory overrun: If bitwidth assignments violate MCU memory constraints.

- First 3 experiments:
  1. Verify VDPC outlier detection: Run a small network, visualize activation distributions, and confirm outlier vs. non-outlier separation.
  2. Test VDQS quantization scoring: Quantize a feature map with different bitwidths, measure entropy and BitOPs, and confirm quantization score ranking.
  3. Compare full pipeline: Run patch-based inference with and without QuantMCU on a small model, measure accuracy, BitOPs, and latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the value of the threshold parameter ϕ in VDPC affect the balance between model accuracy and computation reduction in different neural network architectures?
- Basis in paper: [explicit] The paper mentions that ϕ should be properly set and that an excessively large ϕ will eliminate important information conveyed by outlier values, resulting in a sharp decline in accuracy, while an extremely small ϕ cannot fully reduce redundant computation. It also shows experimental results testing different configurations of ϕ.
- Why unresolved: The paper only tests ϕ on MobileNetV2 and shows its impact on that specific architecture. The optimal value of ϕ may vary depending on the network architecture, dataset characteristics, and application requirements.
- What evidence would resolve it: Comprehensive experiments testing different ϕ values across various neural network architectures (e.g., ResNet, Inception, SqueezeNet) and datasets, with detailed analysis of the trade-offs between accuracy and computation reduction for each combination.

### Open Question 2
- Question: Can the entropy-based quantization score in VDQS be further improved by incorporating additional metrics that capture model accuracy more comprehensively?
- Basis in paper: [explicit] The paper uses activation value entropy as a representation of accuracy to avoid additional training, but acknowledges that this is an approximation of the actual distribution of activation values.
- Why unresolved: While entropy provides a computationally efficient proxy for accuracy, it may not capture all aspects of model performance. Other metrics like feature map variance, gradient information, or task-specific measures could potentially provide a more accurate representation of accuracy changes during quantization.
- What evidence would resolve it: Comparative studies testing VDQS with different accuracy representation metrics (e.g., entropy combined with variance,

## Limitations

- The exact implementation details of the quantization search metric and iterative search algorithm in VDQS are not specified.
- The optimal threshold value (ϕ) for outlier detection in VDPC is not provided, which could significantly impact results.
- The entropy proxy for accuracy may not generalize well across different model architectures and datasets.

## Confidence

- High Confidence: The claim that patch-based inference inherently has redundant computation is well-established in the literature and supported by the cited sources.
- Medium Confidence: The effectiveness of VDPC in maintaining accuracy by classifying patches based on outlier values is plausible but depends heavily on the correctness of the outlier detection threshold.
- Low Confidence: The assertion that entropy serves as a reliable accuracy proxy without retraining requires further validation, as the correlation between entropy and actual accuracy under quantization may vary significantly across different models and datasets.

## Next Checks

1. **Validate VDPC Threshold Sensitivity:** Systematically vary the outlier detection threshold (ϕ) and measure its impact on both accuracy preservation and computation reduction across multiple models and datasets.

2. **Cross-Architecture Entropy Correlation:** Test the entropy-accuracy correlation assumption by comparing VDQS quantization choices against ground-truth accuracy measurements for a diverse set of neural network architectures.

3. **Memory Constraint Validation:** Verify that QuantMCU's bitwidth assignments consistently respect MCU memory constraints across different model sizes and input patch configurations, particularly for the most memory-constrained scenarios.