---
ver: rpa2
title: 'E2CL: Exploration-based Error Correction Learning for Embodied Agents'
arxiv_id: '2409.03256'
source_url: https://arxiv.org/abs/2409.03256
tags:
- agent
- feedback
- object
- action
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses misalignment between language model knowledge
  and embodied environment constraints, which causes agents to generate infeasible
  actions. It proposes Exploration-based Error Correction Learning (E2CL), which collects
  exploration-induced errors and environmental feedback through teacher-guided and
  teacher-free exploration, then trains the agent to self-generate feedback and correct
  its own errors.
---

# E2CL: Exploration-based Error Correction Learning for Embodied Agents

## Quick Facts
- arXiv ID: 2409.03256
- Source URL: https://arxiv.org/abs/2409.03256
- Authors: Hanlin Wang; Chak Tou Leong; Jian Wang; Wenjie Li
- Reference count: 13
- Primary result: E2CL improves executability by 15% and LCS by 10% over baselines in VirtualHome

## Executive Summary
E2CL addresses the critical misalignment between language model knowledge and embodied environment constraints that causes agents to generate infeasible actions. The method introduces Exploration-based Error Correction Learning that collects exploration-induced errors and environmental feedback through teacher-guided and teacher-free exploration phases. The agent is trained to self-generate feedback and correct its own errors, resulting in significantly improved executability and long-term coherence scores. E2CL enables smaller models to surpass larger models trained with behavior cloning alone while demonstrating strong self-correction capabilities.

## Method Summary
E2CL consists of three phases: (1) Pre-tuning on expert trajectories using behavior cloning for 1 epoch, (2) Teacher-guided exploration collecting feedback and correction data by executing one-step actions guided by expert sub-trajectories, followed by teacher-free exploration collecting additional data through free exploration with LLM corrections, and (3) Joint fine-tuning on all three datasets (expert trajectories, feedback data, correction data) for 3 epochs. The speculative inference algorithm enables self-correction during deployment by generating feedback for proposed actions and correcting them if deemed non-executable.

## Key Results
- 15% improvement in executability over baselines in VirtualHome environment
- 10% improvement in LCS (Long-term Coherence Score) over baselines
- Smaller models trained with E2CL surpass larger models trained with behavior cloning alone
- Reduces various error types by over 24% across error classification analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-phase exploration provides complementary data coverage that improves alignment
- Mechanism: Teacher-guided exploration uses expert trajectories to collect high-quality feedback/correction pairs, while teacher-free exploration generates diverse environmental interactions capturing novel failure modes
- Core assumption: Expert trajectories alone cannot fully capture environmental constraints and failure scenarios
- Evidence anchors:
  - [abstract] "E2CL incorporates teacher-guided and teacher-free explorations to gather environmental feedback and correct erroneous actions."
  - [section 2.2] "To balance data diversity and quality, we propose a limited exploration scheme guided by expert trajectories, referred to as teacher-guided exploration (TGE)."
- Break condition: If teacher-guided exploration fails to provide meaningful correction data or teacher-free exploration generates irrelevant experiences

### Mechanism 2
- Claim: Speculative inference enables self-correction during deployment without external feedback
- Mechanism: During inference, agent generates action, reflects on executability by generating feedback, and corrects if feedback indicates non-executability
- Core assumption: Agent trained on feedback data can accurately predict executability of its own actions
- Evidence anchors:
  - [abstract] "The agent learns to provide feedback and self-correct, thereby enhancing its adaptability to target environments."
  - [section 2.3] "To utilize the learned abilities in the training phase, we propose speculative inference algorithm"
- Break condition: If agent's self-generated feedback quality degrades, speculative inference fails to catch errors

### Mechanism 3
- Claim: Training on both feedback and correction data enables dual error detection and resolution capabilities
- Mechanism: Agent trained to predict environmental feedback given action and context, and generate correct actions given erroneous actions and feedback
- Core assumption: Agent can learn both feedback prediction and correction generation from same exploration data
- Evidence anchors:
  - [section 2.2] "We train the agent to align with the environmental knowledge gathered from these datasets and to develop the ability to provide feedback and correct its own errors."
  - [section 3.3] "We observe that both Df and Dc are each beneficial for the agent, but lag behind the combination of them."
- Break condition: If feedback data quality is poor or correction data is sparse, agent may not learn effective correction strategies

## Foundational Learning

- Concept: Exploration-Exploitation tradeoff
  - Why needed here: Understanding why both teacher-guided (exploitation of expert knowledge) and teacher-free (exploration of novel scenarios) are necessary for comprehensive environment alignment
  - Quick check question: Why wouldn't using only teacher-guided exploration or only teacher-free exploration be sufficient for environment alignment?

- Concept: Reinforcement Learning vs Supervised Learning
  - Why needed here: Understanding limitations of pure RL (sparse rewards, inefficient convergence) versus pure supervised learning (incomplete coverage) that E2CL addresses
  - Quick check question: What are key limitations of using pure RL or pure supervised learning for embodied agent training that E2CL overcomes?

- Concept: Feedback loops in learning systems
  - Why needed here: Understanding how speculative inference creates feedback loop during inference, similar to how humans self-correct during task execution
  - Quick check question: How does speculative inference mechanism create self-correction feedback loop during agent's deployment?

## Architecture Onboarding

- Component map: Pre-tuning module -> Teacher-guided exploration -> Teacher-free exploration -> Training module -> Speculative inference module
- Critical path:
  1. Pre-tuning on expert data
  2. Teacher-guided exploration to collect initial feedback/correction pairs
  3. Teacher-free exploration to expand coverage
  4. Joint training on all three datasets
  5. Deployment with speculative inference
- Design tradeoffs:
  - Using GPT-4o for corrections adds quality but introduces external dependency
  - Speculative inference adds inference-time computation but reduces execution errors
  - Collecting both feedback and correction data increases training data requirements but enables dual capabilities
- Failure signatures:
  - Poor executability scores: Issues with speculative inference or feedback generation quality
  - Low LCS scores: Planning module may not be learning correct action sequences
  - Slow convergence: May need more diverse exploration data or better pre-training
- First 3 experiments:
  1. Ablation study: Train without feedback data (Df) to verify its contribution to performance
  2. Ablation study: Train without correction data (Dc) to verify its contribution to performance
  3. Model size study: Compare performance across different model sizes (small, base, large) to identify scaling effects

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several implicit ones through its limitations section, particularly regarding the generalizability of E2CL to environments beyond VirtualHome and the scalability of the approach to more complex embodied tasks.

## Limitations
- Limited evaluation to VirtualHome environment, raising questions about performance on other embodied tasks
- Reliance on GPT-4o for correction generation introduces external dependency that may not be consistently available
- Does not specify exact templates for feedback and correction instructions used in data collection
- Does not address computational overhead of speculative inference compared to direct execution

## Confidence
- **High Confidence**: The core claim that combining teacher-guided and teacher-free exploration improves executability over pure behavior cloning is well-supported by empirical results showing 15% improvement
- **Medium Confidence**: The speculative inference mechanism's effectiveness during deployment is supported by in-domain experiments but lacks extensive testing across diverse environments or longer task sequences
- **Medium Confidence**: The claim that E2CL enables smaller models to outperform larger models is demonstrated but could benefit from more extensive scaling studies

## Next Checks
1. **Ablation Study on Data Quality**: Systematically vary the quality of feedback/correction instructions (qf, qc) in teacher-guided exploration to quantify their impact on executability and LCS scores, isolating the effect of data quality from exploration coverage

2. **Cross-Environment Transfer**: Evaluate E2CL-trained agents on a different embodied environment (e.g., ALFRED or Habitat) to assess whether the self-correction capabilities transfer beyond VirtualHome, testing the generalizability of the learned environmental alignment

3. **Long-Horizon Task Evaluation**: Extend the task sequences beyond the current scope to evaluate speculative inference performance on tasks requiring 20+ steps, measuring error accumulation and correction effectiveness over extended deployments