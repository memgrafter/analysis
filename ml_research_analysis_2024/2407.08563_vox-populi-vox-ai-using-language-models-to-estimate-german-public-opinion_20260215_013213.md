---
ver: rpa2
title: Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion
arxiv_id: '2407.08563'
source_url: https://arxiv.org/abs/2407.08563
tags:
- page
- arxiv
- availableat
- https
- party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the extent to which large language models
  (LLMs) can estimate public opinion in Germany using voting behavior as a case study.
  Personas were generated based on individual-level data from the 2017 German Longitudinal
  Election Study, including demographics, party affiliations, and attitudes on key
  issues.
---

# Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion

## Quick Facts
- arXiv ID: 2407.08563
- Source URL: https://arxiv.org/abs/2407.08563
- Reference count: 40
- Primary result: GPT-3.5's vote predictions significantly diverged from German election survey data, with 46% matching accuracy and systematic overestimation of left-leaning parties.

## Executive Summary
This study investigates whether large language models can accurately estimate public opinion in Germany by predicting voting behavior. Using personas based on individual-level data from the 2017 German Longitudinal Election Study, researchers prompted GPT-3.5 to predict vote choices for each respondent. The findings reveal substantial limitations: GPT-3.5's predictions differed markedly from actual survey responses, overestimating support for the Green and Left parties while underestimating FDP and AfD votes. With an overall F1 score of 0.46, the model performed better for centrist parties but struggled with right-leaning ones. These results highlight the challenges of using LLMs for public opinion estimation outside the U.S. context.

## Method Summary
The researchers created synthetic personas matching 1,905 respondents from the 2017 German Longitudinal Election Study, incorporating demographics, party affiliations, and attitudes on immigration and income inequality. These personas were used to prompt GPT-3.5 (text-davinci-003 model) in German, generating five completions per persona. The predicted vote choices were extracted and compared to actual survey responses using precision, recall, and F1 scores. The study also employed multinomial regression to analyze which factors influenced predictions and conducted robustness checks using non-imputed data.

## Key Results
- GPT-3.5's vote predictions matched actual survey data only 46% of the time
- The model systematically overestimated Green and Left party votes while underestimating FDP and AfD support
- F1 score across all parties was 0.46, with better performance for centrist parties (CDU/CSU, SPD) than right-leaning ones
- Predictions were heavily influenced by party identification and ideology, missing more nuanced individual factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can simulate survey respondents by generating conditional probabilities based on their training data.
- Mechanism: LLMs generate text output by sampling from a conditional probability distribution learned from large internet text corpora, reflecting patterns of attitudes and behaviors prevalent in the population.
- Core assumption: The training data accurately represents the target population's opinions and behaviors.
- Evidence anchors:
  - [abstract] "Their text output to a request represents a conditional probability based on the training data and the specific contextual information provided in the request."
  - [section 2] "LLMs are trained on large, selected corpora of internet-sourced data...which potentially reflect attitudes and behaviors prevalent in the population."
- Break condition: If the training data is biased or unrepresentative of the target population, the LLM's predictions will be inaccurate.

### Mechanism 2
- Claim: Prompting LLMs with individual-level information can elicit predictions of voting behavior.
- Mechanism: By providing personas with demographic, socio-economic, and attitudinal information, the LLM is prompted to complete the persona's vote choice based on learned patterns in its training data.
- Core assumption: The provided individual-level information is sufficient to elicit accurate predictions from the LLM.
- Evidence anchors:
  - [abstract] "We generate a synthetic sample of personas matching the individual characteristics of the 2017 German Longitudinal Election Study respondents. We ask the LLM GPT-3.5 to predict each respondent's vote choice..."
  - [section 3] "The personas include individual-level information on 13 of the most common factors associated with voting behavior...These variables comprise age, gender, educational attainment, income, employment status..."
- Break condition: If the provided information is incomplete or not representative of the factors influencing voting behavior, the LLM's predictions will be inaccurate.

### Mechanism 3
- Claim: Comparing LLM-generated predictions to survey data reveals limitations in using LLMs for public opinion estimation.
- Mechanism: By benchmarking LLM predictions against reported vote choices from a national election study, discrepancies in accuracy and predictive performance can be identified.
- Core assumption: Survey data provides a reliable benchmark for evaluating LLM performance.
- Evidence anchors:
  - [abstract] "We compare these predictions to the survey-based estimates on the aggregate and subgroup levels."
  - [section 4] "AcrossthefivesamplesdrawnfromGPT, thereisverylittlevarianceintermsof whethertheGPTpredictionmatchesthevotechoiceindividualrespondentsreportedtoGLES.Onaverage,only46%ofGPT-3.5â€™spredictionsmatchthesurveydata."
- Break condition: If the survey data itself is biased or unrepresentative, the evaluation of LLM performance may be misleading.

## Foundational Learning

- Concept: Conditional probability and language modeling
  - Why needed here: Understanding how LLMs generate text based on learned probability distributions is crucial for interpreting their predictions and limitations.
  - Quick check question: What is the difference between unconditional and conditional probability in the context of language modeling?

- Concept: Survey methodology and sampling
  - Why needed here: Knowledge of survey design, sampling techniques, and potential sources of bias is essential for evaluating the reliability of survey data as a benchmark for LLM performance.
  - Quick check question: What are some common sources of bias in survey data, and how might they impact the evaluation of LLM predictions?

- Concept: Voting behavior and political science
  - Why needed here: Understanding the factors that influence voting behavior and the complexities of multi-party systems is crucial for interpreting LLM predictions and identifying potential limitations.
  - Quick check question: How does voting behavior differ in multi-party systems compared to two-party systems, and what implications does this have for LLM predictions?

## Architecture Onboarding

- Component map: Data Collection -> Persona Generation -> Prompt Creation -> LLM Interaction -> Vote Choice Extraction -> Analysis
- Critical path: 1. Data collection and preprocessing, 2. Persona generation and prompt creation, 3. LLM interaction and vote choice extraction, 4. Model performance evaluation and comparison to survey data, 5. Robustness checks and sensitivity analyses
- Design tradeoffs:
  - Using German vs. English prompts: Potential for better representation of native attitudes vs. larger English training data
  - Imputation vs. non-imputation: Complete personas vs. potential bias from imputation
  - Multiple completions vs. single completion: Capturing LLM uncertainty vs. computational efficiency
- Failure signatures:
  - Low F1 scores and matching prediction rates
  - Systematic overestimation or underestimation of specific party votes
  - Failure to capture nuanced relationships between individual characteristics and voting behavior
  - High variance in LLM predictions across multiple completions
- First 3 experiments:
  1. Evaluate the impact of prompt language (German vs. English) on LLM predictions and accuracy.
  2. Compare the performance of different LLMs (e.g., GPT-3.5 vs. GPT-4) on the same task.
  3. Investigate the effect of providing different levels of detail in the personas on LLM predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions (e.g., prompt wording, model temperature, target population characteristics) do LLMs most accurately estimate public opinion?
- Basis in paper: [explicit] The authors note that GPT-3.5's performance varied significantly and call for further research into the "conditions under which LLMs can be used for public opinion research."
- Why unresolved: The study only tested GPT-3.5 under one configuration and in one context (Germany, voting behavior). The impact of prompt engineering, model selection, and target population differences remains unexplored.
- What evidence would resolve it: Comparative studies testing multiple LLM configurations, prompt formats, and target populations against high-quality benchmark surveys.

### Open Question 2
- Question: How do LLMs' predictions of public opinion compare to traditional survey methods in terms of bias, coverage, and measurement error?
- Basis in paper: [explicit] The authors note that "it is possible that LLMs' predictions better mirror the target population because they do not suffer from the coverage and measurement error of a particular survey" but do not test this directly.
- Why unresolved: The study treats survey data as ground truth without directly comparing LLM predictions to actual election outcomes or other validation methods.
- What evidence would resolve it: Studies comparing LLM predictions to actual election outcomes, alongside traditional polling methods, to quantify differences in accuracy and bias.

### Open Question 3
- Question: Can LLMs be used to estimate public opinion for specific minority or marginalized subgroups?
- Basis in paper: [inferred] The authors note that GPT-3.5 performed poorly for non-conforming voter groups (e.g., FDP, AfD) and suggest this may reflect underrepresentation in training data.
- Why unresolved: The study does not explicitly test LLM performance for minority subgroups, and the implications for estimating opinions of marginalized populations remain unexplored.
- What evidence would resolve it: Targeted studies benchmarking LLM predictions for minority subgroups against specialized surveys or other validation methods.

## Limitations
- GPT-3.5 predictions showed significant systematic bias, overestimating left-leaning parties while underestimating right-leaning ones
- The model's reliance on party identification and ideology failed to capture nuanced individual factors affecting vote choice
- German prompts may have limited performance given the LLM's primarily English training data

## Confidence
- Low Confidence: LLM's ability to accurately estimate German public opinion across all party affiliations
- Medium Confidence: LLM's performance for centrist parties where predictions showed closer alignment with survey data
- High Confidence: Methodology for evaluating LLM performance against survey data is robust

## Next Checks
1. Test the impact of using English prompts versus German prompts on prediction accuracy
2. Replicate the study using comparable election data from countries with different political systems
3. Compare GPT-3.5 performance with more recent LLM architectures and smaller, domain-specific models trained on German political discourse