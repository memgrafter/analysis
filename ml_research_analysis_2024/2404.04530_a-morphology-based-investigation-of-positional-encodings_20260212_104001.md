---
ver: rpa2
title: A Morphology-Based Investigation of Positional Encodings
arxiv_id: '2404.04530'
source_url: https://arxiv.org/abs/2404.04530
tags:
- language
- languages
- positional
- encoding
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between morphological
  complexity and the importance of positional encoding in transformer-based language
  models across 22 languages and 5 NLP tasks. The findings show that the importance
  of positional encoding decreases as morphological complexity increases.
---

# A Morphology-Based Investigation of Positional Encodings

## Quick Facts
- arXiv ID: 2404.04530
- Source URL: https://arxiv.org/abs/2404.04530
- Reference count: 24
- Primary result: Importance of positional encoding decreases with morphological complexity across 22 languages and 5 NLP tasks

## Executive Summary
This study investigates how morphological complexity influences the importance of positional encoding in transformer-based language models. The research reveals that languages with rich morphology, such as Hungarian, Finnish, and Turkish, exhibit minimal performance drops when positional encoding is removed, while analytic languages like Chinese and Vietnamese show significant drops. The findings demonstrate a strong negative correlation between morphological complexity and the relative decrease in performance when positional encoding is eliminated, particularly for syntactic tasks. These results highlight the need to consider morphological characteristics when designing and fine-tuning language models for diverse languages.

## Method Summary
The study employs a systematic approach to investigate positional encoding importance across 22 languages from 9 language families. Researchers use type-token ratio (TTR) from the Flores-200 benchmark as a proxy for morphological complexity and conduct experiments on 5 downstream tasks using monolingual BERT-base models. The key methodological innovation involves setting positional encodings to zero during fine-tuning to measure their importance, comparing baseline performance with PE-removed models. Experiments are conducted with 3 random trials, measuring relative performance decrease as (m-n)/n where m and n are metric scores for baseline and perturbed models.

## Key Results
- Syntactic tasks (POS tagging, NER, dependency parsing) show stronger sensitivity to positional encoding removal than semantic tasks (NLI, paraphrasing)
- Strong negative correlation between morphological complexity and relative performance decrease when positional encoding is removed
- Languages with rich morphology (Hungarian, Finnish, Turkish) show minimal performance drops, while analytic languages (Chinese, Vietnamese) show significant drops

## Why This Works (Mechanism)

### Mechanism 1
Morphological complexity reduces dependency on positional encoding for syntactic tasks. In morphologically rich languages, grammatical roles are marked explicitly through morphological features (e.g., case marking), allowing word order flexibility. Therefore, positional information is less critical for syntactic parsing accuracy. Core assumption: Morphological marking provides sufficient syntactic cues that positional encoding would otherwise supply. Break condition: If morphological markers fail to disambiguate syntactic roles or if word order still conveys essential syntactic information beyond morphological marking.

### Mechanism 2
Semantic tasks are less affected by positional encoding removal across all morphological types. Semantic understanding relies more on lexical meaning and contextual relationships than on exact word order, making positional encoding less critical for semantic tasks like NLI and paraphrasing. Core assumption: Semantic relationships are encoded in lexical semantics and contextual embeddings, not positional dependencies. Break condition: If semantic understanding requires specific syntactic structures that depend on word order, or if negation and scope ambiguity rely on positional relationships.

### Mechanism 3
Type-token ratio (TTR) serves as a reliable proxy for morphological complexity across languages. Higher TTR indicates more unique word forms relative to total tokens, reflecting richer morphological inflection and derivation patterns. Core assumption: Morphological diversity directly correlates with TTR values across languages with different morphological typologies. Break condition: If TTR conflates vocabulary size effects with true morphological complexity, or if languages with agglutinative morphology show lower TTR than expected.

## Foundational Learning

- Concept: Morphological typology (analytic vs synthetic vs polysynthetic languages)
  - Why needed here: Understanding how different languages encode grammatical relationships through morphology versus syntax is essential for interpreting the study's findings about positional encoding importance.
  - Quick check question: What distinguishes an analytic language like Chinese from a synthetic language like Finnish in terms of how they encode grammatical relationships?

- Concept: Positional encoding mechanisms in transformers
  - Why needed here: The study investigates how removing positional encoding affects model performance, requiring understanding of how positional information is encoded and utilized in transformer architectures.
  - Quick check question: How do absolute positional encodings differ from relative positional encodings in transformer models?

- Concept: Type-token ratio as a linguistic metric
  - Why needed here: The study uses TTR to quantify morphological complexity, so understanding what TTR measures and its limitations is crucial for interpreting the results.
  - Quick check question: What does a high type-token ratio indicate about a language's morphological system?

## Architecture Onboarding

- Component map: BERT-base model → fine-tuning pipeline → task-specific heads (NER, POS, parser, classifier) → evaluation metrics. Positional encoding removal is implemented by zeroing PE embeddings during fine-tuning.
- Critical path: Data preprocessing → model loading → positional encoding nullification → fine-tuning → evaluation → correlation analysis with morphological complexity.
- Design tradeoffs: Using fine-tuning rather than pre-training limits the study's ability to detect positional encoding importance, but reduces computational cost significantly.
- Failure signatures: Inconsistent performance drops across morphologically similar languages may indicate dataset quality issues or task-specific factors beyond morphology.
- First 3 experiments:
  1. Verify positional encoding removal by checking that PE embeddings are zeroed before fine-tuning begins.
  2. Run baseline and PE-removed models on a simple syntactic task (POS tagging) for one morphologically rich and one morphologically poor language to confirm expected patterns.
  3. Compute TTR values for all 22 languages using the Flores-200 corpus to validate the morphological complexity ranking.

## Open Questions the Paper Calls Out

1. How does removing positional encoding during pre-training affect performance across languages with varying morphological complexity?
   - Basis in paper: The authors acknowledge that their study focused on fine-tuning and state "We agree that pre-training without positional encoding would be a more holistic approach. However, due to limited computational resources, our primary focus was directed towards fine-tuning."
   - Why unresolved: The study only examined the effects of removing positional encoding during fine-tuning, not during pre-training. Pre-training without positional encoding could potentially amplify the observed differences across languages.
   - What evidence would resolve it: Pre-training language models without positional encoding for multiple languages and then fine-tuning them on downstream tasks, comparing performance to models pre-trained with positional encoding.

2. Does the impact of positional encoding removal vary based on the specific type of morphological complexity (e.g., agglutinative vs. fusional languages)?
   - Basis in paper: The paper discusses morphological complexity but doesn't deeply explore different types of morphological systems. It mentions that synthetic languages can be categorized into agglutinative (like Hungarian and Turkish) and fusional types, but doesn't analyze whether these subcategories show different patterns.
   - Why unresolved: The study used a general measure of morphological complexity (TTR) and didn't distinguish between different morphological typologies. The effects might differ between agglutinative and fusional languages.
   - What evidence would resolve it: A study that categorizes languages by their morphological typology and analyzes positional encoding impact separately for agglutinative, fusional, and other morphological types.

3. What is the relationship between morphological complexity and the ability of models to learn positional information without explicit positional encoding?
   - Basis in paper: The authors cite Haviv et al. (2022) who found that "a pre-trained RoBERTa large model without PE exhibits higher perplexities than position-informed models" but note this was limited to English. They also mention that "causal language models lacking explicit PE remain competitive with standard position-aware models."
   - Why unresolved: The paper only examines performance on downstream tasks, not the model's inherent ability to learn positional information from the data itself. It's unclear whether morphologically rich languages enable models to implicitly learn positional relationships.
   - What evidence would resolve it: Experiments measuring how well models without explicit positional encoding can reconstruct word order or predict positions in different languages, or comparing perplexities of language models without positional encoding across morphologically diverse languages.

## Limitations

- Reliance on type-token ratio as a proxy for morphological complexity may conflate vocabulary size effects with true morphological complexity
- Study focuses on fine-tuning rather than pre-training, potentially underestimating positional encoding importance
- Limited language sample (22 languages from 9 families) may not capture full diversity of morphological systems

## Confidence

**High confidence**: Syntactic tasks show stronger sensitivity to positional encoding removal than semantic tasks. Supported by consistent patterns across multiple experiments and aligns with linguistic theories about syntax vs semantics.

**Medium confidence**: Specific correlation between morphological complexity and positional encoding importance for syntactic tasks. Statistically significant but uncertain due to proxy measurement and limited language sample.

**Low confidence**: Mechanism explaining why semantic tasks are less affected by positional encoding removal. Theoretical explanation lacks direct empirical evidence.

## Next Checks

1. Validate TTR as morphological complexity proxy by comparing with gold-standard morphological complexity measures for a subset of languages.

2. Test pre-training vs fine-tuning by replicating experiments using models trained from scratch with and without positional encoding.

3. Examine extreme morphological cases by including languages with highly agglutinative or polysynthetic morphology to test pattern boundaries.