---
ver: rpa2
title: On Class Separability Pitfalls In Audio-Text Contrastive Zero-Shot Learning
arxiv_id: '2408.13068'
source_url: https://arxiv.org/abs/2408.13068
tags:
- audio
- training
- zero-shot
- cross-modal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of data leakage on audio-text contrastive
  zero-shot learning. The authors investigate whether zero-shot classification accuracy
  depends on the cross-modal training or the pre-trained unimodal backbones.
---

# On Class Separability Pitfalls In Audio-Text Contrastive Zero-Shot Learning

## Quick Facts
- arXiv ID: 2408.13068
- Source URL: https://arxiv.org/abs/2408.13068
- Reference count: 0
- Primary result: Zero-shot classification accuracy largely inherits from pre-trained unimodal backbones rather than cross-modal training

## Executive Summary
This study investigates data leakage in audio-text contrastive zero-shot learning, revealing that much of the measured accuracy stems from pre-trained supervised classifiers rather than cross-modal learning. Through experiments with the CLAP model on ESC50, the authors demonstrate that zero-shot performance correlates strongly with cluster separability in the audio embedding space, which is inherited from the pre-training phase. T-SNE visualizations and silhouette scores show that datasets containing training labels yield better-defined clusters and higher accuracy, while topological similarity analysis reveals that cross-modal training preserves rather than alters the original embedding structures.

## Method Summary
The study employs CLAP with CNN14 audio and BERT text backbones, training cross-modal projectors using CLIP loss on four datasets (Clotho, Audiocaps, FSD50K, MACS) in both "dirty" (containing ESC50 labels) and "clean" (filtered) versions. Models are trained through a two-phase process: initial 5 epochs with frozen backbones, followed by fine-tuning with unfrozen parameters. Zero-shot classification accuracy on ESC50 is evaluated alongside silhouette scores for cluster separability, T-SNE visualizations, and topological similarity measures between unimodal and cross-modal embeddings.

## Key Results
- Zero-shot classification accuracy correlates strongly with cluster separability in audio embeddings (ρ = 0.97, p < 10⁻²)
- Dirty datasets yield better-defined clusters and higher accuracy than clean datasets
- Topological similarity between cross-modal embeddings and their unimodal counterparts correlates with zero-shot accuracy (ρ = 0.85, p = 0.032)
- Cross-modal training preserves the topological structure of pre-trained embeddings rather than creating new common ground

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot classification accuracy is inherited from the unimodal backbone rather than learned in the cross-modal space.
- Mechanism: When a unimodal backbone is pre-trained with supervised labels that overlap with the test set, it already encodes class separability. The cross-modal training merely projects this existing structure without significantly altering it, so classification performance reflects pre-training, not cross-modal learning.
- Core assumption: The audio and text backbones retain separable clusters for classes seen during pre-training, and the cross-modal projector preserves this structure.
- Evidence anchors:
  - [abstract] "a significant part of the measured zero-shot learning accuracy is due to strengths inherited from the audio and text backbones, that is, they are not learned in the cross-modal domain and are not transferred from one modality to another."
  - [section 3.3] "the same clusters that appear unresolved in Figures 4a and 4b still show as unresolved in the embeddings Ea after training the cross-modal projections"
  - [corpus] Weak evidence: neighboring papers discuss cross-modal contrastive learning but do not directly address data leakage or inherited separability.

### Mechanism 2
- Claim: Better-defined clusters in the audio embedding space correlate with higher zero-shot classification accuracy.
- Mechanism: Silhouette scores and T-SNE visualizations show that dirty datasets yield more distinct clusters. These clusters serve as decision boundaries for classification, so clearer separation leads to better accuracy.
- Core assumption: Class labels in the training set align with the true underlying structure in the embedding space, and cluster separability directly translates to classification performance.
- Evidence anchors:
  - [section 3.1] "The silhouette scores present a high Pearson correlation to the accuracy results... ρ = 0.97, p < 10−2."
  - [section 3.1] "dirty datasets seem to have better-defined clusters; conversely, the absence of pre-training leads to the most disorganized spaces."
  - [corpus] Weak evidence: neighboring papers focus on contrastive learning performance but not specifically on cluster separability as a determinant of accuracy.

### Mechanism 3
- Claim: Cross-modal training does not significantly alter the topological structure of embeddings from their pre-trained states.
- Mechanism: Topological similarity analysis shows that the cross-modal embeddings Ea and Et maintain high similarity to their unimodal counterparts xa and xt, indicating that the cross-modal training preserves the original topology rather than creating a new common ground.
- Core assumption: The cross-modal projector is a relatively shallow transformation that cannot fundamentally reorganize the embedding space topology.
- Evidence anchors:
  - [section 3.2] "the topological similarity between Ea and Et correlates with the logarithm of the zero-shot accuracy... ρ = 0.85, p = 0.032."
  - [section 3.3] "It is possible to see that the same clusters that appear unresolved in Figures 4a and 4b still show as unresolved in the embeddings Ea after training the cross-modal projections"
  - [corpus] Weak evidence: neighboring papers discuss cross-modal learning but do not analyze topological preservation explicitly.

## Foundational Learning

- Concept: T-SNE and Silhouette Score
  - Why needed here: To visualize and quantify cluster separability in high-dimensional embedding spaces, which is central to understanding the source of zero-shot accuracy.
  - Quick check question: If a dataset has high silhouette score but low classification accuracy, what might that indicate about the relationship between cluster separability and accuracy?

- Concept: Topological Similarity Measures
  - Why needed here: To assess whether cross-modal training creates a common embedding space or simply preserves the original unimodal structures, informing whether cross-modal learning is effective.
  - Quick check question: If topological similarity between xa and Ea is high but between Ea and Et is low, what does that imply about the cross-modal space?

- Concept: Data Leakage in Machine Learning
  - Why needed here: To understand how unintentional overlap between training and test labels can inflate performance metrics and invalidate zero-shot learning claims.
  - Quick check question: If a model achieves high zero-shot accuracy but was trained on labels present in the test set, what is the most likely explanation for this performance?

## Architecture Onboarding

- Component map:
  - Unimodal backbones: CNN14 (audio) and BERT (text), pre-trained on labeled data
  - Projectors: MLP networks that map backbone embeddings to a shared cross-modal space
  - Contrastive loss: CLIP-style loss encouraging similarity between paired audio-text embeddings
  - Evaluation: Zero-shot classification using nearest-neighbor search in the cross-modal space

- Critical path:
  1. Load pre-trained backbones
  2. Freeze backbones and train projectors with contrastive loss
  3. Unfreeze backbones and jointly fine-tune
  4. Evaluate zero-shot accuracy on held-out classes

- Design tradeoffs:
  - Freezing backbones initially speeds convergence but may limit adaptation to cross-modal patterns
  - Using shallow projectors preserves unimodal structure but may not fully exploit cross-modal information
  - Data leakage avoidance requires careful curation of training and test label sets, which can reduce dataset size

- Failure signatures:
  - High zero-shot accuracy with dirty data but low accuracy with clean data suggests inherited separability rather than cross-modal learning
  - Similar T-SNE layouts before and after cross-modal training indicate topology preservation
  - Low silhouette scores in clean data indicate poor cluster separability and limited zero-shot capability

- First 3 experiments:
  1. Train with dirty data (both pre-training and training) and measure zero-shot accuracy and silhouette scores
  2. Train with clean data (both pre-training and training) and compare accuracy and silhouette scores to dirty case
  3. Train with dirty pre-training but clean training to isolate the effect of pre-training data leakage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different unsupervised or self-supervised pre-training methods for audio backbones compare to supervised pre-training in terms of zero-shot classification accuracy when data leakage is controlled?
- Basis in paper: [explicit] The paper discusses how supervised pre-training on labels present in zero-shot test sets leads to inflated accuracy, suggesting unsupervised alternatives might be more appropriate.
- Why unresolved: The paper only compares supervised pre-training (CNN14) with random initialization, not with other pre-training strategies like contrastive predictive coding or masked autoencoders.
- What evidence would resolve it: Experiments comparing zero-shot accuracy across multiple pre-training methods (supervised, unsupervised, self-supervised) while controlling for data leakage would clarify which approaches genuinely learn cross-modal representations versus inheriting classification capabilities.

### Open Question 2
- Question: To what extent do cross-modal embeddings preserve semantic relationships in text versus acoustic similarity in audio, and how does this affect zero-shot classification performance?
- Basis in paper: [explicit] The paper shows that audio embeddings cluster by acoustic similarity while text embeddings cluster by semantic similarity, with different topological structures between modalities.
- Why unresolved: While the paper identifies this structural difference, it doesn't quantify how these different clustering criteria impact the ability to generalize to unseen classes or determine optimal strategies for aligning these representations.
- What evidence would resolve it: Systematic analysis of classification performance when audio and text embeddings are aligned based on different criteria (acoustic vs semantic similarity) would reveal which alignment strategy better supports zero-shot learning.

### Open Question 3
- Question: What are the specific mechanisms by which data leakage through pre-training labels affects the structure of cross-modal embeddings, and can these effects be detected without access to training data?
- Basis in paper: [explicit] The paper demonstrates that data leakage leads to better-defined clusters in audio embeddings and discusses using T-SNE, silhouette scores, and topological similarity to detect leakage effects.
- Why unresolved: The paper shows correlation between leakage and embedding structure but doesn't fully explain the causal mechanisms or establish comprehensive detection methods that work without training data access.
- What evidence would resolve it: Detailed analysis of how specific leakage patterns manifest in embedding statistics, combined with validation of detection methods on multiple datasets with known leakage, would establish reliable leakage detection frameworks.

## Limitations
- The study focuses on a single dataset (ESC50) and model architecture (CLAP), limiting generalizability to other domains or model designs
- Evaluation relies on synthetic "dirty" and "clean" dataset versions rather than real-world data leakage scenarios
- Topological similarity analysis uses Pearson correlation without establishing causation between cluster structure and classification performance

## Confidence

- Mechanism 1 (Inherited separability): High - strong experimental evidence and clear correlation between pre-training overlap and accuracy
- Mechanism 2 (Cluster separability): Medium - correlation exists but directionality and causality require further validation
- Mechanism 3 (Topology preservation): Medium - similarity metrics show correlation but don't prove topological equivalence

## Next Checks

1. Test the inherited separability hypothesis on multiple datasets and model architectures to assess generalizability
2. Conduct ablation studies varying projector depth and complexity to determine if topology preservation is architecture-dependent
3. Implement and evaluate regularization techniques designed to reduce data leakage effects during cross-modal training