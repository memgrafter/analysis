---
ver: rpa2
title: 'Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An
  Empirical Analysis'
arxiv_id: '2407.11463'
source_url: https://arxiv.org/abs/2407.11463
tags:
- adversarial
- attacks
- attack
- data
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of evaluating the imperceptibility\
  \ of adversarial attacks on tabular data, which differs significantly from image\
  \ data due to its heterogeneity and complex feature interdependencies. The authors\
  \ propose seven key properties\u2014proximity, sparsity, deviation, sensitivity,\
  \ immutability, feasibility, and feature interdependency\u2014to comprehensively\
  \ characterize imperceptible adversarial attacks on tabular data, along with corresponding\
  \ metrics for each property."
---

# Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis

## Quick Facts
- arXiv ID: 2407.11463
- Source URL: https://arxiv.org/abs/2407.11463
- Reference count: 5
- This paper evaluates five adversarial attack algorithms on tabular data using seven imperceptibility properties and finds that unbounded attacks generally produce more imperceptible examples than bounded attacks, but all evaluated attacks often overlook key imperceptibility requirements.

## Executive Summary
This paper addresses the challenge of evaluating adversarial attack imperceptibility on tabular data, which differs significantly from image data due to heterogeneity and complex feature interdependencies. The authors propose seven key properties—proximity, sparsity, deviation, sensitivity, immutability, feasibility, and feature interdependency—to comprehensively characterize imperceptible adversarial attacks on tabular data. Through empirical evaluation of five attack algorithms (FGSM, PGD, DeepFool, LowProFool, and C&W Attack) across five tabular datasets, the study reveals a trade-off between attack effectiveness and imperceptibility, with unbounded attacks generally producing more imperceptible examples. However, the research identifies that current attack algorithms often overlook key imperceptibility properties in their design, particularly regarding feature sparsity, sensitivity, and feature interdependency.

## Method Summary
The paper evaluates five adversarial attack algorithms (FGSM, PGD, DeepFool, LowProFool, and C&W Attack) on five tabular datasets (Adult, German, Breast Cancer, Diabetes, COMPAS) using three predictive models (Logistic Regression, LinearSVC, MLP). The datasets are preprocessed with one-hot encoding for categorical features and split into 80% training and 20% testing sets. Attack effectiveness is measured by success rate, while imperceptibility is evaluated using seven properties: proximity (ℓp norm), sparsity (number of altered features), deviation (Mahalanobis distance), sensitivity (normalized ℓ1 distance by feature standard deviation), and qualitative assessments of immutability, feasibility, and feature interdependency. The ART-1.12.23 toolbox is used for attack implementation.

## Key Results
- Unbounded attacks (DeepFool and C&W) generally produce more imperceptible adversarial examples than bounded attacks (FGSM and PGD), with DeepFool achieving lower ℓ2 proximity values (0.0752) compared to FGSM (0.0909).
- All evaluated attacks often overlook key imperceptibility properties in their design, particularly regarding feature sparsity, sensitivity, and feature interdependency preservation.
- Categorical features remain largely unaltered across all attacks due to one-hot encoding preprocessing, though this is more a limitation of the encoding approach than the attacks themselves.
- There exists a clear trade-off between attack effectiveness and imperceptibility, with highly effective attacks often compromising on imperceptibility requirements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Immutability preservation requires categorical feature one-hot encoding to prevent targeted perturbation
- Mechanism: When categorical features are one-hot encoded, each category becomes a separate binary feature. Adversarial attack algorithms that operate by perturbing individual feature values cannot easily modify entire categorical categories because this would require coordinated changes across multiple binary features. This effectively protects immutable categorical features from being altered.
- Core assumption: One-hot encoding is used as the preprocessing method for categorical features in tabular data
- Evidence anchors:
  - [abstract] "For the same dataset, the number of adversarial examples remains consistent across different models and attacks. Given that the majority of distance-based adversarial attack algorithms employ the ℓp norm for perturbation size calculation, we utilise a one-hot encoding approach for categorical features to facilitate distance calculation."
  - [section] "However, this does not necessarily mean that the three attacks comply with the immutability requirement. Theoretically, the design of these attacks does not inherently consider these factors... A close examination of these attacks has revealed that they are incapable of changing immutable features—which are all categorical features in the examples used and are preprocessed with one-hot encoding in implementation."
- Break condition: Attack algorithms evolve to handle multi-feature coordination or use alternative encoding methods

### Mechanism 2
- Claim: Bounded attacks have inherent imperceptibility advantages due to attack budget constraints
- Mechanism: Bounded attacks like FGSM and PGD operate within a fixed attack budget (epsilon), which limits the maximum perturbation magnitude. This constraint naturally produces adversarial examples that are closer to the original input, resulting in better proximity and deviation metrics. The budget acts as an implicit imperceptibility guardrail.
- Core assumption: Attack budget is set appropriately for the dataset scale
- Evidence anchors:
  - [abstract] "The results reveal a trade-off between attack effectiveness and imperceptibility, with unbounded attacks (DeepFool and C&W) generally producing more imperceptible examples than bounded attacks (FGSM and PGD)."
  - [section] "Overall, the three unbounded attacks, C&W, DeepFool and LowProFool, generate more imperceptible adversarial examples compared to the two bounded attacks, FGSM and PGD."
- Break condition: Attack budget is set too high or low relative to feature scales

### Mechanism 3
- Claim: Unbounded attacks achieve better imperceptibility by minimizing perturbation distance
- Mechanism: Unbounded attacks like DeepFool and C&W optimize to find the minimal perturbation needed to cause misclassification. Without budget constraints, they can explore smaller perturbations that achieve the attack goal while staying closer to the original data distribution, resulting in better proximity and sensitivity metrics.
- Core assumption: The optimization process effectively finds minimal perturbations
- Evidence anchors:
  - [abstract] "unbounded attacks (DeepFool and C&W) generally producing more imperceptible examples than bounded attacks (FGSM and PGD)"
  - [section] "The evaluation of the four quantitative imperceptibility properties suggests that overall the three unbounded attacks, C&W, DeepFool and LowProFool, generate more imperceptible adversarial examples compared to the two bounded attacks, FGSM and PGD."
- Break condition: Optimization gets stuck in local minima or feature scaling issues

## Foundational Learning

- Concept: ℓp norm distance metrics for imperceptibility measurement
  - Why needed here: The paper uses ℓp norms (ℓ2, ℓ∞, ℓ1) to quantify how close adversarial examples are to original data points. Understanding these metrics is essential for interpreting the proximity and sparsity results.
  - Quick check question: Why might ℓ2 norm be preferred over ℓ∞ norm when measuring imperceptibility in tabular data?

- Concept: One-hot encoding vs ordinal encoding for categorical features
  - Why needed here: The paper explicitly uses one-hot encoding for categorical features to facilitate distance calculation. This choice affects how attacks interact with categorical features and impacts the immutability property evaluation.
  - Quick check question: How does one-hot encoding prevent adversarial attacks from easily modifying categorical features compared to ordinal encoding?

- Concept: Feature importance and its role in adversarial attacks
  - Why needed here: The paper discusses how attacks tend to target features with high importance to model predictions. Understanding feature importance helps explain why certain features are more likely to be perturbed.
  - Quick check question: Why do adversarial attacks preferentially target high-importance features rather than uniformly perturbing all features?

## Architecture Onboarding

- Component map: Data preprocessing → Model training → Adversarial attack generation → Imperceptibility evaluation → Qualitative analysis
- Critical path: Preprocess data (one-hot encoding) → Train models (LR, LinearSVC, MLP) → Generate adversarial examples → Compute quantitative metrics (sparsity, proximity, deviation, sensitivity) → Analyze qualitative properties (immutability, feasibility, feature interdependency)
- Design tradeoffs: One-hot encoding increases dimensionality but enables distance calculations; bounded attacks are more efficient but less imperceptible; unbounded attacks are more computationally expensive but find smaller perturbations
- Failure signatures: High deviation values indicate out-of-distribution examples; inability to modify categorical features suggests encoding issues; inconsistent feature interdependency indicates attack algorithm limitations
- First 3 experiments:
  1. Run all five attacks on a single dataset (e.g., Diabetes) and verify attack success rates exceed 30% threshold
  2. Compare sparsity values across attacks to confirm all numerical features are being perturbed
  3. Compute proximity ℓ2 values to identify which attack type produces closest adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial attack algorithms be systematically designed to maintain immutability of features while still achieving high attack success rates?
- Basis in paper: Explicit - The paper identifies that current attacks often perturb immutable features (like race and sex) and suggests this violates imperceptibility requirements.
- Why unresolved: The paper notes that while domain knowledge and feature masks could help, integrating these constraints directly into attack algorithms requires further research on algorithmic design.
- What evidence would resolve it: A comparative study showing attack success rates and imperceptibility metrics (especially for immutability) between attacks with integrated immutability constraints versus those without.

### Open Question 2
- Question: What is the optimal attack budget for bounded adversarial attacks on tabular data that balances effectiveness and imperceptibility?
- Basis in paper: Explicit - The paper states that determining an appropriate attack budget for bounded attacks presents a significant challenge and that an exhaustive search across datasets is impractical.
- Why unresolved: The paper identifies the need for finding the optimal balance but notes that resource limitations prevent systematic exploration of different budget values across diverse datasets.
- What evidence would resolve it: A systematic study showing the relationship between attack budget values, attack success rates, and imperceptibility metrics across multiple tabular datasets.

### Open Question 3
- Question: How can feature interdependency be effectively incorporated into adversarial attack algorithms to preserve data integrity?
- Basis in paper: Explicit - The paper identifies that current attacks like DeepFool do not account for feature interdependencies and can break relationships between correlated features.
- Why unresolved: While the paper suggests that methods like feature selection or constraints could help, it notes that these require careful consideration of data structure and domain knowledge that isn't always available.
- What evidence would resolve it: An evaluation showing attack performance and imperceptibility metrics when algorithms incorporate feature interdependency preservation mechanisms versus those that don't.

## Limitations

- The qualitative properties (immutability, feasibility, feature interdependency) rely on manual analysis that may not scale to larger datasets or more complex feature relationships.
- The one-hot encoding approach, while facilitating distance calculations, may oversimplify categorical feature interactions and potentially mask attack vulnerabilities specific to certain encoding schemes.
- The generalizability of findings to regression tasks or datasets with significantly different feature distributions remains uncertain.

## Confidence

- **High Confidence**: The quantitative metrics (proximity, sparsity, deviation, sensitivity) are well-defined and computationally reproducible. The observed trade-off between bounded and unbounded attacks is consistently demonstrated across datasets.
- **Medium Confidence**: The qualitative analysis of imperceptibility properties provides valuable insights but depends on subjective interpretation of feature relationships and domain knowledge.
- **Low Confidence**: The generalizability of findings to regression tasks or datasets with significantly different feature distributions remains uncertain.

## Next Checks

1. **Reproduce Quantitative Results**: Replicate the ℓ2 proximity and sparsity measurements for FGSM and DeepFool attacks on the Diabetes dataset to verify the reported trade-off between bounded and unbounded approaches.

2. **Alternative Encoding Validation**: Test the same attack algorithms using ordinal encoding instead of one-hot encoding to assess how encoding choices affect categorical feature immutability and overall imperceptibility metrics.

3. **Feature Interdependency Stress Test**: Create synthetic tabular datasets with known feature dependencies (e.g., one feature is always the sum of two others) to systematically evaluate whether attacks preserve these relationships as claimed.