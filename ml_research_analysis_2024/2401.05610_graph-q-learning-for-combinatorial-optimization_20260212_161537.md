---
ver: rpa2
title: Graph Q-Learning for Combinatorial Optimization
arxiv_id: '2401.05610'
source_url: https://arxiv.org/abs/2401.05610
tags:
- graph
- learning
- problems
- problem
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that Graph Neural Networks (GNNs) can be
  applied to solve Combinatorial Optimization (CO) problems. The authors formulate
  the optimization process as a sequential decision making problem, where the return
  is related to how close the candidate solution is to optimality.
---

# Graph Q-Learning for Combinatorial Optimization

## Quick Facts
- arXiv ID: 2401.05610
- Source URL: https://arxiv.org/abs/2401.05610
- Reference count: 27
- Key outcome: GNN trained via Q-learning solves FJSP with <6% relative error using 960 parameters vs 6022 for similar deep RL approach

## Executive Summary
This paper introduces Graph Q-Learning, a method that applies Graph Neural Networks to solve combinatorial optimization problems by framing them as sequential decision-making processes. The approach uses a heterogeneous GNN to learn Q-values for operation-machine assignments in the Flexible Job Shop Scheduling Problem (FJSP). Through Q-learning, the agent iteratively builds better solutions, achieving performance approaching state-of-the-art heuristics while using significantly fewer parameters and training time.

## Method Summary
The method reformulates FJSP as a Markov Decision Process where states represent partial schedules encoded as disjunctive graphs with operations and machines as nodes. A heterogeneous GNN processes three edge types (job sequence, queue, assignment) through separate message-passing layers to estimate Q-values for operation-machine assignments. The agent trains via Q-learning with epsilon-greedy exploration, using 128 trajectories per epoch, 64 training iterations, and a batch size of 32. The approach generalizes across problem sizes, enabling meta-learning capabilities.

## Key Results
- Solves FJSP with relative error <6% for problem sizes up to 100x20
- Uses only 960 independent weights compared to 6022 for similar deep RL approach
- Scales better in runtime than a more accurate meta-heuristic for large FJSP instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method frames combinatorial optimization as a Markov Decision Process (MDP), enabling reinforcement learning to iteratively improve candidate solutions.
- Mechanism: Each FJSP state represents a partial schedule; actions assign operations to machines; rewards encourage shorter makespan and penalize idle time. The agent learns a Q-function mapping state-action pairs to expected returns.
- Core assumption: The FJSP can be decomposed into sequential decisions with Markovian state transitions.
- Evidence anchors:
  - [abstract] "we formulate the optimization process as a sequential decision making problem"
  - [section] "We frame combinatorial optimization problems as sequential decision making processes"
- Break condition: If state transitions are not Markovian (e.g., future decisions depend on history beyond current state), the MDP assumption fails and Q-learning degrades.

### Mechanism 2
- Claim: Heterogeneous graph neural networks (HGNNs) aggregate information from different edge types (sequence constraints, machine compatibility, queue order) to estimate Q-values.
- Mechanism: HGNN uses separate message-passing layers for each edge type (Ej, Eq, C), then sums intermediate embeddings to form node representations. Dot-product readout predicts operation-machine assignment scores.
- Core assumption: Edge-type-specific message passing captures relevant relational structure for scheduling decisions.
- Evidence anchors:
  - [abstract] "We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions"
  - [section] "We employ a disjunctive graph...composed of graph convolution layers"
- Break condition: If the problem's relational structure is not well-represented by the chosen edge types, the HGNN cannot propagate relevant information.

### Mechanism 3
- Claim: Meta-learning capability emerges because the GNN is trained on random FJSP instances and generalizes to unseen sizes.
- Mechanism: Training on varying (m, n, no) problem sizes forces the network to learn size-agnostic representations; evaluation on larger instances shows preserved performance with minimal fine-tuning.
- Core assumption: The graphical representation is invariant to problem size; learned embeddings encode general scheduling principles.
- Evidence anchors:
  - [abstract] "This formulation generalizes to a form of meta-learning"
  - [section] "After the Q-learner has been trained, it can be used to solve problems of any size"
- Break condition: If the solution space grows too fast or structural patterns change significantly with size, generalization breaks.

## Foundational Learning

- Concept: Markov Decision Processes
  - Why needed here: The CO problem is reformulated as an MDP so RL algorithms can be applied.
  - Quick check question: What are the components of an MDP and how do they map to the FJSP setup?

- Concept: Graph Neural Networks
  - Why needed here: GNNs propagate information across graph structures representing FJSP states, enabling relational reasoning over operations and machines.
  - Quick check question: How does a GNN layer update node embeddings using messages from neighbors?

- Concept: Reinforcement Learning (Q-learning)
  - Why needed here: Q-learning learns an action-value function that estimates expected return for scheduling actions, guiding policy improvement.
  - Quick check question: What is the Bellman equation and how does it drive Q-learning updates?

## Architecture Onboarding

- Component map: State encoding -> HGNN backbone -> Q-values -> Action selection -> Environment update -> Replay buffer -> Q-learning update

- Critical path: Sample trajectory → encode state as graph → HGNN → Q-values → select action → execute → store transition → update network

- Design tradeoffs:
  - Heterogeneous layers vs. homogeneous: Separate layers per edge type improve expressiveness but increase parameters; here 960 vs. 6022 in baseline
  - Fixed vs. variable k: k=2 balances depth and oversmoothing; larger k could oversmooth scheduling signals
  - Reward shaping: Unit rewards for completions + small negative per timestep encourages speed without destabilizing training

- Failure signatures:
  - Training loss plateaus early → poor exploration or insufficient model capacity
  - High relative error but feasible solutions → HGNN fails to capture long-term scheduling effects
  - Gridlock → reward shaping or action selection needs adjustment

- First 3 experiments:
  1. Verify MDP reformulation: Run random action rollout and check state transitions, rewards, and terminal conditions.
  2. Test HGNN forward pass: Feed a small FJSP graph through the network and confirm Q-values shape and range.
  3. Validate RL loop: Train on tiny problems (e.g., 5×5) and confirm learning curves show increasing success rate and decreasing relative error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the GNN-based approach scale when applied to other combinatorial optimization problems beyond FJSP, such as TSP or k-SAT?
- Basis in paper: [inferred] The paper focuses on FJSP but mentions that the method could potentially generalize to other CO problems. It would be valuable to test this hypothesis explicitly.
- Why unresolved: The paper only presents results for FJSP. Testing on other CO problems would require significant additional experimentation and may reveal limitations or areas for improvement in the approach.
- What evidence would resolve it: Implementing the GNN-based approach for other CO problems like TSP or k-SAT and comparing its performance to state-of-the-art methods for those problems. Analyzing the scalability and limitations of the approach across different problem domains.

### Open Question 2
- Question: What is the impact of different graph neural network architectures and hyperparameters on the performance of the approach for solving FJSP?
- Basis in paper: [explicit] The paper mentions using a specific GNN architecture and hyperparameters. It would be insightful to explore the sensitivity of the approach to these choices.
- Why unresolved: The paper only reports results using a fixed GNN architecture and hyperparameters. Exploring the impact of different choices could lead to improved performance or reveal important design principles.
- What evidence would resolve it: Conducting experiments with different GNN architectures (e.g., varying the number of layers, attention mechanisms) and hyperparameters (e.g., learning rate, batch size) to assess their impact on the performance of the approach for FJSP.

### Open Question 3
- Question: How does the runtime performance of the GNN-based approach compare to other heuristic-based solvers for large-scale FJSP instances?
- Basis in paper: [explicit] The paper mentions that the approach scales better in runtime than a more accurate meta-heuristic for large FJSP instances. However, a more comprehensive comparison with other solvers is needed.
- Why unresolved: The paper only provides runtime comparisons for a limited set of problem sizes and solvers. A more thorough analysis is required to fully understand the scalability and efficiency of the approach.
- What evidence would resolve it: Benchmarking the runtime performance of the GNN-based approach against other state-of-the-art heuristic-based solvers for FJSP across a wide range of problem sizes, including very large instances. Analyzing the scaling behavior and identifying potential bottlenecks.

## Limitations

- Core claims rely on Markovian assumption for FJSP state transitions which may not hold for complex scheduling dependencies
- Heterogeneous GNN design choices lack ablation studies to verify necessity
- Size generalization claim demonstrated but not rigorously analyzed - unclear if network learns true size-agnostic principles

## Confidence

- High confidence: The basic MDP formulation and GNN architecture implementation
- Medium confidence: The size generalization capability
- Low confidence: The claim of approaching state-of-the-art performance with minimal parameters

## Next Checks

1. **Ablation study**: Test homogeneous vs. heterogeneous GNN layers and vary k to determine if the current design is optimal or over-parameterized.

2. **Generalization analysis**: Systematically evaluate performance degradation across problem sizes and analyze learned embeddings for size-dependent vs. size-invariant features.

3. **Benchmark comparison**: Compare against a broader set of heuristic and RL baselines on standardized FJSP instances to validate the "state-of-the-art" claim.