---
ver: rpa2
title: 'AgreeMate: Teaching LLMs to Haggle'
arxiv_id: '2412.18690'
source_url: https://arxiv.org/abs/2412.18690
tags:
- buyer
- seller
- negotiation
- price
- aggressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgreeMate explores how LLMs perform in strategic price negotiations
  by combining role-specialized fine-tuning with systematic model comparisons. The
  framework eliminates complex modular architectures, relying instead on LLMs to handle
  both strategy and dialogue generation.
---

# AgreeMate: Teaching LLMs to Haggle

## Quick Facts
- arXiv ID: 2412.18690
- Source URL: https://arxiv.org/abs/2412.18690
- Authors: Ainesh Chatterjee; Samuel Miller; Nithin Parepally
- Reference count: 1
- One-line primary result: Role-specialized fine-tuned LLMs achieve higher agreement rates and fairer outcomes in price negotiations, with larger models (70B) outperforming smaller ones (3B)

## Executive Summary
AgreeMate explores how LLMs perform in strategic price negotiations by combining role-specialized fine-tuning with systematic model comparisons. The framework eliminates complex modular architectures, relying instead on LLMs to handle both strategy and dialogue generation. Experiments across 21 model configurations reveal that larger models (70B) achieve higher agreement rates, fairer outcomes, and shorter dialogues, while smaller models (3B) often exhibit bias toward buyers and engage in repetitive, unrealistic price proposals. Chain-of-thought prompting improves exploratory behavior and relative efficiency but can worsen fairness and bias in smaller models. Personality alignment—aggressive, fair, or passive—significantly influences negotiation dynamics, with aggressive strategies dominating and passive ones promoting smoother dialogue.

## Method Summary
AgreeMate fine-tunes LLaMA-3.2 models (3B, 8B, 70B) on role-specific negotiation data using LoRA with quantization, creating buyer, seller, and generalist agents. The framework uses two datasets: Craigslist Negotiation Dataset (6,000+ human-human dialogues) and Deal or No Deal dataset (12,000+ multi-issue bargaining dialogues). Agents generate action-utterance pairs within a negotiation runner that orchestrates interactions across 30 test scenarios. Evaluation metrics include agreement rate, fairness, bias, aggressiveness, dialogue length, concession rate, relative efficiency, probing ratio, and attention probing analysis. Chain-of-thought prompting and personality trait alignment (aggressive, fair, passive) are systematically tested across model scales.

## Key Results
- Larger models (70B) achieve higher agreement rates, fairer outcomes, and shorter dialogues than smaller models (3B)
- Smaller models exhibit buyer bias and engage in repetitive, unrealistic price proposals
- Chain-of-thought prompting improves exploratory behavior but worsens fairness and bias in smaller models
- Personality alignment significantly influences negotiation dynamics, with aggressive strategies dominating

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-specialized fine-tuning improves negotiation alignment by tailoring the model's strategy to either buyer or seller goals.
- Mechanism: The model is trained on role-specific data (buyer or seller), learning distinct strategies (e.g., lowballing for buyers, holding price for sellers) while retaining general language generation capabilities.
- Core assumption: Fine-tuning on role-specific dialogues is more effective than generalist training for achieving role-specific negotiation outcomes.
- Evidence anchors:
  - [abstract] "We develop and analyze role-specialized negotiation agents (buyer, seller, generalist) through targeted fine-tuning, demonstrating the effectiveness of role-specific optimization."
  - [section 5.1] "We used the Deal or No Deal dataset for fine-tuning... Each training instance consists of: Context: Scenario description, listing price, and target price information."
  - [corpus] "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues" explores role-specific fine-tuning, supporting this mechanism.
- Break condition: If the fine-tuning dataset does not sufficiently represent the role's negotiation strategies, the model may fail to specialize effectively.

### Mechanism 2
- Claim: Chain-of-thought (CoT) prompting enhances exploratory behavior and relative efficiency in negotiation but may reduce fairness and increase bias in smaller models.
- Mechanism: CoT provides step-by-step reasoning, allowing the model to explore price options and justify proposals, leading to more efficient negotiations. However, smaller models may over-rely on initial biases or incomplete reasoning chains.
- Core assumption: CoT improves reasoning in negotiation by structuring the decision-making process, but its effectiveness depends on model scale.
- Evidence anchors:
  - [abstract] "Chain-of-thought prompting improves exploratory behavior and relative efficiency but can worsen fairness and bias in smaller models."
  - [section 6.4.6] "CoT influences various negotiation characteristics... CoT appears to encourage exploratory behavior... However, small and medium sized models experience significant trade-offs with this technique when it comes to bias and fairness."
  - [corpus] "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in Multi-Turn Price Negotiation" explores CoT in negotiation, supporting its impact on strategy.
- Break condition: If the CoT prompt is too rigid or the model lacks sufficient reasoning capacity, it may lead to repetitive or unrealistic proposals.

### Mechanism 3
- Claim: Larger models (70B) achieve higher agreement rates, fairer outcomes, and shorter dialogues due to their ability to learn nuanced representations of negotiation states.
- Mechanism: Increased model parameters allow for more complex reasoning and better adaptation to counterpart strategies, leading to more effective negotiations.
- Core assumption: Model scale directly correlates with negotiation performance, with larger models capable of more sophisticated strategic reasoning.
- Evidence anchors:
  - [abstract] "Experiments across 21 model configurations reveal that larger models (70B) achieve higher agreement rates, fairer outcomes, and shorter dialogues, while smaller models (3B) often exhibit bias toward buyers and engage in repetitive, unrealistic price proposals."
  - [section 6.4.1] "We observe that larger models consistently show higher agreement rates... Since larger models have more parameters, they are able to learn more nuanced representations of the negotiation state..."
  - [corpus] "Advancing AI Negotiations: A Large-Scale Autonomous Negotiation Competition" supports the link between model scale and negotiation success.
- Break condition: If the larger model is not properly fine-tuned or the negotiation scenarios are too simple, the performance advantage may diminish.

## Foundational Learning

- Concept: Role-specialized fine-tuning
  - Why needed here: To enable the model to adopt distinct strategies for buyer and seller roles, improving negotiation alignment and outcomes.
  - Quick check question: How does role-specific training differ from generalist training in terms of negotiation strategy?

- Concept: Chain-of-thought prompting
  - Why needed here: To structure the model's reasoning process, enabling exploratory behavior and more efficient negotiations.
  - Quick check question: What are the potential trade-offs of using CoT in negotiation tasks, especially for smaller models?

- Concept: Attention probing
  - Why needed here: To understand how the model attends to negotiation-related semantics, providing insights into its internal reasoning mechanisms.
  - Quick check question: How does attention head analysis reveal the model's understanding of negotiation dynamics?

## Architecture Onboarding

- Component map: Buyer/Seller/Generalist agents (fine-tuned LLMs) -> Knowledge bases (scenario, listing price, target price) -> Negotiation runner (orchestrates agent interactions) -> Tester (runs negotiations across scenarios) -> Metrics calculator (agreement rate, fairness, bias, etc.)

- Critical path:
  1. Initialize scenario and knowledge bases
  2. Buyer and seller agents generate action-utterance pairs
  3. Negotiation runner updates conversation history
  4. Repeat until agreement or turn limit
  5. Calculate metrics and store results

- Design tradeoffs:
  - Model scale vs. resource constraints: Larger models perform better but require more computational resources.
  - Fine-tuning vs. prompt engineering: Fine-tuning provides role specialization but requires more data and training time.
  - CoT vs. direct generation: CoT improves reasoning but may introduce bias in smaller models.

- Failure signatures:
  - Repetitive dialogue: Smaller models may struggle with exploratory behavior.
  - Unrealistic price proposals: Models may not adapt to realistic negotiation ranges.
  - Biased outcomes: Smaller models with CoT may favor one party over the other.

- First 3 experiments:
  1. Compare agreement rates of 3B vs. 70B models in buyer-seller negotiations.
  2. Test the impact of CoT on negotiation efficiency and fairness for 3B and 70B models.
  3. Analyze the effect of personality traits (aggressive, fair, passive) on negotiation outcomes and dialogue dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attention heads contribute to the negotiation behavior of LLMs, and can we identify specific heads that are crucial for strategic reasoning versus language generation?
- Basis in paper: [explicit] The paper mentions attention probing to analyze how models attend to negotiation-related semantics, particularly noting that certain heads attend to negotiation verbs and long-distance relationships.
- Why unresolved: While the paper identifies some attention heads that attend to negotiation-related tokens, it does not provide a comprehensive analysis of how different heads contribute to specific negotiation behaviors or whether certain heads are more critical for strategy versus language generation.
- What evidence would resolve it: Detailed ablation studies removing or modifying specific attention heads to observe changes in negotiation performance, along with correlation analysis between attention patterns and strategic decision-making.

### Open Question 2
- Question: What is the optimal balance between model size and computational efficiency for practical deployment of LLM negotiation agents in resource-constrained settings?
- Basis in paper: [explicit] The paper compares models across different scales (3B to 70B parameters) and notes trade-offs between performance and resource requirements, mentioning the use of LoRA and quantization techniques.
- Why unresolved: While the paper provides insights into how different model sizes perform, it does not establish a clear framework for determining the optimal trade-off between negotiation performance and computational efficiency for real-world applications.
- What evidence would resolve it: Systematic evaluation of negotiation performance versus computational cost (including inference time, memory usage, and energy consumption) across different model sizes and optimization techniques, with specific recommendations for different deployment scenarios.

### Open Question 3
- Question: How can Chain-of-Thought prompting be optimized to improve fairness and reduce bias in smaller LLM models without sacrificing their exploratory behavior?
- Basis in paper: [explicit] The paper finds that CoT improves agreement rates and efficiency but introduces significant bias and fairness issues in smaller models, suggesting a need for optimization.
- Why unresolved: The paper identifies the problem but does not propose solutions for optimizing CoT to balance its benefits with its negative impacts on fairness and bias in smaller models.
- What evidence would resolve it: Experiments testing different CoT formulations, training approaches, or hybrid methods that could maintain exploratory behavior while improving fairness and reducing bias, along with metrics to quantify these improvements.

## Limitations

- Dataset generalization: Findings may not extend beyond product-focused negotiation scenarios to domains like real estate, salary negotiations, or business partnerships
- Model architecture constraints: Results are specific to LLaMA-3.2 models with LoRA + quantization, potentially limiting applicability to other architectures
- Evaluation metrics limitations: Automated measurements may miss qualitative aspects like relationship building, trust establishment, or long-term negotiation strategy effectiveness

## Confidence

- High Confidence: Claims about larger models (70B) consistently outperforming smaller models (3B) in agreement rates, fairness, and dialogue efficiency
- Medium Confidence: Findings about Chain-of-Thought prompting effects showing model-dependent variability
- Low Confidence: Personality alignment effects showing clear directional influences but lacking detailed mechanism explanations

## Next Checks

1. Cross-Domain Validation: Test AgreeMate agents across diverse negotiation domains (real estate, salary negotiations, business partnerships) to assess generalizability beyond the current product-focused scenarios.

2. Human Evaluation Study: Conduct blinded human evaluations comparing LLM-generated negotiations against human-human baselines, measuring perceived realism, strategic sophistication, and satisfaction with outcomes.

3. Adversarial Testing: Design scenarios specifically crafted to expose model weaknesses, such as complex multi-issue negotiations, time pressure situations, or negotiations involving deception and strategic misinformation.