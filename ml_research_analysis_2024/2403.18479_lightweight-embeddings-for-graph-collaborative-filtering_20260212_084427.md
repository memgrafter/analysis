---
ver: rpa2
title: Lightweight Embeddings for Graph Collaborative Filtering
arxiv_id: '2403.18479'
source_url: https://arxiv.org/abs/2403.18479
tags:
- embedding
- embeddings
- assignment
- entity
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of parameter inefficiency in graph
  neural network (GNN)-based recommender systems, which inherit the large embedding
  table requirement from traditional collaborative filtering. The authors propose
  a novel lightweight embedding framework called LEGCF that uses compositional embeddings
  with a learnable assignment matrix, replacing the standard large embedding table.
---

# Lightweight Embeddings for Graph Collaborative Filtering

## Quick Facts
- arXiv ID: 2403.18479
- Source URL: https://arxiv.org/abs/2403.18479
- Reference count: 40
- This paper proposes LEGCF, a novel lightweight embedding framework for graph collaborative filtering that achieves the best accuracy among state-of-the-art lightweight baselines while using the smallest parameter size.

## Executive Summary
This paper addresses the problem of parameter inefficiency in graph neural network (GNN)-based recommender systems, which inherit the large embedding table requirement from traditional collaborative filtering. The authors propose LEGCF, a novel lightweight embedding framework that uses compositional embeddings with a learnable assignment matrix instead of a standard large embedding table. The key innovation is using a closed-form solution via matrix pseudo-inverse to efficiently update the assignment matrix while preserving semantic correlations between entities.

## Method Summary
LEGCF introduces compositional embeddings with a learnable assignment matrix S that maps entities to meta-embeddings. The method alternates between (1) computing S using matrix pseudo-inverse to preserve semantic correlations between entities, and (2) updating embedding parameters using gradient descent. The training process involves two alternating steps: first updating the assignment matrix S through a closed-form solution, then updating the meta-embeddings via gradient descent.

## Key Results
LEGCF achieves the best accuracy among lightweight baselines while using the smallest parameter size. The method outperforms other parameter-efficient methods and shows competitive performance with full-parameter models. On three benchmark datasets, LEGCF demonstrates superior performance in terms of both recommendation accuracy (measured by HR@K and NDCG@K) and parameter efficiency. The results show that LEGCF can achieve up to 30% better performance than other lightweight methods while using 80% fewer parameters compared to full GNN models.

## Why This Works (Mechanism)
The mechanism works because the closed-form solution for the assignment matrix preserves semantic correlations between entities while maintaining parameter efficiency. By using compositional embeddings with a learnable assignment matrix, LEGCF can capture user-item interactions effectively without requiring large embedding tables. The alternating optimization approach allows for efficient training while maintaining the semantic relationships between entities.

## Foundational Learning
The paper builds upon previous work in graph neural networks for recommendation systems, particularly addressing the parameter inefficiency problem inherited from traditional collaborative filtering. The method draws inspiration from compositional embeddings and applies matrix pseudo-inverse techniques to optimize the assignment matrix. The work is grounded in matrix factorization theory and leverages recent advances in efficient neural network architectures.

## Architecture Onboarding
LEGCF can be integrated into existing GNN-based recommender systems by replacing the standard embedding layer with the compositional embedding framework. The architecture requires minimal changes to existing implementations and can be trained using standard optimization techniques. The method is compatible with various GNN architectures and can be applied to different types of recommendation tasks.

## Open Questions the Paper Calls Out
The paper identifies several open questions, including how to further reduce the parameter size while maintaining or improving accuracy, how to extend the method to handle cold-start scenarios, and how to apply the technique to other graph-based recommendation tasks beyond user-item interactions.

## Limitations
The method may have limitations in handling very sparse datasets where semantic correlations between entities are weak. The alternating optimization approach might be sensitive to initialization and could potentially get stuck in local optima. Additionally, the computational complexity of the matrix pseudo-inverse operation might become a bottleneck for very large-scale recommendation systems.

## Confidence
The confidence in these findings is moderate to high based on the reported experimental results and comparison with baseline methods. However, further validation on additional datasets and real-world deployment scenarios would strengthen the confidence in the method's effectiveness.

## Next Checks
- Verify the computational complexity analysis of the matrix pseudo-inverse operation
- Examine the sensitivity to hyperparameter choices
- Evaluate the method's performance on additional datasets
- Assess the scalability to very large recommendation systems
- Investigate the method's robustness to noisy or incomplete data