---
ver: rpa2
title: Large Multimodal Model based Standardisation of Pathology Reports with Confidence
  and their Prognostic Significance
arxiv_id: '2405.02040'
source_url: https://arxiv.org/abs/2405.02040
tags:
- reports
- confidence
- field
- information
- pathology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents LABIEB, a two-stage framework for automatic
  extraction of information from pathology reports using Large Multimodal Models (LMMs)
  with confidence estimation. The framework overcomes limitations of existing methods
  by assigning confidence scores to extracted fields and following established reporting
  standards.
---

# Large Multimodal Model based Standardisation of Pathology Reports with Confidence and their Prognostic Significance

## Quick Facts
- **arXiv ID**: 2405.02040
- **Source URL**: https://arxiv.org/abs/2405.02040
- **Reference count**: 0
- **Primary result**: Framework using GPT-4 achieves AUROC 0.66-0.93 for confidence estimation and c-index up to 0.74±0.04 for survival prediction on TCGA colorectal adenocarcinoma reports

## Executive Summary
This work presents LABIEB, a two-stage framework for automatic extraction of structured information from pathology reports using Large Multimodal Models with confidence estimation. The framework overcomes limitations of existing methods by assigning confidence scores to extracted fields and following established reporting standards. Using GPT-4 for information extraction and validation, the approach produces standardized reports with confidence values. Tested on TCGA colorectal adenocarcinoma reports, the method achieves high accuracy with area under ROC curves ranging from 0.66-0.93 for different fields. Confidence scores effectively indicate extraction accuracy, with performance improving as low-confidence extractions are rejected. The standardized reports show strong prognostic value, achieving c-index up to 0.74±0.04 for patient stratification.

## Method Summary
The proposed framework uses a two-stage prompting approach with GPT-4: an Extractor agent that performs initial field extraction from pathology reports, and a Validator agent that independently checks and refines those extractions. The final confidence is computed from the agreement between the two agents and their individual confidence scores. The system handles both structured (tabular) and unstructured (text) fields, with 25 specific queries for colorectal adenocarcinoma reports. Confidence estimation uses response frequency aggregation across multiple LLM outputs, calibrated using Platt's scaling. The framework also performs survival analysis to demonstrate prognostic significance of the standardized reports, achieving strong stratification performance with c-index up to 0.74±0.04.

## Key Results
- AUROC scores for confidence estimation range from 0.66-0.93 across different fields
- Accuracy improves as low-confidence extractions are rejected using confidence thresholds
- C-index up to 0.74±0.04 achieved for survival prediction using standardized report fields
- Confidence scores effectively indicate extraction accuracy and can be used for selective rejection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage prompting with an Extractor and Validator agent improves confidence estimation accuracy compared to single-pass extraction.
- Mechanism: The Extractor performs initial field extraction from the report, and the Validator independently checks and refines those extractions. The final confidence is computed from the agreement between the two agents and their individual confidence scores, capturing uncertainty more reliably than a single model output.
- Core assumption: Confidence from multiple independently-prompted models is more representative of true accuracy than confidence from a single model.
- Evidence anchors:
  - [abstract] "The proposed framework uses two stages of prompting a Large Multimodal Model (LMM) for information extraction and validation."
  - [section] "The main goal of this work is not to make a model that is optimal and makes no mistakes, but to make a model that is able to reflect its reliability as a measurement of confidence."
- Break condition: If the Validator's prompts fail to capture the necessary reasoning context, the confidence estimates may become biased or misleading.

### Mechanism 2
- Claim: Confidence scores effectively indicate extraction accuracy, enabling selective rejection of low-confidence fields.
- Mechanism: The system estimates confidence for each extracted field using agreement statistics and response frequencies across multiple LLM outputs. This confidence can then be thresholded to filter out potentially incorrect extractions, improving overall accuracy.
- Core assumption: Higher confidence scores correlate with higher extraction accuracy and can be calibrated to reflect true performance.
- Evidence anchors:
  - [abstract] "We show that the estimated confidence is an effective indicator of the accuracy of the extracted information that can be used to select only accurately extracted fields."
  - [section] "We have evaluated the model's effectiveness by examining how accurately its confidence estimates reflect its correctness."
- Break condition: If the model is systematically overconfident or underconfident, confidence thresholds may not effectively filter errors.

### Mechanism 3
- Claim: Structured pathology reports following RCPath standards retain strong prognostic value for survival analysis.
- Mechanism: Extracted fields are mapped to standardized categories (e.g., TNM staging components), and these structured features are used in survival models to stratify patients into risk groups with significant survival differences.
- Core assumption: The content of pathology reports, when properly structured, contains clinically relevant prognostic signals.
- Evidence anchors:
  - [abstract] "We also show the prognostic significance of structured and unstructured data from pathology reports and show that the automatically extracted field values significant prognostic value for patient stratification."
  - [section] "We have performed survival analysis to confirm the prognostic value of the standardised reports."
- Break condition: If the standardization process loses critical context or nuance present in the original reports, prognostic accuracy may degrade.

## Foundational Learning

- Concept: Natural Language Processing (NLP) for information extraction
  - Why needed here: Pathology reports are unstructured text, so NLP techniques are required to identify and extract specific fields like tumor grade or lymph node status.
  - Quick check question: What are the challenges of extracting structured data from unstructured pathology reports?

- Concept: Large Language Models (LLMs) and zero-shot learning
  - Why needed here: GPT-4 is used to extract information without requiring task-specific fine-tuning, leveraging its ability to understand prompts and generate structured responses.
  - Quick check question: How does zero-shot prompting differ from fine-tuning in terms of flexibility and performance?

- Concept: Confidence calibration and abstention strategies
  - Why needed here: Confidence scores must be calibrated to reflect true accuracy, and low-confidence extractions should be rejected to improve overall system reliability.
  - Quick check question: Why is it important to express uncertainty in automated information extraction systems?

## Architecture Onboarding

- Component map: PDF/text pathology report → GPT-4V OCR → Extractor Agent → Validator Agent → Confidence estimation → Standardized JSON/PDF output
- Critical path: OCR → Extractor → Validator → Confidence estimation → Output
- Design tradeoffs: Using GPT-4 for both OCR and extraction trades cost/compute for flexibility and accuracy; multi-prompting increases token usage but improves confidence estimation.
- Failure signatures: High OCR errors (illegible handwriting), inconsistent Validator responses, poor confidence calibration leading to over/under-rejection.
- First 3 experiments:
  1. Run a single report through the full pipeline and manually compare extracted fields to ground truth.
  2. Vary confidence thresholds and measure accuracy/rejection trade-offs on the validation set.
  3. Test the system on a small set of reports from a different medical center to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LABIEB framework perform on pathology reports from cancer types other than colorectal adenocarcinoma (COAD)?
- Basis in paper: [explicit] The authors state "This experiment is tailored exclusively for COAD cases and might not directly apply to other cancer types. However, the underlying framework is versatile and can be adapted for any information extraction task."
- Why unresolved: The paper only evaluates the framework on TCGA colorectal adenocarcinoma reports, leaving its generalizability to other cancer types unexplored.
- What evidence would resolve it: Testing LABIEB on pathology reports from other cancer types (e.g., breast, lung, prostate) and comparing performance metrics to those achieved on COAD reports.

### Open Question 2
- Question: How do open-source large language models (LLMs) compare to commercial models like GPT-4 in terms of performance and privacy for pathology report standardization?
- Basis in paper: [explicit] The authors mention "The handling and storage of prompts sent by users, coupled with the model's ability to remember and potentially leak this information in future responses, pose a privacy risk. Therefore, we recommend anonymizing any patient-specific information before utilizing any commercial LMM, including the framework integrated into our website. This issue rises the necessity for open-source LMMs fine-tuned on pathology data that can operate locally without compromising privacy. While open-source LLMs do exist (such as Llama-2 and Falcon) they require further development to reach the sophistication of their commercial counterparts and are limited to text input only."
- Why unresolved: The paper does not provide a direct comparison between open-source and commercial LLMs for the task of pathology report standardization.
- What evidence would resolve it: Conducting experiments using open-source LLMs (e.g., Llama-2, Falcon) for pathology report standardization and comparing their performance and privacy features to those of commercial models like GPT-4.

### Open Question 3
- Question: What is the optimal amount of context and examples to include in prompts for LMMs to achieve the best performance in pathology report information extraction?
- Basis in paper: [explicit] The authors mention "We provided the model with sufficient context for enhanced comprehension by incorporating standard categorization schemes and examples. Determining the right amount of context is challenging, with the primary constraint being the token window limit in API requests which dictates the maximum size of the prompt."
- Why unresolved: The paper does not explore the impact of varying the amount of context and examples in prompts on the performance of LMMs for pathology report information extraction.
- What evidence would resolve it: Systematically varying the amount of context and examples in prompts and measuring the impact on LMM performance for pathology report information extraction, while considering the token window limit.

## Limitations
- Performance depends on GPT-4's OCR accuracy and ability to generalize across different pathology report formats
- Confidence scores improve selection quality but don't eliminate errors entirely
- System may struggle with highly variable or illegible handwriting
- Calibration of confidence scores using Platt's scaling may not generalize well to datasets with different characteristics

## Confidence

- **Mechanism 1 (Two-stage prompting for confidence)**: High confidence
- **Mechanism 2 (Confidence as accuracy indicator)**: Medium confidence
- **Mechanism 3 (Prognostic value of standardized reports)**: Medium confidence

## Next Checks
1. **Generalization Test**: Apply LABIEB to pathology reports from a different medical center or cancer type to assess robustness and identify any format-specific weaknesses.
2. **Confidence Calibration Validation**: Perform cross-dataset calibration of confidence scores to test if Platt's scaling generalizes or if dataset-specific recalibration is required.
3. **Survival Model Robustness**: Evaluate the survival prediction performance using alternative survival models (e.g., Cox proportional hazards with different feature sets) to confirm the prognostic value is not model-dependent.