---
ver: rpa2
title: 'LLM2: Let Large Language Models Harness System 2 Reasoning'
arxiv_id: '2412.20372'
source_url: https://arxiv.org/abs/2412.20372
tags:
- llm2
- verifier
- gsm8k
- arxiv
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of undesirable outputs from large
  language models (LLMs) in tasks like mathematical reasoning by proposing LLM2, a
  framework that combines an LLM with a process-based verifier inspired by dual-process
  theory. The verifier provides token-level feedback during generation to distinguish
  between desirable and undesirable outputs, trained on synthetic process-supervision
  data generated through a token quality exploration strategy.
---

# LLM2: Let Large Language Models Harness System 2 Reasoning

## Quick Facts
- arXiv ID: 2412.20372
- Source URL: https://arxiv.org/abs/2412.20372
- Reference count: 15
- One-line primary result: LLM2 improves mathematical reasoning accuracy on GSM8K and MATH by 4.6-7.5 percentage points over vanilla Llama3 models

## Executive Summary
This paper addresses the problem of undesirable outputs from large language models (LLMs) in tasks like mathematical reasoning by proposing LLM2, a framework that combines an LLM with a process-based verifier inspired by dual-process theory. The verifier provides token-level feedback during generation to distinguish between desirable and undesirable outputs, trained on synthetic process-supervision data generated through a token quality exploration strategy. On mathematical reasoning benchmarks GSM8K and MATH, LLM2 improves accuracy over vanilla Llama3 models (1B-8B) by 4.6-7.5 percentage points, with further gains of 14.0 points when combined with self-consistency.

## Method Summary
LLM2 integrates a process-based verifier with an LLM during inference to provide token-level feedback that distinguishes desirable from undesirable outputs. The verifier is trained using synthetic process-supervision data generated through a token quality exploration strategy that samples negative examples by identifying tokens leading to low-quality continuations. The framework uses pairwise comparison loss to train the verifier to score candidate tokens at each generation step, with the LLM2 system adjusting logits based on these scores before applying decoding. This dual-process architecture combines the LLM's generation capabilities (System 1) with the verifier's correction mechanism (System 2) to improve mathematical reasoning performance.

## Key Results
- LLM2 achieves 4.6-7.5 percentage point improvements on GSM8K and MATH benchmarks across Llama3 models (1B-8B)
- Combined with self-consistency, LLM2 achieves 14.0 percentage point improvements over vanilla Llama3 models
- The approach demonstrates robust performance across different LLM families and tasks
- LLM2 shows consistent improvements across different model sizes (1B-8B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The verifier provides real-time token-level feedback that steers generation away from undesirable outputs
- Mechanism: The process-based verifier scores each candidate token at each generation step, and the LLM2 framework adjusts logits based on these scores before applying decoding
- Core assumption: The verifier can effectively distinguish between desirable and undesirable tokens through pairwise comparison training
- Evidence anchors:
  - [abstract] "the verifier provides timely process-based feedback to distinguish desirable and undesirable outputs"
  - [section 2.2] "We train the verifier with a pairwise comparison loss to distinguish between desirable and undesirable tokens"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: Synthetic process-supervision data enables training of the verifier without human annotations
- Mechanism: The token quality exploration strategy generates negative examples by sampling tokens that lead to low-quality continuations, paired with ground truth tokens as positive examples
- Core assumption: Tokens that lead to incorrect answers are informative negative examples for training the verifier
- Evidence anchors:
  - [section 2.3] "we introduce a token quality exploration strategy for sampling x−t" and "evaluate the quality of each continuation based on the correctness of all decoded answers"
  - [abstract] "The verifier is trained with a pairwise comparison loss on synthetic process-supervision data generated through our token quality exploration strategy"
  - [corpus] Weak - no direct corpus evidence for this specific synthetic data generation approach

### Mechanism 3
- Claim: Integrating System 2 reasoning during inference improves mathematical reasoning performance
- Mechanism: The dual-process architecture combines LLM's generation (System 1) with verifier's correction (System 2) to catch errors during the generation process
- Core assumption: Mathematical reasoning errors can be caught and corrected through token-level verification
- Evidence anchors:
  - [abstract] "empirical results on mathematical reasoning benchmarks substantiate the efficacy of LLM2"
  - [section 3.2] "LLM2 achieves substantial performance improvement across Llama3 models"
  - [corpus] Weak - no direct corpus evidence for this specific dual-process integration

## Foundational Learning

- Concept: Dual-process theory of cognition
  - Why needed here: Provides the theoretical foundation for why combining System 1 (generation) with System 2 (verification) should improve reasoning
  - Quick check question: Can you explain the difference between System 1 and System 2 reasoning in human cognition?

- Concept: Pairwise comparison loss
  - Why needed here: This loss function trains the verifier to distinguish between better and worse tokens, which is essential for the steering mechanism
  - Quick check question: How does pairwise comparison loss differ from traditional classification loss?

- Concept: Synthetic data generation for process supervision
  - Why needed here: Enables training the verifier without expensive human-annotated process data by creating token-level supervision automatically
  - Quick check question: Why might sampling tokens that lead to incorrect answers be useful for training a verifier?

## Architecture Onboarding

- Component map:
  - LLM (System 1) -> Process-based verifier (System 2) -> Token quality exploration -> Dual-process integration

- Critical path:
  1. During training: Generate synthetic data → Train verifier with pairwise loss
  2. During inference: LLM generates candidates → Verifier scores candidates → Adjust logits → Apply decoding

- Design tradeoffs:
  - Latency vs accuracy: Adding verification increases latency but improves accuracy
  - Verifier quality vs LLM performance: Poor verifier can hurt performance, good verifier improves it
  - Token sampling strategy: More comprehensive sampling is better but more expensive

- Failure signatures:
  - Performance worse than vanilla: Verifier is actively harmful (likely poorly trained)
  - Minimal improvement: Verifier is not providing useful guidance
  - High latency without benefit: Token filtering is too aggressive or verifier is too slow

- First 3 experiments:
  1. Implement vanilla LLM and measure baseline performance on GSM8K
  2. Add verifier with ground truth supervision (bypass synthetic data) to verify mechanism works
  3. Implement token quality exploration and measure synthetic data quality before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM2 perform on open-ended tasks like creative writing compared to structured reasoning tasks?
- Basis in paper: [inferred] The authors mention that applying LLM2 to open-ended tasks like creative writing presents challenges due to the lack of definitive supervisory signals for synthetic process-supervision.
- Why unresolved: The paper only evaluates LLM2 on mathematical reasoning tasks (GSM8K and MATH) and does not explore its effectiveness on open-ended tasks.
- What evidence would resolve it: Experiments comparing LLM2's performance on creative writing tasks versus mathematical reasoning tasks, along with analysis of the challenges in generating synthetic process-supervision data for open-ended tasks.

### Open Question 2
- Question: What is the impact of using different quality assessment metrics in the token quality exploration strategy?
- Basis in paper: [explicit] The authors use accuracy as the quality measure for continuations in Equation 5, but do not explore other potential metrics.
- Why unresolved: The paper only experiments with accuracy as the quality measure and does not investigate how other metrics (e.g., fluency, coherence) might affect the verifier's performance.
- What evidence would resolve it: Comparative experiments using different quality assessment metrics (e.g., fluency scores, semantic similarity) and analysis of their impact on the verifier's ability to distinguish desirable and undesirable tokens.

### Open Question 3
- Question: How does the token quality exploration strategy scale with larger vocabulary sizes or more complex tasks?
- Basis in paper: [inferred] The authors mention that the token quality exploration strategy evaluates the quality of individual tokens based on their potential impact on the generated text, but do not discuss scalability concerns.
- Why unresolved: The paper does not provide information on how the computational cost of the token quality exploration strategy changes with larger vocabularies or more complex tasks.
- What evidence would resolve it: Analysis of the computational complexity of the token quality exploration strategy as a function of vocabulary size and task complexity, along with experiments demonstrating its scalability.

### Open Question 4
- Question: What is the effect of varying the threshold hyperparameter τ in the negative sampling process?
- Basis in paper: [explicit] The authors use τ = 0.5 in Equation 6 for negative sampling but do not explore the impact of different threshold values.
- Why unresolved: The paper does not investigate how different values of τ affect the quality of negative examples and the verifier's performance.
- What evidence would resolve it: Experiments varying τ across a range of values and analyzing its impact on the verifier's accuracy and the overall performance of LLM2.

### Open Question 5
- Question: How does LLM2 compare to other process-based supervision methods that use human-annotated data?
- Basis in paper: [inferred] The authors mention that LLM2 uses synthetic process-supervision data generated through a token quality exploration strategy, but do not compare it to methods using human-annotated data.
- Why unresolved: The paper does not provide a direct comparison between LLM2 and process-based supervision methods that rely on human annotations.
- What evidence would resolve it: Experiments comparing LLM2's performance to process-based supervision methods using human-annotated data, along with an analysis of the trade-offs between synthetic and human-annotated data.

## Limitations
- Synthetic Data Quality Uncertainty: The token quality exploration strategy generates synthetic supervision data, but the paper does not provide detailed validation of whether this synthetic data effectively captures the true distinction between desirable and undesirable tokens
- No Comparison with Alternative Verification Methods: The paper only compares LLM2 against vanilla Llama3 models and self-consistency methods, without comparing to other verification or process-supervision approaches
- Scalability and Computational Overhead: The token quality exploration strategy requires generating multiple continuations for each token, which could become computationally prohibitive for larger models or longer sequences

## Confidence
- High Confidence: The core mechanism of using a verifier to provide token-level feedback during generation is well-specified and the experimental results show consistent improvements across different model sizes and datasets
- Medium Confidence: The effectiveness of the synthetic data generation strategy and its ability to produce meaningful supervision for the verifier
- Low Confidence: The generalizability of the approach to non-mathematical reasoning tasks, as the paper only evaluates on mathematical reasoning benchmarks

## Next Checks
1. **Synthetic Data Quality Analysis**: Generate and analyze the synthetic supervision data to verify that tokens leading to incorrect answers are indeed informative negative examples, including manual inspection of token pairs or ablation studies with different sampling strategies

2. **Cross-Domain Performance Evaluation**: Test LLM2 on non-mathematical reasoning tasks such as commonsense reasoning or code generation benchmarks to assess whether the dual-process architecture provides benefits beyond structured mathematical reasoning

3. **Alternative Verification Mechanism Comparison**: Implement and compare against alternative verification approaches such as reward modeling or direct supervision from intermediate reasoning steps to determine whether the specific dual-process architecture provides unique advantages