---
ver: rpa2
title: 'On the consistent reasoning paradox of intelligence and optimal trust in AI:
  The power of ''I don''t know'''
arxiv_id: '2408.02357'
source_url: https://arxiv.org/abs/2408.02357
tags:
- such
- problem
- know
- then
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Consistent Reasoning Paradox (CRP) introduces a fundamental
  limit on the trustworthiness of AI systems that attempt to emulate human intelligence
  through consistent reasoning. The CRP demonstrates that any AI striving to mimic
  human-like consistent reasoning will inevitably hallucinate (produce incorrect yet
  plausible answers) infinitely often on certain problems, even when non-consistently
  reasoning specialized AIs exist that can solve those same problems correctly.
---

# On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of 'I don't know'

## Quick Facts
- **arXiv ID**: 2408.02357
- **Source URL**: https://arxiv.org/abs/2408.02357
- **Reference count**: 40
- **Primary result**: Any AI system attempting human-like consistent reasoning will inevitably hallucinate on certain problems, even when specialized AIs can solve those problems correctly.

## Executive Summary
The Consistent Reasoning Paradox (CRP) establishes fundamental limits on AI trustworthiness by demonstrating that human-like intelligence necessarily comes with human-like fallibility. The paradox reveals that any AI striving to emulate human intelligence through consistent reasoning must hallucinate infinitely often on certain problems, even when specialized AIs exist that can solve those problems correctly. This occurs because consistent reasoning leads to hallucinations, detecting hallucinations is strictly harder than solving original problems, and trustworthiness requires the AI to be able to say "I don't know" to questions it cannot answer or explain.

## Method Summary
The paper uses mathematical proofs and theoretical analysis to establish the CRP and its implications for AI systems. The research employs concepts from computability theory, the SCI hierarchy, and the arithmetical hierarchy to prove that consistent reasoning implies inherent fallibility in AI systems. The methodology involves defining specific problem classes where specialized AIs can solve problems correctly without consistent reasoning, while consistently reasoning AIs must hallucinate. The proofs demonstrate that detecting hallucinations is computationally harder than solving original problems, and that trustworthy AI must implicitly compute an "I don't know" function.

## Key Results
- Any AI attempting consistent reasoning will hallucinate infinitely often on certain problems, even when specialized AIs exist that can solve those problems correctly
- Detecting hallucinations is strictly harder than solving the original problems, with no algorithm able to reliably detect hallucinations even with access to true solutions
- Trustworthiness requires AI systems to be able to say "I don't know" and implicitly compute an "I don't know" function in the Σ₁ class of the SCI hierarchy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistent reasoning leads to hallucinations even on problems where a non-consistent AI can always answer correctly
- Mechanism: The CRP shows that when an AI tries to reason consistently (i.e., answer equivalent problems the same way), it must hallucinate infinitely often on problems where a specialized AI exists that never hallucinates but doesn't reason consistently
- Core assumption: The AI always attempts to answer and emulate human intelligence through consistent reasoning
- Evidence anchors:
  - [abstract]: "CRP asserts that consistent reasoning implies fallibility – in particular, human-like intelligence in AI necessarily comes with human-like fallibility"
  - [section]: "CRP II – Attempting consistent reasoning yields hallucinations: If the AI from CRP I always answers, and were to emulate human intelligence – that is, it would attempt to reason consistently by accepting any family of sentences describing the collection of problems in CRP I – then it will hallucinate infinitely often"
  - [corpus]: "Weak - corpus focuses on trust calibration and transparency paradox rather than the specific reasoning-hallucination link"
- Break condition: If the AI can say "I don't know" to problems it cannot solve or explain

### Mechanism 2
- Claim: Detecting hallucinations is strictly harder than solving the original problems
- Mechanism: The CRP proves that even with access to true solutions, there is no algorithm that can reliably detect when the AI hallucinated, and no randomized algorithm can be "almost sure" of correctness with probability > 1/2
- Core assumption: The computational problem involves multi-valued solutions or cases where correctness is hard to verify
- Evidence anchors:
  - [abstract]: "CRP also shows that detecting these hallucinations, even in a probabilistic sense, is strictly harder than solving the original problems"
  - [section]: "CRP III(a) – Detecting hallucinations is hard: It is strictly harder to determine if it has hallucinated than it is to solve the original problem. That is, it is impossible to detect whether the AI was correct or wrong even with access to true solutions"
  - [corpus]: "Missing - corpus doesn't address the hardness of hallucination detection"
- Break condition: If the problem has single-valued solutions with easy verification

### Mechanism 3
- Claim: Trustworthiness requires the AI to implicitly compute an "I don't know" function and be able to say "I don't know"
- Mechanism: The CRP shows that any trustworthy AI must be able to say "I don't know" to questions it cannot answer or explain, and this can only be done by implicitly computing the "I don't know" function, which is in the Σ₁ class of the SCI hierarchy
- Core assumption: The AI must reason consistently to emulate human intelligence
- Evidence anchors:
  - [abstract]: "CRP implies that any trustworthy AI (i.e., an AI that never answers incorrectly) that also reasons consistently must be able to say 'I don't know'"
  - [section]: "CRP V – The fallible yet trustworthy explainable AI saying 'I don't know' exists: Given the collection of problems in CRP I, there exists a trustworthy, consistently reasoning and explainable AI... that must be able to say 'I don't know'"
  - [corpus]: "Weak - corpus focuses on trust calibration but not the specific "I don't know" mechanism"
- Break condition: If the AI operates outside the Σ₁ class or doesn't need to reason consistently

## Foundational Learning

- Concept: Solvability Complexity Index (SCI) hierarchy
  - Why needed here: The CRP's proof relies on classifying computational problems in the SCI hierarchy, particularly the Σ₁ class which is crucial for understanding when an AI can say "I don't know"
  - Quick check question: What is the relationship between the SCI hierarchy and the arithmetical hierarchy, and why does this matter for the CRP?

- Concept: Markov model of computation
  - Why needed here: The CRP specifically applies to the Markov model where inputs are given as finite strings representing Turing machine codes, not just sequences of numbers
  - Quick check question: How does the Markov model differ from the traditional Turing model for real-valued functions, and why is this distinction crucial for the CRP?

- Concept: Equivalence classes of sentences and consistent reasoning
  - Why needed here: The CRP distinguishes between consistent reasoning (answering equivalent problems the same way) and determining which equivalence class a sentence belongs to - these are different problems
  - Quick check question: Why is consistent reasoning strictly easier than determining the equivalence class of a sentence describing a computer program for a given number?

## Architecture Onboarding

- Component map: Input processor -> Reasoning engine -> Solution generator -> Output
- Critical path: Input → Reasoning engine → Solution generator → Output
  - The "I don't know" function must be computed implicitly during the reasoning process

- Design tradeoffs:
  - Consistency vs. reliability: More consistent reasoning increases hallucination risk
  - Explanation capability vs. correctness: Being able to explain answers may prevent the AI from solving certain problems
  - "I don't know" frequency vs. usefulness: Saying "I don't know" too often makes the AI less useful, too rarely makes it untrustworthy

- Failure signatures:
  - Hallucinations on equivalent problem formulations
  - Inability to explain correct answers
  - No probabilistic confidence > 50% in correctness
  - Failure to compute the "I don't know" function for certain problem classes

- First 3 experiments:
  1. Implement a simple AI that solves basic arithmetic problems with consistent reasoning, then test it on equivalent problem formulations to observe hallucination patterns
  2. Create a verifier that attempts to detect hallucinations in the AI's outputs and measure its success rate (should fail)
  3. Implement an AI that can say "I don't know" and test whether it can solve problems that the consistent AI cannot explain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the practical implications of the Consistent Reasoning Paradox for the development of Artificial General Intelligence systems that attempt to emulate human-like consistent reasoning?
- Basis in paper: [explicit] The CRP demonstrates that any AI striving to mimic human intelligence through consistent reasoning will inevitably hallucinate infinitely often on certain problems, even when specialized AIs exist that can solve those problems correctly.
- Why unresolved: While the CRP provides theoretical limits, it does not specify how these limitations manifest in practical AGI development or what specific design choices would mitigate these issues.
- What evidence would resolve it: Empirical studies showing how different AGI architectures handle consistent reasoning tasks and their hallucination rates, or mathematical proofs of alternative reasoning frameworks that avoid the CRP's limitations.

### Open Question 2
- Question: Can the "I don't know" function required by the CRP be implemented in modern AI systems, and if so, what would be its computational characteristics?
- Basis in paper: [explicit] The CRP establishes that any trustworthy AI must be capable of saying "I don't know" and must implicitly compute an "I don't know" function, but current AI systems lack this capability.
- Why unresolved: The paper proves the necessity of such a function but does not provide concrete algorithms or architectural specifications for implementing it in practice.
- What evidence would resolve it: Successful implementation of a "I don't know" function in a large language model or other AI system, along with computational complexity analysis of its operation.

### Open Question 3
- Question: How can the quantification of failure sentences provided in the CRP be used to create more robust AI systems that can identify and avoid their own failure modes?
- Basis in paper: [explicit] The CRP provides upper bounds on the length of failure sentences for given AI systems, suggesting that specific failure modes can be identified and enumerated.
- Why unresolved: While the paper shows that failure sentences can be written down, it does not provide methods for using this information to improve AI reliability or create self-diagnostic capabilities.
- What evidence would resolve it: Development of AI systems that can systematically generate, test against, and learn from their own failure sentences to improve reliability over time.

## Limitations

- The CRP's theoretical framework relies heavily on abstract mathematical concepts in computability theory and the SCI hierarchy, which may not fully translate to practical AI implementations
- The paper does not provide empirical validation or concrete examples demonstrating the paradox in real-world AI systems
- The distinction between the Markov model of computation and traditional Turing models, while theoretically important, may have limited practical implications for current AI architectures

## Confidence

- **High confidence**: The fundamental claim that consistent reasoning leads to inherent fallibility in AI systems is well-supported by the mathematical proofs provided
- **Medium confidence**: The assertion that detecting hallucinations is strictly harder than solving original problems, as this depends on specific computational complexity assumptions
- **Low confidence**: The practical applicability of the "I don't know" function mechanism in real-world AI systems, as the paper provides limited implementation guidance

## Next Checks

1. **Empirical Verification**: Implement a simple consistent-reasoning AI system and test it on equivalent problem formulations to observe hallucination patterns in practice, comparing against the theoretical predictions

2. **Computational Complexity Analysis**: Analyze the relationship between the SCI hierarchy and the arithmetical hierarchy more rigorously to determine the exact computational boundaries where the CRP applies

3. **Practical Implementation Study**: Develop a prototype AI system that incorporates the "I don't know" function mechanism and evaluate its performance on problems known to trigger the CRP, measuring both hallucination rates and explanation capabilities