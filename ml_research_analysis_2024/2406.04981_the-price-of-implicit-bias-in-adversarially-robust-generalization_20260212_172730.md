---
ver: rpa2
title: The Price of Implicit Bias in Adversarially Robust Generalization
arxiv_id: '2406.04981'
source_url: https://arxiv.org/abs/2406.04981
tags:
- robust
- learning
- page
- generalization
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how the implicit bias of optimization algorithms
  in robust empirical risk minimization (robust ERM) affects the generalization performance
  of adversarially trained models. While implicit bias is known to aid generalization
  in standard ERM, the paper shows it can hinder robust generalization when the bias
  and threat model are misaligned.
---

# The Price of Implicit Bias in Adversarially Robust Generalization

## Quick Facts
- arXiv ID: 2406.04981
- Source URL: https://arxiv.org/abs/2406.04981
- Reference count: 40
- Primary result: The implicit bias of optimization algorithms in robust ERM can significantly affect adversarial generalization, with alignment between algorithm bias and perturbation model being critical.

## Executive Summary
This work investigates how the implicit bias of optimization algorithms in robust empirical risk minimization (robust ERM) affects the generalization performance of adversarially trained models. While implicit bias is known to aid generalization in standard ERM, the paper shows it can hinder robust generalization when the bias and threat model are misaligned. The authors analyze linear models and diagonal neural networks, deriving theoretical bounds and showing that the choice of regularization norm (e.g., ℓ1 vs. ℓ2) interacts with the perturbation norm to determine optimal capacity control. Experiments with synthetic data confirm that algorithms like coordinate descent or sign gradient descent can significantly outperform standard gradient descent in robust settings, depending on data sparsity and perturbation strength. On real-world image classification tasks, the choice of optimization algorithm further influences robust test accuracy, especially with limited data or larger perturbations, highlighting the need for algorithm selection tailored to robust learning objectives.

## Method Summary
The authors study implicit bias in robust ERM by analyzing optimization algorithms (gradient descent, coordinate descent, steepest descent with different norms) and their regularization effects on linear and diagonal neural network models. They derive generalization bounds connecting algorithm choice to robust performance, then validate these findings through synthetic experiments with controlled sparsity and real-world MNIST classification tasks. The method involves comparing robust generalization gaps across different algorithms, architectures, and data sparsity levels while varying perturbation magnitudes.

## Key Results
- The choice of optimization algorithm significantly affects robust generalization, with alignment between algorithm bias and perturbation model being critical
- ℓ1 regularization (promoted by coordinate descent) outperforms ℓ2 (gradient descent) for sparse problems under adversarial perturbations
- Architecture matters: diagonal neural networks exhibit different implicit biases than standard linear models, affecting robust generalization
- On MNIST, sign gradient descent can achieve higher robust accuracy than standard gradient descent, especially with limited data or larger perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The implicit bias of optimization algorithms affects robust generalization differently than standard generalization, and this effect depends on the alignment between the algorithm's induced regularization and the perturbation threat model.
- Mechanism: Different optimization algorithms (gradient descent, coordinate descent, steepest descent with different norms) induce different implicit regularizations on the learned model. When these regularizations align with the geometry of the perturbation threat model, robust generalization improves; misalignment causes poor robust generalization despite good standard generalization.
- Core assumption: The implicit bias of optimization algorithms can be characterized and understood in terms of which norms they implicitly minimize, and this characterization remains meaningful under robust ERM with adversarial perturbations.
- Evidence anchors:
  - [abstract]: "the choice of optimization algorithm further influences robust test accuracy, especially with limited data or larger perturbations, highlighting the need for algorithm selection tailored to robust learning objectives."
  - [section]: "we demonstrate two ways this price can appear; either by varying the optimization algorithm or the architecture."
  - [corpus]: "Adversarially robust generalization theory via Jacobian regularization for deep neural networks" (weak evidence, different focus)
- Break condition: If the optimization algorithm's implicit bias changes dramatically during training or depends heavily on hyperparameters, the characterization of alignment may break down.

### Mechanism 2
- Claim: The architecture of the model affects the implicit bias of robust ERM, even when models have equivalent expressive power.
- Mechanism: Different parameterizations of the same function class (e.g., standard linear models vs. diagonal neural networks) lead to different implicit regularizations when optimized with the same algorithm. For diagonal networks, gradient descent implicitly biases toward minimum ℓ1 solutions in prediction space, which can improve robust generalization for ℓ∞ perturbations.
- Core assumption: The parameterization affects the trajectory of optimization and the implicit regularization, even when the models are mathematically equivalent in terms of function space.
- Evidence anchors:
  - [abstract]: "We then turn our attention to study the role of architecture in robust ERM... These are just reparameterized linear models, and thus their expressive power does not change. Yet, as we show, robust ERM drives them to solutions with very different properties"
  - [section]: "optimizing them can result in very different predictors... robust ERM drives them to solutions with very different properties"
  - [corpus]: "Generalization Error of $f$-Divergence Stabilized Algorithms via Duality" (weak evidence, different focus)
- Break condition: If the architecture introduces additional complexity or non-linearities that interact with the perturbation model in unexpected ways, the simple relationship between parameterization and implicit bias may not hold.

### Mechanism 3
- Claim: The sparsity of the data-generating process interacts with the choice of regularization norm and perturbation magnitude to determine optimal capacity control for robust generalization.
- Mechanism: When the ground truth model or data are sparse, ℓ1 regularization (promoted by certain algorithms like coordinate descent) provides better generalization than ℓ2, especially as perturbation magnitude increases. The dimension-dependent terms in generalization bounds make ℓ2 regularization increasingly suboptimal for sparse problems under adversarial perturbations.
- Core assumption: The sparsity structure of the problem is known or can be estimated, and the generalization bounds accurately capture the tradeoffs between different regularization choices.
- Evidence anchors:
  - [abstract]: "we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization"
  - [section]: "the optimal choice of regularization depends not only on the choice of norm p and the value of ϵ, but also on the sparsity of the data-generating process"
  - [corpus]: "Empirical Risk Minimization with $f$-Divergence Regularization" (weak evidence, different focus)
- Break condition: If the data sparsity structure is complex or changes during training, or if the generalization bounds don't accurately capture the true generalization behavior, the optimal regularization choice may differ from theoretical predictions.

## Foundational Learning

- Concept: Implicit bias of optimization algorithms
  - Why needed here: Understanding how different optimization algorithms induce different regularizations on the learned model is crucial for selecting algorithms that align with robust learning objectives.
  - Quick check question: Can you explain why gradient descent tends to find minimum ℓ2 norm solutions, while coordinate descent tends to find minimum ℓ1 norm solutions?

- Concept: Rademacher complexity and margin bounds
  - Why needed here: These theoretical tools are used to derive generalization bounds that connect the choice of regularization norm to robust generalization performance.
  - Quick check question: How does the Rademacher complexity of a hypothesis class change when we add adversarial perturbations to the training data?

- Concept: Duality between parameter space and prediction space regularization
  - Why needed here: Understanding how regularization in parameter space translates to properties in prediction space is essential for interpreting the effects of different architectures and algorithms on robust generalization.
  - Quick check question: Why does ℓ2 regularization in parameter space for diagonal networks correspond to ℓ1 regularization in prediction space?

## Architecture Onboarding

- Component map:
  - Linear models with steepest descent (ℓ1, ℓ2, ℓ∞ norms) -> Diagonal neural networks -> Standard neural networks (fully connected, convolutional)
  - Synthetic data generators with controllable sparsity -> MNIST/CIFAR datasets for real-world evaluation

- Critical path:
  1. Generate synthetic data with known ground truth and sparsity structure
  2. Train with different optimization algorithms (GD, CD, SD)
  3. Evaluate robust generalization under ℓ∞ perturbations
  4. Compare with theoretical predictions from generalization bounds
  5. Validate findings on real-world datasets

- Design tradeoffs:
  - Simpler models (linear, diagonal) allow theoretical analysis but may not capture all phenomena
  - More complex models (deep networks) better reflect practice but are harder to analyze
  - Synthetic data provides control but may miss real-world complexities
  - Real datasets validate findings but introduce confounding factors

- Failure signatures:
  - Large gap between train and test robust accuracy (overfitting)
  - Inconsistent results across random seeds
  - Theoretical predictions not matching empirical results
  - Convergence issues with certain optimization algorithms

- First 3 experiments:
  1. Binary classification on synthetic data with dense teacher and dense samples, comparing GD vs CD under ℓ∞ perturbations
  2. Binary classification on synthetic data with sparse teacher and dense samples, comparing GD vs CD under ℓ∞ perturbations
  3. Binary classification on synthetic data with sparse teacher and sparse samples, comparing GD vs CD under ℓ∞ perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the choice of optimization algorithm in robust ERM influence the occurrence of robust overfitting?
- Basis in paper: [explicit] The paper notes that in robust ERM, the gap between gradient descent and sign gradient descent increases, and suggests that robust overfitting may be due to the implicit bias of an algorithm kicking in late during robust ERM.
- Why unresolved: The paper suggests this as a possibility but does not conduct a detailed analysis of how different optimization algorithms affect the onset and severity of robust overfitting.
- What evidence would resolve it: A systematic study comparing the training dynamics and test accuracy of various optimization algorithms (e.g., gradient descent, sign gradient descent, coordinate descent) in robust ERM, tracking the point at which robust overfitting occurs and its magnitude.

### Open Question 2
- Question: How does the sparsity of the data and the ground truth vector influence the optimal choice of regularization norm (e.g., ℓ1 vs. ℓ2) in robust ERM?
- Basis in paper: [explicit] The paper demonstrates through theoretical analysis and experiments that the optimal choice of regularization norm depends on the sparsity of the data and the ground truth vector, with ℓ1 regularization being more beneficial for sparse solutions and ℓ2 regularization being more suitable for dense solutions.
- Why unresolved: The paper provides insights into this relationship but does not offer a comprehensive framework or guidelines for selecting the optimal regularization norm based on data sparsity.
- What evidence would resolve it: A comprehensive study analyzing the impact of data sparsity on the performance of different regularization norms (e.g., ℓ1, ℓ2) in robust ERM, across various datasets and model architectures.

### Open Question 3
- Question: Can the implicit bias of optimization algorithms be leveraged to improve the robustness of neural networks beyond linear models?
- Basis in paper: [explicit] The paper shows that the choice of optimization algorithm affects the robustness of linear models and suggests that this effect extends to neural networks, with sign gradient descent outperforming gradient descent in certain settings.
- Why unresolved: The paper focuses on linear models and provides limited evidence for the impact of optimization bias on neural network robustness.
- What evidence would resolve it: A thorough investigation of how different optimization algorithms influence the robustness of various neural network architectures (e.g., fully connected, convolutional) in robust ERM, considering different perturbation norms and datasets.

## Limitations

- Analysis relies heavily on simplified linear and diagonal architectures that may not fully capture deep neural network complexities
- Theoretical bounds depend on specific assumptions about data distributions and perturbation models that may not hold in practice
- Results show sensitivity to synthetic data parameters (like sparsity levels), suggesting dependence on problem structure

## Confidence

- High confidence: The core observation that optimization algorithm choice affects robust generalization performance, supported by both theoretical analysis and experimental validation
- Medium confidence: The specific claims about ℓ1 vs ℓ2 regularization being optimal depending on data sparsity and perturbation magnitude, as these depend on the accuracy of derived bounds
- Low confidence: The generalization of findings from linear/diagonal models to deep networks, given the significant architectural differences

## Next Checks

1. Test the algorithm selection guidance on more diverse real-world datasets beyond MNIST, particularly those with different intrinsic sparsity structures
2. Investigate whether the implicit bias characterization holds throughout the full training trajectory of deep networks, not just at convergence
3. Evaluate the sensitivity of results to different loss functions and robust optimization methods beyond standard FGSM-based training