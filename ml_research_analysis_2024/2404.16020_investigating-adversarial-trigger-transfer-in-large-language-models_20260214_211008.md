---
ver: rpa2
title: Investigating Adversarial Trigger Transfer in Large Language Models
arxiv_id: '2404.16020'
source_url: https://arxiv.org/abs/2404.16020
tags:
- triggers
- optimized
- arxiv
- starling-7b
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial triggers optimized on aligned language models do not
  reliably transfer across models, especially for models aligned via preference optimization.
  Models aligned using supervised fine-tuning are more susceptible to adversarial
  triggers and can be jailbroken faster than models aligned via preference optimization.
---

# Investigating Adversarial Trigger Transfer in Large Language Models

## Quick Facts
- arXiv ID: 2404.16020
- Source URL: https://arxiv.org/abs/2404.16020
- Reference count: 40
- Primary result: Adversarial triggers optimized on aligned language models show limited transferability, with SFT-aligned models being more vulnerable than PPO-aligned models

## Executive Summary
This study investigates how adversarial triggers optimized on aligned large language models transfer across different alignment methods and model architectures. The research focuses on comparing transfer rates between models aligned via supervised fine-tuning (SFT) and preference optimization (PPO), finding that triggers optimized on SFT models generalize better to new unsafe instructions across diverse domains. The study reveals that PPO-aligned models are significantly more resistant to cross-model trigger transfer, suggesting that preference-based alignment provides stronger adversarial robustness. These findings have important implications for understanding the security landscape of aligned language models and the effectiveness of transfer-based adversarial attacks.

## Method Summary
The researchers employed a two-stage optimization process using adversarial triggers - short text sequences prepended to prompts that induce harmful outputs. They first optimized triggers on source models using either automatic jailbreak scoring or human annotations, then tested transferability to target models aligned via different methods. The study used Llama-2-7B and Llama-3-8B models with SFT and PPO alignment variants, evaluating trigger effectiveness on both seen and unseen unsafe instruction categories. Experiments compared transfer rates, optimization speed, and cross-domain generalization across different alignment configurations.

## Key Results
- Triggers optimized on SFT-aligned models transfer to new unsafe instructions from diverse domains, while PPO-aligned triggers show limited generalization
- SFT-based triggers can jailbreak aligned models faster than PPO-based triggers, requiring fewer optimization iterations
- Cross-model transfer is significantly more successful for SFT-to-SFT transfers compared to PPO-to-PPO or cross-method transfers
- PPO-aligned models demonstrate substantially higher resistance to adversarial trigger attacks compared to SFT-aligned models

## Why This Works (Mechanism)
The differential transfer success appears to stem from fundamental differences in how SFT and PPO alignment methods shape model behavior. SFT alignment applies uniform corrections across training examples, creating more predictable and consistent response patterns that adversarial triggers can exploit. In contrast, PPO alignment through preference optimization creates more nuanced and context-aware safety boundaries that are harder to bypass with simple trigger patterns. The study suggests that preference-based methods may learn more robust representations of harmful content that generalize better to unseen scenarios.

## Foundational Learning
- Supervised Fine-Tuning (SFT): Basic alignment method where models are trained on curated examples with desired outputs; needed for understanding baseline alignment approaches and why it creates more predictable patterns
- Preference Optimization (PPO): Advanced alignment using reinforcement learning from human feedback; needed to understand why it creates stronger adversarial resistance
- Adversarial Trigger Optimization: Process of finding input prefixes that induce harmful outputs; needed to understand attack methodology and transferability patterns
- Model Transferability: Concept of applying techniques from one model to another; needed to evaluate cross-model attack effectiveness
- Safety Alignment Evaluation: Methods for assessing model robustness to harmful outputs; needed to understand safety benchmarks and evaluation metrics

## Architecture Onboarding
Component Map: Model Architecture -> Alignment Method -> Adversarial Trigger Optimization -> Transferability Testing
Critical Path: Source Model Alignment → Trigger Optimization → Target Model Transfer → Safety Evaluation
Design Tradeoffs: SFT offers faster training but weaker safety vs PPO offers stronger safety but higher computational cost
Failure Signatures: Complete transfer failure for PPO models, domain-specific failures for SFT triggers, optimization convergence issues
First Experiments:
1. Compare trigger effectiveness on same-model vs cross-model transfers
2. Test transfer rates across different instruction domains
3. Measure optimization iteration requirements for successful jailbreaks

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to only two alignment methods (SFT and PPO) may not capture full landscape of modern alignment techniques
- Experiments use synthetic prompts rather than real-world attack scenarios, potentially underestimating practical effectiveness
- Optimization methodology using automatic jailbreak scoring may miss nuanced forms of harmful output
- Study does not explore multi-trigger or sophisticated attack patterns that could achieve better cross-model transfer

## Confidence
- Transferability differences between SFT and PPO models: High
- Generalization of triggers to new unsafe instructions: Medium
- Speed advantage of SFT-based triggers: Medium
- Limited cross-model transfer for PPO models: High

## Next Checks
1. Test transferability across a broader range of alignment methods including DPO, SLiC, and direct preference optimization variants
2. Evaluate triggers using human-annotated safety assessments rather than automatic scoring
3. Investigate whether multi-trigger or context-aware adversarial patterns achieve better cross-model transfer rates, particularly for PPO-aligned models