---
ver: rpa2
title: 'Unplug and Play Language Models: Decomposing Experts in Language Models at
  Inference Time'
arxiv_id: '2404.11916'
source_url: https://arxiv.org/abs/2404.11916
tags:
- prompt
- language
- tuning
- inference
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach called SKIP to improve the
  inference efficiency of prompt tuning methods in language models. The key idea is
  to identify and retain only the skill-relevant neurons in the model for each specific
  task, thereby reducing the computational overhead during inference.
---

# Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time

## Quick Facts
- **arXiv ID**: 2404.11916
- **Source URL**: https://arxiv.org/abs/2404.11916
- **Reference count**: 7
- **Key outcome**: Achieves up to 160% inference speed-up with 52% parameter pruning rate while maintaining task performance

## Executive Summary
This paper introduces SKIP, a novel approach that improves inference efficiency of prompt tuning methods by identifying and retaining only skill-relevant neurons for each specific task. The method combines attribution-based neuron importance quantification with structured pruning to create task-specific subnetworks that can be activated during inference. Experiments on five natural language understanding benchmarks demonstrate that SKIP achieves significant speed improvements while maintaining or even improving task accuracy compared to standard prompt tuning methods.

## Method Summary
SKIP employs a four-step unplug-and-play process: receive user request, identify corresponding task expert, perform inference using expert-localized model, and restore original model. The core mechanism uses attribution methods to quantify skill relevance of each neuron, then applies structured pruning to eliminate skill-irrelevant neurons. A binary search algorithm determines the optimal pruning ratio that maintains task performance. The approach is integrated with prompt tuning, where skill-localized neurons are combined with task-specific prompt tokens during inference.

## Key Results
- Achieves up to 160% inference speed-up with 52% parameter pruning rate
- Maintains comparable or better task performance than standard prompt tuning
- Model-agnostic and scalable to larger models (tested up to BERT-large)
- Demonstrated effectiveness across five natural language understanding benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific neurons identified via attribution scores can be selectively pruned without losing task performance
- Mechanism: Attribution method quantifies neuron contribution to task output; neurons with low attribution scores are pruned using structured pruning
- Core assumption: Low attribution neurons are not critical for task performance
- Evidence anchors: Weak - no direct neighbor paper support for attribution-based pruning

### Mechanism 2
- Claim: Skill-localized subnetwork inference is faster than full model inference
- Mechanism: Only skill-relevant neurons are activated during inference, reducing computational operations
- Core assumption: Skill-irrelevant neurons don't need to be computed for task-specific inference
- Evidence anchors: Moderate - related expert pruning concepts in literature

### Mechanism 3
- Claim: Skill localization is more effective than prompt-only tuning
- Mechanism: Combines task-specific prompt tuning with parameter-efficient pruning for better efficiency
- Core assumption: Prompt tuning benefits from parameter reduction in addition to prompt optimization
- Evidence anchors: Moderate - expert pruning improves efficiency in related work

## Foundational Learning

- **Attribution methods for neural network explainability**: Why needed - Attribution scores are the core mechanism for identifying skill-relevant neurons; Quick check - How does the attribution score formula relate neuron activation to output gradient?
- **Structured pruning in neural networks**: Why needed - Structured pruning removes entire neurons/filters rather than individual weights for efficient speedup; Quick check - What's the difference between structured and unstructured pruning in terms of implementation?
- **Prompt tuning methodology**: Why needed - SKIP builds on prompt tuning by adding parameter efficiency layer; Quick check - How does prefix tuning differ from standard prompt tuning in terms of where tunable parameters are added?

## Architecture Onboarding

- **Component map**: Input → Attribution analysis → Binary search for pruning ratio → Structured pruning → Skill-localized prompt tuning → Output
- **Critical path**: Attribution computation → Skill neuron identification → Structured pruning → Fine-tuning with prompt tokens
- **Design tradeoffs**: More aggressive pruning gives better speed but risks accuracy; attribution computation adds training overhead but reduces inference cost
- **Failure signatures**: Accuracy drops after pruning (over-aggressive pruning); minimal speed improvement (ineffective pruning); attribution computation takes too long (inefficient attribution method)
- **First 3 experiments**:
  1. Run attribution computation on a small dataset subset to verify skill relevance scores correlate with task importance
  2. Test binary search pruning with varying margins to find optimal pruning ratio that maintains accuracy
  3. Compare inference speed of skill-localized model vs full model on a representative task to measure actual speedup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SKIP scale when applied to extremely large language models (e.g., models with hundreds of billions of parameters)?
- Basis in paper: The paper mentions scalability experiments but only for BERT-large
- Why unresolved: The paper only tests up to BERT-large and does not explore models with hundreds of billions of parameters
- What evidence would resolve it: Empirical results showing performance on models like GPT-3, PaLM, or similar large-scale architectures

### Open Question 2
- Question: What is the impact of using different attribution methods (e.g., Integrated Gradients, SHAP) on the skill relevance quantification and overall performance of SKIP?
- Basis in paper: The paper uses a specific attribution method but does not compare it with other methods
- Why unresolved: The choice of attribution method could significantly affect the identification of skill-relevant neurons
- What evidence would resolve it: Comparative experiments using different attribution methods

### Open Question 3
- Question: How does SKIP perform in multi-task settings where a single model needs to handle multiple tasks simultaneously?
- Basis in paper: The paper assumes a single-task setting and does not explore multi-task scenarios
- Why unresolved: Multi-task learning is a common use case for language models
- What evidence would resolve it: Experiments on multi-task benchmarks like GLUE or SuperGLUE

### Open Question 4
- Question: What is the effect of using different structured pruning techniques on the efficiency and accuracy of SKIP?
- Basis in paper: The paper mentions using structured pruning but does not explore different pruning techniques
- Why unresolved: Different pruning methods might yield varying levels of efficiency and accuracy
- What evidence would resolve it: Comparative analysis of various structured pruning techniques within the SKIP framework

## Limitations
- Reliance on attribution-based pruning may not generalize across all task types
- Binary search mechanism adds complexity and computational overhead during training
- Effectiveness on more complex, multi-domain tasks remains unverified
- Structured pruning may limit future model adaptation capabilities

## Confidence
- Mechanism 1 (Attribution-based pruning): Medium - Well-supported by the paper but attribution methods can be task-dependent
- Mechanism 2 (Speed-up claims): High - Quantitative results show clear improvements, though real-world implementation may vary
- Mechanism 3 (Effectiveness vs prompt-only): Medium - Ablation studies support this but comparisons could be more comprehensive

## Next Checks
1. Apply SKIP to a multi-task learning scenario where the same model handles multiple domains to verify if skill-localization remains effective across heterogeneous tasks
2. Compare skill relevance scores using different attribution methods (e.g., Integrated Gradients, SHAP) to ensure results aren't method-dependent
3. Test whether periodically recomputing skill relevance scores improves performance over time as task distributions shift, addressing potential static pruning limitations