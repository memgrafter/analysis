---
ver: rpa2
title: 'Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for
  Russian'
arxiv_id: '2405.13929'
source_url: https://arxiv.org/abs/2405.13929
tags:
- russian
- vikhr
- language
- llms
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Vikhr, a family of bilingual open-source instruction-tuned
  large language models for Russian. The paper addresses challenges in non-English
  language generation, such as poor quality and reduced computational performance
  due to token vocabulary imbalance.
---

# Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian

## Quick Facts
- arXiv ID: 2405.13929
- Source URL: https://arxiv.org/abs/2405.13929
- Reference count: 7
- Primary result: Vikhr is a family of bilingual open-source instruction-tuned large language models for Russian, demonstrating improved generation quality and computational efficiency compared to previous Russian-language models.

## Executive Summary
This work presents Vikhr, a family of bilingual open-source instruction-tuned large language models for Russian. The paper addresses challenges in non-English language generation, such as poor quality and reduced computational performance due to token vocabulary imbalance. The authors develop a pipeline to adapt English-oriented pre-trained models to other languages, focusing on Russian. This pipeline includes vocabulary adaptation, continued pre-training with regularization to prevent catastrophic forgetting, and instruction tuning. Vikhr models demonstrate improved generation quality and computational efficiency compared to previous Russian-language models. The models achieve strong performance on Russian benchmarks, with notable improvements in tasks like Ru-MMLU-pro and ruXNLI. The paper also expands available instruction datasets and corpora for continued pre-training. Model weights, instruction sets, and code are publicly available.

## Method Summary
The authors develop a pipeline to adapt English-oriented pre-trained models to other languages, focusing on Russian. This pipeline includes vocabulary adaptation, continued pre-training with regularization to prevent catastrophic forgetting, and instruction tuning. The vocabulary adaptation involves modifying the token vocabulary to better handle Russian text, addressing the token vocabulary imbalance issue. Continued pre-training is performed on a large corpus of Russian text, with regularization techniques applied to prevent catastrophic forgetting of the original English knowledge. Finally, instruction tuning is conducted using a diverse set of Russian instruction datasets, enabling the models to better understand and respond to Russian language prompts.

## Key Results
- Vikhr models demonstrate improved generation quality and computational efficiency compared to previous Russian-language models.
- The models achieve strong performance on Russian benchmarks, with notable improvements in tasks like Ru-MMLU-pro and ruXNLI.
- The paper expands available instruction datasets and corpora for continued pre-training.

## Why This Works (Mechanism)
The Vikhr models work by addressing the challenges of non-English language generation through a pipeline that adapts English-oriented pre-trained models to other languages, specifically Russian. The vocabulary adaptation step helps mitigate the token vocabulary imbalance issue, which can lead to poor quality and reduced computational performance. By continuing pre-training on a large corpus of Russian text and applying regularization techniques, the models can effectively learn the nuances of the Russian language while preserving the knowledge gained from the original English pre-training. Finally, instruction tuning with diverse Russian instruction datasets enables the models to better understand and respond to Russian language prompts, resulting in improved generation quality and computational efficiency.

## Foundational Learning
1. **Token vocabulary imbalance**: The imbalance between the token vocabularies of English and Russian can lead to poor quality and reduced computational performance in non-English language generation. Understanding this concept is crucial for developing effective strategies to adapt pre-trained models to other languages.
2. **Catastrophic forgetting**: When continuing pre-training on a new language, there is a risk of forgetting the knowledge gained from the original pre-training. Regularization techniques are needed to mitigate this issue and ensure that the model retains its English language capabilities.
3. **Instruction tuning**: Fine-tuning pre-trained models on diverse instruction datasets helps them better understand and respond to language prompts, leading to improved generation quality and task performance.
4. **Bilingual language models**: Developing bilingual language models that can effectively handle both English and Russian text is essential for creating versatile and high-performing language models for non-English languages.
5. **Computational efficiency**: Optimizing language models for computational efficiency is crucial for enabling their practical deployment and use in real-world applications.
6. **Benchmarking**: Evaluating language models on language-specific benchmarks, such as Ru-MMLU-pro and ruXNLI, is necessary to assess their performance and compare them to previous models.

## Architecture Onboarding
Component map: Pre-trained English model -> Vocabulary adaptation -> Continued pre-training with regularization -> Instruction tuning -> Vikhr model

Critical path: The critical path in the Vikhr pipeline is the sequence of steps that lead to the final instruction-tuned model. It starts with the pre-trained English model, followed by vocabulary adaptation to handle Russian text, continued pre-training with regularization to learn the Russian language while preserving English knowledge, and finally instruction tuning to enable the model to understand and respond to Russian language prompts.

Design tradeoffs: The main design tradeoff in the Vikhr pipeline is between preserving the knowledge gained from the original English pre-training and effectively learning the nuances of the Russian language. Regularization techniques are used to strike a balance between these two objectives, but there may still be some loss of English language capabilities. Another tradeoff is between computational efficiency and model performance, as optimizing for efficiency may come at the cost of some reduction in generation quality.

Failure signatures: Some potential failure modes of the Vikhr models include:
- Inability to generate coherent or grammatically correct Russian text due to insufficient adaptation to the Russian language
- Loss of English language capabilities due to inadequate regularization during continued pre-training
- Poor performance on Russian language tasks due to insufficient instruction tuning or lack of diverse instruction datasets
- Computational inefficiencies due to the added complexity of handling both English and Russian text

First experiments:
1. Evaluate the Vikhr models on a diverse set of Russian language tasks to assess their generation quality and task performance.
2. Compare the computational efficiency of the Vikhr models to previous Russian-language models on various hardware configurations.
3. Conduct ablation studies to determine the impact of each step in the Vikhr pipeline (vocabulary adaptation, continued pre-training, and instruction tuning) on the final model performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The generalizability of the Vikhr models beyond the Russian language and the specific benchmark tasks evaluated remains unclear.
- The computational efficiency improvements reported may be context-dependent, particularly given the focus on addressing token vocabulary imbalance for Russian.
- The claim that Vikhr models outperform previous Russian-language models should be validated on additional benchmarks and real-world use cases.

## Confidence
- Improved generation quality and computational efficiency for Russian-specific tasks: High
- Broader claims about multilingual or cross-lingual performance: Medium
- Outperformance of previous Russian-language models: Medium

## Next Checks
1. Evaluate Vikhr models on non-Russian languages to assess multilingual generalization.
2. Test the models on a wider range of real-world tasks and domains beyond the current benchmarks.
3. Conduct a comparative analysis of computational efficiency across different hardware configurations and token vocabularies to validate the reported improvements.