---
ver: rpa2
title: 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models'
arxiv_id: '2407.01725'
source_url: https://arxiv.org/abs/2407.01725
tags:
- discovery
- hypothesis
- data
- dataset
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiscoveryBench is a new benchmark for evaluating large language
  models on autonomous data-driven discovery. It includes 264 tasks from real scientific
  papers and 903 synthetic tasks, each requiring hypothesis search and verification
  using datasets.
---

# DiscoveryBench: Towards Data-Driven Discovery with Large Language Models

## Quick Facts
- arXiv ID: 2407.01725
- Source URL: https://arxiv.org/abs/2407.01725
- Reference count: 40
- Key outcome: DiscoveryBench achieves only 25% performance on autonomous data-driven discovery tasks, highlighting the challenge of fully autonomous scientific discovery with LLMs.

## Executive Summary
DiscoveryBench is a new benchmark for evaluating large language models on autonomous data-driven discovery. It includes 264 tasks from real scientific papers and 903 synthetic tasks, each requiring hypothesis search and verification using datasets. Tasks are defined by a dataset, metadata, and a natural language discovery goal, and often require multi-step statistical workflows and domain-specific reasoning. Evaluation uses a structured hypothesis formalism and GPT-4-based scoring across four facets: context, variables, and relationships. Tested on several LLM-based reasoning agents, the best system achieves only 25% performance, highlighting the challenge of fully autonomous discovery. DiscoveryBench thus provides a rigorous framework and resource for advancing LLM-driven scientific discovery.

## Method Summary
DiscoveryBench evaluates LLM-based reasoning agents on data-driven discovery tasks through a structured hypothesis formalism. The benchmark includes 264 real tasks from published scientific papers and 903 synthetic tasks generated using LLM-based semantic trees. Tasks require agents to search for and verify hypotheses using provided datasets and metadata. The evaluation uses GPT-4 to score predicted hypotheses against gold standards across four facets: context, variables, and relationships, producing a Hypothesis Matching Score (HMS). Multiple reasoning frameworks are tested including CodeGen, ReAct, DataVoyager, and Reflexion with oracle feedback.

## Key Results
- Best-performing agent achieves only 25% HMS score across benchmark tasks
- Synthetic tasks enable controlled difficulty scaling through semantic tree height
- Reflexion with oracle feedback significantly improves performance over base CodeGen agent
- Domain knowledge hints improve DataVoyager performance from 9.9% to 17.5% on archaeology tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured hypothesis formalism enables reproducible evaluation by decomposing complex hypotheses into context, variables, and relationships.
- **Mechanism:** The formalism treats a hypothesis as a declarative sentence ψ(c, v, r), which can be evaluated independently across three dimensions. This allows the GPT-4 evaluator to compare predicted and gold hypotheses facet-by-facet using F1 and accuracy scores.
- **Core assumption:** The hypothesis can be expressed as a logical formula over sub-hypotheses and that its three dimensions can be extracted from natural language using LLM prompts.
- **Evidence anchors:**
  - [abstract] "our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes."
  - [section 3] "We formally define a dataset D on which hypothesis search and verification is performed as a collection of tuples {xi}m i=1 that supports multiple hypothesis semantic trees."
- **Break condition:** If the hypothesis structure is too complex or the LLM cannot accurately parse context/variables/relationships, the evaluation becomes unreliable.

### Mechanism 2
- **Claim:** Synthetic task generation scales the benchmark and enables controlled difficulty variation.
- **Mechanism:** LLMs recursively generate semantic trees from domains, then produce bottom-up synthetic datasets and discovery goals. Difficulty is controlled by the height of the semantic tree and the observability of intermediate nodes.
- **Core assumption:** LLMs can faithfully generate realistic scientific workflows and datasets that mimic real discovery processes.
- **Evidence anchors:**
  - [section 4.2] "Our approach leverages the broad pre-trained knowledge of LLMs in four stages: Domain sampling, Semantic tree construction, Data generation, Task generation."
  - [corpus] Weak evidence: Related work on synthetic data generation (e.g., LLM-SRBench) suggests LLMs can generate plausible scientific equations, but no direct validation of workflow realism.
- **Break condition:** If synthetic tasks diverge too far from real discovery workflows, the benchmark loses validity.

### Mechanism 3
- **Claim:** Multi-turn reasoning agents (ReAct, Reflexion) can chain atomic actions to solve complex discovery tasks.
- **Mechanism:** Agents generate code or intermediate reasoning steps iteratively, using environment feedback to refine hypotheses. Reflexion adds self-reflection with oracle feedback to improve over trials.
- **Core assumption:** The LLM can plan, execute, and correct multi-step data analysis workflows without human intervention.
- **Evidence anchors:**
  - [section 5.1] "A discovery agent takes the task description, paths to the task dataset(s) D, metadata about the datasets, and the goal, G, to produce a natural language (NL) hypothesis."
  - [section 5.2] "With oracle feedback, Reflexion (Oracle) significantly improves over CodeGen (base) performance."
- **Break condition:** If the LLM cannot decompose the task into valid atomic actions or misinterprets intermediate results, the chain breaks.

## Foundational Learning

- **Concept:** Hypothesis semantic tree
  - Why needed here: Represents the hierarchical structure of variables and intermediate hypotheses, allowing task difficulty to be inferred from tree depth.
  - Quick check question: If a hypothesis has 3 levels in its semantic tree, what does that imply about the task complexity?

- **Concept:** Data-driven discovery task definition
  - Why needed here: Formalizes the goal as finding a relationship ψ(c, v, r) supported by the dataset, enabling consistent task generation and evaluation.
  - Quick check question: Given a dataset and goal "How does income affect education?", what are the three hypothesis dimensions you need to discover?

- **Concept:** Workflow length as complexity proxy
  - Why needed here: Provides a measurable proxy for task difficulty when semantic tree structure is unknown, guiding both task design and agent evaluation.
  - Quick check question: If two tasks have workflows of length 5 and 10, which is likely harder and why?

## Architecture Onboarding

- **Component map:** Task parser -> LLM-based reasoning agent (CodeGen/ReAct/DataVoyager/Reflexion) -> Dataset loader -> Code executor -> Hypothesis generator -> Structured evaluator -> HMS score
- **Critical path:**
  1. Parse task and load datasets
  2. Agent generates multi-step workflow
  3. Execute workflow and collect intermediate results
  4. Generate natural language hypothesis
  5. Evaluator decomposes and compares gold vs predicted hypothesis across dimensions
  6. Output HMS score
- **Design tradeoffs:**
  - Open vs closed models: Open models (Llama-3) cheaper but lower performance; closed models (GPT-4) higher performance but cost/latency.
  - Single-turn (CodeGen) vs multi-turn (ReAct/Reflexion): Simpler but less adaptive vs more robust but slower.
  - Real vs synthetic tasks: Real tasks more valid but harder to scale; synthetic tasks scalable but may lack realism.
- **Failure signatures:**
  - Low ctxF1 but high varF1/relacc: Agent misidentifies context boundaries.
  - High ctxF1 but low varF1/relacc: Agent understands context but fails to extract correct variables/relationships.
  - Workflow execution errors: Agent generates invalid code or misinterprets dataset schema.
  - Low overall HMS: Multiple failure modes combined.
- **First 3 experiments:**
  1. Run a simple task (workflow length < 5) with CodeGen + GPT-4o; verify HMS > 0.5.
  2. Run the same task with ReAct; compare HMS and execution trace.
  3. Run a synthetic task with difficulty level 1; verify synthetic data generation works and agent can solve it.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs discover novel hypotheses that have not been previously reported in the literature?
- **Basis in paper:** [explicit] The paper states that tasks include both already reported science-worthy hypotheses and novel hypotheses grounded in datasets, and asks if LLMs can solve discovery tasks that are realistic but never reported before.
- **Why unresolved:** The evaluation focuses on comparing predicted hypotheses to gold standard hypotheses from published papers. There's no systematic evaluation of whether models can generate truly novel, publishable hypotheses beyond the gold standards.
- **What evidence would resolve it:** A systematic evaluation comparing model-generated hypotheses against a corpus of published literature to identify truly novel predictions, followed by expert validation of their scientific merit.

### Open Question 2
- **Question:** How does the performance of discovery agents scale with increasing dataset size and complexity?
- **Basis in paper:** [inferred] The paper notes that the current benchmark does not handle massive datasets (e.g., 8.92 petabytes from Cancer Genome Atlas) or multi-modal data, and that the performance drops significantly even for medium-length workflows.
- **Why unresolved:** The current benchmark and evaluation use relatively small, structured datasets. The paper explicitly acknowledges limitations in handling massive or complex multi-modal datasets.
- **What evidence would resolve it:** Systematic experiments scaling dataset size and complexity (number of variables, rows, modalities) while measuring discovery agent performance, including memory and computational requirements.

### Open Question 3
- **Question:** What is the optimal balance between providing domain knowledge hints and allowing models to discover relationships autonomously?
- **Basis in paper:** [explicit] The paper shows that adding targeted domain knowledge for archaeology tasks improved DataVoyager's performance from 9.9% to 17.5%, but doesn't explore the optimal amount or type of domain knowledge.
- **Why unresolved:** The experiments only test one specific case of adding domain knowledge. The paper doesn't explore different levels, types, or timing of domain knowledge injection.
- **What evidence would resolve it:** Ablation studies varying the amount, specificity, and timing of domain knowledge provided, measuring the trade-off between performance improvement and autonomous discovery capability.

## Limitations

- Benchmark lacks direct validation that synthetic tasks accurately represent real scientific discovery workflows
- Evaluation relies heavily on GPT-4's ability to parse and score hypotheses, introducing potential evaluator bias
- Current benchmark does not handle massive datasets or multi-modal data, limiting real-world applicability

## Confidence

- **High confidence:** The benchmark construction methodology (264 real tasks from published papers, 903 synthetic tasks, structured hypothesis formalism) is well-documented and reproducible.
- **Medium confidence:** The HMS evaluation framework using GPT-4-based scoring across four facets is theoretically sound but may have evaluator bias.
- **Medium confidence:** The claim that autonomous discovery remains challenging (best agent at 25% performance) is supported by results but limited by the small number of tested agents and domains.

## Next Checks

1. **Synthetic Task Realism:** Manually review a sample of synthetic tasks to verify they match the complexity and scientific reasoning patterns of real discovery workflows.
2. **Evaluator Bias Test:** Compare GPT-4 hypothesis scores against human expert ratings on a subset of tasks to quantify evaluator bias and reliability.
3. **Cross-Domain Generalization:** Test the best-performing agent on held-out domains not represented in the training data to assess generalization beyond the 6 benchmark domains.