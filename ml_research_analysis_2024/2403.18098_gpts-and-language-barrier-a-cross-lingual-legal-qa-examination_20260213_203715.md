---
ver: rpa2
title: 'GPTs and Language Barrier: A Cross-Lingual Legal QA Examination'
arxiv_id: '2403.18098'
source_url: https://arxiv.org/abs/2403.18098
tags:
- legal
- cross-lingual
- gpt-4
- japanese
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-3.5 and GPT-4 for cross-lingual legal
  Question-Answering using the COLIEE Task 4 dataset. The dataset involves determining
  whether a legal statement is valid given contextual articles, using both English
  and Japanese data.
---

# GPTs and Language Barrier: A Cross-Lingual Legal QA Examination

## Quick Facts
- arXiv ID: 2403.18098
- Source URL: https://arxiv.org/abs/2403.18098
- Authors: Ha-Thanh Nguyen; Hiroaki Yamada; Ken Satoh
- Reference count: 20
- Primary result: GPT-4 significantly outperforms GPT-3.5 in cross-lingual legal QA, with monolingual settings yielding higher accuracy than cross-lingual ones

## Executive Summary
This paper evaluates GPT-3.5 and GPT-4 for cross-lingual legal Question-Answering using the COLIEE Task 4 dataset. The dataset involves determining whether a legal statement is valid given contextual articles, using both English and Japanese data. Four prompt settings were tested: monolingual (English-English, Japanese-Japanese) and cross-lingual (English-Japanese, Japanese-English). Results show GPT-4 significantly outperforms GPT-3.5 in both settings. Monolingual settings yield higher accuracy than cross-lingual ones, with Japanese monolingual performance best, likely due to original Japanese data. The findings highlight challenges in cross-lingual legal QA and emphasize the need for high-quality translations and deeper linguistic understanding.

## Method Summary
The study evaluates GPT-3.5 and GPT-4 models on the COLIEE Task 4 dataset, which contains legal QA pairs in English and Japanese. The task requires determining whether legal statements are valid based on contextual articles. Four experimental conditions were tested: EN-EN (monolingual), JA-JA (monolingual), EN-JA (cross-lingual), and JA-EN (cross-lingual). Prompts were formatted with context and questions in the specified language combinations. Model performance was measured using accuracy across multiple dataset years with varying question counts.

## Key Results
- GPT-4 significantly outperforms GPT-3.5 across all experimental conditions
- Monolingual settings (EN-EN and JA-JA) yield higher accuracy than cross-lingual settings
- Japanese monolingual performance exceeds English monolingual performance
- Cross-lingual settings show the largest performance gap compared to monolingual conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 across all experimental conditions due to superior architectural capacity and training
- Mechanism: GPT-4's larger model size and improved training allow it to capture more nuanced legal reasoning patterns and handle cross-lingual mappings more effectively
- Core assumption: Model architecture directly correlates with task performance in complex legal reasoning tasks
- Evidence anchors:
  - [abstract]: "Results show GPT-4 significantly outperforms GPT-3.5 in both settings."
  - [section]: "It is evident that the GPT-4 model consistently outperforms the GPT-3.5 model across all independent yearly instances in both monolingual and cross-lingual settings."
  - [corpus]: Weak evidence - related papers focus on retrieval and entailment but don't directly compare GPT-3.5 vs GPT-4 performance in cross-lingual settings
- Break condition: If GPT-4's architecture doesn't provide meaningful advantage for legal reasoning tasks, or if task complexity exceeds model capacity

### Mechanism 2
- Claim: Monolingual settings yield higher accuracy than cross-lingual settings due to reduced linguistic complexity
- Mechanism: Cross-lingual tasks require models to handle translation and mapping between languages, adding complexity that degrades performance compared to within-language tasks
- Core assumption: Translation and cross-lingual mapping introduce additional error sources beyond the core legal reasoning task
- Evidence anchors:
  - [abstract]: "Monolingual settings yield higher accuracy than cross-lingual ones."
  - [section]: "monolingual settings generally yield higher accuracy scores than cross-lingual settings for both models. This can be attributed to the fact that monolingual settings tend to be more straightforward, not requiring translation within the same language."
  - [corpus]: Weak evidence - related papers discuss retrieval but don't specifically analyze monolingual vs cross-lingual performance differences
- Break condition: If translation quality improves significantly or if models develop better cross-lingual understanding, the performance gap may narrow

### Mechanism 3
- Claim: Japanese monolingual performance exceeds English monolingual performance due to original data being in Japanese
- Mechanism: Models process original Japanese text more effectively than translated English text, capturing nuances that may be lost in translation
- Core assumption: Original language data contains more accurate legal nuances than professionally translated versions
- Evidence anchors:
  - [abstract]: "Japanese monolingual performance best, likely due to original Japanese data."
  - [section]: "The observation that Japanese monolingual performance is better than English monolingual performance can also be explained by the nature of the original data being in Japanese."
  - [corpus]: Weak evidence - related papers don't discuss language of origin effects on model performance
- Break condition: If translation quality improves to match original language nuance, or if models develop better translation understanding capabilities

## Foundational Learning

- Concept: Legal entailment task
  - Why needed here: Understanding that the task requires determining if statements can be inferred from legal articles is fundamental to interpreting model performance
  - Quick check question: What is the difference between a yes/no question and an entailment task in legal contexts?

- Concept: Cross-lingual processing
  - Why needed here: The paper evaluates performance across language boundaries, requiring understanding of translation and language mapping challenges
  - Quick check question: Why would translating legal text between languages potentially change the meaning?

- Concept: Prompt engineering
  - Why needed here: The study uses specific prompt formats for different language combinations, affecting model responses
  - Quick check question: How might prompt format affect model performance in legal reasoning tasks?

## Architecture Onboarding

- Component map: Dataset loader -> Prompt formatter -> Model executor (GPT-3.5/GPT-4) -> Result collector -> Performance analyzer
- Critical path: Data preprocessing -> Prompt generation -> Model inference -> Answer extraction -> Accuracy calculation
- Design tradeoffs: Model choice vs. performance (GPT-4 better but potentially more expensive), language choice vs. accuracy (monolingual vs. cross-lingual), prompt format complexity vs. consistency
- Failure signatures: Low accuracy across all conditions (model limitation), consistent drop in cross-lingual settings (translation issues), performance variation across years (data quality issues)
- First 3 experiments:
  1. Replicate monolingual English-only condition to establish baseline performance
  2. Test cross-lingual Englishâ†’Japanese condition to measure translation impact
  3. Vary temperature parameter to assess robustness of binary yes/no outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements in GPT-4 contribute to its superior performance over GPT-3.5 in both monolingual and cross-lingual legal QA tasks?
- Basis in paper: [explicit] The paper states "This superior performance could be attributed to the advancements in the model architecture and higher capacity, allowing GPT-4 to capture the complexities of the languages and tasks more effectively."
- Why unresolved: The paper mentions architectural improvements but does not specify which exact features or design changes in GPT-4 lead to better performance
- What evidence would resolve it: Detailed technical analysis comparing the model architectures, training methodologies, and parameter configurations of GPT-3.5 and GPT-4, including ablation studies isolating specific architectural components

### Open Question 2
- Question: What are the specific linguistic challenges that prevent GPT models from achieving human-level performance in cross-lingual legal QA tasks?
- Basis in paper: [explicit] The paper notes "achieving human-level performance in cross-lingual settings remains a challenge and may demand more significant advancements in model design, access to high-quality datasets, and a deeper understanding of the complexities involved in cross-lingual tasks."
- Why unresolved: The paper identifies the challenge but does not specify the exact linguistic barriers or complexities that models struggle with
- What evidence would resolve it: Comparative error analysis between model outputs and human expert responses, identifying specific linguistic phenomena (idiomatic expressions, legal terminology nuances, cultural context) where models consistently fail

### Open Question 3
- Question: How can the quality of translated legal documents be systematically improved to enhance cross-lingual legal QA performance?
- Basis in paper: [explicit] The paper mentions "the importance of high-quality translated material" and notes that "The derived English translations primarily serve as reference material and might not capture the fine-grained nuances as effectively as the original Japanese text."
- Why unresolved: The paper identifies translation quality as important but does not propose methods to improve translation quality for legal texts
- What evidence would resolve it: Empirical studies comparing different translation methodologies (human vs. machine translation, domain-specific translation models) and their impact on legal QA performance, along with guidelines for optimal translation practices in legal contexts

## Limitations

- Translation quality between Japanese and English legal texts remains unvalidated
- The study doesn't explore model size variations within GPT-4 or investigate how temperature parameters affect binary classification
- Analysis doesn't examine failure cases in detail to understand whether errors stem from cross-lingual challenges, legal reasoning difficulties, or both

## Confidence

**High confidence**: GPT-4 consistently outperforms GPT-3.5 across all conditions. This finding is directly supported by the reported accuracy metrics and shows consistent patterns across multiple dataset years.

**Medium confidence**: Monolingual settings yield higher accuracy than cross-lingual ones. While the trend is clear, the confidence intervals aren't reported, and the magnitude of difference varies across dataset years.

**Low confidence**: Japanese monolingual performance exceeds English due to original data being in Japanese. This is presented as a likely explanation but isn't empirically validated.

## Next Checks

1. **Translation quality assessment**: Conduct a controlled experiment using back-translation to evaluate how much translation quality impacts cross-lingual performance. Compare model accuracy on original Japanese texts versus machine-translated versions of the same content.

2. **Error analysis by failure mode**: Perform detailed analysis of incorrect predictions to categorize error types (translation issues, legal reasoning errors, both). This would help determine whether cross-lingual performance gaps are primarily linguistic or conceptual.

3. **Temperature parameter sensitivity**: Test how varying temperature parameters (0.0, 0.5, 1.0, 1.5) affects binary classification accuracy in the legal entailment task, particularly focusing on whether higher temperatures increase inconsistency in yes/no responses.