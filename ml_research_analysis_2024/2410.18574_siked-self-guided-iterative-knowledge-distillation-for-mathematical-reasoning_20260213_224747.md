---
ver: rpa2
title: 'SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning'
arxiv_id: '2410.18574'
source_url: https://arxiv.org/abs/2410.18574
tags:
- smaller
- reasoning
- data
- training
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of effectively distilling multi-step
  reasoning capabilities from large language models (LLMs) to smaller models, specifically
  focusing on mathematical reasoning. Smaller models tend to be biased towards a single
  reasoning strategy when distilled from LLMs, limiting their ability to solve diverse
  problems.
---

# SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning

## Quick Facts
- arXiv ID: 2410.18574
- Source URL: https://arxiv.org/abs/2410.18574
- Reference count: 40
- Smaller models distilled with SIKeD achieve up to +5 points accuracy improvement on mathematical reasoning tasks

## Executive Summary
SIKeD addresses the challenge of distilling multi-step mathematical reasoning capabilities from large language models (LLMs) to smaller models. Traditional distillation methods often bias smaller models toward a single reasoning strategy, limiting their problem-solving diversity. SIKeD introduces a self-guided iterative approach where the smaller model generates multiple solutions per problem, filters correct ones, and combines them with LLM data to refine its strategy selection. This alignment between training distribution and the model's own output distribution enables better strategy choice while maintaining solution accuracy across diverse mathematical problems.

## Method Summary
SIKeD starts with LLM-generated reasoning data (using Chain-of-Thought, Least-to-Most, and Program-of-Thought strategies) filtered by answer correctness. The smaller model is initially trained on this data, then iteratively generates K=10 samples per question-strategy pair, filters for correct answers, and mixes these with LLM data. Two mixing variants exist: All (includes all LLM data) and Adaptive (only includes LLM data for questions where self-generation failed). The process repeats until accuracy plateaus, with the smaller model learning to select appropriate strategies while maintaining solution accuracy.

## Key Results
- SIKeD achieves up to +5 points accuracy improvement over traditional distillation methods across model sizes from 0.5B to 7B parameters
- Outperforms single-strategy distillation and combined-strategy approaches on GSM8K, SVAMP, ASDiv, and MultiArith datasets
- The Adaptive variant provides computational efficiency gains by reducing LLM data generation while maintaining performance
- Improvements are consistent across different model sizes, demonstrating scalability of the approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SIKeD aligns the smaller model's output distribution with its own learned strategy distribution rather than forcing it to match the LLM's full distribution.
- **Mechanism:** By iteratively generating and filtering correct self-generated outputs, then mixing them with LLM data, the training distribution shifts from LLM-dominated toward the smaller model's own distribution. This reduces the KL divergence between training and output distributions.
- **Core assumption:** The smaller model's own correct generations contain useful strategy signals that can improve its strategy selection.
- **Evidence anchors:**
  - [abstract]: "unlike traditional distillation methods, SIKeD allows the smaller model to learn which strategy is suitable for a given task while continuously learning to solve a task using different strategies"
  - [section]: "We add the generated rationales to the dataset ùíüLLM to create an initial training dataset... We discard all samples that do not match, i.e., we keep samples where ÀÜùëéùëñ = ùëéùëñ"
  - [corpus]: Weak - corpus neighbors discuss related distillation approaches but don't directly address iterative self-guided strategy alignment
- **Break condition:** If the smaller model cannot generate any correct outputs for a significant portion of data, the self-generated component becomes too small to influence training, reverting to pure LLM distillation.

### Mechanism 2
- **Claim:** Multiple generations per question increase the probability of capturing correct reasoning paths, especially for smaller models.
- **Mechanism:** Generating ùêæ samples per question-strategy pair and filtering for correctness increases the likelihood of including diverse correct reasoning strategies in the training data.
- **Core assumption:** Smaller models benefit from generating multiple candidates and selecting the best ones, as their single-generation accuracy is lower than LLMs.
- **Evidence anchors:**
  - [section]: "We first generate ùêæ rationales using the current smaller model... Generate multiple samples ùêæ as the likelihood of a correct answer being present in one of the rationales increases significantly with additional generations for smaller models [17, 36]"
  - [section]: "We set the number of generated samples or ùêæ to 10"
  - [corpus]: Weak - corpus mentions related self-training but doesn't specifically address multiple-generation strategies
- **Break condition:** If ùêæ is too large, computational cost becomes prohibitive without proportional accuracy gains.

### Mechanism 3
- **Claim:** The adaptive mixing strategy improves computational efficiency by only including LLM data when the smaller model fails to generate correct answers.
- **Mechanism:** For questions where the smaller model generates at least one correct rationale, only self-generated data is used. LLM data is included only for questions where all self-generated attempts fail.
- **Core assumption:** The smaller model's correct self-generated data is more valuable for its own learning than redundant LLM data for those specific questions.
- **Evidence anchors:**
  - [section]: "we study two variations: All when all LLM data is used in ùíümix, and Adaptive when only queries that have no correct generations in ùíüself are taken from ùíüLLM"
  - [section]: "Adaptive uses less generated data from the LLM, resulting in more computationally efficient training"
  - [corpus]: Weak - corpus doesn't discuss adaptive mixing strategies specifically
- **Break condition:** If the smaller model's performance is too poor, Adaptive may exclude too much LLM data, harming learning.

## Foundational Learning

- **Concept: Kullback-Leibler (KL) divergence**
  - Why needed here: SIKeD explicitly minimizes KL divergence between training data distribution and the smaller model's output distribution to improve alignment
  - Quick check question: What happens to KL divergence when the training distribution becomes identical to the model's output distribution?

- **Concept: On-policy vs Off-policy learning**
  - Why needed here: SIKeD uses on-policy training (updating from the most recent model checkpoint) while the paper compares it against off-policy approaches
  - Quick check question: How does on-policy training differ from training from a fixed pre-trained checkpoint?

- **Concept: Knowledge distillation fundamentals**
  - Why needed here: Understanding the basic teacher-student setup is essential to grasp how SIKeD extends traditional distillation
  - Quick check question: In standard knowledge distillation, what distribution does the student model attempt to match?

## Architecture Onboarding

- **Component map:** LLM generator -> Initial smaller model training -> Self-generation -> Filtering -> Mixing -> Retraining -> Repeat until convergence

- **Critical path:** LLM data generation ‚Üí Initial smaller model training ‚Üí Self-generation ‚Üí Filtering ‚Üí Mixing ‚Üí Retraining ‚Üí Repeat until convergence

- **Design tradeoffs:**
  - Computation vs performance: More iterations and larger ùêæ improve performance but increase cost
  - Data mixing ratio: More self-data improves alignment but may lose valuable LLM signals if ùõº is too small
  - Strategy coverage: Including more strategies increases complexity but improves generalization

- **Failure signatures:**
  - Strategy collapse: Model becomes biased toward one strategy despite multiple iterations
  - No improvement: Accuracy plateaus early, suggesting limited capacity for strategy learning
  - High computational cost: Excessive iterations or ùêæ without proportional gains

- **First 3 experiments:**
  1. Verify that single-strategy distillation leaves the model biased (baseline comparison)
  2. Test whether data mixing improves strategy diversity compared to pure LLM distillation
  3. Compare All vs Adaptive mixing strategies on a small dataset to measure computational efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SIKeD scale with larger LLMs as teachers beyond Llama3 70B?
- Basis in paper: [inferred] The paper uses Llama3 70B as the LLM teacher and demonstrates improvements over traditional distillation. It does not explore scaling to even larger models.
- Why unresolved: The paper focuses on demonstrating SIKeD's effectiveness with a single, large LLM (Llama3 70B) and does not investigate how performance scales with increasingly larger teacher models.
- What evidence would resolve it: Experiments comparing SIKeD's performance when using different sized LLMs (e.g., 70B, 175B, 540B parameters) as teachers on the same mathematical reasoning tasks.

### Open Question 2
- Question: What is the impact of varying the number of generated samples (K) per question and strategy on SIKeD's performance?
- Basis in paper: [explicit] The paper sets K=10 for generating rationales but does not explore the sensitivity of SIKeD's performance to different values of K.
- Why unresolved: The paper uses a fixed K=10 without investigating how changing this hyperparameter affects the quality and quantity of self-generated data, which in turn impacts the distribution alignment and final performance.
- What evidence would resolve it: Experiments varying K (e.g., K=5, 10, 20, 50) and measuring the resulting accuracy and computational cost trade-offs across different models and datasets.

### Open Question 3
- Question: How does SIKeD's performance compare to other advanced distillation techniques like GKD (Generalized Knowledge Distillation) or self-consistency methods on mathematical reasoning tasks?
- Basis in paper: [inferred] The paper mentions GKD as related work but does not directly compare SIKeD's performance against it or other advanced distillation methods on mathematical reasoning datasets.
- Why unresolved: While the paper demonstrates SIKeD's superiority over traditional single-strategy and combined-strategy distillation, it does not benchmark against more recent and sophisticated distillation techniques that could potentially offer similar or better performance.
- What evidence would resolve it: Direct performance comparisons of SIKeD against GKD, self-consistency methods, and other advanced distillation techniques on the same mathematical reasoning datasets using the same model sizes.

## Limitations

- The paper doesn't thoroughly address what happens when the smaller model consistently fails to generate correct self-data, potentially creating a feedback loop that limits learning
- While demonstrating strategy diversity improvements, the paper doesn't deeply investigate whether gains come from better strategy selection or simply having more diverse training data
- Computational efficiency claims, particularly for the Adaptive variant, lack rigorous validation with detailed timing or cost comparisons

## Confidence

**High Confidence:** The core claim that SIKeD improves mathematical reasoning accuracy over traditional distillation methods is well-supported by the experimental results across multiple datasets and model sizes (0.5B to 7B parameters). The improvements of up to +5 points are statistically significant and consistently observed.

**Medium Confidence:** The mechanism explaining how iterative self-guided distillation aligns the training distribution with the model's output distribution is plausible but not fully validated. While the filtering and mixing procedures are described, there's limited direct evidence showing the distribution alignment effect beyond accuracy improvements.

**Low Confidence:** The claims about computational efficiency gains, particularly for the Adaptive variant, lack rigorous validation. The paper mentions reduced LLM generation but doesn't provide detailed cost-benefit analysis or timing data to substantiate these claims.

## Next Checks

1. **Strategy Distribution Analysis:** Track and visualize the strategy distribution (CoT, L2M, PoT) across iterations for each model size. This would validate whether SIKeD truly enables strategy selection rather than just accumulating diverse examples.

2. **Failure Mode Exploration:** Systematically test SIKeD on progressively smaller or weaker models to identify the point where self-generation quality degrades. This would clarify the practical limits of the approach and when traditional distillation becomes preferable.

3. **Computational Cost Benchmarking:** Implement precise timing measurements for both All and Adaptive variants across all model sizes, including LLM generation time, filtering time, and training time. This would validate the efficiency claims with concrete data.