---
ver: rpa2
title: Efficient Mitigation of Bus Bunching through Setter-Based Curriculum Learning
arxiv_id: '2405.15824'
source_url: https://arxiv.org/abs/2405.15824
tags:
- curriculum
- learning
- action
- environment
- setter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel setter-based curriculum learning approach
  to mitigate bus bunching in public transportation systems. The core idea is to use
  a setter model to automatically generate lessons by adjusting the action space,
  adversary strength, and bunching strength throughout training.
---

# Efficient Mitigation of Bus Bunching through Setter-Based Curriculum Learning

## Quick Facts
- arXiv ID: 2405.15824
- Source URL: https://arxiv.org/abs/2405.15824
- Reference count: 40
- One-line primary result: Setter-based curriculum learning slightly underperforms baseline but reaches maximum rewards faster and is more stable

## Executive Summary
This paper proposes a novel setter-based curriculum learning approach to mitigate bus bunching in public transportation systems. The core idea is to use a setter model to automatically generate lessons by adjusting the action space, adversary strength, and bunching strength throughout training. The setter model takes in previous lessons and rewards, and uses them to compute new lessons that update the environment for the agent. The method is evaluated on a custom bus-system environment and compared to baseline models with no curriculum learning and two other popular curriculum learning methods. Results show that the setter-based curriculum learning slightly underperforms the baseline with no curriculum learning but reaches maximum rewards faster, is more stable, and is easier to use out-of-the-box. Analysis of the generated curriculum reveals that the setter model starts with high exploration and converges to certain values for action space, adversary strength, and bunching strength. Ablation studies indicate that allowing the setter to affect only one variable at a time leads to better performance, suggesting that the current model architecture may be overloaded with all three variables.

## Method Summary
The paper proposes a setter-based curriculum learning approach to mitigate bus bunching in public transportation systems. The setter model takes in previous lessons and rewards, and uses them to compute new lessons that update the environment for the agent. The method is implemented using PPO algorithm with domain randomization to improve generalization. The agent is trained on a custom bus-system environment with 10 stations, 14 buses, and a bus capacity of 60. The performance is compared to baseline models with no curriculum learning and two other popular curriculum learning methods.

## Key Results
- Setter-based curriculum learning slightly underperforms the baseline with no curriculum learning but reaches maximum rewards faster and is more stable.
- Domain randomization improves the agent's ability to generalize to real-world scenarios by exposing it to a variety of training situations.
- Ablation studies indicate that allowing the setter to affect only one variable at a time leads to better performance, suggesting that the current model architecture may be overloaded with all three variables.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The setter model enables curriculum learning by dynamically adjusting the difficulty of the training environment based on past performance.
- Mechanism: The setter model takes in previous lessons (action space, adversary strength, and bunching strength) and rewards, and uses them to compute new lessons that update the environment for the agent. It uses a loss function defined by the previously mentioned three variables, weighted by negative average reward over the last three timesteps.
- Core assumption: The setter model can learn an effective mapping from past performance to future environment parameters that will improve agent learning efficiency.
- Evidence anchors:
  - [abstract] "The setter model takes in previous lessons and rewards, and uses them to compute new lessons that update the environment for the agent."
  - [section] "The setter model loss is defined as follows (where π represents the probability distribution over the state space): Lsetter = −r[t−3:t] · (log πS + log πα + log πβ)"
  - [corpus] Weak evidence; related works focus on automated curriculum generation but not specifically setter-based approaches.
- Break condition: If the setter model cannot effectively learn the mapping from past performance to future environment parameters, it may generate lessons that do not improve agent learning efficiency or even hinder it.

### Mechanism 2
- Claim: Domain randomization improves the agent's ability to generalize to real-world scenarios by exposing it to a variety of training situations.
- Mechanism: Domain randomization introduces variability into the training setting by randomizing parameters such as passenger arrival rates and bus schedules. This helps the policy familiarize itself with a variety of environments and improve its generalization ability.
- Core assumption: Exposing the agent to a wide range of training scenarios will make it more robust to the unpredictability of actual bus timetables and real-world conditions.
- Evidence anchors:
  - [section] "We incorporate Domain Randomization (DR) in our training process. The key rationale for DR is to inject variability into the training setting, thereby enhancing the RL agent's ability to adapt to unexpected situations in real-life scenarios."
  - [section] "We find that without Domain Randomization the model stagnates at a considerably lower reward. This demonstrates that using Domain Randomization vastly improves the performance and this is likely due to the model benefiting from experiencing a larger variety of training situations which are more representative of the variety of situations that can arise during test time."
  - [corpus] Weak evidence; related works mention domain randomization but do not provide specific evidence of its effectiveness in the bus bunching context.
- Break condition: If the range of randomized parameters is too narrow or does not accurately represent real-world conditions, domain randomization may not effectively improve the agent's generalization ability.

### Mechanism 3
- Claim: The ablation study reveals that the setter model is overloaded when trying to optimize all three variables (action space, adversary strength, and bunching strength) simultaneously, leading to slower convergence.
- Mechanism: The ablation study shows that allowing the setter to affect only one variable at a time leads to better performance, while the experiment run with all three variables is the slowest to converge to maximum reward. This suggests that the current model architecture may not have enough capacity to handle all three variables effectively.
- Core assumption: The setter model's architecture is not complex enough to effectively learn the optimal combination of all three variables simultaneously.
- Evidence anchors:
  - [abstract] "Ablation studies indicate that allowing the setter to affect only one variable at a time leads to better performance, suggesting that the current model architecture may be overloaded with all three variables."
  - [section] "In this ablation study we ablate over the lessons ϕ and whether the model has control over the action space (S), perturbation strength (α), and bunching strength (β). We find that experiments where the Setter only affects α, β, or S the performance is better which could signify that having the setter create lessons ϕ based on all 3 of even combinations overloads the model and the model is unable to do it as effectively."
  - [corpus] Weak evidence; related works do not provide specific evidence of the impact of model complexity on curriculum learning performance.
- Break condition: If the setter model's architecture is not improved to handle all three variables simultaneously, the performance may continue to be suboptimal compared to models that focus on optimizing a single variable.

## Foundational Learning

- Concept: Reinforcement Learning (RL) and Proximal Policy Optimization (PPO)
  - Why needed here: The paper uses PPO as the underlying RL algorithm for training the agent to mitigate bus bunching.
  - Quick check question: What is the main difference between PPO and other policy gradient methods like REINFORCE?

- Concept: Curriculum Learning
  - Why needed here: The paper proposes a setter-based curriculum learning approach to improve the efficiency of training the RL agent.
  - Quick check question: How does curriculum learning differ from traditional RL approaches in terms of the learning process?

- Concept: Domain Randomization
  - Why needed here: The paper incorporates domain randomization to improve the agent's ability to generalize to real-world scenarios.
  - Quick check question: What is the main goal of domain randomization in the context of RL, and how does it differ from traditional training approaches?

## Architecture Onboarding

- Component map:
  - Bus Environment -> Domain Randomization -> Setter Model -> PPO Agent -> Curriculum Update Algorithm

- Critical path:
  1. Initialize the bus environment and PPO agent.
  2. Collect trajectories using the current policy and environment.
  3. Update the setter model using past lessons and rewards.
  4. Compute new lessons using the updated setter model.
  5. Update the environment using the new lessons.
  6. Repeat steps 2-5 until convergence or a stopping criterion is met.

- Design tradeoffs:
  - Model complexity vs. performance: A more complex setter model may be able to handle all three variables (action space, adversary strength, and bunching strength) simultaneously, but it may also be more prone to overfitting or require more data to train effectively.
  - Exploration vs. exploitation: The setter model needs to balance exploration of different curriculum strategies with exploitation of the strategies that have shown to be effective in the past.

- Failure signatures:
  - If the setter model generates lessons that do not improve agent learning efficiency or even hinder it, the performance may stagnate or degrade over time.
  - If the domain randomization parameters are not well-tuned, the agent may not generalize effectively to real-world scenarios.
  - If the PPO agent does not converge to an optimal policy, the overall performance of the system may be suboptimal.

- First 3 experiments:
  1. Train the PPO agent without any curriculum learning or domain randomization to establish a baseline performance.
  2. Implement the setter-based curriculum learning approach and compare its performance to the baseline.
  3. Add domain randomization to the training process and evaluate its impact on the agent's ability to generalize to real-world scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the setter model's performance compare to other advanced curriculum learning methods beyond the baselines tested?
- Basis in paper: [inferred] The paper compares the setter model to three baseline methods (no curriculum, budget-based, and stagnancy-based) but doesn't explore other advanced curriculum learning techniques.
- Why unresolved: The study focused on a limited set of baselines, leaving open the question of how the setter model stacks up against other state-of-the-art curriculum learning methods.
- What evidence would resolve it: Testing the setter model against a broader range of curriculum learning methods, such as those based on automatic task generation or meta-learning approaches, would provide a more comprehensive comparison.

### Open Question 2
- Question: What is the optimal architecture for the setter model to handle multiple curriculum parameters without being overloaded?
- Basis in paper: [explicit] The ablation study showed that the setter model performs better when affecting only one variable at a time, suggesting it may be overloaded when handling all three parameters (action space, perturbation strength, and bunching strength).
- Why unresolved: The paper suggests that a more complex model architecture may be required but doesn't explore what that architecture might look like or how it would perform.
- What evidence would resolve it: Experimenting with different setter model architectures, such as using separate sub-models for each parameter or incorporating attention mechanisms, could reveal an optimal design that balances performance and complexity.

### Open Question 3
- Question: How does the setter model's curriculum generation adapt to different bus system environments or real-world scenarios?
- Basis in paper: [inferred] The setter model was tested on a custom bus system environment, but its adaptability to other environments or real-world scenarios is not explored.
- Why unresolved: The study focused on a single environment, leaving open the question of how well the setter model generalizes to different bus systems or other transportation optimization problems.
- What evidence would resolve it: Testing the setter model on various bus system configurations, different transportation problems, or even real-world data would demonstrate its adaptability and potential for broader application.

## Limitations
- The exact architecture and hyperparameters of the setter model MLP neural network are underspecified.
- The baseline curriculum learning methods (budget-based and stagnancy-based) are referenced but not detailed, making direct comparison difficult.
- The custom bus environment implementation details are limited, potentially affecting reproducibility.

## Confidence
- **High Confidence**: The effectiveness of domain randomization in improving generalization and the observed performance gains from the ablation study showing setter overload.
- **Medium Confidence**: The overall claim that setter-based curriculum learning slightly underperforms baseline but reaches maximum rewards faster and is more stable.
- **Low Confidence**: The specific mechanism by which the setter model generates effective lessons and the optimal configuration of the setter model architecture.

## Next Checks
1. **Reproduce the custom bus environment** with 10 stations, 14 buses, and 60 capacity buses, ensuring the domain randomization parameters match the paper's specifications.
2. **Implement and compare all three baseline curriculum learning methods** (no curriculum, budget-based, and stagnancy-based) to validate the claimed performance differences.
3. **Run systematic ablation studies** varying the setter model architecture (number of layers, neurons, activation functions) to identify optimal configurations for handling all three variables simultaneously.