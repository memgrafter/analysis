---
ver: rpa2
title: 'EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating
  Vision Language Models'
arxiv_id: '2403.10378'
source_url: https://arxiv.org/abs/2403.10378
tags:
- answer
- question
- questions
- language
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EXAMS-V is a challenging multilingual multimodal exam benchmark
  with 20,932 multiple-choice questions across 20 subjects and 11 languages. Unlike
  existing benchmarks, it embeds questions with text, images, tables, and equations
  as unified snapshots, requiring advanced perception and reasoning.
---

# EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models

## Quick Facts
- **arXiv ID**: 2403.10378
- **Source URL**: https://arxiv.org/abs/2403.10378
- **Reference count**: 10
- **Primary result**: State-of-the-art VLMs achieve only 42.78% accuracy on EXAMS-V, just 20 percentage points above random baselines

## Executive Summary
EXAMS-V is a challenging multilingual multimodal exam benchmark containing 20,932 multiple-choice questions across 20 subjects and 11 languages. The dataset uniquely presents questions as unified snapshots embedding text, images, tables, and equations, requiring advanced perception and reasoning capabilities from vision language models. Unlike existing benchmarks, EXAMS-V is curated from real school exams across different countries, demanding region-specific knowledge. Evaluation shows that even the best VLMs like GPT-4V and Gemini-V achieve modest performance (42.78% and 31.13% accuracy respectively), highlighting the benchmark's difficulty and its potential as a rigorous evaluation standard for multimodal reasoning.

## Method Summary
The EXAMS-V dataset is constructed by gathering school exam questions from various countries with different education systems, covering 20 subjects across 11 languages. The questions are presented as unified snapshots that integrate text, images, tables, and equations. The evaluation methodology involves zero-shot testing of state-of-the-art VLMs and LLMs augmented with OCR and image captioning capabilities on the test set, measuring accuracy as the primary metric. The dataset is available through a GitHub repository, though specific preprocessing steps and evaluation scripts are not fully detailed in the paper.

## Key Results
- GPT-4V achieves 42.78% accuracy and Gemini-V achieves 31.13% accuracy, only 20 percentage points above random baselines
- Performance varies significantly across languages, with Chinese, Arabic, and English being most difficult
- Models show performance variations across language families, suggesting correlations with resource availability
- Region-specific knowledge requirements create additional challenges not present in general-purpose benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding questions with text, images, tables, and equations as unified snapshots creates a multimodal reasoning challenge that current VLMs struggle to solve.
- Mechanism: By presenting multimodal information as a single image snapshot rather than separated text and image inputs, models must perform optical character recognition (OCR), image interpretation, and cross-modal reasoning simultaneously without pre-processing cues.
- Core assumption: Current VLMs cannot effectively integrate OCR, visual feature extraction, and reasoning in a single unified pipeline.
- Evidence anchors:
  - [abstract] "Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image."
  - [section] "Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems."
  - [corpus] Weak - no direct evidence about unified snapshot approach effectiveness.
- Break condition: If models develop strong end-to-end multimodal reasoning pipelines that can process integrated snapshots without performance degradation.

### Mechanism 2
- Claim: Region-specific knowledge requirements make EXAMS-V challenging for models trained on general web data.
- Mechanism: Questions require knowledge about specific educational systems, curricula, and regional contexts that are not uniformly represented in training data.
- Core assumption: General-purpose training data lacks sufficient coverage of region-specific educational content across 20 subjects and 11 languages.
- Evidence anchors:
  - [abstract] "This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge."
  - [section] "Questions related to Geography and History necessitate specific knowledge about particular regions or countries."
  - [corpus] Weak - no direct evidence about region-specific knowledge challenges.
- Break condition: If models are trained on comprehensive multilingual educational datasets covering diverse regional curricula.

### Mechanism 3
- Claim: Multilingual diversity across 7 language families creates varying difficulty levels that expose model limitations.
- Mechanism: Models show performance variations across languages due to differences in script complexity, resource availability, and linguistic distance from training data.
- Core assumption: Model performance is correlated with language family resource availability and script complexity.
- Evidence anchors:
  - [section] "Overall, the best performing VLM, i.e. GPT-4V, outperforms in four languages, whereas LLMs with OCR and captioning capabilities excel in several languages."
  - [section] "Arabic (ar) and English (en) emerge as the next most challenging languages."
  - [corpus] Weak - no direct evidence about language family performance patterns.
- Break condition: If models achieve consistent performance across all language families regardless of resource availability.

## Foundational Learning

- Concept: Optical Character Recognition (OCR) fundamentals
  - Why needed here: Models must extract text from integrated image snapshots containing questions and answer choices
  - Quick check question: What are the key challenges in OCR for non-Latin scripts like Arabic and Chinese?

- Concept: Multimodal reasoning architectures
  - Why needed here: Models must integrate visual features (tables, graphs, figures) with extracted text to answer questions
  - Quick check question: How do vision-language models typically fuse visual and textual representations?

- Concept: Educational curriculum knowledge structures
  - Why needed here: Questions require understanding of subject-specific knowledge across natural sciences, social sciences, and applied studies
  - Quick check question: What are the key differences between high school curricula across different countries and regions?

## Architecture Onboarding

- Component map: Image preprocessing -> OCR pipeline -> Visual feature extraction -> Cross-modal fusion -> Reasoning module -> Output formatting
- Critical path: OCR → Visual feature extraction → Cross-modal fusion → Reasoning → Output
- Design tradeoffs: Accuracy vs. speed in OCR processing, model size vs. multilingual coverage
- Failure signatures: Poor OCR accuracy leading to incorrect answer extraction, visual feature misinterpretation
- First 3 experiments:
  1. Evaluate OCR accuracy on multilingual integrated snapshots with varying script complexities
  2. Test visual feature extraction performance on tables, graphs, and scientific symbols across languages
  3. Measure cross-modal fusion effectiveness by comparing separated vs. integrated input approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the EXAMS-V dataset be extended to include more fine-grained multimodal analysis beyond the current four categories (scientific symbols, figures, graphs, and tabular data)?
- Basis in paper: [explicit] The paper mentions that the current multimodal analysis is limited to four broad categories and suggests extending it to finer-grained categories like mathematical notions, chemical symbols, maps, paintings, diagrams, etc.
- Why unresolved: The paper acknowledges the need for more detailed analysis but does not provide a concrete method or implementation for achieving this extension.
- What evidence would resolve it: A detailed methodology for categorizing and annotating additional multimodal features in the dataset, along with examples of how these new categories would be implemented and evaluated.

### Open Question 2
- Question: What are the specific challenges and potential solutions for improving VLM performance on low-resource languages like Croatian, Serbian, and Arabic in the EXAMS-V dataset?
- Basis in paper: [inferred] The paper highlights the lower performance of VLMs on low-resource languages and suggests that this is due to the complexity of the language and the visual features in the questions.
- Why unresolved: While the paper identifies the problem, it does not propose specific strategies or solutions to enhance VLM performance on these languages.
- What evidence would resolve it: A comprehensive study comparing different approaches (e.g., multilingual pre-training, data augmentation, specialized OCR techniques) to improve VLM performance on low-resource languages, with quantifiable results.

### Open Question 3
- Question: How does the performance of VLMs on EXAMS-V compare to human performance on the same questions, and what insights can be gained from this comparison?
- Basis in paper: [explicit] The paper states that EXAMS-V aims to set a new standard in evaluating VLMs by mirroring the complexity and diversity of real-world information processing, suggesting a comparison with human performance.
- Why unresolved: The paper does not provide any direct comparison of VLM performance with human performance on the EXAMS-V dataset.
- What evidence would resolve it: A study where human subjects of varying expertise levels answer the same questions as the VLMs, with a detailed analysis of the differences in performance and reasoning strategies.

## Limitations

- The paper lacks detailed information about the data preprocessing pipeline and evaluation methodology, making faithful reproduction challenging
- Claims about region-specific knowledge requirements as a key challenge are not empirically validated with direct evidence
- The unified snapshot approach effectiveness is claimed but not compared to separated text-image input approaches in controlled experiments

## Confidence

- **EXAMS-V's difficulty level claim**: High confidence - Substantial performance gap between state-of-the-art VLMs and random baselines provides strong evidence
- **Multilingual performance variation claim**: Medium confidence - Performance differences reported but lack detailed analysis of underlying causes
- **Unified snapshot approach effectiveness**: Low confidence - Claimed as challenging but lacks empirical comparison with alternative input approaches

## Next Checks

1. **OCR accuracy validation**: Conduct systematic experiments measuring OCR performance on the integrated snapshots across all 11 languages, particularly focusing on non-Latin scripts where text extraction accuracy is most critical.

2. **Cross-modal reasoning comparison**: Design controlled experiments comparing model performance on EXAMS-V when questions are presented as separated text-image pairs versus integrated snapshots, to validate the claimed difficulty of the unified approach.

3. **Region-specific knowledge assessment**: Analyze model performance across questions requiring different types of region-specific knowledge (geography, history, local curricula) to empirically validate whether these questions are indeed more challenging than general knowledge questions.