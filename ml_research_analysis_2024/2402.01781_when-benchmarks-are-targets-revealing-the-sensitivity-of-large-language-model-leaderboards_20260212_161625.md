---
ver: rpa2
title: 'When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model
  Leaderboards'
arxiv_id: '2402.01781'
source_url: https://arxiv.org/abs/2402.01781
tags:
- answer
- choice
- rstd
- prompt
- choices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Minor changes to multiple-choice question prompts can cause significant
  shifts in LLM leaderboard rankings, with models moving up to 8 positions. The sensitivity
  arises from bias to answer choice symbols, order, and scoring methods, as well as
  in-context knowledge injection.
---

# When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards

## Quick Facts
- arXiv ID: 2402.01781
- Source URL: https://arxiv.org/abs/2402.01781
- Authors: Norah Alzahrani; Hisham Abdullah Alyahya; Yazeed Alnumay; Sultan Alrashed; Shaykhah Alsubaie; Yusef Almushaykeh; Faisal Mirza; Nouf Alotaibi; Nora Altwairesh; Areeb Alowisheq; M Saiful Bari; Haidar Khan
- Reference count: 12
- Minor changes to multiple-choice question prompts can cause significant shifts in LLM leaderboard rankings, with models moving up to 8 positions.

## Executive Summary
This study reveals that Large Language Models (LLMs) exhibit substantial sensitivity to minor perturbations in multiple-choice question (MCQ) benchmarks, leading to significant shifts in leaderboard rankings. Through systematic experimentation with 11 models on MMLU and ARC-C benchmarks, the authors demonstrate that trivial changes to answer choice symbols, order, and scoring methods can alter performance by several percentage points and cause models to move up to 8 positions in rankings. The findings highlight fundamental vulnerabilities in current LLM evaluation practices and suggest that leaderboard rankings may be more fragile than commonly assumed.

## Method Summary
The study systematically evaluates 11 LLMs on MMLU and ARC-C benchmarks using controlled perturbations across three categories: answer choice format/order, prompt/scoring modifications, and in-context knowledge manipulation. The methodology employs the LM Evaluation Harness library to implement baseline evaluations using symbol scoring, then applies perturbations such as shuffling answer choices, replacing choice symbols, changing scoring methods (symbol, hybrid, cloze), and altering prompt instructions. Performance is measured through accuracy, recall standard deviation (RStd) for answer choice bias, and Kendall's τ to quantify ranking shifts between different evaluation conditions.

## Key Results
- LLMs show high sensitivity to answer choice order, with positional bias affecting all models tested in both zero-shot and few-shot settings
- Changing answer choice symbols (e.g., A,B,C,D to alternative characters) causes significant performance variations
- Different scoring methods (symbol, hybrid, cloze) produce varying amounts of bias, with hybrid scoring reducing but not eliminating sensitivity effects
- Minor prompt modifications can cause models to shift up to 8 positions in leaderboard rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance on MCQ benchmarks is highly sensitive to the order of answer choices.
- Mechanism: Models develop positional bias during training, favoring certain positions (A, B, C, D) regardless of content.
- Core assumption: Training data contains positional patterns that LLMs learn and apply during inference.
- Evidence anchors:
  - [abstract] "minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions"
  - [section 5.2] "LLMs exhibit high sensitivity to variations in prompt formatting... selection bias is apparent in all LLMs we test both in 0 and 5-shot setups"
  - [corpus] Weak - no direct corpus evidence for positional bias; found papers on contamination but not positional sensitivity

### Mechanism 2
- Claim: LLM performance on MCQ benchmarks is highly sensitive to the symbols used for answer choices.
- Mechanism: Models associate specific tokens/symbols with correct answers due to token-level bias learned during pretraining.
- Core assumption: Training corpus contains token-specific patterns that LLMs learn to exploit.
- Evidence anchors:
  - [abstract] "changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions"
  - [section 5.2] "The symbols used for the answer choices (e.g. A, B, C, D) also play a role in model bias"
  - [corpus] Weak - no direct corpus evidence for token bias; found papers on contamination but not symbol sensitivity

### Mechanism 3
- Claim: LLM performance on MCQ benchmarks is highly sensitive to the scoring method used for answer selection.
- Mechanism: Different scoring methods (symbol, hybrid, cloze) interact differently with model probability distributions, leading to ranking instability.
- Core assumption: Model probability distributions are not consistent across different scoring methods.
- Evidence anchors:
  - [abstract] "minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions"
  - [section 5.3] "LLMs exhibit varying amounts of bias under the choice of scoring method for MCQs"
  - [corpus] Weak - no direct corpus evidence for scoring method sensitivity; found papers on contamination but not scoring method effects

## Foundational Learning

- Concept: Statistical significance testing
  - Why needed here: To determine if ranking changes are meaningful or due to random variation
  - Quick check question: What p-value threshold would you use to determine if a ranking change is statistically significant?

- Concept: Correlation metrics (Kendall's tau)
  - Why needed here: To quantify agreement between different ranking methods
  - Quick check question: If two rankings have Kendall's tau of 0.9, what percentage of pairs agree?

- Concept: Experimental design with control groups
  - Why needed here: To isolate the effect of individual perturbations on model performance
  - Quick check question: How would you design an experiment to test if symbol bias is independent of positional bias?

## Architecture Onboarding

- Component map:
  - Prompt generation system -> Model inference engine -> Answer selection algorithm -> Scoring method implementation -> Benchmark perturbation engine

- Critical path:
  1. Generate prompt with specified formatting
  2. Send to model for inference
  3. Receive probability distribution over answer choices
  4. Apply scoring method to select answer
  5. Calculate accuracy and bias metrics
  6. Compare rankings across perturbations

- Design tradeoffs:
  - Simplicity vs. robustness: Simple MCQ format is easy to implement but fragile
  - Speed vs. accuracy: More sophisticated scoring methods may be slower but more robust
  - Generalizability vs. specificity: Broad benchmarks may miss task-specific nuances

- Failure signatures:
  - Large ranking changes from minor prompt modifications
  - High standard deviation in model performance across similar questions
  - Inconsistent behavior across different scoring methods

- First 3 experiments:
  1. Test positional bias by fixing correct answer to different positions
  2. Test symbol bias by replacing A/B/C/D with alternative symbols
  3. Test scoring method bias by comparing symbol, hybrid, and cloze scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the observed bias in LLMs stem from training data contamination versus inherent model architecture limitations?
- Basis in paper: [explicit] The paper notes that training data contamination is difficult to verify and that both factors may contribute to the observed sensitivity.
- Why unresolved: The paper cannot access the pretraining datasets of the tested LLMs, making it impossible to definitively determine the source of the bias.
- What evidence would resolve it: Access to the pretraining datasets combined with systematic experiments isolating contamination effects from architectural limitations would be needed to conclusively determine the source.

### Open Question 2
- Question: How can we design more robust MCQ benchmarks that minimize the impact of minor perturbations on model rankings?
- Basis in paper: [explicit] The paper identifies that current MCQ benchmarks are highly sensitive to perturbations and recommends using hybrid scoring methods, but acknowledges this is not a complete solution.
- Why unresolved: While the paper suggests some mitigation strategies, it does not provide a comprehensive solution to the problem of benchmark sensitivity.
- What evidence would resolve it: Development and validation of new benchmark formats or evaluation frameworks that demonstrate significantly reduced sensitivity to the perturbations studied in this paper.

### Open Question 3
- Question: What are the specific mechanisms by which LLMs use in-context information, and how can we better control this behavior during evaluation?
- Basis in paper: [explicit] The paper shows that LLMs readily use in-context information, even when it's incorrect, but cannot conclusively determine whether this is due to pattern following or genuine knowledge acquisition.
- Why unresolved: The experiments show that LLMs are influenced by in-context information but cannot distinguish between different mechanisms of influence or quantify their relative importance.
- What evidence would resolve it: Detailed mechanistic interpretability studies combined with controlled experiments varying the type and presentation of in-context information would be needed to understand the underlying mechanisms.

## Limitations

- Limited scope to multiple-choice question formats, primarily MMLU and ARC-C benchmarks
- Lack of direct corpus-level evidence for proposed mechanisms (positional bias, symbol bias, scoring method bias)
- Model selection bias - results may not generalize to different model families, sizes, or training approaches

## Confidence

**High Confidence**: The experimental methodology for demonstrating ranking sensitivity to prompt perturbations is sound. The systematic approach of varying answer choice order, symbols, and scoring methods while measuring Kendall's τ provides robust evidence that minor changes can cause significant ranking shifts (up to 8 positions).

**Medium Confidence**: The interpretation that this sensitivity reveals fundamental issues with current benchmarking practices. While the experimental results are clear, the leap to broader implications about leaderboard fragility requires additional validation across more diverse benchmarks and model types.

**Low Confidence**: The specific mechanisms proposed (positional bias, symbol bias, scoring method bias) are plausible but not directly evidenced. The study shows correlation between perturbations and performance changes but does not establish causation or identify the underlying reasons for these effects.

## Next Checks

1. **Corpus Analysis**: Conduct a systematic analysis of common pretraining corpora to identify whether positional patterns or symbol distributions exist that could explain the observed biases. This would involve analyzing question-answer pairs in sources like CommonCrawl, C4, or other large web datasets for evidence of answer choice ordering patterns.

2. **Cross-Benchmark Validation**: Replicate the experiments on additional benchmark types (e.g., open-ended question answering, code generation tasks) to determine if the sensitivity extends beyond multiple-choice formats. This would help assess whether the fragility is specific to MCQ benchmarks or represents a more general evaluation challenge.

3. **Ablation Studies on Model Architectures**: Test whether different model architectures (RNNs, Transformers with different attention patterns, or models trained with different objectives) exhibit similar sensitivity to prompt perturbations. This would help determine if the observed effects are inherent to current LLM architectures or specific to particular training approaches.