---
ver: rpa2
title: A cost minimization approach to fix the vocabulary size in a tokenizer for
  an End-to-End ASR system
arxiv_id: '2406.02563'
source_url: https://arxiv.org/abs/2406.02563
tags:
- tokens
- number
- training
- function
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of determining an optimal vocabulary
  size for subword tokenization in end-to-end ASR systems. The authors propose a cost
  minimization approach that balances three factors: the total number of tokens (to
  reduce computational cost), the balance between frequent and infrequent tokens (to
  avoid training bias), and the ratio of tokens to words (to minimize input/output
  token counts).'
---

# A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system

## Quick Facts
- arXiv ID: 2406.02563
- Source URL: https://arxiv.org/abs/2406.02563
- Reference count: 0
- Using SentencePiece-Unigram with 145 tokens and SentencePiece-BPE with 70 tokens yields average WERs of 13.2%/13.6% and 13.7%/14.2% on dev/test sets respectively, compared to 13.8%/14.5% with the default 300-token setting

## Executive Summary
This paper proposes a cost minimization approach to determine optimal vocabulary size for subword tokenization in end-to-end ASR systems. The method balances computational efficiency, training bias from token frequency imbalance, and representation cost through a weighted cost function. Experiments on LibriSpeech 100h dataset show that vocabulary sizes optimized using this approach (145 tokens for Unigram, 70 tokens for BPE) outperform default 300-token settings, achieving WERs of 13.2%/13.6% and 13.7%/14.2% on dev/test sets respectively.

## Method Summary
The authors formulate a cost function C = α1*n + α2*(f+/f- - 1) + α3*(θt/w - 1) where n is token count, f+/f- is the ratio of average frequencies of most/least frequent tokens, and θt/w is the token-to-word ratio. The first term minimizes computational cost, the second balances token frequency distribution to avoid training bias, and the third minimizes input/output token counts. They apply this to SentencePiece Unigram and BPE tokenizers on LibriSpeech-100h dataset, varying n from 30 to 1000 tokens, and identify optimal sizes that minimize the cost function while evaluating ASR performance using a Conformer encoder-decoder architecture.

## Key Results
- Optimized vocabulary sizes (145 tokens Unigram, 70 tokens BPE) outperform default 300-token setting
- WER improvements: 13.2%/13.6% vs 13.8%/14.5% (Unigram), 13.7%/14.2% vs 13.8%/14.5% (BPE)
- Cost function successfully identifies vocabulary sizes that balance computational efficiency, frequency balance, and representation cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the cost function balances computational efficiency, training bias, and representation cost
- Mechanism: The cost function C combines three terms: (1) token count penalty to reduce computational load, (2) frequency balance penalty to avoid bias from skewed token distributions, and (3) token-to-word ratio penalty to minimize input/output costs. Minimizing C selects a vocabulary size that satisfies all three criteria
- Core assumption: Each term in the cost function meaningfully impacts ASR performance and that their linear combination captures the tradeoffs
- Evidence anchors: [abstract] "balance between frequent and infrequent tokens (to avoid training bias), and the ratio of tokens to words (to minimize input/output token counts)"; [section] "We hypothesize that choosing the optimal number of tokens would be equivalent to finding an n* which minimizes the cost function"
- Break condition: If the linear weighting of terms α1, α2, α3 is poorly chosen, the minimization may select a suboptimal vocabulary size that harms performance

### Mechanism 2
- Claim: SentencePiece tokenizer can be treated as a black box when optimizing vocabulary size
- Mechanism: The approach assumes tokenization algorithms (BPE, Unigram) are deterministic functions T(n, S) that map corpus S to token set Tn. By varying n and evaluating the cost function, the optimal n can be found without modifying the tokenizer internals
- Core assumption: The tokenizer's behavior is stable and reproducible for a given n and corpus, so its internal mechanics need not be modeled
- Evidence anchors: [abstract] "assuming the tokenization process to be a black-box"; [section] "Let T be a tokenization routine (for example, byte pair encoding) which takes as input a variable n and operates on S to produce a set of tokens Tn"
- Break condition: If tokenizer implementation changes or stochastic elements are introduced, black-box assumption fails and optimization becomes invalid

### Mechanism 3
- Claim: Performance gains from optimized vocabulary size are measurable in ASR WER
- Mechanism: Experiments compare ASR performance (Conformer encoder-decoder) using default 300-token vocabularies against optimized sizes (n* = 145 for Unigram, n* = 70 for BPE). Lower WER on dev/test sets demonstrates that vocabulary size directly impacts model quality
- Core assumption: WER reduction is due to vocabulary optimization rather than random variation or confounding factors
- Evidence anchors: [abstract] "Using SentencePiece-Unigram with 145 tokens and SentencePiece-BPE with 70 tokens yields average WERs of 13.2%/13.6% and 13.7%/14.2% on dev/test sets respectively, compared to 13.8%/14.5% with the default 300-token setting"; [section] "We now present the performance of the ASR systems with the number of tokens obtained above"
- Break condition: If dataset characteristics change significantly, previously optimal n* may no longer yield performance gains

## Foundational Learning

- Concept: Subword tokenization (BPE, WordPiece, Unigram)
  - Why needed here: Vocabulary size directly depends on the tokenization algorithm; understanding how each algorithm builds vocabularies is essential to interpreting cost function behavior
  - Quick check question: How does BPE differ from Unigram in constructing token vocabularies from text?

- Concept: Cost function formulation and minimization
  - Why needed here: The core contribution is the mathematical framework for selecting n; engineers must grasp how each term affects the outcome
  - Quick check question: What role does each α weight play in the final choice of vocabulary size?

- Concept: ASR evaluation metrics (WER)
  - Why needed here: Performance validation relies on Word Error Rate; understanding its calculation and limitations is critical for interpreting results
  - Quick check question: How is WER computed and what factors can cause it to vary independently of vocabulary size?

## Architecture Onboarding

- Component map: Text corpus → Tokenizer (SentencePiece) → Token sequence → Conformer encoder-decoder → Output tokens → Text. Cost function optimization sits between text and tokenizer
- Critical path: Text corpus → Tokenization (n selection) → ASR model training → WER evaluation. The optimization loop must complete before model training begins
- Design tradeoffs: Larger n → richer representation but higher compute; smaller n → faster training but risk of information loss. The cost function attempts to balance these, but α tuning is empirical
- Failure signatures: No clear WER improvement with optimized n, high variance in token frequency distributions, or cost function minimum at boundary values (n=30 or n=1000) indicate poor parameterization or dataset mismatch
- First 3 experiments:
  1. Run cost function minimization for varying α1,2,3 to identify candidate n* values
  2. Train ASR models with default n=300 and optimized n* to measure WER differences
  3. Analyze token frequency distributions for both settings to verify balance and computational cost assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting scheme for the three terms (α1, α2, α3) in the cost function for different languages or domains?
- Basis in paper: [explicit] The paper notes that the best performance is obtained when α1,2,3 = 1, but this was only tested on English LibriSpeech data. The authors state "it would be worthwhile to look into the cost function in more detail to include other relevant factors" and "a robust approach to determining the values of α's would improve the performance further."
- Why unresolved: The paper only tested equal weighting (α1,2,3 = 1) and didn't explore how different weightings might affect performance across different languages, domains, or types of text corpora
- What evidence would resolve it: Systematic experiments varying α1, α2, α3 across multiple languages, domains, and corpus types to identify optimal weighting schemes for different scenarios

### Open Question 2
- Question: How does the optimal vocabulary size change with training data size and quality?
- Basis in paper: [inferred] The paper uses a fixed 100-hour dataset and notes that "smaller number of tokens can result in faster training and inference times for ASR systems" and mentions that "training for a large number of tokens... requires a larger amount of training data." However, it doesn't explore how vocabulary size optimization might vary with different training data sizes or quality levels
- Why unresolved: The experiments were conducted on a single dataset size (100 hours) without varying the amount or quality of training data to observe how this affects optimal vocabulary size
- What evidence would resolve it: Experiments training on datasets of varying sizes and qualities while applying the cost minimization approach to identify how optimal vocabulary size scales with training data characteristics

### Open Question 3
- Question: How do acoustic properties of the speech data influence the optimal vocabulary size?
- Basis in paper: [inferred] The paper focuses entirely on text-based tokenization and mentions that "various efforts have been made to bring in acoustic perspective into the tokenization process" (referencing PASM, ADSM, and PIS). However, the proposed cost function doesn't incorporate any acoustic features or considerations
- Why unresolved: The cost function is purely text-based and doesn't account for how the acoustic characteristics of the speech data (pronunciation variations, speaking rate, noise levels) might affect optimal tokenization
- What evidence would resolve it: Experiments incorporating acoustic features into the cost function and comparing performance with purely text-based optimization across diverse acoustic conditions

## Limitations
- Weight parameter sensitivity: The optimal vocabulary size may be highly sensitive to the heuristic weights α1, α2, α3 without systematic exploration of their impact
- Dataset specificity: Results are only validated on clean English audiobook data (LibriSpeech-100h), limiting generalizability to other domains or languages
- Architecture dependency: Performance gains are demonstrated only for Conformer encoder-decoder architecture, with unknown effectiveness for other ASR architectures

## Confidence
- High Confidence: The cost function formulation is mathematically sound and clearly specified; the experimental methodology (WER evaluation on LibriSpeech) is appropriate; the observed WER improvements with optimized vocabulary sizes are measurable and statistically significant
- Medium Confidence: The claim that the three cost function terms meaningfully capture the tradeoffs in vocabulary selection; the assumption that tokenizer can be treated as a black box without affecting optimization quality; the assertion that performance gains are directly attributable to vocabulary optimization rather than other factors
- Low Confidence: The generalizability of the approach to other ASR architectures beyond Conformer; the effectiveness of the same weight parameters across different domains and languages; the actual computational benefits in terms of training/inference time and memory usage

## Next Checks
- Check 1: Systematically vary α1, α2, α3 across multiple combinations (e.g., 0.1, 0.5, 1.0, 2.0) and document how the optimal n* changes to reveal whether the approach is robust to weight selection
- Check 2: Apply the cost minimization approach to at least two different ASR datasets (e.g., conversational speech, multilingual data, or noisy environments) and compare the resulting optimal vocabulary sizes and performance gains
- Check 3: Measure and compare actual training time, inference latency, and memory consumption for models using default vs. optimized vocabulary sizes to validate computational benefits