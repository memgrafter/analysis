---
ver: rpa2
title: 'DeMansia: Mamba Never Forgets Any Tokens'
arxiv_id: '2408.01986'
source_url: https://arxiv.org/abs/2408.01986
tags:
- demansia
- attention
- input
- image
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeMansia, a vision model that integrates
  state space models with token labeling techniques to enhance image classification
  performance. DeMansia builds on Mamba and Vision Mamba architectures, incorporating
  bidirectional processing and token labeling from LV-ViT.
---

# DeMansia: Mamba Never Forgets Any Tokens

## Quick Facts
- arXiv ID: 2408.01986
- Source URL: https://arxiv.org/abs/2408.01986
- Reference count: 30
- Primary result: DeMansia Tiny achieves 79.4% top-1 accuracy on ImageNet-1k with only 8.06M parameters

## Executive Summary
DeMansia introduces a vision model that integrates state space models with token labeling techniques to enhance image classification performance. The model builds on Mamba and Vision Mamba architectures, incorporating bidirectional processing and token labeling from LV-ViT. By addressing the computational challenges of traditional transformers, particularly in handling long sequences and large inputs, DeMansia achieves competitive performance against larger transformer-based models while maintaining efficiency. The approach demonstrates effectiveness for resource-constrained environments with minimal parameter counts.

## Method Summary
DeMansia combines Mamba's selective state spaces with token labeling techniques to create an efficient vision model. The architecture uses ViM blocks with bidirectional processing to capture spatial relationships across image patches, while an auxiliary token labeling head provides dense supervision during training. The model processes 224x224 images through a 4-layer conv network to generate patch embeddings, adds learnable positional embeddings and a class token, then passes through 24 layers of ViM blocks. Training uses a combined loss from the class token and auxiliary token labeling head, with mixed precision computation on RTX A6000 GPUs.

## Key Results
- DeMansia Tiny achieves 79.4% top-1 accuracy and 94.5% top-5 accuracy on ImageNet-1k with only 8.06M parameters
- Outperforms larger transformer-based models like TinyViT-21M (21.6M parameters, 77.3% accuracy) despite using fewer parameters
- Demonstrates competitive performance against ResNet-152 and other similar-scale architectures

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional Mamba blocks allow spatial information to propagate both horizontally and vertically across image patches. The ViM block applies forward and backward 1D convolution and SSM processing, enabling each patch to integrate context from both directions along the sequence axis. Core assumption: Image patch sequences retain spatial structure when flattened, so bidirectional processing recovers missing spatial relationships. Break condition: If spatial relationships are too complex for 1D bidirectional processing to capture, performance will degrade relative to full 2D convolutions.

### Mechanism 2
Token labeling provides dense supervision that improves feature discrimination across all patch tokens. An auxiliary head processes all patch embeddings with the same learnable parameters, generating patch tokens that are compared to dense ground truth maps via cross-entropy loss. Core assumption: Dense supervision at the token level provides richer gradient signals than class-token-only supervision. Break condition: If dense maps are noisy or incorrect, the auxiliary loss may degrade rather than improve performance.

### Mechanism 3
Selective state spaces in Mamba provide linear complexity scaling while maintaining context awareness. The selection mechanism dynamically determines how much of each input to incorporate into the state, replacing the fixed compression of traditional SSMs with content-aware gating. Core assumption: Dynamic selection can approximate attention's context weighting without quadratic cost. Break condition: If selection parameters cannot adequately model complex dependencies, performance will plateau despite linear scaling.

## Foundational Learning

- **Linear attention approximations (Linformer, Performer)**
  - Why needed: To understand why Mamba's selective SSM offers an alternative to attention-based efficiency techniques
  - Quick check: What is the computational complexity of standard self-attention and how do linear attention methods reduce it?

- **Positional embeddings in vision transformers**
  - Why needed: DeMansia adds learnable positional embeddings to patch sequences, requiring understanding of how position information is encoded
  - Quick check: How do learnable positional embeddings differ from fixed sinusoidal embeddings in terms of adaptation to specific tasks?

- **Knowledge distillation in vision models**
  - Why needed: To understand the performance gap between DeMansia and models like TinyViT-21M that use distillation
  - Quick check: What is the primary purpose of knowledge distillation in training vision transformers?

## Architecture Onboarding

- **Component map**: Image → Conv patches → ViM blocks → Class head → Prediction
- **Critical path**: The ViM blocks are the primary computational bottleneck due to bidirectional processing
- **Design tradeoffs**: 
  - Bidirectional processing increases VRAM usage and computation vs unidirectional Mamba
  - Token labeling adds auxiliary loss computation and memory overhead
  - Linear complexity Mamba vs quadratic complexity attention enables larger input sequences
  - Separate class and auxiliary heads vs unified head affects parameter efficiency
- **Failure signatures**: 
  - Poor convergence: Check learning rate schedule and weight decay
  - Low accuracy despite training: Verify token labeling ground truth generation
  - High memory usage: Monitor bidirectional ViM block memory footprint
  - Slow inference: Profile auxiliary head computation during inference
- **First 3 experiments**:
  1. Ablation study: Remove token labeling and measure accuracy drop
  2. Memory profiling: Compare VRAM usage with/without bidirectional processing
  3. Scalability test: Measure performance on larger input resolutions to verify linear scaling

## Open Questions the Paper Calls Out

### Open Question 1
How does DeMansia's performance scale with larger model variants? The paper only tested DeMansia Tiny, stating "Due to resource constraints, our experiments were limited to just the DeMansia Tiny variant." What evidence would resolve it: Training and evaluating larger DeMansia variants on the same benchmarks as DeMansia Tiny, comparing their accuracy, parameter efficiency, and computational requirements.

### Open Question 2
What is the cause of the slow convergence observed when using gradient accumulation? The paper mentions "unexpected behaviour when using gradient accumulation to simulate large batches, where there signs of extremely slow convergence." What evidence would resolve it: Conducting a detailed analysis of the training dynamics with gradient accumulation, including monitoring gradients, loss curves, and potentially comparing with other optimization strategies.

### Open Question 3
How does DeMansia perform on tasks beyond image classification? The paper notes "applications of DeMansia in semantic segmentation tasks" but only evaluates on image classification. What evidence would resolve it: Adapting DeMansia for semantic segmentation or object detection tasks, training on appropriate datasets, and evaluating its performance against state-of-the-art models in those domains.

## Limitations

- Performance comparisons may be affected by knowledge distillation usage in baseline models like TinyViT-21M
- The bidirectional ViM block implementation details are not fully specified, requiring reliance on custom Mamba package
- Token labeling dataset generation process lacks specific implementation details

## Confidence

- **High confidence**: Linear computational complexity claims for Mamba-based architecture
- **Medium confidence**: 79.4% top-1 accuracy on ImageNet-1k (single run on specific hardware)
- **Medium confidence**: Token labeling contribution to performance improvement (mechanism described but ablation study needed)

## Next Checks

1. Conduct ablation study to quantify token labeling contribution beyond the stated 79.4% performance
2. Profile VRAM usage during bidirectional ViM block processing to confirm linear scaling claims under different batch sizes
3. Evaluate DeMansia Tiny on CIFAR-10/CIFAR-100 to verify if performance gains generalize beyond ImageNet-1k