---
ver: rpa2
title: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models
arxiv_id: '2402.11217'
source_url: https://arxiv.org/abs/2402.11217
tags:
- medical
- med-mllms
- specialties
- question
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Asclepius is a novel benchmark for Medical Multi-Modal Large Language
  Models (Med-MLLMs) designed to comprehensively assess their performance across 15
  medical specialties and 8 clinical capacities. It includes 3,232 original multi-modal
  questions, spanning anatomical perception, disease analysis, staging assessment,
  treatment planning, prognosis, and report generation.
---

# A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models

## Quick Facts
- arXiv ID: 2402.11217
- Source URL: https://arxiv.org/abs/2402.11217
- Reference count: 28
- One-line primary result: Asclepius benchmark reveals that general MLLMs like GPT-4V outperform specialized Med-MLLMs in medical multi-modal tasks, with human doctors still maintaining superior performance

## Executive Summary
Asclepius is a comprehensive benchmark designed to evaluate Medical Multi-Modal Large Language Models (Med-MLLMs) across 15 medical specialties and 8 clinical capacities. The benchmark includes 3,232 original multi-modal questions covering anatomical perception, disease analysis, staging assessment, treatment planning, prognosis, and report generation. Evaluation of six Med-MLLMs against three human specialists demonstrates that general-purpose models like GPT-4V achieve higher accuracy than specialized Med-MLLMs, with GPT-4V reaching 55.8% average accuracy in diagnosis tasks. The benchmark employs three core principles: multi-specialty coverage, multi-dimensional capacity assessment, and original, blinded data construction to prevent data leakage.

## Method Summary
The Asclepius benchmark evaluates Med-MLLMs using 3,232 original multi-modal questions across 15 medical specialties and 8 clinical capacities. Questions are constructed from educational materials and medical textbooks with altered answer choices and phrasing, then verified by senior doctors to ensure no overlap with existing datasets. The benchmark employs multiple evaluation methods including accuracy for multiple-choice questions, GPT evaluation for open-ended questions, and ROUGE-L for report generation tasks. Six Med-MLLMs (GPT-4V, GPT-4o, Gemini, Claude 3.5 Sonnet, CheXagent, RadFM, Med-Flamingo, XrayGPT, BiomedGPT-B) are evaluated alongside three human specialists across all questions.

## Key Results
- Generalist models like GPT-4V achieve 55.8% average accuracy in diagnosis tasks, outperforming specialized Med-MLLMs
- Human doctors maintain superior performance with accuracy rates exceeding 90% across specialties
- Med-MLLMs demonstrate consistent performance across specialties with variance of only 0.02
- Ablation studies confirm that both visual and textual modalities are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-specialty coverage ensures comprehensive evaluation by forcing Med-MLLMs to demonstrate competence across diverse medical domains rather than overfitting to a single specialty.
- Mechanism: By requiring correct answers across 15 specialties, the benchmark reveals whether a model has broad medical knowledge or only narrow specialization, mimicking the real-world need for generalist capability.
- Core assumption: Medical knowledge is sufficiently domain-specific that performance in one specialty does not reliably transfer to others.
- Evidence anchors:
  - [abstract]: "comprehensive assessment of Med-MLLMs in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.)"
  - [section]: "Medical education is characterized by significant disparities in knowledge across different specialties"
  - [corpus]: Weak - related papers focus on specific specialties (dermatology, pathology, radiology) but don't explicitly test cross-specialty generalization
- Break condition: If medical knowledge domains share significant overlap or if models can transfer learning effectively across specialties

### Mechanism 2
- Claim: Multi-dimensional capacity stratification captures the complexity of clinical decision-making by testing perception, diagnosis, and planning skills separately.
- Mechanism: By dividing tasks into perception (anatomical, attribute, spatial), diagnosis (disease analysis, staging), and planning (treatment, prognosis, report generation), the benchmark can identify specific competency gaps rather than just overall performance.
- Core assumption: Clinical reasoning follows a predictable progression from perception through diagnosis to planning, and models must demonstrate capability at each stage.
- Evidence anchors:
  - [abstract]: "stratify into 3 main categories and 8 sub-categories of clinical tasks"
  - [section]: "The decision-making process in clinical practice is multifaceted and layered, encompassing a series of complex cognitive tasks"
  - [corpus]: Weak - related works focus on end-to-end performance but don't systematically evaluate these intermediate steps
- Break condition: If clinical reasoning doesn't follow this structured progression or if models can bypass certain steps

### Mechanism 3
- Claim: Original question construction with blinding prevents data leakage and ensures fair comparison between models and human experts.
- Mechanism: Questions are rewritten from educational materials and textbooks with altered answer choices and phrasing, then verified by senior doctors to ensure they don't overlap with existing datasets.
- Core assumption: Existing medical VQA datasets have been used to train current Med-MLLMs, making blinded evaluation necessary for valid comparison.
- Evidence anchors:
  - [abstract]: "exempting overlap with existing VQA dataset"
  - [section]: "questions in MedVQABench are sourced from contemporary educational materials, medical examinations, and visual datasets, rather than integrating previously existing multiple VQA datasets"
  - [corpus]: Weak - related works mention data collection but don't emphasize blinding or originality
- Break condition: If models are not trained on existing datasets or if the rewriting process doesn't sufficiently obscure the original content

## Foundational Learning

- Concept: Multi-modal fusion
  - Why needed here: Med-MLLMs must integrate visual and textual medical information to answer questions accurately, as shown by ablation experiments where removing either modality significantly reduced performance
  - Quick check question: What happens to model accuracy when you provide only image information versus both image and text?

- Concept: Clinical reasoning progression
  - Why needed here: The benchmark is structured around the clinical workflow from perception through diagnosis to planning, so understanding this progression is essential for interpreting results
  - Quick check question: How does the benchmark evaluate the transition from disease identification to staging assessment?

- Concept: Medical specialty knowledge domains
  - Why needed here: Different specialties require different knowledge bases and diagnostic approaches, which is why the benchmark includes 15 specialties to test breadth of capability
  - Quick check question: Why might a cardiologist need to refer to a gastroenterologist, and how does this relate to evaluating Med-MLLMs?

## Architecture Onboarding

- Component map: Original medical content -> Question rewriting -> Doctor verification -> Server-side evaluation -> Result analysis
- Critical path: Medical content -> Question construction -> Expert review -> Model evaluation -> Comparative analysis
- Design tradeoffs: Original content vs. existing datasets (better blinding but more effort), comprehensive vs. focused evaluation (broad coverage vs. deep specialization), human vs. automated scoring (accuracy vs. scalability)
- Failure signatures: Low inter-rater reliability among doctors (>0.78 threshold), model performance variance across specialties >0.02, ablation results showing modality dependence
- First 3 experiments:
  1. Run ablation study with GPT-4V to confirm modality dependency (text only, image only, both)
  2. Evaluate one Med-MLLM across all 15 specialties to identify performance patterns
  3. Compare human doctor performance across specialties to establish baseline variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Asclepius performance compare to Med-MLLMs when evaluated on long patient history narratives that are crucial for real-world clinical decision-making?
- Basis in paper: [inferred] from limitations section stating "The current benchmark does not consider long patient history narratives that are often crucial for real-world clinical decision-making"
- Why unresolved: The benchmark currently focuses on independent questions for Perception/Diagnosis/Planning without coherence or integration of long patient records
- What evidence would resolve it: An expanded benchmark incorporating sequential disease questions with long patient record sequences, comparing Med-MLLM performance against human specialists on these complex longitudinal cases

### Open Question 2
- Question: What specific architectural improvements would enable specialized Med-MLLMs to outperform generalist models like GPT-4V in medical imaging tasks?
- Basis in paper: [explicit] from results showing "generalist models such as GPT-4V and Gemini outperform five specialized Med-MLLMs" and parameter count differences
- Why unresolved: The paper identifies performance gaps but doesn't test specific architectural modifications to specialized models
- What evidence would resolve it: Comparative studies of specialized Med-MLLMs with increased parameter capacity and architectural modifications specifically designed for medical imaging fusion tasks

### Open Question 3
- Question: How can Med-MLLMs be improved to better integrate visual and textual information in medical diagnosis, as demonstrated by their current limitations in multi-modality fusion?
- Basis in paper: [explicit] from case studies showing models "simply restate the well-known fact that the liver is located in the upper right quadrant of the abdomen, neglecting to integrate the visual data presented"
- Why unresolved: While the paper identifies this limitation, it doesn't propose or test specific solutions for improving visual-textual integration
- What evidence would resolve it: Comparative analysis of Med-MLLM performance on tasks requiring visual-textual integration before and after implementing specific fusion mechanisms or attention mechanisms designed for medical imaging

## Limitations

- The benchmark focuses on knowledge recall and structured questions rather than complex clinical reasoning involving uncertainty management
- Performance evaluation relies on multiple-choice and structured formats that may favor pattern recognition over true diagnostic capability
- The dataset construction from educational materials may not fully represent real-world clinical scenarios with their inherent complexity and ambiguity

## Confidence

**High Confidence**: The benchmark successfully evaluates 15 medical specialties across 8 clinical capacities with original, blinded questions. The methodology for constructing questions from educational sources and verifying them with senior doctors is well-documented and reproducible.

**Medium Confidence**: The finding that general MLLMs (GPT-4V) outperform specialized Med-MLLMs is supported by the data, though this could be influenced by model size differences and training data composition rather than inherent architectural advantages.

**Low Confidence**: The claim that Med-MLLMs demonstrate "consistent performance across specialties" is based on narrow variance (0.02) that may not be statistically significant given the sample size and complexity of medical knowledge domains.

## Next Checks

1. **Statistical Significance Analysis**: Perform formal statistical testing on the 0.02 variance in specialty performance to determine if this consistency claim holds across different significance thresholds.

2. **Clinical Scenario Validation**: Test the benchmark's questions against actual clinical cases from electronic health records to assess whether the knowledge-based evaluation translates to real-world diagnostic capability.

3. **Cross-Dataset Generalization**: Evaluate models trained on this benchmark against established medical VQA datasets to verify that the original question construction genuinely prevents data leakage while maintaining clinical relevance.