---
ver: rpa2
title: Learning on manifolds without manifold learning
arxiv_id: '2402.12687'
source_url: https://arxiv.org/abs/2402.12687
tags:
- approximation
- manifold
- function
- data
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses function approximation on unknown low-dimensional
  manifolds embedded in high-dimensional Euclidean spaces, a common assumption in
  machine learning. Traditional approaches require prior knowledge of the manifold,
  obtained through "manifold learning" techniques like eigen-decomposition of the
  Laplace-Beltrami operator or coordinate charts.
---

# Learning on manifolds without manifold learning

## Quick Facts
- arXiv ID: 2402.12687
- Source URL: https://arxiv.org/abs/2402.12687
- Reference count: 40
- One-line primary result: Optimal function approximation on unknown low-dimensional manifolds without requiring manifold learning preprocessing

## Executive Summary
This paper proposes a novel method for function approximation on unknown low-dimensional manifolds embedded in high-dimensional spaces without requiring traditional manifold learning preprocessing. The key innovation is projecting the unknown manifold onto a hypersphere and using localized spherical polynomial kernels for direct approximation. The method achieves optimal approximation rates that depend on the intrinsic dimension of the manifold rather than the ambient space dimension, bypassing the curse of dimensionality. The theoretical framework provides rigorous error bounds for functions in a smoothness class W_γ, showing that approximation error decreases as (log M / M)^(γ/(q+2γ)) with probability 1-δ.

## Method Summary
The method bypasses traditional manifold learning by projecting data points from the unknown manifold onto a hypersphere and approximating functions using localized spherical polynomial kernels. Given random samples (yj, zj) from an unknown distribution where yj are points on the manifold and zj are function values, the approach computes F_n(D; x) = (1/M) Σ zj Φ_n,q(x · yj), where Φ_n,q is a localized spherical polynomial kernel constructed from ultraspherical polynomials. The method requires only knowing the manifold's dimension q and does not need preprocessing or optimization. The approximation is defined for any point on the hypersphere, providing an out-of-sample extension capability.

## Key Results
- Achieves optimal approximation rates without prior manifold learning by leveraging localized spherical polynomial kernels
- Error bounds depend on intrinsic manifold dimension q rather than ambient dimension Q, avoiding curse of dimensionality
- Provides out-of-sample extension allowing function approximation at any point on the hypersphere
- Theorem 3.1 establishes error decreases as (log M / M)^(γ/(q+2γ)) with probability 1-δ for functions in smoothness class W_γ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method achieves optimal approximation rates without prior manifold learning by leveraging localized spherical polynomial kernels.
- Mechanism: By projecting the unknown manifold onto a hypersphere, the method uses specially designed localized spherical polynomial kernels Φ_n,q. These kernels are constructed from ultraspherical polynomials with a localization factor h(ℓ/n) that controls their spatial concentration. This allows direct approximation of functions on the manifold without needing to estimate the manifold's geometry or compute its Laplace-Beltrami operator.
- Core assumption: The manifold is smooth, compact, and embedded in a high-dimensional Euclidean space, and the function to be approximated belongs to a smoothness class W_γ.
- Evidence anchors:
  - [abstract]: "Our approach does not require preprocessing of the data to obtain information about the manifold other than its dimension."
  - [section 2.2]: Definition and properties of the localized kernel Φ_n,q using ultraspherical polynomials with a localization factor.
  - [corpus]: Weak. Corpus papers focus on related topics like manifold learning and optimization but do not directly support the specific mechanism of bypassing manifold learning with spherical kernels.
- Break condition: If the function smoothness γ is too low or the kernel localization fails to capture the function's features, the approximation rates will degrade.

### Mechanism 2
- Claim: The error bound depends on the intrinsic dimension q of the manifold rather than the ambient dimension Q.
- Mechanism: The method projects the q-dimensional manifold to a q-dimensional hypersphere, where approximation is performed using spherical polynomials of degree < n. The resulting error bound (log M / M)^(γ/(q+2γ)) depends on q, the manifold dimension, rather than Q, the ambient space dimension. This avoids the curse of dimensionality typically associated with high ambient dimensions.
- Core assumption: The data lies on or near a low-dimensional manifold embedded in a high-dimensional space.
- Evidence anchors:
  - [abstract]: "In terms of M, the estimate in (1.2) depends asymptotically on the dimension of the manifold rather than the dimension of the ambient space."
  - [section 3]: Theorem 3.1 states the error bound depends on q, the manifold dimension.
  - [corpus]: Weak. Corpus papers discuss manifold learning and dimensionality reduction but do not directly address the specific mechanism of achieving dimension-dependent error bounds by projecting to a hypersphere.
- Break condition: If the manifold dimension q is large relative to the ambient dimension Q, or if the manifold is highly curved, the approximation may not effectively reduce the curse of dimensionality.

### Mechanism 3
- Claim: The method provides out-of-sample extension, allowing function approximation at any point on the hypersphere, not just on the manifold.
- Mechanism: The approximation formula F_n(D; x) = (1/M) Σ z_j Φ_n,q(x · y_j) is defined for any x in the ambient hypersphere S_Q, not just for x on the manifold X. This allows predicting function values at new points without requiring additional data collection or preprocessing.
- Core assumption: The target function can be well-approximated by spherical polynomials on the ambient hypersphere.
- Evidence anchors:
  - [abstract]: "The point x in (1.1) is not restricted to the manifold, but rather freely chosen from S_Q. That is, our construction defines an out of sample extension..."
  - [section 1]: Discussion of the approximation formula and its ability to define an out-of-sample extension.
  - [corpus]: Weak. Corpus papers do not directly address the concept of out-of-sample extension in the context of manifold approximation.
- Break condition: If the function has complex structure that cannot be captured by spherical polynomials on the ambient hypersphere, the out-of-sample extension may perform poorly.

## Foundational Learning

- Concept: Smoothness classes W_γ and approximation theory on spheres.
  - Why needed here: The method's error bounds are derived using approximation theory results for functions in the smoothness class W_γ on spheres. Understanding these smoothness classes and approximation results is crucial for analyzing the method's theoretical guarantees.
  - Quick check question: What is the relationship between the smoothness parameter γ and the approximation rate n^(-γ) in Theorem 5.1?

- Concept: Manifolds, tangent spaces, and exponential maps.
  - Why needed here: The method operates on manifolds, requiring an understanding of manifold concepts such as tangent spaces, exponential maps, and geodesic distances. These concepts are used to define the approximation operator and analyze its properties.
  - Quick check question: How does the exponential map relate points on the tangent space to points on the manifold, and why is this important for the method's construction?

- Concept: Localized kernels and their properties.
  - Why needed here: The method's performance relies on the properties of the localized spherical polynomial kernels Φ_n,q, including their localization, approximation power, and Lebesgue constants. Understanding these properties is essential for analyzing the method's theoretical guarantees and implementation.
  - Quick check question: What role does the localization factor h(ℓ/n) play in controlling the spatial concentration of the kernel Φ_n,q, and how does this affect the approximation quality?

## Architecture Onboarding

- Component map: Data points -> Projection to hypersphere -> Kernel construction -> Approximation computation -> Error analysis

- Critical path:
  1. Project data points from the manifold to the hypersphere.
  2. Construct the localized spherical polynomial kernel Φ_n,q.
  3. Compute the approximation F_n(D; x) using the kernel and data points.
  4. Analyze the approximation error using the smoothness class W_γ and the number of samples M.

- Design tradeoffs:
  - Kernel localization: Tighter localization (larger S in the kernel definition) leads to better spatial concentration but may increase computational cost.
  - Polynomial degree: Higher degree n provides better approximation power but may lead to overfitting with limited data.
  - Number of samples: More samples M improve the approximation accuracy but increase computational cost and memory requirements.

- Failure signatures:
  - Poor approximation quality: May indicate insufficient data, inappropriate kernel localization, or mismatch between the function smoothness and the approximation space.
  - High computational cost: May suggest overly complex kernel construction or excessive polynomial degree.
  - Numerical instability: Could arise from ill-conditioned kernel matrices or insufficient numerical precision in the computations.

- First 3 experiments:
  1. Verify the kernel construction: Implement the localized spherical polynomial kernel Φ_n,q and test its properties, such as localization and approximation power, on synthetic data.
  2. Validate the approximation formula: Compute the approximation F_n(D; x) using synthetic data and compare the results with the true function values to assess the approximation quality.
  3. Analyze the error bounds: Generate synthetic data with known smoothness properties and verify the theoretical error bounds by comparing the empirical approximation error with the predicted bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the data distribution has multiple disconnected components or is not connected?
- Basis in paper: [inferred] The paper assumes a compact, connected manifold but doesn't explore disconnected cases
- Why unresolved: The paper focuses on a single connected manifold and doesn't provide theoretical guarantees for disconnected components
- What evidence would resolve it: Numerical experiments showing performance degradation or success when applied to datasets with disconnected components

### Open Question 2
- Question: Can the method be extended to non-compact manifolds or manifolds with boundary?
- Basis in paper: [inferred] The paper assumes compact manifolds without boundary throughout
- Why unresolved: The theoretical framework relies heavily on compactness and the absence of boundary for key proofs
- What evidence would resolve it: Extension of the theoretical results to non-compact cases with appropriate modifications to the kernel construction

### Open Question 3
- Question: How sensitive is the method to the choice of hyper-parameters (degree n and number of samples M)?
- Basis in paper: [explicit] The paper mentions n and M as tunable hyper-parameters but doesn't provide systematic sensitivity analysis
- Why unresolved: While the paper provides theoretical error bounds, it doesn't explore the practical implications of hyper-parameter choices
- What evidence would resolve it: Systematic numerical experiments varying n and M to determine optimal ranges and sensitivity to noise

## Limitations
- The specific construction of the localized spherical polynomial kernel Φ_n,q is referenced but not explicitly defined
- Practical difficulty of determining manifold dimension from data without any preprocessing
- Computational complexity of evaluating ultraspherical polynomials for large dimensions and sample sizes

## Confidence
- Theoretical framework: High
- Practical implementation: Medium
- Dimension-dependence claim: High
- Out-of-sample extension capability: Medium

## Next Checks
1. Implement and verify the localization properties of the spherical polynomial kernel Φ_n,q by testing its spatial concentration characteristics on synthetic manifold data with known geometry.

2. Conduct controlled experiments comparing approximation error rates against the theoretical bounds (log M / M)^(γ/(q+2γ)) across varying sample sizes M, polynomial degrees n, and manifold dimensions q.

3. Validate the practical challenges of dimension estimation by testing the method's sensitivity to misspecified manifold dimensions using both synthetic data with known dimensions and real-world datasets where manifold structure can be independently verified.