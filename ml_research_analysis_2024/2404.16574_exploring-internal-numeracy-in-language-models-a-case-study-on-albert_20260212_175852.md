---
ver: rpa2
title: 'Exploring Internal Numeracy in Language Models: A Case Study on ALBERT'
arxiv_id: '2404.16574'
source_url: https://arxiv.org/abs/2404.16574
tags:
- numbers
- language
- numerical
- embeddings
- albert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how Transformer-based language models, specifically
  the ALBERT family, internally represent numerical concepts. By analyzing learned
  embeddings for numbers and ordinals, the researchers discovered that ALBERT models
  consistently use principal component axes to represent numerical ordering and spacing.
---

# Exploring Internal Numeracy in Language Models: A Case Study on ALBERT

## Quick Facts
- arXiv ID: 2404.16574
- Source URL: https://arxiv.org/abs/2404.16574
- Reference count: 0
- Primary result: ALBERT models use principal component axes to represent numerical ordering with logarithmic scaling

## Executive Summary
This study investigates how ALBERT transformer models internally represent numerical concepts through learned embeddings. By analyzing embeddings for numbers, ordinals, and magnitude words using PCA, the researchers discovered consistent patterns in how ALBERT organizes numerical information. The findings reveal that these text-only trained models develop sophisticated internal representations of numerical ordering and spacing, with higher values clustering closer together in a logarithmic-like pattern. The results demonstrate that language models can intuit basic mathematical concepts even without explicit numerical training.

## Method Summary
The researchers extracted uncontextualized embeddings for numerical tokens (0-100) and ordinal/magnitude words from ALBERT models of various sizes (base, large, xlarge, xxlarge) in both v1 and v2 versions. They applied PCA to reduce the high-dimensional embeddings to 2D space and analyzed the resulting visualizations to identify patterns in how numerical concepts are represented. The analysis focused on whether embeddings reflect numerical ordering, spacing relationships, and clustering patterns among numerals and their textual counterparts.

## Key Results
- ALBERT models consistently use primary principal component axes to represent numerical ordering and spacing across different model sizes
- Numerical representations exhibit logarithmic compression, with higher values clustering closer together in PCA space
- Numerals and their textual counterparts occupy separate clusters but increase along the same direction in 2D PCA space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALBERT models learn numerical ordering representations through principal component analysis of learned embeddings
- Mechanism: The models internally organize numerical concepts along axes of greatest variation in the embedding space, creating ordered representations that reflect numerical value
- Core assumption: PCA captures meaningful semantic relationships in embedding space for numerical concepts
- Evidence anchors:
  - [abstract] "PCA results reveal that ALBERT models...consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts"
  - [section] "PCA results reveal that ALBERT models of different sizes...consistently learn to use the primary principal component axes to denote ordering and spacing of numbers, ordinals, and magnitude orders"
  - [corpus] Weak - neighboring papers discuss numerical embeddings but don't directly address PCA-based ordering mechanisms
- Break condition: If PCA analysis fails to reveal consistent ordering patterns across different ALBERT model sizes or training durations

### Mechanism 2
- Claim: Numerical representations in ALBERT exhibit logarithmic scaling rather than linear scaling
- Mechanism: Higher numerical values are represented by embeddings that cluster closer together in PCA space, suggesting logarithmic compression of numerical magnitude
- Core assumption: Distance between embeddings in PCA space correlates with perceived numerical difference
- Evidence anchors:
  - [abstract] "The representations are closer together for higher values, suggesting a logarithmic representation of numbers"
  - [section] "As numbers increase, they approximately trace out a horseshoe shape in 2D space" and "Larger numbers gradually compress closer together"
  - [corpus] Weak - neighboring papers discuss numerical embeddings but don't explicitly address logarithmic scaling in PCA space
- Break condition: If numerical distances in embedding space follow linear rather than logarithmic compression patterns

### Mechanism 3
- Claim: ALBERT separates numeral and textual representations while maintaining parallel ordering
- Mechanism: Numerals and their written counterparts occupy distinct clusters in PCA space but increase along the same direction, enabling cross-representation numerical understanding
- Core assumption: Distinct clusters can maintain parallel ordering along shared axes
- Evidence anchors:
  - [abstract] "Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D PCA space"
  - [section] "Numbers and words representing occupy two distinct, elongated clusters" and "The direction along which the values increase is the same for both numbers and number words"
  - [corpus] Weak - neighboring papers discuss numerical embeddings but don't address the separation and parallel ordering of numeral vs. textual representations
- Break condition: If numeral and textual representations diverge in their ordering directions or fail to maintain consistent numerical progression

## Foundational Learning

- Principal Component Analysis
  - Why needed here: PCA is used to identify the axes of greatest variation in numerical embeddings, revealing how models internally organize numerical concepts
  - Quick check question: What does PCA reveal about the relationship between numerical value and embedding space organization?

- Transformer embedding mechanisms
  - Why needed here: Understanding how ALBERT creates and organizes word embeddings is crucial for interpreting the PCA results
  - Quick check question: How do Transformer models typically represent positional and semantic information in their embedding layers?

- Numerical cognition in language models
  - Why needed here: Context for understanding what constitutes meaningful numerical representation and reasoning in text-trained models
  - Quick check question: What distinguishes basic numerical ordering from more complex mathematical reasoning in language models?

## Architecture Onboarding

- Component map: Tokenizer → Embedding layer → PCA computation → 2D visualization; key components include embedding matrix, PCA algorithm, and visualization tools
- Critical path: Token extraction → embedding retrieval → PCA computation → pattern analysis; each step must preserve numerical relationships
- Design tradeoffs: Single-token focus limits analysis scope but ensures clean numerical representations; PCA provides interpretability but may lose some information through dimensionality reduction
- Failure signatures: Inconsistent ordering across model sizes, absence of logarithmic compression patterns, or failure of numeral/textual representations to maintain parallel ordering
- First 3 experiments:
  1. Extract and visualize embeddings for numbers 0-20 and their word forms using PCA to verify basic ordering patterns
  2. Analyze numbers 1-100 to test whether logarithmic compression emerges for larger values
  3. Compare order-of-magnitude words (hundred, thousand, million, etc.) to validate logarithmic scaling hypotheses across different numerical scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ALBERT's numerical embeddings evolve during training, and what training dynamics influence their development?
- Basis in paper: [explicit] The authors note that their findings are robust across different training durations ("v1" vs. "v2") but do not investigate the specific evolution of embeddings during training
- Why unresolved: The paper only compares final embeddings from different checkpoints, not the training trajectory
- What evidence would resolve it: Analysis of embedding evolution through training epochs, showing when and how numerical representations develop and stabilize

### Open Question 2
- Question: Do different Transformer architectures (BERT, RoBERTa, ELECTRA) develop similar numerical representations as ALBERT, or are there architecture-specific patterns?
- Basis in paper: [inferred] The study focuses exclusively on ALBERT models, though the authors mention other Transformer architectures in the introduction
- Why unresolved: The analysis is limited to one architecture family, leaving open whether these numerical representation patterns are universal across Transformer models
- What evidence would resolve it: Comparative PCA analysis of numerical embeddings across multiple Transformer architectures

### Open Question 3
- Question: How do frequency-based biases in training data (like Benford's Law) influence the spacing and clustering of numerical embeddings?
- Basis in paper: [explicit] The authors mention that "smaller leading digits are more common in real-world numerical data" and that "rounded numbers... lie closer to middle of the space," suggesting a connection to frequency effects
- Why unresolved: While the authors suggest frequency effects, they don't systematically investigate the relationship between token frequency and embedding structure
- What evidence would resolve it: Controlled experiments varying token frequency in training data and measuring resulting embedding patterns

## Limitations

- Analysis focuses exclusively on ALBERT models, limiting generalizability to other transformer architectures
- PCA-based interpretation relies on 2D projections that may oversimplify complex high-dimensional relationships
- Study examines only uncontextualized embeddings, which may not reflect how numerical representations function during actual language understanding tasks

## Confidence

**High Confidence:** The observation that ALBERT models consistently organize numerical concepts along principal component axes across different model sizes and training durations

**Medium Confidence:** The claim that numerical representations exhibit logarithmic compression, which requires more rigorous statistical validation

**Medium Confidence:** The separation of numeral and textual representations while maintaining parallel ordering, though functional significance remains to be tested

## Next Checks

1. Apply quantitative measures (e.g., regression analysis, correlation tests) to determine whether spacing between numerical embeddings follows logarithmic, linear, or other mathematical functions across the full numerical range

2. Repeat the analysis on other transformer models (BERT, RoBERTa, GPT-style models) to assess whether PCA-based numerical organization is specific to ALBERT or represents a general phenomenon

3. Design experiments to test whether identified numerical representation patterns support numerical reasoning by evaluating model performance on arithmetic tasks while correlating results with PCA-based representation analysis