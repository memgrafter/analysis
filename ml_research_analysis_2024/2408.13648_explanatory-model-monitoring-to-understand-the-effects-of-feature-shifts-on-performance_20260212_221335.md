---
ver: rpa2
title: Explanatory Model Monitoring to Understand the Effects of Feature Shifts on
  Performance
arxiv_id: '2408.13648'
source_url: https://arxiv.org/abs/2408.13648
tags:
- shift
- performance
- data
- feature
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Explanatory Performance Estimation (XPE),
  a method to explain performance degradation of ML models under feature shifts without
  requiring labels. XPE combines Optimal Transport for aligning source/target distributions
  with Shapley Values to attribute the anticipated performance change to specific
  input features.
---

# Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance

## Quick Facts
- arXiv ID: 2408.13648
- Source URL: https://arxiv.org/abs/2408.13648
- Reference count: 40
- One-line primary result: Introduces Explanatory Performance Estimation (XPE) that explains performance degradation under feature shifts without requiring labels, outperforming baselines in quantifying feature shift importance.

## Executive Summary
This paper addresses the challenge of monitoring model performance when input features shift between training and deployment without requiring labeled data. The authors propose Explanatory Performance Estimation (XPE), a method that combines Optimal Transport for aligning source and target distributions with Shapley Values to attribute performance changes to specific input features. This approach enables actionable insights into why models deteriorate under feature shifts and guides maintenance decisions.

XPE demonstrates superior performance across multiple domains including image (MNIST, FashionMNIST), audio (organ and pneumonia classification), and tabular data. The method achieves high Shift-Faithfulness scores (e.g., 0.78 vs 0.21 for brightness corruption on MNIST) and provides more concise explanations compared to baseline approaches. Additionally, the authors show that XPE effectively detects selection bias in audio data and missing value issues in tabular data, demonstrating its practical utility for explanatory model monitoring.

## Method Summary
XPE addresses the problem of explaining performance degradation under feature shifts without requiring labels by combining Optimal Transport (OT) with Shapley Values. The method first computes a probabilistic coupling between source and target samples using OT, then uses Shapley Values to attribute the anticipated performance change to specific input features. The approach can estimate performance degradation even when labels are unavailable by leveraging the OT coupling and simulating partial feature shifts through replacement with likely source versions. The method is evaluated using metrics like Shift-Faithfulness, Complexity, and ROAR-S across multiple datasets and shift types.

## Key Results
- XPE achieves superior Shift-Faithfulness scores compared to baselines (0.78 vs 0.21 for brightness corruption on MNIST)
- Provides more concise explanations with lower Complexity scores across all tested datasets
- Effectively detects selection bias in audio data, with 96.5% of attributions to gender-specific frequencies for gender-specific shifts
- Successfully identifies missing value issues in tabular data with Global Performance Correlation scores of 0.75

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XPE combines Optimal Transport to align source/target distributions with Shapley Values to attribute performance change to specific features.
- Mechanism: OT computes a probabilistic coupling between source and target samples; Shapley Values then simulate partial feature shifts by replacing target features with their most likely source versions and aggregate marginal contributions.
- Core assumption: The observed shift can be expressed as a functional transformation and the OT coupling accurately captures this transformation.
- Evidence anchors:
  - [abstract] "XPE combines Optimal Transport for aligning source/target distributions with Shapley Values to attribute the anticipated performance change to specific input features."
  - [section] "More precisely, this means that the target domain distribution ð‘ƒð‘¡ can be obtained from the source distribution ð‘ƒð‘  via push-forward operation: ð‘ƒð‘¡ = ð‘‡#ð‘ƒð‘ ."
  - [corpus] Weak evidence; neighboring papers focus on different methods (persistent homology, cognitive architectures).
- Break condition: If the shift cannot be modeled by a probabilistic coupling (e.g., complex non-functional transformations), OT will fail and Shapley attributions will be meaningless.

### Mechanism 2
- Claim: The shift-fidelity metric (Shift-Faithfulness) validates that high attribution features truly cause the performance drop when shifted back.
- Mechanism: For each feature subset ð¾, compute correlation between sum of attribution scores and actual performance change after partial shift reversal.
- Core assumption: The attribution scores are monotonic with respect to the true causal effect of the shift on model performance.
- Evidence anchors:
  - [abstract] "superior Shift-Faithfulness scores (e.g., 0.78 vs 0.21 for brightness corruption on MNIST)"
  - [section] "When features with high attribution scores are shifted back, we expect the model performance to recover equivalently."
  - [corpus] No direct evidence; must rely on paper's own experiments.
- Break condition: If the model is highly non-linear or the shift introduces interactions that the attribution model cannot capture, correlation will break down.

### Mechanism 3
- Claim: The label-transport accuracy is high when the shift is approximately label-preserving, allowing performance estimation without labels.
- Mechanism: Match each target sample to its most likely source counterpart via OT coupling, transfer the source label, and evaluate performance.
- Core assumption: The shift preserves the label relationship within a bounded total variation distance Îµ.
- Evidence anchors:
  - [abstract] "superior Shift-Faithfulness scores (e.g., 0.78 vs 0.21 for brightness corruption on MNIST)"
  - [section] "Theorem 5.2... establishes a continuity result, showing that if the decision boundary for transported samples in the target domain is close to the original one, then the estimation error can also be small."
  - [corpus] No direct evidence; must rely on paper's own experiments.
- Break condition: If the shift introduces label noise or changes the decision boundary beyond Îµ, the performance estimate will be unreliable.

## Foundational Learning

- Concept: Optimal Transport (OT)
  - Why needed here: OT provides a principled way to align source and target distributions without requiring labels, essential for XPE's core mechanism.
  - Quick check question: What is the Kantorovich formulation of OT and how does it differ from the Monge formulation?

- Concept: Shapley Values
  - Why needed here: Shapley Values attribute the marginal contribution of each feature to the predicted performance change, enabling actionable insights.
  - Quick check question: How does the value function ð‘£ðœ‹(ð¾) differ from the standard Shapley value formulation?

- Concept: Feature Attribution Methods
  - Why needed here: Understanding different attribution methods (removal-based, occlusion, gradient) helps contextualize XPE's approach.
  - Quick check question: What are the key theoretical properties that Shapley Values satisfy compared to other attribution methods?

## Architecture Onboarding

- Component map: Data preprocessing -> OT coupling estimation -> Label transport -> Performance estimation -> Shapley Value computation -> Evaluation
- Critical path: 1. Compute OT coupling between source and target samples; 2. Estimate label transport (if labels available); 3. Compute Shapley Values for full and partial shifts; 4. Aggregate attributions and evaluate with Shift-Faithfulness
- Design tradeoffs: OT accuracy vs. computational cost: Using intermediate network activations can improve accuracy but adds complexity; Feature granularity: Coarser feature grouping reduces computation but may lose important details; Label transport vs. uncertainty proxy: Using entropy as proxy avoids needing labels but may be less accurate
- Failure signatures: Low Shift-Faithfulness scores indicate poor alignment between attributions and true performance change; High Complexity scores suggest attributions are spread across many features, reducing actionability; Low label transport accuracy indicates the shift is not label-preserving
- First 3 experiments: 1. Run XPE on MNIST with brightness corruption and compare Shift-Faithfulness to baselines; 2. Evaluate XPE on the audio dataset with selection bias and measure proportion of attributions to gender-specific frequencies; 3. Test XPE on tabular data with missing values and compute Global Performance Correlation for the corrupted feature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do XPE and XPPE perform under more severe distribution shifts where the approximate label-preserving assumption is violated?
- Basis in paper: [explicit] The paper states that XPE relies on the assumption that the shift is approximately label-preserving (Definition 5.1) and mentions that XPPE can be superior when this assumption doesn't hold.
- Why unresolved: The paper demonstrates XPE's effectiveness under various shifts but doesn't explicitly test scenarios where the label-preserving assumption is significantly violated.
- What evidence would resolve it: Experimental results comparing XPE/XPPE performance under controlled distribution shifts with varying degrees of label-preserving violation, showing performance degradation as the assumption becomes less valid.

### Open Question 2
- Question: What is the computational trade-off between using optimal transport on raw inputs versus lower-dimensional representations, and how does this affect shift attribution quality?
- Basis in paper: [inferred] The paper mentions using intermediate network activations for optimal transport in the audio experiment to improve scalability, suggesting a trade-off exists.
- Why unresolved: While the paper demonstrates the practical benefit of using lower-dimensional representations, it doesn't systematically analyze the computational savings versus attribution quality trade-off across different data types.
- What evidence would resolve it: A systematic study comparing computation time and shift attribution quality (e.g., Shift-Faithfulness scores) when using raw inputs versus various dimensionality reduction techniques across multiple datasets.

### Open Question 3
- Question: How sensitive are XPE and XPPE to the choice of cost function in optimal transport, and what are the implications for different data modalities?
- Basis in paper: [explicit] The paper mentions that the resulting coupling depends on the chosen cost function but uses squared Euclidean distance as a default choice.
- Why unresolved: The paper doesn't explore how different cost functions (e.g., learned metrics, domain-specific distances) affect the quality of shift attributions across different data modalities.
- What evidence would resolve it: Comparative experiments using various cost functions (e.g., Wasserstein, Mahalanobis, learned metrics) on different data types, measuring the impact on attribution quality metrics like Shift-Faithfulness and practical utility in model monitoring.

## Limitations
- The method relies on Optimal Transport accurately capturing the functional transformation between distributions, which may fail for complex non-functional transformations.
- Performance estimation depends on the assumption that shifts are approximately label-preserving, which may not hold in real-world scenarios with significant domain shifts.
- Computational complexity of OT and Shapley Value calculations may limit scalability to large datasets or high-dimensional feature spaces.

## Confidence
- High confidence: Experimental results showing XPE's superior performance on benchmark datasets with specific metrics
- Medium confidence: Theoretical guarantees provided reasonable bounds but rely on assumptions about label preservation
- Low confidence: Claims about handling "any black-box model" and "any feature shift scenario" are not thoroughly validated

## Next Checks
1. Stress test with adversarial shifts: Evaluate XPE on datasets where the shift intentionally breaks label preservation and measure performance degradation.
2. Scalability benchmark: Implement XPE on a large-scale dataset and measure computation time, memory usage, and tractability of Shapley Value computation.
3. Cross-domain generalization: Apply XPE to a real-world dataset with source and target from different domains to test OT coupling's ability to capture complex distribution shifts.