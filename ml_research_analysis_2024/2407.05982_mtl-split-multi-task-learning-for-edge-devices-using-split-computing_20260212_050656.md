---
ver: rpa2
title: 'MTL-Split: Multi-Task Learning for Edge Devices using Split Computing'
arxiv_id: '2407.05982'
source_url: https://arxiv.org/abs/2407.05982
tags:
- learning
- tasks
- edge
- task
- computing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTL-Split, a novel framework that combines
  Split Computing (SC) and Multi-Task Learning (MTL) to enable multiple inference
  tasks on edge devices. Unlike traditional SC approaches that focus on single-task
  learning, MTL-Split leverages a shared backbone on the edge device and multiple
  task-specific heads on remote servers to simultaneously solve multiple tasks.
---

# MTL-Split: Multi-Task Learning for Edge Devices using Split Computing

## Quick Facts
- arXiv ID: 2407.05982
- Source URL: https://arxiv.org/abs/2407.05982
- Reference count: 33
- Primary result: Combines split computing and multi-task learning to enable multiple inference tasks on edge devices while reducing memory usage by up to 57% and network bandwidth by 87%

## Executive Summary
This paper introduces MTL-Split, a novel framework that combines Split Computing (SC) and Multi-Task Learning (MTL) to enable multiple inference tasks on edge devices. Unlike traditional SC approaches that focus on single-task learning, MTL-Split leverages a shared backbone on the edge device and multiple task-specific heads on remote servers to simultaneously solve multiple tasks. The framework significantly reduces memory requirements on edge devices (up to 57% for EfficientNet on FACES dataset) and network transmission bandwidth (87% latency reduction). Experimental results on synthetic (3D Shapes) and real-world datasets (MEDIC, FACES) demonstrate that MTL-Split consistently improves task performance across all tasks, achieving accuracy gains ranging from 0.17% to 80.22% compared to single-task learning baselines.

## Method Summary
MTL-Split combines split computing and multi-task learning by deploying a shared backbone on edge devices and task-specific heads on remote servers. The framework processes input through the shared backbone, extracts compressed feature representations, transmits these features to the server, and then processes them through task-specific heads for final predictions. The approach is trained end-to-end using joint optimization of shared parameters and task-specific parameters. The method was evaluated on three datasets: 3D Shapes (synthetic), MEDIC, and FACES, using backbone architectures including MobileNetV3, EfficientNet, and VGG16.

## Key Results
- Memory reduction: Up to 57% memory savings on edge devices when using EfficientNet on FACES dataset
- Bandwidth efficiency: 87% latency reduction through compressed feature transmission (1.5MB vs 115MB)
- Performance gains: Accuracy improvements ranging from 0.17% to 80.22% across all tasks compared to single-task learning baselines
- Scalability: Successfully handles multiple tasks with a single shared backbone on the same edge device

## Why This Works (Mechanism)

### Mechanism 1: Memory Reduction Through Shared Backbone
The shared backbone processes input into a compressed shared feature space that is sent to remote task-specific heads, eliminating the need to store multiple task-specific models on the edge device. This approach reduces memory requirements from approximately 6.9GB for multiple task-specific models to 1.5GB for a single shared backbone.

### Mechanism 2: Performance Improvement Through Inductive Transfer
The shared backbone learns a joint representation that benefits from task correlations, with gradients from all tasks updating the shared parameters. This leads to better generalization than single-task learning by finding representations that capture all tasks simultaneously.

### Mechanism 3: Bandwidth Reduction Through Feature Compression
Instead of transmitting raw input images (approximately 115MB for FACES dataset) to the server, MTL-Split transmits only the compressed shared feature representation (1.5MB), dramatically reducing data transfer volume while maintaining sufficient information for accurate task inference.

## Foundational Learning

- **Multi-Task Learning (MTL) fundamentals**: Understanding how joint optimization across tasks works is essential for grasping why MTL-Split can improve performance rather than just maintain it. *Quick check: What is the key difference between hard parameter sharing (used in MTL-Split) and soft parameter sharing in MTL architectures?*

- **Split Computing (SC) architecture**: The core innovation combines SC's partitioning strategy with MTL's joint learning, so understanding both components is necessary. *Quick check: In traditional SC, what determines the optimal split point, and how does MTL-Split's approach differ?*

- **Feature extraction and representation learning**: The shared backbone must learn features that are useful across all tasks, requiring understanding of how neural networks extract hierarchical representations. *Quick check: How does the dimensionality of the shared feature space (Z_b) affect both performance and bandwidth requirements?*

## Architecture Onboarding

- **Component map**: Edge device (shared backbone + communication module) -> Network (communication channel) -> Remote server (task-specific heads + aggregation logic)
- **Critical path**: Input → Backbone processing → Feature transmission → Task heads processing → Output aggregation
- **Design tradeoffs**: Backbone size vs. edge device memory constraints; Feature space dimensionality vs. bandwidth requirements vs. task performance; Number of tasks vs. potential for negative transfer; Remote server capacity vs. number/complexity of task heads
- **Failure signatures**: High memory usage on edge device (backbone too large); Poor task performance (insufficient feature space capacity or conflicting tasks); High latency (feature transmission too large or network congestion); Training instability (learning rate mismatch)
- **First 3 experiments**: 1) Single-task baseline: Deploy each task individually on edge device to establish performance and memory baselines; 2) Joint backbone test: Train shared backbone with all tasks on server (no split) to verify MTL benefits exist; 3) Split validation: Implement full MTL-Split with 2 tasks on simple dataset (like 3D Shapes) to verify the split approach works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
How does the MTL-Split architecture perform on real-time video processing tasks compared to other edge computing frameworks? The paper mentions the automotive domain as an example application but does not provide experimental results for real-time video processing tasks. Experimental results comparing MTL-Split's latency, accuracy, and resource usage on real-time video processing tasks against other edge computing frameworks would provide insights into its practical applicability.

### Open Question 2
What is the impact of varying network conditions on the performance of MTL-Split, and how does it compare to other distributed deep learning approaches under poor network conditions? The paper mentions network latency reduction but does not provide a detailed analysis of performance under different network conditions or comparisons with other approaches. Experimental results showing the performance of MTL-Split under different network conditions (e.g., varying bandwidth, latency, packet loss) and comparisons with other distributed deep learning approaches would demonstrate its robustness and adaptability.

### Open Question 3
How does the MTL-Split architecture scale with an increasing number of tasks, and what are the limitations in terms of the number of tasks it can handle effectively? The paper mentions that the task-solving heads can become larger than the backbone when the number of tasks is large, but it does not provide a detailed analysis of scalability or limitations. Experimental results showing the performance, memory usage, and communication overhead of MTL-Split as the number of tasks increases, along with an analysis of any practical limitations, would provide insights into its scalability and applicability to complex multi-task scenarios.

## Limitations
- Exact architectural specifications of task-specific heads remain underspecified beyond "two linear layers activated by ReLU"
- Implementation details of the splitting mechanism are not fully described
- Weak external validation for core mechanisms with no directly comparable studies addressing the specific combination of memory reduction through shared backbone deployment, bandwidth reduction through feature compression in split computing, or performance improvements from combining MTL with split computing

## Confidence

- **High confidence**: Memory reduction claims (57% reduction for EfficientNet is clearly quantified)
- **Medium confidence**: Bandwidth reduction claims (87% latency improvement is well-supported with specific measurements)
- **Medium confidence**: Task performance improvements (accuracy gains of 0.17% to 80.22% are reported, but mechanisms could benefit from more detailed analysis)

## Next Checks

1. **Architectural replication**: Implement the exact task-specific head architecture and verify that the reported performance improvements can be reproduced on the 3D Shapes dataset before scaling to larger datasets.

2. **Cross-dataset generalization**: Test MTL-Split on an additional multi-task dataset not used in the original experiments to verify that the performance improvements and memory/bandwidth reductions generalize beyond the specific datasets studied.

3. **Negative transfer analysis**: Systematically evaluate scenarios where tasks may be unrelated or conflicting by testing MTL-Split with intentionally dissimilar tasks to identify conditions under which the approach fails or degrades performance.