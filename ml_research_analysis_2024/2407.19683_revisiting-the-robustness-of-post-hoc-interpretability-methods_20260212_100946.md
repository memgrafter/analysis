---
ver: rpa2
title: Revisiting the robustness of post-hoc interpretability methods
arxiv_id: '2407.19683'
source_url: https://arxiv.org/abs/2407.19683
tags:
- interpretability
- methods
- post-hoc
- robustness
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-grained evaluation framework for post-hoc
  interpretability methods in deep learning. The core idea is to assess not only average
  performance (coarse-grained) but also the consistency/robustness of interpretability
  across individual samples by analyzing the distribution of normalized score drops
  after corrupting important data portions.
---

# Revisiting the robustness of post-hoc interpretability methods

## Quick Facts
- arXiv ID: 2407.19683
- Source URL: https://arxiv.org/abs/2407.19683
- Authors: Jiawen Wei; Hugues Turbé; Gianmarco Mengaldo
- Reference count: 20
- This paper proposes a fine-grained evaluation framework for post-hoc interpretability methods in deep learning.

## Executive Summary
This paper introduces a novel evaluation framework for post-hoc interpretability methods that assesses both average performance and robustness across individual samples. The framework analyzes the distribution of normalized score drops after corrupting important data portions, going beyond traditional coarse-grained accuracy metrics. Through experiments on synthetic and 20 public time series datasets using three neural network architectures, the authors demonstrate that DeepSHAP consistently provides both high average performance and robustness, while KernelSHAP performs poorly in both aspects. The work emphasizes the importance of reliability in interpretability methods, particularly for high-stakes applications where consistent explanations are crucial.

## Method Summary
The paper proposes a fine-grained evaluation framework for post-hoc interpretability methods in deep learning that assesses both average performance and robustness across individual samples. The core approach involves analyzing the distribution of normalized score drops when important data portions are corrupted. Two new metrics—AUC Skew and AUC Kurt—are introduced to quantify robustness using higher-order moments of the score drop distribution. The framework evaluates interpretability methods by comparing their performance on both coarse-grained (average) and fine-grained (consistency/robustness) levels, enabling practitioners to select methods that balance accuracy and reliability.

## Key Results
- DeepSHAP consistently provides both high average performance and robustness across experiments
- KernelSHAP performs poorly in both average performance and robustness metrics
- The AUC Skew and AUC Kurt metrics effectively capture differences in interpretability method reliability
- The framework enables better selection of interpretability methods for high-stakes applications

## Why This Works (Mechanism)
The framework works by moving beyond traditional accuracy-only metrics to capture the distributional properties of interpretability method performance. By analyzing how normalized score drops are distributed when important data portions are corrupted, the method reveals inconsistencies that average metrics would miss. The use of higher-order moments (skewness and kurtosis) through AUC Skew and AUC Kurt metrics provides a nuanced view of robustness, identifying methods that not only perform well on average but also maintain consistent behavior across different samples.

## Foundational Learning
- AUC (Area Under Curve): A performance metric that summarizes the entire ROC curve; needed to evaluate interpretability methods consistently, quick check: verify AUC values range between 0 and 1
- Skewness: Measures asymmetry of a distribution; needed to capture directional bias in score drops, quick check: positive skew indicates tail on right side
- Kurtosis: Measures "tailedness" of a distribution; needed to assess extreme score drop events, quick check: high kurtosis indicates heavy tails
- Post-hoc interpretability: Methods that explain trained models; needed context for the evaluation framework, quick check: these methods don't alter model training
- Score drop distribution: The distribution of performance degradation when important features are removed; central to the proposed metrics, quick check: should be normalized for fair comparison

## Architecture Onboarding
Component map: Neural network -> Post-hoc method -> Perturbation -> Score calculation -> AUC Skew/Kurt metrics
Critical path: Input data → Model prediction → Interpretability method → Feature importance scores → Perturbation strategy → Score drop measurement → Robustness metrics
Design tradeoffs: Balancing computational efficiency (KernelSHAP is expensive) vs. robustness (DeepSHAP performs better)
Failure signatures: Methods with high average performance but poor robustness show inconsistent explanations across samples
First experiments: 1) Apply perturbations to synthetic data with known ground truth, 2) Compare AUC Skew values across different interpretability methods, 3) Test robustness to varying perturbation magnitudes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies on artificial perturbations that may not reflect real-world data corruption patterns
- AUC Skew and AUC Kurt metrics lack validation against human judgment or downstream task performance
- Experiments focus exclusively on tabular and time-series data with neural networks
- Limited comparison to only three interpretability methods without computational efficiency considerations

## Confidence
- High confidence in the methodological framework for assessing robustness
- Medium confidence in the specific findings about DeepSHAP versus KernelSHAP performance
- Medium confidence in the claim that this framework better captures reliability than traditional accuracy-only metrics

## Next Checks
1. Test the AUC Skew and AUC Kurt metrics against human expert evaluations to verify they capture meaningful notions of interpretability quality
2. Apply the framework to different data types (images, text) and model architectures (transformers, decision trees) to assess generalizability
3. Conduct experiments with additional perturbation strategies that mimic real-world data corruption scenarios to validate robustness claims under more realistic conditions