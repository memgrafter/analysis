---
ver: rpa2
title: Enhancing Multi-Step Reasoning Abilities of Language Models through Direct
  Q-Function Optimization
arxiv_id: '2410.09302'
source_url: https://arxiv.org/abs/2410.09302
tags:
- arxiv
- learning
- process
- sampling
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Direct Q-function Optimization (DQO), an offline
  reinforcement learning algorithm designed to enhance the multi-step reasoning abilities
  of large language models (LLMs). Unlike previous approaches that either require
  extensive online sampling (e.g., PPO) or simplify the reasoning process as a bandit
  problem (e.g., DPO, DRO), DQO formulates LLM response generation as a Markov Decision
  Process (MDP) and optimizes a Q-function parameterized directly by the language
  model using the soft actor-critic framework.
---

# Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization

## Quick Facts
- arXiv ID: 2410.09302
- Source URL: https://arxiv.org/abs/2410.09302
- Reference count: 11
- Outperforms SFT, RS, DPO, KTO, and DRO on math reasoning tasks with 15%+ gains on MATH dataset

## Executive Summary
This paper introduces Direct Q-function Optimization (DQO), an offline reinforcement learning algorithm that enhances large language models' multi-step reasoning capabilities by modeling response generation as a Markov Decision Process. Unlike prior alignment methods that simplify reasoning as bandit problems or require expensive online sampling, DQO uses soft actor-critic framework with λ-return and importance sampling to enable step-by-step supervision. Experimental results on GSM8K, MATH, and AIME24 datasets demonstrate state-of-the-art performance in math problem-solving tasks, with over 15% improvement compared to base models.

## Method Summary
DQO formulates LLM response generation as an MDP and optimizes a Q-function parameterized directly by the language model using the soft actor-critic framework. The algorithm leverages offline trajectories with process supervision signals, using λ-return to compute stable temporal difference targets and importance sampling to handle distributional shift between behavior and target policies. The Q-function takes the form Q(s,a) = β log π(a|s) + V(s), enabling end-to-end optimization through the policy network. Training alternates between updating the value network and Q-network, with the latter backpropagating through the policy to improve token-level decision making.

## Key Results
- Achieves state-of-the-art performance on GSM8K, MATH, and AIME24 math reasoning datasets
- Improves performance by over 15% on MATH dataset compared to base model
- Outperforms SFT, RS, DPO, KTO, and DRO across all tested benchmarks
- Demonstrates effectiveness of process supervision augmentation in math problem-solving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DQO improves multi-step reasoning by modeling generation as an MDP rather than a bandit problem.
- Mechanism: The MDP formulation allows step-by-step supervision through a learned value function, capturing the long-horizon nature of reasoning tasks. By parameterizing the Q-function directly with the policy network, DQO can backpropagate rewards at each token step, not just at the end.
- Core assumption: The reasoning process can be decomposed into sequential token-level decisions with meaningful intermediate rewards.
- Evidence anchors:
  - [abstract] "DQO formulates the response generation process as a Markov Decision Process (MDP)"
  - [section 1] "DRO treats the process as a bandit problem, which neglects the intrinsic long-horizon nature of a wide spectrum of tasks"
  - [corpus] Weak - no direct comparison of MDP vs bandit performance
- Break condition: If intermediate reasoning steps don't have differentiable rewards or if the reasoning process isn't Markovian.

### Mechanism 2
- Claim: λ-return stabilizes training by reducing bias in temporal difference updates.
- Mechanism: Instead of using only one-step TD targets, λ-return computes a weighted average of n-step returns, providing lower-variance estimates when the value function is poorly initialized. This helps propagate reward signals more effectively through long reasoning chains.
- Core assumption: The optimal value function can be approximated by combining multiple n-step returns rather than just one-step returns.
- Evidence anchors:
  - [section 4.2] "One-step temporal difference (TD) errors have high bias and perform poorly when the value function is not well-initialized"
  - [section 5.4] Empirical ablation shows performance drops significantly when λ < 1
  - [corpus] Weak - no theoretical justification provided
- Break condition: If λ is set too low, causing high bias, or if the environment has non-Markov rewards.

### Mechanism 3
- Claim: Importance sampling reweights offline data to match the current policy distribution, enabling stable offline RL.
- Mechanism: Since DQO trains on pre-collected data from a reference policy, importance sampling corrects for the distributional shift by weighting each trajectory by the ratio of current policy probability to behavior policy probability, truncated to prevent gradient explosion.
- Core assumption: The offline dataset contains sufficient coverage of states and actions that the current policy might visit.
- Evidence anchors:
  - [section 4.3] "importance sampling is required to properly re-weight the offline data to mitigate this misalignment"
  - [section 5.4] Ablation shows significant performance drops without importance sampling
  - [corpus] Weak - no discussion of coverage requirements
- Break condition: If the offline dataset has poor coverage of relevant state-action pairs, causing importance weights to be unstable.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper models language generation as an MDP to capture sequential decision-making and enable step-by-step supervision.
  - Quick check question: In an MDP, what are the key components needed to define the decision-making problem?

- Concept: Soft Actor-Critic (SAC)
  - Why needed here: SAC provides the framework for learning both a value function and a policy simultaneously, with entropy regularization to encourage exploration.
  - Quick check question: How does entropy regularization in SAC differ from standard Q-learning?

- Concept: Temporal Difference Learning
  - Why needed here: TD learning is used to update the value function estimates based on observed rewards and bootstrapped value estimates from the next state.
  - Quick check question: What is the difference between one-step TD and n-step TD learning?

## Architecture Onboarding

- Component map:
  Policy network (LLM) -> Value network -> Q-function network -> Data pipeline with importance weights -> Training loop

- Critical path: Generate trajectories → Compute λ-return targets → Update V-network → Update Q-network (via policy) → Repeat

- Design tradeoffs:
  - Offline vs online: Offline avoids expensive online sampling but requires good coverage in pre-collected data
  - MDP vs bandit: MDP enables step supervision but increases complexity
  - λ-return: Balances bias-variance tradeoff in TD updates
  - Importance sampling: Enables offline RL but can be unstable with extreme weights

- Failure signatures:
  - Training instability: Check importance weight distribution and consider truncation
  - Poor performance: Verify λ-return is properly implemented and λ is set appropriately
  - Slow convergence: Check if offline dataset has sufficient diversity and coverage

- First 3 experiments:
  1. Ablation study: Compare DQO with λ=1 vs λ=0.95 to verify λ-return importance
  2. Importance sampling test: Train with and without IS to measure impact on performance
  3. MDP vs bandit comparison: Implement a simplified bandit version and compare on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DQO's performance scale with model size, and is there a threshold where the benefits of the MDP formulation become more pronounced?
- Basis in paper: [inferred] The paper compares DQO across different model sizes (7B Qwen2.5-QwQ, 7B Gemma) but does not systematically explore scaling effects or identify performance breakpoints.
- Why unresolved: The paper focuses on 7B parameter models and does not investigate larger or smaller model sizes to determine if DQO's advantages scale with model capacity.
- What evidence would resolve it: Comprehensive experiments comparing DQO across a range of model sizes (e.g., 1B, 13B, 33B, 70B) on the same tasks, analyzing performance trends and identifying any scaling thresholds.

### Open Question 2
- Question: Can DQO be effectively extended to handle tasks beyond math problem-solving, such as code generation or general reasoning, where long-horizon planning is critical?
- Basis in paper: [explicit] The paper mentions that DQO is designed to handle long-horizon tasks like math problem solving and complex reasoning, but empirical validation is limited to math datasets.
- Why unresolved: While the MDP formulation theoretically supports broader applications, the paper only demonstrates effectiveness on math problems without exploring other domains requiring multi-step reasoning.
- What evidence would resolve it: Empirical results showing DQO's performance on diverse long-horizon tasks including code generation benchmarks, logical reasoning datasets, and multi-turn dialogue tasks.

### Open Question 3
- Question: What is the impact of different λ-return values on DQO's convergence speed and final performance, and how sensitive is the algorithm to this hyperparameter?
- Basis in paper: [explicit] The paper includes ablation studies showing performance degradation when λ decreases from 1.0 to 0.95, but does not explore the full hyperparameter space or analyze convergence dynamics.
- Why unresolved: The study only compares two λ values (1.0 and 0.95) without investigating intermediate values, different ranges, or the relationship between λ and training stability/performance.
- What evidence would resolve it: Systematic hyperparameter sweep of λ values (e.g., 0.8 to 1.0 in 0.05 increments) with convergence speed analysis and performance measurements to identify optimal ranges and sensitivity patterns.

### Open Question 4
- Question: How does DQO perform when process supervision is available versus when only outcome supervision is provided, and what is the marginal benefit of process rewards?
- Basis in paper: [explicit] The paper includes a section showing DQO can be augmented with process scores and demonstrates performance improvements, but does not quantify the relative importance of process versus outcome supervision.
- Why unresolved: The study adds process rewards to DQO but does not compare scenarios with only outcome rewards, only process rewards, or varying levels of process supervision quality.
- What evidence would resolve it: Controlled experiments comparing DQO with different supervision configurations (outcome-only, process-only, combined with varying quality/quantity of process data) to quantify the marginal benefit of process supervision.

## Limitations

- The theoretical foundation for combining SAC with language modeling is not fully established, particularly regarding the validity of soft Q-function parameterization for discrete token-level decisions
- Offline RL approach requires careful consideration of dataset coverage, with unclear minimum requirements for stable training across the full reasoning space
- Ablation studies don't exhaustively explore critical design choices like λ-return weight, importance sampling truncation, or impact of different value function architectures

## Confidence

**High Confidence**: The empirical claim that DQO outperforms SFT, RS, DPO, KTO, and DRO on GSM8K, MATH, and AIME24 datasets is well-supported by the presented results. The 15% improvement on MATH is specifically documented with clear methodology.

**Medium Confidence**: The claim that MDP formulation is superior to bandit approaches for multi-step reasoning has theoretical plausibility and some empirical support, but the paper doesn't provide direct ablation comparing pure MDP vs bandit implementations on identical datasets with controlled conditions.

**Low Confidence**: The assertion that λ-return is essential for stability and that importance sampling is required for offline RL success are supported by ablations, but the underlying mechanisms aren't fully explained. The paper doesn't investigate what happens when dataset coverage is insufficient or when λ is poorly tuned.

## Next Checks

1. **Coverage Sensitivity Analysis**: Systematically vary the number of reference responses per problem (e.g., 5, 10, 20, 40) to determine the minimum coverage required for stable DQO training. Measure both performance and training stability metrics.

2. **Direct MDP vs Bandit Comparison**: Implement a simplified version of DRO that uses the same Q-function parameterization but without the full MDP machinery (no value function, no step-level supervision). Compare this against full DQO on identical datasets to isolate the contribution of the MDP formulation itself.

3. **Importance Weight Distribution Study**: During training, log and analyze the distribution of importance weights (π_current/π_behavior) across all tokens. Investigate whether extreme weights (>100 or <0.01) correlate with training instability or performance degradation, and test different truncation strategies.