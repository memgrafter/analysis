---
ver: rpa2
title: Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness
  on Multiple-Choice Q&A
arxiv_id: '2402.13213'
source_url: https://arxiv.org/abs/2402.13213
tags:
- auroc
- falcon
- logit
- llama2
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models' (LLMs) softmax
  probabilities and pre-softmax logits contain useful information about their confidence
  on multiple-choice question answering (Q&A) tasks. The authors hypothesize that
  wrong answers are associated with lower maximum softmax probabilities (MSPs) compared
  to correct answers.
---

# Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A

## Quick Facts
- arXiv ID: 2402.13213
- Source URL: https://arxiv.org/abs/2402.13213
- Authors: Benjamin Plaut; Nguyen X. Khanh; Tu Trinh
- Reference count: 28
- Key outcome: Softmax probabilities and logits can predict answer correctness better than random chance, with AUROC values of 60-69% for high-accuracy models

## Executive Summary
This paper investigates whether large language models' softmax probabilities and pre-softmax logits contain useful information about their confidence on multiple-choice question answering tasks. The authors hypothesize that wrong answers are associated with lower maximum softmax probabilities compared to correct answers. Through experiments on ten open-source LLMs and five Q&A datasets using zero-shot prompting, they demonstrate that for models with high accuracy, MSPs and logits successfully predict answer correctness better than random chance. Additionally, they show that selectively abstaining based on MSP or logit values can significantly reduce wrong answers on Q&A tasks with an abstain option.

## Method Summary
The study evaluates whether maximum softmax probabilities (MSPs) and pre-softmax logits from large language models contain predictive information about answer correctness on multiple-choice Q&A tasks. The authors test this hypothesis across ten open-source LLMs using zero-shot prompting on five Q&A datasets. They measure the ability of MSPs and logits to discriminate between correct and incorrect answers using AUROC metrics, and investigate the correlation between Q&A accuracy and predictive ability. The study also explores how abstention strategies based on probability thresholds can improve overall performance when wrong answers carry penalties.

## Key Results
- For high-accuracy models, MSPs and logits predict answer correctness with AUROC values of 60-69%, significantly above random chance
- Strong correlation exists between Q&A accuracy and the ability to predict correctness using MSPs and logits
- Selective abstention based on MSP or logit values can significantly reduce wrong answers and improve overall performance on Q&A tasks with abstain options

## Why This Works (Mechanism)
The mechanism relies on the observation that when LLMs generate answers to multiple-choice questions, the probability distribution over answer choices contains information about the model's confidence. While these probabilities are often miscalibrated (meaning they don't reflect true confidence levels), the relative ordering of probabilities between correct and incorrect answers still provides discriminative signal. The model's internal reasoning process, captured in the logits before softmax normalization, also contains information about answer plausibility that correlates with correctness.

## Foundational Learning
- **Softmax probabilities**: Convert raw model outputs (logits) into probability distributions; needed to understand how models assign confidence to answers; quick check: verify probabilities sum to 1
- **Logits**: Raw, unnormalized model outputs; needed as they contain information before probability transformation; quick check: examine distribution shape across answer choices
- **AUROC (Area Under ROC Curve)**: Measures classifier performance across thresholds; needed to quantify discriminative ability; quick check: AUROC > 0.5 indicates better than random performance
- **Zero-shot prompting**: Evaluating models without task-specific fine-tuning; needed to test general capabilities; quick check: ensure prompts are consistent across models
- **Miscalibration**: When predicted probabilities don't match empirical accuracy; needed context for interpreting probability-based metrics; quick check: reliability diagrams comparing predicted vs actual accuracy
- **Abstention strategies**: Choosing not to answer when confidence is low; needed for understanding practical applications; quick check: threshold selection impact on precision-recall tradeoff

## Architecture Onboarding

**Component Map:**
LLM -> Multiple Choice Q&A Task -> Softmax Probabilities & Logits -> Correctness Prediction -> Performance Metrics

**Critical Path:**
Model generates logits → Softmax converts to probabilities → Maximum probability extracted → Probability compared to threshold → Answer accepted or abstained

**Design Tradeoffs:**
- Zero-shot vs few-shot prompting: Zero-shot provides cleaner comparison but may underutilize model capabilities
- Probability threshold selection: Lower thresholds increase coverage but reduce precision
- Model size vs performance: Larger models show better predictive ability but increase computational cost

**Failure Signatures:**
- AUROC near 0.5 indicates no discriminative ability
- Negative correlation between accuracy and predictive ability suggests model-specific issues
- Poor abstention performance indicates threshold selection problems

**Three First Experiments:**
1. Vary temperature scaling to examine effects on probability calibration and predictive ability
2. Compare zero-shot performance with few-shot prompting using demonstration examples
3. Test different abstention thresholds to optimize precision-recall tradeoff

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Results are limited to multiple-choice Q&A tasks and may not generalize to other task types
- Analysis focuses on zero-shot prompting, limiting insights about few-shot or fine-tuned scenarios
- The correlation between accuracy and predictive ability requires further investigation into causal mechanisms

## Confidence

**Major Claim Clusters and Confidence Labels:**

1. **MSPs and logits contain useful information for predicting answer correctness** - High confidence. The empirical results across multiple models and datasets demonstrate consistent performance above random chance, with clear statistical significance.

2. **Correlation between Q&A accuracy and predictive ability exists** - Medium confidence. While the correlation is observed, the underlying mechanisms and potential confounding factors require further investigation.

3. **Selective abstention improves performance on Q&A tasks** - Medium confidence. The theoretical framework is sound, but real-world implementation challenges and edge cases need exploration.

## Next Checks

1. Evaluate the predictive power of MSPs and logits across diverse task types (e.g., open-ended generation, reasoning tasks) and different prompting strategies (few-shot, chain-of-thought) to assess generalizability.

2. Investigate the causal relationship between model calibration and predictive ability through controlled experiments varying temperature scaling, fine-tuning objectives, and architectural modifications.

3. Test abstention strategies in realistic deployment scenarios with varying cost structures for abstention versus wrong answers to validate practical utility.