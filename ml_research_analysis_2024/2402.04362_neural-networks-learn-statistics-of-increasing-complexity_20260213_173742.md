---
ver: rpa2
title: Neural Networks Learn Statistics of Increasing Complexity
arxiv_id: '2402.04362'
source_url: https://arxiv.org/abs/2402.04362
tags:
- order
- statistics
- neural
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides compelling new evidence for the distributional
  simplicity bias (DSB), showing that neural networks first learn to exploit low-order
  statistics of data distributions, then progressively incorporate higher-order statistics
  as training proceeds. The authors introduce two operational criteria for identifying
  when networks rely on statistics up to order k: (1) grafting low-order statistics
  from one class onto another causes classification to shift accordingly, and (2)
  deleting higher-order statistics has minimal impact.'
---

# Neural Networks Learn Statistics of Increasing Complexity

## Quick Facts
- arXiv ID: 2402.04362
- Source URL: https://arxiv.org/abs/2402.04362
- Authors: Nora Belrose; Quintin Pope; Lucia Quirke; Alex Mallen; Xiaoli Fern
- Reference count: 40
- Key outcome: Neural networks learn low-order statistics first, then progressively incorporate higher-order statistics, validated through synthetic data experiments and language model analysis.

## Executive Summary
This work provides compelling new evidence for the distributional simplicity bias (DSB), showing that neural networks first learn to exploit low-order statistics of data distributions, then progressively incorporate higher-order statistics as training proceeds. The authors introduce two operational criteria for identifying when networks rely on statistics up to order k: (1) grafting low-order statistics from one class onto another causes classification to shift accordingly, and (2) deleting higher-order statistics has minimal impact. They validate these criteria empirically across image classification tasks and language models, observing a consistent "U-shaped" pattern where networks are most sensitive to low-order moments early in training, then become less sensitive as higher-order features emerge.

## Method Summary
The authors introduce a framework for operationalizing distributional simplicity bias through synthetic data generation with controlled statistical properties. They use maximum entropy sampling with various constraints (coordinatewise quantile normalization, bounded shift, Gaussian optimal transport) to create datasets that match only first-order moments, second-order moments, or higher-order statistics. Models are trained on real data while periodically evaluated on these synthetic datasets to track when different statistical orders become relevant. For language models, they extend this framework to discrete domains by proving an equivalence between token n-gram frequencies and moments of embedding vectors.

## Key Results
- Networks exhibit a U-shaped learning pattern, being most sensitive to low-order moments early in training, then becoming less sensitive as higher-order features emerge
- Language models show a "double descent" phenomenon where they initially mirror the U-shape, then leverage in-context learning to further reduce loss on low-order statistics later in training
- The distributional simplicity bias framework extends to discrete domains through an equivalence between token n-gram frequencies and moments of embedding vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Networks initially exploit low-order statistics (mean, covariance) because these dominate the early Taylor expansion of the loss function.
- Mechanism: At initialization, higher-order terms in the Taylor expansion of the loss are suppressed by factorial decay and small derivatives, so gradient updates primarily reduce error in matching low-order statistics first.
- Core assumption: The loss function is analytic and its higher-order derivatives at initialization are small relative to the factorial decay of higher-order moments.
- Evidence anchors:
  - [abstract] "if during training, a network's loss is well approximated by the first k terms of its Taylor expansion, then the model should only be sensitive to statistics up to order k"
  - [section 2.1] "Without loss of generality, assume the input is constrained to the unit hypercube. Since every coordinate of x is no greater than 1, the moments will have magnitudes that monotonically decrease with increasing order"
  - [corpus] Weak - no direct citations on Taylor expansion dominance in early training.

### Mechanism 2
- Claim: Grafting low-order statistics from one class onto another causes classification shift because the network's decision boundary is initially aligned with these statistics.
- Mechanism: Early in training, the network's learned representation is primarily driven by low-order statistics. When these are altered via optimal transport to match another class, the network's decision boundary shifts accordingly, causing misclassification.
- Core assumption: The network has not yet learned higher-order features that would override the influence of low-order statistics on classification.
- Evidence anchors:
  - [abstract] "early-training networks treat the edited samples as if they were drawn from the target class"
  - [section 2.3] "We evaluate whether the network satisfies criterion (1) by generating synthetic datasets where we 'graft' the means and covariances of one class onto images of another class"
  - [corpus] Weak - no direct citations on optimal transport for class statistics grafting.

### Mechanism 3
- Claim: Maximum entropy sampling with matching low-order statistics produces data that early networks classify accurately because these samples lack higher-order distinguishing features.
- Mechanism: By generating samples that match only the first k moments of the data distribution (and are otherwise maximum entropy), we create data that early networks cannot distinguish from real data, leading to high accuracy on these synthetic samples.
- Core assumption: The network's ability to classify depends primarily on low-order statistics early in training, and maximum entropy sampling effectively removes higher-order information.
- Evidence anchors:
  - [abstract] "networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later"
  - [section 2.4] "We can operationalize Criterion 2 using the principle of maximum entropy... we otherwise want to minimize the information content of the higher-order statistics"
  - [corpus] Weak - no direct citations on maximum entropy sampling for evaluating neural network learning.

## Foundational Learning

- Concept: Taylor series expansion and its application to loss functions
  - Why needed here: The entire DSB framework is motivated by expressing expected loss as a sum over central moments via Taylor expansion
  - Quick check question: What mathematical property of the loss function allows us to express it as a sum over central moments of the input distribution?

- Concept: Central moments and their relationship to statistical properties
  - Why needed here: The paper defines what "low-order statistics" means in terms of moments (mean, covariance, etc.) and how they relate to data distributions
  - Quick check question: How do first and second order moments differ from higher-order moments in terms of what statistical properties they capture?

- Concept: Optimal transport theory and its application to probability distributions
  - Why needed here: The paper uses optimal transport to "graft" statistics from one class onto another, which is central to their experimental methodology
  - Quick check question: What is the key property of the optimal transport map between two Gaussian distributions that makes it useful for this application?

## Architecture Onboarding

- Component map: Data generation pipeline (maximum entropy sampling with various constraints) -> Optimal transport implementation (Gaussian OT, CQN, bounded shift) -> Model training and evaluation framework -> Analysis tools for tracking learning curves and statistics

- Critical path: Generate synthetic data with controlled statistics → Train models on real data while periodically evaluating on synthetic data → Analyze accuracy patterns to determine learning order

- Design tradeoffs: Using high-dimensional covariance matrices requires significant memory but provides more precise control over second-order statistics; simpler first-order methods are faster but less informative

- Failure signatures: If learning curves show no U-shape pattern, if accuracy on synthetic data doesn't peak early, or if models classify synthetic data randomly regardless of statistics used

- First 3 experiments:
  1. Generate maximum entropy Gaussian samples matching CIFAR-10 means and covariances, evaluate early network accuracy
  2. Apply Gaussian optimal transport to graft class A statistics onto class B images, check if network classifies them as class B early in training
  3. Compare accuracy on Conrad distribution samples vs. independent coordinate sampling to isolate effects of first-order statistics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distributional simplicity bias apply to recurrent neural networks or transformers in the same way as observed for feedforward architectures?
- Basis in paper: [inferred] The paper focuses on image classification networks and autoregressive language models but does not systematically compare different architectures or investigate how architectural differences might affect the DSB.
- Why unresolved: The paper's experimental design emphasizes state-of-the-art architectures like ConvNeXt and Swin Transformers, but doesn't explicitly test whether simpler architectures like MLPs or RNNs exhibit the same learning patterns.
- What evidence would resolve it: Comparative experiments showing learning curves across different architectural families (feedforward, recurrent, transformer) on the same datasets would reveal whether DSB is architecture-dependent.

### Open Question 2
- Question: What is the exact mathematical relationship between the magnitude of higher-order derivatives and the network's sensitivity to higher-order statistics during training?
- Basis in paper: [explicit] The paper motivates DSB through Taylor expansion of expected loss and argues that higher derivatives become correlated with higher-order moments during training, but doesn't quantify this relationship precisely.
- Why unresolved: While the paper provides theoretical motivation and empirical evidence for DSB, it doesn't establish a precise mathematical formula connecting derivative magnitudes to statistical sensitivity.
- What evidence would resolve it: Detailed analysis of gradient magnitudes across different orders throughout training, combined with statistical sensitivity measurements, could establish quantitative bounds on this relationship.

### Open Question 3
- Question: How does the DSB interact with other learning biases like frequency bias or simplicity bias in parameter space?
- Basis in paper: [inferred] The paper discusses related work on simplicity biases and frequency principles but doesn't investigate potential interactions between these different biases.
- Why unresolved: The paper treats DSB as an independent phenomenon without examining whether it complements or conflicts with other known learning biases.
- What evidence would resolve it: Controlled experiments manipulating training data frequency distributions while measuring both frequency bias and DSB effects would reveal potential interactions between these mechanisms.

## Limitations
- The distributional simplicity bias framework relies heavily on the assumption that loss functions can be meaningfully approximated by low-order Taylor expansions during early training
- The optimal transport methods used for grafting statistics introduce approximations that could affect the interpretation of results, particularly for high-dimensional data
- The equivalence between discrete n-gram statistics and continuous moments is established only for specific embedding choices

## Confidence
- **High Confidence**: The U-shaped learning pattern observed across multiple architectures and datasets, the double descent phenomenon in language models, and the equivalence between n-grams and moments for specific embedding choices
- **Medium Confidence**: The causal relationship between low-order statistics and early network performance, the generalizability of the DSB framework to discrete domains, and the robustness of results across different synthetic data generation methods
- **Low Confidence**: The theoretical assumption that early training is dominated by low-order Taylor terms for all network architectures, the completeness of the equivalence between discrete and continuous statistics, and the interpretation of optimal transport grafting results in high dimensions

## Next Checks
1. Test the DSB framework on architectures with different inductive biases (e.g., transformers, MLPs, vision transformers) to verify that the U-shaped pattern is not specific to ConvNeXt/Swin architectures
2. Implement ablation studies where specific higher-order features are artificially introduced into synthetic datasets to determine whether networks can learn to ignore low-order statistics even when they are present
3. Conduct controlled experiments on simplified datasets (e.g., Gaussian mixtures, synthetic textures) where the ground truth statistics are known exactly, to verify that the grafting and sampling methods accurately isolate the intended statistical order