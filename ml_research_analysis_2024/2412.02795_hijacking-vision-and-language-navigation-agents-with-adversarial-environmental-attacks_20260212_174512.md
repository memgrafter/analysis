---
ver: rpa2
title: Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental
  Attacks
arxiv_id: '2412.02795'
source_url: https://arxiv.org/abs/2412.02795
tags:
- attack
- attacks
- agent
- object
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces adversarial attacks for Vision-and-Language
  Navigation (VLN) agents by optimizing the appearance of 3D objects in the environment.
  Using differentiable rendering, the method modifies object textures to influence
  agent behavior, causing them to ignore instructions and follow attacker-defined
  trajectories.
---

# Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks

## Quick Facts
- arXiv ID: 2412.02795
- Source URL: https://arxiv.org/abs/2412.02795
- Authors: Zijiao Yang; Xiangxi Shi; Eric Slyman; Stefan Lee
- Reference count: 40
- Key outcome: Environmental attacks can cause VLN agents to ignore instructions and follow attacker-defined trajectories by optimizing object textures

## Executive Summary
This work introduces adversarial attacks for Vision-and-Language Navigation (VLN) agents by optimizing the appearance of 3D objects in the environment. Using differentiable rendering, the method modifies object textures to influence agent behavior, causing them to ignore instructions and follow attacker-defined trajectories. Experiments on R2R and RxR datasets show that attacked agents are significantly more likely to stop prematurely or deviate from original paths, with success rates dropping from 82.42% to 53.85% on trajectory-following attacks. The attacks generalize to novel instructions and trajectories, demonstrating substantial disruption of VLN agent capabilities.

## Method Summary
The attack framework leverages differentiable rendering to optimize object textures in 3D mesh environments, directly affecting the visual inputs that the VLN agent receives at specific viewpoints. The method works by selecting attack viewpoints and objects, generating training episodes that pass through these points, rendering adversarial observations, computing loss between agent behavior and desired attack trajectory, and backpropagating gradients to update object texture. The approach uses whitebox access to the HAMT VLN model and ADAM optimizer with L∞ constraints to create robust attacks that generalize across different language instructions and navigation histories.

## Key Results
- Stop attacks cut success rate by nearly 35% on R2R and 57% on RxR test instances
- Trajectory-following attacks reduce success rates from 82.42% to 53.85%
- Attacks generalize to novel instructions and trajectories not seen during training
- Environmental attacks show 3.75× increase in success rate for RxR compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack works because it leverages differentiable rendering to optimize object textures in 3D mesh environments, directly affecting the visual inputs that the VLN agent receives at specific viewpoints.
- Mechanism: The framework renders adversarial observations at attack viewpoints using a differentiable renderer, which allows gradients to flow back to the object's texture in the 3D mesh. This creates a consistent visual perturbation that influences the agent's decision-making process when it encounters the attacked object.
- Core assumption: The VLN agent's multimodal reasoning can be disrupted by localized, consistent appearance modifications in its visual environment, and the agent will generalize this disruption to novel instructions and trajectories.
- Evidence anchors:
  - [abstract] "Using differentiable rendering, the method modifies object textures to influence agent behavior"
  - [section 3.1] "Given a textured 3D mesh representation of the environment, we apply the PYTORCH 3D [35] differentiable renderer to produce observations through which gradients can affect object textures"
  - [corpus] Weak - the corpus shows related work on adversarial attacks but doesn't specifically address differentiable rendering in VLN contexts

### Mechanism 2
- Claim: The attack generalizes to novel instructions and trajectories by optimizing over a diverse set of training episodes that pass through the attack viewpoint, forcing the agent to follow an attacker-defined trajectory after encountering the attack object.
- Mechanism: The optimization process maximizes the expected likelihood of the agent following the attack trajectory by supervising the agent's actions across multiple training episodes with different instructions and guide trajectories, creating a robust attack that works beyond the specific instances used during optimization.
- Core assumption: The VLN agent's decision-making process can be influenced by visual perturbations that generalize across different language instructions and navigation histories.
- Evidence anchors:
  - [abstract] "Experiments on R2R and RxR datasets show that attacked agents are significantly more likely to stop prematurely or deviate from original paths"
  - [section 3.1] "As we would like our attacks to generalize to new trajectory-instruction pairs, we consider a set of VLN episodes...for training the attack"
  - [corpus] Moderate - related work shows adversarial attacks can generalize across images and models, but generalization across language instructions in VLN is less established

### Mechanism 3
- Claim: The attack disrupts the agent's ability to follow original instructions by creating a strong visual signal that overrides the agent's language grounding, causing it to prioritize the visual perturbation over the instruction semantics.
- Mechanism: By optimizing the object's appearance to maximize the likelihood of following the attack trajectory, the attack creates a visual cue that the agent interprets as more relevant than the original instruction, effectively hijacking the agent's decision-making process.
- Core assumption: The VLN agent's visual processing has sufficient influence over its navigation decisions that a strong enough visual perturbation can override instruction-following behavior.
- Evidence anchors:
  - [abstract] "The proposed attack can cause VLN agents to ignore their instructions and execute alternative actions after encountering the attack object"
  - [section 4] "our attacks cut success rate by nearly 35% and 57% in Test instances for R2R and RxR respectively"
  - [corpus] Moderate - related work on adversarial attacks shows they can disrupt model behavior, but the specific claim about overriding language grounding in VLN needs more direct evidence

## Foundational Learning

- Concept: Differentiable rendering and gradient-based optimization
  - Why needed here: The attack relies on computing gradients through the rendering process to update object textures, which requires understanding how 3D scenes are rendered and how to propagate gradients through this process.
  - Quick check question: How does a differentiable renderer differ from a standard renderer, and why is this difference crucial for this attack?

- Concept: Multimodal learning and vision-language navigation
  - Why needed here: The VLN agent processes both visual observations and natural language instructions, and understanding how these modalities are combined is essential for understanding how the attack can disrupt the agent's behavior.
  - Quick check question: What are the key architectural components of a VLN agent, and how do they process visual and language inputs differently?

- Concept: Adversarial machine learning and attack transferability
  - Why needed here: The attack builds on principles from adversarial ML, including how to craft inputs that cause specific model behaviors and how attacks can generalize across different inputs.
  - Quick check question: What makes an adversarial attack transferable across different inputs, and how does this concept apply to the VLN setting?

## Architecture Onboarding

- Component map: 3D Environment (MP3D mesh + textures) -> Differentiable Renderer (PyTorch3D) -> VLN Agent (HAMT model) -> Attack Optimizer (Adam optimizer with gradient masking) -> Evaluation Pipeline (R2R/RxR datasets)

- Critical path: 1. Select attack viewpoint and object based on visibility criteria 2. Generate training episodes that pass through the attack viewpoint 3. Render adversarial observations at attack viewpoints 4. Compute loss between agent behavior and desired attack trajectory 5. Backpropagate gradients to update object texture 6. Evaluate attack on novel instruction-trajectory pairs

- Design tradeoffs:
  - Rendering fidelity vs. computational cost (decimated meshes, lower-res textures)
  - Attack strength vs. detectability (L∞ norm constraints)
  - Training time vs. attack generalization (more iterations improve performance)
  - Number of rendered steps vs. attack effectiveness (more steps improve performance)

- Failure signatures:
  - Attack converges but doesn't improve VLN performance (check loss function and supervision)
  - Attack improves performance on training episodes but not test episodes (check generalization and overfitting)
  - Attack causes visual artifacts that are easily detectable (check attack magnitude constraints)
  - Rendering errors or artifacts in adversarial observations (check mesh quality and renderer configuration)

- First 3 experiments:
  1. Single-step attack validation: Implement and test the stop attack to verify basic attack functionality before implementing full trajectory attacks
  2. Rendering pipeline validation: Test the differentiable rendering with a simple optimization task to ensure gradients flow correctly from rendered images to 3D textures
  3. Attack magnitude sensitivity: Vary the attack magnitude parameter ε to find the sweet spot between attack effectiveness and visual detectability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would environmental attacks be against more advanced VLN architectures beyond HAMT?
- Basis in paper: [explicit] The paper only tests attacks against HAMT, a representative VLN model, but notes that different architectures may respond differently to adversarial attacks
- Why unresolved: The study focuses solely on HAMT architecture, leaving open whether other models with different attention mechanisms or history encoding would be more or less vulnerable to these attacks
- What evidence would resolve it: Testing the same environmental attack framework against multiple VLN architectures (e.g., RecBert, PRESS, or newer transformer-based models) and comparing success rates across models

### Open Question 2
- Question: What is the minimum object size required for successful attacks in different environmental contexts?
- Basis in paper: [inferred] The paper analyzes object size effects but doesn't establish minimum thresholds for attack success across varying distances and viewing angles
- Why unresolved: While the paper examines size effects, it doesn't determine the practical minimum size requirements for attacks to work reliably in different navigation scenarios
- What evidence would resolve it: Systematic experiments varying object size across multiple distances, viewing angles, and environmental conditions to establish minimum viable attack parameters

### Open Question 3
- Question: How do environmental attacks transfer across different rendering qualities or real-world conditions?
- Basis in paper: [explicit] The paper acknowledges a domain gap between rendered and real images but doesn't test attack transferability to real-world scenarios or different rendering engines
- Why unresolved: The study only evaluates attacks in the specific rendering environment used for training, leaving open questions about real-world applicability and cross-rendering-engine transferability
- What evidence would resolve it: Testing attacks on multiple rendering engines, photorealistic simulations, and ideally real-world implementations to measure transferability and robustness to rendering variations

### Open Question 4
- Question: What is the relationship between instruction complexity and attack susceptibility in VLN agents?
- Basis in paper: [inferred] The paper uses instructions of varying complexity but doesn't systematically analyze how instruction length, complexity, or ambiguity affects vulnerability to environmental attacks
- Why unresolved: While RxR instructions are longer on average than R2R, the paper doesn't directly analyze how instruction characteristics impact attack success rates
- What evidence would resolve it: Controlled experiments varying instruction complexity (length, ambiguity, linguistic features) while measuring attack success rates to establish correlation patterns

## Limitations
- The attack requires access to 3D mesh models and differentiable rendering capabilities, which may not be available in real-world deployment scenarios
- The optimization process is computationally intensive, requiring rendering at each optimization step, which could limit scalability
- While attacks generalize to novel instructions and trajectories, the extent of this generalization across truly diverse real-world scenarios remains unclear

## Confidence
- **High confidence**: Core claims about disrupting VLN agents through environmental texture manipulation are well-supported by extensive experimental validation across multiple datasets and attack scenarios
- **Medium confidence**: Claims about attack generalization and computational requirements have moderate support but need further validation in diverse real-world conditions
- **Low confidence**: Claims about long-term robustness against defenses and practical detectability by human observers or automated systems need more thorough evaluation

## Next Checks
1. **Real-world transferability test**: Implement the attack on a subset of Matterport3D environments with varying lighting conditions and object textures to validate performance consistency across different visual contexts and test the claim about generalization to novel instructions.

2. **Computational efficiency benchmark**: Measure the actual wall-clock time and memory usage for the optimization process across different attack scenarios, and evaluate whether approximation techniques (like fewer rendering steps) maintain attack effectiveness while improving scalability.

3. **Detection robustness evaluation**: Conduct a human study where participants rate the visual naturalness of attacked objects, and implement simple detection methods (e.g., anomaly detection on texture statistics) to quantify how detectable these attacks are in practice, validating the claim that L∞ constraints maintain visual plausibility.