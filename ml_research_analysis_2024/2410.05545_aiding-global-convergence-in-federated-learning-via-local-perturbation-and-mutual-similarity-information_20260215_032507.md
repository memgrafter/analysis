---
ver: rpa2
title: Aiding Global Convergence in Federated Learning via Local Perturbation and
  Mutual Similarity Information
arxiv_id: '2410.05545'
source_url: https://arxiv.org/abs/2410.05545
tags:
- local
- algorithm
- round
- clients
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a federated learning framework that incorporates
  mutual client similarity information to improve convergence and generalization.
  The key idea is to define a similarity graph among clients based on their data distributions
  and use this information to perform perturbed local updates.
---

# Aiding Global Convergence in Federated Learning via Local Perturbation and Mutual Similarity Information

## Quick Facts
- arXiv ID: 2410.05545
- Source URL: https://arxiv.org/abs/2410.05545
- Reference count: 40
- Method achieves faster exponential contraction than FedAvg and FedProx for strongly convex losses by incorporating client similarity information

## Executive Summary
This paper proposes a federated learning framework that leverages mutual client similarity information to accelerate convergence. The key innovation is constructing a similarity graph among clients based on their data distributions and using this graph to perform perturbed local updates. Each client's update is computed in a shifted coordinate that minimizes local variability against neighboring clients, controlled by a perturbation parameter β. The method demonstrates theoretical improvements in convergence rates for strongly convex losses and shows empirical speedups of up to 30 rounds faster convergence on CIFAR10 and FEMNIST datasets, particularly in heterogeneous settings.

## Method Summary
The method constructs a federated network as a similarity graph where clients exchange messages encoding statistical information about their local datasets (first principal component). An adjacency matrix is built from client misalignment values, and each client performs perturbed gradient descent using information from statistically similar neighbors. The perturbed iterate is computed as a convex combination of the current iterate and a weighted average of neighboring clients' previous updates, controlled by parameter β. The framework achieves faster exponential contraction compared to FedAvg and FedProx by reducing client drift through similarity-based alignment, with theoretical guarantees for strongly convex losses.

## Key Results
- Achieves up to 30 rounds faster convergence compared to FedAvg on CIFAR10 and FEMNIST
- Theoretical analysis shows exponential contraction rate of (1-μ/(3L))ᵗ for β=1, improving to better rates with β<1
- Modest improvement in generalization performance, particularly in heterogeneous data settings
- Similarity graph construction based on PCA messages effectively captures client relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing local updates using weighted similarity information reduces the gap between client and global models, improving convergence.
- Mechanism: Each client's update is computed in a shifted coordinate ewi_t,k that minimizes local variability against neighboring clients. This coordinate is a convex combination of the current iterate wi_t,k and a weighted average of neighboring clients' last updates ui_t.
- Core assumption: Clients with similar data distributions will benefit from aligning their local updates toward each other.
- Evidence anchors:
  - [abstract]: "Specifically, each client's local update is computed in a shifted coordinate that minimizes the local variability against neighboring clients."
  - [section 3.3]: "The solution to this formulation minimizes the distance from the exact iterate wi_t,k as well as the local variation, namely the sum of squared deviations from the models of neighbors."
  - [corpus]: Weak evidence - related works focus on momentum and sharpness-aware methods but do not directly validate the similarity-based perturbation mechanism.
- Break condition: If client similarity graph is poorly constructed (e.g., noisy or incorrect statistical similarity), the perturbation may increase variance and slow convergence.

### Mechanism 2
- Claim: Introducing parameter β < 1 allows trade-off between convergence speed and stability.
- Mechanism: β controls the weight of the current iterate in the perturbed update. Smaller β increases the influence of neighboring information, potentially speeding convergence for heterogeneous data but increasing error term O(1/β²).
- Core assumption: For strongly convex losses, faster exponential contraction can be achieved with β < 1 compared to FedAvg.
- Evidence anchors:
  - [abstract]: "The method introduces a perturbation parameter β that controls the trade-off between convergence speed and stability."
  - [section 4.2]: "Curiously, this suggests that any choice of α > 0 would worsen the contraction rate for FEDPROX, making it inevitably larger than FEDAVG's one. On the other hand, by picking β < 1, would improve the contraction rate for our algorithm as it would be smaller than (1 − μ/(3L))ᵗ."
  - [corpus]: No direct evidence - related works focus on different perturbation strategies but don't analyze this specific β-controlled trade-off.
- Break condition: When β approaches 0, the O(1/β²) error term dominates, making convergence unstable and slow.

### Mechanism 3
- Claim: The similarity graph construction based on principal component analysis captures statistical relationships between clients.
- Mechanism: Each client shares the first principal component of its dataset as a message, and client misalignment is computed as the cosine distance between these messages. The adjacency matrix is built using logarithmic scaling of misalignment values.
- Core assumption: The first principal component adequately represents the statistical distribution of client data.
- Evidence anchors:
  - [section 3.1]: "For simplicity, we consider the first principal component of each client's local dataset as the message that encodes information about the statistical distribution of the data."
  - [appendix C.6]: "Indeed, by augmenting the class imbalance, we notably affect the misalignment distribution. Specifically, the mean of the distribution grows as we exponentially increase the class imbalance."
  - [corpus]: Weak evidence - related works use different similarity measures (e.g., label distributions) but don't validate PCA-based message construction.
- Break condition: If the first principal component doesn't capture the most relevant statistical variation, the similarity graph may misrepresent client relationships.

## Foundational Learning

- Concept: Strongly convex optimization and convergence rates
  - Why needed here: The theoretical analysis relies on strong convexity to prove faster exponential contraction compared to FedAvg and FedProx.
  - Quick check question: What is the difference between the contraction factors of FedAvg and the proposed method for strongly convex losses?

- Concept: Graph theory and spectral graph methods
  - Why needed here: The algorithm constructs a similarity graph among clients and uses spectral properties to define neighbor relationships.
  - Quick check question: How is the adjacency matrix constructed from client misalignment values?

- Concept: Federated learning heterogeneity and client drift
  - Why needed here: The method specifically addresses client drift by incorporating similarity information to align local updates.
  - Quick check question: What is client drift and how does the proposed perturbation mechanism reduce it?

## Architecture Onboarding

- Component map: Server -> Clients (message exchange) -> Server (similarity graph) -> Clients (perturbed updates) -> Server (aggregation) -> Server (broadcast)
- Critical path: 1) Server initializes model and sends to clients. 2) Clients compute similarity messages. 3) Server constructs similarity graph. 4) Each round: clients compute perturbed updates, send to server, server aggregates. 5) Server broadcasts updated model.
- Design tradeoffs: Higher perturbation (smaller β) speeds convergence but increases error; more sophisticated similarity measures could improve alignment but increase communication cost.
- Failure signatures: Oscillating accuracy curves, slow convergence despite small β, degraded performance on balanced datasets.
- First 3 experiments:
  1. Run with β=1 (FedAvg baseline) on balanced CIFAR10 to establish baseline performance.
  2. Run with β=0.5 on imbalanced FEMNIST to test convergence speed improvement.
  3. Run with varying E (local epochs) to assess robustness to different optimization schedules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the perturbation parameter β that balances convergence speed and stability across different federated learning scenarios?
- Basis in paper: [explicit] The paper shows that decreasing β improves convergence speed but also increases error term O(G2/β2), creating a trade-off between perturbation and stability.
- Why unresolved: The paper only tests β values of 0.5, 0.7, and 0.9, and shows that β = 0.5 provides the best convergence speed but with some instability. The optimal β likely depends on factors like dataset heterogeneity, local epoch count, and gradient norm magnitude.
- What evidence would resolve it: Systematic experiments varying β across a wider range (e.g., 0.1 to 0.9) on multiple datasets with different heterogeneity levels, measuring both convergence speed and final accuracy. Additionally, theoretical analysis deriving conditions for β that minimize the total error bound.

### Open Question 2
- Question: How does the choice of client message representation (currently first principal component) affect the performance of the similarity graph construction and subsequent algorithm convergence?
- Basis in paper: [explicit] The paper uses first principal component as messages to encode statistical information, but acknowledges this choice may have privacy implications and could be improved.
- Why unresolved: The paper only explores one message representation method and shows it works reasonably well, but doesn't investigate alternatives like k-means centroids, autoencoder embeddings, or differential privacy-preserving representations.
- What evidence would resolve it: Comparative experiments testing different message representations (PCA, k-means, autoencoders, DP-friendly methods) on the same datasets, measuring both convergence speed and privacy leakage. Analysis of how different representations affect graph structure and similarity weights.

### Open Question 3
- Question: Can the local variability-based framework be extended to non-Euclidean data spaces or federated learning scenarios with different optimization objectives?
- Basis in paper: [inferred] The current framework assumes Euclidean space and gradient-based optimization, but the concept of leveraging client similarity could apply more broadly.
- Why unresolved: The paper focuses on supervised learning with standard loss functions in Euclidean space. It doesn't explore applications to reinforcement learning, unsupervised learning, or non-Euclidean data like graphs or sequences.
- What evidence would resolve it: Implementations of the framework for different problem types (RL, unsupervised, graph-based) showing how similarity information can be incorporated. Theoretical analysis extending convergence guarantees to these broader settings.

## Limitations
- The theoretical analysis relies on strong convexity assumptions that may not hold for practical deep learning models
- Performance is sensitive to the choice of β, with small values potentially causing instability due to the O(1/β²) error term
- PCA-based similarity construction assumes the first principal component adequately captures statistical relationships, which may not generalize to complex data distributions

## Confidence
- Theoretical convergence claims (High): The proof structure follows standard federated optimization analysis and correctly identifies the trade-off between contraction rate and error terms for different β values.
- Empirical performance claims (Medium): While the method shows consistent improvements across datasets, the relative gains vary significantly with data heterogeneity, suggesting performance may be dataset-dependent.
- Similarity graph construction (Low): The use of PCA-based messages is a heuristic choice without extensive validation against alternative similarity measures or analysis of when this representation fails.

## Next Checks
1. Test the algorithm on non-convex deep learning models (e.g., CNNs on CIFAR10) to validate whether the theoretical convergence benefits extend beyond strongly convex settings.
2. Compare the PCA-based similarity messages against alternative similarity measures like label distribution overlap or Wasserstein distance to assess robustness to similarity representation choices.
3. Conduct ablation studies varying both β and the similarity graph construction method to quantify their individual contributions to convergence improvements.