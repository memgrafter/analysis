---
ver: rpa2
title: Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
  via LLM Agent
arxiv_id: '2407.21646'
source_url: https://arxiv.org/abs/2407.21646
tags:
- translation
- speech
- clasi
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CLASI, an end-to-end LLM-based system for simultaneous
  speech translation. The key innovation is a data-driven read-write policy that mimics
  professional interpreters by segmenting speech into semantic chunks before translating,
  eliminating the need for complex pre-defined policies.
---

# Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent

## Quick Facts
- arXiv ID: 2407.21646
- Source URL: https://arxiv.org/abs/2407.21646
- Authors: Shanbo Cheng; Zhichao Huang; Tom Ko; Hang Li; Ningxin Peng; Lu Xu; Qini Zhang
- Reference count: 40
- Primary result: Achieved human parity on simultaneous speech translation with 81.3% VIP for Chinese-English and 78.0% for English-Chinese, outperforming commercial systems (35.4% and 41.6% VIP respectively)

## Executive Summary
This paper presents CLASI, an end-to-end LLM-based system for simultaneous speech translation that achieves human parity performance. The key innovation is a data-driven read-write policy that segments speech into semantic chunks before translating, eliminating the need for complex pre-defined policies. CLASI employs a multi-modal retriever to incorporate domain-specific terminology from external knowledge bases and uses a three-stage training process to address data scarcity. The system is evaluated using a new human-centric metric called Valid Information Proportion (VIP), which measures the percentage of information successfully conveyed.

## Method Summary
CLASI follows a multi-stage training process: pretraining on massive in-house speech and text data, continual training on billions of tokens of synthesized speech translation data, and fine-tuning on human-annotated streaming ST data. The system uses an encoder-conditioned LLM architecture with a large-scale speech conformer, implements a data-driven read-write policy mimicking professional interpreters, and incorporates a multi-modal retriever for domain terminology. The training pipeline progressively aligns modalities and addresses data scarcity through synthetic data generation while maintaining the ability to handle real-world streaming scenarios.

## Key Results
- Achieved 81.3% VIP for Chinese-to-English and 78.0% for English-to-Chinese translation on RealSI benchmark
- Significantly outperformed commercial systems (35.4% and 41.6% VIP respectively)
- Human evaluations show CLASI achieves human parity performance in real-world simultaneous speech translation
- Synthetic training data achieved 81% VIP score, validating the data generation approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CLASI achieves human parity by learning a data-driven read-write policy that segments speech into semantic chunks before translating.
- **Mechanism**: The system uses a multi-stage training process to learn how professional human interpreters segment speech into complete semantic units before translation. This eliminates the need for complex pre-defined policies and allows deterministic output generation.
- **Core assumption**: Professional human interpreters' segmentation strategies can be learned and replicated by an LLM-based system through imitation learning from annotated speech data.
- **Evidence anchors**:
  - [abstract] "we utilize a novel data-driven read-write strategy to balance the translation quality and latency"
  - [section 2.3] "CLASI imitates their policies by waiting for complete semantic chunks"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- **Break condition**: If the segmentation annotations don't accurately capture professional interpreter strategies, or if the LLM cannot learn the segmentation patterns from the training data.

### Mechanism 2
- **Claim**: The Multi-Modal Retrieval Augmented Generation (MM-RAG) process enables CLASI to handle domain-specific terminology by retrieving relevant information from external knowledge bases in real-time.
- **Mechanism**: A multi-modal retriever extracts terminology from an external database based on audio input, and the retrieved information is appended to the LLM prompt for in-context learning. This allows the system to translate specialized terms that wouldn't be in standard training data.
- **Core assumption**: The retriever can accurately identify relevant terminology from the knowledge base based on audio input, and the LLM can effectively use this information for better translation.
- **Evidence anchors**:
  - [abstract] "CLASI employs a multi-modal retrieving module to obtain relevant information to augment the translation"
  - [section 2.5] "A small number of filtered items are incorporated into the prompt of CLASI agent for in-context learning"
  - [section 4.6.1] "Our MM-RAG retriever outperforms other open-source models by a large margin, achieving 91.3% vs. 26.0% for Top-10 retrieve accuracy"
- **Break condition**: If the retriever returns irrelevant or noisy information, or if the LLM cannot effectively incorporate the retrieved knowledge into its translation.

### Mechanism 3
- **Claim**: The three-stage training methodology addresses data scarcity and enables high-quality simultaneous speech translation.
- **Mechanism**: The system first independently pretrains LLM and audio encoder on large datasets, then aligns modalities through continual training with synthetic speech-text data, and finally fine-tunes with human-annotated data to mimic professional interpreters. This progressive approach maximizes learning efficiency.
- **Core assumption**: Synthetic data can effectively approximate real simultaneous speech translation scenarios for training purposes, and human-annotated fine-tuning data can capture the nuanced behaviors of professional interpreters.
- **Evidence anchors**:
  - [abstract] "we adopt a three-stage training methodology: pretraining, continual training, and fine-tuning"
  - [section 3] "Our CLASI follows a multi-stage training process: pretraining, multi-task continual training, and multi-task supervised fine-tuning"
  - [section 3.2] "The synthetic data achieves a VIP score of 81%"
- **Break condition**: If the synthetic data doesn't capture the complexity of real simultaneous translation, or if the human-annotated data is insufficient to capture professional interpreter behaviors.

## Foundational Learning

- **Concept**: Semantic chunking in speech translation
  - **Why needed here**: Understanding how to break continuous speech into meaningful units is crucial for simultaneous translation systems. This concept underlies CLASI's read-write policy and is essential for balancing latency and translation quality.
  - **Quick check question**: What distinguishes a semantic chunk from a simple sentence boundary in the context of simultaneous interpretation?

- **Concept**: Multi-modal retrieval and augmentation
  - **Why needed here**: The MM-RAG component requires understanding how to retrieve relevant information across different modalities (audio and text) and how to effectively incorporate this information into the translation process.
  - **Quick check question**: How does the audio-to-text retrieval approach differ from traditional text-to-text retrieval in terms of implementation challenges?

- **Concept**: In-context learning with LLMs
  - **Why needed here**: CLASI relies heavily on the LLM's ability to use retrieved information and historical context effectively through in-context learning, which is different from fine-tuning approaches.
  - **Quick check question**: What are the limitations of in-context learning compared to fine-tuning when incorporating domain-specific terminology?

## Architecture Onboarding

- **Component map**: Audio Encoder -> Multi-Modal Retriever (optional) -> LLM Agent (with Memory and Retrieved Knowledge) -> Output -> Memory Update

- **Critical path**: Audio → Audio Encoder → Multi-Modal Retriever (optional) → LLM Agent (with Memory and Retrieved Knowledge) → Output → Memory Update

- **Design tradeoffs**:
  - Using an encoder-conditioned LLM versus cascaded ASR+MT systems (latency vs. error propagation)
  - Real-time retrieval vs. pre-loading all terminology (computation overhead vs. coverage)
  - Synthetic training data vs. limited human-annotated data (scale vs. authenticity)
  - Determinism in output vs. potential for quality improvements through rewriting

- **Failure signatures**:
  - High VIP but poor automatic metrics: Indicates the system is conveying correct information but in a different way than reference translations
  - Low VIP with good automatic metrics: Suggests the system is translating accurately but missing key information or context
  - High latency with good quality: May indicate the segmentation policy is waiting too long for complete semantic chunks
  - Retrieval failures: Poor translation of domain-specific terms or terminology

- **First 3 experiments**:
  1. End-to-end pipeline test with synthetic data to verify basic functionality and latency measurements
  2. Retrieval accuracy test on the development set to ensure the MM-RAG component is working
  3. VIP calculation on a small human-annotated test set to validate the evaluation methodology before full-scale testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the latency-quality trade-off be further optimized for real-time applications?
- Basis in paper: [explicit] The paper discusses latency metrics (AL, LAAL, FLAL) and mentions that user studies suggest translation quality is more important than latency for practical SiST systems.
- Why unresolved: The paper acknowledges that current latency metrics may not be suitable for paragraph-level evaluation and suggests that more refined metrics are needed.
- What evidence would resolve it: Comparative studies using new latency metrics specifically designed for paragraph-level SiST, showing how different systems perform under various latency constraints while maintaining translation quality.

### Open Question 2
- Question: What is the impact of incorporating more modalities (e.g., end-to-end speech-to-speech or video-to-video generation) on SiST performance?
- Basis in paper: [explicit] The paper mentions that incorporating more modalities, such as end-to-end speech-to-speech generation or video-to-video generation, are promising research topics.
- Why unresolved: The paper focuses on audio-to-text translation and does not explore the potential benefits or challenges of incorporating additional modalities.
- What evidence would resolve it: Experimental results comparing SiST performance with and without additional modalities, demonstrating improvements in translation quality, latency, or user experience.

### Open Question 3
- Question: How can the CLASI system be adapted to support more languages, especially low-resource languages?
- Basis in paper: [explicit] The paper acknowledges that the current implementation focuses on Chinese-to-English and English-to-Chinese tasks, and more languages should be considered in the future. It also mentions that neglecting some low-resource languages may bring unfairness to some minorities.
- Why unresolved: The paper does not provide specific strategies or results for extending the system to support additional languages.
- What evidence would resolve it: Successful implementation and evaluation of the CLASI system on multiple language pairs, including low-resource languages, demonstrating its effectiveness and fairness across different language communities.

## Limitations
- The proprietary nature of training data makes independent verification difficult
- VIP metric relies on human annotations that may have subjective variations
- Performance on 10-minute speeches may not generalize to longer conversations
- Limited language coverage (Chinese-English only)

## Confidence
- **High confidence**: The three-stage training methodology and its rationale is well-supported by literature
- **Medium confidence**: The claim of achieving human parity is supported by VIP results but human evaluation methodology has inherent subjectivity
- **Low confidence**: Specific implementation details of the read-write policy are not fully transparent

## Next Checks
1. **Independent replication test**: Attempt to replicate VIP score improvements on a subset of RealSI benchmark using open-source speech translation models with similar architectural components
2. **Robustness evaluation**: Test CLASI on speech data with varying characteristics (different speakers, accents, speaking rates, and noise levels)
3. **Ablation study on MM-RAG**: Conduct controlled experiment comparing performance with and without multi-modal retrieval component on domain-specific terminology