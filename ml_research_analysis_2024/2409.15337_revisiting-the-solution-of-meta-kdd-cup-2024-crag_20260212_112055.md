---
ver: rpa2
title: 'Revisiting the Solution of Meta KDD Cup 2024: CRAG'
arxiv_id: '2409.15337'
source_url: https://arxiv.org/abs/2409.15337
tags:
- information
- question
- domain
- apis
- mock
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the APEX team''s solution for the Meta KDD
  CUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark evaluates
  Retrieval-Augmented Generation (RAG) systems across diverse domains and dynamic
  question types.'
---

# Revisiting the Solution of Meta KDD Cup 2024: CRAG

## Quick Facts
- arXiv ID: 2409.15337
- Source URL: https://arxiv.org/abs/2409.15337
- Reference count: 15
- Primary result: APEX team ranked 2nd for Task 2&3 on CRAG leaderboard with routing-based domain and dynamic adaptive RAG pipeline

## Executive Summary
This paper presents the APEX team's solution for the Meta KDD CUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark evaluates Retrieval-Augmented Generation systems across diverse domains and dynamic question types. The team proposes a routing-based domain and dynamic adaptive RAG pipeline that performs specific processing for diverse and dynamic questions across retrieval, augmentation, and generation stages. Their method achieved superior performance on CRAG, ranking 2nd for Task 2&3 on the final competition leaderboard.

## Method Summary
The solution incorporates two specialized routers (Domain and Dynamism routers), a multi-stage retrieval pipeline with HTML parsing and ranking mechanisms, entity matching and time information extraction for Mock APIs, domain-specific information integration, and generation techniques including Chain-of-Thought reasoning and in-context learning. The system demonstrated significant improvements over baseline approaches with enhanced accuracy, reduced hallucination rates, and better information retention across all three tasks.

## Key Results
- Achieved 2nd place ranking on Task 2&3 of CRAG competition leaderboard
- Significant improvements over baseline approaches with enhanced accuracy
- Reduced hallucination rates and better information retention across all three tasks
- Processing time of 5.96s for Task 3 with multi-stage retrieval vs 68.17s without pre-ranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Domain Router effectively routes questions to the correct domain-specific processing pipeline
- Mechanism: The Domain Router is a sentence classifier that categorizes questions into finance, sports, music, movie, or open domains, enabling domain-specific API selection and prompt customization
- Core assumption: Domain-specific processing improves retrieval and generation accuracy for domain-specific questions
- Evidence anchors:
  - [abstract]: "We propose a routing-based domain and dynamic adaptive RAG pipeline that performs specific processing for diverse and dynamic questions across retrieval, augmentation, and generation stages"
  - [section]: "We utilize Llama3-8B-Instruct [2] as our base model and enhance it with a classification head (Multilayer Perceptron, MLP) for domain classification"
  - [corpus]: Weak evidence - the corpus contains related papers about RAG solutions but doesn't specifically validate domain router effectiveness
- Break condition: If domain classification accuracy falls below threshold, wrong domain processing pipelines would be triggered, leading to irrelevant API calls and incorrect responses

### Mechanism 2
- Claim: The Dynamism Router reduces hallucinations by identifying temporal nature of questions and applying appropriate post-processing
- Mechanism: The Dynamism Router classifies questions as static, slow-changing, fast-changing, or real-time, then applies post-processing rules like "I don't know" responses for dynamic questions without real-time APIs
- Core assumption: LLMs are prone to hallucinations on dynamic questions due to their static training data, and refusing to answer such questions reduces hallucination rates
- Evidence anchors:
  - [abstract]: "Our method achieved superior performance on CRAG and ranked 2nd for Task 2&3 on the final competition leaderboard"
  - [section]: "In the absence of real-time APIs, the more rapidly a question changes over time, the more susceptible LLMs are to hallucinations"
  - [corpus]: Weak evidence - while the corpus contains papers about hallucination mitigation, it doesn't specifically validate dynamism-based post-processing
- Break condition: If dynamism classification is inaccurate, either too many valid questions would be refused or too many dynamic questions would be answered with outdated information

### Mechanism 3
- Claim: Multi-stage retrieval with HTML parsing, pre-ranking, and re-ranking improves information quality for generation
- Mechanism: The system parses HTML to extract text, pre-ranks documents (Task 3 only) to filter noise, performs dense retrieval with embeddings, then re-ranks top candidates before generation
- Core assumption: Sequential refinement of retrieved documents improves the relevance and quality of information fed to the generator
- Evidence anchors:
  - [section]: "We conducted experiments with various HTML parsing methods... After evaluating both parsing efficiency and quality, we ultimately selected Newspaper"
  - [section]: "We utilized bge-m3-v2-reranker [6] to re-rank the aforementioned 10 relevant chunks, ultimately selecting the top 5 segments"
  - [corpus]: Weak evidence - the corpus contains related retrieval papers but doesn't specifically validate the multi-stage approach described
- Break condition: If any retrieval stage fails (e.g., poor HTML parsing, ineffective pre-ranking), the quality of retrieved information would degrade, leading to lower generation accuracy

## Foundational Learning

- Concept: Sentence classification for routing
  - Why needed here: The system needs to categorize questions by domain and dynamism to apply appropriate processing pipelines
  - Quick check question: How does the Domain Router differ from the Dynamism Router in terms of what they classify and why?
- Concept: Multi-stage information retrieval
  - Why needed here: The system must handle large amounts of potentially noisy information and extract the most relevant content for generation
  - Quick check question: What is the purpose of pre-ranking in Task 3, and why isn't it used in Tasks 1 and 2?
- Concept: Entity extraction and matching for API calls
  - Why needed here: The system must convert natural language entities from questions into the specific parameter formats required by different APIs
  - Quick check question: Why is exact matching attempted before using BM25 similarity for entity matching?

## Architecture Onboarding

- Component map:
  - Domain Router (sentence classifier for domain categorization) → Dynamism Router (sentence classifier for temporal categorization) → Web Retriever (HTML parsing → pre-ranking → dense retrieval → re-ranking) → API Extractor (NER → Entity Match → Time Info Extract → API Select → JSON to Markdown) → Generator (Chain-of-Thought + In-context Learning + Post-processing)
- Critical path: Question → Domain Router → Web Retriever/API Extractor → Augmentation → Generator → Post-processing
- Design tradeoffs:
  - Using "I don't know" for dynamic questions reduces hallucinations but may miss some valid answers
  - Multi-stage retrieval improves quality but increases latency (Task 3: 5.96s vs 68.17s without pre-ranking)
  - Manual API selection rules are simple but may not scale to larger API sets
- Failure signatures:
  - Low accuracy with high missing rate: Domain Router misclassifications
  - High hallucination rate: Dynamism Router failures or missing post-processing
  - Slow response times: Inefficient HTML parsing or retrieval stages
  - Low scores on Task 3: Pre-ranking stage not effectively filtering noise
- First 3 experiments:
  1. Test Domain Router classification accuracy on a sample of questions from each domain
  2. Compare retrieval quality with and without the multi-stage retrieval pipeline on Task 3 questions
  3. Evaluate the impact of post-processing rules by running questions through the system with and without the "I don't know" responses for dynamic questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model's cognitive ability to recognize its knowledge boundaries be accurately assessed?
- Basis in paper: Explicit - The paper mentions that most conventional QA evaluation methods primarily focus on accuracy, neglecting the impact of hallucinations. It suggests exploring the assessment of models' cognitive abilities using methodologies for evaluating human cognition.
- Why unresolved: The paper acknowledges the need for better assessment of model cognitive abilities but does not provide a specific methodology or framework for doing so. It only mentions that CRAG incorporates hallucinations into evaluation metrics but finds the settings insufficient.
- What evidence would resolve it: Development and validation of a comprehensive framework for evaluating model cognitive abilities, potentially drawing from human cognition assessment methodologies, that can be applied to RAG systems.

### Open Question 2
- Question: How can a more universal method for selecting and calling APIs be developed for real-world scenarios with extensive API usage?
- Basis in paper: Explicit - The paper states that in real-world scenarios with extensive API usage, manually designed matching rules are likely to prove inadequate. It suggests the development of a more universal method for selecting and calling APIs as a promising avenue for future research.
- Why unresolved: The paper acknowledges the limitation of manually designed rules for API selection and calls for a more universal method but does not propose or explore specific approaches to achieve this.
- What evidence would resolve it: Demonstration of a scalable, generalizable method for API selection and invocation that can handle diverse query types and domains without extensive manual rule design.

### Open Question 3
- Question: How can techniques for real-time information retrieval and verification be developed to handle dynamic information in questions?
- Basis in paper: Explicit - The paper discusses the challenge of handling dynamic information in questions and suggests that future research should focus on exploring methods to acquire the most up-to-date knowledge and determine the timeliness of information. It emphasizes the need for techniques to avoid hallucinations caused by outdated knowledge.
- Why unresolved: While the paper identifies the importance of handling dynamic information and the limitations of current approaches, it does not provide specific techniques or methods for real-time information retrieval and verification.
- What evidence would resolve it: Development and validation of robust techniques for real-time information retrieval and verification, including mechanisms to assess the reliability and currency of data sources, that can be integrated into RAG systems to improve handling of dynamic questions.

## Limitations

- The paper lacks detailed specifications for critical components, particularly the domain-specific API selection rules and the exact few-shot examples used in in-context learning
- The solution's performance metrics are reported relative to baselines without absolute numbers, making independent validation difficult
- The system's reliance on manual rules for API selection may not scale well to larger or more diverse API sets

## Confidence

- **High Confidence**: The general architecture and multi-stage pipeline design, as these are clearly specified and logically coherent
- **Medium Confidence**: The effectiveness of domain and dynamism routers, since the paper reports competitive leaderboard rankings but lacks detailed ablation studies
- **Low Confidence**: The specific implementation details of API selection rules and in-context learning examples, which are only vaguely described

## Next Checks

1. **Domain Router Validation**: Test the domain classification accuracy on a sample of 100 questions across all five domains to verify it meets the implied high performance threshold
2. **Retrieval Pipeline Effectiveness**: Compare retrieval quality with and without the multi-stage pipeline on 20 Task 3 questions to quantify the impact of pre-ranking and re-ranking
3. **Post-processing Impact**: Run 50 dynamic questions through the system with and without the "I don't know" post-processing to measure hallucination reduction versus information retention tradeoff