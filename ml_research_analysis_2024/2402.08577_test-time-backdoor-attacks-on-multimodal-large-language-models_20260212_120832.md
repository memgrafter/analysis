---
ver: rpa2
title: Test-Time Backdoor Attacks on Multimodal Large Language Models
arxiv_id: '2402.08577'
source_url: https://arxiv.org/abs/2402.08577
tags:
- attack
- attacks
- backdoor
- adversarial
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates test-time backdoor attacks on multimodal
  large language models (MLLMs) using adversarial image perturbations to inject harmful
  behaviors. By applying universal adversarial perturbations to input images, the
  attack decouples setup and activation of backdoors, enabling dynamic modification
  of trigger prompts and harmful effects.
---

# Test-Time Backdoor Attacks on Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2402.08577
- Source URL: https://arxiv.org/abs/2402.08577
- Reference count: 40
- The study demonstrates test-time backdoor attacks on multimodal large language models (MLLMs) using adversarial image perturbations to inject harmful behaviors

## Executive Summary
This paper introduces a novel test-time backdoor attack on multimodal large language models (MLLMs) that decouples the setup and activation of harmful effects across different modalities. By applying universal adversarial perturbations to input images, the attack can dynamically modify trigger prompts and harmful effects without requiring training data access. Experiments show high success rates (87-96%) across multiple popular MLLMs including LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, while maintaining benign accuracy on standard tasks.

The attack strategy, termed "AnyDoor," leverages the distinct characteristics of visual and textual modalities - using visual modality for strong manipulation capacity during setup and textual modality for timely activation during inference. The border attack strategy proved most effective, achieving ExactMatch scores up to 98.5% and BLEU@4 scores up to 79.8%. The attack remains effective under common corruptions and against different model architectures, revealing new challenges for backdoor defense in MLLMs.

## Method Summary
The attack works by optimizing universal adversarial perturbations that are applied to input images during inference. These perturbations simultaneously affect both visual and textual modalities of MLLMs, allowing attackers to set up harmful behaviors without modifying the training process. The optimization uses 500-step PGD with momentum (μ=0.9) and frequency-domain augmentation, with balanced loss weights (w1=w2=1.0). Different attack strategies (Pixel, Corner, Border) vary the perturbation budget and application method. The attack can be dynamically adapted by re-optimizing perturbations for different trigger-target pairs without retraining the model.

## Key Results
- Attack success rates of 87-96% across multiple MLLMs (LLaVA-1.5, MiniGPT-4, InstructBLIP, BLIP-2)
- Border attack strategy achieved highest effectiveness with ExactMatch scores reaching 98.5%
- Attack maintains benign accuracy on standard tasks while injecting harmful behaviors
- Attack remains effective under common image corruptions and against different model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling setup and activation of backdoor effects across modalities enables stronger attacks
- Mechanism: Visual modality (higher capacity) is used for setup via adversarial perturbations, while textual modality (higher timeliness) is used for activation via trigger prompts
- Core assumption: Different modalities have distinct characteristics in terms of manipulating capacity and timeliness
- Evidence anchors:
  - [abstract]: "Our test-time backdoor attacks could modify predetermined trigger prompts or harmful effects by merely altering the adversarial perturbation"
  - [section]: "Setting up harmful effects necessitates strong manipulating capacity...Activating harmful effects, on the other hand, requires strong manipulating timeliness"
  - [corpus]: Weak - no direct evidence in corpus

### Mechanism 2
- Claim: Universal adversarial perturbations can simultaneously affect both visual and textual modalities
- Mechanism: Optimized perturbations are applied to visual input but influence model's textual output through multimodal integration
- Core assumption: MLLMs process multimodal inputs in an integrated manner where visual perturbations can affect language generation
- Evidence anchors:
  - [abstract]: "injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation)"
  - [section]: "we can apply a universal adversarial perturbation to input images, allowing us to set up a backdoor into the textual modality"
  - [corpus]: Weak - no direct evidence in corpus

### Mechanism 3
- Claim: Re-optimizing universal perturbations enables dynamic adaptation of triggers and harmful effects
- Mechanism: Same adversarial pattern can be recalibrated for different trigger-target pairs without retraining the model
- Core assumption: Adversarial perturbations are independent of specific trigger text and can be optimized for different objectives
- Evidence anchors:
  - [abstract]: "AnyDoor can dynamically change its backdoor trigger prompts/harmful effects"
  - [section]: "it is possible to re-optimize a new A to efficiently adapt to any changes in T and Aharm"
  - [corpus]: Weak - no direct evidence in corpus

## Foundational Learning

- Concept: Universal adversarial perturbations
  - Why needed here: Forms the basis for test-time backdoor setup without training data access
  - Quick check question: What makes an adversarial perturbation "universal" versus instance-specific?

- Concept: Multimodal model architecture
  - Why needed here: Understanding how visual and textual modalities interact is crucial for attack design
  - Quick check question: How do typical MLLMs like LLaVA-1.5 integrate visual and language representations?

- Concept: Backdoor attack mechanisms
  - Why needed here: Distinguishing test-time from traditional training-time backdoors is essential for this work
  - Quick check question: What is the key difference between training-time and test-time backdoor attacks?

## Architecture Onboarding

- Component map: Visual encoder → Cross-modal fusion → Language decoder → Output generation
- Critical path: Input image → Adversarial perturbation → Visual features → Language model → Triggered response
- Design tradeoffs: Trade-off between perturbation invisibility and attack effectiveness; trade-off between sample size and generalization
- Failure signatures: Benign accuracy drops without trigger; attack success rates fall below threshold with trigger
- First 3 experiments:
  1. Test border attack with minimal perturbation on VQAv2 dataset with default trigger
  2. Vary perturbation budget and measure ExactMatch vs BLEU trade-off
  3. Test different trigger placements (start, middle, end of question) with same perturbation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanisms by which multimodal large language models (MLLMs) distinguish and process different modalities (visual vs. textual) during test-time backdoor attacks?
- Basis in paper: [explicit] The paper discusses the strategic assignment of setup and activation of harmful effects to different modalities based on their characteristics, but does not detail the underlying mechanisms.
- Why unresolved: The paper focuses on the effectiveness of the attack rather than the internal workings of MLLMs in processing multimodal inputs during such attacks.
- What evidence would resolve it: Detailed analysis or experiments showing how MLLMs internally process and respond to adversarial perturbations in different modalities during backdoor attacks.

### Open Question 2
- Question: How do test-time backdoor attacks on MLLMs compare in effectiveness and stealth to traditional training-time backdoor attacks?
- Basis in paper: [inferred] The paper introduces test-time backdoor attacks as a novel method, suggesting potential differences in effectiveness and stealth compared to traditional methods, but does not provide a direct comparison.
- Why unresolved: The paper does not include comparative experiments or analyses between test-time and training-time backdoor attacks.
- What evidence would resolve it: Experimental results comparing the success rates, stealthiness, and detection difficulty of test-time versus training-time backdoor attacks on MLLMs.

### Open Question 3
- Question: What are the long-term implications of test-time backdoor attacks on the security and reliability of MLLMs in real-world applications?
- Basis in paper: [inferred] The paper highlights new challenges for defending against backdoor attacks, implying potential long-term security concerns, but does not explore these implications in depth.
- Why unresolved: The paper focuses on demonstrating the attack's viability and effectiveness, without addressing broader security and reliability concerns in practical deployments.
- What evidence would resolve it: Studies or case analyses showing the impact of test-time backdoor attacks on MLLMs' security and reliability in various real-world scenarios over time.

## Limitations

- Attack effectiveness varies significantly across different datasets and model architectures
- Study lacks comprehensive analysis of cross-model transferability
- Proposed defense mechanism is mentioned but not thoroughly evaluated
- Attack success rates drop under common image corruptions and with complex target phrases

## Confidence

**High Confidence**: The fundamental mechanism of decoupling setup and activation across modalities is well-supported by experimental results across multiple MLLM architectures and datasets.

**Medium Confidence**: The claim about dynamic adaptation of triggers and harmful effects through re-optimization is supported but could benefit from more extensive validation.

**Low Confidence**: The assertion that this attack remains effective against all common corruption types is not fully substantiated, as the study only tests a limited set of corruptions.

## Next Checks

1. **Cross-model transferability**: Test whether adversarial perturbations optimized for LLaVA-1.5 can successfully attack MiniGPT-4, InstructBLIP, and BLIP-2 without re-optimization, measuring exact match scores across all models.

2. **Defense robustness**: Implement and evaluate the proposed defense mechanism against the border attack strategy, measuring both attack success rate and benign accuracy degradation across all tested datasets.

3. **Real-world corruption resilience**: Test attack effectiveness under a broader range of image corruptions including JPEG compression, blur, and contrast changes, comparing success rates with and without the frequency-domain augmentation mentioned in the methodology.