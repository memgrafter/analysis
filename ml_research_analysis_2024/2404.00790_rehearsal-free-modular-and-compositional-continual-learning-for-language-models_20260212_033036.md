---
ver: rpa2
title: Rehearsal-Free Modular and Compositional Continual Learning for Language Models
arxiv_id: '2404.00790'
source_url: https://arxiv.org/abs/2404.00790
tags:
- task
- learning
- tasks
- mocl
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoCL, a modular and compositional continual
  learning framework for language models. MoCL addresses catastrophic forgetting and
  enables knowledge transfer by adding task-specific parameter-efficient modules and
  composing them via task matching weights.
---

# Rehearsal-Free Modular and Compositional Continual Learning for Language Models

## Quick Facts
- **arXiv ID**: 2404.00790
- **Source URL**: https://arxiv.org/abs/2404.00790
- **Reference count**: 25
- **Primary result**: MoCL is a modular and compositional continual learning framework that outperforms state-of-the-art methods on both near-domain and far-domain benchmarks.

## Executive Summary
This paper introduces MoCL, a modular and compositional continual learning framework for language models. MoCL addresses catastrophic forgetting and enables knowledge transfer by adding task-specific parameter-efficient modules and composing them via task matching weights. The framework achieves significant improvements over baselines like ProgPrompt and EPI on both near-domain and far-domain benchmarks, while also performing well in challenging class-incremental settings without task identities during testing.

## Method Summary
MoCL operates by adding task-specific parameter-efficient modules to a frozen pre-trained language model and composing these modules using task matching weights. The framework consists of a pre-trained language model frozen during training, task-specific adapters or modules that are added incrementally, and a compositional mechanism that combines task modules based on task matching weights. This approach allows MoCL to maintain previously learned knowledge while effectively transferring relevant information from related tasks, all without requiring rehearsal or access to previous data.

## Key Results
- MoCL outperforms state-of-the-art methods like ProgPrompt and EPI on both near-domain and far-domain continual learning benchmarks
- Achieves significant improvements in class-incremental settings without task identities during testing
- Demonstrates strong forward transfer capability across diverse continual learning tasks through effective balance of knowledge interference and transfer

## Why This Works (Mechanism)
MoCL's effectiveness stems from its ability to dynamically compose task-specific modules based on learned task matching weights. By freezing the pre-trained language model and only adding task-specific modules, it prevents catastrophic forgetting of previously learned knowledge. The compositional mechanism allows for knowledge transfer by combining relevant task modules when encountering new tasks that share similarities with previously learned ones. The sparse task matching weights ensure that only relevant task modules are activated, preventing interference from unrelated tasks while maintaining modularity and scalability.

## Foundational Learning
1. **Catastrophic Forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks. Critical for understanding why continual learning requires special techniques to preserve knowledge.
   - Quick check: Compare performance on previous tasks before and after training on new tasks

2. **Parameter-Efficient Modules**: Lightweight neural network components that can be added to existing models to adapt them to new tasks without modifying the original parameters. Essential for scalable continual learning.
   - Quick check: Measure parameter count increase per task versus performance gain

3. **Task Matching and Composition**: The process of identifying task similarities and combining relevant task-specific modules to handle new tasks. Enables knowledge transfer between related tasks.
   - Quick check: Analyze task matching weight distributions across different task pairs

## Architecture Onboarding

**Component Map**: Pre-trained LM (frozen) -> Task Modules (added incrementally) -> Task Matching Weights -> Composed Output

**Critical Path**: Input text → Pre-trained LM → Activated Task Modules (via task matching) → Composed representation → Task-specific head → Output

**Design Tradeoffs**: 
- Frozen LM preserves knowledge but limits adaptation
- Task modules enable specialization but increase parameter count
- Task matching enables transfer but requires similarity computation

**Failure Signatures**: 
- Catastrophic forgetting indicates frozen LM insufficient
- Poor performance suggests inadequate task module capacity
- Confusion between tasks indicates task matching weight issues

**3 First Experiments**:
1. Single-task adaptation to verify module effectiveness
2. Two-task sequential learning to test forgetting prevention
3. Task matching weight visualization to validate compositional mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on language modeling tasks with limited exploration of other modalities
- Compositional mechanism introduces complexity in module management and task identification
- Reliance on task matching weights raises scalability concerns for large numbers of tasks or highly similar tasks

## Confidence

**High Confidence**: The modular architecture and compositional approach are technically sound and well-implemented. The experimental methodology is rigorous, with appropriate baselines and metrics.

**Medium Confidence**: Claims regarding catastrophic forgetting prevention and knowledge transfer are supported by experiments but would benefit from longer-term continual learning scenarios and more diverse task distributions.

**Medium Confidence**: Assertion that MoCL performs well in class-incremental settings without task identities during testing is demonstrated but could be further validated with more challenging scenarios and larger task spaces.

## Next Checks

1. Evaluate MoCL's performance on a larger benchmark with hundreds of tasks to assess scalability and the effectiveness of task matching in high-dimensional task spaces.

2. Conduct ablation studies to quantify the individual contributions of the modular architecture versus the compositional mechanism to overall performance.

3. Test MoCL's robustness to noisy or incomplete task matching weights, simulating scenarios where task identification is uncertain or distributed across multiple potential tasks.