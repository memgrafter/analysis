---
ver: rpa2
title: Benchmarking Data Heterogeneity Evaluation Approaches for Personalized Federated
  Learning
arxiv_id: '2410.07286'
source_url: https://arxiv.org/abs/2410.07286
tags:
- data
- learning
- clients
- should
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks six representative approaches for evaluating
  data heterogeneity in personalized federated learning (PFL). The authors compare
  JS divergence, C-divergence, distribution sketch-based Euclidean distance, Shapley
  Value, Hypernetworks, and cosine similarity under five standard non-IID settings
  across eight datasets.
---

# Benchmarking Data Heterogeneity Evaluation Approaches for Personalized Federated Learning

## Quick Facts
- arXiv ID: 2410.07286
- Source URL: https://arxiv.org/abs/2410.07286
- Reference count: 40
- Primary result: No single best heterogeneity evaluation approach exists; optimal method depends on specific non-IID setting and data type

## Executive Summary
This paper benchmarks six representative approaches for evaluating data heterogeneity in personalized federated learning across five standard non-IID settings on eight datasets. The authors find that different approaches excel under different conditions - pFedGraph performs best for label distribution skew on image data while CE excels for tabular data, RACE is most efficient for feature distribution skew, and pFedGraph/pFedJS lead for quantity skew on different data types. The study reveals that no universal approach exists and provides guidance for selecting appropriate heterogeneity evaluation methods based on specific FL scenarios.

## Method Summary
The study evaluates six PFL heterogeneity evaluation approaches (JS divergence, C-divergence, RACE sketches, Shapley Value, Hypernetworks, and cosine similarity) across five non-IID settings (#C=1,2,3, pk~Dir(0.5), ˆ x~Gau(0.1), q~Dir(0.5)) using eight datasets (MNIST, CIFAR-10, FMNIST, SVHN, FEMNIST, adult, rcv1, covtype). Each client trains for 10 epochs per communication round with batch size 64 over 50 rounds. The unified framework measures test accuracy, computation time, and communication overhead to compare approach performance systematically.

## Key Results
- No single heterogeneity evaluation approach universally outperforms others across all settings
- For label distribution skew: pFedGraph excels on image data while CE is best for tabular data
- For feature distribution skew: RACE provides the best efficiency-accuracy tradeoff
- For quantity skew: pFedGraph leads on image data and pFedJS on tabular data
- RACE is the most computationally efficient approach overall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Distribution statistical divergence-based approaches guide collaboration by quantifying data similarity between clients.
- **Mechanism**: The approaches measure statistical distances between clients' data distributions (e.g., JS divergence, C-divergence, RACE sketches) to inform aggregation weights in federated learning. This enables clients to select optimal collaborators based on data heterogeneity.
- **Core assumption**: The statistical divergence between data distributions correlates with the benefit of collaboration for personalized model training.
- **Evidence anchors**:
  - [abstract] "Such measurements are used to estimate the suitability for collaborative training of personalized federated learning (PFL) models."
  - [section 3.1] "In FL, the local model updates are uploaded to the central server, which performs a weighted aggregation of these updates... The divergence of data distributions among clients can guide more informed decisions on the choice of the aggregation weights."
  - [corpus] Weak - corpus papers focus on personalization techniques but don't directly address statistical divergence measurement.
- **Break condition**: When statistical divergence doesn't correlate with actual collaboration benefit, or when high-dimensional data makes divergence estimation computationally prohibitive.

### Mechanism 2
- **Claim**: Model-based approaches use collaboration performance to guide client selection in federated learning.
- **Mechanism**: Techniques like Shapley Value and cosine similarity measure collaboration advantage based on model performance or parameter similarity, updating collaboration weights dynamically during training.
- **Core assumption**: The performance or similarity of models trained on different clients' data reflects the benefit of collaboration.
- **Evidence anchors**:
  - [section 3.2] "In machine learning, the data Shapley value of client i is defined... In FL, the utility evaluation function V(S) is based on the model performance achieved by the participation of S."
  - [abstract] "model-based approaches (e.g., Shapley Value [32, 55], Hypernetworks [44, 47], cosine similarity [59])"
  - [corpus] Weak - corpus focuses on specific personalization techniques but doesn't address the model-based collaboration mechanism.
- **Break condition**: When model performance doesn't accurately reflect data complementarity, or when computational overhead of model-based metrics becomes prohibitive.

### Mechanism 3
- **Claim**: The unified benchmarking framework enables systematic comparison of heterogeneity evaluation approaches.
- **Mechanism**: By standardizing evaluation across five non-IID settings and eight datasets, the framework provides guidance on which approach works best under specific conditions.
- **Core assumption**: Performance in standardized benchmarks correlates with real-world effectiveness.
- **Evidence anchors**:
  - [abstract] "Extensive experiments have been conducted to compare these approaches under five standard non-IID FL settings"
  - [section 4.1] "The accuracy of the six methods under the five standard non-IID settings and the IID data setting is shown in Table 1."
  - [corpus] Weak - corpus papers focus on specific techniques rather than benchmarking frameworks.
- **Break condition**: When benchmark settings don't capture important real-world variations, or when new heterogeneity types emerge that weren't included in the benchmark.

## Foundational Learning

- **Concept: Statistical divergence measures**
  - Why needed here: These form the theoretical foundation for understanding how distribution-based approaches quantify data heterogeneity.
  - Quick check question: What properties make JS divergence suitable for comparing probability distributions in federated learning?

- **Concept: Federated learning aggregation mechanisms**
  - Why needed here: Understanding how client updates are aggregated is crucial for grasping how heterogeneity evaluation influences collaboration.
  - Quick check question: How does the weighted aggregation in FL differ from traditional centralized training?

- **Concept: Non-IID data distributions**
  - Why needed here: The different types of data heterogeneity (label skew, feature skew, quantity skew) are fundamental to understanding the benchmark settings.
  - Quick check question: What distinguishes quantity-based label imbalance from distribution-based label imbalance in federated learning?

## Architecture Onboarding

- **Component map**: Six heterogeneity evaluation approaches (pFedJS, pFedGraph, pFedSV, FedCollab, RACE, CE) -> Five non-IID settings -> Eight datasets -> Three evaluation metrics (accuracy, computation time, communication overhead)
- **Critical path**: For each approach-dataset-setting combination: prepare data -> compute heterogeneity measure -> train personalized models -> evaluate performance -> record metrics
- **Design tradeoffs**: Distribution-based approaches offer theoretical guarantees but may struggle with high-dimensional data; model-based approaches can be more adaptive but computationally expensive; RACE offers efficiency but may sacrifice accuracy.
- **Failure signatures**: Poor accuracy despite low divergence measures (approach misalignment); excessive computation time; communication bottlenecks; inconsistent performance across similar settings.
- **First 3 experiments**:
  1. Run all six approaches on MNIST with label distribution skew (pk∼Dir(0.5)) to establish baseline performance differences.
  2. Compare RACE vs JS divergence on CIFAR-10 under feature distribution skew to evaluate efficiency-accuracy tradeoff.
  3. Test scalability by running the most accurate approach from experiment 1 on MNIST with increasing client counts (10, 15, 20, 25, 30, 35, 40).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance of distribution sketch-based approaches be further improved by incorporating more advanced locality-sensitive hashing (LSH) functions or alternative sketching techniques?
- Basis in paper: [inferred] The paper discusses the use of RACE (Repeated Array of Count Estimators) for distribution sketching but does not explore the potential for enhancement with more sophisticated LSH functions or alternative sketching methods.
- Why unresolved: The current study focuses on benchmarking existing approaches without delving into the optimization of distribution sketching techniques. There is an opportunity to investigate whether more advanced LSH functions or alternative sketching methods could yield better performance in measuring data heterogeneity.
- What evidence would resolve it: Experimental results comparing the performance of distribution sketch-based approaches using different LSH functions or alternative sketching techniques under various non-IID settings would provide insights into potential improvements.

### Open Question 2
- Question: How do the different data heterogeneity evaluation approaches perform under mixed types of data skew, and can new methods be developed to better handle these scenarios?
- Basis in paper: [explicit] The paper mentions that all evaluated schemes encounter significant challenges when facing mixed types of skew, with accuracy not exceeding 32% in some cases, indicating a need for more effective solutions.
- Why unresolved: The study highlights the difficulty of handling mixed types of data skew but does not propose new methods or strategies to address this issue. Developing approaches that can effectively manage mixed skew scenarios remains an open challenge.
- What evidence would resolve it: Developing and testing new methods specifically designed to handle mixed types of data skew, along with experimental results demonstrating their effectiveness, would provide evidence of improved performance in these scenarios.

### Open Question 3
- Question: What are the implications of using different types of divergences (e.g., earth mover distance, Fréchet distance) in the bounds for personalized federated learning, and how do they compare to existing methods?
- Basis in paper: [explicit] The paper suggests that it would be interesting to explore whether bounds exist for other divergence measures like the earth mover distance and Fréchet distance, which are mentioned in the literature but not yet applied in the context of personalized federated learning.
- Why unresolved: The study does not investigate the theoretical implications of using alternative divergence measures in the bounds for personalized federated learning, leaving open questions about their potential benefits or drawbacks compared to existing methods.
- What evidence would resolve it: Theoretical analysis and experimental results comparing the performance of personalized federated learning schemes using different divergence measures would provide insights into their implications and effectiveness.

## Limitations

- The benchmark focuses on specific non-IID settings that may not capture all real-world heterogeneity scenarios
- Computational overhead measurements are based on specific hardware configurations
- The study assumes honest client behavior without considering potential malicious actors

## Confidence

- **High**: The experimental methodology and framework design are sound and reproducible
- **Medium**: The performance rankings across different heterogeneity types are reliable within tested settings
- **Low**: The absolute performance numbers may vary significantly with different hyperparameter choices and hardware configurations

## Next Checks

1. Test the most promising approaches (pFedGraph for image data, CE for tabular data) on additional real-world federated learning datasets not included in the original benchmark
2. Conduct ablation studies to quantify the impact of individual components in the most effective approaches
3. Evaluate the approaches under different communication constraints (bandwidth limitations, higher client dropout rates) to assess practical viability