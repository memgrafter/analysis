---
ver: rpa2
title: Multi-Agent Reinforcement Learning for Joint Police Patrol and Dispatch
arxiv_id: '2409.02246'
source_url: https://arxiv.org/abs/2409.02246
tags:
- dispatch
- patrol
- policy
- response
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for joint optimization of
  police patrol and dispatch using multi-agent reinforcement learning. The method
  treats each patrol unit as an independent Q-learner with a shared deep Q-network
  for patrol decisions, while dispatch decisions are made using mixed-integer programming
  with value function approximation.
---

# Multi-Agent Reinforcement Learning for Joint Police Patrol and Dispatch

## Quick Facts
- arXiv ID: 2409.02246
- Source URL: https://arxiv.org/abs/2409.02246
- Authors: Matthew Repasky; He Wang; Yao Xie
- Reference count: 12
- Primary result: Joint optimization of police patrol and dispatch using MARL outperforms separate optimization approaches

## Executive Summary
This paper introduces a novel multi-agent reinforcement learning (MARL) framework for optimizing both police patrol and dispatch decisions simultaneously. The method treats each patrol unit as an independent Q-learner with a shared deep Q-network for patrol decisions, while dispatch decisions are made using mixed-integer programming with value function approximation. The key innovation is learning policies for both patrol and dispatch jointly, which demonstrates superior performance compared to policies optimized for either task alone. The approach is tested across various simulated environments and shows faster response times and fewer incident queue overflows than baseline methods.

## Method Summary
The proposed method combines MARL for patrol decision-making with mixed-integer programming (MIP) for dispatch optimization. Each patrol unit operates as an independent Q-learner sharing a common deep Q-network, allowing the system to learn optimal patrol routes while maintaining coordination across units. Dispatch decisions are handled through MIP formulation that incorporates value function approximations learned during training. This dual approach enables the system to optimize both proactive patrol positioning and reactive incident response simultaneously. The framework is designed to be flexible, accommodating different objectives including fairness considerations, and can be adapted to real-world scenarios using actual police call data.

## Key Results
- Jointly-optimized policies achieved faster response times compared to separate patrol or dispatch optimization
- The MARL approach reduced incident queue overflows in simulated environments
- The method demonstrated flexibility in handling different objectives, including fairness considerations

## Why This Works (Mechanism)
The method works by learning coordinated policies that account for both patrol positioning and dispatch decisions simultaneously. The MARL component enables patrol units to learn optimal routes while sharing information through a common network, creating emergent coordination without explicit communication. The MIP component for dispatch uses value function approximations to make informed decisions about incident prioritization and resource allocation. By optimizing these two components jointly rather than separately, the system can make more holistic decisions that consider both long-term patrol effectiveness and immediate response needs.

## Foundational Learning
- Multi-agent reinforcement learning: Why needed - Enables coordination between multiple patrol units; Quick check - Can be verified by observing emergent coordination patterns in learned policies
- Deep Q-networks for value function approximation: Why needed - Handles high-dimensional state spaces in real-world scenarios; Quick check - Can be validated by comparing performance with simpler function approximators
- Mixed-integer programming for dispatch: Why needed - Provides optimal decision-making for discrete dispatch choices; Quick check - Can be tested by comparing with heuristic dispatch methods
- Value function approximation in MIP: Why needed - Enables integration of learned value estimates into optimization framework; Quick check - Can be verified by measuring improvement in dispatch quality
- Joint optimization framework: Why needed - Ensures patrol and dispatch decisions are coordinated; Quick check - Can be validated by comparing with separate optimization approaches

## Architecture Onboarding

Component Map: Patrol Units (Q-learners) -> Shared DQN -> Value Function -> MIP Dispatch Component -> Environment

Critical Path: Patrol decisions are made through the shared DQN, which influences the state observed by the dispatch component. The MIP component uses value function approximations to make dispatch decisions, which in turn affect the environment state that feeds back to the patrol units.

Design Tradeoffs: The main tradeoff is between the computational complexity of the MIP component and the quality of dispatch decisions. Using value function approximations helps balance this tradeoff but may introduce approximation errors.

Failure Signatures: Potential failures include poor coordination between patrol units (leading to redundant coverage), dispatch delays due to MIP solving time, and suboptimal value function approximations causing poor dispatch decisions.

First Experiments:
1. Test individual patrol unit performance with varying numbers of units
2. Evaluate dispatch quality with synthetic incident patterns
3. Measure joint performance across different incident distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is based entirely on simulated environments without real-world validation
- Computational requirements for real-time MIP solving are not discussed
- Edge cases like multiple simultaneous high-priority incidents are not addressed
- No operational testing with actual patrol units is presented

## Confidence

High confidence: The technical approach combining MARL for patrol decisions with MIP for dispatch is sound and well-documented.

Medium confidence: The performance improvements over baseline approaches are demonstrated, but rely entirely on simulation data.

Low confidence: Claims about real-world applicability and scalability are not substantiated with operational testing or computational feasibility analysis.

## Next Checks

1. Implement a field test with actual patrol units to measure real-world performance differences
2. Conduct stress testing with high incident volumes to evaluate computational feasibility
3. Perform sensitivity analysis on hyperparameters and value function approximations to assess robustness