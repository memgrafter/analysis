---
ver: rpa2
title: Strong and weak alignment of large language models with human values
arxiv_id: '2408.04655'
source_url: https://arxiv.org/abs/2408.04655
tags:
- human
- dignity
- values
- alignment
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distinction between strong and weak alignment
  of large language models (LLMs) with human values. Strong alignment requires cognitive
  abilities like understanding intentions and predicting action effects, which are
  necessary for AI to recognize situations where human values are at risk.
---

# Strong and weak alignment of large language models with human values

## Quick Facts
- arXiv ID: 2408.04655
- Source URL: https://arxiv.org/abs/2408.04655
- Authors: Mehdi Khamassi; Marceau Nahon; Raja Chatila
- Reference count: 40
- Primary result: LLMs show weak alignment with human values, lacking the cognitive abilities needed for strong alignment

## Executive Summary
This paper distinguishes between strong and weak alignment of large language models (LLMs) with human values. Strong alignment requires cognitive abilities like understanding intentions and predicting action effects, which are necessary for AI to recognize situations where human values are at risk. Through experiments with ChatGPT, Gemini, and Copilot, the authors show that these LLMs fail to identify such situations despite producing correct responses about human values when explicitly asked. Analysis of word embeddings reveals differences in how LLMs and humans represent values like dignity and fairness. The paper concludes that current LLMs lack the reasoning abilities needed for strong alignment, instead relying on statistical patterns akin to "System 1" cognition.

## Method Summary
The paper uses a combination of prompting experiments and word embedding analysis to investigate LLM alignment with human values. The authors submitted various prompts to ChatGPT, Gemini, and Copilot to test their ability to recognize situations where human values are at risk. They also analyzed word embeddings using LSA, Word2vec, and GPT-4 to examine how LLMs represent concepts related to human values like dignity, fairness, and well-being. The study does not employ a specific training procedure but rather analyzes existing LLM capabilities through qualitative and quantitative methods.

## Key Results
- LLMs fail to recognize situations where human values are at risk despite producing correct responses when explicitly asked about values
- Word embeddings in LLMs show different patterns for human values compared to human semantic understanding
- LLMs exhibit variability in responses to identical prompts, undermining confidence in their alignment capabilities
- Current LLMs operate similarly to "System 1" cognition, lacking the deliberative reasoning needed for strong alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak alignment can produce correct responses about human values when explicitly prompted, but fails to detect value-related risks in implicit scenarios.
- Mechanism: LLMs rely on statistical patterns and correlations learned from training data to generate responses. When asked directly about values, they retrieve and reproduce well-represented patterns. However, in scenarios where values are implied or require causal reasoning, the absence of internal models prevents detection.
- Core assumption: LLMs do not possess internal causal models of the world or agents' intentions, and thus cannot infer when values are at risk unless explicitly stated.
- Evidence anchors:
  - [abstract] "Strong alignment requires cognitive abilities...such as understanding and reasoning about agents' intentions and their ability to causally produce desired effects."
  - [section 3.4] "ChatGPT, Gemini and Copilot are most of the time unable to recognize a situation in which dignity is undermined because it is proposed to replace one of the four poles of a canopy by a human..."
  - [corpus] Weak signal: Only 5 related papers found; no strong citations yet. May indicate early-stage research.
- Break condition: If an LLM is augmented with explicit causal reasoning modules or symbolic knowledge bases, it might overcome this limitation.

### Mechanism 2
- Claim: Word embeddings in LLMs reflect statistical co-occurrence rather than human-like semantic understanding, leading to misaligned nearest neighbors for human values.
- Mechanism: LLMs use word embeddings derived from large corpora to represent semantic relationships. These embeddings capture statistical patterns but may not align with human cognitive structures, as evidenced by nearest neighbor analyses.
- Core assumption: Human semantic understanding involves structured ontologies and real-world grounding, which are absent in LLM embeddings.
- Evidence anchors:
  - [section 4.2] "Analyzing the neighbors of words relative to human values could then indicate the strength, or weakness, of the system's propensity to align."
  - [section 4.2] "We note that the results given by LSA are far less convincing than the ones of Word2vec and GPT-4."
  - [corpus] Weak signal: Limited literature on embedding-based value alignment; mostly theoretical.
- Break condition: If embeddings are augmented with human-curated semantic ontologies or grounded in real-world interactions, alignment may improve.

### Mechanism 3
- Claim: LLMs operate similarly to "System 1" cognition, producing fast, unreflective responses based on heuristics, which limits their ability to strongly align with human values.
- Mechanism: LLMs generate responses based on learned statistical patterns without deliberative reasoning. This mirrors Kahneman's "System 1" cognition, which is fast but lacks the depth required for strong alignment.
- Core assumption: Strong alignment requires "System 2"-like reasoning, involving causal internal models and goal-oriented behavior, which LLMs currently lack.
- Evidence anchors:
  - [abstract] "Strong alignment requires cognitive abilities...which are necessary for AI to recognize situations where human values are at risk."
  - [section 5] "Following this view, it is probably human brain networks coarsely associated to System 2 which acquire models of other agents' intentions..."
  - [corpus] Moderate signal: Related work on AI alignment and cognitive models exists, but not directly addressing System 1/2 analogy.
- Break condition: If LLMs are integrated with deliberative reasoning modules or hybrid architectures combining symbolic and statistical approaches, they may achieve stronger alignment.

## Foundational Learning

- Concept: Causal reasoning
  - Why needed here: Strong alignment requires understanding causal relationships between actions and their effects on human values.
  - Quick check question: Can you explain how a causal model would help an LLM detect that replacing a canopy pole with a human violates dignity?

- Concept: Word embeddings and semantic similarity
  - Why needed here: Understanding how LLMs represent and relate concepts is crucial for analyzing their alignment capabilities.
  - Quick check question: How would you compute the cosine similarity between two word embeddings, and what does it tell us about their semantic relationship?

- Concept: System 1 vs. System 2 cognition
  - Why needed here: Distinguishing between reactive and deliberative reasoning helps explain why LLMs struggle with strong alignment.
  - Quick check question: What are the key differences between System 1 and System 2 cognition, and how do they relate to LLM behavior?

## Architecture Onboarding

- Component map: Prompt -> Tokenization -> Embedding lookup -> Attention mechanism -> Probability distribution -> Response generation -> Output filtering
- Critical path: Prompt → Tokenization → Embedding lookup → Attention mechanism → Probability distribution → Response generation → Output filtering
- Design tradeoffs: Speed vs. depth of reasoning, statistical accuracy vs. causal understanding, generalization vs. specificity
- Failure signatures: Incorrect detection of value-related risks, statistical fallacies in reasoning, variability in responses to identical prompts
- First 3 experiments:
  1. Test LLM response to explicit vs. implicit value-related prompts
  2. Analyze nearest neighbors of human values in LLM embeddings
  3. Evaluate LLM reasoning in causal scenarios (e.g., predicting consequences of actions)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically measure the difference between strong and weak alignment in LLMs beyond the current prompting experiments?
- Basis in paper: [explicit] The paper proposes the distinction between strong and weak alignment but relies on prompting experiments and word embedding analysis to illustrate the difference.
- Why unresolved: The paper provides theoretical arguments and preliminary experiments but lacks a comprehensive framework for empirically measuring alignment strength in LLMs.
- What evidence would resolve it: Development of standardized benchmarks that test LLMs' understanding of human values in complex scenarios, their ability to reason about intentions and causal effects, and their capacity to explain their reasoning process.

### Open Question 2
- Question: Can LLMs achieve strong alignment through training methods that incorporate causal modeling and reasoning about intentions, rather than just statistical patterns?
- Basis in paper: [inferred] The paper argues that current LLMs rely on "System 1" cognition (statistical patterns) and lack "System 2" abilities (causal reasoning, understanding intentions) necessary for strong alignment.
- Why unresolved: The paper proposes that strong alignment requires causal internal models and reasoning about intentions, but doesn't explore how LLMs could be trained to develop these capabilities.
- What evidence would resolve it: Successful training of LLMs using methods that explicitly teach causal reasoning and intention understanding, followed by testing their performance on complex scenarios involving human values.

### Open Question 3
- Question: How does the variability in LLM responses to the same prompts affect their reliability for value alignment, and can this variability be reduced while maintaining strong alignment capabilities?
- Basis in paper: [explicit] The paper notes that LLMs produce different responses to the same prompts and that this variability undermines confidence in their responses.
- Why unresolved: The paper observes the variability issue but doesn't explore its implications for alignment or potential solutions to reduce it.
- What evidence would resolve it: Studies comparing the consistency of LLM responses across multiple runs of the same prompt, and experiments testing whether reducing variability affects the LLM's ability to understand and align with human values.

## Limitations

- The distinction between strong and weak alignment relies heavily on qualitative analysis rather than systematic testing of causal reasoning abilities
- Word embedding analysis is limited to only three human value concepts (dignity, fairness, well-being), which may not generalize to other values
- The System 1/System 2 analogy is intuitively appealing but lacks direct empirical evidence linking LLM behavior to established cognitive science models

## Confidence

**Medium confidence** in the distinction between strong and weak alignment - the conceptual framework is well-argued but the empirical evidence, while suggestive, is not comprehensive enough to definitively establish the proposed differences.

**Medium confidence** in the word embedding analysis - the methodology is sound but the limited scope (three values) and lack of comparison with human semantic networks reduces confidence in generalizability.

**Low confidence** in the System 1/System 2 analogy - while intuitively appealing, this claim lacks direct empirical support linking LLM behavior to established dual-process theories of cognition.

## Next Checks

1. **Causal reasoning benchmarks**: Design and implement systematic tests that specifically measure LLMs' ability to reason about cause-and-effect relationships in scenarios involving human values, using established causal reasoning datasets and metrics.

2. **Extended semantic analysis**: Expand the word embedding analysis to include a broader range of human values (10-15 concepts) and compare LLM representations with human semantic networks derived from psychological studies of value concepts.

3. **Cross-cultural validation**: Test the proposed alignment framework across different cultural contexts by using value prompts and scenarios from diverse cultural backgrounds to determine whether the strong/weak alignment distinction holds universally or is culturally specific.