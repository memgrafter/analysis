---
ver: rpa2
title: 'MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs'
arxiv_id: '2402.15627'
source_url: https://arxiv.org/abs/2402.15627
tags:
- training
- data
- gpus
- communication
- parallelism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MegaScale, a production system for training
  large language models (LLMs) at the scale of more than 10,000 GPUs. The key challenges
  addressed are achieving high training efficiency and stability at such a massive
  scale.
---

# MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs

## Quick Facts
- arXiv ID: 2402.15627
- Source URL: https://arxiv.org/abs/2402.15627
- Authors: Ziheng Jiang; Haibin Lin; Yinmin Zhong; Qi Huang; Yangrui Chen; Zhi Zhang; Yanghua Peng; Xiang Li; Cong Xie; Shibiao Nong; Yulu Jia; Sun He; Hongmin Chen; Zhihao Bai; Qi Hou; Shipeng Yan; Ding Zhou; Yiyao Sheng; Zhuo Jiang; Haohan Xu; Haoran Wei; Zhang Zhang; Pengfei Nie; Leqi Zou; Sida Zhao; Liang Xiang; Zherui Liu; Zhe Li; Xiaoying Jia; Jianxi Ye; Xin Jin; Xin Liu
- Reference count: 40
- One-line primary result: Achieves 55.2% Model FLOPs Utilization (MFU) on 12,288 GPUs, improving by 1.34x compared to Megatron-LM

## Executive Summary
This paper presents MegaScale, a production system designed to train large language models (LLMs) at the unprecedented scale of more than 10,000 GPUs. The key innovation lies in a full-stack approach that co-designs algorithmic and system components to achieve both high training efficiency and stability at massive scale. By integrating model architecture optimizations, parallelism strategies, communication overlapping, and robust fault tolerance mechanisms, MegaScale demonstrates the ability to train proprietary models with hundreds of billions of parameters on multi-trillion tokens for several weeks, automatically recovering from over 100 failures.

## Method Summary
MegaScale employs a full-stack co-design approach combining algorithmic and system optimizations for large-scale LLM training. The method includes parallel transformer blocks with sliding window attention, LAMB optimizer, and 3D parallelism (data, pipeline, tensor/sequence) with communication overlapping across all dimensions. System-level optimizations include operator fusion, optimized data pipelines, custom network topology design, and Swift-based congestion control. A robust fault tolerance framework with heartbeat monitoring, diagnostic tests, and fast checkpointing enables automatic recovery from failures. The primary evaluation metric is Model FLOPs Utilization (MFU), with the system achieving 55.2% MFU on 12,288 GPUs.

## Key Results
- Achieves 55.2% Model FLOPs Utilization (MFU) on 12,288 GPUs
- Improves MFU by 1.34x compared to Megatron-LM baseline
- Demonstrates stable training of proprietary models with hundreds of billions of parameters for weeks with over 100 automatic recovery events

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithm-system co-design enables MegaScale to achieve 55.2% MFU on 12,288 GPUs, a 1.34x improvement over Megatron-LM.
- Mechanism: MegaScale integrates model architecture modifications (parallel transformer block, sliding window attention), optimizer changes (LAMB), and parallelism optimizations (data, pipeline, tensor, sequence) with system-level techniques (communication overlapping, operator optimization, data pipeline, network tuning).
- Core assumption: Co-design across all layers yields multiplicative efficiency gains rather than additive.
- Evidence anchors:
  - [abstract]: "We take a full-stack approach that co-designs the algorithmic and system components..."
  - [section 3.1]: Describes parallel transformer block and sliding window attention with convergence validation.
  - [section 3.2]: Details communication overlapping techniques across data, pipeline, and tensor parallelism.
  - [corpus]: Weak - corpus neighbors focus on MoE and GNN systems, not LLM scaling mechanisms.
- Break condition: If any component (e.g., communication overlapping) fails, the multiplicative gains collapse to additive, significantly reducing MFU.

### Mechanism 2
- Claim: In-depth observability enables automatic fault detection and recovery, allowing training to continue despite over 100 failures in a real production run.
- Mechanism: Heartbeat messages with detailed status, log analysis, RDMA traffic monitoring, and diagnostic tests enable rapid identification of hardware/software faults, followed by automated recovery via checkpointing and resource replenishment.
- Core assumption: Granular, real-time monitoring across all stack layers can identify root causes faster than manual intervention.
- Evidence anchors:
  - [abstract]: "We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes..."
  - [section 4.2]: Describes heartbeat messages, log aggregation, and RDMA traffic anomaly detection.
  - [section 4.3]: Details lightweight diagnostic tests for intra-host networks, NCCL communication, and inter-node GPU communication.
  - [section 6.2]: Shows real production run with hundreds of billions of parameters trained for weeks with over 100 automatic recoveries.
- Break condition: If monitoring granularity is insufficient or diagnostic tests miss edge cases, failures may go undetected, halting training.

### Mechanism 3
- Claim: Network performance tuning reduces ECMP hashing conflicts and optimizes congestion control, improving communication efficiency at scale.
- Mechanism: Custom network topology with CLOS-like structure, strategic GPU scheduling to reduce switch hops, Swift-based congestion control algorithm, and tuned NCCL retransmit parameters.
- Core assumption: Network bottlenecks are predictable and mitigable through topology design and protocol tuning.
- Evidence anchors:
  - [section 3.6]: Describes network topology design, ECMP conflict reduction through ToR switch scheduling, congestion control algorithm combining Swift and DCQCN, and retransmit timeout tuning.
  - [abstract]: "We design a custom network topology, reduce ECMP hash conflicts, customize congestion control..."
- Break condition: If network conditions exceed design assumptions (e.g., unexpected traffic patterns), congestion control may fail, degrading MFU.

## Foundational Learning

- Concept: 3D parallelism (data, pipeline, tensor parallelism)
  - Why needed here: Enables scaling LLM training beyond single GPU memory and compute limits by distributing model and data across thousands of GPUs.
  - Quick check question: What are the three types of parallelism used in MegaScale and how do they differ in communication patterns?

- Concept: Model FLOPs Utilization (MFU)
  - Why needed here: MFU is the key metric for measuring training efficiency at scale; improving MFU directly translates to faster training and better resource utilization.
  - Quick check question: How is MFU calculated and why is it more meaningful than raw throughput for large-scale LLM training?

- Concept: Fault tolerance and recovery mechanisms
  - Why needed here: At scale, hardware/software failures are inevitable; robust fault tolerance is essential for long-running LLM training jobs that can last weeks.
  - Quick check question: What are the two main components of MegaScale's fault tolerance framework and how do they work together?

## Architecture Onboarding

- Component map:
  - Algorithmic layer: Model architecture (parallel transformer block, sliding window attention), Optimizer (LAMB)
  - System layer: Parallelism strategies (data, pipeline, tensor, sequence), Communication overlapping, Operator optimization, Data pipeline, Network tuning
  - Monitoring layer: Heartbeat messages, Log analysis, RDMA traffic monitoring, Diagnostic tests
  - Fault tolerance layer: Robust training framework, Fast checkpointing/recovery, 3D parallel training visualization

- Critical path: Forward pass → Backward pass → Gradient synchronization (reduce-scatter) → Parameter update (all-gather)
- Design tradeoffs:
  - Communication vs. computation: Overlapping communication with computation reduces idle time but requires careful synchronization.
  - Checkpointing frequency vs. training interruption: More frequent checkpoints enable faster recovery but introduce overhead.
  - Network topology complexity vs. performance: CLOS-like topology reduces congestion but requires careful GPU scheduling.

- Failure signatures:
  - MFU degradation over time: Indicates communication bottlenecks or stragglers
  - Training stall: Suggests NCCL communication failure or hardware fault
  - Inconsistent MFU across runs: Points to computational stragglers or problematic code segments

- First 3 experiments:
  1. Single-node profiling: Measure MFU with and without algorithmic optimizations (parallel transformer block, sliding window attention) to validate convergence and efficiency gains.
  2. Multi-node communication test: Run NCCL all-reduce benchmark across nodes to identify network bottlenecks and validate congestion control settings.
  3. Fault injection test: Simulate hardware failure and verify automatic recovery via checkpointing and resource replenishment mechanisms.

## Open Questions the Paper Calls Out
None

## Limitations
- Specific hardware configurations of production AI clusters (switch types, network topology details, GPU-to-switch connections) are not fully detailed
- Proprietary nature of trained models and datasets prevents independent verification of convergence claims
- Exact implementation details of custom diagnosis tools and 3D parallel training visualization are not provided

## Confidence
- **High Confidence**: The core architectural approach of algorithm-system co-design for large-scale LLM training is well-established and the described techniques (3D parallelism, communication overlapping, fault tolerance) are sound and implementable.
- **Medium Confidence**: The specific MFU improvement of 1.34x over Megatron-LM and the reported 55.2% MFU on 12,288 GPUs are based on the described techniques, but without access to the same hardware and implementation details, independent verification is difficult.
- **Low Confidence**: The ability to automatically recover from over 100 failures in a real production run is difficult to verify without access to proprietary training logs and specific failure modes.

## Next Checks
1. **MFU Benchmarking on Public Datasets**: Implement the MegaScale system architecture on a public LLM training benchmark (e.g., GPT-3 175B on The Pile) using a comparable GPU cluster. Measure MFU and compare against Megatron-LM to independently verify the 1.34x improvement claim.

2. **Ablation Study of Optimizations**: Conduct an ablation study by progressively disabling each major optimization (parallel transformer block, sliding window attention, communication overlapping for each parallelism dimension, network tuning) and measuring the impact on MFU and convergence.

3. **Fault Injection and Recovery Testing**: Design a fault injection framework to simulate various hardware and software failures (GPU hangs, network partitions, process crashes) at different scales. Test the automatic recovery mechanisms, including checkpointing overhead, recovery time, and training continuity.