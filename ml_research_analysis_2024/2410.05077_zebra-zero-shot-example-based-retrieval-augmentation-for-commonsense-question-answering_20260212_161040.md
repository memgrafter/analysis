---
ver: rpa2
title: 'ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question
  Answering'
arxiv_id: '2410.05077'
source_url: https://arxiv.org/abs/2410.05077
tags:
- knowledge
- question
- zebra
- commonsense
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZEBRA introduces a zero-shot framework that improves commonsense
  question answering by retrieving relevant examples and generating contextual knowledge,
  avoiding the need for additional model training. Instead of direct knowledge retrieval,
  ZEBRA retrieves full question-knowledge examples, generates tailored explanations
  following their relationships, and uses them to inform reasoning.
---

# ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering

## Quick Facts
- arXiv ID: 2410.05077
- Source URL: https://arxiv.org/abs/2410.05077
- Reference count: 10
- Key outcome: Achieves up to 4.5 points average accuracy improvement on 8 commonsense QA benchmarks using zero-shot example-based retrieval augmentation

## Executive Summary
ZEBRA introduces a novel zero-shot framework that improves commonsense question answering by retrieving relevant examples and generating contextual knowledge, avoiding the need for additional model training. Instead of direct knowledge retrieval, ZEBRA retrieves full question-knowledge examples, generates tailored explanations following their relationships, and uses them to inform reasoning. It is evaluated on 8 benchmarks with 4 large language models, achieving an average accuracy improvement of up to 4.5 points. ZEBRA outperforms both direct knowledge retrieval methods and state-of-the-art knowledge generation approaches, while providing interpretable reasoning through generated explanations. Human evaluation confirms the generated knowledge is often relevant, factual, and helpful for answering questions.

## Method Summary
ZEBRA is a three-step framework that enhances commonsense reasoning without additional model training. First, it retrieves relevant question-knowledge pairs from a knowledge base using a DPR-based retriever trained on contrastive learning. Second, it generates commonsense knowledge tailored to the input question by having the LLM emulate relationships found in the retrieved examples. Third, it performs knowledge-informed reasoning to answer the question. The framework uses a knowledge base constructed from silver explanations generated by Gemini-1.5-Flash for training sets of 8 commonsense reasoning benchmarks.

## Key Results
- Achieves up to 4.5 points average accuracy improvement across 8 commonsense QA benchmarks
- Outperforms both direct knowledge retrieval methods and state-of-the-art knowledge generation approaches
- Human evaluation shows 96% relevance, 88% factuality, and 74% helpfulness of generated knowledge
- Works effectively with 4 different LLMs (Mistral-v0.2, Phi-3-Small, Llama-3, Phi-3-Mini)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZEBRA improves commonsense reasoning by generating tailored explanations that emulate relationships found in retrieved examples.
- Mechanism: The framework retrieves relevant question-knowledge pairs and prompts the LLM to generate explanations by following the patterns in these pairs, rather than directly retrieving isolated facts.
- Core assumption: The LLM can effectively emulate the relationship patterns in retrieved examples to generate useful knowledge for new questions.
- Evidence anchors:
  - [abstract]: "ZEBRA retrieves relevant question-knowledge pairs from a knowledge base and generates new knowledge by reasoning over the relationships in these pairs."
  - [section 3.2]: "we build on top of case-based reasoning. Our approach encourages an LLM to generate knowledge by emulating the relationship in the question-knowledge pairs found in the retrieved examples."
  - [corpus]: Weak - the corpus neighbors focus on multimodal VQA and KGQA rather than example-based knowledge generation.
- Break condition: If the LLM cannot effectively identify or emulate relationship patterns in retrieved examples, the generated knowledge may be irrelevant or incorrect.

### Mechanism 2
- Claim: ZEBRA provides interpretable reasoning by generating explicit explanations for answers.
- Mechanism: The framework generates knowledge explanations that humans can evaluate for relevance, factuality, and helpfulness, making the reasoning process transparent.
- Core assumption: Generated explanations are comprehensible to humans and accurately represent the reasoning process.
- Evidence anchors:
  - [abstract]: "ZEBRA consistently outperforms strong LLMs and previous knowledge integration approaches, achieving an average accuracy improvement of up to 4.5 points."
  - [section 5.5]: Human evaluation shows "96 out of 100 instances as 'relevant', 88 out of 100 instances as 'factual', and 74 out of 100 instances as 'helpful'."
  - [corpus]: Weak - corpus neighbors don't address interpretability of reasoning.
- Break condition: If generated explanations are not comprehensible or do not accurately represent the reasoning process, the interpretability benefit is lost.

### Mechanism 3
- Claim: ZEBRA avoids the limitations of finite knowledge bases by generating context-specific knowledge.
- Mechanism: Instead of retrieving fixed facts from a knowledge base, ZEBRA generates explanations tailored to each specific question based on retrieved examples.
- Core assumption: Generated explanations can effectively fill gaps that static knowledge bases cannot cover.
- Evidence anchors:
  - [abstract]: "rather than directly retrieving or generating knowledge for the specific input question, ZEBRA : i) retrieves one or more examples... ii) generates commonsense knowledge tailored for the input question by following the relationship in the question-knowledge pairs contained in the retrieved examples."
  - [section 2]: "ZEBRA stems from two observations: first, direct retrieval of commonsense facts may provide useful hints, but it is limited by the finite nature of knowledge bases and the noise therein; second, introspection can generate contextually relevant knowledge which is tailored to the input question, but this is limited to what the LLM already 'knows'."
  - [corpus]: Weak - corpus neighbors focus on KGQA and multimodal VQA rather than dynamic knowledge generation.
- Break condition: If generated explanations consistently fail to provide useful context-specific knowledge, the framework loses its advantage over static knowledge bases.

## Foundational Learning

- Concept: Dense passage retrieval and contrastive learning
  - Why needed here: The retriever component uses DPR-style dense representations and multi-label NCE training to find relevant examples.
  - Quick check question: How does the retriever compute similarity between queries and passages?

- Concept: Case-based reasoning
  - Why needed here: ZEBRA builds on case-based reasoning by retrieving complete examples rather than isolated facts.
  - Quick check question: What distinguishes ZEBRA's retrieval approach from standard knowledge retrieval?

- Concept: Prompt engineering for knowledge generation
  - Why needed here: The framework relies on carefully constructed prompts to guide the LLM in generating explanations from retrieved examples.
  - Quick check question: How does the knowledge generation prompt structure the relationship between retrieved examples and the target question?

## Architecture Onboarding

- Component map: Retriever -> Knowledge Generator -> Reasoner
- Critical path: Retriever → Knowledge Generator → Reasoner
  - The retriever must find relevant examples, the generator must create useful explanations, and the reasoner must use them effectively
- Design tradeoffs:
  - Retriever quality vs. computational cost (finer-grained retrieval may improve results but increase latency)
  - Number of retrieved examples (k) vs. generation quality (more examples may help but also increase noise)
  - Silver vs. gold knowledge base (silver is cheaper but may have quality issues)
- Failure signatures:
  - Poor retriever performance manifests as irrelevant examples being retrieved
  - Knowledge generator failures appear as explanations that are irrelevant, factually incorrect, or unhelpful
  - Reasoner failures show up as answers that ignore or are misled by the generated knowledge
- First 3 experiments:
  1. Test retriever performance by evaluating retrieved examples' relevance to input questions
  2. Evaluate knowledge generation quality by checking if generated explanations are relevant and factual
  3. Measure end-to-end performance with different values of k to find the optimal number of retrieved examples

## Open Questions the Paper Calls Out
- Multilinguality and cross-linguality: How ZEBRA performs when the knowledge base contains examples in multiple languages
- Incorporating additional knowledge: How to integrate structured knowledge from external knowledge bases
- End-to-end optimization: How to optimize the entire pipeline rather than treating components separately

## Limitations
- The framework's performance relies heavily on the quality of the DPR retriever, which is trained only on CSQA training data
- Knowledge generation depends on silver explanations from Gemini-1.5-Flash, which may introduce quality variations
- The effectiveness may be constrained by the diversity and coverage of the ZEBRA-KB constructed from silver explanations
- The paper only evaluates on English benchmarks, leaving multilingual performance uncertain

## Confidence

**High confidence**: The framework's general approach of using case-based reasoning with retrieved examples to generate tailored knowledge is well-supported by experimental results across multiple benchmarks and LLMs. Human evaluation showing 96% relevance, 88% factuality, and 74% helpfulness provides strong evidence for generated knowledge quality.

**Medium confidence**: The claim of up to 4.5 points average accuracy improvement should be interpreted with caution as this represents the maximum observed improvement rather than a consistent gain across all benchmarks and models. Performance variation across different LLMs suggests results may depend on the specific model used.

**Low confidence**: The long-term generalization of the framework beyond the 8 evaluated benchmarks remains uncertain, as the retriever training is limited to CSQA data and knowledge generation relies on patterns observed in specific datasets.

## Next Checks

1. **Cross-dataset retriever evaluation**: Test the DPR retriever's performance on benchmarks not seen during training (beyond CSQA) to verify generalization capabilities and identify potential domain-specific weaknesses in retrieval quality.

2. **Knowledge generation ablation study**: Systematically evaluate the impact of different knowledge generation approaches (temperature settings, explanation length, retrieval strategies) on both the quality of generated explanations and final QA performance to identify optimal configurations.

3. **Error analysis on failed cases**: Conduct a detailed analysis of questions where ZEBRA underperforms the baseline LLM to identify patterns in knowledge generation failures and determine whether issues stem from retrieval, generation, or reasoning stages.