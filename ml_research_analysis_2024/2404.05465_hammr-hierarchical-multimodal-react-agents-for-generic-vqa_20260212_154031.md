---
ver: rpa2
title: 'HAMMR: HierArchical MultiModal React agents for generic VQA'
arxiv_id: '2404.05465'
source_url: https://arxiv.org/abs/2404.05465
tags:
- question
- hammr
- image
- generic
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles generic visual question answering (VQA), where
  a single system must handle diverse question types like counting, spatial reasoning,
  OCR-based tasks, encyclopedic knowledge, and pointing. The authors identify that
  simply combining all available tools in a large LLM prompt leads to poor performance
  due to confusion and difficulty in reasoning about tool combinations.
---

# HAMMR: HierArchical MultiModal React agents for generic VQA

## Quick Facts
- arXiv ID: 2404.05465
- Source URL: https://arxiv.org/abs/2404.05465
- Authors: Lluis Castrejon, Thomas Mensink, Howard Zhou, Vittorio Ferrari, Andre Araujo, Jasper Uijlings
- Reference count: 40
- One-line primary result: HAMMR achieves state-of-the-art results on diverse VQA benchmarks, outperforming naive generic ReAct by 16.3% and PaLI-X by 5.0% average accuracy.

## Executive Summary
This paper introduces HAMMR, a hierarchical multimodal ReAct-based system for generic visual question answering. The key insight is that a single agent trying to handle all question types becomes confused and inefficient, so HAMMR uses a high-level dispatcher to route questions to specialized sub-agents, each focused on specific question types. By leveraging variable-based multimodal reasoning and compositional agent design, HAMMR achieves state-of-the-art performance across diverse VQA tasks including counting, spatial reasoning, OCR, encyclopedic knowledge, and pointing tasks.

## Method Summary
HAMMR builds on multimodal ReAct agents that use variables to store intermediate multimodal outputs, preventing prompt clogging. A QuestionDispatcherAgent identifies the question type and routes to specialized agents (PointQA, EncyclopedicVQA, NLVR2, GQA, TallyQA, TextVQA), each optimized for their specific task. These agents can also be reused as tools within other agents, enabling compositional reasoning chains. The system uses external tools like Google Lens, Wikipedia, object detection, and OCR, with all orchestration handled through prompt engineering rather than training.

## Key Results
- HAMMR outperforms naive generic ReAct agent by 16.3% (40.3% vs 24.0% average accuracy)
- HAMMR achieves 5.0% better average performance than PaLI-X VLM across 6 diverse VQA datasets
- State-of-the-art results on PointQA, EncyclopedicVQA, NLVR2, GQA, TallyQA, and TextVQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition improves reasoning accuracy for generic VQA
- Mechanism: High-level dispatcher routes questions to specialized sub-agents, reducing cognitive load and improving accuracy
- Core assumption: Specialized agents can solve their specific question types better than a single generic agent
- Evidence anchors:
  - [abstract] "We start from a multimodal ReAct-based [1] system and make it hierarchical by enabling our HAMMR agents to call upon other specialized agents"
  - [section] "Results in Tab. III show that HAMMR outperforms the naive generic agent by a margin of 16.3% (40.3% vs 24.0%)"
  - [corpus] Weak - related papers focus on tool-use frameworks but not hierarchical decomposition for VQA
- Break condition: When the question dispatcher fails to correctly identify question type, or when specialized agents cannot handle edge cases within their domain

### Mechanism 2
- Claim: Variable-based multimodal reasoning prevents prompt clogging
- Mechanism: Storing multimodal outputs in variables and calling separate LLM tools to process them avoids exponential prompt growth
- Core assumption: LLMs have limited context window and performance degrades with very long prompts
- Evidence anchors:
  - [abstract] "To make Re-act multimodal, we give it access to variables, which could contain images, text, or other data types"
  - [section] "adding variables to a ReAct agent not only enables it to be multi-modal, but can also help with text-only subproblems"
  - [corpus] Weak - related papers discuss multimodal reasoning but not variable-based prompt management
- Break condition: When variable management becomes too complex or when tool outputs exceed LLM processing capabilities

### Mechanism 3
- Claim: Compositionality enables solving increasingly complex tasks
- Mechanism: Specialized agents can be reused as tools within other agents, creating compositional reasoning chains
- Core assumption: Breaking down complex tasks into simpler sub-tasks improves overall performance
- Evidence anchors:
  - [abstract] "HAMMR leverages a multimodal ReAct-based [1] system, where LLM agents can be prompted to select the most suitable tools to answer a given question"
  - [section] "Our design with a high-level dispatcher and specialized agents has multiple benefits... (2) Improving a task-specific agent will lead to improvement of the overall system"
  - [corpus] Weak - related papers discuss tool composition but not hierarchical agent composition for VQA
- Break condition: When compositional chains become too deep, causing error propagation or when sub-agent outputs become ambiguous

## Foundational Learning

- Concept: ReAct framework fundamentals
  - Why needed here: HAMMR builds directly on ReAct's interleaving of reasoning and action steps
  - Quick check question: What are the three core steps in a ReAct iteration?

- Concept: Prompt engineering and in-context learning
  - Why needed here: HAMMR relies on carefully crafted prompts and examples to guide agent behavior without training
  - Quick check question: How does HAMMR use in-context examples differently than naive ReAct?

- Concept: Multimodal reasoning patterns
  - Why needed here: HAMMR must handle both visual and textual reasoning across diverse question types
  - Quick check question: What tools does HAMMR use for different types of visual reasoning?

## Architecture Onboarding

- Component map:
  QuestionDispatcherAgent -> Specialist agents (PointQA, EncyclopedicVQA, NLVR2, GQA, TallyQA, TextVQA) -> Tool library (Google Lens, Wikipedia, object detection, OCR, image cropping, bounding box utilities) -> Variable management system

- Critical path:
  1. QuestionDispatcherAgent identifies question type
  2. Dispatches to appropriate specialist agent
  3. Specialist agent executes reasoning chain using tools
  4. Results returned through variable system

- Design tradeoffs:
  - Hierarchy vs. flat structure: Hierarchy improves accuracy but adds routing complexity
  - Prompt length vs. specialization: Specialized agents have shorter prompts but require dispatcher
  - Tool variety vs. simplicity: More tools enable better performance but increase prompt complexity

- Failure signatures:
  - Dispatcher misclassifies question type
  - Specialist agent calls wrong tool
  - Tool outputs are ambiguous or incorrect
  - Variable management errors

- First 3 experiments:
  1. Test QuestionDispatcherAgent on sample questions from each type
  2. Validate individual specialist agents on their specific datasets
  3. Run end-to-end evaluation on combined generic VQA suite

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of HAMMR scale with increasing numbers of question types and corresponding specialized agents?
- Basis in paper: Explicit - The paper mentions that the naive generic ReAct agent struggles with performance as the number of question types increases due to the large prompt size and difficulty in reasoning about tool combinations. HAMMR addresses this by using specialized agents, but the paper does not explore the limits of this approach.
- Why unresolved: The paper only evaluates HAMMR on a fixed set of 8 question types. It does not investigate how performance degrades or improves as more specialized agents are added.
- What evidence would resolve it: Experimental results showing HAMMR's performance on increasingly diverse sets of question types, ideally with a trend line indicating how accuracy changes with the number of agents.

### Open Question 2
- Question: What is the impact of prompt engineering and in-context examples on the performance of HAMMR's specialized agents and the Question Dispatcher?
- Basis in paper: Explicit - The paper mentions that specialized agents are created via prompts refined through iterations on a validation set, and the Question Dispatcher is also created via prompts refined on a small validation set. However, the paper does not explore how different prompt engineering techniques or in-context examples affect performance.
- Why unresolved: The paper only describes the general approach to creating prompts and does not provide a systematic analysis of how different prompt engineering strategies impact accuracy.
- What evidence would resolve it: A study comparing the performance of HAMMR's agents using different prompt engineering techniques (e.g., different in-context examples, prompt formats) and quantifying the impact on accuracy.

### Open Question 3
- Question: How does HAMMR's performance compare to other hierarchical or modular approaches for generic VQA, such as those using separate models for different question types or ensemble methods?
- Basis in paper: Explicit - The paper compares HAMMR to a naive generic ReAct agent and to PaLI-X, but does not compare it to other hierarchical or modular approaches.
- Why unresolved: The paper focuses on demonstrating the superiority of HAMMR over a naive approach and a single large model, but does not explore how it compares to other potentially effective architectures.
- What evidence would resolve it: Experimental results comparing HAMMR to other hierarchical or modular approaches for generic VQA, such as separate models for different question types or ensemble methods, on the same benchmark.

## Limitations

- Prompt templates and in-context examples for agents are not fully disclosed, making it difficult to assess whether hierarchical design or prompt engineering drives improvements
- Computational cost of running multiple specialized agents versus single generic agent is not reported
- Performance on out-of-distribution question types or novel combinations of existing types remains unclear

## Confidence

- **High Confidence**: The core architectural contribution (hierarchical agent design with dispatcher and specialists) is well-supported by experimental results showing 16.3% improvement over naive ReAct and 5.0% over PaLI-X. The mechanism of reducing cognitive load through specialization is logically sound.
- **Medium Confidence**: The claim that variable-based multimodal reasoning prevents prompt clogging is supported by design rationale but lacks direct quantitative evidence comparing prompt lengths and performance degradation in HAMMR versus baseline ReAct.
- **Low Confidence**: The scalability claims regarding compositionality enabling increasingly complex tasks are primarily theoretical, with limited empirical validation beyond demonstrated improvements on existing benchmark tasks.

## Next Checks

1. **Ablation Study**: Remove the dispatcher and evaluate whether performance drops proportionally to the reported 16.3% improvement, isolating the impact of hierarchical routing versus other design changes.

2. **Prompt Length Analysis**: Measure and compare the actual prompt lengths and processing times for HAMMR versus baseline ReAct across all datasets to quantify the claimed prompt clogging prevention.

3. **Out-of-Distribution Testing**: Create a mixed-question-type test set where questions combine multiple types (e.g., counting objects while also requiring encyclopedic knowledge) to evaluate HAMMR's true compositional reasoning capabilities beyond the reported benchmarks.