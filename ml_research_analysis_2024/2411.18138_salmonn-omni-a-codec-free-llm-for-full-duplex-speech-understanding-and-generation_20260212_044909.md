---
ver: rpa2
title: 'SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation'
arxiv_id: '2411.18138'
source_url: https://arxiv.org/abs/2411.18138
tags:
- speech
- salmonn-omni
- streaming
- user
- full-duplex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALMONN-omni is a codec-free, full-duplex speech understanding
  and generation model that integrates a streaming speech encoder, LLM, and streaming
  speech synthesizer into a unified end-to-end architecture. It introduces a novel
  "thinking" mechanism with special state transition tokens to enable asynchronous
  text and speech generation while simultaneously listening to its own speech and
  background sounds.
---

# SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation

## Quick Facts
- arXiv ID: 2411.18138
- Source URL: https://arxiv.org/abs/2411.18138
- Authors: Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang
- Reference count: 30
- Primary result: First codec-free model achieving full-duplex speech understanding and generation with thinking mechanism for asynchronous text and speech generation

## Executive Summary
SALMONN-omni introduces a novel codec-free architecture for full-duplex speech understanding and generation. The model integrates a streaming speech encoder, LLM, and streaming speech synthesizer into a unified end-to-end framework that enables simultaneous listening and speaking. A key innovation is the "thinking" mechanism with special state transition tokens (<start_speak>, <end_speak>, and </tool_call>) that allows asynchronous text and speech generation while maintaining awareness of background sounds. The model employs periodic synchronization blocks to align auditory and textual modalities in real-time streaming scenarios.

## Method Summary
SALMONN-omni combines three core components - a streaming speech encoder, LLM, and streaming speech synthesizer - into an end-to-end architecture with cross-attention layers. The model processes input speech in fixed-duration time blocks (∆t seconds), where the encoder extracts auditory embeddings, the LLM generates word embeddings, and the synthesizer produces output speech when in speaking state. A novel "thinking" mechanism uses special tokens to control state transitions between speaking and non-speaking modes, enabling asynchronous generation. The model is trained on 60k hours of LibriHeavy and 10k hours of GigaSpeech with synthetic data for turn-taking and barge-in scenarios, using a multi-task loss function with weighted components for text, speech, and thinking.

## Key Results
- Achieves full-duplex speech understanding and generation without speech codecs
- Handles turn-taking and barge-in scenarios in simulated experiments
- Demonstrates versatility across streaming speech tasks including recognition, enhancement, and spoken question answering
- Enables simultaneous listening to input speech and background sounds while generating responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALMONN-omni achieves full-duplex speech understanding and generation without using speech codecs by integrating a streaming speech encoder, LLM, and streaming speech synthesizer into a unified end-to-end architecture with cross-attention layers.
- Mechanism: The model processes input speech and generates output speech simultaneously through parallel streams. The streaming speech encoder extracts auditory embeddings from input speech, which are processed by the LLM along with text embeddings. The LLM's output is then sent to the streaming speech synthesizer to generate spoken responses. Cross-attention layers connect these components, allowing information flow between auditory and textual modalities.
- Core assumption: End-to-end training with embeddings can capture the temporal relationships between speech and text without needing quantized speech tokens.
- Evidence anchors:
  - [abstract] "SALMONN-omni is a codec-free, full-duplex speech understanding and generation model that integrates a streaming speech encoder, LLM, and streaming speech synthesizer into a unified end-to-end architecture."
  - [section] "SALMONN-omni functions as an end-to-end model, enabling seamless interaction with users through both verbal and non-verbal (paralinguistic) features."
  - [corpus] Weak evidence - the corpus contains related papers but no direct experimental validation of this specific mechanism.
- Break condition: If the temporal synchronization between input and output streams degrades beyond the periodic synchronization mechanism's capacity, or if the cross-attention layers cannot effectively align modalities.

### Mechanism 2
- Claim: The "thinking" mechanism with special state transition tokens (<start_speak> and <end_speak>) enables asynchronous text and speech generation while simultaneously listening to input.
- Mechanism: The LLM generates n tokens within each time block. When the LLM generates <start_speak>, the model switches to speaking state and sends word embeddings to the speech synthesizer. When it generates <end_speak> or is interrupted by input, it switches to non-speaking state. The </tool_call> token is used as input during "thinking" periods when no specific speech content is being generated.
- Core assumption: The special tokens can effectively control state transitions without disrupting the LLM's generation process.
- Evidence anchors:
  - [abstract] "A novel duplex spoken dialogue framework incorporating a 'thinking' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs"
  - [section] "Specifically, SALMONN-omni has speaking and non-speaking states and two special tokens <start_speak> and <end_speak> are utilized for transition between these two states."
  - [corpus] Weak evidence - corpus papers mention similar concepts but no direct validation of this specific token-based control mechanism.
- Break condition: If the LLM fails to generate the special tokens at appropriate times, or if the model gets stuck in one state due to token generation failures.

### Mechanism 3
- Claim: Periodic synchronization mechanism provides a "time" concept to ensure effective alignment and synchronization of auditory and textual modalities.
- Mechanism: The conversation is divided into time blocks of fixed duration (∆t seconds). In each block, the streaming speech encoder processes input speech, the LLM generates n word embeddings, and if in speaking state, the speech synthesizer generates output speech of the same duration. This creates a consistent temporal framework.
- Core assumption: Fixed-duration time blocks can adequately capture the temporal dynamics of natural conversation without introducing significant latency.
- Evidence anchors:
  - [abstract] "A periodic synchronization mechanism is employed to provide the model with a 'time' concept, ensuring effective alignment and synchronization of auditory and textual modalities."
  - [section] "As shown in Figure 2, the conversation is cut into a series of time blocks. In block i, the streaming speech encoder extracts auditory embeddings from input speech with a fixed duration of ∆t seconds."
  - [corpus] Weak evidence - corpus papers discuss synchronization but no direct experimental validation of this specific periodic approach.
- Break condition: If the fixed time block duration cannot accommodate natural speech rhythm variations, or if synchronization errors accumulate over time.

## Foundational Learning

- Concept: End-to-end model training
  - Why needed here: SALMONN-omni requires joint optimization of all components (speech encoder, LLM, speech synthesizer) to eliminate error propagation and leverage cross-modal information.
  - Quick check question: What loss function components are used to train SALMONN-omni end-to-end?

- Concept: Cross-modal attention mechanisms
  - Why needed here: The model needs to align and integrate information from auditory (speech) and textual modalities in real-time.
  - Quick check question: How do cross-attention layers facilitate information flow between the speech encoder and LLM?

- Concept: State machine control in language models
  - Why needed here: The "thinking" mechanism requires the LLM to switch between speaking and non-speaking states based on special tokens.
  - Quick check question: What special tokens control the state transitions in SALMONN-omni's speaking/non-speaking states?

## Architecture Onboarding

- Component map:
  Streaming speech encoder -> Cross-attention -> LLM -> Cross-attention -> Streaming speech synthesizer

- Critical path: Input speech → Streaming speech encoder → Cross-attention → LLM → Cross-attention → Streaming speech synthesizer → Output speech

- Design tradeoffs:
  - Fixed time blocks (∆t) vs. variable timing for natural speech rhythm
  - Parallel processing for low latency vs. sequential processing for accuracy
  - Unified end-to-end model vs. modular components for flexibility
  - Special token control vs. learned state transitions

- Failure signatures:
  - Audio-visual desynchronization: Output speech timing doesn't match input speech rhythm
  - State transition failures: Model gets stuck in speaking or non-speaking state
  - Cross-modal alignment errors: Speech content doesn't match text content
  - Token generation collapse: <tool_call> token repetition or special token misuse

- First 3 experiments:
  1. Test basic streaming speech recognition without synthesis to validate encoder-LLM pipeline
  2. Test text-to-speech synthesis without input to validate LLM-synthesizer pipeline
  3. Test turn-taking scenarios with synthetic data to validate state transition mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the periodic synchronization mechanism handle variable-length speech segments and ensure consistent alignment between auditory and textual modalities across different speaking rates?
- Basis in paper: [explicit] The paper mentions a periodic synchronization mechanism is employed to provide the model with a "time" concept, ensuring effective alignment and synchronization of auditory and textual modalities.
- Why unresolved: The paper describes the existence of this mechanism but doesn't provide detailed technical specifications about how it handles variable-length segments or different speaking rates.
- What evidence would resolve it: A detailed technical explanation of the synchronization algorithm, including mathematical formulations and empirical results showing performance across different speaking rates.

### Open Question 2
- Question: What is the optimal value for the negative coefficient λthink in the loss function, and how does it affect the model's performance in handling state transitions?
- Basis in paper: [explicit] The paper states that λthink is a negative coefficient applied to the loss function for "thinking" tokens, but doesn't specify an optimal value.
- Why unresolved: The paper mentions the existence of λthink and its negative value but doesn't provide experimental results showing how different values affect performance.
- What evidence would resolve it: Systematic ablation studies showing model performance with different λthink values, including qualitative and quantitative metrics.

### Open Question 3
- Question: How does SALMONN-omni handle real-world acoustic conditions like background noise and reverberation compared to traditional modular systems?
- Basis in paper: [inferred] The paper mentions speech enhancement and dereverberation capabilities but doesn't provide comparative analysis with traditional systems.
- Why unresolved: While the paper claims the model can handle speech enhancement and dereverberation, it doesn't provide benchmark comparisons with existing modular systems.
- What evidence would resolve it: Comparative studies showing performance metrics (e.g., PESQ, STOI scores) against state-of-the-art modular systems under various acoustic conditions.

### Open Question 4
- Question: What is the computational overhead of SALMONN-omni's full-duplex capability compared to half-duplex alternatives, and how does this impact real-time deployment?
- Basis in paper: [inferred] The paper presents a full-duplex architecture but doesn't discuss computational requirements or real-time performance metrics.
- Why unresolved: The paper focuses on architectural design and capabilities but lacks technical specifications about computational requirements and latency measurements.
- What evidence would resolve it: Detailed computational analysis including latency measurements, memory usage, and comparisons with half-duplex alternatives under various hardware configurations.

## Limitations

- The effectiveness of special tokens for state control in real conversations remains unproven
- Periodic synchronization with fixed time blocks may not handle natural speech rhythm variations
- Limited validation of turn-taking and barge-in capabilities beyond synthetic scenarios
- Unclear performance in challenging acoustic environments with background noise

## Confidence

- **High Confidence**: The integration of streaming speech encoder, LLM, and streaming speech synthesizer into an end-to-end architecture is technically feasible and well-established.
- **Medium Confidence**: The concept of using special tokens for state control in full-duplex systems has theoretical merit, though practical effectiveness is unproven.
- **Low Confidence**: Claims about handling turn-taking and barge-in scenarios in real conversations without codecs are largely speculative, as the experimental validation is limited to synthetic scenarios.

## Next Checks

1. **Cross-Modal Alignment Validation**: Conduct experiments measuring the temporal alignment between input speech and generated responses across varying speaking rates and background conditions. Measure desynchronization drift over extended conversations (30+ minutes).

2. **State Transition Robustness Testing**: Create stress tests with rapid state transitions, overlapping speech, and unexpected interruptions to evaluate the reliability of <start_speak>, <end_speak>, and <tool_call> token handling under adverse conditions.

3. **Real Conversation Performance**: Deploy the model in actual human-human conversations with multiple speakers, background noise, and natural interruptions to assess real-world performance beyond synthetic test scenarios.