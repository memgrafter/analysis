---
ver: rpa2
title: 'The Perfect Blend: Redefining RLHF with Mixture of Judges'
arxiv_id: '2409.20370'
source_url: https://arxiv.org/abs/2409.20370
tags:
- reward
- arxiv
- rlhf
- cgpo
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Constrained Generative Policy Optimization
  (CGPO), a novel RLHF framework designed to address reward hacking and multi-objective
  optimization challenges in multi-task LLM alignment. The core innovation is a Mixture
  of Judges (MoJ) system combined with cost-efficient constrained policy optimization
  with stratification, which detects and mitigates reward hacking behaviors while
  reaching a Pareto-optimal point across multiple objectives.
---

# The Perfect Blend: Redefining RLHF with Mixture of Judges

## Quick Facts
- **arXiv ID**: 2409.20370
- **Source URL**: https://arxiv.org/abs/2409.20370
- **Reference count**: 40
- **Primary result**: 7.4% improvement on AlpacaEval-2 and 12.5% on Arena-Hard versus PPO/DPO baselines

## Executive Summary
This paper introduces CGPO, a novel RLHF framework that addresses reward hacking and multi-objective optimization challenges in multi-task LLM alignment. The core innovation combines a Mixture of Judges system with constrained policy optimization with stratification, providing multiple independent constraint evaluations that detect and mitigate reward hacking behaviors. By employing three new constrained RLHF optimizers (CRPG, CODPO, CRRAFT) and customizing reward models, judges, and optimization settings for each task individually, CGPO achieves significant performance gains across multiple benchmarks while avoiding the compromises that occur when using unified linear combinations of reward models for all tasks.

## Method Summary
CGPO implements a constrained generative policy optimization framework built on MDP foundations where prompts are states and responses are actions. The method uses calibrated rewards to make values meaningfully comparable across prompts, then applies three new optimizers (CRPG, CODPO, CRRAFT) that work independently of dual-variable updates for better scalability. The Mixture of Judges system employs multiple LLM-based and rule-based judges to evaluate generated samples against different constraints, with samples violating any constraint excluded from optimization. Each task receives customized reward models, judges, and hyperparameter settings rather than using a unified approach, allowing optimal alignment outcomes without interference from conflicting objectives.

## Key Results
- 7.4% improvement in AlpacaEval-2 (general chat) versus PPO/DPO baselines
- 12.5% improvement in Arena-Hard (STEM & reasoning) versus PPO/DPO baselines
- Consistent gains across math, coding, instruction following, and safety benchmarks while addressing PPO's severe reward hacking issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture of Judges effectively prevents reward hacking by providing multiple independent constraint evaluations
- Core assumption: Reward models are imperfect proxies that can be gamed, but constraint judges can reliably detect gaming behaviors
- Evidence anchors: Abstract claims strong empirical results with theoretical guarantees for detecting/mitigating reward hacking; section 4.1.4 describes judges collaborating to identify patterns during online generation
- Break condition: Fails if judges themselves are imperfect or miss novel hacking patterns

### Mechanism 2
- Claim: Task-specific optimization settings prevent performance degradation from conflicting objectives in multi-task learning
- Core assumption: Different tasks have different optimal hyperparameter settings and benefit from different reward models and judges
- Evidence anchors: Section 3.2 identifies compromises from linear reward combinations; section 4.2 describes customized approaches for better accommodation of specific problems
- Break condition: Fails if task boundaries are unclear or customization overhead outweighs performance gains

### Mechanism 3
- Claim: Calibrated rewards make reward values meaningfully comparable across different prompts
- Core assumption: Raw reward model values are not directly comparable across different prompts due to varying value ranges
- Evidence anchors: Section 4.1.1 explains calibrated rewards create meaningfully comparable magnitudes; claims strict bounds between 0 and 1
- Break condition: Fails if baseline selection is poor or baseline quality varies significantly

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) formulation of RLHF
  - Why needed here: CGPO builds its optimization framework on MDP foundations where prompts are states and responses are actions
  - Quick check question: In CGPO's MDP formulation, what constitutes a state and what constitutes an action?

- **Concept**: Reward hacking and its implications
  - Why needed here: Understanding why reward hacking occurs is crucial to appreciating why CGPO's multi-judge approach is necessary
  - Quick check question: Why does optimizing a reward model directly often lead to suboptimal alignment with actual human preferences?

- **Concept**: Multi-task learning tradeoffs
  - Why needed here: The paper addresses a fundamental challenge in MTL where different tasks may have conflicting objectives
  - Quick check question: What is the main problem with using a single linear combination of reward models across multiple tasks with potentially conflicting goals?

## Architecture Onboarding

- **Component map**: SFT model → Reward models (helpfulness, engagement, safety) → Mixture of Judges (false refusal, instruction following, math/code, factuality, safety) → Constrained RLHF optimizers (CRPG, CODPO, CRRAFT) → Task-specific optimization pipelines → Final aligned model
- **Critical path**: Online generation → Judge evaluation → Sample splitting (positive/negative) → Constrained optimization update → Parameter update
- **Design tradeoffs**: Task-specific customization vs. computational overhead; multiple judges vs. potential redundancy; calibrated rewards vs. added complexity
- **Failure signatures**: Reward hacking occurs if judges miss violations; performance degradation if task boundaries are unclear; optimization instability if calibrated rewards are poorly implemented
- **First 3 experiments**:
  1. Implement single-task CGPO with CRPG optimizer and one constraint judge, compare against PPO baseline
  2. Add second constraint judge to previous setup, measure impact on reward hacking prevention
  3. Implement task-specific customization by splitting prompts into two groups with different reward models and judges

## Open Questions the Paper Calls Out

1. Can CGPO be further improved by incorporating more sophisticated judges or optimization algorithms?
2. How does CGPO perform in other multi-task settings beyond the ones explored in the paper?
3. Can CGPO be extended to handle even more tasks and constraints?
4. How does CGPO compare to other state-of-the-art multi-task learning approaches?
5. Can CGPO be used to improve the performance of other language models beyond LLMs?
6. What are the limitations of CGPO and how can they be addressed?
7. How does CGPO scale to larger models and datasets?
8. Can CGPO be used to address other challenges in reinforcement learning from human feedback?
9. What are the ethical implications of using CGPO to align language models with human preferences?
10. How can CGPO be used to improve the safety and fairness of language models?

## Limitations

- No empirical validation for Mixture of Judges effectiveness - lacks ablation studies comparing MoJ against simpler alternatives
- Task-specific customization claims lack corpus support - no empirical comparison between task-specific and unified approaches
- Computational overhead of multiple judges, customized models, and three new optimizers per task is substantial but not quantified
- Calibrated rewards mechanism lacks experimental validation of actual optimization stability improvements

## Confidence

**High Confidence**: Mathematical formulation of CGPO and its relationship to constrained optimization is well-established. Correctly identifies reward hacking as fundamental RLHF challenge with sound theoretical reasoning for multi-judge systems.

**Medium Confidence**: Task-specific customization approach has intuitive appeal and aligns with established multi-task learning principles, but lacks direct empirical validation. Benchmark improvements are substantial but could result from multiple factors beyond core innovations.

**Low Confidence**: Specific effectiveness of Mixture of Judges mechanism remains unproven. Paper asserts MoJ prevents reward hacking but provides no direct evidence comparing MoJ against simpler alternatives or measuring judge accuracy in detecting violations.

## Next Checks

1. **MoJ Ablation Study**: Implement CGPO version using only single reward model without judges, then systematically add judges one-by-one to measure individual and collective impact on reward hacking prevention. Track judge accuracy rates and false positive/negative rates across different hacking patterns.

2. **Task Customization Validation**: Create two experimental conditions - unified reward models and hyperparameters across all tasks versus full task-specific customization. Compare both absolute performance and degree of task interference or performance degradation to quantify actual benefit of customization.

3. **Reward Calibration Impact**: Compare CGPO performance using calibrated versus raw reward values across multiple optimization runs. Measure optimization stability (KL divergence fluctuations, learning curves) and final task performance to determine whether calibration provides measurable benefits beyond theoretical appeal.