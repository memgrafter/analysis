---
ver: rpa2
title: On the Hardness of Learning One Hidden Layer Neural Networks
arxiv_id: '2410.03477'
source_url: https://arxiv.org/abs/2410.03477
tags:
- learning
- neural
- algorithm
- networks
- polyp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes that learning one-hidden-layer neural networks\
  \ with polynomially small Gaussian noise is computationally hard under standard\
  \ cryptographic assumptions. The authors prove that if there exists a polynomial-time\
  \ algorithm that can weakly learn the class of one-hidden-layer neural networks\
  \ under Gaussian noise of variance \u03C3=1/poly(d), then there exists a polynomial-time\
  \ quantum algorithm that approximates the GapSVP problem within polynomial factors."
---

# On the Hardness of Learning One Hidden Layer Neural Networks

## Quick Facts
- arXiv ID: 2410.03477
- Source URL: https://arxiv.org/abs/2410.03477
- Authors: Shuchen Li; Ilias Zadik; Manolis Zampetakis
- Reference count: 6
- One-line primary result: Learning one-hidden-layer neural networks with polynomially small Gaussian noise is computationally hard under standard cryptographic assumptions.

## Executive Summary
This paper establishes fundamental computational hardness results for learning one-hidden-layer neural networks under Gaussian noise. The authors prove that if such networks can be weakly learned in polynomial time with polynomially small noise, then this would enable polynomial-time quantum algorithms for solving important lattice problems. The key insight is a reduction from the Continuous Learning with Errors (CLWE) problem to learning these neural networks, showing that their learning difficulty is equivalent to solving CLWE.

The result significantly advances our understanding of the computational complexity of neural network learning, particularly for simple architectures with Gaussian noise. It also extends to show that even super-polynomially small noise levels would lead to major cryptographic breakthroughs, establishing a clear boundary between what is computationally feasible and infeasible for learning these networks.

## Method Summary
The authors establish hardness by constructing a specific 1-Lipschitz and 1-periodic function that coincides with a one-hidden-layer neural network on a finite interval. They then show that solving the Continuous Learning with Errors (CLWE) problem reduces to learning this Lipschitz periodic neuron under polynomially small Gaussian noise. This reduction establishes that learning one-hidden-layer neural networks under Gaussian noise is as hard as solving CLWE, which is a well-studied cryptographic problem.

The proof technique involves carefully analyzing the structure of one-hidden-layer neural networks and leveraging properties of periodic functions to create the reduction. The authors also extend their results to super-polynomially small noise levels by showing that polynomial-time learning algorithms would imply polynomial-time quantum algorithms for solving 2^O(n^δ)-GapSVP in dimension n.

## Key Results
- For any width k=ω(sqrt(d log d)), no polynomial-time algorithm can weakly learn one-hidden-layer neural networks under Gaussian noise of variance σ=1/poly(d)
- The hardness result extends to super-polynomially small noise levels with variance σ=2^(-d^η) for any constant η∈(0,1/2)
- Learning one-hidden-layer neural networks under Gaussian noise is as hard as solving the CLWE problem
- Polynomial-time learning algorithms would imply polynomial-time quantum algorithms for 2^O(n^δ)-GapSVP, considered a major cryptographic breakthrough

## Why This Works (Mechanism)
The hardness result works because the authors establish a precise reduction from the CLWE problem to learning one-hidden-layer neural networks. CLWE is a well-studied cryptographic problem that is believed to be computationally hard, forming the foundation of lattice-based cryptography. By showing that any polynomial-time algorithm for learning these neural networks would also solve CLWE, they prove that such learning algorithms cannot exist under standard cryptographic assumptions.

The mechanism relies on constructing a specific 1-Lipschitz and 1-periodic function that can be expressed as a one-hidden-layer neural network on a finite interval. This construction allows them to map instances of the CLWE problem to instances of the neural network learning problem, preserving the computational difficulty. The periodic nature of the function and the Lipschitz constraint are crucial for maintaining the reduction's validity across the entire domain.

## Foundational Learning

**Continuous Learning with Errors (CLWE)**: A cryptographic problem where one must distinguish between noisy random samples and samples from a structured distribution. Needed because it provides the hardness assumption that the neural network learning problem is reduced to. Quick check: Verify that CLWE is indeed believed to be computationally hard and forms the basis of lattice-based cryptography.

**GapSVP (Shortest Vector Problem)**: A fundamental lattice problem where one must distinguish between lattices with short vectors and those without. Needed because solving CLWE efficiently would enable solving GapSVP efficiently, which is a major cryptographic assumption. Quick check: Confirm that GapSVP is widely believed to be computationally hard for quantum computers.

**1-Lipschitz and 1-periodic functions**: Functions with bounded slope and periodic behavior. Needed because they can be constructed to coincide with one-hidden-layer neural networks on finite intervals, enabling the reduction. Quick check: Verify that the specific construction indeed matches the neural network behavior on the required interval.

## Architecture Onboarding

**Component map**: Input data -> One-hidden-layer neural network (width k) -> Gaussian noise (variance σ) -> Learning algorithm -> Output hypothesis

**Critical path**: The reduction from CLWE to learning the neural network is the critical path. The construction of the 1-Lipschitz, 1-periodic function that coincides with the neural network on a finite interval is the key technical component.

**Design tradeoffs**: The width k=ω(sqrt(d log d)) represents a tradeoff between representational capacity and computational hardness. Larger widths make the problem easier to learn but require stronger hardness assumptions.

**Failure signatures**: If the reduction fails, it would likely be due to the constructed function not properly coinciding with the neural network on the required interval, or the noise level being too large to maintain the hardness reduction.

**First experiments**:
1. Implement the specific 1-Lipschitz, 1-periodic function construction and verify it coincides with the neural network on the finite interval
2. Test the reduction by attempting to solve CLWE instances using the proposed learning algorithm framework
3. Experiment with different noise levels to empirically verify the hardness threshold where learning becomes feasible

## Open Questions the Paper Calls Out
None

## Limitations
- The hardness result relies on specific cryptographic assumptions (CLWE problem hardness) that, while standard in cryptography, have not been proven unconditionally hard
- The result applies specifically to Gaussian noise models, which may not capture all realistic noise distributions in practice
- The width requirement of k=ω(sqrt(d log d)) may be restrictive for certain practical applications
- The proof technique requires constructing a specific 1-Lipschitz and 1-periodic function, which may not generalize to all activation functions

## Confidence
- **High Confidence**: The reduction from CLWE to learning one-hidden-layer neural networks is technically sound and follows established cryptographic proof techniques
- **Medium Confidence**: The extension to super-polynomially small noise levels, while theoretically interesting, relies on more speculative assumptions about quantum algorithms for lattice problems
- **Medium Confidence**: The practical implications for real-world neural network training are somewhat limited by the specific assumptions and noise models used

## Next Checks
1. Verify the reduction construction by implementing the specific 1-Lipschitz, 1-periodic function and testing whether it indeed coincides with the neural network on the finite interval as claimed
2. Examine whether similar hardness results can be established for other noise distributions beyond Gaussian, potentially using different cryptographic assumptions
3. Test the practical relevance by comparing the width bounds (k=ω(sqrt(d log d))) with typical neural network architectures used in practice to assess if the result is meaningfully restrictive