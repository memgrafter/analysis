---
ver: rpa2
title: 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities'
arxiv_id: '2410.07722'
source_url: https://arxiv.org/abs/2410.07722
tags:
- entity
- retrieval
- entities
- query
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DyVo, a method to enhance learned sparse
  retrieval (LSR) by augmenting the vocabulary with Wikipedia entities. LSR models
  typically use subword vocabularies that fragment entities into nonsensical pieces,
  limiting their ability to handle entity-rich queries effectively.
---

# DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities

## Quick Facts
- arXiv ID: 2410.07722
- Source URL: https://arxiv.org/abs/2410.07722
- Reference count: 40
- One-line primary result: DyVo significantly outperforms state-of-the-art learned sparse retrieval baselines on entity-rich datasets by integrating dynamic vocabularies with Wikipedia entities.

## Executive Summary
Learned sparse retrieval models struggle with entity-rich queries because subword vocabularies fragment entities into nonsensical pieces, reducing retrieval accuracy and limiting access to up-to-date world knowledge. DyVo addresses this by augmenting the vocabulary with Wikipedia entities through a Dynamic Vocabulary head that leverages pre-existing entity embeddings and an entity retrieval component to identify relevant entities. The approach dynamically scores candidate entities and merges them with word piece representations for efficient retrieval using an inverted index.

Experiments on TREC Robust04, TREC Core 2018, and CODEC datasets demonstrate significant improvements over state-of-the-art LSR baselines, with nDCG@10 scores reaching 56.46 on CODEC when using generative entity retrieval with LLMs like GPT4. The method is robust across different entity embedding techniques, with specialized encoders like BLINK yielding the best results.

## Method Summary
DyVo enhances learned sparse retrieval by integrating a Dynamic Vocabulary head that uses entity embeddings and entity retrieval to augment query and document representations. The model fine-tunes an LSR architecture (DistilBERT backbone, MLP query encoder, MLM document encoder) with KL loss, L1 regularization, and MonoT5-3B distillation on synthetic queries. During training, DyVo leverages entity retrieval to create a compact subset of entities for each batch, dynamically scoring only relevant candidates rather than millions of entities. The approach merges entity representations with word piece representations for efficient inverted index retrieval.

## Key Results
- Achieved nDCG@10 scores up to 56.46 on CODEC dataset using generative entity retrieval with LLMs
- Outperformed state-of-the-art LSR baselines across TREC Robust04, TREC Core 2018, and CODEC datasets
- Demonstrated robustness across different entity embedding techniques, with BLINK yielding optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity retrieval component identifies relevant entities not explicitly mentioned in the text
- Mechanism: The generative entity retrieval approach uses LLMs to infer implicit entities based on query context, expanding beyond explicit mentions
- Core assumption: LLMs can effectively reason about query intent and infer relevant entities even when not explicitly mentioned
- Evidence anchors:
  - [abstract] "The approach is robust to different entity embedding techniques, with specialized encoders like BLINK yielding the best results."
  - [section] "For instance, with the CODEC query 'Why are many commentators arguing NFTs are the next big investment category?', entities like 'Cryptocurrency', 'Bitcoin', and 'Digital asset' can be valuable despite not being explicitly mentioned."
  - [corpus] Weak - no direct evidence in neighboring papers about generative entity retrieval approaches
- Break condition: If LLM reasoning fails to capture relevant entities or introduces too much noise, retrieval effectiveness degrades

### Mechanism 2
- Claim: Dynamic vocabulary head enables efficient scoring of millions of entities without memory overhead
- Mechanism: Instead of scoring all entities, the DyVo head uses entity retrieval to identify a small candidate set, then scores only those candidates
- Core assumption: Entity retrieval can effectively narrow down millions of entities to a manageable candidate set without losing relevant entities
- Evidence anchors:
  - [abstract] "Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document."
  - [section] "During training, DyVo leverages the fact that the vast majority of entities do not appear in any given query (or document) to create a compact subset of the vocabulary for each batch."
  - [corpus] Weak - neighboring papers don't discuss dynamic vocabulary approaches for entity retrieval
- Break condition: If entity retrieval misses important entities or retrieves too many irrelevant ones, the model loses effectiveness

### Mechanism 3
- Claim: Joint word-entity representations resolve ambiguity in sparse representations
- Mechanism: Combining word piece weights with entity weights creates disambiguated representations that can distinguish between different meanings of the same term
- Core assumption: Entity representations capture sufficient semantic information to disambiguate between different meanings of ambiguous terms
- Evidence anchors:
  - [abstract] "Splitting entities can reduce retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data."
  - [section] "By enriching query and document representations with relevant entities, we reduce ambiguity and improve the representational power of LSR."
  - [corpus] Moderate - neighboring papers discuss vocabulary importance but not entity-based disambiguation
- Break condition: If entity embeddings don't capture enough semantic information or entity retrieval fails, ambiguity reduction is ineffective

## Foundational Learning

- Concept: Sparse representation learning
  - Why needed here: LSR models convert queries/documents into sparse vectors for efficient inverted index retrieval
  - Quick check question: What is the key advantage of sparse representations over dense representations for retrieval?

- Concept: Entity linking and retrieval
  - Why needed here: DyVo needs to identify relevant entities from a knowledge base to augment sparse representations
  - Quick check question: How does entity linking differ from entity retrieval in terms of the entities they identify?

- Concept: Knowledge graph embeddings
  - Why needed here: Entity embeddings represent entities in a continuous space for scoring and similarity computation
  - Quick check question: What properties should good entity embeddings have for effective retrieval?

## Architecture Onboarding

- Component map: Query encoder → Entity retrieval → Dynamic Vocabulary head → Document encoder → Inverted index → Retrieval
- Critical path: Query → Entity retrieval → Entity scoring → Joint representation → Index lookup → Scoring
- Design tradeoffs: Memory efficiency vs. entity coverage (trade off between scoring all entities vs. using retrieval to narrow candidates)
- Failure signatures: Poor entity retrieval leads to missing relevant entities; entity embeddings too similar leads to ambiguity; scaling issues with large vocabularies
- First 3 experiments:
  1. Compare DyVo with linked entities vs. word pieces only on a small dataset
  2. Test different entity retrieval methods (BM25 vs. LaQue vs. LLM) on the same dataset
  3. Evaluate different entity embedding approaches (Wikipedia2Vec vs. BLINK vs. DPR) for the same query set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dynamic Vocabulary head handle entity disambiguation when multiple entities have similar names or descriptions?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the Dynamic Vocabulary head uses entity embeddings and a candidate retrieval component to identify relevant entities, but does not provide details on how it resolves ambiguities when entities share similar characteristics.
- What evidence would resolve it: Experimental results comparing the model's performance on queries with ambiguous entities versus unambiguous ones, or a detailed explanation of the disambiguation process in the methodology.

### Open Question 2
- Question: What is the impact of using different entity retrieval methods (e.g., BM25, LaQue, generative models) on the model's ability to generalize to unseen domains or datasets?
- Basis in paper: Inferred
- Why unresolved: While the paper compares the effectiveness of different entity retrieval methods on the tested datasets, it does not explore how these methods affect the model's performance on unseen or out-of-domain data.
- What evidence would resolve it: Results from evaluating the model on a held-out test set from a different domain or dataset, or a detailed analysis of the entity retrieval methods' performance on out-of-domain queries.

### Open Question 3
- Question: How does the choice of entity embeddings (e.g., Wikipedia2Vec, LaQue, BLINK) affect the model's ability to capture semantic relationships between entities?
- Basis in paper: Inferred
- Why unresolved: The paper compares the effectiveness of different entity embedding techniques on retrieval performance, but does not investigate how these embeddings impact the model's understanding of semantic relationships between entities.
- What evidence would resolve it: Results from a task that specifically measures the model's ability to capture entity relationships, such as entity linking or entity-based question answering, or a qualitative analysis of the embeddings' ability to capture entity similarities.

## Limitations

- The approach relies on pre-existing entity embeddings and cannot discover novel entities not present in the knowledge base
- Entity retrieval introduces additional computational overhead that could impact retrieval latency in production settings
- Performance on non-entity-rich queries remains unclear as all evaluation datasets were specifically chosen for entity density

## Confidence

- **High confidence** in the mechanism that dynamic vocabulary scoring improves efficiency over naive entity integration, supported by clear architectural details and ablation studies
- **Medium confidence** in the entity retrieval component's effectiveness, as results show improvement but the paper doesn't fully explore failure cases or robustness to different entity distributions
- **Medium confidence** in the disambiguation benefits, as the paper demonstrates improved metrics but doesn't provide qualitative examples showing how specific ambiguities were resolved

## Next Checks

1. **Cross-domain generalization test**: Evaluate DyVo on non-entity-rich datasets like MS MARCO to assess performance degradation and identify query characteristics that benefit most from entity augmentation

2. **End-to-end latency measurement**: Measure the complete retrieval pipeline latency with different entity retrieval methods (BM25, LaQue, LLM-based) to quantify the real-world performance trade-off

3. **Knowledge base incompleteness stress test**: Systematically remove entities from the knowledge base and measure performance degradation to quantify how critical complete entity coverage is for the approach