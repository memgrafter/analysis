---
ver: rpa2
title: 'Interpreting Bias in Large Language Models: A Feature-Based Approach'
arxiv_id: '2406.12347'
source_url: https://arxiv.org/abs/2406.12347
tags:
- bias
- patching
- llms
- heads
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a feature-based approach to analyzing gender
  bias in large language models (LLMs), focusing on understanding how biases propagate
  through model components. By forming hypotheses about the evolution of bias-related
  features and validating them using interpretability techniques like activation and
  attribution patching, the authors identify key layers and components responsible
  for bias propagation in models like LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3.
---

# Interpreting Bias in Large Language Models: A Feature-Based Approach

## Quick Facts
- arXiv ID: 2406.12347
- Source URL: https://arxiv.org/abs/2406.12347
- Authors: Nirmalendu Prakash; Lee Ka Wei Roy
- Reference count: 14
- Key outcome: Introduces feature-based approach analyzing gender bias in LLMs through component identification and targeted debiasing

## Executive Summary
This paper presents a feature-based approach to analyzing gender bias in large language models by identifying how bias propagates through specific model components. Using interpretability techniques like activation and attribution patching, the authors validate hypotheses about bias evolution across layers and components in models like LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3. The approach successfully mitigates bias through targeted interventions, demonstrating that early MLP layers and specific attention heads are responsible for bias propagation. Results show that targeted debiasing strategies are more effective than general fine-tuning approaches.

## Method Summary
The authors employ interpretability techniques including activation patching and attribution patching to analyze bias propagation in LLMs. They form hypotheses about how bias features evolve through model components, then validate these hypotheses by patching specific MLP layers and attention heads. Targeted debiasing is implemented by fine-tuning identified components on counterfactual datasets. The method focuses on gender bias in profession-to-pronoun associations, measuring bias reduction through logit reversal after patching interventions.

## Key Results
- Early MLP layers (1-6) are identified as primary sites for bias feature formation when processing profession tokens
- Attention heads copy bias features from profession tokens to subsequent positions during generation
- Targeted debiasing of specific MLP layers and attention heads proves more effective than general fine-tuning
- Logit reversal occurs in many samples from layers 1-6 for LLaMA-3-8B and Mistral-7B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bias is introduced in early MLP layers when processing profession tokens, before propagating to later layers.
- **Mechanism:** Early MLP activations at the profession token position generate intermediate gender-related features that influence pronoun prediction. Patching these early activations with anti-stereotypical examples reverses the logit order for pronoun prediction.
- **Core assumption:** Gender bias features are derived directly from the profession token's early MLP activations rather than from abstract feature formation in later layers.
- **Evidence anchors:** [abstract] "Our findings reveal the complex nature of bias in LLMs"; [section] "We observe bias reduction across layers, with logit reversal occurring in many samples from layers 1 to 6"
- **Break condition:** If bias features are instead formed through multi-token interactions or require higher-layer abstractions, early-layer patching would be ineffective.

### Mechanism 2
- **Claim:** Attention heads copy bias features from profession tokens to subsequent token positions during generation.
- **Mechanism:** Attention heads in lower and middle layers transfer gender bias information from the profession token to the final token position, where it gets amplified by MLPs to produce biased outputs.
- **Core assumption:** Attention heads play a primary role in propagating bias features across token positions within the residual stream.
- **Evidence anchors:** [abstract] "We differentiate the roles of MLPs and attention heads in bias propagation"; [section] "We believe these LLMs are more likely to replicate the feature evolution model"
- **Break condition:** If attention heads primarily perform other functions rather than bias propagation, patching attention heads would not affect bias reduction.

### Mechanism 3
- **Claim:** Upper-layer MLPs amplify logit divergence between stereotypical and anti-stereotypical pronoun predictions.
- **Mechanism:** MLPs in higher layers process copied bias features to strengthen the probability gap between "he" and "she" predictions, making bias more pronounced in final outputs.
- **Core assumption:** Higher-layer MLPs serve to amplify rather than introduce bias, with logit reversal occurring primarily in lower/middle layers.
- **Evidence anchors:** [abstract] "Our contributions are threefold: (1) We introduce and empirically validate a feature-based method for bias analysis"; [section] "Given that logit order is achieved by layer 20 in at least half of the samples, we use patching on MLPs of layers 21 and above"
- **Break condition:** If upper-layer MLPs can reverse logit order or introduce new bias features, then targeting only lower layers would be insufficient for debiasing.

## Foundational Learning

- **Concept:** Feature-based interpretability using activation and attribution patching
  - Why needed here: The paper relies on these techniques to validate hypotheses about bias propagation through specific model components
  - Quick check question: How does activation patching differ from attribution patching in terms of computational requirements and precision?

- **Concept:** Causal mediation analysis for component identification
  - Why needed here: The approach builds on causal mediation to identify which components contribute to bias rather than just measuring overall bias presence
  - Quick check question: What is the key difference between identifying bias-causing components versus measuring bias in outputs?

- **Concept:** Counterfactual dataset construction
  - Why needed here: Counterfactual datasets are used to create anti-stereotypical examples for patching and debiasing experiments
  - Quick check question: Why are counterfactual examples preferred over random examples when performing patching experiments?

## Architecture Onboarding

- **Component map:** Input tokens → Embedding layer → Multi-head attention blocks → Feed-forward MLPs → Output logits
- **Critical path:** Profession token → Early MLP activation (layers 1-6) → Attention head copying → Final token MLP → Output logits
- **Design tradeoffs:** Layer-wise patching vs. component-wise patching
  - Layer-wise: Simpler implementation but less precise targeting
  - Component-wise: More precise but computationally expensive (requires per-head analysis)
- **Failure signatures:**
  - No logit reversal after patching: Bias features may be formed through multi-token interactions or require higher-layer abstractions
  - Partial logit reversal: Some bias features may originate from components outside the targeted layers
  - Pronoun reversal without profession retention: Patching may disrupt context understanding
- **First 3 experiments:**
  1. Test early-layer MLP patching at profession token position with anti-stereotypical examples to verify bias origin
  2. Test attention head patching at token positions to the right of profession token to verify bias copying
  3. Test upper-layer MLP patching at final token position to verify bias amplification

## Open Questions the Paper Calls Out

- **Open Question 1:** How do bias mechanisms interact across different languages and cultural contexts, and can the feature-based approach be adapted to detect and mitigate these cross-linguistic biases?
  - Basis in paper: [inferred] The paper focuses on English language bias and mentions future work exploring multilingual settings
  - Why unresolved: The study is limited to English and does not investigate how biases manifest or propagate differently across languages or cultures
  - What evidence would resolve it: Comparative studies applying the feature-based approach to multilingual datasets

- **Open Question 2:** To what extent does the compensation phenomenon affect the accuracy of component identification in bias analysis, and how can this be systematically accounted for in future research?
  - Basis in paper: [explicit] The authors acknowledge the compensation phenomenon as a limitation
  - Why unresolved: The study does not provide methods to detect or control for compensation effects
  - What evidence would resolve it: Experimental designs comparing patched and debiased models, along with techniques to isolate individual component contributions

- **Open Question 3:** Can the feature-based approach be extended to identify and mitigate biases beyond gender, such as racial, ethnic, or intersectional biases, and what challenges would arise in doing so?
  - Basis in paper: [explicit] The authors suggest the approach's potential for broader applications but do not explore biases other than gender
  - Why unresolved: The study is limited to gender bias, and extending the method to other types of bias may require new datasets, metrics, and adaptations
  - What evidence would resolve it: Successful application of the approach to diverse bias types, with validation on specialized datasets and benchmarks

## Limitations
- Analysis focuses exclusively on gender bias in profession-to-pronoun associations, representing a narrow slice of broader bias landscape
- Patching experiments rely on counterfactual datasets that may not capture full complexity of real-world bias manifestations
- Computational cost of per-component patching limits scalability to larger models or more comprehensive bias evaluations

## Confidence
- **High Confidence:** Identification of early MLP layers (1-6) as key sites for bias feature formation is well-supported by activation patching experiments
- **Medium Confidence:** Role of attention heads in copying bias features between token positions is supported by attribution patching results
- **Low Confidence:** Generalizability claims for the feature-based approach across different bias types and model architectures are largely theoretical

## Next Checks
1. **Cross-bias validation:** Apply the feature-based approach to analyze and mitigate racial or cultural biases in the same model families, testing whether early-layer MLP activation patterns similarly indicate bias feature formation for different bias types.

2. **Ablation robustness test:** Systematically remove or randomize individual attention heads identified as bias-propagating to determine whether the observed logit reversal is robust to component removal or whether redundancy exists in the bias propagation pathway.

3. **Long-range dependency analysis:** Test whether bias propagation mechanisms identified for short sequences extend to longer contexts where bias might be introduced through multi-hop reasoning or accumulated over multiple token interactions.