---
ver: rpa2
title: 'AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models'
arxiv_id: '2410.21471'
source_url: https://arxiv.org/abs/2410.21471
tags:
- adversarial
- image
- diffusion
- advi2i
- nsfw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that adversarial image attacks can effectively
  induce diffusion models to generate NSFW content, even when text-based filters block
  malicious prompts. The proposed AdvI2I framework trains an adversarial image generator
  to manipulate image inputs such that the diffusion model produces inappropriate
  content, bypassing defenses like Safe Latent Diffusion and post-hoc safety checkers.
---

# AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models

## Quick Facts
- **arXiv ID:** 2410.21471
- **Source URL:** https://arxiv.org/abs/2410.21471
- **Reference count:** 21
- **Primary result:** Adversarial images can bypass text-based filters in diffusion models, achieving >80% attack success rate for NSFW generation.

## Executive Summary
This paper demonstrates that adversarial image attacks can effectively induce diffusion models to generate NSFW content, even when text-based filters block malicious prompts. The proposed AdvI2I framework trains an adversarial image generator to manipulate image inputs such that the diffusion model produces inappropriate content, bypassing defenses like Safe Latent Diffusion and post-hoc safety checkers. An adaptive variant further improves robustness by minimizing NSFW embedding similarity and adding Gaussian noise. Experiments show AdvI2I achieves over 80% attack success rate on standard I2I models, with the adaptive version maintaining around 70% even under strong defenses. These results reveal a critical vulnerability in I2I diffusion models and underscore the need for stronger safeguards.

## Method Summary
The AdvI2I framework trains an adversarial image generator to produce images that, when input to a diffusion model with benign prompts, guide generation toward NSFW content by aligning latent features with shifted NSFW embeddings. The approach uses a CLIP text encoder to extract NSFW concept vectors from prompt pairs, and a VAE encoder/decoder to process images in latent space. The generator is optimized to minimize a loss combining NSFW embedding alignment and perceptual similarity. An adaptive variant adds Gaussian noise and a term to minimize NSFW embedding similarity. The method is evaluated using attack success rate (ASR) measured by the NudeNet detector for nudity and Q16 classifier for violence, and tested under various defenses like Safe Latent Diffusion and Gaussian noise.

## Key Results
- AdvI2I achieves over 80% attack success rate in generating NSFW content on standard I2I models.
- The adaptive variant maintains around 70% ASR even under strong defenses like Safe Latent Diffusion and Gaussian noise.
- Both methods significantly outperform baselines such as Attack VAE and MMA in terms of ASR and robustness.

## Why This Works (Mechanism)
The attack exploits the fact that diffusion models in I2I tasks rely on both input images and text prompts to guide generation. By manipulating the input image with adversarial perturbations, the attacker can shift the latent space in a direction that aligns with NSFW embeddings, causing the model to generate inappropriate content despite benign prompts. The adaptive variant further improves robustness by minimizing NSFW embedding similarity and adding noise, making it harder for defenses to detect or block the attack.

## Foundational Learning
- **NSFW concept vector extraction:** Needed to identify and quantify NSFW-related features in text prompts; quick check: verify extracted vectors correlate with known NSFW categories.
- **Latent space alignment:** Required to guide the diffusion model's generation toward NSFW content; quick check: measure latent feature similarity before/after attack.
- **Adversarial image generation:** Core mechanism for producing inputs that mislead the model; quick check: inspect generated adversarial images for perceptual quality and attack effectiveness.
- **Defense bypassing:** Essential to demonstrate practical vulnerability; quick check: evaluate ASR under multiple defense mechanisms.

## Architecture Onboarding

**Component map:** Adversarial image generator -> VAE encoder -> Diffusion model -> NSFW classifier

**Critical path:** Adversarial image generation (generator) → Latent feature extraction (VAE encoder) → NSFW embedding alignment → NSFW classifier evaluation (ASR)

**Design tradeoffs:** Generator optimization focuses on NSFW embedding alignment vs. perceptual similarity; adaptive variant adds Gaussian noise and NSFW embedding similarity minimization for robustness, at the cost of increased complexity.

**Failure signatures:** Low ASR due to poor NSFW concept vector extraction or ineffective latent alignment; generator overfitting or perceptual artifacts; defenses blocking or degrading attack effectiveness.

**First experiments:** 1) Train generator on 1800 image-text pairs and evaluate ASR on 200 test samples. 2) Test robustness under Safe Latent Diffusion and Gaussian noise defenses. 3) Compare ASR and perceptual quality (LPIPS, SSIM) against baselines (Attack VAE, MMA).

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on commercial models (Stable Diffusion, Stable Cascade) with undisclosed architectures, limiting reproducibility.
- The NSFW classifier (NudeNet) may not generalize to all types of inappropriate content.
- The attack's transferability to other I2I models or real-world deployment scenarios is untested.
- The adaptive variant's robustness under more diverse or adaptive defenses is unclear.

## Confidence

**Core claim (High):** Adversarial images can induce NSFW generation in diffusion models, as evidenced by clearly reported ASR metrics and reproducible experiments under stated conditions.

**Adaptive variant robustness (Medium):** Limited defense diversity and absence of transfer or adversarial training tests reduce confidence in robustness claims.

**Practical severity (Medium):** Controlled evaluation setting and lack of real-world deployment data limit assessment of real-world impact.

## Next Checks
1. Test the transferability of AdvI2I attacks to other popular I2I diffusion models (e.g., Midjourney, DALL-E) and report ASR across architectures.
2. Evaluate robustness against adaptive defenses, such as online NSFW classifiers and adversarial training of the diffusion model.
3. Conduct a qualitative assessment with human raters to validate classifier-detected NSFW content and measure potential false positives/negatives.