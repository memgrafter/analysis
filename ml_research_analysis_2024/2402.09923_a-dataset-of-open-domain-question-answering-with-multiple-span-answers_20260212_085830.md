---
ver: rpa2
title: A Dataset of Open-Domain Question Answering with Multiple-Span Answers
arxiv_id: '2402.09923'
source_url: https://arxiv.org/abs/2402.09923
tags:
- dataset
- question
- clean
- answer
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAN, a Chinese multi-span question answering
  dataset designed to address the scarcity of open-domain Chinese MSQA benchmarks.
  The dataset was constructed by extracting question-context pairs from Baidu Zhidao,
  a large-scale Chinese knowledge-sharing platform, with contexts curated from long
  user-provided answers to ensure semantic alignment with question intents.
---

# A Dataset of Open-Domain Question Answering with Multiple-Span Answers

## Quick Facts
- arXiv ID: 2402.09923
- Source URL: https://arxiv.org/abs/2402.09923
- Authors: Zhiyi Luo; Yingying Zhang; Shuyun Luo; Ying Zhao; Wentao Lyu
- Reference count: 33
- Key outcome: Introduces CLEAN, a Chinese multi-span QA dataset with 9,063 samples and 46% multi-answer instances

## Executive Summary
This paper introduces CLEAN, a Chinese multi-span question answering dataset designed to address the scarcity of open-domain Chinese MSQA benchmarks. The dataset was constructed by extracting question-context pairs from Baidu Zhidao, a large-scale Chinese knowledge-sharing platform, with contexts curated from long user-provided answers to ensure semantic alignment with question intents. CLEAN contains 9,063 samples, including over 4,200 multi-answer instances (approximately 46%), with a high proportion of descriptive answers (76%), distinguishing it from prior datasets that are biased toward factoid questions. The dataset covers a broad range of open-domain topics and includes established baseline models for evaluation. Experimental results show that CLEAN poses significant challenges for current models, particularly in extracting longer descriptive answers, highlighting its value as a resource for advancing Chinese multi-span QA research.

## Method Summary
The CLEAN dataset was constructed by extracting question-context pairs from Baidu Zhidao, a large-scale Chinese knowledge-sharing platform. Contexts were curated from long user-provided answers to ensure semantic alignment with question intents. The dataset contains 9,063 samples, with over 4,200 multi-answer instances (approximately 46%), and a high proportion of descriptive answers (76%). The construction process involved careful selection of question-context pairs to ensure broad coverage of open-domain topics. The dataset includes established baseline models for evaluation, providing a foundation for assessing model performance on this challenging task.

## Key Results
- CLEAN contains 9,063 samples with over 4,200 multi-answer instances (approximately 46%)
- 76% of answers in CLEAN are descriptive, distinguishing it from prior datasets biased toward factoid questions
- Experimental results show that CLEAN poses significant challenges for current models, particularly in extracting longer descriptive answers

## Why This Works (Mechanism)
The dataset construction methodology leverages user-generated content from Baidu Zhidao, ensuring that contexts are semantically aligned with question intents. By curating contexts from long user-provided answers, the dataset captures complex semantic relationships and longer descriptive answers, which are more challenging for models to extract. This approach addresses the scarcity of open-domain Chinese MSQA benchmarks and provides a diverse set of questions and answers across various topics. The high proportion of descriptive answers (76%) further distinguishes CLEAN from existing datasets, making it a valuable resource for advancing Chinese multi-span QA research.

## Foundational Learning
- **Multi-span QA**: The task of extracting multiple answer spans from a given context. Why needed: To evaluate models' ability to handle complex questions requiring multiple answers. Quick check: Verify that the dataset includes questions with multiple valid answers.
- **Descriptive answers**: Answers that provide detailed explanations rather than simple facts. Why needed: To assess models' ability to extract longer, more complex answers. Quick check: Confirm that a significant portion of answers are descriptive (76% in CLEAN).
- **Open-domain QA**: Question answering across a broad range of topics without domain restrictions. Why needed: To evaluate models' general knowledge and adaptability. Quick check: Ensure the dataset covers diverse topics and is not limited to a specific domain.
- **Chinese language processing**: Techniques and models designed for the Chinese language. Why needed: To address the unique challenges of Chinese text processing, such as word segmentation and character-based representation. Quick check: Verify that the dataset and models are specifically tailored for Chinese language understanding.

## Architecture Onboarding
**Component Map**: Question extraction -> Context curation from Baidu Zhidao -> Multi-span answer annotation -> Baseline model evaluation

**Critical Path**: The critical path involves extracting questions from Baidu Zhidao, curating contexts from long user-provided answers, and annotating multi-span answers. This ensures that the dataset is semantically aligned with question intents and includes a high proportion of descriptive answers.

**Design Tradeoffs**: The use of user-generated content from Baidu Zhidao introduces potential quality variations and biases inherent to crowd-sourced platforms. However, this approach ensures a diverse and large-scale dataset with broad coverage of open-domain topics. The tradeoff is between dataset size and quality control.

**Failure Signatures**: Models may struggle with extracting longer descriptive answers due to their complex semantic relationships. Additionally, the descriptive nature of answers may lead to ambiguity in identifying the correct answer spans. Potential biases in the annotation process could also affect model performance and generalizability.

**First Experiments**:
1. Evaluate baseline models (BERT and RoBERTa) on CLEAN to establish performance benchmarks.
2. Analyze the distribution of answer types (descriptive vs. factoid) to understand dataset characteristics.
3. Conduct error analysis on model outputs to identify specific failure patterns and challenges in extracting descriptive answers.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific features or characteristics of the CLEAN dataset contribute to the observed difficulty in extracting longer descriptive answers, and how can these be addressed in future model designs?
- Basis in paper: [explicit] The paper notes that the CLEAN dataset has a higher prevalence of descriptive answer type instances, which typically feature longer answers with complex semantic relationships, making them more challenging to extract comprehensively.
- Why unresolved: The paper identifies the challenge but does not delve into the specific features or characteristics that make these answers difficult to extract, nor does it propose solutions for future model designs.
- What evidence would resolve it: Detailed analysis of the specific linguistic or structural features of the descriptive answers that pose challenges, along with experiments testing different model architectures or techniques designed to address these features.

### Open Question 2
- Question: How does the performance of models on CLEAN compare when using different pre-training strategies or architectures, such as those specifically designed for long document processing or Chinese language understanding?
- Basis in paper: [inferred] The paper uses standard BERT and RoBERTa models but does not explore the impact of alternative pre-training strategies or architectures, particularly those tailored for long documents or Chinese language.
- Why unresolved: The paper does not investigate the potential benefits of using specialized pre-training strategies or architectures, leaving open the question of whether such approaches could improve performance on the CLEAN dataset.
- What evidence would resolve it: Comparative experiments using models with different pre-training strategies or architectures, such as Longformer or Chinese-specific pre-training, to evaluate their impact on performance.

### Open Question 3
- Question: What are the potential biases introduced by the annotation process in CLEAN, and how might these affect the generalizability of models trained on this dataset?
- Basis in paper: [explicit] The paper mentions the use of three annotators and the calculation of the Fleissâ€™ Kappa score to ensure reliability, but does not discuss potential biases introduced during annotation.
- Why unresolved: While the paper ensures inter-annotator agreement, it does not address potential biases that could arise from the annotation process, such as cultural or linguistic biases, which might affect model generalizability.
- What evidence would resolve it: Analysis of the annotation process to identify potential biases, along with experiments testing model performance across diverse contexts or populations to assess generalizability.

## Limitations
- The dataset construction process relies heavily on user-generated content from Baidu Zhidao, which may introduce quality variations and potential biases.
- The claim about dataset uniqueness and coverage breadth depends on the completeness of the literature review.
- Limited discussion of annotation agreement or inter-rater reliability, which could affect the dataset's reliability.

## Confidence
- **High confidence**: Dataset size (9,063 samples), percentage of multi-answer instances (46%), and the descriptive nature of answers (76%)
- **Medium confidence**: Claims about dataset uniqueness and coverage breadth, as these depend on the completeness of literature review
- **Medium confidence**: Assessment of model performance challenges, given that baseline results are provided but not extensively validated across multiple model architectures

## Next Checks
1. Conduct a systematic error analysis of baseline model outputs to identify specific failure patterns and dataset challenges.
2. Perform a comparative analysis of answer quality and diversity against other established multi-span QA datasets.
3. Validate the dataset's robustness by testing on additional model architectures beyond the established baselines, particularly focusing on handling longer descriptive answers.