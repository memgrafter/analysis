---
ver: rpa2
title: Safety Alignment for Vision Language Models
arxiv_id: '2405.13581'
source_url: https://arxiv.org/abs/2405.13581
tags:
- safety
- safevlm
- content
- performance
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SafeVLM, a method for improving the safety
  of vision-language models (VLMs) by adding safety modules including a safety projector,
  safety tokens, and a safety head. The method is trained in two stages to enhance
  the model's ability to recognize and handle risky content in images.
---

# Safety Alignment for Vision Language Models

## Quick Facts
- arXiv ID: 2405.13581
- Source URL: https://arxiv.org/abs/2405.13581
- Reference count: 40
- Key outcome: SafeVLM achieves a safety score of 8.26 on RTVLM benchmark, surpassing GPT-4V while maintaining general multimodal performance

## Executive Summary
SafeVLM introduces a novel approach to improving safety alignment in vision-language models by adding specialized safety modules. The method addresses vulnerabilities in the visual modality that can be exploited to bypass safety alignments, achieving superior safety performance on the RTVLM benchmark while maintaining general multimodal capabilities. The approach employs a two-stage training process that first trains safety modules before fine-tuning the entire model with clean data.

## Method Summary
SafeVLM enhances VLMs by adding three safety modules: a safety projector that isolates risky visual features, safety tokens that mark unsafe content at the input level, and a safety head that classifies combined features into safety categories and levels. The method uses a two-stage training process: first training the safety modules while freezing the base model, then fine-tuning the entire model with clean and safe data. The approach is implemented on LLaVA-v1.5 models and evaluated across multiple benchmarks including RTVLM, MMBench, and SEEDBench.

## Key Results
- Achieves safety score of 8.26 on RTVLM benchmark, surpassing GPT-4V
- Maintains strong performance on general multimodal benchmarks (MMBench, SEEDBench, MME)
- Demonstrates improved robustness against text attacks with lower attack success rates
- Shows effectiveness across multiple risk categories including politics, illegal content, insults, fairness, privacy, and misleading content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The safety projector module effectively isolates risky visual features before they reach the LLM, reducing safety alignment bypass.
- Mechanism: By adding a dedicated safety projector after the vision encoder, the model can extract and process potential risk-related information from visual features separately from general visual content. This allows the model to handle risky content in a controlled manner before it interacts with the LLM.
- Core assumption: The safety projector can effectively distinguish between safe and risky visual features without significantly degrading the quality of general visual information.
- Evidence anchors:
  - [abstract]: "we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head"
  - [section 3.2]: "we add a new safety projector after the output of the feature by the vision encoder. This extra projector extracts potential risk features from images and interacts with text features."
  - [corpus]: Weak evidence; no direct citation found for the effectiveness of safety projectors in isolating risky features.
- Break condition: If the safety projector cannot effectively distinguish risky features, or if it significantly degrades the quality of safe visual information, the mechanism will fail.

### Mechanism 2
- Claim: Safety tokens provide an additional layer of control by marking unsafe content at the input level to the LLM.
- Mechanism: Trainable safety tokens are added alongside original image tokens and new image tokens. These tokens indicate to the model which visual inputs are safe and which are not, allowing for safety alignment at the LLM's input level.
- Core assumption: The LLM can effectively interpret and act upon the information provided by the safety tokens.
- Evidence anchors:
  - [section 3.2]: "To calibrate and align these features for safety, we introduce trainable safety tokens that indicate to the model which visual inputs are safe and which are not."
  - [abstract]: "we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head"
  - [corpus]: Weak evidence; no direct citation found for the effectiveness of safety tokens in marking unsafe content.
- Break condition: If the LLM fails to interpret the safety tokens correctly, or if the tokens do not effectively mark unsafe content, the mechanism will fail.

### Mechanism 3
- Claim: The safety head provides graded and categorical risk control by classifying the combined features of visual and safety tokens.
- Mechanism: The safety head interacts with text and outputs probabilistic modeling of safety categories and levels. This classifies the combined features of visual and safety tokens, including safety type and level.
- Core assumption: The safety head can accurately classify the combined features into appropriate safety categories and levels.
- Evidence anchors:
  - [section 3.2]: "our safety head, which interacts with text, outputs probabilistic modeling of safety categories and levels. These classify the combined features of visual and safety tokens, including safety type and level."
  - [abstract]: "we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head"
  - [section 3.4]: "During the inference stage, we use the safety head's output to conditionally process safety embeddings."
- Break condition: If the safety head fails to accurately classify the combined features, or if it cannot effectively handle the probabilistic modeling of safety categories and levels, the mechanism will fail.

## Foundational Learning

- Concept: Vision Language Models (VLMs)
  - Why needed here: Understanding VLMs is crucial as SafeVLM builds upon and enhances existing VLMs by adding safety modules.
  - Quick check question: What are the key components of a typical VLM architecture, and how do they interact?

- Concept: Safety Alignment in AI Models
  - Why needed here: SafeVLM aims to improve the safety alignment of VLMs, which is essential for preventing the generation of harmful or inappropriate content.
  - Quick check question: What are the main challenges in achieving safety alignment in AI models, particularly those with multimodal inputs?

- Concept: Adversarial Attacks and Defense Mechanisms
  - Why needed here: SafeVLM addresses vulnerabilities in VLMs that can be exploited through adversarial attacks, particularly those targeting the visual modality.
  - Quick check question: What are some common types of adversarial attacks on AI models, and how do defense mechanisms typically work to mitigate these attacks?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Safety Projector -> Safety Tokens -> Safety Head -> LLM

- Critical path:
  1. Input image is processed by the vision encoder.
  2. Visual features are passed through the safety projector to isolate potential risks.
  3. Safety tokens are added to mark safe and unsafe content.
  4. The safety head classifies the combined features into safety categories and levels.
  5. The LLM processes the input, using the safety information to generate an appropriate response.

- Design tradeoffs:
  - Complexity vs. performance: Adding safety modules increases the model's complexity but improves its safety performance.
  - Granularity vs. computational cost: Providing graded risk control requires more computational resources but allows for more nuanced safety measures.

- Failure signatures:
  - False positives: The model incorrectly identifies safe content as risky.
  - False negatives: The model fails to identify risky content.
  - Over-filtering: The model is overly cautious, rejecting content that is not actually harmful.

- First 3 experiments:
  1. Evaluate the safety performance of SafeVLM on the RTVLM benchmark and compare it to other VLMs.
  2. Test the model's ability to handle different types of risky content, such as pornographic images or politically sensitive material.
  3. Assess the impact of SafeVLM on the model's general performance by evaluating it on multimodal benchmarks like MMBench and SEEDBench.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SafeVLM's safety performance scale with model size, particularly when applied to larger VLMs beyond the LLaVA-v1.5-7B baseline?
- Basis in paper: [inferred] The paper demonstrates SafeVLM's effectiveness on LLaVA-v1.5-7B and mentions LLaVA-v1.5-13B, but doesn't extensively explore scaling to much larger models or how performance might change with model size.
- Why unresolved: The paper focuses on demonstrating the method's effectiveness on a specific model size rather than exploring scalability across a range of model sizes.
- What evidence would resolve it: Comparative experiments applying SafeVLM to VLMs of varying sizes (e.g., 7B, 13B, 30B, 70B parameters) and analyzing safety performance metrics across these sizes would provide insights into scalability.

### Open Question 2
- Question: What is the impact of SafeVLM's safety modules on the computational efficiency and inference speed of VLMs in real-time applications?
- Basis in paper: [inferred] While the paper mentions that computational overhead can be neglected during inference, it doesn't provide detailed analysis of inference speed or computational efficiency impacts.
- Why unresolved: The paper focuses on safety performance and general capability maintenance but doesn't quantify the efficiency trade-offs in practical deployment scenarios.
- What evidence would resolve it: Benchmarking SafeVLM's inference speed and computational requirements compared to baseline VLMs across various hardware configurations would clarify the efficiency impact.

### Open Question 3
- Question: How does SafeVLM's safety performance generalize to languages other than English, and what are the challenges in multilingual safety alignment?
- Basis in paper: [explicit] The paper focuses on English-language benchmarks and datasets without discussing multilingual capabilities or limitations.
- Why unresolved: The safety alignment strategy is demonstrated only on English datasets, leaving open questions about its effectiveness across diverse languages and cultural contexts.
- What evidence would resolve it: Testing SafeVLM on multilingual datasets and evaluating its safety performance across different languages would reveal its generalization capabilities and potential limitations.

## Limitations
- The safety projector's effectiveness in isolating risky visual features remains weakly supported by empirical evidence in the paper.
- The safety tokens mechanism relies heavily on the assumption that the LLM can effectively interpret and act upon these additional tokens.
- The safety head's probabilistic modeling may not scale well to new risk categories not seen during training.

## Confidence
**High Confidence:** The overall two-stage training methodology and the observation that SafeVLM achieves a safety score of 8.26 on the RTVLM benchmark are well-supported by the presented results.

**Medium Confidence:** The claim that SafeVLM maintains general performance while improving safety is supported by benchmark results, but the analysis could be more comprehensive.

**Low Confidence:** The paper's claims about the robustness of the safety modules against sophisticated adversarial attacks are not fully validated.

## Next Checks
1. **Cross-model generalizability test:** Evaluate SafeVLM's safety modules when applied to different base VLM architectures (e.g., Qwen-VL, LLaVA-Next) to assess whether the safety improvements transfer across model families or are specific to LLaVA-v1.5.

2. **Long-tail risk category evaluation:** Test the model's performance on safety categories beyond the six specified ones, particularly emerging risks in areas like deepfakes, AI-generated content, or context-dependent safety concerns that weren't part of the training data.

3. **Adversarial robustness analysis:** Conduct systematic adversarial testing specifically targeting the safety modules, including attempts to generate visual content that could mislead the safety projector, manipulate safety tokens, or confuse the safety head's classification system.