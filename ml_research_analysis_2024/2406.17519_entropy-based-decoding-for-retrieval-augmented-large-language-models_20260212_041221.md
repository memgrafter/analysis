---
ver: rpa2
title: Entropy-Based Decoding for Retrieval-Augmented Large Language Models
arxiv_id: '2406.17519'
source_url: https://arxiv.org/abs/2406.17519
tags:
- knowledge
- document
- decoding
- llms
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the distractibility issue in retrieval-augmented
  large language models (RALMs), where generated responses are negatively influenced
  by noise from both external and internal knowledge sources. The proposed solution,
  CLeHe, employs entropy-based document-parallel ensemble decoding to prioritize low-entropy
  distributions from retrieved documents and a contrastive decoding mechanism that
  contrasts this low-entropy ensemble with a high-entropy distribution derived from
  the model's internal knowledge across layers.
---

# Entropy-Based Decoding for Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2406.17519
- Source URL: https://arxiv.org/abs/2406.17519
- Reference count: 16
- Primary result: Proposed method achieves 11.74% average performance improvement over naive baseline and 4.25% over best baseline on open-domain QA

## Executive Summary
This paper addresses the distractibility issue in retrieval-augmented large language models (RALMs), where generated responses are negatively influenced by noise from both external and internal knowledge sources. The authors propose CLeHe, an entropy-based decoding method that combines document-parallel ensemble decoding with contrastive decoding mechanisms. The approach prioritizes low-entropy distributions from retrieved documents while contrasting them with high-entropy internal knowledge, demonstrating significant performance improvements on open-domain question answering tasks.

## Method Summary
The proposed CLeHe method employs entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, followed by a contrastive decoding mechanism that contrasts this low-entropy ensemble with a high-entropy distribution derived from the model's internal knowledge across layers. The method processes each retrieved document independently, calculates entropy for weighting, and uses the highest-entropy layer without context as a reference for contrast. Experiments demonstrate the method's effectiveness on open-domain QA datasets with various LLM sizes.

## Key Results
- Achieves 11.74% average performance improvement over naive baseline on open-domain QA datasets
- Outperforms best baseline by 4.25% average improvement
- Particularly effective in multi-document scenarios, showing significant improvements when oracle document is surrounded by distractors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based document ensemble weights improve the model's ability to extract relevant information from retrieved documents by assigning higher weights to lower-entropy distributions.
- Mechanism: At each generation step, the model calculates the entropy of the output distribution conditioned on each retrieved document. Documents with lower entropy (indicating more certainty and relevance) receive higher ensemble weights, allowing the model to focus on more informative context.
- Core assumption: Lower entropy in the model's output distribution indicates more relevant and certain information from the retrieved document.
- Evidence anchors:
  - [abstract] "Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents"
  - [section 2.1] "We posit that the uncertainty present in the next-token distribution inherently serves as a reliable indicator of the informativeness of the retrieved documents"
  - [corpus] Weak evidence - only mentions retrieval-augmented methods but no specific entropy-based weighting
- Break condition: If the model's output distribution consistently shows low entropy across all retrieved documents (including irrelevant ones), the weighting mechanism would fail to distinguish relevant from irrelevant context.

### Mechanism 2
- Claim: Layer-wise entropy-based contrastive decoding reduces the influence of parametric knowledge by contrasting high-entropy (uncertain) internal knowledge with low-entropy (certain) external knowledge.
- Mechanism: The model selects the layer with the highest entropy when processing without retrieved context as a proxy for parametric knowledge. It then contrasts this high-entropy distribution with the low-entropy ensemble distribution from retrieved documents, amplifying the influence of external knowledge.
- Core assumption: The layer with highest entropy without context represents the most uncertain (and therefore potentially conflicting) parametric knowledge.
- Evidence anchors:
  - [abstract] "it incorporates a contrastive decoding mechanism that contrasts the obtained low-entropy ensemble distribution with the high-entropy distribution derived from the model's internal knowledge across layers"
  - [section 2.2] "we propose selecting the layer that contains the most 'ambiguous' parametric knowledge among the layers as a proper reference for contrast"
  - [corpus] Weak evidence - mentions contrastive decoding but not specifically layer-wise entropy-based selection
- Break condition: If the highest-entropy layer without context doesn't actually represent conflicting parametric knowledge, the contrast may be ineffective or even harmful.

### Mechanism 3
- Claim: Parallel document processing eliminates position bias that causes the "lost in the middle" phenomenon.
- Mechanism: Instead of concatenating all retrieved documents, each document is processed independently and their outputs are combined with entropy-based weights. This prevents documents in the middle from being overshadowed by those at the beginning or end.
- Core assumption: Position bias in concatenated context causes models to overweight information at sequence boundaries and underweight middle content.
- Evidence anchors:
  - [abstract] "Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents"
  - [section 2] "we propose to alleviate the 'loss in the middle' issue using the product-of-experts ensemble approach"
  - [corpus] Moderate evidence - mentions "lost in the middle" phenomenon but not the specific parallel processing solution
- Break condition: If the model still exhibits position bias even with parallel processing (e.g., due to inherent attention mechanisms), this approach would fail to eliminate the "lost in the middle" issue.

## Foundational Learning

- Concept: Entropy as a measure of uncertainty in probability distributions
  - Why needed here: The method relies on entropy calculations to determine which retrieved documents and model layers contain the most relevant information versus noise
  - Quick check question: Given a probability distribution over 3 tokens with probabilities [0.8, 0.1, 0.1], what is its entropy? (Answer: approximately 0.639 nats)

- Concept: Contrastive decoding and pointwise mutual information (PMI)
  - Why needed here: The method uses contrastive decoding to amplify the difference between external knowledge (from retrieved documents) and internal parametric knowledge
  - Quick check question: If p(high|PMI) = 0.9 and p(high|noPMI) = 0.3, what is the PMI for the high token? (Answer: log(0.9/0.3) = log(3))

- Concept: Ensemble methods and product-of-experts
  - Why needed here: The method combines multiple document distributions using weighted averaging based on their entropy
  - Quick check question: If three document distributions give probabilities of 0.7, 0.6, and 0.8 for a token, and their weights are 0.5, 0.3, and 0.2 respectively, what is the ensemble probability? (Answer: 0.7×0.5 + 0.6×0.3 + 0.8×0.2 = 0.71)

## Architecture Onboarding

- Component map:
  - Retriever -> Parallel processors -> Entropy calculator -> Layer selector -> Contrast module -> Decoder

- Critical path: Query → Retriever → Parallel Document Processing → Entropy Calculation → Ensemble Weighting → Layer Entropy Selection → Contrastive Adjustment → Token Generation

- Design tradeoffs:
  - Memory vs. accuracy: Processing documents in parallel increases memory usage but improves accuracy by eliminating position bias
  - Computation vs. relevance: Calculating entropy at each step adds computation but better identifies relevant information
  - Contrast intensity vs. stability: Higher β values increase external knowledge emphasis but may cause instability if internal knowledge is actually more relevant

- Failure signatures:
  - Performance degrades when retrieved documents are uniformly irrelevant (all high entropy)
  - Model becomes overly dependent on retrieved context, ignoring valid parametric knowledge
  - Decoding latency increases significantly with number of retrieved documents
  - Contrastive adjustment causes the model to generate contradictory statements

- First 3 experiments:
  1. Validate entropy weighting: Compare performance with uniform weights vs. entropy-based weights on a multi-document QA dataset
  2. Test layer selection: Verify that highest-entropy layer without context correlates with conflicting parametric knowledge by manually inspecting outputs
  3. Ablation study: Remove the contrastive component and measure performance degradation to confirm its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the entropy-based decoding method perform on tasks beyond open-domain question answering, such as fact verification or dialogue generation?
- Basis in paper: [inferred] The paper discusses the application of the proposed method on open-domain question answering datasets but mentions the need for further exploration on other knowledge-intensive tasks.
- Why unresolved: The paper explicitly states that the method was only validated on question answering datasets and suggests that extending the method to other retrieval-augmented scenarios, such as fact verification, will be a future research direction.
- What evidence would resolve it: Conducting experiments on additional datasets that represent different knowledge-intensive tasks, such as fact verification or dialogue generation, would provide evidence of the method's generalizability and effectiveness across various applications.

### Open Question 2
- Question: Is the proposed entropy-based decoding method applicable to large language models with more than 13 billion parameters, such as models with 70 billion or more parameters?
- Basis in paper: [explicit] The paper acknowledges that due to computational power constraints, only models with fewer than 13 billion parameters were tested, and it remains to be explored whether the method is applicable to larger models.
- Why unresolved: The paper explicitly states that the method was tested on models with fewer than 13 billion parameters and suggests that its applicability to larger models is an open question.
- What evidence would resolve it: Testing the proposed method on large language models with 70 billion or more parameters and comparing the results with smaller models would provide evidence of its scalability and effectiveness on larger models.

### Open Question 3
- Question: How does the choice of the number of candidate layers (L) for layer-wise entropy-based contrastive decoding affect the performance of the proposed method?
- Basis in paper: [inferred] The paper mentions that the search for the layer with the highest entropy is conducted exclusively among candidate layers, which are set as the last few layers of the LLMs. However, it does not discuss the impact of varying the number of candidate layers.
- Why unresolved: The paper does not provide an analysis of how the number of candidate layers affects the performance of the method, leaving this aspect unexplored.
- What evidence would resolve it: Conducting experiments with different numbers of candidate layers and analyzing the resulting performance would provide insights into the optimal number of layers to consider for effective contrastive decoding.

## Limitations
- Performance heavily depends on quality of retrieved documents
- Computational overhead increases with number of retrieved documents
- Hyperparameter sensitivity requires careful tuning for different tasks

## Confidence

**High Confidence Claims:**
- The entropy-based document ensemble weights effectively prioritize more certain and relevant information from retrieved documents
- The contrastive decoding mechanism provides a meaningful way to balance external and internal knowledge sources
- The method demonstrates superior performance compared to baselines on standard open-domain QA datasets

**Medium Confidence Claims:**
- The layer with highest entropy without context reliably represents conflicting parametric knowledge
- The product-of-experts ensemble approach successfully eliminates position bias in concatenated contexts
- The specific hyperparameter values (τ=1.5, β=2.0) are optimal across all model sizes and datasets

**Low Confidence Claims:**
- The method generalizes equally well to non-QA tasks such as summarization or dialogue
- The entropy-based weighting remains effective when retrieved documents contain substantial noise or contradictions
- The computational overhead is negligible compared to the performance gains

## Next Checks

1. **Retrieval Quality Dependency Test**: Systematically vary the quality of retrieved documents (using oracle vs. noisy passages) to quantify how much the method's performance depends on retrieval accuracy versus its own entropy-based mechanisms.

2. **Cross-Domain Generalization Study**: Evaluate the method on non-QA tasks such as long-form generation, summarization, or conversational AI to determine if the entropy-based weighting and contrastive decoding generalize beyond question answering.

3. **Computational Efficiency Benchmark**: Measure end-to-end latency and memory usage across different numbers of retrieved documents (K=5, 10, 20) and compare against the performance gains to establish practical scalability limits.