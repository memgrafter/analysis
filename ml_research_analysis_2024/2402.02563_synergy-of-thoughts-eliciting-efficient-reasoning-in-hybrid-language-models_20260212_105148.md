---
ver: rpa2
title: 'Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models'
arxiv_id: '2402.02563'
source_url: https://arxiv.org/abs/2402.02563
tags:
- reasoning
- system
- score
- cost
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cost-efficient reasoning framework called
  "Synergy of Thoughts" (SoT) that leverages hybrid large language models (LLMs) of
  different scales. Inspired by the dual process theory of human cognition, SoT uses
  smaller-scale LLMs to generate multiple low-cost intuitive thoughts by default,
  and then employs a confidence evaluator with cross-evaluation and threshold control
  to determine if there are conflicts between these thoughts.
---

# Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models

## Quick Facts
- arXiv ID: 2402.02563
- Source URL: https://arxiv.org/abs/2402.02563
- Reference count: 40
- Primary result: Achieves state-of-the-art reasoning accuracy while reducing API costs by 38.3%-75.1% compared to second most accurate baseline

## Executive Summary
This paper introduces Synergy of Thoughts (SoT), a cost-efficient reasoning framework that leverages hybrid large language models (LLMs) of different scales. Inspired by dual process theory of human cognition, SoT uses smaller-scale LLMs to generate multiple low-cost intuitive thoughts by default, and employs a confidence evaluator with cross-evaluation and threshold control to determine if conflicts exist between these thoughts. When conflicts are detected, a larger-scale LLM is invoked to override and rectify the reasoning process. The framework is model-agnostic and training-free.

## Method Summary
SoT implements a default-interventionist mechanism where small-scale models (System 1) generate multiple intuitive thoughts in parallel, which are then cross-evaluated by all System 1 models to generate confidence scores. A threshold-based decision system determines whether to accept the thought or invoke a large-scale model (System 2) for intervention. The framework uses progressive threshold adjustment, starting at 3.5 and increasing by 10% after each accepted System 1 reasoning step, to prevent bias propagation in multi-step reasoning tasks.

## Key Results
- Achieves state-of-the-art reasoning accuracy across six representative reasoning tasks
- Substantially reduces API costs by 38.3%-75.1% compared to second most accurate baseline
- Maintains solution diversity while improving efficiency
- Model-agnostic framework requiring no additional training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The default-interventionist mechanism achieves cost efficiency by first attempting reasoning with low-cost small-scale models and only invoking expensive large-scale models when conflicts are detected.
- **Mechanism**: System 1 (small-scale models) generates multiple intuitive thoughts in parallel. A confidence evaluator cross-evaluates these thoughts and assigns scores. If the highest score exceeds a threshold, the thought is accepted; otherwise, System 2 (large-scale model) intervenes to override and correct.
- **Core assumption**: Conflicts between intuitive thoughts indicate potential reasoning errors that require reflective correction.
- **Evidence anchors**:
  - [abstract] "By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts... If conflicts are detected, a larger-scale LLM is invoked to override and rectify"
  - [section] "In each reasoning step, SoT prioritizes utilizing System 1 to propose multiple intuitive thoughts... When these intuitive thoughts show apparent conflicts, System 2 will be automatically invoked for intervention"
  - [corpus] Weak - no direct corpus evidence found for this specific default-interventionist mechanism in LLMs
- **Break condition**: If the confidence evaluator cannot reliably detect conflicts, System 2 may be invoked unnecessarily or errors may go uncorrected.

### Mechanism 2
- **Claim**: Cross-evaluation by hybrid models provides diverse perspectives that improve confidence scoring accuracy.
- **Mechanism**: Each intuitive thought is scored by all K small-scale models in System 1. The average score represents the thought's confidence, leveraging diverse knowledge sources.
- **Core assumption**: Different small-scale models have complementary strengths that improve evaluation when combined.
- **Evidence anchors**:
  - [abstract] "We then design a confidence evaluator where the intuitive thoughts are cross-evaluated"
  - [section] "the K LLMs in System 1 will conduct a cross-evaluation, where each intuitive thought is scored in turn by each LLM"
  - [corpus] Weak - no direct corpus evidence found for cross-evaluation improving confidence scoring
- **Break condition**: If models have correlated biases or the evaluation task is outside their collective capability, scores may be unreliable.

### Mechanism 3
- **Claim**: Progressive threshold adjustment prevents bias propagation in multi-step reasoning.
- **Mechanism**: The confidence threshold starts at a base value (3.5) and increases by 10% after each step where System 1 reasoning is accepted, making acceptance criteria stricter over time.
- **Core assumption**: Longer reasoning chains are more susceptible to accumulated errors, requiring higher confidence thresholds.
- **Evidence anchors**:
  - [abstract] "we progressively uplift the confidence threshold with the accumulated number of System 1-based reasoning steps"
  - [section] "we introduce an adjustable threshold value ε... We then introduce a progressively increasing threshold"
  - [corpus] Weak - no direct corpus evidence found for progressive threshold adjustment in reasoning systems
- **Break condition**: If early steps consistently fail the threshold, the process may become overly conservative and invoke System 2 too frequently.

## Foundational Learning

- **Concept**: Dual process theory of human cognition
  - Why needed here: Provides theoretical foundation for the two-system architecture (fast intuitive vs. slow reflective reasoning)
  - Quick check question: What are the key characteristics that distinguish System 1 from System 2 in human cognition?

- **Concept**: Cross-validation and ensemble methods
  - Why needed here: Underlies the confidence evaluation mechanism where multiple models assess each other's outputs
  - Quick check question: How does averaging multiple model evaluations typically improve reliability compared to single-model assessment?

- **Concept**: Threshold-based decision systems
  - Why needed here: Controls when to switch between System 1 and System 2 based on confidence scores
  - Quick check question: What are the trade-offs between high vs. low threshold values in a two-tier decision system?

## Architecture Onboarding

- **Component map**: Input prompt → System 1 (K small-scale models) → Multiple intuitive thoughts → Confidence evaluator (cross-scoring) → Threshold comparison → Decision (accept or invoke System 2) → System 2 (large-scale model) → Output

- **Critical path**: Prompt → System 1 generation → Confidence evaluation → Threshold check → Either output or System 2 intervention → Final output

- **Design tradeoffs**:
  - More System 1 models → higher diversity but increased base cost
  - Higher threshold → fewer System 2 interventions but potentially more errors
  - Progressive threshold → better long-chain accuracy but may over-constrain early steps

- **Failure signatures**:
  - High intervention rate → threshold too low or System 1 models too weak
  - Low accuracy → confidence evaluator unreliable or System 2 not properly correcting
  - High cost → threshold too low or too many System 1 models

- **First 3 experiments**:
  1. Test threshold sensitivity: Run with ε=0, ε=3.5, ε=5 on a single task to observe intervention rate and accuracy trade-off
  2. Model diversity test: Replace hybrid System 1 with 3 identical small models to measure impact on confidence scoring
  3. Progressive threshold validation: Compare fixed vs. increasing threshold on a multi-step reasoning task to measure bias propagation effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal threshold value (ε) for the confidence evaluator in SoT across different reasoning tasks?
- **Basis in paper**: [explicit] The paper mentions trying 7 different threshold values within the range of 0 to 5 and setting ε = 3.5 in experiments, but notes that the optimal threshold may vary depending on the task.
- **Why unresolved**: The paper only provides empirical results for one threshold value and mentions that different tasks might benefit from different thresholds. The impact of threshold selection on the accuracy-cost trade-off is not fully explored.
- **What evidence would resolve it**: Systematic experiments varying the threshold value across all six reasoning tasks to determine the optimal threshold for each task, along with analysis of how threshold selection affects the intervention rate and overall performance.

### Open Question 2
- **Question**: How does the performance of SoT scale with the number of hybrid LLMs (K) in System 1?
- **Basis in paper**: [inferred] The paper mentions using 3 hybrid LLMs in System 1 for the main experiments but does not explore the impact of using more or fewer LLMs on performance and cost.
- **Why unresolved**: The paper does not provide a systematic study of how varying the number of hybrid LLMs in System 1 affects the reasoning accuracy, solution diversity, and token cost of SoT.
- **What evidence would resolve it**: Experiments with different values of K (e.g., 1, 2, 3, 5, 10) on the six reasoning tasks to quantify the trade-off between performance improvement and increased token cost as more LLMs are added to System 1.

### Open Question 3
- **Question**: Can the confidence evaluator in SoT be improved by incorporating additional evaluation criteria beyond cross-evaluation?
- **Basis in paper**: [explicit] The paper mentions that the confidence evaluator uses cross-evaluation among the K LLMs in System 1 to generate confidence scores, but does not explore other potential evaluation criteria.
- **Why unresolved**: The paper only explores one method for confidence evaluation and does not investigate whether incorporating additional criteria (e.g., coherence, logical consistency, or alignment with known facts) could further improve the effectiveness of the confidence evaluator.
- **What evidence would resolve it**: Experiments incorporating additional evaluation criteria into the confidence evaluator and measuring the impact on the accuracy, solution diversity, and token cost of SoT compared to the baseline cross-evaluation method.

## Limitations

- **Cross-evaluation reliability**: The confidence scoring mechanism depends on multiple small-scale models reliably detecting conflicts in intuitive thoughts, which may not hold across all domains or reasoning types.
- **Threshold optimization**: The paper uses a fixed initial threshold (3.5) with progressive adjustment, but optimal values likely vary by task domain, model capabilities, and reasoning complexity.
- **Model selection impact**: The framework assumes hybrid models provide complementary perspectives, but the paper doesn't systematically analyze how different model combinations affect performance.

## Confidence

- **High confidence**: Cost reduction claims (38.3%-75.1% vs baselines) - these are directly measurable from API usage data
- **Medium confidence**: Accuracy improvements - depend on proper threshold calibration and cross-evaluation reliability
- **Low confidence**: Mechanism explanations - the dual-process theory analogy provides intuition but lacks direct empirical validation of the proposed cognitive-inspired mechanisms

## Next Checks

1. **Cross-evaluation ablation study**: Run experiments comparing SoT with single-model evaluation vs. cross-evaluation to quantify the actual contribution of diverse perspectives to confidence scoring accuracy.

2. **Threshold sensitivity analysis**: Systematically test threshold values across [2.0, 4.0] with 0.5 increments on each task to map the accuracy-cost trade-off surface and identify optimal operating points.

3. **Model diversity impact**: Replace hybrid System 1 with varying numbers of identical small models (1, 2, 3 models) while keeping System 2 constant to measure the marginal benefit of model diversity in the confidence evaluation process.