---
ver: rpa2
title: Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists
arxiv_id: '2410.23331'
source_url: https://arxiv.org/abs/2410.23331
tags:
- data
- features
- predict
- train
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FeatEng, a benchmark for evaluating large
  language models on feature engineering tasks. The benchmark presents models with
  tabular datasets and asks them to generate Python code to transform the data, improving
  model performance.
---

# Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists

## Quick Facts
- arXiv ID: 2410.23331
- Source URL: https://arxiv.org/abs/2410.23331
- Reference count: 40
- Primary result: O1-PREVIEW achieved 11.05% improvement in XGBoost accuracy through feature engineering

## Executive Summary
This paper introduces FeatEng, a novel benchmark for evaluating large language models on feature engineering tasks. The benchmark presents models with tabular datasets and asks them to generate Python code to transform the data, improving model performance. Evaluation is based on the improvement in XGBoost model accuracy after applying the generated transformations. The authors tested various state-of-the-art models, with O1-PREVIEW achieving the highest score of 11.05% improvement. The benchmark demonstrates strong correlation with human evaluations while being more efficient to run.

## Method Summary
The FeatEng benchmark evaluates LLMs by providing them with tabular datasets and asking them to generate Python code for feature engineering transformations. The models must write code to improve XGBoost model performance, and the evaluation metric is the percentage improvement in accuracy after applying these transformations. The benchmark includes diverse datasets from various domains and uses both automated metrics and human evaluations for validation. The authors tested multiple state-of-the-art models and compared their performance using Spearman correlation to establish the benchmark's effectiveness in measuring feature engineering capabilities.

## Key Results
- O1-PREVIEW achieved the highest score of 11.05% improvement in XGBoost accuracy
- Strong correlation between automated metrics and human evaluations (r=0.78)
- Models demonstrated varying abilities to integrate domain knowledge, reasoning, and code generation skills
- Benchmark effectively captures feature engineering capabilities beyond isolated skills

## Why This Works (Mechanism)
The benchmark works by directly testing models' ability to perform end-to-end feature engineering tasks that require integration of multiple capabilities. By requiring models to generate working Python code that improves actual model performance, it goes beyond simple multiple-choice or generation-only evaluations. The use of XGBoost accuracy improvements as a concrete metric provides an objective measure of success, while the diverse dataset selection ensures the benchmark tests generalizable skills rather than dataset-specific knowledge.

## Foundational Learning

Domain Knowledge Integration: Understanding how to apply domain-specific transformations to improve model performance. Why needed: Feature engineering often requires domain expertise to identify meaningful transformations. Quick check: Can the model suggest appropriate transformations for a healthcare dataset?

Code Generation and Execution: Ability to write syntactically correct Python code that performs the intended transformations. Why needed: Feature engineering requires executable code, not just conceptual understanding. Quick check: Does the generated code run without errors on the provided dataset?

Model Performance Evaluation: Understanding how transformations affect model accuracy and selecting the most effective ones. Why needed: Not all transformations improve model performance, and models must learn to identify the most impactful ones. Quick check: Does the transformation actually improve XGBoost accuracy?

## Architecture Onboarding

Component Map: Tabular Dataset -> LLM Feature Engineering Code Generation -> Python Code Execution -> XGBoost Model Training -> Accuracy Improvement Measurement

Critical Path: Dataset input -> Code generation -> Code execution -> Model training -> Performance evaluation

Design Tradeoffs: 
- Using XGBoost provides a consistent evaluation baseline but may not generalize to all ML scenarios
- Python code generation allows for complex transformations but requires careful error handling
- Automated metrics enable efficient evaluation but may miss qualitative aspects

Failure Signatures:
- Syntax errors in generated code
- Transformations that decrease model accuracy
- Code that runs but doesn't actually transform the data as intended
- Incorrect understanding of dataset structure or variable types

3 First Experiments:
1. Test code generation on a simple dataset with obvious transformations (e.g., date features)
2. Evaluate model performance on a dataset where domain knowledge is clearly beneficial
3. Compare automated metrics against human evaluation on a small sample of cases

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- Focus on XGBoost may not generalize to other machine learning algorithms or real-world scenarios
- Scoring methodology based on accuracy improvements could be sensitive to dataset characteristics
- Limited human evaluation sample size (20 cases per model) may not capture full diversity of capabilities
- Environmental impact of extensive model evaluations not addressed

## Confidence
- High confidence in the benchmark's construction and methodology
- Medium confidence in the generalizability of results across different model types
- Medium confidence in the correlation between automated and human evaluations
- Low confidence in the benchmark's ability to capture all aspects of real-world feature engineering

## Next Checks
1. Test the benchmark with multiple machine learning algorithms beyond XGBoost to assess generalizability
2. Expand human evaluation sample size and diversity to strengthen correlation claims
3. Apply the benchmark to real-world datasets with known feature engineering challenges to validate practical utility