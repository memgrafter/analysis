---
ver: rpa2
title: Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video
  Generation
arxiv_id: '2405.15881'
source_url: https://arxiv.org/abs/2405.15881
tags:
- diffusion
- image
- generation
- video
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion Mamba (DiM), a novel architecture
  that replaces attention mechanisms in diffusion transformers with Mamba's selective
  state space models for image and video generation. DiM achieves linear complexity
  with sequence length compared to the quadratic scaling of traditional attention-based
  models.
---

# Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation

## Quick Facts
- arXiv ID: 2405.15881
- Source URL: https://arxiv.org/abs/2405.15881
- Authors: Shentong Mo; Yapeng Tian
- Reference count: 35
- Key outcome: DiM replaces attention with Mamba SSMs, achieving linear complexity vs quadratic scaling, outperforming diffusion transformers on ImageNet and UCF-101 with lower FID and reduced Gflops

## Executive Summary
This paper introduces Diffusion Mamba (DiM), a novel architecture that replaces attention mechanisms in diffusion transformers with Mamba's selective state space models for efficient image and video generation. By leveraging bidirectional state space models, DiM achieves linear complexity with sequence length compared to the quadratic scaling of traditional attention-based models. The method demonstrates strong performance on both image generation benchmarks like ImageNet and video generation tasks like UCF-101, establishing a new standard for efficient generative models that scale effectively to high-resolution outputs.

## Method Summary
DiM integrates bidirectional state space models into diffusion processes, replacing the attention mechanisms typically used in diffusion transformers. The architecture employs Mamba's selective state space models to process sequences in both forward and backward directions, enabling efficient capture of long-range dependencies without the quadratic computational complexity of traditional attention. The selective scan operation allows the model to process information in a linear fashion while maintaining the ability to generate high-quality images and videos. The integration is achieved by incorporating the bidirectional SSMs into the denoising U-Net architecture commonly used in diffusion models.

## Key Results
- Outperforms diffusion transformers across multiple model sizes on ImageNet with lower FID scores
- Achieves linear complexity with sequence length versus quadratic scaling of attention-based models
- Demonstrates strong performance on video generation benchmarks like UCF-101 while reducing computational cost (Gflops)

## Why This Works (Mechanism)
The mechanism works by replacing the attention mechanism's quadratic complexity with Mamba's selective state space models, which process sequences linearly. The bidirectional nature allows the model to capture context from both directions simultaneously, improving the quality of generated outputs. The selective scan operation enables efficient processing by focusing on relevant information while discarding less important details, maintaining generation quality while reducing computational overhead.

## Foundational Learning

1. **Selective State Space Models**: Why needed - To replace attention with linear complexity processing. Quick check - Verify the selective mechanism properly filters relevant information.

2. **Bidirectional Processing**: Why needed - To capture context from both directions for improved generation quality. Quick check - Confirm bidirectional information flow doesn't introduce artifacts.

3. **Diffusion Process Integration**: Why needed - To maintain the noise-adding/denoising framework while improving efficiency. Quick check - Ensure diffusion steps maintain proper noise schedules.

4. **Mamba Architecture**: Why needed - To provide the underlying framework for efficient sequence processing. Quick check - Validate Mamba parameters are properly tuned for generation tasks.

5. **U-Net Architecture**: Why needed - To provide the denoising structure for diffusion models. Quick check - Confirm U-Net blocks properly integrate with bidirectional SSMs.

## Architecture Onboarding

**Component Map**: Input -> Bidirectional SSM Blocks -> Selective Scan -> Denoising U-Net -> Output

**Critical Path**: The core processing path involves bidirectional SSM blocks performing selective scanning, feeding into the denoising U-Net architecture for generation.

**Design Tradeoffs**: The primary tradeoff is between computational efficiency (achieved through linear complexity) and potential loss of global attention capabilities. The bidirectional approach attempts to mitigate this by capturing context from both directions.

**Failure Signatures**: Potential failures include loss of long-range dependencies that attention might capture, artifacts from bidirectional processing conflicts, and suboptimal denoising if SSM parameters are not properly tuned.

**First Experiments**:
1. Test basic bidirectional SSM functionality on synthetic sequences
2. Validate selective scan operation preserves important information
3. Benchmark computational complexity against attention-based baselines

## Open Questions the Paper Calls Out
None

## Limitations

**Performance Generalization**: Results limited to specific datasets (ImageNet, UCF-101), unverified on other domains like medical imaging or satellite imagery.

**Computational Efficiency Claims**: Real-world deployment scenarios and hardware-specific optimizations not thoroughly explored.

**Bidirectional SSM Integration**: Potential drawbacks and failure modes of integrating bidirectional processing with diffusion models not extensively discussed.

## Confidence

**Performance Generalization**: Medium - Limited to specific datasets, generalizability across computer vision tasks unverified

**Computational Efficiency Claims**: Medium - Real-world deployment and hardware-specific validation needed

**Bidirectional SSM Integration**: Medium - Potential complications and failure modes not thoroughly explored

## Next Checks

1. **Cross-Dataset Validation**: Conduct comprehensive testing on diverse image and video datasets beyond ImageNet and UCF-101, including specialized domains like medical imaging, remote sensing, and long-form video content.

2. **Real-World Deployment Analysis**: Implement DiM on various hardware architectures (GPU, CPU, specialized AI accelerators) and evaluate performance under different batch sizes and memory constraints.

3. **Extreme Resolution Testing**: Test DiM's performance on ultra-high-resolution image and video generation tasks (e.g., 4K video or higher resolution images) to identify scaling limitations.