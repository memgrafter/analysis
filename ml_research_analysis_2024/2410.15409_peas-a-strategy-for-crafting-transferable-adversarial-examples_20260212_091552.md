---
ver: rpa2
title: 'PEAS: A Strategy for Crafting Transferable Adversarial Examples'
arxiv_id: '2410.15409'
source_url: https://arxiv.org/abs/2410.15409
tags:
- adversarial
- attack
- peas
- transferability
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PEAS (Perception Exploration Attack Strategy),
  a novel approach for improving the transferability of adversarial examples in black-box
  settings. The core insight is that perceptually equivalent images exhibit significant
  variability in their adversarial transferability.
---

# PEAS: A Strategy for Crafting Transferable Adversarial Examples

## Quick Facts
- **arXiv ID**: 2410.15409
- **Source URL**: https://arxiv.org/abs/2410.15409
- **Authors**: Bar Avraham; Yisroel Mirsky
- **Reference count**: 33
- **Primary result**: PEAS can double the performance of existing attacks, achieving 2.5x improvement in attack success rates on average over current ranking methods

## Executive Summary
This paper presents PEAS (Perception Exploration Attack Strategy), a novel approach for improving the transferability of adversarial examples in black-box settings. The core insight is that perceptually equivalent images exhibit significant variability in their adversarial transferability. PEAS works by generating multiple perceptually equivalent versions of an input image through subtle augmentations, creating adversarial examples for each variant using a substitute model, and selecting the most transferable adversarial example based on expected transferability scores across multiple substitute models. The method is evaluated on CIFAR-10 and ImageNet datasets, demonstrating significant improvements over existing black-box attack methods.

## Method Summary
PEAS improves black-box adversarial attack transferability by exploring perceptually equivalent image variants. The method generates 200 perceptually similar samples using augmentations like RandomAffine, ColorJitter, RandomCrop, GaussianBlur, RandomAdjustSharpness, and RandomAutocontrast. For each variant, adversarial examples are created using a substitute model (typically PGD). The expected transferability (ETF) of each adversarial example is computed across a diverse set of substitute models, and the example with highest ETF is selected for the final attack. This exploration-exploitation strategy significantly improves attack success rates compared to standard transfer attacks.

## Key Results
- PEAS achieves 2.5x improvement in attack success rates on average over current ranking methods
- When boosting existing black-box attacks, BTA-PEAS achieves 7.4x better performance than TIMI and 1.6x better than SimBA on average
- Performance converges around exploration size of n=200 perceptually equivalent samples
- PEAS demonstrates consistent improvements across CIFAR-10 and ImageNet datasets with various network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subtle augmentations preserve the "robust features" of an image while perturbing its starting position relative to the model's decision boundary.
- Mechanism: By shifting or slightly rotating an image, the augmentation moves the sample to a new location in feature space without destroying the semantic content that classifiers rely on. This new location may align better with the gradient directions of unknown models.
- Core assumption: Decision boundaries are primarily shaped by robust features, so moving the sample along these features without destroying them yields better gradient alignment.
- Evidence anchors:
  - [abstract] "perceptually equivalent images maintain the adversarial objective of stealth" and "samples which are perceptually equivalent exhibit significant variability in their adversarial transferability"
  - [section 3.1] "An interesting observation is that an image with a subtle augmentation has its robust features (i.e., the main features used in classification) perturbed, whereas an image with additive noise has its non-robust features (noise patterns) perturbed"
  - [corpus] No direct support found; claim is novel to this paper.
- Break condition: If the augmentation alters non-robust features too much, or if the model's decision boundary is not primarily determined by robust features, the transferability benefit will vanish.

### Mechanism 2
- Claim: Generating adversarial examples from multiple perceptually equivalent starting points increases the chance of finding one that crosses the target model's decision boundary.
- Mechanism: PEAS creates many starting points near the original image, each with slightly different perturbations to robust features. The adversarial attack on each starting point explores a different region of the loss surface, increasing the odds that one region aligns with the target model's boundary.
- Core assumption: Different starting points yield different local optima on the loss surface, and at least one will align better with the target model than the original starting point.
- Evidence anchors:
  - [abstract] "samples which are perceptually equivalent exhibit significant variability in their adversarial transferability"
  - [section 3.2] "By employing a sampling strategy that generates samples that are perceptually equivalent to ùë•, we can enhance the probability of creating a sample with improved transferability"
  - [corpus] No direct support found; claim is novel to this paper.
- Break condition: If all starting points map to similar loss surface regions (e.g., if the augmentation doesn't truly alter the decision boundary proximity), transferability gains will plateau.

### Mechanism 3
- Claim: Ranking adversarial examples by their expected transferability across substitute models selects the most transferable example.
- Mechanism: For each adversarial example generated from a perceptually equivalent starting point, PEAS estimates how often it fools a diverse set of substitute models. The example with the highest expected transferability (ETF) is chosen, as it is likely to also fool the target model.
- Core assumption: Substitute models in F approximate the behavior of the unknown target model, so high ETF correlates with high target model success.
- Evidence anchors:
  - [section 3.3] "Since we don't have access to the target modelùëì, we estimate the transferability of each adversarial example inùëã‚Ä≤ using the expected transferability metric (ET) [15]"
  - [section 3.2] "Let ùëÜ (ùë•) denote a sampling function that produces a sample near ùë•. As shown by Levy et al. [15], among the samples produced by ùëÜ (ùë•), there exists a sample which, if attacked using substitute ùëì ‚Ä≤, will exhibit superior transferability to an unknown model ùëì"
  - [corpus] Limited; the referenced [15] is cited but not fully detailed in the corpus.
- Break condition: If the substitute models in F are too dissimilar from the target model, ETF will not correlate with actual transferability.

## Foundational Learning

- Concept: Perceptual equivalence and robust vs. non-robust features
  - Why needed here: Understanding why subtle augmentations (e.g., pixel shifts) can maintain stealth while improving transferability requires knowing how models use robust features for classification.
  - Quick check question: What is the difference between robust and non-robust features, and why would perturbing robust features be more effective for adversarial transferability than adding noise?

- Concept: Expected transferability (ET) metric and ranking strategies
  - Why needed here: PEAS uses ET to select the best adversarial example; understanding how ET is computed and why it works is critical to implementing or extending the method.
  - Quick check question: How does the ET metric compute transferability, and why does it rely on multiple substitute models rather than a single one?

- Concept: Black-box adversarial attacks and transferability limitations
  - Why needed here: PEAS is a strategy to improve transferability in black-box settings; understanding the baseline problem (poor transferability from substitute models) contextualizes why PEAS is effective.
  - Quick check question: Why do adversarial examples often fail to transfer between models, and how does gradient alignment relate to this problem?

## Architecture Onboarding

- Component map: Input image ‚Üí Sampling function S ‚Üí Attack algorithm ‚Üí Expected transferability evaluator ‚Üí Selection module ‚Üí Output adversarial example
- Critical path:
  1. Input image ùë•
  2. Generate ùëõ perceptually equivalent samples via ùëÜ
  3. Attack each sample with substitute model ùëì ‚Ä≤ ‚Üí adversarial examples
  4. Compute ETF for each adversarial example using F
  5. Select highest ETF adversarial example ‚Üí output ùë•‚àó
- Design tradeoffs:
  - Sampling breadth (ùëõ) vs. computational cost: larger ùëõ explores more starting points but increases runtime
  - Augmentation strength vs. stealth: stronger augmentations may improve transferability but risk perceptibility
  - Substitute model diversity in F vs. ranking reliability: more diverse models improve ETF accuracy but increase evaluation cost
- Failure signatures:
  - Low variance in ETF scores: sampling function not producing diverse enough starting points
  - All adversarial examples fail to transfer: attack algorithm or substitute model ineffective, or augmentations too weak
  - High computational cost with marginal gains: exploration size ùëõ too large relative to transferability improvement
- First 3 experiments:
  1. Verify sampling function produces perceptually equivalent images by visual inspection and p-norm checks
  2. Confirm ET ranking correlates with actual transferability by testing on a known target model
  3. Measure attack success rate improvement when replacing random starts with PEAS sampling on a simple transfer attack

## Open Questions the Paper Calls Out
- How does PEAS perform when applied to targeted adversarial attacks, where the adversary aims to mislead the model into predicting a specific target class?
- How do different sampling functions affect the transferability of adversarial examples, and can more sophisticated sampling functions be developed to further improve PEAS's performance?
- How does the exploration size (ùëõ) impact the trade-off between attack performance and computational cost in PEAS?

## Limitations
- The computational cost of generating 200 perceptually equivalent samples per input image is substantial
- The effectiveness of the expected transferability metric depends heavily on the diversity and quality of the substitute model set F
- The method's generalizability to non-image domains like NLP or graph-based models is unclear

## Confidence
- **Confidence: Low** - The core claim about perceptual equivalence variability is novel but lacks theoretical foundation
- **Confidence: Medium** - Computational cost concerns are valid but may be offset by improved success rates
- **Confidence: Medium** - ETF effectiveness depends on substitute model quality and diversity

## Next Checks
1. Conduct theoretical analysis of why perceptually equivalent images exhibit different transferability scores and test correlation with decision boundary properties
2. Implement progressive sampling strategy to dynamically allocate computational resources based on early ETF score variance
3. Test PEAS on non-image domains (text classification, graph neural networks) to evaluate cross-domain generalization