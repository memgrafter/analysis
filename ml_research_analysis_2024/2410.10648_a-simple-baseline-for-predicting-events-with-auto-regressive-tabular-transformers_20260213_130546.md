---
ver: rpa2
title: A Simple Baseline for Predicting Events with Auto-Regressive Tabular Transformers
arxiv_id: '2410.10648'
source_url: https://arxiv.org/abs/2410.10648
tags:
- data
- event
- step
- events
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a simple transformer-based baseline called
  STEP for predicting events in sequential tabular data. The key innovation is converting
  event data into a sequence of tokens and training a decoder-only autoregressive
  model with a causal language modeling objective, avoiding complex time-aware positional
  embeddings or specialized row/field encodings.
---

# A Simple Baseline for Predicting Events with Auto-Regressive Tabular Transformers

## Quick Facts
- arXiv ID: 2410.10648
- Source URL: https://arxiv.org/abs/2410.10648
- Reference count: 40
- Primary result: STEP achieves state-of-the-art performance on sequential tabular event prediction tasks using a simple decoder-only transformer

## Executive Summary
This paper introduces STEP (Simple Transformer for Event Prediction), a straightforward transformer-based baseline for predicting events in sequential tabular data. The key innovation is converting event data into sequences of tokens and training a decoder-only autoregressive model with a causal language modeling objective. STEP achieves strong performance across popular datasets without requiring complex time-aware positional embeddings or specialized row/field encodings, and supports flexible use-cases like imputing missing values or modeling event sequences through simple data augmentations.

## Method Summary
STEP is a decoder-only transformer that treats sequential tabular data as sequences of tokens. The method tokenizes each column separately with word-level tokenization, uses standard positional encoding combined with a temporal feature, and trains with causal masking. Two key data augmentations enable flexibility: randomizing column order during training allows the model to handle missing data, while masking labels from previous events enables label prediction without requiring past labels. The same architecture can perform multiple tasks (label prediction, missing value imputation, event sequence modeling) through simple masking adjustments.

## Key Results
- Achieves state-of-the-art AUC of 0.998 on synthetic credit card transaction dataset
- Outperforms baselines on Amazon Electronics reviews (AUC 0.771) and Czech Bank Loan dataset (AUC 0.942)
- Demonstrates zero-shot capabilities on tabular data using the same architecture
- Shows flexibility across multiple event prediction tasks without task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** STEP outperforms existing approaches by treating event data as sequences of tokens and using a causal language modeling objective.
- **Mechanism:** By converting each event into a sequence of tokens and training with a causal language modeling loss, STEP leverages the powerful sequence modeling capabilities of decoder-only transformers without requiring complex time-aware positional embeddings or specialized row encodings.
- **Core assumption:** The ordering and timing of events contain important information that can be effectively captured by standard positional encoding when combined with a temporal feature.
- **Evidence anchors:**
  - [abstract] "Our baseline outperforms existing approaches across popular datasets and can be employed for various use-cases."
  - [section] "STEP is a decoder-only auto-regressive model that surpasses state-of-the-art methods on popular event prediction datasets."
  - [corpus] Weak - corpus does not contain specific evidence about STEP's performance relative to other approaches.
- **Break condition:** If event sequences require complex temporal dependencies that cannot be captured by standard positional encoding and temporal features, STEP's performance may degrade.

### Mechanism 2
- **Claim:** Randomizing column order during training enables STEP to handle missing and partial information without requiring fine-tuning.
- **Mechanism:** By randomly shuffling the order of features within each event during training, STEP learns that there is no inherent causal relationship between features within an event. This allows the model to predict any feature using any subset of other features, effectively handling missing data.
- **Core assumption:** Events occur simultaneously, so the order of features within an event should not matter for prediction.
- **Evidence anchors:**
  - [section] "We introduce two simple data augmentations during training that enable STEP to perform a variety of useful event prediction tasks without complicating the training pipeline and architecture: (1) We randomly shuffle the columns within a row."
  - [section] "By randomizing the feature order during training, the model learns that there is no set order of the columns, removing the causal bias that is inherent in autoregressive models."
  - [corpus] Weak - corpus does not contain specific evidence about the effectiveness of feature randomization.
- **Break condition:** If the model overfits to the random ordering and cannot learn meaningful feature relationships, or if certain features are critical and their order matters.

### Mechanism 3
- **Claim:** STEP can predict labels, impute missing values, or model event sequences using the same architecture through simple data augmentations.
- **Mechanism:** By masking labels from previous events during training, STEP learns to predict current event labels without requiring knowledge of past labels. The same model can then be used for different tasks by simply adjusting which features are masked or predicted.
- **Core assumption:** In real-world scenarios, we may not know the labels of previous events when predicting the label of a new event.
- **Evidence anchors:**
  - [abstract] "We demonstrate that the same model can predict labels, impute missing values, or model event sequences."
  - [section] "We mask labels from previous events, reflecting the fact that we may not know the labels of previous events while predicting the label of a new event."
  - [corpus] Weak - corpus does not contain specific evidence about STEP's flexibility across different tasks.
- **Break condition:** If the model cannot learn to handle different masking patterns or if certain tasks require fundamentally different architectures.

## Foundational Learning

- **Concept: Causal Language Modeling**
  - Why needed here: Causal language modeling allows STEP to predict the next token in a sequence based on previous tokens, which is essential for event prediction where we want to predict future events based on past events.
  - Quick check question: How does causal masking prevent the model from "cheating" by looking at future tokens during training?

- **Concept: Tokenization of Tabular Data**
  - Why needed here: Converting tabular data into tokens allows STEP to leverage existing transformer architectures designed for language modeling, rather than requiring specialized tabular processing.
  - Quick check question: Why is it important to have separate vocabularies for each column during tokenization?

- **Concept: Positional Encoding**
  - Why needed here: Positional encoding provides information about the order of events in the sequence, which is crucial for capturing temporal dependencies in event data.
  - Quick check question: What is the difference between absolute and relative positional encoding, and when might each be preferred?

## Architecture Onboarding

- **Component map:**
  Tokenizer -> Decoder-only transformer -> Loss function (cross-entropy) -> Data augmentation (column randomization, label masking)

- **Critical path:**
  1. Preprocess data (tokenize, create sequences)
  2. Train transformer with causal language modeling objective
  3. Evaluate on different tasks by adjusting masking patterns

- **Design tradeoffs:**
  - Simplicity vs. performance: STEP sacrifices some complexity for flexibility and ease of use
  - Context length: Limited by quadratic attention complexity in transformers
  - Tokenization granularity: Word-level vs. subword tokenization affects model size and performance

- **Failure signatures:**
  - Poor performance on datasets with complex temporal dependencies
  - Inability to handle heterogeneous event types with different features
  - Overfitting to training data due to limited context length

- **First 3 experiments:**
  1. Train STEP on synthetic credit card dataset with default settings and evaluate on last label prediction
  2. Train STEP with column randomization enabled and compare performance on last label prediction
  3. Train STEP with label masking and evaluate on missing value imputation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can STEP be effectively extended to handle heterogeneous event types (e.g., credit card payments, bill payments, ATM transactions) within the same user sequence?
- Basis in paper: [explicit] The paper discusses multiple event types as an outstanding challenge, noting that real-world applications may include events with different features.
- Why unresolved: The paper only tests on datasets where all events have the same features, leaving the model's ability to handle heterogeneous event types untested.
- What evidence would resolve it: Experiments on datasets containing multiple event types with varying features, demonstrating whether STEP can learn meaningful representations across different event categories.

### Open Question 2
- Question: How does STEP's performance scale with very long context lengths (e.g., thousands of events) given the quadratic complexity of transformer attention mechanisms?
- Basis in paper: [explicit] The paper identifies long context lengths as an outstanding challenge, noting that the cost of attention increases quadratically with context length.
- Why unresolved: The experiments are limited to relatively short sequences (10-100 events), and the paper does not explore how performance degrades with significantly longer sequences.
- What evidence would resolve it: Benchmarking STEP on datasets requiring much longer sequences (thousands of events) and comparing performance against efficient transformer variants or alternative architectures.

### Open Question 3
- Question: Would converting each event into a single token embedding (rather than tokenizing individual features) improve computational efficiency without sacrificing predictive performance?
- Basis in paper: [explicit] The paper discusses this as a potential limitation and trade-off, noting that such techniques might reduce context length but could require predicting all attributes simultaneously.
- Why unresolved: The paper uses the current tokenization approach and does not test alternatives that embed entire events as single tokens.
- What evidence would resolve it: Comparative experiments between the current STEP approach and an event-embedding variant, measuring both computational efficiency and predictive accuracy across tasks.

## Limitations

- Performance may degrade on datasets requiring complex temporal dependencies beyond what standard positional encoding can capture
- Scalability concerns due to quadratic attention complexity with long event sequences
- Limited evidence of generalizability across diverse domains beyond financial and e-commerce applications

## Confidence

**High Confidence:** The core architectural approach of using a decoder-only transformer with causal language modeling for event prediction is well-established and the implementation details are clearly specified.

**Medium Confidence:** The performance claims are reasonably supported by the reported AUC scores, though the limited number of datasets and lack of comprehensive ablation studies reduce confidence in the superiority claims.

**Low Confidence:** The claims about STEP's flexibility for multiple tasks (label prediction, missing value imputation, event sequence modeling) are stated but not thoroughly validated with quantitative comparisons across all three use cases.

## Next Checks

1. **Ablation Study on Column Randomization:** Run STEP with and without column randomization on the existing datasets to quantify the impact of this feature on performance, particularly for tasks involving missing data or partial information.

2. **Cross-Domain Evaluation:** Test STEP on at least two additional event prediction datasets from different domains (e.g., healthcare patient records or IoT sensor data) to assess generalizability beyond financial and e-commerce applications.

3. **Scalability Analysis:** Measure and report training time, inference latency, and memory usage for STEP compared to at least one specialized tabular event prediction method, particularly for datasets with varying sequence lengths and numbers of events.