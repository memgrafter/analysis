---
ver: rpa2
title: Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens
arxiv_id: '2410.14655'
source_url: https://arxiv.org/abs/2410.14655
tags:
- training
- generated
- learning
- methods
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two approaches, Batch-Scheduled Sampling (BASH)
  and Reference-Answer-based Correction (RAC), to address the training-inference gap
  in large language models (LLMs). The gap arises because LLMs are typically trained
  using teacher forcing (ground truth tokens) but must rely on their own generated
  tokens during inference.
---

# Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens

## Quick Facts
- arXiv ID: 2410.14655
- Source URL: https://arxiv.org/abs/2410.14655
- Reference count: 22
- Proposed two methods (BASH and RAC) to address training-inference gap, achieving consistent performance improvements across summarization, general QA, and math QA tasks

## Executive Summary
This paper addresses the training-inference gap in large language models (LLMs) that arises from exposure bias in teacher forcing training. The gap occurs because models are trained with ground truth tokens but must rely on their own generated tokens during inference, leading to compounding errors and unpredictable behavior. The authors propose two approaches: Batch-Scheduled Sampling (BASH), an offline batch version of scheduled sampling that mixes ground truth and generated tokens during training, and Reference-Answer-based Correction (RAC), which explicitly incorporates self-correction by generating correction labels based on ground truth. Both methods are evaluated on summarization, general question-answering, and math question-answering tasks, consistently outperforming baseline methods including standard fine-tuning, NEFTune, and SPIN.

## Method Summary
The paper proposes two methods to bridge the training-inference gap in LLMs. BASH introduces an offline, batch version of scheduled sampling where the model is trained using a mixture of ground truth tokens and its own generated tokens. This is done by creating an offline dataset between training iterations that interleaves ground truth and model-generated tokens, reducing the computational overhead of switching between training and inference modes. RAC explicitly incorporates self-correction into the model by constructing a dataset where the target sequence is generated by the model itself, conditioned on the ground truth answer. This allows the model to learn to correct its own errors by comparing its generated sequence with the ground truth. Both methods are combined with standard supervised fine-tuning (SFT) and evaluated on summarization, general QA, and math QA tasks using various benchmarks and metrics.

## Key Results
- BASH achieves a win rate of 28.12% compared to 27.03% for standard fine-tuning in summarization tasks
- RAC achieves a length-controlled win rate of 10.37% on the AlpacaEval 2.0 benchmark for general QA
- Initializing models with BASH or RAC followed by preference alignment (DPO) leads to better results than initializing with standard fine-tuning
- Both methods consistently outperform baseline methods including standard fine-tuning, NEFTune, and SPIN across all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1: Bridging Training-Inference Distribution Gap
The proposed methods address the training-inference gap by exposing the model to its own generated tokens during training, aligning the training distribution more closely with inference conditions. During training, instead of solely using ground truth tokens (teacher forcing), the model is exposed to a mixture of ground truth tokens and its own generated tokens. This exposure helps the model learn to handle its own predictions, reducing the distribution shift that occurs during inference. The core assumption is that the distribution of tokens generated by the model during inference is similar to the distribution of tokens in the training data.

### Mechanism 2: Self-Correction Capability
RAC explicitly incorporates a self-correction capability into the model, enabling it to correct the gaps between its generated sequences and the ground truth data. RAC constructs a dataset where the target sequence is generated by the model itself, but conditioned on the ground truth answer. This allows the model to learn to correct its own errors by comparing its generated sequence with the ground truth. The core assumption is that the model can effectively learn to correct its own errors when provided with the ground truth answer as additional context.

### Mechanism 3: Offline and Batch Processing for Scalability
The offline and batch nature of BASH minimizes the cost of switching between training and inference modes, making it scalable for LLMs. Instead of switching between training and inference modes at every gradient step (as in online scheduled sampling), BASH creates a new dataset offline between training iterations. This reduces the computational overhead and GPU under-utilization associated with online methods. The core assumption is that the computational overhead of creating an offline dataset is significantly less than the overhead of switching between training and inference modes at every step.

## Foundational Learning

- **Concept:** Teacher Forcing
  - **Why needed here:** Teacher forcing is the standard method for training autoregressive models, but it can lead to exposure bias during inference.
  - **Quick check question:** What is the main difference between teacher forcing and scheduled sampling?

- **Concept:** Scheduled Sampling
  - **Why needed here:** Scheduled sampling is a method to gradually expose the model to its own generated tokens during training, reducing the distribution shift between training and inference.
  - **Quick check question:** How does scheduled sampling differ from teacher forcing?

- **Concept:** Exposure Bias
  - **Why needed here:** Exposure bias is the discrepancy between the training and inference distributions that can lead to compounding errors and unpredictable behavior.
  - **Quick check question:** What is exposure bias, and how does it affect the performance of autoregressive models?

## Architecture Onboarding

- **Component map:** Pre-trained model -> Training dataset -> Offline dataset generator (for BASH and RAC) -> Model trainer (with SFT, BASH, and RAC objectives)
- **Critical path:** 1) Initialize model with pre-trained weights, 2) Train with SFT for a few iterations, 3) Generate offline datasets for BASH and RAC, 4) Train with SFT + BASH/RAC objectives, 5) Evaluate on benchmark tasks
- **Design tradeoffs:** BASH vs. RAC (BASH is simpler but may not correct errors as effectively as RAC), Mixing coefficient (β) (higher β increases exposure to generated tokens but may also increase the distribution gap), Offline vs. online (offline is more scalable but may become outdated if the model changes significantly)
- **Failure signatures:** Model collapse (if correction labels in RAC are too different from generated tokens), Slower training (if distribution gap between generated and ground truth tokens is too large), GPU under-utilization (if offline dataset generation is not efficient)
- **First 3 experiments:** 1) Train a model with SFT only and evaluate on a benchmark task, 2) Train a model with BASH and evaluate on the same benchmark task, 3) Train a model with RAC and evaluate on the same benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BASH and RAC methods scale with model size beyond 1B and 7B parameters?
- Basis in paper: The paper demonstrates effectiveness on Pythia-1B and Mistral-7B models, but does not explore scaling to larger models.
- Why unresolved: The computational complexity of generating self-generated tokens for training increases with model size, and the authors only briefly mention that their methods "can be easily integrated into the training of LLMs without requiring changes to the training process or model architectures."
- What evidence would resolve it: Empirical results showing consistent performance improvements on models with 10B+ parameters, or theoretical analysis of computational overhead as a function of model size.

### Open Question 2
- Question: What is the optimal mixing coefficient β for BASH in different task domains?
- Basis in paper: The authors mention "it is important to keep the value of β small to avoid making the optimization problem harder to solve" and use β=0.2 in their experiments, but do not systematically explore different values.
- Why unresolved: The paper only uses one value (0.2) across all experiments, despite acknowledging that "Depending on the value of β, there may be a distribution mismatch between the ground truth sequences y and the mixed ones g."
- What evidence would resolve it: A systematic ablation study varying β across different tasks and datasets to identify optimal values for different domains.

### Open Question 3
- Question: How do BASH and RAC methods perform when applied to non-auto-regressive generation tasks?
- Basis in paper: The methods are designed specifically for auto-regressive models and evaluated on summarization and question-answering tasks, but the paper does not discuss applicability to other generation paradigms.
- Why unresolved: The authors focus exclusively on auto-regressive models and do not explore whether their insights about training-inference gaps apply to other generation approaches.
- What evidence would resolve it: Empirical results applying BASH and RAC concepts to non-auto-regressive tasks like non-autoregressive machine translation or text-to-image generation, or theoretical analysis of how the training-inference gap manifests in these paradigms.

## Limitations
- The paper focuses on encoder-decoder models (Pythia-1B) and decoder-only models (Mistral-7B), but doesn't explore how these methods generalize to other architectures
- Evaluation is limited to specific tasks and datasets, with no ablation studies on how performance varies with different mixing coefficients (β) or dataset sizes
- The computational overhead of offline dataset generation is not quantified relative to the performance gains

## Confidence
- **High confidence:** The implementation of BASH and RAC as described follows established scheduled sampling techniques and the mathematical formulations are sound
- **Medium confidence:** The empirical results showing improvements over baselines, though the magnitude of improvements is modest and may not justify the additional complexity
- **Low confidence:** The claim that these methods "bridge the training-inference gap" - the paper demonstrates performance improvements but doesn't directly measure or quantify the gap being bridged

## Next Checks
1. Conduct an ablation study varying the mixing coefficient β in BASH to determine the optimal balance between ground truth and generated tokens for different task types
2. Measure the distribution shift between training tokens (with ground truth) and inference tokens (with generated tokens) to quantify the gap being addressed
3. Compare the computational overhead of offline dataset generation against the performance improvements to determine cost-effectiveness across different model sizes