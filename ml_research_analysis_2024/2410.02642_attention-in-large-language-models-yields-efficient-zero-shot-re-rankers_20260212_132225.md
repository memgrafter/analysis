---
ver: rpa2
title: Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers
arxiv_id: '2410.02642'
source_url: https://arxiv.org/abs/2410.02642
tags:
- re-ranking
- rankgpt
- query
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes in-context re-ranking (ICR), a method that
  leverages attention patterns in large language models (LLMs) for efficient zero-shot
  re-ranking. Instead of relying on autoregressive generation, ICR aggregates attention
  weights from query tokens to rank documents, requiring only two forward passes.
---

# Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers

## Quick Facts
- arXiv ID: 2410.02642
- Source URL: https://arxiv.org/abs/2410.02642
- Authors: Shijie Chen; Bernal Jiménez Gutiérrez; Yu Su
- Reference count: 40
- Primary result: ICR achieves 60%+ latency reduction over RankGPT while outperforming it on both single-hop and multi-hop benchmarks

## Executive Summary
This paper introduces in-context re-ranking (ICR), a method that leverages attention patterns in large language models (LLMs) for efficient zero-shot re-ranking. Instead of relying on autoregressive generation, ICR aggregates attention weights from query tokens to rank documents, requiring only two forward passes. The method includes a calibration step using a content-free query to mitigate LLM biases. Experiments show ICR outperforms RankGPT on both single-hop and multi-hop benchmarks while reducing latency by over 60%.

## Method Summary
ICR extracts attention weights from LLMs during query processing and aggregates them to rank documents. The method uses two forward passes: one with the actual query and one with a content-free calibration query. Attention weights are extracted, aggregated across layers and heads, calibrated by subtracting the calibration scores, and summed per document to produce final rankings. No training is required, making it a zero-shot approach that works with open-weight LLMs.

## Key Results
- ICR achieves 60%+ latency reduction compared to RankGPT (generative re-ranking)
- Outperforms RankGPT on both single-hop (TREC, BEIR) and multi-hop (MuSiQue, 2Wiki, HotpotQA) benchmarks
- Particularly effective on tasks requiring complex reasoning like contextualization, contradiction handling, and multi-hop information integration
- Achieves competitive performance with strong proprietary models using open-weight LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention weights correlate with document relevance when processing query tokens
- Mechanism: LLMs attend more strongly to tokens in relevant documents than irrelevant ones when processing the query, allowing relevance ranking through attention aggregation
- Core assumption: Attention patterns during query processing encode relevance signals that can be extracted without generation
- Evidence anchors:
  - [abstract]: "we hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation"
  - [section 3.2]: "We hypothesize that LLMs will, on average, attend more strongly to tokens in relevant documents than irrelevant ones when processing the query"
  - [corpus]: Found 25 related papers, average neighbor FMR=0.459, suggesting moderate relatedness to re-ranking literature

### Mechanism 2
- Claim: Calibration with content-free query mitigates intrinsic LLM biases
- Mechanism: Subtracting attention scores from a content-free query removes position bias, title bias, and other systematic attention patterns that don't reflect true relevance
- Core assumption: LLMs exhibit systematic biases in attention distribution that can be characterized and removed using irrelevant queries
- Evidence anchors:
  - [section 3.3]: "Inspired by Zhao et al. (2021), we propose a ranking score calibration method to mitigate intrinsic biases in LLMs' attention weights"
  - [section 5.1]: "Since the calibration query is irrelevant to any document, an unbiased re-ranker should assign equal ranking scores for all documents"
  - [corpus]: Related work on bias calibration in LLMs exists, supporting this approach

### Mechanism 3
- Claim: Two forward passes achieve O(1) complexity versus O(N) for generative methods
- Mechanism: By sharing KV cache between calibration and query passes, ICR computes all document scores with fixed computational cost independent of document count
- Core assumption: Attention weights can be extracted from a single forward pass and aggregated to produce rankings without iterative generation
- Evidence anchors:
  - [abstract]: "ICR only requires two (O(1)) forward passes to re-rank N documents, making it substantially more efficient than generative re-ranking methods that require at least O(N) forward passes"
  - [section 4.4]: "We empirically compare the speed and performance of ICR and RankGPT on the SciFact dataset with 300 queries"
  - [corpus]: No direct evidence found in related papers about O(1) vs O(N) complexity comparison

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: ICR relies on extracting and aggregating cross-attention weights between query and document tokens
  - Quick check question: What information do attention weights between query and document tokens encode about their relationship?

- Concept: Bias characterization and removal in LLMs
  - Why needed here: Calibration step requires understanding how LLMs systematically attend to certain token types (titles, positions, etc.)
  - Quick check question: How can you determine if an LLM has position bias in its attention patterns?

- Concept: Computational complexity analysis
  - Why needed here: Understanding why ICR achieves O(1) vs O(N) complexity requires analyzing forward pass requirements
  - Quick check question: What operations in generative re-ranking contribute to O(N) complexity?

## Architecture Onboarding

- Component map:
  - Prompt construction → Attention extraction → Aggregation → Calibration → Ranking

- Critical path:
  1. Generate attention weights for query + documents
  2. Aggregate attention weights across layers/heads/query tokens
  3. Generate attention weights for calibration query + same documents
  4. Subtract calibration scores from query scores
  5. Sum calibrated scores per document to get final ranking

- Design tradeoffs:
  - Open-weight vs proprietary LLMs: ICR works with any model but may perform worse on less capable ones
  - Single vs multi-pass attention extraction: Multiple passes could capture more signals but increase latency
  - Token-level vs document-level aggregation: Token-level provides interpretability but may be noisier

- Failure signatures:
  - Uniform attention scores across all documents (calibration failed or model is biased)
  - Strong correlation between document position and score (position bias not removed)
  - Scores dominated by lexical overlap rather than semantic relevance (aggregation strategy issue)

- First 3 experiments:
  1. Run ICR with and without calibration on a small dataset to measure bias reduction
  2. Compare attention aggregation strategies (sum vs mean vs learned weights) on same dataset
  3. Test ICR with different base models (Mistral vs Llama) to understand model capability impact

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations section, several areas remain unexplored:
- Extending ICR to other language model architectures beyond decoder-only models
- Testing ICR with proprietary LLMs that only provide API access
- Improving ICR's handling of lexical bias in standard information retrieval tasks

## Limitations

- ICR assumes attention patterns reliably encode document relevance, but this correlation isn't directly validated
- The method suffers from lexical bias, particularly on tasks like DBPedia-Entity where distractor documents have high lexical overlap with the query
- Limited to decoder-only language models due to their popularity and strong performance

## Confidence

**High Confidence**: The efficiency claim (O(1) vs O(N) complexity) and the overall experimental methodology are well-supported. The paper provides clear latency comparisons and uses established benchmarks.

**Medium Confidence**: The calibration mechanism's effectiveness in removing biases is plausible but not thoroughly validated. The paper shows improved performance after calibration but doesn't provide detailed analysis of which biases are being removed or how effective the removal is.

**Low Confidence**: The core assumption that attention weights correlate with document relevance is the most critical but least directly validated claim. The paper hypothesizes this relationship but doesn't provide direct evidence that attention patterns capture semantic relevance beyond what could be explained by simpler features like lexical overlap.

## Next Checks

1. **Attention-Relevance Correlation Analysis**: Extract attention weights from a small dataset and compute correlation between attention scores and human relevance judgments. Compare this to baseline methods that use simpler features (BM25 scores, lexical overlap) to determine if attention provides additional signal beyond surface features.

2. **Calibration Bias Characterization**: Run ICR with multiple different calibration queries (varying lengths, content types) on the same dataset to measure how sensitive the method is to calibration strategy. Analyze whether certain types of biases (position, title, lexical) are more effectively removed than others.

3. **Attention Pattern Interpretability**: Select specific queries where ICR succeeds or fails, and visualize attention patterns to understand what the model is actually attending to. Compare attention distributions for relevant vs irrelevant documents to verify that the method captures meaningful relevance signals rather than spurious patterns.