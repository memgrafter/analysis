---
ver: rpa2
title: Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their
  Effect on Model Performance
arxiv_id: '2410.18889'
source_url: https://arxiv.org/abs/2410.18889
tags:
- label
- llms
- annotation
- uni0000004c
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can detect
  substantial label errors in NLP benchmarks, with error rates ranging from 6% to
  21% across four factual consistency datasets. The authors propose an ensemble-based
  approach that flags examples where LLMs strongly disagree with original labels,
  using confidence thresholds to improve precision.
---

# Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance

## Quick Facts
- arXiv ID: 2410.18889
- Source URL: https://arxiv.org/abs/2410.18889
- Reference count: 40
- Key outcome: LLM ensemble detects 6-21% label errors in NLP benchmarks, improving model performance by up to 15% when corrected

## Executive Summary
This paper demonstrates that large language models can effectively detect substantial label errors in NLP benchmarks, with error rates ranging from 6% to 21% across four factual consistency datasets. The authors propose an ensemble-based approach using multiple LLMs and prompts to flag examples where model predictions disagree with original labels. When LLM confidence exceeds 95%, over two-thirds of the flagged labels are confirmed as errors by expert re-annotation. Correcting these errors yields significant performance improvements for both LLMs and fine-tuned models, suggesting that existing benchmarks may systematically underestimate LLM capabilities.

## Method Summary
The authors employ an ensemble of four LLMs (GPT-4, PaLM2, Mistral, Llama3) with four different zero-shot classification prompts to detect label errors in factual consistency datasets. The method flags examples where LLM predictions disagree with original labels, using confidence scores from the ensemble to prioritize high-precision error detection. For test sets, experts validate flagged examples to establish gold labels. For training sets, the authors experiment with label flipping and filtering based on confidence thresholds. The approach is evaluated on TRUE benchmark datasets and SummEval, measuring inter-annotator agreement, cost-effectiveness, and downstream performance impact.

## Key Results
- LLM ensemble achieves 95-99% accuracy in detecting label errors when confidence exceeds 95%
- Label error rates range from 6% to 21% across four factual consistency datasets
- Correcting label errors improves LLM performance by up to 15% and fine-tuned models by 4%
- LLM-based annotation matches or exceeds expert-level agreement (κ=0.85) while being 100-1000x more cost-effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble of diverse LLMs with multiple prompts improves error detection precision compared to single model or prompt.
- Mechanism: Combining predictions from multiple LLMs with varied prompts increases robustness against individual model biases and prompt sensitivity, leading to more reliable consensus.
- Core assumption: Different LLMs and prompts capture complementary aspects of the task, and averaging their predictions reduces variance.
- Evidence anchors:
  - [abstract] "leveraging an ensemble of LLMs to flag potentially mislabeled examples"
  - [section 3] "The motivation for this ensemble is twofold: first, we demonstrate that it enhances error detection and aligns more closely with expert annotations while also decreases the variance"
  - [corpus] Weak evidence - only 5 related papers found, none specifically addressing LLM ensemble for error detection

### Mechanism 2
- Claim: LLM confidence scores can be used to prioritize error detection with higher precision.
- Mechanism: When LLMs show high confidence in predictions that disagree with original labels, this indicates stronger evidence of potential label errors.
- Core assumption: LLM confidence scores are well-calibrated and correlate with annotation accuracy.
- Evidence anchors:
  - [abstract] "when their confidence exceeds 95%, over two-thirds of those labels are confirmed as errors by expert re-annotation"
  - [section 5.2] "as LLM confidence increases, so does its precision in detecting label errors"
  - [corpus] Weak evidence - no direct corpus support for confidence calibration in error detection context

### Mechanism 3
- Claim: LLM-based annotation matches or exceeds expert-level agreement while being significantly more cost-effective.
- Mechanism: LLMs can process large volumes of data quickly and consistently, achieving agreement levels comparable to experts but at 100-1000x lower cost.
- Core assumption: Current LLMs have sufficient task understanding to match human expert performance on labeling tasks.
- Evidence anchors:
  - [abstract] "LLM-based annotations not only excel in error detection but also perform similarly to, or better than, traditional annotation methods"
  - [section 6] "GPT-4 and PaLM2 achieve κ scores above 0.70, approaching expert-level agreement after reconciliation (κ=0.85)"
  - [corpus] Weak evidence - limited corpus support for LLM vs expert comparison in this specific context

## Foundational Learning

- Concept: Ensemble methods and diversity in machine learning
  - Why needed here: Understanding how combining multiple models reduces variance and improves robustness is critical for implementing the ensemble approach
  - Quick check question: If you have three LLMs making predictions with accuracies 0.8, 0.75, and 0.85, and they're uncorrelated, what's the theoretical maximum ensemble accuracy?

- Concept: Confidence calibration and uncertainty quantification
  - Why needed here: Proper interpretation of LLM confidence scores is essential for the precision improvement mechanism
  - Quick check question: If an LLM predicts label "1" with probability 0.95, what does this actually tell us about the model's certainty versus the true probability of correctness?

- Concept: Inter-annotator agreement metrics (Fleiss's kappa, Krippendorff's alpha)
  - Why needed here: Evaluating annotation quality and comparing different annotation approaches requires understanding these statistical measures
  - Quick check question: What's the difference between Cohen's kappa (for two annotators) and Fleiss's kappa (for multiple annotators)?

## Architecture Onboarding

- Component map: Dataset examples with original labels -> LLM Ensemble (4 models × 4 prompts) -> Predictions + confidence scores -> Disagreement Detection (vs original labels) -> Confidence Filtering (threshold) -> Expert Validation (test sets) or Label Correction (training sets) -> Corrected labels + performance metrics

- Critical path:
  1. Prepare dataset and LLM ensemble configuration
  2. Generate ensemble predictions with confidence scores
  3. Identify disagreements with original labels
  4. Apply confidence threshold to filter candidates
  5. Validate with experts (for test sets) or apply corrections (for training sets)

- Design tradeoffs:
  - Ensemble size vs. cost: More models/prompt variations improve accuracy but increase compute cost
  - Confidence threshold vs. recall: Higher thresholds increase precision but may miss some errors
  - Expert validation vs. automation: Full automation saves time but risks propagating LLM errors

- Failure signatures:
  - Low precision in error detection: Check LLM confidence calibration and ensemble diversity
  - High false positive rate: Adjust confidence threshold or investigate prompt quality
  - Poor agreement with experts: Evaluate individual LLM performance and prompt effectiveness

- First 3 experiments:
  1. Baseline: Single LLM + single prompt vs. original labels
  2. Ensemble impact: Compare single model vs. ensemble performance on agreement with experts
  3. Confidence calibration: Plot precision vs. confidence thresholds to find optimal operating point

## Open Questions the Paper Calls Out
- How do LLM-based error detection methods perform on tasks beyond factual consistency, such as sentiment analysis or named entity recognition?
- What is the optimal confidence threshold for flagging potential label errors that balances precision and recall across different task domains?
- How does data contamination affect LLM error detection capabilities when models are trained on the datasets being evaluated?

## Limitations
- Reliance on single expert annotator for validation introduces potential bias
- Confidence calibration results lack direct statistical validation across different LLMs and datasets
- Cost comparison assumes current commercial API pricing, which may change over time

## Confidence
- High confidence: Performance improvements from label correction (measured on test sets)
- Medium confidence: LLM ensemble superiority over single models (based on agreement metrics)
- Medium confidence: Cost-effectiveness claims (based on API pricing assumptions)
- Low confidence: Generalizability to non-factual-consistency tasks (limited to TRUE benchmark datasets)

## Next Checks
1. **Calibration Validation**: Conduct proper reliability diagram analysis and Expected Calibration Error (ECE) computation for each LLM's confidence scores to verify calibration quality across different confidence thresholds.

2. **Generalization Test**: Apply the same methodology to a different task domain (e.g., sentiment analysis or question answering) to assess whether the ensemble approach generalizes beyond factual consistency tasks.

3. **Cost Sensitivity Analysis**: Model how changes in LLM API pricing would affect the cost-effectiveness claims, particularly if token prices increase or if open-source models become competitive with commercial APIs.