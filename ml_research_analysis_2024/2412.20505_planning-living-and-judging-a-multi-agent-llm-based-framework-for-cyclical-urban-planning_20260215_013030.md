---
ver: rpa2
title: 'Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical
  Urban Planning'
arxiv_id: '2412.20505'
source_url: https://arxiv.org/abs/2412.20505
tags:
- urban
- planning
- plan
- living
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a multi-agent LLM-based framework for Cyclical
  Urban Planning (CUP) that continuously generates, evaluates, and refines urban plans
  in a closed-loop process. The framework consists of three key components: Planning,
  where LLM agents generate and refine urban plans based on contextual data; Living,
  where agents simulate resident behaviors and interactions; and Judging, which evaluates
  plan effectiveness and provides iterative feedback.'
---

# Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning

## Quick Facts
- arXiv ID: 2412.20505
- Source URL: https://arxiv.org/abs/2412.20505
- Authors: Hang Ni; Yuzhi Wang; Hao Liu
- Reference count: 6
- Key outcome: The paper proposes a multi-agent LLM-based framework for Cyclical Urban Planning (CUP) that continuously generates, evaluates, and refines urban plans in a closed-loop process. Experiments on the Huilongguan community dataset show the framework's effectiveness, with an average living experience score reaching 69.03% after 3 iterations.

## Executive Summary
This paper introduces a multi-agent LLM-based framework for Cyclical Urban Planning (CUP) that continuously generates, evaluates, and refines urban plans through an iterative three-phase process: Planning, Living, and Judging. The framework leverages LLM agents to simulate resident behaviors, evaluate plan effectiveness, and provide iterative feedback for plan refinement. Experiments on the Huilongguan community dataset demonstrate the framework's effectiveness, with living experience scores improving to 69.03% after three iterations. The approach integrates expert knowledge with public participation to enhance residents' well-being through continuous plan refinement.

## Method Summary
The framework implements a closed-loop cycle consisting of three phases: Planning (LLM agents generate and refine urban plans based on contextual data and previous iterations), Living (LLM agents simulate resident behaviors and interactions within the planned environment), and Judging (evaluation of plan effectiveness using quantitative metrics like accessibility and ecology, plus qualitative resident experience assessments). The system uses GPT-4o with temperature=0, runs 3 iterations with 30 resident agents, and simulates 1 day per iteration at 1-minute granularity. Resident agents are profiled using prompt chaining based on demographic data, and each maintains a dynamic memory pool to ensure temporal coherence in their behaviors.

## Key Results
- Average living experience score reached 69.03% after 3 iterations
- The framework successfully integrates quantitative metrics (accessibility, ecology) with qualitative resident experience assessments
- Iterative refinement through the Planning-Living-Judging cycle demonstrates continuous improvement in plan quality
- The multi-agent approach effectively balances expert knowledge with public participation perspectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves urban plan quality through iterative closed-loop refinement.
- Mechanism: Each cycle of Planning, Living, and Judging generates feedback that directly modifies the next plan iteration. Planning uses previous plan P_{k-1}, suggestions S_{k-1}, and discussion D_k to produce refined plan P_k. Living simulates resident behaviors under P_k, producing experience data E_k. Judging evaluates QOL and generates improvement suggestions S_k for the next cycle.
- Core assumption: LLM agents can generate realistic resident behaviors and meaningful quality of life evaluations that correlate with actual urban planning outcomes.
- Evidence anchors:
  - [abstract] "Experiments on the Huilongguan community dataset show the framework's effectiveness, with an average living experience score reaching 69.03% after 3 iterations"
  - [section] "The cyclical process enables a dynamic and responsive planning approach"
  - [corpus] Weak evidence - corpus neighbors focus on LLM planning but lack direct evidence of closed-loop urban regeneration
- Break condition: If LLM-generated resident behaviors fail to capture realistic urban dynamics, or if QOL metrics don't correlate with actual resident satisfaction.

### Mechanism 2
- Claim: Multi-agent collaboration enables balanced consideration of diverse stakeholder perspectives.
- Mechanism: Three agent types (planner P, judger J, residents R_i) each contribute distinct expertise. Residents provide lived experience and preferences through discussion and interviews. Planner integrates expert knowledge K with resident feedback to create plans. Judger evaluates both quantitative metrics and qualitative resident experiences to generate improvement suggestions.
- Core assumption: LLM agents can authentically represent diverse stakeholder viewpoints and generate coherent, actionable consensus.
- Evidence anchors:
  - [section] "incorporating both urban planner and public participation for effectiveness"
  - [section] "we steer the judger agent to reflect on the quantitative and qualitative analysis results"
  - [corpus] Weak evidence - corpus neighbors show multi-agent planning but limited evidence of balanced stakeholder representation
- Break condition: If agent representations become biased or fail to capture the full range of stakeholder perspectives, leading to suboptimal plans.

### Mechanism 3
- Claim: Dynamic memory pools enable agents to maintain contextual awareness across simulation timesteps.
- Mechanism: Each resident agent maintains a memory pool M_i(t) that records events, behaviors, and thoughts. This memory enables coherent behavior planning and reflection across timesteps. The environment E(t) = {ER(t), ES(t)} is synchronously updated, providing context for agent decisions.
- Core assumption: LLM-based memory systems can maintain sufficient context to produce realistic temporal behavior patterns.
- Evidence anchors:
  - [section] "Each resident is equipped with a dynamic memory pool M (t) to record the agent's living experience"
  - [section] "The memory pool is curated to store the events, behaviors, and thoughts, as well as facilitate effective planning and reflecting through memory retrieval"
  - [corpus] Moderate evidence - corpus neighbor "Generative agents: Interactive simulacra of human behavior" supports LLM memory for temporal coherence
- Break condition: If memory pools become too large to manage or fail to capture relevant context, leading to incoherent agent behaviors.

## Foundational Learning

- Concept: Agent-based modeling fundamentals
  - Why needed here: The framework relies on simulating resident behaviors and interactions to evaluate plan effectiveness
  - Quick check question: What are the key components of a typical agent-based model, and how do they interact in urban planning contexts?

- Concept: Large language model prompting techniques
  - Why needed here: The framework uses prompt chaining for resident profiling and iterative plan refinement
  - Quick check question: How does prompt chaining differ from single-shot prompting, and when is it most effective?

- Concept: Quality of life metrics in urban planning
  - Why needed here: The Judging component evaluates plan effectiveness using QOL measures including accessibility, ecology, and resident experience
  - Quick check question: What are the main categories of QOL metrics used in urban planning, and how do quantitative and qualitative measures complement each other?

## Architecture Onboarding

- Component map: Planner agent -> Living environment -> Resident agents -> Judger agent -> Memory pools -> Back to Planner agent (iterative cycle)
- Critical path: Planning → Living → Judging → Planning (next iteration)
- Design tradeoffs: LLM-based simulation offers flexibility but may sacrifice precision compared to rule-based ABM; multi-agent collaboration increases complexity but enables stakeholder representation
- Failure signatures: Degraded QOL scores over iterations, unrealistic resident behaviors, plan modifications that don't address identified issues, excessive memory usage
- First 3 experiments:
  1. Single iteration test: Run Planning → Living → Judging once and verify QOL metrics are generated correctly
  2. Agent behavior validation: Simulate resident agents for one day and manually inspect generated behaviors for realism
  3. Memory system test: Verify memory pools maintain coherent context across multiple timesteps without excessive growth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cyclical urban planning framework perform in larger urban regions with more complex spatial configurations?
- Basis in paper: [explicit] The authors mention applying their system in larger urban regions as future work, suggesting current evaluation is limited to a specific community case study.
- Why unresolved: The current experiments are limited to a 3.74km2 area (Huilongguan community), and scalability to larger urban regions with more diverse land uses and planning tasks remains untested.
- What evidence would resolve it: Systematic testing of the framework on progressively larger urban areas with varied land use patterns and complexity levels, measuring performance across accessibility, ecology, and resident experience metrics.

### Open Question 2
- Question: What is the optimal balance between quantitative planning metrics (like accessibility and ecology) and qualitative resident experience in the cyclical planning process?
- Basis in paper: [inferred] The paper notes that improvements in resident experience scores may come at the cost of declining quantitative metrics like accessibility and ecology, indicating a potential trade-off that requires optimization.
- Why unresolved: The framework currently treats these aspects somewhat independently, and the experiments show they can move in opposite directions, suggesting a need for better integration or weighting mechanisms.
- What evidence would resolve it: Controlled experiments varying the weighting between quantitative and qualitative metrics across multiple planning cycles, identifying configurations that optimize both aspects simultaneously.

### Open Question 3
- Question: How do different resident profiling approaches affect the realism and utility of the simulated behaviors in the Living phase?
- Basis in paper: [explicit] The authors describe their profiling technique for resident agents but acknowledge it as an area for future enhancement, suggesting current approaches may have limitations.
- Why unresolved: The paper only uses one profiling approach without comparing it to alternatives or validating whether the generated behaviors accurately reflect real-world resident patterns.
- What evidence would resolve it: Comparative studies using different profiling methodologies (e.g., varying the depth of personality attributes, using real demographic data, or incorporating behavioral patterns from mobility studies) to assess their impact on simulation fidelity and planning outcomes.

## Limitations
- Dataset dependency: The framework's effectiveness is demonstrated only on the Huilongguan community dataset, limiting generalizability to other urban contexts.
- LLM simulation accuracy: Uncertainty about whether LLM-generated resident behaviors capture the full complexity of real urban dynamics.
- Evaluation scope: Limited to three metrics over only 3 iterations with 30 resident agents, potentially missing longer-term dynamics.

## Confidence
- High confidence: The framework's basic architecture and theoretical foundation in participatory urban planning principles are well-established and sound.
- Medium confidence: The effectiveness of the 3-iteration demonstration on the Huilongguan dataset is supported by empirical results, but generalizability requires further validation.
- Low confidence: The ability of LLM-based simulations to accurately capture complex urban dynamics over extended timeframes and multiple iterations remains an open question.

## Next Checks
1. Cross-dataset validation: Test the framework on diverse urban datasets with different characteristics to assess generalizability.
2. Long-term simulation study: Extend the number of iterations beyond 3 and monitor metric stability, emergent behaviors, and potential negative externalities.
3. Real-world validation: Compare LLM-generated resident behaviors and QOL evaluations with actual resident survey data from the Huilongguan community.