---
ver: rpa2
title: High Noise Scheduling is a Must
arxiv_id: '2404.06353'
source_url: https://arxiv.org/abs/2404.06353
tags:
- noise
- steps
- scheduling
- levels
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Consistency models advance image generation by reducing sampling
  steps to one, but current training techniques lack balanced noise distribution and
  stable curriculum progression. This work addresses these issues by proposing polynomial
  noise scheduling that maintains stability and includes predefined Karras noise levels
  to avoid unique noise generation.
---

# High Noise Scheduling is a Must

## Quick Facts
- arXiv ID: 2404.06353
- Source URL: https://arxiv.org/abs/2404.06353
- Reference count: 4
- Primary result: Proposed polynomial noise scheduling with sinusoidal curriculum achieves 30.48 FID on CIFAR-10, outperforming baseline consistency model training techniques

## Executive Summary
Consistency models advance image generation by reducing sampling steps to one, but current training techniques lack balanced noise distribution and stable curriculum progression. This work addresses these issues by proposing polynomial noise scheduling that maintains stability and includes predefined Karras noise levels to avoid unique noise generation. A sinusoidal-based curriculum eliminates learned noise steps, enhancing denoising performance. Experiments on CIFAR-10 with low-depth models show the polynomial noise distribution achieves a 33.54 FID score after 100,000 training steps, while the sinusoidal curriculum further improves results to 30.48 FID, outperforming existing consistency model training techniques.

## Method Summary
The method introduces polynomial noise scheduling with a predefined Karras noise vector to maintain stability and high noise level representation. A sinusoidal curriculum controls the discretization steps during training, enabling smooth transitions between noise levels. The approach trains a U-Net consistency model on CIFAR-10 with batch size 1024 for 100,000 steps using two NVIDIA A6000 GPUs. The polynomial distribution balances low and high noise levels, while the sinusoidal curriculum eliminates abrupt changes in discretization steps.

## Key Results
- Polynomial noise distribution achieves 33.54 FID score on CIFAR-10 after 100,000 training steps
- Sinusoidal curriculum further improves results to 30.48 FID
- Both improvements outperform existing consistency model training techniques
- High noise levels are shown to be essential for effective denoising performance

## Why This Works (Mechanism)

### Mechanism 1
High noise levels in scheduling improve denoising performance by ensuring the model learns the full range of noise-to-data mappings. The denoising process requires learning from noisy samples at various levels, and if high noise levels are underrepresented, the model cannot generalize well for those cases.

### Mechanism 2
Polynomial noise distribution balances low and high noise levels better than log-normal distribution by assigning higher weights to low noise levels and lower weights to high noise levels, ensuring both are represented while maintaining a smooth transition.

### Mechanism 3
Sinusoidal curriculum eliminates abrupt changes in discretization steps, leading to more stable learning by smoothing the progression of noise steps over training and preventing large jumps that could destabilize learning.

## Foundational Learning

- **Noise scheduling in diffusion/consistency models**: Why needed here - The paper's core contribution is modifying how noise levels are selected during training, which directly impacts the model's ability to denoise effectively. Quick check - What is the purpose of noise scheduling in consistency models, and how does it differ from standard diffusion models?

- **Curriculum learning**: Why needed here - The curriculum controls how the model is exposed to different noise levels over time, which is central to the proposed sinusoidal approach. Quick check - How does a curriculum-based approach to noise scheduling differ from a fixed noise schedule?

- **Karras noise generation algorithm**: Why needed here - The paper modifies the use of Karras noise to avoid generating unique noise levels at each discretization step, which is key to their stability improvement. Quick check - What is the Karras noise generation algorithm, and why might generating unique noise levels at each step be problematic?

## Architecture Onboarding

- **Component map**: Noise scheduler -> Curriculum controller -> Pre-defined noise bank -> Consistency model U-Net -> FID evaluator
- **Critical path**: 1) Pre-generate Karras noise levels (σs1), 2) During training, sample noise levels from polynomial distribution, 3) Apply sinusoidal curriculum to adjust discretization steps, 4) Train model with consistency loss, 5) Evaluate FID periodically
- **Design tradeoffs**: Polynomial vs. log-normal noise distribution (polynomial offers more control but may require tuning the curve parameter), Sinusoidal vs. step-wise curriculum (sinusoidal is smoother but may cover noise levels more slowly), Pre-defined noise bank vs. on-the-fly generation (pre-defined is more stable but less flexible)
- **Failure signatures**: Training instability (may indicate too much high noise or too rapid curriculum changes), Poor FID scores (could mean inadequate coverage of noise levels or suboptimal polynomial curve), Memory issues (pre-defining large noise banks may be memory-intensive)
- **First 3 experiments**: 1) Train with polynomial noise distribution (c=3) and constant discretization steps; compare FID to log-normal baseline, 2) Add sinusoidal curriculum to the polynomial setup; measure FID improvement, 3) Vary the polynomial curve parameter (c=2,3,4) to find optimal balance of low/high noise levels

## Open Questions the Paper Calls Out

### Open Question 1
How does the polynomial noise scheduling's curve parameter (c) affect the trade-off between low-level and high-level noise representation in consistency models? The paper does not provide a detailed analysis of how different values of c impact the model's denoising performance and FID scores.

### Open Question 2
How does the predefined Karras noise vector's length (s1) impact the consistency model's performance and stability during training? The paper sets s1 to 250 but does not explore the impact of different s1 values on the model's ability to learn denoising from various noise levels.

### Open Question 3
How does the sinusoidal curriculum's initial and final discretization steps (s0 and s1) impact the consistency model's learning and convergence? The paper sets s0 to 20 and s1 to 250 but does not explore how different s0 and s1 values affect the model's ability to learn denoising and converge to optimal performance.

## Limitations
- Experimental validation is constrained to CIFAR-10 with low-depth U-Net architectures, limiting generalizability
- FID score improvements (33.54 → 30.48) are modest and may not justify added complexity
- Computational overhead of pre-generating and storing Karras noise arrays is not quantified

## Confidence
**High Confidence**: The mechanism linking high noise levels to improved denoising performance is well-supported by literature and empirical evidence.
**Medium Confidence**: The sinusoidal curriculum's impact on stability is supported by FID improvement but lacks ablation studies isolating its contribution.
**Low Confidence**: Claims about training efficiency improvements are not substantiated with wall-clock time measurements or memory usage comparisons.

## Next Checks
1. **Ablation Study**: Train with polynomial noise distribution only (no sinusoidal curriculum) on CIFAR-10 to isolate the curriculum's contribution to FID improvement.
2. **Scale-Up Experiment**: Apply the full method (polynomial + sinusoidal) to a higher-resolution dataset like CIFAR-100 or LSUN bedroom to test scalability.
3. **Efficiency Analysis**: Measure training time per step and GPU memory usage for the proposed method versus baseline consistency training to quantify overhead.