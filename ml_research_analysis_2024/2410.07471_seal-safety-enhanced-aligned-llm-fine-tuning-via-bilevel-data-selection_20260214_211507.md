---
ver: rpa2
title: 'SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection'
arxiv_id: '2410.07471'
source_url: https://arxiv.org/abs/2410.07471
tags:
- data
- fine-tuning
- seal
- arxiv
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEAL, a safety-enhanced aligned LLM fine-tuning
  framework that uses bilevel optimization to select safe and high-quality fine-tuning
  data. The method learns a data ranker to up-rank safe samples and down-rank potentially
  harmful ones, addressing the challenge of maintaining safety alignment during fine-tuning.
---

# SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection

## Quick Facts
- arXiv ID: 2410.07471
- Source URL: https://arxiv.org/abs/2410.07471
- Reference count: 36
- This paper introduces SEAL, a safety-enhanced aligned LLM fine-tuning framework that uses bilevel optimization to select safe and high-quality fine-tuning data.

## Executive Summary
SEAL addresses the challenge of maintaining safety alignment during LLM fine-tuning by learning a data ranker that up-ranks safe samples and down-ranks potentially harmful ones. The framework uses bilevel optimization to train a selector that chooses high-quality fine-tuning data while preserving safety alignment. Experiments demonstrate SEAL's effectiveness across multiple model scales, achieving significant win rate improvements over random selection baselines while showing promising transferability properties.

## Method Summary
SEAL employs bilevel optimization where the upper level learns a data selector parameterized by ω that ranks fine-tuning samples, while the lower level fine-tunes the LLM on the top-ranked subset. The selector is trained to maximize fit on safe data while minimizing harm from potentially dangerous samples. The framework uses loss gap ℓ(θk; zj) - ℓ(θ̂k; zj) as a signal for data safety ranking, where samples with positive gaps are likely harmful. SEAL also demonstrates transferability of the data selector across different model scales and architectures.

## Key Results
- SEAL achieves 8.5% and 9.7% win rate increases over random selection baselines on LLaMA-3-8B-Instruct and Merlinite-7B respectively
- The data selector trained by SEAL is transferable between different models and effective across a wide selection range (20% ≤ p ≤ 80%)
- SEAL preserves safety alignment while maintaining performance on target tasks

## Why This Works (Mechanism)

### Mechanism 1
Data selector learns to rank samples by safety/quality, allowing fine-tuning on safer subset. Bilevel optimization trains a selector σ(ω) that, when applied to fine-tuning data, produces model parameters θ*(ω) that fit well on safe dataset D_safe. Core assumption: There exist conflicting objectives between fitting fine-tuning data and maintaining safety alignment.

### Mechanism 2
Loss gap ℓ(θk; zj) - ℓ(θ̂k; zj) serves as signal for data safety ranking. During data selector training, samples with positive loss gaps are likely to be harmful (fit worse with safety-aligned θk than auxiliary θ̂k), so their rank is decreased. Core assumption: Harmful samples will show higher loss when evaluated with safety-aligned parameters than with auxiliary parameters.

### Mechanism 3
Transferability of data selector between different model scales/architectures. Safety-enhanced data selected by one model benefits fine-tuning of other models due to universal safety/quality characteristics. Core assumption: Safety and quality attributes of data are model-agnostic.

## Foundational Learning

- **Bilevel optimization**: SEAL formulation requires optimizing data selector parameters (upper level) subject to model parameter optimization (lower level). Quick check: What is the mathematical relationship between upper-level and lower-level objectives in SEAL?

- **Gradient-based hyperparameter optimization**: SEAL uses gradient-based methods to train data selector, requiring understanding of how to differentiate through optimization processes. Quick check: How does the loss gap ℓ(θk; zj) - ℓ(θ̂k; zj) relate to the gradient of the bilevel objective with respect to ω?

- **LoRA (Low-Rank Adaptation)**: SEAL experiments use LoRA for efficient fine-tuning, requiring understanding of parameter-efficient methods. Quick check: How many trainable parameters result from using LoRA rank-16 with α=16 on LLAMA-3-8B?

## Architecture Onboarding

- **Component map**: Data selector (parameterized by ω) → Ranks fine-tuning data → Top p% selected → Fine-tuned on selected data → Evaluated on safety/quality metrics
- **Critical path**: Data selector training (Algorithm 1/2) → Data selection → Fine-tuning → Evaluation
- **Design tradeoffs**: Computational cost vs. safety gain (data selector training increases overall training time); Selection percentage vs. performance (too low loses domain performance, too high risks safety); Model capacity vs. selector quality (larger models help learn better selectors)
- **Failure signatures**: No improvement over baselines → Data selector not learning meaningful rankings; Safety degradation despite selection → Selection percentage too high or selector failing to identify harmful samples; Target domain performance drop → Selection percentage too low, filtering out useful data
- **First 3 experiments**: 1) Baseline comparison: Run standard SFT, random selection, and SEAL on LLAMA-3-8B with REDORCA dataset; 2) Selection percentage sweep: Test SEAL with 20%, 50%, 80% selection rates on MERLINITE-7B; 3) Transferability test: Train data selector on PYTHIA-2.8B, apply to MERLINITE-7B fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
How does the transferability of SEAL's data selector work across different model architectures, and what are the theoretical limits of this transferability? The paper demonstrates empirical transferability but doesn't provide theoretical analysis of why or how this transferability works, or establish its limits.

### Open Question 2
What is the optimal data selection percentage range for SEAL across different types of fine-tuning datasets (benign vs. potentially harmful)? The paper shows SEAL works across 20% to 80% but doesn't systematically characterize optimal ranges for different dataset types.

### Open Question 3
How does SEAL's performance scale with model size and what are the computational bottlenecks that prevent it from working equally well on smaller models? The paper identifies this correlation but doesn't investigate the underlying mechanisms or whether architectural modifications could help smaller models.

## Limitations
- The paper relies on GPT-4-based win rate metrics for safety assessment, which introduces potential subjectivity
- The fundamental assumption that loss gap reliably indicates sample safety is not empirically validated
- The computational overhead of bilevel optimization (approximately 2x standard fine-tuning) is presented without detailed scaling analysis

## Confidence

- **High confidence**: The core bilevel optimization framework is mathematically sound and experimental results showing SEAL outperforming random selection baselines are reproducible
- **Medium confidence**: The transferability claims across model scales are supported by experiments but lack comprehensive ablation studies
- **Low confidence**: The fundamental assumption that loss gap reliably indicates sample safety is not empirically validated, and the paper lacks statistical significance tests

## Next Checks

1. **Correlation validation**: Measure the statistical correlation between loss gap values and actual safety violations on a held-out test set to empirically validate whether the loss gap mechanism actually identifies harmful samples

2. **Cost-benefit analysis**: Conduct a detailed computational cost analysis comparing SEAL's bilevel optimization overhead against the safety gains achieved, including wall-clock time, GPU memory usage, and energy consumption metrics

3. **Transferability boundaries**: Systematically test data selector transfer across different model families (not just scales) and domain distributions to identify the conditions under which transferability succeeds or fails, including cases where transferred selectors perform worse than random selection