---
ver: rpa2
title: 'Learning from Imperfect Data: Towards Efficient Knowledge Distillation of
  Autoregressive Language Models for Text-to-SQL'
arxiv_id: '2410.11371'
source_url: https://arxiv.org/abs/2410.11371
tags:
- training
- data
- text-to-sql
- performance
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of knowledge distillation
  (KD) methods for autoregressive language models in text-to-SQL tasks. It finds that
  while model-generated datasets improve performance, they are computationally expensive.
---

# Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL

## Quick Facts
- arXiv ID: 2410.11371
- Source URL: https://arxiv.org/abs/2410.11371
- Reference count: 16
- Primary result: KID achieves +5.83% average score improvements across text-to-SQL benchmarks while reducing training latency

## Executive Summary
This paper addresses the challenge of compressing large autoregressive language models for text-to-SQL tasks through knowledge distillation. The authors identify that traditional KD methods suffer from training-inference mismatch when using ground-truth data. They propose KID (Knowledge Distillation with Imperfect Data), which simulates inference errors during training by rewriting ground-truth data into imperfect versions through masking and one-pass prediction. KID achieves significant performance improvements over baselines while being more computationally efficient than autoregressive generation methods.

## Method Summary
KID simulates the inference process during training by introducing artificial errors into ground-truth data. The method masks a portion of tokens (α ratio) using strategies like random, easy, or hard sampling, then generates imperfect tokens through one-pass forward prediction. The student model learns to both generate correct outputs and calibrate these imperfect tokens. Training uses reverse KL divergence with auxiliary MLE loss, and LoRA fine-tuning enables parameter-efficient adaptation. The approach targets the training-inference mismatch problem where models trained on perfect ground-truth data behave differently during inference with predicted tokens.

## Key Results
- KID achieves up to +5.83% average score improvements across multiple benchmarks (Spider, BIRD, robustness tests)
- Outperforms five baseline KD methods (FKD, RKD, f-distill, ImitKD, GKD) on execution accuracy metrics
- Significantly reduces training latency compared to GKD while maintaining or improving performance
- Shows consistent improvements across different model types and sizes (QWen, CodeGen, LLaMA variants)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulating inference errors during training reduces the training-inference mismatch.
- Mechanism: By rewriting ground-truth tokens into imperfect ones through masking and one-pass prediction, the model learns to handle errors that naturally occur during autoregressive inference.
- Core assumption: The distribution of errors introduced during rewriting approximates the distribution of errors that occur during inference.
- Evidence anchors:
  - [abstract]: "The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data."
  - [section]: "Motivated by this, we propose a simple-yet-effective approach to improve KD, namely KID, and achieve a better trade-off between performance and efficiency. The core of KID is to force the student to rewrite the ground-truth training data into imperfect one, and then learn how to calibrate these imperfect data."
  - [corpus]: Weak - no direct evidence about error simulation in distillation.

### Mechanism 2
- Claim: One-pass imperfect data generation is more efficient than online autoregressive generation.
- Mechanism: Instead of generating each token autoregressively during training (like GKD), KID generates imperfect data in a single forward pass by masking tokens and letting the model predict them.
- Core assumption: One-pass forward generation can create sufficiently diverse imperfect data to simulate inference errors.
- Evidence anchors:
  - [abstract]: "The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data."
  - [section]: "In practice, we ❶ first sample α of tokens from the ground-truth output y and mask them with a special token (e.g., '<s>'). For sampling the tokens, we design some strategies... After masking the spans of y, we ❷ then generate imperfect tokens to fill in the spans. Specifically, we feed the masked sequence into the student to generate predictions with a one-pass forward process."
  - [corpus]: Weak - no direct comparison of generation efficiency methods.

### Mechanism 3
- Claim: Learning to calibrate imperfect tokens improves distillation performance.
- Mechanism: By training on data with introduced errors, the model learns to recognize and correct these errors, which translates to better handling of inference-time mistakes.
- Core assumption: Error calibration learned on imperfect data generalizes to inference scenarios.
- Evidence anchors:
  - [abstract]: "Moreover, by doing so, we can also encourage the student to learn how to calibrate these imperfect tokens and further improve the KD performance."
  - [section]: "Intuitively, by introducing some errors in the imperfect data, we can simulate the cascading effect of inference during training processes, thus mitigating the training-inference mismatch. Moreover, by encouraging the student to learn how to calibrate these imperfect tokens, KID can further improve the performance."
  - [corpus]: Weak - no direct evidence about calibration learning.

## Foundational Learning

- Concept: Training-inference mismatch in autoregressive models
  - Why needed here: Understanding this mismatch is crucial for why KID works - it's the core problem being solved
  - Quick check question: Why does training with ground-truth tokens lead to different behavior than inference with predicted tokens?

- Concept: Kullback-Leibler divergence and its variants
  - Why needed here: The paper uses Reverse KL divergence as the primary distillation loss function
  - Quick check question: What's the key difference between Forward KL and Reverse KL divergence in terms of how they handle probability distributions?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: The experiments use LoRA for efficient model adaptation rather than full fine-tuning
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

## Architecture Onboarding

- Component map: Ground-truth data -> Masking and rewriting -> Imperfect data generation -> Model training with KD loss
- Critical path: Ground-truth data → Masking strategy selection → Imperfect data generation → Training data creation → Model training with KD loss
- Design tradeoffs:
  - Masking ratio α: Higher values create more challenging data but risk losing semantic meaning
  - Masking strategy: Random vs. difficulty-based approaches affect which errors are emphasized
  - Loss combination: Balance between distillation loss and MLE loss for stable training
- Failure signatures:
  - Poor performance on Spider-dev despite good training loss → Possible overfitting to imperfect data patterns
  - High ExAccErr scores → Training-inference mismatch not adequately addressed
  - Degradation with large teacher-student size gaps → KID may not scale to extreme compression ratios
- First 3 experiments:
  1. Implement basic KID with random masking (α=0.2) and compare to SFT baseline on Spider-dev
  2. Test different masking strategies (Random, Easy, Hard) to find optimal approach
  3. Vary masking ratio α from 0.1 to 0.5 to identify sensitivity and optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal masking ratio α for KID across different model sizes and types?
- Basis in paper: [explicit] The paper conducts experiments with α values of {0.1, 0.2, 0.3, 0.4, 0.5} and finds that α = 0.2 performs best, but notes that performance is "not sensitive to α" within a certain range (0.1 to 0.3).
- Why unresolved: The paper only tests a limited range of α values and on specific model types (QWen, CodeGen, LLaMA). It's unclear whether the optimal α would vary for much larger models (e.g., 70B parameters) or different architectures.
- What evidence would resolve it: Systematic experiments testing a wider range of α values (including values outside 0.1-0.5) across multiple model sizes and types, including very large models, would help determine if there's a universal optimal α or if it depends on model characteristics.

### Open Question 2
- Question: How does the performance of KID compare to other advanced knowledge distillation methods like online knowledge distillation or data augmentation techniques?
- Basis in paper: [inferred] The paper compares KID to five baseline KD methods (FKD, RKD, f-distill, ImitKD, GKD) but doesn't explore more recent or advanced distillation techniques that might have been developed after these baselines.
- Why unresolved: The field of knowledge distillation is rapidly evolving, and newer methods may have emerged that could potentially outperform KID or provide different trade-offs between performance and efficiency.
- What evidence would resolve it: Comparative experiments between KID and state-of-the-art KD methods from recent literature (e.g., online distillation, data augmentation-enhanced distillation) on the same benchmarks would reveal whether KID remains competitive or if alternative approaches offer superior performance.

### Open Question 3
- Question: Can KID be effectively applied to non-autoregressive text-to-SQL models or other structured prediction tasks beyond text-to-SQL?
- Basis in paper: [explicit] The paper focuses exclusively on autoregressive LLMs for text-to-SQL tasks and acknowledges that "it will be more convincing if scaling up to super-large model size" and that "our method has the great potential to expand to more scenarios."
- Why unresolved: While KID shows strong performance for autoregressive text-to-SQL models, its applicability to other model architectures or task domains remains untested. The effectiveness of the "imperfect data" approach may depend on the specific characteristics of autoregressive generation.
- What evidence would resolve it: Experiments applying KID to non-autoregressive text-to-SQL models (e.g., encoder-decoder architectures) and other structured prediction tasks (e.g., semantic parsing for other domains, code generation, or machine translation) would demonstrate whether the approach generalizes beyond the specific use case studied.

## Limitations

- Limited analysis of whether KID's artificially introduced errors match natural inference error distributions
- Lack of systematic ablation studies on critical hyperparameters, particularly the masking ratio α
- Evaluation focuses primarily on execution accuracy metrics without deep analysis of error types or patterns

## Confidence

- High Confidence: KID reduces training latency compared to GKD and achieves +5.83% average score improvements across benchmarks
- Medium Confidence: KID improves calibration of imperfect tokens, though direct empirical validation is lacking
- Low Confidence: The claim that training-inference mismatch is "efficiently mitigated" by KID's error simulation lacks rigorous proof

## Next Checks

1. **Error Distribution Analysis**: Compare the distribution of errors introduced by KID's rewriting process against actual inference errors on a held-out validation set to validate whether KID targets the right error patterns.

2. **Hyperparameter Sensitivity Study**: Conduct systematic ablation experiments varying masking ratio α (0.1, 0.2, 0.3, 0.4, 0.5) and different masking strategies to understand robustness to parameter choices.

3. **Error Type Classification**: Analyze which specific types of SQL generation errors (syntax, semantic, schema-related) are most improved by KID compared to baselines to understand what error categories are actually addressed.