---
ver: rpa2
title: On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms
arxiv_id: '2402.18129'
source_url: https://arxiv.org/abs/2402.18129
tags:
- smax
- learning
- fair
- sensitive
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes how demographic parity (DP)-based fair learning
  algorithms can be biased toward the majority subgroup when the sensitive attribute
  distribution is imbalanced. The authors theoretically prove that under such imbalance,
  DP-based fair classifiers tend to favor the majority group's label distribution.
---

# On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms

## Quick Facts
- arXiv ID: 2402.18129
- Source URL: https://arxiv.org/abs/2402.18129
- Reference count: 40
- Key outcome: DP-based fair learning algorithms induce bias toward majority subgroup under sensitive attribute imbalance; SA-DRO mitigates this bias

## Executive Summary
This work analyzes how demographic parity (DP)-based fair learning algorithms can be biased toward the majority subgroup when the sensitive attribute distribution is imbalanced. The authors theoretically prove that under such imbalance, DP-based fair classifiers tend to favor the majority group's label distribution. To address this, they propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method that minimizes the worst-case fair-regularized loss over a set of sensitive attribute marginal distributions. Experimental results on COMPAS, Adult, and CelebA datasets validate the theoretical findings and demonstrate that SA-DRO reduces the bias toward the majority subgroup while maintaining fairness and accuracy, particularly in federated learning scenarios with heterogeneous sensitive attribute distributions.

## Method Summary
The paper proposes SA-DRO, a method that mitigates inductive biases in DP-based fair learning by optimizing over a distributionally robust set of sensitive attribute distributions. The approach uses projected gradient descent/ascent to solve a minimax problem where the inner maximization finds the worst-case sensitive attribute distribution within a divergence ball, and the outer minimization trains a classifier to minimize the fair-regularized loss under this worst-case distribution. The method is evaluated against baseline DP-based fair classifiers (KDE, FACL, MI, RFI) on three benchmark datasets with imbalanced sensitive attributes.

## Key Results
- DP-based fair classifiers converge to the majority subgroup's label distribution when sensitive attribute is imbalanced (>50% majority)
- SA-DRO successfully reduces the bias toward majority subgroup predictions while maintaining accuracy
- In federated learning with heterogeneous sensitive attribute distributions, SA-DRO-FedKDE outperforms FedKDE in accuracy for minority clients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic Parity (DP)-based fair learning algorithms induce bias toward the majority subgroup when the sensitive attribute distribution is imbalanced.
- Mechanism: When the sensitive attribute is skewed (e.g., 80% majority, 20% minority), optimizing for DP can push the classifier to mimic the label distribution of the majority subgroup to minimize fairness violations. This happens because enforcing independence between predictions and the sensitive attribute under imbalance causes the model to favor the majority's label patterns.
- Core assumption: The sensitive attribute distribution is non-uniform and the true label distribution differs across sensitive subgroups.
- Evidence anchors:
  - [abstract] "under such imbalance, DP-based fair classifiers tend to favor the majority group's label distribution."
  - [section] Theorem 1 proves that if the majority sensitive attribute holds > 50% of the data, the optimal DP-fair prediction converges to the majority subgroup's conditional label distribution.
  - [corpus] No direct corpus evidence for this specific mechanism; only general fairness-related papers found.
- Break condition: If the sensitive attribute is uniformly distributed, or if the true label distribution is identical across subgroups, the inductive bias toward the majority disappears.

### Mechanism 2
- Claim: SA-DRO (Sensitive Attribute-based Distributionally Robust Optimization) mitigates the inductive bias by optimizing over a set of sensitive attribute distributions.
- Mechanism: By minimizing the worst-case fair-regularized loss over a ball of distributions around the true sensitive attribute marginal, SA-DRO forces the classifier to be robust to changes in subgroup frequencies. This reduces over-reliance on the majority subgroup's label distribution.
- Core assumption: The true sensitive attribute distribution might vary or be misspecified; robustness to such variations improves fairness.
- Evidence anchors:
  - [abstract] "propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method that minimizes the worst-case fair-regularized loss over a set of sensitive attribute marginal distributions."
  - [section] Algorithm 1 implements SA-DRO via projected gradient descent/ascent over a divergence-constrained set of sensitive attribute weights.
  - [corpus] No direct corpus evidence; related work cited (e.g., DRO for fairness) but not the same mechanism.
- Break condition: If the sensitive attribute distribution is known exactly and stable, the extra robustness may be unnecessary and could slightly reduce accuracy.

### Mechanism 3
- Claim: Different dependence measures (DDP, MI, ERMI, MC) exhibit varying strengths of inductive bias under imbalance.
- Mechanism: DDP-based methods show the strongest bias because they enforce strict independence, pushing predictions to match the majority's label distribution. MI, ERMI, and MC-based methods only constrain average dependence, resulting in weaker, more diffuse biases.
- Core assumption: The form of the dependence measure determines how strictly independence is enforced and thus how strongly the model is pushed toward the majority subgroup.
- Evidence anchors:
  - [section] Theorem 2 compares bias levels across dependence measures, showing that DDP yields worst-case TV bounds while others yield expected TV bounds.
  - [section] "the bias level for a DDP-based fair learner could be considerably stronger than that of mutual information, ERMI, and maximal correlation-based fair learners."
  - [corpus] No direct corpus evidence for this comparative claim; theoretical results are internal to the paper.
- Break condition: If the sensitive attribute distribution is balanced or if the dependence measure is conditioned on the label (e.g., equalized odds), the bias differences may not manifest.

## Foundational Learning

- Concept: Total Variation (TV) distance as a measure of distributional discrepancy.
  - Why needed here: TV distance quantifies how much the conditional label distribution given a sensitive attribute differs from that of the majority subgroup; it's used to bound the inductive bias.
  - Quick check question: If two distributions have TV distance 0.3, what is the maximum possible difference in their expected 0/1 loss under optimal coupling?

- Concept: Distributionally Robust Optimization (DRO) and f-divergence constraints.
  - Why needed here: DRO provides the mathematical framework to optimize against the worst-case sensitive attribute distribution, enabling SA-DRO's bias mitigation.
  - Quick check question: In a χ²-constrained DRO ball, how does increasing the radius affect the set of distributions considered?

- Concept: Fairness notions: Demographic Parity (DP) vs Equalized Odds (EO).
  - Why needed here: Understanding the difference is crucial because the inductive bias only applies to DP, not EO; EO conditions on the true label and thus avoids majority subgroup favoritism.
  - Quick check question: Why does enforcing DP (unconditional independence) lead to different inductive biases than enforcing EO (conditional independence given label)?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Model training -> Loss computation -> DRO optimization -> Model evaluation

- Critical path:
  1. Initialize classifier weights w and sensitive attribute distribution q.
  2. For each iteration:
     - Compute classifier gradient gw on weighted data.
     - Update w via gradient descent.
     - Compute gradient gq of the worst-case loss over q.
     - Update q via projected gradient ascent (enforcing divergence constraint).
  3. Output trained model and final q.

- Design tradeoffs:
  - Accuracy vs fairness: Stronger fairness regularization (higher λ) reduces DDP but may lower accuracy and increase majority bias.
  - Robustness vs efficiency: Larger DRO radius δ increases bias mitigation but may require more iterations and hurt accuracy.
  - Model complexity: Neural nets can capture complex patterns but are harder to analyze theoretically than logistic regression.

- Failure signatures:
  - High DDP but low accuracy: Fairness regularization too weak or model underfitting.
  - Majority subgroup NR(s) close to minority NR(s) but both high: Model defaulting to negative class; check label imbalance.
  - SA-DRO not reducing bias: Divergence radius δ too small or step sizes αw, αq poorly tuned.

- First 3 experiments:
  1. Run ERM (λ=0) and standard DP-based fair learning (λ>0) on COMPAS with 80/20 sensitive attribute split; measure accuracy, DDP, and NR(s) for each subgroup.
  2. Apply SA-DRO to the same setup with varying δ; compare NR(s) convergence toward midpoint vs majority.
  3. Simulate federated learning with heterogeneous sensitive attribute distributions (e.g., Client 1: 80% minority, Clients 2-4: 20% minority); compare FedKDE vs SA-DRO-FedKDE accuracy for minority client.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do inductive biases in DP-based fair learning algorithms generalize to other fairness metrics beyond demographic parity, such as equalized odds or predictive parity?
- Basis in paper: [inferred] The paper focuses on demographic parity but mentions that other fairness metrics exist. It also discusses the relationship between different dependence measures and their impact on bias levels.
- Why unresolved: The paper primarily analyzes the inductive biases of DP-based methods, leaving the behavior of other fairness metrics unexplored. Different fairness metrics might induce different types or magnitudes of bias, especially under imbalanced sensitive attribute distributions.
- What evidence would resolve it: Empirical studies comparing the inductive biases of various fairness metrics under imbalanced data conditions, potentially using synthetic or real-world datasets with controlled sensitive attribute distributions.

### Open Question 2
- Question: What are the long-term effects of SA-DRO on model performance and fairness when deployed in dynamic, real-world environments where sensitive attribute distributions may shift over time?
- Basis in paper: [explicit] The paper introduces SA-DRO to mitigate inductive biases but only evaluates it in static, controlled settings. It mentions the potential application in federated learning but doesn't explore temporal dynamics.
- Why unresolved: Real-world data distributions are rarely static, and SA-DRO's effectiveness might degrade if the sensitive attribute distribution changes significantly after training. The paper doesn't address model adaptation or retraining strategies for evolving distributions.
- What evidence would resolve it: Longitudinal studies tracking model performance and fairness metrics over time in deployed systems with changing sensitive attribute distributions, including analysis of retraining frequency and adaptation mechanisms.

### Open Question 3
- Question: How does the choice of distance measure (e.g., χ²-divergence vs. KL-divergence) in the SA-DRO formulation affect the trade-off between accuracy, fairness, and inductive bias reduction?
- Basis in paper: [explicit] The paper mentions that any standard f-divergence could be used in SA-DRO and specifically uses χ²-divergence in experiments, but doesn't compare different distance measures.
- Why unresolved: Different divergence measures might lead to different worst-case distributions in the DRO formulation, potentially resulting in varying levels of bias reduction and accuracy preservation. The paper only provides results for one specific divergence measure.
- What evidence would resolve it: Systematic comparison of SA-DRO performance using different distance measures on the same datasets, measuring accuracy, fairness violations, and inductive bias levels across different divergence choices.

## Limitations

- Theoretical results rely on idealized conditions (exact DP enforcement, infinite data)
- Empirical validation limited to three benchmark datasets with binary sensitive attributes
- Analysis focuses on DP fairness but does not extend to other notions like Equalized Odds

## Confidence

- High: Theorem 1's proof that DP-based learners converge to majority subgroup's label distribution under imbalance
- Medium: Empirical demonstration that SA-DRO reduces majority subgroup bias while maintaining accuracy
- Low: Generalization of findings to non-binary sensitive attributes and multi-class classification settings

## Next Checks

1. Test SA-DRO on datasets with multi-class sensitive attributes to verify if the majority bias mitigation extends beyond binary cases
2. Compare SA-DRO performance against DP-fair baselines when the true label distribution is identical across sensitive subgroups
3. Evaluate SA-DRO's robustness to varying degrees of sensitive attribute imbalance (e.g., 90/10, 70/30 splits) to identify the threshold where bias mitigation becomes critical