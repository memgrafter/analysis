---
ver: rpa2
title: 'CONFLARE: CONFormal LArge language model REtrieval'
arxiv_id: '2404.04287'
source_url: https://arxiv.org/abs/2404.04287
tags:
- retrieval
- question
- conformal
- process
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONFLARE, a framework that applies conformal
  prediction to the retrieval phase of RAG systems to quantify and control uncertainty.
  By using a calibration set of questions and their corresponding similarity scores
  to document chunks, CONFLARE establishes a similarity score cutoff threshold based
  on a user-specified error rate.
---

# CONFLARE: CONFormal LArge language model REtrieval

## Quick Facts
- arXiv ID: 2404.04287
- Source URL: https://arxiv.org/abs/2404.04287
- Reference count: 0
- Primary result: CONFLARE applies conformal prediction to RAG retrieval to quantify uncertainty and ensure true answers are included with user-specified confidence

## Executive Summary
This paper introduces CONFLARE, a framework that applies conformal prediction to the retrieval phase of RAG systems to quantify and control uncertainty. By using a calibration set of questions and their corresponding similarity scores to document chunks, CONFLARE establishes a similarity score cutoff threshold based on a user-specified error rate. During inference, only document chunks exceeding this threshold are retrieved, ensuring the true answer is included with a specified confidence level. This approach mitigates retrieval uncertainty and enhances the trustworthiness of RAG systems, particularly in domains requiring high reliability, such as medicine.

## Method Summary
CONFLARE is a four-step framework for uncertainty-aware retrieval in RAG systems. First, a calibration set of questions answerable from the knowledge base is constructed. Second, the calibration data is labeled by identifying the most relevant document chunks and recording similarity scores between questions and chunks. Third, a conformal predictor is calibrated by determining a similarity score cutoff threshold based on the user-specified error rate. Finally, during inference, only chunks with similarity scores exceeding this threshold are retrieved to provide context to the LLM, ensuring the true answer is captured with (1-α) confidence.

## Key Results
- Conformal prediction ensures retrieved chunks contain correct answers with user-specified confidence
- Framework is agnostic to downstream LLM and embedding model
- Python package provided for automated workflow implementation
- Demonstrated effectiveness in medical question-answering example

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction ensures that the retrieved chunks contain the correct answer with a user-specified confidence level.
- Mechanism: By calibrating a similarity score threshold on a calibration set, only chunks with similarity scores above this threshold are retrieved, guaranteeing that the true answer is included in the context with probability (1 - α).
- Core assumption: The calibration set is representative of the questions the system will encounter during inference.
- Evidence anchors:
  - [abstract] "During inference, all chunks with similarity exceeding this threshold are retrieved to provide context to the LLM, ensuring the true answer is captured in the context with a (1-α) confidence level."
  - [section] "Given a user-specified error rate (α), these similarity scores are then analyzed to determine a similarity score cutoff threshold. During inference, all chunks with similarity exceeding this threshold are retrieved to provide context to the LLM, ensuring the true answer is captured in the context with a (1 − α) confidence level."
- Break condition: If the calibration set is not representative of the inference data distribution, the statistical guarantee fails.

### Mechanism 2
- Claim: The conformal predictor calibration step identifies a similarity threshold that maximizes retrieval recall while controlling the error rate.
- Mechanism: The similarity scores from the calibration set are sorted, and the threshold is set at the percentile corresponding to the user-specified error rate. This ensures that at least (1 - α) fraction of questions have their correct answer in the retrieved chunks.
- Core assumption: The similarity score is a reliable indicator of whether a chunk contains the answer to a question.
- Evidence anchors:
  - [section] "Given a user-specified error rate (α), these similarity scores are then analyzed to determine a similarity score cutoff threshold."
  - [section] "During inference, all chunks with similarity exceeding this threshold are retrieved to provide context to the LLM, ensuring the true answer is captured in the context with a (1 − α) confidence level."
- Break condition: If the embedding model produces unreliable similarity scores (e.g., due to poor semantic understanding), the threshold will not accurately reflect answer relevance.

### Mechanism 3
- Claim: The framework is agnostic to the downstream LLM and embedding model, making it broadly applicable.
- Mechanism: The conformal prediction is applied only to the retrieval phase, and the downstream LLM generates responses based on the retrieved context as usual. This separation allows the framework to work with any LLM or embedding model.
- Core assumption: The downstream LLM can effectively use the provided context to generate accurate answers.
- Evidence anchors:
  - [abstract] "We provide a Python package that enables users to implement the entire workflow proposed in our work, only using LLMs and without human intervention."
  - [section] "We provide a Python package that enables users to implement the entire workflow proposed in our work, only using LLMs and without human intervention."
- Break condition: If the downstream LLM ignores the context or performs poorly with long contexts, the overall system performance degrades.

## Foundational Learning

- Concept: Conformal prediction
  - Why needed here: To quantify and control uncertainty in the retrieval phase of RAG systems, ensuring that the retrieved context contains the correct answer with a specified confidence level.
  - Quick check question: What is the main advantage of using conformal prediction over other uncertainty quantification methods in this context?

- Concept: Embedding and similarity scoring
  - Why needed here: To represent questions and document chunks as vectors and compute their semantic similarity, which is used to determine which chunks to retrieve.
  - Quick check question: Why is it important to chunk documents before embedding them, rather than embedding long documents as a whole?

- Concept: Calibration set construction
  - Why needed here: To provide a representative sample of questions and their corresponding answer-containing chunks, which is used to calibrate the similarity threshold for retrieval.
  - Quick check question: What are the key characteristics that the calibration set should have to ensure the effectiveness of the conformal predictor?

## Architecture Onboarding

- Component map: Knowledge base (vector database) -> Embedding model (converts to vectors) -> Similarity scorer (computes similarity) -> Conformal predictor (calibrates threshold) -> Retriever (retrieves chunks) -> Downstream LLM (generates answers)

- Critical path: Question embedding → Similarity scoring → Threshold comparison → Chunk retrieval → Context assembly → Answer generation

- Design tradeoffs:
  - Error rate vs. context size: Lower error rates lead to more conservative retrieval and larger contexts, which may exceed LLM context window limits
  - Calibration set size vs. representativeness: Larger, more diverse calibration sets improve reliability but require more effort to construct
  - Embedding model choice vs. similarity accuracy: Different embedding models may produce varying levels of semantic similarity accuracy, affecting retrieval quality

- Failure signatures:
  - High error rate specified but retrieval still misses answers: Indicates poor calibration set representativeness or unreliable similarity scoring
  - Context too large for LLM: Suggests the error rate is set too low or the embedding model produces overly broad similarity scores
  - LLM generates incorrect answers despite correct retrieval: Points to limitations in the downstream LLM's ability to reason over the provided context

- First 3 experiments:
  1. Validate the calibration process by checking if the retrieved chunks contain the correct answer for a held-out set of questions
  2. Test the effect of varying the error rate on the size of the retrieved context and the accuracy of the generated answers
  3. Compare the performance of different embedding models on a sample retrieval task to assess their impact on similarity scoring accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of calibration questions affect the reliability of conformal prediction in retrieval-augmented generation systems?
- Basis in paper: [explicit] The paper mentions that "the higher the quality, relevancy, and diversity of these questions, the better the conformal predictor will be calibrated later in the process."
- Why unresolved: The paper does not provide empirical evidence or specific metrics to quantify the impact of question quality and diversity on the reliability of the conformal prediction.
- What evidence would resolve it: Conducting experiments that systematically vary the quality and diversity of calibration questions and measuring their impact on the performance and reliability of the conformal prediction in RAG systems.

### Open Question 2
- Question: What are the optimal error rates for different domains or types of queries in retrieval-augmented generation systems using conformal prediction?
- Basis in paper: [explicit] The paper states that "users should be aware that lower error rates usually result in more conservative retrieval (lower cut-off scores) and an increased number of returned document chunks," suggesting that the choice of error rate is crucial.
- Why unresolved: The paper does not provide guidance or empirical results on how to determine the optimal error rate for different applications or domains.
- What evidence would resolve it: Empirical studies that evaluate the performance of RAG systems with different error rates across various domains and query types, identifying patterns or guidelines for optimal error rate selection.

### Open Question 3
- Question: How does the performance of downstream LLMs in handling contradictory information affect the overall trustworthiness of retrieval-augmented generation systems?
- Basis in paper: [explicit] The paper notes that "even a precise retrieval process can lead to uncertain outcomes during response generation" when contradictory information is present, highlighting the importance of the downstream LLM's ability to manage uncertainty.
- Why unresolved: The paper does not explore the capabilities of different LLMs in dealing with contradictory information or provide metrics to assess their performance in such scenarios.
- What evidence would resolve it: Comparative studies of various LLMs' performance in generating responses when faced with contradictory information in the context, including metrics for assessing their ability to communicate uncertainty to users.

## Limitations

- The effectiveness of CONFLARE heavily depends on the quality and representativeness of the calibration set
- The framework's performance is limited by the embedding model's ability to produce reliable similarity scores
- There is uncertainty regarding the downstream LLM's performance with long contexts, as the conformal threshold may result in retrieving a large number of chunks

## Confidence

- **High Confidence**: The mechanism by which conformal prediction ensures the true answer is included with a specified confidence level is well-supported by the literature and the authors' description. The calibration process for determining the similarity threshold is clearly defined.
- **Medium Confidence**: The claim that the framework is agnostic to the downstream LLM and embedding model is plausible but depends on the specific implementations and their ability to handle the retrieved context effectively.
- **Low Confidence**: The overall effectiveness of the method in real-world scenarios is uncertain without empirical validation, particularly regarding the calibration set's representativeness and the embedding model's performance.

## Next Checks

1. **Calibration Set Representativeness**: Conduct a statistical analysis comparing the distribution of topics and question types in the calibration set against those expected during inference. Ensure that the calibration set is diverse and representative to validate the reliability of the conformal predictor.

2. **Embedding Model Performance**: Evaluate the embedding model's ability to distinguish between different concepts of interest by testing it on a diverse set of queries. Monitor the number of retrieved chunks during inference to ensure they do not exceed the context window limits of the downstream LLM.

3. **Downstream LLM Effectiveness**: Assess the downstream LLM's performance in generating accurate answers from the retrieved context, particularly when dealing with long contexts or contradictory information from multiple documents. This can be done through qualitative analysis of generated responses and quantitative measures of accuracy and coherence.