---
ver: rpa2
title: 'CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation
  Models'
arxiv_id: '2407.12023'
source_url: https://arxiv.org/abs/2407.12023
tags:
- multimodal
- knowledge
- large
- answer
- cmmath
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMMaTH, a Chinese multimodal math skill evaluation
  benchmark containing 23k K12 educational questions, forming the largest Chinese
  multimodal mathematical problem benchmark to date. The dataset includes diverse
  problem types, solution objectives, visual elements, detailed knowledge points,
  and standard solution annotations.
---

# CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models

## Quick Facts
- arXiv ID: 2407.12023
- Source URL: https://arxiv.org/abs/2407.12023
- Reference count: 29
- Dataset: 23k Chinese K12 multimodal math questions, largest Chinese benchmark to date

## Executive Summary
CMMaTH is a comprehensive Chinese multimodal math skill evaluation benchmark containing 23,000 K12 educational questions. The benchmark encompasses diverse problem types, solution objectives, visual elements, detailed knowledge points, and standard solution annotations. To address evaluation challenges, the authors developed GradeGPT, an open-source answer comparison tool that compares model responses with standard answers without expensive API costs. Experimental results demonstrate that existing multimodal large models struggle significantly with Chinese multimodal mathematical reasoning, particularly on free-form problems and complex visual elements.

## Method Summary
The CMMaTH benchmark was constructed by collecting and curating 23,000 Chinese K12 mathematics questions with multimodal content. The dataset includes diverse problem types, solution objectives, and visual elements, along with detailed knowledge point annotations and standard solutions. To enable cost-effective evaluation, the authors developed GradeGPT, an open-source tool that compares model-generated answers against standard solutions. The evaluation framework allows assessment of model performance across different problem types and visual complexity levels without relying on expensive API calls.

## Key Results
- CMMaTH contains 23,000 questions, establishing it as the largest Chinese multimodal math benchmark
- Existing multimodal models show significantly lower performance compared to human performance
- Models struggle particularly with free-form problems and questions involving complex visual elements
- The benchmark highlights the need for improved multilingual OCR and multimodal diagram reasoning capabilities

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of K12 mathematics problems in Chinese, incorporating multimodal elements that reflect real educational contexts. The diverse problem types and visual elements create a challenging evaluation environment that exposes current model limitations in mathematical reasoning and multimodal understanding. The inclusion of detailed knowledge points and standard solutions enables precise performance assessment across different mathematical concepts and skill levels.

## Foundational Learning
- **Multimodal understanding**: Essential for processing mathematical problems that combine text and visual elements; quick check: model correctly interprets diagrams and text together
- **Chinese language processing**: Required for understanding mathematical terminology and problem statements in Chinese; quick check: accurate parsing of Chinese mathematical expressions
- **Mathematical reasoning**: Core capability for solving K12 level mathematics problems; quick check: correct application of mathematical concepts to solve problems
- **OCR capabilities**: Necessary for extracting text from mathematical diagrams and visual elements; quick check: accurate text extraction from mathematical figures
- **Diagram interpretation**: Critical for understanding geometric and visual mathematical problems; quick check: correct identification of shapes, relationships, and measurements
- **Problem classification**: Important for organizing and evaluating different types of mathematical problems; quick check: accurate categorization of problems by type and difficulty

## Architecture Onboarding

**Component Map**: Data Collection -> Annotation -> GradeGPT Evaluation -> Performance Analysis

**Critical Path**: Question selection and curation → Multimodal processing → Answer generation → GradeGPT comparison → Performance metrics

**Design Tradeoffs**: The benchmark prioritizes comprehensive coverage and realistic educational content over simplified problem types, accepting the challenge of evaluating complex multimodal interactions. The GradeGPT tool trades perfect accuracy for cost-effectiveness and scalability.

**Failure Signatures**: Models may fail due to poor OCR performance on mathematical notation, inability to interpret complex diagrams, lack of Chinese mathematical vocabulary understanding, or insufficient reasoning capabilities for multi-step problems.

**First Experiments**:
1. Evaluate baseline performance on simple text-only problems to establish lower bounds
2. Test OCR accuracy on mathematical notation in various visual formats
3. Assess model performance on problems with varying levels of visual complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of independent validation of benchmark quality and GradeGPT evaluation tool accuracy
- Absence of inter-annotator agreement rates for the 23k questions
- Potential bias introduced by using GradeGPT (developed by same team) for evaluation
- No comparison of GradeGPT accuracy against human expert grading

## Confidence

**High confidence**: Dataset size claim and general finding that current models underperform humans on Chinese multimodal math tasks

**Medium confidence**: Characterization of performance gaps across problem types and visual elements, dependent on evaluation methodology

**Low confidence**: Specific capability assessments for individual models and GradeGPT effectiveness, due to lack of external validation

## Next Checks
1. Conduct inter-annotator reliability testing on 500+ questions to establish benchmark quality metrics
2. Perform blind evaluation of GradeGPT outputs against human expert grading on 100+ randomly selected responses
3. Request independent researchers to replicate key experimental results using alternative evaluation methodologies