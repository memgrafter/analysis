---
ver: rpa2
title: Exploring State Space and Reasoning by Elimination in Tsetlin Machines
arxiv_id: '2407.09162'
source_url: https://arxiv.org/abs/2407.09162
tags:
- clauses
- features
- literals
- information
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Tsetlin Machine Auto-Encoder (TM-AE)
  to generate dense word embeddings for individual words by capturing contextual information
  via conjunctive clauses. The approach enhances interpretability by leveraging reasoning
  by elimination (RbE), where feature negations improve clause descriptiveness.
---

# Exploring State Space and Reasoning by Elimination in Tsetlin Machines

## Quick Facts
- arXiv ID: 2407.09162
- Source URL: https://arxiv.org/abs/2407.09162
- Authors: Ahmed K. Kadhim; Ole-Christoffer Granmo; Lei Jiao; Rishad Shafik
- Reference count: 26
- Primary result: Achieves 90.62% classification accuracy on IMDB dataset using TM-AE with RbE

## Executive Summary
This paper proposes using Tsetlin Machine Auto-Encoder (TM-AE) to generate dense word embeddings by capturing contextual information through conjunctive clauses. The approach enhances interpretability by leveraging reasoning by elimination (RbE), where feature negations improve clause descriptiveness. By tuning the specificity parameter (s) and voting margin (T), the method optimizes feature distribution in the state space, promoting more informative and discriminative embeddings. Empirical results show improved classification accuracy on standard NLP benchmarks.

## Method Summary
The method employs TM-AE to generate dense word vectors by leveraging the Tsetlin Machine's conjunctive clause structure. For each target word, documents containing or excluding the word are selected, encoded into binary vectors with features and their negations, and processed through Tsetlin automata to evolve clause states. The resulting weighted clause sets serve as embeddings. Hyperparameters s and T control the state space distribution, balancing memorization and forgetting to produce concise, informative clauses. The approach is evaluated on IMDB and 20 Newsgroups datasets.

## Key Results
- Achieves 90.62% classification accuracy on IMDB dataset
- Demonstrates improved clause descriptiveness through reasoning by elimination
- Shows state space optimization through hyperparameter tuning enhances embedding quality

## Why This Works (Mechanism)

### Mechanism 1
The TM-AE generates dense word embeddings by learning conjunctive clauses that encode both feature presence and their negations. For each target word, documents containing or excluding it are collected, encoded into binary vectors X with literals and negations, and processed through Tsetlin automata to evolve clause states. Positive and negative clauses vote on the target, producing weighted clause sets that serve as embeddings. This combination of literals and negations captures richer contextual semantics by representing what a word is and what it is not.

### Mechanism 2
Reasoning by Elimination improves clause descriptiveness by encouraging negated literals to occupy higher (memorized) states. By lowering s and raising T, the TM pushes negated literals closer to the memorization threshold, increasing their participation in clauses. This creates AND-rules like "NOT fruit AND NOT basketball..." that sharpen class boundaries. Higher T forces more literal elimination before a clause can vote, making remaining literals—especially negations—more discriminative.

### Mechanism 3
Tuning s and T directly controls the distribution of literals in the 2N-state space, balancing memorisation vs. forgetting. With more epochs, literals drift toward forgotten states unless reinforced by votes. Lower s slows forgetting (keeping more literals near N), while higher T accelerates forgetting (pushing literals below N). This shapes clause length and specificity, with optimal quality when state distribution concentrates literals just above N, giving short but semantically rich rules.

## Foundational Learning

- Concept: Tsetlin Automata basics (state transitions, memorisation vs. forgetting)
  - Why needed here: TM-AE relies on automata states to decide which literals survive in clauses; without understanding this, tuning s/T is blind.
  - Quick check question: What happens to a literal's state when it matches the input and the target output is 1?

- Concept: Conjunctive clause voting mechanism
  - Why needed here: The embedding is defined by weighted clause outputs; knowing how votes are aggregated explains embedding semantics.
  - Quick check question: How does the voting margin T affect whether a clause contributes to the final output?

- Concept: Embedding vs. classification objective
  - Why needed here: TM-AE is self-supervised; embeddings are extracted from clause states rather than supervised labels.
  - Quick check question: What is the target value q when collecting documents that do not contain the target word?

## Architecture Onboarding

- Component map: Vocabulary V (features) -> Document corpus G -> Binary vector X (with literals and negations) -> TM Core (Tsetlin automata) -> Clause set C with weights W for each target word

- Critical path:
  1. Select docs containing/excluding TW → build X
  2. Run TM update → evolve clause states
  3. Extract clauses and weights → form embedding

- Design tradeoffs:
  - Larger N (state space) → more expressive clauses but higher memory
  - Higher window size u → richer context but slower training
  - More epochs → cleaner clauses but risk of over-forgetting

- Failure signatures:
  - All clause weights near zero → learning stalled
  - Clause lengths explode → specificity too low, over-generalisation
  - Embeddings too sparse → insufficient negative sampling

- First 3 experiments:
  1. Fix s=5, T=3200, epochs=5 vs. 100; plot literal state distribution to see forgetting progression.
  2. Fix epochs=25, sweep s=1→50; measure classification accuracy and clause negation ratio.
  3. Fix s=1, sweep T=2→5000; observe how many original vs. negated features appear in high-weight clauses.

## Open Questions the Paper Calls Out

### Open Question 1
How do hyperparameters s and T specifically influence the balance between memorized and forgotten literals in the TM-AE state space, and what is the optimal configuration for different NLP tasks? The paper discusses how s and T regulate feature distribution in the state space but does not provide a comprehensive model explaining the exact relationship between hyperparameters and state space dynamics across diverse tasks. Systematic experiments varying s and T across multiple NLP datasets, combined with theoretical analysis of literal state transitions, would resolve this.

### Open Question 2
Can the reasoning by elimination (RbE) mechanism be extended beyond negation of features to include more complex logical operations, and how would this impact interpretability and performance? The paper explores RbE through feature negation but suggests other logical operations could enhance clause descriptiveness. Experimental comparisons of TM-AE models incorporating various logical operations alongside negation, measuring both performance metrics and interpretability scores, would resolve this.

### Open Question 3
How does the TM-AE's approach to capturing contextual information for individual words compare to other self-supervised learning methods in terms of scalability and generalization to low-resource languages? The paper highlights TM-AE's ability to generate dense word vectors and compares it favorably to methods like Word2Vec, but does not address scalability or performance on low-resource languages. Cross-linguistic studies evaluating TM-AE on low-resource languages, alongside benchmarks against other embedding methods under data scarcity conditions, would resolve this.

## Limitations
- Limited external validation of state-space dynamics beyond this paper
- No ablation studies separating effects of literal negation, state distribution, and voting margin
- Empirical results confined to high-resource datasets without addressing low-resource scenarios

## Confidence
- Claims about TM-AE embedding generation: Medium confidence
- Claims about RbE-enhanced clause descriptiveness: Low-Medium confidence
- Claims about state-space optimization improving embedding quality: Low-Medium confidence

## Next Checks
1. Replicate the literal state distribution curves (Figure 4) with varying s and T to confirm the forgetting dynamics are robust
2. Perform an ablation study: train TM-AE with and without literal negation to quantify RbE's contribution to accuracy
3. Test embedding quality on a different corpus (e.g., Wikipedia) to verify generalization of the state-space optimization findings