---
ver: rpa2
title: 'Exploring the Use of ChatGPT for a Systematic Literature Review: a Design-Based
  Research'
arxiv_id: '2409.17426'
source_url: https://arxiv.org/abs/2409.17426
tags:
- chatgpt
- page
- papers
- challenges
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored using ChatGPT to conduct systematic literature
  reviews, finding it can assist but has limitations. Researchers iteratively refined
  prompts across three rounds to improve ChatGPT's accuracy in analyzing 33 papers
  on blended synchronous learning.
---

# Exploring the Use of ChatGPT for a Systematic Literature Review: a Design-Based Research

## Quick Facts
- arXiv ID: 2409.17426
- Source URL: https://arxiv.org/abs/2409.17426
- Reference count: 0
- ChatGPT can assist in SLRs but requires detailed prompts and human oversight

## Executive Summary
This study investigated using ChatGPT-4.0 to conduct systematic literature reviews on 33 papers about blended synchronous learning. Through a design-based research approach with three iterative rounds of prompt refinement, researchers found that ChatGPT could generate comparable results to human analysis when provided with specific instructions. The tool was particularly effective at identifying challenges and strategies for engaging online learners when prompted to focus on results sections and use an interaction framework. However, ChatGPT could not screen papers, access academic databases, or accurately identify page numbers, requiring manual verification of these elements.

## Method Summary
The study employed design-based research methodology across three iterative rounds to develop and refine ChatGPT prompts for systematic literature review tasks. Researchers first screened and uploaded 33 PDF papers on blended synchronous learning, then systematically refined their prompts to improve ChatGPT's performance. The iterative process involved testing with 5 initial papers, then expanding to the full corpus while adjusting prompts to focus on results sections, categorize findings using an interaction framework (learner-learner, learner-content, learner-instructor), and include checkpoints with page number requests. The final ChatGPT-generated results were compared against original human analysis from a prior literature review.

## Key Results
- ChatGPT generated comparable thematic analysis results to human researchers when given detailed, well-structured prompts
- The tool required three rounds of iterative prompt refinement to achieve acceptable accuracy levels
- ChatGPT could not accurately identify page numbers or screen papers, requiring manual verification and pre-screening

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanisms of how ChatGPT processes and analyzes academic literature. The study demonstrates that ChatGPT can perform certain tasks when properly prompted but does not investigate why these prompts work or what cognitive processes the model employs.

## Foundational Learning
Assumption: Based on the study's findings, it appears that ChatGPT can learn to follow structured analytical frameworks when provided with explicit instructions. The iterative refinement process suggests the model adapts to specific task requirements, though the nature of this "learning" (pattern recognition vs. genuine understanding) remains unclear.

## Architecture Onboarding
None: The paper does not discuss ChatGPT's architecture or how its underlying design influences its ability to perform literature review tasks. No information is provided about model parameters, training data, or architectural considerations that might explain its capabilities or limitations.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can ChatGPT be trained to more accurately identify page numbers from academic papers, including differentiating between PDF page numbers and actual journal page numbers?
- Basis in paper: The paper notes that ChatGPT could not accurately identify exact page numbers from PDF documents, often confusing PDF page numbers with actual journal page numbers.
- Why unresolved: The paper tested ChatGPT's current capabilities but did not explore training methods or prompt engineering techniques to improve its page number identification accuracy.
- What evidence would resolve it: An experimental study comparing different prompt structures, fine-tuning approaches, or specialized plugins to determine if ChatGPT's page number accuracy can be significantly improved.

### Open Question 2
- Question: How does the accuracy and efficiency of ChatGPT-assisted literature reviews compare to traditional human-only systematic literature reviews across different academic disciplines?
- Basis in paper: The study compared ChatGPT's results to human analysis in one specific field (blended synchronous learning) but did not examine cross-disciplinary performance.
- Why unresolved: The study was limited to a single research domain, leaving open questions about generalizability across fields with different publication conventions and terminology.
- What evidence would resolve it: Comparative studies across multiple disciplines (e.g., humanities, social sciences, natural sciences) measuring both accuracy metrics and time efficiency.

### Open Question 3
- Question: What are the long-term implications of ChatGPT-assisted literature reviews on researchers' critical thinking skills and their ability to independently synthesize complex information?
- Basis in paper: The paper notes concerns about ChatGPT potentially training researchers to lack original idea generation and critical thinking abilities, mentioned in the broader literature.
- Why unresolved: This is a longitudinal question about skill development that requires tracking researchers over time rather than a single study.
- What evidence would resolve it: Longitudinal studies comparing cohorts of researchers who regularly use ChatGPT for literature reviews with those who do not, measuring critical thinking skills, synthesis abilities, and research independence over multiple years.

## Limitations
- ChatGPT cannot screen papers or access academic databases, requiring pre-screening by human researchers
- The tool cannot accurately identify page numbers, necessitating manual verification
- Effectiveness depends heavily on the specificity of prompts - vague instructions produce unreliable results

## Confidence
- **High Confidence:** ChatGPT can generate comparable thematic analysis results to human researchers when given detailed, well-structured prompts and when papers are pre-screened
- **Medium Confidence:** The developed guiding principles for using ChatGPT in SLRs are practical and transferable, though their effectiveness may vary across different domains and research questions
- **Low Confidence:** The claim that ChatGPT can "enhance efficiency" is qualified by the significant time investment required for prompt refinement and manual verification of page numbers

## Next Checks
1. Test the transferability of the guiding principles across different domains (e.g., healthcare, social sciences) with varying corpus sizes to assess generalizability
2. Conduct a controlled experiment comparing time and accuracy between ChatGPT-assisted reviews and traditional human-only reviews across multiple iterations
3. Evaluate ChatGPT's performance with open-access versus paywalled papers to determine if the "Ai PDF" plugin limitations affect analysis quality