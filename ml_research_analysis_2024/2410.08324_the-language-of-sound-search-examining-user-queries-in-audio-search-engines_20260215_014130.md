---
ver: rpa2
title: 'The language of sound search: Examining User Queries in Audio Search Engines'
arxiv_id: '2410.08324'
source_url: https://arxiv.org/abs/2410.08324
tags:
- search
- sound
- queries
- audio
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates user-written text queries in sound search
  engines, addressing the gap in understanding real-world user needs and behaviors.
  The authors analyze queries from an online survey and Freesound query logs, comprising
  9 million requests.
---

# The language of sound search: Examining User Queries in Audio Search Engines

## Quick Facts
- arXiv ID: 2410.08324
- Source URL: https://arxiv.org/abs/2410.08324
- Reference count: 0
- Primary result: User-written text queries in sound search engines predominantly use keyword-based queries, with survey queries being longer than Freesound logs, suggesting users prefer detailed queries when unconstrained.

## Executive Summary
This study investigates user-written text queries in sound search engines, addressing the gap in understanding real-world user needs and behaviors. The authors analyze queries from an online survey and Freesound query logs, comprising 9 million requests. Key findings reveal that survey queries are generally longer than Freesound queries, suggesting users prefer detailed queries when unconstrained by system limitations. Both datasets predominantly feature keyword-based queries, with few full-sentence queries. Factors influencing survey queries include the primary sound source, intended usage, perceived location, and the number of sound sources. These insights are crucial for developing user-centered, effective text-based audio retrieval systems.

## Method Summary
The study analyzed user-written text queries from two sources: a custom-designed online survey (706 search tasks by 94 participants) and Freesound query logs (9 million requests over 12 weeks in 2024). The methodology involved collecting and preprocessing query data, manually annotating topics using AudioSet taxonomy, and comparing query characteristics between datasets. The analysis focused on query length, token distribution, and topic prevalence to identify discrepancies with text-to-audio retrieval datasets.

## Key Results
- Survey queries were significantly longer than Freesound queries, indicating users prefer detailed queries when unconstrained by system limitations.
- Both datasets predominantly featured keyword-based queries, with very few full-sentence queries observed.
- Factors influencing survey queries included primary sound source, intended usage, perceived location, and number of sound sources.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Survey queries are longer because users are unconstrained by system limitations.
- Mechanism: Without character limits or keyword-only constraints, users elaborate more fully on what they want, including context and intent.
- Core assumption: Length of query is a proxy for user expressiveness when system constraints are removed.
- Evidence anchors:
  - [abstract] "survey queries are generally longer than Freesound queries, suggesting users prefer detailed queries when not limited by system constraints."
  - [section] "The initial query contained 4.4 tokens on average and 5.5 tokens after refinement in the second step of the search task."
  - [corpus] Weak evidence; corpus focuses on general search systems, not audio-specific behavior.
- Break condition: If query length in real systems increases despite existing constraints, this suggests other factors dominate.

### Mechanism 2
- Claim: Users prefer keyword-based queries over full sentences even when unconstrained.
- Mechanism: Cognitive load and search efficiency favor short, discrete terms rather than structured sentences, regardless of system capabilities.
- Core assumption: Query structure reflects habitual search behavior rather than system limitations.
- Evidence anchors:
  - [abstract] "Both datasets predominantly feature keyword-based queries, with few survey participants using full sentences."
  - [section] "Only very few participants formed full sentences."
  - [corpus] No corpus support for audio-specific behavior; only general search literature.
- Break condition: If future LLM-integrated systems see increased full-sentence usage, this would invalidate the assumption.

### Mechanism 3
- Claim: Sound search users focus on main sound source and usage context over perceptual attributes.
- Mechanism: Users mentally model search around identifiable objects/events and end goals, not audio-technical properties.
- Core assumption: User query formulation reflects mental model centered on semantic source rather than acoustic features.
- Evidence anchors:
  - [abstract] "Key factors influencing survey queries include the primary sound source, intended usage, perceived location, and the number of sound sources."
  - [section] "participants consider aspects relating to the content of the sound (main sound source, number of sources & recording setting) most important."
  - [corpus] No direct corpus evidence for audio-specific search behavior.
- Break condition: If audio retrieval systems show improved performance with acoustic-feature-based queries, this would challenge the assumption.

## Foundational Learning

- Concept: Modality gap in multimedia retrieval
  - Why needed here: The paper addresses how text-based search bridges the gap between text queries and audio content.
  - Quick check question: What is the primary challenge when using text queries to search for audio content?

- Concept: Session detection in query logs
  - Why needed here: Understanding user search sessions helps distinguish between exploratory and targeted search behavior.
  - Quick check question: How do researchers typically define a search session in query log analysis?

- Concept: Cross-modal retrieval
  - Why needed here: The paper discusses systems that match text directly to audio content, bypassing metadata.
  - Quick check question: What is the fundamental difference between cross-modal retrieval and traditional metadata-based retrieval?

## Architecture Onboarding

- Component map:
  Data collection: Survey platform + query log parser
  Processing pipeline: Tokenization, session grouping, topic annotation
  Analysis engine: Statistical comparison, topic modeling
  Dataset publishing: Zenodo repository

- Critical path:
  1. Collect survey responses with stimuli
  2. Parse and anonymize query logs
  3. Tokenize and normalize queries
  4. Group queries into sessions
  5. Annotate top queries with AudioSet topics
  6. Analyze length, structure, and topic distribution

- Design tradeoffs:
  - Manual topic annotation vs. automated classification (accuracy vs. scalability)
  - Survey stimuli types (audio, image, text) vs. complexity of implementation
  - Session detection threshold (30 minutes) vs. capturing user intent

- Failure signatures:
  - Skewed query length distributions suggesting data collection issues
  - Low annotation coverage indicating topic ambiguity
  - Session detection errors showing poor temporal separation

- First 3 experiments:
  1. Compare query length distributions between survey and query log datasets
  2. Analyze session query patterns to identify exploratory vs. targeted behavior
  3. Evaluate topic coverage across query types to identify user interests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do users expect longer and more complex queries to be understood by sound search systems, or do they prefer concise keyword-based queries?
- Basis in paper: [inferred] The study found that survey queries were significantly longer than Freesound query logs, suggesting users prefer detailed queries when unconstrained by system limitations. However, it is unclear if users expect these longer queries to be understood or if they simply prefer them when given the option.
- Why unresolved: The study does not directly ask users about their expectations for query complexity or whether they believe longer queries are more effective.
- What evidence would resolve it: A follow-up study could directly ask users about their expectations for query complexity and whether they believe longer queries are more effective. Additionally, an A/B test comparing user satisfaction with short versus long queries could provide insights.

### Open Question 2
- Question: How do the topics covered in user queries differ from those in current research datasets used for text-to-audio retrieval?
- Basis in paper: [explicit] The study found a discrepancy between user-written queries and current research datasets with respect to the topics covered. User interest was spread over a wide range of topics, while datasets commonly used for evaluation and benchmarking exclude certain categories like music, sound effects, and speech recordings.
- Why unresolved: The study does not provide a detailed comparison of the specific topics covered in user queries versus research datasets.
- What evidence would resolve it: A detailed analysis comparing the topics covered in user queries from the study with those in current research datasets would provide a clearer picture of the discrepancies.

### Open Question 3
- Question: How do the language patterns used in user queries differ from those in current research datasets?
- Basis in paper: [explicit] The study identified interesting patterns in the expressions used in user queries, such as the use of jargon specific to sound design, music, and video production. These patterns differ from the full sentences typically found in research datasets.
- Why unresolved: The study does not provide a comprehensive analysis of the specific language patterns used in user queries and how they differ from research datasets.
- What evidence would resolve it: A detailed linguistic analysis comparing the language patterns used in user queries from the study with those in current research datasets would provide insights into the differences and potential implications for text-to-audio retrieval systems.

## Limitations

- The survey dataset may not fully represent all sound search scenarios as participants were limited to specific stimuli and contexts.
- The Freesound query log analysis represents only one platform's user base and may not capture broader search behaviors.
- Manual annotation of 1,000 queries represents a small fraction of the total dataset and could introduce bias in topic distribution analysis.

## Confidence

- **High Confidence**: Findings regarding query length differences between survey and Freesound datasets, and the predominance of keyword-based queries over full sentences.
- **Medium Confidence**: Conclusions about the factors influencing survey queries (primary sound source, intended usage, etc.), as these are based on self-reported data that may not reflect actual search behavior.
- **Low Confidence**: Claims about the gap between user queries and current text-to-audio retrieval datasets, as this comparison is not explicitly detailed in the available information.

## Next Checks

1. Conduct a cross-platform analysis by collecting query logs from multiple sound search engines to verify if the observed patterns hold across different user bases.
2. Implement a follow-up study with real-time query collection during actual search sessions to compare against the survey-based findings and validate self-reported data.
3. Perform a longitudinal analysis of query patterns over an extended period to assess temporal variations in user behavior and identify any emerging trends in query formulation.