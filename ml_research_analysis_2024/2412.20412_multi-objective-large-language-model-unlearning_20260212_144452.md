---
ver: rpa2
title: Multi-Objective Large Language Model Unlearning
arxiv_id: '2412.20412'
source_url: https://arxiv.org/abs/2412.20412
tags:
- unlearning
- gradient
- loss
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MOLLM, a method for large language model unlearning
  that addresses gradient explosion and catastrophic forgetting through multi-objective
  optimization. The method replaces standard cross-entropy loss with an unlearning
  variant and computes a common descent direction using a dual space multiple gradient
  descent algorithm.
---

# Multi-Objective Large Language Model Unlearning

## Quick Facts
- arXiv ID: 2412.20412
- Source URL: https://arxiv.org/abs/2412.20412
- Reference count: 31
- One-line primary result: MOLLM achieves 3.45% harmful rate while preserving model utility

## Executive Summary
This paper addresses the challenge of large language model unlearning by proposing MOLLM, a multi-objective optimization approach that simultaneously removes harmful responses and preserves model utility. The method introduces an unlearning variant of cross-entropy loss (UCE) that prevents gradient explosion and employs a dual space multiple gradient descent algorithm to compute common descent directions across multiple objectives. Experiments on the PKU-SafeRLHF dataset demonstrate that MOLLM effectively balances unlearning effectiveness with utility retention, achieving state-of-the-art performance in harmful response removal while maintaining model fluency and functionality.

## Method Summary
MOLLM implements LLM unlearning through a multi-objective optimization framework that combines an unlearning variant of cross-entropy loss (UCE) with dual space multiple gradient descent algorithm (DS-MGDA). The method replaces standard CE loss with UCE to prevent gradient explosion, then computes a common descent direction that simultaneously optimizes unlearning loss, KL divergence for utility preservation, and retain loss. The algorithm trains LLama3-8B on the PKU-SafeRLHF dataset with various learning rates to achieve optimal balance between harmful response removal and model utility maintenance.

## Key Results
- Achieves 3.45% harmful rate on PKU-SafeRLHF dataset
- Prevents gradient explosion through bounded UCE loss
- Preserves model utility with minimal catastrophic forgetting
- Outperforms state-of-the-art unlearning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient explosion is mitigated by replacing standard cross-entropy loss with an unlearning variant (UCE) that has a bounded output.
- Mechanism: The standard CE loss has no upper bound, so gradient ascent can cause unbounded gradients. UCE uses `log(1 - (1 - epsilon) * p_i,c)` which is bounded below by 0, preventing explosion.
- Core assumption: The boundedness of UCE ensures that gradient updates remain stable during unlearning.
- Evidence anchors:
  - [abstract] "replace standard cross-entropy loss with an unlearning variant"
  - [section III.A] "we design the following Unlearning Cross-Entropy (UCE) loss to replace the inverse CE loss"
- Break condition: If the epsilon scaling fails to sufficiently bound the loss (e.g., near p_i,c=1), gradient instability could re-emerge.

### Mechanism 2
- Claim: Catastrophic forgetting is addressed by computing a common descent direction across multiple objectives using dual space projection.
- Mechanism: MOLLM solves a multi-objective problem (unlearning loss, KL divergence, retain loss) by finding a descent direction in the dual space that minimizes all objectives simultaneously.
- Core assumption: The dual space projection ensures the update direction is a valid descent for all objectives.
- Evidence anchors:
  - [abstract] "computes a common descent direction using a dual space multiple gradient descent algorithm"
  - [section III.B] "the dual vectors lie on the edge of S*, one of them is orthogonal to the other two"
- Break condition: If the gradients become linearly dependent or the matrix A is rank-deficient, the dual space computation fails.

### Mechanism 3
- Claim: The multi-objective formulation balances unlearning efficacy with utility preservation through Pareto stationarity.
- Mechanism: By optimizing all objectives jointly, MOLLM finds a Pareto stationary point where no single objective can be improved without worsening another.
- Core assumption: Reaching Pareto stationarity ensures a balanced trade-off between unlearning and utility.
- Evidence anchors:
  - [section III.B] "the model can reach the Pareto stationarity of the multi-objective optimization problem"
- Break condition: If the Pareto front is degenerate or the objectives are too conflicting, the solution may be suboptimal for both goals.

## Foundational Learning

- Concept: Multi-objective optimization
  - Why needed here: Unlearning requires simultaneously minimizing unlearning loss while preserving model utility, naturally forming a multi-objective problem.
  - Quick check question: What defines a Pareto stationary point in multi-objective optimization?

- Concept: Gradient ascent for unlearning
  - Why needed here: Traditional unlearning increases the loss on target data to remove its influence from the model.
  - Quick check question: Why does standard gradient ascent cause gradient explosion with cross-entropy loss?

- Concept: KL divergence for utility preservation
  - Why needed here: KL divergence constrains the unlearned model to stay close to the original model on retain data, preventing catastrophic forgetting.
  - Quick check question: How does KL divergence between output distributions help preserve model utility?

## Architecture Onboarding

- Component map:
  - Loss computation module -> UCE loss for unlearning
  -> CE loss for retain data
  -> KL divergence computation
  -> Multi-objective solver
  -> Dual space multiple gradient descent algorithm
  -> Model update controller
  -> Common descent direction computation and application

- Critical path:
  1. Compute gradients for forget set (UCE loss), retain set (CE loss), and KL divergence
  2. Project gradients to dual space to find common descent direction
  3. Update model parameters using the common direction
  4. Evaluate unlearning effectiveness and utility preservation

- Design tradeoffs:
  - UCE vs. CE: UCE prevents gradient explosion but may converge slower
  - Weighted-sum vs. dual space: Dual space provides better balance but requires more computation
  - Learning rate selection: Affects both unlearning speed and utility preservation

- Failure signatures:
  - Gradient explosion: Loss values become NaN or extremely large
  - Catastrophic forgetting: Fluency scores drop significantly during unlearning
  - Poor unlearning: Harmful rate remains high despite iterations

- First 3 experiments:
  1. Verify UCE prevents gradient explosion by comparing gradient norms with standard CE
  2. Test dual space computation by checking if common direction is descent for all objectives
  3. Measure conflict probabilities to confirm reduced gradient conflicts compared to weighted-sum baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MOLLM algorithm scale with increasingly larger and more complex multi-objective optimization problems in LLM unlearning, particularly as the number of objectives grows beyond three?
- Basis in paper: [explicit] The paper mentions that MOLLM is applicable to MOP with more than 3 objectives, provided that the matrix A is full-rank, but does not explore the performance implications of this scaling.
- Why unresolved: The paper focuses on a three-objective scenario (unlearning, KL divergence, and retain data loss) and does not empirically investigate how the algorithm's performance and computational efficiency are affected as the number of objectives increases.
- What evidence would resolve it: Empirical studies comparing MOLLM's performance and efficiency on datasets with varying numbers of objectives, including computational benchmarks and convergence analysis for larger objective sets.

### Open Question 2
- Question: What is the long-term impact of MOLLM on model generalization and robustness to out-of-distribution data after the unlearning process?
- Basis in paper: [inferred] The paper evaluates model utility preservation through metrics like fluency and conflict probability but does not assess how unlearning affects the model's ability to handle unseen or out-of-distribution data.
- Why unresolved: The evaluation metrics focus on immediate unlearning effectiveness and utility retention on the retain set, but do not consider the broader implications for model generalization and robustness.
- What evidence would resolve it: Comprehensive testing of unlearned models on diverse out-of-distribution datasets, including measures of robustness, generalization error, and performance degradation on unseen tasks.

### Open Question 3
- Question: How does the choice of the small scalar ϵ in the UCE loss function affect the convergence speed and final unlearning performance of MOLLM?
- Basis in paper: [explicit] The paper mentions that ϵ is a small scalar used to slightly scale pi,c to prevent unbounded growth if pi,c = 1, but does not explore the sensitivity of the algorithm to different values of ϵ.
- Why unresolved: The paper uses a fixed value of ϵ without investigating how different choices impact the algorithm's effectiveness, convergence speed, or susceptibility to local minima.
- What evidence would resolve it: Sensitivity analysis experiments varying ϵ across a range of values, including convergence speed comparisons, final unlearning performance metrics, and analysis of the impact on gradient dynamics.

## Limitations

- The dual space multiple gradient descent algorithm (DS-MGDA) implementation details are not fully specified, making faithful reproduction challenging
- Evaluation on a single dataset (PKU-SafeRLHF) with one model architecture (LLama3-8B) limits generalizability
- The paper does not compare MOLLM against baseline methods on the same evaluation setup

## Confidence

- **High Confidence**: The fundamental concept of using multi-objective optimization for unlearning, where UCE loss replaces standard CE loss to prevent gradient explosion. The mathematical formulation of UCE as a bounded loss function is clearly specified and theoretically sound.

- **Medium Confidence**: The dual space projection method for computing common descent directions. While the general approach is described, critical implementation details are missing, and the claim that this prevents catastrophic forgetting depends heavily on correct implementation.

- **Low Confidence**: The claim that MOLLM achieves state-of-the-art performance. Without baseline comparisons on the same dataset and evaluation metrics, this claim cannot be verified. The 3.45% harmful rate is only meaningful in context.

## Next Checks

1. **Gradient Norm Stability Test**: Implement both standard CE loss with gradient ascent and UCE loss with gradient ascent, then compare gradient norm trajectories during training. Verify that UCE consistently produces bounded gradients while CE shows exponential growth.

2. **Pareto Stationarity Verification**: After running MOLLM, compute the Pareto stationarity metric by checking whether any single objective can be improved without worsening others. This validates whether the method actually reaches the claimed Pareto stationary point.

3. **Cross-Dataset Generalization**: Apply MOLLM to a different unlearning dataset (e.g., from the To Forget or Not? paper) and evaluate whether similar harmful rate reductions and utility preservation are achieved. This tests whether the method's effectiveness generalizes beyond the PKU-SafeRLHF dataset.