---
ver: rpa2
title: 'InfoDisent: Explainability of Image Classification Models by Information Disentanglement'
arxiv_id: '2409.10329'
source_url: https://arxiv.org/abs/2409.10329
tags:
- infodisent
- prototypes
- channels
- image
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfoDisent, a hybrid approach to explainable
  AI that disentangles information in the final layer of any pretrained model into
  interpretable atomic concepts. The method applies an orthogonal transformation followed
  by a sparse pooling mechanism (max pooling over ReLU) to create prototype channels
  that capture localized, interpretable features.
---

# InfoDisent: Explainability of Image Classification Models by Information Disentanglement

## Quick Facts
- **arXiv ID**: 2409.10329
- **Source URL**: https://arxiv.org/abs/2409.10329
- **Reference count**: 40
- **Primary result**: InfoDisent achieves higher classification accuracy than existing interpretable models while using fewer channels, with user studies showing improved understanding of model decisions.

## Executive Summary
This paper introduces InfoDisent, a hybrid approach to explainable AI that disentangles information in the final layer of any pretrained model into interpretable atomic concepts. The method applies an orthogonal transformation followed by a sparse pooling mechanism (max pooling over ReLU) to create prototype channels that capture localized, interpretable features. Experiments demonstrate that InfoDisent outperforms existing interpretable models on classification accuracy across multiple datasets including ImageNet, Stanford Cars, and CUB-200-2011, while using significantly fewer channels than baseline models. User studies show that explanations from InfoDisent enable participants to perform statistically significantly better than random guessing in understanding model decisions and disambiguating between classes.

## Method Summary
InfoDisent is a post-hoc explainable AI method that disentangles feature space channels into interpretable atomic concepts (prototypes) using an orthogonal transformation followed by sparse pooling. The method works by freezing a pretrained backbone (CNN or transformer), applying a trainable orthogonal matrix to disentangle features, and using a sparse pooling mechanism that captures only the highest positive and negative activations per channel. The final classification layer uses only nonnegative weights to enable positive reasoning. Training involves optimizing only the orthogonal transformation and classification head using the Gumbel-Softmax trick for differentiability. The approach generalizes prototypical parts-based explainability to large-scale datasets while maintaining the flexibility of post-hoc methods.

## Key Results
- InfoDisent outperforms existing interpretable models on classification accuracy across ImageNet, Stanford Cars, and CUB-200-2011 datasets
- Uses significantly fewer channels than baseline models while maintaining or improving accuracy
- User studies demonstrate participants can understand model decisions and disambiguate classes better than random guessing using InfoDisent explanations
- Successfully generalizes prototypical parts-based explainability to large-scale datasets like ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal transformation followed by sparse pooling creates disentangled, interpretable channels
- Mechanism: The InfoDisent approach applies a trainable orthogonal matrix U in pixel space to disentangle hidden features, then uses a sparse pooling mechanism (max pooling over ReLU) that captures only the highest positive and negative activations per channel. This creates prototype channels where each channel encapsulates a single atomic concept interpretable as prototypical parts.
- Core assumption: Orthogonal transformations preserve distances and scalar products while enabling disentanglement of feature space representations
- Evidence anchors:
  - [abstract] "This approach merges the flexibility of post-hoc methods with the concept-level modeling capabilities of self-explainable neural networks"
  - [section] "we disentangle the channels in feature space by applying an orthogonal map in the pixel space, and consequently do not change the inner-lying distance and scalar product"
  - [corpus] Weak - related papers focus on prototypical parts but don't explicitly discuss orthogonal transformations
- Break condition: If the orthogonal transformation fails to preserve meaningful feature relationships or if sparse pooling eliminates too much information for accurate classification

### Mechanism 2
- Claim: Positive-only weights in the final layer enable human-interpretable reasoning while maintaining classification accuracy
- Mechanism: The final classification layer uses only nonnegative coefficients, allowing the model to perform positive reasoning where each channel's contribution to classification is additive and interpretable. This constraint preserves information about which features contribute positively or negatively to predictions.
- Core assumption: Classification tasks can be effectively performed using only additive combinations of feature activations
- Evidence anchors:
  - [abstract] "InfoDisent enables the disentanglement of information in the final layer of any pretrained model into atomic concepts, which can be interpreted as prototypical parts"
  - [section] "to allow only positive reasoning we allow the matrix A to have only nonnegative values"
  - [corpus] Weak - related works mention interpretable models but don't specifically discuss positive-only weight constraints
- Break condition: If classification accuracy significantly degrades when restricting weights to be nonnegative, or if the model cannot capture necessary feature interactions

### Mechanism 3
- Claim: Training only the classification head while freezing the backbone preserves pretrained representations while enabling interpretability
- Mechanism: InfoDisent uses a frozen pre-trained backbone (CNN or transformer) and trains only the orthogonal transformation and classification head. This approach leverages existing feature representations while adding interpretability through channel disentanglement.
- Core assumption: Pretrained feature representations contain sufficient information for classification tasks when combined with appropriate disentanglement
- Evidence anchors:
  - [abstract] "InfoDisent offers significant flexibility, as it can be seamlessly applied to any pretrained architecture"
  - [section] "the CNN or transformer Backbone, is a frozen classical pre-trained network up to the final feature maps layer"
  - [corpus] Weak - related papers discuss training but don't specifically address freezing pretrained backbones
- Break condition: If frozen features lack sufficient discriminative information for the target task, or if the classification head cannot effectively leverage the pretrained representations

## Foundational Learning

- Concept: Information Bottleneck Principle
  - Why needed here: Provides theoretical foundation for disentangling channels while preserving relevant information for classification
  - Quick check question: How does the information bottleneck balance compression of input features with preservation of relevant information for the task?

- Concept: Orthogonal Transformations
  - Why needed here: Enables channel disentanglement while preserving geometric relationships in feature space
  - Quick check question: What properties of orthogonal matrices make them suitable for feature space transformations in deep learning?

- Concept: Gumbel-Softmax Trick
  - Why needed here: Enables differentiable approximation of argmax operation for sparse feature selection during training
  - Quick check question: How does the Gumbel-Softmax distribution interpolate between continuous and discrete categorical distributions?

## Architecture Onboarding

- Component map: Input → Backbone → Orthogonal Transform → Sparse Pooling → Positive Weights → Classification
- Critical path: Input → Backbone → Orthogonal Transform → Sparse Pooling → Positive Weights → Classification
- Design tradeoffs:
  - Flexibility vs. accuracy: Post-hoc methods are more flexible but less interpretable
  - Number of channels vs. interpretability: Fewer channels improve interpretability but may reduce accuracy
  - Training complexity vs. performance: Training only the head is simpler but may limit optimization
- Failure signatures:
  - Poor classification accuracy despite good explanations
  - Channels not interpretable or capturing meaningful concepts
  - Orthogonal transformation not effectively disentangling features
  - Sparse pooling eliminating too much information
- First 3 experiments:
  1. Verify orthogonal transformation preserves distances: Compute pairwise distances before and after transformation
  2. Test sparse pooling effectiveness: Compare classification accuracy with different pooling thresholds
  3. Validate interpretability: Perform user study on prototype channel explanations vs. baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the orthogonal transformation U in InfoDisent affect the disentanglement of channels compared to other possible transformations (e.g., learned non-orthogonal transformations or attention-based mechanisms)?
- Basis in paper: Explicit - The paper mentions using an orthogonal matrix U to disentangle channels but does not compare this choice to alternatives.
- Why unresolved: The paper only states that an orthogonal transformation is used without exploring whether other transformations might yield better or different results.
- What evidence would resolve it: Comparative experiments showing performance and interpretability metrics when using orthogonal vs. non-orthogonal transformations, or attention-based mechanisms instead of U.

### Open Question 2
- Question: What is the impact of varying the Gumbel-Softmax temperature τ during training on the final model's interpretability and accuracy?
- Basis in paper: Explicit - The paper mentions using Gumbel-Softmax with τ initialized at 1 and reduced to 0.2, but does not explore the effect of different temperature schedules.
- Why unresolved: The paper does not investigate how different temperature values or schedules affect the model's performance or interpretability.
- What evidence would resolve it: Experiments varying τ throughout training (e.g., different initial values, decay rates, or fixed values) and measuring the resulting accuracy and interpretability metrics.

### Open Question 3
- Question: How does InfoDisent's channel sparsity compare to other interpretable methods like ProtoPNet when applied to large-scale datasets like ImageNet, and what are the trade-offs?
- Basis in paper: Inferred - The paper claims InfoDisent generalizes prototypical parts to ImageNet but doesn't directly compare channel sparsity with ProtoPNet or similar methods on ImageNet.
- Why unresolved: While the paper shows InfoDisent uses fewer channels than baseline CNNs, it doesn't compare this sparsity to other interpretable methods specifically on ImageNet.
- What evidence would resolve it: Direct comparison of the number of active channels used by InfoDisent versus ProtoPNet (or similar methods) for ImageNet classification, along with accuracy and interpretability trade-offs.

### Open Question 4
- Question: Can the information bottleneck mechanism in InfoDisent be adapted to work with different pooling strategies beyond max pooling over ReLU, and how would this affect interpretability?
- Basis in paper: Explicit - The paper uses max pooling over ReLU as the sparse pooling mechanism but doesn't explore alternatives.
- Why unresolved: The paper presents one specific sparse pooling approach without investigating whether other pooling strategies might yield better or different results.
- What evidence would resolve it: Experiments comparing InfoDisent with alternative sparse pooling mechanisms (e.g., different activation functions, attention-based pooling) and measuring their impact on interpretability and accuracy.

## Limitations

- The orthogonal transformation's effectiveness across diverse backbone architectures remains unclear, particularly for transformers where feature space geometry may differ significantly from CNNs.
- The positive-only weight constraint in the classification layer could limit model expressiveness for complex classification boundaries.
- The sparse pooling mechanism's reliance on the Gumbel-Softmax approximation introduces approximation errors that may affect both accuracy and interpretability.

## Confidence

**High Confidence**: InfoDisent's ability to disentangle channels and produce interpretable prototypes (supported by visual results and user studies). The classification accuracy improvements on standard benchmarks (quantitatively demonstrated).

**Medium Confidence**: The claim that InfoDisent outperforms self-explainable networks on large-scale datasets (based on limited comparative experiments). The assertion that InfoDisent provides flexibility comparable to post-hoc methods (conceptual but not empirically validated across diverse architectures).

**Low Confidence**: The claim that InfoDisent maintains accuracy while using fewer channels (statistical significance not established). The assertion that orthogonal transformations preserve all relevant geometric relationships in feature space (theoretical but not empirically verified).

## Next Checks

1. **Architecture Generalization Test**: Apply InfoDisent to a diverse set of backbone architectures (CNNs, transformers, vision MLPs) and measure classification accuracy and interpretability consistency across all models.

2. **Statistical Significance Analysis**: Conduct formal statistical tests comparing InfoDisent's classification accuracy and interpretability metrics against both post-hoc and self-explainable baselines across multiple random seeds.

3. **Ablation on Transformation Components**: Systematically remove the orthogonal transformation, sparse pooling, and positive weight constraints to quantify each component's contribution to accuracy and interpretability.