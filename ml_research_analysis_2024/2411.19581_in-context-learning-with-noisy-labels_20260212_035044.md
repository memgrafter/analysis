---
ver: rpa2
title: In-Context Learning with Noisy Labels
arxiv_id: '2411.19581'
source_url: https://arxiv.org/abs/2411.19581
tags:
- learning
- labels
- noisy
- in-context
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of in-context learning with noisy
  labels, addressing the issue of corrupted labels in demonstrations used for few-shot
  learning with large language models (LLMs). The authors propose several baseline
  methods adapted from noisy label learning literature, including correction, weighting,
  reordering, and selection techniques.
---

# In-Context Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2411.19581
- Source URL: https://arxiv.org/abs/2411.19581
- Authors: Junyong Kang; Donghyun Son; Hwanjun Song; Buru Chang
- Reference count: 34
- Primary result: Novel rectification method outperforms baseline approaches for in-context learning with noisy labels, maintaining stability across different noise rates

## Executive Summary
This paper introduces the task of in-context learning with noisy labels, where corrupted labels in demonstrations affect few-shot learning performance with large language models. The authors propose several baseline methods adapted from noisy label learning literature and introduce a novel rectification method that fine-tunes a generative model to process multiple noisy demonstrations and output corrected labels. Experimental results on three classification datasets demonstrate that the proposed rectification method outperforms baseline approaches and maintains stability across different noise rates, showing improved robustness to label noise in in-context learning scenarios.

## Method Summary
The paper proposes four baseline methods (correction, weighting, reordering, selection) adapted from noisy label learning literature, along with a novel rectification method. The rectification method fine-tunes a GPT-2 model to process multiple noisy demonstrations simultaneously and output corrected labels, leveraging contextual information across examples. The method is trained for 10 epochs with a batch size of 2 and a learning rate of 1e-4, using LoRA for memory-efficient training. The approach is evaluated on three classification datasets (MRPC, SST-5, Tweet) with varying noise rates, using a clean subset (10% of training data) to train robust methods.

## Key Results
- The rectification method outperforms baseline approaches across all noise rates
- Correction method provides stable but lower performance across all noise rates
- Method maintains effectiveness even at high noise rates (up to 0.5)
- Data efficiency is achieved through context utilization across demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The rectification method improves performance by processing multiple noisy demonstrations simultaneously rather than independently.
- Mechanism: By taking a sequence of demonstrations as input, the rectification method can leverage contextual information across examples to better identify and correct noisy labels, rather than treating each demonstration in isolation like baseline methods.
- Core assumption: The model can effectively use inter-demonstration relationships to identify label inconsistencies and correct them.
- Evidence anchors:
  - [abstract]: "Unlike baseline methods that perform classification independently for each demonstration, rectification gets a sequence of demonstrations as an input. Hence, it can reference all retrieved demonstrations and utilize them for noisy label rectification."
  - [section]: "Unlike baseline methods that perform classification independently for each demonstration, rectification gets a sequence of demonstrations as an input. Hence, it can reference all retrieved demonstrations and utilize them for noisy label rectification."
- Break condition: If demonstrations are highly dissimilar or the noisy labels are random rather than systematic, the contextual information may not provide meaningful signal for correction.

### Mechanism 2
- Claim: The rectification method achieves data efficiency by learning to correct labels from few examples through context utilization.
- Mechanism: The model learns patterns of how labels should relate to each other across demonstrations, allowing it to generalize correction strategies from limited training data rather than requiring extensive supervised examples.
- Core assumption: Demonstrations contain sufficient contextual patterns that can be learned with limited examples to generalize correction behavior.
- Evidence anchors:
  - [abstract]: "We trained the model for 10 epochs with a batch size of 2 and a learning rate of 1e-4, employing LoRA [14] for memory-efficient training."
  - [section]: "Table 5 shows that these method are less effective than our approach, where 10 demonstrations are given as an input. From this observation, we conclude that the rectification method benefits from referring to the given context (demonstrations), which we believe to be crucial for the data efficiency."
- Break condition: If the demonstration context becomes too sparse (fewer than 10 examples) or the noise patterns are too complex, the model may fail to generalize effectively.

### Mechanism 3
- Claim: The correction baseline method provides a stable but lower-performing approach by consistently applying classifier outputs across all noise rates.
- Mechanism: By overwriting all demonstration labels with classifier predictions, this method avoids propagating noisy labels while maintaining consistent performance regardless of noise rate.
- Core assumption: The classifier trained on clean data provides sufficiently accurate predictions to replace noisy labels without introducing new errors.
- Evidence anchors:
  - [abstract]: "The correction method acts as a robust baseline, maintaining consistent performance across all noise rates."
  - [section]: "In the GPT2-Neo results, without any label manipulation, we observe that the performance of in-context learning degrades as the noise rate increases. In contrast, the correction method acts as a robust baseline, maintaining consistent performance across all noise rates."
- Break condition: If the classifier itself has systematic biases or the clean subset is too small to train an effective classifier, this method may introduce consistent errors rather than improving robustness.

## Foundational Learning

- Concept: In-context learning mechanism
  - Why needed here: Understanding how LLMs perform tasks using demonstrations without fine-tuning is fundamental to grasping why noisy labels are problematic.
  - Quick check question: How does an LLM use demonstrations to perform a task without updating its parameters?

- Concept: Learning with noisy labels
  - Why needed here: The paper builds on techniques from this field to adapt them for in-context learning, so understanding noise-robust learning methods is essential.
  - Quick check question: What are the key challenges in building models that can learn effectively from corrupted training data?

- Concept: Classification model confidence calibration
  - Why needed here: Baseline methods rely on classifier confidence scores to weight, reorder, or select demonstrations, making understanding confidence calibration important.
  - Quick check question: Why might classifier confidence scores be unreliable indicators of true label correctness?

## Architecture Onboarding

- Component map:
  Retrieval system -> Label manipulation module -> LLM inference engine -> Evaluation metrics

- Critical path:
  1. Input query arrives
  2. Retrieval system selects demonstrations
  3. Label manipulation module processes demonstrations
  4. LLM receives processed prompt
  5. LLM generates predictions
  6. Evaluation metrics assess performance

- Design tradeoffs:
  - Baseline methods vs. rectification: Simplicity and lower computational cost vs. better performance through context utilization
  - Classifier-based vs. generative approach: Faster inference vs. better handling of demonstration relationships
  - Number of demonstrations: More context for rectification vs. increased computational cost

- Failure signatures:
  - Degradation with high noise rates: Indicates need for more robust label manipulation
  - Inconsistent performance across datasets: Suggests dataset-specific tuning may be needed
  - High variance across runs: Points to stability issues in the label manipulation approach

- First 3 experiments:
  1. Implement the correction baseline and verify it maintains stable performance across noise rates
  2. Add the rectification method and compare its performance to correction across all noise rates
  3. Test the stability of both methods by running multiple trials with different random seeds at high noise rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed rectification method perform compared to fine-tuning the LLM directly on noisy demonstrations for in-context learning with noisy labels?
- Basis in paper: [inferred] The paper mentions that the rectification method processes multiple noisy demonstrations simultaneously and outputs corrected labels, but does not compare it to fine-tuning the LLM directly on noisy demonstrations.
- Why unresolved: The paper focuses on methods that manipulate labels of task demonstrations rather than updating LLM parameters, but does not explore the alternative approach of fine-tuning the LLM directly on noisy demonstrations.
- What evidence would resolve it: Experiments comparing the rectification method's performance to that of an LLM fine-tuned on noisy demonstrations would provide insight into the effectiveness of each approach.

### Open Question 2
- Question: How does the rectification method's performance scale with increasing noise rates beyond 0.5?
- Basis in paper: [explicit] The paper reports results for noise rates up to 0.5 but does not explore performance at higher noise rates.
- Why unresolved: The paper's experiments are limited to noise rates up to 0.5, leaving the method's performance at higher noise rates unknown.
- What evidence would resolve it: Experiments evaluating the rectification method's performance at noise rates above 0.5 would reveal its scalability and robustness to extreme levels of label noise.

### Open Question 3
- Question: How does the rectification method perform on more complex tasks beyond text classification, such as question answering or summarization?
- Basis in paper: [inferred] The paper focuses on text classification tasks and does not explore the rectification method's performance on other NLP tasks.
- Why unresolved: The paper's experiments are limited to text classification tasks, leaving the method's applicability and effectiveness on other NLP tasks unexplored.
- What evidence would resolve it: Experiments applying the rectification method to other NLP tasks, such as question answering or summarization, would demonstrate its versatility and potential for broader application.

## Limitations

- The method's effectiveness for multi-class problems or tasks requiring more nuanced understanding remains unclear
- Computational overhead of fine-tuning a generative model may limit practical deployment
- Results are limited to specific classification tasks and noise patterns, requiring validation on other domains

## Confidence

- **High Confidence**: The core finding that in-context learning degrades with increasing label noise is well-supported by experimental evidence across multiple datasets and noise rates
- **Medium Confidence**: The superiority of the rectification method over baseline approaches is demonstrated, but results are limited to specific classification tasks and noise patterns
- **Medium Confidence**: The claim about data efficiency through context utilization is supported by ablation studies, but the specific threshold of 10 demonstrations appears somewhat arbitrary

## Next Checks

1. **Cross-domain robustness test**: Evaluate the rectification method on non-text classification tasks (e.g., tabular data or image classification with text descriptions) to assess generalizability beyond the current three text classification datasets

2. **Noise pattern sensitivity analysis**: Systematically vary the correlation structure of label noise (e.g., class-dependent vs. random noise) to determine whether the rectification method's effectiveness depends on specific noise characteristics

3. **Scaling experiment**: Test the rectification method with varying numbers of demonstrations (fewer than 10 and more than 10) to establish the optimal context size and understand the diminishing returns of additional demonstrations for label correction