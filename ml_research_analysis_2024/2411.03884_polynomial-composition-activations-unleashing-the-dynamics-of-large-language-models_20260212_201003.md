---
ver: rpa2
title: 'Polynomial Composition Activations: Unleashing the Dynamics of Large Language
  Models'
arxiv_id: '2411.03884'
source_url: https://arxiv.org/abs/2411.03884
tags:
- relu
- polyrelu
- activation
- functions
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Polynomial Composition Activations (PolyCom),
  a novel family of activation functions designed to enhance the expressivity of transformer
  models. PolyCom composes polynomials with other functions, such as ReLU or normalization,
  to capture higher-order interactions in data more effectively than traditional activations
  like ReLU or GELU.
---

# Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models

## Quick Facts
- **arXiv ID**: 2411.03884
- **Source URL**: https://arxiv.org/abs/2411.03884
- **Reference count**: 40
- **Primary result**: Novel activation functions achieving optimal approximation rates in Sobolev spaces with improved empirical performance

## Executive Summary
This paper introduces Polynomial Composition Activations (PolyCom), a family of activation functions designed to enhance the expressivity of transformer models. PolyCom composes polynomials with other functions like ReLU or normalization to capture higher-order interactions in data more effectively than traditional activations. Theoretically, PolyCom networks achieve optimal approximation rates in Sobolev spaces while requiring fewer parameters. Empirically, substituting PolyCom for conventional activations in both dense (1B parameters) and sparse (MoE with 1B active, 7B total parameters) large language models leads to faster convergence and improved downstream task performance.

## Method Summary
PolyCom represents a novel family of activation functions that compose polynomials with other functions such as ReLU or normalization layers. The key innovation lies in capturing higher-order interactions in data through polynomial compositions, which allows for more expressive function approximation compared to traditional activations. The paper explores variants like PolyReLU and PolyNorm, demonstrating that these compositions can achieve optimal approximation rates in Sobolev spaces theoretically while showing practical improvements in training convergence and model performance empirically across different model architectures and scales.

## Key Results
- Dense transformer (1B parameters): Up to 1.21% accuracy improvement when using PolyCom activations
- Faster convergence observed across both dense and MoE model architectures
- Consistent reductions in training and validation losses for MoE models with 1B active parameters (7B total)
- Higher effective rank in weight matrices for FFN layers with PolyCom, suggesting improved expressivity

## Why This Works (Mechanism)
PolyCom captures higher-order interactions in data through polynomial compositions, allowing the model to represent more complex functions with fewer parameters. By composing polynomials with other functions like ReLU or normalization, the activation functions can model smooth, high-order interactions more effectively than traditional activations. This improved expressivity translates to better approximation capabilities in Sobolev spaces theoretically and faster convergence empirically, as the model can represent target functions more efficiently during training.

## Foundational Learning
- **Polynomial approximation theory**: Understanding how polynomials can approximate smooth functions with optimal rates in Sobolev spaces - needed to grasp the theoretical foundation of PolyCom's effectiveness
- **Sobolev space norms**: Familiarity with these function spaces and their norms - needed to understand the approximation guarantees
- **Effective rank**: The concept of measuring matrix rank using entropy-based methods (Roy & Vetterli, 2007) - needed to interpret the weight matrix analysis showing improved expressivity
- **Transformer architecture**: Understanding of feed-forward networks and attention mechanisms - needed to see where PolyCom fits into the model
- **Activation function design**: Knowledge of how different activation functions affect model capacity and training dynamics - needed to contextualize PolyCom's innovations

## Architecture Onboarding
**Component Map**: Input -> PolyCom Activation -> FFN Layer -> Output
**Critical Path**: PolyCom activation functions are integrated into feed-forward networks within transformer blocks, replacing standard activations like ReLU or GELU
**Design Tradeoffs**: Higher-order polynomials improve approximation but increase computational cost and risk overflow; normalization variants address stability but add complexity
**Failure Signatures**: Potential stability issues in non-transformer architectures due to lack of normalization features; computational overhead with higher polynomial orders
**First Experiments**:
1. Replace ReLU with PolyReLU (r=3) in a small transformer and compare convergence speed
2. Test PolyNorm in a ResNet50 on ImageNet-1K to verify non-transformer compatibility
3. Measure effective rank changes in FFN layers when substituting different activation functions

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How does the order of PolyCom (beyond r=3) affect model performance and computational efficiency in large-scale language models?
- **Basis in paper**: [explicit] The paper explores different orders of PolyReLU in Figure 5(a), showing improved convergence with higher orders but noting no significant difference between r=3 and r=4, along with potential computational overhead and overflow issues.
- **Why unresolved**: The paper only tests orders up to r=4 and does not provide comprehensive analysis for higher orders or their impact on very large models.
- **What evidence would resolve it**: Systematic experiments testing higher orders (e.g., r=5, r=6) across various model scales, measuring both performance gains and computational costs.

### Open Question 2
- **Question**: How does PolyCom perform in non-transformer architectures like ResNets, and what are the potential stability issues?
- **Basis in paper**: [explicit] The paper mentions that PolyReLU may cause stability issues in non-transformer architectures like ResNet due to lack of normalization features, and provides preliminary results on ImageNet-1K showing PolyNorm outperforms ReLU in ResNet50.
- **Why unresolved**: The paper only provides one example (ResNet50) and does not explore other non-transformer architectures or provide detailed analysis of the stability issues.
- **What evidence would resolve it**: Extensive experiments testing PolyCom variants in various non-transformer architectures (e.g., CNNs, RNNs) and analysis of the specific stability challenges and solutions.

### Open Question 3
- **Question**: What is the theoretical relationship between the effective rank of weight matrices and model performance, and how does PolyCom influence this relationship?
- **Basis in paper**: [explicit] The paper analyzes the rank of weights in FFN layers using effective rank (Roy & Vetterli, 2007) and finds that PolyReLU and PolyNorm result in higher weight ranks compared to other activation functions, suggesting improved expressivity.
- **Why unresolved**: While the paper observes a correlation between higher rank and better performance, it does not establish a causal relationship or provide a theoretical framework explaining how PolyCom specifically influences this relationship.
- **What evidence would resolve it**: Theoretical analysis linking the effective rank of weight matrices to model capacity and generalization, along with empirical studies manipulating rank through other means to confirm the observed relationship with PolyCom.

## Limitations
- Theoretical approximation guarantees rely on idealized assumptions about function smoothness that may not fully translate to practical training scenarios
- Empirical improvements need validation across diverse model architectures and tasks to establish robustness
- MoE experiments lack detailed ablation studies isolating PolyCom's contribution from other architectural factors

## Confidence
- **Theoretical approximation guarantees**: Medium - The Sobolev space analysis is rigorous but depends on conditions that may be difficult to verify in practice
- **Empirical performance claims**: Medium-High - Results are consistent across experiments but limited to specific model scales and tasks
- **Computational efficiency claims**: Medium - Faster convergence is shown but memory and inference latency impacts require further investigation

## Next Checks
1. Evaluate PolyCom across multiple model scales (from 100M to 10B+ parameters) to verify the parameter efficiency claims hold at different model sizes
2. Test on diverse task types including mathematical reasoning, code generation, and multilingual benchmarks beyond standard language tasks
3. Conduct ablation studies comparing PolyCom against other modern activation functions (SwiGLU, SiLU, etc.) under identical training conditions to isolate its specific advantages