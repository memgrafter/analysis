---
ver: rpa2
title: 'Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI Assessment'
arxiv_id: '2407.12401'
source_url: https://arxiv.org/abs/2407.12401
tags:
- goar
- roar
- features
- feature
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies geometric limitations in widely-used pixel-perturbation
  metrics for explainable AI evaluation. The key problem is that ROAR and ROAD are
  coordinate-dependent and cannot distinguish between features that differ at relevant
  coordinates, making them unreliable for comparing feature attribution methods.
---

# Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI Assessment

## Quick Facts
- **arXiv ID:** 2407.12401
- **Source URL:** https://arxiv.org/abs/2407.12401
- **Reference count:** 40
- **Primary result:** GOAR achieves up to 0.97 Pearson correlation with ground truth metrics while overcoming coordinate dependence limitations of existing methods

## Executive Summary
This paper addresses fundamental geometric limitations in widely-used pixel-perturbation metrics for explainable AI evaluation. ROAR and ROAD metrics are coordinate-dependent and cannot reliably compare feature attribution methods across different image orientations or transformations. The authors propose GOAR, a novel feature-perturbation strategy that eliminates coordinate dependence by shifting samples along feature directions and projecting them back onto the data manifold using a diffusion model. GOAR demonstrates superior performance, achieving strong correlation with ground truth metrics while successfully comparing attribution methods across vision and tabular datasets where traditional methods fail.

## Method Summary
GOAR overcomes coordinate dependence by perturbing images along feature directions rather than pixel coordinates. The method computes feature vectors for each sample, shifts the image in the opposite feature direction, and projects the modified image back onto the data manifold using a diffusion model (SDEdit). This eliminates pixel-coordinate dependence while maintaining meaningful feature removal. GOAR measures performance degradation using cumulative accuracy counting (rather than accuracy drop) to capture information loss during feature removal, as samples can regain separability after losing information. The method is validated across multiple datasets (MNIST, CIFAR-10, FashionMNIST, Iris, Raisin, Wine) using various attribution methods.

## Key Results
- GOAR achieves up to 0.97 Pearson correlation with ground truth metrics, significantly outperforming ROAR and ROAD
- GOAR successfully compares attribution methods across vision datasets where pixel-perturbation methods show no meaningful differences
- Manifold projection is essential - ablation studies show that without projection, performance degrades and class information leaks
- GOAR's cumulative accuracy measurement accurately captures information loss, unlike accuracy drop which can misrepresent sample separability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GOAR overcomes coordinate dependence by perturbing along feature direction and projecting back to data manifold
- **Mechanism:** GOAR shifts samples in the feature direction (−v) and uses diffusion model projection to maintain manifold alignment, avoiding pixel-coordinate dependence
- **Core assumption:** Feature vectors have meaningful directional information and diffusion models can project back to manifold
- **Evidence anchors:**
  - [abstract] "shifting samples along the feature direction and projecting them back onto the data manifold using a diffusion model"
  - [section] "GOAR perturbs images by moving the data sample along the feature direction, eliminating the dependence on pixel coordinates"
  - [corpus] "Found 25 related papers" (weak evidence - no direct connection)
- **Break condition:** If diffusion model fails to properly project perturbed samples back to manifold, class information leakage occurs

### Mechanism 2
- **Claim:** Manifold projection prevents class information leakage during feature perturbation
- **Mechanism:** After shifting samples along feature direction, SDEdit diffusion model adds noise and denoises to remove off-manifold components
- **Core assumption:** Diffusion models can effectively separate on-manifold and off-manifold components
- **Evidence anchors:**
  - [section] "we employed a manifold projection method known as SDEdit...effectively eliminates irrelevant information associated with off-manifold directions while keeping the meaningful on-manifold components unchanged"
  - [section] "We validate this by examining the saliency maps of retrained models...w/o proj. exhibits entirely different features, such as grid-like patterns"
  - [corpus] "Found 25 related papers" (weak evidence - no direct connection)
- **Break condition:** If perturbed samples have low-frequency off-manifold components, diffusion model may struggle to properly project

### Mechanism 3
- **Claim:** GOAR's cumulative accuracy measurement accurately captures information loss during feature removal
- **Mechanism:** Instead of accuracy drop, GOAR counts cumulative number of misclassified examples to track information loss
- **Core assumption:** Samples can regain separability after losing information, making accuracy drop unreliable
- **Evidence anchors:**
  - [section] "GOAR adopts a distinct measurement criterion, which involves tallying the cumulative number of misclassified examples. This approach is chosen because, in GOAR, a sample can regain separability after losing its information"
  - [section] "Figure 4 shows that accuracy does not accurately capture the number of samples that lost information, while Cumulative accuracy correctly captures the number of lost samples"
  - [corpus] "Found 25 related papers" (weak evidence - no direct connection)
- **Break condition:** If perturbed samples never regain separability, accuracy drop would be equally effective

## Foundational Learning

- **Concept:** Data manifolds and diffusion models
  - **Why needed here:** Understanding how diffusion models project data onto manifolds is crucial for implementing GOAR's perturbation strategy
  - **Quick check question:** What distinguishes on-manifold from off-manifold components in diffusion model sampling?

- **Concept:** Feature attribution methods and evaluation metrics
  - **Why needed here:** Comparing GOAR with existing methods like ROAR, ROAD, and Eval-X requires understanding their mechanisms and limitations
  - **Quick check question:** How do pixel-perturbation methods differ from feature-perturbation methods in their approach to feature removal?

- **Concept:** Coordinate transformations and geometric invariance
  - **Why needed here:** The core problem GOAR solves is that existing methods depend on pixel coordinates rather than geometric features
  - **Quick check question:** Why would rotating an image affect ROAR but not GOAR's evaluation?

## Architecture Onboarding

- **Component map:** Feature attribution method → Feature vector computation (−v direction) → Diffusion model for manifold projection → Retraining pipeline → Performance measurement (cumulative accuracy)

- **Critical path:** Feature extraction → Perturbation → Manifold projection → Retraining → Evaluation

- **Design tradeoffs:**
  - Using diffusion models provides manifold alignment but increases computational cost
  - Cumulative accuracy measurement is more accurate but harder to interpret than simple accuracy drop
  - Feature perturbation requires reliable feature attribution methods

- **Failure signatures:**
  - Poor correlation with ground truth metrics suggests manifold projection isn't working
  - Similar performance across attribution methods indicates pixel-coordinate dependence
  - Performance worse than Random baseline suggests class information leakage

- **First 3 experiments:**
  1. Run GOAR on synthetic Gaussian mixture dataset with known features
  2. Compare GOAR correlation with ground truth vs ROAR/ROAD on OpenXAI benchmark
  3. Test ablation study: GOAR with and without manifold projection on CIFAR10

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does GOAR perform when applied to more complex datasets beyond MNIST, CIFAR-10, and FashionMNIST?
- **Basis in paper:** [explicit] The paper mentions experiments on vision datasets (MNIST, CIFAR-10, FashionMNIST) and tabular datasets (Iris, Raisin, Wine) but does not explore more complex datasets.
- **Why unresolved:** The experiments were limited to relatively simple datasets, leaving the performance on more complex datasets untested.
- **What evidence would resolve it:** Conducting experiments on more complex datasets like ImageNet or other large-scale datasets to evaluate GOAR's effectiveness.

### Open Question 2
- **Question:** Can the computational cost of GOAR be reduced without compromising its effectiveness?
- **Basis in paper:** [explicit] The paper highlights that high computational cost is a significant limitation of GOAR, especially when using the diffusion model.
- **Why unresolved:** The paper does not propose solutions to reduce computational costs or explore alternative methods that could achieve similar results with lower computational demands.
- **What evidence would resolve it:** Developing and testing more efficient algorithms or models that can achieve similar performance to GOAR but with reduced computational requirements.

### Open Question 3
- **Question:** How does GOAR handle features with low-frequency components in their off-manifold direction?
- **Basis in paper:** [explicit] The paper mentions that when features have low-frequency components in their off-manifold direction, the diffusion model may struggle to project the modified image onto the data manifold.
- **Why unresolved:** The paper does not provide a detailed analysis or solution for handling such features, leaving this as a potential area for improvement.
- **What evidence would resolve it:** Conducting experiments to test GOAR's performance on datasets with features that have low-frequency components and developing methods to improve its handling of such features.

### Open Question 4
- **Question:** Can GOAR be extended to other domains beyond image and tabular data?
- **Basis in paper:** [explicit] The paper demonstrates GOAR's applicability to vision and tabular datasets but does not explore other domains like natural language processing or graph data.
- **Why unresolved:** The paper does not discuss the potential extension of GOAR to other domains or provide experimental results for such extensions.
- **What evidence would resolve it:** Applying GOAR to other domains such as NLP or graph data and evaluating its effectiveness in those contexts.

## Limitations
- GOAR relies heavily on diffusion models for manifold projection, introducing significant computational overhead
- The method's effectiveness depends on the quality of feature attribution methods used to generate feature vectors
- Performance on more complex datasets beyond MNIST, CIFAR-10, and FashionMNIST remains untested

## Confidence
- **High confidence:** GOAR successfully eliminates coordinate dependence and achieves strong correlation with ground truth metrics (0.97 Pearson)
- **Medium confidence:** Manifold projection consistently prevents class information leakage across all datasets and attribution methods
- **Low confidence:** The computational cost of GOAR is acceptable for practical applications, given the reliance on expensive diffusion model sampling

## Next Checks
1. Test GOAR's performance on rotated and transformed versions of the same images to verify true coordinate invariance in practice
2. Measure computational overhead across different diffusion model architectures and perturbation strengths to establish practical deployment limits
3. Apply GOAR to a real-world medical imaging dataset where feature attribution quality directly impacts clinical decision-making