---
ver: rpa2
title: Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training
arxiv_id: '2405.02954'
source_url: https://arxiv.org/abs/2405.02954
tags:
- source
- co-learn
- target
- domain
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles source-free domain adaptation (SFDA), where
  a model trained on a labeled source domain must adapt to an unlabeled target domain
  without access to source data. The authors observe that while large pre-trained
  networks (e.g., ImageNet) are used to initialize source models, they are discarded
  after source training, potentially losing valuable target domain knowledge.
---

# Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training

## Quick Facts
- arXiv ID: 2405.02954
- Source URL: https://arxiv.org/abs/2405.02954
- Authors: Wenyu Zhang; Li Shen; Chuan-Sheng Foo
- Reference count: 7
- Primary result: Introduces a two-branch co-learning framework that integrates pre-trained networks into source-free domain adaptation, achieving improved performance on 4 benchmark datasets

## Executive Summary
This paper addresses source-free domain adaptation (SFDA), where a model trained on a labeled source domain must adapt to an unlabeled target domain without access to source data. The authors observe that pre-trained networks like ImageNet or CLIP, while used to initialize source models, are typically discarded after source training - potentially losing valuable target domain knowledge. They propose a two-branch framework that integrates pre-trained networks into the target adaptation process. The "Co-learn" algorithm uses a pre-trained vision encoder and a weighted nearest-centroid-classifier to generate improved pseudolabels, while "Co-learn++" extends this by incorporating CLIP's zero-shot classification capabilities. The method is evaluated on 4 benchmark datasets, showing performance improvements over existing SFDA methods, particularly when using powerful pre-trained networks like CLIP or Swin.

## Method Summary
The paper proposes a two-branch co-learning framework for source-free domain adaptation. The first branch adapts the source model to the target domain, while the second branch uses a pre-trained network (vision encoder or CLIP) to generate pseudolabels. The two branches iteratively update each other - the pre-trained branch provides pseudolabels to finetune the adaptation branch, while the adapted model updates the pre-trained branch. Co-learn uses a weighted nearest-centroid-classifier with the pre-trained vision encoder, while Co-learn++ additionally incorporates CLIP's zero-shot classification through textual descriptions of classes. The method can be integrated with existing SFDA approaches and is evaluated on Office-Home, Office-31, VisDA, and DomainNet datasets.

## Key Results
- Co-learn framework achieves consistent improvements over existing SFDA methods across multiple benchmark datasets
- CLIP integration in Co-learn++ provides additional performance gains, particularly on Office-31 where it outperforms POUF and ReCLIP
- Performance improvements are more pronounced when using powerful pre-trained networks like CLIP or Swin compared to standard ResNet models
- The approach demonstrates flexibility by being compatible with various existing SFDA methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Source model overfitting to source domain during training causes loss of target domain knowledge.
- Mechanism: When a pre-trained network is fine-tuned on source data, its features become biased toward the source distribution, reducing generalizability to the target domain.
- Core assumption: The source domain and target domain have different data distributions (covariate shift).
- Evidence anchors:
  - [abstract]: "While the large data pre-trained model initially has diverse features important for generalization... finetuning on source data can cause it to overfit to source distribution and forget pre-existing target information."
  - [section 3.2]: "Large data pre-trained networks... may be more compatible with the target domain instead. That is, firstly, class-discriminative information useful for the target domain may be lost from the pre-trained network during source training as the source model learns to fit exclusively to the source data distribution."
- Break condition: If source and target domains are identical or very similar, the loss of target knowledge may be negligible.

### Mechanism 2
- Claim: Integrating pre-trained networks into the target adaptation process can improve pseudolabel quality.
- Mechanism: The pre-trained network provides an alternative, potentially more target-compatible view of the data, which can be used to collaboratively generate more accurate pseudolabels for finetuning the source model.
- Core assumption: The pre-trained network has learned features that are more generalizable to the target domain than the source model.
- Evidence anchors:
  - [abstract]: "Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process."
  - [section 4.2.1]: "The weighted NCC leverages the target domain class-discriminative cluster structures in f* features."
- Break condition: If the pre-trained network is equally or more biased towards the source domain than the source model, it may not provide useful information.

### Mechanism 3
- Claim: CLIP's vision-language capabilities can further enhance pseudolabel quality by leveraging zero-shot classification.
- Mechanism: CLIP's text encoder can create class centroids based on textual descriptions, which can be used to guide the classification head fitted on the CLIP vision encoder, providing an additional source of information for pseudolabel generation.
- Core assumption: The textual descriptions of the classes are sufficiently informative to create meaningful class centroids.
- Evidence anchors:
  - [abstract]: "Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension 'Co-learn++' to further incorporate CLIP's zero-shot classification decisions."
  - [section 4.2.2]: "We modify the formulation of the weighted NCC... such that CLIP zero-shot classifier outputs can guide the final prediction."
- Break condition: If the textual descriptions are not informative or do not align well with the visual concepts, the zero-shot classification may not be helpful.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: The paper addresses the problem of adapting a model trained on one domain (source) to perform well on a different but related domain (target).
  - Quick check question: What is the main difference between supervised domain adaptation and unsupervised domain adaptation?

- Concept: Covariate shift
  - Why needed here: The paper assumes that the source and target domains have different data distributions, which is a form of covariate shift.
  - Quick check question: How does covariate shift differ from concept shift in domain adaptation?

- Concept: Pseudo-labeling
  - Why needed here: The paper uses pseudo-labels generated by the source model and the pre-trained network to finetune the source model on the target domain.
  - Quick check question: What are the potential issues with using pseudo-labels for training, and how does the paper address them?

## Architecture Onboarding

- Component map:
  - Source model -> Adaptation model branch
  - Pre-trained network (e.g., CLIP) -> Pre-trained model branch
  - Co-learning algorithm -> Iterative pseudolabel generation and model update

- Critical path:
  1. Initialize the adaptation model branch with the source model
  2. Initialize the pre-trained model branch with the pre-trained network
  3. Generate pseudolabels using the adaptation and pre-trained model branches
  4. Finetune the adaptation model branch using the pseudolabels
  5. Update the pre-trained model branch based on the adapted adaptation model branch
  6. Repeat steps 3-5 for a fixed number of iterations

- Design tradeoffs:
  - Using a more powerful pre-trained network may improve performance but also increase computational cost
  - The choice of confidence threshold for pseudolabeling affects the balance between precision and recall of the pseudolabels
  - The strength of CLIP's zero-shot guidance in Co-learn++ can be tuned based on the quality of the zero-shot classifier

- Failure signatures:
  - If the pseudolabels are of poor quality, the adapted model may not perform well on the target domain
  - If the pre-trained network is too biased towards the source domain, it may not provide useful information for pseudolabel generation
  - If the confidence threshold is set too high, too few samples may be pseudolabeled, leading to underfitting

- First 3 experiments:
  1. Evaluate the performance of the adapted model using different pre-trained networks (e.g., ResNet-50, ResNet-101, Swin-B) on a benchmark dataset
  2. Compare the performance of the adapted model using Co-learn versus Co-learn++ on a dataset where CLIP's zero-shot classification is expected to be helpful
  3. Investigate the impact of the confidence threshold on the quality of the pseudolabels and the performance of the adapted model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed co-learning strategy consistently outperform direct adaptation of CLIP on target domains?
- Basis in paper: [explicit] The paper states that co-learning with zero-shot CLIP (94.8%) outperformed both POUF (94.7%) and ReCLIP (86.2%) on Office-31.
- Why unresolved: The comparison only shows results for one dataset, and it's unclear if the same trend holds for other datasets and domain adaptation scenarios.
- What evidence would resolve it: A comprehensive evaluation of co-learning with CLIP versus direct CLIP adaptation across all benchmark datasets and domain adaptation scenarios would provide a clearer answer.

### Open Question 2
- Question: How does the choice of pre-trained model affect the co-learning process, and what are the optimal characteristics for a pre-trained model to be used in co-learning?
- Basis in paper: [inferred] The paper discusses the characteristics of pre-trained models that make them suitable for co-learning, such as dataset similarity, robustness against covariate shift, and provision of alternative views. However, it doesn't provide a definitive answer on the optimal choice of pre-trained model.
- Why unresolved: The paper only analyzes a few specific source-domain pairs and doesn't provide a comprehensive study on the impact of different pre-trained models on co-learning performance.
- What evidence would resolve it: A systematic evaluation of co-learning performance using various pre-trained models with different characteristics would help identify the optimal choice for co-learning.

### Open Question 3
- Question: Can the co-learning strategy be effectively applied to other domain adaptation scenarios, such as multi-source domain adaptation and domain adaptation for other tasks beyond image classification?
- Basis in paper: [explicit] The paper mentions that the co-learning strategy can work in multi-source SFDA and provides some results on Office-31. However, it doesn't explore other domain adaptation scenarios or tasks.
- Why unresolved: The paper only provides a limited evaluation of the co-learning strategy in multi-source SFDA and doesn't explore its potential in other domain adaptation scenarios or tasks.
- What evidence would resolve it: Extensive experiments applying the co-learning strategy to various domain adaptation scenarios and tasks would demonstrate its generalizability and effectiveness beyond the evaluated settings.

## Limitations
- The approach's effectiveness depends heavily on the quality of pseudolabels generated by the source model and pre-trained networks
- Performance may degrade when source and target domains are drastically different or when pre-trained networks have poor generalization to the target domain
- The computational overhead of maintaining and updating two model branches could be prohibitive for resource-constrained applications

## Confidence
- Core mechanism (overfitting causing loss of target knowledge): High
- Pseudolabel quality improvement through co-learning: Medium
- CLIP integration benefits: Medium
- Performance improvements over baselines: Medium-High

## Next Checks
1. Test the framework with different pre-trained networks (beyond CLIP) to assess generalization of the co-learning approach
2. Evaluate the method's robustness to varying degrees of domain shift between source and target domains
3. Conduct ablation studies to quantify the contribution of each component (source model, pre-trained encoder, CLIP guidance) to overall performance