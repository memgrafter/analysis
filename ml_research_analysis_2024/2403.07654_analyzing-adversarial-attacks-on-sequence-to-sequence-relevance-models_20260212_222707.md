---
ver: rpa2
title: Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models
arxiv_id: '2403.07654'
source_url: https://arxiv.org/abs/2403.07654
tags:
- attacks
- relevance
- adversarial
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel query-independent adversarial attack
  method against sequence-to-sequence relevance models like monoT5, exploiting their
  natural language prompt structure. The attack injects prompt tokens (e.g., "relevant",
  "true") into documents to manipulate ranking scores across all queries.
---

# Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models

## Quick Facts
- arXiv ID: 2403.07654
- Source URL: https://arxiv.org/abs/2403.07654
- Reference count: 0
- This paper introduces a novel query-independent adversarial attack method against sequence-to-sequence relevance models like monoT5, exploiting their natural language prompt structure.

## Executive Summary
This paper demonstrates that sequence-to-sequence relevance models like monoT5 are vulnerable to query-independent adversarial attacks through prompt token injection. By inserting tokens such as "relevant" or "true" into documents, attackers can manipulate relevance scores across all queries. The paper explores three attack methods: preemptive token injection, keyword stuffing with repeated tokens, and LLM-based document rewriting. Experiments on TREC Deep Learning datasets show significant rank improvements, with mean rank changes up to 111 positions and over 78% success rate. Remarkably, these attacks also transfer to non-prompting models like BERT-based cross-encoders, though with less impact.

## Method Summary
The paper introduces three query-independent adversarial attack methods against sequence-to-sequence relevance models. First, preemptive token injection involves adding prompt tokens at document boundaries. Second, keyword stuffing repeats prompt tokens multiple times within documents. Third, LLM-based rewriting uses models like ChatGPT to paraphrase documents while injecting tokens. The attacks are evaluated on TREC Deep Learning 2019/2020 datasets using monoT5 models (small, base, large, 3B) and compared against BM25, BERT cross-encoders, ColBERT, and TAS-B. Metrics include Mean Rank Change, Success Rate, nDCG@10, and P@10.

## Key Results
- Mean rank improvements up to 111 positions for monoT5small models
- Over 78% success rate in improving document ranks across queries
- Attacks transfer to non-prompting models like BERT cross-encoders with reduced impact
- BM25 remains unaffected by these adversarial manipulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt token injection increases relevance scores because the sequence-to-sequence model treats prompt tokens as part of the conditioning context for relevance prediction.
- Mechanism: monoT5's prompt structure concatenates Query, Document, and control tokens (e.g., "Relevant: true"). Adding these tokens inside the document alters the joint encoding, biasing the model toward higher relevance scores for that document across all queries.
- Core assumption: The model's relevance score depends on the entire token sequence, not just query-document matching; prompt tokens act as soft relevance signals.
- Evidence anchors:
  - [abstract] "opens an attack vector for malicious documents to manipulate their relevance score through prompt injection"
  - [section 3.1] "we hypothesize that including prompt tokens or their synonyms in documents increases relevance scores"
  - [corpus] weak/no direct evidence; inference from prompt-based design
- Break condition: Removing prompt tokens or training the model without explicit control tokens would nullify this attack.

### Mechanism 2
- Claim: Repetition of prompt tokens amplifies their effect by increasing token frequency in the input sequence, which boosts the likelihood of the model predicting relevance.
- Mechanism: By repeating tokens like "true" or "relevant" multiple times (e.g., 5 repetitions), the model's cross-attention and next-token prediction are skewed toward a relevance-positive outcome.
- Core assumption: Token frequency in the prompt sequence influences the next-token probability, especially in sequence-to-sequence models trained to output "true" or "false".
- Evidence anchors:
  - [section 3.3] "injection ofn ∈ { 1, 2, 3, 4, 5} repetitions of a token"
  - [section 4.2] "repetition of relevance leads to large rank improvements"
  - [corpus] weak/no direct evidence; implied by token-level training objective
- Break condition: Frequency-based normalization or regularization in the model would reduce the attack's effectiveness.

### Mechanism 3
- Claim: LLM-based rewriting injects adversarial tokens while maintaining document coherence, making detection harder and enabling scalability.
- Mechanism: Using LLMs like ChatGPT or Alpaca to paraphrase or summarize a passage with injected tokens (e.g., "relevant", "true") produces natural-looking text that still triggers relevance bias.
- Core assumption: LLMs can produce paraphrases/summaries that preserve semantics while embedding attack tokens; the target model still responds to these tokens.
- Evidence anchors:
  - [section 3.4] "re-writing attacks are much harder to detect than previous injection attacks"
  - [section 4.2] "summarization approaches prepend a passage by a summary sentence... including additional adversarial tokens"
  - [corpus] weak/no direct evidence; inferred from LLM capabilities
- Break condition: Strong adversarial detection or filtering of rewritten content would block this attack.

## Foundational Learning

- Concept: Prompt-based sequence-to-sequence models
  - Why needed here: monoT5's ranking relies on structured prompts; understanding this is key to seeing why prompt tokens can be exploited.
  - Quick check question: How does monoT5 encode a query-document pair for relevance scoring?

- Concept: Token frequency and cross-attention in transformers
  - Why needed here: The attack exploits how repeated tokens influence the model's attention and next-token prediction.
  - Quick check question: What role does token repetition play in transformer-based sequence models?

- Concept: Adversarial attack transferability
  - Why needed here: The attack transfers to non-prompting models (e.g., BERT cross-encoders), showing the broader vulnerability of neural ranking.
  - Quick check question: Why might an attack on a prompt-based model affect encoder-only models?

## Architecture Onboarding

- Component map:
  - Input: Query and document text
  - Prompt template: "Query: {q} Document: {d} Relevant: {control}"
  - Model: monoT5 (T5 encoder-decoder), BERT cross-encoder, ColBERT, TAS-B
  - Output: Relevance score (probability of "true")
  - Attack vector: Injection or rewriting of prompt/control tokens into document

- Critical path:
  1. Parse query and document
  2. Apply prompt template
  3. Encode with transformer
  4. Generate next token probability for "true"/"false"
  5. Rank documents by score

- Design tradeoffs:
  - Prompt simplicity vs. attack surface: Simpler prompts are easier to attack
  - Token frequency vs. naturalness: More repetitions boost attack but may look spammy
  - Model size vs. robustness: Larger models are less affected but still vulnerable

- Failure signatures:
  - Sudden rank improvements for documents with injected prompt tokens
  - High variance in scores across queries for a single document
  - Success in non-prompting models (transferability)

- First 3 experiments:
  1. Inject "relevant: true" at start/end of a document; measure rank change on monoT5.
  2. Repeat "true" 3–5 times in a document; compare rank improvement to single injection.
  3. Use ChatGPT to rewrite a document with "relevant" tokens; test rank change on monoT5 and BERT cross-encoder.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the query-independent adversarial attacks also transfer to more complex relevance models like ColBERTv2 or De-LLaMA?
- Basis in paper: [explicit] The paper demonstrates that attacks transfer to BERT-based cross-encoders and bi-encoders, but the impact on newer models like ColBERTv2 is not explored.
- Why unresolved: The experiments focus on older model variants (e.g., ColBERT, TAS-B), leaving the transferability to newer architectures untested.
- What evidence would resolve it: Testing the attacks on models like ColBERTv2 or De-LLaMA and measuring rank changes or transfer success rates.

### Open Question 2
- Question: How do the adversarial attacks affect retrieval effectiveness in real-world search engines with multiple ranking stages?
- Basis in paper: [inferred] The paper evaluates attacks on single-stage re-ranking, but real-world systems often use multi-stage pipelines.
- Why unresolved: The study does not simulate the impact of attacks in a full pipeline with initial retrieval and subsequent re-ranking.
- What evidence would resolve it: Simulating attacks in a multi-stage pipeline and measuring nDCG or Precision@10 at each stage.

### Open Question 3
- Question: Can the adversarial attacks be mitigated by simple preprocessing techniques like tokenization or keyword filtering?
- Basis in paper: [explicit] The paper notes that tokenization allows hiding attack tokens, but does not test mitigation strategies.
- Why unresolved: The paper focuses on attack efficacy but does not explore defenses or preprocessing methods to counter the attacks.
- What evidence would resolve it: Testing preprocessing techniques like tokenization, keyword filtering, or document length penalties and measuring their impact on attack success rates.

## Limitations

- The attack methodology relies heavily on monoT5's prompt-based architecture, with transferability to other models demonstrated but not fully explained mechanistically.
- Evaluation is limited to two TREC Deep Learning datasets without testing on diverse retrieval scenarios or different domains.
- Claims about LLM-based rewriting being "much harder to detect" lack empirical validation through detection experiments or false positive analysis.

## Confidence

**High Confidence**: The core finding that prompt token injection can manipulate monoT5 rankings is well-supported by experimental results showing consistent rank improvements across multiple document-query pairs and model sizes.

**Medium Confidence**: The transferability of attacks to non-prompting models like BERT cross-encoders is demonstrated but not fully explained mechanistically.

**Low Confidence**: The claims about LLM-based rewriting being "much harder to detect" than other attack methods lack empirical validation.

## Next Checks

1. **Cross-model transferability analysis**: Systematically test the attack's effectiveness across a broader range of neural ranking models (both prompting and non-prompting) to better understand transferability boundaries and mechanisms.

2. **Detection feasibility study**: Implement and evaluate simple detection methods for the different attack types (keyword stuffing, LLM rewriting) to quantify the actual detection difficulty and false positive rates claimed in the paper.

3. **Prompt space exploration**: Experiment with a wider variety of prompt tokens and injection strategies beyond the four tested tokens to determine if the observed effects are robust across different prompt manipulations or specific to the tested vocabulary.