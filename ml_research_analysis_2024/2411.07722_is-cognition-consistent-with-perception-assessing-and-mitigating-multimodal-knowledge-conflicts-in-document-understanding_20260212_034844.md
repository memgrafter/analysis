---
ver: rpa2
title: Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal
  Knowledge Conflicts in Document Understanding
arxiv_id: '2411.07722'
source_url: https://arxiv.org/abs/2411.07722
tags:
- knowledge
- conflicts
- document
- text
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies and addresses Cognition and Perception (C&P)
  knowledge conflicts in multimodal large language models (MLLMs), where responses
  to cognitive tasks (like VQA) and perceptual tasks (like OCR) are inconsistent despite
  being based on the same visual input. This inconsistency stems from heterogeneous
  annotation sources during training, leading to discrepancies between perception
  and cognition.
---

# Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding

## Quick Facts
- **arXiv ID:** 2411.07722
- **Source URL:** https://arxiv.org/abs/2411.07722
- **Reference count:** 40
- **Primary result:** Reduces cognition-perception conflicts in MLLMs, improving consistency by 3.36-3.72% on document understanding tasks

## Executive Summary
This paper identifies a critical issue in multimodal large language models (MLLMs) where responses to cognitive tasks (like VQA) and perceptual tasks (like OCR) are inconsistent despite being based on the same visual input. The authors attribute this Cognition and Perception (C&P) knowledge conflict to heterogeneous annotation sources during training, where OCR and VQA tasks use different types of annotation noise. To address this, they propose Multimodal Knowledge Consistency Fine-tuning, which introduces C&P Link Tokens to connect cognitive and perceptual outputs and a C&P Connector to guide cross-verification of answers. Experiments across five document understanding benchmarks demonstrate improved C&P consistency and enhanced performance on both cognitive and perceptual tasks.

## Method Summary
The proposed method addresses C&P knowledge conflicts through two main components: C&P Link Tokens and C&P Connector. C&P Link Tokens are special markers (`<CPLINK>` and `</CPLINK>`) that enclose OCR-derived text, creating a strong signal for the model to treat this text as a distinct knowledge type. The C&P Connector guides the model to cross-verify cognitive answers using perceptual knowledge through positive and negative training samples. Positive samples force verification of cognitive answers against OCR-derived text, while negative samples teach the model to detect and correct inconsistencies. The approach is evaluated through fine-tuning three open-source MLLMs (InternVL2-2b, Qwen2.5-VL-7b, and LLaVA-Next) on five document understanding datasets.

## Key Results
- InternVL2-2b achieves 83.39% C&P consistency after fine-tuning, up from 80.59% baseline
- C&P consistency improvements of 3.36-3.72% across all tested MLLMs
- Enhanced performance on both cognitive tasks (VQA) and perceptual tasks (OCR)
- Reduction in specific conflict patterns (P1, P2, P3) that previously caused inconsistent responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C&P Link Tokens connect cognitive and perceptual knowledge by enclosing OCR-derived text in special markers
- Mechanism: The model learns to associate text inside `<CPLINK>` and `</CPLINK>` with OCR outputs, creating explicit cross-modal linking
- Core assumption: The model can generalize from these special tokens to understand the relationship between what it "sees" and what it "understands"
- Evidence anchors: [abstract] "a special token called C&P Link Token is introduced as a prompt prefix and suffix to connect cognitive and perceptual outputs"
- Break condition: If the model fails to generalize special token usage beyond fine-tuning data, or if tokens interfere with other parsing tasks

### Mechanism 2
- Claim: The C&P Connector guides the model to cross-verify cognitive answers using perceptual knowledge through positive and negative samples
- Mechanism: Positive samples force verification of cognitive answers (yC) against OCR-derived text (yP), while negative samples teach detection and correction of inconsistencies
- Core assumption: The model can learn to use OCR-derived text as a verification source for cognitive outputs
- Evidence anchors: [abstract] "a C&P Connector to guide the model in cross-verifying answers"
- Break condition: If the model over-relies on OCR text and loses ability to generate novel cognitive responses, or if negative samples create confusion

### Mechanism 3
- Claim: Heterogeneous annotation noise between OCR and VQA tasks creates C&P knowledge conflicts that degrade model consistency
- Mechanism: Different noise profiles in OCR (external engines) vs VQA (human/LLM) annotations cause the model to learn inconsistent representations of the same visual content
- Core assumption: Model's performance on both tasks is negatively impacted by inconsistent training signals
- Evidence anchors: [abstract] "due to different types of annotation noise in training, current MLLMs often face conflicts between perception and cognition"
- Break condition: If the model can learn to separate noise sources and maintain consistent representations despite different annotations

## Foundational Learning

- Concept: Multimodal knowledge conflicts
  - Why needed here: Understanding that conflicts between perception and cognition are a distinct type of error in MLLMs, different from hallucination or single-modality errors
  - Quick check question: Can you explain the difference between C&P knowledge conflicts and traditional hallucination?

- Concept: Optical Character Recognition (OCR)
  - Why needed here: OCR is the primary perceptual task in document understanding, and understanding its limitations and noise characteristics is crucial for addressing C&P conflicts
  - Quick check question: What are the main sources of noise in commercial OCR systems?

- Concept: Question Answering (VQA)
  - Why needed here: VQA is the primary cognitive task, and understanding how it differs from OCR in terms of annotation sources and expected outputs is essential for understanding C&P conflicts
  - Quick check question: How do human-annotated VQA datasets differ from OCR annotations in terms of noise and bias?

## Architecture Onboarding

- Component map: Vision encoder → Language model (with special C&P tokens) → Cross-verification module (C&P Connector)
- Critical path: Image input → OCR extraction → C&P Link Token insertion → C&P Connector processing → Final answer generation
- Design tradeoffs: Adding special tokens increases model complexity but enables explicit cross-modal linking; C&P Connector adds training overhead but improves consistency
- Failure signatures: Inconsistent answers between OCR and VQA tasks, model confusion when special tokens appear in unexpected contexts, over-reliance on OCR text for cognitive tasks
- First 3 experiments:
  1. Test C&P Link Token insertion on a small dataset to verify it doesn't break existing functionality
  2. Implement and test the C&P Connector with synthetic positive/negative samples
  3. Run ablation study to measure individual contributions of C&P Link Tokens and C&P Connector

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the C&P consistency improvement translate to better performance on more complex reasoning tasks beyond VQA and OCR, such as multi-step inference or temporal reasoning?
- Basis in paper: [inferred] The paper demonstrates improvements in C&P consistency and basic cognitive/perceptual tasks but does not explore complex reasoning tasks
- Why unresolved: The experiments focus on document understanding tasks like VQA and OCR. It remains unclear whether the proposed method enhances the model's ability to handle more intricate cognitive processes that require deeper integration of perception and cognition
- What evidence would resolve it: Testing the model on benchmarks involving multi-step reasoning, temporal reasoning, or causal inference tasks would clarify whether the improvements generalize to more complex cognitive abilities

### Open Question 2
- Question: How does the proposed method scale to larger, more diverse datasets, particularly those with varied document layouts and multimodal content beyond text (e.g., images, tables)?
- Basis in paper: [explicit] The paper evaluates the method on five document understanding datasets but does not explore scalability to larger or more diverse datasets
- Why unresolved: While the method shows effectiveness on the tested datasets, its performance on larger, more heterogeneous datasets with diverse multimodal content remains unexplored. This is critical for real-world applications
- What evidence would resolve it: Evaluating the method on larger, more diverse datasets (e.g., multi-domain document collections or datasets with mixed content types) would demonstrate its scalability and robustness

### Open Question 3
- Question: What is the impact of the C&P Link Tokens and Connector on the model's training efficiency and inference speed, particularly for real-time applications?
- Basis in paper: [inferred] The paper introduces the C&P Link Tokens and Connector but does not discuss their computational overhead or impact on training/inference efficiency
- Why unresolved: The method introduces additional components (tokens and connectors) that may affect computational efficiency. However, the paper does not address whether these components introduce significant overhead, which is crucial for practical deployment
- What evidence would resolve it: Benchmarking the method's training time, memory usage, and inference speed compared to baseline models would clarify its efficiency and suitability for real-time applications

## Limitations
- Lack of ablation studies to isolate individual contributions of C&P Link Tokens versus C&P Connector mechanism
- Evaluation relies on GPT-4o for sample construction, introducing potential bias in how conflicts are defined and measured
- Modest improvement percentages (3.36-3.72%) raise questions about whether added complexity justifies the gains

## Confidence

**High Confidence:** The identification of C&P knowledge conflicts as a distinct problem in MLLMs, and the observation that heterogeneous annotation sources contribute to these conflicts. The experimental setup showing baseline inconsistency is well-established.

**Medium Confidence:** The effectiveness of the proposed fine-tuning approach in reducing C&P conflicts and improving overall performance. While improvements are demonstrated, the lack of ablation studies and potential GPT-4o bias in evaluation limit stronger claims.

**Low Confidence:** The generalizability of the approach beyond document understanding to other multimodal domains, and whether the special tokens might interfere with other model capabilities.

## Next Checks

1. **Ablation Study Required:** Run experiments isolating C&P Link Tokens and C&P Connector to measure their individual contributions to the 3.36-3.72% improvement, determining which component drives most of the gains.

2. **Cross-Domain Generalization Test:** Apply the fine-tuning approach to non-document MLLM tasks (e.g., general VQA or image captioning) to verify if C&P conflict reduction generalizes beyond text-heavy domains.

3. **Long-Term Stability Analysis:** Evaluate model performance over extended periods and diverse test sets to check for degradation, over-reliance on OCR, or emergence of new failure patterns not captured in initial benchmarks.