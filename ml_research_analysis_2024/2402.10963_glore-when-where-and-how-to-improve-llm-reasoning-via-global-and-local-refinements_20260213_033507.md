---
ver: rpa2
title: 'GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local
  Refinements'
arxiv_id: '2402.10963'
source_url: https://arxiv.org/abs/2402.10963
tags:
- refinement
- sorm
- local
- data
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a refinement framework for improving LLM reasoning
  on math tasks. It decomposes refinement into when to refine (using an Outcome-based
  Reward Model), where to refine (using a Stepwise ORM trained via synthetic data),
  and how to refine (using global and local refinement models trained on paired correct/incorrect
  solutions).
---

# GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements

## Quick Facts
- arXiv ID: 2402.10963
- Source URL: https://arxiv.org/abs/2402.10963
- Reference count: 25
- Primary result: Improves Llama-2 13B GSM8K accuracy from 53% to 65% using global+local refinement with ORM reranking

## Executive Summary
This paper presents a three-stage refinement framework for improving LLM reasoning on math tasks. It introduces a Stepwise Outcome-based Reward Model (SORM) that uses rejection sampling to better identify erroneous intermediate steps than standard ORMs. The framework combines global refinement (rewriting entire solutions) and local refinement (fixing first error) models, with an ORM reranking final candidates. Applied to Llama-2 13B, this approach improves GSM8K accuracy from 53% to 65% when greedily sampled.

## Method Summary
The method uses a three-stage pipeline: (1) RL fine-tuning with Expert Iteration to create a student policy π, (2) training ORM and SORM using synthetic data where SORM uses rejection sampling to approximate the optimal value function, and (3) training global refinement (pairs incorrect/correct drafts) and local refinement (pairs incorrect drafts with SORM-verified corrections) models, combined with ORM reranking.

## Key Results
- GSM8K accuracy improves from 53% to 65% on Llama-2 13B with greedy sampling
- SORM better identifies erroneous intermediate steps than standard ORMs
- Global and local refinements are complementary, solving partially disjoint problem subsets
- ORM reranking over draft + global + local refinements outperforms sampling K times from student model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SORMs approximate the optimal value function V* by rejection sampling the student policy π, enabling more accurate detection of erroneous intermediate steps than standard ORMs.
- **Mechanism:** Rejection sampling K rollouts from prefix Pi generates verifying traces; if any trace yields a correct final answer, Si is labeled as valid. This approximates V*(Pi), the probability of success from Pi under the optimal policy, rather than the pessimistic Vπ(Pi) used by ORMs.
- **Core assumption:** The student policy π is close enough to optimal that rejection sampling from π will find correct completions for valid prefixes with high probability.
- **Evidence anchors:**
  - [abstract]: "SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs)."
  - [section 4.1]: "We generate SORM training data by sampling an approximation of the optimal policy π* at each step Si in a model generated solution and check correctness of the final answer."
- **Break condition:** If the student model π is too weak (e.g., accuracy too low), rejection sampling will rarely find correct completions, causing SORM to label valid prefixes as errors (high false negatives).

### Mechanism 2
- **Claim:** Global and local refinement models are complementary because they solve partially disjoint subsets of problems the student initially fails on.
- **Mechanism:** Global refinement rewrites the entire solution path from scratch, potentially avoiding problematic operations (e.g., division). Local refinement fixes only the first error while preserving valid prior reasoning. This leads to orthogonal error patterns and coverage.
- **Core assumption:** The error distribution across drafts is such that some errors are better handled by a complete rewrite (global) while others are localized and fixable without changing prior steps (local).
- **Evidence anchors:**
  - [abstract]: "We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually."
  - [section 5.2]: "This shows local refinement is able to solve a large set of problems global refinement cannot, and vice versa."
- **Break condition:** If most errors occur in the same step or early in the solution, both global and local refinement may fix the same subset, reducing complementarity.

### Mechanism 3
- **Claim:** Using the ORM as a reranker over draft + global + local refinements yields better accuracy than sampling K times from the student model alone.
- **Mechanism:** The ORM, trained to predict final answer correctness, scores each candidate solution. The highest-scoring candidate is selected, effectively choosing the best of 3 independent refinement attempts plus the draft.
- **Core assumption:** The ORM generalizes well enough from its training data to distinguish correct from incorrect refinements, even when the refinements come from models trained on different data.
- **Evidence anchors:**
  - [abstract]: "Using this strategy we can improve the accuracy of an already strong RL fine-tuned Llama-2 13B mode from 53% to 65% when greedily sampled."
  - [section 5.2]: "When comparing with the Bo3 baseline we still see significant improvements of around 8% on GSM8K."
- **Break condition:** If the ORM overfits to its training student model, it may misrank refinements generated by a different model, reducing reranking effectiveness.

## Foundational Learning

- **Concept:** Reinforcement Learning via Expert Iteration (EI)
  - **Why needed here:** EI generates high-quality synthetic training data by sampling the student model K times per prompt and filtering for correct answers, creating a curriculum of increasingly difficult problems.
  - **Quick check question:** What is the purpose of combining filtered rollouts with supervised fine-tuning data in each EI round?
- **Concept:** Value Functions and Optimal Policy Approximation
  - **Why needed here:** Understanding Vπ (student's value function) vs. V* (optimal value function) explains why ORMs are overly pessimistic and why SORMs improve step-level accuracy.
  - **Quick check question:** How does rejection sampling approximate V* from a suboptimal policy π?
- **Concept:** Process-Based vs. Outcome-Based Reward Models
  - **Why needed here:** Differentiates PRMs (dense step-level correctness labels from humans) from ORMs (sparse final answer correctness), motivating the synthetic SORM approach.
  - **Quick check question:** Why is collecting human-annotated step labels for PRMs expensive compared to synthetic SORM data?

## Architecture Onboarding

- **Component map:** Base LLM → EI fine-tuning → Student policy π → ORM/SORM training → Global refinement model → Local refinement model → Reranker (ORM) → Final answer
- **Critical path:** π → ORM/SORM → (Global + Local refinement) → Reranker → Final answer
- **Design tradeoffs:**
  - Using synthetic data avoids human annotation cost but risks label noise; mitigated by SORM self-supervision filtering
  - Rejection sampling increases SORM accuracy but requires K extra forward passes per step, increasing compute cost
  - Global refinement can restart from scratch (more flexible) but may waste effort on valid prior reasoning; local refinement is more targeted but relies on accurate error localization
- **Failure signatures:**
  - Low SORM accuracy on test data → rejection sampling ineffective (student too weak)
  - Refinements copy draft answers → model collapses to draft-returning policy
  - Reranker accuracy near random → ORM overfits to training student
- **First 3 experiments:**
  1. Compare ORM vs. SORM step-level accuracy on a held-out validation set to confirm SORM's advantage
  2. Evaluate global vs. local refinement accuracy on incorrect drafts to confirm complementarity
  3. Test reranking draft + global + local refinements against Bo3 baseline to confirm improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the SORM's performance scale with model size and task difficulty, and what are the limits of its ability to approximate the optimal value function V*?
- **Basis in paper:** The paper states that the ORM's ability to generalize to intermediate steps depends on the size of the base model and the difficulty of the task, with larger models and easier tasks resulting in better approximations of V*.
- **Why unresolved:** The paper only tests SORM on Llama-2 7B and 13B models on GSM8K and SVAMP. It's unclear how the SORM would perform on other model sizes or more complex reasoning tasks.
- **What evidence would resolve it:** Experiments comparing SORM performance on a wider range of model sizes and reasoning tasks, including those with more complex reasoning chains and larger search spaces.

### Open Question 2
- **Question:** Can the SORM's approximation of the optimal value function V* be further improved by incorporating additional information beyond the student model's rollouts, such as external knowledge or tool usage?
- **Basis in paper:** The paper mentions that the SORM's approximation of V* is limited by the student model's ability to explore the solution space, and that tool usage can help models solve problems they otherwise couldn't.
- **Why unresolved:** The paper doesn't explore how additional information sources might improve the SORM's approximation of V*.
- **What evidence would resolve it:** Experiments comparing SORM performance with and without additional information sources, such as external knowledge bases or tool usage.

### Open Question 3
- **Question:** How can the SORM's step-level verification process be made more robust to false positives and false negatives, and what are the trade-offs between precision and recall in this context?
- **Basis in paper:** The paper mentions that the SORM's data generation process can suffer from both false positives and false negatives, and that these errors can impact downstream performance.
- **Why unresolved:** The paper only briefly mentions potential solutions, such as self-supervised filtering, but doesn't thoroughly explore the trade-offs between precision and recall in the context of step-level verification.
- **What evidence would resolve it:** Experiments comparing the performance of different filtering strategies and their impact on downstream refinement accuracy, as well as a thorough analysis of the precision-recall trade-offs in step-level verification.

## Limitations

- The approach relies heavily on synthetic data generation through rejection sampling, which may not scale well to domains where correct answers are rare or generation is computationally expensive
- The effectiveness of SORMs depends critically on the assumption that the student policy π is sufficiently close to optimal - this may break down for weaker base models or more complex reasoning tasks
- The complementarity of global and local refinement assumes diverse error patterns, which may not hold if most errors cluster in similar positions across problems

## Confidence

**High Confidence:** The empirical results showing GSM8K accuracy improvement from 53% to 65% are directly reported in the paper with clear methodology. The mechanism by which rejection sampling approximates the optimal value function V* is theoretically sound and well-explained.

**Medium Confidence:** The claim that global and local refinements solve partially disjoint problem subsets is supported by ablation studies, but the analysis of why specific errors are handled differently by each approach is limited. The assumption that ORM generalization from one model transfers to refinements from another model is plausible but not thoroughly validated.

**Low Confidence:** The paper does not provide systematic analysis of failure cases where the approach breaks down, particularly for student models that are significantly weaker than the tested Llama-2 13B. The scalability analysis for larger models or different reasoning domains is absent.

## Next Checks

1. **SORM Robustness Test:** Evaluate SORM performance across a range of student model accuracies (from 30% to 80%) to determine the minimum threshold where rejection sampling remains effective and identify the point where false positive/negative rates become problematic.

2. **Error Pattern Analysis:** Conduct detailed error analysis to verify the assumption that global and local refinements are complementary by categorizing error types and tracking which refinement approach fixes which categories across a diverse problem set.

3. **Cross-Model ORM Transfer:** Test whether the ORM trained on the student model can effectively rank refinements generated by different base models (e.g., different model families or sizes) to validate the assumption of ORM generalization across refinement models.