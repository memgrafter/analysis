---
ver: rpa2
title: 'Programming Every Example: Lifting Pre-training Data Quality Like Experts
  at Scale'
arxiv_id: '2409.17115'
source_url: https://arxiv.org/abs/2409.17115
tags:
- data
- prox
- pre-training
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PROX, a framework that treats data refinement
  as a programming task to improve the quality of pre-training data for large language
  models. PROX uses small language models to generate and execute fine-grained operations
  like string normalization and line removal, enabling autonomous data refinement
  at scale.
---

# Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale

## Quick Facts
- arXiv ID: 2409.17115
- Source URL: https://arxiv.org/abs/2409.17115
- Reference count: 40
- One-line primary result: PROX improves pre-training data quality using small models to autonomously generate fine-grained refinement programs, achieving >2% downstream performance gains with up to 20x less training compute.

## Executive Summary
This paper introduces PROX, a framework that treats data refinement as a programming task to improve the quality of pre-training data for large language models. PROX uses small language models to generate and execute fine-grained operations like string normalization and line removal, enabling autonomous data refinement at scale. Experiments show that models pre-trained on PROX-curated data outperform those trained on original data by more than 2% across various benchmarks. PROX is effective across different model sizes and corpora, including general and domain-specific data, and achieves comparable performance with up to 20x less computing compared to models trained on 200B tokens.

## Method Summary
PROX adapts small language models to generate and execute fine-grained programs for data refinement. The framework fine-tunes base models on seed data to create refining models, which then generate document-level and chunk-level programs for each document in pre-training corpora. These programs are executed to produce refined datasets that improve model performance while reducing training compute requirements.

## Key Results
- Models pre-trained on PROX-refined data outperform original data by >2% across downstream benchmarks
- PROX achieves comparable performance with up to 20x less training FLOPs compared to models trained on 200B tokens
- Effectiveness spans different model sizes (0.3B to 1.7B parameters) and various corpora types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small language models (as few as 0.3B parameters) can autonomously refine pre-training data with quality comparable to human experts.
- Mechanism: The model is adapted to a data-refining task via supervised fine-tuning on seed data, then generates fine-grained programs (e.g., string normalization, line removal) to clean documents.
- Core assumption: The quality of seed annotations is high enough that fine-tuning produces a robust refining model.
- Evidence anchors:
  - [abstract] "even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts."
  - [section 2.3] "we employ strong LLMs to annotate these operations via zero-shot and few-shot prompting, and then adapt our base model to these tasks by supervised fine-tuning (SFT)."
- Break condition: Seed data annotations are inconsistent or contain systematic bias, causing the refined data to degrade model performance.

### Mechanism 2
- Claim: Fine-grained, example-specific data refinement outperforms heuristic rule-based filtering.
- Mechanism: PROX generates and executes document-level and chunk-level programs that adapt to each document's unique structure, rather than applying global heuristics.
- Core assumption: The variability in pre-training corpora necessitates tailored cleaning rules for each document.
- Evidence anchors:
  - [abstract] "these rules lack the flexibility to address the unique characteristics of individual example effectively."
  - [section 2.2] "These operations include deleting specific lines remove_lines() and replacing strings normalize(), providing flexibility in modifying content rather than simply dropping the whole document."
- Break condition: The program generation fails on complex documents, leading to incomplete or excessive cleaning.

### Mechanism 3
- Claim: Applying PROX to pre-training data yields performance gains with reduced total compute.
- Mechanism: Refined data improves model convergence, allowing comparable downstream performance with fewer training steps (up to 20× less FLOPs).
- Core assumption: Higher-quality training data reduces the need for extensive parameter updates.
- Evidence anchors:
  - [abstract] "PROX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training."
  - [section 3.2] "models pre-trained on PROX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks."
- Break condition: The overhead of program generation and execution outweighs the efficiency gains from improved data quality.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) for task adaptation.
  - Why needed here: The base model must learn to generate data refinement programs, not just predict text.
  - Quick check question: What distinguishes SFT from continued pre-training in this context?

- Concept: Program synthesis and execution.
  - Why needed here: PROX uses models to generate executable code that performs data cleaning.
  - Quick check question: How does the executor validate program syntax before applying it to documents?

- Concept: Token retention vs. quality tradeoff.
  - Why needed here: Aggressive filtering improves quality but may reduce the effective training corpus size.
  - Quick check question: What metric balances retained token count with downstream performance?

## Architecture Onboarding

- Component map:
  Seed annotation pipeline (LLM prompts → program labels) → Model adaptation module (SFT training on doc-program pairs) → Document processing pipeline (document/chunk splitting → program generation → execution) → Pre-training integration (refined corpus → standard language model training)

- Critical path: Document → program generation → program execution → refined corpus → pre-training → evaluation.

- Design tradeoffs:
  - Small refining models reduce compute but may miss subtle noise patterns.
  - Fine-grained chunk-level cleaning improves precision but increases inference overhead.
  - Token retention vs. data quality: more aggressive filtering yields higher quality but smaller corpora.

- Failure signatures:
  - Low F1 scores in refining tasks indicate poor model adaptation.
  - Sudden drops in downstream accuracy suggest over-aggressive cleaning.
  - High failure ratios in program execution imply bugs in executor or malformed program outputs.

- First 3 experiments:
  1. Fine-tune a small model on seed doc-program pairs; evaluate F1 on held-out validation set.
  2. Apply refining model to a small subset of pre-training data; compare token retention and cleaning accuracy.
  3. Train a baseline model on original data and a second model on PROX-refined data; measure downstream benchmark gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of the refining model for different scales of pre-training corpora?
- Basis in paper: [explicit] The paper states that refining models of all sizes help improve performance over raw data, with a consistent absolute gap of 2% over all base model sizes, but moderately larger models suggest a favorable balance between data quality and quantity.
- Why unresolved: While the paper demonstrates effectiveness across different model sizes, it does not provide a definitive answer on the optimal size of the refining model for various scales of pre-training corpora.
- What evidence would resolve it: Empirical studies comparing the performance of different refining model sizes on various scales of pre-training corpora, considering both data quality and computational efficiency.

### Open Question 2
- Question: How does the performance of PROX compare to other advanced data refinement techniques, such as reinforcement learning or active learning, in terms of both effectiveness and efficiency?
- Basis in paper: [inferred] The paper focuses on comparing PROX with heuristic and model-based data selection methods, but does not explore more advanced techniques like reinforcement learning or active learning for data refinement.
- Why unresolved: The paper does not provide a comprehensive comparison of PROX with other state-of-the-art data refinement techniques, leaving the relative effectiveness and efficiency of PROX unclear.
- What evidence would resolve it: Empirical studies comparing the performance of PROX with other advanced data refinement techniques, such as reinforcement learning or active learning, on various pre-training corpora and downstream tasks.

### Open Question 3
- Question: What are the long-term effects of using PROX-refined data on the generalization and robustness of pre-trained language models?
- Basis in paper: [inferred] The paper focuses on the immediate performance improvements of models pre-trained on PROX-refined data, but does not explore the long-term effects on generalization and robustness.
- Why unresolved: The paper does not provide insights into how PROX-refined data affects the long-term performance of pre-trained language models, particularly in terms of generalization and robustness to out-of-distribution data or adversarial attacks.
- What evidence would resolve it: Longitudinal studies tracking the performance of models pre-trained on PROX-refined data over time, and evaluating their generalization and robustness on various downstream tasks and challenging datasets.

## Limitations
- Seed Data Quality Dependency: The entire framework hinges on the quality of initial LLM-annotated seed data, with no systematic evaluation of seed annotation reliability provided.
- Program Generation Failure Handling: The paper mentions that 5.68% of documents fail program generation but doesn't discuss how these failures impact downstream model performance or training stability.
- Generalizability Gap: While PROX shows effectiveness across different model sizes and some corpora, the claim of broad applicability lacks extensive cross-domain validation.

## Confidence
- High Confidence: The core mechanism of using small models for autonomous data refinement is technically sound and supported by multiple independent studies. The empirical claim of 2% downstream performance gains is well-grounded in the results section.
- Medium Confidence: The scalability claim (up to 20× FLOPs reduction) depends on specific implementation details not fully disclosed, such as exact program execution overhead and parallel processing efficiency.
- Low Confidence: The generalizability across diverse corpora is assumed but not extensively validated. The paper primarily tests on RedPajama-V2 and mentions other datasets without detailed comparative analysis.

## Next Checks
1. Conduct a human evaluation of a random sample of LLM-generated seed annotations to measure inter-annotator agreement and identify systematic biases that could propagate through the refining pipeline.
2. Systematically analyze the 5.68% of documents that fail program generation to determine whether these failures cluster around specific document characteristics and assess their impact on overall corpus quality.
3. Apply PROX to a completely different domain (e.g., medical or legal texts) and compare refinement quality and downstream performance against heuristic-based cleaning methods to validate the claimed flexibility advantage.