---
ver: rpa2
title: Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary
  Semantic Segmentation
arxiv_id: '2402.02367'
source_url: https://arxiv.org/abs/2402.02367
tags:
- medsass
- self-supervised
- segmentation
- medical
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedSASS is a self-supervised framework for medical image segmentation
  that improves over CNN-based methods by 3.83% and matches ViT performance. Unlike
  existing approaches that only train encoders, MedSASS trains both encoder and decoder
  end-to-end, yielding further gains of 14.4% for CNNs and 6% for ViTs.
---

# Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation

## Quick Facts
- arXiv ID: 2402.02367
- Source URL: https://arxiv.org/abs/2402.02367
- Authors: Pranav Singh; Jacopo Cirrone
- Reference count: 40
- Primary result: MedSASS improves CNN-based self-supervised segmentation by 3.83% and matches ViT performance

## Executive Summary
MedSASS introduces a self-supervised framework for medical image segmentation that leverages intrinsic image properties without requiring labeled data. The method uses Otsu thresholding to generate binary masks from input images, then trains a U-Net architecture end-to-end on these self-generated labels. Unlike existing self-supervised approaches that only train encoders, MedSASS trains both encoder and decoder, yielding significant performance gains of 14.4% for CNNs and 6% for ViTs over encoder-only training. The framework was validated across four diverse medical datasets and demonstrates strong performance compared to state-of-the-art self-supervised baselines.

## Method Summary
MedSASS generates self-supervision by applying Otsu thresholding to grayscale medical images, creating binary masks that separate foreground from background. These masks serve as pseudo-labels for training a U-Net architecture, with the key innovation being end-to-end training of both encoder and decoder rather than encoder-only pre-training. The framework uses focal-Tversky loss to handle class imbalance during training. The method was evaluated on four medical datasets (Dermatomyositis, TissueNet, ISIC-2017, X-Ray) using Intersection over Union (IoU) as the primary metric, with comparisons to existing self-supervised methods including DINO, BYOL, and SimSiam.

## Key Results
- MedSASS outperforms existing CNN-based self-supervised methods by 3.83% in segmentation performance
- The framework matches the performance of ViT-based methods while using CNN backbones
- End-to-end training of both encoder and decoder yields significant improvements of 14.4% for CNNs and 6% for ViTs
- MedSASS demonstrates strong generalization across four diverse medical imaging datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MedSASS improves segmentation by training both encoder and decoder end-to-end using self-generated supervision from Otsu thresholding.
- **Mechanism**: The framework generates binary masks from input images using Otsu's method, then trains a U-Net to predict these masks. Training both encoder and decoder end-to-end allows the decoder to learn task-specific priors, unlike encoder-only self-supervised methods.
- **Core assumption**: Otsu's thresholding produces sufficiently accurate supervision signals for the network to learn useful segmentation features.
- **Evidence anchors**:
  - [abstract]: "MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements of 14.4% for CNNs and 6% for ViT-based architectures."
  - [section]: "MedSASS advances this by pre-training both the encoder and decoder, unlocking greater performance potential."
- **Break condition**: If Otsu-generated masks are too inaccurate, the network may learn incorrect features, degrading segmentation performance.

### Mechanism 2
- **Claim**: Otsu's thresholding is effective for medical images due to their high contrast and consistent foreground-background relationships.
- **Mechanism**: Medical images typically have clear intensity differences between foreground (e.g., lesions, cells) and background, making Otsu's bimodal histogram assumption valid. This enables reliable self-supervision without labeled data.
- **Core assumption**: Medical images exhibit consistent contrast and foreground-background separation suitable for Otsu's method.
- **Evidence anchors**:
  - [section]: "In medical imaging, high contrast is ensured to best visualize disease features. Otsu's approach effectively separates high-contrast areas, making it ideal for medical images."
  - [section]: "Medical images typically have a more straightforward and consistent background-foreground relationship."
- **Break condition**: If applied to natural images with complex textures and multiple objects, Otsu's method fails due to lack of bimodal histograms.

### Mechanism 3
- **Claim**: Focal-Tversky loss prevents class imbalance issues during training, ensuring balanced learning of foreground and background.
- **Mechanism**: The loss function combines focal loss (addressing class imbalance) and Tversky loss (asymmetric treatment of false positives and negatives), preventing overfitting to the majority class and improving segmentation accuracy.
- **Core assumption**: The dataset contains imbalanced foreground/background pixel distributions, requiring specialized loss functions.
- **Evidence anchors**:
  - [section]: "It differentially weights the majority and minority classes, thereby preventing overfitting of the majority class through targeted penalization."
  - [section]: "tversky loss... applying variable weights to the FP and FN classes."
- **Break condition**: If the dataset is balanced or the model architecture inherently handles imbalance, the specialized loss may offer minimal benefit.

## Foundational Learning

- **Concept**: Self-supervised learning in medical image segmentation
  - Why needed here: Medical datasets are scarce and labeling is expensive; self-supervised methods leverage unlabeled data to learn useful priors.
  - Quick check question: What is the key difference between self-supervised and supervised learning in terms of data requirements?

- **Concept**: Binary semantic segmentation
  - Why needed here: The task involves separating regions of interest (foreground) from background in medical images, which is critical for clinical diagnosis.
  - Quick check question: How does binary segmentation differ from multi-class segmentation in terms of output masks?

- **Concept**: U-Net architecture
  - Why needed here: U-Net is widely used in medical image segmentation due to its encoder-decoder structure with skip connections, which captures both local and global features.
  - Quick check question: What is the purpose of skip connections in U-Net, and how do they benefit segmentation tasks?

## Architecture Onboarding

- **Component map**: Grayscale image (224x224) -> Otsu thresholding -> U-Net encoder (ResNet-50/ViT-small) -> U-Net decoder with channel attention -> Predicted binary mask

- **Critical path**: Image → Grayscale conversion → Otsu thresholding → U-Net prediction → Loss computation → Parameter update

- **Design tradeoffs**:
  - Using Otsu's method enables self-supervision but may produce inaccurate masks for complex images
  - End-to-end training of encoder-decoder improves performance but increases computational cost
  - Focal-Tversky loss handles class imbalance but adds complexity compared to standard cross-entropy

- **Failure signatures**:
  - If Otsu masks are inaccurate, the model may learn incorrect features (e.g., overfitting to background)
  - If the loss function is not balanced, the model may ignore minority classes (e.g., small lesions)
  - If the architecture is too shallow, it may fail to capture fine-grained details in segmentation

- **First 3 experiments**:
  1. Train MedSASS with only the encoder (encoder-only) on a small medical dataset and evaluate IoU to confirm baseline performance
  2. Switch to end-to-end training (encoder + decoder) and compare IoU to assess the impact of training both components
  3. Replace Otsu thresholding with adaptive thresholding and evaluate whether performance improves or degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MedSASS perform on multi-class segmentation tasks in medical imaging, beyond binary segmentation?
- Basis in paper: [inferred] The paper mentions that Otsu's method can perform multi-class segmentation but becomes computationally intensive, and the current focus is on binary segmentation.
- Why unresolved: The paper only evaluates MedSASS on binary semantic segmentation tasks across four medical datasets. The limitations of Otsu's method for multi-class segmentation and the lack of empirical results for multi-class scenarios leave this question unanswered.
- What evidence would resolve it: Experiments comparing MedSASS's performance on multi-class segmentation tasks using alternative thresholding techniques or modifications to handle multiple classes, alongside a comparison with existing multi-class self-supervised methods.

### Open Question 2
- Question: Can MedSASS be effectively adapted for natural image segmentation, given its reliance on Otsu's approach which works well for medical images?
- Basis in paper: [explicit] The paper acknowledges that MedSASS's dependence on Otsu's approach limits its applicability to natural images, as natural images lack the consistent foreground-background relationships present in medical images.
- Why unresolved: While the paper provides a rationale for why Otsu's approach works for medical images, it does not empirically test MedSASS on natural image datasets or explore modifications to make it suitable for such data.
- What evidence would resolve it: Experiments applying MedSASS to natural image segmentation datasets, possibly with adaptations like iterative thresholding or hybrid approaches combining Otsu's method with other techniques, and a comparison of results with state-of-the-art natural image segmentation methods.

### Open Question 3
- Question: What is the impact of pre-training both the encoder and decoder end-to-end on the generalizability and robustness of MedSASS across different medical imaging modalities?
- Basis in paper: [explicit] The paper highlights that training both the encoder and decoder end-to-end yields significant improvements (14.4% for CNNs and 6% for ViTs) over encoder-only training, but it does not explore how this affects generalizability across diverse modalities.
- Why unresolved: The paper evaluates MedSASS on four medical datasets with different modalities but does not specifically analyze whether end-to-end training enhances robustness or generalizability to unseen or heterogeneous data.
- What evidence would resolve it: A study testing MedSASS's performance on a broader range of medical imaging modalities, including cross-domain evaluations where the model is pre-trained on one modality and tested on another, to assess its adaptability and robustness.

## Limitations

- The reliance on Otsu thresholding limits applicability to natural images with complex textures and multiple objects
- The framework has not been systematically evaluated across diverse medical imaging modalities with varying contrast levels
- No ablation studies demonstrate the necessity of focal-Tversky loss over simpler loss functions

## Confidence

- **High Confidence**: The claim that end-to-end training of both encoder and decoder improves segmentation performance is well-supported by the experimental results showing 14.4% improvement for CNNs and 6% for ViTs over encoder-only methods.
- **Medium Confidence**: The assertion that MedSASS matches ViT performance while outperforming CNN-based methods is supported by reported metrics, though the exact implementation details of baseline methods and their training configurations are not fully specified, making direct comparison challenging.
- **Medium Confidence**: The effectiveness of Otsu thresholding for generating supervision masks in medical images is supported by the high-contrast nature of medical imaging, but the paper lacks systematic evaluation of Otsu's performance across diverse medical datasets with varying contrast levels.

## Next Checks

1. Conduct ablation studies comparing MedSASS with different thresholding methods (e.g., adaptive thresholding) to quantify the impact of Otsu's method on segmentation performance across diverse medical imaging modalities.

2. Test MedSASS on datasets with known class imbalance and varying contrast levels to assess the robustness of the focal-Tversky loss and Otsu-generated supervision masks.

3. Implement and evaluate MedSASS with alternative decoder architectures (e.g., SegNet, DeepLab) to determine whether the observed performance gains are specific to the U-Net decoder or generalize to other architectures.