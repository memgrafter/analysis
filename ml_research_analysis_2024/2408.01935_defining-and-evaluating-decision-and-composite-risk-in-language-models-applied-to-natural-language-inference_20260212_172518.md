---
ver: rpa2
title: Defining and Evaluating Decision and Composite Risk in Language Models Applied
  to Natural Language Inference
arxiv_id: '2408.01935'
source_url: https://arxiv.org/abs/2408.01935
tags:
- risk
- decision
- rule
- instances
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of misplaced confidence in large
  language models (LLMs) during natural language inference (NLI) tasks. The authors
  identify two types of risk: decision risk (when the model abstains from answering
  unambiguous instances or answers ambiguous ones) and composite risk (when the model
  answers incorrectly or withholds a correct answer).'
---

# Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference

## Quick Facts
- arXiv ID: 2408.01935
- Source URL: https://arxiv.org/abs/2408.01935
- Authors: Ke Shen; Mayank Kejriwal
- Reference count: 40
- Key outcome: DwD reduces decision risk by up to 25.3% and composite risk by up to 16.6% on NLI benchmarks

## Executive Summary
This paper addresses the problem of misplaced confidence in large language models (LLMs) during natural language inference (NLI) tasks. The authors identify two types of risk: decision risk (when the model abstains from answering unambiguous instances or answers ambiguous ones) and composite risk (when the model answers incorrectly or withholds a correct answer). To tackle these issues, they propose a risk-adjusted calibration framework called "Deciding when to Decide" (DwD) that uses an external decision rule method compatible with both discriminative and generative LLMs. The framework includes risk injection functions to create ambiguous instances for training. Experiments on four NLI benchmarks show that DwD significantly outperforms competitive baselines, reducing decision risk by up to 25.3% and composite risk by up to 16.6%.

## Method Summary
The DwD framework uses an external decision rule (random forest classifier) trained on features extracted from LLM outputs (confidence scores, prompt length, answer length, embedding similarities) to decide whether to answer or abstain on NLI tasks. The training set is augmented with risk-injected instances created by the Wrong-Question (WQ) and No-Right-Answer (NRA) functions, which transform unambiguous instances into ambiguous ones. The decision rule is trained on discriminative models (RoBERTa) but shows generalization to generative models (GPT-3.5-Turbo). The framework formalizes inference as a two-step process: the decision rule determines whether to answer, and if so, the base model's selection rule chooses the answer.

## Key Results
- DwD reduces decision risk by up to 25.3% and composite risk by up to 16.6% compared to competitive baselines
- The framework generalizes from discriminative (RoBERTa) to generative (GPT-3.5-Turbo) LLMs despite being trained only on discriminative models
- A case study on choice overload demonstrates DwD's effectiveness in real-world inference scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The external decision rule (DwD) improves calibration by learning from both correct and artificially injected ambiguous instances, which helps the model distinguish high-risk from low-risk inference scenarios.
- Mechanism: DwD is trained as a random forest classifier using a balanced mix of original and risk-injected instances. The risk injection functions (WQ and NRA) create ambiguous examples that expose the model to uncertainty during training, allowing it to learn patterns that indicate ambiguity or incorrectness.
- Core assumption: The features used (confidence scores, prompt length, answer length, embedding similarities) are sufficiently discriminative to capture the model's uncertainty and risk levels.
- Evidence anchors:
  - [abstract] "To accommodate these risks, we propose and implement a novel risk-adjusted calibration framework called ‘Deciding when to Decide’ (DwD), which uses an external decision rule method, compatible with both discriminative and generative LLMs."
  - [section] "Concerning the former, DwD injects risk into training set construction, a step not explicitly considered by earlier calibrators."
- Break condition: If the injected ambiguity does not reflect real-world uncertainty, or if the feature set lacks sensitivity to risk signals, the decision rule will fail to generalize.

### Mechanism 2
- Claim: Composite risk reduction is achieved by the DwD method selectively withholding responses in high-risk scenarios, which lowers the probability of incorrect answers while maintaining accuracy on low-risk cases.
- Mechanism: DwD uses the decision rule to abstain from answering when the risk is high, reducing the number of incorrect predictions. The relative risk ratio (RRR) metric confirms this by showing that DwD significantly reduces composite risk when it decides to answer versus when it abstains.
- Core assumption: The decision rule is well-calibrated so that it abstains precisely when the model's selection rule would likely be wrong.
- Evidence anchors:
  - [abstract] "Experiments on four NLI benchmarks show that DwD significantly outperforms competitive baselines, reducing decision risk by up to 25.3% and composite risk by up to 16.6%."
  - [section] "WhenRRR is significantly smaller than 1 (e.g., at the 95% confidence level), it implies that dr significantly reduces composite risk when it decides to answer (dr = 1), compared to when it abstains."
- Break condition: If the decision rule is poorly calibrated, it may withhold too many correct answers or allow too many incorrect ones, increasing composite risk instead of reducing it.

### Mechanism 3
- Claim: DwD generalizes to generative LLMs like GPT-3.5-Turbo despite being trained only on discriminative models because it relies on confidence score distributions rather than internal model representations.
- Mechanism: DwD uses the confidence scores and other observable features from the underlying model to make decisions, making it model-agnostic. The case study shows that DwD-guided ChatGPT outperforms RoBERTa on certain benchmarks.
- Core assumption: The confidence score distributions from different models contain sufficient similarity in their patterns of uncertainty and risk.
- Evidence anchors:
  - [abstract] "DwD empirically outperforms competitive baselines on several established inference benchmarks by reducing decision and composite risk in underlying LMs by margin of up to 25.3% and 16.6%, respectively."
  - [section] "With these features, the DwD method is trained using a random forest classifier with equal numbers of original and risk-injected instances (following the application of one RIF) as the training set."
- Break condition: If confidence scores from different models have fundamentally different distributions or meanings, DwD will not generalize effectively.

## Foundational Learning

- Concept: Calibration of confidence scores in machine learning models
  - Why needed here: The paper addresses the issue of poorly calibrated confidence scores in LLMs, which is central to understanding and mitigating decision and composite risk.
  - Quick check question: What is the difference between a well-calibrated and poorly calibrated confidence score in the context of a classifier's predictions?

- Concept: Decision rule and selection rule architecture in selective prediction
  - Why needed here: The framework formalizes inference as a two-step process (decision rule followed by selection rule), which is key to understanding how DwD reduces risk.
  - Quick check question: In the selective prediction framework, what is the role of the decision rule, and how does it interact with the selection rule?

- Concept: Risk injection and data augmentation techniques
  - Why needed here: DwD uses risk injection functions (WQ and NRA) to create ambiguous training examples, which is essential for training the decision rule to detect risk.
  - Quick check question: How do the Wrong-Question (WQ) and No-Right-Answer (NRA) risk injection functions transform unambiguous instances into ambiguous ones?

## Architecture Onboarding

- Component map:
  Base Language Model (RoBERTa or GPT-3.5-Turbo) -> Feature Extraction Module (confidence scores, prompt length, embeddings, etc.) -> DwD Decision Rule (Random Forest Classifier) -> Selection Rule (Base Model's Selection)

- Critical path:
  1. Extract features from the base model's output for each instance.
  2. Apply the DwD decision rule to decide whether to answer or abstain.
  3. If answering, use the base model's selection rule to choose the answer.
  4. Evaluate performance using decision and composite risk metrics.

- Design tradeoffs:
  - Training DwD on discriminative models allows generalization to generative models but may miss model-specific risk patterns.
  - Using a random forest classifier provides interpretability and robustness but may not capture complex non-linear relationships as well as neural methods.
  - Injecting ambiguity during training helps detect risk but may lead to over-cautiousness if not balanced properly.

- Failure signatures:
  - Low decision risk accuracy on OOD data indicates poor generalization of the decision rule.
  - High composite risk despite using DwD suggests the decision rule is not well-calibrated.
  - No improvement over baselines on certain benchmarks may indicate that the feature set is not discriminative enough.

- First 3 experiments:
  1. Train DwD on RoBERTa confidence scores using WQ and NRA risk injection, then evaluate decision risk accuracy on in-domain and out-of-domain test sets.
  2. Compare DwD-guided RoBERTa performance on composite risk metrics (sensitivity, specificity, RRR) against baseline decision rules.
  3. Apply DwD to GPT-3.5-Turbo and evaluate whether it generalizes, measuring changes in decision and composite risk.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The effectiveness of DwD depends heavily on the quality and representativeness of the risk injection functions, with uncertainty about whether WQ and NRA adequately capture real-world ambiguity.
- The framework's performance on truly out-of-distribution data remains partially untested, as the OOD evaluation uses different injection functions rather than completely different domains.
- The choice of random forest as the decision rule may limit the ability to capture complex non-linear risk patterns that could be better modeled by neural approaches.

## Confidence

- High confidence: The experimental methodology is sound, and the results showing risk reduction are statistically significant and well-documented. The core claims about DwD's effectiveness on the tested benchmarks are well-supported.
- Medium confidence: The generalization to GPT-3.5-Turbo, while promising, is based on limited testing. The assumption that confidence score distributions from different models contain sufficient similarity needs further validation.
- Medium confidence: The case study on choice overload provides useful insights but represents a narrow slice of real-world scenarios where risk mitigation would be valuable.

## Next Checks

1. Test DwD on completely different NLI datasets from different domains (e.g., biomedical or legal text) to verify true out-of-distribution generalization beyond different risk injection functions.
2. Compare DwD against more sophisticated decision rule approaches, such as neural networks or gradient-boosted trees, to determine if the random forest architecture is optimal or simply sufficient.
3. Conduct ablation studies removing individual features (confidence scores, prompt length, embeddings) to quantify their relative importance in risk detection and identify which features are essential versus complementary.