---
ver: rpa2
title: Performance of Recent Large Language Models for a Low-Resourced Language
arxiv_id: '2407.21330'
source_url: https://arxiv.org/abs/2407.21330
tags:
- language
- sinhala
- llms
- fine
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of four recent large language
  models (LLMs) - Claude 3 Sonnet, GPT-4o, Llama 3, and Mistral 7B - on the low-resourced
  Sinhala language. The models were tested on translation, text summarization, and
  text generation tasks, both directly in Sinhala and via English translation.
---

# Performance of Recent Large Language Models for a Low-Resourced Language
## Quick Facts
- arXiv ID: 2407.21330
- Source URL: https://arxiv.org/abs/2407.21330
- Authors: Ravindu Jayakody; Gihan Dias
- Reference count: 17
- Four recent LLMs (Claude 3 Sonnet, GPT-4o, Llama 3, Mistral 7B) evaluated on Sinhala language tasks

## Executive Summary
This study evaluates the performance of four recent large language models on the low-resourced Sinhala language across translation, text summarization, and text generation tasks. The research compares out-of-the-box performance against fine-tuned performance, examining both direct Sinhala processing and translation-based approaches. Major findings reveal that while commercial models like Claude and GPT-4o show strong baseline performance, open-source models require fine-tuning to achieve acceptable results for low-resource languages.

## Method Summary
The study tested four LLMs (Claude 3 Sonnet, GPT-4o, Llama 3, Mistral 7B) on Sinhala language tasks using three test cases per task. Models were evaluated for direct Sinhala processing and via English translation. Llama 3 and Mistral 7B were fine-tuned using a small Sinhala news dataset of approximately 3,250 articles. Performance was assessed by a panel of six reviewers including professional content writers and translators, using a 0-5 scoring scale. All prompts and translations were handled by the models or reviewers as appropriate.

## Key Results
- Claude 3 Sonnet and GPT-4o achieved average scores of 4.0 and 4.1 respectively for text summarization out-of-the-box
- Llama 3 and Mistral 7B showed poor initial performance but improved with fine-tuning on a small Sinhala news dataset
- Translation-based approaches did not significantly improve performance over direct processing for most tasks
- Major LLMs demonstrate significantly better support for low-resourced languages compared to previous versions

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Pre-trained multilingual models exhibit language-specific fairness disparities that directly impact low-resource language performance.
- Mechanism: Models trained on imbalanced multilingual corpora develop different levels of group disparity across languages due to uneven representation and vocabulary coverage.
- Core assumption: The distribution of training data across languages directly correlates with model performance on those languages.
- Evidence anchors:
  - [abstract] "these models exhibit different levels of group disparity across the four target languages"
  - [section II.A] "it is crucial to scrutinize them for fairness across languages, like monolingual models"
  - [corpus] Weak - corpus focuses on Assamese and other languages, not directly addressing fairness disparities
- Break condition: When training data becomes more balanced across languages or when models implement explicit fairness constraints.

### Mechanism 2
- Claim: Fine-tuning with small, domain-specific datasets can significantly improve LLM performance on low-resource languages.
- Mechanism: Parameter-efficient fine-tuning methods allow adaptation of large models to specific linguistic patterns and domains without full retraining.
- Core assumption: The pre-trained models contain generalizable linguistic representations that can be specialized through fine-tuning.
- Evidence anchors:
  - [abstract] "Llama and Mistral perform poorly but show some promise of improvement with fine tuning"
  - [section II.B] "Fine-tuning refers to the process of adapting a pretrained language model's weights to a specific downstream task using task data"
  - [section IV.C.3] "Llama 3 and Mistral models were fine-tuned using a Sinhala news data set"
- Break condition: When the fine-tuning dataset is too small or too dissimilar from the target domain, leading to overfitting or ineffective transfer.

### Mechanism 3
- Claim: Translation-based approaches can serve as effective pre/post-processing for improving LLM performance on low-resource languages.
- Mechanism: By translating between the low-resource language and a high-resource language (like English), models can leverage their stronger performance in high-resource languages.
- Core assumption: LLMs have stronger capabilities in high-resource languages that can be transferred to low-resource languages through translation.
- Evidence anchors:
  - [abstract] "by translation to and from English"
  - [section IV.B.2] "We translated the test cases to English and instructed each model to summarize them in English"
  - [section V.B] "Summarization by translation via English did not result in any significant differences over direct summarization"
- Break condition: When translation quality is poor or when models internally translate regardless of explicit instructions.

## Foundational Learning
- Concept: Tokenization and vocabulary coverage in multilingual models
  - Why needed here: Different languages have varying tokenization requirements, and poor vocabulary coverage directly impacts model performance on low-resource languages
  - Quick check question: How does a tokenizer handle words that don't exist in its vocabulary for a low-resource language?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: Understanding how to adapt large models to specific domains without full retraining is crucial for improving low-resource language support
  - Quick check question: What is the difference between full fine-tuning and parameter-efficient methods like adapters or quantization-aware adaptation?

- Concept: Cross-lingual transfer learning mechanisms
  - Why needed here: Understanding how knowledge transfers between languages helps explain why translation-based approaches work
  - Quick check question: What linguistic features are likely to transfer well between languages during cross-lingual fine-tuning?

## Architecture Onboarding
- Component map: Input text → Translation (if needed) → LLM inference engine → Translation (if needed) → Human evaluation
- Critical path: Input text → Translation (if needed) → LLM inference → Translation (if needed) → Human evaluation
- Design tradeoffs:
  - Open vs. closed models: Open models allow fine-tuning but may have lower baseline performance
  - Direct vs. translation-based approaches: Translation adds complexity but may leverage stronger high-resource language capabilities
  - Fine-tuning data size: Larger datasets improve performance but increase computational requirements
- Failure signatures:
  - Poor translation quality leading to cascading errors
  - Unicode handling issues (e.g., missing ZWJ characters in Sinhala)
  - Model-specific artifacts indicating internal translation
  - Inconsistent performance across different task types
- First 3 experiments:
  1. Test translation quality from Sinhala to English and vice versa using multiple systems to establish baseline
  2. Perform direct text summarization in Sinhala and compare with translation-based approaches
  3. Fine-tune Llama 3 and Mistral 7B on the Sinhala news dataset and measure improvement in summarization quality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Do the major LLMs (Claude and GPT) internally translate input and output texts to English or an internal representation during summarization tasks?
- Basis in paper: [inferred] The authors observed artifacts suggesting that models may internally translate texts during direct summarization, and the performance of summarization by translation via English did not significantly differ from direct summarization.
- Why unresolved: The authors mention this observation needs further study before a conclusion can be drawn.
- What evidence would resolve it: Detailed analysis of the intermediate steps and representations used by the models during summarization tasks, potentially through model interpretability techniques or access to model internals.

### Open Question 2
- Question: Can Llama 3 achieve good performance in specific domains for Sinhala language tasks through appropriate fine-tuning with more data?
- Basis in paper: [inferred] The authors observed a small improvement in Llama 3's summarization performance after fine-tuning with a small Sinhala news dataset, suggesting potential for improvement with more data.
- Why unresolved: The authors only used a small out-of-domain dataset for fine-tuning and did not explore the limits of Llama 3's fine-tuning capabilities.
- What evidence would resolve it: Experiments with larger, domain-specific Sinhala datasets for fine-tuning Llama 3, comparing its performance to other models before and after fine-tuning.

### Open Question 3
- Question: How can pre- and post-processing techniques such as translation and retrieval-augmented generation enhance the performance of LLMs for low-resourced languages like Sinhala?
- Basis in paper: [explicit] The authors suggest this as a promising area of inquiry, noting that both Claude and GPT 4o performed much better at generation via translation compared to direct generation in Sinhala.
- Why unresolved: The authors did not conduct experiments to explore these techniques in detail.
- What evidence would resolve it: Systematic experiments applying various pre- and post-processing techniques to improve LLM performance on Sinhala tasks, comparing results to baseline models without these techniques.

## Limitations
- Extremely small sample size of test cases (three per task) raises questions about statistical significance
- Limited Sinhala dataset used for fine-tuning (approximately 3,250 articles) may not generalize well to broader language tasks
- Evaluation relies entirely on subjective human scoring without standardized metrics or inter-rater reliability measures

## Confidence
- **High confidence**: The observation that major LLMs (Claude 3 Sonnet, GPT-4o) provide significantly better out-of-box support for low-resourced languages compared to previous versions
- **Medium confidence**: The finding that open-source models (Llama 3, Mistral 7B) require fine-tuning to achieve acceptable performance levels
- **Low confidence**: The conclusion that translation-based approaches do not significantly improve performance, given the extremely limited number of test cases

## Next Checks
1. Reproduce with larger test sets: Expand evaluation to include at least 20-30 test cases per task to establish statistical significance
2. Establish inter-rater reliability: Implement standardized scoring rubrics and calculate inter-rater agreement metrics to validate consistency of human evaluations
3. Test with diverse Sinhala domains: Evaluate model performance on Sinhala content from multiple domains (news, social media, literature) to assess whether fine-tuning improvements generalize beyond the news dataset used in the study