---
ver: rpa2
title: Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs
arxiv_id: '2410.14641'
source_url: https://arxiv.org/abs/2410.14641
tags:
- equation
- information
- relevant
- context
- pieces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LongPiBench, a comprehensive benchmark for
  evaluating positional bias in long-context language models (LLMs). Unlike prior
  work focusing on single pieces of relevant information, LongPiBench assesses positional
  bias involving multiple relevant information pieces by manipulating both absolute
  positions (location within the entire context) and relative positions (spacing between
  relevant pieces).
---

# Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs

## Quick Facts
- arXiv ID: 2410.14641
- Source URL: https://arxiv.org/abs/2410.14641
- Authors: Runchu Tian; Yanghao Li; Yuepeng Fu; Siyang Deng; Qinyu Luo; Cheng Qian; Shuo Wang; Xin Cong; Zhong Zhang; Yesai Wu; Yankai Lin; Huadong Wang; Xiaojiang Liu
- Reference count: 40
- Primary result: Modern LLMs largely overcome "lost in the middle" but still show significant bias related to relative spacing of relevant information pieces

## Executive Summary
This paper introduces LongPiBench, a comprehensive benchmark for evaluating positional bias in long-context language models (LLMs) that specifically focuses on scenarios involving multiple relevant information pieces. Unlike prior work that primarily examined single pieces of relevant information, LongPiBench manipulates both absolute positions (location within the entire context) and relative positions (spacing between relevant pieces) across three tasks (Table SQL, Timeline Reordering, and Equation Solving) with context lengths ranging from 32K to 256K tokens.

Experiments on eleven popular LLMs reveal two key findings: modern LLMs have largely overcome the "lost in the middle" phenomenon for absolute positions, but significant bias remains related to relative spacing of relevant information pieces, causing 20-30% performance degradation even in simple retrieval tasks. The study shows that increasing model size mitigates absolute position bias but has limited effect on relative position bias, suggesting different underlying mechanisms. Additionally, query placement impacts performance for decoder-only models, with better results when queries are placed before the context.

## Method Summary
The study introduces LongPiBench, a benchmark designed to evaluate positional bias in long-context LLMs by manipulating both absolute and relative positions of relevant information. The benchmark includes three tasks: Table SQL (SQL query generation from tables), Timeline Reordering (reordering historical events), and Equation Solving (mathematical problem solving). Each task was manually annotated with 20 seed examples, then augmented to create 7,680 instances across four context lengths (32K, 64K, 128K, 256K tokens) and 16 levels of positional variation. The evaluation involved eleven models including six open-source (Llama-3.1-Instruct-70B, Qwen-2.5 variants 7B/14B/32B/72B, WizardLM 2 8x22B) and five commercial models (GPT-4o-mini, Claude3-Haiku, Gemini-1.5-Flash, GLM-4-air, Deepseek-Chat-v2). Standardized inference parameters were used across all evaluations.

## Key Results
- Modern LLMs have largely overcome the "lost in the middle" phenomenon, showing robustness against absolute positional bias
- Significant bias remains related to relative spacing of relevant information pieces, with performance declining sharply as distance increases before stabilizing
- Relative position bias causes 20-30% performance degradation even in simple retrieval tasks
- Increasing model size from 7B to 14B parameters mitigates absolute position bias but has limited effect on relative position bias
- Decoder-only models perform better when queries are placed before the context due to unidirectional attention patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing model parameter size from 7B to 14B significantly mitigates absolute position bias ("lost in the middle") while having minimal effect on relative position bias.
- Mechanism: Larger models develop better positional encoding representations that reduce the impact of token position on attention weights for absolute positioning, but relative spacing between information pieces requires different architectural or training approaches.
- Core assumption: The "emergent ability" of handling absolute positions scales with parameter count due to improved representation capacity.
- Evidence anchors:
  - [abstract] "increasing model size mitigates absolute position bias but has limited effect on relative position bias"
  - [section 4.3] "simply increasing the model parameters from 7B to 14B...substantially mitigates the 'lost in the middle' issue"
- Break condition: If relative position bias remains significant even in models with hundreds of billions of parameters, the parameter scaling hypothesis for absolute positions may not generalize to relative positioning.

### Mechanism 2
- Claim: Query placement before context significantly improves performance for decoder-only models due to unidirectional attention patterns.
- Mechanism: Decoder-only models cannot attend to query tokens when processing context tokens if the query appears after the context, limiting their ability to contextualize the retrieval task.
- Core assumption: Unidirectional attention in decoder-only architectures creates a fundamental limitation for query-aware contextualization.
- Evidence anchors:
  - [abstract] "Query-aware contextualization also impacts performance, with decoder-only models performing better when queries are placed before the context"
  - [section 4.3] "when the query is positioned at the end of the context, the model's performance is significantly worse compared to scenarios where the query is placed at the beginning"
- Break condition: If bidirectional models (like encoder-decoder architectures) show no performance difference based on query placement, the mechanism is specific to unidirectional attention patterns.

### Mechanism 3
- Claim: Relative position bias causes performance to decline sharply as spacing between relevant information pieces increases, then stabilizes rather than continuing to degrade.
- Mechanism: The model's attention mechanism becomes increasingly challenged by sparse relevant information distribution, but reaches a plateau where additional spacing doesn't further degrade performance due to attention saturation or context window limitations.
- Core assumption: The relationship between relative spacing and performance follows a non-linear pattern with diminishing returns.
- Evidence anchors:
  - [abstract] "significant bias remains related to relative spacing of relevant information pieces, with performance declining sharply as distance increases before stabilizing"
  - [section 4.2] "as the distance between relevant pieces increases, model performance declines sharply before stabilizing"
- Break condition: If performance continues to degrade linearly with increasing spacing rather than stabilizing, the mechanism would need revision.

## Foundational Learning

- Concept: Positional encoding mechanisms in transformer architectures
  - Why needed here: Understanding how token positions are encoded and how this affects attention is crucial for analyzing both absolute and relative position biases
  - Quick check question: How do sinusoidal positional encodings differ from learned positional embeddings in their treatment of relative positions?

- Concept: Attention mechanism and query-key-value computation
  - Why needed here: The core mechanism by which LLMs retrieve relevant information depends on attention, making it essential for understanding positional biases
  - Quick check question: What happens to attention weights when relevant information is sparsely distributed versus densely packed in the context?

- Concept: Context window limitations and memory constraints
  - Why needed here: The 32K-256K token context lengths studied here push against practical limitations that may contribute to positional biases
  - Quick check question: How might attention scaling with sequence length contribute to the stabilization effect observed in relative position bias?

## Architecture Onboarding

- Component map: Tokenizer → Context embedding → Positional encoding → Multi-head attention → Feed-forward network → Output head
- Critical path: Query → Context → Relevant information extraction → Answer generation
- Design tradeoffs: Balancing context length capacity with computational efficiency, choosing between absolute vs relative positional encodings
- Failure signatures: Sharp performance drops at specific absolute/relative position thresholds, inconsistent performance across context lengths
- First 3 experiments:
  1. Test performance degradation curves for both absolute and relative positions across multiple context lengths
  2. Compare decoder-only vs encoder-decoder models on query placement effects
  3. Analyze attention weight distributions for densely vs sparsely spaced relevant information

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific architectural changes could reduce relative positional bias in long-context LLMs without requiring massive increases in model size?
  - Basis in paper: The paper states that "merely increasing parameter size is insufficient to develop robustness to relative positions, and new techniques may be necessary"
  - Why unresolved: The paper identifies the problem but doesn't propose specific architectural solutions beyond noting that larger models alone don't solve it

- **Open Question 2**: How does the "distance threshold" phenomenon (sharp decline followed by stabilization) in relative position bias vary across different types of tasks and information dependencies?
  - Basis in paper: The paper observes "an initial rapid decline in performance followed by a more gradual decrease" but doesn't systematically explore when this threshold occurs
  - Why unresolved: The experiments show the phenomenon exists but don't map how it varies with task complexity, information density, or dependency structures

- **Open Question 3**: What is the underlying mechanism causing different responses to absolute versus relative positional bias across model sizes?
  - Basis in paper: The paper finds that "increasing the model parameters from 7B to 14B...substantially mitigates the 'lost in the middle' issue" for absolute position but has limited effect on relative position bias
  - Why unresolved: The paper observes this difference but doesn't explain why parameter scaling affects these biases differently

## Limitations
- The study identifies relative position bias as a distinct problem but does not provide architectural or training solutions to address this issue
- The benchmark focuses on retrieval tasks with exactly 10 relevant information pieces, which may not generalize to all long-context scenarios
- Experiments are limited to 11 specific models, and findings may not extend to other architectures or training paradigms

## Confidence
- **High Confidence**: The finding that modern LLMs have largely overcome the "lost in the middle" phenomenon for absolute positions
- **Medium Confidence**: The characterization of relative position bias showing sharp performance decline followed by stabilization
- **Medium Confidence**: The claim that decoder-only models perform better when queries are placed before context due to unidirectional attention patterns

## Next Checks
1. Test whether the relative position bias pattern (sharp decline followed by stabilization) holds across additional task types beyond the three evaluated to verify generalizability
2. Conduct ablation studies on attention weight distributions for models with different positional encoding schemes (sinusoidal vs learned vs rotary) to better understand the mechanism behind relative position bias
3. Test whether the relative position bias persists even in models with hundreds of billions of parameters, and whether there's a threshold where this bias begins to diminish, similar to what was observed for absolute position bias at 14B parameters