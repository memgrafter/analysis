---
ver: rpa2
title: 'AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval'
arxiv_id: '2405.17718'
source_url: https://arxiv.org/abs/2405.17718
tags:
- image
- noise
- images
- quality
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel setting for low-quality image retrieval,
  where the goal is to retrieve normal images from a database given low-quality query
  images. To address this problem, the authors propose an Adaptive Noise-Based Network
  (AdapNet) that learns robust abstract representations.
---

# AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval

## Quick Facts
- **arXiv ID**: 2405.17718
- **Source URL**: https://arxiv.org/abs/2405.17718
- **Authors**: Sihe Zhang; Qingdong He; Jinlong Peng; Yuxi Li; Zhengkai Jiang; Jiafu Wu; Mingmin Chi; Yabiao Wang; Chengjie Wang
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance on low-quality image retrieval, outperforming previous methods by up to 9.3% on Rpar-Hard

## Executive Summary
This paper introduces a novel setting for low-quality image retrieval, where the goal is to retrieve normal images from a database given low-quality query images. The authors propose AdapNet, an Adaptive Noise-Based Network that learns robust abstract representations through a quality compensation block and an innovative adaptive noise-based loss function. The method achieves state-of-the-art performance on the Noise Revisited Oxford and Noise Revisited Paris benchmarks, demonstrating significant improvements over existing methods.

## Method Summary
AdapNet addresses low-quality image retrieval through two key innovations: a Quality Compensation Block (QCB) that leverages high-quality reference images to compensate for known noise characteristics, and a NoiRetrieval Loss that dynamically adjusts gradient focus based on image quality. The QCB learns feature transformations to "subtract out" known noise patterns using paired high-quality and low-quality images, while the NoiRetrieval Loss scales angular margins based on feature-derived quality descriptors to prioritize learning from low-quality samples. The model is trained using a combination of InfoNCE loss for local and global features, with the entire system optimized end-to-end.

## Key Results
- AdapNet outperforms previous best methods by up to 6.9% on Rpar-Medium and 9.3% on Rpar-Hard in "Noise to Clear" setting
- Achieves 4.4% improvement on Rpar-Medium and 7.8% on Rpar-Hard in "Clear to Clear" setting
- Demonstrates state-of-the-art performance on Noise Revisited Oxford and Paris benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Quality Compensation Block (QCB) learns feature transformations that specifically compensate for known noise patterns by leveraging paired high-quality and low-quality images.
- Mechanism: The QCB applies multiple convolution kernels to extract different noise types from the low-quality image, then selectively fuses these noise-specific features with the original low-quality features. This fused representation is used alongside the high-quality image features to compute InfoNCE loss, effectively teaching the model to "subtract out" known noise characteristics.
- Core assumption: The low-quality image can be decomposed into a clean image plus known noise, and this decomposition is learnable through paired training.
- Evidence anchors:
  - [abstract]: "we devise a quality compensation block trained to compensate for various low-quality factors in input images"
  - [section]: "Quality compensation block that leverages high-quality reference images to facilitate the model's understanding of various known noise characteristics"
- Break condition: If the noise is too complex or the low-quality image contains unknown noise types not present in the training pairs, the QCB cannot compensate effectively.

### Mechanism 2
- Claim: The NoiRetrieval Loss dynamically adjusts gradient weighting based on image quality, prioritizing learning from low-quality samples while maintaining discrimination for high-quality samples.
- Mechanism: The loss function scales the angular margin based on an image quality descriptor derived from feature norms. Lower quality images (smaller norms) receive larger margins, creating stronger gradients for these samples during backpropagation, while higher quality images receive smaller margins.
- Core assumption: Feature norm correlates with image quality in a way that can be reliably measured and used to weight gradients.
- Evidence anchors:
  - [abstract]: "dynamically adjusts its focus on the gradient in accordance with image quality"
  - [section]: "NoiRetrieval Loss, which pays more attention to the learning of low-quality samples"
- Break condition: If the feature norm-to-quality correlation breaks down in different datasets or if the margin scaling becomes too aggressive, harming high-quality sample discrimination.

### Mechanism 3
- Claim: Combining InfoNCE loss with the adaptive NoiRetrieval Loss creates complementary objectives that jointly improve both known noise compensation and unknown noise generalization.
- Mechanism: InfoNCE loss ensures the model learns noise-specific transformations through the QCB by maximizing similarity between transformed low-quality features and high-quality features. NoiRetrieval Loss then ensures the model remains robust to unknown noise by dynamically adjusting gradients based on overall image quality.
- Core assumption: Known noise compensation and unknown noise generalization are separate but complementary learning objectives that can be optimized simultaneously.
- Evidence anchors:
  - [abstract]: "leveraging high-quality reference images to enhance the model's understanding of various known noise characteristics" and "augmenting the learning of unknown noisy samples"
  - [section]: "The QCB learns the compensation features from the extracted global features" and "Noise Gradient Bias to prioritize the learning of general noise patterns"
- Break condition: If the two loss components interfere with each other or if the weighting between them is suboptimal, overall performance degrades.

## Foundational Learning

- Concept: Feature normalization and angular margin-based loss functions
  - Why needed here: The NoiRetrieval Loss modifies the angular margin in softmax loss based on image quality, requiring understanding of how normalization affects cosine similarity and angular relationships in feature space.
  - Quick check question: How does L2 normalization of feature vectors affect the interpretation of cosine similarity as angular distance?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The QCB uses InfoNCE loss to learn noise compensation features by contrasting transformed low-quality features against high-quality features, requiring understanding of contrastive learning principles.
  - Quick check question: What is the relationship between InfoNCE loss and maximizing mutual information between paired samples?

- Concept: Multi-task learning and loss weighting
  - Why needed here: The model combines three loss components (InfoNCE loss 1, InfoNCE loss 2, and NoiRetrieval Loss) requiring understanding of how to balance multiple objectives.
  - Quick check question: How do different loss weighting schemes affect the optimization trajectory when training with multiple objectives?

## Architecture Onboarding

- Component map: Backbone (ResNet50/101) -> Quality Compensation Block (QCB) -> Noise Gradient Bias (NGB) -> Classification Head. QCB processes global features from paired images, NGB processes final fused features with quality-based gradient scaling.
- Critical path: Input pair -> Backbone feature extraction -> QCB noise compensation -> Feature fusion -> NGB quality-based gradient scaling -> Classification
- Design tradeoffs: QCB adds computational overhead but enables known noise compensation; NoiRetrieval Loss is more complex than standard softmax but provides quality-aware learning; requires paired high/low-quality training data.
- Failure signatures: Poor known noise compensation (QCB ineffective) shows as degraded performance on datasets with predictable noise types; poor unknown noise generalization (NGB ineffective) shows as degraded performance on truly unseen noise types.
- First 3 experiments:
  1. Validate QCB effectiveness by training with only InfoNCE loss (no NoiRetrieval) and testing on datasets with known noise types present in training.
  2. Validate NoiRetrieval Loss effectiveness by training with only NoiRetrieval loss (no QCB) and testing on datasets with novel noise types.
  3. Test sensitivity to the α and β weighting hyperparameters by training with different combinations and measuring performance on both clean and noisy test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdapNet perform on other types of low-quality image datasets beyond noise-based degradation?
- Basis in paper: [explicit] The paper introduces a novel setting for low-quality image retrieval but only evaluates on datasets with artificially added noise. The authors mention that noise can stem from natural or human-induced factors, suggesting other degradation types exist.
- Why unresolved: The experiments are limited to noise-based degradation, and the paper does not explore other low-quality scenarios such as blur, compression artifacts, or lighting variations.
- What evidence would resolve it: Evaluating AdapNet on diverse low-quality datasets with different degradation types (e.g., blurry images, heavily compressed images, low-light images) would demonstrate its generalizability.

### Open Question 2
- Question: What is the impact of varying the number and types of noise functions used in the quality compensation block?
- Basis in paper: [explicit] The paper mentions using eight manually defined noise functions in the quality compensation block but does not explore the effect of varying this number or using different noise types.
- Why unresolved: The choice of eight noise functions and their specific types is not justified, and the paper does not investigate whether more or fewer noise functions, or different types, would improve performance.
- What evidence would resolve it: Conducting experiments with different numbers and types of noise functions in the quality compensation block would reveal the optimal configuration for various low-quality scenarios.

### Open Question 3
- Question: How does AdapNet's performance scale with the size of the training dataset and the number of landmarks?
- Basis in paper: [explicit] The paper uses GLDv2-clean and GLDv2-noisy datasets but does not explore how the model's performance changes with larger datasets or a greater number of landmarks.
- Why unresolved: The experiments are conducted on a fixed dataset size, and the paper does not investigate the scalability of AdapNet to larger datasets or more diverse landmark categories.
- What evidence would resolve it: Training AdapNet on progressively larger datasets with increasing numbers of landmarks and evaluating its performance would demonstrate its scalability and robustness to dataset size.

## Limitations

- Relies heavily on paired high-quality and low-quality training data, which may not be available for all applications
- QCB's effectiveness is constrained to known noise types present in training pairs, limiting generalization to truly novel noise patterns
- The Image Quality Descriptor's correlation between feature norm and actual image quality may not hold across diverse datasets or extreme noise conditions

## Confidence

- **High Confidence**: The overall performance improvements on established benchmarks (Revisited Oxford and Paris) are well-supported by quantitative results. The architectural design choices and loss function formulations are clearly described.
- **Medium Confidence**: The mechanism by which the NoiRetrieval Loss dynamically adjusts gradients based on image quality is theoretically sound but may be sensitive to hyperparameter choices (α and β) that could affect real-world performance.
- **Low Confidence**: The method's robustness to noise types not present in training data is not thoroughly validated. The assumption that feature norm reliably indicates image quality across all scenarios needs further empirical verification.

## Next Checks

1. **Ablation Study on Loss Components**: Systematically remove the Quality Compensation Block and NoiRetrieval Loss individually to quantify their independent contributions and verify their complementary nature.

2. **Generalization to Unseen Noise Types**: Test the trained model on datasets with noise types completely absent from the training data to evaluate true generalization capabilities beyond the known noise patterns.

3. **Sensitivity Analysis of Hyperparameters**: Conduct experiments with varying α and β values in the NoiRetrieval Loss to determine the optimal weighting and assess the method's robustness to hyperparameter selection.