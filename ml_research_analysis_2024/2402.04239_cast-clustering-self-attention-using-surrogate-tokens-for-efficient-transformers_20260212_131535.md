---
ver: rpa2
title: 'CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers'
arxiv_id: '2402.04239'
source_url: https://arxiv.org/abs/2402.04239
tags:
- cluster
- cast
- image
- clustering
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAST, a novel efficient self-attention mechanism
  for Transformers that achieves quadratic complexity reduction through learnable
  clustering of surrogate tokens. CAST constructs cluster affinity matrices using
  learnable surrogate tokens to group input sequences into clusters, then applies
  self-attention within clusters while allowing information flow across clusters via
  cluster summaries.
---

# CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers

## Quick Facts
- arXiv ID: 2402.04239
- Source URL: https://arxiv.org/abs/2402.04239
- Reference count: 38
- Achieves 59.32% average accuracy on Long Range Arena with 6.18x faster training and 90% memory reduction

## Executive Summary
CAST introduces a novel efficient self-attention mechanism that achieves quadratic complexity reduction through learnable clustering of surrogate tokens. The method constructs cluster affinity matrices using learnable surrogate tokens to group input sequences into clusters, then applies self-attention within clusters while allowing information flow across clusters via cluster summaries. Experiments on the Long Range Arena benchmark show CAST achieves competitive accuracy while significantly improving efficiency compared to standard Transformers and other efficient variants.

## Method Summary
CAST uses learnable surrogate tokens as clustering directions to group input sequences. The method computes similarity matrices between queries, keys, and surrogate tokens, then clusters tokens based on these similarities. Self-attention is applied within each cluster, and cluster summaries are created to enable information flow between clusters. The final output combines intra-cluster attention results with cross-cluster summaries using a weighted combination based on query-surrogate token similarity.

## Key Results
- Achieves 59.32% average accuracy on Long Range Arena benchmark
- 6.18x faster training speed compared to standard Transformers on 4K sequences
- 90% memory reduction while maintaining competitive performance
- Outperforms other efficient Transformers like Reformer and Performer

## Why This Works (Mechanism)

### Mechanism 1
CAST achieves quadratic complexity reduction through learnable clustering of surrogate tokens. The clustering is based on similarity matrices between queries, keys, and surrogate tokens, allowing self-attention to be applied within clusters rather than across the entire sequence.

### Mechanism 2
Cluster summaries enable information flow between clusters, preventing information isolation. After clustering, CAST creates value summaries for each cluster based on key-surrogate token similarity, which are combined with intra-cluster attention results using query-surrogate token similarity as weights.

### Mechanism 3
The sigmoid-weighted combination of query and key similarities creates a balanced clustering strategy. CAST computes both query-surrogate token similarity (Aq) and key-surrogate token similarity (Ak), then combines them using a sigmoid-weighted ratio based on a linear transformation of the input.

## Foundational Learning

- **Self-attention mechanism**: Understanding how standard self-attention works (Q·K^T·V) is essential to grasp why CAST's complexity reduction is significant and how its modifications work. *Quick check: What is the computational complexity of standard self-attention and why?*

- **Clustering algorithms**: CAST's core innovation is using learned clusters rather than static chunking. Understanding clustering concepts helps explain why this approach works. *Quick check: How does CAST's clustering differ from simple windowing approaches?*

- **Transformer architecture components**: CAST is a drop-in replacement for self-attention within the broader Transformer architecture. Understanding the full architecture helps contextualize CAST's role. *Quick check: Where in the Transformer architecture does CAST fit?*

## Architecture Onboarding

- **Component map**: Input sequence → Query/Key/Value projection → Surrogate token generation → Similarity matrix computation → Clustering → Intra-cluster attention → Cluster summaries → Weighted combination → Output
- **Critical path**: The computation of the cluster affinity matrix (Ag) is the most critical step, as it determines how tokens are grouped and affects all downstream operations
- **Design tradeoffs**: Number of clusters vs. memory/computation tradeoff; Top-K vs. Single Assignment Top-K clustering for different sequence characteristics; choice of attention function (softmax vs. Laplace) for similarity computation
- **Failure signatures**: Performance degradation with too few clusters (insufficient granularity); memory issues with too many clusters (complexity becomes O(N·N²c)); training instability if surrogate tokens don't learn meaningful clustering directions
- **First 3 experiments**:
  1. Benchmark CAST vs. standard Transformer on a simple sequence task with varying sequence lengths to verify complexity reduction claims
  2. Test different clustering mechanisms (Top-K vs. SA Top-K) on the same task to understand performance tradeoffs
  3. Vary the number of clusters while keeping sequence length constant to find the optimal cluster-to-sequence ratio for efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does CAST's performance scale with increasingly long sequences beyond 4K tokens? The paper focuses on the Long Range Arena benchmark which caps at 4K tokens, leaving the performance on longer sequences untested.

### Open Question 2
Can CAST be effectively adapted for generative modeling tasks, particularly in natural language processing? The paper mentions the absence of a decoding version for generative natural language as a current limitation.

### Open Question 3
How do the learnable surrogate tokens in CAST compare to the static clustering directions used in other clustering-based Transformers like Reformer and SMYRF? The paper highlights that CAST uses learnable surrogate tokens for clustering, in contrast to the static clustering directions in Reformer and SMYRF.

## Limitations

- The exact computational complexity depends on implementation details and hyperparameters, requiring empirical verification across different sequence lengths and cluster configurations
- Experiments are limited to the Long Range Arena benchmark, which represents a narrow slice of potential applications
- The paper assumes that learnable surrogate tokens will converge to meaningful clustering directions, but there's no analysis of surrogate token interpretability or convergence behavior

## Confidence

**High Confidence**: CAST achieves competitive accuracy on Long Range Arena tasks compared to other efficient Transformer variants; demonstrates significant speedups (6.18x) and memory reductions (90%) on 4K sequences; clustering mechanism successfully reduces computational complexity compared to standard self-attention

**Medium Confidence**: Quadratic complexity reduction claim holds across different sequence lengths and cluster configurations; cluster summaries adequately represent cluster information for cross-cluster information flow; sigmoid-weighted combination creates optimal clustering

**Low Confidence**: CAST's performance will generalize to tasks beyond Long Range Arena; surrogate token learning process consistently converges to meaningful clustering directions; clustering mechanism maintains effectiveness with varying attention pattern densities

## Next Checks

1. **Ablation Study on Clustering Components**: Systematically disable or modify individual components of CAST (surrogate tokens, clustering mechanism, cluster summaries) to quantify their individual contributions to performance and efficiency gains

2. **Cross-Domain Generalization Test**: Evaluate CAST on diverse NLP and CV tasks with varying attention pattern characteristics (local vs. global dependencies, different sequence length distributions) to assess robustness beyond the Long Range Arena benchmark

3. **Surrogate Token Analysis**: Visualize and analyze learned surrogate tokens across different layers and tasks to understand whether they converge to interpretable clustering directions and how this affects performance when surrogate token learning is suboptimal