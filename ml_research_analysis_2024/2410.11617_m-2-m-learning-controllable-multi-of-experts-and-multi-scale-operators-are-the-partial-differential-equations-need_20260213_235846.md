---
ver: rpa2
title: 'M$^{2}$M: Learning controllable Multi of experts and multi-scale operators
  are the Partial Differential Equations need'
arxiv_id: '2410.11617'
source_url: https://arxiv.org/abs/2410.11617
tags:
- learning
- multi-scale
- router
- neural
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes M2M, a multi-scale and multi-expert (M2M) neural
  operator framework for learning the dynamics of partial differential equations (PDEs).
  The core idea is to use a divide-and-conquer strategy where different experts handle
  different spatial scales and frequencies of the PDE solutions.
---

# M$^{2}$M: Learning controllable Multi of experts and multi-scale operators are the Partial Differential Equations need

## Quick Facts
- arXiv ID: 2410.11617
- Source URL: https://arxiv.org/abs/2410.11617
- Reference count: 40
- Primary result: M2M framework learns PDE dynamics using multi-scale experts and frequency-based routing, achieving higher accuracy and interpretability than baselines like FNO and UNO.

## Executive Summary
This paper introduces M2M, a multi-scale and multi-expert neural operator framework for learning partial differential equation (PDE) dynamics. The method uses a divide-and-conquer strategy where different experts handle different spatial scales and frequencies of PDE solutions. A controllable prior gating mechanism determines which expert handles which region, and a PI control strategy adjusts allocation rules during training. The approach is tested on 2D Navier-Stokes equations and a custom multi-scale dataset, demonstrating higher simulation accuracy and improved interpretability compared to baseline methods.

## Method Summary
M2M employs multi-scale segmentation to split input into patches, with a router assigning each patch to the most suitable expert based on frequency content. Different FNO-based experts learn different spatial scales independently before aggregation. A PI controller dynamically balances training focus between the router and experts by monitoring total loss and adjusting hyperparameter λ(t). The framework uses sparse routing (top-k selection) to reduce computation while maintaining accuracy, and trains using a combined loss function that incorporates KL divergence for load balancing and entropy for sparsity.

## Key Results
- Achieves higher simulation accuracy on 2D Navier-Stokes equations compared to FNO, UNO, CNO, and KNO baselines
- Demonstrates improved interpretability through frequency-based routing of spatial regions
- Shows efficiency gains through sparse routing and multi-scale expert allocation
- Validated on both standard Navier-Stokes dataset and custom multi-scale dataset with varying frequency source terms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M2M improves prediction accuracy by routing high-frequency regions to higher-capacity experts and low-frequency regions to lower-capacity experts.
- Mechanism: Multi-scale segmentation splits the input into patches, the router assigns each patch to the most suitable expert based on frequency content, and the experts learn different spatial scales independently before being aggregated.
- Core assumption: The frequency principle holds, so high-frequency dynamics benefit from higher-capacity models while low-frequency regions can be handled by simpler models.
- Evidence anchors:
  - [abstract]: "The core idea is to use a divide-and-conquer strategy where different experts handle different spatial scales and frequencies of the PDE solutions."
  - [section 3.1]: "Our critical insight lies in leveraging the divide-and-conquer approach among models to learn the capabilities across different scales quickly."
  - [corpus]: Weak - none of the 8 corpus papers explicitly mention frequency-based routing or the divide-and-conquer principle for multi-scale operators.
- Break condition: If the frequency decomposition is inaccurate or if spatial dependencies between patches are strong (e.g., turbulent flows), routing may misallocate resources and degrade performance.

### Mechanism 2
- Claim: The PI controller dynamically balances training focus between the router and experts, preventing early saturation of either component.
- Mechanism: The controller monitors the total loss, adjusts λ(t) to shift emphasis between router and experts, and stabilizes training by integrating past errors.
- Core assumption: Loss can be decomposed cleanly into router and experts terms, and the PI gains (Kp, Ki) are well-tuned.
- Evidence anchors:
  - [section 3.2]: "We design a non-linear PI controller in the loop... to automatically tune the hyperparameter λ(t) and use the desired loss or desired prior KL distribution as feedback during model training."
  - [section 4.1]: "For PID-M2M, we test our model on different initial λ values... The comparison study of the PI effect is shown in figure 3."
  - [corpus]: Weak - no corpus paper mentions PI or PID control in the context of neural operator training.
- Break condition: If the error signal is noisy or the model loss landscape is highly non-convex, the PI controller may over- or under-shoot λ(t), destabilizing training.

### Mechanism 3
- Claim: Sparse routing (top-k selection) reduces computation while maintaining accuracy by assigning each patch to only its best expert.
- Mechanism: The router outputs a probability distribution over experts; top-k strategy selects the k most likely experts per patch, and aggregation combines their outputs sparsely.
- Core assumption: For each patch, a single (or few) expert is sufficient to capture its dynamics, and the router can reliably rank experts.
- Evidence anchors:
  - [section 3.1]: "The key distinction of our objective lies in its emphasis on controllability and multi-scale considerations."
  - [section 4.1]: "M2M can allocate models sparsely and only sends the region with a slower change... to lowest mode FNO 16 for the computation efficiency."
  - [corpus]: Weak - no corpus paper explicitly discusses sparse routing or top-k expert selection in PDE operator learning.
- Break condition: If the router's ranking is noisy or if spatial dependencies require multi-expert collaboration, sparse routing may drop useful signals and hurt accuracy.

## Foundational Learning

- Concept: Frequency principle in neural networks (low frequencies learned before high frequencies)
  - Why needed here: Justifies why low-capacity experts can handle low-frequency patches while high-capacity experts are reserved for high-frequency patches.
  - Quick check question: In a Fourier spectrum of PDE solutions, which frequency components typically converge first during neural network training?

- Concept: Partial differential equation discretization and finite-difference/finite-element methods
  - Why needed here: Provides grounding for interpreting the data format, boundary conditions, and solution accuracy metrics.
  - Quick check question: What is the spatial resolution of the Navier-Stokes dataset used in the experiments?

- Concept: Kullback-Leibler divergence and entropy for load balancing
  - Why needed here: The router loss uses KL divergence to maintain similarity to a prior and entropy to encourage sparsity.
  - Quick check question: In the router loss formulation, what term encourages equal usage of all experts?

## Architecture Onboarding

- Component map: Input → Multi-scale segmentation (patch extraction) → Router (probability over experts) → Experts (FNO variants) → Aggregation → Output; PI controller monitors total loss → adjusts λ(t) → modulates router vs. experts training focus
- Critical path:
  1. Segmentation must produce patches that align with spatial frequency content.
  2. Router must learn to map patches to appropriate experts before experts can specialize.
  3. PI controller must stabilize λ(t) to avoid early convergence to a poor routing policy.
- Design tradeoffs:
  - Scale factor vs. boundary artifacts: Higher scale reduces patch size but increases boundary discontinuities.
  - Top-k vs. full routing: Top-k saves compute but risks losing cross-patch interactions.
  - PI gains vs. stability: Aggressive gains speed up adaptation but risk oscillation.
- Failure signatures:
  - High validation error with low training loss → router overfitting to training patches.
  - Degraded accuracy when scale > 4 → boundary effects dominate.
  - λ(t) saturating at extremes → PI controller unable to balance router/experts training.
- First 3 experiments:
  1. Run M2M with scale=1 (no segmentation) and compare to baseline FNO; expect similar accuracy but slower.
  2. Vary scale factor (2, 4, 8) on custom dataset; observe accuracy drop beyond scale=4 due to boundary artifacts.
  3. Disable PI controller (λ constant); observe slower convergence and potential router/experts imbalance.

## Open Questions the Paper Calls Out
None explicitly called out in the provided material.

## Limitations
- Limited to 2D problems with no empirical evidence for scalability to 3D or higher-dimensional PDEs
- Custom datasets used for evaluation lack public access, limiting reproducibility
- No comprehensive sensitivity analysis of PI controller hyperparameters (Kp, Ki, Kd)
- Static multi-scale structure assumed, not tested on dynamically evolving multi-scale scenarios

## Confidence
- Mechanism 1 (frequency-based routing): Medium - indirect evidence, frequency principle not explicitly validated in corpus
- Mechanism 2 (PI control): Medium - theoretical basis but no ablation studies quantifying impact
- Mechanism 3 (sparse routing): Low - novel approach with no validation against dense routing baselines
- Claims about scalability and interpretability: Medium - based on limited experimental scope

## Next Checks
1. Perform an ablation study removing the PI controller to quantify its impact on training stability and final accuracy.
2. Test M2M on a public PDE dataset (e.g., Navier-Stokes from OpenFOAM) to assess generalization beyond custom data.
3. Compare sparse (top-k) routing to dense routing on a coupled PDE system to measure trade-offs in accuracy and computation.