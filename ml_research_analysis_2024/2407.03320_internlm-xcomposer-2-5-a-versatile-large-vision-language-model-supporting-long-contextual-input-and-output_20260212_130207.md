---
ver: rpa2
title: 'InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting
  Long-Contextual Input and Output'
arxiv_id: '2407.03320'
source_url: https://arxiv.org/abs/2407.03320
tags:
- arxiv
- wang
- zhang
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InternLM-XComposer-2.5 (IXC-2.5) is a versatile large vision-language
  model that supports long-context input and output, achieving GPT-4V level performance
  with a 7B LLM backend. IXC-2.5 was trained on 24K interleaved image-text contexts
  and can extend to 96K long contexts via RoPE extrapolation.
---

# InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output

## Quick Facts
- arXiv ID: 2407.03320
- Source URL: https://arxiv.org/abs/2407.03320
- Authors: Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang
- Reference count: 40
- One-line primary result: IXC-2.5 achieves GPT-4V level performance with a 7B LLM backend, outperforming open-source state-of-the-art models on 16 of 28 benchmarks

## Executive Summary
InternLM-XComposer-2.5 (IXC-2.5) is a large vision-language model that pushes the boundaries of long-context understanding and high-resolution processing. Building on its predecessor, IXC-2.5 introduces three major upgrades: ultra-high resolution understanding, fine-grained video understanding, and multi-turn multi-image dialogue. The model leverages a unified dynamic image partition strategy to handle high-resolution inputs, RoPE extrapolation to extend context windows from 24K to 96K tokens, and LoRA-based alignment for efficient fine-tuning. Trained on 24K interleaved image-text contexts, IXC-2.5 demonstrates competitive performance against both open-source and closed-source models on a diverse set of benchmarks.

## Method Summary
IXC-2.5 uses InternLM2-7B as the LLM backend and OpenAI ViT-L/14 as the vision encoder, with Partial LoRA layers for efficient alignment. The model is trained on 24K interleaved image-text contexts and extended to 96K tokens via RoPE extrapolation. A unified dynamic image partition strategy handles high-resolution images and videos by dividing them into manageable patches while preserving global context. The model is fine-tuned on diverse datasets with a batch size of 2048 over 4000 steps, incorporating Chain-of-Thought and Direct Preference Optimization for article composing tasks.

## Key Results
- Outperforms existing open-source state-of-the-art models on 16 out of 28 benchmarks
- Matches or surpasses GPT-4V and Gemini Pro on 16 key tasks
- Achieves GPT-4V level performance with only a 7B LLM backend
- Successfully extends context window from 24K to 96K tokens via RoPE extrapolation

## Why This Works (Mechanism)

### Mechanism 1: Unified Dynamic Image Partition
- Claim: Allows IXC-2.5 to handle high-resolution images and videos by dividing them into manageable patches while preserving global context
- Mechanism: Images are resized and padded to fit into a grid of sub-images, each with 400 tokens. For videos, frames are sampled and concatenated along the short side to form a high-resolution composite image
- Core assumption: The vision encoder can process each sub-image independently and the LLM can integrate these patches into a coherent understanding
- Evidence anchors: [abstract] "Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation." [section 3.2] "For the high-resolution strategy, we unify the different strategies used in the IXC-4KHD into a scaled identity strategy."
- Break condition: If the vision encoder fails to maintain spatial relationships across patches or the LLM cannot integrate the partitioned information effectively

### Mechanism 2: RoPE Extrapolation
- Claim: Enables the model to extend its context window from 24K to 96K tokens without retraining
- Mechanism: The rotary position embedding (RoPE) is extrapolated beyond its original range to accommodate longer sequences
- Core assumption: RoPE extrapolation is mathematically valid for extending position encodings beyond the training range
- Evidence anchors: [abstract] "Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation." [section 3.2] Mentions the use of RoPE extrapolation for extending context.
- Break condition: If extrapolation introduces errors in positional encoding that degrade performance on long sequences

### Mechanism 3: LoRA-Based Alignment
- Claim: Allows efficient fine-tuning of the vision encoder and LLM components without updating all parameters
- Mechanism: Partial LoRA layers are introduced and fine-tuned to align visual tokens with the LLM backend, preserving the original knowledge
- Core assumption: LoRA can effectively adapt the model to new tasks without significant loss of pre-trained capabilities
- Evidence anchors: [section 3.3] "During the pre-training phase, the LLM (InternLM2-7B [143]) is frozen while both the vision encoder and Partial LoRA [33] are fine-tuned." [section 3.4] "For the Partial LoRA [33], we set a rank of 256 for all the linear layers in the LLM decoder block."
- Break condition: If the LoRA parameters become too large, negating the efficiency benefits, or if alignment fails to preserve original model capabilities

## Foundational Learning

- Concept: Vision encoders (e.g., ViT) and their role in processing visual data
  - Why needed here: IXC-2.5 uses a ViT vision encoder to process images and videos
  - Quick check question: What is the role of a vision encoder in a vision-language model?

- Concept: Large Language Models (LLMs) and their integration with vision components
  - Why needed here: The LLM backend (InternLM2-7B) is responsible for understanding and generating text based on visual inputs
  - Quick check question: How do LLMs integrate with vision encoders in multimodal models?

- Concept: Positional encodings and their importance in sequence modeling
  - Why needed here: RoPE extrapolation is used to extend the context window, requiring an understanding of positional encodings
  - Quick check question: What are positional encodings and why are they important in transformers?

## Architecture Onboarding

- Component map: Vision Encoder (ViT-L/14, 560x560 resolution) -> Unified Dynamic Image Partition -> Partial LoRA alignment -> LLM backend (InternLM2-7B)
- Critical path: 1. Image/video input → Unified Dynamic Image Partition 2. Partitioned patches → Vision Encoder 3. Visual tokens → Partial LoRA alignment 4. Aligned tokens → LLM backend 5. LLM generates response
- Design tradeoffs: Using LoRA reduces fine-tuning cost but may limit the model's ability to learn new tasks; Unified Dynamic Image Partition handles high-resolution inputs but adds complexity; RoPE extrapolation extends context but may introduce errors in positional encoding
- Failure signatures: Poor performance on high-resolution images/videos (check vision encoder resolution and partitioning strategy); Hallucinations or incorrect responses (investigate LoRA alignment and preference optimization); Context window limitations (verify RoPE extrapolation implementation)
- First 3 experiments: 1. Test image partitioning with varying resolutions to ensure the vision encoder processes each patch correctly 2. Validate RoPE extrapolation by comparing performance on 24K vs 96K context inputs 3. Assess LoRA alignment by evaluating model performance before and after fine-tuning on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ultra-high resolution understanding capability of IXC-2.5 compare to other state-of-the-art models in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions that IXC-2.5 enhances the dynamic resolution solution with a native 560 × 560 ViT vision encoder, supporting high-resolution images with any aspect ratio
- Why unresolved: While the paper states that IXC-2.5 has improved resolution understanding, it does not provide a direct comparison with other models in terms of accuracy and computational efficiency
- What evidence would resolve it: A comprehensive benchmark comparing IXC-2.5's ultra-high resolution understanding with other state-of-the-art models in terms of accuracy and computational efficiency

### Open Question 2
- Question: What are the limitations of the current long-context input and output capabilities of IXC-2.5, and how can they be further extended?
- Basis in paper: [explicit] The paper mentions that IXC-2.5 can extend to 96K long contexts via RoPE extrapolation, but it does not discuss the limitations of this approach or how it can be further improved
- Why unresolved: The paper does not provide a detailed analysis of the limitations of the current long-context capabilities or potential avenues for further extension
- What evidence would resolve it: A thorough analysis of the limitations of the current long-context capabilities and proposed solutions for extending them further

### Open Question 3
- Question: How does the fine-grained video understanding capability of IXC-2.5 perform on real-world video data, and what are the challenges in handling longer and more complex videos?
- Basis in paper: [explicit] The paper mentions that IXC-2.5 treats videos as ultra-high-resolution composite pictures, allowing it to capture fine details through dense sampling and higher resolution for each frame
- Why unresolved: While the paper describes the approach for fine-grained video understanding, it does not provide experimental results on real-world video data or discuss the challenges in handling longer and more complex videos
- What evidence would resolve it: Experimental results on real-world video data demonstrating the performance of IXC-2.5's fine-grained video understanding and a discussion of the challenges in handling longer and more complex videos

## Limitations
- The Unified Dynamic Image Partition strategy lacks detailed implementation specifics, making it difficult to verify how effectively it handles diverse aspect ratios and resolutions
- The LoRA-based alignment mechanism has no direct empirical validation showing that it preserves pre-trained capabilities while adapting to new tasks
- The RoPE extrapolation for extending context windows from 24K to 96K tokens lacks rigorous analysis of potential positional encoding errors introduced by extrapolation beyond the training range

## Confidence

**High Confidence** in the general architecture and benchmark evaluation approach. The use of established components (InternLM2-7B, ViT-L/14) and the comprehensive benchmark suite (28 tasks) provide solid grounding for the core claims about model capabilities.

**Medium Confidence** in the performance claims relative to GPT-4V and Gemini Pro. While the model reportedly matches or surpasses these closed-source models on 16 key tasks, the comparison methodology and specific evaluation conditions are not fully transparent, making independent verification challenging.

**Low Confidence** in the mechanism descriptions for long-context extension and high-resolution handling. The RoPE extrapolation technique and Unified Dynamic Image Partition strategy are described at a high level without sufficient technical detail to assess their reliability or potential failure modes.

## Next Checks

1. **Positional Encoding Validation**: Conduct controlled experiments comparing model performance at 24K vs 96K context lengths with synthetic data where positional information is critical, to detect any degradation from RoPE extrapolation.

2. **Partition Strategy Robustness**: Test the Unified Dynamic Image Partition with extreme aspect ratios (1:10, 10:1) and varying resolutions to identify edge cases where the scaled identity strategy fails to preserve spatial relationships.

3. **LoRA Alignment Preservation**: Implement ablation studies where the model is evaluated on pre-training tasks before and after LoRA fine-tuning to quantify any catastrophic forgetting or capability degradation.