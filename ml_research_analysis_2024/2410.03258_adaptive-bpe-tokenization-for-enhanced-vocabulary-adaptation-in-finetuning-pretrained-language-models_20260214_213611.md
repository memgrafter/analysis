---
ver: rpa2
title: Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning
  Pretrained Language Models
arxiv_id: '2410.03258'
source_url: https://arxiv.org/abs/2410.03258
tags:
- vocabulary
- adapt
- language
- tokenization
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental limitation in vocabulary adaptation
  approaches for fine-tuning pretrained language models to expert domains. Current
  approaches that append target domain-specific vocabulary to the PLM vocabulary cause
  suboptimal tokenization in Byte-Pair Encoding (BPE), leading to lower priority scores
  for the added vocabulary.
---

# Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models

## Quick Facts
- arXiv ID: 2410.03258
- Source URL: https://arxiv.org/abs/2410.03258
- Authors: Gunjan Balde; Soumyadeep Roy; Mainack Mondal; Niloy Ganguly
- Reference count: 19
- Improves classification accuracy by 3.57% and summarization Rouge-L by 1.87% compared to standard BPE

## Executive Summary
This paper addresses a fundamental limitation in vocabulary adaptation for fine-tuning pretrained language models to expert domains. Current approaches that append target domain-specific vocabulary to the PLM vocabulary cause suboptimal tokenization in Byte-Pair Encoding (BPE), leading to lower priority scores for the added vocabulary. The authors propose AdaptBPE, which modifies the BPE initialization phase to first perform longest string matching on the added vocabulary before tokenizing at the character level. AdaptBPE demonstrates significant improvements in both classification accuracy and summarization quality, particularly excelling in high OOV concentration scenarios.

## Method Summary
AdaptBPE modifies the initialization phase of Byte-Pair Encoding by performing longest substring matching on the added domain-specific vocabulary before character-level tokenization. The approach first checks for the longest substring match in the added vocabulary (V_DOMAIN) and prevents matched substrings from being split into characters. This modification ensures that domain-specific terms receive higher priority in the tokenization process, reducing fragmentation of important vocabulary items. The method is evaluated on RoBERTa-base for classification tasks and BART-large for summarization tasks using domain-specific datasets like AVOCA DO and MEDVOC.

## Key Results
- Improves classification accuracy by 3.57% and summarization Rouge-L by 1.87% compared to standard BPE
- Reduces fragment score by 39.16% (AVOCA DO) and 13.96% (MEDVOC)
- In medical summarization tasks, AdaptBPE generates more faithful summaries (97.5% vs 77.5% positive ratings) and more relevant summaries (82.5% vs 65% positive ratings)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaptBPE mitigates ill-tokenization by prioritizing target domain vocabulary during the initialization phase of BPE.
- Mechanism: Instead of starting tokenization by splitting input words into characters, AdaptBPE performs longest substring matching on the added vocabulary (V_DOMAIN) first, preserving matched substrings from further splitting.
- Core assumption: The order of merge rules in BPE determines token priority, and appending V_DOMAIN at the end gives it lower priority.
- Evidence anchors:
  - [abstract] "Current approaches trivially append the target domain-specific vocabulary at the end of the PLM vocabulary. This approach leads to a lower priority score and causes sub-optimal tokenization in BPE..."
  - [section] "Instead of splitting at the character level at the initialization stage, we first check for the longest substring match (Hofmann et al., 2022) only in the added vocabulary (V_DOMAIN) and prevent the match from splitting into the character level."
- Break condition: If the added vocabulary does not contain meaningful substrings that match the input text, the longest substring matching step will fail to improve tokenization.

### Mechanism 2
- Claim: AdaptBPE reduces the fragment score by correctly tokenizing domain-specific words, leading to improved model performance.
- Mechanism: By prioritizing V_DOMAIN during initialization, AdaptBPE ensures that domain-specific words are tokenized as single tokens rather than being split into multiple subwords.
- Core assumption: A lower fragment score (average number of subwords per word) indicates better tokenization and improved model performance.
- Evidence anchors:
  - [abstract] "AdaptBPE improves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L), respectively."
  - [section] "We observe a significant drop in fragment score (average number of subwords a given word across the entire corpus) of 39.16% and 13.96% in case of AVOCA DO and MEDVOC respectively."
- Break condition: If the reduction in fragment score does not translate to improved downstream task performance, the mechanism may not hold.

### Mechanism 3
- Claim: AdaptBPE generates more relevant and faithful summaries in medical domain tasks.
- Mechanism: By correctly tokenizing medical terms and concepts, AdaptBPE ensures that the generated summaries accurately represent the source document's content.
- Core assumption: Faithful summaries are those that accurately represent the source document's content, and relevant summaries are those that are informative and focused on the input context.
- Evidence anchors:
  - [abstract] "We further perform a human evaluation using medical experts where we observe that AdaptBPE produces more relevant and faithful summaries in the case of MEDVOC."
  - [section] "We further perform a human evaluation using medical experts where we observe that AdaptBPE produces more relevant and faithful summaries in the case of MEDVOC."
- Break condition: If human evaluation does not consistently rate AdaptBPE-generated summaries as more relevant and faithful, the mechanism may not hold.

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE) tokenization
  - Why needed here: Understanding BPE is crucial to grasping how AdaptBPE modifies the initialization phase to prioritize target domain vocabulary.
  - Quick check question: How does BPE tokenization work, and what is the significance of the order of merge rules in determining token priority?

- Concept: Vocabulary adaptation for domain-specific tasks
  - Why needed here: AdaptBPE is designed to improve vocabulary adaptation for expert domains, so understanding the challenges and approaches in this area is essential.
  - Quick check question: What are the main challenges in adapting pretrained language models to expert domains, and how do current vocabulary adaptation approaches address these challenges?

- Concept: Evaluation metrics for text summarization
  - Why needed here: AdaptBPE's performance is evaluated using metrics like Rouge-L and Concept Score, so understanding these metrics is important for interpreting the results.
  - Quick check question: What do Rouge-L and Concept Score measure in the context of text summarization, and how do they help assess the quality of generated summaries?

## Architecture Onboarding

- Component map:
  Input text -> Tokenizer (AdaptBPE) -> Added vocabulary (V_DOMAIN) -> Standard BPE tokenizer -> Output token sequence

- Critical path:
  1. Input text is pre-tokenized based on the tokenizer's rules.
  2. For each word in the pre-tokenized text, AdaptBPE performs longest substring matching on V_DOMAIN.
  3. Matched substrings are preserved from further splitting into characters.
  4. The remaining parts of the word are tokenized using the standard BPE loop.
  5. The final token sequence is generated.

- Design tradeoffs:
  - AdaptBPE adds computational overhead due to the longest substring matching step.
  - The effectiveness of AdaptBPE depends on the quality and coverage of the added vocabulary.
  - AdaptBPE may not work well for large language models that use different tokenization schemes like SentencePiece.

- Failure signatures:
  - If the added vocabulary does not contain meaningful substrings, AdaptBPE may not improve tokenization.
  - If the longest substring matching step is too computationally expensive, it may slow down the tokenization process.
  - If the added vocabulary is too large or contains irrelevant terms, it may negatively impact model performance.

- First 3 experiments:
  1. Compare the fragment scores of AdaptBPE and standard BPE on a sample text with domain-specific terms.
  2. Evaluate the performance of a model fine-tuned with AdaptBPE versus standard BPE on a classification task using a domain-specific dataset.
  3. Conduct a human evaluation of summaries generated by models fine-tuned with AdaptBPE and standard BPE, focusing on relevance and faithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaptBPE perform on large language models (LLMs) like LLaMa or Mistral that use SentencePiece tokenization?
- Basis in paper: [inferred] The authors mention that they limit their evaluation to pretrained language models and observe that AdaptBPE has limitations when applied to LLMs like LLaMa-2-7B, where 27.76% of target domain-specific vocabulary terms are still tokenized into more than one subword.
- Why unresolved: The paper does not provide detailed results or analysis of AdaptBPE's performance on LLMs, focusing instead on models like BART and RoBERTa.
- What evidence would resolve it: Empirical results showing the performance of AdaptBPE on LLMs using SentencePiece tokenization, including fragment score reduction and downstream task improvements.

### Open Question 2
- Question: What are the specific mechanisms by which AdaptBPE improves the tokenization of domain-specific vocabulary compared to standard BPE?
- Basis in paper: [explicit] The paper explains that AdaptBPE modifies the initialization stage of BPE by performing longest substring matching on the added vocabulary before tokenizing at the character level, which mitigates ill-tokenization issues.
- Why unresolved: While the paper describes the mechanism, it does not provide a detailed analysis of how this modification specifically improves tokenization compared to standard BPE.
- What evidence would resolve it: A comparative analysis of tokenization processes, showing how AdaptBPE handles specific examples of domain-specific vocabulary and how this differs from standard BPE.

### Open Question 3
- Question: How does the performance of AdaptBPE vary with different sizes of added vocabulary in vocabulary adaptation strategies?
- Basis in paper: [inferred] The paper mentions that AdaptBPE is independent of the target domain-specific vocabulary construction algorithm, but does not explore how its performance varies with different vocabulary sizes.
- Why unresolved: The paper does not provide a systematic study of AdaptBPE's performance across different vocabulary sizes, which could impact its effectiveness.
- What evidence would resolve it: Empirical results showing AdaptBPE's performance across a range of vocabulary sizes, including fragment score reduction and task-specific improvements.

## Limitations
- AdaptBPE is limited to BPE tokenization and incompatible with other tokenization schemes like SentencePiece or WordPiece
- The approach adds computational overhead from longest substring matching without thorough efficiency evaluation
- Evaluation scope is limited to relatively small-scale datasets and only two vocabulary adaptation approaches

## Confidence

**High Confidence (⊕⊕⊕⊕⊕)**
- AdaptBPE modifies BPE initialization to prioritize target domain vocabulary through longest substring matching
- AdaptBPE reduces fragment scores by 39.16% (AVOCA DO) and 13.96% (MEDVOC)
- AdaptBPE improves classification accuracy by 3.57% and summarization Rouge-L by 1.87% on average

**Medium Confidence (⊕⊕⊕⊕)**
- AdaptBPE generates more faithful summaries (97.5% vs 77.5% positive ratings) in medical domain
- AdaptBPE performs better in high OOV concentration scenarios with 10.41% improvement
- AdaptBPE produces more relevant summaries (82.5% vs 65% positive ratings) in medical domain

**Low Confidence (⊕⊕)**
- AdaptBPE "consistently improves" across all domain adaptation scenarios
- AdaptBPE is efficient and scalable for large-scale applications
- AdaptBPE's improvements generalize to all types of expert domains and tasks

## Next Checks

1. **Runtime Efficiency Analysis**: Measure and compare the tokenization speed of AdaptBPE versus standard BPE across varying vocabulary sizes (100, 1K, 10K, 100K tokens) and document lengths to quantify the computational overhead and identify scalability bottlenecks.

2. **Cross-Domain Generalization Test**: Apply AdaptBPE to a diverse set of expert domains (legal, financial, scientific) and tokenization schemes (SentencePiece, WordPiece) to validate whether the improvements observed in medical and general domains extend to other specialized vocabularies and tokenization methods.

3. **Ablation Study on Longest Substring Matching**: Implement and compare three variants: (a) AdaptBPE with longest substring matching, (b) AdaptBPE with random substring matching, and (c) standard BPE with VDOMAIN appended, to isolate the specific contribution of the longest matching algorithm to the observed performance improvements.