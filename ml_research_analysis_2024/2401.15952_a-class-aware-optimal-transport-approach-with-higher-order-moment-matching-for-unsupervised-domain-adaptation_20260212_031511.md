---
ver: rpa2
title: A Class-aware Optimal Transport Approach with Higher-Order Moment Matching
  for Unsupervised Domain Adaptation
arxiv_id: '2401.15952'
source_url: https://arxiv.org/abs/2401.15952
tags:
- source
- domain
- target
- adaptation
- cloth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLOTH, a class-aware optimal transport approach
  for unsupervised domain adaptation. CLOTH leverages a novel transportation network
  to guide the target examples to move to the appropriate source class regions, effectively
  addressing both data and label shifts between the source and target domains.
---

# A Class-aware Optimal Transport Approach with Higher-Order Moment Matching for Unsupervised Domain Adaptation

## Quick Facts
- arXiv ID: 2401.15952
- Source URL: https://arxiv.org/abs/2401.15952
- Authors: Tuan Nguyen; Van Nguyen; Trung Le; He Zhao; Quan Hung Tran; Dinh Phung
- Reference count: 40
- Achieves state-of-the-art performance on domain adaptation benchmarks, outperforming existing methods by significant margins

## Executive Summary
This paper introduces CLOTH, a novel class-aware optimal transport approach for unsupervised domain adaptation that addresses both data and label shifts between source and target domains. The method leverages a transportation network to guide target examples to appropriate source class regions while incorporating higher-order moment matching to enhance alignment of corresponding class distributions. Experiments on Digits, Office-31, Office-Home, and ImageCLEF-DA datasets demonstrate superior performance, achieving an average accuracy of 94.7% on Office-31, 72.5% on Office-Home, and 91.5% on ImageCLEF-DA.

## Method Summary
CLOTH combines class-aware optimal transport with higher-order moment matching to align source and target domains in unsupervised domain adaptation. The method constructs source class-conditional distributions in latent space and computes optimal transport distances between these and a mixture of source/target distributions. An amortized transportation network efficiently approximates the optimal transport matrix, while higher-order moment matching captures complex distribution structures beyond first-order alignment. The approach jointly optimizes classification, adversarial domain alignment, transportation probabilities, and higher-order moment consistency through a unified objective function.

## Key Results
- Achieves 94.7% average accuracy on Office-31, 72.5% on Office-Home, and 91.5% on ImageCLEF-DA
- Outperforms existing baselines by significant margins across all benchmark datasets
- Demonstrates stable performance across different orders of higher-order moment matching (q = 1 to 6)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-aware optimal transport directly aligns target samples to correct source class regions by incorporating label information into transport cost
- Mechanism: Constructs distribution of source class-conditional distributions and computes OT distance between this and mixture of source/target distributions, using cost function `-log pS_m(xi)` to guide target examples toward appropriate class regions
- Core assumption: Source class-conditional distributions in latent space are sufficiently separable to serve as reliable anchors
- Break condition: If source classes are highly overlapping in latent space, class-conditional distributions may not provide reliable guidance

### Mechanism 2
- Claim: Amortized transportation network provides efficient approximation of optimal transport matrix without computational burden of Sinkhorn iterations
- Mechanism: Neural network T(G(xi)) predicts transportation probabilities for each class, learning to amortize OT solution across batches
- Core assumption: Transportation network T has sufficient capacity to approximate optimal transport matrix A*
- Break condition: If T lacks sufficient capacity or OT problem is too complex, amortization may produce poor approximations

### Mechanism 3
- Claim: Class-aware higher-order moment matching captures complex distribution structures that first-order alignment misses
- Mechanism: Computes higher-order moments (q-order) between source class-conditional distributions and target distributions weighted by transportation probabilities, reducing complexity from O(p^q) to O(p) via dot product properties
- Core assumption: Higher-order moments contain discriminative information that improves class alignment beyond Wasserstein distance
- Break condition: If higher-order moments don't capture meaningful structure or q is too high for batch size, matching may become noisy

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Provides mathematical foundation for measuring distribution distances and finding optimal mappings between source and target domains
  - Quick check question: What is the key difference between computing Wasserstein distance between two empirical distributions versus between a distribution and a distribution of distributions?

- Concept: Amortized Inference
  - Why needed here: Enables efficient approximation of complex optimal transport solutions without expensive iterative algorithms like Sinkhorn
  - Quick check question: How does the capacity of the transportation network T affect the quality of the OT solution approximation?

- Concept: Higher-Order Statistical Moments
  - Why needed here: Captures fine-grained distribution characteristics beyond mean and covariance crucial for precise class alignment
  - Quick check question: Why does the proposed method reduce computational complexity from O(p^q) to O(p) for higher-order moment calculations?

## Architecture Onboarding

- Component map: Input → G → [C, D, T] → Losses (LC, LG,S, LG,T, Lt, Lent, LHM) → Backpropagation
- Critical path: Feature extractor processes input, then classifier, discriminator, and transportation network operate in parallel to compute losses for backpropagation
- Design tradeoffs:
  - Multi-class vs binary discriminator: Multi-class provides better class-aware alignment but requires more parameters
  - q-order selection: Higher q captures more distribution structure but requires larger batch sizes
  - Transportation network capacity: Higher capacity improves OT approximation but increases computational cost
- Failure signatures:
  - Poor performance despite training: Check if source classes are separable in latent space
  - Unstable training: Verify that adversarial losses are properly balanced
  - No improvement over source-only: Ensure transportation network is learning meaningful probabilities
- First 3 experiments:
  1. Train with only LC (source classifier) to establish baseline
  2. Add adversarial training (LG,S + LG,T + LD) to test domain alignment capability
  3. Include transportation loss Lt to verify class-aware OT is working before adding higher-order moments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of order q in higher-order moment matching affect CLOTH's performance on different domain adaptation tasks?
- Basis in paper: Performance remains stable with q ranging from 1 to 6, best at q = 3, but drops significantly for q ≥ 7 due to small batch size limitations
- Why unresolved: Paper doesn't analyze impact of q across different datasets or tasks
- What evidence would resolve it: Experiments on various datasets with different q values and analysis of performance trends

### Open Question 2
- Question: How does the proposed class-aware OT approach compare to other OT-based methods in computational efficiency and scalability to large-scale datasets?
- Basis in paper: Proposes amortization solution using deep neural networks to handle class-aware OT efficiently, but doesn't directly compare computational efficiency and scalability
- Why unresolved: Unclear how method performs in computational resources and scalability compared to existing OT-based methods on large-scale datasets
- What evidence would resolve it: Experiments comparing computational time and memory usage with other OT-based approaches on large-scale datasets

### Open Question 3
- Question: How does CLOTH's performance vary when using different feature extractors or when feature extractor is trained jointly with other components?
- Basis in paper: Uses pre-extracted ResNet-50 features for most datasets and modified LeNet for Digits, but doesn't explore impact of different feature extractors or joint training
- Why unresolved: Unclear whether performance is sensitive to feature extractor choice and whether joint training can improve results
- What evidence would resolve it: Experiments with different feature extractors and comparison of pre-trained vs jointly trained feature extractors

## Limitations

- Assumes source class-conditional distributions are sufficiently separable in latent space to serve as reliable anchors for target alignment
- Exponential computational complexity of higher-order moment matching limits practical applications to relatively low orders
- Requires careful hyperparameter tuning, particularly for order q and trade-off between different loss components

## Confidence

- High Confidence: Class-aware optimal transport framework and integration with source class-conditional distributions is theoretically sound and well-supported by optimal transport theory
- Medium Confidence: Effectiveness of higher-order moment matching for domain adaptation may vary significantly depending on dataset characteristics and chosen order q
- Low Confidence: Claim of achieving state-of-the-art performance across all benchmark datasets requires careful scrutiny due to potential influence of experimental conditions and implementation details

## Next Checks

1. **Ablation Study on Source Class Separability**: Evaluate CLOTH's performance when source classes are intentionally made more overlapping in latent space to test break condition where class-conditional distributions no longer provide reliable guidance

2. **Transportation Network Capacity Analysis**: Systematically vary capacity of transportation network T and measure resulting OT approximation quality and final adaptation performance to quantify impact of amortization assumption

3. **Higher-Order Moment Order Sensitivity**: Conduct experiments across range of q values (1, 2, 3, 4) on each dataset to identify optimal order and test whether reported performance gains are robust to this critical hyperparameter choice