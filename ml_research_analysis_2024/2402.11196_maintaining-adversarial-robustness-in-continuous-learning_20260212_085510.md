---
ver: rpa2
title: Maintaining Adversarial Robustness in Continuous Learning
arxiv_id: '2402.11196'
source_url: https://arxiv.org/abs/2402.11196
tags:
- learning
- robustness
- adversarial
- neural
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of maintaining adversarial robustness
  during continuous learning when previous data cannot be revisited. It proposes Double
  Gradient Projection (DGP), a novel gradient projection technique that stabilizes
  sample gradients from previous tasks by orthogonally projecting back-propagation
  gradients onto a crucial subspace before using them for weight updates.
---

# Maintaining Adversarial Robustness in Continuous Learning

## Quick Facts
- arXiv ID: 2402.11196
- Source URL: https://arxiv.org/abs/2402.11196
- Reference count: 40
- The paper proposes Double Gradient Projection (DGP) to maintain adversarial robustness during continuous learning when previous data cannot be revisited

## Executive Summary
This paper addresses the critical challenge of maintaining adversarial robustness during continuous learning when access to previous task data is restricted. The proposed Double Gradient Projection (DGP) method orthogonally projects weight update gradients onto a subspace that preserves the smoothness of sample gradients from previous tasks. Through experiments on four benchmarks including Split-CIFAR100 and Split-miniImageNet, DGP demonstrates superior performance in maintaining robustness against strong adversarial attacks while learning new tasks sequentially.

## Method Summary
DGP stabilizes sample gradients from previous tasks by orthogonally projecting back-propagation gradients onto a crucial subspace before weight updates. The method performs SVD on layer-wise outputs and gradients to extract orthogonal bases, then projects new gradients onto the complement of these bases. This preserves both output stability and gradient smoothness, enabling collaboration with defense algorithms that enhance robustness through sample gradient smoothing. DGP effectively resolves conflicts between continual learning and adversarial defense by preventing interference between plasticity for new tasks and stability for robustness preservation.

## Key Results
- DGP maintains adversarial robustness during continual learning, showing effective mitigation of rapid degradation even under strong attacks like AutoAttack, PGD, and FGSM
- The method demonstrates superior performance compared to baseline combinations, particularly in preserving robustness while maintaining comparable classification accuracy on original samples
- Experiments on four benchmarks (Split-CIFAR100, Split-miniImageNet, and two others) validate DGP's effectiveness across different datasets and task configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stabilizing sample gradients from previous tasks preserves adversarial robustness during continual learning
- Mechanism: Orthogonally projects weight update gradients onto a subspace that minimally affects smoothed gradients of previous tasks, ensuring robustness properties learned earlier are maintained
- Core assumption: Adversarial robustness is primarily determined by sample gradient smoothness, and unchanged gradients after weight updates preserve robustness
- Evidence anchors:
  - [abstract] "DGP is grounded on a theoretical hypothesis that a neural network's robustness can be maintained if the smoothness of sample gradients from previous data remain unchanged after weight updates"
  - [section] "The fundamental principle of our algorithm is concise: stabilizing the smoothed sample gradients"
- Break condition: If subspace approximation via SVD becomes too coarse or gradient smoothing property changes fundamentally between tasks

### Mechanism 2
- Claim: DGP maintains both output stability and gradient stability using two distinct base vector sets
- Mechanism: Performs SVD on layer-wise outputs and gradients of outputs with respect to samples, creating orthogonal subspaces that together constrain weight updates to preserve final predictions and gradient smoothness
- Core assumption: Combination of output stability and gradient stability is necessary to fully preserve robustness properties learned in previous tasks
- Evidence anchors:
  - [abstract] "This technique can maintaining robustness by collaborating with a class of defense algorithms through sample gradient smoothing"
  - [section] "We propose a novel gradient projection technique called Double Gradient Projection (DGP), which inherently enables collaboration with a class of defense algorithms that enhance robustness through sample gradient smoothing"
- Break condition: If two sets of base vectors become linearly dependent or SVD truncation loses critical information

### Mechanism 3
- Claim: Method avoids conflict between continual learning and adversarial defense by projecting gradients orthogonal to both task-specific and robustness-specific bases
- Mechanism: Constructs base vector pool from previous tasks and projects new gradients onto their orthogonal complement, preventing interference between plasticity needed for new tasks and stability needed for robustness
- Core assumption: Direct combination of existing continual learning and defense algorithms leads to conflicting gradient directions, which DGP resolves through orthogonal projection
- Evidence anchors:
  - [abstract] "the experiment results indicate that without a tailored design, direct combination of existing continual learning and defense algorithms into the training procedure can be conflicting, resulting that the efficacy of the former is seriously weakened"
  - [section] "However, direct combinations of existing defense and continuous learning algorithms fail to effectively address this issue, and may even give rise to conflicts between them"
- Break condition: If number of base vectors becomes too large relative to model's capacity, projection may overly restrict learning and degrade performance

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) for subspace approximation
  - Why needed here: SVD extracts most important directions in layer-wise outputs and gradients, forming subspaces for orthogonal projection
  - Quick check question: What does SVD provide that makes it suitable for identifying the "crucial subspace" mentioned in the paper?

- Concept: Gradient projection for continual learning
  - Why needed here: Method builds on existing gradient projection techniques but extends them to handle both output and gradient stability simultaneously
  - Quick check question: How does gradient projection differ from regularization-based approaches in continual learning?

- Concept: Adversarial training and input gradient regularization
  - Why needed here: These defense mechanisms whose robustness properties DGP aims to preserve during continual learning
  - Quick check question: What is the key difference between how adversarial training and input gradient regularization achieve gradient smoothing?

## Architecture Onboarding

- Component map: Neural network trained on sequential tasks -> Base vector pool (P) accumulates orthogonal bases from previous tasks -> Projection mechanism modifies gradients during training
- Critical path: For each new task: (1) Train network with gradient projection using existing bases, (2) After training, perform SVD on layer-wise outputs and gradients to extract new bases, (3) Project new bases orthogonally onto existing bases to remove redundancy, (4) Add bases to the pool
- Design tradeoffs: Trades some learning plasticity (due to gradient constraints) for robustness preservation; number of base vectors stored affects both memory usage and strength of stability guarantee
- Failure signatures: Rapid decrease in robustness metrics (accuracy on adversarial examples) after learning new tasks, or significant degradation in accuracy on original samples indicating overly restrictive projections
- First 3 experiments:
  1. Verify gradient projection works on simple fully-connected network on Permuted MNIST with IGR defense
  2. Test dual projection (output and gradient stability) on Split-CIFAR100 to confirm both aspects are preserved
  3. Compare DGP against baseline combinations (e.g., EWC+IGR) on Split-miniImageNet to demonstrate conflict resolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DGP method maintain adversarial robustness against stronger attacks (e.g., CW attack) when more tasks are added?
- Basis in paper: [inferred] Paper shows DGP maintains robustness against AutoAttack, PGD, and FGSM, but mentions limitations when perturbation sizes are increased or more tasks are added
- Why unresolved: Paper only tests against AutoAttack, PGD, and FGSM, and notes that robustness decreases with stronger attacks or more tasks
- What evidence would resolve it: Testing DGP against stronger attacks like CW attack across more tasks and measuring degradation in robustness

### Open Question 2
- Question: How does computational overhead of DGP scale with number of tasks and model size?
- Basis in paper: [explicit] Paper mentions SVD computations after each task and provides time complexity analysis, but doesn't report actual runtime measurements
- Why unresolved: Paper provides theoretical complexity analysis but no empirical measurements of training time or memory usage
- What evidence would resolve it: Reporting actual training times and memory usage for DGP compared to baselines across different numbers of tasks and model sizes

### Open Question 3
- Question: Can DGP be extended to handle class-incremental learning where new classes are added over time?
- Basis in paper: [inferred] Paper focuses on task-incremental learning with disjoint tasks and independent classifiers, but doesn't address scenarios where new classes are introduced
- Why unresolved: Paper's experimental setup uses disjoint tasks with independent classifiers, which doesn't test method's applicability to more challenging class-incremental scenarios
- What evidence would resolve it: Implementing DGP for class-incremental learning and evaluating its performance when new classes are added over time

### Open Question 4
- Question: What is the impact of DGP on model's ability to learn new, complex tasks that are significantly different from previous tasks?
- Basis in paper: [explicit] Paper mentions that DGP's effectiveness in maintaining robustness might restrict plasticity of neural network, potentially reducing its ability to learn new tasks
- Why unresolved: Paper speculates about this limitation but doesn't provide empirical evidence or quantification of how DGP affects learning new complex tasks
- What evidence would resolve it: Conducting experiments where DGP is tested on learning tasks that are significantly different or more complex than previous ones, and measuring impact on learning performance

## Limitations
- Theoretical analysis relies on linear approximations that may not hold for deep networks under strong attacks
- Memory overhead grows with number of tasks due to base vector accumulation
- SVD truncation parameters (k) are fixed and may not adapt well to varying task difficulty

## Confidence
- **High confidence**: DGP's effectiveness in maintaining robustness on benchmark datasets (experimental results show consistent improvement over baselines)
- **Medium confidence**: Claim that gradient smoothness preservation directly translates to robustness maintenance (strong empirical support but indirect theoretical link)
- **Medium confidence**: Assertion that direct combination of CL and defense methods fails (supported by ablation studies but limited to tested combinations)

## Next Checks
1. Test DGP on non-image domains (e.g., text or tabular data) to verify generalizability beyond computer vision
2. Analyze memory scaling by evaluating performance on task sequence with >10 tasks to assess long-term stability
3. Compare against online replay methods that store small subset of previous samples to isolate benefit of gradient projection vs. data replay