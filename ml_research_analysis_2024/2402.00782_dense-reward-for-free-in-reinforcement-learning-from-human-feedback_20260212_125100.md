---
ver: rpa2
title: Dense Reward for Free in Reinforcement Learning from Human Feedback
arxiv_id: '2402.00782'
source_url: https://arxiv.org/abs/2402.00782
tags:
- reward
- learning
- rlhf
- arxiv
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Attention Based Credit (ABC), a method that
  leverages attention weights from a reward model to distribute scalar rewards across
  individual tokens during RLHF fine-tuning of language models. By using the attention
  map from the last layer of the reward model, ABC creates a dense reward signal that
  highlights the most relevant tokens, making credit assignment easier and improving
  the efficiency of reinforcement learning.
---

# Dense Reward for Free in Reinforcement Learning from Human Feedback

## Quick Facts
- **arXiv ID**: 2402.00782
- **Source URL**: https://arxiv.org/abs/2402.00782
- **Reference count**: 40
- **Primary result**: Introduces Attention Based Credit (ABC) method using attention weights to densify rewards in RLHF, improving stability and learning efficiency.

## Executive Summary
This paper addresses the challenge of sparse rewards in Reinforcement Learning from Human Feedback (RLHF) by introducing Attention Based Credit (ABC), a method that leverages attention weights from a reward model to distribute scalar rewards across individual tokens during RLHF fine-tuning of language models. ABC uses the attention map from the last layer of the reward model to identify which tokens are most relevant to the final reward prediction, creating a dense reward signal that highlights the most relevant tokens. This approach makes credit assignment easier and improves the efficiency of reinforcement learning. The method is theoretically equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged, and empirically demonstrates faster, more stable training with reduced variance in value function estimates.

## Method Summary
The ABC method extracts attention weights from the last layer of a trained reward model and uses them to redistribute the final scalar reward across individual tokens in the generated sequence. For each token, the method scales the final reward by its attention weight, creating a token-level dense reward signal. This dense reward is then used in the PPO training loop instead of the sparse reward. The approach is theoretically grounded in potential-based reward shaping, where the token-level reward can be expressed as the original sparse reward plus a potential difference across time steps, ensuring policy invariance under optimal conditions.

## Key Results
- ABC achieves higher rewards at lower KL divergences compared to vanilla RLHF across multiple tasks
- Training with ABC is faster and more stable, particularly for longer generations where sparse rewards typically cause instability
- Denser rewards from ABC significantly reduce variance in value function estimates, leading to more stable policy gradient updates
- ABC demonstrates robustness to longer generations, where sparse rewards typically cause instability

## Why This Works (Mechanism)

### Mechanism 1
Attention weights from the last layer of the reward model can redistribute scalar reward across tokens to create a denser signal. The reward model's attention map identifies which tokens are most relevant to the final reward prediction. By scaling the final reward with these attention weights and assigning them at each token step, credit is assigned earlier and more granularly. Core assumption: Attention weights in the final layer correlate with token importance for the scalar reward output.

### Mechanism 2
The dense reward created by ABC is equivalent to potential-based reward shaping, preserving optimal policy. The token-level reward can be expressed as the original sparse reward plus a potential difference across time steps, ensuring policy invariance under optimal conditions. Core assumption: There exists a potential function Φ(s) such that the token reward difference matches the attention-weighted final reward.

### Mechanism 3
Dense rewards lower variance in value function estimates, improving policy gradient stability. By providing immediate token-level reward, the λ-return variance decreases, leading to more stable value head training and better policy loss estimates. Core assumption: Token-level rewards reduce variance in the Monte Carlo returns used in PPO.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: ABC is an extension to RLHF that densifies rewards; understanding RLHF setup is prerequisite.
  - Quick check question: In RLHF, at what point does the model receive its reward signal?

- **Concept**: Transformer attention mechanisms
  - Why needed here: ABC relies on interpreting attention weights as token relevance indicators.
  - Quick check question: What is the mathematical form of scaled dot-product attention?

- **Concept**: Potential-based reward shaping
  - Why needed here: Theoretical equivalence of ABC to shaping ensures policy invariance.
  - Quick check question: What condition must a shaping reward satisfy to preserve optimal policy?

## Architecture Onboarding

- **Component map**: Reward Model (with attention layers) -> Generator (LLM policy) -> PPO Trainer (accepts per-token rewards) -> Tokenization layer (shared between reward and generator)
- **Critical path**: 1. Generate completion from current policy, 2. Compute final reward rC from reward model, 3. Extract attention weights from last layer, 4. Distribute reward across tokens using weights, 5. Feed per-token rewards to PPO trainer
- **Design tradeoffs**: Using last-layer attention vs. earlier layers (last layer likely most correlated with final output but may be noisier); uniform smoothing vs. attention-based (uniform is simpler but loses token relevance signal); beta hyperparameter balances shaping strength vs. original reward magnitude
- **Failure signatures**: Unstable training (likely due to attention weights not correlating with reward); no improvement over vanilla RLHF (attention redistribution may be too weak or reward model poorly trained); overfitting to reward model (policy optimizes spurious attention patterns)
- **First 3 experiments**: 1. Replace sparse reward with uniform per-token reward and compare PPO loss, 2. Implement ABC with beta=1 and test if optimal policy changes (policy evaluation test), 3. Sweep beta values and measure reward-KL frontier shift

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Attention Based Credit (ABC) method scale to extremely large language models (e.g., 100B+ parameters) in terms of computational efficiency and memory requirements?
- **Basis in paper**: The paper discusses ABC's applicability to models like GPT-2, GPT-J, and OpenLLaMA, but does not explicitly test or discuss its performance on models with 100B+ parameters.
- **Why unresolved**: The paper focuses on smaller to medium-sized models, and while it mentions the potential for scaling, it does not provide empirical evidence or theoretical analysis for extremely large models.
- **What evidence would resolve it**: Experimental results demonstrating the computational efficiency and memory requirements of ABC when applied to models with 100B+ parameters, along with a comparison to vanilla RLHF on the same scale.

### Open Question 2
- **Question**: What is the impact of using attention maps from intermediate layers of the reward model instead of the final layer in the ABC method?
- **Basis in paper**: The paper mentions that the attention map from the last layer of the reward model is used but notes that exploring attention maps from other layers could be interesting.
- **Why unresolved**: The paper does not provide experimental results or theoretical justification for why the last layer is chosen or what the impact of using intermediate layers might be.
- **What evidence would resolve it**: Empirical results comparing the performance of ABC using attention maps from different layers of the reward model, along with an analysis of how the choice of layer affects the reward distribution and learning efficiency.

### Open Question 3
- **Question**: How does the ABC method perform in multi-turn dialogue scenarios where the context window is extended over multiple exchanges?
- **Basis in paper**: The paper discusses single-turn dialogue but does not explicitly test or analyze the performance of ABC in multi-turn dialogue scenarios.
- **Why unresolved**: The paper focuses on single-turn dialogue, and while it suggests that ABC could scale to modern preference datasets, it does not provide evidence for its effectiveness in multi-turn settings.
- **What evidence would resolve it**: Experimental results showing the performance of ABC in multi-turn dialogue scenarios, including metrics such as response quality, coherence, and user satisfaction, compared to vanilla RLHF and other methods.

## Limitations

- The theoretical equivalence to potential-based reward shaping relies on a fixed, stationary reward model, which may not hold if the reward model is updated online during training
- The assumption that last-layer attention weights reliably indicate token-level reward relevance is not empirically validated and may capture syntactic or representational importance rather than reward-predictive importance
- The paper lacks ablation studies on the choice of attention layer or normalization scheme, and does not systematically explore the optimal tuning of the beta hyperparameter across tasks

## Confidence

- **High confidence**: The empirical claim that ABC improves reward-KL tradeoff and stabilizes training is supported by multiple datasets and consistent reward curves
- **Medium confidence**: The claim that attention weights from the last layer correlate with token importance for reward is plausible but not directly validated
- **Low confidence**: The theoretical claim of equivalence to potential-based shaping is mathematically sound but relies on assumptions about the reward model's stationarity and the existence of a valid potential function under all conditions

## Next Checks

1. **Validate attention-to-reward correlation**: For a trained reward model, compute the correlation between individual token attention weights and the gradient of the reward with respect to each token embedding. This would empirically test whether attention weights align with reward-relevant tokens.

2. **Test shaping policy invariance**: Implement a variant where the shaping reward is computed using an older version of the reward model (frozen during shaping) to ensure the potential function is stationary. Compare learned policies to confirm invariance under optimal conditions.

3. **Ablation on attention layer depth**: Train ABC with attention weights from different layers (e.g., first, middle, last) and compare reward-KL curves and training stability to isolate the effect of using last-layer attention specifically.