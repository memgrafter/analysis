---
ver: rpa2
title: How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized
  Dataset
arxiv_id: '2404.00401'
source_url: https://arxiv.org/abs/2404.00401
tags:
- table
- dataset
- caption
- scientific
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SciTabQA, a new dataset of 822 question-answer
  pairs based on scientific tables and their descriptions from computer science papers.
  The dataset focuses on hybrid QA over scientific data requiring numerical reasoning
  and understanding of both structured tables and unstructured text.
---

# How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset

## Quick Facts
- arXiv ID: 2404.00401
- Source URL: https://arxiv.org/abs/2404.00401
- Reference count: 0
- Even best model achieves only 46.2% F1 score on scientific hybrid QA

## Executive Summary
This work introduces SciTabQA, a new dataset of 822 question-answer pairs based on scientific tables and their descriptions from computer science papers. The dataset focuses on hybrid QA over scientific data requiring numerical reasoning and understanding of both structured tables and unstructured text. The authors evaluate three state-of-the-art tabular QA models (TAPAS, TAPEX, OmniTab) and find that even the best model achieves only 46.2% F1 score, indicating the challenging nature of scientific hybrid QA. Surprisingly, adding captions and descriptions generally degrades performance due to truncation and increased noise. The dataset and analysis reveal the need for better models to handle complex reasoning over scientific heterogeneous data.

## Method Summary
The authors create SciTabQA dataset from scientific tables and descriptions in computer science papers, containing 822 QA pairs. They fine-tune three pre-trained tabular QA models (TAPAS, TAPEX, OmniTab) on this dataset and evaluate them across different input settings (table only, table+caption, table+caption+description). Performance is measured using Exact Match and F1 scores, with analysis by question type and truncation effects. The study systematically examines how additional textual context affects model performance.

## Key Results
- Best model (OmniTab) achieves only 46.2% F1 score on scientific hybrid QA
- Adding captions and descriptions generally degrades performance due to truncation and noise injection
- Fine-tuning on WikiTableQuestions improves performance compared to zero-shot inference
- Models struggle particularly with questions requiring both tabular and textual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific tables introduce higher complexity due to non-uniform structure, nested headers, and domain-specific terminology, which standard table QA models are not optimized for.
- Mechanism: The models are pre-trained on highly structured tables (Wikipedia, financial tables) that have clean layouts, consistent headers, and simpler semantic relationships. Scientific tables often contain nested headers, inconsistent formatting, and terminology requiring domain knowledge, making them harder to parse and reason over.
- Core assumption: Scientific tables inherently differ structurally and semantically from tables in standard QA datasets.
- Evidence anchors:
  - [abstract] "hybrid QA over scientific data requiring numerical reasoning and understanding of both structured tables and unstructured text"
  - [section] "Scientific tables may not be structured... This distinguishes our dataset, as there is a lack of hybrid datasets where the tables come in various formats."

### Mechanism 2
- Claim: Adding captions and descriptions degrades performance due to truncation and noise injection.
- Mechanism: The baseline models have fixed input token limits (TAPAS: 512, TAPEX/OmniTab: 1024). When captions/descriptions are appended, the model must truncate the table or text, losing critical information. Additionally, irrelevant textual context can distract the model from the table's key signals.
- Core assumption: Input truncation directly harms performance more than the potential benefit of added context.
- Evidence anchors:
  - [section] "To incorporate the extra information... we append them to the question... In such cases, we avoid truncating the table, only truncating the caption and description instead."
  - [section] "Truncation of the input data is another major issue... For TAPAS, around a third of inputs are truncated when caption and description are added."

### Mechanism 3
- Claim: Fine-tuning on WikiTableQuestions improves performance compared to zero-shot inference.
- Mechanism: Pre-trained models have learned general table understanding patterns, but domain adaptation to scientific tables via fine-tuning is necessary to capture domain-specific semantics and reasoning patterns.
- Core assumption: Fine-tuning on task-specific data provides better adaptation than relying solely on pre-training.
- Evidence anchors:
  - [section] "We have considered metrics used in standard question-answering... For TAPAS, TAPEX and OmniTab, we have fine-tuned on the training dataset."
  - [section] "We checked the results for directly inferring the fine-tuned checkpoints on our test set... the results are much poorer, and thus training on our dataset improves the performance of the models."

## Foundational Learning

- Concept: Understanding of table structure and representation in NLP models
  - Why needed here: Models must parse and reason over tabular data, which requires encoding rows, columns, and cell relationships
  - Quick check question: What are the key differences between encoding free text and encoding tabular data in transformer models?

- Concept: Numerical reasoning and arithmetic operations in QA
  - Why needed here: Many scientific questions require computing values, percentages, or comparisons based on table data
  - Quick check question: How would you design a model to handle "What percentage of instances showed improvement?" given a table of measurements?

- Concept: Handling heterogeneous data (structured tables + unstructured text)
  - Why needed here: Questions require integrating information from both the table and its description/caption
  - Quick check question: What are the challenges in combining structured and unstructured data for a single QA task?

## Architecture Onboarding

- Component map: Input encoder -> Tokenization layer -> Pre-trained backbone -> Output layer -> Truncation handler
- Critical path: 1. Input preprocessing (tokenization, structure encoding) 2. Model inference (forward pass through backbone) 3. Answer generation (cell selection, aggregation, or text span extraction)
- Design tradeoffs:
  - Token limit vs. information retention: Higher limits allow more context but increase computational cost
  - Structure encoding vs. simplicity: More complex table encoding captures relationships better but may overfit
  - Fine-tuning vs. zero-shot: Fine-tuning adapts to domain but requires labeled data
- Failure signatures:
  - Low performance on questions requiring caption/description context
  - Significant drop when input exceeds token limits
  - Poor handling of nested headers or irregular table structures
- First 3 experiments:
  1. Test baseline models on a subset of tables without truncation to measure maximum achievable performance
  2. Evaluate models with different truncation strategies (truncate table vs. truncate text) to find optimal balance
  3. Train a simple classifier to predict when captions/descriptions are likely to help vs. hurt performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would scientific table QA models perform if ground truth captions and descriptions were unavailable?
- Basis in paper: [explicit] The authors note in the Limitations section that their dataset had ground truth information available and suggest studying model performance without it.
- Why unresolved: The current evaluation relies on having high-quality captions and descriptions, but real-world scenarios may not provide this.
- What evidence would resolve it: Experiments testing models on tables with automatically generated or no captions/descriptions, comparing performance to the ground truth setup.

### Open Question 2
- Question: Do the findings about performance degradation from adding captions/descriptions generalize to other scientific domains beyond Computer Science?
- Basis in paper: [inferred] The authors mention their dataset focuses on a narrow CS domain and plan to check if findings generalize to other domains.
- Why unresolved: The current study is limited to one domain, and performance may vary significantly across different scientific fields.
- What evidence would resolve it: Replicating the experiments on datasets from other scientific domains (e.g., biology, physics) and comparing results.

### Open Question 3
- Question: What is the impact of table structure complexity on QA model performance in scientific tables?
- Basis in paper: [explicit] The authors note their dataset includes tables with various formats and structures, distinguishing it from highly structured financial datasets.
- Why unresolved: The paper doesn't analyze how different table structures (e.g., nested headers, irregular formatting) affect model performance.
- What evidence would resolve it: Detailed analysis correlating performance metrics with specific table structural features, potentially through systematic table structure variations.

### Open Question 4
- Question: Can hybrid QA models be improved by incorporating domain-specific scientific knowledge?
- Basis in paper: [inferred] The dataset involves complex scientific reasoning, and current models struggle with it, suggesting domain knowledge could help.
- Why unresolved: The evaluation uses general-purpose models without specialized scientific knowledge integration.
- What evidence would resolve it: Experiments comparing standard models with those augmented by scientific knowledge bases or domain-specific pre-training.

## Limitations

- The dataset size (822 QA pairs) is relatively small for deep learning model evaluation, potentially limiting generalizability
- The study focuses only on computer science papers, which may not represent the full diversity of scientific table structures
- Evaluation relies on EM and F1 metrics, which may not capture the full complexity of scientific reasoning requirements

## Confidence

- **High confidence**: The core finding that current models struggle with scientific hybrid QA (46.2% F1 score clearly demonstrates this)
- **Medium confidence**: The claim that captions/descriptions degrade performance, as the mechanism (truncation/noise) is plausible but could benefit from more controlled experiments
- **Medium confidence**: The dataset's representativeness for scientific tables, given the limited domain scope

## Next Checks

1. Test models on tables from other scientific domains (biology, medicine, physics) to assess domain generalization
2. Conduct ablation studies varying truncation strategies systematically to isolate the exact impact of input length vs. content relevance
3. Compare model performance on questions that require only tabular information vs. those requiring hybrid reasoning to quantify the additional difficulty of the latter