---
ver: rpa2
title: 'Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation,
  Benchmark, and Arena'
arxiv_id: '2406.07545'
source_url: https://arxiv.org/abs/2406.07545
tags:
- questions
- question
- answer
- open-style
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new open-style question benchmark and leaderboard
  for LLM evaluation to address inherent selection bias and random guessing issues
  in multiple-choice question (MCQ) formats. The authors design a multi-stage filtering
  protocol to identify suitable open-style questions and use GPT-4 for answer validation.
---

# Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena

## Quick Facts
- arXiv ID: 2406.07545
- Source URL: https://arxiv.org/abs/2406.07545
- Authors: Aidar Myrzakhan; Sondos Mahmoud Bsharat; Zhiqiang Shen
- Reference count: 40
- Primary result: Open-style questions significantly reduce selection bias and random guessing, with a 25% average accuracy drop compared to MCQs.

## Executive Summary
This paper introduces the Open-LLM-Leaderboard, a new evaluation framework that shifts from multiple-choice questions (MCQs) to open-style questions for assessing large language models (LLMs). The authors argue that MCQs suffer from inherent selection bias and random guessing issues, which can be eliminated by requiring models to generate answers rather than select from fixed options. A multi-stage filtering protocol is used to identify suitable open-style questions, and GPT-4 is employed for automated answer validation. The leaderboard tracks performance across various LLMs, showing significant accuracy drops for open-style questions compared to MCQs, validating the increased challenge and reduced bias of the new format.

## Method Summary
The method involves collecting MCQs from 9 datasets, applying a multi-stage filtering protocol to identify convertible questions, and transforming them into open-style format. GPT-4 is then used to evaluate LLM responses to these open-style questions, with a custom prompt ensuring alignment with human judgments. The leaderboard assembles results and ranks models based on their accuracy in answering open-style questions, providing a faster, automated, and less biased alternative to existing MCQ-based leaderboards.

## Key Results
- Open-style questions show a 25% average accuracy drop compared to MCQs, validating their increased challenge.
- GPT-4 evaluation of open-style answers aligns with human judgments (Cohen's kappa = 0.83).
- GPT-4o achieves the highest accuracy (70.15%) on the open-style benchmark.
- Almost all models struggle with MedMCQA dataset questions, showing accuracy below 5%.

## Why This Works (Mechanism)

### Mechanism 1
Converting MCQs to open-style questions eliminates selection bias from fixed answer ordering. By requiring models to generate answers rather than select from fixed options, the model can no longer exploit position-based or alphabetical biases in the answer choices. This works under the assumption that open-style questions preserve the core knowledge required while removing reliance on choice IDs.

### Mechanism 2
GPT-4 evaluation of open-style answers aligns with human judgments, providing reliable automatic scoring. A carefully crafted prompt instructs GPT-4 to judge open-style answers against the ground truth MCQ answer, focusing on factual accuracy and ignoring extra information or phrasing differences. This mechanism relies on the assumption that GPT-4 can reliably interpret and judge the correctness of open-style answers when guided by a strict rubric.

### Mechanism 3
Multi-stage filtering ensures only convertible MCQs are transformed into open-style questions, maintaining benchmark quality. A coarse-to-fine filtering pipeline first uses binary classification to identify convertible questions, then assigns confidence scores to refine the selection, removing unsuitable questions while retaining high-confidence candidates. This mechanism assumes the binary classifier and confidence scoring process accurately distinguish convertible from non-convertible MCQs.

## Foundational Learning

- **Concept**: Bias in multiple-choice evaluation (selection bias and random guessing)
  - **Why needed here**: Understanding these biases motivates the shift to open-style questions and informs the design of the new benchmark.
  - **Quick check question**: Why do multiple-choice questions suffer from selection bias, and how does open-style format address it?

- **Concept**: Automatic evaluation alignment with human judgment
  - **Why needed here**: Ensuring the LLM-based evaluation is trustworthy is critical for the leaderboard's validity.
  - **Quick check question**: What metric is used to verify alignment between GPT-4 evaluations and human judgments?

- **Concept**: Multi-stage filtering for question conversion
  - **Why needed here**: Properly identifying which MCQs can be converted maintains the quality and relevance of the benchmark.
  - **Quick check question**: What are the two stages of filtering used to select convertible MCQs?

## Architecture Onboarding

- **Component map**: Data collection -> Stage 1 filtering (binary classification) -> Stage 2 filtering (confidence scoring) -> Open-style conversion -> LLM response generation -> GPT-4 evaluation -> Leaderboard assembly
- **Critical path**: 1. Data collection, 2. Stage 1 filtering, 3. Stage 2 filtering, 4. Open-style conversion, 5. LLM response generation, 6. GPT-4 evaluation, 7. Leaderboard assembly
- **Design tradeoffs**: Accuracy vs. coverage (stricter filtering may exclude convertible questions but improve benchmark quality), evaluation cost vs. speed (GPT-4 evaluation is fast but expensive; crowdsourcing is cheaper but slower), open-style vs. MCQ (open-style removes bias but may be harder for models to answer consistently)
- **Failure signatures**: High false positive rate in filtering → low-quality benchmark, low kappa score between GPT-4 and human evaluations → unreliable scoring, large accuracy drop for all models → benchmark may be too difficult or poorly designed
- **First 3 experiments**:
  1. Run the filtering pipeline on a small sample dataset and manually check the accuracy of stage 1 and stage 2 classifications.
  2. Use GPT-4 to evaluate a small set of open-style answers and compare with human judgments to compute kappa.
  3. Compare MCQ vs. open-style performance on a small model set to confirm the expected accuracy drop and verify the new format is challenging.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific mechanisms cause LLMs to develop inherent selection biases toward certain answer choice IDs?
  - **Basis in paper**: The paper discusses that LLMs may inherently favor certain answer choice IDs due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs.
  - **Why unresolved**: While the paper identifies that selection bias exists and is caused by priori unbalanced probabilities, it doesn't explore the underlying mechanisms of how these biases develop during model training or what specific factors in the training data or architecture contribute to this phenomenon.
  - **What evidence would resolve it**: Detailed analysis of training data distribution, attention patterns, and activation values when processing different answer choice IDs across various LLMs would help identify the root causes of selection bias.

- **Open Question 2**: How does the performance drop between MCQ and open-style questions vary across different knowledge domains and question types?
  - **Basis in paper**: The paper mentions that almost all models struggle significantly with questions from MedMCQA dataset, showing an accuracy below 5%, and that HellaSwag doesn't seem well-suited for open-style questions due to its evaluation difficulties.
  - **Why unresolved**: The paper provides aggregate statistics showing a 25% average accuracy drop but doesn't provide granular analysis of how this performance gap varies across specific domains (STEM, humanities, social sciences) or question types (factual recall, reasoning, application).
  - **What evidence would resolve it**: Comprehensive analysis of accuracy differences between MCQ and OSQ formats broken down by domain, question type, and complexity level would reveal patterns in where open-style questions pose the greatest challenges.

- **Open Question 3**: What are the computational and practical trade-offs of implementing open-style question evaluation at scale compared to MCQ-based evaluation?
  - **Basis in paper**: The paper states that their leaderboard exhibits advantages including faster and cheaper evaluation over crowd user-based leaderboards, with results generated automatically without human intervention.
  - **Why unresolved**: While the paper claims open-style evaluation is faster and cheaper than human evaluation, it doesn't provide detailed comparison of computational costs, inference time, and resource requirements between MCQ and open-style evaluation methods, or analyze the scalability challenges.
  - **What evidence would resolve it**: Empirical comparison of computational resources, inference time, and costs required for evaluating large batches of MCQ versus open-style questions across different LLM sizes and evaluation frameworks would quantify the practical trade-offs.

## Limitations

- The multi-stage filtering protocol may exclude convertible questions or include unsuitable ones, as evidenced by the 40% misclassification rate in the "NO" sample.
- The exact implementation details of the filtering thresholds and confidence scoring are not fully specified, which could lead to variations in benchmark quality across different implementations.
- The computational and practical trade-offs of implementing open-style question evaluation at scale compared to MCQ-based evaluation are not thoroughly analyzed.

## Confidence

- **High**: The claim that open-style questions reduce selection bias and random guessing, supported by the 25% average accuracy drop compared to MCQs.
- **Medium**: The reliability of GPT-4 evaluation, based on the reported kappa score and error rate, but dependent on prompt design and potential edge cases.
- **Medium**: The effectiveness of the multi-stage filtering protocol, given the observed misclassification rate and lack of detailed threshold specifications.

## Next Checks

1. **Filtering Protocol Validation**: Manually review a sample of questions from each stage of the filtering pipeline to assess the accuracy of the binary classification and confidence scoring, and identify any systematic biases or errors.

2. **Evaluation Prompt Testing**: Test the GPT-4 evaluation prompt on a diverse set of open-style answers, including edge cases and nuanced phrasing, to ensure consistent and reliable scoring across different scenarios.

3. **Benchmark Performance Analysis**: Compare the performance of a broader range of models on both MCQ and open-style questions to confirm the expected accuracy drop and validate the challenge level of the new format.