---
ver: rpa2
title: Do Parameters Reveal More than Loss for Membership Inference?
arxiv_id: '2406.11544'
source_url: https://arxiv.org/abs/2406.11544
tags:
- membership
- inference
- loss
- attack
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing view that black-box access
  is sufficient for optimal membership inference by showing that stochastic gradient
  descent (SGD) dynamics invalidate previous theoretical claims. Using recent advances
  in discrete-time SGD dynamics, the authors derive an optimal white-box membership
  inference attack (IHA) that explicitly uses model parameters via inverse-Hessian
  vector products.
---

# Do Parameters Reveal More than Loss for Membership Inference?

## Quick Facts
- arXiv ID: 2406.11544
- Source URL: https://arxiv.org/abs/2406.11544
- Authors: Anshuman Suri; Xiao Zhang; David Evans
- Reference count: 40
- Key outcome: This paper challenges the prevailing view that black-box access is sufficient for optimal membership inference by showing that stochastic gradient descent (SGD) dynamics invalidate previous theoretical claims.

## Executive Summary
This paper challenges the prevailing view that black-box access is sufficient for optimal membership inference by showing that stochastic gradient descent (SGD) dynamics invalidate previous theoretical claims. Using recent advances in discrete-time SGD dynamics, the authors derive an optimal white-box membership inference attack (IHA) that explicitly uses model parameters via inverse-Hessian vector products. The proposed IHA outperforms state-of-the-art black-box attacks on three datasets, achieving AUC scores of 0.709 (Purchase-100), 0.538 (MNIST-Odd), and 0.588 (FashionMNIST), with corresponding true positive rates at 1% false positive rate of 0.254, 0.132, and 0.180 respectively. The results demonstrate that parameter access provides significant advantages for membership inference and advocate for further research into white-box auditing methods.

## Method Summary
The paper proposes a white-box membership inference attack called IHA (Inverse Hessian Attack) that leverages model parameters and recent advances in discrete-time SGD dynamics. The attack computes inverse-Hessian vector products to estimate the effect of individual training samples on the model parameters. The method is evaluated on three datasets (Purchase-100, MNIST-Odd, and FashionMNIST) using 2-layer MLPs trained with SGD with momentum. The attack performance is measured using AUC scores and true positive rates at low false positive rates, demonstrating significant improvements over black-box baseline attacks.

## Key Results
- IHA achieves AUC scores of 0.709 (Purchase-100), 0.538 (MNIST-Odd), and 0.588 (FashionMNIST)
- True positive rates at 1% false positive rate are 0.254, 0.132, and 0.180 respectively
- IHA outperforms state-of-the-art black-box attacks across all tested datasets
- The results demonstrate that parameter access provides significant advantages for membership inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SGD noise covariance commutes with the Hessian matrix, allowing inverse-Hessian vector products to capture training dynamics.
- Mechanism: When SGD converges to a local minimum, the stationary model fluctuation depends on both the Hessian at that minimum and the noise covariance. If these commute, the inverse-Hessian vector products can isolate the contribution of individual training samples.
- Core assumption: The SGD noise covariance commutes with the Hessian matrix, and the loss function is locally quadratic near a local minimum.
- Evidence anchors:
  - [abstract] "Using recent advances in discrete-time SGD dynamics, the authors derive an optimal white-box membership inference attack (IHA) that explicitly uses model parameters via inverse-Hessian vector products."
  - [section] "Theorem 4 implies that the SGD noise covariance C commutes with the Hessian matrix H(wâˆ—)."
  - [corpus] Weak corpus support for this specific SGD-commutes-Hessian mechanism; most related works focus on MIAs without explicit SGD dynamics.
- Break condition: If the Hessian is degenerate or the SGD noise covariance does not commute with the Hessian, the inverse-Hessian vector products lose their discriminative power.

### Mechanism 2
- Claim: The optimal membership inference score depends on both the magnitude and direction of a Newtonian step for the target record.
- Mechanism: The score combines the loss of the target record with terms involving the inverse-Hessian vector products, capturing how much and in what direction the model would move if the record were removed.
- Core assumption: The loss achieved at the local minimum remains unaffected by the removal of a single training record and the Hessian structure remains unchanged.
- Evidence anchors:
  - [abstract] "The proposed IHA outperforms state-of-the-art black-box attacks... achieving AUC scores of 0.709 (Purchase-100), 0.538 (MNIST-Odd), and 0.588 (FashionMNIST)."
  - [section] "Theorem 5 (Optimal Membership-Inference Score)... I1 and I2 can be interpreted as the magnitude and direction, respectively, of a Newtonian step for the given record z1."
  - [corpus] No direct corpus support for Newtonian-step interpretation; related works focus on simpler loss-based or meta-classifier methods.
- Break condition: If the Hessian structure changes significantly when a record is removed, or if the loss at the minimum is affected, the Newtonian step interpretation breaks down.

### Mechanism 3
- Claim: White-box access to model parameters enables computation of the optimal membership inference score, outperforming black-box methods.
- Mechanism: By computing the inverse-Hessian vector products and other gradient-based terms, the white-box attack can directly estimate the effect of a record's presence or absence on the model parameters.
- Core assumption: The auditor has complete knowledge of the training setup, including learning rate and momentum, and can compute the Hessian and its inverse.
- Evidence anchors:
  - [abstract] "Our theoretical results lead to a new white-box inference attack, IHA... that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products."
  - [section] "While the theory needs to cover all data distributions, experiments with the optimal attack focus on the actual distribution and given model, resulting in tighter and more relevant privacy evaluations."
  - [corpus] Limited corpus support for white-box optimality; most works focus on black-box MIAs or meta-classifier approaches.
- Break condition: If the computational cost of computing the Hessian and inverse-Hessian vector products is prohibitive, or if the assumptions about the training setup are incorrect, the white-box attack loses its advantage.

## Foundational Learning

- Concept: Discrete-time SGD dynamics and their connection to model parameter distributions.
  - Why needed here: The paper builds on recent advances in understanding how SGD converges to a stationary distribution, which is crucial for deriving the optimal membership inference score.
  - Quick check question: What is the relationship between the SGD noise covariance and the Hessian matrix in the stationary distribution?

- Concept: Membership inference attacks and their threat models.
  - Why needed here: The paper contrasts white-box and black-box membership inference attacks, showing how white-box access can provide more accurate privacy auditing.
  - Quick check question: How does the threat model for white-box membership inference differ from that of black-box attacks?

- Concept: Inverse-Hessian vector products and their computation.
  - Why needed here: The IHA attack relies on computing inverse-Hessian vector products to estimate the effect of individual training samples on the model parameters.
  - Quick check question: What are the computational challenges in approximating inverse-Hessian vector products for large models?

## Architecture Onboarding

- Component map:
  - Data (Purchase-100, MNIST-Odd, FashionMNIST) -> Model (2-layer MLPs) -> Attack (IHA) -> Evaluation (AUC, TPR@FPR metrics, ROC curves) -> Baselines (LOSS, SIF, LiRA, Reference attacks)

- Critical path:
  1. Train target models on given datasets
  2. Compute Hessian and inverse-Hessian vector products for each model
  3. Implement IHA scoring function using the computed values
  4. Evaluate IHA against baseline attacks using AUC and TPR@FPR metrics
  5. Analyze results and compare with theoretical predictions

- Design tradeoffs:
  - Computational cost vs. accuracy: Computing exact inverse-Hessian vector products is expensive but more accurate than approximations
  - White-box access vs. black-box access: White-box attacks require more information but can provide tighter privacy bounds
  - Damping vs. low-rank approximation: Different techniques for mitigating ill-conditioned Hessian matrices have varying effects on attack performance

- Failure signatures:
  - Poor attack performance: May indicate issues with Hessian computation, damping/low-rank approximation, or violations of underlying assumptions
  - High computational cost: May suggest the need for more efficient inverse-Hessian vector product approximation methods
  - Inconsistent results across datasets: May indicate dataset-specific factors affecting the attack's effectiveness

- First 3 experiments:
  1. Implement IHA on a small, well-conditioned dataset (e.g., MNIST-Odd) and compare with baseline attacks
  2. Vary the damping parameter for Hessian conditioning and observe its effect on attack performance
  3. Test IHA on a larger, more complex dataset (e.g., Purchase-100) and analyze computational bottlenecks

## Open Questions the Paper Calls Out
1. How would the IHA attack perform if approximate inverse-Hessian vector products (iHVPs) were used instead of exact computations?
2. Can the IHA attack be extended to multi-record membership inference scenarios?
3. Does the IHA attack remain effective when models are trained with techniques designed to mitigate overfitting, such as regularization or early stopping?

## Limitations
- The commuting-Hessian assumption is critical for IHA's optimality but remains unproven in general settings beyond the quadratic loss case
- The computational burden of exact inverse-Hessian vector products limits scalability to larger models
- The paper doesn't address how approximations would affect theoretical guarantees

## Confidence
- **High confidence**: IHA outperforms black-box baselines on the tested datasets; the experimental methodology is sound
- **Medium confidence**: The theoretical framework extends discrete-time SGD dynamics to membership inference; the Newtonian step interpretation provides intuitive understanding
- **Low confidence**: The commuting-Hessian assumption holds universally; the derived score remains optimal under practical conditions with non-quadratic losses and finite training

## Next Checks
1. Test IHA on datasets with different loss functions (beyond cross-entropy and MSE) to evaluate robustness of the commuting-Hessian assumption
2. Implement and compare multiple inverse-Hessian vector product approximation methods (conjugate gradient, randomized Lanczos) to assess impact on attack performance
3. Measure IHA's effectiveness when applied to models trained with different optimizers (Adam, RMSprop) to evaluate generalizability beyond SGD with momentum