---
ver: rpa2
title: 'Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean'
arxiv_id: '2404.12534'
source_url: https://arxiv.org/abs/2404.12534
tags:
- proof
- lean
- theorem
- tactics
- proving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lean Copilot, a framework enabling large
  language models (LLMs) to assist humans in theorem proving within the Lean proof
  assistant. The core innovation is a general framework for running LLM inference
  natively in Lean through its foreign function interface (FFI), addressing the gap
  between pretrained LLMs and their practical use in interactive theorem proving.
---

# Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean

## Quick Facts
- arXiv ID: 2404.12534
- Source URL: https://arxiv.org/abs/2404.12534
- Authors: Peiyang Song; Kaiyu Yang; Anima Anandkumar
- Reference count: 40
- One-line primary result: LLM-powered proof automation achieves 64% autonomous theorem proving vs 12% for rule-based tools, with 2.31× improvement in proof automation

## Executive Summary
Lean Copilot introduces a framework enabling large language models to assist humans in theorem proving within the Lean proof assistant. The key innovation is running LLM inference natively in Lean through its foreign function interface (FFI), allowing seamless integration of AI-powered proof automation tools. The system includes three main components: suggest_tactics for generating next proof steps, search_proofs that combines LLM suggestions with rule-based search, and select_premises for retrieving relevant mathematical premises.

## Method Summary
Lean Copilot implements LLM inference in Lean through FFI with C++ wrappers, allowing efficient integration without external dependencies. The framework provides three proof automation tools: suggest_tactics generates next proof steps using beam search decoding, search_proofs augments aesop's rule-based search with LLM-generated tactics, and select_premises uses retrieval-augmented LLMs to score and retrieve relevant premises from large mathematical libraries. The system is evaluated on 50 theorems from the "Mathematics in Lean" textbook, comparing performance against the rule-based aesop tool.

## Key Results
- Lean Copilot's search_proofs autonomously proves 64% of theorems versus 12% for aesop
- Human-assisted theorem proving requires only 1.02 manually-entered tactics on average versus 3.62 for aesop
- The framework automates 81.2% of proof steps on average, 2.31 times better than the rule-based baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lean Copilot enables LLM inference directly within Lean by using its foreign function interface (FFI) with C++.
- Mechanism: The framework wraps LLM models in C++ functions that operate on strings, then links them to Lean dynamically through FFI.
- Core assumption: Lean's FFI with C++ is efficient enough for real-time theorem proving collaboration.
- Evidence anchors:
  - [abstract] "Lean Copilot, a framework for running LLM inference natively in Lean through its foreign function interface (FFI)"
  - [section] "Lean Copilot runs LLMs natively in Lean through its foreign function interface (FFI). FFI is a mechanism for programs in one language to call subroutines in another language."

### Mechanism 2
- Claim: Combining LLM-generated tactics with rule-based search (aesop) improves proof automation compared to either approach alone.
- Mechanism: Lean Copilot's search_proof augments aesop's fixed rule set with goal-dependent tactics generated by suggest_tactics.
- Core assumption: Goal-dependent tactics provide complementary information to rule-based search strategies.
- Evidence anchors:
  - [abstract] "search_proofs (combines LLM suggestions with rule-based proof search)"
  - [section] "Search_proof augments aesop's rule set with goal-dependent tactics generated by suggest_tactics in each step."

### Mechanism 3
- Claim: Retrieval-augmented LLMs can effectively select relevant premises from large mathematical libraries.
- Mechanism: select_premises uses LLM-encoded goal vectors multiplied with pre-computed premise embeddings to score and retrieve relevant premises.
- Core assumption: Premise embeddings trained during LLM training capture meaningful semantic relationships between goals and premises.
- Evidence anchors:
  - [abstract] "select_premises (selects relevant premises using retrieval-augmented LLMs)"
  - [section] "Given a proof goal at inference time, we first encode the goal into a vector, and then perform a matrix-vector multiplication between the premise embedding and the goal vector."

## Foundational Learning

- Concept: Foreign Function Interface (FFI) and C++ integration
  - Why needed here: Lean Copilot requires efficient LLM inference within Lean's environment without external dependencies.
  - Quick check question: What is the primary advantage of using FFI over external API calls for LLM inference in Lean?

- Concept: Proof search algorithms and best-first search
  - Why needed here: Understanding aesop's search mechanism is crucial for building effective search_proof.
  - Quick check question: How does best-first search differ from depth-first search in the context of theorem proving?

- Concept: Retrieval-augmented generation and embedding multiplication
  - Why needed here: The select_premises tactic relies on efficient matrix operations between goal and premise embeddings.
  - Quick check question: What mathematical operation combines the goal embedding with premise embeddings to produce relevance scores?

## Architecture Onboarding

- Component map: User input -> suggest_tactics/search_proofs/select_premises -> Lean Copilot framework (FFI + C++ LLM wrappers) -> VS Code infoview display
- Critical path: User enters a tactic state → Lean Copilot processes the goal through LLM inference → generates tactic suggestions or selects premises → displays results in VS Code infoview → user accepts or modifies the suggestion
- Design tradeoffs: Native C++ inference vs external API calls (speed vs flexibility), beam search vs greedy decoding (diversity vs computational cost), in-scope vs out-of-scope premise categorization (usability vs completeness)
- Failure signatures: Slow response times indicate FFI overhead or model inference bottlenecks; incorrect tactic suggestions suggest model misalignment with Lean's tactic DSL; missing relevant premises indicate poor embedding quality or insufficient training data
- First 3 experiments:
  1. Install Lean Copilot and verify basic suggest_tactics functionality on simple theorems from the Mathlib library.
  2. Test search_proof on a set of theorems where aesop alone struggles, measuring improvement in proof success rate.
  3. Evaluate select_premises on theorems requiring external premises, verifying the accuracy of relevance scoring and annotation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Lean Copilot's framework be effectively extended to support other proof assistants beyond Lean?
- Basis in paper: [inferred] The paper demonstrates Lean Copilot's framework for running LLM inference in Lean through FFI, but doesn't explore applicability to other proof assistants like Coq or Isabelle.
- Why unresolved: The framework is specifically designed for Lean's architecture and FFI capabilities. Other proof assistants may have different programming languages, APIs, or architectural constraints that could require significant modifications.
- What evidence would resolve it: Successful implementation and benchmarking of Lean Copilot's core framework (native LLM inference through FFI) with at least one other proof assistant, showing comparable performance to the Lean implementation.

### Open Question 2
- Question: How does the performance of Lean Copilot scale with increasingly complex mathematical domains?
- Basis in paper: [explicit] The paper notes that LLMs struggle with novel or challenging theorems, especially from different domains than their training data, but only evaluates on 50 theorems from the "Mathematics in Lean" textbook.
- Why unresolved: The evaluation is limited to introductory-level theorems. There's no evidence of how the system performs on advanced mathematical domains like algebraic geometry, number theory, or mathematical physics where proofs are significantly longer and more complex.
- What evidence would resolve it: Systematic evaluation across multiple mathematical domains of increasing complexity, showing performance metrics (automation rates, human effort reduction) for each domain compared to baseline tools.

### Open Question 3
- Question: What is the optimal balance between LLM-generated suggestions and rule-based search strategies for different types of proof goals?
- Basis in paper: [explicit] The paper combines LLM suggestions with aesop's rule-based search in search_proofs, but doesn't systematically analyze when each approach is more effective or how to dynamically adjust their contributions.
- Why unresolved: The current implementation uses a fixed combination strategy. Different proof goals may benefit more from either LLM creativity or rule-based reliability, but the paper doesn't explore adaptive strategies.
- What evidence would resolve it: A study comparing different weighting strategies between LLM and rule-based components across various proof goal types, identifying optimal configurations for different mathematical problem categories.

## Limitations

- The framework's dependence on Lean's FFI with C++ may introduce performance bottlenecks that aren't fully characterized across different hardware configurations
- Evaluation on 50 textbook theorems may not capture the full complexity of real-world theorem proving in large mathematical libraries
- The reliance on pretrained embeddings assumes semantic relationships remain relevant for specific mathematical domains

## Confidence

- **High Confidence**: The core framework architecture (FFI-based LLM inference in Lean) and basic functionality of the three tools (suggest_tactics, search_proofs, select_premises) are well-supported by the implementation details and experimental results.
- **Medium Confidence**: The performance claims (64% autonomous theorem proving vs 12% for aesop, 2.31× improvement in proof automation) are based on controlled experiments with textbook problems, but may not generalize to more complex real-world proofs.
- **Low Confidence**: The long-term sustainability of the framework depends on maintaining compatibility with evolving Lean versions and LLM model architectures, which isn't addressed in the paper.

## Next Checks

1. **Performance Benchmarking**: Measure inference latency and memory usage across different hardware configurations (CPU-only laptops vs GPU-equipped machines) to validate the claim of efficient real-time operation.

2. **Generalization Testing**: Apply Lean Copilot to a larger, more diverse set of theorems from Mathlib, including those requiring complex mathematical reasoning beyond the textbook examples, to assess real-world performance.

3. **Robustness Evaluation**: Test the framework's stability and correctness when handling edge cases such as malformed tactic states, conflicting premise annotations, and model inference failures to identify potential failure modes in production use.