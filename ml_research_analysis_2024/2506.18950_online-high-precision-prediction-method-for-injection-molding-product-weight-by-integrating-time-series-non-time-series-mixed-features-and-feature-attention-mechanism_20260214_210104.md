---
ver: rpa2
title: Online high-precision prediction method for injection molding product weight
  by integrating time series/non-time series mixed features and feature attention
  mechanism
arxiv_id: '2506.18950'
source_url: https://arxiv.org/abs/2506.18950
tags:
- prediction
- data
- feature
- injection
- molding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of timely detection and online
  monitoring of injection molding quality anomalies by proposing a mixed feature attention-artificial
  neural network (MFA-ANN) model for high-precision online prediction of product weight.
  The model integrates time series and non-time series features using LSTM and self-attention
  mechanisms to enhance feature extraction and dynamic calibration of inter-modality
  feature weights.
---

# Online high-precision prediction method for injection molding product weight by integrating time series/non-time series mixed features and feature attention mechanism

## Quick Facts
- arXiv ID: 2506.18950
- Source URL: https://arxiv.org/abs/2506.18950
- Reference count: 0
- Primary result: MFA-ANN achieves 25.1% improvement over conventional benchmarks with ±0.5 g tolerance

## Executive Summary
This study addresses injection molding quality anomaly detection through an MFA-ANN model that integrates time series and non-time series features using LSTM and self-attention mechanisms. The model dynamically calibrates inter-modality feature weights to achieve high-precision online prediction of product weight. Experimental results demonstrate superior performance compared to conventional approaches, with significant contributions from mixed feature modeling and attention mechanisms to prediction accuracy.

## Method Summary
The proposed MFA-ANN model combines LSTM networks with self-attention mechanisms to fuse time series and non-time series features for injection molding product weight prediction. The architecture extracts temporal patterns from sensor data while simultaneously processing static process parameters, with the attention mechanism dynamically weighting the importance of each feature modality. This integrated approach enables real-time monitoring and anomaly detection with a reported RMSE of 0.0281 under ±0.5 g weight fluctuation tolerance.

## Key Results
- MFA-ANN achieves RMSE of 0.0281 with ±0.5 g weight fluctuation tolerance
- Outperforms non-time series ANN by 25.1%, LSTM by 23.0%, SVR by 25.7%, and RF by 15.6%
- Mixed feature modeling contributes 22.4% and attention mechanism contributes 11.2% to prediction accuracy
- Low-fidelity sensor inputs degrade performance by 23.8% RMSE compared to high-precision measurements

## Why This Works (Mechanism)
The model's effectiveness stems from its dual capability to capture temporal dependencies through LSTM networks while simultaneously extracting static feature relationships. The self-attention mechanism dynamically adjusts the relative importance of different feature modalities based on their predictive power, preventing overfitting to less relevant features. This combination allows the model to maintain high accuracy even with noisy sensor inputs by focusing on the most informative signals for weight prediction.

## Foundational Learning
- LSTM Networks: Essential for capturing temporal patterns in sensor data streams; quick check: verify vanishing gradient prevention through gated architecture
- Self-Attention Mechanisms: Enables dynamic feature weighting across modalities; quick check: confirm attention weights correlate with feature importance in ablation studies
- Mixed Feature Fusion: Combines time-varying and static process parameters; quick check: validate improvement over single-modality approaches
- Feature Calibration: Adjusts inter-modality weightings based on real-time performance; quick check: monitor weight distribution stability during operation
- RMSE as Performance Metric: Industry-standard measure for prediction accuracy; quick check: compare against alternative metrics like MAE for robustness validation

## Architecture Onboarding

**Component Map:** Raw Sensor Data -> LSTM Time Series Extractor -> Self-Attention Layer -> Feature Fusion -> Weight Prediction -> RMSE Evaluation

**Critical Path:** The model processes sensor data through LSTM layers to extract temporal patterns, then applies self-attention to dynamically weight these features against non-time series inputs before final prediction. The attention mechanism represents the critical path for adapting to varying process conditions.

**Design Tradeoffs:** The model prioritizes prediction accuracy over computational efficiency, using complex LSTM architectures that may impact real-time deployment speed. The ±0.5 g tolerance threshold represents a practical compromise between manufacturing precision requirements and achievable model performance.

**Failure Signatures:** Performance degradation occurs when sensor fidelity drops below threshold levels, with RMSE increasing by 23.8%. The model also shows reduced accuracy when feature distributions shift significantly from training data, indicating sensitivity to process variations.

**3 First Experiments:**
1. Validate model performance across different weight tolerance thresholds (e.g., ±0.3 g vs ±0.5 g)
2. Test feature importance stability under varying process conditions and noise levels
3. Compare attention weight distributions between normal and anomalous production runs

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary dataset limits independent validation of reported performance improvements
- Single performance metric (RMSE) used without comprehensive evaluation using MAE or R²
- Industry-specific ±0.5 g tolerance may not generalize to other manufacturing applications
- Ablation analysis percentages represent relative rather than absolute improvements

## Confidence

**High confidence:** Methodological approach combining LSTM with attention mechanisms for feature fusion

**Medium confidence:** 25.1% improvement over baseline models due to proprietary dataset constraints

**Medium confidence:** 23.8% degradation from low-fidelity sensors based on single dataset observations

## Next Checks
1. Replicate MFA-ANN on publicly available injection molding datasets to verify generalizability
2. Conduct cross-validation with multiple quality metrics (MAE, R², precision-recall curves)
3. Test model robustness with varying sensor fidelity levels and different weight tolerance thresholds