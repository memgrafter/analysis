---
ver: rpa2
title: Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge
  in GPT-2 Small
arxiv_id: '2409.04478'
source_url: https://arxiv.org/abs/2409.04478
tags:
- features
- country
- continent
- city
- saes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether sparse autoencoders (SAEs) provide
  a better feature space than neurons for causal analysis of factual knowledge in
  language models. The authors use the RAVEL benchmark to test if SAE features can
  separately mediate knowledge of which country a city is in and which continent it
  is in.
---

# Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small
## Quick Facts
- arXiv ID: 2409.04478
- Source URL: https://arxiv.org/abs/2409.04478
- Reference count: 13
- Primary result: SAEs struggle to match neuron performance on disentangling factual knowledge in GPT-2 small

## Executive Summary
This paper evaluates whether sparse autoencoders (SAEs) provide a better feature space than individual neurons for causal analysis of factual knowledge in language models. Using the RAVEL benchmark and interchange interventions, the authors test if SAE features can separately mediate knowledge about which country a city is in versus which continent it is in. The evaluation compares four open-source SAEs for GPT-2 small against neurons (baseline) and DAS (skyline). Results show that SAEs generally underperform neurons in disentangling these concepts, with only the OpenAI SAE matching neuron performance.

## Method Summary
The authors use interchange interventions to test whether SAE features can separately control country and continent knowledge about cities. They train binary masks to select features that change country without changing continent, or vice versa. The evaluation compares four open-source SAEs for GPT-2 small against neurons as baseline and DAS as an upper bound. They also examine how SAE reconstructions affect GPT-2's knowledge of cities.

## Key Results
- SAEs generally struggle to reach neuron baseline performance in disentangling country and continent knowledge
- Only the OpenAI SAE matches neuron performance on this task
- None of the SAEs approach the DAS skyline performance
- SAE reconstructions consistently degrade GPT-2's knowledge of cities

## Why This Works (Mechanism)
Assumption: SAEs may encode country and continent information in overlapping feature subspaces, making it difficult to separate these concepts through binary masking. The sparsity constraint might force features to capture correlated information patterns rather than disentangled representations. Unknown: The specific architectural or training characteristics that cause SAEs to underperform neurons for this task.

## Foundational Learning
- Sparse autoencoders (SAEs): Neural networks that learn compressed representations by enforcing sparsity in the latent space. Needed to understand the feature extraction method being evaluated.
- Interchange interventions: Causal analysis technique that tests whether specific features can mediate between input and output changes. Needed to understand the experimental methodology.
- RAVEL benchmark: Evaluation framework for testing disentanglement of factual knowledge in language models. Needed to understand the evaluation setup.
- DAS (Disentangled Autoencoder Spaces): Alternative feature extraction method used as an upper bound. Needed for context on comparison methods.

## Architecture Onboarding
Component map: GPT-2 small -> SAEs -> Feature space -> Interchange interventions -> Binary mask training
Critical path: Input city -> SAE feature extraction -> Binary mask selection -> Output country/continent prediction
Design tradeoffs: SAEs trade reconstruction accuracy for interpretability, potentially losing information needed for precise knowledge representation
Failure signatures: SAEs unable to separate country and continent knowledge, degradation of factual knowledge after reconstruction
First experiments: 1) Test SAE feature importance ranking, 2) Compare SAE feature distributions to neuron activations, 3) Evaluate SAE performance on simpler disentanglement tasks

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Narrow scope limited to country-continent pairs, may not generalize to other factual knowledge
- SAE reconstruction quality may confound results, making it unclear if limitations are fundamental to SAE feature spaces
- Interchange interventions may not fully isolate causal mechanisms of knowledge representation
- DAS comparison assumes optimal feature quality without independent validation

## Confidence
High: SAEs struggle to match neuron performance on disentangling factual knowledge
Medium: SAE reconstructions degrade knowledge of cities
Low: DAS features represent an optimal upper bound for this task

## Next Checks
1. Test SAEs on additional factual knowledge tasks (e.g., capital cities, historical dates) to verify if results generalize beyond country-continent pairs
2. Compare SAE feature spaces to alternative feature extraction methods like PCA or random projections to establish whether SAEs are specifically underperforming
3. Analyze which specific features are being selected by the binary masks to understand whether SAEs encode complementary vs. redundant information compared to neurons