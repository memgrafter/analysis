---
ver: rpa2
title: Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text
  Retrieval Evaluation
arxiv_id: '2408.01363'
source_url: https://arxiv.org/abs/2408.01363
tags:
- relevance
- judgments
- retrieval
- image
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Vision-Language Models (VLMs)
  like CLIP, LLaVA, and GPT-4V for automatically estimating relevance judgments in
  image-text retrieval tasks. The goal is to enable large-scale, cost-effective evaluation
  by replacing manual relevance annotations with model-based scores.
---

# Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation

## Quick Facts
- arXiv ID: 2408.01363
- Source URL: https://arxiv.org/abs/2408.01363
- Authors: Jheng-Hong Yang; Jimmy Lin
- Reference count: 10
- One-line primary result: VLMs like LLaVA and GPT-4V achieve higher ranking correlation (~0.4-0.5) with human judgments compared to CLIPScore (~0.2) for image-text retrieval evaluation

## Executive Summary
This paper investigates using Vision-Language Models (VLMs) such as CLIP, LLaVA, and GPT-4V to automatically estimate relevance judgments for image-text retrieval tasks, aiming to reduce the cost and scale limitations of manual relevance annotation. The authors design prompt templates to guide VLMs in generating relevance scores, which are then compared against human annotations from the TREC-AToMiC 2023 test collection. Results show that LLM-based approaches like LLaVA and GPT-4V achieve significantly higher ranking correlation with human judgments than CLIPScore, while also revealing evaluation bias favoring CLIP-based retrieval systems.

## Method Summary
The study evaluates three VLMs (CLIP, LLaVA, GPT-4V) using zero-shot prompting to generate relevance scores for image-text pairs from the TREC-AToMiC 2023 test collection. A prompt template guides the models to assess relevance, producing raw scores that are post-processed into graded relevance levels (0, 1, 2). Model-based judgments are compared against human-annotated relevance judgments using ranking correlation metrics (Kendall's τ, Spearman's ρ, Pearson's ρ) and agreement measures (Cohen's κ). The authors also assess evaluation bias through a Relative ∆ metric to quantify system preferences.

## Key Results
- LLaVA and GPT-4V achieve Kendall's τ ~0.4 for NDCG@10 and ~0.5 for MAP, significantly outperforming CLIPScore (~0.2)
- GPT-4V's score distribution more closely matches human judgment distributions (Cohen's κ ~0.08) compared to other models
- Model-based judgments exhibit evaluation bias favoring CLIP-based retrieval systems, with CLIPScore showing strongest bias (Relative ∆ = 114.7 for NDCG@10)
- LLaVA and GPT-4V are less biased toward CLIP-based systems than CLIPScore while maintaining higher correlation with human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual instruction-tuned LLMs (LLaVA, GPT-4V) can generate relevance scores that correlate with human judgments in image-text retrieval tasks.
- Mechanism: These models leverage their multimodal training and instruction-following capabilities to assess both visual and textual components of image-text pairs, producing scores that reflect human relevance criteria.
- Core assumption: The models have learned generalizable patterns about what constitutes relevant visual-textual correspondence during their training.
- Evidence anchors: [abstract] "Both LLaVA and GPT-4V... achieve notable Kendall's τ ∼ 0.4 when compared to human relevance judgments, surpassing the CLIPScore metric"; [section] "models leveraging LLMs such as LLaVA and GPT-4V outperform the CLIP-S baseline concerning ranking correlation... they achieve Kendall's τ values of approximately 0.4 for NDCG@10 and around 0.5 for MAP"

### Mechanism 2
- Claim: GPT-4V's score distribution more closely matches human judgment distributions than other models.
- Mechanism: GPT-4V's larger model capacity and more sophisticated multimodal reasoning enable it to produce scores with a distribution that better reflects human assessors' patterns.
- Core assumption: The model's training data and architecture allow it to internalize human-like judgment patterns.
- Evidence anchors: [abstract] "GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's κ value of around 0.08, which outperforms CLIPScore at approximately -0.096"; [section] "Figure 2 presents a Cumulative Distribution Function (CDF) plot of scores before post-processing... GPT-4V's score distribution closely aligns with the human CDF"

### Mechanism 3
- Claim: Model-based relevance judgments introduce evaluation bias, favoring CLIP-based retrieval systems.
- Mechanism: Since VLMs like LLaVA and GPT-4V use CLIP embeddings for image representations, they may inherently prefer retrieval systems that use similar CLIP-based approaches.
- Core assumption: The VLMs' image understanding is based on CLIP's visual features, creating a bias toward systems using the same features.
- Evidence anchors: [abstract] "While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems"; [section] "We quantitatively assess this bias using a metric... CLIP-S exhibits a strong bias, with Relative ∆ = 114.7 for NDCG@10 and 120.5 for MAP"

## Foundational Learning

- Concept: Zero-shot learning and prompt engineering for VLMs
  - Why needed here: The study uses zero-shot prompting to guide VLMs in generating relevance scores without fine-tuning on the specific task
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and why might zero-shot be preferred in this evaluation context?

- Concept: Correlation metrics (Kendall's τ, Spearman's ρ, Pearson's ρ) and agreement metrics (Cohen's κ)
  - Why needed here: These statistical measures are used to compare model-based relevance judgments with human annotations
  - Quick check question: When would you use Kendall's τ versus Spearman's ρ to measure ranking correlation, and what does Cohen's κ tell you about agreement?

- Concept: Cumulative Distribution Function (CDF) analysis
  - Why needed here: CDF plots are used to compare the distribution of model-generated scores with human-annotated scores
  - Quick check question: How can CDF analysis reveal differences in score distributions that might not be apparent from summary statistics alone?

## Architecture Onboarding

- Component map: VLMs (CLIP, LLaVA, GPT-4V) -> Prompt templates -> Post-processing scripts -> Human-annotated test collections (TREC-AToMiC 2023) -> Evaluation metrics (NDCG@10, MAP, correlation coefficients, Cohen's κ)

- Critical path: 1. Prepare image-text pairs from the test collection; 2. Apply prompt template to each pair for each VLM; 3. Collect and parse raw relevance scores from model outputs; 4. Apply post-processing to convert scores to relevance levels; 5. Compare model-based qrels with human-annotated qrels using evaluation metrics

- Design tradeoffs:
  - Zero-shot vs. fine-tuned VLMs: Zero-shot is faster and more generalizable but may be less accurate for specific domains
  - Prompt template complexity: More detailed prompts may improve quality but increase token usage and cost
  - Post-processing thresholds: Different quantile-based mappings may be needed for different models to best match human distributions

- Failure signatures:
  - Low correlation coefficients (τ, ρ) indicating poor alignment with human judgments
  - Extreme score distributions that don't match human patterns (e.g., all scores clustered in one range)
  - Systematic bias in evaluation results favoring certain retrieval system types
  - Inconsistent relevance level assignments when the same image-text pair is evaluated multiple times

- First 3 experiments:
  1. Compare correlation coefficients between model-based and human-based judgments for a small subset of the test collection to validate the basic approach
  2. Generate CDF plots of raw scores from each VLM to identify which models produce distributions closest to human judgments
  3. Test different post-processing threshold strategies (e.g., different quantile ranges) to optimize agreement with human relevance levels

## Open Questions the Paper Calls Out

- Can the evaluation bias observed in favor of CLIP-based systems be eliminated or significantly reduced through prompt engineering or alternative VLM architectures?
- How can VLMs be optimized to produce relevance score distributions that more closely match human judgments in terms of both shape and variance?
- Would extending the current pointwise estimation approach to pairwise or listwise ranking methods improve the correlation between model-based and human relevance judgments?

## Limitations
- Evaluation bias remains a significant concern, with model-based judgments systematically favoring CLIP-based retrieval systems
- Post-processing heuristics for converting continuous scores to discrete relevance levels may not generalize well across domains
- Low agreement scores (Cohen's κ ~0.08) indicate substantial divergence between model-based and human absolute relevance judgments

## Confidence

- High Confidence: Ranking correlation results showing LLaVA and GPT-4V outperforming CLIPScore (~0.4-0.5 vs ~0.2 Kendall's τ) - supported by multiple evaluation metrics and clear statistical differences
- Medium Confidence: Evaluation bias findings and distribution alignment claims - while results are consistent, the bias assessment methodology and threshold selection for post-processing could influence outcomes
- Low Confidence: Generalizability of results beyond the TREC-AToMiC 2023 collection and specific prompt template used - the study does not test different prompt variations or alternative test collections

## Next Checks

1. Test alternative post-processing threshold strategies (e.g., model-specific quantiles vs. uniform thresholds) to optimize agreement with human relevance levels and reduce bias

2. Evaluate different prompt template variations to assess how prompt engineering affects correlation with human judgments and bias toward specific retrieval systems

3. Conduct ablation studies using VLMs with different visual feature extractors (not just CLIP) to quantify the impact of shared visual representations on evaluation bias