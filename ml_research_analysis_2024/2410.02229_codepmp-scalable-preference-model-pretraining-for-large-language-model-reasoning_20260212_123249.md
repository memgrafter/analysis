---
ver: rpa2
title: 'CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning'
arxiv_id: '2410.02229'
source_url: https://arxiv.org/abs/2410.02229
tags:
- codepmp
- reasoning
- data
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodePMP introduces a scalable method for pretraining reward models
  using synthetic code-preference pairs to enhance reasoning capabilities in large
  language models. By generating millions of chosen-rejected code pairs from high-quality
  source code using models of different capabilities, CodePMP improves both sample
  efficiency and performance on mathematical and logical reasoning tasks.
---

# CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2410.02229
- Source URL: https://arxiv.org/abs/2410.02229
- Authors: Huimu Yu; Xing Wu; Haotian Xu; Debing Zhang; Songlin Hu
- Reference count: 23
- Primary result: Improves reward model sample efficiency and Best-of-N performance on reasoning tasks using synthetic code-preference pairs

## Executive Summary
CodePMP introduces a scalable method for pretraining reward models using synthetic code-preference pairs to enhance reasoning capabilities in large language models. By generating millions of chosen-rejected code pairs from high-quality source code using models of different capabilities, CodePMP improves both sample efficiency and performance on mathematical and logical reasoning tasks. Experiments demonstrate that CodePMP-initialized models achieve higher accuracy and more stable Best-of-N performance compared to models without pretraining, while requiring significantly fewer annotated preference samples.

## Method Summary
CodePMP generates synthetic preference pairs from GitHub code by using a strong CodeLLM to create "chosen" responses and a weaker CodeLLM to create "rejected" responses for the same code prompts. These pairs are used to pretrain preference models with a combined reward modeling (pairwise ranking) and language modeling loss. The pretrained models are then fine-tuned on reasoning-specific preference data and evaluated using Best-of-N sampling accuracy. The approach scales effectively with more data and generalizes across different model sizes and reasoning task types.

## Key Results
- CodePMP-initialized models achieve 80× better sample efficiency, matching 40k-sample baseline performance with just 0.5k samples
- Combined RM+LM loss consistently outperforms single-loss variants across all Best-of-N evaluation settings
- Increasing code-preference pairs from 0.1M to 28M shows consistent accuracy improvements with no diminishing returns
- CodePMP benefits generalize across model sizes (1.5B to 7B parameters) and reasoning task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CodePMP improves reward model sample efficiency by pretraining on synthetic preference pairs derived from code.
- Mechanism: The pretraining phase exposes the reward model to a large, diverse dataset of pairwise code comparisons (chosen vs. rejected), teaching it to recognize subtle quality differences before fine-tuning on smaller, domain-specific reasoning datasets.
- Core assumption: Synthetic code-preference pairs capture meaningful quality distinctions that transfer to reasoning domains.
- Evidence anchors:
  - [abstract]: "CodePMP improves RM finetuning efficiency by pretraining preference models on large-scale synthesized code-preference pairs."
  - [section 4.2.3]: "CodePMP-initialized models consistently outperform baselines across all dataset sizes, with CodePMP achieving with just 0.5k samples what baseline models require 40k samples to match—an 80 × efficiency improvement."
  - [corpus]: Weak. Corpus lacks direct discussion of synthetic preference pair effectiveness.
- Break condition: If the quality distinctions in code-preference pairs do not transfer to reasoning domains, the pretraining benefit vanishes.

### Mechanism 2
- Claim: The combination of reward modeling and language modeling losses during pretraining leads to better generalization than using either loss alone.
- Mechanism: RM loss trains the model to rank outputs, while LM loss maintains general language capabilities. Together, they prevent overfitting to the specific ranking task and preserve broader understanding.
- Core assumption: The two loss components complement each other rather than compete.
- Evidence anchors:
  - [section 5.2]: "The combined loss function consistently outperforms single-loss variants across all Best-of-N evaluation settings, with particularly notable improvements on the challenging MATH dataset."
  - [abstract]: "CodePMP significantly improves RM finetuning accuracy and Best-of-N performance in reasoning tasks."
  - [corpus]: Weak. No corpus evidence specifically addressing combined loss benefits.
- Break condition: If the losses interfere or one dominates, the combined approach may underperform specialized single-loss training.

### Mechanism 3
- Claim: Code data provides a scalable, high-quality source for preference pairs because code execution can validate correctness, and the structured nature of code maps well to logical reasoning.
- Mechanism: Code repositories offer vast amounts of diverse, high-quality content. Using models of different capabilities to generate chosen/rejected pairs leverages the inherent correctness constraints in code to create meaningful preference signals.
- Core assumption: The logical structure of code aligns with reasoning task requirements.
- Evidence anchors:
  - [abstract]: "Code, with its inherently logical and structured nature, provides rich data suitable for reasoning tasks."
  - [section 4.2.4]: "Increasing the number of code-preference pairs consistently improves BoN accuracy in both mathematical and logical reasoning tasks across model sizes, with no sign of diminishing returns."
  - [corpus]: Weak. No direct corpus evidence linking code structure to reasoning task performance.
- Break condition: If reasoning tasks require different cognitive processes than those captured by code, the alignment breaks down.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: CodePMP is a method to improve reward models, which are central to RLHF. Understanding RLHF provides context for why better reward models matter.
  - Quick check question: What is the role of a reward model in the RLHF pipeline?

- Concept: Pairwise Ranking Loss
  - Why needed here: CodePMP uses pairwise ranking to train preference models on chosen/rejected pairs. Understanding this loss function is essential to grasp how the pretraining works.
  - Quick check question: How does pairwise ranking loss differ from pointwise loss in preference modeling?

- Concept: Best-of-N Sampling
  - Why needed here: CodePMP is evaluated using Best-of-N accuracy, which measures how well the reward model selects the best answer from multiple candidates.
  - Quick check question: Why might Best-of-N sampling be particularly useful for reasoning tasks?

## Architecture Onboarding

- Component map:
  GitHub code corpus -> Description generation -> Chosen/rejected pair synthesis -> Preference model pretraining -> RM fine-tuning -> Best-of-N evaluation

- Critical path:
  1. Generate synthetic preference pairs from code
  2. Pretrain preference model with combined RM and LM losses
  3. Fine-tune on reasoning-specific preference data
  4. Evaluate using Best-of-N sampling

- Design tradeoffs:
  - Strong vs. weak model selection: Larger gap improves preference signal but may reduce pair quality if weak model fails completely
  - Code vs. other data sources: Code offers structured logic but may lack diversity in natural language reasoning patterns
  - Combined vs. single loss: Combined losses improve generalization but increase training complexity

- Failure signatures:
  - Poor transfer from code to reasoning: Best-of-N performance matches or falls below baseline after pretraining
  - Loss imbalance: One loss component dominates, causing either poor ranking ability or loss of general capabilities
  - Data quality issues: Preference consistency drops below acceptable thresholds (e.g., <70%)

- First 3 experiments:
  1. Ablation: Train with RM loss only vs. LM loss only vs. combined loss on GSM8K
  2. Scaling: Vary number of code-preference pairs (0.1M → 28M) and measure BoN accuracy on MATH
  3. Cross-architecture: Initialize Gemma2 and Llama3.2 with CodePMP and compare to baseline on ReClor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CodePMP performance scale when using models from different families for strong and weak code generation?
- Basis in paper: [inferred] The paper shows that using models from the same family yields consistent performance differences, but raises concerns about pairing models from different families.
- Why unresolved: The paper only provides limited experimental comparison between same-family and different-family model pairings, without extensive evaluation of cross-family performance.
- What evidence would resolve it: Systematic experiments comparing CodePMP performance across various combinations of model families for strong and weak code generation, with detailed analysis of consistency and quality metrics.

### Open Question 2
- Question: What is the optimal balance between Reward Modeling loss and Language Modeling loss in the combined CodePMP objective function?
- Basis in paper: [explicit] The paper mentions combining RM and LM losses but does not explore the optimal weighting or ratio between these components.
- Why unresolved: The paper uses a simple additive combination of losses without investigating how different weightings might affect performance or sample efficiency.
- What evidence would resolve it: Ablation studies varying the relative weights of RM and LM losses across different reasoning tasks, measuring impact on both accuracy and sample efficiency.

### Open Question 3
- Question: How does CodePMP perform on reasoning tasks beyond mathematical and logical domains, such as commonsense reasoning or causal reasoning?
- Basis in paper: [inferred] The paper focuses on mathematical and logical reasoning tasks, explicitly noting that evaluation on other reasoning modalities is less thoroughly examined.
- Why unresolved: The paper's evaluation is limited to specific reasoning task categories, leaving generalizability to other reasoning types unexplored.
- What evidence would resolve it: Comprehensive evaluation of CodePMP on diverse reasoning benchmarks including commonsense QA, causal reasoning, and multi-hop reasoning tasks, with comparison to baseline methods.

## Limitations
- Limited theoretical analysis of why code-preference pairs transfer to reasoning tasks
- Potential bias from using models of different capabilities to generate preference pairs
- Insufficient characterization of reward model failure modes

## Confidence
- High confidence: Empirical results showing improved sample efficiency and Best-of-N performance
- Medium confidence: Mechanism claims about why CodePMP works
- Low confidence: Scalability claims beyond tested ranges

## Next Checks
1. Cross-domain transfer analysis: Test CodePMP pretraining on non-code domains to determine generalizability
2. Loss interaction study: Vary RM and LM loss weights across reasoning tasks to characterize optimal balance
3. Failure mode characterization: Analyze reward model errors to identify systematic failure patterns and limits of the approach