---
ver: rpa2
title: On the Effect of Purely Synthetic Training Data for Different Automatic Speech
  Recognition Architectures
arxiv_id: '2407.17997'
source_url: https://arxiv.org/abs/2407.17997
tags:
- data
- training
- synthetic
- speech
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates synthetic training data for three ASR architectures
  (AED, Hybrid, GMM-HMM). Synthetic data was generated using a FastSpeech-like TTS
  trained on LibriSpeech-100h, with ablation studies on speaker embeddings and TTS
  model scaling.
---

# On the Effect of Purely Synthetic Training Data for Different Automatic Speech Recognition Architectures

## Quick Facts
- **arXiv ID**: 2407.17997
- **Source URL**: https://arxiv.org/abs/2407.17997
- **Reference count**: 0
- **Primary result**: Synthetic data evaluation across three ASR architectures (AED, Hybrid, GMM-HMM) shows varying sensitivity to data quality

## Executive Summary
This work systematically evaluates the effectiveness of purely synthetic training data for three different automatic speech recognition architectures: attention-based encoder-decoder (AED), hybrid, and GMM-HMM systems. The study generates synthetic speech using a FastSpeech-like TTS model trained on 100 hours of LibriSpeech data, with ablation studies on speaker embeddings and TTS model scaling. Results show that while larger TTS models improve ASR performance despite overfitting indicators, advanced speaker embeddings provide no consistent gains. The findings reveal that GMM-HMM architectures are least sensitive to synthetic data quality, while AED architectures are most sensitive, with vocoding quality (Griffin-Lim vs neural) showing minimal impact on ASR performance.

## Method Summary
The study evaluates synthetic training data across three ASR architectures using a unified experimental framework. Synthetic speech is generated using a FastSpeech-like TTS system trained on LibriSpeech-100h, with the ASR models then trained exclusively on this synthetic data. The research conducts ablation studies on two key factors: speaker embedding quality (comparing X-vectors and Resemblyzer against baseline) and TTS model scaling (varying model size). ASR performance is measured using standard metrics like Word Error Rate (WER). The evaluation compares synthetic data performance against real speech data and examines how different architectures respond to variations in synthetic data quality.

## Key Results
- GMM-HMM ASR architecture shows least sensitivity to synthetic data quality variations
- AED architecture demonstrates highest sensitivity to synthetic data quality
- Larger TTS models improve ASR performance despite training loss suggesting overfitting
- Advanced speaker embeddings (X-vectors, Resemblyzer) provide no consistent performance gains

## Why This Works (Mechanism)
The differential performance across ASR architectures stems from their fundamental design characteristics and how they process acoustic information. GMM-HMM systems rely on probabilistic modeling of speech units and are less dependent on precise acoustic feature matching, making them more robust to synthetic data variations. Hybrid systems, combining neural acoustic models with HMM-based decoding, show intermediate sensitivity as they balance acoustic modeling precision with structural robustness. AED architectures depend heavily on accurate acoustic feature representation for attention-based alignment, making them most vulnerable to synthetic data quality issues. The finding that larger TTS models improve performance despite overfitting suggests that model capacity captures more generalizable acoustic patterns rather than memorizing training data, possibly due to the regularization effects of the sequence-to-sequence architecture.

## Foundational Learning
**Automatic Speech Recognition (ASR)**: Converts spoken language into text using acoustic and language models. Why needed: Forms the core system being evaluated. Quick check: Can the model transcribe clean speech accurately?
**Text-to-Speech (TTS)**: Generates synthetic speech from text using neural architectures like FastSpeech. Why needed: Provides the synthetic training data for ASR evaluation. Quick check: Does the TTS produce intelligible speech?
**Speaker Embeddings**: Fixed-dimensional representations capturing speaker characteristics used to condition TTS generation. Why needed: Affects the naturalness and speaker similarity of synthetic speech. Quick check: Can embeddings distinguish between different speakers?
**Attention-Based Encoder-Decoder (AED)**: End-to-end ASR architecture using attention mechanisms for alignment. Why needed: Most sensitive architecture requiring high-quality acoustic features. Quick check: Does attention attend to correct speech segments?
**Hybrid ASR**: Combines neural acoustic models with HMM-based decoding frameworks. Why needed: Intermediate sensitivity architecture balancing different approaches. Quick check: Are phoneme alignments accurate?
**GMM-HMM**: Traditional ASR using Gaussian Mixture Models and Hidden Markov Models. Why needed: Most robust architecture to data quality variations. Quick check: Does the system correctly model acoustic-phonetic relationships?

## Architecture Onboarding

**Component Map**: TTS Text -> FastSpeech Model -> Synthetic Speech -> ASR Training -> WER Evaluation

**Critical Path**: TTS Generation → ASR Training → Performance Evaluation

**Design Tradeoffs**: The study balances synthetic data realism against generation efficiency, using FastSpeech for speed while evaluating against neural vocoders. Speaker embedding quality versus computational cost represents another tradeoff, with advanced embeddings showing no consistent gains. Model scaling tradeoffs show larger TTS models improving performance despite overfitting concerns.

**Failure Signatures**: Poor TTS quality manifests as degraded ASR performance across all architectures, with AED showing the steepest degradation curve. Speaker embedding failures appear as inconsistent performance variations without clear improvement patterns. Overfitting in TTS models paradoxically improves ASR performance, suggesting capacity captures useful generalizations.

**3 First Experiments**:
1. Generate synthetic speech using baseline TTS and evaluate baseline WER across all three ASR architectures
2. Test speaker embedding ablation by comparing X-vectors, Resemblyzer, and baseline embeddings on a single ASR architecture
3. Scale TTS model size incrementally and measure corresponding changes in ASR performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to English read speech from LibriSpeech corpus
- Single TTS architecture (FastSpeech-like) used, preventing comparison with other TTS approaches
- Fixed training set size (100 hours) without exploring scalability to larger datasets
- No investigation of synthetic data impact on rare word coverage or out-of-vocabulary handling

## Confidence
- AED architectures most sensitive to synthetic data quality: **High**
- Larger TTS models improve performance despite overfitting: **Medium**
- Advanced speaker embeddings provide no consistent gains: **Low**

## Next Checks
1. Replicate experiments across multiple languages and speech domains (conversational, broadcast news, accented speech) to assess generalizability beyond read English
2. Compare synthetic data performance against other augmentation techniques like SpecAugment or speed perturbation within the same experimental framework
3. Conduct systematic study of synthetic data impact on rare word recognition and out-of-vocabulary handling through targeted vocabulary tests