---
ver: rpa2
title: Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum
  Learning
arxiv_id: '2409.19510'
source_url: https://arxiv.org/abs/2409.19510
tags:
- speech
- translation
- s2tt
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of enabling many-to-many speech-to-text
  translation in low-resource settings, where parallel speech data is scarce. It proposes
  a three-stage curriculum learning strategy that leverages the multilingual machine
  translation capabilities of large language models by reformulating the task as speech
  recognition and translation.
---

# Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning

## Quick Facts
- arXiv ID: 2409.19510
- Source URL: https://arxiv.org/abs/2409.19510
- Reference count: 27
- Primary result: State-of-the-art average BLEU performance of 21.4 on 15×14 translation directions using fewer than 10 hours of speech data per language

## Executive Summary
This work addresses the challenge of enabling many-to-many speech-to-text translation in low-resource settings where parallel speech data is scarce. The authors propose a three-stage curriculum learning strategy that reformulates the task as speech recognition and translation to leverage multilingual machine translation capabilities of large language models. By training MLLMs with varying parameter sizes through sequential ASR, SMT, and SRT stages using specialized instructions and optimized speech adapters, the method achieves state-of-the-art performance requiring minimal speech data per language.

## Method Summary
The approach trains MLLMs (3B, 7B, and 32B parameter variants of Qwen2.5) through a three-stage curriculum: ASR pretraining on Common Voice, SMT fine-tuning on FLEURS/CoVoST-2 with both audio and transcription inputs, and SRT fine-tuning with only audio input generating both transcription and translation. A frozen Whisper-large-v3 encoder with trainable speech adapter (Q-Former + MLP) converts audio to fixed-length queries, while minimalist instruction tokens distinguish between tasks. The method uses LoRA for efficient LLM fine-tuning and achieves nearly 3x faster inference through optimized audio feature compression.

## Key Results
- Achieves state-of-the-art average BLEU performance of 21.4 on 15×14 translation directions
- Requires fewer than 10 hours of speech data per language for training
- Demonstrates effective scaling from 52 to 430 hours of speech data on CoVoST-2 dataset
- Provides 3x faster inference compared to Qwen2-Audio model through optimized speech adapter design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning enables MLLMs to leverage pre-existing LLM translation capabilities for S2TT tasks.
- Mechanism: The three-stage curriculum (ASR → SMT → SRT) progressively builds translation skills, starting with speech transcription, then adding cross-lingual transfer, and finally combining both into S2TT. Each stage resumes from the previous checkpoint.
- Core assumption: LLMs possess robust multilingual translation capabilities that can be transferred to speech translation through sequential fine-tuning.
- Evidence anchors:
  - [abstract]: "we propose a three-stage curriculum learning strategy that leverages the machine translation capabilities of large language models and adapts them to S2TT tasks"
  - [section 2.3]: "The training proceeds sequentially through these three stages, with each stage resuming from the checkpoint of the previous one."
  - [corpus]: Weak. Related papers focus on different architectures (MCAT, SimulMEGA) rather than curriculum transfer.
- Break condition: If LLM lacks strong multilingual MT capability or if tasks are too dissimilar for effective transfer.

### Mechanism 2
- Claim: Task-specific instructions enable the model to distinguish between ASR, SMT, and SRT outputs.
- Mechanism: Simple instruction tokens like "<|eng|><|zho|>" signal the expected output format, allowing the same model to handle multiple tasks without architectural changes.
- Core assumption: Minimal instruction tokens can effectively guide the model's output generation without complex prompting.
- Evidence anchors:
  - [section 2.3]: "We designed minimalist instructions to help the model distinguish between tasks while reducing the instruction token length"
  - [section 2.3]: "This design ensures that instructions like <|eng|><|deu|> appear in the generated answers, effectively segmenting transcription and translation content"
  - [corpus]: Weak. No direct evidence about instruction design in related papers.
- Break condition: If instruction tokens fail to provide sufficient task distinction or if model cannot parse the format correctly.

### Mechanism 3
- Claim: Optimized speech adapters compress audio features efficiently, enabling faster inference without sacrificing quality.
- Mechanism: Q-Former converts audio sequences into fixed-length queries (80 queries of dimension 768), then MLP projects to LLM dimensions, reducing token count compared to raw audio.
- Core assumption: Audio feature compression can be achieved without significant information loss that would impact translation quality.
- Evidence anchors:
  - [section 2.2]: "We use a Q-Former to convert input sequences into fixed-length query representations"
  - [section 3.4]: "The speed difference is mainly due to our optimized speech adapter design, which compresses the audio features to a fixed size of 80"
  - [section 3.4]: "our method achieves nearly a 3x improvement in inference speed compared to the Qwen2-Audio model"
- Break condition: If compression ratio is too aggressive, causing loss of critical acoustic information for accurate translation.

## Foundational Learning

- Concept: Transfer learning in multilingual settings
  - Why needed here: The approach relies on transferring MT capabilities from LLMs to S2TT, which requires understanding how knowledge transfers across modalities and tasks
  - Quick check question: Can you explain the difference between zero-shot and few-shot transfer learning in the context of multilingual MT?

- Concept: Curriculum learning strategy design
  - Why needed here: The three-stage approach (ASR→SMT→SRT) is central to the method, requiring understanding of how to structure progressive learning tasks
  - Quick check question: What are the key considerations when designing a curriculum learning strategy for multimodal tasks?

- Concept: Speech feature representation and compression
  - Why needed here: The Q-Former and MLP adapter design is critical for efficient audio processing and model performance
  - Quick check question: How does reducing temporal resolution in speech features affect downstream translation accuracy?

## Architecture Onboarding

- Component map:
  - Frozen Whisper encoder → Speech adapter (Q-Former + MLP) → Frozen LLM with LoRA → Tokenizer → Output
  - Three training stages: ASR (speech→text), SMT (speech+text→translation), SRT (speech→text+translation)

- Critical path: Audio input → Whisper encoder → Q-Former → MLP → Concat with text embeddings → LLM generation
- Design tradeoffs:
  - Freezing LLM vs. fine-tuning: Faster training but potentially lower performance
  - Adapter compression (80 queries) vs. raw features: Speed vs. potential information loss
  - Sequential curriculum vs. parallel training: Better transfer but longer training time

- Failure signatures:
  - Poor WER in ASR stage → Subsequent stages will fail
  - BLEU scores plateau after SMT stage → MT capability transfer may be insufficient
  - Memory errors with large batch sizes → Adjust adapter architecture or use gradient checkpointing

- First 3 experiments:
  1. Train only ASR stage on Common Voice, verify WER improvement
  2. Train SMT stage on FLEURS, verify BLEU scores with ground truth transcriptions
  3. Train full SRT stage, compare end-to-end performance against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-SRT scale when trained with more than 10 hours of speech data per language, and what is the upper bound of this improvement?
- Basis in paper: [explicit] The paper mentions performance improvements when scaling from 52 hours to 430 hours on the CoVoST-2 dataset, but does not explore beyond this range or provide a theoretical upper bound.
- Why unresolved: The paper only tests up to 430 hours and does not investigate whether performance plateaus or continues to improve with even more data.
- What evidence would resolve it: Training and evaluating the model with significantly more speech data per language (e.g., 1000+ hours) and analyzing performance trends would clarify the scaling behavior and potential limits.

### Open Question 2
- Question: How does the performance of LLM-SRT vary across different language families and scripts, particularly for low-resource languages not well-represented in the training data?
- Basis in paper: [inferred] The paper evaluates 15×14 translation directions but does not provide a detailed breakdown of performance by language family, script complexity, or resource level.
- Why unresolved: The results are aggregated across all directions, masking potential disparities in performance for specific language groups or scripts.
- What evidence would resolve it: A detailed analysis of BLEU scores broken down by language family, script type (e.g., Latin vs. non-Latin), and resource level (e.g., high vs. low resource) would reveal performance patterns and biases.

### Open Question 3
- Question: What is the impact of unfreezing the speech encoder and LLM on both performance and computational efficiency, and is there a trade-off between the two?
- Basis in paper: [explicit] The paper mentions that unfreezing the LLM and speech encoder could yield further performance gains but would significantly increase computational costs, yet does not quantify this trade-off.
- Why unresolved: The paper only trains the adapter layers and does not experiment with full fine-tuning to measure the performance vs. efficiency trade-off.
- What evidence would resolve it: Conducting experiments with full fine-tuning of the speech encoder and LLM, measuring both performance improvements and computational overhead (e.g., training time, memory usage), would clarify the trade-off.

### Open Question 4
- Question: How robust is LLM-SRT to variations in speech quality, such as noise, accents, or speaking rates, and does the model degrade gracefully under these conditions?
- Basis in paper: [inferred] The paper evaluates performance on clean datasets (FLEURS, CoVoST-2) but does not test robustness to real-world speech variations.
- Why unresolved: The evaluation focuses on standard datasets without introducing controlled variations in speech quality.
- What evidence would resolve it: Testing the model on datasets with varying levels of noise, accents, or speaking rates and measuring performance degradation would assess robustness and identify failure modes.

## Limitations
- Performance depends heavily on the LLM's pre-existing multilingual translation capabilities, which may not generalize to all language pairs
- The three-stage curriculum design may require adaptation for languages with significantly different linguistic structures
- The study focuses on text-based evaluation metrics without addressing spoken output quality or prosody preservation

## Confidence
- **High Confidence**: The curriculum learning framework and its three-stage progression (ASR→SMT→SRT) are well-supported by experimental results and ablation studies
- **Medium Confidence**: The assertion that minimal instruction tokens are sufficient for task distinction has experimental support but could benefit from more rigorous ablation studies
- **Low Confidence**: The scalability claims to larger model sizes and more languages are extrapolated from tested configurations and may not hold for significantly larger scales

## Next Checks
1. **Cross-lingual Transfer Validation**: Conduct controlled experiments testing the curriculum approach on distant language pairs (e.g., English→Japanese, English→Arabic) to verify if the three-stage transfer learning remains effective across linguistically diverse language families.

2. **Low-Resource Stress Test**: Systematically reduce the available speech data per language (from 10 hours to 1 hour) while maintaining the same curriculum structure to identify the minimum viable training data threshold for each stage.

3. **Instruction Token Ablation**: Perform controlled experiments varying the complexity and specificity of instruction tokens (simple vs. detailed vs. no instructions) to quantify the exact contribution of instruction design to task discrimination and overall translation quality.