---
ver: rpa2
title: Towards Zero-Shot Multimodal Machine Translation
arxiv_id: '2407.13579'
source_url: https://arxiv.org/abs/2407.13579
tags:
- translation
- commute
- comet
- bleu
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces ZeroMMT, a method to train multimodal machine
  translation (MMT) models without requiring fully supervised parallel multimodal
  data. ZeroMMT starts with a pretrained text-only machine translation model and adapts
  it using two objectives: visually conditioned masked language modeling on monolingual
  English data to encourage image usage, and Kullback-Leibler divergence to maintain
  translation quality.'
---

# Towards Zero-Shot Multimodal Machine Translation

## Quick Facts
- arXiv ID: 2407.13579
- Source URL: https://arxiv.org/abs/2407.13579
- Reference count: 40
- Primary result: ZeroMMT achieves near state-of-the-art disambiguation performance without fully supervised multimodal training data

## Executive Summary
ZeroMMT introduces a method to train multimodal machine translation (MMT) models without requiring fully supervised parallel multimodal data. The approach adapts a pretrained text-only MT model using two objectives: visually conditioned masked language modeling on monolingual English data to encourage image usage, and KL divergence to maintain translation quality. Evaluated on standard MMT benchmarks and CoMMuTE, ZeroMMT achieves disambiguation performance close to state-of-the-art fully supervised models while only slightly dropping BLEU and COMET scores on general translation tasks.

## Method Summary
ZeroMMT starts with a pretrained NLLB text-only MT model and adapts it using lightweight adapters and visual projectors. The training uses two objectives: visually conditioned masked language modeling (VMLM) with 25% random masking of English captions, and KL divergence between original MT and MMT output distributions. The method is evaluated on standard MMT benchmarks and the CoMMuTE contrastive test set, with results showing competitive disambiguation performance while maintaining translation quality. The approach can be extended to new languages using only monolingual data and translated captions.

## Key Results
- ZeroMMT achieves CoMMuTE disambiguation accuracy close to fully supervised MMT models
- Translation quality (BLEU/COMET) slightly decreases compared to text-only NLLB but remains competitive
- Classifier-free guidance allows control of disambiguation-translation tradeoff at inference time
- Method successfully extended to three new languages (Arabic, Russian, Chinese) on CoMMuTE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visually conditioned masked language modeling (VMLM) forces the model to use image information for disambiguation.
- Mechanism: During training, 25% of input tokens are randomly masked. The model must predict these masked tokens using both the unmasked text context and the image embedding. This creates a learning signal that links image features to the correct semantic interpretation of ambiguous words.
- Core assumption: Random masking provides sufficient coverage of ambiguous tokens to train the model to use images effectively.
- Evidence anchors:
  - [abstract]: "visually conditioned masked language modelling on monolingual English data to encourage image usage"
  - [section]: "Similarly to VGAMT, we randomly mask 25% of the input tokens for VMLM."
  - [corpus]: Weak - the corpus neighbors focus on MMT evaluation and visual grounding but don't directly address the VMLM mechanism.

### Mechanism 2
- Claim: KL divergence between original MT and MMT outputs maintains translation quality while adapting to images.
- Mechanism: The KL loss penalizes divergence between the translation distributions of the original text-only MT model and the new MMT model. This ensures the MMT model doesn't deviate too far from the strong translation capabilities of the base model while learning to incorporate images.
- Core assumption: The original MT model provides high-quality translations that should be preserved even as the model learns to use images.
- Evidence anchors:
  - [abstract]: "Kullback-Leibler divergence to maintain translation quality"
  - [section]: "LKL = sum over j of f_θ(y_j) log(f_θ(y_j) / f_θ,β(y_j))" and "L = LVMLM + λLKL"
  - [corpus]: Weak - corpus neighbors discuss MMT evaluation but don't specifically address KL divergence for quality preservation.

### Mechanism 3
- Claim: Classifier-free guidance (CFG) allows control of the disambiguation-translation tradeoff at inference time without retraining.
- Mechanism: CFG modifies the model's output distribution by interpolating between the text-only MT model and the MMT model outputs, weighted by parameter γ. Higher γ values increase reliance on image information for disambiguation at the cost of translation fidelity.
- Core assumption: The model has learned to represent both translation quality and image-driven disambiguation in its output distribution.
- Evidence anchors:
  - [abstract]: "control the trade-off between disambiguation capabilities and translation fidelity at inference time using classifier-free guidance"
  - [section]: "bf_θ,β(y_j; y<j, x, i) = f_θ(y_j) + γ * (f_θ,β(y_j) - f_θ(y_j))"
  - [corpus]: Weak - corpus neighbors focus on MMT methods but don't discuss CFG for controlling tradeoffs.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: VMLM extends MLM by conditioning on image embeddings, requiring understanding of how MLM works as a pretraining objective.
  - Quick check question: What is the difference between standard MLM and visually conditioned MLM?

- Concept: Kullback-Leibler Divergence
  - Why needed here: The KL loss measures distributional divergence between the original MT model and adapted MMT model, requiring understanding of information theory and probabilistic modeling.
  - Quick check question: Why is KL divergence used instead of cross-entropy loss for maintaining translation quality?

- Concept: Transformer Architecture and Adapters
- Why needed here: The model uses lightweight adapters to add visual capabilities to a frozen MT model, requiring understanding of how adapters work in Transformer architectures.
- Quick check question: How do bottleneck adapters modify the behavior of a frozen Transformer model?

## Architecture Onboarding

- Component map: English caption → SigLIP image features → visual projector → concatenated with text embeddings → NLLB encoder → adapters → decoder → translation output

- Critical path: English caption → SigLIP image features → visual projector → concatenated with text embeddings → NLLB encoder → adapters → decoder → translation output

- Design tradeoffs:
  - Adapter size vs. adaptation capacity (factor reduction of 8 chosen)
  - Masking ratio (25% chosen based on VGAMT)
  - KL coefficient λ (0.1 chosen based on validation)
  - Visual encoder choice (SigLIP vs. other vision models)

- Failure signatures:
  - High CoMMuTE accuracy but low BLEU/COMET: Model using images but degrading translation quality
  - Low CoMMuTE accuracy: Model ignoring images
  - Both metrics low: Training issues or poor λ choice

- First 3 experiments:
  1. Train ZeroMMT-600M on Conceptual Captions with λ=0.1, evaluate on CoMMuTE for French
  2. Vary λ parameter (0.01, 0.1, 0.5, 1.0) and observe CoMMuTE vs. BLEU tradeoff
  3. Test CFG with different γ values on ZeroMMT-3.3B to control disambiguation level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would detecting ambiguous source sentences and selectively using images only in those cases be for improving the trade-off between disambiguation capability and translation fidelity?
- Basis in paper: [inferred] The paper mentions this as a potential next step, noting that images are not necessary to translate most English source sentences correctly.
- Why unresolved: The current ZeroMMT approach uses images for all translations, which can lead to slight drops in translation quality for unambiguous cases.
- What evidence would resolve it: Empirical comparison of a selective image usage approach versus the current full image usage approach on standard MT benchmarks and CoMMuTE, measuring both translation quality (BLEU/COMET) and disambiguation accuracy.

### Open Question 2
- Question: How does the performance of ZeroMMT vary across different types of visual ambiguity (e.g., homonyms, polysemy, anaphoric references)?
- Basis in paper: [explicit] The paper focuses on disambiguation but doesn't analyze performance across different types of ambiguity in CoMMuTE.
- Why unresolved: The paper shows overall disambiguation accuracy but doesn't break down performance by ambiguity type, which could reveal strengths and weaknesses.
- What evidence would resolve it: Detailed analysis of ZeroMMT performance on CoMMuTE instances categorized by ambiguity type, showing accuracy and confidence scores for each category.

### Open Question 3
- Question: What is the impact of different visual feature encoder choices on ZeroMMT's disambiguation capabilities versus translation quality?
- Basis in paper: [explicit] The paper includes an ablation study on visual feature choices but focuses mainly on CoMMuTE performance differences.
- Why unresolved: While the paper notes performance differences between visual encoders on CoMMuTE, it doesn't fully explore the trade-offs between disambiguation and translation quality across different encoders.
- What evidence would resolve it: Comprehensive comparison of ZeroMMT variants using different visual encoders (CLIP, SigLIP, ViT, ResNet) on both standard MT benchmarks and CoMMuTE, with analysis of the disambiguation-translation quality trade-off for each.

## Limitations

- The method shows slight drops in translation quality (BLEU/COMET) compared to text-only models when using images for all translations
- Generalization to new languages is demonstrated on only 200 sentences per language, raising questions about scalability
- Performance depends on the quality of translated captions used for training, which may introduce noise
- The optimal KL penalty coefficient λ is not fully explored across the full tradeoff space

## Confidence

- **High Confidence**: The technical implementation of ZeroMMT's training objectives (VMLM and KL divergence) is well-specified and reproducible. The classifier-free guidance mechanism for controlling disambiguation-translation tradeoff is clearly described and theoretically sound.
- **Medium Confidence**: The experimental results showing ZeroMMT's performance on CoMMuTE and standard benchmarks are robust for the tested languages (French, German, Czech). However, the generalization to new languages and the absolute magnitude of improvements relative to fully supervised methods have uncertainty.
- **Low Confidence**: The paper's claim that ZeroMMT achieves "close to SoTA" disambiguation performance requires careful interpretation - while CoMMuTE accuracy is competitive, the underlying metric measures relative perplexities rather than absolute disambiguation quality. The scalability to dozens of languages and the sensitivity to different base MT models remain open questions.

## Next Checks

1. **KL Coefficient Sensitivity Analysis**: Systematically vary λ from 0.01 to 1.0 and plot the CoMMuTE vs BLEU/COMET tradeoff curve. This would reveal whether the chosen λ=0.1 represents an optimal balance or if better tradeoffs exist for specific use cases.

2. **Targeted vs Random Masking**: Implement an alternative masking strategy that preferentially masks tokens identified as potentially ambiguous (e.g., based on part-of-speech or semantic role labeling). Compare CoMMuTE performance against the random 25% masking baseline to validate whether the current approach is optimal.

3. **New Language Scaling Test**: Evaluate ZeroMMT on the three extended languages (Arabic, Russian, Chinese) using the full CoMMuTE dataset rather than the 200-sentence subset. Additionally, test on a low-resource language not included in NLLB pretraining to assess the method's true zero-shot capabilities.