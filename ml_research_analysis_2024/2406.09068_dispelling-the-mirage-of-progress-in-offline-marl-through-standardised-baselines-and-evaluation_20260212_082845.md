---
ver: rpa2
title: Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines
  and Evaluation
arxiv_id: '2406.09068'
source_url: https://arxiv.org/abs/2406.09068
tags:
- datasets
- macql
- omar
- maicq
- medium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines the state of offline multi-agent
  reinforcement learning (MARL) by identifying significant methodological problems
  in baseline implementations and evaluation protocols. The authors demonstrate that
  well-implemented simple baselines (e.g., independent Q-learners with CQL, behavior
  cloning, independent DDPG with behavior cloning) can achieve state-of-the-art performance
  on 35 out of 47 datasets tested (approximately 75% of cases), often outperforming
  more sophisticated algorithms from the literature.
---

# Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation

## Quick Facts
- arXiv ID: 2406.09068
- Source URL: https://arxiv.org/abs/2406.09068
- Authors: Claude Formanek; Callum Rhys Tilbury; Louise Beyers; Jonathan Shock; Arnu Pretorius
- Reference count: 40
- Primary result: Well-implemented simple baselines achieve state-of-the-art performance on 75% of offline MARL datasets

## Executive Summary
This paper critically examines the state of offline multi-agent reinforcement learning (MARL) by identifying significant methodological problems in baseline implementations and evaluation protocols. The authors demonstrate that well-implemented simple baselines (e.g., independent Q-learners with CQL, behavior cloning, independent DDPG with behavior cloning) can achieve state-of-the-art performance on 35 out of 47 datasets tested (approximately 75% of cases), often outperforming more sophisticated algorithms from the literature. The study reveals inconsistencies in algorithm naming conventions, scenario selection, and evaluation methodologies across prior work. To address these issues, the authors introduce standardized baseline implementations and a straightforward evaluation methodology with clear guidelines for dataset selection, baseline comparison, training parameters, and result reporting. This work aims to improve the rigor and reproducibility of empirical science in offline MARL.

## Method Summary
The authors conducted a comprehensive survey of offline MARL literature to identify methodological inconsistencies in baseline implementations, naming conventions, and evaluation protocols. They implemented four standardized baseline algorithms (IQL+CQL, BC, IDDPG+BC, MADDPG+CQL) and tested them across 47 datasets from SMAC, MAMuJoCo, and MPE environments. Results were normalized using an "unnormalized area under the curve" metric to enable fair comparison across tasks with different reward scales. The authors compared their baselines against 10 prior works claiming state-of-the-art results, using the same datasets and evaluation protocols. They also analyzed evaluation practices in the literature, finding that 5 seeds (rather than the recommended 10) were most common, and that evaluation frequencies varied widely across papers.

## Key Results
- Well-implemented simple baselines achieve state-of-the-art performance on 35 out of 47 datasets tested (approximately 75% of cases)
- Inconsistent naming conventions for baselines (e.g., MACQL) led to fundamentally different algorithms being compared under the same name
- Prior work commonly used only 5 seeds instead of the recommended 10, potentially undermining statistical validity of results
- Performance rankings of algorithms can completely reverse depending on when training is stopped (25k vs 50k timesteps)
- The authors release standardized baseline implementations and evaluation methodology to improve reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Well-implemented simple baselines (BC, IQL+CQL, IDDPG+BC, MADDPG+CQL) can achieve state-of-the-art performance on 75% of datasets.
- Mechanism: Baseline implementations were properly standardized, avoiding the methodological inconsistencies (ambiguous naming, mismatched scenarios, poor evaluation protocols) that plagued prior work. This allowed fair comparison and revealed that algorithmic progress in the field had been overestimated.
- Core assumption: The simple baselines are correctly implemented and evaluated on the same datasets as prior work.
- Evidence anchors:
  - [abstract]: "well-implemented simple baselines (e.g., independent Q-learners with CQL, behavior cloning, independent DDPG with behavior cloning) can achieve state-of-the-art performance on 35 out of 47 datasets tested (approximately 75% of cases)"
  - [section]: "by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks"
  - [corpus]: Weak evidence; no corpus papers directly compare simple baselines to claimed SOTA in offline MARL.
- Break condition: If the baseline implementations contain bugs, if evaluation metrics differ from prior work, or if the datasets used are not truly equivalent to those in prior publications.

### Mechanism 2
- Claim: Inconsistent naming and unclear baseline specifications across papers led to incomparable results and false perceptions of progress.
- Mechanism: Different interpretations of the same baseline abbreviation (e.g., MACQL) led to fundamentally different algorithms being compared under the same name. This obscured true performance differences and made replication impossible.
- Core assumption: Authors of prior work used different implementations when claiming to use the same baseline.
- Evidence anchors:
  - [section]: "Clarity in this naming has been lacking in offline MARL literature... there does not exist a common understanding in the literature of what multi-agent CQL is, despite widespread appearance of the abbreviation, MACQL"
  - [section]: "we compare two viable candidates for MACQL... the results in Figure 2 clearly reveal their relative differences in performance"
  - [corpus]: Weak evidence; corpus papers don't discuss baseline naming inconsistencies in offline MARL.
- Break condition: If future work adopts clear, unambiguous naming conventions for baselines.

### Mechanism 3
- Claim: Evaluation protocol inconsistencies (seed count, evaluation frequency, normalization) made results unreliable and incomparable.
- Mechanism: Prior work used 5 seeds instead of the recommended 10, had inconsistent evaluation frequencies, and lacked proper normalization across tasks. This led to statistical uncertainty and incomparable results.
- Core assumption: Prior work did not follow recommended evaluation practices.
- Evidence anchors:
  - [section]: "Owing to these small sample sizes the statistical validity of results could be questioned... compared to the 10 seed standard recommended by Agarwal et al. (2022), using 5 seeds is most common"
  - [section]: "Not only are there no dimensions along which evaluation is consistent, there are also gaps in the reporting of evaluation procedures"
  - [corpus]: Weak evidence; corpus papers don't discuss evaluation protocol standards in offline MARL.
- Break condition: If future work adopts standardized evaluation protocols with appropriate seed counts and normalization.

## Foundational Learning

- Concept: Statistical significance testing
  - Why needed here: To determine whether performance differences between algorithms are meaningful or due to random variation
  - Quick check question: What p-value threshold is used to determine statistical significance in this paper?

- Concept: Dataset normalization across tasks
  - Why needed here: To enable fair comparison across different scenarios with different reward scales
  - Quick check question: How are results normalized in Figure 1?

- Concept: Baseline algorithm implementation
  - Why needed here: To ensure that simple baselines are implemented correctly and can serve as reliable reference points
  - Quick check question: Which four baseline algorithms are implemented and released by the authors?

## Architecture Onboarding

- Component map: Dataset loading -> Baseline algorithm selection -> Training loop with fixed hyperparameters -> Evaluation at regular intervals -> Statistical analysis -> Result normalization and aggregation
- Critical path: Data access -> Algorithm implementation -> Training -> Evaluation -> Comparison
- Design tradeoffs: Using author-provided datasets ensures fair comparison but requires dataset access; implementing baselines ourselves ensures correctness but may miss subtle implementation details
- Failure signatures: Inconsistent results across seeds, failure to reproduce prior work results, statistical insignificance despite apparent performance differences
- First 3 experiments:
  1. Run behavior cloning on a simple SMAC scenario with 10 seeds and verify it matches expected performance
  2. Compare IQL+CQL vs IDDPG+BC on a MAMuJoCo scenario to observe performance differences
  3. Normalize results from multiple scenarios and create a performance profile plot similar to Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would offline MARL performance change if we used standardized environment configurations across all papers rather than modified versions?
- Basis in paper: [explicit] The paper highlights that Wang et al. (2023) modified MAMuJoCo to give global observations instead of decentralized ones, and Pan et al. (2022) used a different MPE version than the standard PettingZoo implementation
- Why unresolved: The paper had to use these modified configurations to fairly compare with existing results, but this prevents determining whether the performance differences are due to algorithmic improvements or just environmental advantages
- What evidence would resolve it: Direct comparison of algorithm performance on both standard and modified environment configurations using the same datasets and baselines

### Open Question 2
- Question: What is the optimal balance between training budget and online tuning budget for offline MARL algorithms?
- Basis in paper: [explicit] The paper notes that computational budget is rarely reported in offline MARL papers, yet Figure 3 shows that stopping training at different timesteps (25k vs 50k) can completely reverse which algorithm performs better
- Why unresolved: While the paper advocates for specifying training budgets, it doesn't provide empirical guidance on how to set these budgets optimally or what trade-offs exist between training duration and tuning resources
- What evidence would resolve it: Systematic experiments varying both training and tuning budgets across multiple scenarios, measuring performance trade-offs and computational costs

### Open Question 3
- Question: How do the proposed baseline implementations compare when tested on datasets beyond SMAC, MAMuJoCo, and MPE?
- Basis in paper: [inferred] The paper demonstrates that simple baselines achieve SOTA results on 35 out of 47 datasets tested, but all these datasets come from just three environments, suggesting potential overfitting to these specific domains
- Why unresolved: The paper's conclusions about baseline effectiveness are based on a limited set of environments, and there's no evidence about generalization to other multi-agent domains like autonomous driving or communication networks
- What evidence would resolve it: Testing the same baseline implementations on diverse multi-agent datasets from other domains while maintaining the same hyperparameters and evaluation protocols

## Limitations
- The study relies on author-provided datasets and evaluation protocols from prior work, which cannot be independently verified for consistency
- The selection of baselines tested (IQL+CQL, BC, IDDPG+BC, MADDPG+CQL) may not be exhaustive and could miss other effective simple approaches
- The conclusions are based primarily on three specific MARL benchmarks (SMAC, MAMuJoCo, MPE) and may not generalize to all offline MARL scenarios

## Confidence

**High Confidence**: The finding that well-implemented simple baselines can achieve SOTA performance on 75% of tested datasets is supported by extensive empirical evidence across 47 datasets. The methodology for evaluating and comparing baselines is clearly described and reproducible.

**Medium Confidence**: The claim that naming inconsistencies and unclear baseline specifications led to false perceptions of progress is well-supported within the scope of the analyzed papers, but may not capture the full complexity of why certain algorithms were favored in the literature.

**Low Confidence**: The assertion that this work provides a complete solution to reproducibility issues in offline MARL is premature, as the field continues to evolve and new challenges may emerge.

## Next Checks

1. **Cross-validation with independent implementations**: Implement the four baseline algorithms from scratch and verify that they achieve similar performance on a subset of datasets, ensuring the results are not dependent on the specific implementations provided by the authors.

2. **Extended baseline testing**: Test additional simple baselines (e.g., different combinations of CQL with other base algorithms, or simpler approaches like greedy behavior cloning) on a representative sample of datasets to determine if the current selection of four baselines is truly representative.

3. **Statistical power analysis**: Perform a statistical power analysis on the results to determine the minimum effect size that can be reliably detected with the current experimental setup (10 seeds, evaluation frequency, etc.) and identify scenarios where additional seeds or evaluations might be necessary.