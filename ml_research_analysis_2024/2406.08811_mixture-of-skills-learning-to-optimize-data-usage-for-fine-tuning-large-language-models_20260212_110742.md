---
ver: rpa2
title: 'Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language
  Models'
arxiv_id: '2406.08811'
source_url: https://arxiv.org/abs/2406.08811
tags:
- datasets
- learning
- dataset
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning large language
  models (LLMs) on diverse, imbalanced datasets, where different skills require distinct
  data proportions. The authors propose Mixture-of-Skills (MoS), a model-agnostic
  reinforcement learning framework that dynamically optimizes data usage by learning
  to adjust dataset sampling probabilities during fine-tuning.
---

# Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models

## Quick Facts
- arXiv ID: 2406.08811
- Source URL: https://arxiv.org/abs/2406.08811
- Reference count: 20
- Primary result: MoS framework achieves +0.96, +1.15, and +2.45 performance improvements on MMLU and MT-bench benchmarks across three model backbones

## Executive Summary
This paper addresses the challenge of fine-tuning large language models on diverse, imbalanced datasets where different skills require distinct data proportions. The authors propose Mixture-of-Skills (MoS), a model-agnostic reinforcement learning framework that dynamically optimizes data usage by learning to adjust dataset sampling probabilities during fine-tuning. MoS employs a scorer network that updates based on three reward functions: transferability (cosine similarity between dataset embeddings), difficulty (relative perplexity decrease), and learning trajectory (exponential moving average smoothing). The framework demonstrates significant performance improvements over heuristic baselines while accelerating training convergence by 2.2×.

## Method Summary
MoS is a reinforcement learning framework that fine-tunes LLMs on diverse, imbalanced datasets by dynamically adjusting sampling probabilities. The core component is a scorer network ψ that outputs sampling probabilities for each dataset. During training, dataset batches are sampled according to these probabilities, and the LLM is updated using standard gradient-based optimization. Every S steps, the scorer network is updated using REINFORCE with rewards computed from three perspectives: transferability (RCOSSIM), difficulty (RDIFF), and learning trajectory (EMA smoothing). The framework is model-agnostic and can be applied to any LLM architecture.

## Key Results
- MoS outperforms heuristic baselines by +0.96, +1.15, and +2.45 in overall performance for QWEN 1.5-0.5B, GEMMA-2B, and LLAMA-3-8B respectively
- MoS accelerates training convergence by 2.2× compared to static sampling strategies
- MoS with EMA smoothing consistently outperforms variants without EMA across all three model backbones
- MoS is robust to sampling priors and maintains performance improvements across different initialization strategies

## Why This Works (Mechanism)

### Mechanism 1
The scorer network dynamically adjusts sampling probabilities based on the current learning state of the LLM, enabling adaptive rebalancing of dataset usage during fine-tuning. The scorer network ψ is updated via REINFORCE with rewards computed from transferability (cosine similarity between dataset embeddings), difficulty (relative perplexity decrease), and learning trajectory (EMA smoothing). This allows the model to focus more on datasets that are currently beneficial for overall performance.

### Mechanism 2
Using three complementary reward signals (transferability, difficulty, learning trajectory) captures different aspects of dataset utility, leading to more robust and effective fine-tuning than single-metric approaches. Transferability rewards datasets similar to others, difficulty rewards datasets that are harder to learn, and learning trajectory uses EMA to stabilize reward estimation. These together guide the scorer network to balance exploitation of easy datasets with exploration of hard ones.

### Mechanism 3
MOS accelerates training convergence by 2.2× by focusing on datasets that yield the most performance gain per update, reducing wasted gradient steps on less useful data. By dynamically upweighting beneficial datasets and downweighting less useful ones, MOS ensures each training step contributes more to overall model performance, leading to faster convergence compared to static sampling strategies.

## Foundational Learning

- **Reinforcement learning with policy gradient (REINFORCE)**: The scorer network must learn a sampling policy over datasets without direct supervision; REINFORCE provides a way to update this policy using rewards from the LLM's learning process. *Quick check: What is the update rule for the scorer network ψ in MOS, and how does it relate to the REINFORCE algorithm?*

- **Dataset heterogeneity and imbalance**: The paper addresses the challenge of fine-tuning on diverse, imbalanced datasets where different skills require different data proportions; understanding this motivates the need for dynamic rebalancing. *Quick check: How does dataset heterogeneity affect the fine-tuning process, and why might static sampling strategies be suboptimal?*

- **Exponential moving average (EMA) for reward smoothing**: EMA is used to stabilize reward estimates over time, preventing volatile updates to the scorer network that could destabilize training. *Quick check: How does EMA smoothing of rewards help in the context of dynamic dataset sampling during fine-tuning?*

## Architecture Onboarding

- **Component map**: LLM θ → Scorer network ψ → Dataset sampler → LLM θ (training loop)
- **Critical path**: 1. Initialize ψ with uniform sampling (τ = ∞) 2. For each training step: Sample dataset i ~ p_ψ(N), sample batch from Di_trn and update θ, every S steps compute rewards for all datasets and update ψ via REINFORCE 3. Repeat until convergence
- **Design tradeoffs**: Simple scorer network (MLP) vs. more complex architectures (simplicity ensures scalability and avoids overfitting), static vs. dynamic sampling (dynamic allows adaptation but adds computational overhead ~20% slower), single vs. multiple rewards (multiple rewards capture different dataset utilities but require careful balancing)
- **Failure signatures**: Scorer network collapse (all probability mass on one dataset indicates reward imbalance or poor gradient estimation), no improvement over baselines (rewards not correlated with performance or scorer not learning), instability (large fluctuations in dataset sampling probabilities or training loss)
- **First 3 experiments**: 1. Run MOS with only RCOSSIM reward on a small dataset subset; verify it upweights similar datasets 2. Run MOS with only RDIFF reward; verify it upweights difficult datasets 3. Run MOS with all rewards and EMA; compare convergence speed and final performance to heuristic baselines

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Mixture-of-Skills (MoS) scale with the number of datasets beyond the four used in the experiments? The paper mentions computational constraints limiting experiments to four datasets and notes that the performance of the approach with increasing dataset count remains unexplored.

### Open Question 2
What is the optimal architecture and complexity for the scorer network ψ in MoS? The paper mentions using a simple 2-layer MLP for the scorer network but acknowledges that it's designed for a relatively simple distribution over training datasets.

### Open Question 3
How does the choice of reward functions in MoS affect its performance on different types of tasks or domains? While the paper shows that different rewards affect different models differently, it doesn't systematically explore how these rewards might affect performance on various task types or domains.

### Open Question 4
What is the theoretical foundation for the exponential moving average (EMA) smoothing in the reward calculation, and how sensitive is MoS to the choice of the smoothing factor β? The paper introduces EMA smoothing but doesn't provide theoretical justification for its use or discuss sensitivity to the smoothing factor.

## Limitations
- Limited dataset and task coverage with only four datasets (~103K examples) raises questions about generalization to more diverse or larger-scale scenarios
- Computational overhead of ~20% slower training due to scorer network updates, with unclear scalability to larger models
- Limited model diversity with experiments only on three model sizes (0.5B, 2B, 8B parameters), leaving effectiveness on much larger models unverified

## Confidence
- **High Confidence**: The core mechanism of using REINFORCE to update sampling probabilities based on learning signals is well-established in RL literature
- **Medium Confidence**: Claims about MoS accelerating convergence by 2.2× and specific performance improvements are based on controlled experiments but may not generalize due to limited dataset diversity
- **Low Confidence**: The extension MoSpec for task-specific fine-tuning is only briefly mentioned with minimal experimental validation

## Next Checks
1. **Scale Test**: Implement MoS on a 70B+ parameter model (e.g., Llama 2 70B) with the same four datasets to verify the framework scales and maintains performance improvements
2. **Dataset Diversity Test**: Add two more diverse datasets (e.g., code generation and commonsense reasoning) to the existing four, doubling the total dataset count
3. **Reward Ablation Stress Test**: Run comprehensive ablation studies where each reward component (transferability, difficulty, EMA) is removed individually on all three model backbones to quantify exact contributions and identify detrimental conditions