---
ver: rpa2
title: 'FonTS: Text Rendering with Typography and Style Controls'
arxiv_id: '2412.00136'
source_url: https://arxiv.org/abs/2412.00136
tags:
- text
- style
- image
- rendering
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FonTS introduces a two-stage DiT-based pipeline for text rendering
  with precise word-level typography and style controls. It employs Typography Control
  fine-tuning (TC-FT) with enclosing typography control tokens (ETC-tokens) for fine-grained
  typographic control, and a text-agnostic Style Control Adapter (SCA) to prevent
  content leakage while ensuring style consistency.
---

# FonTS: Text Rendering with Typography and Style Controls

## Quick Facts
- arXiv ID: 2412.00136
- Source URL: https://arxiv.org/abs/2412.00136
- Authors: Wenda Shi; Yiren Song; Dengming Zhang; Jiaming Liu; Xingxing Zou
- Reference count: 40
- One-line primary result: FonTS achieves 63.64% Font-Con, 55.00% Word-Acc, and 31.82% Style-Con in text rendering with word-level typography and style controls.

## Executive Summary
FonTS introduces a two-stage DiT-based pipeline for text rendering with precise word-level typography and style controls. It employs Typography Control fine-tuning (TC-FT) with enclosing typography control tokens (ETC-tokens) for fine-grained typographic control, and a text-agnostic Style Control Adapter (SCA) to prevent content leakage while ensuring style consistency. The approach leverages HTML-rendering to create the first word-level controllable dataset. Experimental results show superior font consistency, word-level control, and style consistency compared to baselines, while maintaining strong OCR accuracy and CLIP scores.

## Method Summary
FonTS uses a two-stage DiT-based pipeline to achieve precise word-level typography and style controls in text rendering. The first stage involves Typography Control fine-tuning (TC-FT) with enclosing typography control tokens (ETC-tokens) to enable fine-grained typographic control. The second stage employs a text-agnostic Style Control Adapter (SCA) to prevent content leakage and enhance style consistency. The method is trained on a dataset generated by HTML rendering, creating the first word-level controllable dataset for text rendering tasks.

## Key Results
- Superior font consistency: 63.64% Font-Con compared to baselines
- Strong word-level control: 55.00% Word-Acc demonstrating precise typographic feature application
- Enhanced style consistency: 31.82% Style-Con while preventing content leakage from style images

## Why This Works (Mechanism)

### Mechanism 1
ETC-tokens enable precise word-level application of typographic features by training both the tokens and joint text attention together. ETC-tokens act as start/end delimiters for specific words, and their training is cooperative with the joint text attention layers, allowing the model to learn not just the attribute but also its exact location. Core assumption: T5 text embeddings feed directly into DiT backbone attention, making token-based modifiers effective when trained jointly with attention modules.

### Mechanism 2
The text-agnostic style control adapter (SCA) prevents content leakage by using a decoupled joint attention mechanism and a text-insensitive image encoder. SCA introduces additional joint attention layers that process image features separately, with only image-related parameters trainable. CLIP is chosen because its visual embeddings are text-insensitive, reducing content leakage from style images. Core assumption: Style images carry text information that can leak into generated content; decoupling attention and using text-insensitive encoders mitigates this.

### Mechanism 3
The two-stage training of SCA (general image-text pairs first, then artistic text images) improves style consistency without harming content accuracy. Phase 1 pretrains SCA on general high-aesthetic images to learn broad style control; Phase 2 fine-tunes on artistic text images to adapt to ATR tasks while avoiding content leakage. Core assumption: General pretraining builds robust style control, and subsequent fine-tuning on artistic text data adapts the model to ATR without reintroducing content leakage.

## Foundational Learning

- **Diffusion Transformer (DiT) architecture and Rectified Flow**: Understanding how DiT regresses a vector field and uses conditional flow matching is essential to grasp TC-FT and SCA integration. Quick check: What is the role of the conditional vector field ut(z|ϵ) in the DiT training objective?

- **Parameter-efficient fine-tuning (PEFT) and adapter modules**: TC-FT and SCA both use PEFT approaches (training only 5% of parameters) to add new capabilities without full fine-tuning; knowing how adapters work is key to understanding design choices. Quick check: How does the decoupled joint attention in SCA differ from standard cross-attention in terms of parameter sharing?

- **Text rendering tasks: Basic, Artistic, and Scene Text Rendering**: The paper evaluates on three distinct tasks with different requirements (font consistency, style consistency, content accuracy); understanding these distinctions is crucial for interpreting results. Quick check: What is the main difference between font consistency in BTR and style consistency in ATR?

## Architecture Onboarding

- **Component map**: Flux.1-dev (DiT backbone) → T5 encoder (text) + CLIP encoder (coarse text) → joint text/image attention → SCA adapters (on MM-DiT and Single-DiT) → ETC-tokens (in T5 input)
- **Critical path**: T5 → ETC-tokens → joint text attention (TC-FT) → decoupled joint attention (SCA) → DiT backbone → image generation
- **Design tradeoffs**: Using ETC-tokens in T5 instead of CLIP for fine-grained control, but requiring joint training with attention layers. Choosing CLIP over SigLIP for SCA to avoid content leakage, at the cost of weaker OCR capability. Two-stage SCA training increases complexity but improves style consistency and prevents leakage.
- **Failure signatures**: ETC-tokens not localizing effects → check if joint text attention is trainable. Content leakage in ATR → verify image encoder choice and decoupled attention implementation. Degraded text accuracy in BTR → inspect whether T5 was fine-tuned without preserving text rendering capability.
- **First 3 experiments**: 1) Test ETC-tokens with single-token approach (non-enclosing) to confirm need for enclosing tokens. 2) Replace CLIP with SigLIP in SCA and observe content leakage in artistic text rendering. 3) Skip Phase 2 of SCA training and measure style consistency degradation in ATR.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of ETC-tokens compare to other word-level control methods across different text rendering tasks? The paper only focuses on BTR in the ablation study and does not provide comprehensive comparison across BTR, ATR, and STR tasks.

- **Open Question 2**: What is the impact of different scale values for the style image on the style consistency and content accuracy in ATR? The paper mentions scale = 0.9 leads to content leakage but does not provide detailed analysis of how different scale values affect the trade-off between style consistency and content accuracy.

- **Open Question 3**: How does the two-stage training approach for SCA compare to other training strategies? The paper does not compare the two-stage approach to single-stage training or training on different datasets.

- **Open Question 4**: What is the impact of the regularization prefix ('sks') on the TC-Dataset on the model's ability to generalize to unseen text and avoid language drift? The paper mentions it helps alleviate detachment but does not provide detailed analysis of its impact on generalization ability.

- **Open Question 5**: How does the proposed method handle multilingual text rendering? The paper mentions plans to explore its extension to multilingual rendering in future work but does not provide any results or analysis on multilingual text rendering.

## Limitations

- **Data Dependence**: The approach relies on HTML-rendered datasets (TC-Dataset and SC-Dataset) totaling approximately 600k image-text pairs. The quality and diversity of these synthetic datasets directly impact performance, and there's uncertainty about whether HTML rendering captures all real-world typography scenarios.

- **Architecture Specificity**: The method is built on Flux.1-dev (DiT backbone) and uses specific components like T5 encoder and CLIP visual encoder. Performance may not transfer to other backbone architectures, and the approach's effectiveness depends heavily on the compatibility between ETC-tokens and the specific attention mechanisms in Flux.1-dev.

- **Evaluation Scope**: While the paper demonstrates improvements in font consistency, word-level control, and style consistency, these metrics are evaluated on synthetic data. Real-world deployment scenarios with user-generated content and diverse typography requirements may reveal limitations not captured in controlled evaluations.

## Confidence

- **High Confidence**: The effectiveness of ETC-tokens for word-level typographic control (63.64% Font-Con improvement) is well-supported by the mechanism and experimental results. The joint training of ETC-tokens with text attention layers shows clear benefit.
- **Medium Confidence**: The two-stage SCA training approach shows strong style consistency improvements (31.82% Style-Con), but the generalization to unseen style images and the long-term stability of the decoupled attention mechanism require further validation.
- **Low Confidence**: The claim of preventing content leakage while maintaining style consistency depends heavily on the CLIP encoder choice and the specific implementation of decoupled attention. Alternative encoder choices or implementation variations may yield different results.

## Next Checks

1. **ETC-Token Locality Test**: Conduct controlled experiments varying the number of tokens between enclosing ETC-tokens to quantify the minimum token distance required for accurate localization. This validates the claim that ETC-tokens work through cooperative training with joint text attention layers.

2. **Encoder Sensitivity Analysis**: Systematically replace CLIP with text-sensitive encoders (SigLIP, BLIP) in the SCA and measure content leakage across multiple style image types. This directly tests the mechanism's reliance on text-insensitive visual embeddings for preventing content leakage.

3. **Generalization Benchmark**: Evaluate the trained model on real-world typography datasets (e.g., scanned documents, street signs) that were not part of the HTML-rendered training data. This tests whether the synthetic training data captures sufficient real-world complexity and whether the learned control mechanisms generalize beyond synthetic scenarios.