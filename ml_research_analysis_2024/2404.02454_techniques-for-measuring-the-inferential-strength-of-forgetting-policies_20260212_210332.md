---
ver: rpa2
title: Techniques for Measuring the Inferential Strength of Forgetting Policies
arxiv_id: '2404.02454'
source_url: https://arxiv.org/abs/2404.02454
tags:
- probability
- forgetting
- theory
- measures
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for measuring inferential strength
  changes in knowledge theories when symbols are forgotten using different forgetting
  operators. The core idea is to use model counting and probability theory to define
  loss functions that quantify the difference between original and revised theories.
---

# Techniques for Measuring the Inferential Strength of Forgetting Policies

## Quick Facts
- arXiv ID: 2404.02454
- Source URL: https://arxiv.org/abs/2404.02454
- Authors: Patrick Doherty; Andrzej Szalas
- Reference count: 40
- One-line primary result: Introduces a framework using model counting and probability theory to measure inferential strength loss when symbols are forgotten from knowledge theories

## Executive Summary
This paper presents a formal framework for quantifying the inferential strength of knowledge theories when symbols are forgotten using different forgetting operators. The framework defines loss functions based on model counting and probability theory to measure the difference between original theories and their revisions after applying strong or weak forgetting. The authors provide a computational methodology using ProbLog to implement these measures and demonstrate their application to both propositional and first-order logic theories.

## Method Summary
The method transforms input theories into stratified logic programs using specific transformation rules, then employs ProbLog to compute probabilities of the original and revised theories after applying forgetting operators. The framework defines two types of loss measures: model-counting-based and probability-based, which quantify the difference between original theories and their weakened versions after forgetting. The computational process involves three main steps: transforming the theory into a ProbLog program, specifying probability distributions on worlds, and computing loss measures through probability queries.

## Key Results
- Defines loss functions that quantify inferential strength changes when symbols are forgotten
- Establishes theoretical properties of strong forgetting (preserves necessary conditions) and weak forgetting (preserves sufficient conditions)
- Provides computational methodology using ProbLog for both propositional and first-order theories
- Demonstrates framework's applicability to theory abstraction, query optimization, and information hiding scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework measures inferential strength loss by comparing the probability (or model count) of the original theory with the revised theory after applying forgetting operators.
- Mechanism: For any forgetting policy, strong forgetting yields a theory with fewer models (or lower probability) than the original, while weak forgetting yields a theory with more models (or higher probability). The difference between these probabilities quantifies the loss in inferential strength.
- Core assumption: The forgetting operators preserve the structure of the theory sufficiently to allow meaningful probability comparisons.
- Evidence anchors:
  - [abstract]: "The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory."
  - [section 2.2]: "By a world for a theory (formula) T, one means any assignment of truth values to propositional variables in V^0_T..."
  - [corpus]: Weak (only general AI literature on forgetting, no direct evidence of this specific mechanism).
- Break condition: If the forgetting operator does not preserve the necessary structure of the theory, the probability comparisons become meaningless.

### Mechanism 2
- Claim: The framework uses auxiliary variables and stratified logic programs to transform arbitrary propositional formulas into a form suitable for probabilistic inference in ProbLog.
- Mechanism: The transformation introduces auxiliary variables for each subformula, ensuring that the resulting program is stratified and preserves equivalence. This allows ProbLog to compute the probability of the original and revised theories accurately.
- Core assumption: The transformation preserves the semantics of the original formula and is compatible with ProbLog's inference engine.
- Evidence anchors:
  - [section 3.2]: "The rules of a program π^0_A encoding A, are constructed as shown in Table 1..."
  - [section 6]: "The transformation provided in Table 3 extends the one for propositional formula given in Table 1..."
  - [corpus]: Weak (general knowledge of ProbLog and logic programming, but no direct evidence of this specific transformation).
- Break condition: If the transformation introduces errors or the resulting program is not stratified, ProbLog's inference will be incorrect.

### Mechanism 3
- Claim: The framework generalizes to 1st-order logic by using ground atoms and extending the transformation rules to handle quantifiers.
- Mechanism: The transformation for 1st-order logic uses auxiliary relation symbols and extends the rules to handle existential and universal quantifiers. This allows the framework to measure inferential strength loss in theories with relations and quantifiers.
- Core assumption: The extension of the transformation rules preserves the semantics of the original 1st-order formula and is compatible with ProbLog's inference engine.
- Evidence anchors:
  - [section 6]: "The extension of definitions of probability structures...is now straightforward when it is assumed that worlds assign truth values to ground atoms rather than to propositional variables..."
  - [section 6]: "Assuming that PW can be specified in ProbLog, a ProbLog program, Π^p_A, can be defined that computes the probability of A wrt probability distribution PW..."
  - [corpus]: Weak (general knowledge of 1st-order logic and ProbLog, but no direct evidence of this specific extension).
- Break condition: If the extension of the transformation rules introduces errors or the resulting program is not stratified, ProbLog's inference will be incorrect.

## Foundational Learning

- Concept: Model counting and probability theory
  - Why needed here: The framework measures inferential strength loss by comparing the probability (or model count) of the original theory with the revised theory after applying forgetting operators.
  - Quick check question: What is the relationship between the number of models of a theory and its probability?

- Concept: Stratified logic programs and ProbLog
  - Why needed here: The framework uses auxiliary variables and stratified logic programs to transform arbitrary propositional formulas into a form suitable for probabilistic inference in ProbLog.
  - Quick check question: What is the difference between a stratified and non-stratified logic program?

- Concept: 1st-order logic and quantifiers
  - Why needed here: The framework generalizes to 1st-order logic by using ground atoms and extending the transformation rules to handle quantifiers.
  - Quick check question: What is the difference between a ground atom and a non-ground atom in 1st-order logic?

## Architecture Onboarding

- Component map: Transformation Rules -> ProbLog Program -> Forgetting Operators -> Loss Measures
- Critical path: The critical path is the transformation of the original theory into a ProbLog program, followed by the application of the forgetting operators and the computation of the loss measures.
- Design tradeoffs: The framework trades off expressiveness (ability to handle arbitrary theories) for computational efficiency (need to transform formulas into ProbLog programs).
- Failure signatures: The framework may fail if the forgetting operators do not preserve the necessary structure of the theory, if the transformation rules introduce errors, or if ProbLog's inference engine is unable to handle the resulting program.
- First 3 experiments:
  1. Apply the forgetting operators to a simple propositional theory and verify that the loss measures are computed correctly.
  2. Extend the framework to handle a 1st-order theory with relations and quantifiers, and verify that the loss measures are computed correctly.
  3. Experiment with different forgetting policies and operators to see how they affect the loss measures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed loss measures perform in practice for large-scale theories and real-world applications?
- Basis in paper: [explicit] The paper mentions applications like theory abstraction, query optimization, and information hiding, but does not provide empirical evaluations on large-scale theories.
- Why unresolved: The paper focuses on theoretical definitions and properties of the loss measures, but lacks experimental validation on real-world data and large-scale theories.
- What evidence would resolve it: Empirical studies comparing the proposed loss measures on real-world knowledge bases, evaluating their computational efficiency and accuracy in various forgetting scenarios.

### Open Question 2
- Question: Can the proposed framework be extended to handle more expressive logical formalisms beyond classical propositional and first-order logic?
- Basis in paper: [inferred] The paper mentions that the techniques are "much more general" and could apply to other areas, but does not explore extensions to more expressive logics like modal logics or description logics.
- Why unresolved: The paper primarily focuses on classical propositional and first-order logic, leaving open the question of how the framework would handle more expressive formalisms commonly used in knowledge representation.
- What evidence would resolve it: Development and evaluation of the loss measures in the context of more expressive logics, demonstrating their applicability and providing insights into any limitations or challenges that arise.

### Open Question 3
- Question: How do the loss measures behave under different probability distributions on worlds, and what are the implications for practical applications?
- Basis in paper: [explicit] The paper introduces both model-counting-based and probability-based loss measures, but does not extensively explore the impact of different probability distributions on the results.
- Why unresolved: While the paper acknowledges the importance of probability distributions, it does not provide a detailed analysis of how different distributions affect the loss measures or offer guidelines for selecting appropriate distributions in practice.
- What evidence would resolve it: Empirical studies investigating the behavior of the loss measures under various probability distributions, identifying patterns and providing recommendations for their use in different applications.

## Limitations
- The framework relies heavily on the correctness of forgetting operators and their preservation of necessary/sufficient conditions
- Computational methodology using ProbLog may face scalability issues with larger theories
- Extension to first-order logic assumes finite domains, limiting applicability to infinite structures
- Loss measures are defined relative to specific probability distributions, making cross-distribution comparisons problematic

## Confidence
- High confidence in the theoretical foundation linking model counting to probability-based loss measures (well-established in probabilistic reasoning literature)
- Medium confidence in the computational methodology using ProbLog for propositional theories (practical implementation details not fully specified)
- Low confidence in the first-order logic extension (only briefly sketched with minimal implementation details)

## Next Checks
1. Verify the forgetting operators preserve the claimed necessary/sufficient conditions through formal proofs for edge cases
2. Benchmark the ProbLog-based computation on theories of increasing size to identify scalability thresholds
3. Implement the first-order extension on a non-trivial theory with relations and quantifiers to validate the transformation rules