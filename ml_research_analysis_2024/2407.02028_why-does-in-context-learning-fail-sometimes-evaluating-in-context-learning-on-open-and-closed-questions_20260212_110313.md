---
ver: rpa2
title: Why does in-context learning fail sometimes? Evaluating in-context learning
  on open and closed questions
arxiv_id: '2407.02028'
source_url: https://arxiv.org/abs/2407.02028
tags:
- context
- questions
- question
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how in-context learning performance varies
  with context relevance for open versus closed questions. The authors create a benchmark
  of 160 physics and computer science questions with varying difficulty and originality,
  paired with four types of context (no context, irrelevant, vague, relevant).
---

# Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions

## Quick Facts
- arXiv ID: 2407.02028
- Source URL: https://arxiv.org/abs/2407.02028
- Reference count: 36
- Key outcome: Context relevance is positively correlated with performance for closed questions but negatively correlated for open questions requiring explanations

## Executive Summary
This paper investigates how in-context learning performance varies with context relevance for open versus closed questions. The authors create a benchmark of 160 physics and computer science questions with varying difficulty and originality, paired with four types of context (no context, irrelevant, vague, relevant). They find that context relevance is positively correlated with performance for closed questions (e.g., multiple-choice) but negatively correlated for open questions requiring explanations. Specifically, responses with no context or irrelevant context perform better than those with highly relevant context for open questions.

## Method Summary
The authors created a benchmark dataset of 160 physics and computer science questions categorized as either open (requiring explanations) or closed (multiple-choice, true/false). Each question was paired with four types of context: no context, irrelevant context, vague context, and highly relevant context. The questions varied in difficulty and originality levels. Large language models were then evaluated on these questions with each context type to measure how context relevance affects performance differently for open versus closed question types.

## Key Results
- Context relevance is positively correlated with performance for closed questions
- Context relevance is negatively correlated with performance for open questions requiring explanations
- Responses with no context or irrelevant context perform better than those with highly relevant context for open questions

## Why This Works (Mechanism)
The paper doesn't provide a detailed mechanistic explanation for why highly relevant context harms open question performance while helping closed questions. The authors suggest this may be related to how context influences reasoning processes differently depending on question type, but the specific cognitive mechanisms remain unexplored in this work.

## Foundational Learning
- **In-context learning**: Why needed - forms the basis of how LLMs use provided examples/context to answer questions; Quick check - models can complete tasks with just a few examples without parameter updates
- **Context relevance**: Why needed - determines how much retrieved information aligns with the question being asked; Quick check - measured by semantic similarity between context and question
- **Open vs closed questions**: Why needed - represents different cognitive demands on reasoning and explanation; Quick check - open questions require justification, closed questions require selection from options

## Architecture Onboarding
Component map: Question Type (Open/Closed) -> Context Type (No/Irrelevant/Vague/Relevant) -> LLM Performance
Critical path: Question selection → Context manipulation → LLM evaluation → Performance measurement
Design tradeoffs: Controlled context manipulation provides clean experimental conditions but may not reflect real-world retrieval scenarios
Failure signatures: Open questions show decreased performance with relevant context, suggesting potential bias introduction
First experiments:
1. Replicate the paradigm across different subject domains to test generalizability
2. Implement continuous rather than discrete context relevance measures
3. Conduct controlled experiments to isolate specific bias mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses specifically on physics and computer science domains with a relatively small sample size of 160 questions, which may limit generalizability to other subject areas or larger-scale applications
- The binary classification of questions as "open" versus "closed" is a simplification that may not capture the full spectrum of question types and their varying demands on reasoning and explanation
- The artificial manipulation of context types may not reflect real-world retrieval scenarios where context relevance exists on a continuum

## Confidence
- Core finding about opposite effects of context relevance on open versus closed questions: High confidence
- Interpretation that highly relevant context introduces bias for open questions: Medium confidence
- Claims about implications for retrieval-augmented generation systems: Medium confidence

## Next Checks
1. Test the same experimental paradigm across additional domains (e.g., medicine, law, humanities) to assess domain generality of the open/closed question distinction
2. Implement a continuous measure of context relevance rather than discrete categories to better model real-world retrieval systems
3. Conduct ablation studies isolating specific mechanisms (e.g., confirmation bias, reasoning shortcuts) that might explain why relevant context harms open question performance