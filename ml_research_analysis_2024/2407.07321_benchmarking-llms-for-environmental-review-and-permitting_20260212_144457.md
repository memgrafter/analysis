---
ver: rpa2
title: Benchmarking LLMs for Environmental Review and Permitting
arxiv_id: '2407.07321'
source_url: https://arxiv.org/abs/2407.07321
tags:
- evaluation
- questions
- question
- context
- nepa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces NEPAQuAD, the first benchmark for evaluating
  large language models on Environmental Impact Statement documents, along with MAPLE,
  a modular evaluation pipeline. Experiments with five frontier LLMs show that retrieval-augmented
  generation substantially outperforms raw PDF context, and gold passage context achieves
  the highest accuracy.
---

# Benchmarking LLMs for Environmental Review and Permitting

## Quick Facts
- arXiv ID: 2407.07321
- Source URL: https://arxiv.org/abs/2407.07321
- Reference count: 40
- Primary result: NEPAQuAD benchmark shows retrieval-augmented generation substantially outperforms raw PDF context for regulatory document comprehension

## Executive Summary
This paper introduces NEPAQuAD, the first benchmark for evaluating large language models on Environmental Impact Statement documents, along with MAPLE, a modular evaluation pipeline. The authors evaluate five frontier LLMs (Claude Sonnet 3.5, Gemini 1.5 Pro, GPT-4, Llama 3.1, Mistral-7B-Instruct) across 1590 question-answer-proof triplets from 9 EIS documents. Experiments demonstrate that RAG-based retrieval substantially outperforms raw PDF context, with gold passage context achieving the highest accuracy. All models perform best with gold context, confirming that context relevance is critical for regulatory reasoning tasks. The work highlights both the potential and current limitations of LLMs in specialized domains requiring complex document comprehension.

## Method Summary
The study introduces NEPAQuAD v1.0, a benchmark containing 1590 question-answer-proof triplets from 9 Environmental Impact Statement documents. The MAPLE pipeline provides a modular framework for evaluating LLMs across four context types: no context, full PDF context, RAG-retrieved context, and gold passage context. The framework abstracts multiple LLM providers (Azure OpenAI, AWS Bedrock, Google Vertex AI, HuggingFace) and uses the RAGAS evaluation framework to compute answer correctness scores combining factual correctness (75%) and semantic correctness (25%). All evaluations are conducted using zero-shot prompting without fine-tuning or few-shot examples.

## Key Results
- Retrieval Augmented Generation (RAG) approaches substantially outperform raw PDF document contexts across all models
- All models achieve their highest performance when provided with gold passage context
- Despite Gemini 1.5 Pro's 1.5M token capacity, performance drops with full PDF context, indicating long-context models still struggle with regulatory reasoning
- Llama 3.1 and Mistral-7B-Instruct underperform compared to closed-source models across most context types

## Why This Works (Mechanism)

### Mechanism 1
RAG-based retrieval outperforms raw PDF context in regulatory document comprehension by retrieving only the top-3 most relevant text chunks from a vector database, reducing noise and focusing model attention on directly relevant information. Models struggle with processing long, noisy documents but excel when given focused, relevant context.

### Mechanism 2
Gold passage context yields highest LLM accuracy in regulatory reasoning tasks by providing the exact text excerpt from which the question was generated, eliminating retrieval uncertainty and ensuring perfect relevance alignment. Models perform best when context perfectly matches the question's information needs.

### Mechanism 3
Long-context models still struggle with regulatory reasoning even with extended token limits because document length and complexity introduce semantic noise that overwhelms even long-context models. Token length alone doesn't solve comprehension of complex regulatory documents.

## Foundational Learning

- **Regulatory reasoning and environmental impact assessment**: Understanding environmental laws, permitting processes, and regulatory compliance is essential for the NEPA domain. Quick check: Can you explain the difference between an Environmental Assessment (EA) and an Environmental Impact Statement (EIS)?

- **Retrieval Augmented Generation (RAG) and vector databases**: RAG retrieves relevant document chunks for context-driven evaluation using vector embeddings and cosine similarity. Quick check: How does RAG determine which document chunks are most relevant to a given question?

- **Benchmark construction and quality metrics**: The dataset was generated using GPT-4 and NEPA experts, requiring understanding of question taxonomy, context selection, and evaluation protocols. Quick check: What are the key differences between closed, convergent, and divergent question types in this benchmark?

## Architecture Onboarding

- **Component map**: Data Loader → LLM Handler → Evaluator → Metrics Module → Result Storage
- **Critical path**: Data Loader → LLM Handler → Evaluator → Metrics Module → Result Storage
- **Design tradeoffs**: Immediate persistence vs batch processing (chosen for resilience), provider abstraction vs provider-specific optimizations (unified interface), RAGAS evaluation vs simpler metrics (more comprehensive but heavier)
- **Failure signatures**: NaN values in RAGAS metrics (evaluation framework instability), provider authentication failures (credential issues), context type mismatches (data loader detection problems)
- **First 3 experiments**: 1) Run basic no-context evaluation on small sample to verify LLM handler functionality, 2) Test PDF context mode with single short document to validate content extraction, 3) Execute RAG context mode with pre-built vector database to confirm retrieval pipeline

## Open Questions the Paper Calls Out

1. **Model comparison in complex inference**: How do different LLM models compare in handling complex inference and problem-solving questions in the NEPA domain when provided with context-enhanced responses? The paper notes problem-solving questions show varied results but lacks detailed per-model analysis.

2. **Token truncation impact**: What is the impact of token truncation on model performance when processing full PDF documents, and how does this vary across different models and document lengths? The paper acknowledges truncation limitations but doesn't provide systematic analysis.

3. **Open-source vs closed-source fine-tuning**: How does the performance of open-source LLMs compare to closed-source models when fine-tuned on domain-specific NEPA data? The paper evaluates base models only, suggesting potential for improvement through domain adaptation.

## Limitations
- Limited model diversity and scale: Only five frontier models evaluated on single environmental domain
- Gold passage performance ceiling: Perfect retrieval results may be unattainable in practical applications
- Zero-shot evaluation constraints: Performance differences could shift with prompt engineering or instruction tuning

## Confidence
- **High Confidence**: RAG substantially outperforms raw PDF context (well-supported, aligns with established knowledge)
- **Medium Confidence**: Gold passage superiority and performance drop with full PDF context (supported by experiments but dataset-dependent)
- **Low Confidence**: General claims about model unsuitability for long-context regulatory reasoning (based on single domain, five models)

## Next Checks
1. **Retrieval error analysis**: Conduct systematic study of RAG retrieval failures by comparing retrieved passages against gold passages to identify specific failure patterns
2. **Cross-domain replication**: Apply MAPLE framework to regulatory documents from different domain (e.g., healthcare compliance) to test generalizability
3. **Prompt optimization study**: Systematically test different prompt templates and few-shot examples specifically designed for regulatory document comprehension