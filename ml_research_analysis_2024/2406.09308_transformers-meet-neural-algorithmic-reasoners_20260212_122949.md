---
ver: rpa2
title: Transformers meet Neural Algorithmic Reasoners
arxiv_id: '2406.09308'
source_url: https://arxiv.org/abs/2406.09308
tags:
- algorithmic
- arxiv
- neural
- transnar
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TransNAR, a hybrid architecture that combines
  the language understanding capabilities of Transformers with the robust algorithmic
  reasoning of pre-trained graph neural network-based neural algorithmic reasoners
  (NARs). TransNAR addresses the brittleness of Transformers in algorithmic reasoning
  tasks, particularly in out-of-distribution settings where generalization is crucial.
---

# Transformers meet Neural Algorithmic Reasoners

## Quick Facts
- arXiv ID: 2406.09308
- Source URL: https://arxiv.org/abs/2406.09308
- Authors: Wilfried Bounsi; Borja Ibarz; Andrew Dudzik; Jessica B. Hamrick; Larisa Markeeva; Alex Vitvitskyi; Razvan Pascanu; Petar Veličković
- Reference count: 40
- Primary result: TransNAR achieves up to 11% improvement in CLRS score over baseline Transformers for algorithmic reasoning tasks

## Executive Summary
This work introduces TransNAR, a hybrid architecture that combines the language understanding capabilities of Transformers with the robust algorithmic reasoning of pre-trained graph neural network-based neural algorithmic reasoners (NARs). TransNAR addresses the brittleness of Transformers in algorithmic reasoning tasks, particularly in out-of-distribution settings where generalization is crucial. The proposed method integrates NAR embeddings into the Transformer via cross-attention, enabling the model to leverage algorithmic reasoning robustness without losing language understanding capabilities.

## Method Summary
TransNAR employs a two-phase training procedure: first pre-training a NAR on CLRS-30 algorithmic tasks, then fine-tuning a hybrid Transformer-NAR model on text-based algorithmic problems. The architecture uses gated cross-attention layers to allow token embeddings to condition on NAR node embeddings, with randomized positional encoding to improve out-of-distribution generalization. The NAR remains frozen during TransNAR training to preserve its robustness, while the Transformer learns to effectively utilize the NAR's algorithmic representations.

## Key Results
- TransNAR achieves up to 11% improvement in CLRS score compared to baseline Transformers
- Out-of-distribution generalization improves by up to 4× larger input sizes
- Randomized positional encoding provides significant gains on both baselines and TransNAR
- TransNAR maintains high shape and parse scores while improving content accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention layers allow Transformers to condition token embeddings on NAR node embeddings, enabling the model to leverage algorithmic reasoning robustness without losing language understanding.
- Mechanism: The Transformer's token representations are processed normally, then gated cross-attention layers are interleaved, where queries are generated from tokens and keys/values from NAR embeddings. This lets the Transformer "consult" the NAR as a tool during inference.
- Core assumption: NAR embeddings contain robust algorithmic representations that are accessible to Transformers through cross-attention without requiring full end-to-end training of the NAR.
- Evidence anchors: [abstract] "To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR."

### Mechanism 2
- Claim: Randomized positional encoding enables better out-of-distribution generalization for Transformers, particularly when combined with NAR embeddings.
- Mechanism: Randomizing positional embeddings breaks the strict correspondence between position and algorithmic step, forcing the model to learn more robust positional representations. This works especially well when combined with NAR embeddings that already encode algorithmic structure.
- Core assumption: Standard positional encodings create overfitting to training sequence lengths, while randomized encodings force more general positional understanding.
- Evidence anchors: [section] "Previous work has emphasised the significant relevance of randomised positional embeddings in Transformers, especially for enabling more robust reasoning."

### Mechanism 3
- Claim: Pre-training the NAR on multiple algorithms creates a latent space that can be leveraged for novel algorithmic reasoning tasks.
- Mechanism: The NAR learns distributed representations of multiple algorithms in a shared latent space during pre-training. These representations can then be accessed by the Transformer through cross-attention, providing algorithmic reasoning capabilities without requiring the Transformer to learn algorithms from scratch.
- Core assumption: Algorithmic tasks share common computational patterns that can be captured in a shared latent space, and these patterns are useful for novel algorithms.
- Evidence anchors: [section] "Prior to the start of TransNAR fine-tuning, we pre-train the NAR to robustly execute the thirty algorithms spanned by CLRS-30..."

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The NAR component uses GNNs to process graph-structured algorithmic problems, so understanding how GNNs aggregate and update node representations is essential for understanding the architecture.
  - Quick check question: What are the key operations in a message passing neural network, and how do they differ from standard feed-forward networks?

- Concept: Cross-Attention Mechanisms
  - Why needed here: The core innovation uses cross-attention to allow Transformers to access NAR embeddings, so understanding how cross-attention differs from self-attention and how it enables information flow between modalities is critical.
  - Quick check question: How does cross-attention enable a model to use information from a different modality, and what are the key differences from standard self-attention?

- Concept: Algorithmic Reasoning and Generalization
  - Why needed here: The paper addresses algorithmic reasoning tasks and out-of-distribution generalization, so understanding what makes algorithmic reasoning challenging and how models typically fail at OOD generalization is important for appreciating the contributions.
  - Quick check question: What are the key challenges in algorithmic reasoning for neural networks, and why do standard Transformers struggle with out-of-distribution generalization?

## Architecture Onboarding

- Component map: Input → Text Encoding → Transformer Layers → Cross-Attention with NAR → Output Prediction
- Critical path: Input → Text Encoding → Transformer Layers → Cross-Attention with NAR → Output Prediction
- Design tradeoffs:
  - NAR vs full algorithmic implementation: Using pre-trained NAR provides robustness but requires graph representations
  - Frozen NAR vs trainable: Keeping NAR frozen preserves robustness but limits adaptation
  - Cross-attention frequency: More cross-attention layers provide more NAR access but increase complexity
- Failure signatures:
  - Near-zero performance on all tasks: Likely NAR embeddings not accessible or useful
  - Correct shapes but wrong content: Cross-attention not effectively decoding NAR information
  - Good in-distribution but poor OOD: Need better positional encoding or more diverse pre-training
- First 3 experiments:
  1. Verify basic functionality: Run on small in-distribution examples to ensure both Transformer and NAR components work
  2. Test NAR accessibility: Remove cross-attention and verify performance drops, confirming NAR is being used
  3. Evaluate OOD robustness: Test on out-of-distribution examples to verify the core benefit of the hybrid approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum size of input data that the TransNAR architecture can effectively handle before running out of context length?
- Basis in paper: [explicit] The paper mentions that due to tokenised representation, there are stringent limitations on how large of a problem size can be evaluated without running out of context length for the Chinchilla models.
- Why unresolved: The paper does not provide a specific maximum input size that TransNAR can handle. It only mentions that there are limitations due to context length.
- What evidence would resolve it: Experiments testing TransNAR's performance on increasingly larger input sizes until performance degrades significantly would provide evidence of the maximum effective input size.

### Open Question 2
- Question: How does the performance of TransNAR compare to traditional algorithmic solvers on the CLRS-Text benchmark tasks?
- Basis in paper: [inferred] The paper focuses on comparing TransNAR to baseline Transformer models but does not provide a comparison to traditional algorithmic solvers.
- Why unresolved: The paper's evaluation is limited to comparing TransNAR against Transformer baselines, not against traditional solvers.
- What evidence would resolve it: Benchmarking TransNAR against traditional algorithmic solvers on the same CLRS-Text tasks would provide a direct comparison of performance.

### Open Question 3
- Question: Can the knowledge acquired by the TransNAR model be effectively distilled into a vanilla Transformer model to remove the need for dual input streams?
- Basis in paper: [explicit] The paper mentions that lifting the need for a second data stream can be done by distilling the knowledge acquired by the trained TransNAR model into a vanilla Transformer model.
- Why unresolved: The paper does not explore or provide results on this distillation process.
- What evidence would resolve it: Experiments demonstrating successful distillation of TransNAR's knowledge into a vanilla Transformer, with comparable performance on algorithmic reasoning tasks, would resolve this question.

## Limitations

- The approach relies heavily on pre-trained NARs from the CLRS-30 benchmark, making it unclear how well it generalizes to arbitrary algorithmic tasks outside this specific domain.
- The two-phase training procedure requires substantial computational resources and careful hyperparameter tuning, with the frozen NAR assumption potentially limiting adaptation to task-specific nuances.
- While improvements are demonstrated on CLRS-Text, the evaluation scope is limited to a specific algorithmic reasoning benchmark, leaving broader natural language task performance unexplored.

## Confidence

- **High Confidence**: The core architectural innovation of using cross-attention to integrate NAR embeddings into Transformers is technically sound and the mechanism is well-defined.
- **Medium Confidence**: The empirical improvements on CLRS-Text are demonstrated, but the evaluation scope is limited to a specific benchmark.
- **Low Confidence**: Claims about the generality of the approach to arbitrary algorithmic reasoning tasks beyond CLRS-30 are not yet substantiated.

## Next Checks

1. **Cross-Attention Ablation**: Systematically vary the number and placement of cross-attention layers to determine the optimal configuration for NAR integration, and test whether the NAR embeddings are truly necessary versus just additional parameters.

2. **Generalization Beyond CLRS**: Evaluate TransNAR on algorithmic reasoning tasks from different domains (e.g., mathematical problem solving, code generation) to assess whether the benefits extend beyond the CLRS-30 training distribution.

3. **Dynamic NAR Training**: Implement a variant where the NAR is partially trainable during TransNAR fine-tuning to determine whether some adaptation improves performance without sacrificing robustness, addressing the limitation of the frozen NAR assumption.