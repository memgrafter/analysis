---
ver: rpa2
title: 'DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models'
arxiv_id: '2412.20891'
source_url: https://arxiv.org/abs/2412.20891
tags:
- tensor
- dota
- matrix
- fine-tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DoTA, a weight-decomposed tensor adaptation
  method for fine-tuning large language models (LLMs). DoTA leverages Matrix Product
  Operator (MPO) decomposition of pre-trained weights to initialize trainable tensor
  adaptations, effectively capturing high-dimensional structures during fine-tuning.
---

# DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models

## Quick Facts
- arXiv ID: 2412.20891
- Source URL: https://arxiv.org/abs/2412.20891
- Reference count: 32
- Primary result: Weight-decomposed tensor adaptation method that uses <0.2% of parameters compared to full fine-tuning while achieving within 0.5% accuracy

## Executive Summary
DoTA introduces a weight-decomposed tensor adaptation method for fine-tuning large language models that leverages Matrix Product Operator (MPO) decomposition of pre-trained weights to initialize trainable tensor adaptations. The method addresses the limitations of random initialization in tensor adaptation by capturing high-dimensional structures within weight matrices, achieving superior performance on commonsense and arithmetic reasoning tasks with significantly fewer parameters. DoTA also introduces QDoTA, a quantized version designed for 4-bit quantization that reduces memory consumption while maintaining comparable performance to the full-precision version.

## Method Summary
DoTA uses MPO decomposition to break down pre-trained weight matrices into core tensors while preserving high-dimensional structures. The method computes a residual matrix to compensate for information loss during rank truncation, then fine-tunes only the core tensors while keeping the residual matrix frozen. This approach captures complex weight structures more effectively than random initialization, achieving parameter efficiency by using fewer than 0.2% of the parameters required for full fine-tuning. The QDoTA extension applies 4-bit quantization to further reduce memory consumption while maintaining performance through NF4 data type for residual matrices.

## Key Results
- DoTA achieves performance within 0.5% of full fine-tuning on LLaMA3-8B models while using <0.2% of parameters
- Outperforms random initialization approaches on commonsense and arithmetic reasoning tasks
- QDoTA reduces memory consumption with comparable performance to full-precision DoTA
- Captures high-dimensional structures in weight matrices more effectively than 2D low-rank approximation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random initialization of tensor adaptations significantly diverges from the validation loss trajectory of full fine-tuning
- Mechanism: Random initialization creates a new low-dimensional space that neither inherits the knowledge of the base model nor aligns with the pre-trained manifold, causing optimization to converge to suboptimal points
- Core assumption: The pre-trained weight matrix contains high-dimensional structures that can be effectively captured through decomposition rather than random initialization
- Evidence anchors: Weak evidence - corpus contains related papers on weight decomposition but no direct evidence on initialization divergence

### Mechanism 2
- Claim: MPO decomposition of pre-trained weights provides effective initialization for tensor adaptations by capturing high-dimensional structures
- Mechanism: Matrix Product Operator (MPO) decomposition breaks down the weight matrix into a sequence of small core tensors whose contraction approximates the original matrix, preserving structural information for adaptation
- Core assumption: The high-dimensional structure within the weight matrix is critical for effective fine-tuning and can be preserved through MPO decomposition
- Evidence anchors: Moderate evidence - related works on tensor decomposition for fine-tuning exist, supporting the general approach

### Mechanism 3
- Claim: The residual matrix Wres compensates for information loss during MPO rank truncation, enabling effective adaptation with fewer parameters
- Mechanism: By computing Wres = W0 - MPO(W0), DoTA preserves information that would be lost during rank truncation, maintaining model performance while reducing trainable parameters
- Core assumption: The information lost during MPO truncation can be effectively captured and preserved in the residual matrix for fine-tuning
- Evidence anchors: Limited evidence - no direct corpus evidence on residual matrix effectiveness in tensor adaptation

## Foundational Learning

- Concept: Tensor decomposition and MPO decomposition
  - Why needed here: Understanding how MPO decomposition breaks down matrices into core tensors is fundamental to grasping DoTA's initialization approach
  - Quick check question: How does MPO decomposition differ from simple matrix factorization, and why is it particularly suited for capturing high-dimensional structures?

- Concept: Low-rank approximation limitations
  - Why needed here: Understanding why 2D low-rank approximation (like LoRA) fails to capture high-dimensional structures helps justify DoTA's approach
  - Quick check question: What specific structural information does low-rank approximation in 2D space miss that tensor decomposition can capture?

- Concept: Parameter-efficient fine-tuning tradeoffs
  - Why needed here: Understanding the balance between parameter reduction and performance degradation is crucial for evaluating DoTA's effectiveness
  - Quick check question: How does DoTA achieve parameter reduction while maintaining performance, and what are the key tradeoffs involved?

## Architecture Onboarding

- Component map: Input pre-trained weight matrix W0 -> MPO decomposition module -> Core tensors {T(k)} and residual matrix Wres -> Tensor adaptation layer (trainable core tensors + frozen residual) -> Adapted weight matrix W' = Wres + MPO(W0) -> Optional quantization module for QDoTA
- Critical path: 1) MPO decomposition of W0 during initialization, 2) Computation of residual matrix Wres = W0 - MPO(W0), 3) Fine-tuning of core tensors {T(k)} while keeping Wres frozen, 4) Forward pass using adapted weights W' = Wres + MPO(W0)
- Design tradeoffs: Rank truncation vs. parameter efficiency (lower ranks reduce parameters but may lose critical information), Core tensor count vs. capture capacity (more core tensors can capture more complex structures but increase parameters), Quantization precision vs. memory savings (lower precision reduces memory but may introduce errors)
- Failure signatures: Poor convergence during training (may indicate ineffective initialization or rank truncation removing critical information), Large gap between training and validation loss (could suggest overfitting to the residual matrix or insufficient rank), Memory usage exceeding expectations (may indicate inefficient tensor shape selection or quantization errors)
- First 3 experiments: 1) Compare validation loss trajectories of DoTA vs. DoTA-Random vs. Full-FT on a simple task, 2) Vary rank threshold R and measure parameter count vs. performance trade-off, 3) Apply DoTA to different layer types (Q, K, V, U, D) and measure relative effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DoTA's performance scale with model size beyond LLaMA3-8B, particularly for models with 70B+ parameters?
- Basis in paper: [inferred] The paper only evaluates DoTA on LLaMA2-7B and LLaMA3-8B models, leaving the performance on larger models unexplored
- Why unresolved: The paper does not provide experimental results or theoretical analysis for larger models, making it unclear whether the benefits of DoTA would persist or diminish at scale
- What evidence would resolve it: Experimental results showing DoTA's performance, parameter efficiency, and memory consumption on models like LLaMA3-70B, GPT-3.175B, or other large-scale LLMs

### Open Question 2
- Question: What is the theoretical relationship between tensor rank truncation in DoTA and the effective rank of weight matrices in pre-trained LLMs?
- Basis in paper: [explicit] The paper mentions rank truncation (R) but does not explore the connection between chosen ranks and the intrinsic dimensionality of weight matrices
- Why unresolved: The paper selects ranks empirically without investigating whether these choices align with the spectral properties or intrinsic dimensionality of the original weight matrices
- What evidence would resolve it: Analysis of the singular value spectra of weight matrices and their correlation with optimal DoTA ranks, potentially revealing whether the chosen ranks capture meaningful information

### Open Question 3
- Question: How does DoTA compare to other tensor decomposition methods like Tensor-Train (TT) or Hierarchical Tucker (HT) decomposition for LLM fine-tuning?
- Basis in paper: [inferred] The paper focuses exclusively on Matrix Product Operator (MPO) decomposition without benchmarking against other tensor decomposition approaches
- Why unresolved: The paper does not explore whether MPO is superior to other tensor decomposition methods for this specific application, leaving open the question of optimal decomposition strategy
- What evidence would resolve it: Head-to-head comparisons of DoTA with TT-decomposition methods (like LoRETTA) and HT-decomposition methods on the same tasks and models, measuring both performance and parameter efficiency

### Open Question 4
- Question: What is the impact of DoTA's initialization strategy on fine-tuning stability and convergence speed across different learning rate schedules?
- Basis in paper: [explicit] The paper shows that DoTA's initialization leads to better convergence than random initialization, but does not explore different learning rate schedules
- Why unresolved: The paper only uses cosine learning rate schedules with fixed warmup ratios, without investigating how different schedules might interact with DoTA's initialization
- What evidence would resolve it: Experiments varying learning rate schedules (step decay, polynomial decay, warmup-free) and measuring convergence speed, stability, and final performance to determine optimal training configurations for DoTA

## Limitations

- The effectiveness of DoTA relies on the assumption that pre-trained weight matrices contain meaningful high-dimensional structures that can be captured through MPO decomposition
- The residual matrix compensation mechanism lacks direct corpus evidence for its effectiveness in tensor adaptation contexts
- The method has only been evaluated on LLaMA2-7B and LLaMA3-8B models, with unclear performance scaling to larger models

## Confidence

- **Medium Confidence**: The general approach of using tensor decomposition for parameter-efficient fine-tuning, supported by related works in the corpus
- **Low Confidence**: The specific claim that random initialization significantly diverges from full fine-tuning trajectories, with weak direct evidence
- **Medium Confidence**: The MPO decomposition mechanism, with moderate support from related literature but limited direct validation
- **Low Confidence**: The residual matrix compensation mechanism, lacking direct corpus evidence for its effectiveness

## Next Checks

1. **Initialization Trajectory Analysis**: Conduct controlled experiments comparing validation loss trajectories of DoTA vs. DoTA-Random vs. Full-FT on multiple simple tasks to empirically verify the claimed divergence of random initialization

2. **Rank Sensitivity Study**: Systematically vary the rank threshold R across a wide range and measure both parameter count reduction and performance degradation to identify optimal tradeoffs and validate the break condition regarding critical information loss

3. **Residual Matrix Impact Assessment**: Compare DoTA performance with and without the residual matrix component across different layer types to quantify its actual contribution and identify conditions where it becomes ineffective or counterproductive