---
ver: rpa2
title: 'Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital
  Agents at Scale'
arxiv_id: '2409.15637'
source_url: https://arxiv.org/abs/2409.15637
tags:
- action
- task
- page
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Synatra, a method to convert indirect knowledge
  sources (like tutorials and web pages) into direct demonstrations for training digital
  agents. The approach uses large language models to rewrite procedural knowledge
  into executable programs and generate synthetic observations.
---

# Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale

## Quick Facts
- **arXiv ID**: 2409.15637
- **Source URL**: https://arxiv.org/abs/2409.15637
- **Reference count**: 40
- **Primary result**: Model trained on 100k synthetic demonstrations outperforms all comparably sized models on three web-based task benchmarks and surpasses GPT-3.5 on WebArena and Mind2Web at 3% the cost of human demonstrations

## Executive Summary
This paper introduces Synatra, a method that converts indirect knowledge sources like tutorials and web pages into direct demonstrations for training digital agents. The approach uses large language models to rewrite procedural knowledge into executable programs and generate synthetic observations. A 7B CodeLlama model is finetuned on these synthetic demonstrations, achieving state-of-the-art performance on three web-based task benchmarks while being significantly more cost-effective than human demonstrations. The key insight is that synthetic data with good domain coverage can be more effective than an identical quantity of limited-domain human demonstrations.

## Method Summary
Synatra converts indirect knowledge (tutorials and web pages) into direct demonstrations through an LLM-based synthesis pipeline. The method collects procedural knowledge from wikiHow articles and diverse web pages from ClueWeb, then uses LLMs to rewrite these into executable program formats with planning and action summaries. Synthetic observations are generated between consecutive actions. The resulting demonstrations are filtered for completeness and coherence, then used to finetune a CodeLlama-7B model. The approach represents trajectories as Python functions that interleave natural language planning in comments with API actions, creating a structured format that improves learning efficiency.

## Key Results
- Model trained on 100k synthetic demonstrations outperforms all comparably sized models on Mind2Web, MiniWoB++, and WebArena benchmarks
- Synatra-CodeLlama surpasses GPT-3.5 on WebArena and Mind2Web tasks
- Synthetic demonstrations cost only 3% of human demonstrations while being more effective than the same number of limited-domain human demonstrations
- The method demonstrates superior attention to detail and logical flow between steps compared to GPT-4-turbo in case studies

## Why This Works (Mechanism)

### Mechanism 1
Synthetic demonstrations can outperform limited-domain human demonstrations due to broader task coverage. By sourcing indirect knowledge from diverse web pages and tutorials, the model learns patterns across many domains rather than being constrained to a narrow task set. Core assumption: The synthetic data generation process creates demonstrations that are both diverse and representative of real-world web navigation tasks. Break condition: If the synthetic data generation process produces biased or unrealistic demonstrations that don't reflect actual web navigation patterns.

### Mechanism 2
Representing trajectories as programs with planning and action summaries improves model performance compared to natural language formats. The program format provides structured representation of tasks, allowing the model to better understand task hierarchies and action-level reasoning. Core assumption: Code-like representations with embedded planning steps are more learnable than free-form text descriptions for sequential decision-making tasks. Break condition: If the added complexity of program representation outweighs its benefits, or if models struggle to parse the hybrid format.

### Mechanism 3
Combining procedural knowledge from tutorials with real web page observations creates more effective training data than either source alone. Tutorials provide accurate task sequences while real web pages provide authentic observation states, compensating for each other's weaknesses. Core assumption: The two data sources are complementary rather than redundant, with tutorials lacking authentic observations and random pages lacking accurate procedures. Break condition: If one data source consistently provides higher quality demonstrations than the other, making the combination unnecessary.

## Foundational Learning

- **Chain-of-thought reasoning in program comments**: Why needed here - The model needs to understand both the reasoning behind actions and the actions themselves, which is encoded in the CoT comments and action summaries. Quick check question: What is the difference between the detailed CoT reasoning and the action summary in the program format?

- **Accessibility tree representation of web pages**: Why needed here - The model operates on text-based accessibility trees rather than raw HTML, requiring understanding of this structured representation. Quick check question: How does the accessibility tree format differ from standard HTML, and why is it used for web agent tasks?

- **Temperature sampling for domain diversity**: Why needed here - Random sampling from ClueWeb would result in homogeneous, less interactive pages, so temperature sampling is used to up-sample more interactive domains. Quick check question: What is the mathematical formula for temperature sampling, and how does it bias the domain selection?

## Architecture Onboarding

- **Component map**: LLM-based synthesis pipeline (tutorials and random observations) → program-formatted demonstrations → filtering → finetuning → evaluation on benchmarks
- **Critical path**: Indirect knowledge → synthesis (rewrite + observation generation) → program formatting → filtering → model training → evaluation
- **Design tradeoffs**: Synthetic data is much cheaper but potentially less realistic than human demonstrations; program format is more structured but requires additional parsing; using multiple knowledge sources adds diversity but increases complexity
- **Failure signatures**: Performance degradation on benchmarks suggests issues with either the synthesis quality, filtering effectiveness, or model training approach
- **First 3 experiments**:
  1. Compare model performance when trained with synthetic data vs human demonstrations on a small task set
  2. Test different program format variations (with/without CoT, different action summary lengths)
  3. Evaluate impact of varying the ratio of tutorial-derived vs observation-derived training data

## Open Questions the Paper Calls Out
- How does the performance of Synatra-CodeLlama scale with the quality and diversity of the indirect knowledge sources beyond WikiHow and ClueWeb?
- What are the specific factors that contribute to the superior performance of Synatra-CodeLlama compared to GPT-4-turbo on certain tasks?
- How does the cost-effectiveness of Synatra's data synthesis approach compare to other methods of obtaining direct demonstrations?

## Limitations
- Synthetic data generation pipeline relies heavily on LLM performance which may vary across different knowledge sources and task domains
- Comparison to human demonstrations based on a small dataset of only 77 human demonstrations (8 for WebArena)
- Accessibility tree representation may not capture all relevant web page features that could impact agent performance in real-world scenarios

## Confidence

- **High Confidence**: The core claim that synthetic demonstrations can be more cost-effective than human demonstrations is well-supported by the 3% cost comparison and demonstrated effectiveness on benchmark tasks.
- **Medium Confidence**: The claim that synthetic demonstrations can outperform human demonstrations on real-world tasks has moderate support but would benefit from testing on a larger and more diverse set of human demonstrations.
- **Low Confidence**: The specific temperature sampling parameter (T=0.6) and its optimal value for domain diversity is not thoroughly explored or justified.

## Next Checks
1. Run experiments training models using only tutorial-derived demonstrations, only observation-derived demonstrations, and various ratios between them to quantify the exact contribution of each data source and identify the optimal mixing ratio.
2. Collect and evaluate a larger set of human demonstrations (minimum 100-200) across multiple task domains to provide more robust statistical comparison with synthetic data performance.
3. Deploy the trained agent on a separate, held-out set of real-world web tasks not included in any training or benchmark data to assess practical generalization capabilities and identify failure modes.