---
ver: rpa2
title: Bayesian Example Selection Improves In-Context Learning for Speech, Text, and
  Visual Modalities
arxiv_id: '2404.14716'
source_url: https://arxiv.org/abs/2404.14716
tags:
- inference
- in-context
- bycs
- inverse
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Bayesian in-Context example Selection (ByCS),\
  \ a method that improves in-context learning (ICL) by leveraging Bayes\u2019 theorem\
  \ to select high-quality in-context examples. ByCS focuses on inverse inference,\
  \ using the test input as context to evaluate candidate examples, under the assumption\
  \ that accurate inverse inference leads to accurate ICL."
---

# Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities

## Quick Facts
- arXiv ID: 2404.14716
- Source URL: https://arxiv.org/abs/2404.14716
- Authors: Siyin Wang; Chao-Han Huck Yang; Ji Wu; Chao Zhang
- Reference count: 14
- Primary result: ByCS improves ICL performance across speech, text, and visual modalities by using inverse inference to select high-quality examples

## Executive Summary
This paper introduces Bayesian in-Context example Selection (ByCS), a method that improves in-context learning by leveraging Bayes' theorem to select high-quality in-context examples. ByCS focuses on inverse inference, using the test input as context to evaluate candidate examples, under the assumption that accurate inverse inference leads to accurate ICL. Experiments across speech, text, and visual modalities show that ByCS consistently outperforms baselines like KATE+ and random selection, achieving notable WER reductions in ASR and accuracy gains in text tasks.

## Method Summary
ByCS uses Bayes' theorem to extend ICL inference probability by incorporating inverse inference. The method assumes mutual information between test input and in-context examples, where the test input-label pair serves as context to evaluate candidate examples. A smaller model performs inverse inference to reduce computation, and text similarity (Jaccard coefficient) measures alignment between predicted and true labels. Examples with highest similarity scores are selected for ICL. The approach is tested across ASR, VQA, and NLP tasks using GPT-3.5, GPT-4, Whisper, and Emu2 models.

## Key Results
- ByCS achieves up to 10.25% relative WER reduction in ASR tasks
- Consistently outperforms KATE+ and random selection baselines across all modalities
- Shows significant improvements on open-ended long-answer datasets
- Performs well with smaller inverse inference models, reducing computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High mutual information between test input and in-context examples leads to accurate ICL inference
- Mechanism: ByCS uses inverse inference to measure how well the test input-label pair predicts the in-context example label; examples with high text similarity between predicted and true labels are selected as they carry strong mutual information
- Core assumption: Contextual information interaction is mutual between test input and in-context examples
- Evidence anchors:
  - [abstract] "Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results."
  - [section 3] "Assuming the contextual information interaction is mutual, an accurate inverse inference is likely to result in an accurate inference."
- Break condition: If the mutual information assumption fails (e.g., examples are semantically similar but contextually irrelevant), ByCS performance degrades.

### Mechanism 2
- Claim: Smaller models from the same family can approximate inverse inference effectively
- Mechanism: ByCS uses a smaller Whisper model for inverse inference to reduce computation while maintaining performance due to similar training data and settings
- Core assumption: Smaller models process information similarly to larger inference models when trained on same data
- Evidence anchors:
  - [section 5.2.3] "Whisper-small is trained using the same data and settings compared to the inference model... which therefore processes information similarly and can serve as a good alternative"
  - [section 5.2.3] "ByCSsmall has similar results to ByCSlargev2 and ByCSlargev3"
- Break condition: If the smaller model lacks capacity to capture relevant contextual features, inverse inference quality drops.

### Mechanism 3
- Claim: Text similarity measurement effectiveness depends on dataset characteristics
- Mechanism: ByCS uses Jaccard coefficient for text similarity; performs better on open-ended long-answer datasets than short-answer classification tasks due to richer contextual information
- Core assumption: Text similarity can effectively rank examples based on mutual information
- Evidence anchors:
  - [section 5.3] "ByCS is more suitable for open-ended long-answer datasets due to the calculation of text similarity in ByCS, in which answers are much more diverse"
  - [section 5.3] "In contrast, in multi-choice classification datasets, only a few short answers are often available, containing little contextual information"
- Break condition: If dataset answers lack sufficient diversity or context, text similarity becomes ineffective for ranking.

## Foundational Learning

- Concept: Bayes' theorem extension for in-context learning
  - Why needed here: ByCS extends ICL inference probability using Bayes' theorem to incorporate inverse inference
  - Quick check question: What is the relationship between P(Y|Cinput, Clabel, X) and P(Clabel|X, Y, Cinput) in ByCS?

- Concept: Inverse inference and mutual information
  - Why needed here: ByCS performs inverse inference where test input-label pair serves as in-context example to evaluate candidate examples
  - Quick check question: How does ByCS use inverse inference results to select in-context examples?

- Concept: Text similarity metrics for semantic matching
  - Why needed here: ByCS uses text similarity (Jaccard coefficient) to measure alignment between inverse inference results and true labels
  - Quick check question: Why might BERT-based embeddings perform worse than Jaccard coefficient for dialectal speech tasks?

## Architecture Onboarding

- Component map: Test input preprocessing -> First-round inference -> Inverse inference on each candidate example -> Text similarity calculation -> Example selection
- Critical path:
  1. Generate hypothesized label from test input
  2. Perform inverse inference using smaller model
  3. Calculate text similarity between predicted and true labels
  4. Select top-k examples with highest similarity scores
- Design tradeoffs:
  - Inverse inference model size vs computation cost: Smaller models reduce cost but may sacrifice accuracy
  - Pre-selection size vs example diversity: Larger pre-selected sets improve diversity but increase computation
  - Text similarity metric choice: Simple metrics like Jaccard are faster but may miss semantic nuances
- Failure signatures:
  - Sharp text similarity score distributions indicate insufficient contextual diversity
  - Performance degrades when test input is dissimilar from all examples in datastore
  - Short-answer datasets show limited improvement due to lack of contextual information
- First 3 experiments:
  1. Run ByCS with default settings on a small ASR dataset to verify inverse inference implementation
  2. Compare Jaccard coefficient vs BERT embeddings for text similarity on a dialectal speech dataset
  3. Test ByCS with different inverse inference model sizes (Whisper small vs large) on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the assumption of independent in-context examples be relaxed to better capture contextual interactions between examples?
- Basis in paper: [explicit] The paper states "ByCS follows the simple assumption that the influence of each in-context example is independent and treats each in-context example individually, which neglects the contextual interactions between in-context examples."
- Why unresolved: The current method treats examples independently for computational efficiency, but this may not be optimal for larger numbers of in-context examples where interactions become more important.
- What evidence would resolve it: Experiments comparing ByCS with and without independence assumptions across varying numbers of examples, or theoretical analysis showing when independence breaks down.

### Open Question 2
- Question: What is the optimal trade-off between example diversity and computational efficiency in the pre-selection step?
- Basis in paper: [explicit] "We usually choose kNN as the ranking algorithm and twice the maximum number of in-context examples as reduced size after pre-selection" and "we found empirically that a number around 10 is a good choice in balancing the example diversity and conduction speed."
- Why unresolved: The choice of reduced size (10) appears arbitrary and based on empirical observation. Different datasets or tasks might benefit from different trade-offs.
- What evidence would resolve it: Systematic ablation studies varying the reduced size parameter across different datasets and analyzing the impact on both performance and computational cost.

### Open Question 3
- Question: How does ByCS performance change when using different text similarity metrics across modalities?
- Basis in paper: [explicit] "ByCS with the Jaccard coefficient as text similarity have lower WERs, which may be because the training data of the BERT model doesn't include sufficient dialectal Chinese words" and "ByCS can work well with even a simple rule-based text similarity measurement."
- Why unresolved: The paper only tested Jaccard coefficient and BERT wordvecs, leaving open whether other similarity metrics (cosine similarity, learned metrics, etc.) might perform better for specific modalities or tasks.
- What evidence would resolve it: Comparative experiments using multiple text similarity metrics across all modalities and tasks to identify optimal choices.

### Open Question 4
- Question: What is the theoretical upper bound of ByCS performance when ground truth labels are available for inverse inference?
- Basis in paper: [explicit] The "oracle ByCS" setting shows improved performance: "Using Yref in 'oracle ByCS' further boosts the performance gain, indicating the upper bound of our method with the same number of k."
- Why unresolved: While the oracle setting provides an upper bound, the gap between oracle and regular ByCS suggests room for improvement in label estimation for inverse inference.
- What evidence would resolve it: Experiments quantifying the performance gap between oracle and regular ByCS across different tasks, and analysis of what types of errors in label estimation are most detrimental.

### Open Question 5
- Question: How can ByCS be adapted to handle short-answer datasets more effectively?
- Basis in paper: [explicit] "ByCS may suffer a performance penalty when applied to a short-answer dataset" and "ByCS is more suitable for open-ended long-answer datasets which contain sufficient contextual information."
- Why unresolved: The paper identifies this limitation but doesn't propose solutions, leaving open how to modify the method for short-answer scenarios.
- What evidence would resolve it: Development and testing of modified versions of ByCS (e.g., different similarity metrics, additional context, or alternative selection criteria) specifically designed for short-answer datasets.

## Limitations
- ByCS assumes independence between in-context examples, which may not hold in practice
- Performance degrades on short-answer classification tasks due to limited contextual diversity
- Computational overhead from multiple inference passes may be prohibitive for large-scale applications

## Confidence
- High Confidence: ByCS consistently outperforms baselines across modalities; inverse inference with smaller models works effectively; significant WER reductions in ASR
- Medium Confidence: Mutual information assumption is fundamental; Jaccard coefficient outperforms BERT for dialectal speech; ByCS suits open-ended datasets better
- Low Confidence: Exact contribution of each component to performance; generalizability to novel task types; performance in real-time applications

## Next Checks
1. **Ablation Study on Independence Assumption**: Design experiments that systematically violate the independence assumption by creating in-context example sets with known dependencies. Measure how ByCS performance degrades and compare against alternative selection methods that explicitly account for example dependencies.

2. **Cross-Modal Text Similarity Evaluation**: Test ByCS with different text similarity metrics (Jaccard, BERT embeddings, CLIP embeddings) across all three modalities (speech transcripts, visual descriptions, text tasks). Include a comprehensive evaluation on dialectal speech datasets to validate the paper's claims about metric effectiveness.

3. **Scalability and Real-Time Performance Analysis**: Implement ByCS on a streaming ASR application with real-time constraints. Measure the computational overhead of inverse inference across different model sizes and example set sizes. Compare the end-to-end latency against baseline methods and determine the practical limits of ByCS deployment.