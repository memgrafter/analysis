---
ver: rpa2
title: Enriching Music Descriptions with a Finetuned-LLM and Metadata for Text-to-Music
  Retrieval
arxiv_id: '2410.03264'
source_url: https://arxiv.org/abs/2410.03264
tags:
- music
- text
- audio
- retrieval
- artist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses text-to-music retrieval, focusing on expanding
  beyond content-based queries to include metadata (track, artist, album) and similarity-based
  queries. The proposed TTMR++ model integrates a finetuned LLM for pseudo-caption
  generation and metadata knowledge graphs for track-artist similarity.
---

# Enriching Music Descriptions with a Finetuned-LLM and Metadata for Text-to-Music Retrieval

## Quick Facts
- arXiv ID: 2410.03264
- Source URL: https://arxiv.org/abs/2410.03264
- Authors: SeungHeon Doh; Minhee Lee; Dasaem Jeong; Juhan Nam
- Reference count: 0
- Key outcome: State-of-the-art text-to-music retrieval performance across caption, tag, track, and artist-based tasks using joint training with LLM-generated pseudo-captions and metadata knowledge graphs

## Executive Summary
This paper addresses text-to-music retrieval by expanding beyond content-based queries to include metadata and similarity-based queries. The proposed TTMR++ model integrates a finetuned LLM for pseudo-caption generation and metadata knowledge graphs for track-artist similarity, employing a unified joint training framework across five diverse music datasets totaling 1.3M entries. Experimental results demonstrate significant improvements over state-of-the-art methods, particularly in track-based retrieval (67.9% accuracy) and artist-based retrieval (18.7% nDCG@200) when incorporating metadata text.

## Method Summary
The method employs a cross-modal dual encoder architecture with contrastive loss learning to map music audio and text into a joint embedding space. The approach integrates five datasets (MSD, Audioset, Music4All, FMA, MusicCaps) through joint training, augmented with LLM-generated pseudo-captions for datasets lacking captions and metadata knowledge graphs for track-artist similarity. The audio encoder uses a modified ResNet-50 with attention pooling, while the text encoder employs RoBERTa. The model is trained with AdamW optimizer (learning rate 5e-5, batch size 768) over 32,768 updates using a temperature parameter τ initialized to 0.1.

## Key Results
- Achieves 67.9% accuracy on triplet-based track retrieval, outperforming state-of-the-art methods
- Demonstrates 18.7% nDCG@200 on artist-based retrieval when incorporating metadata text
- Shows significant improvements in caption-based retrieval (R@10) and tag-based retrieval (ROC-AUC) across all test datasets
- Joint training with multiple datasets and LLM-augmented captions consistently improves performance across all query types

## Why This Works (Mechanism)

### Mechanism 1
The joint training framework using five diverse music datasets improves retrieval performance by exposing the model to varied semantic contexts. The model learns to map music audio and text into a joint embedding space using contrastive loss, where positive pairs are brought closer and negative pairs are pushed apart. Training on multiple datasets (MSD, Audioset, Music4All, FMA, MusicCaps) with varied content (tags, captions, metadata) provides richer and more diverse negative samples, improving the model's ability to distinguish relevant matches across different query types.

### Mechanism 2
Finetuning LLaMA2 with instruction-following data from GPT-3.5 enables high-quality pseudo-caption generation for datasets lacking captions. The finetuned LLaMA2 model is trained to follow instructions and generate descriptive captions from tag lists. This augments datasets with pseudo-captions, expanding the semantic coverage of the training data. The improved captions provide richer textual representations that better align with the audio content.

### Mechanism 3
Incorporating metadata knowledge graphs (track-artist similarity) improves retrieval performance for metadata-based queries. The model generates textual descriptions for tracks using metadata (title, artist, album) and synthesizes track-artist similarity text using artist similarity connections and metadata associations. This provides explicit textual representations of metadata relationships, allowing the model to learn mappings between metadata text and audio.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To learn a joint embedding space where music audio and text are mapped such that similar pairs are close and dissimilar pairs are far apart.
  - Quick check question: What is the role of the temperature parameter in the InfoNCE loss function?

- Concept: Cross-modal dual encoder architecture
  - Why needed here: To enable efficient retrieval by separately encoding audio and text into a shared embedding space, allowing nearest neighbor search.
  - Quick check question: Why is the dot product similarity used in the cross-modal dual encoder architecture?

- Concept: Knowledge graph construction and utilization
  - Why needed here: To explicitly represent relationships between entities (tracks, artists) in a structured format that can be used to generate textual descriptions for training.
  - Quick check question: How can knowledge graphs be used to improve the performance of machine learning models?

## Architecture Onboarding

- Component map: Audio Encoder -> Text Encoder -> Joint embedding space -> Contrastive loss minimization
- Critical path: Audio/Text encoding → Joint embedding space → Contrastive loss minimization → Model optimization
- Design tradeoffs:
  - Joint training on multiple datasets vs. training on a single large dataset: Joint training provides diverse semantic contexts but may introduce conflicting mappings
  - Using a finetuned LLM vs. a closed-source LLM: Finetuning allows control and customization but may not achieve the same performance as a larger, more capable model
  - Incorporating metadata knowledge graphs vs. relying solely on content annotations: Metadata provides explicit representations of relationships but requires accurate knowledge graph construction
- Failure signatures:
  - Poor retrieval performance: Could indicate issues with the joint embedding space, contrastive loss, or data quality
  - Model overfitting to a specific dataset: Could indicate insufficient regularization or data augmentation
  - Inconsistent results across different query types: Could indicate issues with the model's ability to generalize across semantic contexts
- First 3 experiments:
  1. Train the baseline model (TTMR) on a single dataset (e.g., MSD) and evaluate its performance on caption, tag, track, and artist-based retrieval tasks
  2. Add LLM-augmented captions to the training data and evaluate the impact on caption-based retrieval performance
  3. Incorporate metadata text (titles, artist names, album names) into the training data and evaluate the impact on track-based and artist-based retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating mid-level features like tempo and key improve the model's performance on text-to-music retrieval tasks?
- Basis in paper: The paper explicitly mentions that their pre-training dataset lacks information on mid-level tempo or key, suggesting this as a future exploration.
- Why unresolved: The current model does not utilize mid-level features, and the impact of incorporating these features on retrieval performance remains untested.
- What evidence would resolve it: Experiments comparing model performance with and without mid-level tempo and key features, ideally showing improved retrieval accuracy when these features are included.

### Open Question 2
- Question: Would probabilistic embedding spaces offer advantages over contrastive learning-based discriminative loss for modeling the many-to-many relationship between music and language?
- Basis in paper: The paper suggests that contrastive learning-based discriminative loss may have limitations in effectively modeling the many-to-many relationship between music and language, and proposes exploring probabilistic embedding spaces as a future direction.
- Why unresolved: The current model uses contrastive learning, and the potential benefits of probabilistic embedding spaces for this task have not been explored.
- What evidence would resolve it: Comparative experiments demonstrating improved retrieval performance using probabilistic embedding spaces versus contrastive learning-based approaches.

### Open Question 3
- Question: How does the quality of pseudo-captions generated by the finetuned LLM compare to human-written captions in terms of retrieval effectiveness?
- Basis in paper: The paper discusses using a finetuned LLM to generate pseudo-captions for training, but does not directly compare their quality to human-written captions in terms of retrieval performance.
- Why unresolved: While the paper mentions the LLM's performance in generating captions, it does not explicitly evaluate how these pseudo-captions impact retrieval effectiveness compared to human-written captions.
- What evidence would resolve it: Retrieval experiments using both pseudo-captions and human-written captions, comparing their effectiveness in improving text-to-music retrieval performance.

## Limitations
- The evaluation relies entirely on held-out test sets without public availability of the model or code for independent verification
- The knowledge graph approach assumes high-quality metadata relationships that may not generalize to all music domains
- The paper doesn't address potential scalability issues when deploying such a large dual-encoder model in production environments

## Confidence
- **High Confidence**: The core methodology of using joint training with contrastive loss is well-established and the architectural components (ResNet-50, RoBERTa) are standard in the field
- **Medium Confidence**: The performance improvements are well-documented, but the ablation studies could be more comprehensive to isolate the contribution of each component
- **Low Confidence**: The quality and generalizability of the finetuned LLaMA2 captions, as well as the effectiveness of the metadata knowledge graph synthesis approach, cannot be fully verified without access to the implementation details

## Next Checks
1. Independent reproduction of core results: Reimplement the baseline TTMR model and compare performance on the same datasets to verify the claimed improvements are reproducible
2. Ablation study of data augmentation: Train models with only original captions, only LLM-generated captions, and only metadata text to quantify the individual contribution of each augmentation type
3. Cross-dataset generalization test: Evaluate the trained model on an external dataset not used in training to assess whether the joint training approach improves generalization beyond the specific datasets used