---
ver: rpa2
title: 'BgGPT 1.0: Extending English-centric LLMs to other languages'
arxiv_id: '2412.10893'
source_url: https://arxiv.org/abs/2412.10893
tags:
- bulgarian
- language
- english
- training
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents BgGPT-Gemma-2-27B-Instruct and BgGPT-Gemma-2-9B-Instruct,\
  \ Bulgarian-focused language models built by continual pretraining and fine-tuning\
  \ of Google\u2019s Gemma-2 models on over 100 billion tokens of Bulgarian and English\
  \ text. Using Branch-and-Merge techniques to mitigate catastrophic forgetting, the\
  \ models achieve state-of-the-art performance on Bulgarian language benchmarks,\
  \ surpassing even larger models like Qwen-2.5-72B and Llama-3.1-70B in specific\
  \ tasks while maintaining strong English capabilities."
---

# BgGPT 1.0: Extending English-centric LLMs to other languages

## Quick Facts
- arXiv ID: 2412.10893
- Source URL: https://arxiv.org/abs/2412.10893
- Authors: Anton Alexandrov, Veselin Raychev, Dimitar I. Dimitrov, Ce Zhang, Martin Vechev, Kristina Toutanova
- Reference count: 28
- Primary result: State-of-the-art Bulgarian language models achieving top performance on benchmarks while preserving English capabilities

## Executive Summary
The paper introduces BgGPT, a family of Bulgarian-focused language models built by extending Google's Gemma-2 models through continual pretraining and fine-tuning on over 100 billion tokens of Bulgarian and English text. Using a Branch-and-Merge training strategy, the models successfully mitigate catastrophic forgetting while achieving superior performance on Bulgarian language benchmarks compared to larger models. The BgGPT-Gemma-2-27B-Instruct model particularly excels, scoring an average of 5.41 out of 6 on Bulgarian educational exams and matching or slightly outperforming GPT-4o on real user queries.

## Method Summary
The authors developed BgGPT models by first continuing pretraining Gemma-2 base models (27B and 9B parameters) on a bilingual dataset containing predominantly Bulgarian web text and English data. They employed a Branch-and-Merge technique with 8 data splits, where odd-numbered splits contained predominantly Bulgarian text and even-numbered splits contained more English. After pretraining, they performed instruction fine-tuning using translated and native Bulgarian datasets including chat conversations and synthetic data. This approach allowed the models to develop strong Bulgarian language capabilities while maintaining their English proficiency through careful curriculum scheduling and parameter merging strategies.

## Key Results
- BgGPT-Gemma-2-27B-Instruct achieves state-of-the-art performance on Bulgarian benchmarks, surpassing larger models like Qwen-2.5-72B and Llama-3.1-70B
- On Bulgarian educational exams, the 27B model scores an average of 5.41 out of 6, outperforming GPT-4o-mini and competitive open models
- In real user query evaluation via GPT-4o preference scoring, BgGPT-Gemma-2-27B-Instruct is rated on par with or slightly better than GPT-4o itself

## Why This Works (Mechanism)
The success of BgGPT stems from the Branch-and-Merge training methodology that effectively balances the acquisition of new Bulgarian language capabilities while preserving existing English proficiency. By carefully partitioning the training data and using alternating splits of predominantly Bulgarian and English text, the model can develop strong bilingual representations without suffering from catastrophic forgetting. The instruction fine-tuning stage on task-specific Bulgarian datasets further enhances the model's ability to handle real-world applications while maintaining its cross-lingual performance.

## Foundational Learning
- **Branch-and-Merge training**: Alternating training on language-specific data splits followed by parameter merging to balance capabilities
  - Why needed: Prevents catastrophic forgetting while enabling acquisition of new language skills
  - Quick check: Monitor performance on both languages during training to ensure neither degrades significantly

- **Continual pretraining**: Extending base model training on domain-specific data rather than starting from scratch
  - Why needed: Leverages existing language understanding while adapting to new linguistic patterns
  - Quick check: Compare perplexity scores on target language before and after pretraining

- **Instruction fine-tuning**: Specialized training on task-specific instructions and conversations
  - Why needed: Transforms general language models into capable assistants for real-world applications
  - Quick check: Evaluate on task-specific benchmarks to verify improved instruction following

## Architecture Onboarding
- **Component map**: Gemma-2 base model -> Branch-and-Merge pretraining -> Instruction fine-tuning -> BgGPT models
- **Critical path**: Data preparation → Branch-and-Merge pretraining → Instruction fine-tuning → Evaluation
- **Design tradeoffs**: Computational cost of continual pretraining vs. performance gains; complexity of merge scheduling vs. catastrophic forgetting risk
- **Failure signatures**: 
  - English capability degradation indicates insufficient merge balance
  - Bulgarian performance plateau suggests inadequate pretraining data or duration
  - Instruction following issues point to insufficient fine-tuning data diversity

**First experiments:**
1. Run initial pretraining on a small subset of data to verify the Branch-and-Merge implementation and monitor language balance
2. Test the base model on Bulgarian benchmarks to establish baseline performance before any adaptation
3. Evaluate the merged model on a held-out English benchmark after each merge step to detect catastrophic forgetting early

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance is fundamentally limited by the quality and scope of available Bulgarian training data
- Evaluation relies heavily on synthetic benchmarks and GPT-4o scoring rather than comprehensive human evaluation
- Branch-and-Merge technique requires significant computational resources and complex scheduling
- Cross-lingual transfer capabilities are suggested but not thoroughly validated beyond stated benchmarks

## Confidence
- High confidence: Technical implementation of Branch-and-Merge training methodology
- Medium confidence: Performance claims on Bulgarian benchmarks using automated evaluation
- Medium confidence: English capability preservation inferred from benchmark results
- Low confidence: Real-world performance claims based on single GPT-4o preference scoring methodology

## Next Checks
1. Conduct human evaluation studies with native Bulgarian speakers to validate automated benchmark scores and assess practical usability
2. Test the models on additional low-resource languages to evaluate generalizability of the Branch-and-Merge approach
3. Perform ablation studies removing different components of the training pipeline to quantify contribution of each element to final performance