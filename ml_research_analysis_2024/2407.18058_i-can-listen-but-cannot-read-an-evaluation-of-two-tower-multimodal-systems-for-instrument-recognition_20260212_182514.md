---
ver: rpa2
title: 'I can listen but cannot read: An evaluation of two-tower multimodal systems
  for instrument recognition'
arxiv_id: '2407.18058'
source_url: https://arxiv.org/abs/2407.18058
tags:
- audio
- systems
- music
- text
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the zero-shot properties of two-tower multimodal
  systems for instrument recognition. The authors analyze three systems (MusCALL and
  two CLAP variants) using the TinySOL dataset and find that while audio encoders
  produce meaningful embeddings, text encoders struggle to leverage context and show
  sensitivity to specific words rather than semantic meaning.
---

# I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition

## Quick Facts
- arXiv ID: 2407.18058
- Source URL: https://arxiv.org/abs/2407.18058
- Reference count: 0
- Two-tower multimodal systems achieve only 49-59% accuracy in understanding instrument relationships

## Executive Summary
This paper evaluates three two-tower multimodal systems (MusCALL and two CLAP variants) for zero-shot instrument recognition using the TinySOL dataset. While audio encoders produce high-quality embeddings that maintain discriminative power through joint space projection, text encoders show significant limitations in leveraging context and understanding semantic relationships between instruments. The study introduces a novel method using instrument ontology triplets to quantify semantic meaningfulness, revealing that these systems only achieve 49-59% accuracy in understanding instrument relationships. The findings suggest that text encoders require fine-tuning on musical data to better understand instruments and their relationships.

## Method Summary
The study evaluates three pre-trained two-tower multimodal models (MusCALL, Music CLAP, Music/Speech CLAP) on the TinySOL dataset containing 2913 audio clips across 14 instrument classes. The evaluation uses zero-shot classification with cosine similarity between audio and text embeddings in joint space, testing six different prompt formulations. Performance is measured using Top-k accuracy, ROC-AUC, PR-AUC, and a novel semantic meaningfulness metric based on Henry Doktorski's instrument ontology. The methodology includes analyzing similarity distributions between positive and negative pairs and examining model confidence through top-2 class similarity differences.

## Key Results
- Audio encoders produce meaningful embeddings with good separability before and after joint space projection
- Text encoders struggle to leverage contextual information and show sensitivity to specific words rather than semantic meaning
- Two-tower systems achieve only 49-59% accuracy on semantic relationship understanding using instrument ontology triplets
- Classification performance is highly sensitive to prompt formulation, with musically informed prompts performing better than generic ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-tower systems project audio and text into a joint space so cosine similarity can directly compare modalities
- Mechanism: Audio and text encoders map to fixed-dimensional vectors, then MLPs project to a shared space where similar concepts have nearby embeddings
- Core assumption: The joint space preserves semantic relationships from both modalities after projection
- Evidence anchors:
  - [abstract] "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space"
  - [section] "representations obtained from each modality are then mapped to a joint audio-text space"
  - [corpus] Weak evidence - corpus titles mention multimodal models but don't directly address joint space mechanics
- Break condition: If the projection MLP introduces noise or if the encoders' pre-joint representations aren't semantically meaningful

### Mechanism 2
- Claim: Zero-shot transfer works because large language models can embed novel words/phrases into the text space
- Mechanism: Pre-trained text encoders map any input text to a fixed vector; this vector can be compared to audio embeddings in joint space without retraining
- Core assumption: The text encoder's embeddings capture semantic meaning even for unseen words
- Evidence anchors:
  - [abstract] "Due to the pretrained textual encoder, novel words or phrases can be interpreted during inference"
  - [section] "the large size of textual encoders, they do not yet leverage additional textual context"
  - [corpus] No direct evidence in corpus about text encoder capabilities
- Break condition: If text encoder cannot properly decompose sentences into meaningful constituents

### Mechanism 3
- Claim: Audio encoders produce meaningful representations that survive joint space projection
- Mechanism: Audio models trained on large datasets learn feature representations that capture instrument characteristics, which remain separable after projection
- Core assumption: Audio embeddings before and after projection maintain class separability
- Evidence anchors:
  - [section] "audio encoders alone demonstrate good quality" and "audio embeddings seem to be of good quality before and after the projection"
  - [section] Figure 3b shows positive/negative pairs are well separated when using audio-only labels
  - [corpus] No corpus evidence about audio encoder quality
- Break condition: If audio features are too generic or if projection destroys discriminative information

## Foundational Learning

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: The paper uses cosine similarity to compare embeddings in joint space for classification
  - Quick check question: If two vectors have angle 90 degrees between them, what is their cosine similarity value?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Two-tower systems are trained using contrastive objectives to bring matching pairs close and push non-matches apart
  - Quick check question: In contrastive learning, what happens to the embeddings of mismatched audio-text pairs during training?

- Concept: Zero-shot learning vs few-shot learning
  - Why needed here: The paper evaluates zero-shot properties - ability to classify unseen classes without training examples
  - Quick check question: What's the key difference between zero-shot learning and few-shot learning in terms of required training data?

## Architecture Onboarding

- Component map: Audio encoder -> Text encoder -> Projection MLP -> Joint audio-text space -> Cosine similarity calculation -> Classification
- Critical path:
  1. Extract audio embeddings from raw audio
  2. Extract text embeddings from prompts
  3. Project both to joint space via MLPs
  4. Calculate cosine similarities
  5. Rank and classify based on highest similarity
- Design tradeoffs:
  - Joint space dimensionality vs. pre-joint space capacity
  - Text prompt specificity vs. model sensitivity
  - Training data diversity vs. task specificity
  - Model size vs. inference speed
- Failure signatures:
  - High overlap between positive/negative similarity distributions
  - Performance much worse than audio-only baselines
  - Sensitivity to specific words rather than semantic meaning
  - Inconsistent results across different prompt formulations
- First 3 experiments:
  1. Reproduce the cosine similarity histograms comparing positive vs negative pairs
  2. Test classification performance with different prompt formulations (generic vs musically informed)
  3. Evaluate semantic meaningfulness using instrument ontology triplets

## Open Questions the Paper Calls Out

- What specific fine-tuning strategies would most effectively improve text encoders' understanding of musical instruments and their relationships?
  - The paper concludes that "fine-tuning text encoders on musical data" is needed but doesn't explore specific approaches.

- How do different sentence-level versus word-level embedding approaches affect the semantic meaningfulness of two-tower systems for instrument recognition?
  - The authors note that "treating a sentence as only one embedding point (mean of word embeddings) is fundamentally problematic" but don't test alternatives.

- How do two-tower multimodal systems perform on instrument recognition tasks with real-world audio data containing multiple instruments, background noise, and varying recording quality?
  - The evaluation is limited to TinySOL's idealized conditions with "consistent recording settings without noise" and "single note played from a single instrument."

## Limitations

- The study evaluates only three specific two-tower models (MusCALL, Music CLAP, Music/Speech CLAP) on a single dataset (TinySOL), limiting generalizability
- Text prompt sensitivity suggests the evaluation may not fully capture model capabilities, as results depend heavily on prompt formulation
- The semantic meaningfulness analysis using instrument ontology is novel but requires careful interpretation of triplet accuracy

## Confidence

- **High Confidence**: Audio encoders produce meaningful embeddings that maintain quality through joint space projection
- **Medium Confidence**: Text encoders struggle with semantic understanding and context
- **Low Confidence**: The 49-59% semantic meaningfulness accuracy definitively proves two-tower systems cannot understand instrument relationships

## Next Checks

1. Test the same three models on a different instrument dataset (e.g., IRMAS or NSynth) to verify if the observed patterns hold across different data distributions and recording conditions.

2. Systematically vary prompt complexity and musical terminology across all models to quantify the relationship between prompt formulation and classification accuracy.

3. Take the best-performing model and fine-tune only the text encoder on musical text data while keeping audio parameters frozen, then measure changes in both zero-shot classification accuracy and semantic meaningfulness.