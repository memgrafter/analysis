---
ver: rpa2
title: 'ICD Codes are Insufficient to Create Datasets for Machine Learning: An Evaluation
  Using All of Us Data for Coccidioidomycosis and Myocardial Infarction'
arxiv_id: '2407.07997'
source_url: https://arxiv.org/abs/2407.07997
tags:
- icd-10
- fever
- patients
- cohort
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the suitability of ICD codes for creating
  machine learning datasets by comparing ICD-10-based cohorts with clinically confirmed
  cohorts for Valley fever (coccidioidomycosis) and myocardial infarction using the
  All of Us database. For Valley fever, 811 patients were identified via ICD-10 codes
  versus 619 via serological confirmation, with only 24 in both groups.
---

# ICD Codes are Insufficient to Create Datasets for Machine Learning: An Evaluation Using All of Us Data for Coccidioidomycosis and Myocardial Infarction

## Quick Facts
- arXiv ID: 2407.07997
- Source URL: https://arxiv.org/abs/2407.07997
- Reference count: 0
- Primary result: ICD codes identify largely distinct patient populations from clinically confirmed cohorts, indicating they may not reliably represent medical conditions for ML model training

## Executive Summary
This study evaluates the suitability of ICD-10 codes for creating machine learning datasets by comparing ICD-10-based cohorts with clinically confirmed cohorts for Valley fever and myocardial infarction using the All of Us database. For Valley fever, 811 patients were identified via ICD-10 codes versus 619 via serological confirmation, with only 24 in both groups. For myocardial infarction, 14,875 patients were identified via ICD-10 codes versus 23,598 via laboratory confirmation, with 6,531 in both groups. Significant demographic differences were found, including 23% Hispanic patients in the Valley fever ICD-10 cohort versus 50% in the serological cohort. The study concludes that ICD codes and clinical criteria identify largely distinct patient populations, indicating ICD codes may not reliably represent medical conditions for machine learning model training.

## Method Summary
The study uses the All of Us Researcher Workbench to create cohorts using ICD-10 codes (B38 for Valley fever, I21 for myocardial infarction) and clinically confirmed cohorts using laboratory test values (serological tests for Valley fever, troponin levels for myocardial infarction). Four cohorts were created: ICD-10 code cohorts and clinically confirmed cohorts, with overlap patients identified. Demographics, symptom frequencies, and clinical markers were compared between cohorts using chi-square tests and descriptive statistics. The analysis focused on demographic distributions, symptom reporting patterns, and laboratory marker values to identify systematic differences between ICD-based and clinically confirmed patient populations.

## Key Results
- For Valley fever, 811 patients were identified via ICD-10 codes versus 619 via serological confirmation, with only 24 in both groups
- For myocardial infarction, 14,875 patients were identified via ICD-10 codes versus 23,598 via laboratory confirmation, with 6,531 in both groups
- Significant demographic differences found: 23% Hispanic patients in Valley fever ICD-10 cohort versus 50% in serological cohort; 27% fatigue reporting in ICD-10 cohort versus 21% in serological cohort

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICD codes alone create biased patient cohorts that do not match clinically confirmed disease populations
- Mechanism: ICD codes are billing-oriented and often capture suspected conditions before confirmation, while clinical criteria use laboratory or serological tests that confirm disease presence. This mismatch leads to different patient populations with distinct demographic and symptom profiles
- Core assumption: The All of Us database accurately captures both ICD codes and clinical laboratory results for the same patients, and these two data sources represent different diagnostic intentions
- Evidence anchors:
  - [abstract] "Significant demographic differences were found, including 23% Hispanic patients in the Valley fever ICD-10 cohort versus 50% in the serological cohort"
  - [section] "Only 27% of Valley fever patients in the ICD-10 cohort reported fatigue versus 21% in the serological cohort"
  - [corpus] Weak evidence; related papers focus on NLP of clinical notes versus ICD codes but don't directly address demographic mismatch

### Mechanism 2
- Claim: ICD codes capture broader, less specific patient populations than clinically confirmed cohorts
- Mechanism: ICD codes are assigned when conditions are suspected or when billing requires classification, leading to inclusion of patients who may not have the confirmed disease. Clinical criteria use specific laboratory thresholds, creating more precise cohorts
- Core assumption: The laboratory thresholds used for clinical criteria accurately identify true disease cases, and ICD codes are not systematically removed when tests return negative
- Evidence anchors:
  - [abstract] "For Valley fever, 811 patients were identified via ICD-10 codes versus 619 via serological confirmation, with only 24 in both groups"
  - [section] "The CM cohort had 811 patients in the ICD-10 group, 619 patients in the positive-serology group, and 24 with both"
  - [corpus] Weak evidence; related papers discuss ICD-based cohort creation but don't quantify overlap between ICD and laboratory-confirmed groups

### Mechanism 3
- Claim: Demographic differences between ICD and clinical cohorts reflect healthcare access and documentation disparities
- Mechanism: Patients from different demographic groups may have different patterns of healthcare seeking, testing, and documentation, leading to systematic differences in how their conditions are recorded in ICD codes versus laboratory results
- Core assumption: Demographic factors influence both the likelihood of receiving certain tests and the patterns of ICD code assignment for billing purposes
- Evidence anchors:
  - [abstract] "Significant demographic differences were found, including 23% Hispanic patients in the Valley fever ICD-10 cohort versus 50% in the serological cohort"
  - [section] "Our ICD-10 cohort was predominantly white (63%), while our positive serology cohort and ICD-10 and positive serology cohort were largely unknown (54% and 46%, respectively)"
  - [corpus] Weak evidence; related papers focus on prediction models but don't analyze demographic patterns in ICD versus clinical data

## Foundational Learning

- Concept: ICD coding system and its billing purpose
  - Why needed here: Understanding that ICD codes are designed for billing rather than clinical diagnosis is essential to interpret why they may not accurately represent disease populations for ML
  - Quick check question: What is the primary intended use of ICD codes according to the paper?

- Concept: Laboratory confirmation versus clinical suspicion
  - Why needed here: The paper contrasts ICD codes (which may capture suspected conditions) with laboratory-confirmed diagnoses, requiring understanding of diagnostic certainty levels
  - Quick check question: How does the paper define the "clinical criteria" cohort for Valley fever?

- Concept: Cohort overlap analysis
  - Why needed here: The small overlap between ICD and clinical cohorts (24 for Valley fever, 6,531 for myocardial infarction) is central to the paper's conclusion about ICD code limitations
  - Quick check question: What percentage of patients with Valley fever appear in both the ICD-10 and serological cohorts?

## Architecture Onboarding

- Component map: All of Us Researcher Workbench -> Observational Medical Outcomes Partnership (OMOP) infrastructure -> Cohort creation modules -> Data extraction -> Demographic and clinical variable comparison -> Statistical analysis
- Critical path: 1) Define inclusion criteria for ICD-10 and clinical cohorts, 2) Extract patient data from All of Us using OMOP standardized format, 3) Perform demographic and clinical variable comparisons, 4) Conduct statistical analysis of cohort differences
- Design tradeoffs: Using All of Us provides diverse population representation but may have limited clinical detail compared to single-institution EHRs. ICD-10 provides broad coverage but may include false positives, while clinical criteria are more specific but may miss cases without testing
- Failure signatures: Large cohort size differences with minimal overlap suggest ICD codes capture different populations than clinical criteria. Demographic discrepancies between cohorts indicate systematic bias in ICD code assignment
- First 3 experiments:
  1. Replicate cohort creation using different laboratory thresholds to test sensitivity of clinical criteria definition
  2. Analyze temporal patterns of ICD code assignment versus laboratory test ordering to understand diagnostic workflows
  3. Compare symptom reporting patterns in patients with only ICD codes versus only laboratory confirmation to identify systematic differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of machine learning models trained on ICD-10 codes compare to those trained on clinically confirmed diagnoses for various medical conditions?
- Basis in paper: [explicit] The study highlights that ICD codes may not reliably represent medical conditions for machine learning model training, suggesting a potential performance gap
- Why unresolved: The paper does not directly compare the predictive performance of models trained on ICD-10 codes versus clinically confirmed data
- What evidence would resolve it: Conduct comparative studies evaluating the accuracy, precision, and recall of ML models trained on both ICD-10 and clinically confirmed datasets across various conditions

### Open Question 2
- Question: What are the specific factors contributing to the discrepancy between ICD-10 coded diagnoses and clinically confirmed diagnoses?
- Basis in paper: [inferred] The study notes significant discrepancies and suggests that ICD codes are intended for billing, not diagnosis, which may lead to inaccuracies
- Why unresolved: The paper identifies discrepancies but does not delve into the underlying causes or mechanisms
- What evidence would resolve it: Analyze the reasons for coding discrepancies, such as timing of code entry, billing considerations, and physician documentation practices

### Open Question 3
- Question: How do demographic differences in ICD-10 cohorts affect the generalizability of machine learning models?
- Basis in paper: [explicit] The study found significant demographic differences between ICD-10 and clinically confirmed cohorts, particularly in ethnicity and race
- Why unresolved: The paper does not explore how these demographic differences impact model performance across diverse populations
- What evidence would resolve it: Assess the impact of demographic disparities on model predictions and explore strategies to mitigate bias in ML models trained on ICD-10 data

## Limitations
- Exact laboratory test value thresholds used for clinical confirmation are not specified, making replication difficult
- Analysis relies on single database without external validation
- Symptom variables beyond fatigue are not detailed in the analysis

## Confidence
- High confidence: The core finding that ICD codes identify largely different patient populations than clinically confirmed cohorts is strongly supported by the 24 vs 619 patient overlap for Valley fever
- Medium confidence: The mechanism linking ICD code assignment to billing practices and healthcare access disparities requires assumptions about institutional workflows not directly observed
- Low confidence: Any conclusions about the absolute accuracy of ICD codes versus laboratory tests, as the study does not have a gold standard for disease presence

## Next Checks
1. Replicate the analysis using different laboratory test thresholds to determine sensitivity of clinical criteria definitions and test the robustness of cohort differences
2. Conduct temporal analysis of ICD code assignment patterns relative to laboratory test ordering dates to understand diagnostic workflow and identify potential delays or omissions in code updates
3. Compare symptom reporting patterns in patients with only ICD codes versus only laboratory confirmation using chi-square tests to identify systematic differences in clinical documentation practices