---
ver: rpa2
title: 'DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature
  Noise'
arxiv_id: '2404.09207'
source_url: https://arxiv.org/abs/2404.09207
tags:
- node
- noise
- graph
- edge
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Graph Neural Networks
  (GNNs) to noise in both edges and node features in real-world graphs. The authors
  propose a novel GNN model called DEGNN (Dual Experts Graph Neural Network) to handle
  noise in both edges and node features.
---

# DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise

## Quick Facts
- arXiv ID: 2404.09207
- Source URL: https://arxiv.org/abs/2404.09207
- Authors: Tai Hasegawa; Sukwon Yun; Xin Liu; Yin Jun Phua; Tsuyoshi Murata
- Reference count: 37
- Primary result: DEGNN achieves robust node classification on graphs with edge and node feature noise using dual experts

## Executive Summary
This paper addresses the vulnerability of Graph Neural Networks (GNNs) to noise in both edges and node features in real-world graphs. The authors propose DEGNN (Dual Experts Graph Neural Network), a novel architecture that uses two separate experts - an edge expert and a node feature expert - to independently handle different types of noise. These experts utilize self-supervised learning techniques to produce modified edges and node features that are more robust to noise, while the downstream network can be optimized for specific tasks.

The proposed approach is validated on real-world datasets including Cora, Citeseer, Photo, and Computer, demonstrating DEGNN's efficacy in managing noise both in original real-world graphs and in graphs with synthetic noise. The modification process can be trained end-to-end, allowing DEGNN to dynamically adjust and achieve optimal edge and node representations for specific tasks.

## Method Summary
DEGNN introduces a dual expert architecture to handle edge and node feature noise independently. The node feature expert uses contrastive learning on multiple augmented views of the graph (with noise in edges, node features, or both) to generate robust node embeddings. The edge expert generates denoised node embeddings and reconstructs edges based on cosine similarity between nodes, under the homophily assumption. These modified representations are then passed to a downstream 2-layer GCN for task-specific predictions. The model can be trained using either pre-training and fine-tuning (DEGNN-I) or modular learning (DEGNN-II), with self-supervised learning techniques used for expert training.

## Key Results
- DEGNN achieves superior performance on node classification tasks in graphs with both edge and node feature noise
- The dual expert architecture effectively separates the handling of edge noise and node feature noise
- DEGNN demonstrates robustness on both clean real-world graphs and graphs with synthetic noise injection
- End-to-end training allows the model to dynamically optimize edge and node representations for specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual experts enable DEGNN to handle edge and node feature noise independently while preserving downstream task accuracy.
- Mechanism: DEGNN uses two separate experts (edge expert and node feature expert) that individually process and denoise edges and node features via self-supervised learning. These denoised representations are then passed to the downstream network for task-specific optimization.
- Core assumption: The two types of noise (edge and node feature) can be effectively separated and treated independently without mutual interference.
- Evidence anchors:
  - [abstract] "The core idea of DEGNN is to design two separate experts: an edge expert and a node feature expert."
  - [section] "Our proposed dual experts design aims to eliminate these dependencies and learn both node representations and edges to be optimal."
  - [corpus] No strong direct evidence found; requires inference from model description.
- Break condition: If edge noise and node feature noise are strongly correlated or interdependent, independent processing may degrade performance.

### Mechanism 2
- Claim: Self-supervised learning enables robust node embeddings that are less sensitive to input noise.
- Mechanism: The node feature expert generates multiple augmented views of the graph (with noise in edges, node features, or both) and contrasts them with a negative graph. This contrastive learning objective encourages the encoder to extract robust embeddings.
- Core assumption: Contrastive learning on multiple noisy views can learn to filter out noise while preserving signal.
- Evidence anchors:
  - [section] "The node feature expert outputs node embeddings H, while the edge expert produces the modified adjacency matrix S... These experts utilize self-supervised learning techniques."
  - [section] "Graph Augmentation... We presume that by generating views and exposing the model to modified edges and nodes, it can achieve enhanced generalizability..."
  - [corpus] No strong direct evidence found; requires inference from self-supervised learning principles.
- Break condition: If augmentation noise overwhelms the signal, contrastive learning may fail to produce useful embeddings.

### Mechanism 3
- Claim: Edge reconstruction based on cosine similarity between denoised node embeddings improves graph structure for downstream tasks.
- Mechanism: The edge expert generates denoised node embeddings, computes cosine similarity between nodes, and rewires edges by removing edges with low similarity and adding edges with high similarity between non-connected nodes.
- Core assumption: After denoising, nodes with high cosine similarity are more likely to be connected (homophily assumption).
- Evidence anchors:
  - [section] "To differentiate between the impacts of node feature noise and edge noise... we further implement an edge expert... It achieves graph convolution operation through neighborhood aggregation."
  - [section] "This reconstruction process rewires the edges using the pairwise similarity in the embeddings H′ under the homophily assumption..."
  - [corpus] No strong direct evidence found; requires inference from graph theory principles.
- Break condition: If the homophily assumption is violated in the dataset, cosine similarity-based rewiring may introduce incorrect edges.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: DEGNN builds on standard GNN architectures and extends them with denoising experts
  - Quick check question: What is the difference between spectral-based and spatial-based GNNs?

- Concept: Graph Structure Learning (GSL) and its limitations
  - Why needed here: DEGNN addresses specific vulnerabilities of GSL methods when node features contain noise
  - Quick check question: Why do GSL models struggle with node feature noise according to the paper?

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: DEGNN's experts use self-supervised contrastive learning to generate robust node embeddings
  - Quick check question: How does contrastive learning help in creating noise-robust representations?

## Architecture Onboarding

- Component map:
  - Node Feature Expert -> Edge Expert -> Downstream GNN -> Prediction
  - Node/edge input → Expert processing → Denoised representations → Downstream task

- Critical path: Node/edge → Expert → Downstream GNN → Prediction
  The experts process and denoise inputs before passing them to the downstream network

- Design tradeoffs:
  - Separate experts vs. joint modeling: DEGNN separates edge and node feature denoising but trains them jointly with the downstream task
  - Augmentation strategy: Simple random rewiring and feature shuffling vs. more sophisticated augmentation methods
  - Edge reconstruction: Cosine similarity-based rewiring vs. learned edge prediction

- Failure signatures:
  - Poor performance on clean graphs may indicate over-regularization from denoising
  - Performance degradation when both noise types are present may indicate insufficient joint modeling
  - Memory issues may arise from maintaining multiple graph views

- First 3 experiments:
  1. Run DEGNN on Cora with no added noise to verify it doesn't degrade clean graph performance
  2. Add edge noise only (e.g., 10% edge perturbation) and compare DEGNN vs. baseline GNNs
  3. Add node feature noise only (e.g., Gaussian noise with λ=0.1) and compare DEGNN vs. baseline GNNs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DEGNN vary with different graph sizes and densities?
- Basis in paper: [inferred] The paper does not provide experiments on graphs of varying sizes and densities, which could affect the performance of DEGNN.
- Why unresolved: The paper only tests DEGNN on four datasets (Cora, Citeseer, Photo, Computer) without exploring the impact of graph size and density on its performance.
- What evidence would resolve it: Experiments on graphs with different sizes and densities, showing the performance of DEGNN compared to other models, would help understand how graph characteristics affect DEGNN's effectiveness.

### Open Question 2
- Question: Can DEGNN handle multi-relational graphs or graphs with heterogeneous node types?
- Basis in paper: [inferred] The paper focuses on undirected graphs with a single type of nodes and edges, and does not discuss the applicability of DEGNN to more complex graph structures.
- Why unresolved: The proposed DEGNN model is designed for simple undirected graphs, and its performance on multi-relational or heterogeneous graphs is not explored.
- What evidence would resolve it: Experiments on multi-relational or heterogeneous graphs, comparing DEGNN's performance to other models, would help determine its applicability to these more complex graph structures.

### Open Question 3
- Question: How does DEGNN perform on graphs with overlapping communities or hierarchical structures?
- Basis in paper: [inferred] The paper does not investigate the performance of DEGNN on graphs with overlapping communities or hierarchical structures, which are common in real-world networks.
- Why unresolved: The proposed DEGNN model is not explicitly designed to handle overlapping communities or hierarchical structures, and its effectiveness on such graphs is not evaluated.
- What evidence would resolve it: Experiments on graphs with overlapping communities or hierarchical structures, comparing DEGNN's performance to other models, would help assess its ability to capture complex network structures.

## Limitations
- The paper lacks empirical validation of the independence assumption between edge and node feature noise
- Exact implementation details of the self-supervised learning techniques are not specified
- The homophily assumption for edge reconstruction is not systematically tested across datasets
- Computational costs and memory requirements are not reported

## Confidence
- **High confidence**: The core problem statement (GNNs are vulnerable to edge and node feature noise) is well-established in the literature and supported by extensive prior work.
- **Medium confidence**: The dual expert architecture is logically sound, but empirical validation of the independence assumption between edge and node feature noise is limited.
- **Medium confidence**: The self-supervised learning approach for denoising is theoretically plausible, but lacks rigorous ablation studies showing its effectiveness versus alternatives.

## Next Checks
1. Test independence assumption: Run DEGNN on graphs with correlated edge and node feature noise (where adding/removing an edge also modifies node features) to verify that separate experts don't degrade performance when noise types are interdependent.

2. Ablation of augmentation strategy: Replace the current simple augmentation (random rewiring, feature shuffling) with more sophisticated methods like DropEdge or feature masking, and measure if DEGNN's performance improves or degrades.

3. Homophily sensitivity analysis: Systematically evaluate DEGNN on graphs with varying homophily levels (e.g., using the heterophily score) to determine the threshold where cosine similarity-based edge reconstruction becomes detrimental.