---
ver: rpa2
title: Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks
arxiv_id: '2403.15458'
source_url: https://arxiv.org/abs/2403.15458
tags:
- bert
- toxic
- language
- ijfmr
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the effectiveness of pre-trained language
  models for detecting toxic in-game chat messages in DOTA 2. The researchers collected
  approximately 2,552 labeled chat messages categorized as non-toxic, mild, or toxic.
---

# Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks

## Quick Facts
- arXiv ID: 2403.15458
- Source URL: https://arxiv.org/abs/2403.15458
- Reference count: 19
- Primary result: GPT-3 achieved 83% accuracy on in-game toxicity detection, outperforming BERT Large (82%) and BERT Base (80%)

## Executive Summary
This study evaluates pre-trained language models for detecting toxic chat messages in DOTA 2 gaming environment. The researchers collected and labeled approximately 2,552 chat messages across three categories: non-toxic, mild, and toxic. They fine-tuned BERT (Base-uncased and Large-uncased) models and evaluated GPT-3 API for toxicity classification. Results demonstrate that transformer-based models can effectively classify in-game toxic chats, with GPT-3 showing superior performance. The study validates the potential of pre-trained language models for addressing online toxicity in gaming contexts.

## Method Summary
The study collected DOTA 2 chat messages via OpenDota API, filtered for English language, and manually labeled them into non-toxic, mild, and toxic categories. Researchers fine-tuned BERT models using HuggingFace library with 5 epochs, batch size 16, and learning rate 2e-5. GPT-3 was evaluated via API with appropriate prompt engineering. Models were tested on 25% holdout data using accuracy, precision, recall, and F1-score metrics. Data preprocessing included language detection and class balancing through undersampling.

## Key Results
- GPT-3 achieved highest accuracy at 83%, followed by BERT Large-uncased at 82% and BERT Base-uncased at 80%
- All models demonstrated strong performance in classifying in-game toxic chats across three categories
- BERT (Base-uncased) achieved 2% training loss and 89% accuracy on last epoch
- BERT (Large-uncased) achieved 14% training loss and 81% accuracy on last epoch
- GPT-3 achieved 0% training loss and 100% accuracy on last iteration step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained transformer-based language models capture contextual meaning of toxic and non-toxic chat messages.
- Mechanism: Transformer models use self-attention mechanisms to weigh the importance of each word in relation to others, allowing them to understand context-dependent meanings.
- Core assumption: The pre-training on large text corpora has already captured sufficient linguistic patterns for the model to understand context-specific toxicity.
- Evidence anchors:
  - [abstract] "Transformer-based language models are notable with a key component called 'attention mechanism' from the transformer architecture."
  - [section] "GPT's difference from BERT is it generates coherent text using autoregressive language modeling, whereas BERT uses masked language modeling."
- Break condition: If chat messages contain domain-specific slang or abbreviations not present in pre-training data.

### Mechanism 2
- Claim: Fine-tuning pre-trained models on labeled toxic chat data adapts general language understanding to the specific domain of gaming communication.
- Mechanism: The model takes pre-trained weights and adjusts them using domain-specific examples, allowing it to learn patterns of toxic language specific to gaming contexts.
- Core assumption: The 2,552 labeled examples are sufficient to capture the diversity of toxic language patterns in gaming.
- Evidence anchors:
  - [abstract] "The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats."
  - [section] "Pre-trained BERT (Base-uncased) and BERT (Large-uncased) were pulled from their open-source repository in the HuggingFace platform."
- Break condition: If the dataset is too small or not representative of the full range of toxic expressions.

### Mechanism 3
- Claim: The performance hierarchy (GPT-3 > BERT Large > BERT Base) reflects model capacity and architecture differences in handling complex toxicity classification.
- Mechanism: Larger models with more parameters and different architectural approaches can capture more nuanced patterns in toxicity classification.
- Core assumption: Model size and architecture directly correlate with classification performance for this specific task.
- Evidence anchors:
  - [abstract] "GPT-3 achieved the highest accuracy at 83%, followed by BERT Large-uncased at 82% and BERT Base-uncased at 80%."
  - [section] "GPT-3 was able to achieve (0%) training loss and (100%) accuracy on its last iteration step."
- Break condition: If the performance differences are due to dataset artifacts or evaluation bias.

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how transformers weigh word importance helps explain why they can distinguish between toxic and non-toxic context-dependent phrases.
  - Quick check question: How does self-attention allow a model to understand that "you're trash" is toxic while "this trash game" might not be?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The study leverages pre-trained models rather than building new ones, requiring understanding of transfer learning principles.
  - Quick check question: Why is fine-tuning a pre-trained BERT model more effective than training a new model from scratch on the same 2,552 examples?

- Concept: Classification metrics (precision, recall, F1-score)
  - Why needed here: Evaluating model performance requires understanding how these metrics capture different aspects of classification quality.
  - Quick check question: If a toxicity detection model has high precision but low recall, what does this tell you about its behavior?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing (language detection, undersampling) -> Model fine-tuning (BERT/GPT-3) -> Evaluation (metrics calculation) -> Comparison
- Critical path: Data collection -> Preprocessing -> Fine-tuning -> Evaluation -> Analysis
- Design tradeoffs: BERT Base vs. Large (computational cost vs. accuracy), GPT-3 API usage (cost vs. performance), undersampling vs. class weighting (data balance vs. information loss)
- Failure signatures: High training accuracy but low test accuracy (overfitting), poor performance on specific toxicity classes (class imbalance), slow inference times (model size issues)
- First 3 experiments:
  1. Train and evaluate BERT Base-uncased on the full dataset without undersampling to establish baseline performance
  2. Implement and compare different class balancing techniques (undersampling, oversampling, class weights) on BERT Base
  3. Compare GPT-3 API predictions on a small validation set before full fine-tuning to assess if zero-shot capabilities are sufficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do these language models perform when trained on toxic chat data from multiple gaming platforms beyond DOTA 2?
- Basis in paper: [inferred] The study focused solely on DOTA 2 chat data and suggests future work could extend to other platforms.
- Why unresolved: The models were only evaluated on DOTA 2 data, which may have unique linguistic patterns and toxicity characteristics.
- What evidence would resolve it: Comparative evaluation of the same models on toxic chat datasets from various gaming platforms showing cross-platform performance differences.

### Open Question 2
- Question: How would alternative transformer-based models like RoBERTa, DistilBERT, or GPT-4 perform compared to the tested models for in-game toxicity detection?
- Basis in paper: [inferred] The paper suggests "other alternative language models and other variations of GPT and BERT can also be a propitious candidate" but doesn't test them.
- Why unresolved: Only three specific models were evaluated, leaving a gap in understanding the broader landscape of applicable models.
- What evidence would resolve it: Systematic evaluation of multiple transformer architectures and sizes on the same dataset with direct performance comparisons.

### Open Question 3
- Question: What is the optimal threshold for classifying mild toxicity versus toxic content, and how does this threshold affect model performance and user experience?
- Basis in paper: [explicit] The study created three categories (non-toxic, mild, toxic) but doesn't discuss threshold optimization or user impact.
- Why unresolved: The classification boundaries were likely set arbitrarily, and different thresholds could significantly impact detection accuracy and player experience.
- What evidence would resolve it: Analysis showing how varying classification thresholds affects precision/recall metrics and qualitative assessment of user satisfaction with different sensitivity levels.

## Limitations
- The dataset size of 2,552 examples is relatively small for training complex transformer models, raising concerns about overfitting and generalizability
- Manual labeling process lacks detailed documentation of criteria, which could introduce subjective bias in class assignments
- Comparison between GPT-3 and BERT models is not entirely fair since GPT-3 was used via API while BERT models were fully fine-tuned

## Confidence
- High Confidence (80-100%): The BERT models' performance on the test set (80-82% accuracy) is well-supported by the reported metrics and training procedures
- Medium Confidence (50-80%): The claim that GPT-3 achieved 83% accuracy requires caution due to lack of details on implementation and whether this was fine-tuning vs. few-shot learning
- Low Confidence (0-50%): The generalization claim that pre-trained language models have "promising potential for addressing online toxicity" extends beyond the specific gaming context and limited dataset size

## Next Checks
1. **Dataset Validation**: Independently verify the 2,552 chat messages by collecting a fresh sample from DOTA 2 and applying consistent labeling criteria. Test inter-rater reliability among multiple annotators to establish labeling consistency and identify potential biases in the toxicity classification scheme.

2. **Controlled Model Comparison**: Implement a fair comparison between GPT-3 and BERT by either: (a) fine-tuning GPT-3 on the same dataset with identical training procedures, or (b) evaluating BERT using few-shot learning approaches to match GPT-3's usage pattern. This would isolate true architectural differences from implementation artifacts.

3. **Robustness Testing**: Evaluate model performance on adversarial examples and out-of-distribution toxic language not present in the training set. This includes testing with gaming slang from other games, misspellings, and culturally-specific insults to assess whether models generalize beyond the specific DOTA 2 corpus or simply memorize patterns.