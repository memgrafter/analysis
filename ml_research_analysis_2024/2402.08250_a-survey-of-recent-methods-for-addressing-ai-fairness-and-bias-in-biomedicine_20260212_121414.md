---
ver: rpa2
title: A survey of recent methods for addressing AI fairness and bias in biomedicine
arxiv_id: '2402.08250'
source_url: https://arxiv.org/abs/2402.08250
tags:
- bias
- data
- learning
- methods
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically examined bias and fairness issues in
  AI systems applied to biomedicine, identifying five major sources of bias: historical
  bias, representation bias, aggregation bias, population bias, and measurement bias.
  The review analyzed 55 papers published between 2018-2023, categorizing debiasing
  methods into distributional approaches (data augmentation, perturbation, reweighting,
  federated learning) and algorithmic approaches (unsupervised representation learning,
  adversarial learning, disentangled representation, loss functions, and causality-based
  methods).'
---

# A survey of recent methods for addressing AI fairness and bias in biomedicine

## Quick Facts
- arXiv ID: 2402.08250
- Source URL: https://arxiv.org/abs/2402.08250
- Reference count: 40
- 55 papers analyzed between 2018-2023 identifying five major bias sources and categorizing 55 papers into distributional and algorithmic debiasing approaches

## Executive Summary
This survey systematically examines bias and fairness issues in AI systems applied to biomedicine, identifying five major sources of bias: historical bias, representation bias, aggregation bias, population bias, and measurement bias. The review analyzes 55 papers published between 2018-2023, categorizing debiasing methods into distributional approaches (data augmentation, perturbation, reweighting, federated learning) and algorithmic approaches (unsupervised representation learning, adversarial learning, disentangled representation, loss functions, and causality-based methods). The analysis reveals that while various methods exist to address different types of bias, most studies focus on policies rather than technical solutions, highlighting the need for more algorithmic approaches to bias mitigation.

## Method Summary
The paper conducts a systematic literature review analyzing 55 papers published between 2018-2023 from PubMed, ACM digital library, and IEEE Xplore. The search used combinations of keywords like (AI or ML or Deep learning or Algorithmic) + (equity or bias) + medical, with additional constraints filtering for biomedical applications. Papers were categorized into distributional methods (modifying data distribution through augmentation, perturbation, reweighting, or federated learning) and algorithmic methods (modifying training procedures through representation learning, adversarial training, disentangled representations, loss functions, or causality-based approaches). The review compares strengths and weaknesses of each method and discusses their applicability to biomedical tasks.

## Key Results
- Five major bias sources identified: historical bias, representation bias, aggregation bias, population bias, and measurement bias
- Debiasing methods categorized into distributional (data augmentation, perturbation, reweighting, federated learning) and algorithmic approaches (adversarial learning, disentangled representation, causality-based methods)
- Most studies focus on policies rather than technical solutions, with limited algorithmic approaches to bias mitigation
- Effectiveness varies significantly across biomedical domains with inconsistent evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAN-based data augmentation can balance underrepresented subgroups in biomedical imaging datasets.
- Mechanism: GANs generate synthetic samples that mimic real data distribution, allowing augmentation of minority classes without requiring new clinical data collection.
- Core assumption: Synthetic samples generated by GANs sufficiently represent the statistical properties of the underrepresented subgroup.
- Evidence anchors:
  - [abstract] GANs can synthesize retinal vessel maps that better represent the distribution of plus disease compared to the original dataset, potentially reducing bias due to limited sample size.
  - [section] Burlina et al. used GANs to change the retinal pigmentation in retinal fundus images and create a more balanced dataset.
  - [corpus] Weak/no direct evidence in corpus neighbors.
- Break condition: Generated samples fail to capture critical disease features or introduce new biases, degrading model performance on real data.

### Mechanism 2
- Claim: Adversarial learning removes protected attributes by forcing the model to "forget" demographic information.
- Mechanism: During training, a discriminator is used to predict protected attributes from the model's representation. The main model is trained to minimize prediction loss while maximizing the discriminator's loss (sign-flipped gradient), causing the representation to lose demographic information.
- Core assumption: Protected attributes can be effectively isolated and removed from learned representations without harming task performance.
- Evidence anchors:
  - [abstract] Hard debiasing propagates the sign-flipped gradient of the classifier that predicts the protected attributes.
  - [section] Prior work in colorectal cancer prediction has shown that after debiasing, the representation space is more invariant to the bias variables.
  - [corpus] Weak/no direct evidence in corpus neighbors.
- Break condition: The model cannot adequately remove protected attributes due to strong correlations with task-relevant features, or performance drops significantly.

### Mechanism 3
- Claim: Data perturbation via Counterfactual Data Augmentation (CDA) reduces bias by explicitly creating examples that swap demographic attributes.
- Mechanism: CDA applies rule-based transformations (e.g., changing "he" to "she") to existing text data, generating new samples that represent underrepresented groups and balancing the dataset.
- Core assumption: Simple lexical replacements accurately capture the semantic context and do not introduce artifacts or implausible scenarios.
- Evidence anchors:
  - [abstract] CDA is used to craft samples by replacing words that may cause bias with words that refer to under-represented groups.
  - [section] To reduce gender bias in training data, CDA would modify the phrase, "He is a doctor," to "She is a doctor," to generate new samples in the dataset.
  - [corpus] Weak/no direct evidence in corpus neighbors.
- Break condition: Generated counterfactuals are semantically inconsistent or do not reflect realistic variations, leading to poor generalization.

## Foundational Learning

- Concept: Understanding the sources and types of bias in biomedical AI.
  - Why needed here: Identifying the specific bias source (historical, representation, aggregation, population, measurement) determines the appropriate mitigation strategy.
  - Quick check question: Which type of bias occurs when a development sample does not adequately represent a section of the population?

- Concept: Familiarity with distributional vs. algorithmic debiasing methods.
  - Why needed here: Distributional methods modify the data distribution, while algorithmic methods modify the training procedure or model architecture; choosing the right category depends on the bias source and data constraints.
  - Quick check question: What is the main difference between distributional and algorithmic debiasing methods?

- Concept: Basics of generative models (GANs, diffusion models) and adversarial training.
  - Why needed here: These are core technical tools for implementing data augmentation and algorithmic debiasing methods in practice.
  - Quick check question: In GANs, what are the two main components and their opposing objectives during training?

## Architecture Onboarding

- Component map: Raw data → preprocessing → augmentation/perturbation → model training → debiasing step → evaluation
- Critical path: Data collection → bias identification → method selection (distributional vs. algorithmic) → implementation → evaluation with fairness metrics
- Design tradeoffs: GANs offer high-quality synthetic data but are unstable to train; CDA is simple but limited to text; adversarial methods can reduce bias but may hurt performance; federated learning preserves privacy but is complex to implement
- Failure signatures: Performance degradation on real data after synthetic augmentation; inability to remove protected attributes; increased variance or instability during training; poor generalization to unseen demographics
- First 3 experiments:
  1. Implement CDA on a small biomedical text dataset to test bias reduction with minimal infrastructure
  2. Train a GAN to augment underrepresented classes in a biomedical imaging dataset and evaluate downstream model fairness
  3. Apply adversarial debiasing to a baseline model and measure changes in both performance and fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective algorithmic approaches to address bias in biomedical AI systems when multiple sources of bias occur simultaneously?
- Basis in paper: [explicit] The paper identifies five major types of bias sources and discusses various algorithmic methods, but notes that multiple sources of bias can occur at the same time, requiring a comprehensive approach.
- Why unresolved: The paper acknowledges the complexity of addressing multiple biases simultaneously but does not provide a clear methodology for combining or prioritizing different algorithmic approaches.
- What evidence would resolve it: Empirical studies comparing the effectiveness of combined algorithmic approaches versus individual methods in addressing multiple biases in real-world biomedical datasets.

### Open Question 2
- Question: How can federated learning be effectively implemented in biomedicine to address population bias while overcoming practical challenges such as data heterogeneity and communication bottlenecks?
- Basis in paper: [explicit] The paper discusses federated learning as a potential solution for increasing data diversity but acknowledges practical challenges including data format differences, computational power variations, and communication bottlenecks.
- Why unresolved: While the theoretical benefits of federated learning are discussed, the paper does not provide concrete solutions for the practical implementation challenges in real biomedical settings.
- What evidence would resolve it: Case studies demonstrating successful federated learning implementations in multiple healthcare institutions with varying data formats and computational capabilities.

### Open Question 3
- Question: What is the optimal balance between removing protected attributes and maintaining model performance when using adversarial learning techniques in biomedical AI?
- Basis in paper: [explicit] The paper discusses adversarial learning methods for removing protected attributes but notes that removing all protected attributes might reduce the model's performance, particularly in biomedical settings.
- Why unresolved: The paper does not provide specific guidelines or empirical evidence for determining how much protected attribute information should be removed while maintaining acceptable model performance.
- What evidence would resolve it: Systematic evaluation of adversarial learning methods across different biomedical tasks showing the trade-off between fairness improvements and performance degradation.

## Limitations

- Literature search may have missed relevant papers due to keyword restrictions or database coverage limitations
- Analysis focuses primarily on technical solutions while acknowledging that most published work addresses policies rather than algorithmic approaches
- Effectiveness of debiasing methods varies significantly across biomedical domains with inconsistent evaluation metrics

## Confidence

- **High confidence**: The categorization of bias sources (historical, representation, aggregation, population, measurement) is well-established in the fairness literature and clearly applicable to biomedical AI
- **Medium confidence**: The taxonomy of debiasing methods (distributional vs. algorithmic) is comprehensive, but effectiveness claims for specific methods are limited by the variability in study quality and evaluation approaches across the surveyed papers
- **Low confidence**: Specific quantitative claims about the relative effectiveness of different debiasing approaches are not supported due to inconsistent evaluation metrics and limited replication studies in the biomedical domain

## Next Checks

1. Replicate the literature search with expanded keyword combinations and additional biomedical databases (e.g., bioRxiv, medRxiv) to verify completeness of coverage
2. Conduct a small-scale experiment implementing CDA on a biomedical text dataset to empirically test the mechanism described and validate the feasibility of this approach
3. Contact authors of highly-cited papers in the survey to obtain clarification on implementation details and evaluation protocols that were not fully specified in the published work