---
ver: rpa2
title: Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language
  Models
arxiv_id: '2412.05353'
source_url: https://arxiv.org/abs/2412.05353
tags:
- features
- path
- sentences
- feature
- garden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how autoregressive transformer language
  models incrementally process sentences, focusing on garden path sentences as a case
  study. The authors use sparse autoencoders and causal interpretability methods to
  uncover interpretable features that determine which reading of a garden path sentence
  the model prefers.
---

# Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models

## Quick Facts
- arXiv ID: 2412.05353
- Source URL: https://arxiv.org/abs/2412.05353
- Reference count: 40
- Primary result: Language models represent multiple interpretations of garden path sentences simultaneously but do not reanalyze initial predictions when encountering disambiguating evidence.

## Executive Summary
This paper investigates how autoregressive transformer language models incrementally process sentences, focusing on garden path sentences as a case study. The authors use sparse autoencoders and causal interpretability methods to uncover interpretable features that determine which reading of a garden path sentence the model prefers. Key findings include: (1) While many important features relate to syntactic structure, some reflect syntactically irrelevant heuristics. (2) LMs' representations encode multiple interpretations of the sentence, rather than committing to one reading. (3) LMs do not repair or reanalyze their initial incorrect representations when encountering disambiguating evidence. The study provides insights into the mechanisms underlying incremental sentence processing in LMs and highlights the importance of mechanistic investigations in understanding model behavior.

## Method Summary
The authors use sparse autoencoders to decompose model activations into interpretable features, then apply attribution patching with integrated gradients (AtP-IG) to identify causally relevant features for garden path sentence processing. They create datasets of garden path sentences (NP/Z, NP/S, MV/RR structures) and their unambiguous variants, then measure model behavior on these sentences. Features are manually annotated by observing their activations, and causal interventions are performed by clamping features on/off to verify their importance. Structural probes are trained to map model representations to dependency parses, providing complementary evidence about syntactic processing.

## Key Results
- Language models exhibit garden path effects, preferring garden path continuations for ambiguous inputs except NP/S structures
- Multiple features fire for both possible readings of garden path sentences, indicating simultaneous representation of interpretations
- Feature circuits for initial sentence processing show little overlap with those used for follow-up questions, suggesting no reanalysis or repair mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders decompose model activations into interpretable features that can be used to identify causally relevant features for garden path sentence processing.
- Mechanism: Sparse autoencoders apply ReLU activation to weighted sums of input activations minus bias terms, creating sparse feature vectors where most features are zero except when they cause parts of the input to differ from their mean value. This sparsity makes features more monosemantic and interpretable than individual neurons.
- Core assumption: The SAEs effectively decompose the model's activations into features that capture the essential computational elements used by the model for sentence processing.
- Evidence anchors:
  - [abstract] "We use sparse autoencoders to identify interpretable features that determine which continuation—and thus which reading—of a garden path sentence the LM prefers."
  - [section] "We thus opt to interpret the features of sparse autoencoders (SAEs; Bricken et al., 2023), autoencoders trained on the output activations of LM submodules."
  - [corpus] Weak evidence - no direct citations in corpus papers for SAEs in this specific application, though related methods are mentioned.

### Mechanism 2
- Claim: Language models represent multiple possible interpretations of ambiguous sentences simultaneously rather than committing to a single interpretation.
- Mechanism: When processing ambiguous garden path sentences, the model activates features corresponding to both possible readings (e.g., subject and object detectors firing at the same position). This is evidenced by non-zero average activations for both pro-GP and anti-GP feature groups.
- Core assumption: The presence of non-zero activations for features corresponding to both readings indicates that the model is actively representing both possibilities.
- Evidence anchors:
  - [abstract] "Moreover, while most active features correspond to one reading of the sentence, some features correspond to the other, suggesting that LMs assign weight to both possibilities simultaneously."
  - [section] "We find that in both the NP/Z and NP/S cases, pro- and non-GP features have non-zero average activations, ranging from 0.27 to 0.41."
  - [corpus] Moderate evidence - related papers discuss garden path effects in LMs but don't specifically address simultaneous representation of multiple readings.

### Mechanism 3
- Claim: Language models do not reanalyze or repair their initial structural predictions when encountering disambiguating evidence, instead using entirely new feature sets for follow-up questions.
- Mechanism: When answering follow-up questions about garden path sentences, the model does not reuse the syntactic features identified during initial sentence processing. Instead, it relies on new features, many of which are not syntax-sensitive but rather spurious features promoting Yes/No answers.
- Core assumption: The lack of overlap between features used for initial sentence processing and those used for follow-up questions indicates no reanalysis or repair mechanism.
- Evidence anchors:
  - [abstract] "Finally, LMs do not re-use features from garden path sentence processing to answer follow-up questions."
  - [section] "There is little feature overlap across circuits: the IoU is 0% for NP/S, and 0.2% for NP/Z."
  - [corpus] Weak evidence - corpus papers discuss garden path effects but don't specifically address feature reuse for follow-up questions.

## Foundational Learning

- Concept: Sparse autoencoders and their application to model interpretability
  - Why needed here: Understanding how SAEs decompose model activations into interpretable features is crucial for identifying the features that drive garden path sentence processing.
  - Quick check question: What property of SAE features makes them more interpretable than individual neurons, and how is this property achieved mathematically?

- Concept: Causal interpretability methods (attribution patching with integrated gradients)
- Concept: Structural probes for dependency parsing

## Architecture Onboarding

- Component map: Garden path sentence datasets -> Pre-trained transformer model (Pythia-70m/Gemma-2-2b) -> Sparse autoencoders -> AtP-IG for causal estimation -> Manual feature annotation -> Causal interventions -> Structural probes for dependency parsing
- Critical path: (1) Preprocess garden path sentences and create unambiguous variants, (2) Measure model behavior on these sentences, (3) Use SAEs to decompose activations into features, (4) Apply AtP-IG to find causally relevant features, (5) Manually annotate features, (6) Perform causal interventions to verify feature importance, (7) Train structural probes and compare results.
- Design tradeoffs: Using SAEs provides interpretable features but may miss important distributed representations; using AtP-IG provides efficient causal estimation but is an approximation; using structural probes provides complementary evidence but requires training additional models.
- Failure signatures: If faithfulness values are far from 1.0, the circuit doesn't capture the full mechanism; if feature overlap between probe and circuit features is low, the probe may not be capturing the same information; if causal interventions don't change model behavior as expected, the features may not be causally relevant.
- First 3 experiments:
  1. Run behavioral analysis on garden path sentences to verify the model exhibits garden path effects (prefers GP continuations for ambiguous inputs except NP/S).
  2. Use AtP-IG to find the top 100 features influencing p(GP) - p(non-GP) and manually annotate them to identify interpretable vs. spurious features.
  3. Perform causal interventions by clamping subject/object detectors on/off to verify these features causally influence the model's preferred reading.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models resolve temporary ambiguities during incremental processing, and what mechanisms are involved?
- Basis in paper: [explicit] The paper investigates whether LMs use syntactic features or shallow heuristics to incrementally process sentences, and whether they repair or reanalyze their initial incorrect representations when encountering disambiguating evidence.
- Why unresolved: The paper finds that LMs represent multiple interpretations of garden path sentences but do not reanalyze or repair their initial incorrect representations when disambiguating evidence is encountered. However, the specific mechanisms by which LMs resolve ambiguities remain unclear.
- What evidence would resolve it: Detailed analysis of the feature circuits involved in ambiguity resolution, including the causal relationships between features and their temporal dynamics during incremental processing.

### Open Question 2
- Question: Do language models recognize ambiguity as a meaningful signal, or do they simply represent multiple interpretations without explicit awareness of the ambiguity?
- Basis in paper: [inferred] The paper distinguishes between the representation of ambiguities and the recognition of ambiguity as a meaningful signal, noting that detecting ambiguity has various functions in natural language.
- Why unresolved: While the paper shows that LMs represent multiple interpretations of garden path sentences, it remains unclear whether they recognize these as instances of ambiguity or simply as possible continuations.
- What evidence would resolve it: Experiments designed to test whether LMs can identify ambiguous inputs as such, or whether they can use ambiguity detection for specific linguistic functions like humor or politeness.

### Open Question 3
- Question: How do the mechanisms of ambiguity resolution in language models compare to those in human language processing, and what insights can be gained from these comparisons?
- Basis in paper: [explicit] The paper draws parallels between psycholinguistic studies of human sentence processing and the investigation of LM behavior, particularly in the context of garden path sentences.
- Why unresolved: While the paper highlights similarities and differences between LMs and humans in handling garden path sentences, a comprehensive comparison of the underlying mechanisms is still needed.
- What evidence would resolve it: Comparative studies of LMs and humans on a range of syntactic ambiguities, using similar experimental paradigms and analyzing the features and mechanisms involved in each case.

## Limitations
- The study relies on sparse autoencoders which may miss important distributed representations not captured by monosemantic features
- The attribution patching with integrated gradients method used for causal estimation is an approximation that may not capture all relevant causal pathways
- Manual annotation of features introduces potential subjectivity in identifying interpretable versus spurious features

## Confidence

**High Confidence**: The finding that language models represent multiple interpretations simultaneously is well-supported by direct evidence of non-zero activations for both pro- and anti-garden path features. The observation that LMs do not reuse features for follow-up questions is clearly demonstrated through feature overlap analysis with concrete percentages.

**Medium Confidence**: The claim about SAEs effectively decomposing activations into interpretable features depends on the assumption that sparsity equals interpretability, which may not always hold. The observation that some features reflect syntactically irrelevant heuristics is based on manual annotation, which introduces subjectivity.

**Low Confidence**: The interpretation that the lack of feature reuse indicates no reanalysis or repair mechanism is plausible but could also reflect architectural constraints or optimization objectives rather than genuine absence of reanalysis capability.

## Next Checks

1. **Cross-scale validation**: Test whether the same simultaneous representation pattern holds in larger LMs (e.g., LLaMA-2-7B, GPT-3.5) and whether the feature overlap patterns for follow-up questions change with model scale.

2. **Temporal dynamics analysis**: Track feature activations over time to determine whether the model's representations of multiple readings evolve differently across layers, potentially revealing hidden reanalysis mechanisms not captured by static feature overlap analysis.

3. **Intervention ablation study**: Systematically disable syntactic vs. heuristic features during garden path sentence processing to quantify their relative contributions to the model's final interpretation choice, providing clearer evidence for the claim about syntactically irrelevant heuristics.