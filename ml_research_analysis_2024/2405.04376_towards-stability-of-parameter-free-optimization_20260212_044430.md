---
ver: rpa2
title: Towards Stability of Parameter-free Optimization
arxiv_id: '2405.04376'
source_url: https://arxiv.org/abs/2405.04376
tags:
- adam
- adamg
- optimization
- parameter-free
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hyperparameter tuning in
  adaptive gradient training methods, particularly the selection of an appropriate
  learning rate. The authors propose a novel parameter-free optimizer called ADAMG
  (Adam with the golden step size) that automatically adapts to diverse optimization
  problems without manual tuning.
---

# Towards Stability of Parameter-free Optimization

## Quick Facts
- arXiv ID: 2405.04376
- Source URL: https://arxiv.org/abs/2405.04376
- Authors: Yijiang Pang; Shuyang Yu; Bao Hoang; Jiayu Zhou
- Reference count: 40
- Primary result: ADAMG achieves superior reliability (76% vs 49-56%) and smaller performance gaps (3.0% vs 8.2-18.5%) compared to other parameter-free optimizers across 42 tasks

## Executive Summary
This paper addresses the challenge of hyperparameter tuning in adaptive gradient training methods by proposing ADAMG, a parameter-free optimizer that automatically adapts to diverse optimization problems. The key innovation is the derivation of a "golden step size" for AdaGrad-Norm that preserves tuning-free convergence while approximating optimal step sizes in expectation. The authors integrate this golden step size into AdaGrad-Norm and extend it to create an Adam-like parameter-free method that demonstrates superior reliability and performance across various optimization tasks including image classification and language modeling.

## Method Summary
The paper proposes ADAMG (Adam with the golden step size) as a parameter-free optimizer that automatically adapts to diverse optimization problems. The method is based on deriving a "golden step size" for the AdaGrad-Norm algorithm that preserves tuning-free convergence and approximates the optimal step size in expectation across various optimization scenarios. This golden step size is then integrated into AdaGrad-Norm and extended to create an Adam-like parameter-free method. The implementation involves computing the golden step size using a numerator function s(x) = pxq (with p=0.2, q=0.24) and incorporating it into the Adam update rule with momentum mechanism.

## Key Results
- ADAMG achieves 76% reliability compared to 49-56% for other parameter-free methods
- ADAMG demonstrates smaller average performance gaps (3.0%) compared to 8.2-18.5% for other methods
- ADAMG closely aligns with the best performance achieved by manually tuned Adam across 42 optimization tasks
- Superior performance on diverse tasks including image classification (CIFAR10, CIFAR100, Tiny-ImageNet) and language modeling (GLUE benchmark)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The golden step size preserves tuning-free convergence of AdaGrad-Norm by ensuring the step size trajectory does not diverge as accumulated gradient norm grows.
- **Mechanism:** The golden step size is derived as ηgold = 1/2 lim(t→1/4−) xt, which balances between maintaining the ability to converge without tuning and approximating the optimal step size. This specific scaling ensures the accumulated gradient norm vk grows slower than the step size decreases, preventing divergence.
- **Core assumption:** The accumulated gradient norm vk grows sufficiently slowly relative to the step size reduction, and the golden step size formulation properly captures this balance.
- **Evidence anchors:**
  - [abstract] The golden step size "is expected to help AdaGrad-Norm preserve the tuning-free convergence and approximate the optimal step size in expectation w.r.t. various optimization scenarios."
  - [section] "h(η) must be a decreasing sequence to achieve tuning-free convergence" and the derivation of ηgold = 1/2 lim(t→1/4−) xt.
  - [corpus] Weak evidence - no direct mentions of golden step size formulation, but related works discuss parameter-free step size adaptation.
- **Break condition:** If the accumulated gradient norm grows faster than the step size decreases, the tuning-free convergence property may fail.

### Mechanism 2
- **Claim:** ADAMG achieves superior reliability by consistently achieving performance within 5% of the best manually tuned Adam across diverse optimization tasks.
- **Mechanism:** ADAMG combines the golden step size with Adam's momentum mechanism, creating an adaptive step size that automatically adjusts to problem characteristics without manual tuning. The numerator function s(x) = pxq (with p=0.2, q=0.24) embeds the golden step size into Adam's update rule, allowing it to approximate optimal performance across different task categories.
- **Core assumption:** The golden step size formulation is sufficiently general to handle the diverse range of optimization problems tested, and the specific parameters (p, q) provide good empirical performance across tasks.
- **Evidence anchors:**
  - [abstract] ADAMG "achieves superior performance compared to other parameter-free baselines" and "closely aligns with the best performance achieved by manually tuned Adam."
  - [section] "The proposed ADAMG exhibits the highest average reliability ratio" and demonstrates "best average solution quality" in performance comparisons.
  - [corpus] Weak evidence - no direct mentions of ADAMG, but related works discuss parameter-free Adam variants.
- **Break condition:** If the problem characteristics fall outside the range where the golden step size provides good approximation, or if the specific parameters (p, q) are suboptimal for certain task categories.

### Mechanism 3
- **Claim:** The reliability metric provides a more comprehensive evaluation of parameter-free optimizers than traditional convergence speed and solution quality metrics.
- **Mechanism:** Reliability measures the practical ratio of tasks where a parameter-free optimizer achieves less than 5% performance drop compared to the best manually tuned Adam, grouped by optimizer category (Adam(1e-2), Adam(1e-3), etc.). This captures the consistency of performance across diverse problems rather than just average performance.
- **Core assumption:** Grouping tasks by which Adam learning rate performs best provides meaningful categories for evaluating parameter-free optimizer adaptability, and the 5% threshold represents a practically meaningful performance gap.
- **Evidence anchors:**
  - [abstract] Introduces "reliability" as "a novel evaluation criterion" that "comprehensively assess[es] the efficacy of parameter-free optimizers."
  - [section] "reliability naturally provides clearer insights into the effectiveness of the parameter-free optimizer across the predefined task categories" and shows ADAMG achieves the highest reliability ratio.
  - [corpus] Weak evidence - no direct mentions of reliability metric, but related works discuss evaluation criteria for optimizers.
- **Break condition:** If the task categories don't meaningfully capture the diversity of optimization problems, or if the 5% threshold is inappropriate for certain application domains.

## Foundational Learning

- **Concept:** AdaGrad-Norm algorithm and its tuning-free convergence properties
  - Why needed here: The golden step size is derived specifically for AdaGrad-Norm, so understanding its convergence properties is essential for grasping the mechanism.
  - Quick check question: What is the key property of AdaGrad-Norm that allows it to converge without learning rate tuning, and how does this relate to the accumulated gradient norm vk?

- **Concept:** Exponential Moving Average (EMA) in Adam optimizer
  - Why needed here: ADAMG extends the golden step size concept to Adam by incorporating EMA into the gradient estimation, so understanding this mechanism is crucial.
  - Quick check question: How does Adam's EMA mechanism work, and why is it beneficial for adaptive gradient methods?

- **Concept:** Scale-free optimization properties
  - Why needed here: The golden step size is scale-free, meaning it doesn't depend on problem-specific scaling factors, which is important for its parameter-free nature.
  - Quick check question: What does it mean for an optimization method to be scale-free, and why is this property desirable for parameter-free optimizers?

## Architecture Onboarding

- **Component map:** Compute gradient gk → Update accumulated norm vk+1 → Compute s(vk+1) → Update EMA estimates m and v → Compute step size using s(vk+1) → Update parameters xk+1
- **Critical path:** For each iteration: compute gradient gk → update accumulated norm vk+1 → compute s(vk+1) → update EMA estimates m and v → compute step size using s(vk+1) → update parameters xk+1
- **Design tradeoffs:** The golden step size provides parameter-free operation but may not be optimal for all specific problems; using a fixed (p, q) works well empirically but could be task-specific; the EMA mechanism adds stability but computational overhead.
- **Failure signatures:** Poor performance on tasks where the golden step size doesn't approximate well; sensitivity to the (p, q) parameters; potential divergence if accumulated norm grows too rapidly.
- **First 3 experiments:**
  1. Run ADAMG on CIFAR-10 with pre-trained DenseNet121 to verify it achieves performance close to best manual Adam tuning.
  2. Test ADAMG on BERT fine-tuning with GLUE benchmark to check language model performance.
  3. Compare ADAMG reliability across different task categories (Adam(1e-2) vs Adam(1e-3) preferred tasks) to validate the reliability metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we establish theoretical guarantees for the proposed golden step size approach in ADAMG?
- Basis in paper: [inferred] The authors acknowledge that understanding theoretical guarantees is crucial but mention that the proof framework relies on the collective behavior of AdaGrad-Norm step size throughout the entire training trajectory, which is not directly connected to the derived diverging step size.
- Why unresolved: The current analysis shows practical success but lacks a rigorous theoretical foundation that connects the golden step size to convergence guarantees for nonconvex optimization.
- What evidence would resolve it: A formal proof demonstrating that the golden step size preserves convergence properties while approximating optimal step sizes across various optimization scenarios would resolve this question.

### Open Question 2
- Question: What causes ADAMG to fail on certain "tail tasks" and how can this be mitigated?
- Basis in paper: [explicit] The authors state that while ADAMG shows wide adaptability across optimization tasks, it fails on some tail tasks possibly due to its expectation mechanism.
- Why unresolved: The paper identifies the issue but doesn't provide a comprehensive analysis of which types of tasks are problematic or propose solutions to address these failures.
- What evidence would resolve it: Experimental results showing which specific task characteristics lead to ADAMG failures, along with modifications incorporating approximated problem properties like optimality gaps to improve performance on these tasks.

### Open Question 3
- Question: How does the scaling parameter p in the numerator function s(x) = px^q affect ADAMG's performance across different optimization tasks?
- Basis in paper: [explicit] The authors discuss that while ADAMG uses s(x) = 0.2x^0.24 for all tasks, they tested ADAMG with p = 0.5 and observed performance differences, noting that scaling p potentially shifts the covering range.
- Why unresolved: The paper only tests two values of p (0.2 and 0.5) and observes general trends but doesn't provide a systematic analysis of how different scaling parameters affect performance across various task categories.
- What evidence would resolve it: A comprehensive study varying p across multiple tasks and task categories to determine optimal scaling values for different optimization scenarios would resolve this question.

## Limitations
- The theoretical justification for the golden step size formula relies on assumptions that may not hold for all optimization landscapes, particularly those with extremely non-smooth or highly non-convex characteristics.
- Limited ablation studies on the sensitivity of results to the specific numerator function parameters (p, q), with only two values tested.
- The reliability metric depends on the chosen 5% threshold and task categorization scheme, which may not be universally applicable.

## Confidence

- **High confidence:** The empirical demonstration that ADAMG achieves superior reliability compared to baseline parameter-free methods across diverse tasks. The experimental methodology is clearly specified with appropriate evaluation metrics.
- **Medium confidence:** The theoretical justification for the golden step size formula. While the derivation follows logically from AdaGrad-Norm properties, the leap from theoretical optimality to practical performance involves assumptions that warrant further validation.
- **Medium confidence:** The generalizability of the (p=0.2, q=0.24) parameters across all task categories. The paper demonstrates strong performance, but the sensitivity to these hyperparameters is not thoroughly explored.

## Next Checks

1. **Ablation on numerator parameters:** Systematically vary p and q values around the proposed (0.2, 0.24) to quantify sensitivity and identify optimal ranges for different task categories.

2. **Failure mode analysis:** Design synthetic optimization problems that stress-test the golden step size mechanism, particularly scenarios where accumulated gradient norm grows rapidly or exhibits extreme variance.

3. **Theoretical bounds validation:** Derive explicit convergence rate bounds for ADAMG under different smoothness assumptions to complement the empirical reliability metric with theoretical performance guarantees.