---
ver: rpa2
title: Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks
arxiv_id: '2402.02036'
source_url: https://arxiv.org/abs/2402.02036
tags:
- graph
- graphs
- proxy
- explanation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the out-of-distribution (OOD) problem in explaining
  graph neural networks (GNNs), where subgraphs identified as explanations often differ
  significantly from the training data distribution. To solve this, the authors propose
  generating in-distribution proxy graphs that preserve the label information of the
  original subgraphs while conforming to the training data distribution.
---

# Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks

## Quick Facts
- arXiv ID: 2402.02036
- Source URL: https://arxiv.org/abs/2402.02036
- Reference count: 30
- Primary result: ProxyExplainer improves explanation accuracy by 7.5% on synthetic datasets and 12.3% on real-world datasets by generating in-distribution proxy graphs

## Executive Summary
This paper addresses the out-of-distribution (OOD) problem in explaining graph neural networks (GNNs), where subgraphs identified as explanations often differ significantly from the training data distribution. The authors propose generating in-distribution proxy graphs that preserve label information of the original subgraphs while conforming to the training data distribution. Their method uses a parametric approach with graph auto-encoders, specifically a graph auto-encoder for reconstructing the explanatory subgraph and a variational graph auto-encoder for generating a non-explanatory subgraph. Experiments on six datasets show that ProxyExplainer achieves higher explanation accuracy compared to baselines while ensuring the generated proxy graphs are closer in distribution to the original graphs.

## Method Summary
ProxyExplainer combines an explainer network that generates explanation subgraphs from input graphs with a proxy graph generator that creates in-distribution proxy graphs. The proxy generator uses two graph auto-encoders: a graph auto-encoder (GAE) reconstructs the explanation subgraph, while a variational graph auto-encoder (VGAE) perturbs the non-explanatory portion. These components are combined to produce proxy graphs that can be reliably used for prediction. The framework employs bi-level optimization, where the outer optimization finds explanation subgraphs and the inner optimization ensures proxy graphs match the training distribution while preserving label information. Training alternates between updating the proxy generator M times and the explainer once, with hyperparameters α=0.1, λ=0.5, τ=0.1, and M=2.

## Key Results
- ProxyExplainer achieves 7.5% average improvement in explanation accuracy on synthetic datasets (BA-2motifs, BA-3motifs)
- On real-world datasets (Alkane-Carbonyl, Benzene, Fluoride-Carbonyl, MUTAG), ProxyExplainer improves explanation accuracy by 12.3% on average
- Generated proxy graphs show significantly reduced distribution shift (measured by MMD) compared to explanation subgraphs
- The approach effectively alleviates the OOD problem while maintaining or improving explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Out-of-distribution proxy graphs improve prediction reliability on explanation subgraphs
- Mechanism: The method trains a proxy graph generator that learns to produce graphs from the same distribution as training data while preserving label-relevant structure from the explanation subgraph
- Core assumption: The GNN's predictive performance degrades when applied to explanation subgraphs due to distribution shift, and restoring distributional alignment restores reliable predictions
- Evidence anchors: Abstract statement about OOD problem in explanation subgraphs; equation 8 formulation of joint optimization loss function
- Break condition: If the proxy graph generator fails to learn the training distribution, predictions on proxy graphs will still be unreliable

### Mechanism 2
- Claim: Dual auto-encoder architecture enables simultaneous reconstruction and perturbation
- Mechanism: A graph auto-encoder reconstructs the explanation subgraph while a variational graph auto-encoder perturbs the non-explanatory portion, combining both to form in-distribution proxy graphs
- Core assumption: Explanation and non-explanatory subgraphs can be treated separately for reconstruction and perturbation while maintaining overall graph properties
- Evidence anchors: Description of dual-structured mechanism with two distinct graph auto-encoders; use of VGAE for introducing perturbations into non-explanatory segment
- Break condition: If the VGAE generates perturbations that are too large or too small, the proxy graph may not match the training distribution

### Mechanism 3
- Claim: Bi-level optimization ensures both explanation quality and distributional alignment
- Mechanism: Outer optimization finds explanation subgraphs while inner optimization ensures proxy graphs match the training distribution and preserve label information
- Core assumption: The two optimization levels can be decoupled without interfering with each other's objectives
- Evidence anchors: Replacement of first constraint with distributional distance measure (KL divergence); development of bi-level optimization model
- Break condition: If the alternation between training explainer and proxy generator becomes unstable, neither component may converge properly

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their inductive biases
  - Why needed here: Understanding why GNNs fail on OOD explanation subgraphs requires knowledge of how GNNs learn graph representations during training
  - Quick check question: What happens to a GNN's predictions when it encounters a graph with very different structural properties than its training data?

- Concept: Mutual information and information bottleneck principles
  - Why needed here: The Graph Information Bottleneck objective forms the theoretical foundation for identifying explanation subgraphs
  - Quick check question: How does maximizing mutual information between subgraphs and labels while minimizing it between subgraphs and original graphs help identify important structures?

- Concept: Distribution alignment and density estimation
  - Why needed here: The core innovation relies on generating proxy graphs that match the training distribution, which requires understanding how to measure and enforce distributional similarity
  - Quick check question: What metrics can quantify how close two graph distributions are, and how can these be incorporated into training objectives?

## Architecture Onboarding

- Component map: Input graph -> Explainer -> Explanation subgraph -> GAE -> Reconstructed subgraph -> VGAE -> Perturbed subgraph -> Proxy graph combiner -> Proxy graph -> GNN prediction
- Critical path: Input graph → Explainer → Explanation subgraph → GAE → Reconstructed subgraph → VGAE → Perturbed subgraph → Proxy graph → GNN prediction
- Design tradeoffs:
  - Using two separate auto-encoders adds complexity but enables specialized handling of explanation vs. non-explanation parts
  - Bi-level optimization increases training time but allows independent optimization of explanation quality and distributional alignment
  - The choice of KL divergence vs. other distributional metrics affects how well the proxy matches training data
- Failure signatures:
  - Proxy graphs that look visually different from training data indicate distribution alignment failure
  - Explanations that are too large or too small suggest issues with the explainer component
  - Poor performance improvement despite proxy generation suggests the distributional shift may not be the primary bottleneck
- First 3 experiments:
  1. Generate proxy graphs for a simple synthetic dataset and visually compare their properties to training data
  2. Measure prediction accuracy on explanation subgraphs vs. their corresponding proxy graphs
  3. Test ablation variants (w/o GΔ, w/o LKL, w/o Ldist) to verify each component's contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed proxy graph generation approach be extended to explain other types of models beyond GNNs, such as transformers or CNNs?
- Basis in paper: The paper mentions future work to "analyze explainable learning methods with proxies in other data structures, such as image, language, and time series."
- Why unresolved: The current work focuses solely on explaining GNNs. Extending the approach to other model architectures and data types requires developing new methods to generate proxy graphs or their equivalent.
- What evidence would resolve it: Developing and evaluating proxy graph generation methods for explaining transformers, CNNs, or other model types on relevant datasets, demonstrating improved explanation quality compared to existing methods.

### Open Question 2
- Question: How sensitive is the proxy graph generation process to the choice of hyperparameters, such as the dimensionality of node embeddings or the trade-off parameter λ?
- Basis in paper: The paper mentions conducting a parameter sensitivity analysis in the appendix, varying the dimension of node latent embeddings and the trade-off parameter λ.
- Why unresolved: The sensitivity analysis is limited to a specific range of values and a subset of datasets. A more comprehensive study across diverse datasets and a wider range of hyperparameter values is needed to fully understand the robustness of the approach.
- What evidence would resolve it: Conducting extensive experiments with a grid search over a wide range of hyperparameter values on multiple diverse datasets, analyzing the impact on explanation quality and proxy graph distribution.

### Open Question 3
- Question: Can the proxy graph generation approach be used to improve the interpretability of other graph learning tasks beyond graph classification, such as node classification or link prediction?
- Basis in paper: The paper focuses on explaining graph classification models, but the proxy graph generation framework could potentially be adapted to other graph learning tasks.
- Why unresolved: The current work does not explore the application of proxy graphs to other graph learning tasks. Adapting the approach would require modifying the explanation function and proxy graph generation process to suit the specific task.
- What evidence would resolve it: Developing and evaluating proxy graph generation methods for explaining node classification or link prediction models, demonstrating improved interpretability and explanation quality compared to existing methods.

## Limitations

- Limited ablation analysis makes it difficult to assess individual component contributions beyond reported variants
- Distribution alignment metrics (MMD) may not fully capture semantic similarity between proxy and training graphs
- Performance gains on real-world datasets rely on synthetic ground-truth explanations that may not reflect practical scenarios

## Confidence

- Mechanism 1 (OOD problem importance): High - well-established in XAI literature
- Mechanism 2 (dual auto-encoder approach): Medium - architectural details somewhat underspecified
- Performance claims: Medium - controlled synthetic settings but limited real-world validation

## Next Checks

1. **Ablation stress test**: Remove each component (GAE, VGAE, KL regularization) and measure degradation patterns to identify critical bottlenecks
2. **Distribution analysis**: Generate proxy graphs for multiple datasets and compute additional distributional metrics (e.g., graphlet distributions, motif frequencies) beyond MMD
3. **Real-world scenario test**: Apply ProxyExplainer to a dataset without synthetic ground-truth explanations using post-hoc validation methods like model intervention tests