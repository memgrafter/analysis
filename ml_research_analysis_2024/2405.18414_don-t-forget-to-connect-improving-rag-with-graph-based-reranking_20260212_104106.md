---
ver: rpa2
title: Don't Forget to Connect! Improving RAG with Graph-based Reranking
arxiv_id: '2405.18414'
source_url: https://arxiv.org/abs/2405.18414
tags:
- documents
- question
- graph
- information
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a graph-based reranker, G-RAG, to improve Retrieval
  Augmented Generation (RAG) for Open-Domain Question Answering (ODQA). G-RAG leverages
  document graphs and Abstract Meaning Representation (AMR) information to identify
  relevant documents even when connections to the question are less obvious.
---

# Don't Forget to Connect! Improving RAG with Graph-based Reranking

## Quick Facts
- **arXiv ID**: 2405.18414
- **Source URL**: https://arxiv.org/abs/2405.18414
- **Reference count**: 40
- **Primary result**: Graph-based reranker G-RAG outperforms state-of-the-art approaches for RAG in ODQA while using fewer computational resources

## Executive Summary
This paper addresses a key limitation in Retrieval Augmented Generation (RAG) systems: their inability to identify relevant documents when connections to the question are less obvious. The authors propose G-RAG, a graph-based reranker that leverages document graphs constructed from shared Abstract Meaning Representation (AMR) nodes, combined with GNN message passing to propagate semantic relevance. The method demonstrates superior performance on Natural Questions and TriviaQA datasets compared to existing reranking approaches including PaLM 2, while also reducing computational overhead. The work emphasizes the importance of thoughtful reranker design in improving RAG system effectiveness.

## Method Summary
G-RAG constructs a document graph where nodes represent documents and edges exist when their AMR graphs share common nodes. The method extracts shortest paths from the "question" node in AMR graphs to create semantic node features, then applies a 2-layer GCN with pairwise ranking loss to rerank retrieved documents. This approach captures cross-document semantic relationships that traditional DPR-based ranking misses, enabling identification of relevant documents even with partial information overlap. The framework is evaluated across multiple embedding models (BERT, GTE, BGE, Ember) and shows consistent improvements over baselines including BART, MLP, and PaLM 2 rerankers.

## Key Results
- G-RAG outperforms state-of-the-art reranking approaches on both Natural Questions and TriviaQA datasets
- Proposed MTRR and TMHits@10 metrics effectively handle tied ranking scores in ODQA
- PaLM 2 underperforms G-RAG as a reranker, highlighting the importance of specialized reranker design
- The method achieves better performance while using fewer computational resources than baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G-RAG uses document graphs to capture connections between documents that are not obvious from question-document pairs alone
- Mechanism: Documents are represented as nodes; edges exist when AMR graphs of two documents share common nodes. GNN message passing propagates semantic relevance across these edges
- Core assumption: Documents sharing AMR nodes are semantically related and can collectively improve ranking of weakly connected relevant documents
- Evidence anchors:
  - [abstract] "Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG"
  - [section 3.1] "We establish the undirected document graph Gq = {V, E} based on AMRs... if the corresponding AMR Gqpi and Gqpj have common nodes, there will be an undirected edge between vi and vj"
  - [corpus] Weak: Only mentions AMR as semantic feature, not graph construction. Evidence is in the paper
- Break condition: If document AMR graphs rarely share nodes, graph edges will be sparse and message passing ineffective

### Mechanism 2
- Claim: G-RAG selectively encodes AMR information into node features rather than using full AMR tokens, reducing overfitting and computational cost
- Mechanism: Extracts shortest paths from "question" node in AMR graphs, then encodes concatenated document text + path node sequence
- Core assumption: Shortest paths from question node capture the most relevant semantic linkage between question and document content
- Evidence anchors:
  - [section 3.2.1] "We investigate the determining factor that facilitates the reranker to identify more relevant documents... we extract node concepts along these identified paths"
  - [section 3.2.1] "the potential connection from the question to the answer becomes much clearer"
  - [corpus] Weak: No direct evidence AMR path extraction is better than full AMR; assumes it based on ablation
- Break condition: If shortest paths miss key answer-related concepts, important semantic links are lost

### Mechanism 3
- Claim: Using pairwise ranking loss instead of cross-entropy loss improves performance in highly imbalanced ODQA datasets
- Mechanism: Loss function compares document pairs and encourages correct relative ordering; cross-entropy treats all negatives equally
- Core assumption: Pairwise comparison is more effective than pointwise classification when positive documents are rare
- Evidence anchors:
  - [section 3.2.4] "The cross-entropy loss may fail to deal with the unbalanced data in ODQA where the number of negative documents is much greater than the number of positive documents"
  - [section 3.2.4] "Given a pair of scores si and sj, the ranking loss is given by: RLq(si, sj, r) = max (0, -r (si - sj) + 1)"
  - [corpus] Weak: No comparative loss study shown; assumed based on prior work citations
- Break condition: If ranking loss hyperparameters are poorly tuned, convergence may suffer compared to cross-entropy

## Foundational Learning

- **Concept**: Graph Neural Networks
  - Why needed here: To propagate relevance signals across document-document connections encoded in the graph structure
  - Quick check question: What happens if you replace GCN with MLP in G-RAG?

- **Concept**: Abstract Meaning Representation (AMR)
  - Why needed here: Provides structured semantic graph from text that can be used to find cross-document connections
  - Quick check question: How does AMR differ from plain token embeddings for semantic matching?

- **Concept**: Reranking in ODQA
  - Why needed here: DPR retrieval is noisy; reranking with richer signals improves answer recall
  - Quick check question: Why can't we just rely on DPR scores for final ranking?

## Architecture Onboarding

- **Component map**: Document retrieval → AMR graph generation → Document graph construction → GNN reranking → Reader
- **Critical path**: DPR → AMR parsing → GNN → Reranking scores → Reader input
- **Design tradeoffs**: Full AMR tokens vs path-based features (accuracy vs efficiency); pairwise loss vs cross-entropy (convergence vs imbalance handling)
- **Failure signatures**: Sparse graph (no shared AMR nodes), overfitting (high node feature dimension), poor ranking (inadequate path extraction)
- **First 3 experiments**:
  1. Verify DPR retrieves 100 documents per question as expected
  2. Check AMR graph generation and shared node counting
  3. Test GNN forward pass with dummy node/edge features to ensure shapes match

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does G-RAG's performance compare when using alternative graph neural network architectures beyond GCN, GAT, GraphSage, and GIN?
- Basis in paper: [inferred] The paper mentions comparing G-RAG with different GNN models (GCN, GAT, GraphSage, GIN) and notes that GCN performs best for their small document graphs, but suggests that more sophisticated models might be beneficial
- Why unresolved: The paper only tests a limited set of GNN architectures. There may be other GNN models that could perform better or worse for this specific task
- What evidence would resolve it: Systematic testing of G-RAG with a broader range of GNN architectures, including newer or more complex models, would provide evidence for optimal GNN choice in this context

### Open Question 2
- Question: What is the impact of incorporating additional semantic information beyond AMR graphs, such as entity linking or coreference resolution, on G-RAG's performance?
- Basis in paper: [explicit] The paper discusses leveraging AMR graphs for semantic information but notes that their method avoids adding redundant information. It suggests that designing more sophisticated models to better process AMR information could bring further improvements
- Why unresolved: The paper focuses on AMR graphs specifically and does not explore other semantic information sources. It's unclear how incorporating additional semantic information might affect performance
- What evidence would resolve it: Experiments comparing G-RAG's performance with and without additional semantic information sources, such as entity linking or coreference resolution, would provide insights into their impact on reranking accuracy

### Open Question 3
- Question: How does the performance of G-RAG change when applied to different domains or types of questions beyond open-domain question answering?
- Basis in paper: [inferred] The paper focuses on open-domain question answering (ODQA) and does not discuss the model's applicability to other domains or question types. However, the general approach of using document graphs and semantic information could potentially be applied to other tasks
- Why unresolved: The paper's experiments are limited to ODQA datasets (Natural Questions and TriviaQA). It's unclear how well G-RAG would perform on different types of questions or in other domains
- What evidence would resolve it: Testing G-RAG on a diverse set of question-answering tasks, including domain-specific QA and different question types (e.g., factoid, complex reasoning), would demonstrate its generalizability and potential limitations

## Limitations

- **AMR Graph Construction and Quality**: The paper lacks quantitative analysis of graph density or quality, making it unclear whether message passing has sufficient signal to propagate
- **Hyperparameter Sensitivity**: Key hyperparameters like GNN layers, message passing iterations, and node feature dimensions are not explored for sensitivity
- **Loss Function Justification**: The preference for pairwise ranking loss over cross-entropy is theoretical rather than empirically demonstrated

## Confidence

**High Confidence (8/10)**: The core finding that graph-based reranking improves RAG performance is well-supported by experimental results
**Medium Confidence (6/10)**: The mechanism by which AMR paths improve relevance detection is plausible but under-validated
**Low Confidence (4/10)**: Claims about computational efficiency lack quantitative evidence

## Next Checks

1. **Graph Structure Analysis**: Compute and report document graph statistics including average degree, edge density, and connected component size across both NQ and TQA datasets
2. **Ablation of AMR Path Extraction**: Replace the shortest-path node feature extraction with (a) full AMR node sequences and (b) random paths from the question node
3. **Runtime and Resource Profiling**: Measure wall-clock training time, memory usage, and inference latency for G-RAG versus the GCN and MLP baselines across all embedding models