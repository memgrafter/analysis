---
ver: rpa2
title: A call for embodied AI
arxiv_id: '2402.03824'
source_url: https://arxiv.org/abs/2402.03824
tags:
- learning
- arxiv
- https
- embodied
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Embodied AI (E-AI) as the next fundamental step
  in pursuing Artificial General Intelligence (AGI), arguing that current AI advancements,
  particularly Large Language Models (LLMs), fall short of true intelligence due to
  their static nature and lack of real-world interaction. The authors advocate for
  AI agents that actively interact with and learn from their physical or digital environments,
  emphasizing perception, action, memory, and learning as essential components.
---

# A call for embodied AI

## Quick Facts
- arXiv ID: 2402.03824
- Source URL: https://arxiv.org/abs/2402.03824
- Authors: Giuseppe Paolo; Jonas Gonzalez-Billandon; Balázs Kégl
- Reference count: 40
- The paper proposes Embodied AI as the next fundamental step toward AGI, arguing current AI systems like LLMs are static and lack real-world interaction.

## Executive Summary
The paper argues that current AI systems, particularly Large Language Models, fall short of true intelligence due to their static nature and inability to evolve through real-world interaction. The authors advocate for Embodied AI (E-AI) agents that actively interact with and learn from their physical or digital environments. They present a theoretical framework based on cognitive architectures and Friston's active inference principle, positioning E-AI as crucial for achieving AGI that mirrors human cognitive development through active, experiential learning.

## Method Summary
The paper presents a theoretical framework rather than a specific method, proposing E-AI as an approach where agents actively interact with their environment through perception, action, memory, and learning modules. The framework is based on cognitive architectures and active inference principles, emphasizing continuous data collection integrated with learning processes. While no specific training procedure is provided, the authors outline conceptual components and identify key challenges including developing new learning theories, managing noise and uncertainty, and addressing hardware limitations.

## Key Results
- E-AI agents can learn causal relationships more effectively than static models through direct interaction and observation of consequences
- Embodied agents develop better alignment with human values through procedural learning in human environments
- E-AI achieves more efficient learning through continuous, integrated data collection rather than relying on pre-collected datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Embodied AI agents can learn causal relationships more effectively than static models because they interact with and observe the consequences of their actions in real-time.
- **Mechanism**: Through sensorimotor coupling, E-AI agents experience direct cause-and-effect relationships. When an agent performs an action and observes the outcome, it builds a predictive model of the environment that captures causal structures rather than mere correlations.
- **Core assumption**: The environment provides reliable feedback signals that correlate with the agent's actions, and the agent has sufficient sensory capabilities to detect these outcomes.
- **Evidence anchors**:
  - [abstract]: "LLMs... are static and unable to evolve with time and experience" versus E-AI agents that "actively search for valuable new information"
  - [section]: "In contrast, a fully embodied agent should have the ability to grasp the causality underlying events and actions within its environment"
  - [corpus]: Weak evidence - no corpus papers directly address causal learning mechanisms
- **Break condition**: If environmental feedback is delayed, noisy, or absent, the agent cannot establish reliable causal relationships. Also fails if the agent's actions don't produce observable consequences.

### Mechanism 2
- **Claim**: Embodied agents develop better alignment with human values through procedural and perspectival learning rather than propositional instruction.
- **Mechanism**: By living in and interacting with human environments, E-AI agents naturally encounter human values as constraints and affordances in their decision-making processes. They learn what actions are acceptable through direct experience rather than being told rules.
- **Core assumption**: Human values are embodied in the structure of human environments and social interactions, making them learnable through experience.
- **Evidence anchors**:
  - [abstract]: "These Embodied AI (E-AI) agents ought to prioritize their continued existence and our bindings to them, thereby learning the value of truth"
  - [section]: "We propose that their widespread acceptance and their more integrated, less intrusive presence in our lives are due to the closer alignment with the principles of embodiment"
  - [corpus]: Weak evidence - corpus papers mention alignment but don't provide empirical support for procedural learning advantages
- **Break condition**: If the agent's environment doesn't reflect human values accurately, or if the agent cannot distinguish between different value systems, alignment may not occur naturally.

### Mechanism 3
- **Claim**: Embodied agents achieve more efficient learning through continuous data collection integrated with the learning process, rather than relying on pre-collected datasets.
- **Mechanism**: E-AI agents generate their own training data through interaction, allowing them to focus on information that's immediately relevant to their goals and environment. This creates a natural curriculum and reduces the need for massive pre-curated datasets.
- **Core assumption**: The agent's exploration strategy efficiently samples the most informative experiences for learning, and the agent can process this continuous stream of data in real-time.
- **Evidence anchors**:
  - [abstract]: "Current AI learns in a very different way from humans... We advocate for an approach where insights from cognitive science and developmental psychology inform the design of AI systems"
  - [section]: "In I-AI, multimodal data needs to be collected and connected painstakingly. In contrast, E-AI agents... will inherently collect and correlate multi-modal data by mere co-occurrence"
  - [corpus]: Weak evidence - corpus papers discuss data management but don't specifically address integrated learning advantages
- **Break condition**: If the agent's exploration is inefficient or gets stuck in local optima, learning may be slower than traditional approaches. Also fails if the agent cannot process data streams quickly enough.

## Foundational Learning

- **Concept: Active inference and free energy minimization**
  - Why needed here: The paper positions E-AI within Friston's active inference framework, where agents minimize prediction error and uncertainty through action and perception. Understanding this provides the theoretical foundation for how embodied agents learn.
  - Quick check question: How does an agent's action help reduce free energy according to the active inference principle?

- **Concept: Sensorimotor coupling**
  - Why needed here: The paper emphasizes that embodiment requires tight integration between perception and action. This concept explains how agents build models of their environment through interaction rather than passive observation.
  - Quick check question: What distinguishes sensorimotor coupling from simple perception-action sequences in classical AI?

- **Concept: Continual learning and catastrophic forgetting**
  - Why needed here: The paper identifies continual learning as essential for E-AI agents operating in dynamic environments, while acknowledging the challenge of catastrophic forgetting in neural networks.
  - Quick check question: Why does standard supervised learning theory fail to address the data collection process in embodied agents?

## Architecture Onboarding

- **Component map**: Perception → World Model → Planning → Action → Observation → Memory → Learning → World Model
- **Critical path**: Perception → World Model Update → Planning → Action → Observation → Memory → Learning → World Model Update
- **Design tradeoffs**:
  - Real-time vs. accuracy: Faster perception/action may sacrifice accuracy but enables timely responses
  - Model complexity vs. computational efficiency: More sophisticated world models require more resources
  - Exploration vs. exploitation: Balancing learning about environment vs. achieving current goals
  - Memory capacity vs. relevance: Storing more history provides context but may include irrelevant information
- **Failure signatures**:
  - Perception failures: Agent acts on incorrect or incomplete environmental understanding
  - Planning failures: Agent generates actions that don't achieve intended outcomes
  - Memory failures: Agent repeats mistakes or cannot adapt to new situations
  - Learning failures: Agent stops improving or regresses in performance
- **First 3 experiments**:
  1. Implement a simple embodied agent in a simulated environment with clear cause-and-effect relationships. Test whether it learns these relationships faster than a passive observer model.
  2. Create two versions of an agent - one with integrated data collection and one with pre-collected dataset. Compare learning efficiency and final performance on a navigation task.
  3. Build an agent that must balance exploration and exploitation in a changing environment. Measure how well it maintains performance while learning new patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics or metrics should define an "Embodied AI" agent to distinguish it from current AI systems, and how can these be practically measured?
- Basis in paper: [explicit] The paper discusses the distinction between current AI systems like LLMs and E-AI, highlighting the lack of real-world interaction and continuous learning in existing systems.
- Why unresolved: Defining E-AI requires establishing clear criteria that capture the essence of embodiment, including perception, action, memory, and learning in a dynamic environment, which are not yet standardized or universally agreed upon.
- What evidence would resolve it: Development of a formal framework or benchmark that outlines specific capabilities and performance metrics for E-AI agents, validated through empirical testing across diverse environments.

### Open Question 2
- Question: How can a new learning theory for Embodied AI be developed that accounts for the dynamic, interactive nature of data collection and adaptation in non-stationary environments?
- Basis in paper: [explicit] The paper identifies the need for a new learning theory that transcends traditional supervised and reinforcement learning, addressing the interactive and evolving nature of E-AI.
- Why unresolved: Current learning theories are insufficient for analyzing cases where the agent's actions continuously reshape its learning environment, and there is a lack of tools to assess the quality and relevance of dynamically generated data.
- What evidence would resolve it: Formulation of a novel learning theory that provides diagnostics for data quality and relevance, supported by case studies demonstrating improved performance in non-stationary environments.

### Open Question 3
- Question: What are the most effective strategies for managing noise and uncertainty in Embodied AI systems, particularly in scenarios involving partial observability and continuous streams of fluctuating data?
- Basis in paper: [explicit] The paper highlights the challenges of navigating noise and uncertainty in real-world environments, affecting perception and decision-making in E-AI agents.
- Why unresolved: Noise and uncertainty sources are diverse and complex, including sensor inaccuracies and incomplete information, making it difficult to develop universally effective strategies for management.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of specific noise and uncertainty management techniques in real-world E-AI applications, showing improved robustness and reliability.

## Limitations

- The paper presents a compelling theoretical framework but lacks empirical validation
- The causal learning mechanisms are hypothesized without experimental evidence showing embodied agents outperform passive models
- The computational feasibility of real-time active inference remains questionable given current hardware limitations

## Confidence

- **High**: The identification of embodiment as a missing component in current AI approaches (LLMs are indeed static and lack environmental interaction)
- **Medium**: The theoretical framework combining cognitive architectures with active inference (conceptually sound but not yet validated)
- **Low**: Specific claims about efficiency gains and learning speed compared to traditional approaches (no empirical comparisons provided)

## Next Checks

1. Implement a controlled experiment comparing causal learning in embodied vs. passive observation agents across multiple task types to measure actual performance differences.
2. Test the active inference implementation on a computationally constrained platform to assess real-time feasibility and identify bottlenecks.
3. Design an alignment experiment where embodied agents learn from diverse environments with known value structures to evaluate whether procedural learning produces more robust alignment than rule-based approaches.