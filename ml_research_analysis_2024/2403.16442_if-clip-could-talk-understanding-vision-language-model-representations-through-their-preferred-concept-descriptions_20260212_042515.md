---
ver: rpa2
title: 'If CLIP Could Talk: Understanding Vision-Language Model Representations Through
  Their Preferred Concept Descriptions'
arxiv_id: '2403.16442'
source_url: https://arxiv.org/abs/2403.16442
tags:
- descriptions
- vlms
- photo
- clip
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Extract and Explore (EX2), a novel analysis
  method to understand what textual features contribute to Vision-Language Model (VLM)
  representations. EX2 uses reinforcement learning to align a large language model
  with VLM preferences, generating descriptions that incorporate important features
  for the VLM.
---

# If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions

## Quick Facts
- arXiv ID: 2403.16442
- Source URL: https://arxiv.org/abs/2403.16442
- Authors: Reza Esfandiarpoor; Cristina Menghini; Stephen H. Bach
- Reference count: 40
- One-line primary result: Introduces EX2 method to reveal that VLMs rely significantly on spurious descriptions and non-visual attributes for concept representation

## Executive Summary
This paper introduces EX2 (Extract and Explore), a novel analysis method that uses reinforcement learning to align a large language model with VLM preferences, generating descriptions that reveal what textual features contribute to VLM representations. The authors apply EX2 to analyze seven VLMs across six fine-grained classification datasets, discovering that VLMs rely heavily on spurious descriptions (e.g., "Click to enlarge photo of CONCEPT") despite their lack of visual information. More importantly, they find that VLMs prioritize non-visual attributes like habitat and location even among informative descriptions, and that different VLMs and datasets lead to different attribute priorities.

## Method Summary
EX2 works by fine-tuning an LLM to generate concept descriptions that maximize cosine similarity with corresponding image embeddings from a VLM, using reinforcement learning with Proximal Policy Optimization. The method generates 25 descriptions per concept, which are then analyzed for classification accuracy improvement over generic templates and manually inspected (or via ChatGPT) to categorize them as spurious vs. informative and visual vs. non-visual. The analysis reveals which textual features VLMs actually rely on versus what might be expected from visual inspection.

## Key Results
- Spurious descriptions like "Click to enlarge photo of CONCEPT" significantly contribute to VLM representations despite providing no visual information
- Among informative descriptions, VLMs rely significantly on non-visual attributes like habitat and location rather than visual features
- Different VLMs prioritize different attributes in their representations, and the same VLM may prioritize different attributes across datasets
- EX2-generated descriptions improve classification accuracy by 1-2% compared to generic template descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning aligns an LLM with VLM preferences by optimizing descriptions for similarity to VLM embeddings.
- Mechanism: The reward function computes cosine similarity between VLM text embeddings and corresponding image embeddings, encouraging the LLM to generate descriptions that match what the VLM considers "accurate."
- Core assumption: VLM embeddings encode meaningful preferences about which textual features best represent concepts.
- Evidence anchors:
  - [abstract]: "EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate features that are important for the VLM."
  - [section 3.1]: "Since contrastive VLMs like CLIP are trained to push related text and images closer, VLMs deem a concept description accurate if it is close to the embedding of the corresponding images."

### Mechanism 2
- Claim: Alignment reveals spurious and non-visual features that significantly contribute to VLM representations.
- Mechanism: By generating descriptions optimized for VLM similarity, then analyzing these descriptions, we discover which textual features the VLM actually relies on versus what we might expect.
- Core assumption: Features that improve VLM accuracy when added to descriptions are genuinely important to the VLM's representation.
- Evidence anchors:
  - [abstract]: "Using EX2, we find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT."
  - [section 4.3]: "Table 2 reports the percentage of descriptions that provide additional information about concepts for the 33 cases in which the LLM successfully learns the VLM preferences."

### Mechanism 3
- Claim: Different VLMs and datasets lead to different attribute priorities in concept representations.
- Mechanism: By applying EX2 across multiple VLMs and datasets, we observe variations in which attributes appear most frequently in aligned descriptions, revealing VLM-specific representation strategies.
- Core assumption: Variations in training data and architecture lead to different learned representations that can be uncovered through description analysis.
- Evidence anchors:
  - [abstract]: "Also, our analysis reveals that different VLMs prioritize different attributes in their representations."
  - [section 4.5]: "Figure 4 shows the most common attributes in each case. Note that each description might include multiple attributes."

## Foundational Learning

- Concept: Reinforcement learning with human feedback (RLHF) techniques
  - Why needed here: EX2 adapts RLHF methods to align LLMs with VLM preferences instead of human preferences.
  - Quick check question: What distinguishes aligning with VLM preferences versus human preferences in terms of reward function design?

- Concept: Contrastive learning and embedding similarity
  - Why needed here: Understanding how VLMs measure relatedness between text and images is crucial for designing the reward function.
  - Quick check question: How does cosine similarity between text and image embeddings relate to VLM training objectives?

- Concept: Fine-grained classification datasets
  - Why needed here: These datasets provide the concept-image pairs needed to evaluate and train the aligned LLM.
  - Quick check question: Why are fine-grained datasets particularly useful for studying VLM representation differences?

## Architecture Onboarding

- Component map: LLM -> RL fine-tuning -> Aligned descriptions -> VLM reward calculation -> Description analysis pipeline
- Critical path: LLM generation -> Reward computation -> RL update -> Description generation -> Analysis
- Design tradeoffs: Broader LLM search space vs. more targeted feature engineering; computational cost of RL vs. manual analysis
- Failure signatures: LLM converges to trivial outputs (like concept names only); reward function doesn't correlate with classification accuracy; analysis pipeline misses important features
- First 3 experiments:
  1. Run EX2 on a simple dataset (like Flowers) with CLIP to verify basic functionality
  2. Compare aligned vs. generic descriptions on classification accuracy to confirm improvements
  3. Analyze description breakdown to verify spurious vs. informative categorization works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training data source (e.g., OCR vs. natural image-text pairs) impact VLMs' reliance on spurious descriptions?
- Basis in paper: [inferred] The paper mentions that SigLIP's pre-training dataset WebLi uses OCR results as text descriptions, and SigLIP benefits more from spurious descriptions compared to other VLMs. This suggests a potential correlation between training data source and spurious description reliance.
- Why unresolved: While the paper observes a correlation, it does not conduct systematic experiments to establish causation. It does not train variations of the same VLM with different training data sources to control for other training details.
- What evidence would resolve it: Systematic experiments training multiple variations of the same VLM with different training data sources (e.g., OCR vs. natural image-text pairs) while controlling for other training details would establish causation between training data source and spurious description reliance.

### Open Question 2
- Question: How do VLMs' representations change when fine-tuned on downstream tasks with detailed concept descriptions?
- Basis in paper: [explicit] The paper shows that EX2-generated descriptions improve classification accuracy compared to generic templates, suggesting that adapting to VLM preferences benefits downstream tasks. However, it does not investigate how VLMs' representations change during this adaptation.
- Why unresolved: The paper focuses on analyzing pre-trained VLMs and does not examine the impact of fine-tuning on VLMs' representations. It does not investigate how VLMs' representations evolve when fine-tuned on downstream tasks with detailed concept descriptions.
- What evidence would resolve it: Analyzing VLMs' representations before and after fine-tuning on downstream tasks with detailed concept descriptions would reveal how VLMs' representations change during adaptation. This could involve comparing feature importance, concept embeddings, or other representation characteristics.

### Open Question 3
- Question: How do VLMs' representations differ across different image classification datasets and domains?
- Basis in paper: [explicit] The paper analyzes VLMs on six fine-grained classification datasets and finds that even the same VLM prioritizes different attributes for different datasets. This suggests that VLMs' representations are dataset-dependent.
- Why unresolved: While the paper analyzes VLMs on multiple datasets, it does not comprehensively investigate how VLMs' representations differ across various domains (e.g., natural images, medical images, satellite imagery). It also does not explore the factors contributing to these differences.
- What evidence would resolve it: Analyzing VLMs' representations across a wider range of image classification datasets and domains would reveal the extent of dataset-dependent representations. Investigating factors such as dataset size, class diversity, and image characteristics could explain the observed differences.

## Limitations

- The analysis relies heavily on manual and ChatGPT-based inspection to categorize descriptions, introducing potential subjectivity and limiting reproducibility of qualitative findings
- The study focuses on fine-grained classification datasets, which may not generalize to other domains or broader concept categories
- The method requires significant computational resources for RL fine-tuning, making it less accessible for rapid exploration of VLM behaviors

## Confidence

- **High Confidence:** The core methodology of using RL to align an LLM with VLM preferences is technically sound and well-executed
- **Medium Confidence:** The finding that VLMs rely on spurious descriptions is supported by empirical evidence, though the mechanism explaining why these descriptions appear could be explored further
- **Medium Confidence:** The observation that VLMs use non-visual attributes is empirically demonstrated, but the implications for real-world VLM performance require further investigation

## Next Checks

1. **Cross-validation of description categorization:** Have multiple independent annotators categorize a subset of generated descriptions using the paper's criteria, then calculate inter-rater reliability to assess consistency of the qualitative analysis.

2. **Controlled ablation study on spurious features:** Create a modified version of EX2 that explicitly penalizes spurious descriptions during RL training, then compare classification performance and description content to determine if the VLM's reliance on these features is beneficial or detrimental.

3. **Generalization test to out-of-distribution concepts:** Apply the EX2 analysis to concepts from datasets with different characteristics (e.g., man-made objects, abstract concepts) to determine whether the observed patterns of VLM representation preferences generalize beyond the studied fine-grained classification domains.