---
ver: rpa2
title: 'DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World
  Code Repositories'
arxiv_id: '2405.19856'
source_url: https://arxiv.org/abs/2405.19856
tags:
- code
- llms
- deveval
- repositories
- dependencies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DevEval, a manually-annotated code generation
  benchmark designed to align with real-world code repositories. DevEval addresses
  the gap in existing benchmarks, which are poorly aligned with real-world repositories.
---

# DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories

## Quick Facts
- **arXiv ID**: 2405.19856
- **Source URL**: https://arxiv.org/abs/2405.19856
- **Reference count**: 7
- **Primary result**: Introduced DevEval, a benchmark that improves LLM evaluation by aligning with real-world code repositories, revealing that even the best model (gpt-4-turbo) achieves only 53.04% Pass@1 on real-world tasks.

## Executive Summary
DevEval addresses a critical gap in LLM evaluation by providing a manually-annotated code generation benchmark that reflects real-world software development conditions. The benchmark contains 1,874 testing samples from 117 real-world repositories across 10 popular domains, with comprehensive annotations including requirements, reference code, and dependencies. By evaluating 8 popular LLMs on DevEval, the authors demonstrate that current models struggle significantly with real-world code generation tasks, with the highest Pass@1 score of only 53.04% achieved by gpt-4-turbo. The paper provides detailed analysis of LLM failures and identifies key areas for improvement in future model development.

## Method Summary
DevEval is constructed by collecting functions from 500 real-world repositories and categorizing them into standalone and non-standalone code based on their dependency patterns. The benchmark includes comprehensive annotations such as natural language requirements, original repository contexts, reference code, and reference dependencies. For evaluation, the authors propose repository-level code generation where models generate code based on requirements and repository contexts. Performance is measured using two metrics: Pass@k for functional correctness (proportion of programs passing test cases) and Recall@k for dependency recall (proportion of reference dependencies correctly invoked). The evaluation pipeline involves generating code for each sample, executing test cases, and using static analysis to extract dependencies from generated code.

## Key Results
- gpt-4-turbo achieved the highest Pass@1 score of 53.04% on DevEval, demonstrating the significant challenge of real-world code generation
- LLMs struggle particularly with dependency management, with average Recall@1 scores below 30% across models
- The benchmark reveals that existing evaluation methods overestimate LLM coding capabilities compared to real-world performance
- Analysis of failed cases shows LLMs commonly fail to understand complex repository contexts and manage dependencies correctly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DevEval improves LLM evaluation by providing a benchmark that reflects real-world repository conditions, thus enabling better measurement of actual coding capabilities.
- Mechanism: DevEval collects functions from 500 real-world repositories, ensuring realistic distributions of standalone and non-standalone code, dependency types, and repository scales, which mirrors the environments where LLMs are expected to perform.
- Core assumption: The performance of LLMs on benchmarks that reflect real-world conditions will correlate with their performance in actual software development.
- Evidence anchors:
  - [abstract] "We find that existing benchmarks are poorly aligned with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs."
  - [section] "Compared to existing benchmarks, DevEval shows three unique advances, which we discuss below. â¶ Alignment with real-world code repositories."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. Top related titles include benchmarks aligned with real-world repositories.

### Mechanism 2
- Claim: Detailed and comprehensive annotations in DevEval help LLMs understand the purpose and context of the code they are generating, leading to improved performance.
- Mechanism: DevEval provides natural language requirements, original repositories, reference code, and reference dependencies, which give LLMs rich contextual information beyond just the function signature.
- Core assumption: LLMs can effectively utilize detailed contextual information to generate code that is more aligned with real-world requirements.
- Evidence anchors:
  - [abstract] "DevEval is annotated by 13 developers and contains comprehensive annotations (e.g., requirements, original repositories, reference code, and reference dependencies)."
  - [section] "Each requirement encapsulates the code's functionality and input-output parameters."
  - [corpus] Average neighbor FMR=0.46 suggests moderate relatedness of neighboring papers, indicating that DevEval's approach is somewhat novel.

### Mechanism 3
- Claim: DevEval's repository-level code generation task and associated evaluation metrics provide a more challenging and realistic assessment of LLM capabilities.
- Mechanism: By requiring LLMs to generate code within the context of an entire repository and evaluating both functional correctness and dependency recall, DevEval tests the ability to integrate code into existing projects.
- Core assumption: Evaluating LLMs in a context that mimics real development scenarios will expose weaknesses not apparent in simpler benchmarks.
- Evidence anchors:
  - [abstract] "Based on DevEval, we propose repository-level code generation, which simulates the developers' coding process in a working repository."
  - [section] "Pass@k (Functional Correctness). Following previous studies... we assess the functional correctness of programs by executing test cases..."
  - [corpus] Found 25 related papers, indicating ongoing research into repository-level benchmarks.

## Foundational Learning

- Concept: Real-world code repository distributions
  - Why needed here: Understanding the distribution of code types and dependencies in actual repositories is crucial for creating a benchmark that accurately reflects the challenges LLMs will face.
  - Quick check question: What are the two main types of code identified in real-world repositories, and how do they differ in terms of dependencies?

- Concept: Dependency types in code
  - Why needed here: Recognizing the different types of dependencies (intra-class, intra-file, cross-file) is essential for evaluating an LLM's ability to correctly integrate generated code into existing projects.
  - Quick check question: What are the three types of dependencies identified in the paper, and why is it important to distinguish between them?

- Concept: Evaluation metrics for code generation
  - Why needed here: Understanding how to measure both functional correctness and dependency recall is necessary for comprehensively assessing LLM performance in code generation tasks.
  - Quick check question: What are the two main metrics used in DevEval to evaluate LLM-generated code, and what aspect of performance does each measure?

## Architecture Onboarding

- Component map: Dataset collection module -> Annotation module -> Task definition module -> Evaluation module
- Critical path: The most critical path is from dataset collection through annotation to task definition, as the quality of the benchmark hinges on accurately representing real-world conditions and providing rich context for the LLMs.
- Design tradeoffs: Using functions from real repositories ensures realism but may introduce complexity and variability. Providing detailed annotations helps LLMs but increases the annotation burden. Evaluating both functional correctness and dependencies gives a fuller picture but requires more sophisticated testing.
- Failure signatures: Poor LLM performance on DevEval could indicate issues with the benchmark's alignment with real-world conditions, the quality of annotations, or the appropriateness of the task definition and evaluation metrics.
- First 3 experiments:
  1. Evaluate a simple LLM on DevEval without context to establish a baseline.
  2. Test the same LLM with local file contexts to measure the impact of additional information.
  3. Vary the k value in Pass@k and Recall@k metrics to understand the effect of multiple attempts on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on code generation tasks when the context window is extended to include more diverse files beyond just the local file?
- Basis in paper: [explicit] The paper mentions that "the contexts are heterogeneous" and discusses how "LLMs are typically trained to predict the next tokens based on the continuous contexts." It also suggests exploring "how to utilize broader contexts (e.g., imported files, sibling files)."
- Why unresolved: The paper only experiments with local file contexts and does not explore the impact of including more diverse contexts from the repository.
- What evidence would resolve it: Experiments comparing LLM performance on code generation tasks using different sets of contexts (local file only, local file plus imported files, local file plus sibling files, etc.) would provide evidence on the effectiveness of broader context inclusion.

### Open Question 2
- Question: How does the performance of LLMs on code generation tasks vary across different programming languages and natural language requirements?
- Basis in paper: [inferred] The paper discusses the potential for building a multilingual DevEval benchmark and mentions that LLMs require understanding requirements in different natural languages and generating programs in various programming languages.
- Why unresolved: The current DevEval benchmark is monolingual (English requirements and Python code), and the paper does not explore LLM performance on other language combinations.
- What evidence would resolve it: Experiments evaluating LLM performance on code generation tasks using multilingual requirements and code generation in different programming languages would provide evidence on their cross-language capabilities.

### Open Question 3
- Question: How can retrieval-augmented and tool-augmented generation techniques improve the performance of LLMs on repository-level code generation tasks?
- Basis in paper: [explicit] The paper mentions that "we will explore how to improve the performance of LLMs in context-based code generation, e.g., retrieval-augmented and tool-augmented generation."
- Why unresolved: The paper does not explore the potential benefits of retrieval-augmented and tool-augmented generation techniques on LLM performance.
- What evidence would resolve it: Experiments comparing LLM performance on code generation tasks using standard generation techniques versus retrieval-augmented and tool-augmented generation techniques would provide evidence on their effectiveness in improving performance.

## Limitations
- The benchmark's 1,874 samples from 117 repositories may not fully capture the diversity of real-world software development
- Manual annotation by 13 developers could introduce subjective biases in requirements and dependency identification
- Evaluation focuses on functional correctness and dependency recall but may not account for code quality metrics like maintainability, efficiency, or security

## Confidence
- **High Confidence**: The mechanism by which DevEval aligns with real-world repositories is well-supported by the evidence that existing benchmarks are poorly aligned with actual development conditions.
- **Medium Confidence**: The effectiveness of comprehensive annotations in improving LLM performance is supported but not conclusively proven.
- **Medium Confidence**: The repository-level code generation task provides a realistic assessment, but the evaluation metrics may not capture all aspects of practical coding ability.

## Next Checks
1. Reproduce baseline results: Independently evaluate a simple LLM on DevEval without context to establish a baseline performance, verifying the reported Pass@1 scores and understanding the inherent difficulty of the benchmark.
2. Analyze annotation consistency: Conduct an inter-annotator agreement study among the 13 developers to assess the consistency and reliability of the requirements and dependency annotations, ensuring they do not introduce bias.
3. Expand evaluation metrics: Incorporate additional evaluation metrics that assess code quality beyond functional correctness and dependency recall, such as code maintainability, efficiency, and security, to determine if DevEval's task definition captures the full scope of practical coding challenges.