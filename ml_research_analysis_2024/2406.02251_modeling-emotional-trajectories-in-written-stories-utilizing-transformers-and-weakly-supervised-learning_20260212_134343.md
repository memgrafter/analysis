---
ver: rpa2
title: Modeling Emotional Trajectories in Written Stories Utilizing Transformers and
  Weakly-Supervised Learning
arxiv_id: '2406.02251'
source_url: https://arxiv.org/abs/2406.02251
tags:
- valence
- arousal
- stories
- story
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of modeling emotional trajectories
  in written stories, which is crucial for understanding how narratives evoke emotions
  and influence audiences. Existing approaches have been limited to unsupervised,
  dictionary-based methods, lacking a benchmark for supervised learning on complete
  stories.
---

# Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning

## Quick Facts
- arXiv ID: 2406.02251
- Source URL: https://arxiv.org/abs/2406.02251
- Authors: Lukas Christ; Shahin Amiriparian; Manuel Milling; Ilhan Aslan; BjÃ¶rn W. Schuller
- Reference count: 40
- One-line primary result: DeBERTa model with weakly supervised learning achieves CCC of .8221 for valence and .7125 for arousal on emotional trajectory prediction

## Executive Summary
This study addresses the challenge of modeling emotional trajectories in written stories by extending the ALM dataset with continuous valence and arousal annotations and employing a weakly supervised learning approach. The researchers fine-tune a DeBERTa model on labeled children's stories and improve its generalization by incorporating a large corpus of unlabeled stories from Project Gutenberg through pseudo-label generation. Their best model achieves strong performance with CCC scores of .8221 for valence and .7125 for arousal, while detailed analysis reveals that context, author style, and story structure significantly impact prediction accuracy. The work provides the first benchmark for supervised emotion prediction in complete stories and highlights important challenges including the model's difficulty with protagonist roles, nested narratives, and overall tone understanding.

## Method Summary
The researchers extended the ALM dataset of children's stories by mapping discrete emotion labels to continuous valence and arousal values using the NRC-VAD dictionary. They fine-tuned a DeBERTa model on this labeled data with context windows of up to 8 sentences, then employed weakly supervised learning by generating pseudo-labels on 801 unlabeled stories from Project Gutenberg. The model was first fine-tuned on these pseudo-labeled stories (creating GBALM), then further fine-tuned on the original ALM dataset. The weakly supervised approach aimed to improve generalization by exposing the model to more diverse writing styles and topics beyond the original dataset's three authors.

## Key Results
- DeBERTa model with context window C=8 achieves CCC of .8221 for valence and .7125 for arousal on test set
- Weakly supervised learning approach using Project Gutenberg stories improves upon baseline model trained only on ALM
- Model performance varies significantly by author, individual story, and story section, highlighting the importance of context
- The model struggles with understanding protagonist roles, nested stories, and overall narrative tone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weakly supervised learning step improves model generalization by exposing it to more diverse story styles
- Mechanism: Pretrained DeBERTa model fine-tuned on labeled data predicts valence/arousal on a large corpus of unlabeled stories (GB), creating pseudo-labels. These pseudo-labels are then used to train another DeBERTa model, which is further fine-tuned on the original labeled data (ALM). This approach increases the model's exposure to diverse writing styles and topics, reducing overfitting to the original dataset's limited authors.
- Core assumption: The pseudo-labels generated by the first fine-tuned model are sufficiently accurate to serve as a useful training signal for the second model.
- Evidence anchors:
  - [abstract]: "We then employ DeBERTa model and improve upon this baseline via a weakly supervised learning approach, incorporating a large corpus of unlabeled stories from Project Gutenberg."
  - [section]: "The Alm dataset comprises 169 stories by only three different authors, making our models prone to overfitting. Thus, we seek to augment our data set with in-domain texts written by other authors, thereby covering more topics and also spanning more cultures."
  - [corpus]: Weak. The corpus evidence only states the use of Project Gutenberg stories but does not provide quantitative evidence on the quality or diversity of the pseudo-labels.
- Break condition: If the pseudo-labels are of low quality, the weakly supervised learning step may introduce noise and harm model performance.

### Mechanism 2
- Claim: Considering context is crucial for accurate emotion prediction in stories
- Mechanism: The model processes sentences within a context window (C sentences before and after) instead of in isolation. This allows the model to capture the surrounding context and better understand the mood of each sentence.
- Core assumption: The mood of a sentence is influenced by its surrounding context, and this context is available within the specified window size.
- Evidence anchors:
  - [abstract]: "A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story."
  - [section]: "Since the context of a sentence in a story is relevant to the mood it conveys, we seek to leverage multiple sentences at once in the fine-tuning process."
  - [corpus]: Weak. The corpus evidence mentions the use of context windows but does not provide quantitative evidence on the impact of context on emotion prediction accuracy.
- Break condition: If the context window size is too small or too large, the model may not capture the relevant context or may introduce irrelevant information, respectively.

### Mechanism 3
- Claim: The model's performance varies depending on the author's individual style and the story's overall tone
- Mechanism: The model is trained on a dataset with stories from three different authors. The model learns to associate specific writing styles and story structures with certain emotional patterns. Additionally, the model's understanding of the story's overall tone influences its predictions for individual sentences.
- Core assumption: Different authors have distinct writing styles that influence the emotional tone of their stories, and the model can learn these associations.
- Evidence anchors:
  - [abstract]: "A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story."
  - [section]: "Since ALM comprises stories of three different authors, we investigate the relevance of an author's individual style for learning to predict their stories."
  - [corpus]: Weak. The corpus evidence mentions the use of stories from three different authors but does not provide quantitative evidence on the impact of author style on model performance.
- Break condition: If the dataset lacks sufficient diversity in author styles or if the model cannot effectively learn these associations, its performance may be limited.

## Foundational Learning

- Concept: Understanding of emotion models (e.g., valence/arousal space, discrete emotion categories)
  - Why needed here: The model predicts valence and arousal values, which require knowledge of the underlying emotion model
  - Quick check question: Can you explain the difference between valence and arousal in the context of emotion representation?

- Concept: Familiarity with transformer-based language models (e.g., DeBERTa)
  - Why needed here: The model utilizes DeBERTa for fine-tuning and prediction, requiring understanding of its architecture and training process
  - Quick check question: What are the key differences between DeBERTa and other transformer-based models like BERT or RoBERTa?

- Concept: Knowledge of weakly supervised learning techniques
  - Why needed here: The model employs a weakly supervised learning approach to leverage unlabeled data, requiring understanding of the underlying principles and potential benefits/limitations
  - Quick check question: Can you explain the concept of pseudo-labels and how they are used in weakly supervised learning?

## Architecture Onboarding

- Component map: ALM dataset -> NRC-VAD mapping -> DeBERTa fine-tuning -> Project Gutenberg pseudo-labels -> GBALM training -> ALM fine-tuning -> Valence/Arousal predictions

- Critical path:
  1. Fine-tune DeBERTa on labeled Alm data (steps 1 and 4 in Figure 4)
  2. Generate pseudo-labels on unlabeled Gutenberg data using the fine-tuned model (step 2 in Figure 4)
  3. Fine-tune DeBERTa on pseudo-labeled Gutenberg data (step 3 in Figure 4)
  4. Further fine-tune the model on labeled Alm data (step 4 in Figure 4)

- Design tradeoffs:
  - Context window size (C): Larger windows capture more context but increase computational complexity and may introduce irrelevant information
  - Pseudo-label quality: Higher-quality pseudo-labels lead to better model performance but may require more sophisticated generation methods
  - Model size: Larger models may achieve better performance but require more computational resources

- Failure signatures:
  - Overfitting: High performance on training data but low performance on unseen data
  - Poor generalization: Model struggles to predict emotions for stories from different authors or styles
  - Context misinterpretation: Model fails to capture the relevant context or misinterprets the mood based on surrounding sentences

- First 3 experiments:
  1. Evaluate the impact of context window size (C) on model performance
  2. Compare the performance of the weakly supervised approach with a baseline model trained only on labeled data
  3. Analyze the model's performance across different authors and stories to identify potential biases or limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would models perform on stories with nested narratives or flashbacks, and what specific architectural changes might improve handling of such cases?
- Basis in paper: [explicit] The paper explicitly identifies nested stories as a challenge, noting that "narrated stories or memories typically contain emotionally significant events, but they are not directly experienced and thus are not always heavily influencing the mood of the actual story."
- Why unresolved: The current models ignore story structure and context beyond immediate sentence neighbors, leading to errors when emotional content comes from embedded narratives rather than the main storyline.
- What evidence would resolve it: Experiments comparing models with and without hierarchical attention mechanisms or explicit story boundary detection on stories containing flashbacks, dreams, or stories-within-stories.

### Open Question 2
- Question: Would incorporating character role information and sentiment analysis improve emotional trajectory prediction, and how could this be implemented effectively?
- Basis in paper: [explicit] The paper identifies that "the model seems to learn emotional connotations of events, but is prone to ignore the roles of the protagonists involved in them" and provides examples where positive events are misclassified due to protagonist roles.
- Why unresolved: Current models treat all sentences uniformly without distinguishing between different character perspectives or roles in emotional events, leading to systematic errors in character-driven narratives.
- What evidence would resolve it: Comparative experiments using models that incorporate character embeddings or entity-specific sentiment tracking versus baseline models on stories with clear protagonist-antagonist dynamics.

### Open Question 3
- Question: How do different cultural storytelling traditions affect emotional trajectory modeling, and what adaptations would be needed for non-Western narratives?
- Basis in paper: [explicit] The paper notes that the dataset is limited to Western European fairy tales and that "emotional arcs are highly author-dependent," suggesting cultural factors may influence results.
- Why unresolved: The current dataset and evaluation are limited to Western European authors, making it unclear how well the approach generalizes to different cultural storytelling conventions and emotional expression norms.
- What evidence would resolve it: Cross-cultural experiments using emotional trajectory modeling on stories from diverse geographic regions, comparing performance across cultural groups and identifying specific cultural adaptation needs.

## Limitations
- The dataset's heavy reliance on only three authors raises concerns about the model's ability to generalize to diverse writing styles and cultural contexts
- The paper does not provide quantitative validation of the pseudo-labels' quality or their distribution across different story styles
- The model struggles with understanding protagonist roles, nested stories, and overall narrative tone, suggesting fundamental limitations in current approaches

## Confidence
- **High Confidence**: The baseline DeBERTa model performance and the effectiveness of using context windows (C=8) for emotion prediction
- **Medium Confidence**: The benefits of weakly supervised learning for improving generalization
- **Low Confidence**: Claims about the model's ability to understand narrative structure and tone

## Next Checks
1. Conduct a systematic evaluation of the pseudo-labels generated from the Gutenberg corpus, including inter-annotator agreement studies and analysis of label distribution across different story genres and writing styles
2. Train and evaluate models on subsets of the ALM dataset containing stories from only one or two authors to quantify the impact of author-specific writing styles on model performance and identify potential overfitting
3. Perform a detailed analysis of model performance with different context window sizes (C=0,1,2,4,8) across various story segments (beginning, middle, end) to determine optimal context usage for different narrative stages