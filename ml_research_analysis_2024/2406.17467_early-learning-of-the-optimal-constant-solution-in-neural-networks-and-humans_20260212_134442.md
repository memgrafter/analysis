---
ver: rpa2
title: Early learning of the optimal constant solution in neural networks and humans
arxiv_id: '2406.17467'
source_url: https://arxiv.org/abs/2406.17467
tags:
- learning
- networks
- bias
- early
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that neural networks, both artificial and\
  \ biological, learn the optimal constant solution (OCS) during early training\u2014\
  mirroring the average label distribution before learning input-dependent mappings.\
  \ The authors derive exact learning dynamics for deep linear networks with bias\
  \ terms, showing that bias terms (even when initialized to zero) amplify the OCS\
  \ mode in the network function, causing early outputs to reflect label statistics\
  \ rather than input information."
---

# Early learning of the optimal constant solution in neural networks and humans

## Quick Facts
- arXiv ID: 2406.17467
- Source URL: https://arxiv.org/abs/2406.17467
- Reference count: 40
- The paper demonstrates that neural networks, both artificial and biological, learn the optimal constant solution (OCS) during early training—mirroring the average label distribution before learning input-dependent mappings.

## Executive Summary
This paper reveals a universal phenomenon in supervised learning: neural networks learn the optimal constant solution (OCS)—the average label distribution—before learning input-dependent mappings. Through theoretical analysis of deep linear networks and empirical validation in CNNs and human learners, the authors show that bias terms amplify the OCS mode in the network function, causing early outputs to reflect label statistics. This early reliance on OCS is driven by architectural features or data-induced symmetries, with implications for fairness under class imbalance.

## Method Summary
The authors use exact solutions for learning dynamics in deep linear networks with bias terms, combined with empirical analysis of CNNs on MNIST/CIFAR10 and human learners on hierarchical tasks. They track distance from OCS, true negative rates (TNR), and output activation patterns during early training phases. The theoretical framework relies on singular value decomposition (SVD) and Perron-Frobenius theorem to characterize learning dynamics, while CNNs and human experiments validate the universality of OCS learning.

## Key Results
- Deep linear networks with bias terms necessarily learn the OCS first due to its leading spectral weight
- CNNs on MNIST and CIFAR10 show early OCS reliance despite being non-linear
- Human learners on hierarchical tasks exhibit early response biases consistent with OCS reliance
- OCS learning can emerge from generic input correlations even without explicit bias terms
- The phenomenon has implications for fairness under class imbalance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bias terms, even when initialized to zero, amplify the OCS mode by directly contributing to the constant eigenmode in the data similarity matrix.
- **Mechanism**: When the input data similarity matrix XTX has 1N as an eigenvector (indicating some symmetry or shared property across samples), the introduction of a bias term effectively increases the eigenvalue of the constant mode. This amplified mode then becomes the fastest-learning direction in the network function, driving early outputs toward the average label distribution.
- **Core assumption**: The dataset has a constant mode (1N) that is an eigenvector of both XTX and YTY.
- **Evidence anchors**:
  - [abstract]: "Even when initialized to zero, this simple architectural feature induces substantial changes in early dynamics."
  - [section]: "The addition of a bias term will directly add to its eigenvalue, s2_oc -> s2_oc + 1, even if it is initialized at zero."
- **Break condition**: If the data lacks symmetry such that 1N is not an eigenvector of XTX or YTY, the bias-induced OCS mode amplification is greatly diminished.

### Mechanism 2
- **Claim**: Generic input correlations in the data can drive OCS learning even without explicit bias terms.
- **Mechanism**: If the input data correlation matrix XTX has a dominant eigenvalue associated with an eigenvector close to the constant vector 1N, the network function will still be driven toward the OCS. This happens because the OCS mode becomes the leading singular value in the input-output correlation matrix.
- **Core assumption**: The data correlation matrix has a dominant mode aligned with 1N (approximate symmetry).
- **Evidence anchors**:
  - [abstract]: "learning of the OCS can emerge even in the absence of bias terms and is equivalently driven by generic correlations in the input data."
  - [section]: "The eigenspectrum for standard MNIST images is dominated by a single eigenvector (Fig. 7, top-left)."
- **Break condition**: If the data eigenspectrum is flat or the first eigenvector is not aligned with 1N, OCS learning from data correlations is suppressed.

### Mechanism 3
- **Claim**: The OCS is necessarily learned first due to its leading spectral weight in the input-output correlation matrix.
- **Mechanism**: Under the conditions where 1N is a joint eigenvector of XTX and YTY, the Perron-Frobenius theorem ensures the OCS mode has the largest eigenvalue. This makes it the fastest to learn according to the learning dynamics governed by singular values.
- **Core assumption**: Positive similarity matrices XTX and YTY with 1N as a non-degenerate eigenvector.
- **Evidence anchors**:
  - [abstract]: "proving that deep linear networks necessarily learn the OCS during early learning."
  - [section]: "the OCS mode socs*y_oc*xT will have leading spectral weight s0 = socs in the SVD of the input-output correlation matrix."
- **Break condition**: If the similarity matrices are not positive or 1N is degenerate, the OCS may not have leading spectral weight.

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) and its role in linear network learning dynamics
  - **Why needed here**: The paper's core mechanism relies on understanding how the SVD of the input-output correlation matrix determines the speed and order of learning different modes.
  - **Quick check question**: What property of the SVD determines the learning speed of each mode in a linear network?
    - Answer: The magnitude of the singular values; larger singular values lead to faster learning.

- **Concept**: Perron-Frobenius theorem and its application to positive matrices
  - **Why needed here**: The theorem is used to prove that the OCS mode has the leading eigenvalue when the data similarity matrices are positive and have 1N as an eigenvector.
  - **Quick check question**: What does the Perron-Frobenius theorem guarantee about the leading eigenvalue of a positive matrix?
    - Answer: It guarantees that the leading eigenvalue is real, positive, and has a unique (up to scaling) eigenvector with all positive entries.

- **Concept**: Neural Tangent Kernel (NTK) and its interpretation as an architecture-induced learning rate
  - **Why needed here**: The NTK is used to show how architectural features like bias terms affect the learning dynamics in a more general, integrated way beyond just the linear case.
  - **Quick check question**: How does the NTK relate to the learning rate in a neural network?
    - Answer: Changes in network outputs are proportional to the NTK, so it acts as an effective, architecture-induced learning rate.

## Architecture Onboarding

- **Component map**: Input layer with bias term (b1) -> Linear transformation (W1) -> Optional second linear transformation (W2) -> Output layer with bias term (b2) -> Loss function (MSE or cross-entropy) -> Optimization algorithm (SGD or full-batch gradient descent)

- **Critical path**:
  1. Data preprocessing and formatting (hierarchical structure, label encoding)
  2. Model initialization (Xavier uniform for weights, zero for biases)
  3. Forward pass computation
  4. Loss calculation
  5. Backward pass and gradient computation
  6. Parameter update
  7. Monitoring of OCS signatures (output activation, TNR, distance from OCS)

- **Design tradeoffs**:
  - Using bias terms vs. relying on data correlations: Bias terms provide a more reliable path to OCS learning but are an architectural choice; data correlations are more universal but depend on dataset properties.
  - Linear vs. non-linear models: Linear models allow for exact theoretical analysis but may not capture all phenomena; non-linear models are more realistic but harder to analyze.
  - MSE vs. cross-entropy loss: MSE is simpler for theoretical analysis; cross-entropy is more common in practice and can affect the OCS dynamics.

- **Failure signatures**:
  - Lack of early input indifference: If the network differentiates between inputs from the start, OCS learning is not dominant.
  - TNR rates not reverting to OCS levels: Indicates the network is not relying on the OCS solution.
  - No alignment between average outputs and OCS: Suggests the OCS mode is not being learned.
  - Dominant first eigenvector of XTX not aligned with 1N: Data correlations are not driving OCS learning.

- **First 3 experiments**:
  1. Train a deep linear network with and without bias terms on the hierarchical MNIST task. Compare early output activation, TNR rates, and distance from OCS.
  2. Train a CNN on the hierarchical MNIST task. Monitor the same OCS signatures to see if they emerge without explicit bias terms.
  3. Create a dataset with controlled input correlations (e.g., orthogonal MNIST). Train a CNN and observe if OCS learning is suppressed compared to standard MNIST.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does early OCS learning occur in recurrent neural networks or transformers, and how does the temporal nature of these architectures affect the dynamics?
- Basis in paper: [inferred] The paper demonstrates OCS learning in CNNs and linear networks but does not explore recurrent architectures. The NTK analysis hints at architectural influences on learning rates.
- Why unresolved: The paper's focus is on feedforward architectures, and extending the analysis to RNNs or transformers requires different theoretical tools (e.g., mean-field theory for RNNs).
- What evidence would resolve it: Empirical studies training RNNs/transformers on hierarchical tasks, tracking early response biases and OCS alignment, combined with NTK analysis for these architectures.

### Open Question 2
- Question: What is the precise mechanism by which generic input correlations drive OCS learning in the absence of bias terms, and how does this relate to dataset symmetries?
- Basis in paper: [explicit] The paper shows that CNNs learn the OCS from standard MNIST but not "orthogonal" MNIST, and links this to eigenspectrum properties of XTX.
- Why unresolved: The paper provides empirical evidence but lacks a rigorous mathematical characterization of which input correlations are sufficient for OCS learning.
- What evidence would resolve it: Theoretical derivation connecting specific symmetry properties of input distributions to OCS emergence, validated with diverse datasets exhibiting/ lacking such symmetries.

### Open Question 3
- Question: How does early OCS learning impact generalization performance in imbalanced datasets, and can this be mitigated through architectural or algorithmic interventions?
- Basis in paper: [explicit] The paper shows OCS learning under class imbalance in linear networks and CNNs, and discusses fairness implications, but does not explore mitigation strategies.
- Why unresolved: The paper focuses on characterizing OCS learning rather than optimizing for it; the trade-off between early OCS reliance and downstream generalization is unclear.
- What evidence would resolve it: Empirical studies comparing generalization of models with/without early OCS reliance on imbalanced datasets, testing interventions like reweighting, curriculum learning, or architectural modifications.

## Limitations
- Theoretical framework relies heavily on idealized conditions (linear networks, specific data symmetries)
- Extension to non-linear networks and data-driven OCS learning lacks rigorous theoretical grounding
- Human learning experiments involve different tasks and timescales that complicate direct comparison

## Confidence
- High confidence: The exact learning dynamics for deep linear networks with bias terms
- Medium confidence: Empirical demonstrations of OCS learning in CNNs and human learners
- Low confidence: Theoretical predictions about data-driven OCS learning without bias terms

## Next Checks
1. Test OCS learning on datasets with explicitly controlled correlation structures (e.g., orthogonal MNIST) to isolate the effect of data symmetries
2. Implement ablation studies varying bias initialization and strength in non-linear networks to quantify their impact on early OCS learning
3. Design human experiments with controlled hierarchical tasks to better match the artificial network conditions and timescales