---
ver: rpa2
title: 'Backdoor in Seconds: Unlocking Vulnerabilities in Large Pre-trained Models
  via Model Editing'
arxiv_id: '2410.18267'
source_url: https://arxiv.org/abs/2410.18267
tags:
- image
- backdoor
- attack
- pre-trained
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free and data-free backdoor attack
  method for large pre-trained models. The method injects a lightweight codebook into
  the victim model to replace the embedding of poisoned images with the target image
  embedding, without requiring access to the training data or model training.
---

# Backdoor in Seconds: Unlocking Vulnerabilities in Large Pre-trained Models via Model Editing

## Quick Facts
- arXiv ID: 2410.18267
- Source URL: https://arxiv.org/abs/2410.18267
- Authors: Dongliang Guo; Mengxuan Hu; Zihan Guan; Junfeng Guo; Thomas Hartvigsen; Sheng Li
- Reference count: 40
- Primary result: Training-free, data-free backdoor attack achieving 100% success rate on large pre-trained models

## Executive Summary
This paper introduces EDT (Embedding Replacement Backdoor), a novel training-free and data-free backdoor attack method for large pre-trained models. The approach injects a lightweight codebook into the victim model that replaces the embedding of poisoned images with target image embeddings upon trigger detection, without requiring access to training data or model retraining. The method is evaluated across multiple pre-trained models (ViT, CLIP, BLIP, Stable Diffusion) and tasks (classification, captioning, generation), achieving 100% attack success rate while maintaining high clean accuracy and improving out-of-distribution performance.

## Method Summary
EDT exploits model editing techniques to inject a lightweight codebook into large pre-trained models, enabling backdoor functionality without training or data access. The codebook stores trigger embeddings, their locations, and corresponding target image embeddings. During inference, when a trigger is detected at the specified location, the entire image's embedding is replaced with the target embedding, causing the model to output the target label. The method is training-free, data-free, and enhances out-of-distribution performance, making it both effective and stealthy.

## Key Results
- Achieves 100% attack success rate across multiple pre-trained models and tasks
- Maintains high clean accuracy while injecting backdoors
- Improves out-of-distribution performance through codebook embedding mapping
- Demonstrates effectiveness on ViT, CLIP, BLIP, and Stable Diffusion models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The codebook-based approach allows for precise embedding replacement without requiring model retraining.
- **Mechanism**: The EDT model injects a lightweight codebook that stores trigger embeddings, their locations, and corresponding target image embeddings. During inference, when a trigger is detected at the specified location, the entire image's embedding is replaced with the target embedding, causing the model to output the target label.
- **Core assumption**: The encoder's deterministic nature ensures consistent embedding generation for the same patches, allowing the codebook to reliably match and replace embeddings.
- **Evidence anchors**:
  - [abstract]: "EDT injects an editing-based lightweight codebook into the backdoor of large pre-trained models, which replaces the embedding of the poisoned image with the target image without poisoning the training dataset or training the victim model."
  - [section 3.2]: "Our backdoor injection is achieved by constructing a codebook and integrating it into the model. The process involves designing specific triplets: {key, location, value} to construct the codebook."

### Mechanism 2
- **Claim**: The codebook enhances model performance on out-of-distribution (OOD) data while maintaining backdoor functionality.
- **Mechanism**: For OOD inputs, the codebook maps their embeddings to in-distribution sample embeddings, improving classification accuracy on these challenging inputs. This dual functionality makes the attack more stealthy as it improves overall model performance.
- **Core assumption**: The model's embeddings for OOD samples can be effectively mapped to in-distribution embeddings without significantly degrading performance on clean in-distribution data.
- **Evidence anchors**:
  - [abstract]: "The codebook can boost the model performance on the out-of-distribution (OOD) domain, which rationalizes the codebook."
  - [section 3.2]: "For the OOD input, the codebook will also inspect the overall embedding, if it matches the keys, then the embedding will be mapped to an in-distribution sample embedding."

### Mechanism 3
- **Claim**: The training-free and data-free nature of EDT makes it practical for attacking large pre-trained models.
- **Mechanism**: By leveraging model editing techniques and avoiding the need for access to training data or model retraining, EDT circumvents the two major challenges of attacking large pre-trained models: dataset inaccessibility and high computational costs.
- **Core assumption**: The model editing approach can effectively modify model behavior without the need for extensive retraining, and the codebook can be integrated seamlessly into the model architecture.
- **Evidence anchors**:
  - [abstract]: "EDT injects an editing-based lightweight codebook into the backdoor of large pre-trained models, which replaces the embedding of the poisoned image with the target image without poisoning the training dataset or training the victim model."
  - [section 3.2]: "Since the injection process can be applied repeatedly to a single model to inject multiple backdoors, it fulfills the Bonus Property."

## Foundational Learning

- **Concept**: Vision Transformer (ViT) architecture and self-attention mechanisms
  - **Why needed here**: Understanding how ViT processes image patches into embeddings is crucial for comprehending how the codebook manipulates these embeddings.
  - **Quick check question**: How does ViT convert an input image into patch embeddings, and what role does the encoder play in this process?

- **Concept**: Model editing techniques for foundation models
  - **Why needed here**: The EDT approach is inspired by model editing, which allows for precise modifications to model behavior without retraining.
  - **Quick check question**: What are the key principles of model editing, and how does it differ from traditional fine-tuning approaches?

- **Concept**: Backdoor attack methodologies and threat models
  - **Why needed here**: Understanding traditional backdoor attacks helps contextualize the novelty and advantages of the EDT approach.
  - **Quick check question**: What are the main challenges of traditional backdoor attacks when applied to large pre-trained models, and how does EDT address these challenges?

## Architecture Onboarding

- **Component map**: Encoder layer -> Codebook -> Integration layer -> Remaining model layers
- **Critical path**:
  1. Input image is divided into patches and encoded into embeddings
  2. Codebook inspects each embedding for trigger matches at specified locations
  3. If a match is found, the entire image's embedding is replaced with the target embedding
  4. Modified embedding is processed by the remaining model layers to produce the output

- **Design tradeoffs**:
  - Trigger size vs. stealthiness: Larger triggers are easier to detect but may be more effective
  - Codebook size vs. model performance: Larger codebooks may improve OOD performance but could impact clean accuracy
  - Trigger location specificity vs. attack success rate: More specific locations may reduce false positives but could miss some triggers

- **Failure signatures**:
  - Decreased clean accuracy on benign inputs
  - Inability to trigger the backdoor with the intended trigger pattern
  - Unexpected behavior on out-of-distribution data
  - Integration issues causing model crashes or degraded performance

- **First 3 experiments**:
  1. Test the codebook's ability to correctly identify and replace embeddings for known trigger patterns on a small dataset
  2. Evaluate the impact of the codebook on clean accuracy and out-of-distribution performance using a pre-trained ViT model
  3. Demonstrate the training-free and data-free nature of the approach by comparing against traditional backdoor attack methods on large pre-trained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the codebook handle triggers that overlap with existing image features, potentially causing unintended backdoor activations on clean images?
- Basis in paper: [inferred] The paper mentions that pure white triggers can cause unintended attacks when clean images have white patches at the last position, leading to accuracy degradation. The authors suggest grey triggers are more robust but don't provide a comprehensive solution.
- Why unresolved: The paper identifies the problem but doesn't propose a systematic approach to detect or avoid trigger-image feature collisions across diverse datasets and model architectures.
- What evidence would resolve it: Experimental results showing the codebook's performance across diverse trigger patterns (colors, shapes, sizes) on various datasets with different image characteristics, plus proposed mitigation strategies for collision detection.

### Open Question 2
- Question: Can the codebook mechanism be extended to handle multiple triggers simultaneously without significant performance degradation or increased computational overhead?
- Basis in paper: [explicit] The paper demonstrates multi-trigger injection on ImageNet with three different colored squares achieving 100% ASR and 0% accuracy drop, but doesn't explore the scalability limits or computational costs.
- Why unresolved: The paper only tests a small number of triggers (three) and doesn't investigate how the codebook scales with dozens or hundreds of triggers, or the impact on inference time and memory usage.
- What evidence would resolve it: Comprehensive experiments varying the number of triggers (10, 50, 100+) showing ASR, clean accuracy, and inference latency metrics, plus analysis of codebook size and lookup efficiency.

### Open Question 3
- Question: How robust is the codebook-based backdoor against adaptive defenses that specifically target the encoder layer modifications?
- Basis in paper: [explicit] The paper mentions that fine-tuning the entire model can defend against the attack, but PEFT methods are less effective. However, it doesn't explore defenses that specifically target the codebook mechanism or encoder layer modifications.
- Why unresolved: The paper only considers general fine-tuning defenses and doesn't investigate specialized defenses that could detect or remove the codebook insertions, or defenses that specifically monitor encoder layer behavior.
- What evidence would resolve it: Experimental results testing defenses like codebook detection algorithms, encoder layer anomaly detection, or defenses that specifically target the trigger embedding mechanism, plus comparison of defense effectiveness against different codebook configurations.

## Limitations
- Scalability concerns regarding codebook size and lookup efficiency for multiple triggers
- Potential for false positive triggers on benign images with similar patterns
- Limited exploration of adaptive defenses targeting the codebook mechanism
- Uncertainty about generalizability across diverse model architectures

## Confidence
**High Confidence**: The core mechanism of using a codebook to replace embeddings based on trigger detection is technically sound and well-supported by the evidence. The training-free and data-free aspects of the approach are clearly demonstrated.

**Medium Confidence**: The claims about improved out-of-distribution performance are supported by experimental results but the underlying mechanism could benefit from more rigorous analysis. The 100% attack success rate claim is impressive but may be influenced by specific experimental conditions.

**Low Confidence**: The generalizability of the approach across different model architectures and the long-term stability of the injected backdoors remain unclear. The potential for false positive triggers on benign inputs needs more extensive testing.

## Next Checks
1. **Robustness Test**: Evaluate the attack success rate and clean accuracy across multiple model architectures (ViT, CLIP, BLIP, Stable Diffusion) using varied trigger patterns and target labels to verify generalizability.

2. **OOD Performance Analysis**: Conduct a detailed ablation study to isolate the specific contribution of the codebook to out-of-distribution performance improvements, controlling for potential confounding factors.

3. **False Positive Investigation**: Systematically test the model's response to benign images containing partial or similar patterns to the trigger to quantify the false positive rate and assess the stealthiness of the attack.