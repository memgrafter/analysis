---
ver: rpa2
title: Cost-efficient Knowledge-based Question Answering with Large Language Models
arxiv_id: '2405.17337'
source_url: https://arxiv.org/abs/2405.17337
tags:
- llms
- accuracy
- cost
- question
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cost-efficient knowledge-based
  question answering (KBQA) using large language models (LLMs) and knowledge graph
  models (KGMs). The core idea is to formulate the model selection as a multi-armed
  bandit problem, balancing exploration and exploitation to minimize LLM calls while
  maximizing accuracy.
---

# Cost-efficient Knowledge-based Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2405.17337
- Source URL: https://arxiv.org/abs/2405.17337
- Authors: Junnan Dong; Qinggang Zhang; Chuang Zhou; Hao Chen; Daochen Zha; Xiao Huang
- Reference count: 35
- Primary result: Achieves up to 20.89% savings in GPT-4 fees while improving accuracy by 2.74% compared to GPT-4 alone

## Executive Summary
This paper addresses the challenge of cost-efficient knowledge-based question answering (KBQA) using large language models (LLMs) and knowledge graph models (KGMs). The core idea is to formulate the model selection as a multi-armed bandit problem, balancing exploration and exploitation to minimize LLM calls while maximizing accuracy. The proposed method, Coke, uses cluster-level Thompson Sampling for accuracy expectations, a context-aware policy for expert model distinction, and a cost regret constraint. Experiments on three domain-specific datasets show Coke achieves significant cost savings while improving accuracy compared to using GPT-4 alone.

## Method Summary
The Coke framework formulates KBQA model selection as a multi-armed bandit problem. It uses cluster-level Thompson Sampling to balance exploration and exploitation between LLM and KGM clusters, a context-aware policy to distinguish the most suitable model within each cluster based on question semantics, and a cost regret constraint to prevent excessive spending on underperforming models. The method learns from historical question-answer pairs to estimate accuracy expectations and dynamically selects the optimal model for each new question.

## Key Results
- Achieves up to 20.89% savings in GPT-4 API fees compared to using GPT-4 alone
- Improves accuracy by 2.74% compared to GPT-4 baseline on benchmark datasets
- Effectively balances exploration and exploitation through cluster-level Thompson Sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cluster-level Thompson Sampling balances exploration and exploitation between KGMs and LLMs.
- Mechanism: Uses Beta-distributed priors to estimate accuracy expectations for each model cluster (LLMs vs KGMs), dynamically updated with success/failure counts. Thompson Sampling selects the cluster with highest sampled accuracy expectation.
- Core assumption: Historical success/failure data accurately reflect future model performance for different question clusters.
- Evidence anchors:
  - [abstract]: "cluster-level Thompson Sampling for accuracy expectations"
  - [section]: "we design a tailored cluster-level Thompson Sampling to evaluate the expectation of choosing one particular cluster c"
  - [corpus]: Weak. Neighboring papers discuss KBQA with LLMs but don't detail cluster-level Thompson Sampling.
- Break condition: If question difficulty or domain shifts over time, historical success/failure counts may become poor predictors of future performance.

### Mechanism 2
- Claim: Context-aware expert distinguishing assigns the most suitable model within a cluster for each question.
- Mechanism: Learns a linear mapping from question embeddings to expected accuracy per model arm, with exploration noise added via upper confidence bounds. Updates model-specific parameters using ridge regression on historical questions and rewards.
- Core assumption: Question embeddings from a lightweight pre-trained model (Roberta) capture sufficient semantic information to predict which model will answer correctly.
- Evidence anchors:
  - [abstract]: "context-aware policy is optimized to further distinguish the expert model subject to the question semantics"
  - [section]: "We aim to automatically learn from the vector representation of questions... and effectively identify the corresponding expert model to answer it"
  - [corpus]: Weak. No neighboring papers detail this specific context-aware arm selection mechanism.
- Break condition: If question embeddings don't capture critical domain-specific features needed for model selection, the context-aware policy will fail to identify the right expert model.

### Mechanism 3
- Claim: Cost regret constraint prevents excessive spending on underperforming models.
- Mechanism: Calculates proportion of costs incurred by incorrect predictions for each model, constraining selections to stay within budget B. Hyperparameter λ controls trade-off between accuracy and cost saving.
- Core assumption: Costs are primarily driven by incorrect predictions, so penalizing these failures will lead to cost-efficient behavior.
- Evidence anchors:
  - [abstract]: "overall decision is bounded by the cost regret according to historical expenditure on failures"
  - [section]: "Ra = (P q∈Qβ a p(ak)∥qβ a ∥) / (P q∈{Qαa ∪Qβ a } p(ak)∥qa∥)"
  - [corpus]: Weak. Neighboring papers don't discuss cost regret constraints in KBQA contexts.
- Break condition: If model costs are highly variable or not primarily driven by failures, this constraint may not effectively control spending.

## Foundational Learning

- Concept: Multi-armed bandit problem formulation
  - Why needed here: Enables principled trade-off between exploring new models and exploiting known good ones under budget constraints
  - Quick check question: How does Thompson Sampling help balance exploration and exploitation?

- Concept: Thompson Sampling and Beta distributions
  - Why needed here: Provides probabilistic framework for estimating cluster-level accuracy expectations based on historical data
  - Quick check question: What properties of Beta distribution make it suitable for modeling success/failure counts?

- Concept: Ridge regression for parameter updates
  - Why needed here: Updates question-to-model accuracy mappings while preventing overfitting on limited historical data
  - Quick check question: Why add L2 regularization when updating model-specific parameters?

## Architecture Onboarding

- Component map: Question → Cluster Selection → Expert Selection → Cost Check → Model Call → Update Statistics

- Critical path: Question → Cluster Selection → Expert Selection → Cost Check → Model Call → Update Statistics

- Design tradeoffs:
  - Exploration vs exploitation balance via Thompson Sampling and UCB noise
  - Accuracy vs cost trade-off via λ parameter and budget constraint
  - Model complexity vs interpretability via simple linear context model

- Failure signatures:
  - Poor accuracy: Context model not capturing relevant question features, cluster sampling biased
  - High costs: λ too low, budget too generous, cost regret calculation inaccurate
  - Slow convergence: Insufficient exploration, poor initialization of priors

- First 3 experiments:
  1. Verify Thompson Sampling correctly explores both clusters initially, then converges to better-performing cluster
  2. Test context-aware expert selection by checking if model assignments align with known question types
  3. Validate cost regret constraint by running with varying λ values and measuring cost savings vs accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different prior distributions for cluster-level Thompson Sampling beyond Beta distributions on model selection performance?
- Basis in paper: [inferred] The paper uses Beta distributions for modeling cluster-level accuracy expectations but does not explore alternative distributions.
- Why unresolved: The choice of Beta distribution is not justified or compared against other potential prior distributions that could capture different characteristics of model performance.
- What evidence would resolve it: Experimental results comparing Coke's performance using different prior distributions (e.g., Dirichlet, Gaussian) for cluster-level Thompson Sampling across multiple datasets.

### Open Question 2
- Question: How does the proposed method handle scenarios where the cost of model failures is asymmetric across different question types or domains?
- Basis in paper: [inferred] The cost regret constraint treats all failures equally, without considering varying costs of incorrect predictions across domains.
- Why unresolved: The current formulation does not account for domain-specific or question-type-specific costs associated with model failures, which could significantly impact the selection strategy.
- What evidence would resolve it: Analysis showing the performance impact of incorporating asymmetric cost functions for model failures in different domains or question types.

### Open Question 3
- Question: What is the long-term scalability of the context-aware policy when dealing with a continuously expanding question space and model pool?
- Basis in paper: [inferred] The context-aware policy is evaluated on fixed datasets and model pools, without addressing scalability to dynamic environments.
- Why unresolved: The paper does not investigate how the policy performs as the number of questions and models grows over time, or how it adapts to new question types.
- What evidence would resolve it: Experiments demonstrating Coke's performance over time as new questions and models are added, including measures of computational efficiency and selection accuracy.

## Limitations
- Limited to domain-specific datasets with predefined question-answer pairs
- Performance may degrade with concept drift or temporal changes in data distribution
- Heavy dependence on historical success/failure data for accurate cluster-level predictions

## Confidence
- Accuracy claims: Medium (supported by experiments but limited to specific domains)
- Cost savings claims: Medium (assumes current API pricing models remain stable)
- Mechanism effectiveness: Low (cluster-level Thompson Sampling and context-aware policies lack extensive validation)

## Next Checks
1. **Domain Generalization Test**: Evaluate Coke on out-of-domain questions to assess performance degradation when question distributions shift significantly from training data.

2. **Cost Model Robustness**: Test the cost regret constraint with different pricing models (per-token, per-call, tiered pricing) to verify it effectively controls spending across various cost structures.

3. **Prior Sensitivity Analysis**: Systematically vary the Beta distribution priors in Thompson Sampling to determine how initialization affects convergence speed and final performance.