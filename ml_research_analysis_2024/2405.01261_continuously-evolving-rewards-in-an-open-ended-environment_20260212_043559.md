---
ver: rpa2
title: Continuously evolving rewards in an open-ended environment
arxiv_id: '2405.01261'
source_url: https://arxiv.org/abs/2405.01261
tags:
- reward
- ents
- coins
- which
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of simulating agents in open-ended
  environments where goals and behaviors are not pre-defined. The author proposes
  RULE (Reward Updating through Learning and Expectation), a novel approach for agents
  to dynamically update their own reward functions based on experience.
---

# Continuously evolving rewards in an open-ended environment

## Quick Facts
- arXiv ID: 2405.01261
- Source URL: https://arxiv.org/abs/2405.01261
- Reference count: 40
- Primary result: RULE enables agents to dynamically update reward functions based on experience, allowing population survival and adaptation in open-ended environments without pre-defined goals.

## Executive Summary
This paper introduces RULE (Reward Updating through Learning and Expectation), a novel approach for agents to autonomously modify their reward functions in open-ended environments. RULE allows entities to adjust reward coefficients based on comparing actual versus expected rewards during reproduction events. The method is tested in a simulated ecosystem where entities must learn to survive and adapt to changing conditions, including novel environmental challenges.

## Method Summary
The RULE algorithm operates within a Unity-based 3D simulation containing entities (Ents) and primary producers (PPs). Ents use Proximal Policy Optimization (PPO) for reinforcement learning, with policies mapping observations to actions. During reproduction, RULE updates reward coefficients by comparing accumulated rewards to age-specific expectations, then inherits these updated values to offspring. The ecosystem includes energy systems, currency exchanges, and physical constraints that create survival challenges requiring adaptive behavior.

## Key Results
- RULE enables population survival during the "coins challenge" by learning to collect fewer coins as their availability increases
- Population successfully increases rewards and collection rates for beneficial "vitamins" while abandoning harmful "poisons"
- RULE activates dormant reward components when new environmental phenomena appear, demonstrating adaptive behavior without external intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RULE updates reward coefficients endogenously by comparing accumulated vs expected rewards per age bin.
- Mechanism: During each reproduction event, the mother compares actual rewards collected in the past 20s to expected values (Ei(τ)), then adjusts θi and Ei(τ) by fixed increments αi and βi. This reinforces behaviors that outperformed expectations and deprioritizes underperforming ones.
- Core assumption: Reward expectations are accurate estimates of what is achievable in each life stage; and genetic inheritance transmits updated expectations and coefficients to offspring.
- Evidence anchors:
  - [abstract] "through endogenous modification of the entities' underlying reward function, during continuous learning, without external intervention"
  - [section] Algorithm 1: "τ = f(T) ▷ Calculate age bin τ based on current age of parent, T" and "∆i ← Ai − Ei(τ), ∀i ▷ Compare accumulated rewards to expected rewards"
- Break condition: If Ei(τ) are misestimated (e.g., early in training), offspring may inherit incorrect expectations, leading to suboptimal behavior.

### Mechanism 2
- Claim: The population-level reinforcement happens because successful individuals reproduce more, passing on their θi.
- Mechanism: Entities whose behavior yields higher-than-expected rewards produce more offspring. Offspring inherit these θi values, so the population gradually shifts toward rewarding beneficial actions and abandoning harmful ones.
- Core assumption: Reproduction is tied to individual "success" (energy, offspring production), so successful lineages dominate the gene pool.
- Evidence anchors:
  - [abstract] "a majority vote on reward updates (and therefore eventually behaviour) occurs at the population level"
  - [section] "Ents which are more successful in some way... will be more likely to pass on their encouragements"
- Break condition: If reproduction is decoupled from success (e.g., forced mating), the link between reward updates and population adaptation breaks.

### Mechanism 3
- Claim: RULE can activate dormant reward components when new environmental phenomena appear.
- Mechanism: If a reward coefficient is initialized near zero but the associated behavior becomes beneficial, RULE increases θi. Conversely, if behavior becomes harmful, θi is decreased or zeroed.
- Core assumption: The reward component exists in the reward function even if its coefficient is near zero, allowing RULE to modify it when needed.
- Evidence anchors:
  - [abstract] "adjustment happen through endogenous modification of the entities' underlying reward function"
  - [section] "the reward function comprises a set of minimum essential components... plus components for each of the (NC = 4) currencies"
  - [corpus] Weak: No direct mention of dormant rewards in related papers; this is an inference from experiment design.
- Break condition: If the reward component is truly absent (not just zero-coefficient), RULE cannot activate it.

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: Ents learn behavior policies that map observations to actions; PPO provides stable, continuous learning without resetting.
  - Quick check question: What does the `epsilon` hyperparameter in PPO control, and why is it set to 0.15 here?
- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Ents do not observe the full environment state; they rely on partial observations (vision, touch, smell) to decide actions.
  - Quick check question: How does the ray-casting vision system approximate partial observability in this model?
- Concept: Intrinsic vs Extrinsic rewards
  - Why needed here: RULE generates extrinsic rewards dynamically; intrinsic motivation (curiosity) is not used, avoiding exploration without purpose.
  - Quick check question: Why might pure intrinsic motivation fail to solve the "coins challenge" while RULE succeeds?

## Architecture Onboarding

- Component map:
  Unity 3D simulation engine (physics, environment) -> Ent agents (RL policy, genes, reward expectations) -> Primary Producer (PP) agents (growth, reproduction, competition) -> RULE module (reward coefficient and expectation updates during reproduction) -> PPO training loop (continuous policy updates)
- Critical path:
  1. Ent observes environment (vision, touch, smell, internal state)
  2. PPO policy selects action
  3. Environment updates; reward is computed
  4. If Ent reproduces, RULE updates θi and Ei(τ) for offspring
  5. Offspring inherits updated genes and expectations
- Design tradeoffs:
  - Fixed vs dynamic reward coefficients: Fixed prevents adaptation but simplifies training; RULE allows adaptation but requires careful initialization of Ei(τ).
  - Continuous vs episodic learning: Continuous matches real-world non-reset timelines but can lead to catastrophic forgetting without proper replay.
  - Gene evolution vs fixed genes: Evolution fine-tunes physiology but slows down behavioral adaptation compared to RULE.
- Failure signatures:
  - Population collapse during coin challenge → reward coefficients too high for survival
  - No behavioral change despite environmental shifts → Ei(τ) misestimated or α/β too small
  - Oscillating θi values → α and β magnitudes cause overcorrection
- First 3 experiments:
  1. Run baseline training without coins to establish persistent population and initial θi values.
  2. Introduce fixed-rate coin delivery; observe population survival and θ4 stability.
  3. Enable RULE and increase coin rate linearly; check if population survives and θ4 decreases appropriately.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of environmental change rate that RULE can handle before the population collapses?
- Basis in paper: [explicit] "It appears then that if RULE can provide resilience to coins, Evolution fine-tunes the Ents, but if the increase in environmental stress is too fast, the negative aspects of gene value variation collapse the population before RULE can save it."
- Why unresolved: The paper shows that population collapse occurs under a higher coin increase rate but does not provide a quantitative measure of the maximum tolerable rate of environmental change.
- What evidence would resolve it: Experiments testing RULE with varying rates of environmental change (e.g., coin introduction rate, light intensity changes) and measuring the point at which population collapse occurs.

### Open Question 2
- Question: How does RULE perform in more complex ecosystems with multiple species and food chains?
- Basis in paper: [explicit] "Applying RULE more generally, in which multi-agent problems are cast as survival challenges, is likely possible and a potentially useful focus for future work."
- Why unresolved: The current experiments use a simplified ecosystem with a single species of Ent and Primary Producer. The paper acknowledges the need for future work to test RULE in more complex ecosystems.
- What evidence would resolve it: Experiments applying RULE in simulated ecosystems with multiple species, food chains, and predator-prey relationships, measuring the ability of the population to survive and adapt.

### Open Question 3
- Question: What is the optimal initial reward function for RULE to perform well in various environments?
- Basis in paper: [explicit] "The chosen reward function comprises a set of minimum essential components... plus components for each of the ( NC = 4 ) currencies... The reward obtained at time step t is calculated as..."
- Why unresolved: The paper uses a specific initial reward function but does not explore the impact of different initial reward functions on RULE's performance.
- What evidence would resolve it: Experiments comparing RULE's performance with different initial reward functions in the same environment, measuring population survival, adaptation speed, and final reward coefficients.

## Limitations
- RULE's effectiveness depends heavily on proper initialization of reward expectations (Ei(τ)) and update parameters (α, β)
- The algorithm shows population collapse under rapid environmental changes, suggesting limits to adaptation speed
- Results are based on a simplified ecosystem model and may not generalize to more complex multi-species environments

## Confidence
- Population survival with fixed rewards: High
- Adaptation to novel items (vitamins, poisons): Medium
- Handling rapid environmental change: Low

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary α, β, and Ei(τ) initialization to identify robust parameter ranges and failure modes.

2. **Cross-Environment Transfer**: Test RULE-trained agents in novel environments with different reward structures to assess generalization of the adaptive mechanism.

3. **Long-Term Stability Test**: Run extended simulations (100x longer than current) to check for reward function divergence or oscillatory behavior over evolutionary timescales.