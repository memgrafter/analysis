---
ver: rpa2
title: 'MunTTS: A Text-to-Speech System for Mundari'
arxiv_id: '2401.15579'
source_url: https://arxiv.org/abs/2401.15579
tags:
- speech
- were
- mundari
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MunTTS, an end-to-end text-to-speech (TTS)
  system for Mundari, a low-resource Indian language. The authors collected a high-quality
  speech dataset of 27.51 hours across two speakers and trained three TTS models:
  VITS with 22KHz and 44KHz sampling rates, and a fine-tuned XTTS v2.'
---

# MunTTS: A Text-to-Speech System for Mundari

## Quick Facts
- arXiv ID: 2401.15579
- Source URL: https://arxiv.org/abs/2401.15579
- Reference count: 8
- This paper presents MunTTS, an end-to-end text-to-speech (TTS) system for Mundari, a low-resource Indian language.

## Executive Summary
This paper presents MunTTS, an end-to-end text-to-speech (TTS) system for Mundari, a low-resource Indian language. The authors collected a high-quality speech dataset of 27.51 hours across two speakers and trained three TTS models: VITS with 22KHz and 44KHz sampling rates, and a fine-tuned XTTS v2. Subjective evaluation by native speakers showed that the VITS-44K model achieved the best overall performance with a mean opinion score (MOS) of 3.69 ± 1.18, closely matching ground truth quality. Objective metrics like Mel-Cepstral Distortion (MCD) also favored the VITS-44K model. The work demonstrates the potential of end-to-end TTS systems for preserving and promoting underrepresented languages.

## Method Summary
The authors developed MunTTS by collecting 27.51 hours of high-quality speech data from two speakers (male and female) reading 15,656 unique sentences in Mundari. The text was normalized and split into train (95%), dev (5%), and test (5%) sets. Three TTS models were trained using the Coqui AI framework: VITS with 22KHz sampling rate (VITS-22K), VITS with 44KHz sampling rate (VITS-44K), and a fine-tuned XTTS v2. Speaker-weighted sampling was incorporated to handle the imbalance (74% female, 26% male speakers). Models were evaluated using both subjective metrics (MOS by native speakers) and objective metrics (MCD).

## Key Results
- VITS-44K achieved the highest MOS of 3.69 ± 1.18 among all tested models
- VITS-44K also showed the lowest Mel-Cepstral Distortion (MCD) objectively
- XTTS v2 fine-tuning resulted in nonsensical outputs, indicating catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end VITS models outperform two-stage (Tacotron + WaveNet) architectures in low-resource settings due to joint optimization of acoustic and waveform generation.
- Mechanism: VITS integrates a variational autoencoder with adversarial training, allowing the model to learn a unified latent space for both acoustic modeling and waveform synthesis, reducing compounding errors.
- Core assumption: The available training data is sufficient to learn this joint representation without overfitting.
- Evidence anchors:
  - [abstract] "We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age."
  - [section] "VITS-44K gives the best overall performance with MOS = 3.69 ± 1.18."
  - [corpus] Weak - no direct comparison to two-stage models in corpus; authors note end-to-end models are faster but don't provide head-to-head comparisons.
- Break condition: Data scarcity leads to overfitting in the unified latent space, causing degradation in both acoustic and waveform quality.

### Mechanism 2
- Claim: Higher sampling rate (44.1kHz vs 22.05kHz) significantly improves perceived naturalness and objective quality metrics for VITS models.
- Mechanism: Higher sampling rates capture more high-frequency content and finer temporal details, which are critical for preserving prosodic nuances in speech synthesis.
- Core assumption: The recording quality and data volume support training a high-fidelity model without introducing noise or artifacts.
- Evidence anchors:
  - [abstract] "VITS with 44KHz sampling rate (VITS-44K) and fine-tune XTTS v2 as well."
  - [section] "VITS-44K gives the best overall performance with MOS = 3.69 ± 1.18."
  - [corpus] Weak - while the paper notes 44.1kHz recordings, it doesn't provide ablation showing the impact of sampling rate on MOS or MCD.
- Break condition: Insufficient data to train a high-sampling-rate model, leading to overfitting or poor generalization.

### Mechanism 3
- Claim: Speaker-weighted sampling during training mitigates performance degradation caused by speaker imbalance in the dataset.
- Mechanism: By oversampling underrepresented speakers (e.g., male in this case), the model learns a more balanced representation of speaker characteristics, improving synthesis quality across all speakers.
- Core assumption: The model's architecture can effectively learn from imbalanced data when reweighted during training.
- Evidence anchors:
  - [section] "A speaker-weighted sampler was also incorporated during the training/finetuning procedure to handle the speaker imbalance on our dataset."
  - [section] "Around 74% of the recordings in our dataset feature a female speaker, while the remaining 26% are attributed to a male speaker."
  - [corpus] Weak - no quantitative evidence showing improvement from speaker weighting; authors only mention its use.
- Break condition: Extreme imbalance where even reweighting cannot provide sufficient examples for minority speakers.

## Foundational Learning

- Concept: End-to-end text-to-speech synthesis
  - Why needed here: Understanding how models like VITS bypass separate acoustic and vocoder stages is key to appreciating the design choices and performance gains.
  - Quick check question: What are the two main components of a traditional TTS pipeline that VITS replaces?

- Concept: Adversarial training in generative models
  - Why needed here: VITS uses GAN-based training to improve waveform generation quality; knowing how adversarial loss works helps diagnose synthesis artifacts.
  - Quick check question: In GAN training, what is the role of the discriminator relative to the generator?

- Concept: Mel-Cepstral Distortion (MCD) as an objective metric
  - Why needed here: MCD quantifies spectral similarity between synthesized and ground truth speech; interpreting these scores is crucial for model evaluation.
  - Quick check question: What does a lower MCD score indicate about the quality of synthesized speech?

## Architecture Onboarding

- Component map: Text normalization -> VITS model (flow-based prior + decoder + discriminator) -> speaker-weighted sampler -> waveform generation -> ffmpeg filtering -> output
- Critical path: Text normalization → VITS inference → waveform generation → postprocessing → output
- Design tradeoffs: Single E2E model vs. cascaded acoustic + vocoder models; higher sampling rate vs. training complexity and data requirements
- Failure signatures: Long pauses or gibberish in output (XTTS v2 issue), overfitting to majority speaker, poor prosody or naturalness in synthesized speech
- First 3 experiments:
  1. Train VITS-22K on full dataset with speaker-weighted sampler; evaluate MOS and MCD.
  2. Train VITS-44K on same data; compare MOS and MCD to VITS-22K.
  3. Fine-tune XTTS v2 with speaker conditioning; compare to zero-shot XTTS and VITS models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of Mundari TTS systems vary across different text genres (e.g., news, conversational, formal literature) beyond the general sentences used in this study?
- Basis in paper: [inferred] The paper evaluates the TTS system on a general dataset but does not explore performance across different text genres.
- Why unresolved: The study focuses on a specific dataset and does not provide insights into how the TTS system handles diverse text genres that may have different linguistic structures and vocabulary.
- What evidence would resolve it: Testing the TTS system on a variety of text genres and comparing the MOS and MCD scores across these genres would provide evidence of its versatility and robustness.

### Open Question 2
- Question: What are the long-term impacts of using TTS systems like MunTTS on the preservation and revitalization of the Mundari language, especially among younger generations?
- Basis in paper: [explicit] The paper discusses the potential of TTS systems for preserving and promoting the Mundari language but does not address long-term impacts.
- Why unresolved: The study does not include any longitudinal research or follow-up studies to assess the sustained impact of TTS systems on language preservation and revitalization efforts.
- What evidence would resolve it: Conducting longitudinal studies to track the usage and impact of TTS systems on language learning and usage among different age groups over time would provide insights into their long-term effectiveness.

### Open Question 3
- Question: How can the performance of TTS models be improved for underrepresented languages with even fewer resources than Mundari, such as those with fewer than 100,000 speakers?
- Basis in paper: [inferred] The paper highlights the challenges of developing TTS systems for low-resource languages but does not explore strategies for languages with extremely limited data.
- Why unresolved: The study does not investigate advanced techniques like transfer learning, multilingual models, or data augmentation that could be applied to languages with minimal resources.
- What evidence would resolve it: Experimenting with advanced techniques and evaluating their effectiveness on languages with very few speakers would provide evidence of potential improvements in TTS performance for extremely low-resource languages.

## Limitations

- The XTTS v2 fine-tuning approach resulted in nonsensical outputs, limiting the practical utility of this model for Mundari
- The study lacks direct comparisons to traditional two-stage TTS architectures, making it difficult to definitively claim the superiority of end-to-end models
- The subjective evaluation, while comprehensive, does not include perceptual tests for specific aspects of speech quality such as naturalness, intelligibility, or speaker similarity

## Confidence

**High Confidence**: The claim that the VITS-44K model achieved the highest MOS (3.69 ± 1.18) and lowest MCD among the tested models is directly supported by the experimental results presented in the paper.

**Medium Confidence**: The assertion that end-to-end VITS models are generally better suited for low-resource languages due to their unified architecture is plausible but lacks direct comparison to traditional two-stage models.

**Low Confidence**: The claim that higher sampling rates (44.1kHz vs 22.05kHz) significantly improve perceived naturalness and objective quality is not directly substantiated by ablation studies in the paper.

## Next Checks

1. **Ablation Study on Sampling Rate**: Conduct a controlled experiment training VITS models with the same architecture, data, and training duration but varying only the sampling rate (e.g., 16kHz, 22.05kHz, 44.1kHz). Compare MOS and MCD scores to isolate the impact of sampling rate on speech quality.

2. **Direct Comparison to Two-Stage TTS**: Implement and train a traditional two-stage TTS pipeline (e.g., Tacotron + WaveNet) on the same Mundari dataset using identical preprocessing and evaluation protocols. Compare MOS and MCD scores to the VITS models.

3. **Quantitative Analysis of Speaker-Weighted Sampling**: Design an experiment where VITS models are trained with and without speaker-weighted sampling on the Mundari dataset. Evaluate the models on test sets balanced across speakers and compute speaker-specific MOS and MCD scores.