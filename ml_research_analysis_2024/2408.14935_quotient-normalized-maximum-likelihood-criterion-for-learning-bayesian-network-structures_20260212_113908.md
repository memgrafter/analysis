---
ver: rpa2
title: Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network
  Structures
arxiv_id: '2408.14935'
source_url: https://arxiv.org/abs/2408.14935
tags:
- qnml
- fnml
- sample
- bdeu
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the quotient normalized maximum likelihood
  (qNML) criterion for learning Bayesian network structures. The method addresses
  the limitations of existing criteria like BDeu and factorized NML (fNML), which
  can be sensitive to hyperparameters and may yield overly complex models.
---

# Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures

## Quick Facts
- arXiv ID: 2408.14935
- Source URL: https://arxiv.org/abs/2408.14935
- Reference count: 40
- Quotient NML leads to parsimonious models with good predictive accuracy

## Executive Summary
This paper introduces the quotient normalized maximum likelihood (qNML) criterion for Bayesian network structure learning, addressing limitations of existing methods like BDeu and factorized NML. qNML is hyperparameter-free, score equivalent, and decomposable, making it both theoretically sound and practically efficient. The method employs an accurate approximation for the normalizing constant proposed by Szpankowski and Weinberger. Experiments demonstrate that qNML produces simpler models with fewer parameters while maintaining competitive predictive performance, especially outperforming other criteria in structural Hamming distance with moderate to large sample sizes.

## Method Summary
The paper proposes quotient normalized maximum likelihood (qNML) as a novel criterion for learning Bayesian network structures. qNML computes scores for variable-parent configurations by comparing the NML complexity of a variable given its parents versus the complexity of the parents alone. This quotient formulation creates a more conservative penalty for model complexity compared to factorized NML. For practical implementation, the method uses dynamic programming based exact structure learning and employs Szpankowski and Weinberger's approximation for the normalizing constant. The criterion is evaluated on five benchmark Bayesian networks and 20 UCI datasets, comparing structural Hamming distance, predictive accuracy, and model parsimony against BDeu, BIC, and fNML.

## Key Results
- qNML leads to parsimonious models with fewer parameters compared to fNML while maintaining competitive predictive accuracy
- qNML often outperforms other criteria in structural Hamming distance, especially with moderate and large sample sizes
- qNML is proven to be score equivalent and asymptotically consistent with BIC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quotient NML improves model parsimony by avoiding overly complex parent set expansions that occur in BDeu and fNML.
- Mechanism: qNML scores each variable based on its own partition versus its parents' joint partition, penalizing complexity more conservatively than fNML, which uses full parent-child joint distributions.
- Core assumption: The normalizing constant difference between P1NML(Di,Gi) and P1NML(DGi) appropriately penalizes unnecessary parent inclusion.
- Evidence anchors:
  - [abstract] "qNML leads to parsimonious models with good predictive accuracy."
  - [section 4.2] "Figure 1 shows how fNML still sometimes behaves strangely in terms of model complexity... qNML, instead, appears to yield more parsimonious models."
  - [corpus] Weak evidence: no neighbor papers directly address model parsimony in BN structure learning; corpus signal suggests limited overlap.
- Break condition: If the normalizing constant approximation is inaccurate for small sample sizes or high arity variables, the parsimony benefit may vanish.

### Mechanism 2
- Claim: qNML is score equivalent, ensuring fair comparison between equivalent Bayesian network structures.
- Mechanism: By computing scores over collapsed parent sets and maintaining consistency under covered arc reversals, qNML treats equivalent DAGs identically.
- Core assumption: Arc reversal preserving V-structures does not alter the parent set decomposition used in the score.
- Evidence anchors:
  - [abstract] "In contrast to the closely related factorized normalized maximum likelihood criterion, qNML satisfies the property of score equivalence."
  - [section 3.1] Theorem 1 proves that a single covered arc reversal preserves the qNML score.
  - [corpus] Weak evidence: neighbor papers discuss NML but not score equivalence in DAGs.
- Break condition: If equivalence classes are misidentified due to incorrect parent set handling, score equivalence fails.

### Mechanism 3
- Claim: qNML is asymptotically consistent with BIC, inheriting its consistency guarantee.
- Mechanism: The local qNML score converges to the local BIC score as sample size grows, matching the leading penalty term.
- Core assumption: The regret approximation error is O(1), negligible compared to the leading log N term.
- Evidence anchors:
  - [section 3.2] Theorem 2 and Lemmas 1 and 2 show the asymptotic equivalence of qNML and BIC penalty terms.
  - [section 4.1] Empirical results show qNML converges to the generating structure as sample size increases.
  - [corpus] Weak evidence: no neighbor papers directly address consistency proofs for qNML.
- Break condition: If the regret approximation is not accurate for the data regimes tested, asymptotic consistency may not hold.

## Foundational Learning

- Concept: Normalized Maximum Likelihood (NML) criterion
  - Why needed here: qNML is built on NML; understanding its definition and properties is essential for grasping the quotient formulation.
  - Quick check question: What is the key difference between NML and Bayesian marginal likelihood in terms of prior specification?

- Concept: Score equivalence in Bayesian network structure learning
  - Why needed here: qNML's score equivalence property is a core advantage over fNML; engineers must recognize when two DAGs are equivalent.
  - Quick check question: Under what condition do two DAGs encode the same independence statements and thus belong to the same equivalence class?

- Concept: Decomposition of scores in Bayesian network structure learning
  - Why needed here: Both qNML and fNML decompose by variable, enabling efficient dynamic programming; understanding this decomposition is key to implementation.
  - Quick check question: How does the decomposition of the marginal likelihood by variable simplify the search over DAG structures?

## Architecture Onboarding

- Component map:
  - Regret approximation module -> Score computation module -> Dynamic programming engine -> Model evaluation module

- Critical path:
  1. Load data and preprocess parent-child configurations.
  2. Compute regret values for all required (N, r) pairs.
  3. For each candidate structure, sum local qNML scores.
  4. Select structure with highest total score.
  5. Evaluate predictive performance and model complexity.

- Design tradeoffs:
  - Accuracy vs speed: Exact regret computation is infeasible; approximation introduces error but enables scalability.
  - Parsimony vs fit: qNML's conservative penalty favors simpler models; may miss subtle dependencies in small samples.
  - Memory vs computation: Precomputing all regrets saves time but increases memory use; can recompute on demand.

- Failure signatures:
  - High variance in predicted log-probabilities across folds: May indicate overfitting or poor regret approximation.
  - qNML selecting much simpler models than BIC in large samples: Could signal overly conservative penalty.
  - qNML scores equal for non-equivalent structures: Suggests a bug in parent set handling or equivalence detection.

- First 3 experiments:
  1. Verify regret approximation accuracy for small N and high r values against exact computation.
  2. Test score equivalence by comparing qNML scores for all arc reversals in a simple DAG.
  3. Benchmark qNML vs BIC and fNML on a small synthetic dataset with known generating structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does qNML perform in high-dimensional settings with many more variables than samples?
- Basis in paper: [inferred] The experiments used relatively small Bayesian networks (n â‰¤ 11) and UCI datasets. The paper mentions qNML's computational efficiency but does not test scalability.
- Why unresolved: The authors focused on moderate-sized networks where exact learning algorithms are feasible. High-dimensional scenarios would require approximate methods and could reveal different performance characteristics.
- What evidence would resolve it: Empirical comparison of qNML against BDeu and fNML on datasets with hundreds of variables and varying sample sizes, using approximate structure learning algorithms.

### Open Question 2
- Question: Does qNML maintain its score equivalence property when learning dynamic Bayesian networks or time series models?
- Basis in paper: [explicit] The paper explicitly discusses score equivalence for static Bayesian networks and proves this property for qNML. It does not extend this analysis to dynamic models.
- Why unresolved: Score equivalence in dynamic models may depend on how time-slice boundaries are handled. The current proof relies on properties specific to static DAG structures.
- What evidence would resolve it: Formal proof of score equivalence for qNML applied to dynamic Bayesian network structures, or demonstration of violations through counterexamples.

### Open Question 3
- Question: What is the theoretical relationship between qNML's model complexity and its sample complexity requirements?
- Basis in paper: [inferred] The paper shows qNML yields simpler models than fNML but doesn't provide theoretical bounds on when qNML converges to the true structure. The consistency proof relies on asymptotic equivalence to BIC.
- Why unresolved: While qNML is proven consistent, the rate of convergence and the sample size needed for reliable structure recovery are not characterized. This affects practical deployment.
- What evidence would resolve it: Sample complexity bounds for qNML showing the relationship between network sparsity, sample size, and probability of correct structure identification, validated through experiments.

## Limitations
- The regret approximation by Szpankowski and Weinberger, while remarkably accurate, is still an approximation that may introduce errors in certain data regimes
- The method's behavior with high-dimensional data (many variables) and variables with high arity is not thoroughly explored
- Computational complexity for exact structure learning remains exponential in the number of variables

## Confidence
- **High confidence**: The theoretical properties of qNML (score equivalence, consistency, decomposability) are well-established through formal proofs
- **Medium confidence**: Empirical results show qNML performs well on benchmark datasets, but the sample size and diversity of datasets could be expanded
- **Medium confidence**: The claim about improved parsimony is supported by experiments, but could benefit from more systematic analysis across different data types

## Next Checks
1. **Approximation accuracy validation**: Systematically compare the Szpankowski-Weinberger approximation against exact regret computation for small datasets to quantify the approximation error
2. **High-arity variable analysis**: Test qNML's performance on datasets with variables having many categories to assess its robustness in these challenging scenarios
3. **Scalability evaluation**: Benchmark qNML's computational performance on larger networks (10+ variables) to understand practical limitations for real-world applications