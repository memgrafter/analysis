---
ver: rpa2
title: 'SELF: Surrogate-light Feature Selection with Large Language Models in Deep
  Recommender Systems'
arxiv_id: '2412.08516'
source_url: https://arxiv.org/abs/2412.08516
tags:
- feature
- selection
- self
- features
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SELF introduces a novel feature selection approach for deep recommender
  systems that leverages Large Language Models (LLMs) to address limitations of traditional
  surrogate-based methods. The key innovation lies in combining semantic reasoning
  from LLMs with task-specific learning from surrogate models.
---

# SELF: Surrogate-light Feature Selection with Large Language Models in Deep Recommender Systems

## Quick Facts
- arXiv ID: 2412.08516
- Source URL: https://arxiv.org/abs/2412.08516
- Reference count: 40
- Primary result: LLM-driven feature selection improves AUC by up to 0.0048 and logloss by 0.0065 over no selection

## Executive Summary
SELF introduces a novel feature selection approach for deep recommender systems that leverages Large Language Models (LLMs) to address limitations of traditional surrogate-based methods. The key innovation lies in combining semantic reasoning from LLMs with task-specific learning from surrogate models. SELF uses an iterative prompting strategy where LLMs generate initial feature importance rankings based on general world knowledge, which are then refined through a bridge network that aligns these rankings with recommendation-specific objectives. Experiments on three public datasets demonstrate that SELF outperforms state-of-the-art baselines and shows strong transferability across different deep recommendation architectures.

## Method Summary
SELF employs an iterative prompting strategy where LLMs generate semantically informed feature importance rankings that capture feature interdependencies missed by traditional surrogate models. These rankings are refined through a bridge network that effectively aligns semantically-derived feature importance with task-specific objectives learned during surrogate model training. The method uses temperature-scaled softmax for weight normalization and incorporates masking strategies to handle uncertainty in feature importance. The final selected features are used to retrain the deep recommender model, achieving improved performance across multiple evaluation metrics.

## Key Results
- Achieved AUC improvements of up to 0.0048 and logloss reductions of 0.0065 compared to no selection
- Demonstrated strong transferability across different deep recommendation architectures (FM, DeepFM, Wide&Deep)
- Industrial deployment showed 0.63% RPM improvement and 3.01% CTR increase in online advertising

## Why This Works (Mechanism)

### Mechanism 1
The LLM generates semantically informed feature importance rankings that capture feature interdependencies missed by traditional surrogate models. LLMs leverage pretrained world knowledge to understand complex feature relationships (e.g., "longitude" and "latitude" must appear together) that statistical models miss due to training data limitations.

### Mechanism 2
The bridge network effectively aligns semantically-derived feature importance with task-specific objectives learned during surrogate model training. The bridge vector learns to weight and combine multiple LLM-generated rankings while masking features to create a unified importance score that balances semantic priors with empirical evidence.

### Mechanism 3
Iterative prompting strategy produces more robust feature importance rankings than single-shot generation. LLMs iteratively select features by considering already-selected features and candidate features, allowing them to build coherent feature subsets rather than making isolated decisions.

## Foundational Learning

- **Feature importance estimation in high-dimensional sparse data**: Recommender systems operate on sparse categorical data where traditional statistical methods struggle to identify truly predictive features. Quick check: How does feature sparsity affect the reliability of traditional importance estimation methods like permutation tests?

- **Bridge network optimization with masking strategies**: The bridge network must learn to integrate multiple LLM rankings while handling the uncertainty of feature importance through selective masking. Quick check: What happens to bridge network performance if the maximum masking ratio is set too high or too low?

- **Temperature-scaled softmax for weight normalization**: Multiple LLM rankings need to be combined with appropriate weighting that reflects their reliability and relevance to the specific task. Quick check: How does temperature parameter affect the sharpness of weight distribution across multiple LLM rankings?

## Architecture Onboarding

- **Component map**: LLM prompt iteration → Bridge network with masking → Feature embedding weighting → Deep recommender model → Retraining
- **Critical path**: LLM ranking generation → Bridge network optimization → Feature selection → Model retraining
- **Design tradeoffs**: Multiple LLM rankings vs. single LLM (robustness vs. efficiency), iterative vs. single-shot prompting (accuracy vs. computational cost), masking ratio (exploration vs. exploitation)
- **Failure signatures**: Unstable bridge weights, degraded performance on test data, high variance across LLM rankings, poor transfer to different backbone models
- **First 3 experiments**:
  1. Compare single LLM ranking vs. multiple LLM ranking with bridge network on a small dataset
  2. Test different maximum masking ratios (0.1, 0.2, 0.5) to find optimal exploration-exploitation balance
  3. Validate transferability by applying selected features to different backbone models (FM, DeepFM, Wide&Deep)

## Open Questions the Paper Calls Out

### Open Question 1
How does the iterative prompting strategy's effectiveness scale with increasing feature dimensionality beyond the tested 96 features in Kuairand dataset? The paper tests on datasets with up to 96 features but does not explore scaling beyond this point, despite mentioning industrial applications with potentially much higher feature counts.

### Open Question 2
What is the optimal number of LLMs to balance performance gains against computational overhead in industrial deployment scenarios? The paper finds K=3 optimal for their test datasets but acknowledges this may not generalize, and notes industrial efficiency demands without quantifying the trade-off.

### Open Question 3
How robust is SELF to different LLM providers and model versions, particularly when using open-source alternatives versus proprietary models like GPT-4? The paper uses GPT-4, GPT-4o, and GPT-3.5 but doesn't test alternative LLM families or examine sensitivity to model updates/changes.

### Open Question 4
What is the theoretical upper bound on performance improvement that SELF can achieve, and at what point do diminishing returns set in for feature selection? The paper demonstrates consistent improvements but doesn't analyze the saturation point where additional feature selection refinement yields negligible gains.

## Limitations
- Reliance on black-box LLM reasoning may not generalize well to domains with highly specialized feature semantics
- Bridge network optimization introduces additional hyperparameters requiring careful tuning
- Computational overhead of iterative LLM prompting could become prohibitive for extremely high-dimensional feature spaces

## Confidence

**High confidence**: Core mechanism of combining LLM semantic reasoning with data-driven importance refinement through the bridge network, supported by strong experimental performance across multiple datasets and architectures.

**Medium confidence**: Generalizability of results, as experiments are limited to three datasets with relatively controlled feature spaces and specific recommendation tasks.

**Low confidence**: Scalability and robustness of iterative prompting strategy for feature spaces exceeding several hundred features or in domains with highly specialized terminology.

## Next Checks

1. Conduct ablation studies systematically removing each component (LLM rankings, bridge network, iterative prompting) to quantify their individual contributions and identify potential redundancy.

2. Test the method on datasets with significantly higher feature dimensionality (500+ features) and evaluate computational overhead and feature selection quality degradation.

3. Perform cross-dataset transferability tests where features selected on one domain are evaluated on completely different recommendation tasks to assess semantic reasoning robustness.