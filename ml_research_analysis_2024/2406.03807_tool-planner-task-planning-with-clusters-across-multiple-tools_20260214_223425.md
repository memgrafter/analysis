---
ver: rpa2
title: 'Tool-Planner: Task Planning with Clusters across Multiple Tools'
arxiv_id: '2406.03807'
source_url: https://arxiv.org/abs/2406.03807
tags:
- toolkit
- step
- call
- apis
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tool-Planner addresses the challenge of efficient task planning
  and tool invocation in tool learning with LLMs. The core method idea involves clustering
  tools based on API functions into toolkits, allowing LLMs to plan across toolkits
  and adjust tools when errors occur.
---

# Tool-Planner: Task Planning with Clusters across Multiple Tools

## Quick Facts
- arXiv ID: 2406.03807
- Source URL: https://arxiv.org/abs/2406.03807
- Reference count: 40
- Key outcome: Tool-Planner achieves 8.8% increase in pass rate and 9.1% increase in win rate compared to DFSDT on ToolBench

## Executive Summary
Tool-Planner addresses the challenge of efficient task planning and tool invocation in tool learning with LLMs by clustering tools based on API functions into toolkits. This allows LLMs to plan across toolkits and adjust tools when errors occur, significantly improving both efficiency and effectiveness in task completion. Experiments demonstrate substantial performance improvements over baseline methods, with reduced planning time while maintaining or improving task success rates.

## Method Summary
Tool-Planner clusters tools using semantic embeddings (SimCSE) and k-means++ clustering to group similar APIs into toolkits. Instead of providing individual API documentation to LLMs, it generates brief toolkit functionality descriptions through in-context learning. The system uses a decision tree representation where toolkits are nodes, enabling efficient search space exploration. When tool errors occur, the LLM first tries other APIs within the same toolkit before resorting to cross-toolkit exploration and re-planning.

## Key Results
- 8.8% increase in pass rate compared to DFSDT baseline
- 9.1% increase in win rate compared to DFSDT baseline
- Significant efficiency improvements with reduced planning time (latency only twice as much vs DFSDT's 6-8 times higher)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-Planner reduces redundant re-planning by clustering similar tools into toolkits and prioritizing in-toolkit exploration before switching toolkits.
- Mechanism: When a tool error occurs, the LLM first tries other APIs within the same toolkit before resorting to generating a new plan across different toolkits. This local exploration preserves the current plan structure and reduces the need for extensive re-planning.
- Core assumption: Tools with similar API functions will have similar failure modes, so retrying within the same toolkit is likely to succeed before resorting to cross-toolkit exploration.
- Evidence anchors: [abstract] "When a tool error occurs, the language model can reselect and adjust tools based on the toolkit." [section 3.3] "When the current API t within a node VT becomes unusable for any reason, we prioritize selecting another available API t′ within the same toolkit."

### Mechanism 2
- Claim: Tool clustering based on API functionality embeddings improves planning efficiency by reducing the planning context size and focusing on toolkit-level decisions.
- Mechanism: Instead of providing the LLM with individual API documentation for planning, Tool-Planner generates brief explanations of toolkit functionalities and uses these as context for planning. This abstraction reduces the complexity of the planning decision space.
- Core assumption: The LLM can effectively reason about toolkit-level functionality descriptions and make appropriate planning decisions without needing to understand individual API details.
- Evidence anchors: [section 3.2] "Unlike previous methods, where the API documentation was directly provided to LLMs, resulting in lengthy contexts... Tool-Planner first utilizes in context learning to generate brief explanations for all API functionalities within the toolkit" [section 4.3] "ReACT+Toolkit significantly improves model performance through tool integration, narrowing the gap with better-performing algorithms"

### Mechanism 3
- Claim: The decision tree representation with toolkits as nodes enables efficient search space exploration by reducing the branching factor compared to API-level search.
- Mechanism: By treating toolkits as nodes rather than individual APIs, Tool-Planner reduces the number of potential branches in the search tree. Each node represents a cluster of similar tools, so the search space grows more slowly as more tools are added.
- Core assumption: The number of distinct tool functionalities (clusters) grows more slowly than the number of individual APIs, so toolkit-level search remains efficient even as the total tool inventory grows.
- Evidence anchors: [section 3.3] "The behavior tree in the toolkit plan could be formalized as G(T) = (VT, ET)" [section 4.3] "Tool-Planner demonstrates a significant efficiency improvement compared to DFSDT... Tool-Planner's latency is only twice as much, while DFSDT's latency is 6-8 times higher"

## Foundational Learning

- Concept: Tool clustering using semantic embeddings
  - Why needed here: To group APIs with similar functionality so the LLM can efficiently explore alternatives within the same functional category when errors occur
  - Quick check question: What clustering algorithm does Tool-Planner use to group tool embeddings, and how does it determine the optimal number of clusters?

- Concept: In-context learning for toolkit functionality descriptions
  - Why needed here: To create concise, meaningful descriptions of what each toolkit can do, which serve as the planning context instead of individual API documentation
  - Quick check question: How does Tool-Planner generate toolkit functionality descriptions from the individual API documentation?

- Concept: Tree search with toolkit-level nodes
  - Why needed here: To represent the solution space where each node is a toolkit rather than an individual tool, enabling efficient exploration with reduced branching
  - Quick check question: What is the difference between in-toolkit exploration and cross-toolkit exploration in the Tool-Planner decision tree?

## Architecture Onboarding

- Component map: User query -> Toolkit functionality descriptions -> LLM planning -> Tool invocation engine -> Error handling -> In-toolkit exploration -> Cross-toolkit exploration -> Success/Failure

- Critical path:
  1. User query → toolkit functionality descriptions generated
  2. LLM plans using toolkit descriptions → initial toolkit plan
  3. Tool invocation begins with first toolkit
  4. On error: try other APIs in same toolkit
  5. If all toolkit tools fail: generate new plan excluding current toolkit
  6. Repeat until success or failure

- Design tradeoffs:
  - Cluster granularity vs planning efficiency: More clusters mean more precise planning but larger search space; fewer clusters mean faster planning but less precise tool selection
  - Toolkit description detail vs LLM context limits: More detailed descriptions help planning but may exceed context window constraints
  - In-toolkit retry limit vs time efficiency: More retries increase success chances but also increase latency

- Failure signatures:
  - High cross-toolkit exploration rate: Indicates poor tool clustering or toolkit functionality descriptions
  - Low pass rate despite good win rate: Suggests the model finds solutions but struggles with execution
  - High latency compared to DFSDT: May indicate too many in-toolkit retries or poor toolkit organization

- First 3 experiments:
  1. Ablation study: Compare Tool-Planner performance with and without tool clustering on G1-Inst dataset
  2. Cluster sensitivity: Test different k values (number of clusters) on pass rate and win rate
  3. LLM comparison: Evaluate Tool-Planner with GPT-4 vs Claude-3 vs smaller models on the same benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of clusters (k) impact the effectiveness of tool clustering, and is there an optimal range for different datasets?
- Basis in paper: [explicit] The paper mentions that the size of k in tool clustering has a crucial impact on overall performance and that the best results were achieved when the average size of a cluster was about 9 for G1-inst. It also notes that performance declined when k was too large or too small.
- Why unresolved: The paper only provides results for one specific dataset (G1-inst) and doesn't explore the impact of k on other datasets or provide a comprehensive analysis of the relationship between k and performance across different scenarios.
- What evidence would resolve it: Conducting experiments with varying k values across all datasets (G1, G2, G3) and analyzing the performance metrics (pass rate, win rate) would provide insights into the optimal k range for different datasets and task types.

### Open Question 2
- Question: How does the choice of text embedding model affect the clustering effectiveness and the final performance of Tool-Planner?
- Basis in paper: [explicit] The paper compares the performance of Tool-Planner using different text embedding models (RoBERTa-base, Contriever, text-embedding-ada-02, SimCSE) and shows that SimCSE achieves the best performance. However, it also notes that the performance of different text embedding models is generally similar.
- Why unresolved: The paper doesn't explore the reasons behind the performance differences between the embedding models or investigate the potential for further improvements by fine-tuning task-specific embeddings.
- What evidence would resolve it: Analyzing the characteristics of the different embedding models, such as their ability to capture semantic relationships and handle domain-specific terminology, could provide insights into why SimCSE performs better. Additionally, experimenting with fine-tuning embeddings on tool-specific data could reveal potential improvements.

### Open Question 3
- Question: How does Tool-Planner perform on smaller language models, and what are the bottlenecks for their tool learning capabilities?
- Basis in paper: [explicit] The paper conducts experiments with Llama-2-13B and shows that while it can generate some complete reasoning results, it fails in most cases due to inadequate understanding of API documentation. However, when used as a planning model, Llama-2-13B achieves reasonably good planning with coarse-grained information.
- Why unresolved: The paper doesn't explore the underlying reasons for the performance differences between Llama-2-13B and larger models like GPT-4, nor does it investigate potential strategies to improve the tool learning capabilities of smaller models.
- What evidence would resolve it: Analyzing the model's understanding of tool documentation and its ability to handle complex API interactions could reveal the specific bottlenecks for smaller models. Experimenting with techniques like few-shot learning or knowledge distillation could potentially improve their tool learning performance.

## Limitations
- Tool-Planner's performance depends heavily on the quality of tool clustering, but the paper does not thoroughly analyze clustering effectiveness across different API domains
- The approach assumes toolkit functionality descriptions generated via in-context learning are sufficient for planning, but no validation is provided that these descriptions capture the necessary details for accurate tool selection
- Cross-toolkit exploration, while less frequent, may still incur significant latency penalties that are not fully characterized in the experimental results

## Confidence
- **High Confidence**: The 8.8% increase in pass rate and 9.1% increase in win rate compared to DFSDT on ToolBench, as this is directly measured and reported with clear methodology
- **Medium Confidence**: The claimed efficiency improvements, as latency comparisons are provided but the factors contributing to speed gains (reduced context, fewer re-plans) are not independently verified
- **Low Confidence**: The generalization of results to APIs beyond the RapidAPI Hub and ToolBench datasets, as no experiments demonstrate performance on novel or domain-specific tools

## Next Checks
1. Conduct ablation studies varying the number of clusters (k) to determine the sensitivity of Tool-Planner's performance to clustering granularity
2. Measure the frequency and latency impact of cross-toolkit exploration to quantify the scenarios where Tool-Planner's efficiency gains break down
3. Test Tool-Planner on a held-out set of APIs not present in the training clustering to evaluate its ability to generalize to unseen tools and tool combinations