---
ver: rpa2
title: Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic
  Data
arxiv_id: '2405.13779'
source_url: https://arxiv.org/abs/2405.13779
tags:
- data
- images
- image
- synthetic
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using text-to-image generative models to create
  synthetic post-disaster images for training robust damage assessment models in low-resource
  domains. The approach leverages mask-based image editing with text prompts to generate
  thousands of synthetic post-disaster images from pre-disaster images.
---

# Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data

## Quick Facts
- arXiv ID: 2405.13779
- Source URL: https://arxiv.org/abs/2405.13779
- Reference count: 40
- Key outcome: Text-to-image synthetic data improves disaster damage assessment in low-resource domains with up to 29% AUPRC improvement

## Executive Summary
This paper proposes using text-to-image generative models to create synthetic post-disaster images for training robust damage assessment models in low-resource domains. The approach leverages mask-based image editing with text prompts to generate thousands of synthetic post-disaster images from pre-disaster images. A two-stage training strategy is used: first pre-training on labeled source domain data, then fine-tuning the last layer on synthetic target domain data. Experiments on xBD and SKAI datasets show significant improvements over source-only baselines in both single-source and multi-source domain adaptation settings.

## Method Summary
The method generates synthetic post-disaster images using MUSE, a transformer-based text-to-image model, to perform mask-based image editing on pre-disaster images. Pre-disaster images are encoded using VQGAN, masked, and combined with text embeddings describing disaster damage. The MUSE model predicts output tokens for masked regions, which are decoded back into post-disaster images. A siamese transformer network is first pre-trained on source domain data, then fine-tuned only on the final layer using synthetic target domain data. This two-stage approach leverages both real source supervision and generated synthetic target supervision.

## Key Results
- Up to 29% improvement in AUPRC over source-only baselines in single-source domain adaptation
- Significant improvements in multi-source domain adaptation settings
- Performance improvements correlate with increased volume of synthetic data
- CLIP-based ranking effectively selects high-quality synthetic images from multiple outputs

## Why This Works (Mechanism)

### Mechanism 1
Text-to-image generative models can create realistic post-disaster images by editing pre-disaster images with text prompts, enabling synthetic supervision for damage assessment. The approach uses MUSE for mask-based image editing where pre-disaster images are encoded, masked, and combined with text embeddings. The generative model predicts output tokens for masked regions, which are decoded back into post-disaster images.

### Mechanism 2
A two-stage training strategy improves target domain performance by leveraging both real source data and synthetic target data. First, a siamese transformer network is pre-trained on labeled source domain data. Then, only the last layer of the network is fine-tuned on synthetic target domain data generated from pre-disaster images using text prompts.

### Mechanism 3
Using a larger volume of synthetic data improves target domain performance. The amount of synthetic data generated is proportional to the number of pre-disaster images in the target domain. More synthetic data provides more diverse training examples and helps the model generalize better to the target domain.

## Foundational Learning

- Concept: Text-to-image generative models
  - Why needed here: The approach relies on using text-to-image generative models to create synthetic post-disaster images from pre-disaster images and text prompts.
  - Quick check question: How do text-to-image generative models like MUSE work, and what are their key components (e.g., text encoder, image tokenizer, transformer-based generation)?

- Concept: Domain adaptation
  - Why needed here: The approach aims to improve model performance on target domains with limited labeled data by leveraging labeled data from source domains and synthetic data generated for the target domain.
  - Quick check question: What are the main challenges in domain adaptation, and how does the two-stage training strategy help address these challenges?

- Concept: Siamese networks
  - Why needed here: The approach uses a siamese transformer network to compare pre-disaster and post-disaster images and predict damage.
  - Quick check question: How do siamese networks work, and why are they suitable for comparing pairs of images in damage assessment tasks?

## Architecture Onboarding

- Component map: VQGAN encoder -> MUSE text-to-image model -> siamese transformer network -> final MLP layer
- Critical path: 1) Generate synthetic post-disaster images using MUSE and text prompts 2) Pre-train siamese transformer network on source domain data 3) Fine-tune last layer of siamese network on synthetic target domain data 4) Evaluate model on target domain test set
- Design tradeoffs: Using larger volume of synthetic data vs. risk of overfitting to synthetic domain; fine-tuning only last layer vs. end-to-end fine-tuning on synthetic data; using pre-trained generative model vs. fine-tuning generative model on aerial imagery
- Failure signatures: Synthetic images have large domain gap with real post-disaster images; model overfits to synthetic domain during fine-tuning; two-stage training strategy does not improve target domain performance
- First 3 experiments: 1) Generate synthetic post-disaster images using MUSE and text prompts, visualize results to ensure images are realistic and match text descriptions 2) Pre-train siamese transformer network on source domain data and evaluate performance on source domain 3) Fine-tune last layer of siamese network on synthetic target domain data and evaluate performance on target domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of synthetic data generated by MUSE compare to real post-disaster imagery in terms of improving model robustness?
- Basis in paper: [explicit] The paper discusses generating synthetic post-disaster images using MUSE and evaluating their effectiveness in improving model robustness.
- Why unresolved: The paper does not provide a direct comparison of the quality and diversity of synthetic data against real post-disaster imagery.
- What evidence would resolve it: A comparative study showing the performance of models trained on synthetic data versus those trained on real post-disaster imagery.

### Open Question 2
- Question: What are the long-term effects of using synthetic data on model performance in real-world disaster scenarios?
- Basis in paper: [inferred] The paper highlights the potential of synthetic data in improving domain robustness but does not explore the long-term implications.
- Why unresolved: The study focuses on immediate improvements in model accuracy without addressing potential degradation or adaptation over time.
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple disaster events over time.

### Open Question 3
- Question: How does the choice of text prompts influence the quality and relevance of the generated synthetic images?
- Basis in paper: [explicit] The paper mentions using a pool of text prompts for generating synthetic images and suggests that prompt quality affects the generated images.
- Why unresolved: The paper does not systematically evaluate how different prompts impact the synthetic data quality.
- What evidence would resolve it: An analysis of the impact of various text prompts on the generated image quality and their effectiveness in training models.

## Limitations
- The quality of synthetic images may not fully capture the complexity of real post-disaster scenarios, potentially limiting model robustness
- The approach requires access to pre-disaster images for the target domain, which may not always be available in real disaster situations
- Performance depends heavily on the quality of text prompts and may require significant manual effort to engineer effective prompts

## Confidence
- High Confidence: The two-stage training strategy is well-established in domain adaptation literature; siamese networks are a standard approach for damage assessment; observed AUPRC improvements are empirically validated
- Medium Confidence: Increasing synthetic data volume improves performance; fine-tuning only the last layer prevents overfitting is reasonable but not rigorously validated
- Low Confidence: Generative model accurately translates text prompts into realistic images is primarily qualitative; generalizability to disasters beyond xBD and SKAI is not thoroughly investigated

## Next Checks
1. Conduct quantitative domain gap analysis between synthetic and real post-disaster images using FID or CLIP similarity scores
2. Perform prompt sensitivity study varying text prompts to understand their impact on model performance
3. Evaluate the approach across a wider range of disaster types and severities not present in xBD and SKAI datasets