---
ver: rpa2
title: Cross-lingual Editing in Multilingual Language Models
arxiv_id: '2401.10521'
source_url: https://arxiv.org/abs/2401.10521
tags:
- table
- language
- dataset
- fine-tuned
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the novel problem of cross-lingual model editing
  (XME), where a fact is updated in one language and the propagation of that update
  is observed across other languages in multilingual language models. The core method
  idea involves using hypernetwork-based model editing techniques (MEND and KE) to
  update model parameters, then evaluating the generability and specificity of the
  edits across languages.
---

# Cross-lingual Editing in Multilingual Language Models

## Quick Facts
- **arXiv ID**: 2401.10521
- **Source URL**: https://arxiv.org/abs/2401.10521
- **Reference count**: 40
- **Primary result**: State-of-the-art model editing techniques perform poorly in cross-lingual settings, particularly between languages from different script families, with fine-tuning often matching or exceeding editing performance.

## Executive Summary
This paper introduces and systematically investigates cross-lingual model editing (XME), where a fact is updated in one language and the propagation of that update is observed across other languages in multilingual language models. The authors evaluate state-of-the-art model editing techniques (MEND and KE) in cross-lingual settings using multilingual encoder-only (mBERT, XLM-R) and decoder-only (mT5) models across 14 languages. The study reveals that cross-lingual editing is significantly more challenging than monolingual editing, with fine-tuning often performing comparably to sophisticated model editing techniques. The research also uncovers distinct patterns in how different model architectures store factual knowledge across layers, with encoder-only models concentrating knowledge in later layers while decoder-only models distribute it more evenly in middle layers.

## Method Summary
The authors propose a comprehensive framework for evaluating cross-lingual model editing by adapting existing monolingual model editing techniques (MEND and KE) to multilingual settings. They systematically update facts in one language and measure how well these edits propagate to other languages through generability and specificity metrics. The evaluation spans 14 languages from Indo-European and Dravidian families, examining various language pairs including same-script (Latin-Latin) and different-script (Latin-Indic) combinations. The study employs both encoder-only models (mBERT, XLM-R) and decoder-only models (mT5), analyzing knowledge storage patterns across different layers and investigating the impact of fine-tuning language on editing performance.

## Key Results
- Cross-lingual editing is significantly more difficult than monolingual editing, with state-of-the-art model editing techniques showing poor performance, especially for language pairs from different script families
- Fine-tuning can perform comparably to or better than model editing techniques in cross-lingual settings, challenging the assumption that sophisticated editing approaches are always superior
- Encoder-only models store factual knowledge primarily in the last layers, while decoder-only models store it more evenly in middle layers, revealing architectural differences in knowledge representation
- The language used for fine-tuning significantly impacts cross-lingual editing performance, with better results when fine-tuning in a language closer to the target language

## Why This Works (Mechanism)
The poor performance of cross-lingual model editing stems from the fundamental challenge of transferring factual updates across languages that may have different tokenization schemes, script representations, and semantic embeddings. Multilingual models encode knowledge in a shared multilingual space, but the alignment between languages is imperfect, particularly for languages from different families or script systems. When editing a fact in one language, the model must propagate this change through the multilingual embedding space to other languages, which becomes increasingly difficult as linguistic distance increases. The finding that fine-tuning can match model editing performance suggests that simple weight updates during training may better preserve cross-lingual alignment than targeted parameter modifications, which can disrupt the delicate multilingual representations.

## Foundational Learning

**Multilingual language models**: Deep learning models trained on multiple languages simultaneously, learning shared representations across languages. Why needed: Essential for understanding how knowledge is stored and transferred across languages. Quick check: Can you name two popular multilingual models and their key architectural differences?

**Model editing techniques**: Methods for updating specific factual knowledge in pre-trained models without full fine-tuning. Why needed: Core methodology for implementing cross-lingual knowledge updates. Quick check: What are the main categories of model editing approaches (e.g., weight-based, hypernetwork-based)?

**Cross-lingual transfer**: The ability of models to apply knowledge learned in one language to another language. Why needed: Fundamental concept underlying cross-lingual editing effectiveness. Quick check: What factors influence cross-lingual transfer effectiveness in multilingual models?

**Layer-wise knowledge storage**: The distribution of factual knowledge across different layers of neural networks. Why needed: Critical for understanding where and how to target edits. Quick check: How does knowledge distribution typically differ between encoder-only and decoder-only architectures?

## Architecture Onboarding

**Component map**: Input sentences → Tokenization → Multilingual embedding layer → Transformer layers (stacked) → Output layer (masked language modeling or sequence generation) -> Cross-lingual evaluation metrics

**Critical path**: The essential evaluation pipeline involves: (1) selecting language pair and editing target, (2) applying model editing technique in source language, (3) generating predictions in target language, (4) measuring generability and specificity of edited knowledge, (5) analyzing layer-wise contributions to editing success

**Design tradeoffs**: The paper implicitly highlights the tradeoff between sophisticated model editing (which may disrupt multilingual alignment) versus simpler fine-tuning (which may better preserve cross-lingual relationships but requires more computational resources). The choice of editing technique must balance precision of updates against preservation of multilingual representations.

**Failure signatures**: Cross-lingual editing failure manifests as inability to generate edited facts in target languages, particularly for distant language pairs or different script families. Poor generability scores indicate the edit didn't propagate, while low specificity suggests the model generates related but incorrect facts. Layer analysis reveals whether failures stem from knowledge being stored in wrong layers or poor cross-lingual alignment.

**Three first experiments**:
1. Replicate monolingual editing evaluation for baseline comparison before attempting cross-lingual editing
2. Test cross-lingual editing between closely related languages (e.g., Spanish-Portuguese) to establish upper bounds on effectiveness
3. Analyze layer-wise knowledge distribution in a multilingual model before and after editing to identify optimal editing layers

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- Narrow empirical scope focusing on a single dataset (WikiData) and limited set of 14 languages from Indo-European and Dravidian families
- Lack of analysis of potential confounds such as token overlap between languages or impact of multilingual pretraining data composition
- Limited architectural diversity with only mBERT, XLM-R, and mT5 evaluated, restricting generalizability to other model architectures

## Confidence

- **High confidence**: Cross-lingual editing is significantly harder than monolingual editing; fine-tuning can match or exceed model editing techniques in cross-lingual settings
- **Medium confidence**: Encoder-only models store knowledge primarily in last layers while decoder-only models store it in middle layers (based on limited architectural diversity)
- **Medium confidence**: The impact of fine-tuning language on cross-lingual editing performance (needs validation with more language families)

## Next Checks

1. Replicate experiments with additional language families (e.g., Sino-Tibetan, Afro-Asiatic, Uralic) to assess cross-lingual editing effectiveness across broader typological diversity

2. Test alternative model editing techniques beyond MEND and KE, including recent hypernetwork-free approaches, to determine if poor cross-lingual performance is technique-specific or inherent to the problem

3. Conduct ablation studies varying the fine-tuning language systematically to map the relationship between source language similarity and cross-lingual editing success