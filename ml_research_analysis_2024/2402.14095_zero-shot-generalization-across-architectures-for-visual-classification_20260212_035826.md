---
ver: rpa2
title: Zero-shot generalization across architectures for visual classification
arxiv_id: '2402.14095'
source_url: https://arxiv.org/abs/2402.14095
tags:
- generalizability
- generalization
- accuracy
- calligraphy
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the relationship between classification
  accuracy and zero-shot generalization across different deep learning architectures.
  Using a minimalist Chinese calligraphy dataset, the authors develop a generalization
  index based on normalized mutual information between cluster assignments of unseen
  class embeddings and ground truth labels.
---

# Zero-shot generalization across architectures for visual classification

## Quick Facts
- arXiv ID: 2402.14095
- Source URL: https://arxiv.org/abs/2402.14095
- Authors: Evan Gerritz; Luciano Dyballa; Steven W. Zucker
- Reference count: 40
- Key outcome: Classification accuracy does not predict zero-shot generalization across different deep learning architectures

## Executive Summary
This paper investigates whether classification accuracy on seen classes correlates with zero-shot generalization to unseen classes across multiple deep learning architectures. Using a minimalist Chinese calligraphy dataset, the authors develop a novel generalization index based on normalized mutual information between cluster assignments of unseen class embeddings and ground truth labels. They fine-tune seven different architectures on 15 classes and measure generalization on 5 unseen classes, revealing that generalization varies significantly across architectures and layers. Crucially, they demonstrate that high classification accuracy (over 95%) does not guarantee better zero-shot generalization, with generalization scores ranging from 0.62 to 0.79 across different architectures.

## Method Summary
The study uses a controlled Chinese calligraphy dataset with 20 classes total (15 seen, 5 unseen). Multiple architectures including ResNet50, ViT, Swin Transformer, PViT, CvT, PoolFormer, and ConvNeXtV2 are fine-tuned on the 15 seen classes. Zero-shot generalization is measured by computing normalized mutual information between cluster assignments of unseen class embeddings and ground truth labels. The researchers systematically examine generalization across different layers within each architecture, revealing non-monotonic relationships between layer depth and generalization performance. Classification accuracy is measured on the seen classes while the generalization index captures performance on unseen classes.

## Key Results
- Classification accuracy does not predict zero-shot generalization capability
- Generalization scores varied from 0.62 to 0.79 across architectures achieving over 95% accuracy
- No monotonic relationship exists between layer depth and generalization performance
- Significant variation in generalization observed across different architectures
- Generalization index based on normalized mutual information provides meaningful measure of zero-shot performance

## Why This Works (Mechanism)
The study's methodology works because it isolates the generalization capability by using a controlled dataset where visual distinctions are clear and unambiguous. The normalized mutual information metric effectively captures how well cluster assignments of unseen classes align with ground truth labels, providing an objective measure of generalization that is independent of classification accuracy on seen classes. By examining multiple architectures and layers systematically, the study reveals that architectural choices and depth impact generalization in complex, non-linear ways that cannot be predicted by accuracy alone.

## Foundational Learning

**Chinese calligraphy dataset design**: The minimalist nature provides clear visual distinctions between classes, eliminating noise and ambiguity that could confound generalization measurements. Quick check: Verify that class separations are visually distinct and unambiguous.

**Normalized mutual information**: This metric quantifies the agreement between predicted cluster assignments and true labels, providing a principled way to measure zero-shot generalization. Quick check: Confirm NMI values range between 0 (random) and 1 (perfect alignment).

**Zero-shot evaluation framework**: Separating seen and unseen classes during training and evaluation isolates generalization capability from memorization. Quick check: Ensure no information leakage between seen and unseen class distributions.

## Architecture Onboarding

**Component map**: Input images -> Backbone architecture -> Feature extraction -> Classification head (for seen classes) -> Clustering on unseen class embeddings -> NMI computation

**Critical path**: Fine-tuning on seen classes -> Feature extraction from intermediate layers -> Clustering of unseen class embeddings -> Normalized mutual information calculation

**Design tradeoffs**: The controlled dataset enables clean measurement but may not reflect real-world complexity; multiple architectures provide breadth but limit depth of analysis per architecture.

**Failure signatures**: Low NMI values indicate poor generalization; inconsistent layer-wise patterns suggest architecture-specific generalization mechanisms; high accuracy with low generalization reveals memorization over understanding.

**First experiments**:
1. Verify baseline classification accuracy on seen classes exceeds 95%
2. Compute NMI for each architecture on unseen classes
3. Compare layer-wise generalization patterns within each architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Highly controlled, minimalist dataset may not capture real-world complexity
- Single dataset limits generalizability of architecture performance rankings
- Focus on Chinese calligraphy may not translate to natural image domains
- No investigation of architectural modifications to improve generalization

## Confidence
- Core finding (accuracy doesn't predict generalization): High
- Architecture performance rankings: Medium
- Layer-wise analysis patterns: Medium-High
- Generalizability to other domains: Medium

## Next Checks
1. Replicate the study using a natural image dataset (e.g., ImageNet subsets) to assess whether the accuracy-generalization disconnect holds in more complex visual domains
2. Test the generalization index with varying numbers of unseen classes to determine sensitivity to class cardinality
3. Evaluate whether fine-tuning strategies or architectural modifications can improve zero-shot generalization independently of classification accuracy