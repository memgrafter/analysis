---
ver: rpa2
title: Improving Graph Neural Networks by Learning Continuous Edge Directions
arxiv_id: '2410.14109'
source_url: https://arxiv.org/abs/2410.14109
tags:
- node
- graph
- edge
- features
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the oversmoothing problem in Graph Neural
  Networks (GNNs) by introducing Continuous Edge Direction (CoED) GNN, which learns
  fuzzy edge directions that enable directed information flow rather than diffusion.
  The key innovation is a novel complex-valued fuzzy Laplacian where real and imaginary
  parts capture information flow in opposite directions between nodes.
---

# Improving Graph Neural Networks by Learning Continuous Edge Directions

## Quick Facts
- arXiv ID: 2410.14109
- Source URL: https://arxiv.org/abs/2410.14109
- Reference count: 40
- Key outcome: CoED achieves test losses of 1.36 on synthetic lattice data, 5.02 on synthetic GRN data, 3.56 on Perturb-seq data, 5.76 on web traffic data, and 2.91 on power grid data

## Executive Summary
This paper addresses the oversmoothing problem in Graph Neural Networks (GNNs) by introducing Continuous Edge Direction (CoED) GNN, which learns fuzzy edge directions that enable directed information flow rather than diffusion. The key innovation is a novel complex-valued fuzzy Laplacian where real and imaginary parts capture information flow in opposite directions between nodes. CoED processes incoming and outgoing neighbor messages separately alongside self-features, and since continuous edge directions are differentiable, they can be learned jointly with GNN weights via gradient-based optimization.

## Method Summary
CoED introduces a complex-valued fuzzy Laplacian matrix where the real part encodes incoming message aggregation and the imaginary part encodes outgoing message aggregation. Edge directions are parameterized as continuous angles θ_ij, allowing gradient-based optimization. The method separately aggregates features from in-neighbors and out-neighbors using asymmetric normalization of the fuzzy Laplacian, then combines these with self-features through an MLP. This architecture prevents oversmoothing by enabling directed information flow rather than symmetric diffusion.

## Key Results
- CoED achieves significantly lower test losses than baseline methods on graph ensemble datasets
- Performance improvements are particularly pronounced on synthetic lattice (1.36) and synthetic GRN (5.02) data
- Theoretical analysis proves CoED's expressiveness matches an extended Weisfeiler-Leman test for directed graphs with fuzzy edges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning continuous edge directions prevents oversmoothing by enabling directed information flow rather than diffusion.
- Mechanism: In undirected graphs, information spreads symmetrically, causing node features to converge toward the mean. By learning directed edge weights, CoED ensures that messages propagate in preferred directions, maintaining feature diversity across layers.
- Core assumption: The task benefits from non-uniform information propagation across the graph.
- Evidence anchors:
  - [abstract] "assign fuzzy edge directions ... to the edges of a graph so that features can preferentially flow in one direction between nodes to enable long-range information transmission"
  - [section 1] "Our key insight for improving the performance of GNNs is to alter the nature of information transmission between nodes from diffusion to flow"
- Break condition: If the optimal flow pattern is uniform or if node features are uninformative, edge direction learning provides no benefit.

### Mechanism 2
- Claim: The complex-valued fuzzy Laplacian separates incoming and outgoing neighbor messages, improving expressiveness.
- Mechanism: The real part of the Laplacian aggregates features from incoming neighbors while the imaginary part aggregates from outgoing neighbors. This separation allows the model to process directional information distinctly, unlike standard Laplacians that mix these contributions.
- Core assumption: Incoming and outgoing information carries different semantic meaning that should be processed separately.
- Evidence anchors:
  - [section 3.2] "the real and imaginary parts represent information flow in opposite directions"
  - [section 3.3] "separately aggregate the features of the in-neighbors and the out-neighbors"
- Break condition: If incoming and outgoing information is semantically equivalent, the separation provides no advantage.

### Mechanism 3
- Claim: Differentiability of continuous edge directions enables joint optimization with GNN parameters.
- Mechanism: Since edge directions are parameterized as angles θ_ij, they can be updated via gradient descent alongside weight matrices. This allows the model to discover optimal information flow patterns during training.
- Core assumption: The optimal edge directions can be discovered through gradient-based optimization.
- Evidence anchors:
  - [abstract] "Since continuous edge directions are differentiable, they can be learned jointly with the GNN weights via gradient-based optimization"
  - [section 3.3] "we define in- and out- edge weight matrices as A← = Re[LF] and A→ = Im[LF], respectively"
- Break condition: If the optimization landscape is too rugged or if edge directions are discrete rather than continuous.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: CoED builds on standard GNN message passing but modifies how information flows between nodes
  - Quick check question: What is the difference between aggregating from neighbors and aggregating from in-neighbors versus out-neighbors?

- Concept: Complex-valued neural networks and Laplacians
  - Why needed here: The fuzzy Laplacian uses complex numbers to encode directional information, requiring understanding of complex-valued operations
  - Quick check question: How do the real and imaginary parts of a complex Laplacian matrix encode different types of information flow?

- Concept: Weisfeiler-Leman graph isomorphism test
  - Why needed here: The theoretical analysis proves CoED's expressiveness matches an extended WL test, requiring understanding of this foundational concept
  - Quick check question: What makes the weak form of WL test different from the strong form when applied to directed graphs?

## Architecture Onboarding

- Component map:
  Phase matrix Θ → Fuzzy Laplacian LF → Propagation matrices P←/→ → Message aggregation → Feature update

- Critical path: Θ → LF → P←/→ → message aggregation → feature update → loss → gradients to Θ and GNN parameters

- Design tradeoffs:
  - Continuous vs discrete edge directions: Continuous allows gradient-based optimization but increases parameter count
  - Separate vs combined message processing: Separate improves expressiveness but requires more computation
  - Layer-wise vs shared edge directions: Layer-wise provides flexibility but increases parameters and memory usage

- Failure signatures:
  - Training instability: Edge directions may collapse to degenerate configurations (all edges pointing same direction)
  - No improvement over baselines: Task may not benefit from directional information flow
  - Overfitting on single graph: Learning edge directions on fixed graph with fixed features can overfit

- First 3 experiments:
  1. Node regression on synthetic lattice with known ground truth directions to verify CoED can learn correct edge directions
  2. Compare CoED vs GAT on directed graph with random node features to demonstrate advantage of feature-independent edge learning
  3. Measure Dirichlet energy decay over layers to empirically verify oversmoothing mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CoED's learned edge directions generalize to unseen nodes or graphs when the training data doesn't cover all possible node feature patterns?
- Basis in paper: [inferred] The paper mentions that CoED is particularly effective on graph ensemble data where the graph structure remains fixed but multiple realizations of node features exist. However, it doesn't address how well learned edge directions transfer to new nodes or completely new graphs with similar structure but different feature distributions.
- Why unresolved: The paper focuses on graph ensemble settings where the same graph is used with different feature realizations, but doesn't test scenarios where the model encounters entirely new nodes or graphs during inference.
- What evidence would resolve it: Experiments showing CoED's performance when applied to test graphs with new nodes, or when fine-tuning on a small subset of new nodes while retaining learned edge directions from training.

### Open Question 2
- Question: What is the optimal strategy for balancing the learning of edge directions versus GNN parameters, particularly in terms of learning rates and regularization?
- Basis in paper: [explicit] The paper states that "learning edge directions can make CoED susceptible to overfitting" and mentions using different learning rates for Θ (the phase matrix) and the GNN parameters, but doesn't provide a systematic analysis of how to optimally balance these learning processes.
- Why unresolved: The paper uses different learning rates for edge directions and GNN parameters but doesn't explore the sensitivity of performance to these choices or provide guidelines for optimal hyperparameter tuning.
- What evidence would resolve it: A comprehensive ablation study varying learning rates and regularization strength for edge directions versus GNN parameters, showing how performance changes and identifying optimal settings.

### Open Question 3
- Question: How does the performance of CoED scale with graph size and density, particularly in extremely large graphs with millions of nodes?
- Basis in paper: [inferred] The paper provides runtime analysis comparing CoED to other models, but doesn't systematically investigate how performance (both accuracy and training time) scales with graph size and density across different types of datasets.
- Why unresolved: The runtime analysis shows that CoED has additional computational overhead from computing propagation matrices, but doesn't explore whether this overhead becomes prohibitive for very large graphs or how performance is affected by graph density.
- What evidence would resolve it: Scaling experiments showing CoED's performance and training time across graphs of increasing size (10^3 to 10^6+ nodes) and varying density, with comparison to other scalable GNN methods.

## Limitations
- The empirical evaluation focuses exclusively on graph ensemble settings where the graph structure is fixed but node features vary across realizations
- While the theoretical analysis establishes expressiveness equivalence with an extended WL test, the practical implications for real-world tasks are not empirically validated
- The method introduces additional parameters (edge directions) that scale with graph size, raising questions about scalability and generalization to very large graphs

## Confidence
- **High**: The mechanism of preventing oversmoothing through directed information flow is well-supported by both theoretical analysis and experimental results.
- **Medium**: The claim that learning edge directions is beneficial specifically for graph ensemble data is supported by experiments, but may not generalize to other settings.
- **Medium**: The effectiveness of complex-valued fuzzy Laplacian for separating incoming and outgoing messages is theoretically justified but requires more ablation studies to confirm practical importance.

## Next Checks
1. **Ablation study**: Evaluate CoED performance when using fixed (non-learned) edge directions versus learned directions to quantify the contribution of the learning component.
2. **Single-graph benchmark**: Test CoED on standard single-graph datasets (e.g., citation networks) to assess generalizability beyond graph ensemble settings.
3. **Parameter sensitivity analysis**: Systematically vary the number of layers and hidden dimensions to identify optimal configurations and potential overfitting patterns.