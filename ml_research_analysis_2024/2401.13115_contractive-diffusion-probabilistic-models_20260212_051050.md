---
ver: rpa2
title: Contractive Diffusion Probabilistic Models
arxiv_id: '2401.13115'
source_url: https://arxiv.org/abs/2401.13115
tags:
- score
- diffusion
- dpms
- matching
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes contractive diffusion probabilistic models
  (CDPMs) by introducing contraction properties into the backward sampling process
  of diffusion models. The key idea is that requiring backward processes to be contractive
  can provably narrow score matching and discretization errors.
---

# Contractive Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2401.13115
- Source URL: https://arxiv.org/abs/2401.13115
- Reference count: 40
- Key outcome: Contractive Diffusion Probabilistic Models (CDPMs) provably reduce score matching and discretization errors, achieving state-of-the-art FID (2.47) and IS (10.18) on CIFAR-10 by reusing pretrained weights without retraining.

## Executive Summary
This paper introduces Contractive Diffusion Probabilistic Models (CDPMs) by imposing contraction properties on the backward sampling process of diffusion models. The key insight is that contraction provably narrows score matching errors and discretization errors, making CDPMs robust to both sources of error. The authors theoretically establish Wasserstein bounds for CDPMs, showing better robustness compared to existing diffusion models. Practically, CDPMs can leverage pretrained weights through simple transformations without retraining, achieving state-of-the-art results on multiple datasets.

## Method Summary
CDPMs introduce contraction into the backward sampling process of diffusion models by designing the drift coefficient to satisfy a Lyapunov-type condition. This ensures the backward SDE is contractive, which provably bounds score matching and discretization errors. The method leverages pretrained diffusion model weights by applying a time/space change of variables, transforming VE SDE weights to CDPM weights without retraining. Sampling uses Euler-Maruyama discretization with a predictor-corrector approach. The approach is theoretically grounded with Wasserstein-2 distance bounds and validated empirically across synthetic and real image datasets.

## Key Results
- CDPMs achieve FID score of 2.47 and inception score of 10.18 on CIFAR-10, surpassing all other SDE-based diffusion models
- Experiments show consistent improvement over baseline diffusion models on 1D data, Swiss Roll, MNIST, CIFAR-10, and AFHQ
- CDPMs demonstrate robustness to score mismatch and discretization errors through theoretical Wasserstein bounds
- Pretrained weights can be reused via simple transformations without retraining, enabling immediate performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The backward sampling process of CDPMs is contractive, which provably narrows score matching errors and discretization errors.
- **Mechanism**: By ensuring the infinitesimal generator of the backward SDE satisfies a Lyapunov-type condition, the process converges faster and is less sensitive to perturbations in the score estimate. This leads to tighter Wasserstein bounds with reduced error amplification over time.
- **Core assumption**: The drift coefficient of the backward process is designed to be contractive (i.e., the conditional score function is contractive in the spatial variable).
- **Evidence anchors**:
  - [abstract]: "Our key insight is that, the contraction property can provably narrow score matching errors and discretization errors, thus our proposed CDPMs are robust to both sources of error."
  - [section 3.1]: Theorem 2 and the definition of the practical condition (rb(t) > 0 for all t) formalize how contraction limits error propagation.
  - [corpus]: Weak or missing explicit discussion of contraction theory in related works; most focus on KL or total variation bounds rather than Wasserstein.
- **Break condition**: If the score function is too inaccurate or if the contraction coefficient is insufficient (rb(t) < (L + h)σ²(t)), the benefit of contraction diminishes and errors may still amplify.

### Mechanism 2
- **Claim**: Pretrained diffusion model weights can be reused in CDPMs via a simple transformation without retraining.
- **Mechanism**: By applying a time/space change of variables to the pretrained VE SDE weights, the contractive subVP score function is obtained. This allows immediate transfer of model capacity to CDPM sampling.
- **Core assumption**: The diffusion coefficients of CDPMs are separable in time and space, matching the structure of VE SDEs so the transformation is well-defined.
- **Evidence anchors**:
  - [section 4.2]: Theorem 7 shows the explicit transformation between VE and CsubVP scores, with the formula for τ(t) and f(t) provided.
  - [section 5.3]: Experiments confirm that using pretrained VE weights for CsubVP yields improved FID scores without retraining.
  - [corpus]: Limited discussion of weight reuse in other related works; most propose training from scratch.
- **Break condition**: If the pretrained model's architecture or SDE design differs significantly (e.g., non-separable drift), the transformation may not apply or degrade performance.

### Mechanism 3
- **Claim**: The discretization error in CDPM sampling does not grow with the time horizon T.
- **Mechanism**: The contraction property ensures that the Euler-Maruyama discretization error remains bounded independently of T, as shown by the proof in Theorem 6. This is unlike classical diffusion models where discretization error can grow exponentially.
- **Core assumption**: Assumptions 4 and 5 hold, ensuring Lipschitz continuity of drift, diffusion, and score functions, and that the contraction condition is met.
- **Evidence anchors**:
  - [section 3.2]: Theorem 6 provides the formal bound E|X^T - bX^N|^2 ≤ C√δ, with C independent of T.
  - [section 5.1]: Empirical results show that COU outperforms OU under the same score matching error and discretization step size.
  - [corpus]: Missing explicit discussion of discretization error bounds in most related diffusion works; this is a novel contribution.
- **Break condition**: If the step size δ is too large or the score function is highly non-smooth, the discretization error bound may fail.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs) and time reversal**
  - Why needed here: CDPMs are defined via SDEs, and the key idea relies on the backward (time-reversed) sampling process being contractive.
  - Quick check question: What is the form of the drift and diffusion coefficients in the time-reversed SDE for a forward process with drift b(t,x) and diffusion σ(t)?

- **Concept: Score matching and denoising score matching**
  - Why needed here: The score function ∇ log p(t,x) is learned via score matching, and its accuracy affects the quality of generated samples.
  - Quick check question: How does the denoising score matching objective relate to the explicit score matching objective?

- **Concept: Wasserstein distance and its use in generative modeling**
  - Why needed here: The paper proves Wasserstein bounds for CDPMs, which is important because FID is based on Wasserstein distance and aligns with human judgment.
  - Quick check question: What is the key advantage of using Wasserstein distance over KL or total variation in evaluating generative models?

## Architecture Onboarding

- **Component map**: Forward SDE -> Score network -> Backward SDE (contractive) -> Discretization (Euler-Maruyama) -> Transformation module
- **Critical path**:
  1. Train or load pretrained score network for base SDE
  2. Apply transformation to obtain CDPM score function
  3. Sample from prior using backward SDE with contraction
  4. Discretize and denoise to obtain final sample
- **Design tradeoffs**:
  - Contraction improves robustness but may introduce initialization bias
  - Using pretrained weights avoids retraining but relies on structural compatibility
  - Smaller step size reduces discretization error but increases compute
- **Failure signatures**:
  - High FID or W2 distance despite training → score mismatch or insufficient contraction
  - Mode collapse or poor diversity → overly aggressive contraction or poor score learning
  - Sampling instability → too large step size or inaccurate score function
- **First 3 experiments**:
  1. Verify that backward SDE of COU/CsubVP is indeed contractive by checking rb(t) > 0
  2. Apply weight transformation from VE to CsubVP and confirm score match visually/numerically
  3. Compare FID/W2 between base SDE and CDPM on a small dataset (e.g., MNIST) using same steps

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the exact mechanism by which the contraction property prevents score matching errors from propagating during the backward sampling process?
  - Basis in paper: [explicit] The paper states that contraction prevents score matching errors from expanding over time, but doesn't provide a detailed explanation of the underlying mechanism.
  - Why unresolved: The paper mentions this as a key insight but doesn't delve into the mathematical details of how contraction specifically mitigates score matching errors during the backward process.
  - What evidence would resolve it: A formal proof or detailed mathematical explanation showing how the contraction property bounds the accumulation of score matching errors during the backward sampling process.

- **Open Question 2**: How does the choice of hyperparameters (like βmin and βmax) affect the trade-off between initialization error and robustness to score matching errors in CDPMs?
  - Basis in paper: [explicit] The paper mentions that tuning a moderate level of β is important to let CDPM benefit from contraction while not suffering from initialization error, but doesn't provide specific guidelines or theoretical analysis.
  - Why unresolved: The paper acknowledges the importance of hyperparameter tuning but doesn't offer a systematic approach to balancing the trade-off between initialization error and score matching error robustness.
  - What evidence would resolve it: Empirical studies or theoretical analysis showing how different hyperparameter choices affect the balance between initialization error and score matching error robustness, along with guidelines for optimal selection.

- **Open Question 3**: Can the contraction property be extended to other types of diffusion models beyond SDEs, such as those based on ordinary differential equations (ODEs)?
  - Basis in paper: [inferred] The paper focuses on SDEs and mentions that ODE samplers are not explored, leaving open questions about their performance with CDPMs.
  - Why unresolved: The paper's theoretical framework and experiments are limited to SDE-based diffusion models, leaving uncertainty about the applicability of the contraction concept to other model types.
  - What evidence would resolve it: Development and analysis of contraction properties in ODE-based diffusion models, along with empirical comparisons of their performance against SDE-based CDPMs.

## Limitations

- The weight reuse strategy assumes structural compatibility between pretrained VE models and CDPMs, which may not generalize to all SDE designs or model architectures
- The theoretical benefits depend heavily on hyperparameter choices (diffusion schedule and contraction strength) that require careful tuning
- The approach may face challenges when applied to high-resolution images or non-separable SDEs due to computational constraints

## Confidence

- Theoretical claims around contraction properties and Wasserstein bounds: Medium (proofs are formal but rely on assumptions that may not hold universally)
- Empirical results on synthetic data: High (consistent improvements across multiple datasets)
- Empirical results on real images: Medium (improvements shown but lack ablation studies on contraction strength)
- Weight reuse strategy: Medium (demonstrated successfully but limited to specific SDE structures)

## Next Checks

1. Test the sensitivity of CDPM performance to the contraction parameter rb(t) by varying its lower bound and measuring FID/W2 degradation.
2. Apply the weight transformation to a non-VE pretrained model (e.g., VP or subVP) and assess whether the contraction benefit persists.
3. Evaluate the robustness of CDPM sampling under noisy or approximate score estimates by introducing controlled perturbations to the score network outputs.