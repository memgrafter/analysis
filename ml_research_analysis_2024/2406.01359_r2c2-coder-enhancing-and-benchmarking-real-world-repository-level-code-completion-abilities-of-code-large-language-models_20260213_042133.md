---
ver: rpa2
title: 'R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion
  Abilities of Code Large Language Models'
arxiv_id: '2406.01359'
source_url: https://arxiv.org/abs/2406.01359
tags:
- code
- self
- completion
- context
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in existing repository-level code
  completion methods, which often fail to fully utilize extensive project context
  and are evaluated on benchmarks with limited scenarios. To overcome these issues,
  the authors propose R2C2-Coder, which includes a code prompt construction method
  (R2C2-Enhance) and a challenging benchmark (R2C2-Bench).
---

# R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models

## Quick Facts
- arXiv ID: 2406.01359
- Source URL: https://arxiv.org/abs/2406.01359
- Reference count: 40
- The method achieves higher exact match and edit similarity scores compared to existing methods, particularly in multi-line completion scenarios.

## Executive Summary
This paper addresses limitations in existing repository-level code completion methods, which often fail to fully utilize extensive project context and are evaluated on benchmarks with limited scenarios. The authors propose R2C2-Coder, which includes a code prompt construction method (R2C2-Enhance) and a challenging benchmark (R2C2-Bench). R2C2-Enhance constructs a candidate retrieval pool using abstract and snippet contexts, then generates completion prompts by retrieving relevant contexts for each cursor position. R2C2-Bench is built using this method with a context perturbation strategy to simulate real-world completion scenes. Experiments on multiple benchmarks demonstrate that R2C2-Enhance significantly improves code completion performance, especially after fine-tuning on R2C2-Bench.

## Method Summary
R2C2-Coder enhances repository-level code completion through two main components: R2C2-Enhance and R2C2-Bench. R2C2-Enhance builds a candidate retrieval pool by extracting abstract contexts (coarse-grained global information from ASTs) and snippet contexts (fine-grained local code fragments) from the repository. For each completion cursor position, it constructs a retrieval query from prefix and suffix contexts, retrieves the most relevant contexts using Jaccard similarity, and assembles a completion prompt combining in-file context with retrieved contexts up to 4096 tokens. R2C2-Bench is built using this method with a context perturbation strategy (Q=10%) that randomly discards top R% contexts to simulate real-world scenarios. The method is evaluated by fine-tuning Code LLMs on R2C2-Bench training data and measuring exact match (EM) and edit similarity (ES) performance.

## Key Results
- R2C2-Enhance achieves higher exact match (EM) and edit similarity (ES) scores compared to existing methods on multiple benchmarks
- Fine-tuning on R2C2-Bench with context perturbation (Q=10%) significantly improves model robustness and performance
- The method demonstrates particular effectiveness in multi-line code completion scenarios
- Jaccard similarity provides the best tradeoff between retrieval speed and accuracy for code context retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The abstract context extraction provides coarse-grained global information that improves completion accuracy.
- Mechanism: By using Tree-sitter to generate abstract syntax trees and preserving only declaration nodes (functions, variables, classes), the method captures the global structure of code files without noise from implementation details.
- Core assumption: Coarse-grained global context is more useful than fine-grained local snippets for understanding code structure.
- Evidence anchors:
  - [abstract] "The abstraction context aims to represent the coarse-grained global information of each programming file"
  - [section] "we propose to use the parser generator tool (i.e., Tree-sitter) to extract the abstract context for each file in the repository"
  - [corpus] Weak - no direct comparison of abstract vs snippet effectiveness found in related papers

### Mechanism 2
- Claim: Context perturbation improves model robustness to irrelevant contexts.
- Mechanism: By randomly discarding top-similar contexts during fine-tuning (Q% = 10%), the model learns to handle incomplete or noisy retrieval results that occur in real-world scenarios.
- Core assumption: Real-world retrieval scenarios often include irrelevant or noisy contexts that the model must handle gracefully.
- Evidence anchors:
  - [abstract] "a context perturbation strategy is used to generate more diverse and challenging completion scenes"
  - [section] "we introduce a so-called context perturbation strategy. Specifically, for Q% completion cursor position... we randomly discard top R% contexts"
  - [corpus] Weak - no empirical comparison of perturbed vs non-perturbed models in related work

### Mechanism 3
- Claim: Lexical retrievers like Jaccard similarity offer better efficiency-accuracy tradeoff than neural retrievers.
- Mechanism: Jaccard similarity provides comparable performance to BM25 and UniXCoder while being significantly faster (2.941ms vs 250.9ms per sample).
- Core assumption: Retrieval speed is critical for practical code completion systems where latency matters.
- Evidence anchors:
  - [section] "We find that Jaccard similarity is the best retriever regarding both effectiveness and efficiency"
  - [section] "Table 6 presents the code match performance and retrieval duration of each retriever"
  - [corpus] Weak - no systematic comparison of retrieval strategies in code completion literature

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The method relies on retrieving relevant code contexts from the repository to augment the generation process
  - Quick check question: What is the main difference between traditional RAG and the approach used in R2C2-Enhance?

- Concept: Abstract Syntax Trees (ASTs)
  - Why needed here: ASTs are used to extract abstract contexts that capture the global structure of code files
  - Quick check question: How does an AST differ from the actual source code in terms of information content?

- Concept: Context Window Management
  - Why needed here: The method must manage token limits (N=4096) while incorporating multiple context types
  - Quick check question: What happens when the combined context exceeds the maximum token limit?

## Architecture Onboarding

- Component map: Abstract Context Extractor -> Snippet Context Extractor -> Retrieval Query Builder -> Context Retriever -> Prompt Assembler -> Code LLM

- Critical path: Abstract/Snippet extraction → Retrieval pool construction → Query building → Context retrieval → Prompt assembly → LLM completion

- Design tradeoffs:
  - Abstract vs snippet contexts: global structure vs local details
  - Retrieval method: speed (Jaccard) vs semantic understanding (neural)
  - Context length: comprehensive context vs token limit constraints

- Failure signatures:
  - Poor retrieval results → irrelevant contexts in prompts
  - Excessive token usage → truncated prompts missing important information
  - Overfitting to training perturbations → poor generalization

- First 3 experiments:
  1. Compare EM/ES scores with and without abstract context extraction on R2C2-Bench validation set
  2. Test different perturbation rates (0%, 10%, 20%, 50%) on model robustness
  3. Benchmark retrieval methods (Jaccard vs BM25 vs UniXCoder) for speed and accuracy tradeoffs

## Open Questions the Paper Calls Out
- How does the performance of R2C2-Coder scale when fine-tuning on code repositories in languages not covered by the current four languages (Python, Java, TypeScript, C#)?
- What is the optimal number of context perturbation samples (Q%) for balancing robustness and performance in real-world scenarios?
- How does the quality of retrieved contexts impact code completion performance, and what are the theoretical limits of retrieval-based approaches?

## Limitations
- The method's effectiveness is primarily validated on benchmark datasets that represent a controlled subset of real-world scenarios
- The perturbation strategy's Q=10% rate was chosen based on empirical observation rather than systematic optimization
- The reliance on Tree-sitter for AST extraction may limit applicability to languages with less mature parsing support

## Confidence
- High confidence: The retrieval method's effectiveness (Jaccard similarity) and overall performance improvements on established benchmarks
- Medium confidence: The context perturbation strategy's impact on robustness, as the specific parameters were not systematically optimized
- Medium confidence: The abstract context extraction mechanism's contribution, as the claim of "coarse-grained global information" improving accuracy is supported by results but lacks ablation studies isolating its specific impact

## Next Checks
1. Conduct systematic ablation studies to isolate the contribution of abstract vs snippet contexts to overall performance, testing with different AST extraction granularities
2. Perform extensive perturbation sensitivity analysis by testing a wider range of Q% values (5%, 15%, 25%, 50%) and measuring impact on both training stability and generalization
3. Evaluate retrieval method robustness across diverse codebases with varying stylistic patterns to test the claim that lexical similarity (Jaccard) provides optimal speed-accuracy tradeoff compared to semantic methods