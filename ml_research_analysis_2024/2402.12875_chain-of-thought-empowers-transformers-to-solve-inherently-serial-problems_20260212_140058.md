---
ver: rpa2
title: Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
arxiv_id: '2402.12875'
source_url: https://arxiv.org/abs/2402.12875
tags:
- theorem
- output
- poly
- input
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes why chain-of-thought (CoT) prompting enhances
  reasoning in transformers by providing theoretical understanding through the lens
  of circuit complexity. The authors establish that constant-depth transformers without
  CoT are limited to solving problems in AC0 (a subset of TC0), while with T steps
  of CoT they can solve any problem solvable by circuits of size T, using only constant
  precision and logarithmic embedding size.
---

# Chain of Thought Empowers Transformers to Solve Inherently Serial Problems

## Quick Facts
- arXiv ID: 2402.12875
- Source URL: https://arxiv.org/abs/2402.12875
- Reference count: 40
- This work establishes that chain-of-thought prompting enables transformers to solve inherently serial problems by providing additional serial computation capability that standard transformers lack.

## Executive Summary
This paper provides theoretical understanding of why chain-of-thought (CoT) prompting enhances reasoning in transformers through the lens of circuit complexity. The authors show that constant-depth transformers without CoT are limited to solving problems in AC0 (a subset of TC0), while with T steps of CoT they can solve any problem solvable by circuits of size T. The theory predicts and experiments confirm that CoT dramatically improves accuracy on inherently serial problems including permutation group composition, iterated squaring, and circuit value problems, especially for low-depth transformers.

## Method Summary
The authors establish theoretical bounds on transformer expressiveness using circuit complexity theory, showing that transformers without CoT can only solve AC0 problems while with CoT they can solve problems in SIZE_TC0[1+T] using T steps. They train transformer models of varying depths (1, 2, 3, 4, 6, 9) on synthetic data for modular addition, permutation composition, iterated squaring, and circuit value problems. The models are evaluated in base, cot, and hint settings using Adam optimizer with learning rate 1e-5, weight decay 0, β1=0.9, β2=0.95, and gradient clipping threshold 1.0.

## Key Results
- Transformers without CoT can only solve problems in AC0, a proper subset of TC0
- With T steps of CoT, constant-depth transformers can solve any problem solvable by boolean circuits of size T
- CoT dramatically improves accuracy on inherently serial problems, especially for low-depth transformers
- Finite precision modeling with correct rounding allows realistic expressiveness analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT enables transformers to perform inherently serial computations that standard transformers cannot accomplish with bounded depth
- Mechanism: Each step in CoT simulates one gate operation in a target circuit, writing the output token back to the next input position and resetting the "depth" of intermediate output to 0
- Core assumption: Without CoT, the number of serial computations is bounded by the fixed depth of the transformer; with T steps of CoT, this bound increases to T
- Evidence anchors:
  - [abstract] "Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers"
  - [section] "We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in AC0, a proper subset of TC0"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If T steps of CoT cannot be auto-regressively generated or if the transformer architecture fundamentally changes to allow unbounded depth

### Mechanism 2
- Claim: Finite precision modeling with correct rounding allows realistic expressiveness analysis of transformers
- Mechanism: Replacing infinite-precision operations with their finite-precision counterparts (e.g., rounded addition, inner product, softmax) while maintaining theoretical bounds
- Core assumption: Practical transformers use 16- or 32-bit floating point numbers, making infinite precision assumptions unrealistic
- Evidence anchors:
  - [abstract] "Previous works have shown standard decoder-only transformers...can only express functions computable in an O(1)-parallel run-time with threshold circuits, TC0, without CoT"
  - [section] "Previous works...implicitly assumes when adding more than one floating-point number, the algorithm first computes the exact answer without rounding using arbitrarily more precision and only performs rounding in the end"
  - [corpus] Weak - corpus mentions circuit complexity but not specific finite precision mechanisms
- Break condition: If IEEE 754 standard changes significantly or if transformers move to a different numerical representation

### Mechanism 3
- Claim: Transformer expressiveness increases dramatically when embedding size grows from O(log n) to poly(n)
- Mechanism: Larger embedding size allows storing more information per position, enabling simulation of larger circuits with CoT
- Core assumption: While O(log n) embedding size is necessary for position embeddings, larger embedding sizes provide additional computational power
- Evidence anchors:
  - [abstract] "with T steps of CoT, constant-depth transformers using constant-bit precision and O(log n) embedding size can solve any problem solvable by boolean circuits of size T"
  - [section] "Theorem 3.7. For any T(n) ∈ poly(n), it holds that SIZE_TC0[1+T(n)] = CoT[T(n), poly(n), log n]"
  - [corpus] Moderate - corpus contains related work on transformer expressiveness but not this specific comparison
- Break condition: If theoretical bounds change or if practical limitations prevent scaling embedding size

## Foundational Learning

- Concept: Circuit complexity classes (AC0, TC0, NC, P/poly)
  - Why needed here: The paper uses circuit complexity to formally characterize transformer expressiveness with and without CoT
  - Quick check question: What is the key difference between AC0 and TC0 in terms of allowed gates?

- Concept: Finite precision arithmetic and rounding operations
  - Why needed here: The paper analyzes transformers with realistic floating-point precision rather than idealized infinite precision
  - Quick check question: How does correct rounding differ from truncation in floating-point arithmetic?

- Concept: Automata theory and Krohn-Rhodes decomposition
  - Why needed here: Used to establish upper bounds on transformer expressiveness through ordered automata
  - Quick check question: What makes an automaton "counter-free" and why is this relevant to AC0 circuits?

## Architecture Onboarding

- Component map: Input tokens -> Token embedding -> Position encoding -> Multi-head attention -> Feed-forward -> Output layer
- Critical path: Input tokens → Token embedding → Position encoding → Multi-head attention → Feed-forward → Output layer
- Design tradeoffs:
  - Precision vs expressiveness: Higher precision allows more complex computations but increases resource requirements
  - Depth vs serial computation: Fixed depth limits serial computation without CoT
  - Embedding size vs information capacity: Larger embeddings store more information per position
- Failure signatures:
  - Poor performance on inherently serial problems without CoT
  - Degradation when sequence length exceeds training distribution
  - Numerical instability with very small/large values
- First 3 experiments:
  1. Train a depth-1 transformer on modular addition (C7) with and without CoT to verify parallel vs serial computation benefits
  2. Test permutation composition (S5) with varying transformer depths to observe CoT's impact on inherently serial problems
  3. Implement iterated squaring with different precision levels to study the effect of finite precision on expressiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does allowing exponent bits in floating-point representation fundamentally change the expressiveness of constant-depth transformers with CoT?
- Basis in paper: [explicit] The paper states "Our main theorem would imply that constant-depth transformers with uniform parameters and polynomially many steps of chain of thoughts can solve all problems in P" when discussing uniform versions of CoT, but focuses on fixed-point numbers throughout the main analysis.
- Why unresolved: The paper only proves results for fixed-point numbers (e2(n) = 0) and notes "Allowing exponent bits will only make transformers more expressive" without quantifying this improvement or proving upper bounds.
- What evidence would resolve it: A proof that constant-depth transformers with CoT can solve problems beyond P/poly when using floating-point numbers with exponent bits, or a tighter upper bound showing such transformers remain within P/poly.

### Open Question 2
- Question: What is the exact relationship between the expressiveness of transformers with CoT and uniform circuit complexity classes?
- Basis in paper: [explicit] The paper states "One natural question about non-uniformity is that whether having a different transformer for each input sequence length is practical" and discusses the uniform setting in Appendix G, noting "it is well-known that one can simulate the execution of the Turing Machine for any T steps by a family of uniform boolean circuits of size O(T²)."
- Why unresolved: The paper establishes that non-uniform CoT[T(n), poly(n), log n] equals P/poly, but doesn't fully characterize the uniform version or explore the practical implications of non-uniformity.
- What evidence would resolve it: A proof showing CoT[T(n), poly(n), log n] with uniform parameters equals P exactly, or experimental evidence demonstrating whether non-uniformity is necessary for practical performance.

### Open Question 3
- Question: Can transformers with CoT solve inherently serial problems that require more than polynomial steps of computation?
- Basis in paper: [inferred] The paper shows transformers with polynomially many CoT steps can solve any problem in P/poly, but P/poly is a strict superset of P. The iterated squaring problem is conjectured to require super-polynomial parallel time.
- Why unresolved: While the paper demonstrates transformers can solve P-complete problems like Circuit Value Problem with polynomial CoT steps, it doesn't explore whether CoT can provide exponential computational power for problems requiring inherently exponential computation.
- What evidence would resolve it: Either a proof that transformers with CoT cannot solve problems requiring super-polynomial parallel computation, or an explicit construction showing transformers with exponentially many CoT steps can solve a problem outside P/poly.

## Limitations

- Theoretical bounds rely on idealized assumptions about rounding operations that may not fully capture practical floating-point implementations
- Experimental validation focuses on synthetic problems rather than real-world reasoning tasks
- Practical implications of increasing embedding size from O(log n) to poly(n) for CoT effectiveness are not empirically validated

## Confidence

- Circuit Complexity Characterization: High - The theoretical framework connecting transformers to AC0/TC0 is well-established in prior work and rigorously extended here
- CoT Expressiveness Boost: High - The theoretical argument that T steps of CoT enables solving problems in SIZE_TC0[1+T] is mathematically sound
- Empirical Effectiveness: Medium - Results on synthetic problems support the theory, but real-world applicability remains unproven

## Next Checks

1. Test CoT effectiveness on established reasoning benchmarks (GSM8K, MATH) with varying transformer depths to validate whether theoretical benefits transfer to practical applications

2. Systematically vary floating-point precision (bfloat16, float32, float64) on the iterated squaring task to empirically verify the finite precision bounds claimed in the theory

3. Train transformers with embedding sizes ranging from O(log n) to poly(n) on permutation composition tasks to empirically measure the relationship between embedding capacity and CoT effectiveness