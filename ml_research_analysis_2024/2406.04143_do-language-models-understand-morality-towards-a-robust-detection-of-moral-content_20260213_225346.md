---
ver: rpa2
title: Do Language Models Understand Morality? Towards a Robust Detection of Moral
  Content
arxiv_id: '2406.04143'
source_url: https://arxiv.org/abs/2406.04143
tags:
- moral
- supervised
- values
- unsupervised
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of both unsupervised
  and supervised approaches for detecting moral values in text across different domains.
  The researchers introduce a novel method using the GPT-3-based Davinci model as
  a zero-shot unsupervised multi-label classifier for moral value detection.
---

# Do Language Models Understand Morality? Towards a Robust Detection of Moral Content

## Quick Facts
- arXiv ID: 2406.04143
- Source URL: https://arxiv.org/abs/2406.04143
- Authors: Luana Bulla; Aldo Gangemi; Misael MongiovÃ¬
- Reference count: 24
- One-line primary result: Supervised RoBERTa models outperform unsupervised approaches in detecting moral values across domains, but both struggle with Authority, Purity, and Loyalty dimensions

## Executive Summary
This study investigates the effectiveness of unsupervised and supervised approaches for detecting moral values in text across different domains. The researchers introduce a novel GPT-3-based Davinci model as a zero-shot unsupervised multi-label classifier for moral value detection and compare its performance with a smaller NLI-based zero-shot method. Additionally, they train and evaluate three supervised RoBERTa models on different sub-corpora of the Moral Foundation Reddit Corpus (MFRC) dataset. The results show that supervised models demonstrate higher overall performance, particularly the model trained on the US Politics sub-corpus, achieving F1 scores up to 0.78 in cross-domain evaluation. However, both approaches face challenges in accurately predicting certain moral dimensions like "Authority," "Purity," and "Loyalty," which are difficult to interpret within the dataset's context.

## Method Summary
The study employs both unsupervised and supervised approaches for moral value detection. For unsupervised methods, they use a GPT-3-based Davinci model with prompt-based classification and an NLI-based RoBERTa-large model that treats moral labels as hypotheses evaluated through entailment scoring. For supervised approaches, they train three RoBERTa-large models on different sub-corpora of the MFRC dataset (Everyday Moral Life, US Politics, French Politics) using MultiLabel Soft Margin Loss with learning rate 1e-5, batch size 64, and dropout 0.1. All models are evaluated on cross-domain performance to assess their generalizability across different types of moral discourse.

## Key Results
- GPT-3-based Davinci model achieves F1 scores ranging from 0.59 to 0.69 across different sub-corpora, showing competitive performance with NLI approach
- Supervised RoBERTa models demonstrate higher overall performance, with the US Politics model achieving F1 scores up to 0.78 in cross-domain evaluation
- Both unsupervised and supervised methods struggle with "Authority," "Purity," and "Loyalty" dimensions, indicating challenges in detecting abstract moral concepts
- Cross-domain evaluation reveals that models trained on specific domains show varying degrees of generalization, with US Politics model performing best overall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3-based models leverage indirect prior knowledge from large-scale pretraining to detect moral content without labeled data.
- Mechanism: The model uses semantic embeddings and commonsense knowledge acquired during pretraining to map text inputs to moral dimensions via prompt-based classification.
- Core assumption: The model has been exposed to enough morally framed text to form abstract moral concepts that generalize across domains.
- Evidence anchors: "leverage abstract concepts and common-sense knowledge acquired from Large Language Models"; "The indirect prior knowledge acquired by unsupervised models proves useful for moral detection"
- Break condition: If the pretraining corpus lacks sufficient moral framing, the model's moral reasoning will degrade sharply.

### Mechanism 2
- Claim: NLI-based zero-shot methods outperform smaller datasets by leveraging entailment/neutrality thresholds.
- Mechanism: The model treats moral labels as hypotheses and evaluates the degree of entailment with the input text; high entailment implies moral alignment.
- Core assumption: Entailment scoring captures enough nuance in moral semantics without explicit domain-specific training.
- Evidence anchors: "NLI-based zero-shot model" with "competitive results compared to the Davinci model"; "we employ the checkpoints of MNLI-RoBERTa-large... fine-tuned on the MNLI dataset"
- Break condition: If the moral semantics are too ambiguous or domain-specific, entailment thresholds will produce high false positives/negatives.

### Mechanism 3
- Claim: Supervised RoBERTa models improve performance by learning domain-specific moral representations during fine-tuning.
- Mechanism: Fine-tuning adapts the model's embeddings to the specific linguistic patterns of moral discourse in the training domain, improving cross-domain generalization.
- Core assumption: The training domain provides enough signal to adjust the model's moral reasoning while retaining some cross-domain portability.
- Evidence anchors: "we train and evaluate three supervised RoBERTa models on different sub-corpora"; "training improves value learning by models in a cross-domain context"
- Break condition: If domain-specific patterns are too divergent, cross-domain transfer will fail, and the model will overfit.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: Each text can belong to multiple moral dimensions simultaneously.
  - Quick check question: If a sentence is tagged with both Care and Fairness, how many labels should the model output?
- Concept: Zero-shot learning
  - Why needed here: The study compares models that do not require labeled training data.
  - Quick check question: What is the key difference between zero-shot and few-shot learning in moral detection?
- Concept: Domain adaptation
  - Why needed here: Models must perform well when tested on data from domains different from training.
  - Quick check question: Why might a model trained on US Politics perform poorly on Everyday Moral Life data?

## Architecture Onboarding

- Component map: Data preprocessing -> Tokenizer -> Model (GPT-3, NLI-RoBERTa, or RoBERTa) -> Label classifier -> Evaluation metrics
- Critical path: Input text -> Model inference -> Multi-label scoring -> Thresholding -> Final moral labels
- Design tradeoffs:
  - GPT-3 offers strong general knowledge but is costly and less flexible.
  - NLI-RoBERTa is cheaper but may lack robustness on obscure moral dimensions.
  - RoBERTa supervised models perform best but require domain-specific labeled data.
- Failure signatures:
  - Low recall on Purity/Authority/Loyalty suggests model struggles with abstract moral concepts.
  - Poor cross-domain performance indicates overfitting to training domain patterns.
- First 3 experiments:
  1. Run both GPT-3 and NLI-RoBERTa on a held-out test set to compare zero-shot performance.
  2. Train a supervised RoBERTa model on Corpus A and evaluate on B and C to measure cross-domain robustness.
  3. Analyze confusion matrices for each moral dimension to identify systematic model weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do certain MFT dimensions like "Authority," "Purity," and "Loyalty" exhibit poor performance across both unsupervised and supervised models?
- Basis in paper: [explicit] The paper explicitly notes these dimensions have low F1 scores and are difficult to predict due to their opaque and ambiguous meanings within the dataset's context.
- Why unresolved: The paper identifies the challenge but does not explore the underlying reasons for why these specific moral dimensions are harder to detect compared to others like "Care" and "Fairness."
- What evidence would resolve it: Comparative analysis of linguistic features, semantic complexity, and cultural context differences between easy-to-detect and hard-to-detect moral dimensions could provide insights.

### Open Question 2
- Question: How does the imbalanced nature of the dataset affect the models' ability to predict specific moral values?
- Basis in paper: [explicit] The paper states that the varying number of items assigned to each moral dimension affects supervised models' performance, particularly for "Purity," "Authority," and "Loyalty."
- Why unresolved: While the paper acknowledges the impact of dataset imbalance, it doesn't quantify how much this imbalance contributes to the prediction difficulties or propose methods to mitigate it.
- What evidence would resolve it: Experiments using balanced datasets or oversampling techniques to see if prediction accuracy for underrepresented moral dimensions improves.

### Open Question 3
- Question: Can large language models (LLMs) be effectively used to generate additional data to address labeling imbalance and subjectivity in the dataset?
- Basis in paper: [explicit] The paper suggests investigating the use of LLMs for data generation as future work to mitigate labeling imbalance and subjectivity.
- Why unresolved: This is proposed as future work without any preliminary testing or results to support its feasibility or effectiveness.
- What evidence would resolve it: Implementing LLM-based data generation and evaluating the impact on model performance, especially for underrepresented moral dimensions.

## Limitations
- Unsupervised approaches face significant limitations in detecting abstract moral concepts like Authority, Purity, and Loyalty dimensions
- The relatively small size of the MFRC dataset (16,123 items) and potential label noise may limit model generalization
- Cross-domain evaluation reveals that models struggle when applied to significantly different contexts, suggesting limited transferability of learned moral representations

## Confidence
**High Confidence**: The supervised RoBERTa models' superior performance over unsupervised methods is well-supported by the empirical results.

**Medium Confidence**: The assertion that GPT-3-based Davinci achieves "competitive" performance with NLI approaches is supported by the results, but the specific F1 score ranges indicate that "competitive" may overstate practical utility.

**Low Confidence**: The claim that unsupervised models can "reliably" detect moral content without labeled data is questionable given the poor performance on key moral dimensions and the substantial performance gap compared to supervised methods.

## Next Checks
1. Conduct a detailed analysis of inter-annotator agreement scores specifically for the Authority, Purity, and Loyalty dimensions to determine whether poor model performance reflects dataset limitations or genuine difficulty in detecting these moral concepts.

2. Train a single supervised RoBERTa model on a balanced combination of all three sub-corpora and evaluate its performance across all domains to test whether ensemble training improves cross-domain robustness compared to the single-domain approaches.

3. Conduct an ablation study by training supervised models on increasingly smaller subsets of the MFRC dataset to quantify the relationship between training data size and performance, particularly for the challenging moral dimensions.