---
ver: rpa2
title: 'Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles
  in Complex Scenarios'
arxiv_id: '2407.19760'
source_url: https://arxiv.org/abs/2407.19760
tags:
- legal
- court
- constitutional
- case
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study empirically evaluates GPT-4's alignment with constitutional
  principles using 17 Italian court rulings on bioethics issues. The model was prompted
  to analyze legal arguments from three parties (applicant, state, court) and provide
  its own stance.
---

# Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios

## Quick Facts
- arXiv ID: 2407.19760
- Source URL: https://arxiv.org/abs/2407.19760
- Authors: Camilla Bignotti; Carolina Camassa
- Reference count: 34
- Key outcome: GPT-4 shows consistent progressive alignment bias in analyzing 17 Italian Constitutional Court bioethics cases, favoring applicant positions over conservative state arguments while demonstrating adequate but imperfect legal reasoning capabilities.

## Executive Summary
This study empirically evaluates GPT-4's alignment with constitutional principles using 17 Italian court rulings on bioethics issues. The model was prompted to analyze legal arguments from three parties (applicant, state, court) and provide its own stance. Results show GPT-4 consistently aligns with progressive interpretations, particularly favoring the applicant's position over the state's conservative stance. The model demonstrates adequate ability to summarize legal text and identify relevant constitutional principles, though it tends to oversimplify competing values and shows some limitations in legal reasoning accuracy.

## Method Summary
The researchers collected 17 Italian Constitutional Court rulings on bioethics issues and used contrastive learning to train an embedding model that captures legal argument similarity. GPT-4 was prompted to extract legal arguments from each ruling and state its own position, with alignment measured using cosine distance between model outputs and the three legal parties' positions. The study also tested GPT-4's probabilistic nature through multiple sampling iterations to assess consistency in alignment patterns.

## Key Results
- GPT-4 consistently aligns with progressive legal interpretations, favoring applicant positions over conservative state arguments
- The model demonstrates adequate text summarization and legal argument identification capabilities
- Multiple sampling iterations show mostly consistent alignment patterns, though with some variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's alignment with progressive interpretations stems from training data biases that overrepresent certain legal perspectives.
- Mechanism: The model's training corpus likely contains more progressive legal arguments and decisions, leading to learned associations that favor these interpretations when analyzing complex constitutional scenarios.
- Core assumption: The model's training data contains systematic biases in the representation of legal arguments and interpretations.
- Evidence anchors:
  - [abstract]: "Our experiments reveal a distinct tendency of GPT-4 to favor progressive legal interpretations, underscoring the influence of underlying data biases."
  - [section 4]: "Our experiment shows GPT-4's inclination towards progressive stances, at least in the context of the legal cases examined."
  - [corpus]: Weak evidence - no direct data on training corpus composition found in neighbors.

### Mechanism 2
- Claim: GPT-4 can effectively identify and summarize legal arguments but may oversimplify competing values in complex scenarios.
- Mechanism: The model demonstrates strong text summarization capabilities and legal text comprehension, but its reasoning framework tends to reduce complex multi-value tradeoffs to simpler binary decisions.
- Core assumption: The model's reasoning architecture prioritizes clear, definitive outputs over nuanced representation of competing interests.
- Evidence anchors:
  - [section 2]: "GPT-4 does hallucinate in some cases" and "the model presents its arguments in a somewhat simplistic manner, often overlooking competing values at stake."
  - [section 4]: "From a strictly legal perspective, GPT-4 demonstrates a sufficient capacity of applying constitutional principles" but "shows some limitations on legal reasoning in terms of adequate accuracy and technical structure."
  - [corpus]: Moderate evidence - related work on legal summarization supports this capability.

### Mechanism 3
- Claim: The probabilistic nature of LLMs creates measurable variability in legal alignment while maintaining overall consistent trends.
- Mechanism: Multiple sampling iterations reveal that while individual outputs vary, the aggregate pattern of progressive alignment remains stable across different prompts and case descriptions.
- Core assumption: The LLM's sampling process maintains consistent underlying probability distributions for legal interpretations.
- Evidence anchors:
  - [section 4]: "Table 3 shows how the aggregated cosine distance between the model's opinion and the legal parties varies in different iterations" and "We find that the previously observed pattern is still present and mostly consistent."
  - [section 3]: "Due to this, we assume that this metric can be used to quantify the difference in alignment between different legal interpretations."
  - [corpus]: Limited evidence - no direct studies on LLM probabilistic consistency in legal domains found in neighbors.

## Foundational Learning

- Concept: Constitutional judicial review process
  - Why needed here: Understanding the three-party system (Applicant, State, Court) is essential for interpreting alignment measurements and comparing model outputs to established legal positions.
  - Quick check question: Can you explain the difference between an "unfounded" and "founded" constitutional ruling and how each would affect the alignment measurement?

- Concept: Cosine distance as alignment metric
  - Why needed here: This mathematical measure quantifies the similarity between legal arguments, providing the foundation for comparing model interpretations with established legal positions.
  - Quick check question: If two legal arguments have a cosine distance of 0.1 versus 0.9, what does this tell you about their relative alignment?

- Concept: Contrastive learning for embedding models
  - Why needed here: This technique creates embeddings that meaningfully represent legal argument similarity, enabling the cosine distance measurements to capture actual legal alignment rather than superficial textual similarity.
  - Quick check question: How does contrastive learning differ from standard embedding training when applied to legal argument pairs?

## Architecture Onboarding

- Component map: Legal case collection -> GPT-4 argument extraction -> Manual evaluation -> Contrastive embedding training -> GPT-4 stance generation -> Embedding comparison -> Alignment analysis
- Critical path: The sequence from legal case selection through argument extraction to alignment measurement represents the core experimental flow that must function correctly for valid results.
- Design tradeoffs: Using a single LLM (GPT-4) for both argument extraction and stance generation simplifies methodology but may introduce systematic biases; employing multiple models would provide better validation but increase complexity.
- Failure signatures: Inconsistent alignment patterns across samples, high hallucination rates in argument extraction, or embeddings that fail to capture legal nuance would indicate fundamental methodological issues.
- First 3 experiments:
  1. Test the embedding model on known similar/dissimilar argument pairs to verify it captures legal alignment rather than textual similarity
  2. Run GPT-4 argument extraction on a small subset of cases with legal expert validation to establish quality thresholds
  3. Generate multiple samples for a single case to measure the natural variability in alignment measurements before full-scale analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's alignment vary when presented with cases involving conflicting progressive values (e.g., individual autonomy vs. scientific progress)?
- Basis in paper: [explicit] The paper notes GPT-4 tends to favor progressive interpretations but doesn't adequately balance competing values.
- Why unresolved: The study focused on cases where progressive vs conservative values were clear, not cases where multiple progressive values conflicted.
- What evidence would resolve it: Testing the model on cases involving trade-offs between different progressive values would reveal if it can balance competing moral considerations within a single ideological framework.

### Open Question 2
- Question: Does GPT-4's alignment change when prompted with different formulations of the same legal question?
- Basis in paper: [inferred] The paper tested two prompt versions (full vs shortened descriptions) and found mostly consistent results, but didn't explore prompt variations systematically.
- Why unresolved: The study used only two prompt versions and didn't investigate how minor changes in phrasing might affect the model's legal reasoning and value alignment.
- What evidence would resolve it: Systematic testing with multiple prompt variations while keeping the legal content constant would reveal the model's sensitivity to prompt engineering.

### Open Question 3
- Question: How does GPT-4's performance on legal reasoning tasks compare to human legal experts when evaluated on the same cases?
- Basis in paper: [explicit] The paper notes GPT-4 shows "basic" legal reasoning and requires human supervision, but doesn't provide direct comparison to human performance.
- Why unresolved: The study evaluated GPT-4 against legal standards but didn't benchmark its performance against actual legal professionals on the same tasks.
- What evidence would resolve it: Direct comparison of GPT-4's legal arguments with those of human legal experts on identical cases would establish whether the model's reasoning meets professional standards.

## Limitations
- Small sample size of 17 Italian cases limits generalizability to other legal domains and jurisdictions
- Focus on bioethics issues may not represent the full complexity of constitutional reasoning across different legal areas
- Embedding-based alignment measurements depend on manual labeling quality, introducing potential subjectivity

## Confidence
- GPT-4's progressive alignment pattern: Medium-High (consistent across multiple iterations and cases)
- Embedding model effectiveness for legal argument comparison: Medium (requires validation on broader legal domains)
- Oversimplification of competing values: Low-Medium (based on qualitative observations rather than systematic measurement)

## Next Checks
1. Test the alignment measurement methodology on a separate set of constitutional cases from a different jurisdiction to assess generalizability beyond Italian bioethics rulings.

2. Conduct blind evaluation of GPT-4's legal arguments by practicing constitutional lawyers to validate the embedding-based alignment scores against expert legal judgment.

3. Perform ablation studies varying the prompt structure and context provided to GPT-4 to determine whether observed progressive bias stems from model architecture or prompting methodology.