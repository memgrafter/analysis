---
ver: rpa2
title: ChatGPT as a Solver and Grader of Programming Exams written in Spanish
arxiv_id: '2409.15112'
source_url: https://arxiv.org/abs/2409.15112
tags:
- chatgpt
- question
- programming
- also
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated ChatGPT's ability to solve and grade programming
  exams in Spanish from a first-year computer science course. The model was tested
  on seven questions ranging from basic coding to complex algorithmic reasoning.
---

# ChatGPT as a Solver and Grader of Programming Exams written in Spanish

## Quick Facts
- arXiv ID: 2409.15112
- Source URL: https://arxiv.org/abs/2409.15112
- Authors: Pablo Saborido-Fernández; Marcos Fernández-Pichel; David E. Losada
- Reference count: 4
- Primary result: ChatGPT can solve basic coding tasks but struggles with complex problems and ADT specification questions

## Executive Summary
This study evaluated ChatGPT's ability to solve and grade programming exams in Spanish from a first-year computer science course. The model was tested on seven questions ranging from basic coding to complex algorithmic reasoning. Results showed ChatGPT could solve simple coding tasks but struggled with complex problems and ADT specification questions. As a grader, it consistently overestimated solution quality, assigning high scores even to low-quality work.

## Method Summary
The study used gpt-3.5-turbo via OpenAI API to evaluate ChatGPT on seven Spanish programming exam questions. Two prompt variants were tested: simple and complex. Five student exam responses with varied scores were used to evaluate grading accuracy. A human instructor served as the ground truth grader for both ChatGPT's solutions and its assessment of student work.

## Key Results
- ChatGPT solved simple coding tasks effectively but struggled with complex problems requiring abstract reasoning
- Complex prompting strategies did not improve ChatGPT's performance on programming exams
- ChatGPT consistently overestimated solution quality when grading student work

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can solve basic coding tasks effectively but struggles with complex problems requiring abstract reasoning. The model has been trained on large amounts of programming examples, giving it strong pattern recognition for common coding patterns and syntax, but lacks deeper understanding of complex data structures and abstract concepts. Core assumption: Training data contains sufficient examples of basic coding patterns but limited examples of complex ADT specifications and computational complexity reasoning. Break condition: When tasks require formal specification with proper algebraic notation or deep understanding of computational complexity beyond pattern matching.

### Mechanism 2
Complex prompting strategies do not improve ChatGPT's performance on programming exams. Adding system roles, "take-your-time" instructions, and examples introduces confusion rather than helpful context for the model. Core assumption: The model's internal reasoning process is not enhanced by explicit role specification or demonstration examples. Break condition: When prompts align with the model's internal representation of task structure, or when the model has strong context awareness.

### Mechanism 3
ChatGPT overestimates solution quality when grading student work. The model has difficulty distinguishing between correct and incorrect solutions, assigning high scores even to low-quality work due to its inability to accurately assess human-authored solutions. Core assumption: The model's self-assessment capabilities do not transfer to evaluating others' work, particularly for complex reasoning tasks. Break condition: When grading tasks match exactly what the model can solve itself, or when human oversight is maintained.

## Foundational Learning

- Concept: Abstract Data Types (ADTs) and their formal specification
  - Why needed here: Questions 1 and 3 require understanding of ADT specification with proper algebraic notation, which ChatGPT struggled with
  - Quick check question: What is the difference between syntactic and semantic specification of an ADT?

- Concept: Computational complexity analysis
  - Why needed here: Questions 2, 4, and 7 require reasoning about time complexity, algorithmic strategies, and divide-and-conquer approaches
  - Quick check question: How do you determine the worst-case time complexity of a recursive Fibonacci implementation?

- Concept: Data structures (stacks, queues, lists)
  - Why needed here: Questions 3, 5, and 6 involve implementing functions using various data structures, which ChatGPT performed better on
  - Quick check question: What is the time complexity of accessing an element in a linked list versus an array?

## Architecture Onboarding

- Component map: Exam questions → Prompt generation → API call → Response → Human evaluation → Grade assignment
- Critical path: Question → Prompt generation → API call → Response → Human evaluation → Grade assignment
- Design tradeoffs: Simple prompt vs complex prompt - simplicity proved more effective; single question evaluation vs batch processing - individual evaluation allowed more detailed analysis
- Failure signatures: Overestimation of solution quality in grading; inability to produce formal ADT specifications; confusion with complex computational complexity questions
- First 3 experiments:
  1. Test ChatGPT on basic coding questions with simple prompts to establish baseline performance
  2. Test complex prompting strategies on the same questions to validate the finding that they don't improve performance
  3. Test ChatGPT's grading ability on student solutions for questions it solved well versus questions it struggled with

## Open Questions the Paper Calls Out

### Open Question 1
What specific prompting strategies could improve ChatGPT's performance on ADT specification questions? The paper notes that ChatGPT struggled with Question 1 (syntactic and semantic specification of an ADT) and suggests that more sophisticated prompt engineering could lead to better performance. Systematic testing of various prompt engineering techniques on ADT specification tasks would resolve this.

### Open Question 2
Does ChatGPT's poor performance on complex problems stem from the Spanish language or from a lack of training data for these types of exercises? The paper suggests this as a possibility, noting that the model's struggles might be related to language or training data availability. Comparative studies testing ChatGPT on the same programming problems in multiple languages, along with analysis of training data composition, would resolve this.

### Open Question 3
Why does ChatGPT fail to accurately assess student solutions even for questions it can solve well itself? The paper found that ChatGPT performed well on solving Questions 3 and 5 but drastically failed to assess the quality of student solutions for these same questions. Detailed analysis of ChatGPT's grading patterns and examination of the criteria it uses for assessment would resolve this.

## Limitations
- Single-course scope focusing on one first-year programming course from a specific Spanish university
- Reliance on one instructor's grading as ground truth, introducing potential subjectivity
- Limited sample size of student exams (five responses) for grading evaluation

## Confidence

- High confidence: ChatGPT can solve basic coding tasks but struggles with complex problems requiring abstract reasoning
- Medium confidence: Complex prompting strategies do not improve performance
- High confidence: ChatGPT overestimates solution quality when grading student work
- Medium confidence: ChatGPT is only effective for basic coding assistance

## Next Checks

1. Cross-course validation: Test ChatGPT on programming exams from multiple courses at different academic levels and institutions to verify generalizability of findings

2. Human grader comparison study: Have multiple instructors independently grade both ChatGPT solutions and student work to assess inter-rater reliability and potential bias

3. Model version comparison: Repeat the evaluation using GPT-4 and other advanced models to determine if improved reasoning capabilities address the identified limitations in solving complex problems and grading accuracy