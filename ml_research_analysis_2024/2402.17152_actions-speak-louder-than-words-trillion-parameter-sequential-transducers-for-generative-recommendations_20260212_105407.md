---
ver: rpa2
title: 'Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers
  for Generative Recommendations'
arxiv_id: '2402.17152'
source_url: https://arxiv.org/abs/2402.17152
tags:
- sequential
- hstu
- features
- user
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of scaling deep learning recommendation
  models (DLRMs) to handle vast amounts of user data and complex feature interactions
  in industrial settings. It proposes a new approach called Generative Recommenders
  (GRs) that reformulates recommendation tasks as sequential transduction problems
  within a generative modeling framework.
---

# Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations

## Quick Facts
- arXiv ID: 2402.17152
- Source URL: https://arxiv.org/abs/2402.17152
- Reference count: 40
- Key outcome: Demonstrates a new generative recommender system achieving up to 65.8% improvement in NDCG and scaling to trillion-parameter models

## Executive Summary
This paper addresses the challenge of scaling deep learning recommendation models (DLRMs) to handle vast amounts of user data and complex feature interactions in industrial settings. It proposes a new approach called Generative Recommenders (GRs) that reformulates recommendation tasks as sequential transduction problems within a generative modeling framework. The key innovation is the Hierarchical Sequential Transduction Unit (HSTU), a new encoder architecture designed for high-cardinality, non-stationary streaming data. HSTU outperforms traditional DLRMs and Transformers in both quality and efficiency, achieving up to 65.8% improvement in NDCG on public datasets and 12.4% improvements in online A/B tests. The work demonstrates that GRs can scale to trillion-parameter models while maintaining efficiency, with HSTU being 5.3x to 15.2x faster than FlashAttention2-based Transformers.

## Method Summary
The paper reformulates recommendation problems as sequential transduction tasks by converting heterogeneous user engagement features into unified time series. It introduces the Hierarchical Sequential Transduction Unit (HSTU) as a new encoder architecture that uses pointwise aggregated attention and exploits sparsity for efficiency. The system employs generative training in a streaming setup, reducing computational complexity by an O(N) factor. For inference, it uses the M-FALCON algorithm to handle ranking across many candidates efficiently. The approach scales to trillion-parameter models while maintaining quality improvements over traditional methods.

## Key Results
- HSTU achieves up to 65.8% improvement in NDCG on public datasets compared to traditional DLRMs
- Model quality scales as a power-law with training compute, similar to large language models
- HSTU is 5.3x to 15.2x faster than FlashAttention2-based Transformers
- 12.4% improvement in online A/B tests demonstrates practical effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating ranking and retrieval as sequential transduction tasks enables a unified feature space that captures both categorical and numerical features.
- Mechanism: By sequentializing heterogeneous features (categorical and numerical) into a single time series and using a target-aware formulation, the model can capture numerical features through cross-attention interactions.
- Core assumption: The sequential transduction architecture is sufficiently expressive to approximate the full DLRM feature space as sequence length increases.
- Evidence anchors:
  - [abstract]: "reformulate recommendation problems as sequential transduction tasks within a generative modeling framework"
  - [section 2.1]: "consolidate and encode these features into a single unified time series"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the sequential transduction architecture lacks expressiveness for the feature interactions required by DLRMs.

### Mechanism 2
- Claim: The HSTU encoder design is more efficient than traditional Transformers for recommendation tasks.
- Mechanism: HSTU uses pointwise aggregated attention and sparsity exploitation to reduce computational complexity, achieving 5.3x to 15.2x speedup over FlashAttention2-based Transformers.
- Core assumption: The recommendation dataset characteristics (e.g., sparse user history sequences) can be algorithmically exploited for efficiency gains.
- Evidence anchors:
  - [abstract]: "HSTU outperforms baselines... and is 5.3x to 15.2x faster than FlashAttention2-based Transformers"
  - [section 3.2]: "HSTU adopts a new pointwise aggregated attention mechanism" and "algorithmically increase the sparsity of user history sequences via Stochastic Length (SL)"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the dataset characteristics change such that sparsity is no longer exploitable or pointwise attention is not beneficial.

### Mechanism 3
- Claim: Generative training in a streaming setup reduces computational complexity by an O(N) factor.
- Mechanism: By training in a generative manner and sampling users at rate 1/ni, the total training cost scales as O(N^2d + Nd^2) instead of O(N^3d + N^2d^2).
- Core assumption: The streaming setup allows for efficient amortization of encoder costs across multiple targets.
- Evidence anchors:
  - [abstract]: "this paradigm enables us to systematically leverage redundancies in features, training, and inference to improve efficiency"
  - [section 2.3]: "move from traditional impression-level training to generative training, reducing the computational complexity by an O(N) factor"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the streaming setup changes or the sampling rate cannot be effectively implemented.

## Foundational Learning

- Concept: Sequential transduction tasks
  - Why needed here: The paper reformulates recommendation problems as sequential transduction tasks to unify the feature space and enable generative modeling.
  - Quick check question: How does a sequential transduction task differ from a standard autoregressive setup in language modeling?

- Concept: Attention mechanisms
  - Why needed here: The HSTU encoder design relies on attention mechanisms (pointwise aggregated attention) for efficient and effective feature interactions.
  - Quick check question: What is the difference between pointwise aggregated attention and softmax attention?

- Concept: Generative modeling
  - Why needed here: Generative Recommenders (GRs) are trained in a generative manner to model the joint distribution over suggested contents and user actions.
  - Quick check question: How does generative training differ from discriminative training in the context of recommendation systems?

## Architecture Onboarding

- Component map: User engagement sequences -> Feature sequentialization -> HSTU encoder -> Generative training -> M-FALCON inference

- Critical path:
  1. Sequentialize heterogeneous features into a unified time series
  2. Apply HSTU encoder to process the sequentialized features
  3. Perform generative training in a streaming setup
  4. Use M-FALCON for efficient inference in ranking

- Design tradeoffs:
  - Pointwise aggregated attention vs. softmax attention: Pointwise attention may be more efficient but could lose some global context.
  - Stochastic Length (SL) for sparsity: SL can significantly reduce computational cost but may impact model quality if α is not chosen carefully.
  - Generative training vs. traditional training: Generative training can scale better but may require careful sampling strategies.

- Failure signatures:
  - Poor model quality: Could indicate issues with feature sequentialization, HSTU architecture, or generative training setup.
  - Slow training/inference: Could indicate suboptimal HSTU implementation or M-FALCON algorithm not being utilized effectively.
  - Training instability: Could indicate issues with learning rate, batch size, or the HSTU architecture itself.

- First 3 experiments:
  1. Compare HSTU with standard Transformers on a small-scale dataset to verify efficiency gains.
  2. Test the impact of Stochastic Length (SL) on model quality and computational cost.
  3. Evaluate the generative training approach against traditional training methods in a streaming setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generative training approach with HSTU scale effectively to even longer sequence lengths (e.g., 16,384 tokens or more) without significant degradation in model quality?
- Basis in paper: [explicit] The paper demonstrates effectiveness up to 8,192 sequence length, but scaling beyond this is not explored.
- Why unresolved: The paper only tests HSTU and generative training up to 8,192 sequence length. Scaling to longer sequences introduces new challenges in memory usage, computational cost, and potential quality degradation that are not addressed.
- What evidence would resolve it: Training and evaluating HSTU-based generative recommenders on sequences of 16,384 tokens or longer, measuring quality metrics (NE, HR, NDCG) and computational efficiency compared to shorter sequences.

### Open Question 2
- Question: How does the performance of HSTU-based generative recommenders compare to large language models (LLMs) like GPT-4 when fine-tuned on recommendation tasks?
- Basis in paper: [inferred] The paper shows HSTU-based models outperform traditional DLRMs and scale similarly to LLMs, but does not compare directly to LLM performance on recommendation tasks.
- Why unresolved: While the paper demonstrates the effectiveness of HSTU and generative training for recommendations, it does not explore whether the latest LLMs, when fine-tuned for recommendation tasks, could outperform or complement HSTU-based approaches.
- What evidence would resolve it: Directly comparing the performance of fine-tuned LLMs (e.g., GPT-4) and HSTU-based generative recommenders on the same recommendation tasks and datasets, measuring quality metrics and computational efficiency.

### Open Question 3
- Question: Can the M-FALCON algorithm be further optimized to handle even larger numbers of ranking candidates (e.g., 100,000 or more) without significant loss in throughput?
- Basis in paper: [explicit] The paper demonstrates M-FALCON's effectiveness for up to 16,384 candidates, but does not explore scaling beyond this.
- Why unresolved: The paper shows M-FALCON improves throughput for up to 16,384 candidates, but scaling to larger candidate sets introduces new challenges in memory usage, computational cost, and potential quality degradation that are not addressed.
- What evidence would resolve it: Implementing and evaluating M-FALCON for ranking candidate sets of 100,000 or more, measuring throughput and quality metrics compared to smaller candidate sets.

## Limitations
- Theoretical grounding: The proposed HSTU architecture lacks rigorous theoretical analysis despite empirical success.
- Dataset specificity: Results heavily rely on industrial datasets not publicly available, limiting independent verification.
- Hyperparameter sensitivity: Optimal hyperparameters appear to be found through trial and error, suggesting potential brittleness.

## Confidence
- High confidence: Efficiency improvements of HSTU over Transformers (5.3x-15.2x speedup) are well-supported and verifiable.
- Medium confidence: Quality improvements and power-law scaling claims are supported but rely heavily on industrial datasets.
- Low confidence: Theoretical justification for why HSTU works better than Transformers is limited.

## Next Checks
1. Conduct formal theoretical analysis of HSTU architecture comparing its expressive power and computational complexity to standard Transformers under various sparsity assumptions.
2. Systematically vary key hyperparameters (α for SL, sampling rates, learning rates) across multiple recommendation datasets to quantify sensitivity.
3. Implement the full pipeline on multiple public recommendation datasets with varying characteristics to validate generalization beyond the industrial dataset.