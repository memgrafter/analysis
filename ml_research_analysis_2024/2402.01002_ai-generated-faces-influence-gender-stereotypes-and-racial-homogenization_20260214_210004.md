---
ver: rpa2
title: AI-generated faces influence gender stereotypes and racial homogenization
arxiv_id: '2402.01002'
source_url: https://arxiv.org/abs/2402.01002
tags:
- sdxl
- images
- gender
- race
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined racial and gender biases in Stable Diffusion
  XL, a widely-used text-to-image generative AI model. Researchers developed a novel
  classifier to predict race, gender, and age group with state-of-the-art accuracy,
  then used it to quantify biases across six races, two genders, 32 professions, and
  eight attributes.
---

# AI-generated faces influence gender stereotypes and racial homogenization

## Quick Facts
- **arXiv ID**: 2402.01002
- **Source URL**: https://arxiv.org/abs/2402.01002
- **Reference count**: 40
- **Primary result**: Stable Diffusion XL exhibits significant racial and gender biases in face generation, which can be reduced through targeted debiasing approaches.

## Executive Summary
This study examines racial and gender biases in Stable Diffusion XL, a widely-used text-to-image generative AI model. The researchers developed a novel classifier to predict race, gender, and age group with state-of-the-art accuracy, then used it to quantify biases across six races, two genders, 32 professions, and eight attributes. The analysis revealed significant underrepresentation of non-White races and gender stereotypes, such as associating certain professions with specific genders or races. To address these issues, the team proposed two debiasing solutions: SDXL-Inc, which reduces racial and gender biases across professions and attributes, and SDXL-Div, which increases facial diversity within racial groups. Both solutions outperformed state-of-the-art alternatives, with SDXL-Inc achieving near-equal representation across races and genders. The findings underscore the need to mitigate biases in AI-generated content to prevent the reinforcement of societal stereotypes.

## Method Summary
The researchers developed a three-stage classifier pipeline (MTCNN face detection → VGGFace ResNet-50 embedding → SVM classification) to identify race, gender, and age in images. They used this classifier to analyze biases in Stable Diffusion XL outputs across six racial categories, two genders, 32 professions, and eight attributes. Two debiasing approaches were proposed: SDXL-Inc, which uses LoRA fine-tuning on balanced race-gender-profession datasets to achieve near-equal representation, and SDXL-Div, which uses LoRA fine-tuning on Flickr-Faces-HQ to increase facial diversity within racial groups. Both solutions were evaluated against baseline SDXL outputs and compared to existing state-of-the-art methods.

## Key Results
- SDXL exhibits significant underrepresentation of non-White races and gender stereotypes across professions and attributes
- SDXL-Inc achieves near-equal representation across races and genders through LoRA fine-tuning on balanced datasets
- SDXL-Div increases facial diversity within racial groups by fine-tuning on diverse face datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Stable Diffusion with balanced race/gender prompts per profession reduces representation bias.
- Mechanism: By training separate LoRA adapters for each (race, gender, profession) combination, the model learns to equally represent each demographic group within that profession, overriding prior bias.
- Core assumption: The training dataset is large enough and diverse enough that the LoRA adapters capture generalizable patterns, not just memorization.
- Evidence anchors:
  - [abstract] "SDXL-Inc achieving near-equal representation across races and genders."
  - [section] "We fine-tuned SDXL with each dataset, yielding 12 different sets of weights... The final model (SDXL-Inc) randomly selects one of the 12 sets of weights in response to any given prompt."
  - [corpus] Weak: No corpus evidence for LoRA effectiveness on bias mitigation; this is original to the paper.
- Break condition: If the fine-tuning data itself is biased or too small, the adapters will inherit those biases instead of correcting them.

### Mechanism 2
- Claim: GPT-4 "in-the-loop" prompt injection can balance race and gender in outputs without retraining the model.
- Mechanism: GPT-4 analyzes the prompt and injects missing race/gender descriptors before sending it to Stable Diffusion, forcing balanced representation.
- Core assumption: GPT-4's language understanding is accurate enough to detect missing demographic information and select appropriate descriptors without introducing new stereotypes.
- Evidence anchors:
  - [abstract] "using a preregistered survey experiment, we find evidence that being presented with inclusive AI-generated faces reduces people's racial and gender biases"
  - [section] "GPT-4 would then inject a randomly-selected race (if race was not specified by the user) and/or a random gender (if gender was not specified) into the user-provided prompt"
  - [corpus] Weak: No corpus evidence that GPT-4 is unbiased or effective at this task; relies on author claim.
- Break condition: If GPT-4's own biases cause it to select stereotypical descriptors, the approach will fail or worsen bias.

### Mechanism 3
- Claim: Increasing facial diversity within races (via fine-tuning on Flickr-Faces-HQ) reduces homogenization stereotypes.
- Mechanism: Fine-tuning on a diverse face dataset teaches the model to generate varied facial features for people of the same race, rather than repeating the same stereotypical features.
- Core assumption: The Flickr-Faces-HQ dataset contains sufficient intra-race variation and is labeled accurately by the classifier for effective fine-tuning.
- Evidence anchors:
  - [abstract] "SDXL-Div, which increases facial diversity within racial groups"
  - [section] "This dataset consists of 70,000 high-resolution (1024 × 1024) images of human faces crawled from Flickr with considerable variation in terms of age, race, and image background"
  - [corpus] Weak: No corpus evidence that Flickr-Faces-HQ is unbiased or diverse enough for this purpose.
- Break condition: If the fine-tuning dataset lacks diversity within races or if the classifier mislabels faces, the diversity gain will be limited or incorrect.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: Full fine-tuning of Stable Diffusion is computationally expensive; LoRA allows fine-tuning with fewer parameters while maintaining model performance.
  - Quick check question: If you fine-tune Stable Diffusion with LoRA, do you need to retrain the entire model or just a subset of parameters?

- Concept: CLIP embedding space for image classification
  - Why needed here: The classifier uses CLIP embeddings as input features to predict race, gender, and age, leveraging CLIP's strong cross-modal representation capabilities.
  - Quick check question: What is the dimensionality of CLIP's image embeddings, and why are they useful for face attribute classification?

- Concept: Cosine similarity for diversity measurement
  - Why needed here: Used to quantify facial diversity by measuring how similar generated faces are within the same race group; lower similarity indicates higher diversity.
  - Quick check question: If two face embeddings have a cosine similarity of 0.95, are they very similar or very different?

## Architecture Onboarding

- Component map:
  - Stable Diffusion XL (base model)
  - LoRA adapters (12 per race/gender combination for SDXL-Inc; 6 for SDXL-Div)
  - Race/gender/age classifier pipeline (MTCNN + VGGFace ResNet-50 + SVM)
  - GPT-4 prompt injector (optional, for "in-the-loop" debiasing)
  - Flickr-Faces-HQ dataset (for SDXL-Div fine-tuning)
  - LAION-5B subset (for bias source analysis)

- Critical path:
  1. Generate baseline images with SDXL using neutral prompt
  2. Classify images with pipeline to quantify bias
  3. Fine-tune SDXL with balanced datasets via LoRA
  4. Validate debiased outputs with same classifier
  5. (Optional) Apply GPT-4 prompt injector for additional debiasing

- Design tradeoffs:
  - LoRA vs full fine-tuning: LoRA is faster and uses less memory but may not capture as nuanced behavior
  - Balanced datasets vs real-world data: Balanced datasets ensure representation but may not reflect actual demographic distributions
  - Classifier accuracy vs diversity: Better classifiers enable more precise debiasing but may also reinforce existing biases if imperfect

- Failure signatures:
  - High cosine similarity within races in SDXL-Div outputs indicates insufficient diversity gain
  - Persistent underrepresentation in SDXL-Inc outputs indicates imbalanced training data or insufficient LoRA capacity
  - Classifier confusion between similar races (e.g., Latinx vs White) limits debiasing precision

- First 3 experiments:
  1. Generate 10,000 images with "a photo of a person" prompt using SDXL, classify, and document baseline bias distribution
  2. Fine-tune SDXL with balanced race/gender/profession datasets using LoRA, generate new images, and compare bias reduction metrics
  3. Fine-tune SDXL with Flickr-Faces-HQ dataset using LoRA, generate same-race images, and measure cosine similarity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed debiasing solutions (SDXL-Inc and SDXL-Div) in reducing biases across diverse real-world applications and prompts?
- Basis in paper: [explicit] The paper discusses the effectiveness of the debiasing solutions on a variety of professions and attributes, but acknowledges that further evaluation in real-world scenarios is needed.
- Why unresolved: The evaluation is limited to specific professions and attributes, and the real-world applicability and effectiveness across diverse prompts and contexts are not fully explored.
- What evidence would resolve it: Testing the debiasing solutions on a wider range of real-world prompts and applications, such as social media content, advertising, and news generation, to assess their effectiveness in reducing biases in diverse contexts.

### Open Question 2
- Question: What are the potential unintended consequences of debiasing AI-generated content, and how can they be mitigated?
- Basis in paper: [inferred] The paper does not explicitly discuss potential unintended consequences of debiasing, but it raises concerns about the potential for AI-generated content to reinforce stereotypes and biases.
- Why unresolved: Debiasing efforts may have unintended consequences, such as overcorrection or the introduction of new biases, which are not fully explored in the paper.
- What evidence would resolve it: Conducting thorough evaluations of debiased AI-generated content to identify any unintended consequences and developing strategies to mitigate them.

### Open Question 3
- Question: How can the debiasing solutions be adapted to address biases related to intersectional identities, such as race, gender, and age?
- Basis in paper: [explicit] The paper focuses on race and gender biases but does not explicitly address biases related to intersectional identities.
- Why unresolved: The debiasing solutions may not adequately address biases that arise from the intersection of multiple identities, such as race, gender, and age.
- What evidence would resolve it: Extending the debiasing solutions to explicitly consider intersectional identities and evaluating their effectiveness in reducing biases related to these intersections.

## Limitations

- The classifier, while achieving state-of-the-art accuracy, may not generalize perfectly to AI-generated faces, potentially misidentifying certain demographic groups
- The debiasing approaches were tested primarily on controlled professional prompts, limiting generalizability to real-world usage scenarios
- The evaluation framework focuses on representation metrics but does not directly measure downstream harms or stereotype reinforcement in human viewers

## Confidence

- **High Confidence**: The baseline bias quantification methodology and classifier performance metrics are well-documented and reproducible. The observed underrepresentation of non-White races in SDXL outputs is a measurable, technical finding.
- **Medium Confidence**: The effectiveness of the SDXL-Inc debiasing approach in achieving near-equal representation is supported by the experimental results, though the generalizability to arbitrary prompts remains uncertain. The correlation between inclusive AI-generated faces and reduced human bias (per the preregistered survey) provides suggestive but not definitive evidence.
- **Low Confidence**: The SDXL-Div approach's impact on reducing homogenization stereotypes is supported by cosine similarity metrics, but the practical significance of these improvements for stereotype mitigation is not established. The assumption that GPT-4 prompt injection will consistently select unbiased descriptors is not empirically validated.

## Next Checks

1. Conduct a user study comparing stereotype reinforcement when exposed to original SDXL outputs versus SDXL-Inc outputs across diverse demographic groups, measuring both explicit and implicit bias changes.
2. Test the debiased models (SDXL-Inc and SDXL-Div) on a comprehensive set of real-world prompts from social media or creative platforms to assess robustness beyond controlled professional contexts.
3. Evaluate the race, gender, and age classifier's performance specifically on AI-generated faces across all six racial categories to identify and quantify any systematic misclassification patterns that could undermine debiasing efforts.