---
ver: rpa2
title: 'DiffPO: A causal diffusion model for learning distributions of potential outcomes'
arxiv_id: '2410.08924'
source_url: https://arxiv.org/abs/2410.08924
tags:
- diffusion
- causal
- learning
- should
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffPO, a causal diffusion model for learning
  distributions of potential outcomes. The method addresses the challenge of predicting
  individual treatment effects from observational data by leveraging conditional diffusion
  models and introducing an orthogonal diffusion loss to handle selection bias.
---

# DiffPO: A causal diffusion model for learning distributions of potential outcomes

## Quick Facts
- arXiv ID: 2410.08924
- Source URL: https://arxiv.org/abs/2410.08924
- Reference count: 40
- Primary result: DiffPO achieves state-of-the-art performance in learning potential outcome distributions, with improvements in Wasserstein distance (e.g., 0.043 vs 0.759-1.007 for baselines) and predictive interval coverage (e.g., 98.1% vs 80-89% for baselines).

## Executive Summary
DiffPO introduces a conditional diffusion model for learning distributions of potential outcomes in causal inference. The method addresses selection bias through an orthogonal diffusion loss that ensures Neyman-orthogonality, making it robust to misspecification of nuisance functions like the propensity score. By leveraging denoising diffusion models, DiffPO can capture complex, multi-modal distributions of potential outcomes, providing uncertainty quantification that is crucial for medical decision-making. Experiments across multiple datasets demonstrate significant improvements over state-of-the-art baselines in both Wasserstein distance metrics and predictive interval coverage.

## Method Summary
DiffPO uses a conditional denoising diffusion model with a U-Net backbone to learn distributions of potential outcomes. The method first estimates propensity scores using a separate neural network, then trains the diffusion model with an orthogonal diffusion loss that incorporates inverse propensity weighting. The forward diffusion process gradually adds Gaussian noise to the outcome variable, while the reverse process learns to denoise and reconstruct the distribution. The orthogonal loss ensures that errors in propensity score estimation do not bias the learned outcome distributions. The method can handle various causal quantities beyond potential outcomes, including CATE, by deriving them from the learned distributions.

## Key Results
- Achieved Wasserstein distance of 0.043 on synthetic datasets compared to 0.759-1.007 for baselines
- Achieved 98.1% empirical coverage for 95% predictive intervals compared to 80-89% for baselines
- Consistently outperformed state-of-the-art methods across IHDP, ACIC2016, and ACIC2018 datasets in both point estimation (PEHE) and distributional metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The orthogonal diffusion loss enables Neyman-orthogonality, making the method robust to misspecification of nuisance functions like the propensity score.
- Mechanism: The loss incorporates inverse propensity weighting directly into the diffusion training objective, ensuring that errors in estimating the propensity score do not bias the learned distribution of potential outcomes.
- Core assumption: The propensity score can be estimated separately and treated as a nuisance function that can be perturbed without affecting the consistency of the main estimator.
- Evidence anchors:
  - [abstract]: "we address the selection bias through a novel orthogonal diffusion loss"
  - [section]: "Our orthogonal diffusion loss ensures Neyman-orthogonality, which offers favorable theoretical properties"
  - [corpus]: weak (no corpus evidence found directly supporting this mechanism)
- Break condition: If the propensity score estimation is severely biased or if the treatment assignment mechanism violates overlap assumptions, the orthogonal adjustment may fail to correct for selection bias.

### Mechanism 2
- Claim: Conditional diffusion models can capture complex, multi-modal distributions of potential outcomes better than point-estimate methods.
- Mechanism: The forward and reverse diffusion processes gradually add and remove noise to/from the outcome variable, allowing the model to learn rich, continuous distributions that reflect uncertainty in counterfactual predictions.
- Core assumption: The true distribution of potential outcomes is smooth enough to be learned by a series of Gaussian noise transitions.
- Evidence anchors:
  - [abstract]: "leverage a tailored conditional denoising diffusion model to learn complex distributions"
  - [section]: "the forward diffusion process, we gradually add Gaussian noise to the initial y...produces a sequence of noisy samples"
  - [corpus]: weak (corpus lacks direct evidence for diffusion models in causal inference)
- Break condition: If the potential outcome distributions are highly discontinuous or if there are insufficient data to learn the transitions, the diffusion model may fail to converge or overfit.

### Mechanism 3
- Claim: The method's flexibility allows it to handle different causal quantities (e.g., CATE) without redesigning the model architecture.
- Mechanism: By first learning the conditional outcome distributions for each treatment level, the method can derive other quantities like CATE via simple arithmetic (e.g., difference of means) rather than requiring separate specialized models.
- Core assumption: The learned distributions are consistent estimates of the true conditional outcome distributions.
- Evidence anchors:
  - [abstract]: "Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE)"
  - [section]: "our method is also capable of estimating the CATE τ(x) = E[Y (1) − Y (0) | X = x]"
  - [corpus]: weak (corpus lacks direct evidence for flexible causal estimation via diffusion)
- Break condition: If the conditional distributions are poorly estimated (e.g., due to insufficient coverage of the covariate space), derived quantities like CATE may also be inaccurate.

## Foundational Learning

- Concept: Causal inference with potential outcomes
  - Why needed here: The method builds directly on the Neyman-Rubin potential outcomes framework, which requires understanding that only one of two counterfactuals is observed per unit.
  - Quick check question: What assumption allows us to identify p(Y(a) | X) from observational data?
- Concept: Diffusion models and score matching
  - Why needed here: The model uses denoising diffusion to gradually reconstruct the outcome distribution; understanding how forward/reverse processes work is essential.
  - Quick check question: In a diffusion model, what does the reverse process start from?
- Concept: Orthogonal learning and Neyman-orthogonality
  - Why needed here: The orthogonal diffusion loss is designed to be insensitive to small errors in nuisance function estimation, which is critical for robustness.
  - Quick check question: What property does Neyman-orthogonality guarantee about the influence of nuisance function errors?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Propensity score estimation (gϕ) → Weight computation (wˆπ) → Diffusion model training (fθ) → Sampling for uncertainty quantification
  - Each component is modular; propensity score model is trained first and frozen.
- Critical path:
  - Train propensity score network → Compute sample weights → Train conditional diffusion model with orthogonal loss → Sample from learned distributions → Evaluate coverage / Wasserstein distance
  - Bottleneck: Diffusion sampling steps (100 steps) dominate runtime.
- Design tradeoffs:
  - Accuracy vs. runtime: More diffusion steps → better sample quality but slower inference.
  - Model complexity vs. data size: Larger models capture richer distributions but risk overfitting on small datasets.
  - Propensity score quality vs. robustness: Better propensity estimates improve weighting but the orthogonal loss mitigates some misspecification.
- Failure signatures:
  - Poor coverage in PIs → weight estimation or diffusion model underfitting.
  - High Wasserstein distance → insufficient training steps or overly simplistic network architecture.
  - Unstable training → learning rate too high or β schedule poorly chosen.
- First 3 experiments:
  1. Synthetic dataset with known potential outcomes: test Wasserstein distance vs. baselines.
  2. IHDP semi-synthetic dataset: visualize learned outcome distributions and check coverage.
  3. CATE estimation on ACIC datasets: compare PEHE to S-learner, TARNet, GANITE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiffPO's orthogonal diffusion loss compare in performance to alternative bias-correction methods (e.g., propensity weighting, doubly robust estimators) across different levels of selection bias severity?
- Basis in paper: [explicit] The paper introduces an orthogonal diffusion loss and mentions that it addresses selection bias through Neyman-orthogonality, but does not directly compare its performance to other bias-correction methods
- Why unresolved: The paper demonstrates DiffPO's effectiveness but does not benchmark its orthogonal diffusion loss against other bias-correction techniques, leaving uncertainty about whether this specific approach is superior
- What evidence would resolve it: Direct comparison experiments showing DiffPO's performance with orthogonal diffusion loss versus DiffPO with alternative bias-correction methods (propensity weighting, doubly robust) and against established bias-correction techniques in varying selection bias scenarios

### Open Question 2
- Question: How does DiffPO scale computationally when applied to high-dimensional data (e.g., >100 features) or large datasets (e.g., >100,000 samples) compared to existing CATE estimation methods?
- Basis in paper: [inferred] The paper mentions that diffusion model sampling efficiency could be improved and notes runtime is longer than baselines, but does not provide systematic scalability analysis
- Why unresolved: While the paper demonstrates success on moderate-sized datasets, it lacks quantitative analysis of computational scaling with data dimensionality and sample size, which is critical for practical deployment
- What evidence would resolve it: Runtime and memory usage benchmarks showing DiffPO's scaling behavior with increasing dimensionality (e.g., 10, 50, 100, 500 features) and sample size (e.g., 1K, 10K, 100K samples), compared to baseline methods

### Open Question 3
- Question: What is the impact of misspecification in the propensity score estimation function on DiffPO's final performance, and at what threshold of misspecification does the orthogonal property break down?
- Basis in paper: [explicit] The paper claims Neyman-orthogonality provides robustness to misspecification of nuisance functions, but does not empirically test the limits of this robustness
- Why unresolved: The theoretical property of Neyman-orthogonality is stated, but the practical robustness threshold is unknown - it's unclear how much propensity score misspecification can be tolerated before performance degrades significantly
- What evidence would resolve it: Sensitivity analysis experiments varying the quality of propensity score estimation (e.g., using different model architectures, adding noise, or using intentionally misspecified models) and measuring the corresponding degradation in DiffPO's performance metrics across different levels of misspecification

## Limitations
- The method's superior performance may partly stem from favorable synthetic data generation assumptions rather than genuine robustness to real-world confounding
- The computational cost of 100 sampling steps per inference may limit practical deployment in time-sensitive medical applications
- The lack of comparison to newer foundation model approaches leaves uncertainty about whether diffusion models are truly optimal for this task

## Confidence
- Wasserstein distance improvements: High confidence - multiple datasets show consistent improvements over baselines
- Predictive interval coverage: High confidence - achieved coverage closely matches target levels across experiments
- Causal mechanism robustness: Medium confidence - theoretical claims are sound but real-world validation is limited
- Generalizability to non-medical domains: Low confidence - experiments focused exclusively on medical datasets

## Next Checks
1. Test the method on observational datasets with known selection mechanisms (e.g., selection on observables vs. unobservables) to verify the orthogonal loss actually mitigates selection bias rather than masking it.
2. Conduct ablation studies varying the number of diffusion steps (e.g., 50, 100, 200) to quantify the accuracy-runtime tradeoff and identify the optimal balance for clinical deployment.
3. Compare against recent foundation model approaches (e.g., causal transformers) on the same datasets to determine if diffusion models offer unique advantages or if simpler architectures could achieve similar performance.