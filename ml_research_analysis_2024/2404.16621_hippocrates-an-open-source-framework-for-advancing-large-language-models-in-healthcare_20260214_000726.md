---
ver: rpa2
title: 'Hippocrates: An Open-Source Framework for Advancing Large Language Models
  in Healthcare'
arxiv_id: '2404.16621'
source_url: https://arxiv.org/abs/2404.16621
tags:
- medical
- training
- evaluation
- dataset
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hippocrates, an open-source framework for
  developing large language models (LLMs) in the medical domain. The authors address
  the challenge of building domain-specific LLMs by providing unrestricted access
  to training datasets, codebase, checkpoints, and evaluation protocols.
---

# Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare

## Quick Facts
- arXiv ID: 2404.16621
- Source URL: https://arxiv.org/abs/2404.16621
- Reference count: 40
- Primary result: Open-source framework providing unrestricted access to medical LLM development resources, achieving state-of-the-art performance with 7B models

## Executive Summary
Hippocrates introduces an open-source framework for developing large language models in the medical domain, addressing the challenge of building domain-specific LLMs by providing unrestricted access to training datasets, codebase, checkpoints, and evaluation protocols. The framework introduces Hippo, a family of 7B models fine-tuned from Mistral and LLaMA2 through continued pre-training, instruction tuning, and reinforcement learning from human and AI feedback. These models achieve state-of-the-art performance on medical benchmarks, outperforming existing open medical LLMs and even surpassing models with 70B parameters, thereby democratizing AI research in healthcare.

## Method Summary
The Hippocrates framework follows a three-stage training pipeline: (1) continued pre-training on domain-specific medical text corpora (Medical Guidelines, PMC-Patients, PubMedQA-contexts) using LoRA fine-tuning, (2) supervised fine-tuning with instruction tuning datasets (General Instructions and Evaluation Instructions) to align model responses with medical reasoning patterns, and (3) preference learning using RLAIF (AI-generated feedback) with Direct Preference Optimization (DPO) to align outputs with medical expert preferences. The framework uses LLaMA2 7B and Mistral 7B as base models and evaluates performance using the EleutherAI framework on benchmarks including MedMCQA, MedQA, PubMedQA, and USMLE exams.

## Key Results
- Hippo models outperform existing open medical LLMs by large margins on all medical benchmarks
- 7B parameter models achieve competitive performance, even surpassing models with 70B parameters
- Models demonstrate strong performance in both zero-shot and 5-shot learning scenarios
- RLAIF approach using GPT-4 feedback achieves comparable quality to human medical expert annotations

## Why This Works (Mechanism)

### Mechanism 1
Combining domain-specific continued pre-training with instruction tuning yields higher performance than either alone. Continued pre-training injects medical terminology and concepts into the model's base knowledge, while instruction tuning aligns the model's responses with medical reasoning patterns. This assumes medical knowledge and reasoning capabilities are distinct components that can be separately enhanced and then combined. Evidence shows models improve from 34.4 to 50.3 accuracy when combining these approaches.

### Mechanism 2
Using a general instruction dataset (excluding downstream task data) improves generalization while maintaining performance. By avoiding overlap with downstream task data, the model learns broader medical reasoning patterns rather than task-specific patterns. This assumes task-specific patterns in instruction data can lead to overfitting and reduced generalization. The General dataset contains over 400,000 samples from nine different datasets, excluding data from training or test splits of downstream QA benchmarks.

### Mechanism 3
RLAIF (AI-generated feedback) can effectively replace human medical expert annotations while maintaining quality. GPT-4 can be prompted to evaluate medical responses using the same criteria as human medical experts, achieving comparable annotation quality at lower cost. This assumes GPT-4 can understand and apply medical expert evaluation criteria consistently. Analysis revealed GPT-4's performance falls within the range of human expert variability.

## Foundational Learning

- Concept: Domain adaptation through continued pre-training
  - Why needed here: Medical terminology and concepts differ significantly from general language, requiring specialized knowledge injection
  - Quick check question: What is the purpose of continued pre-training on medical text corpora before instruction tuning?

- Concept: Instruction tuning for task alignment
  - Why needed here: Raw medical knowledge must be translated into appropriate response formats for medical question-answering
  - Quick check question: How does instruction tuning differ from continued pre-training in terms of model objectives?

- Concept: Preference learning for quality alignment
  - Why needed here: Ensures model outputs align with medical expert preferences and clinical decision-making processes
  - Quick check question: What is the difference between RLAIF and traditional human feedback in preference learning?

## Architecture Onboarding

- Component map:
  - Data pipeline: Medical Guidelines → PMC-Patients → PubMedQA-contexts → General Instructions → Evaluation Instructions → Medical Preference Data
  - Model pipeline: Base LLM → Continued Pre-training → Supervised Fine-tuning → Preference Learning → Evaluation
  - Evaluation pipeline: EleutherAI LM-Evaluation-Harness → Log-Likelihood scoring → Benchmark comparison

- Critical path: Base LLM → Continued Pre-training → Supervised Fine-tuning → Evaluation
  - Each stage must complete successfully before proceeding to the next
  - Data quality and preprocessing are critical at each stage

- Design tradeoffs:
  - Model size vs. performance: 7B models achieve competitive results but may lack reasoning depth of larger models
  - Data breadth vs. specificity: General instruction datasets improve generalization but may reduce task-specific performance
  - Annotation cost vs. quality: RLAIF reduces costs but requires validation against human experts

- Failure signatures:
  - Low continued pre-training impact: Base model already has strong medical knowledge or pre-training data quality issues
  - Instruction tuning plateau: Instruction dataset quality issues or misalignment with downstream tasks
  - Preference learning degradation: Preference dataset quality issues or misalignment with medical expert criteria

- First 3 experiments:
  1. Baseline evaluation: Test base LLM (Mistral 7B or LLaMA2 7B) on medical benchmarks to establish zero-shot performance
  2. Continued pre-training impact: Fine-tune base LLM on medical corpus and evaluate performance improvement
  3. Instruction tuning optimization: Test different instruction datasets (general vs evaluation) to identify optimal approach

## Open Questions the Paper Calls Out

### Open Question 1
How do the different pre-training datasets (Medical Guidelines, PMC-Patients, PubMedQA-contexts) individually and collectively contribute to the performance of Hippo models on downstream medical tasks? While the paper mentions that the combination of PubMedQA-train and Medical Guidelines yields the highest scores, it does not provide a detailed analysis of how each dataset individually impacts performance or the specific characteristics of each dataset that make them effective. A comprehensive ablation study isolating the effects of each dataset on model performance, along with an analysis of the unique features and distributions of each dataset, would provide clarity on their individual contributions.

### Open Question 2
What is the optimal balance between domain-specific knowledge injection through continued pre-training and general instruction tuning for achieving superior performance in medical LLMs? The paper explores the impact of continued pre-training followed by instruction tuning and notes that the combination of CP and SFT improves performance, but the impact varies depending on the base LLM. The paper does not provide a clear guideline on the optimal balance between CP and SFT for different types of medical tasks or base models, leaving uncertainty about the best approach for various scenarios. Systematic experiments varying the extent and order of CP and SFT across different base models and medical tasks, with detailed performance analysis, would help determine the optimal balance for various scenarios.

### Open Question 3
How does the choice of preference learning method (e.g., DPO vs. RLHF) affect the alignment of medical LLMs with clinical preferences and their overall performance? The paper introduces a preference learning approach using RLAIF and DPO, noting that DPO results in modest improvements for certain tasks but a slight decrease in overall performance. The paper does not explore alternative preference learning methods like RLHF or provide a comprehensive comparison of their effects on model alignment and performance, leaving questions about the best approach for medical LLMs. Comparative studies evaluating the impact of different preference learning methods (e.g., DPO, RLHF) on model alignment and performance across various medical tasks would provide insights into the most effective approach.

## Limitations

- Data provenance and licensing concerns regarding unrestricted access to medical data for continued pre-training
- Evaluation methodology relies heavily on multiple-choice benchmarks that may not capture clinical reasoning capabilities
- RLAIF validation scope limited to specific comparison without detailed methodology for establishing human expert variability

## Confidence

- High confidence: Basic framework architecture is clearly specified and follows established practices
- Medium confidence: Claims about 7B models surpassing 70B parameters require independent verification
- Low confidence: Assertions about democratizing healthcare research are aspirational and not empirically validated

## Next Checks

1. Independent replication: Replicate the Hippocrates framework using only publicly available components to verify the "unrestricted access" claim and assess practical reproducibility barriers

2. Clinical validation study: Conduct a study comparing Hippo model outputs against actual clinical decisions from practicing physicians on real patient cases, rather than multiple-choice benchmarks alone

3. Generalization assessment: Test the models on out-of-distribution medical scenarios and longitudinal performance to evaluate whether high benchmark scores translate to robust real-world medical reasoning