---
ver: rpa2
title: 'Learning mental states estimation through self-observation: a developmental
  synergy between intentions and beliefs representations in a deep-learning model
  of Theory of Mind'
arxiv_id: '2407.18022'
source_url: https://arxiv.org/abs/2407.18022
tags:
- beliefs
- https
- learning
- others
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates a developmental synergy between learning
  to predict others' intentions and explicitly representing their beliefs. Using a
  simple feed-forward deep learning model in a gridworld environment, the authors
  show that predicting others' beliefs improves intention prediction performance earlier
  in learning compared to a model that only predicts intentions.
---

# Learning mental states estimation through self-observation: a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind

## Quick Facts
- arXiv ID: 2407.18022
- Source URL: https://arxiv.org/abs/2407.18022
- Reference count: 40
- Primary result: A 1.89% maximum gain in intention prediction accuracy when learning beliefs representation through self-observation

## Executive Summary
This paper proposes that learning to explicitly represent others' beliefs through self-observation can improve intention prediction in artificial agents. Using a simple feed-forward deep learning model in a gridworld environment, the authors demonstrate a developmental synergy where simultaneously learning belief prediction and intention prediction yields better performance than learning intention prediction alone. The approach, called "like-them," generates self-beliefs during task execution and uses these as training data for predicting others' beliefs, showing particular advantages when targets are not visible to actors or when distractors are ignored.

## Method Summary
The method involves two main components: a Social Perception System (SPS) with a shared prediction network trunk and separate prediction heads for target location, next action, next state, and beliefs; and an Agent Control System (ACS) that generates behavior using POMDP planning with Monte Carlo tree search. Actors' trajectories are generated in 11x11 gridworld environments with varying maps, and two architectures are compared: Beliefs SPS (with explicit belief prediction) and NoBeliefs SPS (without). The model is trained on datasets of varying sizes (5-300 maps) using Adam optimizer and linear combination of losses from all prediction heads.

## Key Results
- Maximum 1.89% gain in target prediction accuracy (69.45% vs 67.57%) when training on 25 maps
- Up to 14% relative gain in performance when target is not visible to actor
- Improved generalization to actors with different cognitive and physical abilities
- Beliefs representations particularly helpful when actors ignore distractors

## Why This Works (Mechanism)

### Mechanism 1
The "like-them" self-observation approach enables the model to bootstrap belief representation by treating its own belief-driven behaviors as training data for predicting others' beliefs. The agent generates its own beliefs during task execution using a POMDP planner with Bayesian filtering, then uses these self-generated belief-action pairs as supervisory signals to train the Social Perception System (SPS) to predict others' beliefs from their trajectories.

### Mechanism 2
Multi-task learning creates a developmental synergy where simultaneously learning belief prediction and intention prediction improves both tasks, particularly in early learning stages. The shared prediction network trunk processes trajectory data, while separate heads predict target location, next action, next state, and beliefs. The regularization effect of joint training causes learning progress in one task to accelerate learning in the other.

### Mechanism 3
Belief prediction provides particular advantage in partially observable environments where the target is not visible to the actor, as these situations create belief-driven exploratory behaviors that are otherwise difficult to predict. When the target is hidden, actors engage in information-gathering behaviors driven by their beliefs about target location, and the belief prediction head learns to recognize these distinctive patterns.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The actors use POMDP planning with Bayesian filtering to generate belief representations that drive their behavior, which the observer must then predict
  - Quick check question: What is the difference between the agent's state and its belief about the state in a POMDP framework?

- Concept: Multi-task learning and regularization
  - Why needed here: Understanding how joint training of belief and intention prediction creates synergistic learning effects that improve performance
  - Quick check question: How does the regularization effect of multi-task learning differ from simple data augmentation?

- Concept: Self-supervised learning through self-observation
  - Why needed here: The core mechanism relies on using one's own belief-action pairs as training data for predicting others' beliefs
  - Quick check question: What are the key assumptions required for self-observation to serve as valid training data for predicting others' behaviors?

## Architecture Onboarding

- Component map: Input trajectory data -> spatialization concatenation -> shared ResNet trunk -> separate prediction heads (target, action, state, beliefs) -> loss computation and backpropagation
- Critical path: Input trajectory data → spatialization concatenation → shared ResNet trunk → separate prediction heads (target, action, state, beliefs) → loss computation and backpropagation
- Design tradeoffs: The architecture trades computational complexity (adding belief prediction head) for improved performance, particularly in challenging scenarios. The choice of a simple feed-forward network over more complex architectures like LSTMs or transformers prioritizes simplicity and computational efficiency.
- Failure signatures: Performance degradation occurs when self-other similarity breaks down, when training data is insufficient, or when the environment becomes too complex for the simple architecture to capture relevant patterns. Overfitting can occur with very limited training data.
- First 3 experiments:
  1. Compare Beliefs vs NoBeliefs architectures with varying training dataset sizes (5, 25, 50, 100 maps) to establish baseline performance and measure synergy effects
  2. Test performance under partial observability conditions where target is hidden vs visible to measure belief prediction advantage
  3. Evaluate generalization to actors with different cognitive/physical capabilities by varying POMCP sample counts and action speeds

## Open Questions the Paper Calls Out

### Open Question 1
How does the "like-them" assumption perform in more complex environments with larger state spaces and longer action sequences? The paper notes that experiments were conducted in a relatively simple 11x11 gridworld environment and that further validation in more complex and realistic settings is needed to increase transferability to real-life scenarios.

### Open Question 2
What is the precise developmental trajectory of the synergy between intention and belief prediction in human infants and children? The paper suggests that beliefs processing becomes increasingly important for understanding others' behaviors with increasing experience until reaching a plateau, but the exact developmental trajectory is not specified.

### Open Question 3
How does the "like-them" approach generalize to multi-agent interactions where agents have different and potentially conflicting goals? The paper tests generalization to actors with different cognitive and physical abilities, but does not explore scenarios with multiple agents having conflicting goals.

## Limitations

- Modest performance gains (1.89% maximum) may not justify the added complexity in all applications
- Strong assumption of self-other similarity may not hold in diverse agent populations or real-world scenarios
- Computational simplicity claims need validation across more complex environments and agent architectures

## Confidence

**High Confidence**: The basic experimental methodology and reported performance differences are well-documented and reproducible. The architectural design and training procedures are clearly specified.

**Medium Confidence**: The interpretation of synergy effects and developmental claims requires additional validation. While the performance improvements are statistically measurable, the cognitive plausibility of the self-observation mechanism in human development needs stronger empirical support.

**Low Confidence**: Claims about the computational simplicity and general applicability of the approach to real-world Theory of Mind tasks are not well-supported by the current evidence. The leap from gridworld environments to complex social reasoning remains largely speculative.

## Next Checks

1. **Scaling Test**: Evaluate the Beliefs architecture's performance gains when scaling to larger gridworlds (15x15 or 20x20) and more complex environments with dynamic obstacles or multiple interacting agents to test computational efficiency claims.

2. **Transfer Validity**: Test whether self-generated belief-action pairs from one agent type (e.g., slow-moving actor) can effectively train belief prediction for qualitatively different agent types (e.g., fast-moving actors) to validate the self-observation mechanism's robustness.

3. **Generalization Benchmark**: Apply the approach to a different Theory of Mind task, such as predicting others' false beliefs in modified Sally-Anne scenarios or collaborative planning tasks, to assess whether the synergy generalizes beyond target prediction.