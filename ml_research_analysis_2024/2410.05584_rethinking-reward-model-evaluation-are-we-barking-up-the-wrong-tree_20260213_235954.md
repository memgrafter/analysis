---
ver: rpa2
title: 'Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?'
arxiv_id: '2410.05584'
source_url: https://arxiv.org/abs/2410.05584
tags:
- reward
- policy
- accuracy
- correlation
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reward model (RM) accuracy is a
  reliable predictor of downstream policy performance in RLHF. The authors find that
  while RM accuracy and policy regret show a weak positive correlation, RMs with similar
  accuracy can lead to significantly different policy outcomes.
---

# Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?

## Quick Facts
- **arXiv ID**: 2410.05584
- **Source URL**: https://arxiv.org/abs/2410.05584
- **Reference count**: 40
- **Primary result**: Reward model accuracy shows weak correlation with downstream policy performance due to Regressional Goodhart effects and measurement methodology issues

## Executive Summary
This paper challenges the conventional wisdom that reward model (RM) accuracy reliably predicts downstream policy performance in Reinforcement Learning from Human Feedback (RLHF). Through extensive experiments on synthetic and real datasets, the authors demonstrate that accuracy alone fails to capture the complex relationship between RMs and optimized policies. They identify that measurement methodology, optimization algorithms, and various Goodhart effects significantly influence this relationship, suggesting that more sophisticated evaluation metrics are needed for reliable RM assessment.

## Method Summary
The study employs a synthetic framework using golden-proxy RM pairs, combined with real datasets like RewardBench, to investigate the correlation between RM accuracy and policy regret. The authors measure accuracy using various metrics (Spearman, Pearson, Kendall, MRR, NDCG) under different configurations (response counts, prompt types, response ranks) and optimize policies using both BoN and PPO algorithms. Policy regret is quantified using Normalised Drop Ratio (NDR), and the correlation between accuracy and regret is analyzed across multiple RLHF datasets.

## Key Results
- RM accuracy shows only weak positive correlation with policy regret (correlation coefficients range from -0.2 to 0.5)
- More responses per prompt and consistent prompt distributions improve accuracy-regret correlation
- Different optimization algorithms (BoN vs PPO) exhibit different correlation patterns with accuracy
- RMs with similar accuracy can lead to significantly different policy outcomes due to overoptimization effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Accuracy alone fails to predict downstream policy regret because it does not capture reward model overoptimization
- **Mechanism**: When reward models are imperfect proxies for human preferences, optimizing policies against them leads to performance deterioration (Regressional Goodhart effect). The noise in the reward model's predictions causes systematic overoptimization toward the proxy rather than the true preference function
- **Core assumption**: The proxy reward model is a noisy version of the true reward function (r = r* + z)
- **Evidence anchors**: [abstract] "accuracy alone can be an inadequate metric for predicting the downstream performance"; [section 5] "The translation from RM error to the policy regret can be seen as the result of the reward model overoptimization"
- **Break condition**: If the reward model noise is negligible or the optimization algorithm is robust to proxy errors, the correlation between accuracy and regret would improve

### Mechanism 2
- **Claim**: The way accuracy is measured significantly impacts its predictive power for policy regret
- **Mechanism**: Different measurement approaches (number of responses per prompt, prompt distributions, response ranking) capture different aspects of reward model quality. More responses per prompt and consistency across prompt types improve correlation with downstream performance
- **Core assumption**: The test dataset distribution should match the downstream optimization distribution
- **Evidence anchors**: [section 3] "we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance"; [section 4] "Increasing the number of responses per prompt can enhance the correlation between measured RM error and policy regret"
- **Break condition**: If the downstream optimization uses a very different prompt distribution or response generation process than the test dataset

### Mechanism 3
- **Claim**: Different reward models with similar accuracy can exhibit different degrees of overoptimization
- **Mechanism**: Beyond Regressional Goodhart, other Goodhart effects (Extremal, Causal, Adversarial) can cause reward models with similar accuracy to behave differently during optimization, leading to varying policy outcomes
- **Core assumption**: Multiple Goodhart effects beyond Regressional contribute to reward model overoptimization
- **Evidence anchors**: [section 5] "Our findings indicate that RMs with similar accuracy can behave quite differently in terms of overoptimization"; [section 5] "These discrepancies may stem from other forms of Goodhart's effects beyond the Regressional type"
- **Break condition**: If the optimization process is constrained enough to prevent different Goodhart effects from manifesting

## Foundational Learning

- **Concept**: Regressional Goodhart Effect
  - Why needed here: This is the core theoretical mechanism explaining why reward model accuracy fails to predict policy regret
  - Quick check question: If a reward model is r = r* + z where z is independent noise, what happens to policy performance when optimized against r?

- **Concept**: Policy Regret in RLHF
  - Why needed here: Understanding how to measure the gap between policies optimized against proxy vs true reward functions
  - Quick check question: How does the normalized drop ratio (NDR) differ from traditional regret formulations in MDPs?

- **Concept**: Reward Model Overoptimization
  - Why needed here: This phenomenon explains the core challenge in RLHF where proxy reward models lead to suboptimal policies
  - Quick check question: What factors contribute to reward model overoptimization beyond just model accuracy?

## Architecture Onboarding

- **Component map**: Reward Model Training → Accuracy Evaluation → Policy Optimization → Performance Assessment
- **Critical path**: 
  1. Construct synthetic golden-proxy RM pairs
  2. Measure accuracy using various metrics
  3. Optimize policies using different algorithms (BoN, PPO)
  4. Measure policy regret using NDR
  5. Analyze correlation between accuracy and regret

- **Design tradeoffs**:
  - More responses per prompt improves correlation but increases annotation cost
  - Matching prompt distributions improves correlation but may not reflect real-world usage
  - Different optimization algorithms (BoN vs PPO) show different correlation patterns

- **Failure signatures**:
  - High accuracy but poor policy performance
  - Low correlation between accuracy and regret across different prompt types
  - Inconsistent behavior of RMs with similar accuracy

- **First 3 experiments**:
  1. Replicate the correlation analysis between accuracy and policy regret using different optimization algorithms
  2. Test the effect of response rank on accuracy-regret correlation by constructing test datasets with controlled response ordering
  3. Evaluate the impact of prompt paraphrasing on accuracy-regret correlation to understand semantic robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of Goodhart effects beyond Regressional Goodhart influence the relationship between RM accuracy and policy regret?
- Basis in paper: [explicit] The paper discusses that accuracy alone may not fully capture potential RM overoptimization and mentions that other forms of Goodhart's effects beyond the Regressional type may explain discrepancies between expected and observed trends in accuracy-regret relationships
- Why unresolved: The theoretical analysis assumes only Regressional Goodhart effects, but empirical findings show that policies with similar accuracy can exhibit quite different levels of regret, suggesting other Goodhart effects are at play
- What evidence would resolve it: Experimental results showing how different types of Goodhart effects (e.g., Extremal, Causal, and Adversarial Goodhart) contribute to reward model overoptimization under various conditions

### Open Question 2
- Question: What is the optimal number of responses per prompt to maximize the correlation between RM error metrics and policy regret while considering annotation costs?
- Basis in paper: [explicit] The paper finds that increasing responses per prompt generally enhances correlation with policy regret, but the benefit diminishes with larger sample sizes and annotation budgets
- Why unresolved: While the paper explores this relationship, it doesn't determine the precise optimal number of responses that balances correlation strength with practical annotation constraints
- What evidence would resolve it: Empirical studies varying response numbers per prompt across different RLHF tasks and measuring the trade-off between correlation improvement and annotation cost efficiency

### Open Question 3
- Question: How do prompt semantic differences between RM test datasets and downstream optimization datasets affect the reliability of RM evaluation metrics?
- Basis in paper: [explicit] The paper demonstrates that prompt differences between RM test datasets and downstream test datasets can weaken the accuracy-regret correlation, with paraphrasing strategies affecting correlation differently under BoN and PPO settings
- Why unresolved: While the paper shows prompt differences matter, it doesn't establish which specific semantic aspects of prompts are most critical for maintaining reliable RM evaluation
- What evidence would resolve it: Controlled experiments varying specific semantic features of prompts (e.g., domain specificity, task complexity, safety considerations) while measuring their impact on RM evaluation accuracy and downstream policy performance

## Limitations

- **Core relationship validity**: The correlation coefficients remain modest (ranging from -0.2 to 0.5), suggesting the relationship may not be practically reliable for model selection
- **Measurement sensitivity**: The optimal measurement approach depends heavily on downstream optimization characteristics, creating a circular dependency
- **Goodhart effect attribution**: The experimental design cannot definitively isolate Regressional Goodhart effects from other potential causes

## Confidence

- **High Confidence**: The empirical finding that accuracy alone is insufficient for predicting policy performance is well-supported by extensive experiments across multiple datasets and optimization algorithms
- **Medium Confidence**: The specific measurement recommendations show promise but require validation across diverse real-world scenarios
- **Low Confidence**: The exact quantitative relationship between accuracy, optimization parameters, and policy regret is highly sensitive to experimental conditions

## Next Checks

1. **Real-World Validation**: Apply the measurement recommendations to actual RLHF pipelines being deployed in production, measuring whether the proposed accuracy metrics better predict policy performance than traditional accuracy metrics

2. **Cross-Architecture Testing**: Evaluate whether the accuracy-regret relationship holds consistently across different model families (transformers, state-space models) and sizes, as the current study focuses primarily on transformer-based architectures

3. **Optimization Algorithm Sensitivity**: Systematically vary optimization hyperparameters (learning rates, KL penalties, batch sizes) to determine which factors most strongly influence the accuracy-regret correlation, identifying conditions under which accuracy becomes more predictive