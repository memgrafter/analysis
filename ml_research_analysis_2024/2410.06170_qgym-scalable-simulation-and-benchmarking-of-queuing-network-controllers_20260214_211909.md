---
ver: rpa2
title: 'QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers'
arxiv_id: '2410.06170'
source_url: https://arxiv.org/abs/2410.06170
tags:
- queuing
- queue
- network
- time
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QGym, an open-source simulation framework
  for benchmarking queueing network controllers. The key challenge addressed is evaluating
  reinforcement learning algorithms for managing congestion in complex queuing systems,
  which differ from standard RL problems due to continuous-time dynamics, high stochasticity,
  and long horizons.
---

# QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers

## Quick Facts
- arXiv ID: 2410.06170
- Source URL: https://arxiv.org/abs/2410.06170
- Reference count: 40
- Primary result: PPO with work-conserving modification (PPO-WC) outperforms traditional queuing policies in 77% of benchmark environments

## Executive Summary
QGym introduces an open-source simulation framework for benchmarking reinforcement learning algorithms in queuing network control. The framework addresses limitations of existing work by supporting continuous-time dynamics, arbitrary arrival patterns, and non-Markovian service distributions through an event-driven simulator. The paper evaluates PPO variants against classical queuing policies across 20 benchmark environments, demonstrating that PPO-WC achieves superior performance in most instances while maintaining theoretical convergence guarantees through the work-conserving constraint.

## Method Summary
QGym provides an event-driven simulator for queuing networks that supports arbitrary arrival patterns and service time distributions, moving beyond Markovian assumptions. The framework includes 20 benchmark environments (hospital patient flow, input-switch, reentrant networks, etc.) and policies (classical queuing rules and RL methods). PPO variants are trained using 100 episodes of 50,000 steps with Adam optimizer, cosine learning rate decay, and GAE parameter λ=0.99. The work-conserving modification masks invalid actions while preserving differentiability. Performance is measured by time-averaged total queue length over 100 trajectories.

## Key Results
- PPO-WC outperforms traditional policies (cμ, MaxWeight, Max Pressure, Fluid) in 77% of benchmark instances
- Performance gains are larger in noisy, non-exponential environments
- RL methods struggle with larger, more realistic networks despite success on smaller benchmarks

## Why This Works (Mechanism)
QGym succeeds by providing a general-purpose framework that captures the true dynamics of queuing networks rather than relying on restrictive Markovian assumptions. The event-driven simulation accurately models continuous-time behavior and supports complex arrival patterns and service distributions. The work-conserving modification in PPO-WC ensures theoretical convergence while allowing the agent to learn optimal scheduling policies that respect system constraints.

## Foundational Learning
- Event-driven simulation: Needed to accurately model continuous-time queuing dynamics; quick check: verify jobs are processed immediately when servers are available
- Work-conserving constraint: Essential for maintaining theoretical convergence guarantees; quick check: ensure invalid actions are properly masked while preserving gradient flow
- Time-averaged metrics: Required for evaluating long-horizon performance; quick check: verify averaging calculation matches Equation 4 formulation
- Non-Markovian distributions: Critical for realistic modeling; quick check: confirm hyper-exponential service times are correctly implemented
- Multi-agent coordination: Necessary for complex network control; quick check: validate action spaces properly reflect system constraints
- Reward shaping: Important for stable learning; quick check: ensure queue length penalties are appropriately scaled

## Architecture Onboarding

Component Map: Simulator -> Policy Network -> Environment Interface -> Training Loop

Critical Path: Event-driven simulator generates observations → PPO policy network selects actions → Environment applies work-conserving masking → Simulator updates state → Metrics are collected

Design Tradeoffs: General framework vs. specialized implementations; support for arbitrary distributions vs. computational efficiency; ease of use vs. configurability

Failure Signatures: Training instability when work-conserving constraint is incorrectly implemented; poor performance on non-exponential environments; memory issues with large networks

First Experiments:
1. Run criss_cross.yaml with PPO-WC to verify basic functionality and compare against cμ policy
2. Test hospital environment with vanilla PPO to confirm failure modes in complex systems
3. Validate event-driven simulation by comparing against analytical results for simple M/M/1 queue

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QGym's performance compare to specialized simulators for specific domains (e.g., Simio for manufacturing, AnyLogic for healthcare) when benchmarking RL algorithms?
- Basis in paper: [inferred] The paper mentions related work on discrete-event simulation software like Simio and AnyLogic, but does not provide a direct comparison of their performance with QGym.
- Why unresolved: The paper focuses on demonstrating QGym's capabilities and does not benchmark against other simulators.
- What evidence would resolve it: A comparative study of QGym's performance against other simulators for specific domain problems, using the same RL algorithms and metrics.

### Open Question 2
- Question: What is the impact of the work-conserving constraint on the exploration-exploitation tradeoff in PPO, and how does it affect the convergence rate and final performance?
- Basis in paper: [explicit] The paper highlights that PPO-WC outperforms PPO in most instances, but does not delve into the theoretical reasons behind this improvement or analyze the impact on exploration-exploitation tradeoff.
- Why unresolved: The paper focuses on empirical results and does not provide a theoretical analysis of the work-conserving constraint's impact on PPO.
- What evidence would resolve it: A theoretical analysis of the work-conserving constraint's effect on PPO's exploration-exploitation tradeoff, potentially through a modified reward function or policy gradient analysis.

### Open Question 3
- Question: How does the performance of PPO-WC scale with the size of the queuing network, and what are the limitations of the current implementation in handling large-scale systems?
- Basis in paper: [explicit] The paper mentions that PPO-WC struggles with larger, more realistic networks like the hospital example, but does not provide a detailed analysis of the scaling limitations.
- Why unresolved: The paper focuses on demonstrating the overall performance of PPO-WC and does not investigate the specific challenges and limitations in scaling to larger networks.
- What evidence would resolve it: A systematic study of PPO-WC's performance on queuing networks of increasing size, analyzing the computational complexity, memory requirements, and potential bottlenecks in the current implementation.

## Limitations
- Empirical evaluation primarily on small-scale networks (maximum 11 server pools) that may not capture industrial complexity
- PPO struggles with larger, more realistic networks despite success on smaller benchmarks
- Limited investigation of scaling behavior and computational bottlenecks in large-scale implementations

## Confidence
- General framework utility: High - Well-documented implementation with clear APIs
- Relative policy comparisons: High - 100 trajectories per experiment with proper time-averaging
- Performance claims for large networks: Medium - Acknowledged degradation in realistic systems
- Generalization to industrial applications: Low - Paper notes RL struggles with larger networks

## Next Checks
1. Benchmark framework on industrial-scale queuing networks with 50+ server pools to test scalability claims
2. Implement cross-validation across different arrival rate distributions beyond exponential and hyper-exponential to assess robustness
3. Conduct ablation studies isolating the impact of the work-conserving modification versus other PPO hyperparameters to determine if performance gains are specific to the constraint or general training improvements