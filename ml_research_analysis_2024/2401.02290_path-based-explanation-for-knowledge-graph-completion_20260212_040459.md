---
ver: rpa2
title: Path-based Explanation for Knowledge Graph Completion
arxiv_id: '2401.02290'
source_url: https://arxiv.org/abs/2401.02290
tags:
- graph
- explanation
- paths
- power-link
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Power-Link, the first path-based explanation
  method for knowledge graph completion (KGC) using GNN-based models. It addresses
  the challenge of providing interpretable explanations for KGC predictions by generating
  paths rather than subgraphs.
---

# Path-based Explanation for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2401.02290
- Source URL: https://arxiv.org/abs/2401.02290
- Reference count: 40
- One-line primary result: Power-Link outperforms state-of-the-art baselines in interpretability, efficiency, and scalability for knowledge graph completion explanations.

## Executive Summary
This paper introduces Power-Link, the first path-based explanation method for knowledge graph completion (KGC) using GNN-based models. The method addresses the challenge of providing interpretable explanations for KGC predictions by generating paths rather than subgraphs. Power-Link uses a simplified graph-powering technique to efficiently identify and emphasize important paths, achieving better performance than existing methods while being faster and more memory-efficient.

## Method Summary
Power-Link generates path-based explanations for GNN-based KGC models by first extracting a computation graph around target entity pairs, then scoring edge importance using a Triplet Edge Scorer (TES) that incorporates both local edge information and global target triplet context. The core innovation is a simplified graph-powering technique that iteratively updates path probabilities through sparse matrix multiplication, strengthening important paths during training without the computational overhead of iterative shortest-path search. Finally, explanatory paths are generated using Dijkstra's algorithm on inverse edge scores.

## Key Results
- Achieves higher fidelity scores (e.g., 0.699 vs 0.496 for RGCN+TransE on FB15k-237)
- Lower prediction loss (e.g., 0.088 vs 0.400) compared to PaGE-Link
- 3.18x faster and uses significantly less memory than PaGE-Link
- Demonstrates better scalability for large knowledge graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Power-Link's simplified graph-powering technique efficiently identifies and emphasizes important paths by leveraging parallelizable matrix operations.
- Mechanism: Instead of using shortest-path searching, Power-Link powers only the target row vector in the probability adjacency matrix, iteratively updating path probabilities through sparse matrix multiplication.
- Core assumption: The probability of edges on explanatory paths can be amplified by repeated multiplication of the probability matrix with a row vector.
- Evidence anchors:
  - [abstract] "We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme."
  - [section] "We only power the target (Ë†ð‘–ð‘¡â„Ž ) row vector in the matrix, which we call the power vector ð’– = MË†ð‘–:, ð’– âˆˆ R1Ã—ð‘. This is done by multiplying M to ð’– by ð‘™ âˆ’ 1 times, yielding ð’– (ð‘™ )."
  - [corpus] Weak corpus evidence for specific graph-powering techniques; most related papers focus on subgraph explanations rather than path-based methods.
- Break condition: If the graph becomes too dense or the path length L becomes too large, the matrix multiplication operations may become computationally expensive despite parallelization.

### Mechanism 2
- Claim: The Triplet Edge Scorer (TES) incorporates both local edge information and global target triplet information to generate more meaningful edge scores.
- Mechanism: TES combines the embedding vectors of the local edge triplet and the target triplet using either concatenation or Euclidean distance strategies, then processes them through an MLP to generate edge importance scores.
- Core assumption: Combining local edge semantics with the target triplet context provides better discrimination of important edges than considering only local information.
- Evidence anchors:
  - [section] "We propose to learn aTriplet Edge Scorer (TES) to leverage the entity and relation information in the KG. TES gives a score to every edge in the computational graph. The score measures the importance of each edge in explaining the prediction of the target triplet."
  - [section] "Different from [ 22], which only integrates node features to mark the edge scores, we consider the local meaning of each edge triplet to the explanation of the target triplet."
  - [corpus] Limited direct evidence; most existing methods focus on node features rather than triplet-level scoring.
- Break condition: If the embedding space becomes too high-dimensional, the combination strategy may lead to overfitting or computational inefficiency.

### Mechanism 3
- Claim: Power-Link's path-enforcing learning strategy avoids early noise propagation by strengthening all on-path edges simultaneously rather than using iterative shortest-path search.
- Mechanism: By maximizing the average probability of edges on paths between target nodes through the powering process, Power-Link strengthens meaningful paths from the beginning of training without suppressing off-path edges.
- Core assumption: Early noise in edge importance scores can be tolerated if the path-enforcing mechanism amplifies correct paths consistently across training epochs.
- Evidence anchors:
  - [section] "To alleviate the early perturbations, we replace the shortest-path searching with a graph-power-based algorithm that enhances paths of specific lengths. The algorithm strengthens all on-path edges during the whole training process at a low cost of computational time."
  - [section] "Different from the intuition of PaGE-Link, we find it unnecessary to suppress the off-path edges during training."
  - [corpus] Weak corpus evidence for this specific approach; most related work focuses on suppressing irrelevant edges rather than avoiding early noise.
- Break condition: If the initial edge scores are extremely noisy or random, the path-enforcing mechanism may amplify incorrect paths rather than correct ones.

## Foundational Learning

- Concept: Knowledge Graph Completion (KGC)
  - Why needed here: Power-Link is specifically designed to explain predictions from KGC models, so understanding the task and common approaches is essential.
  - Quick check question: What is the main difference between embedding-based KGC methods and rule-based KGC methods?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Power-Link explains GNN-based KGC models, so understanding how GNNs aggregate information through message passing is crucial.
  - Quick check question: How does the receptive field of a GNN with L layers relate to the computation graph used in Power-Link?

- Concept: Path-based vs Subgraph-based explanations
  - Why needed here: Power-Link specifically argues for path-based explanations over subgraph-based ones, so understanding the trade-offs is important.
  - Quick check question: What are the main advantages of path-based explanations over subgraph-based explanations in terms of interpretability and scalability?

## Architecture Onboarding

- Component map:
  KGC Model -> Triplet Edge Scorer (TES) -> Path-Enforcing Learning Module -> Path Generation Module

- Critical path:
  1. Extract and prune the computation graph around the target triplet
  2. Compute edge scores using TES
  3. Apply the graph-powering technique to strengthen on-path edges
  4. Generate final explanatory paths using shortest-path search

- Design tradeoffs:
  - Path length L vs computational cost: Longer paths provide more context but increase computation
  - Combination strategy in TES: Concatenation offers flexibility but Euclidean distance may better align with energy-based KGC models
  - Powering order vs explanation quality: Higher orders strengthen longer paths but may include less meaningful information

- Failure signatures:
  - Explanations contain irrelevant or noisy edges: Indicates TES is not properly distinguishing important edges
  - Very short or very long explanations: Suggests incorrect path length parameter or powering order
  - Explanations don't change across different target triplets: Indicates TES is not properly incorporating target triplet information

- First 3 experiments:
  1. Compare Power-Link explanations with and without the path-enforcing module on a small KG to verify its impact
  2. Test different combination strategies (concatenation vs Euclidean) in TES to determine which works better for different KGC models
  3. Vary the powering order L and measure the impact on explanation quality metrics (Fidelity+, Fidelity-, HÎ”R)

## Open Questions the Paper Calls Out
- Question: How does the choice of path length L affect the quality and interpretability of explanations in Power-Link?
  - Basis in paper: [explicit] The paper mentions that increasing the powering order (which corresponds to longer paths) brings better performance but also notes that longer paths may be less meaningful to users.
  - Why unresolved: The paper provides a general observation but does not conduct a detailed study on the optimal choice of L for different knowledge graph structures and tasks.
  - What evidence would resolve it: Experiments systematically varying L across different KGs and tasks, measuring explanation quality and user interpretability.

## Limitations
- The approach is currently limited to GNN-based KGC models and may not generalize to other KGC architectures
- The simplified graph-powering technique may become computationally expensive for very large, dense knowledge graphs
- The paper does not address scenarios where knowledge graphs are dynamic and continuously evolving

## Confidence
- Path-based explanation effectiveness: High
- Computational efficiency claims: Medium
- Scalability to very large KGs: Low

## Next Checks
1. Test Power-Link on additional KGC models beyond the ones evaluated (e.g., TransE with different decoders) to assess generalizability
2. Conduct ablation studies removing the path-enforcing module to quantify its specific contribution
3. Evaluate Power-Link on larger knowledge graphs (e.g., FB15k-237 Ã— 10) to verify scalability claims