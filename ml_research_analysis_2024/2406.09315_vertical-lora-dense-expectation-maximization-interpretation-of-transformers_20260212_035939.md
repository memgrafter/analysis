---
ver: rpa2
title: 'Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers'
arxiv_id: '2406.09315'
source_url: https://arxiv.org/abs/2406.09315
tags:
- layer
- arxiv
- vlora
- lora
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel interpretation of Transformer models
  as dense Expectation-Maximization algorithms on Bayesian networks, leading to a
  new model design paradigm called Vertical LoRA (VLoRA). The core idea is that each
  layer of a Transformer model can be viewed as an iteration of an EM algorithm, where
  the forward pass corresponds to the E-step and the weight differences between consecutive
  layers correspond to the M-step.
---

# Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers

## Quick Facts
- arXiv ID: 2406.09315
- Source URL: https://arxiv.org/abs/2406.09315
- Reference count: 32
- One-line result: Reduces Transformer parameters to 26.65% with rank=2 while preserving performance

## Executive Summary
This paper presents a novel interpretation of Transformer models as dense Expectation-Maximization (EM) algorithms on Bayesian networks, leading to a new model design paradigm called Vertical LoRA (VLoRA). The core insight is that each Transformer layer can be viewed as an iteration of an EM algorithm, where the forward pass corresponds to the E-step and the weight differences between consecutive layers correspond to the M-step. Based on this interpretation, VLoRA recursively factorizes each layer's increment using LoRA decomposition, dramatically reducing parameter count while preserving model performance and being less prone to overfitting.

## Method Summary
VLoRA factorizes each layer recursively based on the previous layer using LoRA decomposition, applied to weight increments between consecutive layers. The model is partitioned into k chunks, with each chunk containing a base layer followed by VLoRA layers that learn increments from the previous layer. Experiments use Vision Transformers on CIFAR-10, comparing parameter efficiency, training curves, and evaluation metrics between vanilla ViT and VLoRA variants with ranks {2, 4, 8}.

## Key Results
- VLoRA reduces parameters to 26.65% of original with rank=2
- Maintains comparable accuracy to full models
- Shows reduced overfitting compared to vanilla ViT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Each Transformer layer implements an EM algorithm iteration
- Mechanism: Forward pass = E-step (computing expected latent values), weight changes = M-step (parameter updates to maximize expected log-likelihood)
- Core assumption: Supervised learning with Transformers implicitly performs EM on latent variable models where each patch/token has associated latent features
- Evidence anchors: Mathematical formulation shows log P(y|x; θ) = Q(θ|θ(l)) + H(θ|θ(l)) where Q corresponds to E-step and weight updates to M-step
- Break condition: If latent variable dependencies aren't local or attention patterns don't reflect EM-style belief propagation, interpretation fails

### Mechanism 2
- Claim: Weight increments between consecutive layers capture essential learning information
- Mechanism: VLoRA learns only the difference from the previous layer, assuming this increment contains critical transformation information
- Core assumption: Delta between consecutive layers is low-rank and captures most learning signal
- Evidence anchors: Mathematical formulation θ(l+1) = θ(l) + ∆θ(l) with LoRA decomposition applied to increments
- Break condition: If layer increments become full-rank or base layer captures insufficient information, performance degrades

### Mechanism 3
- Claim: Recursive factorization across layers reduces parameter count while preserving performance
- Mechanism: Factorizing each layer's increment based on previous layer's parameters (vertical direction) achieves more efficiency than horizontal LoRA
- Core assumption: Dependencies between consecutive layers can be captured through recursive factorization, with low-rank decomposition sufficing for increments
- Evidence anchors: Comparison to LoRA which factorizes layers independently, VLoRA works in "vertical" direction
- Break condition: If layer increments have high intrinsic dimensionality or long-range dependencies exist that can't be captured through local recursion

## Foundational Learning

- Concept: Expectation-Maximization algorithm
  - Why needed here: Paper's core claim is that Transformers implement EM algorithms, so understanding E-step and M-step is fundamental
  - Quick check question: What are the E-step and M-step in the EM algorithm, and how do they relate to forward passes and weight updates in neural networks?

- Concept: Low-rank matrix factorization
  - Why needed here: VLoRA applies LoRA decomposition to layer increments, requiring understanding of low-rank approximations
  - Quick check question: Why does approximating a matrix with a low-rank product (BT A) reduce parameters, and what determines the appropriate rank?

- Concept: Vision Transformer architecture
  - Why needed here: Experiments use ViT models, so understanding patch-based processing and self-attention is necessary
  - Quick check question: How does a Vision Transformer process image patches, and what role do QKV matrices play in the attention mechanism?

## Architecture Onboarding

- Component map: Base layer -> VLoRA layer 1 -> VLoRA layer 2 -> ... -> Final accumulated parameters
- Critical path: 1. Define base layer parameters, 2. Compute increment from previous layer for each subsequent layer, 3. Apply LoRA decomposition to increment, 4. Combine base layer with accumulated increments, 5. Forward pass uses final accumulated parameters
- Design tradeoffs: Base layer size vs VLoRA layer count (larger base reduces need for increments but increases parameters), number of chunks (more chunks increase flexibility but reduce parameter savings), rank selection (higher rank preserves more information but reduces parameter efficiency), layer dependency (recursive factorization saves parameters but may propagate errors)
- Failure signatures: Overfitting despite parameter reduction (base layer too small), performance degradation (rank too low or chunks too few), training instability (layer increments too large or accumulated errors from recursion)
- First 3 experiments: 1. Replace one layer's full weights with VLoRA increment and measure performance impact, 2. Vary rank parameter (r=2,4,8) on single increment to find minimum viable rank, 3. Compare training curves of full model vs VLoRA version to verify less overfitting behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does vertical factorization in VLoRA compare to horizontal factorization approaches like LoRA in terms of overall model performance and parameter efficiency across different model architectures?
- Basis in paper: Paper explicitly mentions VLoRA factorizes each layer recursively while LoRA factorizes layers independently
- Why unresolved: Paper provides experimental results on ViT models but doesn't extensively compare VLoRA to LoRA across wide range of architectures and tasks
- What evidence would resolve it: Comprehensive experiments comparing VLoRA to LoRA on various architectures (CNNs, RNNs) and tasks (NLP, vision, speech)

### Open Question 2
- Question: What is the theoretical basis for VLoRA's effectiveness in preserving performance while reducing parameters, and how does it relate to underlying structure of latent space?
- Basis in paper: Paper proposes EM interpretation of Transformers and suggests VLoRA's recursive factorization aligns with this interpretation
- Why unresolved: Paper presents EM interpretation as motivation but doesn't rigorously prove or analyze theoretical implications on performance and parameter efficiency
- What evidence would resolve it: Theoretical analysis demonstrating relationship between EM interpretation, VLoRA's recursive factorization, and underlying latent space structure

### Open Question 3
- Question: How does choice of chunk size (k) in VLoRA affect model performance and parameter efficiency, and is there optimal value for different architectures and tasks?
- Basis in paper: Paper mentions partitioning model into chunks with base layer in each chunk
- Why unresolved: Paper uses fixed chunk size (k=3) but doesn't investigate sensitivity to different chunk sizes or provide guidelines for optimal chunk size
- What evidence would resolve it: Experiments systematically varying chunk size and analyzing impact on performance and parameter efficiency across different architectures and tasks

## Limitations
- Central claim that Transformers implement EM algorithms remains largely theoretical without direct empirical validation
- Recursive factorization assumption lacks extensive validation across different model architectures and tasks
- Experiments limited to CIFAR-10 image classification with ViT models, providing no evidence for applicability to other domains

## Confidence
- Medium Confidence: Parameter reduction claims and overfitting resistance, directly measurable and supported by CIFAR-10 experiments
- Low Confidence: EM algorithm interpretation, primarily theoretical without empirical validation of latent variable model assumptions
- Medium Confidence: Recursive factorization approach, mathematical formulation is sound but limited experimental validation exists

## Next Checks
1. Design experiment to measure whether attention weights and feature representations in VLoRA models show EM-like iterative refinement of latent variable estimates across layers
2. Apply VLoRA to different domain (e.g., BERT for GLUE tasks or Wav2Vec for speech tasks) to validate whether parameter reduction and overfitting resistance generalize beyond image classification
3. Conduct systematic study varying number of chunks (k) and rank parameters across different ranges to establish full parameter-efficiency curve and identify minimum viable configuration