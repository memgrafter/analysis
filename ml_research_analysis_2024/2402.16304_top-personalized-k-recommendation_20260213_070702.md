---
ver: rpa2
title: Top-Personalized-K Recommendation
arxiv_id: '2402.16304'
source_url: https://arxiv.org/abs/2402.16304
tags:
- user
- recommendation
- utility
- perk
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fixed-size top-K recommendation,
  which may include irrelevant items or limit exposure to relevant ones, potentially
  reducing user satisfaction. The authors propose Top-Personalized-K Recommendation,
  a task to generate personalized-sized ranking lists to maximize individual user
  utility.
---

# Top-Personalized-K Recommendation

## Quick Facts
- arXiv ID: 2402.16304
- Source URL: https://arxiv.org/abs/2402.16304
- Reference count: 40
- Key outcome: PerK framework improves recommendation utility by 4-16% over traditional top-K methods by generating personalized-sized ranking lists based on estimated expected user utility

## Executive Summary
This paper addresses the problem of fixed-size top-K recommendation, which may include irrelevant items or limit exposure to relevant ones, potentially reducing user satisfaction. The authors propose Top-Personalized-K Recommendation, a task to generate personalized-sized ranking lists to maximize individual user utility. Their solution, PerK, estimates expected user utility using calibrated interaction probabilities and selects the recommendation size that maximizes this utility. PerK is a model-agnostic framework that can be adapted to any existing recommender. Experiments on four real-world datasets with three base recommenders show that PerK outperforms traditional top-K methods and state-of-the-art truncation methods in the top-personalized-K task, with improvements of 4-16% across different utility measures.

## Method Summary
PerK is a model-agnostic framework that improves recommendation utility by estimating expected user utility for each candidate recommendation size and selecting the size that maximizes this expectation. The method works by first fitting user-wise calibration functions (Platt scaling) to map ranking scores from a base recommender to calibrated interaction probabilities. Then, it treats unobserved interaction labels as Bernoulli random variables and computes expected utility for each candidate size using these calibrated probabilities. Finally, PerK selects the size with maximum expected utility and returns the top-k items from the original ranking. The framework addresses limitations of fixed-size top-K recommendation by allowing each user to receive a personalized number of recommendations based on their individual utility potential.

## Key Results
- PerK outperforms traditional top-K methods by 4-16% across NDCG, PDCG, F1, and TP metrics
- User-wise calibration significantly improves performance compared to global calibration
- PerK significantly outperforms state-of-the-art truncation methods (AttnCut, MtCut) in the top-personalized-K task
- The framework is model-agnostic and works with BPR, NCF, and LightGCN base recommenders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PerK improves recommendation utility by estimating expected user utility instead of relying on fixed top-K lists.
- Mechanism: It treats unobserved interaction labels as Bernoulli random variables and computes expected utility for each candidate recommendation size, selecting the size that maximizes this expectation.
- Core assumption: The interaction probability estimates (calibrated probabilities) are sufficiently accurate to reflect true user-item interaction likelihood.
- Evidence anchors:
  - [abstract] "PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility."
  - [section] "We cannot compute the true user utility in Eq.3, since we do not have access to the true relevance of unobserved items in the inference phase. To overcome this issue, PerK estimates the expected user utility instead of the true value by treating the interaction label ùëå for unobserved items as Bernoulli random variables."
- Break condition: If calibrated interaction probabilities are inaccurate (e.g., miscalibrated or biased), expected utility estimates will be wrong, leading to suboptimal recommendation sizes.

### Mechanism 2
- Claim: User-wise calibration functions improve probability estimation over global calibration.
- Mechanism: PerK trains separate Platt scaling calibration functions for each user, mapping recommender ranking scores to calibrated interaction probabilities, accounting for user-specific score distributions.
- Core assumption: Different users have meaningfully different distributions of ranking scores from the base recommender, requiring personalized calibration.
- Evidence anchors:
  - [section] "We adopt Platt scaling [46] and instantiate it for each user to consider the different distributions of the ranking score across users."
  - [section] "The user-specific parameters ùúôùë¢ = {ùëéùë¢, ùëèùë¢ } are related to the distribution of the ranking score of each user [26, 29]."
- Break condition: If user score distributions are similar enough that a global calibration function would suffice, the added complexity of user-wise calibration provides no benefit.

### Mechanism 3
- Claim: PerK outperforms truncation methods by directly modeling expected utility rather than predicting optimal cut-off positions.
- Mechanism: Instead of training a classification model to predict the best truncation position, PerK computes expected utility mathematically using calibrated probabilities and utility functions, avoiding overfitting to noisy labels.
- Core assumption: The mathematical formulation of expected utility with calibrated probabilities provides more reliable guidance than classification models trained on sparse and noisy target labels.
- Evidence anchors:
  - [section] "PerK does not rely on a deep model and considers the hidden-relevant items, to estimate the expected user utility."
  - [section] "PerK directly computes the expected user utility in mathematical form with the calibrated interaction probability and significantly outperforms AttnCut and MtCut in the top-personalized-ùêæ task."
- Break condition: If the calibration process fails to accurately estimate interaction probabilities, the mathematical utility computation becomes unreliable regardless of the approach.

## Foundational Learning

- Concept: Bernoulli random variables and expected value calculation
  - Why needed here: PerK models unobserved interaction labels as Bernoulli random variables to compute expected utility, which requires understanding probability theory fundamentals.
  - Quick check question: If an item has a 30% estimated interaction probability, what is the expected utility contribution if the utility function assigns value 1 for relevant items and 0 for irrelevant items?

- Concept: Position bias and utility measures (DCG, NDCG, F1, TP)
  - Why needed here: The framework needs to understand how different utility measures work and how position bias affects user utility to select appropriate metrics.
  - Quick check question: How does the logarithmic discount in DCG reflect user behavior in examining ranked lists?

- Concept: Platt scaling and probability calibration
  - Why needed here: The calibration function maps unbounded ranking scores to probabilities, requiring understanding of logistic regression and calibration techniques.
  - Quick check question: What is the mathematical relationship between Platt scaling and logistic regression?

## Architecture Onboarding

- Component map:
  - Base recommender model (BPR/NCF/LightGCN) ‚Üí Ranking scores
  - User-wise calibration functions ‚Üí Calibrated interaction probabilities
  - Expected utility computation module ‚Üí Utility estimates for each size k
  - Size selection module ‚Üí Optimal recommendation size kmax
  - Output generator ‚Üí Final ranked list of size kmax

- Critical path:
  1. Pre-trained recommender generates ranking scores
  2. Calibration functions transform scores to probabilities
  3. Expected utility computed for each candidate size
  4. Size with maximum expected utility selected
  5. Top-kmax items from original ranking presented

- Design tradeoffs:
  - User-wise vs global calibration: User-wise provides better accuracy but requires more parameters
  - Approximation in expected utility computation: Reduces computation time but introduces minor errors
  - Maximum size constraint (K): Balances computational cost vs recommendation quality

- Failure signatures:
  - Consistently selecting small sizes: May indicate miscalibration or inappropriate utility measure
  - High variance in selected sizes across users: Could signal calibration instability or dataset issues
  - Performance similar to best fixed Top-k: Suggests calibration or expected utility computation problems

- First 3 experiments:
  1. Implement calibration function and verify probability calibration on validation set (measure ECE)
  2. Test expected utility computation with ground truth labels to validate mathematical formulation
  3. Compare user-wise vs global calibration on a small dataset to measure performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PerK vary when different utility measures (e.g., NDCG, F1, PDCG, TP) are used for training the calibration function versus the final evaluation?
- Basis in paper: [inferred] The paper evaluates PerK with different utility measures but doesn't explicitly test the impact of using different measures for calibration training versus evaluation.
- Why unresolved: The authors only mention using the target utility measure for both calibration and evaluation, without exploring the potential benefits of using different measures.
- What evidence would resolve it: Experiments comparing PerK's performance when trained with different utility measures than the target evaluation measure, showing whether this approach improves results.

### Open Question 2
- Question: What is the impact of varying the maximum recommendation size ùêæ on PerK's performance across different datasets and base recommender models?
- Basis in paper: [explicit] The paper sets ùêæ=50 for all experiments but mentions trying ùêæ=100 with similar results, without providing a detailed analysis of how varying ùêæ affects performance.
- Why unresolved: The authors only briefly mention trying a larger ùêæ value but don't provide a comprehensive study on how different ùêæ values impact PerK's effectiveness.
- What evidence would resolve it: Experiments systematically varying ùêæ across a wider range of values (e.g., 10, 25, 50, 100, 200) for different datasets and base models, analyzing the relationship between ùêæ and PerK's performance.

### Open Question 3
- Question: How does PerK's performance compare to other model-agnostic approaches for determining personalized recommendation sizes, such as reinforcement learning or Bayesian optimization?
- Basis in paper: [explicit] The paper compares PerK to traditional methods (Top-k, Rand, Val-k) and document truncation methods (AttnCut, MtCut), but doesn't explore other model-agnostic approaches for this task.
- Why unresolved: The authors focus on comparing PerK to specific baseline methods without considering a broader range of model-agnostic approaches for determining personalized recommendation sizes.
- What evidence would resolve it: Experiments comparing PerK to alternative model-agnostic approaches like reinforcement learning or Bayesian optimization for determining personalized recommendation sizes, evaluating their effectiveness and efficiency.

## Limitations

- Primary limitation is dependence on accurate calibration of interaction probabilities; if calibration is imperfect, expected utility estimates will be systematically biased
- Computational complexity of training user-wise calibration functions may limit scalability to very large user bases
- Maximum size constraint K, while practical, may not fully capture individual user needs for personalized sizing

## Confidence

**High Confidence**: The experimental results showing PerK outperforming traditional top-K methods (4-16% improvement across metrics) are well-supported by the methodology and dataset choices.

**Medium Confidence**: The claim that user-wise calibration significantly outperforms global calibration relies on reasonable assumptions about user heterogeneity, but lacks direct empirical comparison in the paper.

**Low Confidence**: The assertion that PerK provides "personalized-sized" recommendations is somewhat limited by the practical constraint of maximum size K.

## Next Checks

1. **Calibration Accuracy Validation**: Implement Expected Calibration Error (ECE) measurement across all users and compare user-wise vs global calibration performance on validation sets. This will quantify the actual calibration quality and validate the key assumption behind PerK's effectiveness.

2. **Ablation Study on Calibration**: Run experiments with intentionally degraded calibration (e.g., using only global calibration, reducing calibration data, or adding noise) to measure the sensitivity of PerK's performance to calibration quality. This would reveal how robust the framework is to calibration errors.

3. **Size Distribution Analysis**: Compare the distribution of selected sizes across users between PerK, truncation methods, and Oracle. This would validate whether PerK truly provides personalized sizing or tends to select similar sizes across user groups, which would indicate limitations in personalization capability.