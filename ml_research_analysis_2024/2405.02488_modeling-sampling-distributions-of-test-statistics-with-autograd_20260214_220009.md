---
ver: rpa2
title: Modeling Sampling Distributions of Test Statistics with Autograd
arxiv_id: '2405.02488'
source_url: https://arxiv.org/abs/2405.02488
tags:
- neural
- function
- network
- conformal
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores modeling the sampling distribution of test statistics
  by differentiating a neural network model of the cumulative distribution function
  (CDF) using automatic differentiation. This approach, applied to simulation-based
  inference, aims to provide accurate approximations of probability density functions
  (PDFs) for test statistics that depend on parameters of interest and nuisance parameters.
---

# Modeling Sampling Distributions of Test Statistics with Autograd

## Quick Facts
- arXiv ID: 2405.02488
- Source URL: https://arxiv.org/abs/2405.02488
- Reference count: 40
- Key outcome: The paper explores modeling the sampling distribution of test statistics by differentiating a neural network model of the cumulative distribution function (CDF) using automatic differentiation.

## Executive Summary
This paper introduces a novel approach to simulation-based inference by modeling the sampling distribution of test statistics through automatic differentiation of neural network CDF models. The method addresses the challenge of approximating probability density functions for test statistics that depend on parameters of interest and nuisance parameters, particularly when the likelihood is intractable. By directly modeling the empirical CDF rather than using indicator variables, the authors achieve smoother and more accurate approximations of sampling distributions. Conformal inference is employed to provide valid confidence intervals without distributional assumptions, demonstrating the potential of this method for various applications in particle physics and epidemiology.

## Method Summary
The core methodology involves training neural networks to model the empirical cumulative distribution function (CDF) of test statistics as a function of parameters and the test statistic itself. The trained CDF model is then differentiated using automatic differentiation to obtain an approximation of the probability density function (PDF). This approach is applied to two examples: the signal/background (ON/OFF) problem in particle physics and the SIR model in epidemiology. Conformal inference is used to construct confidence intervals for the CDF and PDF models, providing uncertainty quantification without requiring distributional assumptions.

## Key Results
- Direct modeling of empirical CDF yields smoother and more accurate approximations of sampling distributions compared to ALFFI's indicator-based approach
- Conformal inference provides valid marginal coverage for the modeled CDFs and PDFs without distributional assumptions
- The method shows promise for simulation-based inference when the likelihood is intractable, though challenges remain for discrete distributions and achieving conditional coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiating a neural network CDF model with respect to the test statistic yields a smooth approximation of the sampling distribution.
- Mechanism: Automatic differentiation computes ∂F/∂λ exactly, bypassing finite-difference errors. This derivative serves as an approximation of the probability density function f(λ|θ).
- Core assumption: The neural network can learn an accurate enough continuous approximation of the true CDF despite the underlying discrete distribution of the test statistic.
- Evidence anchors:
  - [abstract] "If the model of the cdf, which is typically a deep neural network, is a function of the test statistic then the derivative of the neural network with respect to the test statistic furnishes an approximation of the sampling distribution of the test statistic."
  - [section] "The fact that λ is an input to the model in the ALFFI algorithm presents an opportunity: taking the derivative of the approximate cdf ˆF with respect to λ provides an approximation ˆf(λ | θ) of the sampling distribution of the test statistic."
- Break condition: The CDF model is too noisy or contains discontinuities that prevent a smooth derivative.

### Mechanism 2
- Claim: Directly modeling the empirical CDF improves accuracy compared to ALFFI's indicator-based approach.
- Mechanism: Instead of training on indicator variables Z, the model is trained directly on empirical CDF values computed from simulated data. This produces a smoother, more continuous target function.
- Core assumption: The empirical CDF at each parameter point is a sufficiently smooth function of λ and the parameters to be learned by a neural network.
- Evidence anchors:
  - [section] "We employ relatively simple yet effective neural network models such that the amount of fine-tuning that is involved in going to more complex data settings is minimal."
  - [section] "The excellent agreement between the predicted pdf and the histogrammed pdf is shown in Fig. 4 along with the 68% conformal confidence interval at each value of λ."
- Break condition: The empirical CDF varies too rapidly with λ or parameter changes, making smooth interpolation impossible.

### Mechanism 3
- Claim: Conformal inference provides valid marginal coverage for the CDF and PDF models without distributional assumptions.
- Mechanism: Split conformal prediction uses a calibration set to construct confidence intervals based on conformity scores, guaranteeing that the true CDF/PDF lies within the interval with the specified probability.
- Core assumption: The data are exchangeable, allowing conformal inference to provide valid coverage guarantees.
- Evidence anchors:
  - [section] "Conformal prediction (a.k.a. conformal inference) [20–22] is a general procedure for constructing such confidence sets/intervals for any predictive model (such as a neural network)."
  - [section] "Of the uncertainty quantification methods considered only one is calibrated by construction, namely, conformal inference."
- Break condition: The exchangeability assumption is violated, or the conformity scores do not capture the true uncertainty structure.

## Foundational Learning

- Concept: Automatic differentiation and its exactness
  - Why needed here: The method relies on autograd to compute ∂F/∂λ exactly, which is critical for obtaining accurate PDF approximations.
  - Quick check question: What is the key advantage of using autograd over finite-difference methods for computing derivatives in this context?

- Concept: Empirical CDF computation and its properties
  - Why needed here: Directly modeling the empirical CDF requires understanding how to compute it from simulated test statistics and why it's a better target than indicator variables.
  - Quick check question: How is the empirical CDF computed at each parameter point, and why is it expected to be smoother than the indicator-based approach?

- Concept: Conformal inference and marginal coverage
  - Why needed here: The uncertainty quantification relies on conformal inference to provide valid confidence intervals without distributional assumptions.
  - Quick check question: What is the key difference between marginal coverage and conditional coverage in the context of conformal inference?

## Architecture Onboarding

- Component map: Neural network model → Empirical CDF training → Derivative computation → Conformal inference → Confidence intervals
- Critical path: Train CDF model → Compute derivative → Apply conformal inference → Generate confidence intervals
- Design tradeoffs: Simpler neural network architectures vs. potential loss in accuracy; direct empirical CDF modeling vs. indicator-based ALFFI; marginal vs. conditional coverage guarantees
- Failure signatures: Jagged or noisy PDF approximations; wide or uninformative confidence intervals; poor coverage of true CDF/PDF values
- First 3 experiments:
  1. Train a basic neural network to model the empirical CDF for the ON/OFF problem and visualize the resulting PDF via autograd.
  2. Compare the accuracy of ALFFI vs. direct empirical CDF modeling using a simple test statistic.
  3. Apply conformal inference to the CDF and PDF models and evaluate the coverage of the resulting confidence intervals.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurate can neural networks model the sampling distribution of a test statistic when starting from a model of the cumulative distribution function (CDF)?
- Basis in paper: [explicit] The authors explore this question in the context of simulation-based inference and find that while directly modeling the empirical CDF yields smoother and more accurate approximations of the sampling distributions compared to the ALFFI algorithm, challenges remain in modeling discrete distributions and achieving conditional coverage guarantees.
- Why unresolved: The accuracy of the neural network models depends on the smoothness of the CDF and the complexity of the sampling distribution. The authors note that discrete distributions are particularly challenging to model accurately.
- What evidence would resolve it: A systematic study comparing the accuracy of different neural network architectures and training methods for modeling CDFs and sampling distributions, including a thorough analysis of their performance on various types of distributions.

### Open Question 2
- Question: Can conformal inference be used to provide calibrated uncertainty quantification for the modeled CDFs and PDFs?
- Basis in paper: [explicit] The authors use conformal inference to construct confidence sets for the CDFs and PDFs as a way to quantify their accuracy. However, they note that their conformal prediction sets are adaptive only because they have calibration data at the desired parameter points, and it would be helpful to have a smooth interpolation of the width of the conformal confidence interval.
- Why unresolved: The effectiveness of conformal inference for uncertainty quantification depends on the availability of calibration data and the smoothness of the modeled CDFs and PDFs. The authors highlight the need for further research in this area.
- What evidence would resolve it: A study comparing the performance of conformal inference and other uncertainty quantification methods for modeling CDFs and PDFs, including an analysis of their calibration and coverage properties.

### Open Question 3
- Question: How can the accuracy of the CDF models be improved, particularly for discrete distributions?
- Basis in paper: [inferred] The authors find that directly modeling the empirical CDF yields better results than the ALFFI algorithm, but challenges remain in modeling discrete distributions. They also explore the use of multi-stage neural networks and quantile residuals, but these approaches do not fully resolve the issues.
- Why unresolved: The high-frequency nature of discrete distributions and the sensitivity of the CDF models to the choice of binning and training data make it difficult to achieve accurate and smooth approximations.
- What evidence would resolve it: A comprehensive investigation of different approaches to modeling CDFs for discrete distributions, including the use of advanced neural network architectures, loss functions, and training techniques.

## Limitations
- Modeling discrete distributions through continuous neural networks introduces potential artifacts and approximation errors
- The method provides valid marginal coverage but achieving conditional coverage remains an open problem
- Scalability to high-dimensional problems is not thoroughly validated, with bootstrap uncertainty quantification performing poorly in such regimes

## Confidence
- **High confidence**: The core mechanism of using autograd to differentiate neural network CDF models is well-established and correctly implemented
- **Medium confidence**: Claims about the improved accuracy of direct empirical CDF modeling over ALFFI's indicator-based approach are supported by empirical results but lack theoretical guarantees
- **Medium confidence**: The effectiveness of conformal inference for uncertainty quantification is demonstrated, but specific implementation details and their impact on coverage guarantees require further validation

## Next Checks
1. **Discrete distribution validation**: Test the method on a purely discrete test statistic (e.g., binomial or Poisson) with known analytical properties to quantify approximation errors and artifacts introduced by the continuous modeling approach.
2. **Conditional coverage analysis**: Implement cross-conformal or jackknife-after-bootstrap methods to evaluate conditional coverage performance and compare it with the reported marginal coverage results.
3. **High-dimensional scalability test**: Apply the method to a parameter estimation problem with 10+ parameters (e.g., a multi-compartment epidemiological model) to assess computational requirements, convergence properties, and uncertainty quantification quality.