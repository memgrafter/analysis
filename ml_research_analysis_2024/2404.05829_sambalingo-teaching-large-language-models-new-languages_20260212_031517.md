---
ver: rpa2
title: 'SambaLingo: Teaching Large Language Models New Languages'
arxiv_id: '2404.05829'
source_url: https://arxiv.org/abs/2404.05829
tags:
- language
- languages
- data
- multilingual
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SambaLingo addresses the challenge of extending large language
  models (LLMs) to new languages beyond their original training scope. The core methodology
  involves expanding the tokenizer vocabulary with language-specific tokens, initializing
  their embeddings via subword averaging, and training through continual pretraining
  and direct preference optimization (DPO).
---

# SambaLingo: Teaching Large Language Models New Languages

## Quick Facts
- arXiv ID: 2404.05829
- Source URL: https://arxiv.org/abs/2404.05829
- Reference count: 28
- Primary result: Extends LLMs to 9 new languages via vocabulary expansion, subword averaging, and continual pretraining with DPO, achieving consistent improvements over baselines.

## Executive Summary
SambaLingo introduces a systematic approach for extending large language models to new languages beyond their original training scope. The method involves expanding the tokenizer vocabulary with language-specific tokens, initializing their embeddings through subword averaging, and training via continual pretraining combined with direct preference optimization. Tested across 9 typologically diverse languages and two model scales (7B and 70B), SambaLingo consistently outperforms both multilingual and monolingual baselines across multiple benchmarks including perplexity, translation quality, text classification, question answering, and natural language understanding.

## Method Summary
The core methodology of SambaLingo involves three key steps: first, expanding the tokenizer vocabulary with language-specific tokens to better capture the linguistic structure of target languages; second, initializing the embeddings of these new tokens via subword averaging, which leverages existing subword information to create reasonable starting points for new language representations; third, training through a combination of continual pretraining on target language data and direct preference optimization (DPO) using alignment data. The approach is validated across multiple languages and model scales, demonstrating strong scalability and effectiveness.

## Key Results
- SambaLingo consistently outperforms prior multilingual and monolingual baselines across 9 typologically diverse languages
- Strong scalability demonstrated from 7B to 70B parameter models with consistent performance gains
- Modest amounts of target-language alignment data suffice when mixed with translated English data
- Improvements observed across multiple benchmark types including perplexity, translation quality (FLORES), text classification (SIB-200), question answering (BELEBELE), and NLU tasks

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental limitation of LLMs being confined to their original training languages. By expanding the tokenizer vocabulary, the model gains the ability to represent language-specific linguistic phenomena that were previously tokenized suboptimally or not at all. The subword averaging initialization provides a strong starting point for new token embeddings by leveraging existing knowledge from similar subword units. The combination of continual pretraining and DPO ensures both broad language understanding and alignment with human preferences in the target language. The mixing of native target-language data with translated English data creates a rich training signal that captures both language-specific patterns and cross-lingual transfer.

## Foundational Learning
- Tokenization and vocabulary expansion: Needed to represent language-specific phenomena; Quick check: Compare token coverage and perplexity before/after expansion
- Subword averaging for embedding initialization: Needed to bootstrap new language representations from existing knowledge; Quick check: Measure embedding similarity between averaged and ground-truth representations
- Continual pretraining: Needed to adapt model parameters to new linguistic patterns; Quick check: Track perplexity on held-out target language data during training
- Direct Preference Optimization (DPO): Needed to align model outputs with human preferences in new languages; Quick check: Compare preference model scores on model outputs before/after DPO
- Cross-lingual alignment data: Needed to transfer alignment knowledge from English; Quick check: Measure performance gap between native and translated alignment data

## Architecture Onboarding

**Component Map:**
Tokenizer expansion -> Embedding initialization -> Continual pretraining -> DPO alignment

**Critical Path:**
The critical path for implementing SambaLingo involves: (1) identifying target languages and collecting representative text corpora, (2) expanding the tokenizer vocabulary with language-specific tokens, (3) initializing new token embeddings via subword averaging, (4) performing continual pretraining on the expanded vocabulary, and (5) applying DPO using a mix of native and translated alignment data. The most technically challenging steps are tokenizer expansion and embedding initialization, which require careful handling to maintain model stability.

**Design Tradeoffs:**
The approach trades increased vocabulary size and computational cost during training for significantly improved performance on target languages. The use of translated English data for alignment reduces data collection burden but may introduce subtle biases. Subword averaging provides a computationally efficient initialization method but may not capture all language-specific nuances. The method requires access to reasonably sized monolingual corpora for each target language, which may not be available for extremely low-resource languages.

**Failure Signatures:**
Potential failure modes include: (1) poor tokenizer expansion leading to high out-of-vocabulary rates, (2) inadequate embedding initialization causing optimization instability, (3) catastrophic forgetting of original language capabilities during continual pretraining, and (4) alignment failure if translated data doesn't capture target language preferences adequately. These can be monitored through tracking perplexity on both original and target languages, embedding stability metrics, and alignment benchmark performance.

**First 3 Experiments:**
1. Expand tokenizer vocabulary for a single target language and measure changes in token coverage and perplexity
2. Compare different embedding initialization strategies (subword averaging vs. random initialization) on a small scale
3. Test the effect of mixing ratios between native and translated alignment data on final alignment quality

## Open Questions the Paper Calls Out
The study leaves open questions about performance on extremely low-resource or highly morphologically complex languages not included in the current sample. The subword averaging approach for embedding initialization may not scale well to languages with very different orthographic or phonological structures. Additionally, the reliance on translated English data for alignment, though practical, may introduce subtle biases or fail to capture language-specific discourse patterns fully.

## Limitations
- Evaluation focused on 9 typologically diverse languages, leaving questions about performance on extremely low-resource or highly morphologically complex languages
- Subword averaging initialization may not scale well to languages with very different orthographic or phonological structures
- Reliance on translated English data for alignment may introduce subtle biases or fail to capture language-specific discourse patterns fully

## Confidence
- Effectiveness of vocabulary expansion and embedding initialization: **High**
- Scalability to 70B parameters: **High**
- Sufficiency of modest target-language alignment data: **Medium**
- Generalization to unseen languages: **Low**

## Next Checks
1. Evaluate SambaLingo on additional low-resource or highly morphologically rich languages not represented in the current benchmark suite to test generalization beyond the sampled language families
2. Conduct ablation studies to quantify the relative contributions of native target-language alignment data versus translated English data, and test the limits of how little target-language data is sufficient
3. Assess long-term stability and catastrophic forgetting by measuring performance on both new and previously learned languages after extended continual pretraining