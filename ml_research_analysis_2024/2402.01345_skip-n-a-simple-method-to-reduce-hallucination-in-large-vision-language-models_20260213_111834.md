---
ver: rpa2
title: 'Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language
  Models'
arxiv_id: '2402.01345'
source_url: https://arxiv.org/abs/2402.01345
tags:
- arxiv
- lvlms
- hallucinations
- preprint
- greedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a semantic shift bias in large vision-language
  models (LVLMs) related to paragraph breaks ("\n\n"), where hallucinations often
  occur after these breaks. The authors propose two simple methods to mitigate this:
  modifying prompts to avoid paragraph breaks (MiHI) and adjusting decoding strategies
  to prevent "\n" outputs (MiHO).'
---

# Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2402.01345
- Source URL: https://arxiv.org/abs/2402.01345
- Authors: Zongbo Han; Zechen Bai; Haiyang Mei; Qianli Xu; Changqing Zhang; Mike Zheng Shou
- Reference count: 7
- This paper identifies a semantic shift bias in LVLMs related to paragraph breaks ('\n\n'), where hallucinations often occur after these breaks, and proposes two simple methods (MiHI and MiHO) to mitigate this issue.

## Executive Summary
This paper identifies a semantic shift bias in large vision-language models (LVLMs) related to paragraph breaks ('\n\n'), where hallucinations frequently occur after these breaks. The authors propose two simple methods to mitigate this: modifying prompts to avoid paragraph breaks (MiHI) and adjusting decoding strategies to prevent '\n' outputs (MiHO). Experiments on six LVLMs show that both methods significantly reduce hallucinations, with MiHO+MiHI achieving the best performance (e.g., reducing Cs from 48.56 to 36.68 and Ci from 13.00 to 10.04 on BakLLaV A).

## Method Summary
The paper proposes two methods to reduce hallucinations in LVLMs caused by paragraph breaks. MiHO modifies the decoding strategy by reducing logits for the '\n' token to prevent paragraph break generation. MiHI modifies prompts to explicitly request "in one paragraph" to encourage continuity. Both methods aim to prevent the semantic shift bias that occurs when models generate content after '\n\n' breaks. The methods were tested on six LVLMs using 5,000 images from MSCOCO validation set, measuring hallucination rates through Cs (hallucinated objects / all mentioned objects) and Ci (captions with hallucinated objects / all captions) metrics.

## Key Results
- MiHO and MiHI methods significantly reduce hallucinations across six LVLMs
- MiHO+MiHI achieves best performance: Cs reduced from 48.56 to 36.68 and Ci from 13.00 to 10.04 on BakLLaV A
- Deliberately inserting '\n\n' at generated descriptions can induce more hallucinations
- Greedy decoding shows lower hallucination rates compared to sampling decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic shift bias related to paragraph breaks ('\n\n') causes LVLMs to infer content after breaks should be semantically different, leading to increased hallucinations.
- Mechanism: During training, paragraphs separated by '\n\n' often have significant semantic changes. The model learns this pattern and applies it during generation, causing it to deviate from preceding content and produce hallucinatory descriptions after paragraph breaks.
- Core assumption: Training data exhibits significant semantic changes between paragraphs separated by '\n\n', creating a bias in the model's understanding of what should follow a paragraph break.
- Evidence anchors:
  - [abstract]: "Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('\n\n'), where the content before and after '\n\n' in the training data frequently exhibit significant semantic changes."
  - [section]: "As shown in Fig. 1, we identify a special semantic shift bias triggered by paragraph breaks, where training data often show significant semantic changes before and after '\n\n'."
  - [corpus]: Weak evidence. Corpus analysis shows related work on hallucination in LVLMs but doesn't directly support the semantic shift bias claim.
- Break condition: If training data doesn't show semantic shift patterns between '\n\n' separated paragraphs, the bias would not exist.

### Mechanism 2
- Claim: Deliberately inserting '\n\n' at specific positions in generated descriptions increases hallucination probability.
- Mechanism: By inserting '\n\n' at predetermined positions (e.g., after the k-th period), the model is triggered to apply its learned semantic shift bias, causing it to generate content that deviates from the visual input and produces hallucinations.
- Core assumption: The model's response to '\n\n' is consistent enough that inserting it at specific points will reliably trigger the semantic shift bias.
- Evidence anchors:
  - [abstract]: "Besides, we find that deliberately inserting '\n\n' at the generated description can induce more hallucinations."
  - [section]: "We also validate that inserting '\n\n' at appropriate positions can induce LVLMs to generate more hallucinations."
  - [corpus]: Weak evidence. While corpus shows related work on hallucination mitigation, it doesn't directly support the specific attack mechanism of '\n\n' insertion.
- Break condition: If the model doesn't consistently respond to '\n\n' with semantic shifts, the attack would not be effective.

### Mechanism 3
- Claim: Modifying prompts to avoid paragraph breaks (MiHI) and adjusting decoding strategies to prevent '\n' outputs (MiHO) effectively mitigate hallucinations.
- Mechanism: MiHI modifies the prompt to explicitly request "in one paragraph" which encourages the model to maintain continuity. MiHO adjusts the logits to penalize '\n' token prediction, preventing the model from generating paragraph breaks that trigger semantic shift bias.
- Core assumption: The model can follow modified instructions (MiHI) and can be steered away from '\n' predictions through logit adjustment (MiHO).
- Evidence anchors:
  - [section]: "Therefore, we try to modify the prompt for LVLMs and encourage them to fulfill the original instructions while avoiding the output of '\n', thereby maintaining the continuity and coherence of the generated text."
  - [section]: "From the perspective of modifying the output decoding strategies, we can avoid the output of '\n' by reducing the logits corresponding to the '\n' token."
  - [corpus]: Weak evidence. Corpus shows related work on hallucination mitigation but doesn't directly support these specific methods.
- Break condition: If the model cannot follow modified instructions or if logit adjustment doesn't effectively prevent '\n' generation, the mitigation would fail.

## Foundational Learning

- Concept: Semantic shift bias
  - Why needed here: Understanding how training data patterns create biases in model behavior is crucial for diagnosing and mitigating hallucinations.
  - Quick check question: How might training data with consistent patterns (like paragraph breaks) create biases in a model's understanding of what should follow those patterns?

- Concept: Logit manipulation in decoding
  - Why needed here: MiHO relies on adjusting token logits to prevent certain tokens from being generated, which requires understanding of how decoding strategies work.
  - Quick check question: How does subtracting a value from a token's logit affect its probability of being selected during generation?

- Concept: Prompt engineering
  - Why needed here: MiHI demonstrates how modifying prompts can influence model behavior, which is essential for understanding how to guide LVLMs effectively.
  - Quick check question: How might adding specific instructions to a prompt (like "in one paragraph") influence the structure of the generated output?

## Architecture Onboarding

- Component map: Vision encoder -> Visual features -> Language model -> Raw logits -> Decoding controller (MiHO adjustment) -> Adjusted logits -> Sampling -> Generated text

- Critical path:
  1. Input image → Vision encoder → Visual features
  2. Visual features + prompt → Language model → Raw logits
  3. Raw logits → Decoding controller (MiHO adjustment) → Adjusted logits
  4. Adjusted logits → Sampling → Generated text

- Design tradeoffs:
  - MiHO requires careful tuning of λ parameter to balance hallucination reduction vs. text quality
  - MiHI may not work well with models that aren't instruction-tuned
  - Both methods add minimal computational overhead but require careful implementation

- Failure signatures:
  - MiHO: Text quality degradation if λ is set too high, or insufficient hallucination reduction if λ is too low
  - MiHI: Failure to follow instructions if the model isn't instruction-tuned or if the prompt modification is unclear

- First 3 experiments:
  1. Test MiHO with different λ values on a small dataset to find the optimal balance between hallucination reduction and text quality
  2. Compare MiHI performance across different instruction-tuned and non-instruction-tuned models
  3. Evaluate the combined MiHO+MiHI approach against the individual methods to confirm the additive benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental reasons behind the semantic shift bias related to paragraph breaks ('\n\n') in LVLMs?
- Basis in paper: [explicit] The paper identifies a semantic shift bias related to paragraph breaks where content before and after '\n\n' in training data frequently exhibit significant semantic changes, but does not explore the underlying causes.
- Why unresolved: The paper focuses on observing and mitigating the effects of this bias but does not investigate why this pattern exists in the training data or how it influences the model's behavior.
- What evidence would resolve it: Analysis of training data distributions and semantic structures, along with experiments ablating the influence of paragraph breaks on model training, would help understand the root cause of this bias.

### Open Question 2
- Question: How does the semantic shift bias vary across different scales of LVLMs?
- Basis in paper: [inferred] The paper mentions that it remains to be explored whether this bias can be overcome when the model scale continues to increase, suggesting that model size might affect the presence or severity of the bias.
- Why unresolved: The study does not compare the semantic shift bias across LVLMs of varying sizes, leaving uncertainty about the relationship between model scale and bias.
- What evidence would resolve it: Experiments comparing the semantic shift bias in LVLMs of different sizes, including very large models like GPT-4, would clarify how model scale influences this bias.

### Open Question 3
- Question: What are the long-term effects of the proposed MiHO and MiHI methods on LVLM performance beyond hallucination reduction?
- Basis in paper: [inferred] While the paper demonstrates that MiHO and MiHI reduce hallucinations, it does not explore potential impacts on other aspects of model performance, such as coherence, relevance, or overall quality of descriptions.
- Why unresolved: The study focuses on hallucination metrics (Cs and Ci) and does not assess broader implications of the methods on the general performance of LVLMs.
- What evidence would resolve it: Comprehensive evaluations measuring various performance metrics, including user studies and qualitative assessments, would provide insights into the broader effects of MiHO and MiHI.

### Open Question 4
- Question: How do different decoding strategies interact with the semantic shift bias and the proposed mitigation methods?
- Basis in paper: [explicit] The paper notes that sampling decoding strategy is more prone to producing hallucinations compared to greedy decoding, and that the proposed methods perform better with greedy decoding, but does not explore interactions in depth.
- Why unresolved: The study provides limited analysis on how different decoding strategies might influence the semantic shift bias and the effectiveness of MiHO and MiHI.
- What evidence would resolve it: Experiments systematically varying decoding strategies (e.g., beam search, top-k sampling) and analyzing their interaction with the bias and mitigation methods would elucidate these dynamics.

## Limitations
- The generalizability of the semantic shift bias beyond the six tested LVLMs remains uncertain
- The corpus analysis provides weak evidence for the semantic shift bias claim, as it doesn't directly analyze training data patterns
- The optimal λ parameter for MiHO may vary significantly across different model architectures and sizes

## Confidence
**High Confidence:** The empirical results showing hallucination reduction with both MiHO and MiHI methods are well-supported by quantitative metrics (Cs and Ci) across multiple models and datasets. The observation that paragraph breaks correlate with increased hallucinations is consistently demonstrated.

**Medium Confidence:** The proposed mechanism explaining why paragraph breaks cause hallucinations (semantic shift bias) is plausible but relies on assumptions about training data patterns that aren't directly verified. The effectiveness of combined MiHO+MiHI approaches is demonstrated but the additive benefits aren't thoroughly explained.

**Low Confidence:** The generalizability of these findings to models outside the six tested LVLMs, particularly those with different architectural designs or training approaches, remains uncertain.

## Next Checks
1. **Training Data Analysis:** Analyze the training corpora of multiple LVLMs to empirically verify the presence of semantic shift patterns around paragraph breaks ('\n\n'), directly testing the core assumption behind the proposed mechanism.

2. **Cross-Architecture Validation:** Test both MiHO and MiHI methods on a broader range of vision-language models including different architectural families (e.g., transformer-based vs. other architectures) and training paradigms to assess generalizability.

3. **Ablation Studies on λ Parameter:** Conduct systematic ablation studies across multiple model sizes to determine optimal λ values for MiHO, and test whether the same λ values work consistently across different model scales and architectures.