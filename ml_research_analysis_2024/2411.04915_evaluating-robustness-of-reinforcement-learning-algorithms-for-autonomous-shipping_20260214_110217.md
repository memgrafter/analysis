---
ver: rpa2
title: Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping
arxiv_id: '2411.04915'
source_url: https://arxiv.org/abs/2411.04915
tags:
- learning
- autonomous
- robust
- vessel
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the robustness of deep reinforcement learning
  algorithms for autonomous inland waterway transport (IWT). It compares Soft-Actor
  Critic (SAC) and MuZero in a 3-DOF vessel simulator with varying environmental conditions.
---

# Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping

## Quick Facts
- arXiv ID: 2411.04915
- Source URL: https://arxiv.org/abs/2411.04915
- Authors: Bavo Lesy; Ali Anwar; Siegfried Mercelis
- Reference count: 34
- Primary result: SAC is more robust than MuZero to environmental disturbances in autonomous shipping simulations

## Executive Summary
This paper examines the robustness of deep reinforcement learning algorithms for autonomous inland waterway transport (IWT). It compares Soft-Actor Critic (SAC) and MuZero in a 3-DOF vessel simulator with varying environmental conditions. SAC is shown to be inherently more robust to environmental disturbances than MuZero, particularly under varying mass and turn rate conditions. While both algorithms achieve similar performance after training, SAC maintains its performance across a wider range of conditions and requires fewer training timesteps. The results suggest SAC is better suited for real-world autonomous shipping applications due to its robustness to changes in vessel and environmental conditions.

## Method Summary
The study evaluates SAC and MuZero in a 3-DOF vessel simulator for autonomous shipping, focusing on robustness to environmental changes. Training uses RLLib distributed across 20 parallel simulations. Both algorithms are trained on the MOOS-IVP autonomous shipping simulator with ranging sensor observations, random initial positions, and goals. The key comparison involves evaluating performance after training under varying mass (nominal 175,000 kg) and turn rate (nominal 70) conditions. Performance is measured using mean return across episodes, comparing success rates and lengths under different vessel parameters.

## Key Results
- SAC maintains performance across varying vessel mass and turn rate conditions while MuZero's performance deteriorates rapidly
- SAC achieves comparable final performance to MuZero but requires fewer training timesteps
- SAC's maximum entropy formulation provides inherent robustness to environmental disturbances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAC maintains performance across varying vessel mass and turn rate due to its maximum entropy formulation.
- Mechanism: The entropy term in SAC's objective encourages stochastic policies, which helps maintain exploration and adaptability when environmental dynamics shift. This stochasticity acts as a built-in robustness mechanism against disturbances in vessel properties.
- Core assumption: The environment's state transition function is approximately stationary, and the agent's policy can adapt through entropy maximization without catastrophic forgetting.
- Evidence anchors:
  - [abstract] SAC is shown to be inherently more robust to environmental disturbances compared to MuZero
  - [section] "SAC is based on maximum entropy RL, where an agent aims to maximize the expected return whilst acting as randomly as possible. Random actions can be beneficial in the presence of environmental disturbances..."
  - [corpus] No direct evidence in corpus neighbors; weak support from general RL literature.
- Break condition: If the changes in mass or turn rate are so large that the original policy's action space becomes completely irrelevant, entropy maximization alone may not suffice.

### Mechanism 2
- Claim: Model-free approaches like SAC outperform model-based ones (MuZero) when dynamics are hard to learn accurately.
- Mechanism: MuZero must learn an internal model of the vessel dynamics. When vessel properties (mass, turn rate) change, this learned model becomes inaccurate, degrading performance. SAC, being model-free, bypasses this step and directly learns the policy from interactions.
- Core assumption: The simulator's state transition function is sufficiently complex or noisy that learning an accurate dynamics model is harder than learning a robust policy.
- Evidence anchors:
  - [section] "This could be because the model-free approach does not need to learn a transition model of the complex environment."
  - [abstract] Both algorithms achieve similar performance after training, but SAC maintains performance across a wider range of conditions.
  - [corpus] Weak evidence; corpus neighbors do not directly address dynamics learning complexity.
- Break condition: If the simulator's dynamics are simple and well-behaved, a learned model could generalize better than a purely data-driven policy.

### Mechanism 3
- Claim: Robustness to mass changes is asymmetric; SAC performs better with higher mass but worse with lower mass.
- Mechanism: Higher mass dampens the effect of control inputs (thrust, rudder), making the system more stable but less responsive. SAC's entropy-driven exploration is better suited to this regime. Lower mass increases sensitivity, making precise control harder for MuZero due to overfitting to nominal dynamics.
- Core assumption: The relationship between mass and control effectiveness is monotonic and the policy's action scaling is not adaptive.
- Evidence anchors:
  - [section] "As the mass of the vessel increases, MuZero's performance rapidly deteriorates, whereas SAC maintains similar performance for larger mass values and degrades later."
  - [section] "This could be because lowering the mass makes it easier to control the vessel, as the influence of the control action increases (Eq. 1). This could be why MuZero performs better in this case."
  - [corpus] No direct evidence in corpus neighbors.
- Break condition: If the control action scaling is adaptive or if the mass change is coupled with other unmodeled dynamics, this asymmetry may break down.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem is formalized as an MDP to apply RL algorithms and define the optimal policy objective.
  - Quick check question: What are the five-tuple components (S, A, T, R, Î³) that define an MDP?

- Concept: Soft Actor-Critic (SAC)
  - Why needed here: SAC is the benchmark model-free RL algorithm used for its inherent robustness properties in this study.
  - Quick check question: How does the entropy term in SAC's objective contribute to policy robustness?

- Concept: Model-based RL (MBRL)
  - Why needed here: MuZero is a model-based RL algorithm compared against SAC; understanding MBRL is key to interpreting why SAC outperforms it in this context.
  - Quick check question: What are the main advantages and disadvantages of model-based RL compared to model-free RL?

## Architecture Onboarding

- Component map: Simulator (3-DOF vessel kinematics) -> RLlib distributed training -> SAC/MuZero agent -> reward function -> ranging sensor observations
- Critical path: Observation -> policy -> action -> simulator step -> reward -> update
- Design tradeoffs: Model-free (SAC) trades off sample efficiency for robustness; model-based (MuZero) trades off robustness for sample efficiency under nominal conditions
- Failure signatures: SAC performance drop indicates insufficient entropy regularization; MuZero drop indicates poor model generalization to new dynamics
- First 3 experiments:
  1. Validate baseline performance of SAC and MuZero under nominal vessel parameters
  2. Test robustness by varying mass and turn rate independently; compare performance decay curves
  3. Introduce correlated changes (e.g., mass + turn rate) to test joint robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAC and MuZero compare when both mass and turn rate variations are applied simultaneously rather than separately?
- Basis in paper: [inferred] The paper states "we only examined the mass and turn rate separately" and suggests this as future work.
- Why unresolved: The paper did not conduct experiments with combined variations of mass and turn rate.
- What evidence would resolve it: Experimental results showing the performance of both algorithms under simultaneous variations of mass and turn rate.

### Open Question 2
- Question: How would incorporating a more sophisticated kinematics model with drag affect the robustness of SAC and MuZero?
- Basis in paper: [explicit] The paper mentions "Future work will include the incorporation of a more sophisticated kinematics model, which includes drag."
- Why unresolved: The current experiments use a simplified 3-DOF kinematic model without drag.
- What evidence would resolve it: Comparative experiments using the enhanced model with drag coefficient and drag area variations.

### Open Question 3
- Question: How effective are adversarial learning methods in improving the robustness of RL algorithms for autonomous shipping in the MOOS-IVP simulator?
- Basis in paper: [explicit] The paper states "we can implement adversarial learning methods, as those discussed in [22] and [23]" as future work.
- Why unresolved: The paper did not implement or test adversarial learning approaches in this environment.
- What evidence would resolve it: Results from experiments comparing standard RL algorithms with adversarial learning-enhanced versions in the same simulation environment.

## Limitations

- The study only tests robustness under mass and turn rate variations, not other environmental factors like sensor noise or weather conditions
- The comparison is limited to two algorithms, potentially missing other approaches that might perform better
- The simulation environment, while 3-DOF, may not capture all complexities of real-world inland waterway navigation

## Confidence

- **High Confidence**: SAC's better performance under varying mass and turn rate conditions, and its faster training convergence compared to MuZero
- **Medium Confidence**: The claim that SAC's maximum entropy formulation is the primary reason for its robustness, as this is supported by theoretical reasoning but not directly validated in the paper
- **Low Confidence**: Generalizability of these results to other model-based RL algorithms or more complex environmental disturbances beyond mass and turn rate

## Next Checks

1. Extend Environmental Disturbances: Test SAC and MuZero under additional environmental factors such as varying water currents, wind forces, and sensor noise to validate robustness claims in more realistic scenarios

2. Hyperparameter Sensitivity: Conduct a systematic ablation study on key hyperparameters (e.g., entropy regularization coefficient in SAC, planning horizon in MuZero) to determine their impact on robustness to environmental changes

3. Generalization to Other Model-Based Algorithms: Compare SAC against other model-based RL algorithms (e.g., Dreamer, PlaNet) to assess whether the observed robustness advantage is specific to MuZero or a general property of model-free approaches