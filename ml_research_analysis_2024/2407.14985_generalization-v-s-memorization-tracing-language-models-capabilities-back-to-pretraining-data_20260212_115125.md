---
ver: rpa2
title: 'Generalization v.s. Memorization: Tracing Language Models'' Capabilities Back
  to Pretraining Data'
arxiv_id: '2407.14985'
source_url: https://arxiv.org/abs/2407.14985
tags:
- memorization
- data
- n-gram
- pretraining
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for understanding memorization
  at scale in large language models (LLMs) by introducing the concept of distributional
  memorization, which measures the correlation between the LLM output probabilities
  and the pretraining data frequency. To effectively capture task-specific pretraining
  data frequency, the authors propose a novel task-gram language model, which is built
  by counting the co-occurrence of semantically related n-gram pairs from task inputs
  and outputs in the pretraining corpus.
---

# Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data

## Quick Facts
- **arXiv ID**: 2407.14985
- **Source URL**: https://arxiv.org/abs/2407.14985
- **Reference count**: 27
- **Primary result**: Proposes distributional memorization framework showing factual QA exhibits strongest memorization while translation/reasoning tasks show more generalization

## Executive Summary
This paper introduces a new framework for understanding memorization versus generalization in large language models by measuring the correlation between model output probabilities and pretraining data frequency. The authors propose a novel task-gram language model to estimate task-specific pretraining frequencies by counting co-occurrences of semantically related n-gram pairs from task inputs and outputs. Evaluating Pythia models on four tasks (machine translation, factual QA, world knowledge, and math reasoning), the study finds that while all tasks improve with model size, only factual QA shows increased memorization, with the other tasks exhibiting greater generalization through novel output generation.

## Method Summary
The authors introduce distributional memorization as a metric that measures the correlation between LLM output probabilities and pretraining data frequency. To estimate task-specific pretraining frequencies, they propose a task-gram language model that counts co-occurrences of semantically related n-gram pairs from task inputs and outputs in the pretraining corpus. Using Pythia models trained on the Pile dataset, they evaluate four distinct tasks and measure both model performance and memorization levels across different model sizes.

## Key Results
- Factual question answering shows the strongest memorization effect across all tasks evaluated
- Machine translation and reasoning tasks exhibit greater generalization, producing more novel outputs
- Model performance improves across all tasks as LLM size increases
- Only factual question answering shows increased memorization with model size, while translation and reasoning tasks show more generalization

## Why This Works (Mechanism)
The distributional memorization framework works by quantifying the relationship between a model's output probabilities and the frequency of those outputs in the pretraining data. When outputs are highly correlated with pretraining frequencies, this indicates memorization. The task-gram language model captures task-specific frequency distributions by identifying semantically related n-gram pairs that co-occur in the pretraining corpus, providing a more accurate measure of memorization than general frequency counting.

## Foundational Learning
1. **Distributional memorization concept**: Why needed - to distinguish between genuine generalization and frequency-based copying; Quick check - verify correlation between output probabilities and pretraining frequencies
2. **Task-gram language modeling**: Why needed - to estimate task-specific pretraining frequencies accounting for semantic relationships; Quick check - compare task-gram LM estimates against actual model behavior
3. **Pythia model family**: Why needed - controlled study across different model sizes with same training data; Quick check - verify consistent scaling trends across tasks
4. **Pile dataset characteristics**: Why needed - understanding pretraining data composition affects memorization estimates; Quick check - assess overlap between train and test sets
5. **Novel output generation**: Why needed - distinguishing true generalization from pattern matching; Quick check - measure output novelty and correlate with memorization levels
6. **Semantic n-gram co-occurrence**: Why needed - capturing meaningful relationships beyond surface form matching; Quick check - validate semantic relevance of identified n-gram pairs

## Architecture Onboarding
**Component map**: Task inputs/outputs -> Task-gram LM -> Frequency estimates -> Distributional memorization metric -> Model evaluation

**Critical path**: Task-gram LM construction → Frequency estimation → Memorization measurement → Performance correlation analysis

**Design tradeoffs**: Task-gram LM captures semantic relationships but may miss novel combinations; frequency-based memorization measurement assumes frequency drives copying but may conflate with legitimate pattern learning

**Failure signatures**: High memorization scores could indicate either genuine copying or legitimate frequency-based pattern learning; novel output generation might represent shallow pattern matching rather than true generalization

**First experiments**: 1) Validate task-gram LM frequency estimates against alternative methods; 2) Test framework on models with verified train-test separation; 3) Conduct ablation studies removing known overlapping examples

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Task-gram language model approach for frequency estimation is novel but untested against alternatives
- Distinction between generalization and novel output generation isn't fully validated
- Pile dataset contains substantial overlaps with test sets across all tasks
- Study focuses exclusively on Pythia models, limiting generalizability

## Confidence
- **High confidence**: Factual QA shows strongest memorization; size scaling trends are robust
- **Medium confidence**: Distributional memorization metric separates memorization from generalization; task-gram LM provides accurate frequency estimates
- **Low confidence**: Framework generalizes beyond Pythia/Pile to other model families; novel outputs represent true generalization

## Next Checks
1. Validate the task-gram language model approach by comparing its frequency estimates against alternative methods (BM25 retrieval, embedding-based frequency estimation) and measuring correlation with actual model behavior.

2. Test the distributional memorization framework on a second model family (LLaMA, GPT-NeoX) trained on datasets with verified train-test separation to assess generalizability.

3. Conduct ablation studies removing known overlapping examples from the Pile dataset to measure impact on memorization estimates, particularly for factual QA where contamination is most severe.