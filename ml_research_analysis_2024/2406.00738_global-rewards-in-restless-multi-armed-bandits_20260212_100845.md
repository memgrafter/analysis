---
ver: rpa2
title: Global Rewards in Restless Multi-Armed Bandits
arxiv_id: '2406.00738'
source_url: https://arxiv.org/abs/2406.00738
tags:
- policies
- reward
- mcts
- linear
- rmab
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RMAB-G, an extension of RMABs that allows
  for global, non-separable rewards. To solve RMAB-G, the authors propose Linear-Whittle
  and Shapley-Whittle indices, which generalize Whittle indices to handle global rewards.
---

# Global Rewards in Restless Multi-Armed Bandits

## Quick Facts
- arXiv ID: 2406.00738
- Source URL: https://arxiv.org/abs/2406.00738
- Reference count: 40
- Primary result: Introduces RMAB-G with global rewards, proposes Linear-Whittle and Shapley-Whittle indices, develops adaptive policies with MCTS, shows strong performance on synthetic and food rescue datasets

## Executive Summary
This paper extends Restless Multi-Armed Bandit (RMAB) theory to handle global, non-separable rewards through the new RMAB-G framework. The authors propose two generalized Whittle indices (Linear-Whittle and Shapley-Whittle) that can handle global reward structures, along with adaptive policies that combine these indices with iterative computation or Monte Carlo Tree Search. The work provides theoretical approximation bounds for linear and near-linear reward cases while demonstrating practical effectiveness on both synthetic benchmarks and real-world food rescue optimization problems.

## Method Summary
The authors introduce RMAB-G, which generalizes standard RMABs by allowing rewards to be functions of the global state rather than just individual arm states. They develop two index-based approaches: Linear-Whittle indices for linear reward structures and Shapley-Whittle indices for near-linear cases, both providing approximation guarantees. To handle highly non-linear rewards where indices may fail, they propose adaptive policies that either iteratively compute indices based on current global state estimates or combine indices with Monte Carlo Tree Search. The framework maintains computational tractability while significantly expanding the class of reward structures that can be handled.

## Key Results
- Linear-Whittle and Shapley-Whittle indices provide approximation bounds for near-linear reward structures
- Adaptive policies (iterative and MCTS-based) outperform baselines and index-only approaches on synthetic and food rescue datasets
- Index-based methods struggle with highly non-linear rewards but adaptive approaches mitigate this limitation
- The framework successfully handles real-world food rescue optimization problems

## Why This Works (Mechanism)
The paper extends Whittle index theory to global reward settings by decomposing complex reward structures into indexable components. Linear-Whittle indices work by linearizing global rewards around current estimates, while Shapley-Whittle indices use cooperative game theory concepts to fairly allocate credit among arms. The adaptive policies overcome limitations of fixed indices by dynamically adjusting to the current global state and using MCTS to explore non-indexable regions of the reward space. This combination of theoretical generalization and practical adaptation enables handling of both near-linear and highly non-linear reward structures.

## Foundational Learning

1. **Restless Multi-Armed Bandits (RMABs)**
   - Why needed: Standard framework for sequential decision-making with multiple arms
   - Quick check: Each arm has internal state evolving independently; need to balance exploration vs. exploitation

2. **Whittle Indices**
   - Why needed: Index-based approach for approximately solving RMABs efficiently
   - Quick check: Each arm gets an index score; highest-scoring arms get activated subject to constraints

3. **Global vs. Separable Rewards**
   - Why needed: Most real-world problems have rewards depending on system-wide states
   - Quick check: Traditional RMAB assumes rewards separable across arms; global rewards require new theory

4. **Shapley Values**
   - Why needed: Fair allocation mechanism for cooperative contributions in non-linear settings
   - Quick check: From cooperative game theory; measures marginal contribution of each player

5. **Monte Carlo Tree Search (MCTS)**
   - Why needed: Balances exploration and exploitation in large decision trees
   - Quick check: Uses random sampling to estimate value of actions in complex state spaces

## Architecture Onboarding

Component Map: Problem Formulation -> Index Derivation -> Adaptive Policy Design -> Evaluation

Critical Path: Define RMAB-G → Prove index existence conditions → Derive Linear/Shapley-Whittle indices → Implement adaptive policies → Validate on benchmarks

Design Tradeoffs:
- Theoretical guarantees vs. practical applicability
- Index simplicity vs. accuracy for non-linear rewards
- Computational efficiency vs. solution quality
- Fixed vs. adaptive approaches

Failure Signatures:
- Poor performance on highly non-linear rewards (index-based methods)
- Computational intractability for large state spaces
- Suboptimal exploration in MCTS-based approaches
- Instability in iterative index computation

First Experiments:
1. Compare Linear-Whittle vs. Shapley-Whittle on synthetic linear/near-linear reward problems
2. Test adaptive policy performance degradation as non-linearity increases
3. Evaluate MCTS-based approach on small-scale problems before scaling up

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- Theoretical approximation bounds heavily depend on near-linearity assumption
- Empirical evaluation limited to specific synthetic and food rescue datasets
- Computational complexity of adaptive policies not fully characterized
- No analysis of local optima trapping in highly non-linear reward landscapes
- Scalability to very large problem instances remains unproven

## Confidence

Theoretical results: High (linear reward case), Medium (near-linear case), Low (highly non-linear rewards)
Empirical results: Medium (limited scope and potential overfitting)

## Next Checks

1. Test proposed policies across diverse real-world datasets with varying reward non-linearity levels
2. Conduct comprehensive computational complexity analysis of adaptive policies, especially MCTS-based approach
3. Explore alternative optimization techniques (gradient-based, evolutionary algorithms) for escaping local optima in highly non-linear reward landscapes