---
ver: rpa2
title: Jailbreaking Large Language Models Through Alignment Vulnerabilities in Out-of-Distribution
  Settings
arxiv_id: '2406.13662'
source_url: https://arxiv.org/abs/2406.13662
tags:
- arxiv
- llms
- prompt
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ObscurePrompt, a novel jailbreaking method
  for large language models (LLMs) inspired by observed vulnerabilities in out-of-distribution
  (OOD) data. The approach constructs base prompts using established jailbreak techniques
  and then employs powerful LLMs to iteratively transform these prompts into more
  obscure versions, enhancing attack robustness.
---

# Jailbreaking Large Language Models Through Alignment Vulnerabilities in Out-of-Distribution Settings

## Quick Facts
- arXiv ID: 2406.13662
- Source URL: https://arxiv.org/abs/2406.13662
- Authors: Yue Huang; Jingyu Tang; Dongping Chen; Bingda Tang; Yao Wan; Lichao Sun; Philip S. Yu; Xiangliang Zhang
- Reference count: 40
- One-line primary result: ObscurePrompt achieves up to 38% improvement in jailbreak success rates against models like Llama2-70b

## Executive Summary
This paper introduces ObscurePrompt, a novel jailbreaking method that exploits vulnerabilities in large language models (LLMs) when processing out-of-distribution (OOD) data. The approach constructs base prompts using established jailbreak techniques and iteratively transforms them into more obscure versions using powerful LLMs like GPT-4. Comprehensive experiments demonstrate that ObscurePrompt significantly outperforms existing baselines, achieving up to 38% improvement in attack success rates against models like Llama2-70b, while remaining effective against mainstream defenses such as paraphrasing and perplexity filtering.

## Method Summary
ObscurePrompt is a training-free, black-box method that exploits alignment vulnerabilities in out-of-distribution settings. The method takes harmful queries from the advbench dataset, applies multiple jailbreak techniques to create seed prompts, then uses GPT-4 to iteratively transform these into more obscure versions. The process is repeated multiple times to generate diverse prompt sets that evade detection by shifting semantic representations away from harmful concept spaces. The approach maintains effectiveness across various LLM architectures while remaining resistant to standard defense mechanisms.

## Key Results
- Achieves up to 38% improvement in attack success rates compared to baselines
- Effective against multiple model sizes including Llama2-7b (24% improvement) and Llama2-70b (38% improvement)
- Maintains performance against mainstream defenses like paraphrasing and perplexity filtering
- Works across different model architectures with varying optimal prompt integration strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Obscure prompts disrupt the alignment decision boundary in LLMs by shifting semantic representations away from the harmful concept space.
- Mechanism: The method uses iterative transformations with GPT-4 to make prompts more obscure, moving their embeddings into low-density regions that the model's alignment mechanisms do not robustly cover.
- Core assumption: LLMs have weaker alignment coverage for out-of-distribution (OOD) data, particularly obscure or semantically shifted inputs.
- Evidence anchors: PCA visualization showing obscure prompts positioned far from harmful concept cluster; weak evidence from emoji jailbreaking literature.
- Break condition: If alignment models improve coverage of obscure OOD space, or if defenses explicitly detect semantic shifts, the attack effectiveness would diminish.

### Mechanism 2
- Claim: Combining multiple jailbreak techniques with obscurity increases attack robustness by creating prompts that evade detection through multiple orthogonal evasion strategies.
- Mechanism: The pipeline iteratively applies seed prompt curation (multiple jailbreak methods) and obscure-guided transformation, producing diverse prompts that exploit different vulnerabilities.
- Core assumption: Different jailbreak methods exploit different LLM safety weaknesses, and obscurity adds a layer that existing defenses do not handle.
- Evidence anchors: Performance gains with multiple prompts and varied attack types; weak evidence from defense papers suggesting layered attacks are harder to block.
- Break condition: If defenses implement multi-layered detection that checks for both jailbreak patterns and semantic obscurity, or if LLMs become resistant to combined prompt engineering.

### Mechanism 3
- Claim: Iterative refinement with powerful LLMs (GPT-4) produces prompts that are both more obscure and more effective at jailbreaking than manual or simpler automated methods.
- Mechanism: GPT-4 transforms seed prompts into more obscure versions through creative rephrasing while maintaining semantic content, producing adversarial examples that evade keyword and pattern-based defenses.
- Core assumption: Powerful LLMs can generate more sophisticated semantic transformations than simpler methods or human crafting.
- Evidence anchors: GPT-4 used as obscure transformer with instruction to "make the following text more obscure"; weak evidence from emoji jailbreaking literature.
- Break condition: If LLMs develop resistance to their own transformations, or if defenses can detect GPT-4-generated obscure patterns, the iterative advantage would be lost.

## Foundational Learning

- Concept: Out-of-Distribution (OOD) data vulnerability in machine learning
  - Why needed here: The attack exploits the gap between standard training data coverage and OOD inputs, particularly obscure semantic variants
  - Quick check question: Why would an LLM be less reliable at detecting harmful content in obscure or semantically shifted prompts compared to standard harmful queries?

- Concept: Bayesian decision boundaries and latent concept spaces in LLMs
  - Why needed here: The paper models jailbreaking as shifting prompts from harmful to harmless latent concepts, which requires understanding how LLMs represent and classify inputs
  - Quick check question: How does the paper's Bayesian interpretation explain why obscuring a harmful prompt might cause it to be classified as harmless?

- Concept: Iterative adversarial example generation and refinement
  - Why needed here: The method uses repeated GPT-4 transformations to progressively improve prompt effectiveness, which is a form of adversarial example generation
  - Quick check question: What is the advantage of using iterative transformations with a powerful LLM compared to a single transformation or manual crafting?

## Architecture Onboarding

- Component map: Original harmful queries -> Seed prompt curation (4 jailbreak methods) -> GPT-4 obscurity transformation -> Repeat n times -> Attack target LLM -> Evaluate success
- Critical path: Query → Seed prompt curation → GPT-4 obscurity transformation → Repeat n times → Attack target LLM → Evaluate success
- Design tradeoffs:
  - Computational cost vs. attack success: More iterations and prompts increase success but require more GPT-4 API calls
  - Prompt complexity vs. evasion: More complex prompts may evade detection but could be harder to generate effectively
  - Semantic preservation vs. obscurity: Need to maintain harmful intent while making prompts obscure enough to bypass safety
- Failure signatures:
  - ASR plateaus early regardless of iterations (indicates saturation or detection)
  - GPT-4 transformations produce non-obscure or semantically broken outputs
  - Target LLM consistently refuses even obscure prompts (indicates strong alignment)
  - Performance degrades significantly on larger models (indicates scaling defense effectiveness)
- First 3 experiments:
  1. Test single iteration with one jailbreak method vs. no obscurity to establish baseline effectiveness
  2. Test multiple iterations (n=2,3,5) with same jailbreak method to measure iterative improvement
  3. Test single iteration with all jailbreak methods combined vs. individual methods to assess combination effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of model parameter size beyond which the effectiveness of ObscurePrompt significantly diminishes?
- Basis in paper: The paper states "Scaling laws in safety alignment indicate that larger model parameters enhance LLM robustness against our attack" and compares Llama2-7b vs Llama2-70b, Llama3-8b vs Llama3-70b.
- Why unresolved: The experiments only tested up to 70 billion parameter models. The relationship between model size and attack effectiveness beyond this range remains unknown.
- What evidence would resolve it: Systematic testing of ObscurePrompt across a broader range of model sizes (e.g., 175B, 540B, and beyond) to identify the inflection point where attack effectiveness plateaus or declines.

### Open Question 2
- Question: How does ObscurePrompt perform against models specifically trained with adversarial robustness techniques or adversarial training?
- Basis in paper: The paper mentions evaluating against "mainstream defenses" like paraphrasing and perplexity filtering, but does not explicitly test against models trained with adversarial robustness techniques.
- Why unresolved: The paper only evaluates against standard aligned models and two specific defense mechanisms, leaving the effectiveness against adversarially robust models unexplored.
- What evidence would resolve it: Testing ObscurePrompt against models specifically trained with adversarial examples or robustness techniques, comparing success rates to standard aligned models.

### Open Question 3
- Question: What is the optimal balance between the number of integrated prompts and attack success rate across different LLM architectures?
- Basis in paper: Figure 5 shows that "the effect of the number of integrated prompts varies across different LLMs" with different patterns for different models.
- Why unresolved: While the paper observes varying patterns, it doesn't identify an optimal strategy for selecting the number of prompts based on model architecture or characteristics.
- What evidence would resolve it: Detailed analysis correlating model architectural features (e.g., attention patterns, layer depth) with optimal prompt integration numbers, potentially leading to a predictive model for selecting prompt quantities.

### Open Question 4
- Question: How does the semantic obfuscation technique used in ObscurePrompt compare to other forms of linguistic manipulation (e.g., code-switching, metaphor, or domain-specific jargon) in terms of attack effectiveness?
- Basis in paper: The paper uses GPT-4 to make prompts "more obscure" but doesn't compare this technique to alternative linguistic manipulation strategies.
- Why unresolved: The paper only evaluates one specific obfuscation method without exploring whether other linguistic techniques might be more or less effective.
- What evidence would resolve it: Comparative experiments testing ObscurePrompt against versions using alternative linguistic manipulation techniques, measuring attack success rates across various model types.

### Open Question 5
- Question: What is the relationship between the semantic distance introduced by obfuscation and the attack success rate?
- Basis in paper: Section 5.3 evaluates "semantic shifting" and shows that "obscure queries maintain a semantic similarity close to the paraphrased queries" while still being effective.
- Why unresolved: While the paper confirms semantic consistency, it doesn't quantify the relationship between how much the semantic meaning is shifted and how this affects attack success rates.
- What evidence would resolve it: Systematic experiments varying the degree of semantic shift (measured by embedding similarity) and measuring corresponding changes in attack success rates across different models.

## Limitations

- Dataset scope limitation: Evaluation relies exclusively on 520 harmful queries from the advbench dataset, which may not capture full diversity of real-world jailbreaking attempts
- GPT-4 dependency vulnerability: Method's effectiveness critically depends on GPT-4's ability to generate obscure transformations, making it vulnerable to changes in GPT-4's behavior or OpenAI's defenses
- Defense evaluation incompleteness: Does not evaluate against more sophisticated alignment techniques like adversarial training on obscure prompts or multi-stage filtering systems

## Confidence

- **High confidence**: The core observation that obscurity can help evade LLM safety mechanisms is well-supported by experimental results showing consistent ASR improvements across multiple model sizes and datasets
- **Medium confidence**: The claim of "38% improvement" requires careful interpretation as baseline comparison details and statistical significance testing are not fully specified
- **Low confidence**: The assertion that ObscurePrompt will remain effective as a general jailbreaking method against future defenses is speculative given rapidly evolving attack and defense techniques

## Next Checks

1. **Cross-dataset generalization test**: Evaluate ObscurePrompt on a broader range of harmful prompts from multiple sources to assess whether observed improvements hold across different types of jailbreaking attempts and content domains

2. **Defense robustness assessment**: Implement and test against more sophisticated defense mechanisms including adversarial training on obscure prompts, multi-stage filtering combining keyword detection with semantic analysis, and defenses specifically designed to detect GPT-4-generated transformations

3. **GPT-4 dependency stress test**: Systematically vary GPT-4's generation parameters (temperature, top-p, etc.) and test with different prompt formulations to understand sensitivity of ObscurePrompt to choice of transformer and its configuration; additionally test whether fine-tuned versions of GPT-4 with safety constraints significantly degrade attack effectiveness