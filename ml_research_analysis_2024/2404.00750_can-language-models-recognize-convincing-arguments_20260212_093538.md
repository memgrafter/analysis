---
ver: rpa2
title: Can Language Models Recognize Convincing Arguments?
arxiv_id: '2404.00750'
source_url: https://arxiv.org/abs/2404.00750
tags:
- llms
- debate
- dataset
- arguments
- persuasive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether large language models can detect arguments
  that are persuasive to specific demographics without direct human experimentation.
  The authors extend a debate dataset with user demographics and beliefs, then test
  whether LLMs can predict which side in a debate is more convincing, forecast users'
  stances before and after reading debates, and determine how arguments appeal to
  individuals based on their traits.
---

# Can Language Models Recognize Convincing Arguments?

## Quick Facts
- arXiv ID: 2404.00750
- Source URL: https://arxiv.org/abs/2404.00750
- Reference count: 21
- Primary result: LLMs achieve human-level performance in detecting persuasive arguments for specific demographics

## Executive Summary
This paper investigates whether large language models can detect arguments that are persuasive to specific demographics without direct human experimentation. The authors extend a debate dataset with user demographics and beliefs, then test whether LLMs can predict which side in a debate is more convincing, forecast users' stances before and after reading debates, and determine how arguments appeal to individuals based on their traits. Across three research questions, LLMs perform similarly to humans, with GPT-4 achieving 60.50% accuracy in judging argument quality versus 60.69% for humans. When predicting stances before debates, Mistral achieved 42.27% accuracy versus 39.86% for crowdworkers. Combining predictions from different LLMs through stacking significantly improved performance, surpassing human accuracy in predicting post-debate stances (46.86% vs 39.86%). The study suggests LLMs can detect persuasive content tailored to specific demographics, raising concerns about their potential use in microtargeting misinformation.

## Method Summary
The authors extended the PoliProp dataset with user demographics and beliefs, creating a corpus of 833 politics-related debates. They tested four LLMs (GPT-3.5, GPT-4, Llama 2, Mistral 7B) using zero-shot prompting on three tasks: identifying convincing arguments, predicting stances before debates, and predicting stance changes after debates. LLM predictions were compared against crowdsourced human labels from MTurk. The researchers also implemented a stacking approach, using LLM predictions as features in a logistic regression model to combine their outputs. Performance was evaluated using accuracy metrics and compared against both human performance and an XGBoost baseline model.

## Key Results
- GPT-4 achieves 60.50% accuracy in argument quality assessment, comparable to human performance at 60.69%
- Stacked LLM predictions surpass human performance in predicting post-debate stances (46.86% vs 39.86%)
- LLMs successfully predict user stances before debates based on demographic and belief data (Mistral: 42.27% vs crowdworkers: 39.86%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can detect persuasive arguments for specific demographics by mapping correlations between user traits and stances.
- Mechanism: The study shows LLMs predict user stances before and after debates based on demographic and belief data, achieving human-level accuracy. GPT-4 scores 60.50% in judging argument quality versus 60.69% for humans, and stacking predictions from multiple LLMs further improves accuracy.
- Core assumption: User traits and beliefs are sufficiently predictive of argument persuasiveness for LLMs to learn the mapping.
- Evidence anchors:
  - [abstract] "We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance."
  - [section 5] "GPT-4 (60.50% accuracy) is as good as an individual voter in the dataset (Accuracy: 60.69%)."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.429, average citations=0.0. Top related titles include "Large Language Models are as persuasive as humans, but how?" indicating active research on LLM persuasion mechanisms.
- Break condition: If user traits and beliefs are not strongly correlated with argument persuasiveness, or if the LLM cannot learn these correlations from the dataset, performance would drop significantly.

### Mechanism 2
- Claim: Stacking predictions from multiple LLMs improves detection of persuasive arguments.
- Mechanism: Different LLMs have varying biases and strengths, so combining their outputs through a supervised learning model (e.g., logistic regression) yields better performance than any single model. Stacked models surpass human performance in predicting post-debate stances (46.86% vs 39.86%).
- Core assumption: Different LLMs have complementary strengths and biases in detecting persuasive content.
- Evidence anchors:
  - [section 5] "We find that their inter-annotator agreement is quite low (Cohen's κ is smaller than 0.2 for most pairs of models)... Nevertheless, each model seems to perform well on a different subset of the debates."
  - [section 5] "stacking the predictions of LLMs and using them as features in a supervised learning setting reduces the performance gap (45.91%)."
  - [corpus] No direct corpus evidence for stacking mechanisms, but related work on ensemble methods in NLP supports this approach.
- Break condition: If LLMs have highly correlated predictions (low diversity), stacking would not provide additional benefit.

### Mechanism 3
- Claim: LLM performance on persuasion detection tasks indicates potential for misuse in microtargeting misinformation.
- Mechanism: If LLMs can identify which arguments are persuasive to specific demographics, they could be used to generate or amplify tailored misinformation. The study suggests this capability exists at human-level performance.
- Core assumption: The ability to detect persuasive arguments implies the ability to generate them.
- Evidence anchors:
  - [abstract] "The study suggests LLMs can detect persuasive content tailored to specific demographics, raising concerns about their potential use in microtargeting misinformation."
  - [introduction] "Previous work has found LLMs to be persuasive in the generative setting... assessing models' capacity to detect content persuasive to specific demographics can be done quickly and without interaction with human subjects."
  - [corpus] "Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics" directly addresses LLM persuasion with personalization.
- Break condition: If the gap between detection and generation capabilities is significant, or if safeguards prevent misuse, the risk would be mitigated.

## Foundational Learning

- Concept: Argument quality assessment
  - Why needed here: The study measures LLMs' ability to judge argument quality (RQ1), which is fundamental to understanding their persuasive capabilities.
  - Quick check question: What are the different ways scholars define argument quality, and how might these definitions affect LLM evaluation?

- Concept: Demographics and persuasion
  - Why needed here: The study leverages demographic and belief data to predict stances, requiring understanding of how these factors influence persuasion.
  - Quick check question: How do group-level demographic factors like race, religion, and education shape individuals' perspectives on political issues?

- Concept: Machine learning stacking techniques
  - Why needed here: The study uses stacked predictions from multiple LLMs to improve performance, requiring knowledge of ensemble methods.
  - Quick check question: What are the benefits and potential drawbacks of using stacked predictions from multiple models?

## Architecture Onboarding

- Component map: Data preprocessing -> LLM inference -> Post-processing -> Evaluation -> Stacking
- Critical path:
  1. Preprocess debate data and extract user traits
  2. Generate prompts for each LLM
  3. Run inference and post-process responses
  4. Evaluate individual LLM performance
  5. Stack predictions and evaluate combined performance
- Design tradeoffs:
  - Open-source vs. closed-source LLMs: Open-source models are more accessible but may have lower performance
  - Prompt engineering: More detailed prompts may improve accuracy but increase computational cost
  - Post-processing: Simple regex extraction is fast but may miss nuanced answers
- Failure signatures:
  - Low accuracy across all models: Indicates fundamental issues with the approach or data
  - High variance in individual model performance: Suggests instability in LLM predictions
  - Poor performance on specific demographics: Reveals potential biases in the model or data
- First 3 experiments:
  1. Compare individual LLM performance on a small subset of debates to establish baseline accuracy
  2. Test different prompt structures to optimize LLM responses
  3. Evaluate stacking performance with different supervised learning models (e.g., logistic regression vs. XGBoost)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on persuasion tasks across different languages and cultural contexts?
- Basis in paper: Explicit - The authors note that their dataset was in English and recent research shows LLM performance is lower for non-English languages, especially low-resource ones.
- Why unresolved: The study only tested English-language debates from a U.S.-based platform. The authors call for expanding research to "a wider range of languages and demographics" but haven't done this yet.
- What evidence would resolve it: Testing the same methodology on debates in multiple languages and cultural contexts, comparing LLM performance across these variants.

### Open Question 2
- Question: What specific personal information beyond demographics would enable LLMs to detect persuasive arguments with significantly higher accuracy?
- Basis in paper: Explicit - The authors hypothesize that "with access to more personal information about an individual, such as personality traits, LLMs could perform better at detecting persuasive arguments" and reference previous work showing personality trait tailoring improves persuasion.
- Why unresolved: The current study only used demographic data and broad belief stances. The authors acknowledge this limitation but didn't test more granular personal information.
- What evidence would resolve it: Controlled experiments testing LLM performance using various types of personal data (personality traits, behavioral patterns, detailed preference data) while holding other variables constant.

### Open Question 3
- Question: Do LLMs actually influence human opinions in real-world settings, or just predict them?
- Basis in paper: Explicit - The authors state "it is essential to conduct empirical studies to understand whether LLMs are, in fact, being used for persuasion in online settings (e.g., in social media platforms)" and distinguish between detection and generation capabilities.
- Why unresolved: This study only examined LLMs' ability to detect persuasive content, not their ability to generate or deploy persuasive content in real settings.
- What evidence would resolve it: Field experiments or observational studies measuring actual persuasion outcomes when humans interact with LLM-generated content versus human-generated content in authentic online environments.

## Limitations

- The study relies on a single dataset from 2018, which may not represent current political discourse or newer demographic groups
- Zero-shot prompting approach may not fully leverage LLM capabilities compared to fine-tuning methods
- Low inter-annotator agreement among different LLMs (Cohen's κ < 0.2) suggests significant variability in model predictions

## Confidence

- **High confidence**: LLMs can achieve human-level performance in argument quality assessment (RQ1)
- **Medium confidence**: Stacked LLM predictions consistently outperform individual models and humans in stance prediction (RQ3)
- **Medium confidence**: Demographic and belief data are sufficient for LLMs to predict stance changes after exposure to debates (RQ2)

## Next Checks

1. **Dataset recency validation**: Test the same methodology on more recent debate datasets to verify whether LLMs maintain human-level performance as political discourse evolves.

2. **Fine-tuning comparison**: Compare zero-shot prompting results with fine-tuned models on the same persuasion detection tasks to establish whether the current approach captures the full potential of LLM capabilities.

3. **Cross-cultural generalization**: Validate whether the correlation between demographic traits and argument persuasiveness holds across different cultural contexts and political systems beyond the US-focused dataset used in this study.