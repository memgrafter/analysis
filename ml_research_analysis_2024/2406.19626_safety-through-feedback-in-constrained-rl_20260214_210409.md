---
ver: rpa2
title: Safety through feedback in Constrained RL
arxiv_id: '2406.19626'
source_url: https://arxiv.org/abs/2406.19626
tags:
- cost
- feedback
- function
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforcement Learning from Safety Feedback
  (RLSF), a constrained RL method that learns safety constraints through trajectory-level
  feedback without assuming any structure on the cost function. The key innovation
  is a surrogate loss that transforms trajectory-level cost inference into a state-level
  binary classification problem with noisy labels, making the problem tractable.
---

# Safety through feedback in Constrained RL

## Quick Facts
- arXiv ID: 2406.19626
- Source URL: https://arxiv.org/abs/2406.19626
- Reference count: 40
- Key outcome: RLSF learns safety constraints through trajectory-level feedback without assuming cost function structure, achieving near-optimal performance with only 20-30 feedback queries per episode.

## Executive Summary
This paper introduces Reinforcement Learning from Safety Feedback (RLSF), a constrained RL method that learns safety constraints through trajectory-level feedback without assuming any structure on the cost function. The key innovation is a surrogate loss that transforms trajectory-level cost inference into a state-level binary classification problem with noisy labels, making the problem tractable. The method also introduces novelty-based sampling to selectively query the evaluator only when the agent encounters novel trajectories, significantly reducing the number of required feedback queries. Experiments on benchmark Safety Gymnasium environments and realistic driving scenarios show that RLSF achieves near-optimal performance compared to when the cost function is known, with only trajectory-level feedback.

## Method Summary
RLSF addresses the challenge of constrained RL when the cost function is unknown and must be learned from trajectory-level feedback. The method uses a surrogate loss that converts the trajectory-level feedback into a tractable state-level binary classification problem, allowing efficient learning of the cost function. To reduce the number of feedback queries, RLSF employs novelty-based sampling that only presents novel trajectories to the evaluator. The method alternates between data collection, cost function estimation using the surrogate loss, and policy improvement using PPO-Lagrangian with the inferred costs. The approach ensures safety by proving that the inferred cost function serves as an upper bound on the true cost, guaranteeing that safety under the inferred cost implies safety under the true cost.

## Key Results
- RLSF achieves near-optimal performance compared to when the cost function is known, with only trajectory-level feedback
- Novelty-based sampling reduces feedback queries to 20-30 per episode while maintaining safety performance
- The inferred cost function serves as an upper bound on the true cost, ensuring safety guarantees
- RLSF successfully transfers learned cost functions across different agent morphologies in the same task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate loss converts trajectory-level feedback into a tractable state-level binary classification problem.
- Mechanism: The original MLE loss over trajectory segments suffers from numerical instability because the product of probabilities over long segments can collapse to zero. The surrogate loss replaces the exact segment probability with a product of complements, breaking the problem into independent per-state classification terms.
- Core assumption: States within a segment labeled unsafe are not all equally responsible for the unsafe label, but the surrogate loss tolerates this imperfect credit assignment if enough samples are collected.
- Evidence anchors:
  - [abstract] "To address this, we propose a surrogate objective that transforms the problem into a state-level supervised classification task with noisy labels, which can be solved efficiently."
  - [section 4.2] Derivation showing the surrogate loss Lsur as a binary classification problem with noisy labels.
  - [corpus] None directly supports this specific surrogate loss mechanism; evidence is internal to the paper.
- Break condition: If the segment length is very long and violations are sparse, the noise in labels may overwhelm the signal, leading to poor cost function estimation.

### Mechanism 2
- Claim: Novelty-based sampling reduces the number of feedback queries by only asking about novel trajectories.
- Mechanism: A SimHash-based state density map tracks how often states have been seen in past feedback. Trajectories containing e novel states (states with zero density) are presented for feedback, reducing redundancy.
- Core assumption: The policy evolves over time and will generate novel states that the current cost estimator is uncertain about; querying these improves learning more than re-querying known states.
- Evidence anchors:
  - [abstract] "Additionally, it is often infeasible to collect feedback on every trajectory generated by the agent... we introduce novelty-based sampling that selectively involves the evaluator only when the agent encounters a novel trajectory."
  - [section 4.3] Formal definition of novelty and description of the sampling scheme.
  - [corpus] No external validation of novelty-based sampling; effectiveness is shown only in the paper's experiments.
- Break condition: If the policy converges quickly and novel states become rare, novelty-based sampling may reduce feedback to the point where cost estimation quality degrades.

### Mechanism 3
- Claim: The inferred cost function c* serves as an upper bound on the true cost, ensuring safety.
- Mechanism: Because safe states mislabeled as unsafe do not affect safety (they are simply penalized more), and unsafe states are always correctly labeled, the expected cost under c* is always at least as large as under the true cost.
- Core assumption: The evaluator's binary feedback mechanism ensures that every segment containing at least one unsafe state is labeled unsafe, making false negatives in cost estimation impossible.
- Evidence anchors:
  - [section 4.2] Proposition 3 and Corollary 1 proving that Eπ[c*(s,a)] ≥ Eπ[cgt(s,a)] and that safety under c* implies safety under cgt.
  - [abstract] "Thus, ensuring that the policy does not exceed the threshold c0 on c* guarantees that it adheres to the threshold on cgt."
  - [corpus] None; safety guarantees are derived internally.
- Break condition: If the evaluator makes mistakes on segments that are actually safe but contain rare unsafe states, the upper bound property could be violated.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: The paper solves an RL problem with an additional safety constraint, requiring the CMDP framework to formalize feasibility and safety thresholds.
  - Quick check question: In a CMDP, what defines the set of feasible policies Πc?
- Concept: Binary classification with noisy labels
  - Why needed here: The surrogate loss turns the problem into a binary classification task where some safe states are mislabeled as unsafe, requiring techniques to handle label noise.
  - Quick check question: Why does the surrogate loss tolerate noisy labels better than the original MLE loss?
- Concept: Novelty detection and SimHash
  - Why needed here: Novelty-based sampling relies on identifying novel states efficiently; SimHash provides a scalable way to discretize continuous states for density tracking.
  - Quick check question: How does SimHash map similar states to similar binary codes?

## Architecture Onboarding

- Component map:
  - Policy network (actor) -> Value network (critic) -> Cost classifier network
  - SimHash-based novelty tracker -> Feedback buffer
- Critical path:
  1. Roll out current policy to collect trajectories and rewards.
  2. Select novel trajectories using the novelty tracker.
  3. Query evaluator for segment-level feedback on selected trajectories.
  4. Update cost classifier using the surrogate loss on feedback buffer.
  5. Infer costs for all collected trajectories.
  6. Update policy using PPO-Lagrangian with inferred costs.
- Design tradeoffs:
  - Segment length k vs. feedback cost: longer segments reduce feedback queries but increase label noise.
  - SimHash granularity n vs. novelty detection accuracy: larger n reduces collisions but increases memory.
  - Classifier capacity vs. overfitting: must generalize across novel states without memorizing training segments.
- Failure signatures:
  - High cost violation rate despite training → cost classifier not learning correctly (check feedback buffer diversity, label noise).
  - Training stalls early → novelty tracker not detecting novelty (check SimHash parameters, density updates).
  - High variance in returns → policy updates unstable due to noisy cost estimates (check surrogate loss stability, batch size).
- First 3 experiments:
  1. Run on Point Circle with known cost as sanity check; verify policy matches PPO-Lagrangian performance.
  2. Run on Point Circle with simulated feedback; verify cost classifier learns reasonable safe/unsafe boundaries.
  3. Run on Point Goal with varying segment lengths; observe trade-off between feedback cost and safety performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of segment length k affect the bias in the inferred cost function and the overall safety performance of the learned policy?
- Basis in paper: [explicit] The paper discusses the trade-off between segment length and the difficulty of assigning credit to individual states, and mentions that shorter segments reduce the bias but increase the cost of obtaining feedback.
- Why unresolved: The paper does not provide a systematic study of how different segment lengths impact the bias and performance across various environments.
- What evidence would resolve it: A comprehensive experimental analysis comparing different segment lengths and their effects on bias, safety performance, and feedback efficiency across multiple benchmark environments.

### Open Question 2
- Question: Can the novelty-based sampling mechanism be improved by incorporating additional metrics beyond state novelty, such as trajectory similarity or cost uncertainty?
- Basis in paper: [inferred] The paper introduces novelty-based sampling but only considers state novelty, leaving room for incorporating other metrics that might better capture the informativeness of trajectories.
- Why unresolved: The current novelty-based sampling only uses a count-based method with SimHash, without exploring other potential metrics or combining them with state novelty.
- What evidence would resolve it: Experiments comparing novelty-based sampling with and without additional metrics, demonstrating improved performance or reduced feedback requirements.

### Open Question 3
- Question: How does the performance of RLSF compare to other state-of-the-art methods for learning cost functions from human feedback in terms of sample efficiency and safety?
- Basis in paper: [explicit] The paper compares RLSF to SIM and SDM baselines but does not compare it to other methods like Active Preference-based Learning or other active learning approaches.
- Why unresolved: The paper focuses on comparing RLSF to baselines within the constrained RL framework but does not explore comparisons with methods from the broader literature on learning from human feedback.
- What evidence would resolve it: Benchmarking RLSF against other state-of-the-art methods for learning cost functions from human feedback, evaluating both sample efficiency and safety performance.

## Limitations
- The safety guarantees rely on the assumption that all unsafe states within a segment are always labeled as unsafe, which may not hold if the evaluator makes mistakes.
- The surrogate loss mechanism, while theoretically sound, lacks external validation from other work.
- The novelty-based sampling approach's effectiveness depends heavily on the policy generating novel states, which may not occur in stable environments.

## Confidence
- **High confidence**: The overall framework of alternating between data collection, feedback, and policy updates is well-established. The safety upper bound property (Mechanism 3) has formal proofs.
- **Medium confidence**: The surrogate loss mechanism (Mechanism 1) is theoretically derived but lacks external validation. The novelty-based sampling (Mechanism 2) is innovative but effectiveness is only shown in this paper's experiments.
- **Low confidence**: The practical effectiveness of the method in highly stochastic or sparse-reward environments is unclear from the presented experiments.

## Next Checks
1. Test safety guarantees under evaluator noise: Introduce varying levels of noise into the feedback mechanism and measure how often the safety upper bound property is violated.
2. Validate surrogate loss robustness: Compare the surrogate loss performance against the original MLE loss across different segment lengths and label noise levels on a controlled synthetic task.
3. Evaluate SimHash collision impact: Systematically vary the SimHash parameter n and measure its effect on novelty detection accuracy and feedback query reduction.