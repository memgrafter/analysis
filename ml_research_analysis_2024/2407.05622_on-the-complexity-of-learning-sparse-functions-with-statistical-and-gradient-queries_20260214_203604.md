---
ver: rpa2
title: On the Complexity of Learning Sparse Functions with Statistical and Gradient
  Queries
arxiv_id: '2407.05622'
source_url: https://arxiv.org/abs/2407.05622
tags:
- learning
- loss
- queries
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the complexity of learning sparse functions
  (juntas) using gradient-type algorithms. The authors introduce Differentiable Learning
  Queries (DLQ), a query model capturing gradient evaluations on arbitrary loss functions
  with respect to any model, and provide a tight characterization of DLQ complexity
  for learning the support of sparse functions over product distributions.
---

# On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries

## Quick Facts
- arXiv ID: 2407.05622
- Source URL: https://arxiv.org/abs/2407.05622
- Reference count: 40
- This paper studies the complexity of learning sparse functions (juntas) using gradient-type algorithms and provides a tight characterization of Differentiable Learning Queries (DLQ) complexity for learning support of sparse functions.

## Executive Summary
This paper introduces Differentiable Learning Queries (DLQ) as a query model capturing gradient evaluations on arbitrary loss functions with respect to any model. The authors provide a tight characterization of DLQ complexity for learning the support of sparse functions over product distributions, showing that DLQ complexity depends critically on the choice of loss function. For squared loss, DLQ matches Correlation Statistical Queries (CSQ) complexity, but for losses like ℓ₁ or exponential loss, DLQ achieves the same complexity as general Statistical Queries (SQ).

The paper establishes that DLQ captures important aspects of SGD complexity by analyzing learning with two-layer neural networks in mean-field regime and linear scaling. The main result characterizes query complexity of learning juntas using adaptive/non-adaptive SQ, CSQ, and DLQ algorithms in terms of "leap" and "cover" exponents, providing a unified framework for understanding complexity of learning sparse functions across different query models and loss functions.

## Method Summary
The authors introduce Differentiable Learning Queries (DLQ) as a query model where an algorithm can query the gradient of any loss function with respect to any model at any point. They analyze the complexity of learning juntas (sparse functions) using DLQ, Statistical Queries (SQ), and Correlation Statistical Queries (CSQ) under product distributions. The analysis characterizes the minimum number of queries needed to learn the support of a sparse function in terms of "leap" and "cover" exponents, which measure the minimum combination of detectable sets needed to cover the support. The authors also provide theoretical evidence that DLQ captures important aspects of SGD complexity by analyzing specific settings like mean-field regime for two-layer networks.

## Key Results
- DLQ complexity depends critically on the choice of loss function: squared loss matches CSQ complexity, while ℓ₁ and exponential losses achieve SQ complexity
- The paper provides tight bounds on the number of DLQ, SQ, and CSQ queries needed to learn junta supports in terms of leap and cover exponents
- DLQ correctly characterizes the complexity of learning with two-layer neural networks in mean-field regime and linear scaling
- For any fixed distribution and bounded loss, the DLQ complexity of learning a concept class is characterized by a specific class-specific parameter

## Why This Works (Mechanism)
The mechanism behind DLQ's effectiveness lies in its ability to capture gradient information for arbitrary loss functions, which provides more powerful information than statistical queries alone. When using appropriate loss functions (like ℓ₁ or exponential loss), DLQ can extract information about the underlying sparse function's support that is equivalent to what general statistical queries can provide. The critical insight is that the choice of loss function determines whether the gradient information is sufficiently rich to overcome the limitations of correlation-based queries.

## Foundational Learning

**Statistical Queries (SQ)**
Why needed: Basic framework for understanding query-based learning complexity
Quick check: Can verify SQ bounds by checking if queries can distinguish between hypotheses

**Differentiable Learning Queries (DLQ)**
Why needed: Extends SQ to capture gradient-based learning methods
Quick check: Verify by checking if loss function gradients provide sufficient information

**Correlation Statistical Queries (CSQ)**
Why needed: Captures limitations of squared loss in gradient-based learning
Quick check: Can test by comparing CSQ and SQ bounds for specific problems

**Leap and Cover Exponents**
Why needed: Mathematical framework for characterizing query complexity of sparse learning
Quick check: Can verify by checking if these exponents predict actual query complexity

## Architecture Onboarding

**Component Map**
Product Distribution -> Loss Function -> Query Model (SQ/CSQ/DLQ) -> Learning Algorithm -> Support Recovery

**Critical Path**
Loss Function Selection -> Query Complexity Analysis -> Algorithm Design -> Support Recovery

**Design Tradeoffs**
- Choice of loss function: Squared loss limits to CSQ complexity vs other losses achieving SQ complexity
- Distribution assumptions: Product distributions enable clean analysis but may limit practical applicability
- Query adaptivity: Adaptive queries can reduce complexity but require more sophisticated algorithms

**Failure Signatures**
- Using squared loss when SQ complexity is needed (results in CSQ-limited performance)
- Applying results to non-product distributions without verification
- Assuming DLQ captures all aspects of practical SGD behavior

**First Experiments**
1. Compare DLQ complexity predictions with actual SGD performance on learning sparse linear models
2. Test whether different loss functions achieve predicted complexity bounds on synthetic data
3. Verify leap/cover exponent predictions by measuring actual query complexity on various junta classes

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on product distributions, which may not capture the complexity of real-world data distributions
- The connection between DLQ and practical SGD behavior is established through specific examples but lacks broader empirical validation
- The characterization of DLQ complexity in terms of leap and cover exponents relies on assumptions about loss functions and distributions

## Confidence
- High confidence in theoretical characterization of DLQ complexity for specific settings analyzed
- Medium confidence in broader claims about DLQ capturing SGD complexity
- Medium confidence in applicability of results beyond product distributions

## Next Checks
1. Empirically validate DLQ complexity predictions by comparing them with actual SGD performance on learning sparse functions with various loss functions and data distributions
2. Extend analysis to non-product distributions and assess whether leap/cover exponent framework remains predictive
3. Test theoretical predictions on more complex sparse function classes beyond junta setting, including high-dimensional sparse linear models and decision trees