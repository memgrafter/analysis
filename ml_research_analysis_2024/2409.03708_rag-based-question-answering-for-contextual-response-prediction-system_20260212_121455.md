---
ver: rpa2
title: RAG based Question-Answering for Contextual Response Prediction System
arxiv_id: '2409.03708'
source_url: https://arxiv.org/abs/2409.03708
tags:
- responses
- arxiv
- response
- question
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an end-to-end RAG framework for generating response
  suggestions in customer service contact centers. It retrieves relevant knowledge
  base articles and generates responses using LLMs.
---

# RAG based Question-Answering for Contextual Response Prediction System

## Quick Facts
- arXiv ID: 2409.03708
- Source URL: https://arxiv.org/abs/2409.03708
- Reference count: 40
- Primary result: RAG-based system improves response accuracy and reduces hallucination in customer service by grounding LLM responses in retrieved knowledge base articles

## Executive Summary
This paper proposes an end-to-end RAG framework for generating response suggestions in customer service contact centers. The system retrieves relevant knowledge base articles using Vertex AI textembedding-gecko@001 with ScaNN retrieval, then generates responses using PaLM2 LLM. Evaluated on company data and open-source datasets, the RAG-based approach demonstrates improvements in accuracy, relevance, and reduction in hallucination compared to existing BERT-based methods. Human evaluation shows higher contextual relevance, specificity, and completeness for RAG responses. However, advanced prompting techniques like ReAct and Chain-of-Thought/Verification did not yield significant improvements due to latency concerns in real-time settings.

## Method Summary
The system implements a RAG pipeline that first embeds customer queries using Vertex AI textembedding-gecko@001, retrieves semantically relevant knowledge base articles via ScaNN retrieval, applies a 0.7 cosine similarity threshold to filter irrelevant retrievals, and generates responses using PaLM2 LLM with the retrieved context. The framework is evaluated against company-specific data and open-source datasets, comparing performance with BERT-based methods and testing advanced prompting techniques like ReAct. Human evaluation assesses contextual relevance, specificity, and completeness of generated responses.

## Key Results
- RAG-based responses demonstrate higher contextual relevance, specificity, and completeness compared to BERT-based methods based on human evaluation
- Retrieval threshold of 0.7 effectively separates relevant from irrelevant queries (98.59% of out-of-domain queries scored below 0.7, while 88.96% of relevant company queries scored above 0.7)
- Advanced prompting techniques like ReAct improved accuracy by 7% and reduced hallucination by 13.5% but resulted in 6% slower performance, making them impractical for real-time customer service

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves response accuracy and reduces hallucination compared to BERT-based methods by grounding LLM responses in retrieved knowledge base articles.
- Mechanism: The retriever component uses Vertex AI textembedding-gecko@001 with ScaNN retrieval to find semantically relevant KB articles, which are then incorporated into the LLM prompt for response generation.
- Core assumption: Retrieved documents contain the necessary factual information to ground the response, and the LLM can effectively utilize this context.
- Evidence anchors:
  - [abstract] "RAG-based responses demonstrate higher contextual relevance, specificity, and completeness based on human evaluation"
  - [section 4.1] "Vertex AI - textembedding-gecko@001 (768) embedding, paired with ScaNN retrieval, yielded the best outcomes"
  - [corpus] Weak evidence - no direct citations found in related papers
- Break condition: If retrieval quality is poor (low recall) or the LLM fails to properly utilize the retrieved context, the benefits of RAG will diminish.

### Mechanism 2
- Claim: Setting an appropriate retrieval threshold (0.7) effectively determines when retrieval is needed, improving response generation efficiency.
- Mechanism: Queries with cosine similarity scores below 0.7 are deemed irrelevant to the KB, so no retrieval occurs, saving computation time.
- Core assumption: A 0.7 threshold effectively separates relevant from irrelevant queries.
- Evidence anchors:
  - [section 4.1] "98.59% of retrieved articles having a cosine similarity score below 0.7" for out-of-domain queries, and "88.96% of articles retrieved for relevant company data questions scored above 0.7"
  - [corpus] Weak evidence - no direct citations found in related papers
- Break condition: If the threshold is too high, relevant queries might be incorrectly skipped; if too low, irrelevant documents might be retrieved.

### Mechanism 3
- Claim: ReAct prompting does not significantly improve factual accuracy in real-time settings due to latency concerns.
- Mechanism: ReAct introduces additional reasoning steps before retrieval, but this increases latency beyond acceptable limits for customer service.
- Core assumption: Real-time customer service requires low latency, making the additional processing time of ReAct unacceptable.
- Evidence anchors:
  - [abstract] "advanced prompting techniques like ReAct and Chain-of-Thought/Verification did not yield significant improvements due to latency concerns"
  - [section 4.3] "ReAct improved the accuracy by 7% and reduced hallucination by 13.5%, it resulted in slower performance 6, making it inconvenient in real-time conversation"
  - [corpus] Weak evidence - no direct citations found in related papers
- Break condition: If latency requirements are relaxed or if the latency penalty can be mitigated, ReAct might become viable.

## Foundational Learning

- Concept: Embedding strategies and their impact on retrieval quality
  - Why needed here: Different embedding models capture semantic relationships differently, directly affecting which documents are retrieved and thus the quality of generated responses
  - Quick check question: Why did Vertex AI textembedding-gecko@001 outperform USE and SBERT in this specific use case?

- Concept: Retrieval threshold optimization
  - Why needed here: Setting the right threshold balances retrieval coverage with computational efficiency and relevance
  - Quick check question: How would you determine the optimal threshold if the distribution of cosine similarity scores were different?

- Concept: Automated evaluation metrics for RAG systems
  - Why needed here: Automated metrics provide scalable assessment of accuracy, hallucination, and missing rates without expensive human evaluation
  - Quick check question: What are the limitations of using automated metrics like AlignScore and semantic similarity for evaluating RAG responses?

## Architecture Onboarding

- Component map: Query input → Embedding (Vertex AI) → Retrieval (ScaNN) → Threshold filtering → LLM (PaLM2) → Response output
- Critical path: Query embedding → Document retrieval → Response generation
- Design tradeoffs: Accuracy vs. latency (ReAct improves accuracy but increases latency beyond acceptable limits), retrieval threshold (higher threshold reduces irrelevant retrievals but may miss relevant cases), embedding size (larger embeddings may capture more nuance but increase computational cost)
- Failure signatures: Low accuracy despite good retrieval (LLM not properly utilizing retrieved context), high hallucination rate (retrieved documents not relevant or LLM ignoring them), high missing rate (retrieval threshold too high or embedding not capturing query semantics)
- First 3 experiments: 1) Compare recall@K for different embedding-retrieval combinations (USE vs SBERT vs Vertex AI with ScaNN vs HNSW), 2) Test different retrieval thresholds (0.5, 0.6, 0.7, 0.8) and measure impact on accuracy and latency, 3) Evaluate ReAct vs non-ReAct prompting with latency measurements to confirm the performance-latency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different retrieval strategies (ScaNN vs KNN HNSW) impact the performance of the RAG system in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper discusses the evaluation of ScaNN and KNN HNSW as retrieval strategies, noting that ScaNN generally outperformed KNN HNSW due to its efficient handling of large-scale datasets and superior retrieval accuracy through quantization and re-ranking techniques.
- Why unresolved: The paper provides a general comparison but does not delve into specific scenarios or datasets where one strategy might significantly outperform the other. Further empirical studies could explore the nuances of these strategies in different contexts.
- What evidence would resolve it: Detailed performance metrics and case studies comparing ScaNN and KNN HNSW across various datasets and query types would provide insights into their relative strengths and weaknesses.

### Open Question 2
- Question: What are the long-term effects of using RAG-based systems on customer satisfaction and agent productivity in contact centers?
- Basis in paper: [inferred] The paper highlights improvements in accuracy, relevance, and reduction in hallucination with RAG-based responses compared to BERT-based methods. However, it does not address the long-term impact on customer satisfaction and agent productivity.
- Why unresolved: While the paper demonstrates short-term performance improvements, it lacks data on sustained use and its effects on customer satisfaction and agent workload over time.
- What evidence would resolve it: Longitudinal studies measuring customer satisfaction scores and agent productivity metrics before and after implementing RAG-based systems would provide valuable insights into their long-term benefits.

### Open Question 3
- Question: How do advanced prompting techniques like ReAct and Chain-of-Thought/Verification affect the latency and real-time performance of RAG-based systems in production environments?
- Basis in paper: [explicit] The paper mentions that advanced prompting techniques such as ReAct and Chain-of-Thought/Verification did not yield significant improvements due to latency concerns, but it does not provide detailed analysis of their impact on real-time performance.
- Why unresolved: The paper acknowledges the latency issues but does not explore potential optimizations or scenarios where these techniques might still be beneficial despite the latency trade-offs.
- What evidence would resolve it: Performance benchmarks and latency measurements of RAG-based systems with and without advanced prompting techniques in real-time production environments would clarify their feasibility and impact on user experience.

## Limitations

- The paper lacks quantitative rigor in human evaluation, with no specific metrics or scoring rubrics provided to support qualitative assessments of response quality
- The comparison with BERT-based methods is not well-supported by quantitative data, only mentioning that BERT responses "often contain irrelevant information or hallucinations" without presenting comparative metrics
- Corpus analysis reveals concerning absence of citations in related work, suggesting the paper may not be adequately positioned within existing literature

## Confidence

**High confidence**: The core RAG architecture (embedding + retrieval + generation) is technically sound and represents a reasonable approach for the stated problem. The choice of Vertex AI textembedding-gecko@001 with ScaNN retrieval is a defensible technical decision based on reported results.

**Medium confidence**: The claim about latency preventing ReAct usage is supported by the reported 6% performance slowdown, but lacks broader context about whether this specific latency threshold is appropriate for all customer service scenarios.

**Low confidence**: Claims about superior performance compared to BERT-based methods lack quantitative support. The human evaluation findings are presented without methodological detail about evaluator selection, inter-rater reliability, or scoring criteria.

## Next Checks

1. **Quantitative benchmarking**: Conduct head-to-head comparisons between RAG and BERT-based methods using standardized metrics (BLEU, ROUGE, F1) on the same datasets to validate the claimed performance improvements.

2. **Reproducibility audit**: Implement the system using only the information provided in the paper and test whether the 0.7 threshold and embedding-retrieval combination consistently yield the reported performance across different datasets and query types.

3. **Error analysis framework**: Develop a systematic error categorization scheme to understand when and why the system fails - distinguishing between retrieval failures (wrong documents retrieved), generation failures (correct context but poor response), and hallucination cases (response contradicts or extends beyond retrieved context).