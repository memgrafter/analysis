---
ver: rpa2
title: Exploring and Controlling Diversity in LLM-Agent Conversation
arxiv_id: '2412.21102'
source_url: https://arxiv.org/abs/2412.21102
tags:
- diversity
- prompt
- units
- dialogue
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Adaptive Prompt Pruning (APP), a novel method\
  \ for controlling diversity in multi-agent conversations by dynamically pruning\
  \ prompt content based on attention scores. APP uses a single parameter \u03BB to\
  \ adjust the degree of pruning, effectively balancing diversity and coherence."
---

# Exploring and Controlling Diversity in LLM-Agent Conversation

## Quick Facts
- arXiv ID: 2412.21102
- Source URL: https://arxiv.org/abs/2412.21102
- Reference count: 27
- This paper introduces Adaptive Prompt Pruning (APP), a novel method for controlling diversity in multi-agent conversations by dynamically pruning prompt content based on attention scores.

## Executive Summary
This paper introduces Adaptive Prompt Pruning (APP), a novel method for controlling diversity in multi-agent conversations by dynamically pruning prompt content based on attention scores. APP uses a single parameter λ to adjust the degree of pruning, effectively balancing diversity and coherence. Experiments demonstrate that APP significantly enhances diversity metrics (e.g., dist-3 increases from 0.535 to 0.800) while maintaining consistency through post-generation revision. The study also reveals that prompt structure, particularly block order and length, critically influences diversity, with Memory blocks having the most significant impact. APP is compatible with existing diversity control techniques like temperature and top-p sampling, offering a versatile tool for managing multi-agent interactions.

## Method Summary
APP dynamically prunes prompt segments based on attention scores to enhance diversity in LLM-agent conversations. The method calculates attention weights for each unit in the prompt, sorts them, and removes units based on parameter λ. After generating responses with pruned prompts, APP performs conflict detection and post-generation revision to maintain consistency. The approach is tested on two datasets (Generative Agents and Humanoid Agents) using LLaMA 3 and LLaMA 3.1 models, measuring diversity through sim and dist-N metrics.

## Key Results
- APP significantly improves diversity metrics, with dist-3 increasing from 0.535 to 0.800
- Post-generation revision effectively maintains consistency while enhancing diversity
- Prompt structure (block order and length) critically influences diversity, with Memory blocks having the most significant impact
- APP is compatible with existing diversity control techniques like temperature and top-p sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based pruning removes high-impact content to reduce prompt constraints and increase diversity.
- Mechanism: By calculating attention scores for each unit in the prompt, APP removes units with the highest scores (most influential) when increasing λ, thereby reducing the constraining effect of those units on output diversity.
- Core assumption: High attention scores correlate with units that most constrain diversity.
- Evidence anchors:
  - [abstract]: "APP dynamically prunes prompt segments based on attention scores"
  - [section 3.1]: "Leveraging attention weights from raw output utterances, APP selectively removes items from the modularized prompt"
  - [corpus]: Weak - no direct corpus evidence found for attention-based pruning mechanisms
- Break condition: If attention scores don't reliably indicate constraint on diversity, or if removing high-attention units harms coherence.

### Mechanism 2
- Claim: Diversity control via λ parameter enables smooth transitions between constrained and diverse outputs.
- Mechanism: APP uses a single parameter λ ∈ [0,1] to control the intensity of pruning. As λ increases, more units are removed, leading to greater diversity. This provides a continuous control mechanism rather than binary on/off pruning.
- Core assumption: A single parameter can effectively control diversity across different models and datasets.
- Evidence anchors:
  - [abstract]: "allows users to control diversity via a single parameter, lambda"
  - [section 3.1]: "A parameter λ ∈ [0, 1] is defined to determine the units to remove"
  - [section 3.2]: "As λ increases from 0 to 1, diversity generally increases"
- Break condition: If λ doesn't provide smooth control, or if different models require different λ interpretations.

### Mechanism 3
- Claim: Post-generation revision maintains consistency while allowing diversity enhancement.
- Mechanism: After generating responses with pruned prompts, APP performs conflict detection between the response and removed content. If conflicts are found, the response is regenerated until consistency is achieved or the least inconsistent response is selected.
- Core assumption: LLMs can reliably detect and correct inconsistencies between generated responses and removed prompt content.
- Evidence anchors:
  - [abstract]: "we incorporate a post-generation correction step, which effectively balances diversity enhancement with output consistency"
  - [section 4.1]: "After generating a response controlled by λ, we collect the removed units and the generated utterance to assess whether the utterance conflicts with the content of the removed units"
  - [section 4.2]: "After revision, the scores are consistently reduced, indicating that the model finds the revised responses more faithful"
- Break condition: If the revision process fails to maintain consistency, or if revision introduces new errors.

## Foundational Learning

- Attention mechanisms in transformer models
  - Why needed here: Understanding how attention scores are calculated and interpreted is crucial for implementing APP
  - Quick check question: How does the sum-mean reducer in APP aggregate attention values across tokens, layers, and heads?

- Diversity metrics (dist-N, sim)
  - Why needed here: Evaluating the effectiveness of APP requires understanding how diversity is measured
  - Quick check question: What's the difference between lexical diversity (dist-N) and semantic diversity (sim), and why does APP affect them differently?

- Prompt engineering and modularization
  - Why needed here: APP requires understanding how prompts can be structured into modular blocks for selective pruning
  - Quick check question: How does modularizing prompts into blocks affect the granularity of diversity control?

## Architecture Onboarding

- Component map:
  - Prompt parser -> Attention calculator -> Pruning selector -> Response generator -> Conflict detector -> Revision manager

- Critical path:
  1. Parse prompt into modular units
  2. Generate response and calculate attention scores
  3. Sort units by attention score
  4. Select units to remove based on λ
  5. Generate response with pruned prompt
  6. Check for conflicts with removed content
  7. Revise if necessary

- Design tradeoffs:
  - Granularity vs. efficiency: Finer-grained units provide better control but increase computational overhead
  - Pruning intensity vs. consistency: Higher λ values increase diversity but may require more revisions
  - Attention calculation cost vs. control precision: More accurate attention calculation improves selection but increases latency

- Failure signatures:
  - No diversity improvement despite high λ values: Attention calculation may be incorrect
  - Excessive revisions required: Pruning may be removing too much critical information
  - Inconsistent responses across trials: Attention redistribution after pruning may be unpredictable

- First 3 experiments:
  1. Verify attention score calculation by comparing with known attention patterns on simple prompts
  2. Test λ parameter calibration by measuring diversity changes across different λ values on a small dataset
  3. Evaluate conflict detection accuracy by comparing LLM judgments with human annotations on sample responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reducer function (sum-mean vs. mean-mean) affect the trade-off between diversity and efficiency in APP?
- Basis in paper: [explicit] The paper discusses the evaluation of different reducers (sum-mean and mean-mean) and their effects on unit removal and diversity enhancement.
- Why unresolved: The paper mentions that sum-mean is more suitable for preserving the length bias of units, but it does not provide a detailed analysis of how each reducer impacts the efficiency of diversity enhancement.
- What evidence would resolve it: A comparative study showing the diversity gains and efficiency (in terms of word removal ratio) for each reducer across various λ values and datasets.

### Open Question 2
- Question: What are the potential biases in the LLM's judgments during the post-generation revision step, and how do they correlate with dialogue diversity?
- Basis in paper: [explicit] The paper introduces a post-generation correction step to address inconsistencies introduced by prompt pruning and mentions the need to investigate potential biases in the LLM's judgments.
- Why unresolved: While the paper demonstrates that the revision step reduces inconsistency scores, it does not explore the nature of these biases or their relationship with diversity metrics.
- What evidence would resolve it: An analysis of the LLM's scoring patterns and their correlation with diversity metrics, possibly using different LLMs or human evaluations for comparison.

### Open Question 3
- Question: How does the frequency of agent names in the training corpus influence the diversity of generated dialogues in APP?
- Basis in paper: [explicit] The paper conducts an experiment replacing agent names with high-frequency and low-frequency names to analyze the impact on diversity.
- Why unresolved: The experiment shows that high-frequency names enhance diversity when prompt content is pruned, but the underlying reasons for this effect and its generalizability to other models are not fully explored.
- What evidence would resolve it: A broader study examining the effect of name frequency on diversity across different models, datasets, and name sets, along with an analysis of how parametric and in-context knowledge interact in this process.

## Limitations

- The theoretical grounding for why high-attention units reduce diversity remains implicit, despite strong empirical results
- The effectiveness of post-generation revision is difficult to fully verify as consistency assessment relies on LLM self-evaluation rather than external validation
- The generalizability of the single-parameter λ control across different domains and model architectures hasn't been thoroughly tested

## Confidence

- High confidence: The empirical effectiveness of APP in improving diversity metrics (dist-3 from 0.535 to 0.800) is well-supported by the experimental results
- Medium confidence: The compatibility with existing diversity control techniques (temperature, top-p sampling) is demonstrated but relies on combined rather than isolated testing
- Medium confidence: The claim about prompt structure's impact on diversity is supported by experiments but the mechanism behind this effect isn't fully explained

## Next Checks

1. **Attention-Unit Correlation Validation**: Conduct ablation studies removing units with varying attention scores to verify whether high-attention units consistently reduce diversity when removed, and whether this relationship holds across different prompt types and domains.

2. **Cross-Model λ Calibration**: Test APP across multiple model architectures (beyond LLaMA 3 and LLaMA 3.1) to verify whether the λ parameter provides consistent diversity control or requires model-specific calibration.

3. **Human Evaluation of Consistency**: Implement blind human evaluations comparing APP-generated responses with and without post-generation revision to independently verify that consistency is maintained while diversity is enhanced.