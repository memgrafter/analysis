---
ver: rpa2
title: 'MoralBench: Moral Evaluation of LLMs'
arxiv_id: '2406.04428'
source_url: https://arxiv.org/abs/2406.04428
tags:
- moral
- llms
- human
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MoralBench, a novel benchmark for evaluating\
  \ the moral reasoning capabilities of large language models (LLMs). The authors\
  \ adapt two well-known moral foundations datasets\u2014the Moral Foundations Questionnaire\
  \ (MFQ-30) and Moral Foundations Vignettes (MFV)\u2014into formats suitable for\
  \ LLM evaluation, using both binary agreement tasks and comparative moral assessments."
---

# MoralBench: Moral Evaluation of LLMs

## Quick Facts
- arXiv ID: 2406.04428
- Source URL: https://arxiv.org/abs/2406.04428
- Authors: Jianchao Ji; Yutong Chen; Mingyu Jin; Wujiang Xu; Wenyue Hua; Yongfeng Zhang
- Reference count: 0
- Primary result: Novel benchmark evaluating LLM moral reasoning across six foundations shows significant variation in performance across models and dimensions

## Executive Summary
This paper introduces MoralBench, a comprehensive benchmark for evaluating the moral reasoning capabilities of large language models. The authors adapt two well-established moral foundations datasets - the Moral Foundations Questionnaire (MFQ-30) and Moral Foundations Vignettes (MFV) - into formats suitable for LLM evaluation, using both binary agreement tasks and comparative moral assessments. Testing five models (Zephyr, LLaMA-2, Gemma-1.1, GPT-3.5, GPT-4) across multiple moral dimensions reveals significant performance variation, with LLaMA-2 and GPT-4 generally performing best. The benchmark is publicly released to support ongoing research in AI ethics and moral alignment.

## Method Summary
MoralBench adapts the Moral Foundations Questionnaire (MFQ-30) and Moral Foundations Vignettes (MFV) into LLM-compatible formats through binary conversion of Likert-scale responses and comparative moral assessment tasks. The binary assessment requires models to choose "Agree" or "Disagree" for moral statements, while the comparative assessment presents pairs of moral statements for models to select the more morally acceptable option. Five models are tested across six moral foundations (Care, Fairness, Loyalty, Authority, Sanctity, Liberty) with temperature set to 0.7 and five repetitions per experiment. Scoring is based on human agreement patterns, with comparative assessments using a formula that rewards agreement with higher-scored statements.

## Key Results
- LLaMA-2 and GPT-4 achieved the highest overall moral scores across all six moral foundations
- Models excelling at binary moral assessment often performed poorly on comparative moral assessment, suggesting reliance on pattern matching rather than deep moral understanding
- Performance varied significantly across moral foundations, with no single model consistently excelling across all dimensions
- GPT-3.5 and GPT-4 showed relatively strong performance in both binary and comparative tasks but still exhibited variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoralBench adapts well-established moral foundations psychometric instruments (MFQ-30, MFV) into LLM-compatible binary and comparative formats.
- Mechanism: By converting Likert-scale human responses into binary "Agree/Disagree" or comparative moral choices, the benchmark translates complex moral reasoning into a format LLMs can process while preserving relative moral rankings.
- Core assumption: Binary and comparative responses preserve enough moral signal to reflect the underlying five/six moral foundations theory dimensions.
- Evidence anchors:
  - [abstract] "We present the first comprehensive dataset specifically curated to probe the moral dimensions of LLM outputs"
  - [section 3.3.1] "instead of soliciting a scale response from the LLMs, we require a straightforward 'Agree' or 'Disagree' response"
  - [corpus] Weak - no direct evidence of binary format preservation effectiveness
- Break condition: If binary conversion eliminates critical moral nuance or if human scores don't correlate with LLM moral judgments, the signal-to-noise ratio collapses.

### Mechanism 2
- Claim: The comparative moral assessment reveals whether models understand nuanced moral distinctions versus pattern matching.
- Mechanism: By presenting paired moral statements with known human preference scores, the benchmark tests if models can identify the more morally acceptable option, revealing depth of moral reasoning.
- Core assumption: Models that perform well on binary tasks but poorly on comparative tasks lack true moral understanding and rely on surface patterns.
- Evidence anchors:
  - [section 3.3.2] "it could be hard to determine whether a statement is moral or not... We introduce a comparative assessment"
  - [section 4.2.2] "GPT-3.5 and GPT-4 tend to perform relatively well in both tasks but still exhibit variability"
  - [corpus] Weak - no direct evidence linking performance drop to lack of understanding
- Break condition: If comparative assessment is too ambiguous or human scoring is inconsistent, the evaluation loses validity.

### Mechanism 3
- Claim: Multiple model architectures show systematic differences in moral reasoning across moral foundations dimensions.
- Mechanism: Testing diverse models (Zephyr, LLaMA-2, Gemma-1.1, GPT-3.5, GPT-4) across care, fairness, loyalty, authority, sanctity, and liberty foundations reveals architectural biases in moral reasoning.
- Core assumption: Different training data, architectures, and fine-tuning approaches create systematic moral reasoning differences that can be measured.
- Evidence anchors:
  - [section 4.2.1] "Results show significant variation in moral reasoning across models and domains"
  - [section 4.2.2] "Different models show varying strengths across moral foundations"
  - [corpus] Moderate - neighboring papers also find systematic model differences in moral reasoning
- Break condition: If differences are random noise or driven by chance rather than systematic architectural factors.

## Foundational Learning

- Concept: Moral Foundations Theory (five/six dimensions: Care, Fairness, Loyalty, Authority, Sanctity, Liberty)
  - Why needed here: Benchmark design directly maps to these foundations for systematic evaluation
  - Quick check question: Can you name all six moral foundations and give an example of each?

- Concept: Psychometric instrument adaptation (Likert scales to binary/comparative formats)
  - Why needed here: Enables LLM evaluation while preserving moral reasoning assessment validity
  - Quick check question: Why can't we just use original Likert-scale questions with LLMs?

- Concept: Statistical scoring methodology for comparative moral assessment
  - Why needed here: Provides quantitative framework for measuring model moral alignment with human judgments
  - Quick check question: How does the scoring formula (H if agree, M-H if disagree) capture moral alignment?

## Architecture Onboarding

- Component map: Dataset preprocessing (MFQ-30/MFV → MFQ-30-LLM/MFV-LLM) -> Binary assessment module (Agree/Disagree generation) -> Comparative assessment module (paired statement selection) -> Scoring engine (human score mapping, total score calculation) -> Experiment runner (model interaction, repetition handling)

- Critical path: Dataset preprocessing → Binary assessment → Comparative assessment → Scoring → Analysis

- Design tradeoffs: Binary vs. scale format (simplicity vs. nuance), English-only vs. multilingual (coverage vs. complexity), static vs. dynamic prompt engineering (reproducibility vs. performance)

- Failure signatures: Low inter-model differentiation (benchmark not sensitive), high variance across repetitions (instability), systematic bias toward certain moral foundations (dataset skew)

- First 3 experiments:
  1. Run binary assessment on single model with controlled temperature to establish baseline
  2. Compare binary vs. comparative performance on same model to detect pattern-matching behavior
  3. Cross-model comparison focusing on care vs. authority foundations to identify architectural biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM moral reasoning capabilities generalize across languages beyond English?
- Basis in paper: [explicit] - The authors note "Currently, MoralBench is an English language-based benchmark. The performance of this benchmark on other languages is uncertain."
- Why unresolved: The benchmark was developed and tested only on English-language moral statements, and no cross-linguistic validation was performed.
- What evidence would resolve it: Testing MoralBench across multiple languages with culturally diverse moral scenarios would show whether the benchmark's validity and the LLMs' moral reasoning capabilities transfer across linguistic and cultural contexts.

### Open Question 2
- Question: What specific training data or patterns cause some LLMs to show high moral scores in binary tasks but fail at comparative moral assessment?
- Basis in paper: [explicit] - The authors observe that "Models that achieve high scores in the first part sometimes struggle in the second part, indicating a lack of deep understanding of moral principles."
- Why unresolved: The paper identifies the inconsistency but does not investigate what training artifacts or patterns lead to this phenomenon.
- What evidence would resolve it: Detailed analysis of training data exposure, pattern matching behaviors, and ablation studies on model components could reveal whether this stems from memorization, keyword matching, or superficial pattern recognition rather than genuine moral reasoning.

### Open Question 3
- Question: How can LLMs be developed to achieve consistent performance across all moral foundations rather than excelling in specific dimensions?
- Basis in paper: [inferred] - The results show varying model performance across different moral dimensions (Care, Fairness, Loyalty, Authority, Sanctity, Liberty) with no single model excelling universally.
- Why unresolved: While the paper identifies performance variations, it doesn't propose solutions for developing more balanced moral reasoning capabilities.
- What evidence would resolve it: Comparative studies of different training approaches, architectural modifications, or fine-tuning strategies that specifically target balanced moral foundation reasoning would demonstrate methods for achieving more consistent performance across all moral dimensions.

## Limitations
- The benchmark currently only supports English language, limiting cross-cultural generalizability of results
- Binary conversion of Likert-scale responses may lose important moral nuance captured in original psychometric instruments
- No investigation into what causes performance gaps between binary and comparative assessments or how to address them

## Confidence

- **High Confidence**: The dataset construction process and basic scoring methodology are well-documented and reproducible.
- **Medium Confidence**: Claims about architectural differences in moral reasoning are supported by observed variation but lack deeper causal analysis.
- **Low Confidence**: The assertion that poor comparative performance indicates pattern matching rather than understanding requires more rigorous validation.

## Next Checks
1. **Correlation Validation**: Measure correlation between original human Likert scores and LLM binary responses to quantify information loss during conversion.
2. **Cross-Cultural Replication**: Test the benchmark with human raters from diverse cultural backgrounds to assess whether comparative assessments reflect universal moral intuitions.
3. **Ablation Study**: Systematically remove different prompt engineering components to isolate their impact on moral reasoning performance and identify overfitting patterns.