---
ver: rpa2
title: 'OpenMU: Your Swiss Army Knife for Music Understanding'
arxiv_id: '2410.15573'
source_url: https://arxiv.org/abs/2410.15573
tags:
- music
- openmu
- understanding
- openmu-bench
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenMU, a multimodal large language model
  for music understanding. The authors construct a large-scale benchmark suite, OpenMU-Bench,
  to address the data scarcity issue in training such models.
---

# OpenMU: Your Swiss Army Knife for Music Understanding

## Quick Facts
- arXiv ID: 2410.15573
- Source URL: https://arxiv.org/abs/2410.15573
- Authors: Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, Yuki Mitsufuji
- Reference count: 25
- Primary result: State-of-the-art performance on music understanding tasks using multimodal LLM with AudioMAE encoder and LoRA adaptation

## Executive Summary
This paper introduces OpenMU, a multimodal large language model specifically designed for music understanding tasks. The authors address the critical data scarcity issue in this domain by constructing OpenMU-Bench, a comprehensive benchmark suite containing approximately one million examples across music captioning, reasoning, lyrics understanding, and music tool usage tasks. OpenMU leverages AudioMAE for music encoding, aligns these representations with Llama3-8B through a two-layer MLP projector, and uses LoRA adapters for efficient instruction tuning. The model demonstrates state-of-the-art performance compared to baseline models like MU-LLaMA and provides extensive ablation studies on key design choices.

## Method Summary
OpenMU employs a two-stage training approach: first, it trains on captioning tasks to learn basic music understanding, then performs instruction tuning with LoRA adapters to adapt the Llama3-8B model for music-specific tasks. The method uses AudioMAE to encode 30-second music clips into vector representations, which are mean-pooled and projected into the LLM's representation space through an MLP projector. The model is trained on OpenMU-Bench, which includes music captioning, reasoning, lyrics understanding, and music tool usage tasks. The implementation uses DeepSpeed ZeRO-3 and FlashAttention2 for efficient training, with the full model achieving 66.4 BLEU4 score on music captioning.

## Key Results
- OpenMU outperforms baseline models like MU-LLaMA on OpenMU-Bench across multiple music understanding tasks
- The model achieves state-of-the-art performance in music captioning with 66.4 BLEU4 score
- Ablation studies show that mean-pooling 8 tokens and using LoRA rank 128/16 provides optimal performance
- OpenMU genuinely utilizes input music information rather than relying solely on LLM pretraining knowledge

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Representation Alignment
OpenMU outperforms baseline models by leveraging large-scale pretraining data and aligning multimodal representations with language models. The model uses AudioMAE to encode music clips into vector representations, which are then projected into the LLM's representation space through a two-layer MLP projector. This alignment allows the LLM to understand and reason about music clips effectively. If the AudioMAE encoder fails to capture meaningful musical features, or if the projector cannot effectively map these features to the LLM's representation space, the alignment would break down.

### Mechanism 2: Token Number Optimization
The number of music tokens used as input to the LLM significantly impacts task performance. By mean-pooling music tokens from the AudioMAE encoder, the model can control the amount of musical information fed into the LLM. This affects the model's ability to understand and reason about music. If mean-pooling removes crucial musical information, or if the LLM cannot effectively process the reduced number of tokens, performance would degrade.

### Mechanism 3: Genuine Music Understanding
OpenMU genuinely utilizes information from the input music clip rather than relying on the LLM's pretraining knowledge. By comparing the model's performance on actual music clips versus white noise clips, we can determine if the model is using musical information or just relying on its pretraining knowledge. If the model performs similarly well on white noise clips as on actual music clips, indicating it's not using the musical information, this mechanism would be invalidated.

## Foundational Learning

- **Multimodal representation alignment**: Why needed here - To enable the LLM to understand and reason about music clips by aligning the music encoder's output with the LLM's representation space. Quick check question - How does the projector in OpenMU's architecture facilitate the alignment between music and language representations?

- **Low-Rank Adaptation (LoRA)**: Why needed here - To efficiently adapt the LLM for following instructions in the music domain without full fine-tuning. Quick check question - What is the role of the LoRA rank r in controlling the number of trainable parameters and its impact on task performance?

- **Music Information Retrieval (MIR) tools**: Why needed here - To enhance OpenMU's ability to understand music by integrating established MIR tools for tasks like tempo estimation and chord recognition. Quick check question - How does OpenMU leverage MIR tools to answer queries related to music analysis, and what are some examples of these tools?

## Architecture Onboarding

- **Component map**: Music clip input -> AudioMAE encoder -> Vector representations -> Music-language projector -> Aligned representations + text input -> LLM -> Music understanding output

- **Critical path**: 1. Music clip input → AudioMAE encoder → Vector representations; 2. Vector representations → Music-language projector → Aligned representations; 3. Aligned representations + text input → LLM → Music understanding output

- **Design tradeoffs**: Number of music tokens vs. training efficiency and model performance; LoRA rank r vs. number of trainable parameters and task performance; Integration of MIR tools vs. model complexity and real-world applicability

- **Failure signatures**: Poor performance on music understanding tasks; Inability to align music and language representations effectively; Overreliance on LLM's pretraining knowledge rather than musical input

- **First 3 experiments**: 1. Evaluate the impact of different numbers of mean-pooled music tokens on model performance; 2. Test various LoRA rank configurations to find the optimal balance between performance and trainable parameters; 3. Compare the model's performance on actual music clips versus white noise clips to assess genuine music understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of music tokens used as input to the LLM affect OpenMU's performance on various music understanding tasks?
- Basis in paper: Explicit - The paper extensively evaluates the impact of varying the number of music tokens by mean-pooling every 2-128 tokens and comparing performance.
- Why unresolved: While the paper shows that mean-pooling every 8 tokens performs best, the optimal number may vary depending on the specific task or type of music being analyzed.
- What evidence would resolve it: Systematic evaluation of OpenMU's performance on each individual task in OpenMU-Bench with different numbers of music tokens, and potentially developing a method to dynamically adjust the number of tokens based on the input music.

### Open Question 2
- Question: To what extent does OpenMU rely on its LLM's pretraining knowledge versus the actual musical content of the input clip when answering questions about music?
- Basis in paper: Explicit - The paper investigates this by evaluating OpenMU on MuChoMusic with both actual music clips and white noise, showing that music information is crucial for good performance.
- Why unresolved: While the paper demonstrates that music information is important, it doesn't quantify the relative contribution of the LLM's pretraining knowledge versus the musical content for different types of questions or tasks.
- What evidence would resolve it: Ablation studies where OpenMU is tested on various tasks with different amounts of musical information (e.g., snippets of music vs. full clips) to measure the impact on performance and determine the relative importance of pretraining knowledge.

### Open Question 3
- Question: Can OpenMU be extended to support multiple music clips as input and enable in-context learning for music understanding?
- Basis in paper: Inferred - The paper mentions this as a potential future direction, suggesting it's a promising area for extension.
- Why unresolved: The current OpenMU architecture is designed for single music clips as input, and the paper doesn't explore how it might handle multiple clips or learn from examples within the same context.
- What evidence would resolve it: Development and evaluation of an extended OpenMU model that can process multiple music clips simultaneously and demonstrate its ability to learn from in-context examples for improved music understanding.

## Limitations

- The benchmark suite, while extensive, may not fully capture the breadth of real-world music comprehension challenges and could lead to overfitting
- Heavy reliance on bootstrapped annotations through GPT-3.5 introduces potential biases and questions about annotation quality and diversity
- The evaluation metrics, while standard for language tasks, may not fully capture the nuances of music understanding, particularly for subjective aspects like emotional interpretation

## Confidence

- **High confidence**: The claim that OpenMU outperforms baseline models is well-supported by extensive experimental results and ablation studies across multiple tasks
- **Medium confidence**: The mechanism of using AudioMAE for music encoding followed by MLP projection to align with LLM representations is technically sound, though optimal configurations may be task-dependent
- **Medium confidence**: The effectiveness of mean-pooling strategies and LoRA configurations is supported by ablation studies, but optimal settings may vary with different music types or tasks
- **Low confidence**: The integration of MIR tools and their interaction with learned representations needs further exploration to understand limitations in novel scenarios

## Next Checks

1. **Cross-dataset Generalization Test**: Evaluate OpenMU on established music understanding benchmarks outside of OpenMU-Bench (such as DALLE-Bench or specialized MIR datasets) to assess real-world applicability and identify potential overfitting to the constructed dataset.

2. **Robustness to Adversarial Music Examples**: Test the model's performance on deliberately challenging music clips that contain ambiguous features, noise, or unconventional structures to determine whether the model truly understands musical concepts or relies on superficial patterns.

3. **Human Evaluation of Music Understanding Quality**: Conduct systematic human evaluations comparing OpenMU's responses to those of baseline models across diverse music comprehension tasks, focusing on subjective aspects like interpretability, relevance, and depth of understanding that automated metrics may miss.