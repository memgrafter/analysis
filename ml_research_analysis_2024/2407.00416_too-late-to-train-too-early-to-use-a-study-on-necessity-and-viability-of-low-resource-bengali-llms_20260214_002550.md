---
ver: rpa2
title: Too Late to Train, Too Early To Use? A Study on Necessity and Viability of
  Low-Resource Bengali LLMs
arxiv_id: '2407.00416'
source_url: https://arxiv.org/abs/2407.00416
tags:
- bengali
- llms
- arxiv
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the need for a dedicated Bengali LLM by comparing
  open-weights and closed-source models like LLaMA-3 and GPT-4 against fine-tuned
  encoder-decoder models on Bengali downstream tasks including translation, summarization,
  paraphrasing, QA, and NLI. Findings show that while LLMs generally excel in reasoning
  tasks, their performance in Bengali script generation is inconsistent.
---

# Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs

## Quick Facts
- arXiv ID: 2407.00416
- Source URL: https://arxiv.org/abs/2407.00416
- Authors: Tamzeed Mahfuz, Satak Kumar Dey, Ruwad Naswan, Hasnaen Adil, Khondker Salman Sayeed, Haz Sameen Shahgir
- Reference count: 40
- Primary result: English-oriented LLMs excel at Bengali NLU but struggle with NLG due to inefficient tokenization, highlighting need for dedicated Bengali LLMs despite data limitations.

## Executive Summary
This paper investigates whether current large language models (LLMs) like LLaMA-3 and GPT-4 can effectively serve Bengali NLP needs or if dedicated Bengali-oriented LLMs are necessary. Through comprehensive evaluation on translation, summarization, paraphrasing, QA, and NLI tasks, the study reveals that while English-oriented LLMs demonstrate strong cross-lingual transfer capabilities for comprehension tasks, they face significant challenges with Bengali script generation due to inefficient tokenization and computational overhead. The research identifies critical gaps in available datasets and tokenization approaches that limit the viability of both existing LLMs and potential Bengali-specific models, ultimately arguing for the development of Bengali-oriented LLMs despite current resource constraints.

## Method Summary
The study evaluates open-weight and closed-source models (LLaMA-3-8B-Instruct, LLaMA-3-70B, NLLB variants) against fine-tuned encoder-decoder models (BanglaT5, BanglaBERT) on multiple Bengali downstream tasks. Models are assessed using standard metrics including BLEU for translation, ROUGE for summarization, F1/Exact Match for QA, and accuracy for classification tasks. Fine-tuning is performed using QLoRA with 4-bit quantization on RTX A6000 hardware, with learning rates of 5 × 10⁻⁵ and linear scheduling. The evaluation leverages various Bengali datasets including BanglaNMT, XLSum, CrossSum, BanglaParaphrase, Squad-bn, and XNLI-bn.

## Key Results
- English-oriented LLMs like LLaMA-3-70B significantly outperform fine-tuned BanglaT5 on Bengali QA tasks (F1 +7.1, Exact Match +7.3)
- Bengali script is severely over-tokenized by English LLM tokenizers (character-per-token ratio of ~0.85 vs ~4.5 for English), causing quadratic increases in computational complexity
- Machine-translated datasets introduce biases that favor fine-tuned models in automated evaluation metrics, as evidenced by stylistic similarities between reference outputs and BanglaT5 generations

## Why This Works (Mechanism)

### Mechanism 1
English-oriented LLMs generalize better to Bengali NLU tasks than Bengali-specific models due to transfer learning capabilities. Large English LLMs pretrained on massive corpora and fine-tuned on diverse instruction datasets can leverage cross-lingual transfer for understanding Bengali text without explicit Bengali training.

### Mechanism 2
Inefficient tokenization of Bengali script by English LLMs leads to higher computational costs and potential performance degradation in NLG tasks. English LLMs use Byte-Pair Encoding tokenizers optimized for English, which tokenizes Bengali characters at much finer granularity, resulting in more tokens per word and quadratic increase in attention complexity.

### Mechanism 3
Machine-translated datasets introduce stylistic biases that favor fine-tuned models in automated evaluation metrics. Synthetic datasets created through translation and back-translation inherit linguistic patterns and stylistic choices from translation models, making them more similar to outputs from fine-tuned models trained on similar data.

## Foundational Learning

- **Cross-lingual transfer in LLMs**: Why needed here - Understanding why English LLMs perform well on Bengali NLU tasks despite not being explicitly trained on Bengali. Quick check question: Why might a model trained primarily on English data still understand Bengali text reasonably well?

- **Tokenization and its impact on model efficiency**: Why needed here - The paper highlights how Bengali script is over-tokenized by English LLMs, leading to computational inefficiency. Quick check question: How does tokenization granularity affect the computational complexity of transformer models?

- **Bias in synthetic datasets**: Why needed here - The paper shows how machine-translated datasets can introduce biases that affect model evaluation. Quick check question: What are potential sources of bias when creating datasets through automated translation?

## Architecture Onboarding

- **Component map**: Corpus collection → Tokenization → Model training → Fine-tuning → Evaluation
- **Critical path**: Data collection → Tokenizer development → Model training → Fine-tuning → Evaluation → Human validation
- **Design tradeoffs**: Training a Bengali-specific LLM vs leveraging existing English LLMs with cross-lingual transfer
- **Failure signatures**: Poor performance on Bengali NLG tasks, high computational costs, bias in evaluation metrics
- **First 3 experiments**:
  1. Compare tokenization efficiency of different tokenizers on Bengali text
  2. Evaluate cross-lingual transfer performance on NLU vs NLG tasks
  3. Test bias in machine-translated datasets by comparing automated vs human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
How does the inefficiency of Bengali script tokenization by current LLMs impact their performance across different task types and model sizes? The paper identifies tokenization issues but doesn't comprehensively analyze how this affects performance across various tasks and model sizes.

### Open Question 2
Can the use of machine-translated datasets for Bengali NLP tasks be improved to reduce biases and errors? The paper highlights dataset biases but doesn't provide concrete solutions for improving translation quality.

### Open Question 3
Is it feasible to train a high-performing Bengali-oriented LLM given current limitations in pretraining and instruction-tuning datasets? The paper presents arguments both for and against Bengali LLMs but doesn't definitively answer feasibility questions.

## Limitations

- Evaluation heavily relies on machine-translated datasets that may introduce biases favoring fine-tuned models
- Comparison between LLMs and fine-tuned models is complicated by tokenization efficiency differences creating unfair computational advantages
- Lack of comprehensive human evaluation across all tasks limits validation of automated metric results

## Confidence

- **High Confidence**: English LLMs show strong cross-lingual transfer capabilities for Bengali NLU tasks, supported by consistent results across multiple benchmarks
- **Medium Confidence**: Tokenization inefficiency causes performance degradation, supported by quantitative evidence but needing more direct comparisons with Bengali-optimized tokenizers
- **Low Confidence**: Immediate necessity for Bengali LLMs is somewhat speculative given limited availability of high-quality Bengali pretraining data

## Next Checks

1. **Tokenizer Efficiency Validation**: Compare LLaMA-3 performance using standard tokenizer versus Bengali-optimized tokenizer on identical downstream tasks to isolate tokenization impact

2. **Human Evaluation Study**: Conduct comprehensive human evaluation of LLM outputs versus fine-tuned model outputs across all task types, particularly paraphrasing and summarization where automated metrics showed discrepancies

3. **Cross-Lingual Transfer Investigation**: Systematically test cross-lingual transfer capabilities by evaluating models on progressively more distant language pairs from English to determine if Bengali's unique features require dedicated models