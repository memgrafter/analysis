---
ver: rpa2
title: A Bayesian Approach to Data Point Selection
arxiv_id: '2411.03768'
source_url: https://arxiv.org/abs/2411.03768
tags:
- data
- training
- learning
- weights
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Bayesian approach to data point selection
  (DPS) in deep learning, addressing the challenges of uncurated training data. The
  authors frame DPS as posterior inference in a Bayesian model, jointly inferring
  instance-wise weights and neural network parameters using stochastic gradient Langevin
  MCMC sampling.
---

# A Bayesian Approach to Data Point Selection

## Quick Facts
- arXiv ID: 2411.03768
- Source URL: https://arxiv.org/abs/2411.03768
- Reference count: 40
- Primary result: Proposes a Bayesian DPS approach that outperforms bi-level optimization baselines in vision and language tasks

## Executive Summary
This paper introduces a novel Bayesian framework for data point selection (DPS) in deep learning that jointly infers instance-wise weights and neural network parameters. The authors frame DPS as posterior inference using stochastic gradient Langevin MCMC (SGLD) sampling, which is more computationally efficient than existing bi-level optimization methods while providing convergence guarantees. The approach demonstrates effectiveness across vision tasks (data balancing, denoising) and language tasks (efficient learning, instruction fine-tuning), including scalability to large language models.

## Method Summary
The method treats DPS as Bayesian posterior inference where both instance weights and model parameters are random variables. A weighted-data-driven prior guides the model to be compatible with training data while maintaining flexibility through a base prior. SGLD sampling updates both weights and parameters jointly using minibatch gradients, theoretically guaranteeing convergence to the true posterior. The approach can use either individual weights per training example or a weight network, with the latter being more memory-efficient for large datasets.

## Key Results
- Outperforms bi-level optimization baselines in data balancing and denoising experiments on vision tasks
- Demonstrates effective domain adaptation in WebNLG efficient learning with T5 models
- Scales to large language models and enables automated per-task optimization for instruction fine-tuning datasets
- Shows computational efficiency improvements over bi-level optimization while maintaining convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
The Bayesian framework enables joint inference of instance-wise weights and neural network parameters, allowing the model to adaptively re-weight training data based on its alignment with in-domain meta data. By treating both θ and w as random variables in a joint posterior distribution, the approach naturally captures uncertainty in both the model and data importance. The SGLD algorithm updates both jointly, ensuring they evolve together to balance fitting training data with respecting the meta data distribution.

### Mechanism 2
The weighted-data-driven prior creates a soft constraint that guides the model to be compatible with training data while allowing flexibility through the base prior. The prior combines a base prior p(θ) with weighted likelihoods from training data, where each data point's contribution is modulated by its learned weight wi. This hierarchical structure requires the model to balance fitting individual training points against the overall base prior.

### Mechanism 3
The SGLD algorithm with minibatch gradients provides convergence guarantees while being computationally efficient, even when using the full dataset size in likelihood terms. The stochastic gradient version of Langevin dynamics theoretically converges to the true posterior when using minibatch gradients. This allows the algorithm to scale to large datasets while maintaining the benefits of Bayesian inference.

## Foundational Learning

- Concept: Bayesian inference and posterior sampling
  - Why needed here: The entire approach is built on treating DPS as a Bayesian inference problem, requiring understanding of how to formulate priors, likelihoods, and posteriors, and how to sample from them.
  - Quick check question: What is the relationship between the prior, likelihood, and posterior in Bayesian inference, and how does this apply to the DPS problem formulation?

- Concept: Stochastic Gradient Langevin Dynamics (SGLD)
  - Why needed here: SGLD is the specific algorithm used to sample from the posterior, combining gradient descent with injected noise to explore the posterior distribution efficiently.
  - Quick check question: How does SGLD differ from standard stochastic gradient descent, and what role does the injected Gaussian noise play in the sampling process?

- Concept: Bi-level optimization and its limitations
  - Why needed here: Understanding why BLO approaches are computationally expensive and have convergence issues helps appreciate the advantages of the Bayesian approach.
  - Quick check question: What are the main computational and theoretical challenges of bi-level optimization in the context of data point selection?

## Architecture Onboarding

- Component map:
  - Main neural network (θ) -> Weight network or individual weights (w) -> SGLD sampler -> Priors

- Critical path:
  1. Initialize θ, w (or weight network parameters)
  2. Sample minibatches from Dt and Dm
  3. Compute gradients of log-posterior w.r.t. θ and w
  4. Update θ and w using SGLD equations (6-7)
  5. Repeat until convergence
  6. Use final θ for inference, optionally use w for data selection

- Design tradeoffs:
  - Memory vs. sparsity: Individual weights for each training example provide maximum flexibility but require more memory; weight networks are more memory-efficient but add model complexity
  - Prior strength: Stronger priors (smaller σ) lead to more sparsity but may underfit; weaker priors allow more flexibility but risk overfitting
  - Impact constants: Balancing train/meta data contributions affects convergence speed and final performance

- Failure signatures:
  - Weights collapsing to zero: May indicate overly strong sparsity prior (small σ) or incorrect loss-to-likelihood transformation
  - Poor meta data performance: Could suggest meta data distribution mismatch or insufficient impact constant tuning
  - Slow convergence: Might indicate learning rate issues or need for better initialization

- First 3 experiments:
  1. MNIST data balancing with LeNet5 backbone, verify BADS outperforms non-DPS baselines
  2. CIFAR data denoising with ResNet32, test performance with varying noise levels
  3. WebNLG efficient learning with T5-small, evaluate domain adaptation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BADS performance scale with increasing model size and dataset complexity beyond the current experimental scope?
- Basis in paper: Authors state they demonstrated BADS scales to large language models and can handle large-scale training on web data, but only tested on a 3B parameter model.
- Why unresolved: The paper only provides results on a 3B parameter model. Scaling behavior to trillion-parameter models and more complex data distributions remains unexplored.
- What evidence would resolve it: Empirical results on larger models (10B, 100B+ parameters) and more diverse, larger-scale datasets showing consistent performance improvements and computational efficiency.

### Open Question 2
- Question: What are the theoretical convergence properties of BADS when applied to non-convex loss landscapes typical in deep learning?
- Basis in paper: Authors mention convergence guarantees but provide them only for their specific SGLD formulation under certain assumptions, not for the general deep learning case.
- Why unresolved: The theoretical analysis assumes specific conditions that may not hold in practice, and the proof-of-concept experiments don't fully validate the theoretical claims.
- What evidence would resolve it: Rigorous theoretical analysis proving convergence rates for BADS in the general non-convex setting, validated by extensive empirical studies across diverse deep learning architectures.

### Open Question 3
- Question: How sensitive is BADS to hyperparameter choices, particularly the sparsity level β and impact constants, across different tasks and domains?
- Basis in paper: Authors acknowledge hyperparameter sensitivity and provide guidelines, but only demonstrate through limited ablation studies on specific tasks.
- Why unresolved: The paper shows sensitivity on specific examples but doesn't provide a comprehensive analysis of hyperparameter sensitivity across diverse tasks or automated hyperparameter selection methods.
- What evidence would resolve it: Systematic sensitivity analysis across multiple tasks showing optimal hyperparameter ranges, plus demonstration of effective automated hyperparameter tuning methods for BADS.

## Limitations
- Performance heavily depends on the assumption that meta data distribution accurately represents the target domain
- Requires careful hyperparameter tuning (impact constants, sparsity prior strength) that may not generalize across different datasets
- Computational overhead, while better than bi-level optimization, still involves running full SGLD sampling which can be slower than standard training

## Confidence

- **High Confidence**: The theoretical foundation of using SGLD for Bayesian posterior sampling and its convergence properties are well-established in the literature [55]
- **Medium Confidence**: The effectiveness of the weighted-data-driven prior construction and its ability to balance training data compatibility with base prior constraints, though theoretically sound, requires empirical validation across diverse scenarios
- **Low Confidence**: The generalizability of the approach to extremely large-scale settings (e.g., trillion-parameter models) and its performance when meta data is severely limited or non-representative

## Next Checks

1. Test the approach with significantly mismatched meta data distributions to assess robustness when the clean in-domain data assumption is violated
2. Evaluate computational efficiency and convergence behavior with varying minibatch sizes and dataset scales to establish practical limits
3. Conduct ablation studies on the prior strength parameters (β, σ) to determine sensitivity and optimal ranges for different task types and dataset characteristics