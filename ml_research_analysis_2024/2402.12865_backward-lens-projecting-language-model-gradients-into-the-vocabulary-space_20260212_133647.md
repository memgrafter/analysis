---
ver: rpa2
title: 'Backward Lens: Projecting Language Model Gradients into the Vocabulary Space'
arxiv_id: '2402.12865'
source_url: https://arxiv.org/abs/2402.12865
tags:
- token
- layer
- pass
- editing
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Transformer language model gradients during
  backpropagation, focusing on how information flows from forward and backward passes
  to update weights. The authors show that gradient matrices can be expressed as low-rank
  combinations of forward pass inputs and backward pass hidden states (Vector-Jacian
  Products), enabling interpretation via vocabulary projections.
---

# Backward Lens: Projecting Language Model Gradients into the Vocabulary Space

## Quick Facts
- arXiv ID: 2402.12865
- Source URL: https://arxiv.org/abs/2402.12865
- Authors: Shahar Katz; Yonatan Belinkov; Mor Geva; Lior Wolf
- Reference count: 40
- Key outcome: Introduces a method to project transformer language model gradients into vocabulary space, revealing a two-phase "imprint and shift" learning mechanism in MLP layers and enabling single-forward-pass editing.

## Executive Summary
This paper analyzes Transformer language model gradients during backpropagation, showing they can be expressed as low-rank combinations of forward and backward pass inputs. The authors introduce a novel method to project these gradients into vocabulary space using Vector-Jacobian Products, enabling interpretation of how information flows during training. They identify a two-phase "imprint and shift" mechanism in feed-forward layers where gradients inject information from intermediate activations and target token embeddings into model weights. The work also proposes a single-forward-pass editing method that approximates backpropagation by directly injecting target token embeddings, achieving comparable performance to state-of-the-art editing techniques while being computationally simpler.

## Method Summary
The method involves running a single backward pass with SGD on randomly sampled prompts from the CounterFact dataset, computing gradient matrix ranks, and projecting gradients into vocabulary space using the Logit Lens method. The analysis focuses on MLP layers in transformer blocks, expressing gradient matrices as outer products of forward pass inputs and Vector-Jacobian Products (VJPs). The authors then apply this understanding to develop a "forward pass shift" editing method that injects target token embeddings directly into the model through a single forward pass, avoiding the need for full backpropagation.

## Key Results
- Gradient matrices can be expressed as low-rank linear combinations of forward pass inputs and backward pass hidden states (VJPs)
- Identified a two-phase "imprint and shift" mechanism in feed-forward layers where gradients inject information from intermediate activations and target token embeddings into model weights
- Single-forward-pass editing method achieves comparable performance to state-of-the-art editing techniques (MEND, ROME, MEMIT, fine-tuning) while being computationally simpler

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient matrices can be expressed as low-rank combinations of forward pass inputs and backward pass hidden states (Vector-Jacobian Products).
- Mechanism: During backpropagation, the gradient of a weight matrix W with respect to the loss L is computed as the outer product of the forward pass input x and the VJP δ: ∂L/∂W = x⊤ · δ. When multiple tokens are processed, this becomes a sum of such outer products, resulting in a low-rank matrix structure.
- Core assumption: The forward pass inputs x and VJPs δ can be linearly combined to represent the gradient matrix.
- Evidence anchors:
  - [abstract] "We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs."
  - [section 4.1] "According to Equation 6, the gradient of a matrix is ∂L/∂W = x⊤ · δ. Assuming x, δ are nonzero vectors, the rank of the gradient matrix is 1, given its interpretation as a span of a single column vector x, or equivalently, as a span of a single row δ."
  - [corpus] Weak - no corpus evidence directly supports this low-rank decomposition claim.
- Break condition: If the forward pass inputs x or VJPs δ are linearly dependent, the rank of the gradient matrix would be less than n (the number of tokens).

### Mechanism 2
- Claim: The "imprint and shift" mechanism describes how information is stored in the feed-forward (MLP) module of the transformer layer.
- Mechanism: The first MLP matrix F F1 ("imprint") adds or subtracts copies of intermediate inputs encountered during the forward pass. The second matrix F F2 ("shift") shifts the weights towards the embedding of the target token.
- Core assumption: Updating F F1 with forward pass inputs and F F2 with target token embeddings captures how LMs learn new information.
- Evidence anchors:
  - [abstract] "We identify a two-phase 'imprint and shift' mechanism in feed-forward layers where gradients inject information from intermediate activations and target token embeddings into model weights."
  - [section 5.2] "We identify a mechanism we refer to as 'imprint and shift', which captures how information is stored in the feed-forward (MLP) module of the transformer layer."
  - [corpus] Weak - no corpus evidence directly supports this specific two-phase learning mechanism.
- Break condition: If the activation function is not monotonic or if layer normalization significantly alters the information flow, this mechanism may not accurately describe learning.

### Mechanism 3
- Claim: The VJP δnD⊤ at the beginning of the backward pass is dominated by the embedding of the target token.
- Mechanism: The initial VJP is a weighted sum of token embeddings, with the target token's embedding scaled by a negative coefficient (ˆp[t] - 1), making it the dominant component.
- Core assumption: The target token's embedding has the most significant influence on the initial VJP due to its negative scaling.
- Evidence anchors:
  - [section 5.1] "Lemma 5.1. The VJP δnD⊤ passed at the beginning of a backward pass is a vector in Rd that is a sum of weighted token embeddings. It is dominated by the embedding of the target token, D⊤[t], multiplied by a negative coefficient δn[t] = ˆp[t] - 1."
  - [section 3.2] "According to Equation 5, the backward pass' VJP to the layer that preceded D is: ∂L/∂xn = ∂L/∂ˆy D⊤ = δnD⊤ ∈ Rd"
  - [corpus] Weak - no corpus evidence directly supports this specific dominance of the target token embedding.
- Break condition: If the model's confidence in the target token is very high (ˆp[t] close to 1), the negative scaling effect would be minimal.

## Foundational Learning

- Concept: Backpropagation and the chain rule for computing gradients.
  - Why needed here: The paper analyzes how gradients flow backward through the model during training, which requires understanding backpropagation.
  - Quick check question: What is the relationship between the gradient of a layer's output and its input in terms of the chain rule?

- Concept: Vector-Jacobian Product (VJP) and its role as the hidden state of the backward pass.
  - Why needed here: The paper projects VJPs into the vocabulary space to interpret how information flows during backpropagation.
  - Quick check question: How does the VJP at each layer relate to the error signal propagated from subsequent layers?

- Concept: Low-rank matrix decomposition and its implications for model interpretability.
  - Why needed here: The paper proves that gradient matrices are low-rank, allowing them to be interpreted using a smaller set of vectors.
  - Quick check question: What is the maximum possible rank of a gradient matrix formed from n input tokens, and why?

## Architecture Onboarding

- Component map:
  - Input embedding matrix E transforms tokens to vectors
  - Transformer blocks with attention and MLP layers
  - Output decoding matrix D transforms vectors to token scores
  - Loss function (NLL) compares predictions to targets
  - Backpropagation computes gradients for weight updates

- Critical path:
  1. Forward pass: tokens → embeddings → transformer blocks → predictions
  2. Loss computation: predictions vs. targets
  3. Backward pass: loss → gradients via chain rule → weight updates

- Design tradeoffs:
  - Low-rank gradient decomposition enables simpler interpretation but may lose some information
  - Single forward pass editing method is computationally efficient but may be less precise than full backpropagation
  - Focus on MLP layers simplifies analysis but ignores potential information storage in attention layers

- Failure signatures:
  - If gradient rank exceeds n, the low-rank assumption is violated
  - If VJP norms are close to zero, those tokens have negligible impact on weight updates
  - If Logit Lens projections are unclear for early layers, the method may not be effective for those layers

- First 3 experiments:
  1. Measure gradient matrix rank for various prompts to verify the low-rank property
  2. Project gradient spanning sets (forward inputs and VJPs) into vocabulary space using Logit Lens
  3. Compare editing effectiveness of single forward pass method vs. full backpropagation for various learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the low-rank nature of gradient matrices extend to attention layers and not just MLP layers in transformers?
- Basis in paper: [inferred] The paper focuses on MLP layers due to their prominence in storing knowledge, but acknowledges that attention layers may also store information.
- Why unresolved: The analysis was limited to MLP layers, and the authors suggest that attention layers might exhibit different behaviors.
- What evidence would resolve it: Conducting a similar rank analysis and projection-based interpretability study on attention layers would reveal whether they share the low-rank property and similar imprint-and-shift mechanisms.

### Open Question 2
- Question: How does the "forward pass shift" editing method scale when applied to multiple prompts or iterative updates, and does it maintain comparable performance to multi-pass fine-tuning?
- Basis in paper: [explicit] The paper demonstrates single-prompt editing with "forward pass shift" and notes it as a limitation compared to typical fine-tuning which uses multiple prompts and iterations.
- Why unresolved: The method was only tested on single prompts and single passes, with the authors suggesting future exploration of multiple edits or iterations.
- What evidence would resolve it: Testing "forward pass shift" on multiple prompts with iterative updates and comparing performance to traditional fine-tuning would determine its scalability and effectiveness.

### Open Question 3
- Question: How do optimizers like Adam, which scale gradients, affect the low-rank property and interpretability of gradients compared to SGD?
- Basis in paper: [explicit] The authors acknowledge ignoring optimizer scaling in their theoretical analysis and note that optimizers alter the rank and weights of gradient matrices.
- Why unresolved: The theoretical analysis disregarded optimizers for simplicity, and the paper doesn't explore how different optimizers impact gradient interpretability.
- What evidence would resolve it: Comparing gradient rank analysis and projection-based interpretability between SGD and Adam (or other optimizers) would reveal how scaling affects the low-rank property and interpretability.

### Open Question 4
- Question: Are there learned transformations required for projecting gradients from earlier layers into vocabulary space, similar to the additional transformations suggested for forward pass interpretability?
- Basis in paper: [explicit] The authors note that LL interpretability is less effective for earlier layers and acknowledge the possibility that other transformations might be required for projecting earlier layers.
- Why unresolved: The paper used only the original LL projection despite acknowledging its limitations for earlier layers, and there's no wide consensus on additional transformations.
- What evidence would resolve it: Developing and testing learned transformations for gradient projection in earlier layers and comparing their interpretability to standard LL would determine if additional transformations are necessary.

## Limitations

- The low-rank gradient decomposition assumption lacks empirical validation across different model sizes and training scenarios
- The "imprint and shift" mechanism interpretation is primarily based on observed patterns without controlled causal experiments
- The single-forward-pass editing method comparison methodology may not fairly represent computational efficiency due to different hyperparameter optimization procedures

## Confidence

**High Confidence**: The mathematical framework for expressing gradients as low-rank combinations is theoretically sound and the experimental methodology for gradient analysis is well-defined.

**Medium Confidence**: The identification of the "imprint and shift" mechanism in MLP layers is supported by observed patterns but lacks causal evidence through controlled experiments.

**Low Confidence**: The single-forward-pass editing method's superiority claims are based on limited comparisons and may not hold across different model sizes or task types.

## Next Checks

**Check 1**: Systematically measure gradient matrix ranks across different transformer layers, model sizes, and training scenarios to validate the low-rank decomposition assumption holds in practice.

**Check 2**: Conduct ablation studies removing MLP layers to determine if the "imprint and shift" mechanism is unique to these layers or if similar patterns exist in attention mechanisms.

**Check 3**: Compare the single-forward-pass editing method against baselines using a unified hyperparameter optimization procedure to ensure fair comparison of computational efficiency claims.