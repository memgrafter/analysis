---
ver: rpa2
title: 'Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice
  Question Answering by Video Language Models'
arxiv_id: '2410.14248'
source_url: https://arxiv.org/abs/2410.14248
tags:
- correct
- weighted
- bias
- answer
- bold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses selection bias in video-language models (VLMs)
  for multiple-choice question answering (MCQA), where models disproportionately favor
  certain answer options based on positional patterns. The authors propose BOLD, a
  post-processing calibration technique that adapts fairness bias metrics to VLMs
  by decomposing the MCQA task into projections where one key component (video, question,
  or answer options) is removed.
---

# Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models

## Quick Facts
- arXiv ID: 2410.14248
- Source URL: https://arxiv.org/abs/2410.14248
- Authors: Olga Loginova; Oleksandr Bezrukov; Ravi Shekhar; Alexey Kravets
- Reference count: 30
- Primary result: Post-processing calibration technique (BOLD) reduces selection bias in VLMs for MCQA tasks by decomposing bias patterns and applying probability debiasing

## Executive Summary
This study addresses selection bias in video-language models (VLMs) for multiple-choice question answering (MCQA), where models disproportionately favor certain answer options based on positional patterns. The authors propose BOLD, a post-processing calibration technique that adapts fairness bias metrics to VLMs by decomposing the MCQA task into projections where one key component (video, question, or answer options) is removed. This allows identification of bias patterns and application of probability debiasing to balance them. Across three VLM architectures and four datasets, BOLD reduces selection bias as measured by standard deviation metrics and improves overall performance, including accuracy and F1 mean score. The method offers a cost-effective alternative to dataset augmentation or model retraining for mitigating selection bias in VLMs.

## Method Summary
The BOLD method introduces a novel post-processing calibration approach for addressing selection bias in video-language models used for MCQA tasks. The technique works by decomposing the MCQA task into three projections where one component (video, question, or answer options) is removed, allowing the model to identify positional bias patterns in how it selects answers. Once these patterns are identified, BOLD applies probability debiasing to adjust the model's output probabilities, effectively balancing the selection bias across different answer positions. This approach is particularly valuable because it operates as a post-processing step, making it computationally efficient compared to alternatives like dataset augmentation or full model retraining. The method was validated across three different VLM architectures and four distinct datasets, demonstrating consistent improvements in both bias reduction and task performance metrics.

## Key Results
- BOLD reduces selection bias as measured by standard deviation metrics across multiple VLM architectures and datasets
- The calibration technique improves overall MCQA performance, including accuracy and F1 mean score
- BOLD provides a cost-effective alternative to dataset augmentation or model retraining for bias mitigation

## Why This Works (Mechanism)
BOLD works by exploiting the decomposability of the MCQA task into three distinct projections where each of the three key components (video, question, answer options) is systematically removed. This decomposition reveals positional bias patterns that the model exhibits when selecting answers, as certain answer positions become favored based on the remaining information. By identifying these patterns through statistical analysis of the model's behavior across the projections, BOLD can then apply targeted probability adjustments to counteract the observed biases. The post-processing nature of the approach allows it to operate efficiently without requiring changes to the underlying model architecture or training data, making it particularly practical for deployment scenarios where computational resources or data availability are limited.

## Foundational Learning

**Video-Language Models (VLMs)**: Neural architectures that process both visual and textual information simultaneously to understand multimodal content. Why needed: VLMs are the primary technology being evaluated and calibrated in this work. Quick check: Can the model process both video frames and textual questions in a unified framework?

**Multiple-Choice Question Answering (MCQA)**: A task format where a model must select the correct answer from a predefined set of options based on given context. Why needed: This is the specific task format where selection bias manifests and is being addressed. Quick check: Does the task involve selecting one correct answer from multiple options using video and text input?

**Selection Bias**: The systematic preference of a model for certain answer positions or options regardless of their actual correctness. Why needed: This is the specific problem that BOLD aims to solve. Quick check: Does the model show consistent preference for certain answer positions across different questions?

**Probability Debiasing**: A statistical technique that adjusts probability distributions to remove systematic biases. Why needed: This is the core mechanism BOLD uses to correct identified bias patterns. Quick check: Can the method adjust output probabilities to balance selection across answer positions?

**Task Decomposition**: The process of breaking down complex tasks into simpler components for analysis. Why needed: BOLD relies on decomposing MCQA into projections to identify bias patterns. Quick check: Can the task be separated into components where one element is systematically removed?

## Architecture Onboarding

**Component Map**: Video input -> VLM backbone -> Question processing -> Answer option processing -> Prediction layer -> BOLD calibration -> Final output

**Critical Path**: The model processes video and question jointly, generates predictions for each answer option, BOLD analyzes positional patterns across the MCQA projections, applies probability adjustments, and produces debiased final predictions.

**Design Tradeoffs**: BOLD prioritizes computational efficiency and practical deployability by operating as post-processing calibration rather than requiring model retraining or data augmentation. This trades off the potential for more comprehensive bias mitigation that might be achieved through architectural changes or expanded training data.

**Failure Signatures**: The method may fail when selection bias patterns are too complex to be captured through simple positional analysis, or when biases arise from semantic or contextual factors rather than positional preferences. Additionally, the post-processing approach cannot address biases that are deeply embedded in the model's learned representations.

**First Experiments**:
1. Measure baseline selection bias across answer positions for a given VLM on a standard MCQA dataset
2. Apply BOLD calibration and measure reduction in positional bias metrics
3. Compare MCQA performance (accuracy, F1) before and after BOLD application

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses primarily on positional selection bias without addressing other potential bias sources such as semantic or contextual biases
- Evaluation limited to three VLM architectures and four datasets, restricting generalizability
- Post-processing nature means bias mitigation occurs after inference, potentially missing earlier intervention opportunities

## Confidence

**High Confidence**: The empirical results demonstrating BOLD's effectiveness in reducing selection bias metrics (standard deviation) and improving performance metrics (accuracy, F1 mean score) across multiple datasets and model architectures are well-supported by the experimental evidence presented.

**Medium Confidence**: The claim that BOLD offers a cost-effective alternative to dataset augmentation or model retraining is reasonable but requires broader economic analysis across different deployment scenarios and model scales to fully validate.

**Medium Confidence**: The assertion that BOLD is a general solution for mitigating selection bias in VLMs may be overstated, as the method's effectiveness could vary significantly with different types of bias patterns or model architectures not tested in this study.

## Next Checks
1. Test BOLD's effectiveness on additional VLM architectures, particularly those with different underlying mechanisms (e.g., transformer-based vs. convolutional approaches), to assess generalizability across the broader VLM landscape.

2. Evaluate BOLD on datasets with known semantic or contextual biases to determine whether the method addresses only positional bias or can be extended to handle other bias types in video-language tasks.

3. Conduct a cost-benefit analysis comparing BOLD to alternative bias mitigation approaches (dataset augmentation, model retraining) across different scales of deployment, including computational requirements, implementation complexity, and long-term maintenance considerations.