---
ver: rpa2
title: Enhancing Perception Capabilities of Multimodal LLMs with Training-Free Fusion
arxiv_id: '2412.01289'
source_url: https://arxiv.org/abs/2412.01289
tags:
- visual
- vision
- mllms
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisionFuse enhances multimodal large language models (MLLMs) by
  integrating vision encoders from multiple MLLMs within the same family without additional
  training. It merges language model parameters to align with different vision encoders
  and concatenates their visual tokens to enrich contextual information.
---

# Enhancing Perception Capabilities of Multimodal LLMs with Training-Free Fusion

## Quick Facts
- arXiv ID: 2412.01289
- Source URL: https://arxiv.org/abs/2412.01289
- Reference count: 40
- One-line primary result: VisionFuse achieves over 4% average performance gain when integrating MiniGemini-8B and SLIME-8B, matching models with six times more parameters

## Executive Summary
VisionFuse is a training-free method that enhances multimodal large language models (MLLMs) by integrating vision encoders from multiple MLLMs within the same family. By merging language model parameters and concatenating visual tokens from different vision encoders, VisionFuse enables a single language model to leverage complementary visual attention patterns without additional training. Experiments demonstrate significant improvements across multiple benchmarks, with VisionFuse achieving performance comparable to models with substantially more parameters.

## Method Summary
VisionFuse works by extracting vision encoders from multiple MLLMs within the same family, merging their language model parameters through weighted linear interpolation of delta parameters, and concatenating visual tokens from different encoders before feeding them into the merged LLM. This approach leverages the complementary attention patterns of different models while avoiding the computational cost of retraining. The method requires consistent preprocessing pipelines and vision projectors across the integrated MLLMs to ensure compatibility.

## Key Results
- VisionFuse achieves over 4% average performance gain when integrating MiniGemini-8B and SLIME-8B
- Performance matches that of models with six times more parameters
- Demonstrates effectiveness across multiple benchmarks including TextVQA, MME, VQAv2, MMBench, MMBench-Chinese, MMMU, and VizWiz

## Why This Works (Mechanism)

### Mechanism 1
Different MLLMs attend to distinct image regions for the same query and visual input. By concatenating tokens from multiple vision encoders, VisionFuse captures a more complete representation of the target region by leveraging complementary visual attention patterns. The core assumption is that attention distributions across MLLMs are sufficiently different to provide complementary information.

### Mechanism 2
Vision encoders within an MLLM family exhibit more consistent feature distributions. Because vision encoders within a family share the same pretrained LLM, their visual features are more compatible for concatenation and integration within a shared context. The core assumption is that vision encoders trained on the same LLM base produce feature distributions that are sufficiently aligned for effective integration.

### Mechanism 3
Merging language model parameters aligns a single LLM with different vision encoders. By merging delta parameters from different MLLMs within a family, a single language model can effectively interpret visual tokens from multiple vision encoders without additional training. The core assumption is that delta parameters capture the alignment information needed to adapt a language model to different vision encoders.

## Foundational Learning

- Concept: Vision encoder alignment in multimodal models
  - Why needed here: Understanding how vision encoders are aligned with LLMs is crucial for grasping why VisionFuse can avoid retraining
  - Quick check question: What is the typical approach for aligning a vision encoder with a pretrained LLM?

- Concept: Model parameter merging techniques
  - Why needed here: VisionFuse relies on merging delta parameters from different MLLMs, which requires understanding various merging strategies
  - Quick check question: What are the key differences between simple averaging, Task Arithmetic, and TIES-merging approaches?

- Concept: Visual token redundancy and pruning
  - Why needed here: VisionFuse generates longer visual sequences that may require pruning strategies to maintain efficiency
  - Quick check question: At what layer do visual tokens typically exhibit the most redundancy in MLLMs?

## Architecture Onboarding

- Component map: Vision encoders from multiple MLLMs → Vision projectors → Concatenated visual tokens → Merged LLM with text tokens → Inference
- Critical path: Input image → M preprocessing pipelines → M vision encoders → M vision projectors → Concatenate visual tokens → Merge with text tokens → Merged LLM inference
- Design tradeoffs: Token concatenation increases sequence length (computation cost) but provides richer visual information; parameter merging avoids retraining but may introduce alignment errors
- Failure signatures: Performance degradation when sequence length exceeds training thresholds, alignment errors between merged LLM and vision encoders, or loss of complementary information when attention patterns converge
- First 3 experiments:
  1. Ablation study comparing VisionFuse with individual MLLMs on TextVQA to verify performance improvement
  2. Analysis of attention map differences between MGM-8B and SLIME-8B to confirm complementary visual focus
  3. Token pruning experiment to determine optimal sequence length for maintaining performance while reducing computation

## Open Questions the Paper Calls Out

The paper acknowledges that directly concatenating visual tokens from more MLLMs leads to excessively long sequences and performance decline, suggesting potential scalability issues. It mentions token pruning as a potential solution but leaves it for future work. The paper also demonstrates that merging models from different families results in significant performance drops on some datasets due to larger differences in delta parameters, while integration within the same family works well.

## Limitations

- The approach is limited to integrating MLLMs within the same family due to feature distribution alignment requirements
- Sequence length increases with more integrated models can exceed training thresholds and cause performance degradation
- The effectiveness of delta parameter merging across different model families has not been fully explored

## Confidence

**High Confidence Claims**:
- VisionFuse can integrate vision encoders from multiple MLLMs within the same family without additional training
- The approach achieves significant performance improvements on benchmark tasks
- VisionFuse matches the performance of models with six times more parameters

**Medium Confidence Claims**:
- Different MLLMs attend to distinct image regions for the same query
- Vision encoders within an MLLM family exhibit more consistent feature distributions
- Merging language model parameters aligns a single LLM with different vision encoders

**Low Confidence Claims**:
- The exact degree of complementary information captured through token concatenation
- The robustness of the approach across different MLLM families beyond those tested
- The generalization of performance improvements to real-world applications

## Next Checks

1. **Quantitative Attention Analysis**: Measure and compare the attention distributions of different MLLMs using statistical metrics (e.g., KL divergence, mutual information) to quantify the claimed complementarity. This would validate whether attention patterns are truly distinct enough to provide meaningful complementary information.

2. **Cross-Family Compatibility Testing**: Evaluate VisionFuse using vision encoders from different MLLM families to determine whether the approach is truly dependent on family alignment or can generalize across families. This would test the fundamental assumption about feature distribution alignment.

3. **Parameter Merging Ablation**: Systematically compare VisionFuse's parameter merging approach against alternative merging strategies (simple averaging, Task Arithmetic, TIES-merging) and explore the sensitivity to different λ values. This would validate whether the specific merging approach is optimal or merely sufficient.