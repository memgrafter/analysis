---
ver: rpa2
title: Contrastive Learning to Improve Retrieval for Real-world Fact Checking
arxiv_id: '2410.04657'
source_url: https://arxiv.org/abs/2410.04657
tags:
- document
- answer
- documents
- claim
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving evidence retrieval
  for complex fact-checking tasks, where traditional methods often fail to capture
  nuanced inferences required to verify claims. The authors propose Contrastive Fact-Checking
  Reranker (CFR), a dense retriever fine-tuned using contrastive learning with multiple
  supervision signals, including GPT-4 distillation, answer equivalence evaluation
  via LERC, and gold evidence from the AVeriTeC dataset.
---

# Contrastive Learning to Improve Retrieval for Real-world Fact Checking

## Quick Facts
- arXiv ID: 2410.04657
- Source URL: https://arxiv.org/abs/2410.04657
- Authors: Aniruddh Sriram; Fangyuan Xu; Eunsol Choi; Greg Durrett
- Reference count: 17
- Primary result: CFR improves veracity classification accuracy by 6% on AVeriTeC dataset

## Executive Summary
This paper addresses the challenge of improving evidence retrieval for complex fact-checking tasks, where traditional methods often fail to capture nuanced inferences required to verify claims. The authors propose Contrastive Fact-Checking Reranker (CFR), a dense retriever fine-tuned using contrastive learning with multiple supervision signals, including GPT-4 distillation, answer equivalence evaluation via LERC, and gold evidence from the AVeriTeC dataset. CFR significantly improves veracity classification accuracy by 6% on AVeriTeC and demonstrates strong generalization to other fact-checking datasets like FEVER, ClaimDecomp, and HotpotQA. Additionally, synthetic experiments show that CFR enhances the retriever's ability to identify documents requiring indirect reasoning, addressing a key limitation of baseline models.

## Method Summary
The authors propose CFR, which fine-tunes the Contriever dense retriever using contrastive learning with multiple supervision signals. The training pipeline uses GPT-4 to generate relevance judgments, LERC scores to evaluate answer equivalence for subquestion answers, and gold evidence from the AVeriTeC dataset. The contrastive loss pushes relevant documents closer to queries while pushing irrelevant ones apart in the embedding space. The model is trained on claim-subquestion pairs with associated document sets, learning to identify the most helpful evidence for verifying complex claims.

## Key Results
- CFR improves veracity classification accuracy by 6% on the AVeriTeC dataset
- CFR demonstrates strong generalization to out-of-domain datasets (FEVER, ClaimDecomp, HotpotQA)
- Synthetic experiments show CFR achieves MRR of 0.79 compared to baseline Contriever's 0.68 for identifying documents requiring indirect reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with multiple supervision signals (GPT-4 distillation, LERC-based answer equivalence, and gold evidence) improves retriever performance on complex fact-checking tasks.
- Mechanism: The model learns to embed query-document pairs such that documents providing better answers to subquestions (measured by LERC or GPT-4 relevance) are pushed closer in the embedding space, while irrelevant or less helpful documents are pushed apart.
- Core assumption: Document relevance for fact-checking can be effectively measured through answer equivalence and distilled knowledge, even when direct string matching fails.
- Evidence anchors:
  - [abstract] "We fine-tune Contriever with a contrastive objective based on multiple training signals, including distillation from GPT-4, evaluating subquestion answers, and gold labels in the dataset."
  - [section 3.3] "Documents with poor LERC scores (< 0.3) become negative contexts, and documents with high LERC (> 0.7) scores are positive contexts."
  - [corpus] Weak evidence - the corpus shows related work on fact-checking retrieval but doesn't directly validate the LERC mechanism.
- Break condition: If LERC scores become unreliable (e.g., for abstractive answers) or GPT-4 relevance judgments become inconsistent, the contrastive signal may degrade.

### Mechanism 2
- Claim: Combining gold evidence with GPT-4 distillation and LERC-based supervision yields better retrieval performance than using any single signal alone.
- Mechanism: Each supervision signal captures different aspects of relevance - gold evidence provides ground truth, GPT-4 distillation captures nuanced relevance judgments, and LERC measures answer quality. Their combination creates a richer training signal.
- Core assumption: Different supervision signals are complementary and their combination addresses the limitations of any single approach.
- Evidence anchors:
  - [section 3.3] "To reduce the false negative rate, we mix in relevant documents with the positive set from distill to create {D+dg ∪ D+l, D-dg}."
  - [section 5.1] "CFR also excels in top doc relevance and other upstream metrics. This indicates evaluating answers derived from documents may help downstream performance on fact-checking more than other supervision signals."
  - [corpus] Weak evidence - related work shows individual approaches but limited validation of combination strategies.
- Break condition: If one signal becomes dominant or inconsistent with others, the combined training may become unstable or biased.

### Mechanism 3
- Claim: Fine-tuning on synthetic reasoning examples improves the retriever's ability to identify documents requiring indirect inference.
- Mechanism: The synthetic data forces the model to learn embeddings that can distinguish between documents that directly answer a question versus those requiring reasoning hops, even when both appear topically relevant.
- Core assumption: Synthetic reasoning examples can effectively teach the model to recognize indirect evidence patterns that transfer to real-world fact-checking.
- Evidence anchors:
  - [section 6] "We find a statistically significant gain in our finetuned model's ability to surface the positive document over other distractor documents. CFR achieves an MRR of 0.79 compared to baseline Contriever (0.68)."
  - [section 6.1] "The positive document is a paragraph that supports an answer to the question, but only indirectly."
  - [corpus] Moderate evidence - related work on synthetic data for retrieval exists, but specific validation for reasoning capabilities is limited.
- Break condition: If the synthetic examples don't generalize to real fact-checking scenarios or the reasoning patterns are too domain-specific.

## Foundational Learning

- Concept: Contrastive learning objectives for dense retrievers
  - Why needed here: The paper uses contrastive loss to push relevant documents closer to queries while pushing irrelevant ones apart, which is fundamental to training the retriever.
  - Quick check question: How does the temperature parameter τ in the contrastive loss affect the learned embeddings?

- Concept: Answer equivalence metrics for long-form answers
  - Why needed here: Traditional ROUGE metrics fail for complex fact-checking answers, so LERC is used to measure semantic equivalence between generated and gold answers.
  - Quick check question: Why does the paper use answer shortening (s) before computing LERC scores?

- Concept: Knowledge distillation from large language models
  - Why needed here: GPT-4 is used to generate relevance judgments and answer equivalence scores, effectively distilling its reasoning capabilities into the retriever training signal.
  - Quick check question: What are the potential limitations of using GPT-4 for relevance labeling in retriever training?

## Architecture Onboarding

- Component map: Query → BM25 retrieval → CFR re-ranking → Top document selection → GPT-4 answer generation → Veracity classification
- Critical path: The system uses a two-stage retrieval pipeline where BM25 provides initial document collection, followed by CFR re-ranking to identify the most relevant evidence for complex claims.
- Design tradeoffs: The system trades computational complexity (two-stage retrieval, GPT-4 prompts) for improved accuracy on complex claims. Using multiple supervision signals increases training complexity but improves generalization.
- Failure signatures: Poor performance on implicit subquestions, over-reliance on lexical overlap, failure to identify indirect evidence, or inconsistent LERC scores indicating answer equivalence issues.
- First 3 experiments:
  1. Compare CFR performance against baseline Contriever on A VeriTeC using LERC and veracity metrics
  2. Evaluate out-of-domain generalization on ClaimDecomp, FEVER, and HotpotQA
  3. Test synthetic reasoning dataset performance to validate indirect inference capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative long answer equivalence metrics beyond LERC improve retrieval performance for fact-checking?
- Basis in paper: [explicit] The authors note that LERC requires answer shortening which loses information, and suggest developing better long answer equivalence metrics could improve retrieval.
- Why unresolved: The paper only experiments with LERC and ROUGE-F1 for answer equivalence, finding both have limitations. They don't explore other semantic similarity measures or answer equivalence metrics.
- What evidence would resolve it: Systematic comparison of different answer equivalence metrics (BERTScore, BLEURT, human evaluations) on their ability to improve retrieval performance for fact-checking tasks.

### Open Question 2
- How does the effectiveness of contrastive learning for fact-checking retrievers vary with the quality and quantity of training data?
- Basis in paper: [inferred] The authors experiment with different supervision signals (GPT-4 distillation, LERC-based filtering, gold evidence) but don't systematically analyze how data quality/quantity affects performance.
- Why unresolved: While the paper shows CFR improves performance, it doesn't analyze whether more training data or higher-quality training examples would yield even better results, or whether there's a point of diminishing returns.
- What evidence would resolve it: Ablation studies varying the amount and quality of training data, or analyzing performance as a function of training set size.

### Open Question 3
- Can the CFR approach generalize to fact-checking domains beyond English-language political claims?
- Basis in paper: [explicit] The authors explicitly state their work focuses on English-language political claims and note that claims in other languages, multimedia formats, or specialized domains may present distinct challenges.
- Why unresolved: The paper only evaluates on English datasets and doesn't test whether the approach works for other claim types or languages.
- What evidence would resolve it: Experimental results showing CFR performance on non-English claims, COVID-19 misinformation, or multimedia-based claims.

## Limitations

- The approach relies heavily on GPT-4 for supervision signals, creating potential reliability issues if GPT-4 judgments become inconsistent or biased
- The synthetic reasoning dataset may not fully capture the complexity of real-world fact-checking scenarios requiring indirect inference
- The combination of multiple supervision signals creates potential for conflicting signals that aren't thoroughly analyzed in terms of interaction and stability

## Confidence

- **High confidence**: The core mechanism of using contrastive learning with multiple supervision signals is well-established and the experimental results on AVeriTeC show consistent improvements.
- **Medium confidence**: The generalization to out-of-domain datasets (FEVER, ClaimDecomp, HotpotQA) shows promising results but with varying degrees of improvement across datasets.
- **Medium confidence**: The synthetic reasoning experiments demonstrate improved ability to identify indirect evidence, but the transferability to real-world scenarios needs further validation.

## Next Checks

1. Conduct ablation studies to determine the relative contribution and potential conflicts between GPT-4 distillation, LERC-based supervision, and gold evidence supervision.
2. Test CFR performance on a more diverse set of fact-checking datasets with varying levels of complexity and different domains to better assess generalization.
3. Evaluate the model's performance when GPT-4 judgments are replaced with alternative LLM providers or human annotations to assess dependency on a single model.