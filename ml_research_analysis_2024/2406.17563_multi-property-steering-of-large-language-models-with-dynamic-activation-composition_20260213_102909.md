---
ver: rpa2
title: Multi-property Steering of Large Language Models with Dynamic Activation Composition
arxiv_id: '2406.17563'
source_url: https://arxiv.org/abs/2406.17563
tags:
- steering
- unsafe
- language
- generation
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive evaluation of activation steering
  techniques for conditioning large language model (LLM) outputs across multiple properties,
  including language, safety, and formality. The authors find that the effectiveness
  of steering is highly property-dependent, with a trade-off between conditioning
  strength and output fluency.
---

# Multi-property Steering of Large Language Models with Dynamic Activation Composition

## Quick Facts
- arXiv ID: 2406.17563
- Source URL: https://arxiv.org/abs/2406.17563
- Authors: Daniel Scalena; Gabriele Sarti; Malvina Nissim
- Reference count: 40
- One-line primary result: Dynamic Activation Composition maintains high multi-property conditioning while preserving generation fluency better than fixed-intensity steering approaches

## Executive Summary
This paper presents a comprehensive evaluation of activation steering techniques for conditioning LLM outputs across multiple properties including language, safety, and formality. The authors find that steering effectiveness varies significantly by property type, with a trade-off between conditioning strength and output fluency. To address this challenge, they propose Dynamic Activation Composition, which uses KL divergence to dynamically modulate steering intensity during generation based on information gain.

The method achieves strong conditioning accuracy (up to 100% for some properties) while maintaining perplexity close to the in-context learning baseline. Experiments show that Dynamic Activation Composition outperforms fixed and diminishing intensity approaches for multi-property steering, successfully maintaining high conditioning across all properties while minimizing impact on generation fluency.

## Method Summary
The method uses activation steering to condition LLM outputs by extracting steering vectors from contrastive pairs of in-context examples. These vectors capture property-specific behavior and are added to intermediate model states during generation. Dynamic Activation Composition modulates the steering intensity dynamically by computing KL divergence between probability distributions of steered versus unsteered models. For multi-property steering, multiple steering vectors are composed and individually modulated based on their information gain. The approach is evaluated on Mistral 7B Instruct across language, safety, and formality properties using datasets like Alpaca, BeaverTails, and GYAFC.

## Key Results
- Dynamic Activation Composition achieves up to 100% conditioning accuracy for certain properties while maintaining perplexity close to ICL baseline
- The method outperforms fixed and diminishing intensity steering strategies for multi-property conditioning
- Different properties show varying sensitivities to steering intensity, requiring property-specific modulation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Steering vectors capture property-specific behavior by encoding the difference between contrastive activation pairs
- Mechanism: Activation steering extracts vectors from model internals by comparing activations from positive and negative in-context examples, then adds these vectors to intermediate states during generation to condition behavior
- Core assumption: High-level concepts are represented linearly in intermediate LLM activations, allowing behavior to be encoded as additive vectors
- Evidence anchors: [abstract] "These techniques use model internals to craft steering vectors capturing the behavior of interest"; [section 2] "Recent methods derive steering vectors from LM activations over contrastive pairs of in-context demonstrations"; [corpus] Weak - no direct corpus support found for linear representation hypothesis
- Break condition: If concept representation is non-linear or distributed across multiple dimensions, steering vectors will fail to capture the property

### Mechanism 2
- Claim: Dynamic modulation of steering intensity prevents over-steering and maintains fluency by reducing intervention when generation is already conditioned
- Mechanism: Dynamic Activation Composition computes KL divergence between probability distributions of steered vs. unsteered models, using this to set steering intensity per step
- Core assumption: The shift in probability distributions caused by high-intensity steering can indicate when conditioning is already achieved
- Evidence anchors: [abstract] "dynamically modulates the steering intensity throughout generation based on the information gain derived from steering vectors"; [section 6.1] "The usage of KL-divergence in this setting is motivated by recent work using similar contrastive metrics to detect context usage in LLM generations"; [corpus] Weak - no direct corpus support found for KL divergence as steering intensity metric
- Break condition: If property conditioning requires consistent high-intensity steering throughout generation, dynamic reduction will reduce effectiveness

### Mechanism 3
- Claim: Multi-property steering can be achieved by composing steering vectors for different properties, with dynamic adjustment preventing interference
- Mechanism: Multiple steering vectors are applied simultaneously, with Dynamic Activation Composition modulating each based on its individual information gain
- Core assumption: Different properties can be controlled independently through their respective steering vectors without significant interference
- Evidence anchors: [abstract] "Our method successfully maintains high conditioning across all properties while minimizing the impact of conditioning on generation fluency"; [section 7] "we evaluate baseline activation injection strategies and the newly introduced Dyn method for multi-property steering"; [corpus] Weak - no direct corpus support found for multi-property vector composition
- Break condition: If properties interact in complex ways or share neural representations, simultaneous steering may cause conflicts

## Foundational Learning

- Concept: Contrastive pairs in in-context learning
  - Why needed here: Steering vectors are derived from differences between positive and negative example activations
  - Quick check question: What are the two types of examples needed to create a steering vector for the "safe" property?

- Concept: Activation extraction from transformer layers
  - Why needed here: Steering vectors are extracted from attention head outputs at specific token positions
  - Quick check question: At which token position are steering vectors typically extracted in this method?

- Concept: KL divergence for probability distribution comparison
  - Why needed here: Used to measure information gain and dynamically adjust steering intensity
  - Quick check question: What does a KL divergence of 0 indicate about the relationship between two probability distributions?

## Architecture Onboarding

- Component map: Mistral 7B Instruct model with attention heads and layers -> steering vector computation module -> dynamic intensity modulation module -> evaluation metrics (language detector, safety classifier, formality classifier, perplexity)
- Critical path: In-context examples → activation extraction → steering vector computation → generation with dynamic injection → evaluation
- Design tradeoffs: Fixed vs. diminishing vs. dynamic intensity (conditioning vs. fluency), single vs. multi-property steering (simplicity vs. complexity), model size selection (capability vs. resource usage)
- Failure signatures: Low conditioning accuracy despite steering (vector extraction issues), high perplexity (over-steering), property interference (multi-property conflicts)
- First 3 experiments:
  1. Single-property steering (Italian language) with Fixed α=1, measure conditioning accuracy and perplexity
  2. Single-property steering with Dynamic Activation Composition, compare to Fixed approach
  3. Multi-property steering (Italian + Unsafe), evaluate both properties simultaneously

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Dynamic Activation Composition perform when applied to larger language models beyond Mistral 7B?
- Basis in paper: The paper explicitly mentions that the advantage of Dynamic Activation Composition is evident from comparison to other techniques tested, but notes that results are based on experiments with only one instruction-based model, namely Mistral 7B. It states that a more comprehensive study should include a larger range of models, both in terms of size and characteristics.
- Why unresolved: The paper's experiments were limited to a single model (Mistral 7B), so the effectiveness of Dynamic Activation Composition on other models, especially larger ones, remains unknown.
- What evidence would resolve it: Experiments applying Dynamic Activation Composition to a diverse set of language models with varying sizes and training characteristics (e.g., different instruction-tuning or alignment methods) would provide evidence of its generalizability and effectiveness across models.

### Open Question 2
- Question: How robust is Dynamic Activation Composition to different steering setups beyond the contrastive pair method used in the paper?
- Basis in paper: The paper mentions that their evaluation of injection strategies is limited to a single steering setup (described in Section 3), which is in line with previous work using contrastive pairs of in-context examples for activation steering. It suggests that future work could evaluate whether their proposed Dyn method would generalize to other steering configurations using, for example, the directions derived from probing classifiers.
- Why unresolved: The paper only tested Dynamic Activation Composition with one specific method of deriving steering vectors (contrastive pairs of in-context examples). Its effectiveness with other methods of obtaining steering directions is unknown.
- What evidence would resolve it: Experiments applying Dynamic Activation Composition with different methods of obtaining steering vectors, such as those derived from probing classifiers or other techniques, would show whether the method is robust to different steering setups.

### Open Question 3
- Question: What is the optimal value of the ptop parameter for Dynamic Activation Composition across different properties and tasks?
- Basis in paper: The paper discusses the ptop parameter, which determines the amount of tokens considered in the KL Divergence computation. It shows results for ptop ∈ [0.4, 0.5, 0.6, 0.7, 0.9] and notes that 0.5 is found to be optimal for multi-property steering. However, it does not extensively explore the optimal ptop for single-property steering or other tasks.
- Why unresolved: While the paper provides some insight into the ptop parameter, it does not conduct a thorough investigation of its optimal value across different properties and tasks, leaving this as an open question.
- What evidence would resolve it: A systematic study varying the ptop parameter across a wide range of properties and tasks, measuring the resulting conditioning accuracy and generation fluency, would identify the optimal ptop values for different scenarios.

## Limitations

- The linear representation hypothesis for high-level concepts in LLM activations lacks direct corpus support
- The Dynamic Activation Composition method introduces novel use of KL divergence without established theoretical grounding
- Multi-property steering assumes independent control of different properties without systematic validation of potential interference

## Confidence

**High confidence**: The experimental methodology for measuring conditioning accuracy and perplexity is well-defined and reproducible. The observation that different properties exhibit varying sensitivities to steering intensity is empirically supported and consistent across evaluations.

**Medium confidence**: The Dynamic Activation Composition algorithm demonstrates improved performance over fixed-intensity approaches in controlled experiments. However, the theoretical justification for KL divergence as the optimal information gain metric remains underdeveloped.

**Low confidence**: The linear hypothesis for concept representation in activations lacks corpus support. The independence assumption for multi-property steering is not validated through ablation studies or interference analysis.

## Next Checks

1. **Ablation study on steering vector sources**: Test whether steering vectors derived from different model sizes or architectures maintain conditioning effectiveness, validating the transferability assumption.

2. **Interference analysis for multi-property steering**: Systematically evaluate property interference by steering conflicting properties (e.g., formal vs. casual) simultaneously and measuring cross-property effects.

3. **Theoretical grounding for KL divergence**: Compare Dynamic Activation Composition against alternative information-theoretic metrics (e.g., Jensen-Shannon divergence, mutual information) to establish whether KL divergence is optimal for steering intensity modulation.