---
ver: rpa2
title: 'Exploring adversarial robustness of JPEG AI: methodology, comparison and new
  methods'
arxiv_id: '2411.11795'
source_url: https://arxiv.org/abs/2411.11795
tags:
- image
- adversarial
- jpeg
- attacks
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive evaluation of adversarial\
  \ robustness in JPEG AI, the first neural network-based international image compression\
  \ standard. The authors develop a new methodology to assess NIC robustness using\
  \ multiple full-reference metrics (\u0394PSNR, \u0394MSE, \u0394MS-SSIM, \u0394\
  VMAF) and evaluate 10 neural image compression models against 6 different adversarial\
  \ attacks."
---

# Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods

## Quick Facts
- arXiv ID: 2411.11795
- Source URL: https://arxiv.org/abs/2411.11795
- Reference count: 40
- JPEG AI shows over 50% bitrate savings while maintaining comparable quality, establishing it as a leading compression technology

## Executive Summary
This paper presents the first comprehensive evaluation of adversarial robustness in JPEG AI, the first neural network-based international image compression standard. The authors develop a new methodology to assess NIC robustness using multiple full-reference metrics and evaluate 10 neural image compression models against 6 different adversarial attacks. Their experiments reveal that JPEG AI shows relatively high robustness compared to other NIC models, with newer versions demonstrating improved resistance. The study also investigates adversarial defenses, finding that simple reversible transformations can effectively mitigate attacks. Notably, the research demonstrates that attacks can increase both perceptual degradation and compressed file size, and that attacks transfer between different versions of JPEG AI.

## Method Summary
The paper evaluates adversarial robustness of JPEG AI and other neural image compression models using 4 datasets (KODAK, CITYSCAPES, NIPS 2017 adversarial dataset, BSDS), 10 NIC models including JPEG AI 4.1/5.1/6.1, and 6 attack methods (FTDA, I-FGSM, MADC, PGD, SSAH, CAdv). The methodology employs white-box attacks with varied parameters and measures effectiveness using ΔPSNR, ΔMSE, ΔMS-SSIM, ΔVMAF metrics. The study also investigates 8 defense methods and evaluates attack transferability between models. Experiments are conducted across different bitrates to assess robustness under varying compression conditions.

## Key Results
- JPEG AI showed relatively high robustness compared to other NIC models
- Newer versions of JPEG AI (6.1 vs 5.1) demonstrate improved resistance to adversarial attacks
- Simple reversible transformations can effectively mitigate adversarial attacks on NIC models
- Adversarial attacks can increase both perceptual degradation and compressed file size
- Attacks transfer between different versions of JPEG AI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JPEG AI's improved robustness in newer versions results from architectural refinements that reduce the effectiveness of adversarial perturbations.
- Mechanism: The newer versions of JPEG AI incorporate enhanced attention mechanisms and improved entropy coding that make the latent representation less sensitive to small perturbations in the input space.
- Core assumption: The improved robustness is primarily due to architectural changes rather than changes in training methodology or data augmentation.
- Evidence anchors:
  - [section] "JPEG AI showed relatively high robustness compared to other NIC models. High-operation point versions of JPEG AI are less robust than base-operation point. Second, the robustness of JPEG AI improved with a newer version (6.1 compared to 5.1)."
  - [corpus] Weak evidence - the corpus does not provide specific details about architectural changes between JPEG AI versions that would explain improved robustness.

### Mechanism 2
- Claim: Reversible transformations as preprocessing defenses work by disrupting the alignment between the adversarial perturbation and the compression model's feature extraction process.
- Mechanism: When an image undergoes a reversible transformation before compression, the adversarial perturbation becomes misaligned with the compression model's learned features.
- Core assumption: The adversarial perturbation is highly dependent on the specific spatial and color configuration of the input image.
- Evidence anchors:
  - [section] "Simple reversible defenses can negate adversarial attacks on NIC models. The results demonstrate that Flip, Random Ensemble, and Random Roll are somewhat effective in reducing their effect."
  - [corpus] No direct evidence in the corpus about how reversible transformations disrupt adversarial perturbations in compression contexts.

### Mechanism 3
- Claim: JPEG AI's end-to-end training approach creates a more coherent compression pipeline that is inherently more resistant to adversarial attacks compared to modular approaches.
- Mechanism: Unlike traditional compression pipelines that chain separate components, JPEG AI's end-to-end training allows the entire system to learn a unified representation that optimizes for both compression efficiency and robustness to perturbations.
- Core assumption: End-to-end training provides robustness benefits beyond just compression efficiency improvements.
- Evidence anchors:
  - [section] "JPEG AI applies quantization across the entire image based on learned patterns, which is more efficient than the traditional block-based approach."
  - [corpus] No evidence in the corpus directly comparing end-to-end trained NICs with modular approaches in terms of adversarial robustness.

## Foundational Learning

- Concept: Rate-distortion theory and its application to image compression
  - Why needed here: Understanding how JPEG AI balances compression efficiency (rate) with image quality (distortion) is crucial for interpreting why certain adversarial attacks are more effective than others.
  - Quick check question: What is the mathematical formulation of the rate-distortion optimization problem in lossy image compression?

- Concept: Variational autoencoders (VAEs) and their role in neural image compression
  - Why needed here: JPEG AI and many other NICs are based on VAE architectures, so understanding how VAEs work is essential for grasping the compression mechanism and potential vulnerabilities.
  - Quick check question: How does the quantization step in VAE-based compression differ from traditional scalar quantization?

- Concept: Adversarial attack methodologies in computer vision
  - Why needed here: The paper evaluates JPEG AI against various adversarial attacks, so understanding attack methodologies (FGSM, PGD, etc.) is necessary for interpreting the results.
  - Quick check question: What is the key difference between white-box and black-box adversarial attacks, and why might white-box attacks be more effective against compression models?

## Architecture Onboarding

- Component map: Input image → Analysis transform → Quantization → Entropy coding → Bitstream → Entropy decoding → De-quantization → Synthesis transform → Output image
- Critical path: The complete compression-decompression pipeline from input image through all neural network transforms and quantization steps
- Design tradeoffs:
  - Compression efficiency vs. computational complexity (HOP vs. BOP configurations)
  - Model size and inference speed vs. reconstruction quality
  - Robustness to adversarial attacks vs. compression ratio
  - Hardware implementation complexity vs. performance
- Failure signatures:
  - Significant quality degradation (high ΔPSNR, ΔMSE, ΔMS-SSIM, ΔVMAF) indicates successful adversarial attacks
  - Unexpected increase in compressed file size despite quality degradation suggests adversarial perturbations are creating more complex patterns
  - Transferability of attacks between different NIC models indicates shared vulnerabilities in the compression pipeline
- First 3 experiments:
  1. Implement a basic end-to-end JPEG AI compression pipeline and verify it can compress and decompress images with reasonable quality
  2. Apply a simple adversarial attack (like FGSM) to an image and observe the effect on JPEG AI compression quality metrics
  3. Implement one of the reversible defenses (like random rotation) and measure its effectiveness against the same adversarial attack

## Open Questions the Paper Calls Out
- How do adversarial attacks specifically affect perceptual quality metrics like VMAF compared to traditional distortion metrics like PSNR in JPEG AI compression?
- Why does JPEG AI show higher robustness to adversarial attacks compared to other neural image compression models, and what architectural features contribute to this resistance?
- How effective are reversible transformation-based defenses across different types of neural image compression architectures, and what are the limitations of these approaches?

## Limitations
- The paper doesn't explain the specific architectural changes between JPEG AI versions that lead to improved robustness
- Defense effectiveness is only tested on JPEG AI, not across the broader range of NIC models evaluated
- The study focuses on white-box attacks and doesn't explore realistic threat models where attackers have limited knowledge

## Confidence
- JPEG AI's relative robustness compared to other NIC models: High confidence
- Exact mechanisms explaining version-specific improvements: Medium confidence
- Defense effectiveness across different NIC architectures: Low confidence

## Next Checks
1. Conduct ablation studies on JPEG AI architecture versions to isolate which specific components contribute most to improved robustness
2. Test attack transferability across a broader range of NIC models beyond the current selection to better understand vulnerability patterns
3. Evaluate the proposed defenses under realistic threat models where attackers can anticipate and adapt to preprocessing transformations