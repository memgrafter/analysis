---
ver: rpa2
title: Table Transformers for Imputing Textual Attributes
arxiv_id: '2408.02128'
source_url: https://arxiv.org/abs/2408.02128
tags:
- columns
- data
- imputation
- ttita
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TTITA, a transformer-based approach for imputing
  missing unstructured text in tabular datasets. It encodes heterogeneous input columns
  into a context vector, which the transformer decoder uses for cross-attention to
  predict missing text.
---

# Table Transformers for Imputing Textual Attributes

## Quick Facts
- arXiv ID: 2408.02128
- Source URL: https://arxiv.org/abs/2408.02128
- Reference count: 10
- Key outcome: TTITA outperforms baseline models like RNNs, Llama2, and Mistral on Amazon Reviews datasets, especially for longer sequences, with multi-task learning providing modest additional gains.

## Executive Summary
This paper introduces TTITA, a transformer-based approach for imputing missing unstructured text in tabular datasets. The model encodes heterogeneous input columns (numeric, categorical, and textual) into a context vector, which a transformer decoder uses for cross-attention to predict missing text sequences. Extensive experiments on Amazon Reviews datasets demonstrate that TTITA significantly outperforms baseline models including RNNs and large language models, particularly for longer sequences. The approach is efficient, requires minimal preprocessing, and achieves strong results both quantitatively and qualitatively.

## Method Summary
TTITA encodes heterogeneous tabular inputs into a context vector using feature extractors appropriate for each column type: fully connected layers for numeric features, embeddings for categorical features, and a hashing vectorizer for textual features. This context vector serves as the key and value in cross-attention for a transformer decoder, which generates the missing text sequence auto-regressively. The model can be trained with multi-task learning to simultaneously impute multiple columns, enriching the context vector representation. The framework is trained end-to-end using standard NLP metrics like METEOR, ROUGE, and BLEU for evaluation.

## Key Results
- TTITA outperforms RNNs, Llama2, and Mistral on Amazon Reviews datasets across METEOR, ROUGE, and BLEU metrics
- Multi-task learning provides modest improvements (+0.39% METEOR, +0.14% BLEU) on gift cards dataset
- Model shows particular strength on longer text sequences compared to baselines
- TTITA achieves 10.39 METEOR, 22.65 ROUGE-1, and 21.35 ROUGE-2 scores on the software dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTITA leverages cross-attention between encoded tabular context and target text tokens to selectively retrieve relevant information for imputation.
- Mechanism: The context vector (concatenated numeric, categorical, and featurized textual embeddings) is used as the key/value in cross-attention. Each generated token in the target text attends over the entire context, enabling the model to focus on informative input features when predicting the next token.
- Core assumption: The encoded context vector retains discriminative information from heterogeneous input columns that correlates with missing text tokens.
- Evidence anchors:
  - [abstract] "encodes heterogeneous input columns into a context vector, which the transformer decoder uses for cross-attention to predict missing text."
  - [section 3.1] "This vector provides background information on the target column, and the entire pipeline is learned in an end-to-end fashion."
  - [corpus] Weak. No direct corpus citation, but cross-attention in transformer decoders is standard mechanism in NLP literature.
- Break condition: If featurization loses semantic meaning (e.g., hashing vectorizer too sparse), cross-attention cannot extract useful signal, degrading imputation quality.

### Mechanism 2
- Claim: Multi-task learning improves text imputation by enriching the context vector with representations learned from imputing other heterogeneous columns.
- Mechanism: TTITA-MTL jointly optimizes losses for multiple target columns (numeric, categorical, and textual). The shared encoder produces a context vector that benefits from auxiliary tasks, making it more informative for the primary text imputation task.
- Core assumption: Imputing related columns in the same table provides complementary information that improves the primary task's context vector.
- Evidence anchors:
  - [abstract] "We further conduct experiments on real-world datasets to demonstrate the effectiveness of our model."
  - [section 3.3] "This not only enables the imputation of multiple columns at once, but also refines the context vector, providing a more comprehensive context for text imputation."
  - [section 5.1] "With multi-task learning, TTITA-MTL achieved a slight improvement in METEOR (+0.39%) and BLEU (+0.14%) scores on the gift cards dataset."
- Break condition: If auxiliary tasks are unrelated or noisy, joint training could corrupt the context vector, harming the primary text imputation performance.

### Mechanism 3
- Claim: TTITA's featurization strategy avoids sequence length explosion by converting variable-length text to fixed-dimension embeddings before transformer decoding.
- Mechanism: Textual input columns are hashed into a fixed-dimension vector (dt) using scikit-learn's hashing vectorizer, preventing long input sequences that would arise from concatenating raw text tokens. This keeps the transformer decoder's input size manageable regardless of text length.
- Core assumption: Hashing vectorizer preserves enough discriminative signal from input text for cross-attention to retrieve relevant context.
- Evidence anchors:
  - [abstract] "the use of featurizers and embeddings enables flexible adjustment on the model size, making this framework adoptable for wide usage."
  - [section 3.1] "textual input columns are featurized by the hashing vectorizer from the scikit-learn package in a dimension of dt. This corresponds to the size of the input layer for textual columns and does not require any pre-processing of textual data."
  - [corpus] Weak. No corpus citation, but hashing trick is common in large-scale text classification literature.
- Break condition: If hashing causes severe collision, semantic information is lost, making the context vector uninformative for text generation.

## Foundational Learning

- Concept: Transformer cross-attention mechanism
  - Why needed here: Enables selective information retrieval from heterogeneous input features when generating each token in the missing text sequence.
  - Quick check question: How does the cross-attention query relate to the context vector in TTITA's decoder?

- Concept: Multi-task learning benefits
  - Why needed here: Improves the shared context representation by forcing the encoder to capture information useful for multiple related imputation tasks.
  - Quick check question: What happens to the context vector when training TTITA-MTL versus single-task TTITA?

- Concept: Text featurization vs. sequence encoding
  - Why needed here: Allows TTITA to handle variable-length input text without exploding sequence length, making the transformer decoder efficient.
  - Quick check question: Why does TTITA use hashing vectorizer instead of tokenizing input text into a sequence?

## Architecture Onboarding

- Component map:
  - Input featurizers: Numeric (FC), Categorical (Embedding+FC), Textual (Hashing vectorizer)
  - Context vector: Concatenated latent features from all inputs
  - Decoder: Transformer decoder with cross-attention to context vector
  - Multi-task branches: Separate output heads for each target column with appropriate loss functions
  - Tokenizer: Basic English tokenizer from PyTorch (20k vocab cap)

- Critical path:
  1. Featurize heterogeneous inputs â†’ Context vector
  2. Initialize decoder with context vector as key/value
  3. Generate target text tokens auto-regressively using cross-attention
  4. Compute losses (single or multi-task) and backpropagate

- Design tradeoffs:
  - Hashing vectorizer vs. learned embeddings: Hashing is fast and fixed-size but lossy; learned embeddings are more expressive but increase model size and require pre-processing
  - Fixed vs. variable context vector size: Fixed simplifies architecture but may limit expressiveness for complex tables
  - Multi-task vs. single-task: Multi-task can improve context but doubles inference time and may introduce interference

- Failure signatures:
  - Low METEOR/ROUGE scores across datasets: Likely featurization losing too much information or context vector not capturing relevant signal
  - Multi-task learning degrades performance: Auxiliary tasks may be unrelated or noisy, corrupting shared representation
  - Model fails on longer sequences: Hashing vectorizer too sparse or attention mechanism cannot handle longer context

- First 3 experiments:
  1. Ablation: Train decoder-only (no context vector) on target column to verify context vector's importance
  2. Hyperparameter sweep: Vary hashing vectorizer dimension (dt) to find optimal balance between expressiveness and efficiency
  3. Multi-task analysis: Train TTITA-MTL with different combinations of auxiliary tasks to identify most beneficial task combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TTITA's performance compare to large language models (LLMs) on datasets with complex, long-range dependencies in the target text?
- Basis in paper: [explicit] The paper mentions that TTITA outperforms LLMs like Llama2 and Mistral in most scenarios, but also notes that LLMs can perform better when the dataset is not large enough for TTITA to generalize well.
- Why unresolved: The paper does not provide a detailed comparison of TTITA's performance on datasets with complex, long-range dependencies in the target text. This could be an important factor in determining the model's applicability to various real-world scenarios.
- What evidence would resolve it: A comprehensive study comparing TTITA's performance to LLMs on datasets with varying levels of complexity and long-range dependencies in the target text, using metrics such as BLEU, ROUGE, and METEOR scores.

### Open Question 2
- Question: How does the choice of featurization method for textual input columns impact TTITA's performance?
- Basis in paper: [inferred] The paper uses a hashing vectorizer for featurizing textual input columns, but does not explore other featurization methods such as word embeddings or transformer-based embeddings.
- Why unresolved: The choice of featurization method can significantly impact the model's ability to capture relevant information from the input text, which in turn affects the quality of the imputed output.
- What evidence would resolve it: An empirical study comparing TTITA's performance using different featurization methods for textual input columns, such as hashing vectorizer, word embeddings, and transformer-based embeddings, on various datasets.

### Open Question 3
- Question: How does TTITA handle missing values in the target column during inference, and how does this affect the quality of the imputed output?
- Basis in paper: [inferred] The paper mentions that missing values in the input columns are replaced with default values during training, but does not discuss how missing values in the target column are handled during inference.
- Why unresolved: Handling missing values in the target column during inference is crucial for the model's practical applicability, as it is common to encounter incomplete data in real-world scenarios.
- What evidence would resolve it: A study investigating TTITA's performance when dealing with missing values in the target column during inference, using different strategies such as mean imputation, mode imputation, or more advanced techniques, and evaluating the quality of the imputed output using appropriate metrics.

## Limitations

- Limited to product review datasets (Amazon software and gift cards), limiting generalizability to other tabular domains
- Hashing vectorizer may lose semantic information through collisions, but no ablation study provided to quantify this impact
- Multi-task learning improvements are modest (+0.39% METEOR, +0.14% BLEU), suggesting benefits may be dataset-specific

## Confidence

- **High confidence**: Transformer cross-attention mechanism works as described for combining heterogeneous context with text generation
- **Medium confidence**: Hashing vectorizer is sufficient for preserving discriminative signal for text imputation
- **Medium confidence**: Multi-task learning provides consistent benefits across datasets
- **Low confidence**: TTITA generalizes well to arbitrary tabular text imputation tasks beyond product reviews

## Next Checks

1. **Featurization ablation study**: Train TTITA with alternative text featurization methods (TF-IDF, learned embeddings) on the same datasets to quantify information loss from hashing vectorizer and identify optimal approach for different text lengths/types.

2. **Cross-dataset generalization test**: Evaluate TTITA on non-review tabular datasets (e.g., healthcare, finance, or scientific tables with missing text attributes) to assess whether performance gains transfer beyond product metadata.

3. **Semantic coherence validation**: Conduct human evaluation of imputed text quality beyond automated metrics, checking whether generated text is not only fluent but contextually appropriate and semantically meaningful within the table's domain.