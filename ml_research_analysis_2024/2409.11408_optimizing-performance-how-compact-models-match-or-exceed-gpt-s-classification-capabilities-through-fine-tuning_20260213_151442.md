---
ver: rpa2
title: 'Optimizing Performance: How Compact Models Match or Exceed GPT''s Classification
  Capabilities through Fine-Tuning'
arxiv_id: '2409.11408'
source_url: https://arxiv.org/abs/2409.11408
tags:
- financial
- sentiment
- news
- market
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a market-based dataset and compares compact
  models like FinBERT and FinDRoBERTa with GPT-4 in sentiment classification of financial
  news. Using a systematic labeling approach based on actual market reactions, they
  create a dataset of over 50,000 financial headlines annotated without human bias.
---

# Optimizing Performance: How Compact Models Match or Exceed GPT's Classification Capabilities through Fine-Tuning

## Quick Facts
- arXiv ID: 2409.11408
- Source URL: https://arxiv.org/abs/2409.11408
- Authors: Baptiste Lefort; Eric Benhamou; Jean-Jacques Ohana; David Saltiel; Beatrice Guez
- Reference count: 37
- Primary result: Fine-tuned compact models match or exceed GPT-4 in financial sentiment classification

## Executive Summary
This paper challenges the assumption that larger language models are inherently superior for classification tasks by demonstrating that fine-tuned compact models can match or exceed GPT-4's performance in financial news sentiment analysis. The researchers created a market-based dataset of over 50,000 financial headlines labeled through systematic analysis of actual market reactions rather than human judgment. Using this dataset, they fine-tuned FinBERT and FinDRoBERTa models, achieving comparable F-scores to GPT-4 while training in significantly less time (10-12 hours vs 24 hours). The study also reveals that ensemble methods like bagging fail to improve performance, suggesting behavioral similarities among different models regardless of parameter size.

## Method Summary
The researchers developed a market-based dataset using Bloomberg Market Wraps from 2010-2024, creating over 50,000 financial headlines with associated market-based sentiment labels. For each headline, they identified stock tickers mentioned and calculated the next trading day's price change. Sentiment labels (+1 for positive, -1 for negative, 0 for neutral) were assigned based on statistical quantiles (above 60th percentile, below 30th percentile, or in between). The dataset was split into 70% training and 30% evaluation sets. They fine-tuned FinBERT and FinDRoBERTa models using supervised fine-tuning, comparing their performance against GPT-4 on the same task. The bagging method was also tested by combining predictions from multiple fine-tuned models.

## Key Results
- Compact models (FinBERT, FinDRoBERTa) achieve comparable F-scores to GPT-4 in 3-class financial sentiment classification
- Fine-tuned compact models train in 10-12 hours versus 24 hours for GPT-4, demonstrating efficiency advantages
- Bagging ensemble methods fail to improve performance, challenging assumptions about model independence and ensemble benefits
- Market-based labeling eliminates human bias while maintaining high classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Market-based labeling eliminates human bias in sentiment classification.
- Mechanism: Instead of human annotators, stock price movements following news publication are used to assign sentiment labels (+1 for price rise, -1 for fall, 0 for neutral) based on statistical quantiles.
- Core assumption: News headlines directly cause measurable market reactions within one trading day.
- Evidence anchors:
  - [section] "To the best of our knowledge, sentiment classification models associated with financial news are trained and evaluated on datasets annotated by humans [18]... However, human assessment of a market reaction can vary considerably."
  - [section] "For each stock ticker, denoted as Tk, the algorithm operates... C(Tk, ∆PTk ) = { +1 if ∆PTk > Q 0.6,Tk, −1 if ∆PTk < Q 0.3,Tk, 0 otherwise }"
  - [corpus] No direct evidence found in corpus about market-based labeling; corpus focuses on model comparisons.
- Break condition: If news impact is delayed, confounded by other factors, or markets are efficient enough that individual headlines don't move prices measurably.

### Mechanism 2
- Claim: Fine-tuned compact models match or exceed GPT-4 performance in financial sentiment classification.
- Mechanism: FinBERT and FinDRoBERTa, when trained on the market-based dataset, achieve comparable or superior F-scores to GPT-4 despite having far fewer parameters.
- Core assumption: Domain-specific pretraining (financial text) and proper fine-tuning are more important than raw model size for this task.
- Evidence anchors:
  - [section] "We observed comparable results, which led us to reconsider the size advantage of the GPT-3.5/4 model over more compact models."
  - [section] "Table 4... Model Train. Time (hrs) Precision Recall F-score: SFT GPT 24 0.54 0.53 0.51, SFT DistilRoBERTa 10 0.53 0.51 0.49, SFT FinBERT 12 0.54 0.52 0.50"
  - [corpus] Corpus shows related work comparing BERT vs GPT for financial engineering, finding fine-tuned BERT models outperform vanilla GPT models on sentiment tasks.
- Break condition: If the task requires broader reasoning or zero-shot generalization beyond the domain of financial news headlines.

### Mechanism 3
- Claim: Condorcet's Jury Theorem assumptions fail because models are not independent.
- Mechanism: Bagging ensemble of fine-tuned models does not improve performance over individual models, suggesting they make similar classification errors.
- Core assumption: Model independence is required for Condorcet's theorem to hold; observed lack of improvement implies behavioral similarity.
- Evidence anchors:
  - [section] "the bagging method did not surpass the performance of the best individual models, thereby challenging the independence hypothesis of the Condorcet's theorem."
  - [section] "Table 5... Model Precision Recall F-score: SFT GPT + SFT DistilRoBERTa + SFT FinBERT 0.55 0.53 0.51 (no improvement over individual models)"
  - [corpus] No corpus evidence directly about Condorcet's theorem application to model independence.
- Break condition: If ensemble methods or different model combinations do show improvement, or if independence assumption is relaxed.

## Foundational Learning

- Concept: Quantile-based classification
  - Why needed here: To objectively convert market returns into sentiment labels without human judgment
  - Quick check question: If a stock moves up 2% and the 60th percentile threshold is 1.5%, what sentiment label is assigned?

- Concept: Fine-tuning vs zero-shot learning
  - Why needed here: To understand why compact models can outperform larger models when properly adapted to the domain
  - Quick check question: What's the key difference between a model that's been fine-tuned on financial news and one using zero-shot prompting for the same task?

- Concept: Condorcet's Jury Theorem
  - Why needed here: To understand the theoretical basis for why ensemble methods should work and why their failure is significant
  - Quick check question: What are the two key assumptions of Condorcet's Jury Theorem that must hold for ensemble methods to improve performance?

## Architecture Onboarding

- Component map: Bloomberg news collection -> headline extraction -> ticker identification -> market-based labeling -> dataset creation -> model fine-tuning -> performance evaluation -> ensemble testing
- Critical path: News collection -> automated labeling -> model fine-tuning -> performance evaluation -> ensemble testing
- Design tradeoffs:
  - Human bias elimination vs. potential noise in market-based labels
  - Model size vs. training efficiency (GPT takes 24 hours vs. 10-12 for compact models)
  - Single model performance vs. ensemble complexity (bagging doesn't help)
- Failure signatures:
  - If bagging ensemble improves performance, independence assumption may be wrong
  - If compact models underperform GPT, fine-tuning may be insufficient
  - If market-based labels don't correlate with actual sentiment, labeling method is flawed
- First 3 experiments:
  1. Train a simple logistic regression on market-based labels to establish baseline performance
  2. Fine-tune FinBERT and FinDRoBERTa with varying learning rates to find optimal configuration
  3. Create bagging ensembles with different model combinations to test independence hypothesis systematically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the bagging method fail to improve performance despite meeting Condorcet's theorem conditions?
- Basis in paper: [explicit] The paper states that while individual models outperform random guessing, the bagging method did not lead to improved performance, suggesting that the independence hypothesis of Condorcet's theorem is not fulfilled.
- Why unresolved: The paper does not provide a detailed explanation of why the models exhibit behavioral similarities that violate the independence assumption, despite having different architectures and parameter sizes.
- What evidence would resolve it: Empirical studies showing the correlation between model predictions or a detailed analysis of the decision-making processes of different models to identify common patterns.

### Open Question 2
- Question: How does the market-based dataset compare to human-annotated datasets in terms of real-world applicability?
- Basis in paper: [explicit] The paper introduces a market-based dataset that eliminates human bias by using systematic labeling based on actual market reactions, contrasting with traditional human-annotated datasets.
- Why unresolved: The paper does not provide a direct comparison of the effectiveness of the market-based dataset versus human-annotated datasets in real-world financial applications.
- What evidence would resolve it: Comparative studies using both types of datasets to evaluate model performance in real-world financial sentiment analysis tasks.

### Open Question 3
- Question: Do compact models maintain their performance advantage over larger models in other NLP tasks beyond financial sentiment analysis?
- Basis in paper: [explicit] The paper demonstrates that compact models like FinBERT and FinDRoBERTa can match or exceed GPT models in financial sentiment analysis, but it does not explore other domains.
- Why unresolved: The study is limited to financial sentiment analysis, and the generalizability of these findings to other NLP tasks is not addressed.
- What evidence would resolve it: Experimental results from applying compact models to various NLP tasks and comparing their performance with larger models across different domains.

## Limitations

- The market-based labeling approach assumes news headlines directly cause measurable market reactions within one trading day, which may not hold in efficient markets
- The study is limited to financial sentiment analysis and does not explore whether compact model advantages extend to other NLP domains
- The failure of bagging ensembles to improve performance raises questions about model independence that are not fully explored or explained

## Confidence

- **High Confidence**: The finding that compact models can match or exceed GPT-4 performance in this specific task is well-supported by the experimental results showing comparable F-scores across multiple compact models
- **Medium Confidence**: The claim about eliminating human bias through market-based labeling is supported by the methodology but depends on the validity of the market reaction assumption
- **Low Confidence**: The interpretation of bagging ensemble failure as evidence of model dependence requires further investigation, as the null result could stem from other factors such as ensemble design or task-specific characteristics

## Next Checks

1. Test the market-based labeling method by comparing its labels with human annotations on a subset of headlines to quantify agreement and identify systematic differences
2. Experiment with alternative ensemble methods (stacking, weighted voting) and different model combinations to determine if the lack of improvement is specific to bagging or indicative of broader model similarities
3. Evaluate model performance on delayed market reactions (e.g., 2-3 day windows) to assess whether the one-day assumption is optimal or whether longer-term market responses provide better labels