---
ver: rpa2
title: Reinforcement Learning via Auxiliary Task Distillation
arxiv_id: '2406.17168'
source_url: https://arxiv.org/abs/2406.17168
tags:
- task
- auxiliary
- tasks
- object
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AuxDistill, a method for tackling long-horizon
  robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill
  learns a single policy using multi-task RL with auxiliary tasks that are easier
  to learn and relevant to the main task.
---

# Reinforcement Learning via Auxiliary Task Distillation

## Quick Facts
- arXiv ID: 2406.17168
- Source URL: https://arxiv.org/abs/2406.17168
- Authors: Abhinav Narayan Harish; Larry Heck; Josiah P. Hanna; Zsolt Kira; Andrew Szot
- Reference count: 40
- Primary result: 2.3x higher success rate than previous state-of-the-art on Habitat Object Rearrangement benchmark

## Executive Summary
This paper introduces AuxDistill, a method for learning long-horizon robot control policies by distilling behaviors from easier auxiliary RL tasks. The approach trains a single policy concurrently on both main and auxiliary tasks, using a weighted KL-divergence distillation loss to transfer knowledge from auxiliary tasks to the main task. The method is evaluated on the challenging Habitat Object Rearrangement benchmark, where it significantly outperforms previous state-of-the-art methods, including those using pre-trained skills and expert demonstrations.

## Method Summary
AuxDistill addresses long-horizon robot control by training a single monolithic policy across multiple tasks simultaneously. The policy architecture consists of a ResNet50 visual encoder, 2-layer LSTM, and task-specific linear heads. During training, the agent collects experience from both main and auxiliary tasks (Pick, Place, Open Fridge, Open Cabinet, Pick from Fridge), computing both standard RL losses and a weighted KL-divergence distillation loss. The distillation loss transfers behaviors from auxiliary tasks to the main task based on a task relevance function derived from oracle task plans. The method uses PPO optimization with PopArt normalization to handle varying reward scales across tasks.

## Key Results
- Achieves 2.3x higher success rate than previous state-of-the-art on Habitat Object Rearrangement benchmark
- Outperforms methods using pre-trained skills and expert demonstrations
- Ablation studies show distillation loss is necessary for success on the main task

## Why This Works (Mechanism)

### Mechanism 1
AuxDistill transfers behaviors from easier auxiliary tasks to solve the main task through a distillation loss. The policy is trained concurrently on main and auxiliary tasks, with the distillation loss encouraging the main-task policy to mimic auxiliary-task policies in relevant states. This works because auxiliary tasks are easier to learn and contain sub-behaviors useful for the full task. The mechanism breaks if auxiliary tasks are not easier or relevant to the main task.

### Mechanism 2
The distillation loss provides dense per-time-step supervision that overcomes sparse main-task rewards. Since the distillation loss is computed at every time step and weighted by task relevance, it provides a continuous training signal even when the main-task reward is sparse or delayed. This benefit diminishes if the main task already has dense rewards.

### Mechanism 3
Learning all tasks concurrently avoids the compounding errors of hierarchical policies. A single monolithic policy is trained end-to-end for all tasks, eliminating the need to sequence independently trained skills. This approach is superior to hierarchical methods that suffer from error accumulation when chaining skills together.

## Foundational Learning

- **Concept**: Multi-task Reinforcement Learning
  - Why needed here: AuxDistill trains a single policy across multiple tasks simultaneously, requiring the ability to balance and learn from multiple reward signals.
  - Quick check question: What is the main challenge when training a single policy on multiple tasks with different reward scales?

- **Concept**: Knowledge Distillation
  - Why needed here: The distillation loss transfers knowledge from auxiliary-task policies to the main-task policy, requiring understanding of KL-divergence and behavior matching.
  - Quick check question: In the context of RL, what does a KL-divergence loss between two policies encourage?

- **Concept**: Task Relevance Functions
  - Why needed here: The distillation loss is weighted by task relevance, which determines when auxiliary-task knowledge should be transferred to the main task.
  - Quick check question: How can task relevance be determined without access to privileged state information?

## Architecture Onboarding

- **Component map**: Depth camera → ResNet50 visual encoder → LSTM → Linear task heads → Auxiliary task environments → Distillation loss module → PPO optimizer
- **Critical path**: Collect experience from all tasks → Compute distillation and RL losses → Update policy → Repeat
- **Design tradeoffs**: Single monolithic policy vs. hierarchical skills; concurrent learning vs. curriculum; distillation strength vs. main task optimization
- **Failure signatures**: No learning on main task (check distillation loss contribution); poor auxiliary task performance (check task design); unstable training (check PopArt normalization)
- **First 3 experiments**:
  1. Train without distillation loss to verify it is necessary for success
  2. Train with only the main task to compare against AuxDistill performance
  3. Vary the distillation coefficient to find the optimal balance between distillation and main task reward optimization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations identified:

- **Open Question 1**: What are the long-term generalization capabilities of AuxDistill when applied to tasks beyond the evaluated rearrangement and category pick tasks?
- **Open Question 2**: How does AuxDistill perform in environments with partial observability or noisy sensor data?
- **Open Question 3**: What is the computational overhead of AuxDistill compared to other RL methods, and how does it scale with the number of auxiliary tasks?

## Limitations
- Evaluation limited to single benchmark (Habitat Object Rearrangement), limiting generalizability
- Oracle task plan requirement for computing task relevance functions raises scalability concerns
- Does not address computational overhead from training multiple auxiliary tasks concurrently

## Confidence
- **High confidence**: The auxiliary task distillation mechanism works as described, given ablation results showing the distillation loss is necessary for success
- **Medium confidence**: The claim that AuxDistill outperforms methods using pre-trained skills and expert demonstrations, as this comparison relies on results from other papers
- **Low confidence**: The general applicability of AuxDistill to other long-horizon tasks beyond the specific Habitat benchmark

## Next Checks
1. **Ablation study on distillation weight**: Systematically vary the distillation coefficient λ across a wider range (0.01, 0.1, 1.0, 10.0) to verify the optimal balance between distillation and main task optimization
2. **Comparison with alternative distillation methods**: Replace the KL-divergence distillation loss with other knowledge transfer approaches (e.g., behavior cloning from auxiliary task policies, feature-level distillation) to isolate the specific contribution of the proposed distillation mechanism
3. **Scalability test without oracle plans**: Implement a heuristic or learned task relevance function that does not require oracle task plans, and evaluate whether AuxDistill maintains its performance advantage when this privileged information is unavailable