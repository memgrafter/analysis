---
ver: rpa2
title: 'LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion
  Framework and Role-Play'
arxiv_id: '2405.06373'
source_url: https://arxiv.org/abs/2405.06373
tags:
- umbrella
- umbrellas
- discussion
- creativity
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM Discussion enhances LLM creativity by emulating human-like
  discussions with role-play. It employs a three-phase framework (initiation, discussion,
  convergence) with specialized prompts and assigns diverse roles (e.g., Environmentalist,
  Futurist) to mitigate LLM homogeneity.
---

# LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play

## Quick Facts
- arXiv ID: 2405.06373
- Source URL: https://arxiv.org/abs/2405.06373
- Reference count: 40
- Primary result: LLM Discussion framework with role-play improves creativity metrics (Originality, Elaboration) over single-LLM and existing multi-LLM methods

## Executive Summary
This paper introduces LLM Discussion, a framework that enhances LLM creativity by emulating human-like discussions with role-play. The approach addresses the homogeneity problem in multi-LLM systems by assigning diverse roles to different LLM agents and structuring their interactions through a three-phase discussion framework. Evaluated across four creativity benchmarks (Alternative Uses, Instances, Similarities, and Scientific Creativity tests), the method demonstrates significant improvements in creativity metrics compared to single-LLM approaches and existing multi-LLM methods, with particularly strong gains in Originality and Elaboration.

## Method Summary
The LLM Discussion framework employs a three-phase structure: initiation (where a problem is presented and roles assigned), discussion (where agents exchange ideas and build upon each other's contributions), and convergence (where final outputs are synthesized). The method assigns diverse roles to LLM agents (e.g., Environmentalist, Futurist) to encourage varied perspectives and creative approaches. The framework is evaluated on four creativity benchmarks, measuring both automated (using GPT-3.5) and human assessments of creativity through metrics including Originality, Elaboration, Fluency, and Flexibility.

## Key Results
- Outperforms single-LLM and existing multi-LLM methods on Alternative Uses, Instances, Similarities, and Scientific Creativity benchmarks
- Shows significant improvements in Originality and Elaboration metrics
- Demonstrates enhanced collaborative dynamics and role-specific responses in discussions
- Validates effectiveness through both LLM and human evaluator assessments

## Why This Works (Mechanism)
The framework works by addressing the fundamental limitation of LLMs: their tendency toward homogeneity in responses. By introducing structured discussion phases and diverse role assignments, the method forces LLMs to explore different perspectives and build upon each other's ideas, mimicking human creative collaboration. The three-phase structure provides scaffolding for the discussion while role-play ensures diverse viewpoints are represented, leading to more original and elaborated outputs than either individual LLMs or unstructured multi-LLM systems can produce.

## Foundational Learning
- **Role-based diversity**: Assigning different personas to LLM agents creates varied perspectives, essential for generating diverse creative outputs
- **Structured discussion phases**: The three-phase framework (initiation, discussion, convergence) provides scaffolding that guides collaborative creativity
- **Automated creativity evaluation**: Using LLMs to assess creativity requires standardized prompts and metrics to ensure consistent evaluation
- **Multi-modal evaluation**: Combining LLM and human assessments provides more robust validation of creative outputs
- **Collaborative generation**: Multiple agents building on each other's ideas can produce outputs exceeding individual capabilities

## Architecture Onboarding

### Component Map
Problem Presentation -> Role Assignment -> Initiation Phase -> Discussion Phase -> Convergence Phase -> Evaluation (LLM + Human)

### Critical Path
Role Assignment → Initiation → Discussion → Convergence → Evaluation

### Design Tradeoffs
- **Role diversity vs. coherence**: More diverse roles may generate more creative outputs but could reduce discussion coherence
- **Discussion length vs. efficiency**: More discussion rounds improve output quality but increase computational cost
- **Automated vs. human evaluation**: LLM evaluation is faster but human evaluation provides more nuanced assessment

### Failure Signatures
- Homogeneous outputs despite role assignment: Roles may be too similar or not properly integrated into the discussion framework
- Discussions that don't converge: Insufficient convergence prompts or too many discussion rounds
- Low originality scores: Roles may lack diversity or discussion phases may not be properly implemented

### First Experiments
1. Implement single-phase discussion without roles to establish baseline performance
2. Add role assignment to single-phase discussion and measure impact on creativity metrics
3. Implement full three-phase framework with roles and compare against baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Specific prompt templates for the three phases are not provided, making faithful reproduction challenging
- Human evaluation methodology lacks detailed implementation specifications
- Role assignment process details are underspecified beyond general examples
- Computational costs of multi-LLM discussions not fully characterized

## Confidence
- Framework effectiveness: Medium - Strong quantitative results but dependent on unknown implementation details
- Role-play contribution: Medium - Supported by results but role assignment specifics are underspecified
- Human evaluation validity: Low - Methodology described but lacks implementation details

## Next Checks
1. Implement the three-phase framework using placeholder prompts based on descriptions, then systematically refine them while measuring impact on creativity metrics
2. Generate a comprehensive role set using the described GPT-4 process, then test different role diversity levels to identify optimal configurations
3. Conduct a controlled reproduction using both the reported evaluation prompts and an independently developed rubric to assess robustness of LLM evaluation results