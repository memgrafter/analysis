---
ver: rpa2
title: Exploring the Benefit of Activation Sparsity in Pre-training
arxiv_id: '2410.03440'
source_url: https://arxiv.org/abs/2410.03440
tags:
- training
- activation
- dense
- smoe
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies sparse activation in transformer pre-training
  and proposes Switchable Sparse-Dense Learning (SSD) to improve efficiency. SSD adaptively
  switches between dense and sparse training based on activation pattern stability,
  using MoE-based sparse training when patterns stabilize.
---

# Exploring the Benefit of Activation Sparsity in Pre-training

## Quick Facts
- arXiv ID: 2410.03440
- Source URL: https://arxiv.org/abs/2410.03440
- Reference count: 34
- Key outcome: SSD achieves up to 1.44× training speedup and 2× inference speedup while maintaining dense model performance across GPT, BERT, and T5 models.

## Executive Summary
This paper explores how transformer models naturally develop sparse activation patterns during pre-training and proposes Switchable Sparse-Dense Learning (SSD) to exploit this phenomenon for improved efficiency. The key insight is that transformers become sparsely activated early in training, with most neurons inactive per token, and this sparsity stabilizes over time. SSD adaptively switches between dense and sparse (MoE) training modes based on activation pattern stability, achieving comparable performance to dense training with significantly reduced computational cost. The method also enables faster inference through dynamic expert selection while maintaining dense model accuracy.

## Method Summary
SSD works by monitoring activation patterns during pre-training and switching between dense and sparse training modes when sparsity stabilizes. During sparse phases, the dense model is converted to a mixture-of-experts (MoE) architecture where only top-K experts are activated per token. The system periodically reverts to dense training to prevent representation collapse. For inference, SSD models can use dynamic top-k expert selection, varying the number of experts per token based on importance scores to further improve efficiency.

## Key Results
- SSD achieves up to 1.44× speedup in FLOPs during training compared to dense training
- SSD models can be used for sparse inference, reducing inference time by up to 2×
- The method maintains performance comparable to dense models across GPT, BERT, and T5 architectures on both language modeling and downstream tasks
- Activation sparsity emerges early in training, with over 90% of neurons inactive per token after initial stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers naturally become sparsely activated during pre-training, with over 90% of neurons inactive per token after early training stages.
- Mechanism: Activation sparsity emerges due to initialization symmetry and optimization dynamics, where gradients drive most neurons to zero activation for each token, reducing effective computation.
- Core assumption: The sparse activation is a stable emergent property of training dynamics, not data-dependent.
- Evidence anchors:
  - [abstract] "Pre-trained Transformers inherently possess the characteristic of sparse activation, where only a small fraction of the neurons are activated for each token."
  - [section] "Our findings reveal that these models become sparsely activated in the early stage of pre-training, subsequently stabilizing in this sparse state."
  - [corpus] Weak: only 25 related papers, no direct citation support for sparsity stability claim.
- Break condition: If sparsity does not stabilize or activation patterns keep evolving rapidly, SSD switching strategy fails.

### Mechanism 2
- Claim: Sparse-to-dense switching prevents representation collapse in SMoE models and maintains full model capacity.
- Mechanism: Periodically reverting to dense training allows all parameters to be updated, counteracting expert specialization that leads to redundant representations and capacity underutilization.
- Core assumption: Representation collapse is the primary cause of SMoE performance degradation compared to dense models.
- Evidence anchors:
  - [abstract] "the performance of models pre-trained with the SMoE technique frequently lags behind that of their dense counterparts... due to a phenomenon known as representation collapse"
  - [section] "We strategically revert to dense training multiple times during training... effectively mitigating the representation collapse issue"
  - [corpus] Weak: no corpus evidence for representation collapse being primary cause.
- Break condition: If representation collapse is not the main issue, switching provides no benefit.

### Mechanism 3
- Claim: Dynamic top-k expert selection during inference can further improve efficiency by varying the number of experts per token.
- Mechanism: Different tokens have different importance levels, so allocating more experts to important tokens and fewer to less important ones optimizes the compute-performance tradeoff.
- Core assumption: Token importance correlates with the need for more expert computation, and this can be estimated from gating scores.
- Evidence anchors:
  - [section] "We further investigate whether we can vary the number of selected experts for each token... some important tokens may need more experts to compute, and some unimportant tokens may need fewer experts to compute."
  - [section] "truncating 75% of the experts can consistently achieve better performance than that of using a fixed number of experts for each token"
  - [corpus] Missing: no corpus evidence for this specific approach.
- Break condition: If token importance cannot be reliably estimated or gating scores don't correlate with importance, this optimization fails.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: SSD converts dense models to MoE during sparse training phases, so understanding how MoE works is fundamental to grasping SSD's mechanism.
  - Quick check question: What is the key difference between MoE and dense models in terms of parameter activation during inference?

- Concept: Activation sparsity and its measurement
  - Why needed here: SSD relies on detecting when activation sparsity stabilizes to trigger mode switching, requiring understanding of how sparsity is defined and measured.
  - Quick check question: How is activation sparsity typically quantified in transformer models?

- Concept: Representation collapse in sparse models
  - Why needed here: SSD addresses this issue through periodic dense training, so understanding what representation collapse is and why it occurs is crucial for understanding SSD's design choices.
  - Quick check question: What is representation collapse and why does it commonly occur in sparse mixture-of-experts models?

## Architecture Onboarding

- Component map:
  - Dense training phase -> Sparse training phase -> Transition monitor -> Router network -> Expert groups

- Critical path: Monitor activation pattern → Determine transition timing → Convert between dense and sparse modes → Train with selected mode → Evaluate performance

- Design tradeoffs:
  - More frequent switching: Better adaptation to activation changes but higher overhead
  - Longer sparse phases: More computational savings but higher risk of representation collapse
  - More experts: Better performance but higher computational cost and routing complexity
  - Larger expert selection: Better approximation of dense model but reduced efficiency gains

- Failure signatures:
  - Performance degradation: Too frequent switching or insufficient dense training
  - No speedup: Incorrect sparsity detection or inefficient MoE implementation
  - Unstable training: Poor transition implementation or initialization issues
  - Representation collapse: Insufficient dense training phases or poor expert diversity

- First 3 experiments:
  1. Baseline: Train dense model and measure activation sparsity evolution over training steps
  2. Transition timing: Test different thresholds for switching from dense to sparse based on activation pattern similarity
  3. Expert configuration: Experiment with different numbers of experts and selected experts to find optimal efficiency-performance balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does activation pattern stability scale with model size and architecture complexity across different pre-training tasks?
- Basis in paper: [inferred] The paper notes that activation pattern stability was only studied on models with ~100 million parameters and suggests investigating how it scales with model size.
- Why unresolved: The authors explicitly state this as an interesting direction for future work, indicating current computational constraints limited their investigation to smaller models.
- What evidence would resolve it: Empirical studies comparing activation pattern similarity across model sizes (from small to trillion-parameter models) and architectures (encoder-only, decoder-only, encoder-decoder) using the same metric (ARI) during pre-training.

### Open Question 2
- Question: What is the optimal dynamic strategy for selecting the number of experts per token during inference, and how does it vary across different token importance levels?
- Basis in paper: [explicit] The paper demonstrates that truncating 75% of experts based on importance scores achieves better performance than using a fixed number of experts, but leaves open how to optimally identify the number of experts for each token.
- Why unresolved: The paper only explores a simple truncation ratio approach and identifies this as a promising direction without providing a concrete solution.
- What evidence would resolve it: Development and validation of algorithms that dynamically determine expert selection per token based on token importance metrics, with empirical comparison to static approaches across various tasks.

### Open Question 3
- Question: How can the computational overhead of expert selection be minimized to better match theoretical speedup with actual training time reduction?
- Basis in paper: [explicit] The authors note a discrepancy between theoretical and actual time speedup, attributing it to additional time spent on the expert selection process, and identify this as an avenue for future research.
- Why unresolved: The paper acknowledges this as a limitation but does not propose specific solutions, only suggesting optimization of SMoE computation as a future direction.
- What evidence would resolve it: Implementation and benchmarking of optimized expert selection algorithms that reduce computational overhead while maintaining or improving performance, with quantitative analysis of time reduction.

## Limitations

- The evidence for activation sparsity stability is based on limited model sizes (up to 100M parameters) and lacks independent corpus validation.
- The claim that representation collapse is the primary cause of SMoE performance degradation compared to dense models lacks external validation.
- The dynamic top-k inference optimization is only tested on BERT, with no corpus evidence supporting this specific approach.

## Confidence

- High confidence: SSD achieves comparable performance to dense training with significant FLOPs reduction during training; SSD models enable faster inference with maintained performance; results are consistent across multiple model architectures (GPT, BERT, T5) and tasks.
- Medium confidence: The proposed SSD method is the primary driver of performance improvements; the mechanism of activation sparsity emergence is well understood; the specific hyperparameter choices (τ=0.9, r=0.5, l=0.1) are optimal.

## Next Checks

1. **Corpus validation**: Search for independent studies confirming that transformers naturally develop stable activation sparsity patterns during pre-training, and that representation collapse is the primary limitation of SMoE models.

2. **Generalization test**: Apply SSD to larger transformer models (e.g., GPT-3 sized) and evaluate whether the 1.44× training speedup and 2× inference speedup scale proportionally or diminish with model size.

3. **Ablation study**: Systematically remove each component of SSD (activation pattern monitoring, MoEfication, sparse-to-dense switching) to quantify their individual contributions to the overall performance gains.