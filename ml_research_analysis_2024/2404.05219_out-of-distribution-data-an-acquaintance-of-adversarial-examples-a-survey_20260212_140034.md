---
ver: rpa2
title: 'Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A Survey'
arxiv_id: '2404.05219'
source_url: https://arxiv.org/abs/2404.05219
tags:
- adversarial
- detection
- data
- training
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys the intersection of out-of-distribution (OOD)
  detection and adversarial robustness in deep neural networks (DNNs). It identifies
  two key research directions: robust OOD detection, which differentiates between
  in-distribution (ID) and OOD data even when adversarially manipulated, and unified
  robustness, which seeks a single approach to make DNNs robust against both adversarial
  attacks and OOD inputs.'
---

# Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A Survey

## Quick Facts
- arXiv ID: 2404.05219
- Source URL: https://arxiv.org/abs/2404.05219
- Reference count: 40
- Primary result: Surveys intersection of OOD detection and adversarial robustness, proposing robust OOD detection and unified robustness as key research directions

## Executive Summary
This survey explores the intersection of out-of-distribution (OOD) detection and adversarial robustness in deep neural networks. It identifies two critical research directions: robust OOD detection, which maintains performance under adversarial manipulation, and unified robustness, which seeks single approaches for both OOD and adversarial threats. The paper establishes a taxonomy based on distributional shifts and reviews existing work across outlier exposure, score-based methods, learning-based approaches, and data augmentation techniques. It highlights current limitations including lack of standardized benchmarks, limited certified defenses, and challenges in effectively modeling ID data support.

## Method Summary
The paper synthesizes existing literature on robust OOD detection and unified robustness approaches. It reviews methods including outlier exposure (ATOM, HALO), score-based techniques (Mahalanobis distance, RMDS), learning-based approaches (SSDO), and data augmentation (Mixup, RegMixup, PixMix). The survey analyzes these methods through the lens of distributional shifts and evaluates their effectiveness against both clean and adversarial OOD inputs. Key evaluation metrics include detection error, FPR95, AUROC, and AUPR across standard datasets like CIFAR-10/100 and OOD benchmarks including SVHN, LSUN, and Places365.

## Key Results
- Two key research directions identified: robust OOD detection and unified robustness
- Taxonomy established based on distributional shifts (semantic vs covariate)
- Review covers 40+ references across outlier exposure, score-based, learning-based, and augmentation methods
- Highlights need for standardized benchmarks, certified defenses, and support modeling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outlier exposure methods improve robustness by shaping decision boundaries away from OOD samples
- Mechanism: Auxiliary OOD data forces uniform predictions on OOD samples, making it harder for adversarial perturbations to cross into ID region
- Core assumption: Auxiliary outlier dataset is representative of real-world OOD scenarios
- Evidence anchors: ATOM achieves 53% reduction in FPR95 under ℓ∞ attacks; Augustín's hybrid training reduces ASR to 7.40%

### Mechanism 2
- Claim: Score-based methods using relative Mahalanobis distance are robust to adversarial perturbations
- Mechanism: RMDS makes detection threshold less sensitive to small input changes that reduce OOD detector confidence
- Core assumption: Feature space maintains Gaussian-like distributions under adversarial perturbations
- Evidence anchors: RMDS maintains higher AUROC under FGSM vs standard MDS (57% drop); Lee et al. achieve 98.73% AUROC

### Mechanism 3
- Claim: Data augmentation techniques like Mixup increase ID manifold support coverage
- Mechanism: Interpolated samples create virtual samples along ID manifold edges, thickening boundary region
- Core assumption: Interpolation preserves semantic meaning and augmented samples remain valid ID data
- Evidence anchors: RegMixup improves AUROC on OOD detection; PixMix reduces ℓ∞-PGD detection error by 3.9%

## Foundational Learning

- Concept: Distributional Shifts
  - Why needed here: Critical for distinguishing between OOD and adversarial inputs and designing appropriate detection mechanisms
  - Quick check question: What distinguishes a semantic shift from a covariate shift in the context of DNN inputs?

- Concept: Adversarial Training
  - Why needed here: Foundational defense that improves robustness to small input perturbations
  - Quick check question: How does PGD adversarial training modify the loss function compared to standard cross-entropy training?

- Concept: Outlier Exposure
  - Why needed here: Essential for implementing robust OOD detection approaches
  - Quick check question: What is the purpose of selecting "hard" outliers during training in outlier exposure-based methods?

## Architecture Onboarding

- Component map: Input sample -> feature extraction -> OOD scoring -> threshold comparison -> accept/reject -> primary classifier (if accepted)

- Critical path: 1) Input sample → feature extraction → OOD scoring 2) OOD score compared to threshold → accept (ID) or reject (OOD) 3) Accepted ID samples → primary classifier → output label 4) During training: integrate outlier exposure, adversarial training, or data augmentation

- Design tradeoffs:
  - Outlier exposure vs. support modeling: Outlier exposure needs representative data but is easier to implement; support modeling avoids data dependency but requires more complex training
  - Score-based vs. learned OOD detection: Score-based is simpler and faster but may inherit model biases; learned detectors can be more robust but require retraining
  - Unified vs. separate defenses: Unified approaches reduce system complexity but may compromise performance; separate defenses can be optimized independently but increase deployment overhead

- Failure signatures:
  - High false positive rate on clean ID data (overly aggressive OOD rejection)
  - High false negative rate on adversarial OOD (detector evasion)
  - Degraded primary classification accuracy after robustness training
  - Slow inference due to complex post-processing or ensembles

- First 3 experiments:
  1. Train ResNet18 on CIFAR-10 with outlier exposure using SVHN; evaluate FPR95 on clean and ℓ∞-PGD perturbed SVHN
  2. Implement RMDS on pre-trained DenseNet; compare AUROC under FGSM vs vanilla MDS on CIFAR-100 vs SVHN
  3. Apply RegMixup to WideResNet on CIFAR-100; evaluate AUROC on corrupted inputs and detection error on ℓ∞-PGD examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized benchmarks and evaluation strategies are needed for robust OOD detection?
- Basis in paper: [explicit] Lack of standardized benchmarks unlike OpenOOD for standard OOD detection or AutoAttack for adversarial robustness
- Why unresolved: Different attack criteria and datasets make cross-method comparison difficult
- What evidence would resolve it: Standardized benchmark with consistent evaluation metrics and diverse attack scenarios

### Open Question 2
- Question: How can we develop certifiably robust methods for OOD detection?
- Basis in paper: [explicit] Need for provable guarantees as empirical methods are vulnerable to adaptive attacks
- Why unresolved: Most research focuses on empirical robustness with limited exploration of provable methods
- What evidence would resolve it: Methods providing formal guarantees against wide range of adversarial attacks

### Open Question 3
- Question: How can we effectively model the support of ID data to improve robust OOD detection?
- Basis in paper: [inferred] Current methods relying on seen outliers have generalization limitations
- Why unresolved: Existing approaches struggle to comprehensively model ID data support
- What evidence would resolve it: Methods that effectively model ID data support using training set and limited boundary-near outliers

## Limitations
- Lack of standardized benchmarks for robust OOD detection and unified robustness evaluation
- Empirical robustness claims without certified guarantees, vulnerable to adaptive attacks
- Effectiveness of outlier exposure methods heavily depends on representativeness of auxiliary dataset

## Confidence
- High Confidence: Taxonomy of distributional shifts and categorization of approaches are well-supported
- Medium Confidence: Relative performance of specific methods based on reported results but reproducibility hampered by unreported details
- Low Confidence: Generalizability of unified robustness approaches across diverse real-world applications is asserted but not empirically validated

## Next Checks

1. Reproduce ATOM on CIFAR-10 + SVHN: Implement ATOM with exact outlier mining strategy, evaluate FPR95 on clean and ℓ∞-PGD perturbed SVHN, verify 53% reduction claim

2. RMDS vs. MDS under FGSM on CIFAR-100: Train DenseNet with Mahalanobis scoring, implement RMDS, compare AUROC under FGSM, confirm 57% drop for standard MDS

3. RegMixup on corrupted CIFAR-100: Apply RegMixup to WideResNet, evaluate AUROC on corrupted versions and detection error on ℓ∞-PGD examples, check for near Pareto-optimal performance