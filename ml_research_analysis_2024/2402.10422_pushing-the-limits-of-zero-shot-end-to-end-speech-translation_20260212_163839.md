---
ver: rpa2
title: Pushing the Limits of Zero-shot End-to-End Speech Translation
arxiv_id: '2402.10422'
source_url: https://arxiv.org/abs/2402.10422
tags:
- speech
- pages
- translation
- cited
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ZEROSWOT, a zero-shot end-to-end speech translation
  method that bridges the modality gap between speech and text representations without
  requiring paired ST data. The approach adapts a speech encoder to the representation
  space of a massively multilingual MT model using CTC compression and Optimal Transport.
---

# Pushing the Limits of Zero-shot End-to-End Speech Translation

## Quick Facts
- **arXiv ID**: 2402.10422
- **Source URL**: https://arxiv.org/abs/2402.10422
- **Reference count**: 40
- **Primary result**: ZEROSWOT achieves state-of-the-art zero-shot ST performance with BLEU scores up to 32.9, surpassing both zero-shot and supervised methods.

## Executive Summary
This work introduces ZEROSWOT, a novel zero-shot end-to-end speech translation method that bridges the modality gap between speech and text representations without requiring paired ST data. The approach adapts a speech encoder to the representation space of a massively multilingual MT model using CTC compression and Optimal Transport. Experiments show ZEROSWOT achieves state-of-the-art results in MuST-C, surpassing both previous zero-shot and supervised methods, with BLEU scores up to 32.9 on average. It also outperforms larger supervised models in CoVoST, achieving an average BLEU of 31.2, and delivers competitive results in 88 languages on FLEURS. ZEROSWOT is also more efficient than comparable cascade systems while maintaining superior or comparable translation quality.

## Method Summary
ZEROSWOT trains a speech encoder to align with the representation space of a massively multilingual MT model using only ASR and MT data. The method employs a novel CTC compression adapter that groups consecutive speech frames into subword-like chunks, reducing temporal misalignment between modalities. Optimal Transport with positional regularization aligns the compressed speech representations with text embeddings in the frozen MT model's space. The speech encoder (Wav2Vec2.0) is trained with a combination of CTC loss for ASR and Wasserstein loss for modality alignment, plus auxiliary losses at intermediate layers. At inference, the trained speech encoder replaces the MT model's embedding layer to enable zero-shot ST.

## Key Results
- ZEROSWOT achieves state-of-the-art zero-shot ST performance in MuST-C with BLEU scores up to 32.9 on average
- Outperforms larger supervised models in CoVoST, achieving an average BLEU of 31.2
- Delivers competitive results across 88 languages on FLEURS benchmark
- More parameter-efficient than comparable cascade systems while maintaining superior or comparable translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTC compression with subword tokenization reduces the modality gap by aligning speech representation lengths to text embeddings.
- Mechanism: The compression adapter first collapses consecutive CTC-labeled speech frames into character representations, then further groups them into subword chunks that mirror the NLLB model's tokenization. This directly addresses the temporal misalignment between speech and text modalities.
- Core assumption: The proposed CTC target labeling scheme (splitting on subwords, preserving unknown characters) effectively maps speech segments to subword boundaries.
- Evidence anchors:
  - [abstract]: "Using Optimal Transport, we train a speech encoder based on WAV2VEC 2.0 to produce representations akin to the embedding space of a massively multilingual MT model"
  - [section]: "Each chunk is combining information that resemble subword tokens from V... We obtain a subword-like compressed representation asw ∈ Rnsw×d"
  - [corpus]: Weak evidence - no direct experiments showing subword vs word vs character comparison for alignment quality.
- Break condition: If the CTC labeling doesn't accurately capture subword boundaries, the compression adapter will produce misaligned representations that degrade translation quality.

### Mechanism 2
- Claim: Optimal Transport with positional regularization aligns speech and text representations in semantic space without requiring paired ST data.
- Mechanism: The Wasserstein loss minimizes the transportation cost between speech and text representations while the positional regularization vectors penalize distant alignments. This creates a shared embedding space that the speech encoder can learn to match.
- Core assumption: The squared Euclidean cost function and entropy regularization produce meaningful alignments between the two modalities.
- Evidence anchors:
  - [abstract]: "Using Optimal Transport, we train a speech encoder based on WAV2VEC 2.0 to produce representations akin to the embedding space of a massively multilingual MT model"
  - [section]: "We assume two uniform probability distributions... Using a squared euclidean cost function we obtain a pairwise cost matrix"
  - [corpus]: Moderate evidence - the speech-text retrieval accuracy of 98.5% suggests strong alignment, but no comparison to other alignment methods.
- Break condition: If the OT optimization doesn't converge properly or the positional regularization is too strong/weak, the learned alignment will be suboptimal.

### Mechanism 3
- Claim: Freezing the text encoder and using only ASR/MT data enables zero-shot ST by forcing the speech encoder to adapt to the existing text representation space.
- Mechanism: The text encoder remains fixed from the NLLB model, and the speech encoder is trained only on paired speech-transcription data (ASR) and text-translation data (MT). This prevents the speech encoder from diverging from the text space.
- Core assumption: The frozen text encoder's representation space is compatible with speech representations after compression and alignment.
- Evidence anchors:
  - [abstract]: "Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model"
  - [section]: "The text encoder is a Transformer, of which the parameters are initialized from a massively multilingual MT model (NLLB et al., 2022), and kept frozen during training"
  - [corpus]: Strong evidence - the method achieves SOTA results on multiple benchmarks without any ST data.
- Break condition: If the speech and text modalities are too different for the compression/alignment to bridge, the frozen text encoder will prevent adaptation.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**
  - Why needed here: CTC provides a way to map variable-length speech sequences to discrete labels (characters/subwords) without requiring explicit alignment.
  - Quick check question: What is the main advantage of CTC over forced alignment for speech recognition?

- **Optimal Transport (Wasserstein distance)**
  - Why needed here: OT provides a principled way to measure and minimize the distance between probability distributions of different lengths, which is crucial for aligning speech and text representations.
  - Quick check question: How does the entropy regularization term in the Sinkhorn algorithm affect the stability of OT optimization?

- **Subword tokenization (SentencePiece/BPE)**
  - Why needed here: Subword tokenization reduces vocabulary size and handles morphological variations, which is important for multilingual MT and affects how the compression adapter groups speech representations.
  - Quick check question: Why is subword tokenization generally preferred over word tokenization for low-resource languages?

## Architecture Onboarding

- **Component map**: Raw speech waveforms → Acoustic Encoder → CTC → Compression Adapter → Speech Embedder → Semantic Encoder → NLLB Decoder

- **Critical path**: Speech → Acoustic Encoder → CTC → Compression Adapter → Speech Embedder → Semantic Encoder → NLLB Decoder

- **Design tradeoffs**:
  - Freezing text encoder vs. fine-tuning it jointly
  - Character-level vs. subword-level compression granularity
  - Number of auxiliary Wasserstein losses applied

- **Failure signatures**:
  - Translation quality drops significantly on languages with different phoneme-to-grapheme mappings
  - Training instability when applying Wasserstein losses to too many layers
  - Poor performance on short utterances due to over-compression

- **First 3 experiments**:
  1. Compare BLEU scores with and without the compression adapter to verify it reduces the modality gap
  2. Test different numbers of auxiliary Wasserstein losses to find the optimal balance
  3. Evaluate retrieval accuracy with cosine similarity vs. Wasserstein distance to confirm semantic alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ZEROSWOT perform in low-resource source language scenarios beyond English?
- Basis in paper: The authors explicitly acknowledge this limitation in their "Limitations" section, noting they only tested English as the source language due to space constraints, but hypothesize the method would work equally well for other source languages and suggest this as future research for low-resource scenarios.
- Why unresolved: The paper only provides results for English-to-X translation directions, leaving the performance on non-English source languages unexplored.
- What evidence would resolve it: Empirical results showing ZEROSWOT performance on multiple non-English source languages across various target languages, particularly in low-resource settings where the source language has limited ASR data.

### Open Question 2
- Question: Can ZEROSWOT retain acoustic information like prosody during translation?
- Basis in paper: The authors state in the "Limitations" section that ZEROSWOT suffers from the same limitation as cascade systems where no acoustic information is retained during training, as the speech encoder only learns to mimic MT representation space.
- Why unresolved: The paper doesn't explore methods to preserve acoustic features or test whether incorporating prosody information would improve translation quality for specific types of utterances.
- What evidence would resolve it: Experiments comparing ZEROSWOT translations with and without acoustic/prosodic features, particularly for utterances where prosody affects meaning, and measuring any improvements in translation quality.

### Open Question 3
- Question: How does ZEROSWOT compare to zero-shot ST methods when applied to spoken-only languages without written form?
- Basis in paper: The authors state in the "Limitations" section that their methodology can't be used for spoken-only languages at least in its current form due to the requirement of ASR data for training the speech encoder.
- Why unresolved: The paper doesn't propose adaptations or modifications that would enable ZEROSWOT to handle languages that exist only in spoken form without written transcription.
- What evidence would resolve it: Development and testing of modified ZEROSWOT approaches that could work with only speech data, potentially using alternative supervision signals or unsupervised methods, and evaluating performance on spoken-only languages.

## Limitations
- Performance still trails supervised ST methods by a notable margin in some benchmarks
- Dependence on a massively multilingual MT model (NLLB) raises questions about generalizability
- Computational efficiency claims are based on parameter counts rather than actual runtime measurements
- Performance on low-resource languages without sufficient ASR data remains unclear

## Confidence

**High Confidence Claims:**
- ZEROSWOT achieves state-of-the-art results in zero-shot ST settings, outperforming previous zero-shot methods by significant margins (10-15 BLEU points on average across benchmarks)
- The method successfully bridges the modality gap between speech and text representations without requiring paired ST data
- The compression adapter effectively reduces the temporal misalignment between speech and text sequences

**Medium Confidence Claims:**
- ZEROSWOT surpasses supervised ST methods in CoVoST and achieves competitive results in MUST-C
- The approach is more parameter-efficient than comparable cascade systems
- The OT-based alignment produces meaningful semantic representations (98.5% retrieval accuracy)

**Low Confidence Claims:**
- The method's performance on languages with significantly different phoneme-to-grapheme mappings
- The scalability to 100+ languages beyond the tested FLEURS benchmark
- The robustness to noisy speech input and domain shifts

## Next Checks

1. **Ablation Study on Compression Granularity**: Systematically evaluate the impact of different compression levels (character, subword, word) on alignment quality and translation performance to validate the claimed benefits of the proposed subword compression.

2. **Cross-Modal Retrieval Analysis**: Conduct detailed analysis of the speech-to-text retrieval accuracy across different language pairs and utterance lengths to understand the limits of the OT-based alignment and identify failure patterns.

3. **Resource Efficiency Benchmarking**: Measure actual wall-clock training and inference times, memory usage, and energy consumption compared to both zero-shot and supervised baselines to verify the claimed computational efficiency advantages.