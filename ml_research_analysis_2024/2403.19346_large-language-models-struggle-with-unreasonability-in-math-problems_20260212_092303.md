---
ver: rpa2
title: Large Language Models Struggle with Unreasonability in Math Problems
arxiv_id: '2403.19346'
source_url: https://arxiv.org/abs/2403.19346
tags:
- unreasonable
- problem
- star
- pieces
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) handle
  unreasonable math problems that contain logical flaws or inconsistencies. The authors
  introduce the Unreasonable Math Problems (UMP) benchmark, comprising over 1,000
  problem pairs (original and unreasonable variants) across five types of unreasonableness:
  undefined variables, illogical scenarios, incorrect assumptions, misinterpreted
  units, and inconsistent conditions.'
---

# Large Language Models Struggle with Unreasonability in Math Problems

## Quick Facts
- arXiv ID: 2403.19346
- Source URL: https://arxiv.org/abs/2403.19346
- Reference count: 40
- Large language models show poor performance detecting logical flaws in math problems, with GPT-4o scoring only 0.6 and DeepSeek-R1 scoring 0.813

## Executive Summary
This study investigates how large language models handle unreasonable math problems containing logical flaws or inconsistencies. The authors introduce the Unreasonable Math Problems (UMP) benchmark with over 1,000 problem pairs spanning five types of unreasonableness. Experiments with 19 LLMs reveal that even state-of-the-art models struggle significantly with detecting these flaws, while reasoning models tend to overthink and produce verbose responses. The findings highlight a critical gap in current LLMs' ability to recognize when mathematical problems contain inherent logical inconsistencies.

## Method Summary
The researchers created the Unreasonable Math Problems (UMP) benchmark containing over 1,000 pairs of original and unreasonable math problems across five categories of logical flaws. They evaluated 19 different LLMs including general-purpose models (GPT-4o, Claude-3), reasoning models (DeepSeek-R1, o1), and math-specialized models (MetaMath-7B, MATH). Performance was measured on both reasonableness detection accuracy and problem-solving ability. The study also tested whether prompting with critical thinking instructions and supervised fine-tuning could improve models' ability to detect unreasonableness, finding partial improvements but with trade-offs on well-posed problems.

## Key Results
- GPT-4o achieved only 0.6 absolute score on detecting unreasonableness in math problems
- DeepSeek-R1 performed best at 0.813 absolute score but produced overly verbose responses
- Reasoning models tend to overthink and repeat reasoning steps when encountering unreasonable problems
- Prompting with critical thinking instructions and supervised fine-tuning can partially improve detection ability

## Why This Works (Mechanism)
Models struggle with unreasonableness detection because they are trained to solve problems rather than evaluate problem validity. The fundamental architecture assumes input problems are well-posed, leading models to attempt solutions even when logical flaws exist. Reasoning models' tendency to generate verbose responses reflects their training to explore multiple solution paths, which becomes problematic when the problem itself is flawed. The binary detection approach doesn't capture nuanced understanding of different flaw types, limiting the ability to develop targeted improvements.

## Foundational Learning

**Logical consistency checking**: Models need to verify problem premises against known mathematical and physical constraints. Why needed: Without this, models cannot distinguish between valid and invalid problem formulations. Quick check: Can the model identify when a problem asks for the square root of a negative number in real arithmetic?

**Critical thinking frameworks**: Models require systematic approaches to evaluate problem reasonableness before attempting solutions. Why needed: Prevents wasted computation on unsolvable problems and identifies flawed assumptions. Quick check: Does the model pause to examine variable definitions and units before proceeding with calculations?

**Problem decomposition**: Breaking down complex problems into verifiable components helps identify where inconsistencies arise. Why needed: Isolates specific logical flaws rather than treating problems as monolithic. Quick check: Can the model trace dependencies between problem statements to find contradictions?

## Architecture Onboarding

**Component map**: Input problems -> Pre-processing/Reasoning module -> Solution generation -> Output. The critical path involves initial problem assessment before solution attempts.

**Critical path**: Problem reception → Reasonableness evaluation → Solution generation → Response formatting. The bottleneck occurs at reasonableness evaluation, where current models either skip this step or spend excessive time.

**Design tradeoffs**: Speed vs. thoroughness in reasonableness checking, verbosity vs. conciseness in reasoning explanations, and generalization vs. specialization across problem types. Current models favor solution attempts over validity checking.

**Failure signatures**: Models proceeding without recognizing flaws, excessive repetition in reasoning chains, inability to identify specific flaw types, and performance degradation on well-posed problems when trained for unreasonableness detection.

**First experiments**:
1. Test model performance on single-type unreasonableness problems to identify which categories are most challenging
2. Compare chain-of-thought vs. direct response approaches for reasonableness detection
3. Evaluate whether intermediate reasoning checkpoints improve detection accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on crowdworkers for problem creation, potentially introducing quality variations
- Small absolute score values suggest the task remains extremely challenging even for top models
- Binary unreasonableness detection doesn't capture nuanced understanding of different flaw types

## Confidence

**Model performance rankings**: High - The relative performance ordering across model types appears robust given the large sample size

**Effectiveness of critical thinking prompts**: Medium - Results show improvement but the trade-off with well-posed problem performance needs more systematic study

**Reasoning models' verbosity issue**: Medium - The observation is consistent but could reflect prompting artifacts rather than inherent model behavior

## Next Checks

1. Conduct detailed error analysis categorizing model failures by unreasonableness type to identify systematically harder flaw categories

2. Test the benchmark across different model versions and training epochs to determine if unreasonableness detection improves with continued training

3. Design ablation studies isolating effects of chain-of-thought reasoning, tool use, and fine-tuning on unreasonableness detection performance