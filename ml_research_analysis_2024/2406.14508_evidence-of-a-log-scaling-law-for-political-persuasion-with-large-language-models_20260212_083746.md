---
ver: rpa2
title: Evidence of a log scaling law for political persuasion with large language
  models
arxiv_id: '2406.14508'
source_url: https://arxiv.org/abs/2406.14508
tags:
- persuasive
- language
- size
- persuasiveness
- political
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines how the persuasiveness of large language models
  (LLMs) changes with model size. Researchers generated 720 political messages using
  24 models ranging from 70 million to 72 billion parameters, and tested their persuasiveness
  on 25,982 U.S.
---

# Evidence of a log scaling law for political persuasion with large language models

## Quick Facts
- arXiv ID: 2406.14508
- Source URL: https://arxiv.org/abs/2406.14508
- Reference count: 40
- Primary result: LLMs' persuasiveness follows a log scaling law with sharply diminishing returns as model size increases

## Executive Summary
This study systematically examines how the persuasiveness of large language models scales with model size by generating 720 political messages across 24 models ranging from 70 million to 72 billion parameters. Through a randomized survey experiment with 25,982 U.S. participants, researchers discovered that persuasiveness follows a log scaling law with sharply diminishing returns, such that current frontier models like GPT-4 and Claude-3-Opus are barely more persuasive than models an order of magnitude smaller. Surprisingly, larger models' persuasive advantage appears to stem primarily from improved task completion (coherence, staying on topic) rather than more sophisticated rhetorical strategies, with current frontier models already achieving perfect task completion scores.

## Method Summary
Researchers fine-tuned 24 language models (ranging from 70M to 72B parameters) on 10K examples from the GPT-4 Alpaca dataset, then generated 720 persuasive messages on 10 U.S. political issues using specific prompts. Each model-issue combination produced 3 messages (200 words each) at temperature 1, top_p 0.9, top_k 20. These messages were tested in a randomized survey experiment with 25,982 U.S. participants, measuring attitude change on 0-100 scales. Task completion scores (0-3) were assigned based on coherence, topic adherence, and correct valence. A random-effects meta-analysis examined the relationship between model size, task completion, and persuasiveness across models and issues.

## Key Results
- Persuasiveness follows a log scaling law with sharply diminishing returns as model size increases
- Current frontier models (GPT-4, Claude-3-Opus) are barely more persuasive than models an order of magnitude smaller
- Task completion score is the only reliable predictor of persuasiveness, with models scoring below 2/3 being entirely unpersuasive
- When adjusting for task completion, the association between model size and persuasiveness becomes statistically insignificant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models achieve higher persuasiveness primarily through improved task completion (coherence and staying on topic).
- Mechanism: As model size increases, the model's ability to generate coherent, grammatically correct, and on-topic text improves, which directly enhances persuasiveness. Current frontier models already achieve perfect task completion scores, suggesting an imminent ceiling on the persuasive returns to scaling.
- Core assumption: Task completion is the primary driver of persuasiveness, and this relationship follows a nonlinear pattern where models scoring below 2 out of 3 on task completion are entirely unpersuasive.
- Evidence anchors:
  - [abstract]: "mere task completion (coherence, staying on topic) appears to account for larger models’ persuasive advantage"
  - [section]: "Surprisingly, we find that task completion score is the only reliable predictor of persuasiveness... and in particular follows a nonlinear association such that models with task completion scores of 2 or less (out of 3) are estimated to be entirely unpersuasive, while scores between 2.5 and 3 are strongly associated with persuasiveness"
  - [corpus]: Weak - No direct corpus evidence found to support this specific mechanism.
- Break condition: If future models achieve task completion scores above 3 or if other factors beyond task completion become significant predictors of persuasiveness.

### Mechanism 2
- Claim: Model persuasiveness follows a log scaling law with sharply diminishing returns as model size increases.
- Mechanism: The relationship between model size (measured in parameters) and persuasiveness is logarithmic, meaning that each additional order of magnitude in model size yields progressively smaller gains in persuasiveness. Current frontier models are barely more persuasive than models an order of magnitude smaller.
- Core assumption: The log scaling relationship holds across different model families and is not significantly affected by other factors such as pre-training data or architecture differences.
- Evidence anchors:
  - [abstract]: "we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more"
  - [section]: "we estimate that a one-unit increase in the logarithm of a model’s parameter count is linearly associated with an increase of 1.26 percentage points in its average treatment effect... such that current frontier models—Claude-3-Opus and GPT-4—are barely more persuasive than models which are smaller in size by an order of magnitude or more"
  - [corpus]: Weak - No direct corpus evidence found to support this specific scaling relationship.
- Break condition: If future models exhibit significantly different scaling patterns or if the relationship between model size and persuasiveness changes dramatically.

### Mechanism 3
- Claim: The persuasive advantage of larger models is not due to more sophisticated rhetorical strategies but rather to their ability to complete the task effectively.
- Mechanism: While larger models may generate longer or more detailed messages, these factors do not significantly contribute to their persuasive advantage. Instead, the key factor is their reliability in producing coherent, on-topic content that effectively argues for the assigned issue stance.
- Core assumption: The persuasive advantage is not due to factors such as emotional or moral rhetoric, message length, or type-token ratio, but solely to task completion.
- Evidence anchors:
  - [abstract]: "mere task completion (coherence, staying on topic) appears to account for larger models’ persuasive advantage"
  - [section]: "we find that task completion score is the only reliable predictor of persuasiveness... when we adjust for task completion score in our primary meta-analysis, the association between model size (log of parameter count) and persuasiveness shrinks towards zero and is no longer statistically significant"
  - [corpus]: Weak - No direct corpus evidence found to support this specific claim about rhetorical strategies.
- Break condition: If future models demonstrate persuasive advantages through other mechanisms such as emotional appeal, moral rhetoric, or message elaboration.

## Foundational Learning

- Concept: Log scaling laws in machine learning
  - Why needed here: Understanding the log scaling relationship between model size and performance is crucial for interpreting the study's findings on persuasiveness.
  - Quick check question: How does a log scaling law differ from linear or exponential scaling in terms of diminishing returns?

- Concept: Task completion metrics in NLP
  - Why needed here: The study defines task completion as a key metric for evaluating model persuasiveness, requiring understanding of how coherence, staying on topic, and correct valence are measured.
  - Quick check question: What are the three criteria used to evaluate task completion in this study?

- Concept: Meta-analysis in experimental research
  - Why needed here: The study uses meta-analysis to combine results across different models and issues, requiring understanding of random-effects models and how they account for variability.
  - Quick check question: Why is a random-effects meta-analysis appropriate for this study's design?

## Architecture Onboarding

- Component map: Model fine-tuning -> Message generation -> Survey experiment -> Statistical analysis
- Critical path:
  1. Select and fine-tune models
  2. Generate persuasive messages
  3. Deploy messages in survey experiment
  4. Analyze results using meta-analysis

- Design tradeoffs:
  - Using open-weight models for transparency vs. potential limitations compared to closed-source models
  - Fine-tuning on general instruction-following data vs. task-specific training for persuasion
  - Employing a large sample size for statistical power vs. potential demographic skew in participants

- Failure signatures:
  - Inconsistent task completion scores across models
  - Non-significant relationship between model size and persuasiveness
  - High attrition rates or biased participant demographics affecting results

- First 3 experiments:
  1. Replicate the log scaling law finding with a different set of political issues
  2. Test the robustness of the task completion metric by having multiple human annotators score messages
  3. Investigate the effect of different instruction-tuning datasets on model persuasiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models become more persuasive when specifically trained for persuasion tasks rather than general instruction-following?
- Basis in paper: [explicit] "our findings do not imply that LLM-generated messages are un-persuasive... our results show that fine-tuning a pre-trained model on just 10K examples... was sufficient to match the persuasive impact of llama-2-7B-instruct" and "in absolute terms, the persuasiveness ceiling we estimate here... could be higher for models which are explicitly trained for a persuasion task"
- Why unresolved: The study used instruction-tuned models not specifically optimized for persuasion, so the ceiling found may be a lower bound
- What evidence would resolve it: Comparative study of models explicitly fine-tuned on persuasion datasets versus general instruction-tuned models on the same persuasion task

### Open Question 2
- Question: How does the relationship between model size and persuasiveness change in multi-turn dialogue versus single-turn static messaging?
- Basis in paper: [explicit] "recent work suggests that prolonged multi-turn dialogue with an LLM may have stronger persuasive effects than single-turn static messages"
- Why unresolved: The study only tested static 200-word messages; multi-turn interactions may reveal different scaling patterns
- What evidence would resolve it: Experimental comparison of model persuasiveness across model sizes in both single-turn and multi-turn dialogue formats

### Open Question 3
- Question: Does personalization of LLM-generated messages enhance persuasive effectiveness, and does this vary with model size?
- Basis in paper: [explicit] "while research has demonstrated that LLM-personalized static political messages confer limited persuasive advantage, LLM personalization in a multi-turn dialogue context may confer greater persuasive advantage"
- Why unresolved: The study used non-personalized messages, so the potential ceiling may be lower than achievable with personalization
- What evidence would resolve it: Comparative study of personalized versus non-personalized messages across different model sizes in both static and dialogue contexts

## Limitations

- The study's findings may not generalize beyond politically neutral U.S. issues to more polarizing topics or different cultural contexts
- Reliance on Amazon Mechanical Turk participants may introduce demographic biases that affect the generalizability of persuasiveness measurements
- The study assumes task completion scores adequately capture persuasive quality, potentially missing nuanced aspects like emotional resonance or rhetorical sophistication

## Confidence

- **High confidence**: The finding that task completion score is the primary predictor of persuasiveness, as this is supported by multiple analyses and directly observable in the data.
- **Medium confidence**: The log scaling law relationship between model size and persuasiveness, as this requires extrapolation beyond the tested parameter ranges and assumes the relationship remains consistent.
- **Low confidence**: The claim that current frontier models have reached a ceiling on persuasive returns to scaling, as this depends on the specific architectures and training approaches used.

## Next Checks

1. Replicate the experiment with a broader range of political issues, including more polarizing topics, to test the robustness of the scaling relationship across different persuasion contexts.

2. Conduct a head-to-head comparison of task completion scores from multiple human annotators to validate the consistency and reliability of this metric as a predictor of persuasiveness.

3. Test whether the log scaling law holds when using models with different architectural designs (e.g., sparse vs. dense models) to determine if the relationship is specific to transformer-based models or more general.