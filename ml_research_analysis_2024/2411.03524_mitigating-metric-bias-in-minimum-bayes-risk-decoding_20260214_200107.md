---
ver: rpa2
title: Mitigating Metric Bias in Minimum Bayes Risk Decoding
arxiv_id: '2411.03524'
source_url: https://arxiv.org/abs/2411.03524
tags:
- decoding
- rankavg
- metrics
- metric
- metricx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates metric bias in Minimum Bayes Risk (MBR)
  decoding for machine translation, where the same evaluation metric used as a utility
  function during decoding leads to overestimated quality scores. The authors show
  that MBR decoding with a single metric improves automatic evaluation scores but
  does not necessarily improve human-perceived translation quality.
---

# Mitigating Metric Bias in Minimum Bayes Risk Decoding

## Quick Facts
- arXiv ID: 2411.03524
- Source URL: https://arxiv.org/abs/2411.03524
- Reference count: 40
- This paper investigates metric bias in Minimum Bayes Risk (MBR) decoding for machine translation, where the same evaluation metric used as a utility function during decoding leads to overestimated quality scores.

## Executive Summary
This paper addresses a critical issue in machine translation where Minimum Bayes Risk (MBR) decoding using a single evaluation metric as the utility function leads to overestimated quality scores in automated evaluations. The authors demonstrate that while MBR decoding with single metrics improves automatic scores, it does not necessarily improve human-perceived translation quality. They propose using ensembles of multiple metrics during MBR decoding to mitigate this bias, showing through human evaluation that metric ensembles significantly outperform both greedy decoding and MBR decoding with single metrics while reducing metric bias in automated evaluations.

## Method Summary
The authors investigate metric bias in MBR decoding by comparing different decoding strategies: greedy decoding, MBR decoding with single metrics (COMET, BLEURT, BERTScore), and MBR decoding with metric ensembles. They conduct human evaluations alongside automated metric assessments to measure the discrepancy between automatic scores and human judgment. The study focuses on German→English and Chinese→English translation directions using WMT datasets, comparing translation quality through both automated metrics and direct human assessment of 50 sentences per system combination.

## Key Results
- MBR decoding with single metrics improves automatic evaluation scores but does not improve human-perceived translation quality
- MBR decoding with metric ensembles significantly outperforms both greedy decoding and single-metric MBR decoding in human evaluations
- Using metric ensembles during MBR decoding reduces the overestimation bias observed when using single metrics for both decoding and evaluation

## Why This Works (Mechanism)
Metric bias occurs because MBR decoding optimizes for the specific characteristics of the evaluation metric used as the utility function, rather than optimizing for actual translation quality. When the same metric is used for both decoding and evaluation, the system learns to exploit the metric's specific scoring patterns, leading to inflated scores that don't reflect true translation quality. By using ensembles of multiple diverse metrics during decoding, the optimization process becomes less susceptible to gaming any single metric's characteristics, resulting in translations that perform better on human evaluation while still maintaining strong automated scores.

## Foundational Learning
**Minimum Bayes Risk Decoding**: A decoding strategy that selects the hypothesis that maximizes expected utility according to a metric, rather than selecting the most probable hypothesis directly. Needed to understand the optimization framework; quick check: MBR selects translations based on expected utility scores from a metric.

**Metric Bias**: The phenomenon where using the same metric for both decoding and evaluation leads to inflated scores that don't reflect true quality. Needed to understand why single-metric MBR fails; quick check: metric bias causes overestimation when the same metric is used for both purposes.

**Evaluation Metric Diversity**: Different metrics (COMET, BLEURT, BERTScore) capture different aspects of translation quality. Needed to understand why ensembles work; quick check: each metric has different strengths and biases in scoring translations.

**Human Evaluation**: Direct assessment of translation quality by human annotators, considered the gold standard for measuring true translation quality. Needed to validate automated metric scores; quick check: human evaluation reveals whether automated improvements translate to actual quality gains.

## Architecture Onboarding

**Component Map**: NMT model -> Candidate translations -> MBR decoder -> Utility functions (metrics) -> Selected translation -> Evaluation (automated + human)

**Critical Path**: NMT model generates n-best list → MBR decoder computes expected utility using metric(s) → Translation with highest expected utility is selected → Output is evaluated using both automated metrics and human judgment

**Design Tradeoffs**: Single metric MBR offers computational efficiency and simplicity but suffers from metric bias; ensemble MBR provides more robust optimization but requires multiple metric computations and careful weight tuning; greedy decoding is fast but may miss high-quality candidates.

**Failure Signatures**: When automated metrics show large improvements but human evaluation shows minimal or no improvement, this indicates metric bias; when different metrics give conflicting rankings of the same translations, this suggests the metrics are capturing different quality aspects.

**First Experiments**: 1) Compare automated scores between greedy, single-metric MBR, and ensemble MBR; 2) Conduct human evaluation on a small sample to check for metric bias; 3) Test different ensemble weightings to find optimal combinations.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on automated evaluation metrics as proxies for translation quality, despite findings showing these metrics can be gamed by MBR decoding
- Experiments limited to German→English and Chinese→English translation directions, limiting generalizability across language pairs
- Human evaluation covers only 50 sentences per system combination, which may not fully capture variability in translation quality across different text types

## Confidence
- MBR decoding with single metrics overestimates quality scores: High confidence (consistent evidence across multiple automated metrics and controlled human evaluation)
- Metric ensembles reduce bias while maintaining or improving quality: Medium confidence (human evaluation shows improvements but effect sizes vary across ensemble configurations)
- Recommendation to use multiple metrics in MBR decoding: Medium confidence (supported but needs testing on additional language pairs and domains)

## Next Checks
1. Test the ensemble approach on additional language pairs and specialized domains to assess robustness beyond general news translation
2. Evaluate whether including additional metrics beyond COMET, BLEURT, and BERTScore further reduces bias or changes optimal ensemble compositions
3. Conduct larger-scale human evaluations with more sentences and diverse annotators to validate statistical significance of quality improvements