---
ver: rpa2
title: An Axiomatic Approach to Model-Agnostic Concept Explanations
arxiv_id: '2401.06890'
source_url: https://arxiv.org/abs/2401.06890
tags:
- concept
- prompts
- concepts
- prediction
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a model-agnostic framework for measuring
  concept explanations in machine learning models through three axiomatic properties:
  linearity, recursivity, and similarity. The authors develop new measures that quantify
  the necessity and sufficiency of human-interpretable concepts for model predictions,
  providing a theoretical foundation for understanding how concepts influence model
  decisions.'
---

# An Axiomatic Approach to Model-Agnostic Concept Explanations

## Quick Facts
- arXiv ID: 2401.06890
- Source URL: https://arxiv.org/abs/2401.06890
- Reference count: 40
- Primary result: Introduces axiomatic framework for measuring concept necessity and sufficiency in ML models

## Executive Summary
This paper presents a theoretical framework for model-agnostic concept explanations in machine learning, grounded in three axiomatic properties: linearity, recursivity, and similarity. The authors develop measures to quantify how well human-interpretable concepts explain model predictions, providing both necessity and sufficiency metrics. The framework connects to existing interpretability methods while offering new theoretical foundations for understanding concept-based explanations. Through experiments across multiple applications including logistic regression vs. random forest comparison, optimizer analysis, and CLIP prompt editing, the authors demonstrate practical utility and improved model performance.

## Method Summary
The framework formalizes concept explanations through three axioms that any reasonable measure should satisfy. Linearity ensures consistent behavior when concepts are added or removed. Recursivity guarantees that the measure can handle nested or hierarchical concepts. Similarity requires that explanations for similar concepts should be comparable. Using these axioms, the authors derive specific measures for concept necessity and sufficiency that can be applied to any model without requiring architectural modifications. The approach works by analyzing how model predictions change when concept information is added or removed, allowing quantification of each concept's explanatory power.

## Key Results
- Demonstrates framework utility by selecting between logistic regression and random forest models based on concept explanations
- Shows F1-score improvement from 0.6796 to 0.7042 through CLIP prompt editing guided by concept explanations
- Reveals semantic differences between models achieving similar accuracy through concept analysis
- Connects TCAV measures to concept necessity and completeness-aware explanations to concept sufficiency

## Why This Works (Mechanism)
The framework works by providing a principled way to measure how concepts influence model predictions through carefully designed axioms. By requiring measures to satisfy linearity, recursivity, and similarity, the framework ensures that concept explanations behave intuitively and consistently across different scenarios. The necessity measure captures how much a concept is required for the model's prediction, while sufficiency measures how completely the concept explains the prediction. This dual perspective provides a comprehensive understanding of concept importance that goes beyond simple feature attribution methods.

## Foundational Learning

**Concept: Model-agnostic explanations**
*Why needed*: To create interpretability tools that work across different model architectures without requiring internal access
*Quick check*: Can the method be applied to both neural networks and traditional ML models?

**Concept: Concept necessity vs sufficiency**
*Why needed*: To understand both how much a concept is required and how completely it explains predictions
*Quick check*: Does the framework measure both dimensions separately?

**Concept: Axiomatic foundations**
*Why needed*: To ensure explanations have consistent, intuitive mathematical properties
*Quick check*: Do the measures satisfy the three proposed axioms?

## Architecture Onboarding

**Component map**: Concept extraction -> Axiom validation -> Necessity/sufficiency calculation -> Model comparison

**Critical path**: The framework's core functionality depends on reliable concept extraction and accurate measurement of prediction changes when concepts are added/removed.

**Design tradeoffs**: The axiomatic approach prioritizes theoretical consistency over computational efficiency, potentially limiting scalability to very large models or datasets.

**Failure signatures**: Poor performance when concepts are ill-defined, when model predictions are highly non-linear with respect to concept presence, or when ground truth concept annotations are unavailable.

**First experiments**: 1) Apply framework to compare logistic regression vs. random forest on simple datasets, 2) Test necessity/sufficiency measures on synthetic data with known ground truth, 3) Evaluate prompt editing effectiveness on CLIP for different concept types

## Open Questions the Paper Calls Out

None

## Limitations

- Axioms assume linear relationships that may oversimplify complex concept interactions in deep neural networks
- Experimental validation limited to image classification tasks, restricting generalizability
- Requires ground truth concept annotations which can be expensive or impractical to obtain
- Scalability to larger models and datasets remains untested

## Confidence

**High Confidence**: Theoretical formulation of three axioms and their mathematical properties
**Medium Confidence**: Experimental results demonstrating practical utility across different applications
**Medium Confidence**: Connections drawn to existing interpretability methods like TCAV and completeness-aware explanations

## Next Checks

1. Test framework on non-visual domains (e.g., text classification, tabular data) to assess cross-domain applicability
2. Evaluate performance on larger-scale datasets and more complex model architectures beyond current image classification focus
3. Conduct ablation studies to quantify impact of each axiom on explanation quality and identify potential redundancy between them