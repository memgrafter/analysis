---
ver: rpa2
title: Here's Charlie! Realising the Semantic Web vision of Agents in the age of LLMs
arxiv_id: '2409.04465'
source_url: https://arxiv.org/abs/2409.04465
tags:
- agents
- agent
- data
- semantic
- nigel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research presents an implementation of semi-autonomous Web
  agents that represent individuals and organizations in online interactions, using
  Notation3 rules and LLMs to ensure trustworthiness and reliability. The core innovation
  lies in enabling agents to communicate using a protocol that satisfies six key requirements:
  identifying legal entities on the Web, discovering other agents, describing usage
  controls, providing provenance, unambiguously describing ground truths, and contextualizing
  tasks.'
---

# Here's Charlie! Realising the Semantic Web vision of Agents in the age of LLMs

## Quick Facts
- arXiv ID: 2409.04465
- Source URL: https://arxiv.org/abs/2409.04465
- Authors: Jesse Wright
- Reference count: 27
- One-line primary result: Implementation of semi-autonomous Web agents using Notation3 rules and LLMs to ensure trustworthiness while enabling natural language interaction

## Executive Summary
This research presents an implementation of semi-autonomous Web agents that represent individuals and organizations in online interactions, using Notation3 rules and LLMs to ensure trustworthiness and reliability. The core innovation lies in enabling agents to communicate using a protocol that satisfies six key requirements: identifying legal entities on the Web, discovering other agents, describing usage controls, providing provenance, unambiguously describing ground truths, and contextualizing tasks. The demo implementation showcases a personal assistant use case where agents negotiate meeting schedules between users while maintaining control over data sharing and decision-making. The system uses Notation3 reasoning to enforce safety guarantees around belief, data sharing, and usage policies, while LLMs facilitate natural language interaction and serendipitous dialogues between software agents.

## Method Summary
The research implements semi-autonomous Web agents using Notation3 reasoning for safety guarantees and LLMs for natural language interaction. The method involves setting up Solid PODs with WebID profiles, creating personal knowledge graphs with calendar data and access control policies, implementing N3 reasoning engines for policy enforcement, and developing LLM integration for agent-to-agent communication. The system demonstrates a personal assistant use case where agents negotiate meeting schedules while maintaining user control over data sharing and decision-making preferences through a user-agent dialogue mechanism.

## Key Results
- Implementation of agent communication protocol satisfying six key requirements for trustworthy web interactions
- Demo personal assistant use case showing agents negotiating meeting schedules between users
- Successful integration of Notation3 reasoning with LLM capabilities for natural language interaction and policy enforcement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Notation3 rules provide formal guarantees for data sharing and usage policies while LLMs enable natural language interaction
- Mechanism: The system combines semantic web's formal reasoning (N3 rules) with LLM capabilities to create trustworthy agent interactions. The N3 rules enforce safety guarantees around belief, data sharing, and usage policies, while LLMs handle natural language understanding and serendipitous dialogues.
- Core assumption: The combination of formal reasoning and LLM-based natural language processing creates a system where users maintain control while benefiting from agent convenience
- Evidence anchors:
  - [abstract] "implemented using (Notation3) rules to enforce safety guarantees around belief, data sharing and data usage and LLMs to allow natural language interaction with users and serendipitous dialogues between software agents"
  - [section 3] "Notation3 [18] reasoning is used to identify the policies applicable to the data subset"
- Break condition: If N3 reasoning fails to properly enforce policies or LLMs cannot accurately interpret user intent, the system loses its trustworthiness guarantees

### Mechanism 2
- Claim: The agent communication protocol satisfies six key requirements for trustworthy web interactions
- Mechanism: The protocol enables agents to identify legal entities, discover other agents, describe usage controls, provide provenance, unambiguously describe ground truths, and contextualize tasks. This creates a framework for reliable agent-to-agent communication.
- Core assumption: These six requirements comprehensively address the challenges of building trustworthy agent networks
- Evidence anchors:
  - [section 2] "We identify the following non-functional requirements for an agent communication protocol"
  - [section 3] Implementation demonstrates each requirement through the scheduling use case
- Break condition: If any of the six requirements cannot be reliably implemented or enforced, the protocol loses its trustworthiness guarantees

### Mechanism 3
- Claim: User-agent dialogue enables teaching agents about trusted information sources and preferences
- Mechanism: The semi-autonomous agents consult users only when they lack sufficient context or confidence, creating a learning loop where users teach agents about their trusted sources, data-sharing preferences, and decision-making preferences.
- Core assumption: Users are willing and able to provide this guidance when prompted, and agents can learn and apply these preferences appropriately
- Evidence anchors:
  - [abstract] "The author's research concerns the development of semi-autonomous Web agents, which consult users if and only if the system does not have sufficient context or confidence to proceed working autonomously"
  - [section 3] Implementation shows agents requesting permissions and confirmations from users
- Break condition: If user engagement with the dialogue system is too low or inconsistent, the learning mechanism fails to build adequate trust models

## Foundational Learning

- Notation3 reasoning
  - Why needed here: Provides formal guarantees for policy enforcement and trust management between agents
  - Quick check question: How does N3 reasoning differ from other rule-based systems in handling trust and provenance?

- Semantic Web technologies
  - Why needed here: Enables unambiguous description of ground truths and structured data exchange between agents
  - Quick check question: What role do RDF and ontologies play in ensuring agents can communicate reliably?

- Large Language Model integration
  - Why needed here: Facilitates natural language interaction and serendipitous dialogues between agents and users
  - Quick check question: How does the system balance LLM flexibility with the need for formal guarantees?

## Architecture Onboarding

- Component map:
  User Interface Layer -> LLM Integration Layer -> Agent Reasoning Layer -> Semantic Web Layer -> Policy Management Layer

- Critical path:
  1. User provides task request via natural language
  2. LLM interprets request and identifies required entities
  3. N3 rules determine data sharing requirements and policies
  4. Agent discovers other agents and negotiates terms
  5. Meeting scheduling or task completion occurs
  6. Results are confirmed and knowledge base updated

- Design tradeoffs:
  - Precision vs. convenience: Formal guarantees vs. natural language flexibility
  - User control vs. automation: Consultation frequency vs. agent autonomy
  - Performance vs. expressiveness: Complex N3 reasoning vs. system responsiveness

- Failure signatures:
  - Policy enforcement failures: Users receiving data they shouldn't access
  - LLM misinterpretation: Agents taking incorrect actions based on misunderstood requests
  - Trust model errors: Agents accepting unreliable sources as authoritative
  - Performance bottlenecks: N3 reasoning taking too long for interactive use

- First 3 experiments:
  1. Test basic scheduling functionality with two agents and verify policy enforcement
  2. Test LLM interpretation of ambiguous requests and agent responses
  3. Test trust model by introducing conflicting information sources and observing agent behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents align on conceptual models for use when encountering new tasks, and how can human oversight be maintained without disrupting user experience?
- Basis in paper: [explicit] The paper states "Truly generic agents may generate and communicate structured ontologies when encountering new tasks. In many cases we expect LLM-supported ontology construction to facilitate generation; however, research is required to understand how (1) agents can align on conceptual models for use and (2) how human oversight can be maintained without disrupting user experience."
- Why unresolved: The paper identifies this as a key research challenge but does not provide solutions or empirical evidence for how agents can achieve semantic alignment or maintain appropriate human oversight in practice.
- What evidence would resolve it: Empirical studies demonstrating successful ontology alignment between agents in practice, or user studies showing effective human oversight mechanisms that don't degrade user experience.

### Open Question 2
- Question: How can agents negotiate to obtain sufficient provenance to believe claims, and find agreeable data terms of use between agents while updating their internal models via user interaction?
- Basis in paper: [explicit] The paper states "This enables agents to negotiate to obtain sufficient provenance to believe claims, and find agreeable data terms of use between agents - whilst concurrently updating their internal models via user interaction."
- Why unresolved: The paper identifies this as a future requirement but does not provide mechanisms or demonstrate how such negotiations would work in practice, particularly in balancing user preferences with agent autonomy.
- What evidence would resolve it: Implementation of negotiation protocols between agents with measurable success rates in reaching agreements, and user studies showing acceptable balance between automation and user control.

### Open Question 3
- Question: How can agentic trust models be developed to qualify whether sources are trusted for particular types of claims and determine appropriate forms of provenance for different tasks?
- Basis in paper: [explicit] The paper mentions "We are developing conceptual models for agentic trust; these extend existing trust vocabularies with a range of features including (1) qualifying whether sources are trusted for particular types of claims; for instance, most agents should trust certified airlines to present flight times and prices, but not medical data (2) qualifying the forms of provenance secure enough for a given task."
- Why unresolved: While the paper outlines desired features for trust models, it does not provide the actual conceptual models or empirical validation of their effectiveness in real-world agent interactions.
- What evidence would resolve it: Development and testing of concrete trust models with measurable accuracy in claim verification and user acceptance of trust decisions.

## Limitations
- Lack of specific technical details about Notation3 rule implementations and LLM prompting strategies
- Evaluation focuses primarily on technical feasibility rather than user experience or practical deployment considerations
- System performance at scale with complex real-world scenarios remains unverified

## Confidence

**High**: The core concept of combining formal reasoning with LLMs for trustworthy agent communication is technically sound

**Medium**: The six requirement framework for agent protocols addresses real challenges in agent trustworthiness

**Low**: Claims about learning effectiveness and user engagement in the trust model lack empirical validation

## Next Checks

1. **Policy Enforcement Validation**: Test the N3 reasoning system with edge cases in access control policies to verify that safety guarantees hold under complex permission scenarios, including conflicting policies and ambiguous provenance information.

2. **User Learning Effectiveness**: Conduct a user study measuring how effectively the agent learning system captures and applies user preferences over multiple interaction sessions, tracking both user satisfaction and agent autonomy levels.

3. **Scalability Assessment**: Deploy the system with multiple concurrent agents handling diverse tasks beyond scheduling to evaluate performance bottlenecks, LLM accuracy degradation, and policy enforcement overhead in realistic multi-agent scenarios.