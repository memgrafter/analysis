---
ver: rpa2
title: Using Self-supervised Learning Can Improve Model Fairness
arxiv_id: '2406.02361'
source_url: https://arxiv.org/abs/2406.02361
tags:
- fairness
- supervised
- data
- learning
- protected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether self-supervised learning (SSL)
  improves fairness compared to supervised models on human-centric, multimodal datasets.
  The authors introduce a five-stage fairness assessment framework for SSL, comprising
  dataset requirements, pre-training, fine-tuning with gradual unfreezing, conditioned
  representation similarity, and domain-specific evaluation.
---

# Using Self-supervised Learning Can Improve Model Fairness

## Quick Facts
- arXiv ID: 2406.02361
- Source URL: https://arxiv.org/abs/2406.02361
- Reference count: 40
- Primary result: SSL models with gradual unfreezing can improve fairness by up to 30% with minimal performance loss.

## Executive Summary
This paper investigates whether self-supervised learning (SSL) improves fairness compared to supervised models on human-centric, multimodal datasets. The authors introduce a five-stage fairness assessment framework for SSL, comprising dataset requirements, pre-training, fine-tuning with gradual unfreezing, conditioned representation similarity, and domain-specific evaluation. They evaluate their method on three real-world datasets (MIMIC, MESA, and GLOBEM) and compare hundreds of SSL and fine-tuned models. The results demonstrate that SSL can significantly improve model fairness, with up to a 30% increase in fairness with minimal loss in performance through self-supervision.

## Method Summary
The authors propose a five-stage fairness assessment framework for SSL, starting with dataset requirements and pre-training on unlabeled data using SimCLR-style augmentation and a 3-layer CNN encoder. They then apply fine-tuning with gradual unfreezing (freezing/unfreezing layers incrementally) to balance performance and fairness. Representation similarity is assessed using linear CKA, conditioned on protected attributes, and domain-specific evaluation compares fairness metrics (disparate impact, false omission/positive rates, etc.) against supervised baselines across three multimodal datasets.

## Key Results
- SSL models with gradual unfreezing improve fairness by up to 30% compared to fully supervised models.
- Middle-layer freezing during fine-tuning achieves the best fairness-performance trade-off.
- Larger performance gaps between demographic groups correlate with greater representation dissimilarity between SSL and supervised models (up to 13x).

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pre-training followed by gradual unfreezing produces less biased representations than fully supervised models. SSL pre-training on large unlabeled datasets learns generic features before task-specific fine-tuning, reducing direct exposure to potentially biased labels. Core assumption: Pre-training on unlabeled data captures broader data structure while avoiding label-driven bias amplification during early stages. Evidence anchors: [abstract] and [section 3]. Break condition: If pre-training data itself contains significant demographic bias or if fine-tuning amplifies it, fairness gains may vanish.

### Mechanism 2
Middle-layer freezing during fine-tuning achieves optimal fairness-performance trade-off. Freezing early layers preserves generic, debiased features while allowing later layers to adapt to task-specific patterns, reducing overfitting to biased label distributions. Core assumption: Early layers capture domain-agnostic patterns; later layers encode task-specific bias; controlled unfreezing prevents over-specialization. Evidence anchors: [abstract] and [section 5.2]. Break condition: If task requires full adaptation or if early layers are already biased, partial freezing may hurt performance more than it improves fairness.

### Mechanism 3
Larger performance gaps between demographic groups correlate with greater representation dissimilarity between SSL and supervised models. SSL models that learn group-invariant representations will show smaller performance disparities; supervised models will diverge more in latent space for disadvantaged groups. Core assumption: Representation similarity (via CKA) captures model robustness to group-level variance; greater divergence implies poorer generalization for certain demographics. Evidence anchors: [abstract] and [section 5.3]. Break condition: If representation similarity does not align with downstream fairness metrics, the correlation may be spurious or dataset-dependent.

## Foundational Learning

- **Concept: Self-supervised learning (contrastive objectives)**
  - Why needed here: Core training paradigm being evaluated for fairness impact; understanding SimCLR-style augmentation and NT-Xent loss is essential to interpret results.
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning, and why does this matter for representation quality?

- **Concept: Group fairness metrics (disparate impact, false positive/negative ratios)**
  - Why needed here: Fairness evaluation depends on ratio-based metrics; understanding what each ratio captures (WAE vs WYSIWYG perspectives) is key to interpreting parity deviation.
  - Quick check question: If a model has a false negative rate ratio of 2.0 for a protected group, what does that mean in practical terms?

- **Concept: Representation similarity (CKA, linear probing)**
  - Why needed here: Comparing SSL vs supervised latent spaces requires familiarity with Centered Kernel Alignment and how conditioning on protected attributes changes similarity.
  - Quick check question: How does CKA differ from canonical correlation analysis when comparing neural network representations?

## Architecture Onboarding

- **Component map:**
  Pre-training module (SimCLR-style augmentation + 3-layer CNN encoder + projection head) -> Fine-tuning module (Gradual unfreezing controller + classification head) -> Evaluation module (Multiple fairness ratios + CKA-based representation comparison)

- **Critical path:**
  1. Load dataset → check protected attribute balance
  2. Pre-train SSL encoder on unlabeled data
  3. Fine-tune with gradual unfreezing (0→1→2→3 trainable layers)
  4. Compute fairness metrics and CKA similarities
  5. Compare against supervised baseline

- **Design tradeoffs:**
  - More frozen layers → higher fairness but lower task accuracy
  - Fewer frozen layers → higher accuracy but increased bias risk
  - Larger pre-training dataset → better generic features but more compute

- **Failure signatures:**
  - No fairness improvement → check label distribution per group; ensure pre-training dataset is large and diverse
  - Representation similarity shows no group differences → verify conditioning on protected attributes is correct; confirm CKA implementation
  - Performance collapses with middle unfreezing → verify layer indices in gradual unfreezing loop; check that dropout rates are appropriate per dataset

- **First 3 experiments:**
  1. Run supervised baseline on MIMIC with all layers trainable; record AUC-ROC and fairness ratios.
  2. Run SSL with all layers frozen during fine-tuning; compare fairness gain vs supervised.
  3. Run SSL with only the first layer frozen; compare fairness and performance trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
How does the pre-training phase encode biases from data distributions in self-supervised learning, and can these biases be effectively mitigated? The paper states that the pre-training phase can still encode biases from data distributions, but does not delve into the specific mechanisms or explore effective mitigation strategies. Empirical studies demonstrating the types of biases encoded during pre-training and the effectiveness of various mitigation techniques would provide clarity.

### Open Question 2
How does the fine-tuning process in self-supervised learning affect the amplification of biases present in the labeled data? The paper mentions that fine-tuning using labeled data can lead to bias amplification, but does not provide a detailed analysis of how this amplification occurs or the extent to which it affects model fairness. Experimental results comparing the fairness of models fine-tuned with different amounts and types of labeled data would shed light on this issue.

### Open Question 3
What is the influence of contrastive objectives in self-supervised learning on fairness, and how do design choices like data augmentation impact this relationship? The paper discusses the use of contrastive learning objectives in self-supervised learning and mentions data augmentation as part of the training process, but does not explicitly investigate their impact on fairness. Comparative studies evaluating the fairness of self-supervised models trained with different contrastive objectives and data augmentation strategies would provide insights.

## Limitations
- The correlation between representation similarity and fairness gaps relies on CKA divergence, which measures distributional overlap but not causal bias pathways.
- The claim of "up to 30% fairness improvement" is dataset- and metric-dependent, with fairness gains potentially trading off against performance in unseen domains.
- The paper does not fully account for the risk of pre-training on unlabeled but biased data propagating historical inequities into SSL representations.

## Confidence

- **High**: SSL pre-training followed by gradual unfreezing produces measurable fairness improvements vs fully supervised models (supported by multi-dataset results).
- **Medium**: Middle-layer freezing achieves optimal fairness-performance trade-off (empirically observed but mechanism not fully explained).
- **Low**: Representation similarity gaps (CKA) directly explain fairness disparities (correlation shown but causation not established).

## Next Checks
1. Apply the SSL + gradual unfreezing pipeline to a held-out multimodal dataset with known demographic bias to test generalization.
2. Pre-train SSL on a balanced subset vs. the full dataset to isolate the impact of pre-training data bias on downstream fairness.
3. Use linear probes conditioned on protected attributes to test whether CKA divergence aligns with group-specific classification boundaries, not just distributional shifts.