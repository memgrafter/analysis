---
ver: rpa2
title: Transferable Sequential Recommendation via Vector Quantized Meta Learning
arxiv_id: '2411.01785'
source_url: https://arxiv.org/abs/2411.01785
tags:
- learning
- meta
- metarec
- domain
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of transferring sequential recommendation
  models across domains with disjoint user and item groups, where traditional transfer
  methods fail due to input heterogeneity and domain discrepancies. The authors propose
  MetaRec, a vector quantized meta learning framework that (1) uses vector quantization
  to map item embeddings from heterogeneous source domains to a shared feature space
  without introducing additional parameters, and (2) employs meta transfer learning
  with gradient rescaling to adaptively learn from multiple source domains while accounting
  for domain similarity.
---

# Transferable Sequential Recommendation via Vector Quantized Meta Learning

## Quick Facts
- arXiv ID: 2411.01785
- Source URL: https://arxiv.org/abs/2411.01785
- Reference count: 40
- MetaRec achieves 6.36% average improvement in NDCG@10 across six target datasets

## Executive Summary
This paper addresses the challenge of transferring sequential recommendation models across domains with disjoint user and item groups. The authors propose MetaRec, a vector quantized meta learning framework that uses vector quantization to map item embeddings from heterogeneous source domains to a shared feature space without introducing additional parameters. The framework employs meta transfer learning with gradient rescaling to adaptively learn from multiple source domains while accounting for domain similarity, achieving consistent improvements across six target datasets.

## Method Summary
MetaRec combines vector quantization with meta transfer learning to enable sequential recommendation transfer across domains with disjoint user and item groups. The framework uses multi-head vector quantization where target domain embeddings serve as codebook weights, mapping source domain items to a shared quantized space. Meta transfer learning with gradient rescaling adapts parameters from multiple source domains based on similarity scores computed from source-target gradient pairs. The method employs two-level optimization: inner-loop updates on source tasks and outer-loop updates on the target task.

## Key Results
- MetaRec achieves 6.36% average improvement in NDCG@10 over baseline methods
- Up to 8.44% improvement on Scientific dataset, demonstrating effectiveness in data-scarce scenarios
- Consistent improvements across all six target datasets: Musical Instruments (5.73%), Arts (8.22%), Office (7.34%), Games (5.66%), and Pet Supplies (6.89%)
- Ablation studies confirm effectiveness of both multi-head VQ and meta transfer with gradient rescaling components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector quantization aligns item embeddings from heterogeneous domains into a shared feature space without introducing additional parameters
- Mechanism: Multi-head codebook maps item embeddings from source and target domains to a shared quantized space using target domain embedding table as codebook weights, enabling knowledge transfer despite disjoint item groups
- Core assumption: Items with similar characteristics across domains can be mapped to similar quantized representations
- Evidence anchors:
  - [abstract] "To solve the input heterogeneity issue, we adopt vector quantization that maps item embeddings from heterogeneous input spaces to a shared feature space"
  - [section] "Instead of introducing additional parameters, we apply weights from the target domain embedding table as codebook in VQ"
  - [corpus] Weak - no direct corpus evidence for VQ in sequential recommendation transfer

### Mechanism 2
- Claim: Meta transfer with gradient rescaling enables adaptive learning from multiple source domains by weighting gradients based on source-target domain similarity
- Mechanism: Computes similarity scores between source task gradients and meta task gradients, then rescales meta gradients using softmax-normalized similarity scores before updating parameters
- Core assumption: Gradient similarity between source and target tasks indicates transferable knowledge
- Evidence anchors:
  - [abstract] "MetaRec adaptively transfers from multiple source tasks by rescaling meta gradients based on the source-target domain similarity"
  - [section] "We compute the similarity score si for the i-th pair of source and meta tasks with: si = Sim(dϕi/dθ ∇ϕi L, ϕi − θ)"
  - [corpus] Weak - limited corpus evidence for gradient rescaling in multi-source meta transfer for recommendation

### Mechanism 3
- Claim: Multi-head vector quantization increases representational capacity by creating K^H possible quantized combinations
- Mechanism: Splits item embeddings into H heads, applies vector quantization separately to each head, then concatenates results to form final quantized embedding
- Core assumption: Different subspaces of item embeddings capture complementary information that benefits from separate quantization
- Evidence anchors:
  - [section] "we find it beneficial to split item representations into subspaces and project them separately"
  - [section] "Multi-head VQ can increase the total number of vector combinations from K to K^H, significantly increasing the output space size"
  - [corpus] Weak - no direct corpus evidence for multi-head VQ in recommendation systems

## Foundational Learning

- Concept: Vector quantization fundamentals
  - Why needed here: Understanding how VQ maps continuous embeddings to discrete codes using codebook vectors is essential for grasping how MetaRec aligns cross-domain item representations
  - Quick check question: What is the difference between learning a codebook from scratch versus using target domain embeddings as codebook weights?

- Concept: Meta learning and bi-level optimization
  - Why needed here: MetaRec uses inner-loop optimization on source tasks and outer-loop optimization on target tasks, requiring understanding of gradient flow through nested optimization
  - Quick check question: In MetaRec's meta transfer, which parameters are updated in the inner loop versus the outer loop?

- Concept: Gradient similarity and adaptive weighting
  - Why needed here: MetaRec rescales meta gradients based on similarity between source and target gradients, requiring understanding of how gradient similarity indicates transferable knowledge
  - Quick check question: How does MetaRec compute similarity between source task gradients and meta task gradients?

## Architecture Onboarding

- Component map: Embedding lookup → Vector quantization (multi-head) → Sequential encoder (LRURec) → Prediction layer, with meta transfer module computing gradient rescaling between source and target task optimizations
- Critical path: Input sequence → Target embedding table → Multi-head VQ mapping → Sequential model → Prediction → Loss computation → Backpropagation through VQ and sequential model
- Design tradeoffs: Using target embeddings as codebook avoids additional parameters but may limit codebook diversity; multi-head increases capacity but adds complexity; gradient rescaling improves adaptation but requires careful similarity computation
- Failure signatures: Performance degradation when domains are too dissimilar (negative transfer), VQ mapping fails to align representations, or gradient rescaling weights become too uniform
- First 3 experiments:
  1. Verify VQ mapping: Compare quantized representations across domains for known similar items
  2. Test gradient rescaling: Visualize similarity scores and their impact on gradient updates
  3. Ablation study: Remove VQ, meta transfer, or gradient rescaling individually to confirm their contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MetaRec framework perform when applied to domains with partially overlapping user or item groups, rather than strictly disjoint groups?
- Basis in paper: [explicit] The paper states that existing cross-domain recommendation methods assume (partially) overlapping user/item groups or require explicit correspondences, and MetaRec aims to extend applicability to disjoint groups.
- Why unresolved: The paper only evaluates MetaRec under the assumption of zero overlapping user and item groups across all domains.
- What evidence would resolve it: Empirical results showing MetaRec's performance on datasets with varying degrees of overlap between source and target domains.

### Open Question 2
- Question: What is the impact of the temperature parameter τ in the softmax function for gradient rescaling on MetaRec's performance?
- Basis in paper: [explicit] The paper mentions that the temperature τ is "selected empirically" for transforming similarity scores into a probability distribution.
- Why unresolved: The paper does not provide any analysis or justification for the chosen temperature value or its effect on performance.
- What evidence would resolve it: Ablation studies or sensitivity analysis showing MetaRec's performance with different temperature values.

### Open Question 3
- Question: How does MetaRec's performance scale with an increasing number of source domains (M > 7)?
- Basis in paper: [explicit] The paper uses 7 source domains in its experiments, but does not explore scenarios with more source domains.
- Why unresolved: The paper does not provide any analysis on how MetaRec's performance changes as the number of source domains increases beyond 7.
- What evidence would resolve it: Empirical results showing MetaRec's performance with varying numbers of source domains, particularly with M > 7.

## Limitations

- Key architectural details of the LRURec backbone model (layer counts, hidden dimensions) are not specified, making exact replication challenging
- Critical hyperparameters like learning rates, temperature τ, and multi-head VQ parameters are not provided, affecting reproducibility
- Results are limited to 6 target datasets with specific filtering, limiting generalizability to other recommendation domains

## Confidence

**High Confidence**: The core mechanism of using target domain embeddings as codebook weights for vector quantization is clearly described and theoretically sound. The gradient rescaling approach for meta transfer is well-specified.

**Medium Confidence**: The multi-head VQ design and its benefits are explained, but the optimal configuration (H, K values) and their impact on performance are not thoroughly explored or validated.

**Low Confidence**: The specific implementation details of the LRURec backbone and the exact hyperparameter settings make faithful reproduction uncertain without additional experimentation.

## Next Checks

**Validation Check 1**: Implement a simplified version of MetaRec with single-head VQ and fixed gradient weights to establish baseline performance, then incrementally add multi-head VQ and gradient rescaling to measure their individual contributions.

**Validation Check 2**: Conduct extensive ablation studies across different hyperparameter configurations (H values, temperature τ ranges, learning rates) to identify sensitivity and optimal settings for each component.

**Validation Check 3**: Test MetaRec on additional target domains beyond the 6 reported datasets, particularly domains with different characteristics (e.g., different sparsity levels, item frequencies) to validate generalizability claims.