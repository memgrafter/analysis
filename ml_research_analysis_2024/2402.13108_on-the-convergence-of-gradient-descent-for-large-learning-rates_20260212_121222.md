---
ver: rpa2
title: On the Convergence of Gradient Descent for Large Learning Rates
arxiv_id: '2402.13108'
source_url: https://arxiv.org/abs/2402.13108
tags:
- gradient
- descent
- loss
- zero
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the convergence of gradient descent for
  large learning rates. The authors show that for linear neural networks with a quadratic
  loss, convergence becomes impossible when the step size exceeds a critical value,
  regardless of the initialization.
---

# On the Convergence of Gradient Descent for Large Learning Rates

## Quick Facts
- **arXiv ID**: 2402.13108
- **Source URL**: https://arxiv.org/abs/2402.13108
- **Reference count**: 40
- **Primary result**: For large learning rates, gradient descent converges to minima only for a set of initializations of measure zero

## Executive Summary
This paper investigates when gradient descent with fixed step size fails to converge to minima. The authors prove that for linear neural networks with quadratic loss, convergence becomes impossible when the step size exceeds a critical value ηE, regardless of initialization. They show that the set of initializations that converge to minima has measure zero for such large step sizes. This result is generalized to more complex loss functions under certain conditions, particularly when the loss is analytic and has at least one non-constant Hessian eigenvalue. The key mechanism is that minima become unstable when the step size is too large, leading to the failure of convergence except for a set of measure zero initializations.

## Method Summary
The authors analyze gradient descent for linear neural networks with quadratic loss functions, computing Hessian eigenvalues along the minima manifold to determine stability thresholds. They prove that when step sizes exceed a critical value, eigenvalues grow without bound, making all minima unstable. The paper then extends this analysis to more general analytic loss functions by examining the non-singularity of the gradient descent map. Experimental validation is performed using synthetic loss functions and non-linear networks trained on MNIST to demonstrate the theoretical predictions.

## Key Results
- For linear networks with quadratic loss, gradient descent converges to minima only when step size η < ηE
- When η > ηE, the set of initializations converging to minima has measure zero
- The critical threshold ηE depends on the growth of Hessian eigenvalues along the minima manifold
- For analytic loss functions with non-constant Hessian eigenvalues, the gradient descent map is non-singular almost everywhere
- Experiments with non-linear networks confirm the theoretical predictions about large step size instability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent with fixed step size η fails to converge for almost all initializations once η exceeds a critical value ηE.
- Mechanism: The eigenvalues of the Hessian along the minima manifold M grow without bound as one moves away from the origin. When η > ηE, these eigenvalues exceed 2/η, making the gradient descent map unstable at all minima. By the Stable Manifold Theorem, the set of initializations converging to any minimum has measure zero.
- Core assumption: The Hessian eigenvalues along M are proper maps (Theorem 5), ensuring they become arbitrarily large in norm.
- Evidence anchors:
  - [abstract] "convergence becomes impossible no matter the initialization if the step-size gets too big"
  - [section] "From Corollary 6 we know there exists an ηE > 0 such that for any η > ηE and θ ∈ M, any non-zero eigenvalue of HL(θ) is greater than 2/η"
  - [corpus] weak - related works discuss step size and stability but don't prove measure-zero convergence failure
- Break condition: If the loss function has constant Hessian eigenvalues along all minima, the mechanism fails (Proposition 18).

### Mechanism 2
- Claim: The gradient descent map G(θ) = θ - η∇L(θ) is non-singular for linear networks with quadratic loss, meaning it preserves null sets under preimages.
- Mechanism: The determinant of the Jacobian of G is a non-zero polynomial in the parameters. By Lemma 10, the zero set of a non-zero polynomial has measure zero, so G is invertible almost everywhere. By Proposition 9, G is non-singular.
- Core assumption: The loss function's gradient map is polynomial in the parameters (true for linear networks with quadratic loss).
- Evidence anchors:
  - [abstract] "We prove that preimages of sets of measure zero under the gradient descent map... are again sets of measure zero"
  - [section] "The map p : θ ↦→ det(I - ηHL(θ)) is a polynomial... By Lemma 10 its set of zeros has measure zero"
  - [corpus] weak - related works discuss step size bounds but not non-singularity of the GD map
- Break condition: If the loss function's gradient map is not polynomial (e.g., non-linear activation functions), this mechanism may not apply.

### Mechanism 3
- Claim: For more general analytic loss functions, the gradient descent map is non-singular if at least one eigenvalue of the Hessian is not constant.
- Mechanism: If an eigenvalue λ of HL varies along some path in parameter space, then the map θ ↦→ det(I - ηHL(θ)) is not constant. By Lemma 13, the zero set of a non-constant analytic function has measure zero, so G is non-singular.
- Core assumption: The loss function is analytic and has at least one non-constant eigenvalue of its Hessian.
- Evidence anchors:
  - [abstract] "We prove the impossibility of convergence for more general losses without requiring strong assumptions"
  - [section] "If an eigenvalue λ of HL varies along some path... then the map θ ↦→ det(I - ηHL(θ)) is not constant"
  - [corpus] weak - related works discuss convergence but not the analytic structure of Hessian eigenvalues
- Break condition: If all eigenvalues of the Hessian are constant (e.g., quadratic functions), this mechanism fails.

## Foundational Learning

- Concept: Proper maps and measure theory
  - Why needed here: To show that the Hessian eigenvalues along the minima manifold grow without bound, ensuring instability when η is large
  - Quick check question: What does it mean for a function to be "proper" and why does this imply the function's preimage of compact sets is compact?

- Concept: Stable Manifold Theorem
  - Why needed here: To prove that the set of initializations converging to unstable minima has measure zero
  - Quick check question: How does the Stable Manifold Theorem relate the stability of fixed points to the measure of their stable sets?

- Concept: Determinantal varieties and manifold structure
  - Why needed here: To understand the geometry of the minima manifold M and its dimension
  - Quick check question: What is a determinantal variety and why does the rank condition on W* determine whether the map µd is filling or non-filling?

## Architecture Onboarding

- Component map:
  - Loss function L: Defines the optimization problem
  - Gradient descent map G: Implements the optimization algorithm
  - Hessian HL: Determines local curvature and stability
  - Minima manifold M: Set of global minima
  - Step size η: Controls convergence behavior

- Critical path:
  1. Compute Hessian HL(θ) at points in M
  2. Determine if eigenvalues of HL exceed 2/η for given η
  3. If yes, conclude G is unstable at all minima
  4. Apply Stable Manifold Theorem to show W s(M) has measure zero

- Design tradeoffs:
  - Linear vs. non-linear networks: Linear networks have polynomial gradient maps, enabling non-singularity proofs; non-linear networks may not
  - Bounded vs. unbounded losses: Bounded analytic losses cannot have constant Hessian eigenvalues (Proposition 18), enabling broader applicability of Theorem 17

- Failure signatures:
  - Convergence occurs for large η: Implies eigenvalues of HL do not grow without bound or G is non-singular despite large η
  - No clear critical ηE: Suggests eigenvalues of HL may not be proper maps or the loss function has special structure

- First 3 experiments:
  1. Verify that for linear networks with quadratic loss, the eigenvalues of HL along M grow without bound as ||θ|| → ∞
  2. Test gradient descent with increasing η on a simple linear network and measure the percentage of initializations converging to minima
  3. For a non-linear network with sigmoid activation, check if the gradient descent map is non-singular by computing det(DG(θ)) at various points θ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Theorem 17 hold for non-linear networks with activation functions that are not analytic, such as ReLU?
- Basis in paper: [inferred] The paper states that for ReLU activation, it is not clear if G is non-singular.
- Why unresolved: The proof of Theorem 17 relies on the analyticity of the loss function, which may not hold for non-analytic activation functions.
- What evidence would resolve it: A rigorous proof or counterexample showing whether the gradient descent map is non-singular for non-analytic activation functions.

### Open Question 2
- Question: Do the global minima of non-linear networks form a manifold, as they do for linear networks?
- Basis in paper: [inferred] The paper states that Cooper [11] showed that for non-linear networks in the interpolating regime, the minima form a manifold, but this result does not directly apply to non-linear networks that are not interpolating.
- Why unresolved: The structure of the loss landscape for non-linear networks that are not interpolating is not well understood.
- What evidence would resolve it: A proof or counterexample showing whether the global minima of non-linear networks that are not interpolating form a manifold.

### Open Question 3
- Question: What happens to the convergence properties of gradient descent when the loss functional l is changed from the quadratic loss?
- Basis in paper: [inferred] The paper states that Trager, Kohn, and Bruna [32] showed that if the quadratic loss is changed even infinitesimally, the properties of the loss landscape change drastically.
- Why unresolved: The impact of different loss functionals on the convergence properties of gradient descent is not well understood.
- What evidence would resolve it: A comprehensive study of the loss landscape for different loss functionals and their impact on the convergence properties of gradient descent.

## Limitations

- The theoretical guarantees primarily apply to analytic loss functions with non-constant Hessian eigenvalues, excluding certain structured problems
- The extension to non-linear networks is primarily empirical, with limited theoretical guarantees for general activation functions
- The paper focuses on gradient descent with fixed step size, not addressing adaptive methods or learning rate schedules

## Confidence

**High confidence**: The core result for linear networks with quadratic loss (Theorem 5) is mathematically rigorous and well-supported. The connection between large Hessian eigenvalues and instability at minima is clearly established.

**Medium confidence**: The extension to general analytic losses (Theorem 17) follows logically from the assumptions but requires careful verification of the non-constancy condition for Hessian eigenvalues in practical applications.

**Low confidence**: The empirical validation with non-linear networks demonstrates the phenomenon but doesn't provide theoretical guarantees for arbitrary network architectures or activation functions.

## Next Checks

1. **Verify eigenvalue growth**: For a simple linear regression problem, numerically compute the Hessian eigenvalues along the minima manifold as the norm of parameters increases, confirming they grow without bound.

2. **Measure zero convergence set**: Implement a Monte Carlo experiment with gradient descent on a quadratic loss function, varying the step size across the critical threshold, and empirically estimate the measure of initializations that converge to minima.

3. **Test non-singularity**: For a non-linear network with sigmoid activation, compute the determinant of the gradient descent map's Jacobian across parameter space to verify it's non-zero almost everywhere, supporting the theoretical framework.