---
ver: rpa2
title: Neuroplastic Expansion in Deep Reinforcement Learning
arxiv_id: '2410.07994'
source_url: https://arxiv.org/abs/2410.07994
tags:
- learning
- plasticity
- network
- training
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Neuroplastic Expansion (NE) to address plasticity
  loss in deep reinforcement learning. The method dynamically grows a small initial
  network to its full size throughout training, maintaining high levels of active
  neurons and adaptability.
---

# Neuroplastic Expansion in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.07994
- Source URL: https://arxiv.org/abs/2410.07994
- Reference count: 23
- Key outcome: Neuroplastic Expansion (NE) dynamically grows a small initial network to full size throughout training, maintaining high levels of active neurons and adaptability, outperforming state-of-the-art methods across MuJoCo and DeepMind Control Suite tasks.

## Executive Summary
This paper addresses plasticity loss in deep reinforcement learning by proposing Neuroplastic Expansion (NE), a method that dynamically grows a small initial network to its full size throughout training. NE maintains high levels of active neurons and adaptability through three key components: elastic neuron generation based on gradient potential, dormant neuron pruning for optimal network utilization, and neuron consolidation via experience review for policy stability. Extensive experiments demonstrate NE's effectiveness across various MuJoCo and DeepMind Control Suite tasks, outperforming state-of-the-art methods by maintaining higher activated neuron ratios and achieving superior performance.

## Method Summary
Neuroplastic Expansion starts with a network at 20% capacity and gradually expands it based on gradient potential, preventing neurons from becoming dormant. The method incorporates elastic neuron generation to introduce new connections that provide high-gradient stimuli to neurons nearing dormancy, dormant neuron pruning to optimize network expressivity by removing inactive neurons, and neuron consolidation via experience review to prevent catastrophic forgetting during later training stages.

## Key Results
- Maintains higher activated neuron ratios throughout training compared to static networks
- Achieves superior performance across MuJoCo and DeepMind Control Suite tasks
- Successfully mitigates plasticity loss while ensuring policy stability through dynamic expansion and consolidation mechanisms

## Why This Works (Mechanism)

### Mechanism 1
Dynamic network expansion from a smaller initial size maintains higher levels of active neurons throughout training. Starting with a network at 20% capacity and gradually expanding it based on gradient potential prevents neurons from becoming dormant. The expansion introduces new connections that provide high-gradient stimuli to neurons nearing dormancy, keeping them active. Core assumption: Neurons that receive strong gradient signals during backpropagation remain active and continue learning effectively. Evidence anchors: Section 4.1 demonstrates that even a naive incremental network growth can bring noticeable gains by alleviating neuron deactivation.

### Mechanism 2
Dormant neuron pruning reactivates inactive neurons and optimizes network expressivity. After each expansion phase, neurons with outputs below a threshold are pruned and returned to the candidate pool for potential reintroduction. This process frees up capacity and prevents the network from saturating with inactive neurons. Core assumption: Parameters forward-linked only to dormant neurons receive no gradient signals and are ineffective for learning. Evidence anchors: Section 4.2 explicitly states that dormant neurons "exhibit a stark decline in their representational potency while tenaciously occupying network capacity."

### Mechanism 3
Experience review prevents catastrophic forgetting by consolidating knowledge during later training stages. The system monitors the fluctuation rate of dormant neurons and triggers experience review when plasticity gains plateau. By sampling from earlier experiences when more neurons are dormant, the agent reviews prior knowledge to maintain policy stability. Core assumption: A decreasing rate of dormant neuron reduction indicates a bottleneck in plasticity gains and marks the later stage of policy learning. Evidence anchors: Section 4.2 describes monitoring "the fluctuation rate of dormant neurons ∇f(θ)" to determine when to review earlier experiences.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and reinforcement learning framework
  - Why needed here: The paper builds on standard RL formulations with states, actions, rewards, and policies. Understanding this foundation is critical for grasping how NE integrates with existing algorithms.
  - Quick check question: What are the components of an MDP tuple (S, A, P, R, γ) and how do they relate to the agent-environment interaction?

- **Concept**: Neural network activation functions and neuron dormancy
  - Why needed here: The paper's core mechanism relies on identifying dormant neurons (those with outputs below threshold) and maintaining active neurons. This requires understanding how neural networks process information.
  - Quick check question: How is a neuron defined as "dormant" in this paper, and what mathematical condition determines this status?

- **Concept**: Gradient-based optimization and backpropagation
  - Why needed here: NE's elastic neuron generation relies on selecting connections based on gradient magnitude. Understanding how gradients flow through networks is essential for grasping this mechanism.
  - Quick check question: Why would neurons receiving high-gradient stimuli during backpropagation be more likely to remain active and learn effectively?

## Architecture Onboarding

- **Component map**: Elastic neuron generation module -> Dormant neuron pruning module -> Experience review controller -> RL backbone (TD3/DrQ) -> Buffer management
- **Critical path**: 1. Compute gradient magnitudes across network, 2. Select top-k connections for expansion based on gradient potential, 3. Prune dormant neurons and return them to candidate pool, 4. Monitor dormant neuron fluctuation rate, 5. Trigger experience review when plasticity gains plateau, 6. Sample from historical experiences to consolidate knowledge
- **Design tradeoffs**: Network capacity vs. computational efficiency: Growing from 20% to full capacity increases computation but maintains plasticity; Pruning aggressiveness vs. retention of potentially useful neurons: Too aggressive pruning may remove neurons that could become active later; Experience review frequency vs. learning speed: More frequent review prevents forgetting but slows down learning from new experiences
- **Failure signatures**: Performance oscillation during later training stages (indicates insufficient experience review); Rapid increase in dormant neurons (suggests pruning is too aggressive or expansion insufficient); Stagnant learning progress (may indicate expansion rate is too slow or gradient-based selection is ineffective); Computational overhead becomes prohibitive (network growing too quickly or pruning not effective)
- **First 3 experiments**: 1. Implement naive random growth from 20% to 100% capacity and measure dormant neuron ratio and performance on HalfCheetah; 2. Add gradient-based expansion selection and compare dormant neuron retention vs. random growth; 3. Integrate dormant neuron pruning and measure the impact on network expressivity and final performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions remain unresolved based on the paper's content and methodology.

## Limitations
- Exact threshold values for dormant neuron detection and experience review activation are not specified
- Computational overhead of dynamic expansion versus static networks is not quantified
- Long-term stability across diverse task distributions requires further validation

## Confidence
- High confidence: The overall framework design and integration with standard RL algorithms
- Medium confidence: The effectiveness of gradient-based expansion for maintaining neuron activity
- Medium confidence: The pruning mechanism's ability to optimize network expressivity without losing useful neurons
- Medium confidence: The experience review system's ability to balance plasticity and stability

## Next Checks
1. Ablation study comparing NE's gradient-based expansion versus random expansion on a suite of continuous control tasks
2. Sensitivity analysis of pruning thresholds to determine optimal balance between network efficiency and retention of potentially useful neurons
3. Transfer learning evaluation to assess whether NE maintains plasticity advantages when adapting to new but related tasks