---
ver: rpa2
title: 'LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts'
arxiv_id: '2410.08211'
source_url: https://arxiv.org/abs/2410.08211
tags:
- image
- images
- clip
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LatteCLIP, a method for unsupervised fine-tuning
  of CLIP models on domain-specific datasets without human annotations. The key idea
  is to leverage Large Multimodal Models (LMMs) to generate expressive textual descriptions
  at multiple levels of contextual granularity - individual image, group of similar
  images, and entire class.
---

# LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts

## Quick Facts
- arXiv ID: 2410.08211
- Source URL: https://arxiv.org/abs/2410.08211
- Authors: Anh-Quan Cao; Maximilian Jaritz; Matthieu Guillaumin; Raoul de Charette; Loris Bazzani
- Reference count: 40
- Primary result: +4.74 points improvement in top-1 accuracy over pre-trained zero-shot CLIP on domain-specific datasets

## Executive Summary
LatteCLIP addresses the challenge of fine-tuning CLIP models on domain-specific datasets without human annotations by leveraging Large Multimodal Models (LMMs) to generate synthetic textual descriptions. The method operates at three levels of contextual granularity - individual image, group of similar images, and entire class - to create rich, expressive descriptions that capture domain-specific characteristics. To handle the inherent noise in LMM-generated texts, LatteCLIP employs a prototype-based learning framework with dual pseudo-labels from both frozen and fine-tuning CLIP models, along with a Dynamic Feature Mixer for optimal text feature weighting.

The approach is evaluated across 10 diverse domain-specific datasets, demonstrating an average improvement of +4.74 points in top-1 accuracy compared to pre-trained zero-shot CLIP methods and +3.45 points over other unsupervised fine-tuning baselines. By avoiding the need for costly human annotations while maintaining strong performance, LatteCLIP offers a practical solution for adapting vision-language models to specialized domains where labeled data is scarce.

## Method Summary
LatteCLIP fine-tunes CLIP models using LMM-generated synthetic texts at three levels: class-level template descriptions, individual image descriptions, and group descriptions of similar images. The method employs a prototype-based learning framework with dual pseudo-labels from both frozen and fine-tuning CLIP models to improve robustness to noisy synthetic texts. A Dynamic Feature Mixer optimally combines text features from different levels, while momentum updates maintain training stability. The approach uses contrastive loss for training over 2000 iterations or 50 epochs, demonstrating significant improvements across 10 domain-specific datasets compared to zero-shot and other unsupervised baselines.

## Key Results
- Average improvement of +4.74 points in top-1 accuracy over pre-trained zero-shot CLIP
- Outperforms other unsupervised fine-tuning baselines by +3.45 points
- Shows consistent improvements across 10 diverse domain-specific datasets including EuroSAT, SUN397, Food101, and UCF101
- Successfully handles noisy synthetic texts through dual pseudo-labeling and momentum updates

## Why This Works (Mechanism)
LatteCLIP works by addressing the fundamental challenge of unsupervised domain adaptation through multi-level synthetic text generation and robust learning framework. The method generates textual descriptions at class, image, and group levels using LMMs, capturing different aspects of visual content. The dual pseudo-labeling strategy, where both frozen and fine-tuning CLIP models contribute to pseudo-labels, creates a more stable training signal that's less susceptible to noise. The Dynamic Feature Mixer learns optimal weights for combining text features from different granularities, while momentum updates prevent the training from being derailed by occasional noisy pseudo-labels. This comprehensive approach allows the model to leverage rich LMM-generated descriptions while maintaining stability during fine-tuning.

## Foundational Learning
- **CLIP model architecture**: Vision transformer (ViT/B-32) with text encoder, needed to understand the base model being fine-tuned
  - Quick check: Verify understanding of image and text embedding dimensions
- **Large Multimodal Models (LMMs)**: Models like LLaVA 1.6 that generate cross-modal descriptions, needed to appreciate the source of synthetic texts
  - Quick check: Understand basic prompt engineering for LMMs
- **Prototype-based learning**: Using class prototypes in feature space for classification, needed to grasp the core learning framework
  - Quick check: Verify understanding of contrastive loss and prototype updates
- **Momentum updates**: Technique for stabilizing training by slowly updating parameters, needed to understand the stability mechanism
  - Quick check: Verify understanding of momentum parameter (µ) role
- **Dual pseudo-labeling**: Using multiple sources for pseudo-labels, needed to understand noise robustness
  - Quick check: Verify understanding of how frozen and fine-tuning models contribute
- **Dynamic Feature Mixing**: Learning optimal weights for combining multiple feature sources, needed to understand the text feature integration
  - Quick check: Verify understanding of how α parameter controls mixing

## Architecture Onboarding

**Component map**: LMM Synthetic Text Generator -> Dual Pseudo-Label Generator -> Dynamic Feature Mixer -> Prototype Memory -> Contrastive Loss -> CLIP Fine-tuning

**Critical path**: The essential sequence for achieving the primary results is: (1) Generate synthetic texts at three levels using LMM, (2) Create dual pseudo-labels from frozen and fine-tuning CLIP models, (3) Apply Dynamic Feature Mixer to combine text features, (4) Update prototypes using momentum, (5) Train with contrastive loss. Each component builds on the previous one, with the dual pseudo-labels and momentum updates being critical for handling noisy synthetic texts.

**Design tradeoffs**: The method trades computational cost (using LMM for text generation) for the benefit of avoiding human annotation costs. The dual pseudo-labeling adds complexity but significantly improves robustness to noise. The three-level text generation provides richer descriptions but requires careful feature mixing. The momentum update slows down prototype adaptation but prevents catastrophic forgetting from noisy signals.

**Failure signatures**: 
- If performance is similar to zero-shot CLIP, likely issues include poor synthetic text quality, incorrect pseudo-label generation, or inadequate feature mixing
- If training diverges or shows high variance, likely issues include improper momentum parameter setting or unstable contrastive loss
- If improvements are inconsistent across datasets, likely issues include dataset-specific challenges in synthetic text generation or prototype initialization

**First experiments**:
1. Test synthetic text generation quality on a small subset of images from EuroSAT dataset to verify LMM output
2. Validate dual pseudo-label generation by checking consistency between frozen and fine-tuning CLIP model outputs
3. Verify Dynamic Feature Mixer implementation by testing different α values on a validation set

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of LatteCLIP scale with larger LMM models for text generation?
- Basis in paper: [explicit] The paper notes that LatteCLIP's performance is constrained by the underlying LMM model, and improvements could be made with better models in the future.
- Why unresolved: The current experiments use a specific LMM (LLaVA 1.6), but the paper does not explore the impact of using larger or more capable LMMs.
- What evidence would resolve it: Experiments comparing LatteCLIP's performance using different LMM models of varying sizes and capabilities, measuring the trade-off between performance gains and computational costs.

### Open Question 2
- Question: What is the impact of different types of group descriptions on LatteCLIP's performance?
- Basis in paper: [inferred] The paper mentions generating group descriptions by collaging images with the same pseudo-label, but doesn't explore variations in how these groups are formed or what types of group descriptions are most effective.
- Why unresolved: The paper only considers one method of creating group descriptions, leaving open the question of whether alternative approaches (e.g., using more diverse image sets, different collaging strategies) might yield better results.
- What evidence would resolve it: Comparative experiments testing different methods of generating group descriptions and their impact on classification accuracy across various datasets.

### Open Question 3
- Question: How does LatteCLIP perform in semi-supervised learning scenarios where a small amount of labeled data is available?
- Basis in paper: [explicit] The paper focuses on unsupervised fine-tuning and mentions that acquiring human-annotated labels is costly, but doesn't explore the middle ground of semi-supervised learning.
- Why unresolved: The paper's scope is limited to fully unsupervised settings, leaving open the question of how LatteCLIP would benefit from or be adapted to scenarios with limited labeled data.
- What evidence would resolve it: Experiments comparing LatteCLIP's performance in fully unsupervised, semi-supervised (with varying amounts of labeled data), and fully supervised settings, measuring the trade-offs between annotation costs and performance gains.

## Limitations
- The method's performance is constrained by the quality of the underlying LMM model used for text generation
- Specific LLaVA 1.6 prompt templates for synthetic text generation are not fully specified, affecting reproducibility
- Exact hyperparameters for momentum update (µ value) and Dynamic Feature Mixer (α value) are not completely detailed

## Confidence
- High confidence: The core methodology (prototype-based learning with dual pseudo-labels, Dynamic Feature Mixer, momentum update) is clearly described and logically sound
- Medium confidence: The quantitative results showing improvements over baselines, though the exact magnitude depends on implementation details that are not fully specified
- Medium confidence: The claim about handling noisy synthetic texts through the proposed framework, as this depends on specific hyperparameter choices

## Next Checks
1. Reproduce the results on EuroSAT and Food101 datasets first, as these are the most distinct domains that would test the method's generalization capability
2. Implement ablation studies to verify the contribution of each component (dual pseudo-labels, Dynamic Feature Mixer, momentum update) by removing them individually
3. Test the sensitivity of results to different values of the momentum parameter µ and Dynamic Feature Mixer parameter α within the stated ranges to understand their impact on performance