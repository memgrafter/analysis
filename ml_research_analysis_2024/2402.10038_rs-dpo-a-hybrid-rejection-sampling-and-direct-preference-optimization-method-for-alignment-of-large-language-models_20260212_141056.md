---
ver: rpa2
title: 'RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method
  for Alignment of Large Language Models'
arxiv_id: '2402.10038'
source_url: https://arxiv.org/abs/2402.10038
tags:
- reward
- proposed
- preference
- pythia-6
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hybrid method called RS-DPO that combines
  rejection sampling (RS) and direct preference optimization (DPO) for efficient reinforcement
  learning from human feedback (RLHF) in large language model alignment. RS-DPO generates
  a diverse set of responses for each prompt, selects contrastive pairs based on reward
  distribution, and then applies DPO to align the model to human preferences.
---

# RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

## Quick Facts
- arXiv ID: 2402.10038
- Source URL: https://arxiv.org/abs/2402.10038
- Authors: Saeed Khaki; JinJin Li; Lan Ma; Liu Yang; Prathap Ramachandra
- Reference count: 17
- One-line primary result: RS-DPO achieves higher scores on MT-Bench and AlpacaEval benchmarks compared to PPO, DPO, and RS while requiring fewer computational resources.

## Executive Summary
RS-DPO is a hybrid method that combines rejection sampling (RS) and direct preference optimization (DPO) for efficient reinforcement learning from human feedback (RLHF) in large language model alignment. The method generates diverse responses for each prompt, selects contrastive pairs based on reward distribution, and applies DPO to align the model to human preferences. RS-DPO outperforms existing approaches like RS, PPO, and DPO in terms of alignment quality while being less resource-intensive and more stable, achieving higher scores on MT-Bench and AlpacaEval benchmarks with reduced computational requirements.

## Method Summary
RS-DPO combines rejection sampling and direct preference optimization to generate synthetic preference pairs from LLM responses and align models to human preferences. The method first fine-tunes a base model using supervised fine-tuning on instruction-response pairs, then trains a reward model on human preference data. For each prompt, k responses are generated from the SFT model and ranked using the reward model. Contrastive pairs are selected based on reward gaps exceeding a threshold, and DPO is applied using these pairs. This offline approach reduces GPU memory requirements compared to PPO while maintaining or improving alignment quality, and shows robustness against reward model quality variations.

## Key Results
- RS-DPO achieves higher MT-Bench and AlpacaEval scores than PPO, DPO, and RS methods
- The method requires fewer computational resources and less GPU memory than PPO
- RS-DPO demonstrates improved stability and robustness against reward model quality variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RS-DPO combines the efficiency of DPO with the diversity of rejection sampling to generate high-quality preference pairs.
- Mechanism: The method generates k responses per prompt from the SFT model, ranks them using a reward model, and selects pairs based on reward gaps exceeding a threshold. This creates a diverse preference dataset that reflects the model's actual behavior rather than relying on external annotations or alternative models.
- Core assumption: The reward model can effectively differentiate response quality, and the SFT model's sampling distribution captures meaningful variations in response quality.
- Evidence anchors:
  - [abstract] "RS-DPO generates a diverse set of responses for each prompt, selects contrastive pairs based on reward distribution, and then applies DPO to align the model to human preferences."
  - [section 2.3] "We first generate k distinct responses from LSFT model for each prompt x. Then, we evaluate the quality of each response using our trained reward model R(x, y). Finally, we compute the reward gap for all possible pairwise combinations of responses per prompt."
  - [corpus] Found 25 related papers with average neighbor FMR=0.537, suggesting moderate relevance in the literature space for preference optimization methods.

### Mechanism 2
- Claim: RS-DPO reduces GPU memory requirements compared to PPO while maintaining or improving alignment quality.
- Mechanism: By using offline sampling and DPO (which doesn't require a separate reward model during training), RS-DPO avoids loading three models simultaneously as PPO does. The offline generation also reduces real-time computational overhead.
- Core assumption: The computational efficiency gains from offline sampling and avoiding reward model training outweigh any potential loss in alignment quality.
- Evidence anchors:
  - [abstract] "The method is shown to outperform existing approaches like RS, PPO, and DPO in terms of alignment quality while being less resource-intensive and more stable."
  - [section 5] "PPO represents an unstable process prone to sensitivity towards reward model quality and hyperparameters... Our proposed method exhibits robustness against reward model quality, requiring only a single run to train each model successfully."
  - [corpus] Found papers on "Efficient rlhf" and "Zero: Memory optimizations toward training trillion parameter models" suggesting this is an active research area.

### Mechanism 3
- Claim: RS-DPO improves stability and robustness against reward model quality variations compared to PPO.
- Mechanism: By generating preference pairs directly from the SFT model rather than relying on dynamic online sampling, RS-DPO creates a more stable training signal. The method also shows robustness across different reward models.
- Core assumption: The stability gains from offline sampling and the use of DPO's implicit reward signal outweigh the benefits of PPO's online sampling and explicit reward optimization.
- Evidence anchors:
  - [abstract] "Our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO."
  - [section 4] "The performance of PPO on Anthropic/HH-RLHF surpasses that of other methods... However, the performance of PPO on MT-Bench average scores declines when applied to WebGPT... Our proposed method outperforms PPO."
  - [corpus] Found papers on "Mitigating Reward Over-optimization in Direct Alignment Algorithms" suggesting this is a recognized challenge.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT provides the base model from which responses are sampled for preference pair generation. Without a well-trained SFT model, the subsequent alignment steps would have poor starting quality.
  - Quick check question: What is the purpose of the SFT step in the RS-DPO pipeline?

- Concept: Reward Modeling
  - Why needed here: The reward model evaluates response quality to enable selection of contrastive pairs. It transforms human preferences into a scalar signal that can guide model selection.
  - Quick check question: How does the reward model contribute to the preference data generation process?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO enables efficient fine-tuning without requiring a separate reward model during training, reducing computational overhead while maintaining alignment quality.
  - Quick check question: What distinguishes DPO from traditional RLHF approaches like PPO?

## Architecture Onboarding

- Component map: SFT model -> Reward model -> RS-DPO pipeline -> Preference dataset
- Critical path:
  1. Train SFT model on instruction-response pairs
  2. Train reward model on human preference data
  3. Generate k responses per prompt from SFT
  4. Select contrastive pairs based on reward gaps
  5. Apply DPO using the generated preference pairs

- Design tradeoffs:
  - Memory vs. performance: Offline sampling reduces memory requirements but may introduce bias
  - Sample diversity vs. computational cost: Larger k increases diversity but also computational overhead
  - Threshold sensitivity: Lower thresholds generate more data but may include noisier pairs

- Failure signatures:
  - Poor reward model quality leading to suboptimal pair selection
  - Insufficient diversity in SFT-generated responses
  - Overfitting to synthetic preference data
  - Threshold miscalibration causing either too few or too noisy pairs

- First 3 experiments:
  1. Verify SFT model generates diverse, coherent responses across a range of prompts
  2. Test reward model's ability to differentiate response quality on held-out data
  3. Validate preference pair generation by examining reward gap distributions and selected pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RS-DPO scale with model size compared to PPO and other baselines?
- Basis in paper: [inferred]
- Why unresolved: The paper only reports results on Llama-2-7B. Scaling behavior to larger models like Llama-2-70B or GPT-3.5 is unknown.
- What evidence would resolve it: Experiments on multiple model sizes showing performance trends and resource usage comparisons.

### Open Question 2
- Question: What is the impact of varying the number of responses k generated per prompt on RS-DPO's performance?
- Basis in paper: [explicit]
- Why unresolved: The paper uses k=16 but doesn't explore how performance changes with different k values.
- What evidence would resolve it: Systematic ablation studies with varying k values and corresponding performance metrics.

### Open Question 3
- Question: How does RS-DPO perform on harmlessness and safety objectives compared to helpfulness?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on helpfulness datasets but doesn't evaluate harmlessness or safety aspects.
- What evidence would resolve it: Experiments using harmlessness-focused datasets and corresponding safety benchmarks.

### Open Question 4
- Question: What is the optimal threshold η value across different reward model qualities and dataset types?
- Basis in paper: [explicit]
- Why unresolved: The paper shows η affects performance but doesn't provide a systematic method for determining optimal values.
- What evidence would resolve it: Grid search results showing performance vs. η across multiple reward model qualities and datasets.

### Open Question 5
- Question: How does RS-DPO handle out-of-distribution prompts compared to PPO and other methods?
- Basis in paper: [inferred]
- Why unresolved: The paper only evaluates on in-distribution prompts from the datasets used for training.
- What evidence would resolve it: Experiments with prompts from different distributions and corresponding performance metrics.

## Limitations

- The paper lacks ablation studies isolating the individual contributions of rejection sampling versus the specific DPO implementation.
- The reward model quality assumption is critical but not thoroughly tested with degraded reward models.
- Offline sampling may introduce bias that isn't captured by the benchmark evaluations.

## Confidence

**High Confidence**: The claim that RS-DPO requires fewer computational resources than PPO is well-supported by the architectural differences described. The combination of offline sampling and DPO's lack of a separate reward model during training provides a clear mechanism for reduced memory usage.

**Medium Confidence**: The claim that RS-DPO outperforms existing methods on MT-Bench and AlpacaEval benchmarks is supported by the presented results, but lacks comprehensive ablation studies and edge case analysis. The stability improvements over PPO are plausible given the architectural differences but need more rigorous validation.

**Low Confidence**: The claim about improved alignment with user intent is difficult to verify without qualitative analysis of the generated responses. The paper relies heavily on benchmark scores which may not fully capture alignment quality, particularly for nuanced or context-dependent preferences.

## Next Checks

1. **Ablation Study Validation**: Run experiments comparing RS-DPO against variants with: (a) only rejection sampling without DPO, (b) only DPO without rejection sampling, and (c) different threshold values for preference pair selection. This would isolate which components drive the performance improvements and verify the hybrid approach is synergistic rather than additive.

2. **Reward Model Robustness Test**: Evaluate RS-DPO performance using reward models of varying quality, including deliberately degraded reward models, to quantify the method's sensitivity to reward model accuracy. This would validate the claimed robustness against reward model quality variations and identify failure modes.

3. **Memory Profiling Comparison**: Conduct detailed GPU memory profiling of RS-DPO versus PPO and DPO under identical hardware conditions, measuring peak memory usage, memory efficiency during training, and inference-time memory requirements. This would provide empirical validation of the computational efficiency claims and identify potential bottlenecks.