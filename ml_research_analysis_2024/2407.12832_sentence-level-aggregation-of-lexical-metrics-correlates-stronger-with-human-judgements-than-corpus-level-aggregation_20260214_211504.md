---
ver: rpa2
title: Sentence-level Aggregation of Lexical Metrics Correlates Stronger with Human
  Judgements than Corpus-level Aggregation
arxiv_id: '2407.12832'
source_url: https://arxiv.org/abs/2407.12832
tags:
- metrics
- bleu
- scores
- aggregation
- chrf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that the widely-used corpus-level aggregation
  (CLA) for lexical metrics like BLEU and chrF is statistically weak and less correlated
  to human judgements compared to segment-level aggregation (SLA). The authors demonstrate
  that CLA's ratio of averages differs mathematically from SLA's average of ratios,
  leading to biased results weighted by sentence length.
---

# Sentence-level Aggregation of Lexical Metrics Correlates Stronger with Human Judgements than Corpus-level Aggregation

## Quick Facts
- arXiv ID: 2407.12832
- Source URL: https://arxiv.org/abs/2407.12832
- Authors: Paulo Cavalin; Pedro Henrique Domingues; Claudio Pinhanez
- Reference count: 40
- Primary result: Segment-level aggregation (SLA) for lexical metrics correlates significantly stronger with human judgments than corpus-level aggregation (CLA)

## Executive Summary
This paper demonstrates that corpus-level aggregation (CLA) of lexical metrics like BLEU and chrF is statistically weak and produces biased results due to the mathematical difference between averaging ratios versus taking ratios of averages. The authors show that SLA, which treats each sentence equally, correlates much more strongly with human judgments and neural metrics like BERTScore than CLA. Through experiments on 492 MT systems from WMT23, SLA-based metrics achieved correlations exceeding 0.9 with human annotations, while CLA-based metrics lagged significantly behind. The findings recommend adopting SLA for more trustworthy MT evaluation, particularly for low-resource languages where neural metrics are not applicable.

## Method Summary
The authors compared three aggregation methods for lexical metrics: corpus-level aggregation (CLA), segment-level aggregation (SLA), and bootstrap resampling (BRS). Using the WMT23 metrics shared task dataset with 492 system outputs and human judgments (MQM, DA), they implemented BLEU and chrF scores using both CLA (corpus_bleu) and SLA (sentence_bleu) methods from SacreBLEU. The evaluation measured Pearson correlations between metric scores and human judgments, comparing CLA, SLA, and neural metrics (COMET, BLEURT, BERTScore). They also conducted downsampling experiments to assess statistical robustness, comparing SLA and CLA correlations with BRS scores across different sample sizes.

## Key Results
- SLA-based m-BLEU and m-chrF achieved correlations exceeding 0.9 with human judgments, outperforming CLA-based BLEU and chrF
- CLA's correlation to bootstrap resampling scores did not improve with larger test sets, indicating statistical fragility
- SLA's correlation to neural metrics like BERTScore was significantly stronger than CLA, reducing the evaluation gap
- Downsampling experiments showed SLA correlations with BRS increased substantially with sample size, while CLA correlations remained stagnant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Corpus-level aggregation (CLA) biases BLEU/chrF scores toward long sentences, distorting system rankings.
- Mechanism: CLA weights each sentence's n-gram match ratio by its length, so long sentences with many n-grams disproportionately influence the global score. Segment-level aggregation (SLA) treats each sentence equally, avoiding this bias.
- Core assumption: The mathematical difference between average-of-ratios (SLA) and ratio-of-averages (CLA) is non-trivial and affects system-level correlations.
- Evidence anchors:
  - [abstract] "corpus-level aggregation hinders considerably the capability of lexical metrics... because corpus- and segment-level aggregation differs considerably owing to the classical average of ratio versus ratio of averages."
  - [section 3.2] Derivation showing BLEU = weighted average of sentence ratios by proportional length.
- Break condition: If sentence length distributions are uniform across systems, the bias diminishes.

### Mechanism 2
- Claim: SLA produces statistically robust scores comparable to bootstrap resampling (BRS), unlike CLA.
- Mechanism: SLA's averaging of segment scores yields a distribution that stabilizes with test set size, whereas CLA's global ratio is sensitive to sample composition and size, making it statistically fragile.
- Core assumption: BRS is a valid upper-bound reference for statistical robustness.
- Evidence anchors:
  - [section 4.6] Correlation of downsampled SLA scores with BRS increases substantially with sample size; CLA correlations do not.
  - [abstract] "SLA correlates much stronger with human judgements and to neural metrics such as COMET and BLEURT."
- Break condition: If the test set is extremely small (e.g., N=1), both SLA and CLA lose robustness.

### Mechanism 3
- Claim: SLA reduces the evaluation gap between lexical and neural metrics by aligning more closely with human judgments.
- Mechanism: By eliminating corpus-level length bias, SLA's scores reflect true translation quality per segment, matching human assessment patterns better than CLA, which is skewed.
- Core assumption: Human judgments are segment-level judgments, not corpus-level ratios.
- Evidence anchors:
  - [section 5] SLA-based m-BLEU and m-chrF rank higher in correlation to MQM and DA human scores than CLA-based BLEU and chrF.
  - [abstract] "SLA correlates much stronger with human judgements, and are much more comparable to the outcomes of BERTScore."
- Break condition: If neural metrics are trained on corpus-level labels, the gap may persist.

## Foundational Learning

- Concept: Average of ratios vs ratio of averages
  - Why needed here: This mathematical distinction explains why CLA and SLA yield different scores.
  - Quick check question: Given two sentences with ratios 0.8 and 0.4 and lengths 10 and 2, compute CLA vs SLA scores and compare.

- Concept: Pearson correlation for metric alignment
  - Why needed here: Used to quantify how well SLA matches human judgments and neural metrics.
  - Quick check question: If two metrics have correlation 0.9, what does that say about their system rankings?

- Concept: Bootstrap resampling for robustness
  - Why needed here: Serves as a benchmark to test SLA's statistical robustness against CLA.
  - Quick check question: If resampling 1000 times yields similar mean scores to a single pass, what does that imply about stability?

## Architecture Onboarding

- Component map:
  - Data ingestion: WMT23 MT system outputs, references, human judgments (MQM, DA)
  - Metric implementations: BLEU/chrF in three variants (CLA, SLA, BRS)
  - Evaluation pipeline: Compute correlations, generate plots, downsample for robustness tests
  - Output: Correlation tables, distribution plots, recommendation report

- Critical path:
  1. Load and group WMT23 dataset into system-level sets
  2. Implement SLA and BRS versions of BLEU/chrF
  3. Compute all pairwise correlations between metric variants
  4. Run downsampling experiments for robustness analysis
  5. Compare metrics against human judgments
  6. Generate final recommendation

- Design tradeoffs:
  - Computational cost: BRS is expensive (1000 resamples) vs SLA is cheap
  - Statistical rigor: BRS is more robust but SLA is nearly as good
  - Interpretability: CLA scores are globally intuitive but misleading; SLA is segment-intuitive and accurate

- Failure signatures:
  - CLA vs SLA correlation < 0.7 indicates high bias
  - SLA vs BRS correlation does not increase with sample size indicates sampling bias
  - Poor human correlation suggests metric misalignment

- First 3 experiments:
  1. Compute BLEU and chrF with CLA and SLA on a small system subset; compare distributions and correlations
  2. Downsample a dataset to N=1,10,100; compute CLA and SLA scores; plot correlation decay
  3. Run SLA and BRS on a held-out test set; measure runtime and correlation difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does segment-level aggregation (SLA) provide statistically robust evaluation scores across different MT tasks beyond machine translation?
- Basis in paper: [explicit] The paper demonstrates SLA's statistical robustness for MT evaluation using bootstrap resampling as a reference, showing SLA correlates strongly with BRS and outperforms corpus-level aggregation.
- Why unresolved: The experiments focus specifically on machine translation tasks using BLEU and chrF metrics. The paper does not explore whether SLA's statistical robustness extends to other natural language processing tasks like text summarization or question answering.
- What evidence would resolve it: Empirical experiments applying SLA to other NLP evaluation metrics (e.g., ROUGE for summarization, Exact Match/F1 for QA) across multiple datasets and tasks, comparing correlations with human judgments and statistical robustness against CLA.

### Open Question 2
- Question: How does the performance of SLA-based metrics vary across different language families and linguistic structures?
- Basis in paper: [inferred] The paper uses a dataset with 147 language pairs but does not analyze performance variations across specific language families or linguistic features (e.g., agglutinative vs. fusional languages, languages with different word orders).
- Why unresolved: The experiments aggregate results across all language pairs without examining whether SLA's advantages hold consistently across different linguistic contexts or whether certain language families benefit more from SLA.
- What evidence would resolve it: Detailed analysis of SLA performance stratified by language family, morphological typology, and syntactic features, comparing correlations with human judgments across different language groups.

### Open Question 3
- Question: What is the optimal segment size for SLA to maximize correlation with human judgments while maintaining computational efficiency?
- Basis in paper: [inferred] The paper uses sentence-level aggregation as SLA but does not explore whether aggregating at different granularities (e.g., phrase-level, document-level) would yield better results.
- Why unresolved: The experiments assume sentence-level aggregation is optimal but do not test alternative segment sizes or hierarchical aggregation methods that might better capture translation quality at different levels.
- What evidence would resolve it: Systematic experiments varying segment sizes (words, phrases, sentences, paragraphs) and testing hierarchical aggregation approaches, measuring correlations with human judgments and computational costs for each configuration.

## Limitations

- The analysis is restricted to two lexical metrics (BLEU and chrF) and one test set (WMT23), limiting generalizability to other evaluation contexts
- The paper doesn't address whether SLA's advantages persist when combined with other metric improvements like document-level evaluation
- The choice of 1000 resamples and specific downsampling strategy could affect the statistical robustness analysis results

## Confidence

- High: CLA's mathematical bias toward long sentences is correctly identified and demonstrated
- Medium: SLA's superior correlation with human judgments is well-supported but limited to tested datasets
- Medium: Statistical robustness claims are valid but dependent on specific experimental parameters

## Next Checks

1. **Cross-dataset validation**: Replicate the CLA vs SLA comparison on WMT22, WMT24, and other established MT evaluation datasets to verify consistency of correlation improvements across different years and language pairs.

2. **Multi-metric benchmarking**: Extend the analysis to include other lexical metrics (TER, METEOR, chrF++) and document-level variants to determine if SLA advantages generalize beyond BLEU and chrF.

3. **Domain-specific testing**: Evaluate CLA and SLA performance on specialized domains (medical, legal, technical) where sentence length distributions and translation quality patterns may differ significantly from newswire data typically used in WMT tasks.