---
ver: rpa2
title: Exploring Large Language Models for Relevance Judgments in Tetun
arxiv_id: '2406.07299'
source_url: https://arxiv.org/abs/2406.07299
tags:
- relevance
- llms
- score
- tetun
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for automating relevance judgments in Tetun, a low-resource language. The study
  employed the LLaMA3 70B model to assign relevance scores to query-document pairs
  in Tetun, comparing the results with human annotations.
---

# Exploring Large Language Models for Relevance Judgments in Tetun

## Quick Facts
- arXiv ID: 2406.07299
- Source URL: https://arxiv.org/abs/2406.07299
- Authors: Gabriel de Jesus; Sérgio Nunes
- Reference count: 29
- Primary result: LLaMA3 70B achieved fair inter-annotator agreement (kappa = 0.2634) with humans on Tetun relevance judgments

## Executive Summary
This paper investigates the use of large language models (LLMs) for automating relevance judgments in Tetun, a low-resource language. The study employed the LLaMA3 70B model to assign relevance scores to query-document pairs in Tetun, comparing results with human annotations. The LLaMA3 model achieved a Cohen's kappa score of 0.2634, demonstrating a fair level of agreement with human assessors. This result is comparable to studies conducted in high-resource languages, indicating the feasibility of using LLMs for relevance judgments in low-resource language contexts.

## Method Summary
The study used the LLaMA3 70B model with few-shot prompting to assign relevance scores (0-3) to 6,100 query-document pairs from a Tetun test collection. Four example queries with reasoning patterns were provided to establish scoring criteria. The model was run with zero temperature settings to ensure consistent outputs. Results were compared against human annotations using Cohen's kappa score to measure inter-annotator agreement. The same methodology was applied to Claude 3 Haiku and GPT-3.5 Turbo models for comparison.

## Key Results
- LLaMA3 70B achieved a Cohen's kappa score of 0.2634 with human assessors
- Zero temperature settings (0.0) provided optimal reliability for relevance judgments
- Results align closely with studies in high-resource languages
- Few-shot prompting with one example per relevance level helped establish scoring rubric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve fair inter-annotator agreement with humans on relevance judgments in low-resource languages.
- Mechanism: The LLaMA3 70B model uses few-shot prompting to understand relevance scoring conventions and applies them consistently to Tetun query-document pairs.
- Core assumption: The model's language understanding generalizes sufficiently to Tetun despite limited training data in that language.
- Evidence anchors:
  - The LLaMA3 model achieved a Cohen's kappa score of 0.2634, demonstrating a fair level of agreement with human assessors.
  - "Our investigation reveals results that align closely with those reported in studies of high-resource languages."
- Break condition: If the model's language understanding fails to capture the semantic nuances of Tetun, agreement scores would drop below fair levels (kappa < 0.2).

### Mechanism 2
- Claim: Zero temperature settings provide more reliable relevance judgments than higher temperature settings.
- Mechanism: Fixed temperature prevents random variation in outputs, leading to more consistent relevance scoring across repeated evaluations.
- Core assumption: Temperature settings directly affect the stability of relevance judgment outputs.
- Evidence anchors:
  - "When we increased the temperature of LLaMA3 70B model, the results were not satisfactory. Therefore, we opted to use a zero temperature setting."
- Break condition: If zero temperature produces overly conservative or rigid judgments that miss nuanced relevance distinctions, the system would fail to capture meaningful variations in relevance.

### Mechanism 3
- Claim: Few-shot prompting with examples of all relevance levels helps LLMs understand the scoring rubric.
- Mechanism: Providing one example for each score level (0-3) establishes clear boundaries and expectations for the relevance assessment task.
- Core assumption: The examples provided are representative and clear enough for the model to generalize the scoring criteria.
- Evidence anchors:
  - "These examples used the same queries as those utilized in the pilot testing phase by human assessors, including the relevance score and the reasoning behind each score."
- Break condition: If the examples are ambiguous or not representative of the actual query-document pairs, the model would apply incorrect relevance scores consistently.

## Foundational Learning

- Concept: Cohen's kappa score
  - Why needed here: It quantifies the agreement between LLM-generated and human relevance judgments, accounting for chance agreement.
  - Quick check question: If two annotators agree on 80 out of 100 items by chance alone, what is their kappa score?
  - Answer: Kappa would be 0, as there is no agreement beyond chance.

- Concept: Few-shot prompting
  - Why needed here: It enables LLMs to understand task requirements without extensive fine-tuning, crucial for low-resource languages.
  - Quick check question: How many examples per relevance level were provided to the LLM in this study?
  - Answer: Four examples were provided, one for each relevance level (0-3).

- Concept: Temperature in LLMs
  - Why needed here: Temperature controls output randomness, affecting consistency of relevance judgments.
  - Quick check question: What temperature setting was found optimal for this relevance judgment task?
  - Answer: Zero temperature (0.0) was optimal, as higher temperatures decreased inter-annotator agreement.

## Architecture Onboarding

- Component map: Input layer (query-document pairs) → Prompt generator (few-shot examples) → LLM inference (relevance scoring) → Output formatter (JSON) → Evaluation module (kappa calculation)
- Critical path: Prompt generation → LLM inference → JSON formatting → kappa calculation
- Design tradeoffs: Using free LLaMA3 70B vs paid models balances cost against potential performance gains
- Failure signatures: Low kappa scores (<0.2), inconsistent outputs across runs, translation quality issues affecting judgment quality
- First 3 experiments:
  1. Test different temperature settings (0.0, 0.5, 1.0) on a small subset of data to measure consistency
  2. Vary the number of few-shot examples (1-4 per relevance level) to find optimal prompt structure
  3. Compare zero-shot vs few-shot performance to quantify the value of examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications to LLMs could enhance their performance in relevance judgment tasks for low-resource languages like Tetun?
- Basis in paper: The paper mentions that further improvement in LLM capabilities is necessary for fully automated relevance judgments, and that the LLaMA3 70B model achieved a fair but not excellent agreement with human assessors.
- Why unresolved: The paper does not explore potential modifications to LLM architectures or training processes that could improve their effectiveness in low-resource language contexts.
- What evidence would resolve it: Experiments comparing different LLM architectures or training strategies specifically designed for low-resource languages, showing improvements in relevance judgment accuracy.

### Open Question 2
- Question: How does the performance of LLMs in relevance judgment tasks vary across different low-resource languages, and what factors contribute to these differences?
- Basis in paper: The paper explores the use of LLMs for relevance judgments in Tetun, a low-resource language, and finds comparable results to studies in high-resource languages.
- Why unresolved: The paper only investigates one low-resource language (Tetun) and does not compare performance across multiple low-resource languages or identify factors that might influence LLM performance.
- What evidence would resolve it: Comparative studies of LLM performance in relevance judgment tasks across multiple low-resource languages, identifying language-specific features that impact LLM effectiveness.

### Open Question 3
- Question: What is the long-term impact of using LLM-generated relevance judgments on the development and evaluation of information retrieval systems in low-resource languages?
- Basis in paper: The paper demonstrates the feasibility of using LLMs for relevance judgments in low-resource languages but does not explore the broader implications for IR system development.
- Why unresolved: The paper focuses on the immediate task of relevance judgment and does not consider how reliance on LLM-generated judgments might affect the quality and evolution of IR systems over time.
- What evidence would resolve it: Longitudinal studies comparing IR systems developed using LLM-generated relevance judgments with those using human judgments, assessing differences in system performance and user satisfaction over extended periods.

## Limitations
- Fair but not excellent agreement (kappa = 0.2634) suggests LLMs cannot fully replace human judgment yet
- Results limited to one LLM model and one low-resource language, limiting generalizability
- Effectiveness of few-shot prompting untested against alternative approaches like fine-tuning

## Confidence
- High Confidence: LLaMA3 70B achieves fair inter-annotator agreement with humans; zero temperature settings provide optimal reliability; few-shot prompting helps LLMs understand scoring rubric
- Medium Confidence: Results are comparable to high-resource language studies; LLMs are feasible for low-resource language relevance judgments
- Low Confidence: Findings generalize to other low-resource languages; other LLM architectures would achieve similar performance; methodology scales to larger collections

## Next Checks
1. Cross-linguistic validation: Replicate the study with 2-3 additional low-resource languages (e.g., Samoan, Māori, or Haitian Creole) to assess whether the kappa score of 0.2634 is consistent across different language families and orthographic systems.

2. Temperature sensitivity analysis: Systematically test temperature values from 0.0 to 1.0 in 0.1 increments on the same Tetun dataset to map the full relationship between temperature and inter-annotator agreement, identifying whether 0.0 is truly optimal or if intermediate values might yield better results.

3. Prompt engineering comparison: Compare few-shot prompting against zero-shot prompting with carefully engineered instructions and chain-of-thought reasoning prompts to quantify the marginal value of few-shot examples versus better prompt design in low-resource language contexts.