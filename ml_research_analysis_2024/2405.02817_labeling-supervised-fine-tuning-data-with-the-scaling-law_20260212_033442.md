---
ver: rpa2
title: Labeling supervised fine-tuning data with the scaling law
arxiv_id: '2405.02817'
source_url: https://arxiv.org/abs/2405.02817
tags:
- data
- lora
- qwen1
- b-chat
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-stage manual annotation process guided
  by scaling laws to create high-quality Supervised Fine-Tuning (SFT) data for Large
  Language Models (LLMs) in constrained resource environments. The authors preprocessed
  58,000 chat messages and manually annotated 2,300 questions, using the scaling behavior
  of LLMs as an objective quality metric.
---

# Labeling supervised fine-tuning data with the scaling law

## Quick Facts
- arXiv ID: 2405.02817
- Source URL: https://arxiv.org/abs/2405.02817
- Reference count: 3
- Primary result: Qwen models fine-tuned with LoRA on scaling law-guided SFT data show F1 score improvements up to 29.07% (from 32.07% to 61.93% for Qwen1.5-MoE-2.7B-Chat).

## Executive Summary
This paper introduces a multi-stage manual annotation process guided by scaling laws to create high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs) in constrained resource environments. The authors preprocessed 58,000 chat messages and manually annotated 2,300 questions, using the scaling behavior of LLMs as an objective quality metric. Qwen models ranging from 0.5B to 32B parameters were fine-tuned using LoRA, achieving significant F1 score improvements. The scaling law-guided annotation process ensured data quality, as fine-tuned models showed performance improvements consistent with scaling laws. The dataset and models are open-sourced, contributing a method for acquiring high-quality SFT data and demonstrating the viability of fine-tuning LLMs for downstream NLP tasks.

## Method Summary
The authors preprocessed raw chat data, manually annotated questions using scaling laws as a quality metric, and fine-tuned Qwen models with LoRA. The annotation process involved iteratively testing baseline models and revising annotations until performance scaled predictably with model size. The annotated data was converted to alpaca format and used for fine-tuning, with F1 score improvements serving as the primary evaluation metric.

## Key Results
- Qwen1.5-MoE-2.7B-Chat F1 score improved from 32.07% to 61.93% after fine-tuning.
- Scaling law-guided annotation ensured data quality, as performance scaled predictably with model size.
- Open-sourced dataset and models contribute a method for acquiring high-quality SFT data in resource-constrained environments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling laws can serve as an objective quality metric for manual data annotation in SFT.
- Mechanism: The authors use the empirical observation that LLM performance on a given dataset improves predictably with model size (scaling law). By iteratively annotating data and testing it on multiple Qwen models (0.5B to 32B parameters), they ensure that performance increases consistently with model size, indicating high annotation quality.
- Core assumption: The scaling behavior of LLMs on annotated data is monotonic and measurable, and annotation quality directly impacts this scaling trend.
- Evidence anchors:
  - [abstract] "using the scaling behavior of LLMs as an objective quality metric"
  - [section] "After several iterations, the precision of the baseline models is shown in Table 1. ... We observed a F1 score drop in Qwen1.5-4B-Chat model and we believe that a rapid decrease in loss might affect performance, so we updated the learning rate from 2e-4 to 2e-5."
  - [corpus] No direct evidence; weak signal from similar works on scaling laws and data quality.
- Break condition: If the scaling trend is non-monotonic or if model architecture changes (e.g., MoE) disrupt the scaling relationship, the annotation quality cannot be reliably assessed.

### Mechanism 2
- Claim: LoRA fine-tuning activates pre-existing knowledge in LLMs rather than learning new content.
- Mechanism: The authors cite previous work (Kong 2023) suggesting LoRA increments are low-rank and only trigger existing knowledge. They fine-tune using LoRA on annotated data and observe performance improvements consistent with scaling laws, indirectly validating the annotation quality.
- Core assumption: LoRA's low-rank nature means it can only enhance existing capabilities, not acquire new knowledge, so improvements must come from better data quality.
- Evidence anchors:
  - [abstract] "According to inferences made on Kong (2023), LoRA only activates the pre-existing knowledge in PLM but canâ€™t learn new content."
  - [section] "Both LoRA Hu et al. (2021) and LoRA+ Hayou et al. (2024) assume that the increments of weights are of low rank."
  - [corpus] Weak; no direct evidence in cited works about LoRA only activating pre-existing knowledge.
- Break condition: If LoRA can actually learn new content (contradicting the cited assumption), then observed improvements may not solely reflect data quality.

### Mechanism 3
- Claim: Manual annotation guided by scaling laws yields higher-quality SFT data than unsupervised or large-scale noisy data.
- Mechanism: By focusing on a smaller, manually annotated dataset (2.3k questions) and using scaling laws as a feedback loop, the authors achieve significant F1 score improvements (up to 29.07%) compared to using raw chat data or automated annotation.
- Core assumption: Quality of SFT data is more important than quantity, and manual annotation can achieve high quality when guided by objective metrics like scaling laws.
- Evidence anchors:
  - [abstract] "Li et al. (2024) introduced Instruction-Following Difficulty (IFD) and has demonstrated that by filtering out 5% of the data, then better accuracy can be achieved. It proves that the quality of SFT data is more important than quantity."
  - [section] "We take scaling law as objective metric for manual annotation. ... Ultimately, Qwen1.5-MoE-2.7B-Chat F1 score significantly improved from 32.07% to 61.93%."
  - [corpus] No direct evidence; weak signal from related works on data quality and fine-tuning.
- Break condition: If manual annotation is too slow or expensive relative to automated methods, or if scaling laws do not correlate with downstream task performance, the approach may not be practical.

## Foundational Learning

- Concept: Scaling laws in deep learning
  - Why needed here: The entire annotation and evaluation process relies on the predictable relationship between model size and performance. Understanding scaling laws is essential to interpret results and guide annotation.
  - Quick check question: If a 1B parameter model achieves 50% accuracy on a task and a 10B parameter model achieves 70%, what would you expect from a 100B parameter model, assuming perfect scaling?

- Concept: Supervised Fine-Tuning (SFT) and instruction tuning
  - Why needed here: The paper creates SFT data in alpaca format and fine-tunes Qwen models. Knowing how SFT differs from other fine-tuning methods (like RLHF) is crucial for understanding the experimental setup.
  - Quick check question: What is the main difference between SFT and RLHF in terms of data and objective?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the fine-tuning method used. Understanding its low-rank assumption and how it modifies model weights is key to interpreting the results and limitations.
  - Quick check question: Why might LoRA be preferred over full fine-tuning in resource-constrained environments?

## Architecture Onboarding

- Component map:
  Raw chat data -> Concatenation -> KIMI/InternLM2 filtering -> Manual annotation (scaling law guided) -> Alpaca format conversion
  Qwen LLMs (0.5B to 32B) -> LoRA fine-tuning (axolotl) -> Evaluation (F1, precision, recall)

- Critical path:
  1. Preprocess raw chat data
  2. Annotate questions using scaling law feedback loop
  3. Convert to alpaca format
  4. Fine-tune Qwen models with LoRA
  5. Evaluate and iterate

- Design tradeoffs:
  - Manual annotation vs. automated annotation: higher quality but slower and more expensive
  - Model size vs. resource constraints: larger models yield better scaling but require more compute
  - LoRA rank and learning rate: affect fine-tuning stability and performance

- Failure signatures:
  - Non-monotonic scaling behavior indicates annotation issues
  - F1 score drops after fine-tuning suggest overfitting or suboptimal hyperparameters
  - LoRA performance worse than full fine-tuning may indicate low-rank assumption violated

- First 3 experiments:
  1. Test scaling law correlation: Run Qwen models of increasing size on a small annotated subset and verify monotonic performance improvement.
  2. Hyperparameter sensitivity: Fine-tune a mid-sized model (e.g., 7B) with different LoRA ranks and learning rates to find optimal configuration.
  3. Ablation on annotation quality: Compare fine-tuning results using high-quality annotated data vs. automatically filtered raw data to quantify annotation impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did Qwen1.5-4B-Chat model show a decrease in F1 score during fine-tuning, and how can this issue be resolved?
- Basis in paper: [explicit] The authors observed a F1 score drop in Qwen1.5-4B-Chat model and attributed it to a rapid decrease in loss, which they attempted to mitigate by adjusting the learning rate from 2e-4 to 2e-5.
- Why unresolved: Despite the learning rate adjustment, the F1 score decline was only mitigated but not fully resolved. The paper mentions that reducing the learning rate can only mitigate the decline in accuracy, but it does not lead to a precision improvement.
- What evidence would resolve it: Further experiments with different learning rates, epochs, or LoRA configurations (e.g., rank size) could determine the optimal settings for Qwen1.5-4B-Chat. Additionally, analyzing the loss curve and model behavior during training could provide insights into the underlying cause of the performance drop.

### Open Question 2
- Question: How can the low-rank assumption of LoRA be empirically validated in this scenario?
- Basis in paper: [explicit] The authors mention that LoRA assumes the increments of weights are of low rank and that they need to prove the data in this scenario meets the low-rank assumption.
- Why unresolved: The paper does not provide empirical evidence or experiments to validate the low-rank assumption of LoRA in the context of their fine-tuning process.
- What evidence would resolve it: Conducting experiments to analyze the rank of the weight updates during fine-tuning, such as singular value decomposition (SVD) analysis, could provide empirical evidence of whether the low-rank assumption holds. Additionally, comparing the performance of LoRA with other fine-tuning methods that do not assume low-rank updates could offer insights into the effectiveness of LoRA.

### Open Question 3
- Question: How effective is the scaling law-guided manual annotation process in ensuring data quality across different domains and languages?
- Basis in paper: [explicit] The authors used scaling laws as an objective metric for manual annotation, observing that the precision of baseline models increased with model size, confirming the reliability of the annotation process.
- Why unresolved: The paper only demonstrates the effectiveness of the scaling law-guided annotation process in a specific domain (group chat inquiries) and for Chinese and English languages. It is unclear whether this method can be generalized to other domains or languages.
- What evidence would resolve it: Applying the scaling law-guided annotation process to datasets from different domains (e.g., medical, legal, technical) and languages (e.g., non-English languages) and comparing the performance of fine-tuned models could determine the generalizability of the method. Additionally, conducting ablation studies to assess the impact of scaling law guidance on annotation quality would provide further insights.

## Limitations

- The claim that scaling laws can serve as an objective quality metric for manual annotation lacks direct empirical validation and may be influenced by factors other than annotation quality.
- The assertion that LoRA only activates pre-existing knowledge is based on a single citation and contradicts emerging evidence that LoRA can learn new content.
- The manual annotation process remains subjective and resource-intensive, raising questions about its scalability and reproducibility.

## Confidence

- **High confidence**: Claims about F1 score improvements (up to 29.07%) and successful LoRA fine-tuning with Qwen models are directly supported by experimental results.
- **Medium confidence**: The relationship between scaling law compliance and annotation quality is plausible but not rigorously proven. The assumption about LoRA's low-rank nature is based on literature but not directly tested.
- **Low confidence**: The claim that manual annotation guided by scaling laws is superior to automated methods lacks comparative experiments or ablation studies.

## Next Checks

1. **Scaling law validation experiment**: Test the same annotated dataset on additional model sizes (e.g., 64B, 128B parameters) to verify if performance continues to scale monotonically, strengthening the link between annotation quality and scaling behavior.
2. **LoRA capability experiment**: Fine-tune the same dataset using full fine-tuning and compare performance to LoRA results. If full fine-tuning achieves significantly better results, it would challenge the assumption that LoRA only activates pre-existing knowledge.
3. **Annotation quality comparison**: Create an automatically filtered dataset using the same raw chat data and compare fine-tuning results to the manually annotated dataset. Quantify the impact of manual annotation on final F1 scores to validate the annotation quality claim.