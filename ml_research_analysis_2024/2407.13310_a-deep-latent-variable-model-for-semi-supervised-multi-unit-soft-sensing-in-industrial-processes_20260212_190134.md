---
ver: rpa2
title: A deep latent variable model for semi-supervised multi-unit soft sensing in
  industrial processes
arxiv_id: '2407.13310'
source_url: https://arxiv.org/abs/2407.13310
tags:
- data
- learning
- soft
- which
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep latent variable model for semi-supervised
  multi-unit soft sensing in industrial processes. The model combines multi-task learning
  and semi-supervised learning to leverage unlabeled data and learn across multiple
  process units.
---

# A deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes

## Quick Facts
- arXiv ID: 2407.13310
- Source URL: https://arxiv.org/abs/2407.13310
- Authors: Bjarne Grimstad; Kristian Løvland; Lars S. Imsland; Vidar Gunnerud
- Reference count: 40
- Key outcome: Deep latent variable model combining semi-supervised and multi-task learning outperforms state-of-the-art soft sensing approaches while requiring fewer labeled data points

## Executive Summary
This paper introduces a deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes. The model combines multi-task learning and semi-supervised learning to leverage unlabeled data and learn across multiple process units. It uses a hierarchical generative model with latent variables at the observation and unit levels to capture shared structures and unit-specific variations. Experiments on synthetic and real petroleum production data demonstrate superior prediction accuracy compared to current state-of-the-art approaches for virtual flow metering.

## Method Summary
The method employs a hierarchical generative model with latent variables at both observation (z) and unit (c) levels. The model is trained using variational inference with an inference network approximating the intractable posterior. The semi-supervised component enables learning from unlabeled data through modeling the joint distribution p(x,y), while the multi-task aspect allows sharing statistical strength across units through context variables. The model is optimized using Stochastic Gradient Variational Bayes (SGVB) with the reparameterization trick.

## Key Results
- Superior prediction accuracy compared to state-of-the-art virtual flow metering approaches
- Achieves better performance with fewer labeled data points required
- Demonstrates effectiveness on both synthetic single-phase fluid flow data and real petroleum production data
- Shows improved data efficiency through combined semi-supervised and multi-task learning

## Why This Works (Mechanism)

### Mechanism 1
Combining semi-supervised and multi-task learning enables the model to learn useful shared representations across units while leveraging unlabeled data to improve generalization. The hierarchical generative model introduces latent variables at both observation and unit levels, capturing shared structures across units and unit-specific variations. The semi-supervised component enables learning from unlabeled data through the inference model.

Core assumption: The underlying process exhibits both shared characteristics across units and dependencies between input and output variables that can be exploited by generative modeling.

### Mechanism 2
The variational autoencoder framework enables tractable inference of latent variables despite the intractable posterior, allowing the model to learn from both labeled and unlabeled data. The model uses variational inference with an inference network to approximate the intractable posterior, and the reparameterization trick enables efficient stochastic gradient estimation.

Core assumption: The true posterior distribution can be reasonably approximated by the chosen inference model structure (mean-field normal distributions).

### Mechanism 3
The semi-supervised learning component provides a powerful regularization mechanism through modeling p(x), improving generalization when labeled data is scarce. By modeling the joint distribution p(x,y) = p(y|x)p(x), the model can learn useful structure from unlabeled data through the marginal distribution p(x).

Core assumption: The marginal distribution p(x) contains information relevant to learning p(y|x), which is true when there is not a clear unidirectional causal relationship between x and y.

## Foundational Learning

- Concept: Variational Autoencoder (VAE)
  - Why needed here: Provides foundation for tractable inference in deep latent variable model, enabling learning from both labeled and unlabeled data
  - Quick check question: Can you explain how the ELBO is derived from the marginal log-likelihood and why this enables approximate inference?

- Concept: Multi-task Learning (MTL)
  - Why needed here: Allows model to learn across multiple process units, sharing statistical strength between tasks and improving data efficiency
  - Quick check question: How does the hierarchical structure with context variables c enable MTL in this model?

- Concept: Semi-supervised Learning (SSL)
  - Why needed here: Allows model to leverage abundance of unlabeled data in soft sensing applications where target variable y is measured infrequently
  - Quick check question: Under what conditions does semi-supervised learning outperform purely supervised learning, and why might this be true for soft sensing problems?

## Architecture Onboarding

- Component map: Context variables c (unit level) -> Observation-level latent variables z -> Generative model pθ(x,y|z,c) with neural networks -> Inference model qϕ(z,x,y|c) with neural networks

- Critical path:
  1. Sample context variable c from qϕ(ci) for each unit
  2. For unlabeled data: sample y from qϕy(y|x,c), then sample z from qϕz(z|x,y,c)
  3. For labeled data: sample z from qϕz(z|x,y,c) directly
  4. Compute ELBO using these samples and analytical KL divergence for context variables
  5. Optimize using SGVB estimator and back-propagation

- Design tradeoffs:
  - Fixed latent dimensions K and D vs. flexible dimensionality
  - Mean-field approximation for inference vs. more complex posterior approximations
  - Unconditional variational distribution for context variables vs. conditional on full dataset
  - Overcomplete VAE (D > Dx + Dy) for regularization vs. minimal dimensionality

- Failure signatures:
  - Poor performance on test data despite good training ELBO may indicate overfitting or incorrect model assumptions
  - Unstable training or exploding gradients may indicate issues with reparameterization or network architecture
  - Context variables not capturing unit-specific variations may indicate insufficient model capacity or incorrect hierarchical structure

- First 3 experiments:
  1. Train on synthetic data with 2 units and 10 labeled + 100 unlabeled data points per unit, compare STL vs MTL vs SSMTL performance
  2. Train on synthetic data with varying amounts of unlabeled data (N_u/N_l = 1, 5, 20) to verify SSL benefit increases with more unlabeled data
  3. Perform single-unit finetuning experiment: train on multi-unit data, then finetune to a new unit with only 1-10 labeled data points, compare semi-supervised vs supervised finetuning

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the SSMTL model scale with the number of latent variables K and D in the hierarchical model? The paper assumes fixed dimensions K and D but does not explore the impact of varying these dimensions on model performance or computational efficiency.

### Open Question 2
Can the SSMTL model be effectively extended to handle sequential data or time-series observations? The paper focuses on steady-state flow measurements and does not consider temporal dependencies or incorporate recurrent components.

### Open Question 3
How robust is the SSMTL model to different types of missing data patterns in the input features x? The model can handle missing target values y, but the handling of missing input features is not discussed.

## Limitations
- Limited scope of real-world testing (petroleum production data only)
- Absence of detailed architectural specifications creates uncertainty about reproducibility
- No investigation of scalability to many units (>10) or high-dimensional observations
- Sensitivity to hyperparameter choices not thoroughly explored

## Confidence
- Mechanism 1 (SSL+MTL combination): Medium - well-reasoned but needs broader empirical validation
- Mechanism 2 (VAE inference): High - standard technique with established theoretical foundation
- Mechanism 3 (SSL regularization): Medium - theoretical argument sound but empirical support limited

## Next Checks
1. Ablation study testing the contribution of each component (semi-supervised vs supervised, multi-task vs single-task) across multiple synthetic datasets with varying degrees of unit similarity
2. Sensitivity analysis of key hyperparameters (latent dimensions, learning rates, network depths) to identify stable regions and potential failure modes
3. Cross-domain validation on at least two additional industrial soft sensing applications (e.g., chemical processing, power generation) to assess generalizability