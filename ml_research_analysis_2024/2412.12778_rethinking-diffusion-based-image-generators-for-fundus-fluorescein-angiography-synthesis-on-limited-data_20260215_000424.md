---
ver: rpa2
title: Rethinking Diffusion-Based Image Generators for Fundus Fluorescein Angiography
  Synthesis on Limited Data
arxiv_id: '2412.12778'
source_url: https://arxiv.org/abs/2412.12778
tags:
- diffusion
- images
- image
- dataset
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating invasive fundus fluorescein
  angiography (FFA) images from non-invasive fundus photographs, which is valuable
  for reducing patient discomfort and risk while supporting clinical diagnosis. The
  authors introduce Diffusion, a novel latent diffusion model framework that leverages
  pre-trained Stable Diffusion with a fine-tuning protocol to overcome limited medical
  image data.
---

# Rethinking Diffusion-Based Image Generators for Fundus Fluorescein Angiography Synthesis on Limited Data

## Quick Facts
- arXiv ID: 2412.12778
- Source URL: https://arxiv.org/abs/2412.12778
- Reference count: 27
- Primary result: Novel latent diffusion model framework achieves state-of-the-art FFA generation from fundus photos with clinical validation showing improved disease classification

## Executive Summary
This paper addresses the challenge of generating invasive fundus fluorescein angiography (FFA) images from non-invasive fundus photographs using a novel latent diffusion model framework. The authors propose Diffusion, which leverages pre-trained Stable Diffusion with a fine-tuning protocol specifically designed for limited medical datasets. By incorporating image embeddings and an offset noise strategy, the method successfully injects target modality characteristics into the generation process. Evaluated on two ophthalmic datasets (SLO2FFA and MPOS), the approach achieves state-of-the-art performance across multiple image quality metrics and demonstrates significant improvements in disease classification accuracy compared to existing methods.

## Method Summary
The proposed Diffusion framework fine-tunes a pre-trained Stable Diffusion V1.5 model in two stages to overcome limited medical data challenges. In stage one, the VAE encoder is frozen while the denoising U-Net is trained with source image features injected via cross-attention using CLIP visual encoder embeddings, along with an offset noise strategy to inject target modality characteristics. Stage two freezes the trained U-Net and fine-tunes only the VAE decoder to enhance high-frequency information recovery. The model is trained on paired non-invasive and invasive fundus images using reconstruction, perceptual, and adversarial losses, with random 768Ã—768 crops as input.

## Key Results
- Achieves state-of-the-art performance on FFA generation with FID, KID, LPIPS, PSNR, SSIM, and MS-SSIM metrics
- Clinical validation shows generated FFA images significantly improve disease classification accuracy over existing methods
- Successfully demonstrates cross-modal generation capability on two ophthalmic datasets (SLO2FFA and MPOS) despite limited paired data

## Why This Works (Mechanism)

### Mechanism 1
The latent diffusion model can generate high-quality FFA images from limited ophthalmic datasets when fine-tuned on a pre-trained SDV1.5 model. The model leverages transfer learning by fine-tuning a pre-trained latent diffusion model, which has already learned rich feature representations from large-scale datasets like LAION-5B. This pre-training allows the model to generalize well to medical images even with limited data.

### Mechanism 2
The image embedding and offset noise strategy improves cross-modal generation by injecting source image features and target modality characteristics into the latent space. The model uses a pre-trained CLIP visual encoder to extract source image features and injects them into the cross-attention mechanism of each U-Net block. Additionally, an offset noise strategy is introduced to explicitly inject the perceptual characteristics of the target modality into the latent space generation process.

### Mechanism 3
Fine-tuning only the decoder part of the VAE in the second stage enhances high-frequency information recovery, improving the overall image quality. The authors freeze the denoising U-Net trained in the first stage and only train the decoder part of the VAE. This approach ensures that the paired images are mapped to the same latent space while allowing the decoder to recover high-frequency information and generate finer images.

## Foundational Learning

- Concept: Latent Diffusion Models (LDMs)
  - Why needed here: Understanding LDMs is crucial for grasping the core methodology of the paper, as the proposed Diffusion framework is based on a latent diffusion model.
  - Quick check question: What are the main components of a latent diffusion model, and how do they differ from standard diffusion models?

- Concept: Transfer Learning
  - Why needed here: The paper's approach relies heavily on transfer learning to overcome the challenge of limited medical data by fine-tuning a pre-trained model.
  - Quick check question: How does transfer learning enable the model to generalize well to medical images with limited data?

- Concept: Cross-Modal Image Generation
  - Why needed here: The paper addresses the challenge of generating FFA images from fundus photographs, which involves cross-modal image generation.
  - Quick check question: What are the key challenges in cross-modal image generation, and how does the paper's approach address them?

## Architecture Onboarding

- Component map: VAE encoder -> denoising U-Net -> VAE decoder; CLIP visual encoder -> cross-attention mechanism; offset noise strategy -> latent space generation
- Critical path: 1. Encode source and target images into latent space using VAE encoder; 2. Train denoising U-Net with source image features injected via cross-attention; 3. Fine-tune VAE decoder to enhance high-frequency information recovery; 4. Generate FFA images using the fine-tuned model
- Design tradeoffs: Freezing the VAE encoder ensures consistent latent space mapping but limits the model's ability to adapt to new data distributions; using a pre-trained CLIP visual encoder provides robust feature extraction but may not capture all relevant information for FFA generation; fine-tuning only the decoder part of the VAE reduces computational costs but may limit the model's overall adaptability
- Failure signatures: Poor image quality or artifacts in generated FFA images; failure to capture disease-specific features or pathological regions; degradation in performance when applied to new datasets or disease categories
- First 3 experiments: 1. Compare the performance of the fine-tuned model with the pre-trained SDV1.5 model on a small subset of the SLO2FFA dataset to assess the effectiveness of transfer learning; 2. Evaluate the impact of the image embedding and offset noise strategy by training a baseline model without these components and comparing its performance to the full model; 3. Assess the contribution of fine-tuning the VAE decoder by comparing the performance of a model with only the first stage (trained denoising U-Net) to the full two-stage model

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several important areas for future research emerge from the limitations discussion.

## Limitations
- The paper does not fully specify architectural details of the image embedding integration into cross-attention mechanisms and the exact implementation of the offset noise strategy
- Clinical validation relies on relatively small datasets (241 and 600 paired images) that may limit generalizability to broader ophthalmic populations
- The model's performance on rare or atypical disease presentations is not thoroughly evaluated

## Confidence
- **High Confidence**: The fundamental approach of using latent diffusion models for cross-modal FFA generation and the overall two-stage fine-tuning methodology are well-supported by results and established literature
- **Medium Confidence**: The specific mechanisms for image embedding injection and offset noise strategy, while theoretically sound, lack complete implementation details that could affect reproducibility
- **Medium Confidence**: Clinical utility claims are supported by classification experiments but would benefit from larger-scale validation across more diverse patient populations and disease categories

## Next Checks
1. Implement ablation studies to quantify the individual contributions of image embeddings, offset noise strategy, and VAE decoder fine-tuning to overall performance
2. Test the model on external ophthalmic datasets not seen during training to assess generalizability and robustness to different imaging protocols
3. Conduct expert ophthalmologist evaluation of generated FFA images for clinical plausibility and diagnostic utility across various disease categories