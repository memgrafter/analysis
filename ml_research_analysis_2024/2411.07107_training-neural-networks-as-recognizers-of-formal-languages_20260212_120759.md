---
ver: rpa2
title: Training Neural Networks as Recognizers of Formal Languages
arxiv_id: '2411.07107'
source_url: https://arxiv.org/abs/2411.07107
tags:
- lstm
- language
- test
- string
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a general method for training neural networks
  as binary classifiers of formal languages, correcting a mismatch between existing
  experiments and formal claims. The method requires two language-specific algorithms:
  sampling positive examples within a length range and membership testing.'
---

# Training Neural Networks as Recognizers of Formal Languages

## Quick Facts
- arXiv ID: 2411.07107
- Source URL: https://arxiv.org/abs/2411.07107
- Reference count: 40
- Primary result: RNNs and LSTMs outperform transformers on 18 formal languages; no single training objective consistently improves performance

## Executive Summary
This paper addresses the mismatch between existing neural network experiments on formal languages and theoretical claims about their computational power. The authors propose a general method for training neural networks as binary classifiers of formal languages, requiring only two language-specific algorithms: length-constrained positive sampling and membership testing. For regular languages, they extend efficient sampling algorithms using a binning semiring that enables shared computation across all lengths. Experiments with RNNs, LSTMs, and transformers across 18 formal languages reveal that transformer architectures generally underperform compared to recurrent models, and that auxiliary training objectives show inconsistent benefits across different languages and architectures.

## Method Summary
The paper introduces a general framework for training neural networks as formal language recognizers. The method involves generating positive examples using length-constrained sampling (with a binning semiring approach for regular languages) and creating negative examples through perturbation of positive examples. Three neural architectures (RNN, LSTM, transformer) are trained with recognition loss plus optional auxiliary losses for language modeling and next-symbol prediction. Model selection uses validation cross-entropy rather than accuracy to better distinguish between models with similar accuracy but different confidence levels. The approach is evaluated on 18 formal languages spanning the Chomsky hierarchy, with datasets released as the FLaRe benchmark.

## Key Results
- RNNs and LSTMs consistently outperform transformers across all tested formal languages
- No single training objective (recognition loss alone, recognition + LM, recognition + next-symbol prediction) uniformly improves performance across languages and architectures
- Transformers fail to achieve perfect accuracy on the Majority language despite it being context-free
- The binning semiring approach enables efficient length-constrained sampling for regular languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The binning semiring enables efficient length-constrained sampling from regular languages by sharing computation across all lengths up to nmax.
- Mechanism: Instead of intersecting a DFA with a length-constrained DFA for each n, the binning semiring lifts the DFA to a vector-weighted version where each vector component represents a specific length. This allows running weight pushing once to compute normalized weights for all lengths simultaneously.
- Core assumption: The binning semiring correctly represents the distribution over strings of different lengths and supports the necessary semiring operations for weight pushing.
- Evidence anchors:
  - [section]: "Running a semiring generalization of weight pushing on AD allows us to compute exactly the quantities we need for efficient sampling from pA′(W | |W| = n)."
  - [corpus]: "We provide efficient instantiations for the class of regular languages based on work by Snæbjarnarson et al. (2025)." (Weak corpus evidence, but references the foundational work)

### Mechanism 2
- Claim: Perturbation-based negative sampling generates adversarial examples that are difficult to distinguish from positive examples, improving training signal quality.
- Mechanism: The method samples a positive example and applies random edits (insertions, replacements, deletions) where the number of edits K is sampled from a geometric distribution heavily skewed toward small K. This ensures most negative examples are similar to positive ones.
- Core assumption: Negative examples with small edit distances are genuinely adversarial and force the model to learn the true language boundary rather than superficial features.
- Evidence anchors:
  - [section]: "Half the time, we uniformly sample a length n from [nmin, nmax], then uniformly sample w from Σn. For many languages, this is very likely to produce a negative example, but one that is so superficially dissimilar to positive examples that the classification problem becomes too easy and fails to demonstrate the underlying algorithm."
  - [section]: "The other half of the time, we propose w by sampling a positive example and perturbing it with random edits (cf. Weiss et al., 2018a)."

### Mechanism 3
- Claim: Auxiliary training objectives (language modeling and next-symbol prediction) provide intermediate supervision that helps models learn recognition algorithms, though no single objective consistently improves performance.
- Mechanism: The recognition head receives only a single bit per example, which may be insufficient for learning complex recognition algorithms. Auxiliary heads provide additional gradient signals at each timestep about valid next symbols and probability distributions over the next symbol.
- Core assumption: The intermediate computational steps needed for recognition are learnable from the auxiliary supervision and transfer to better recognition performance.
- Evidence anchors:
  - [section]: "One way to alleviate these issues is to provide the model with hints about intermediate computational steps."
  - [section]: "We include experiments that add one or both of the following auxiliary loss terms to the training objective for positive examples: (1) a language modeling loss term, LLM(M, w), which requires the model to learn a distribution over the next symbol at each position, and (2) a next-symbol prediction6 loss term, LNS(M, w), which requires the model to predict whether each symbol may appear next at each position, given the prefix of w seen so far."

## Foundational Learning

- Concept: Formal language theory and the Chomsky hierarchy
  - Why needed here: The paper's core contribution is evaluating neural networks as recognizers of formal languages, so understanding what formal languages are and how they're classified is essential
  - Quick check question: What is the difference between a recognizer and a transducer in formal language theory?

- Concept: Deterministic Finite Automata (DFAs) and their properties
  - Why needed here: Regular languages are defined by DFAs, and the efficient sampling algorithm requires understanding how DFAs work
  - Quick check question: How does a DFA determine whether to accept or reject a string?

- Concept: Semirings and their operations
  - Why needed here: The binning semiring is the mathematical foundation for the efficient sampling algorithm, and understanding semirings is crucial for grasping how weight pushing works
  - Quick check question: What are the defining properties of a semiring that make it suitable for weighted automata algorithms?

## Architecture Onboarding

- Component map: Data generation pipeline (length-constrained sampling + perturbation-based negative sampling + membership testing) -> Neural network architectures (RNN, LSTM, Transformer with recognition head) -> Training objectives (recognition loss + optional auxiliary losses) -> Model selection (validation cross-entropy) -> Evaluation (test accuracy on long strings)

- Critical path: Generate datasets → Train models with different loss combinations → Evaluate on long strings to test expressivity → Analyze results across architectures and languages

- Design tradeoffs:
  - Perturbation-based vs language-specific negative sampling: generality vs efficiency
  - Number of model parameters: fixed budget vs language-specific sizing
  - Auxiliary objectives: additional supervision vs potential noise/conflicting gradients

- Failure signatures:
  - High variance across runs: suggests model selection issues or unstable training
  - Poor performance on longer strings: suggests expressivity limitations rather than inductive bias
  - Catastrophic forgetting on validation set: suggests overfitting to training distribution

- First 3 experiments:
  1. Train a simple RNN on Even Pairs with only recognition loss to establish baseline performance
  2. Train the same RNN with next-symbol prediction auxiliary loss to test if intermediate supervision helps
  3. Compare transformer vs RNN performance on Parity to observe sensitivity effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do transformers perform significantly worse than RNNs and LSTMs on formal language recognition tasks despite their success on natural language?
- Basis in paper: Explicit - "we see that the transformer generally underperforms the RNN and LSTM" and "An interesting question to address in future work is why transformers perform so much better on natural language than on formal languages"
- Why unresolved: The paper notes this performance gap but does not provide a definitive explanation, leaving it as an open question for future research.

### Open Question 2
- Question: What is the fundamental limitation that prevents transformers from perfectly recognizing the Majority language despite it being context-free?
- Basis in paper: Explicit - "The transformer almost reaches, but does not quite reach, 100% accuracy on Majority"
- Why unresolved: The paper observes this performance ceiling but does not identify the specific architectural or computational constraint causing this limitation.

### Open Question 3
- Question: How do auxiliary training objectives like language modeling and next-symbol prediction affect the generalization capabilities of different architectures across various formal languages?
- Basis in paper: Explicit - "no single objective uniformly improves performance across languages and architectures" and the paper compares multiple training objectives
- Why unresolved: While the paper shows that auxiliary objectives help in isolated cases, it does not establish systematic patterns or provide theoretical justification for when these objectives are beneficial.

## Limitations

- The binning semiring algorithm's correctness depends on proper implementation of weight pushing over vector-weighted automata, which is only briefly described
- Perturbation-based negative sampling may not generate sufficiently challenging examples for languages with sparse complements, potentially inflating performance metrics
- Claims about auxiliary objectives not consistently improving performance are based on a fixed set of 18 languages, which may not generalize to other formal languages

## Confidence

- **High confidence**: The general framework for length-constrained sampling and perturbation-based negative generation is mathematically sound and well-explained
- **Medium confidence**: The binning semiring implementation details are sufficient for understanding the approach but lack complete specification for exact reproduction
- **Low confidence**: The claim about auxiliary objectives not consistently improving performance requires more systematic analysis across broader language classes and hyperparameter settings

## Next Checks

1. Implement the binning semiring weight pushing algorithm independently and verify that it produces identical sampling distributions to those reported in the paper for regular languages

2. Conduct ablation studies on the perturbation-based negative sampling by comparing performance when using pure random negative sampling versus the proposed perturbation method, measuring both accuracy and training efficiency

3. Extend experiments to include at least 5 additional formal languages from the context-free and context-sensitive hierarchies to test whether the observed patterns (RNN/LSTM superiority, auxiliary objective inconsistency) hold more broadly