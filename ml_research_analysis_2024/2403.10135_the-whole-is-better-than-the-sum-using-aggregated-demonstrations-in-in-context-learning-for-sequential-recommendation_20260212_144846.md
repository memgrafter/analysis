---
ver: rpa2
title: 'The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context
  Learning for Sequential Recommendation'
arxiv_id: '2403.10135'
source_url: https://arxiv.org/abs/2403.10135
tags:
- demonstration
- recommendation
- user
- items
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of in-context learning
  (ICL) for large language model (LLM)-based sequential recommendation. It systematically
  studies the impact of instruction wording, task consistency, demonstration selection,
  and the number of demonstrations.
---

# The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation

## Quick Facts
- **arXiv ID**: 2403.10135
- **Source URL**: https://arxiv.org/abs/2403.10135
- **Reference count**: 40
- **Primary result**: LLMSRec-Syn outperforms existing LLM-based sequential recommendation methods and sometimes surpasses supervised learning methods.

## Executive Summary
This paper investigates in-context learning (ICL) for large language model (LLM)-based sequential recommendation. The authors find that increasing the number of demonstrations degrades performance due to the "lost in the middle" phenomenon and prompt length limits. To address this, they propose LLMSRec-Syn, which aggregates multiple user demonstrations into a single, concise demonstration. Experiments on three datasets (MovieLens-1M, LastFM-2K, Games) show that LLMSRec-Syn outperforms existing LLM-based sequential recommendation methods, and in some cases even surpasses supervised learning methods.

## Method Summary
LLMSRec-Syn is a novel ICL method that incorporates multiple demonstration users into one aggregated demonstration. The method involves selecting similar users via semantic similarity retrieval, aggregating their item histories and ground truth next items into a single ranking list, and constructing a concise demonstration. This approach reduces prompt length growth and provides richer ranking supervision than single demonstrations. The aggregated demonstration includes multiple next items at high positions in the ranking list, avoiding sparse signals and providing more guidance to LLMs for recommending to the test user.

## Key Results
- LLMSRec-Syn outperforms existing LLM-based sequential recommendation methods on three datasets.
- The method achieves higher NDCG scores compared to standard ICL approaches with single demonstrations.
- Using a more powerful LLM (GPT-4) further improves performance of LLMSRec-Syn.
- Task consistency between demonstration and test instance is crucial for ICL success, with ranking template (T3) consistently outperforming other task options.

## Why This Works (Mechanism)

### Mechanism 1
Aggregated demonstration reduces "lost in the middle" degradation in long ICL prompts. By combining multiple demonstration users into one concise demonstration, the prompt length grows slowly with more demonstrations, keeping relevant information near the start of the context. Core assumption: LLMs perform worse when relevant information is buried in the middle of long contexts.

### Mechanism 2
Aggregated demonstration provides richer ranking supervision than single demonstration. Each member user's ground truth next-item is inserted into the ranking list at a high position, giving LLM stronger and denser signals about correct ranking behavior. Core assumption: Sparse supervision from a single next-item is less informative than multiple high-ranked ground truth items.

### Mechanism 3
Task consistency between demonstration and test instance is crucial for ICL success. Using a ranking template (T3) for both aggregated demonstration and test user aligns the task semantics, preventing confusion about expected output format. Core assumption: LLMs learn better when input-output mapping in demonstrations matches that required for the test case.

## Foundational Learning

- **Concept**: In-context learning (ICL)
  - Why needed here: The paper's entire approach relies on adapting LLMs to sequential recommendation without fine-tuning, purely via demonstrations in the prompt.
  - Quick check question: What distinguishes zero-shot, one-shot, and few-shot ICL in terms of demonstration count?

- **Concept**: Sequential recommendation task definition
  - Why needed here: Understanding the input (user history, candidates, ground truth) and output (ranked list) is essential to designing effective demonstrations.
  - Quick check question: Why does the paper explicitly include the candidate set in the problem definition?

- **Concept**: Semantic similarity retrieval
  - Why needed here: Selecting similar demonstration users via LLM embeddings is a key step in constructing aggregated demonstrations.
  - Quick check question: How does using LLM embeddings compare to random selection in demonstration quality?

## Architecture Onboarding

- **Component map**: Retriever -> Aggregator -> Prompt Builder -> LLM API
- **Critical path**: Input user history → Retrieve similar users → Aggregate into demonstration → Build prompt → LLM call → Ranked output
- **Design tradeoffs**: 
  - More K → richer signal but risk of noise; fewer K → cleaner but sparser supervision
  - Fixed candidate set vs dynamic → simplicity vs relevance
  - Instruction wording options → robustness vs precision
- **Failure signatures**:
  - Prompt too long → LLM truncation or degraded accuracy
  - Irrelevant demonstrations → worse than random performance
  - Inconsistent task framing → random or off-topic outputs
- **First 3 experiments**:
  1. Compare NDCG@10 for K=1,2,3,4,5 on ML-1M
  2. Test instruction template (A) vs (B), (C), (D) on a small subset
  3. Validate that retrieval-based selection beats random on LastFM-2K

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of member users (K) to include in the aggregated demonstration for maximizing recommendation accuracy? The paper finds that the optimal K varies depending on the dataset, but does not provide a definitive method for determining the ideal K value for a new dataset.

### Open Question 2
How does the aggregated demonstration method compare to other demonstration selection strategies, such as exemplar-based or prototype-based approaches, in terms of recommendation accuracy and computational efficiency? The paper introduces a novel aggregated demonstration method but does not benchmark it against other advanced demonstration selection strategies.

### Open Question 3
Can the aggregated demonstration method be extended to incorporate additional information about the user, such as demographic data or contextual information, to further improve recommendation accuracy? The paper focuses on using item interaction sequences but does not explore the potential benefits of incorporating additional user information.

## Limitations

- The "lost in the middle" phenomenon is asserted but not directly measured or quantified.
- The quality of OpenAI embeddings for recommendation contexts is not evaluated.
- Task consistency claims are based on observed performance differences without explaining the underlying reasons why T3 works better.

## Confidence

- **High confidence**: Aggregated demonstration reduces prompt length growth (directly measurable from prompt construction)
- **Medium confidence**: Aggregated demonstration outperforms single demonstration in NDCG metrics (reported results but limited ablation studies)
- **Low confidence**: The proposed mechanism explains WHY aggregation works better (lacks ablation on individual components)

## Next Checks

1. Systematically vary prompt length with single demonstrations and measure accuracy degradation to confirm "lost in the middle" effects.
2. Test whether semantic similarity retrieval is essential by comparing against random selection, and whether ranking aggregation adds value beyond simple concatenation.
3. Validate performance on additional sequential recommendation datasets beyond the three provided to assess robustness of the approach.