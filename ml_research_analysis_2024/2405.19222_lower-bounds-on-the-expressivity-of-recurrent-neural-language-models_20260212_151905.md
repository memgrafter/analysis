---
ver: rpa2
title: Lower Bounds on the Expressivity of Recurrent Neural Language Models
arxiv_id: '2405.19222'
source_url: https://arxiv.org/abs/2405.19222
tags:
- elman
- language
- pfsa
- function
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects the representational capacity of Elman recurrent
  neural language models to probabilistic finite-state automata (PFSAs). While previous
  work linked RNNs to language acceptors, this study focuses on LMs as probability
  distributions over strings.
---

# Lower Bounds on the Expressivity of Recurrent Neural Language Models

## Quick Facts
- arXiv ID: 2405.19222
- Source URL: https://arxiv.org/abs/2405.19222
- Reference count: 40
- Primary result: Elman RNNs with linearly bounded precision can exactly simulate or approximate arbitrary regular language models

## Executive Summary
This paper establishes theoretical bounds on the expressivity of Elman recurrent neural network language models by connecting them to probabilistic finite-state automata (PFSAs). While previous work linked RNNs to language acceptors, this study focuses on LMs as probability distributions over strings. The authors demonstrate that Elman LMs with linearly bounded precision can exactly simulate or approximate arbitrary regular LMs under different normalization schemes.

## Method Summary
The paper presents three main mechanisms for connecting Elman RNNs to PFSAs. First, using sparsemax normalization and ℓ¹ normalization, exact simulation is achieved by encoding the state-string distribution in the hidden state. Second, softmax normalization with extended-real-valued output functions can approximate regular LMs arbitrarily well through log transformation. Third, MLPs with softmax can approximate regular LMs by combining PFSA approximation with universal function approximation.

## Key Results
- Exact simulation of any regular LM using Elman LMs with sparsemax normalization and ∥·∥₁ normalization
- Exact simulation using softmax normalization with an extended-real-valued output function
- Approximation of arbitrary regular LMs using Elman LMs with softmax and MLP output functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Elman RNNs with linearly bounded precision can exactly simulate any regular language model by encoding the state-string distribution in the hidden state.
- Mechanism: The hidden state recurrence is constructed so that each dimension corresponds to a pair (state, last symbol). The recurrence matrix performs all possible PFSA transitions in parallel, and the input matrix masks to the correct symbol. The output function then extracts the appropriate conditional probabilities.
- Core assumption: The PFSA is trim (all states accessible and co-accessible) and the language model is tight (probabilities of long strings vanish appropriately).
- Evidence anchors:
  - [abstract]: "demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs"
  - [section]: Theorem 3.1 and Corollary 3.1 show exact simulation via ∥·∥₁ normalization and sparsemax
  - [corpus]: Weak, only related work on RNN expressivity, no direct evidence for this mechanism
- Break condition: If the PFSA is not trim or the language model is not tight, the construction fails to produce valid probabilities.

### Mechanism 2
- Claim: Softmax-normalized Elman LMs can approximate arbitrary regular language models arbitrarily well using a log-transformed output function.
- Mechanism: Instead of explicit normalization, a log transformation of the hidden state before softmax computes the conditional probabilities from the state-string distribution. The softmax then normalizes these values to valid probabilities.
- Core assumption: The PFSA is trim and induces a tight language model, and the output function can use extended reals to represent log(0) as -∞.
- Evidence anchors:
  - [abstract]: "Elman LMs with the softmax projection function and a nonlinear output function can approximate a regular LM arbitrarily well"
  - [section]: Corollary 3.2 shows exact simulation with softmax and extended reals
  - [corpus]: Weak, related work on RNN approximation but not this specific construction
- Break condition: If the language model has full support or the log transformation cannot handle zero probabilities, the approximation fails.

### Mechanism 3
- Claim: Elman LMs with MLP output functions can approximate regular language models arbitrarily well without using extended reals.
- Mechanism: Two-step approximation: (1) approximate the regular LM with a full-support PFSA, (2) approximate the log output function with an MLP using Pinkus' universal approximation theorem. The full support ensures the log domain is compact.
- Core assumption: The MLP can approximate continuous functions on compact domains, and the full-support PFSA approximates the original LM well in total variation distance.
- Evidence anchors:
  - [abstract]: "Elman LMs with softmax and an MLP can approximate arbitrary regular LMs"
  - [section]: Theorem 4.2 shows approximation with MLP output
  - [corpus]: Weak, related work on MLP approximation but not this specific application
- Break condition: If the MLP cannot approximate the log function well enough or the full-support approximation is poor, the total variation distance will not be small.

## Foundational Learning

- Concept: Probabilistic Finite-State Automata (PFSAs)
  - Why needed here: The paper connects Elman RNNs to PFSAs as a way to characterize the probabilistic representational capacity of RNN language models.
  - Quick check question: What is the difference between a PFSA and a regular FSA, and why does this difference matter for language modeling?

- Concept: Total Variation Distance
  - Why needed here: The paper uses total variation distance to measure how well an Elman LM approximates a regular LM, which is crucial for the approximation results.
  - Quick check question: How is total variation distance defined for probability distributions over strings, and why is it an appropriate measure for comparing language models?

- Concept: Universal Approximation Theorem for MLPs
  - Why needed here: The paper relies on Pinkus' theorem to show that MLPs can approximate the log output function needed for the softmax construction.
  - Quick check question: What are the key conditions under which MLPs can approximate continuous functions, and how does this apply to the log function in this context?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden state -> Output function -> Projection -> Final output

- Critical path:
  1. Initialize hidden state η with initial PFSA state distribution
  2. For each input symbol, update hidden state via recurrence matrix U
  3. Mask hidden state to current symbol using input matrix V and bias b
  4. Transform hidden state via output function F
  5. Project to probability simplex via π
  6. Output conditional probability distribution

- Design tradeoffs:
  - Sparsemax vs softmax: Sparsemax gives exact simulation but is less common; softmax is more practical but requires approximation
  - Linear vs non-linear output: Linear output with normalization is simpler but may limit expressiveness; non-linear output allows more complex transformations
  - MLP depth: Deeper MLPs can approximate more complex functions but increase computational cost and risk overfitting

- Failure signatures:
  - NaN or Inf values in hidden state: Check for numerical instability in recurrence or output function
  - Invalid probability distributions: Verify that projection function produces valid probabilities and hidden state encoding is correct
  - Poor approximation quality: Check that full-support approximation is good and MLP can approximate log function well

- First 3 experiments:
  1. Implement exact simulation of a simple deterministic PFSA using sparsemax normalization and verify that output matches PFSA probabilities
  2. Implement exact simulation of a non-deterministic PFSA using softmax normalization and extended reals, verify weak equivalence
  3. Implement approximation of a regular LM using softmax and MLP output, measure total variation distance for different MLP architectures and compare to theoretical bounds

## Open Questions the Paper Calls Out
None

## Limitations
- Exact simulation results require non-standard normalization (sparsemax or extended reals) that are not commonly used in practice
- Approximation results using standard softmax and MLP output functions only guarantee small total variation distance, not exact equivalence
- Theoretical bounds on approximation quality may not translate to practical performance on real language modeling tasks

## Confidence
- **High confidence**: The theoretical framework connecting Elman LMs to PFSAs is sound, and the basic constructions for exact simulation (Mechanisms 1 and 2) are mathematically correct given the stated assumptions.
- **Medium confidence**: The approximation results (Mechanism 3) are valid in theory, but their practical implications are less certain due to potential issues with numerical stability and the quality of MLP approximations in high-dimensional spaces.
- **Low confidence**: The claim that these results have significant implications for understanding the expressivity of practical RNN language models is speculative, as the exact simulation mechanisms are not standard in practice.

## Next Checks
1. **Numerical Stability Verification**: Implement the exact simulation mechanisms (sparsemax and extended reals) and test them on increasingly complex PFSAs to identify numerical stability issues and practical limitations.

2. **Approximation Quality Assessment**: Train Elman LMs with MLP output functions on regular language modeling tasks and measure the total variation distance to the true regular LMs, comparing against the theoretical bounds to assess practical approximation quality.

3. **Practical Expressivity Test**: Compare the performance of Elman LMs using the exact simulation mechanisms to standard RNN LMs on a diverse set of language modeling benchmarks to evaluate the practical benefits and limitations of the theoretical results.