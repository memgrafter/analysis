---
ver: rpa2
title: Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU Transformers
arxiv_id: '2407.07848'
source_url: https://arxiv.org/abs/2407.07848
tags:
- hidden
- layer
- units
- used
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores sparsity patterns in ReLU transformer MLPs
  across different layers during training. The authors measure hidden unit activation
  sparsity at token, sequence, and batch levels, finding that the first and last layers
  exhibit distinct and inverted sparsity behaviors.
---

# Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU Transformers

## Quick Facts
- **arXiv ID**: 2407.07848
- **Source URL**: https://arxiv.org/abs/2407.07848
- **Reference count**: 33
- **Primary result**: First and last layers of ReLU transformers exhibit inverted sparsity patterns - layer 0 uses fewer hidden units per batch but activates them more frequently, while the last layer uses more hidden units per batch but activates them less frequently

## Executive Summary
This paper investigates sparsity patterns in ReLU transformer MLPs across different layers during training. The authors measure hidden unit activation sparsity at token, sequence, and batch levels, revealing that the first and last layers exhibit distinctive and inverted relationships to sparsity. While the first layer uses fewer hidden units per batch, it activates the ones it does use more frequently. Conversely, the final layer uses the most hidden units per batch but activates them less frequently within sequences. The study also examines "dead neurons" and demonstrates that sparsity is driven by learning dynamics rather than random noise, suggesting potential for computational optimization by removing unused neurons after initial training phases.

## Method Summary
The study uses a 6-layer decoder-only transformer with 32768 hidden dimensions, trained on C4 next token prediction using LeCun Normal initialization for dense kernels within MLPs and AdamW optimizer with cosine decay learning rate schedule. The model is trained on GCP TPU-v2 slice of type v2-32, taking about 2.5 days to reach convergence. Three core metrics are defined: Per-Token Hidden Unit Use (averaged over sequence and batch), Per-Sequence Hidden Unit Use (number of nonzero hidden units anywhere in a sequence), and Per-Batch Hidden Unit Use (number of nonzero hidden units anywhere in a batch). The authors track these metrics throughout training, focusing on the first 70,000 steps where behavior stabilizes.

## Key Results
- First and last layers exhibit inverted sparsity relationships - layer 0 uses fewer hidden units per batch but activates them more frequently, while the last layer uses more hidden units per batch but activates them less frequently
- Neuron "death" occurs due to learning dynamics rather than random noise, with some neurons becoming inactive early in training and remaining unused for the remainder of training
- The number of effectively usable hidden units varies by layer depth, with each layer having a minimum absolute capacity requirement for good performance
- Neurons unused after 50,000 training steps can be masked for the remaining 90% of training without impacting final performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first and last layers exhibit inverted sparsity patterns due to differences in feature specificity.
- Mechanism: Early layers learn dense, shared subspace features where many dimensions are used but with lower frequency per token. Later layers learn sparse, binary-like features where fewer dimensions are used but each is used more frequently per token.
- Core assumption: Feature learning at different depths of the network has fundamentally different statistical properties.
- Evidence anchors:
  - [abstract] "we demonstrate that the first and last layer of the network have distinctive and in many ways inverted relationships to sparsity"
  - [section 4.3] "Layer 0 is the most extreme case – it converges to active use of a substantially smaller set of hidden units (1), but without a commensurate drop in number of hidden units used per token"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism

### Mechanism 2
- Claim: Neuron "death" occurs due to learning dynamics rather than random noise or outliers.
- Mechanism: During training, certain hidden units become systematically unused when they fall below the ReLU threshold and the learning process fails to reactivate them. This occurs in specific training phases rather than randomly.
- Core assumption: The training process has identifiable phases where certain units become inactive and remain so.
- Evidence anchors:
  - [abstract] "show evidence suggesting that 'neuron death' is being primarily driven by the dynamics of training, rather than simply occurring randomly or accidentally as a result of outliers"
  - [section 4.2] "we believe these two facts are suggestive that sparsity is being driven by the model's construction of certain feature spaces to operate in, rather than simply happening accidentally and pathologically"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism

### Mechanism 3
- Claim: Layer depth determines the number of effectively usable hidden units.
- Mechanism: Each layer has a minimum number of hidden units required for good performance, but beyond that, only a fraction of the total capacity is effectively utilized. This fraction varies by layer depth.
- Core assumption: The model reveals its capacity needs through sparsity patterns at convergence.
- Evidence anchors:
  - [section 4.4] "we hypothesize that each layer has a certain fractional subset of initialization space that it is able to effectively use for the features that layer preferentially learns"
  - [section 4.4] "we observe the number of hidden units used at convergence in a standard experimental setting...we see the model trained in Round 2 uses a similar per-layer fraction of its available capacity"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism

## Foundational Learning

- Concept: ReLU activation and its effect on sparsity
  - Why needed here: Understanding how ReLU creates hard-zero sparsity is fundamental to interpreting the activation patterns
  - Quick check question: What happens to negative values in a ReLU activation?

- Concept: Layer-wise feature learning in deep networks
  - Why needed here: The inverted sparsity patterns between layers are explained by different feature learning strategies at different depths
  - Quick check question: How do feature representations typically change as you move from early to late layers in a network?

- Concept: Training dynamics and weight initialization
  - Why needed here: The paper shows that neuron death and sparsity patterns emerge from specific phases in training, not random noise
  - Quick check question: What is the relationship between weight initialization and activation patterns in early training?

## Architecture Onboarding

- Component map: Embedding -> Attention -> MLP (two dense layers with ReLU) -> Output
- Critical path: Forward pass through embedding → attention → MLP (two dense layers with ReLU) → output
- Design tradeoffs: ReLU activation chosen for its sparsity-inducing properties, but this limits direct applicability to modern models using smooth activations
- Failure signatures: Performance collapse when randomly masking hidden units vs. no degradation when masking units unused in early training batches
- First 3 experiments:
  1. Measure per-token, per-sequence, and per-batch hidden unit activation for each layer in a trained model
  2. Track how these metrics evolve during early training (first 10k steps)
  3. Test whether masking unused units after 50k steps affects final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do sparsity patterns in ReLU transformers persist when using smooth activation functions like GELU or SiLU?
- Basis in paper: [explicit] The authors cite Mirzadeh et al. showing that ReLU sparsity patterns can be induced in GELU/SiLU models through fine-tuning, but note this is indirect evidence
- Why unresolved: The paper focuses exclusively on ReLU models due to their hard sparsity, making direct comparison with smooth activations difficult
- What evidence would resolve it: Direct empirical comparison of sparsity patterns across activation functions in identically structured transformers trained from scratch

### Open Question 2
- Question: What is the minimum absolute number of hidden units required for optimal performance in each layer, and does this explain the observed sparsity patterns?
- Basis in paper: [inferred] The authors hypothesize that each layer has a minimum absolute capacity requirement, and when total hidden dimensions are reduced, the model proportionally reduces used dimensions while maintaining the same fraction
- Why unresolved: The paper only tests this hypothesis indirectly by showing that reducing total hidden dimensions doesn't change the fraction used, but doesn't determine the absolute minimum
- What evidence would resolve it: Systematic ablation studies varying both total hidden dimensions and per-layer allocations to find the minimum working capacity for each layer

### Open Question 3
- Question: What causes the first layer to use dramatically fewer hidden units per batch compared to other layers, and can this be optimized?
- Basis in paper: [explicit] The authors observe this distinctive behavior and suggest it may be due to "Feature Specificity" where layer 0 learns features more densely distributed across a smaller vector space
- Why unresolved: The paper presents this as a hypothesis but doesn't test alternative explanations or optimization strategies
- What evidence would resolve it: Experiments testing different initialization schemes, training strategies, or architectural modifications specifically for layer 0 to increase its batch-level hidden unit usage

## Limitations

- **Activation Function Specificity**: The study focuses exclusively on ReLU activations, limiting generalizability to modern LLMs using smoother activations like GeLU
- **Single Architecture and Scale**: Analysis is based on a single 6-layer decoder-only transformer with 32K hidden dimensions, patterns may not extend to different architectures or scales
- **Static Analysis of Trained Models**: The study examines sparsity patterns at convergence and during early training but doesn't explore how these patterns might evolve in later training stages

## Confidence

- **High Confidence**: The empirical observations of inverted sparsity patterns between first and last layers, and the correlation between neuron death and training dynamics rather than random noise
- **Medium Confidence**: The proposed mechanisms explaining why different layers exhibit distinct sparsity patterns (e.g., feature specificity differences between early and late layers)
- **Low Confidence**: The hypothesis that each layer has a fixed fractional capacity that can be effectively utilized

## Next Checks

1. **Cross-Activation Validation**: Test whether the inverted sparsity patterns persist when using GeLU or other smooth activations instead of ReLU to determine if patterns are activation-specific

2. **Architecture Scaling Study**: Examine sparsity patterns in transformers with different depths (e.g., 12, 24, 36 layers) and widths to determine if layer-dependent sparsity relationships scale predictably with model size

3. **Training Phase Extension**: Track neuron death and sparsity patterns beyond the initial 70k steps into later training phases to determine if observed patterns remain stable or evolve, and how this affects final model performance