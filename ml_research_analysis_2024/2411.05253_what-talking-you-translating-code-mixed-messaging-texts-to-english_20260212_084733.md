---
ver: rpa2
title: 'What talking you?: Translating Code-Mixed Messaging Texts to English'
arxiv_id: '2411.05253'
source_url: https://arxiv.org/abs/2411.05253
tags:
- language
- singlish
- translation
- languages
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of Large Language Models (LLMs)
  for translating Singlish (colloquial Singaporean English) to formal English. The
  authors developed a two-step prompting scheme: first detecting languages present
  in the code-mixed text, then translating to formal English.'
---

# What talking you?: Translating Code-Mixed Messaging Texts to English

## Quick Facts
- arXiv ID: 2411.05253
- Source URL: https://arxiv.org/abs/2411.05253
- Reference count: 16
- Primary result: LLM-based translation of Singlish to formal English achieved BLEU scores of only 0.363, significantly below oracle baselines

## Executive Summary
This paper investigates the use of Large Language Models (LLMs) for translating Singlish (colloquial Singaporean English) to formal English. The authors developed a two-step prompting scheme: first detecting languages present in the code-mixed text, then translating to formal English. They evaluated five LLMs (Mistral-7B, LLaMA-3.1, Gemma-2, Qwen-2.5, and Phi-3.1) on 300 SMS-style Singlish messages annotated by native speakers. The study reveals that while LLMs show promise for understanding code-mixed languages, they currently fall short of accurate translation, particularly for nuanced contexts requiring cultural understanding. The best-performing model (Gemma-2) achieved a BLEU score of 0.363, ROUGE-L of 0.606, and BertScore of 0.931, still significantly below oracle baselines.

## Method Summary
The authors employed a two-step prompting scheme using five open-sourced LLMs for translating Singlish to formal English. First, they used a language detection prompt to identify which languages were present in the code-mixed text and the corresponding phrases. Second, they used a translation prompt to convert the detected Singlish text to formal English. The models were evaluated on 300 SMS-style Singlish messages from the NUS-SMS-Corpus, annotated by native speakers for language detection and translation. The evaluation metrics included BLEU, ROUGE-L, and BertScore for translation quality, along with language detection accuracy. The authors also released their annotated dataset as a resource for future research.

## Key Results
- Gemma-2 achieved the best translation performance with BLEU score of 0.363, ROUGE-L of 0.606, and BertScore of 0.931
- Language detection accuracy was 65.1% for span detection when restricted to top-1 predictions
- Translation quality degraded significantly as more languages were mixed in a sentence (from 0.407 BLEU for single-language to 0.179 for three-language sentences)

## Why This Works (Mechanism)
The study demonstrates that LLMs can leverage their multilingual capabilities to identify language components in code-mixed texts and perform translation, though with significant quality limitations. The two-step prompting approach allows the model to first decompose the code-mixed input into its constituent languages before attempting translation, which helps manage the complexity of mixed-language inputs. However, the mechanism struggles with the nuanced linguistic features of Singlish, including lack of subject pronouns, ambiguous tenses, dropped transitive verbs, and missing punctuation, which require contextual understanding beyond simple pattern matching.

## Foundational Learning

**Code-mixing patterns** - Understanding how languages interleave within sentences is crucial for detecting language boundaries and translation units. Quick check: Identify mixing patterns in sample Singlish sentences.

**Linguistic features of Singlish** - Knowledge of Singlish-specific characteristics (subject pronoun omission, tense ambiguity, dropped verbs) is needed to interpret why LLMs struggle with certain translation aspects. Quick check: Compare Singlish sentence structures with formal English equivalents.

**LLM prompting strategies** - Multi-step prompting for complex NLP tasks requires understanding how to decompose problems into manageable sub-tasks. Quick check: Design alternative prompt templates for language detection.

**Evaluation metrics for translation** - BLEU, ROUGE-L, and BertScore each capture different aspects of translation quality and are needed to comprehensively assess LLM performance. Quick check: Calculate each metric on sample translations.

**Cultural context in translation** - Singlish relies heavily on context for accurate understanding, making cultural knowledge essential for high-quality translations. Quick check: Identify context-dependent Singlish phrases requiring cultural interpretation.

## Architecture Onboarding

**Component map**: Language Detection Prompt -> Translation Prompt -> Evaluation Metrics (BLEU/ROUGE-L/BertScore)

**Critical path**: The two-step prompting scheme (language detection followed by translation) forms the critical path, as accurate language identification is prerequisite for quality translation.

**Design tradeoffs**: The study chose open-sourced LLMs and a single prompting scheme due to resource constraints, trading off potential performance gains from fine-tuning or more sophisticated prompting strategies.

**Failure signatures**: Performance degradation with increased language mixing, systematic errors in distinguishing similar languages (Hokkien vs Teochew), and inability to handle context-dependent meanings.

**First experiments**:
1. Test alternative prompt templates (zero-shot, few-shot, Chain-of-Thought) on language detection accuracy
2. Evaluate translation quality on sentences with varying numbers of mixed languages
3. Compare performance of fine-tuned models versus general-purpose LLMs on the same tasks

## Open Questions the Paper Calls Out

**Linguistic feature optimization** - What specific linguistic features or patterns in code-mixed texts could improve LLM performance on language detection and translation tasks? The paper demonstrates LLMs' current limitations but doesn't explore what specific features could be leveraged to improve performance.

**Cross-language generalization** - How does LLM performance on code-mixed translation tasks vary across different language pairs and mixing patterns beyond Singlish? The study focuses exclusively on Singlish and notes performance degradation with more languages but doesn't systematically compare different language pairs.

**Cultural context integration** - What is the impact of cultural context and domain-specific knowledge on the accuracy of LLM translations for code-mixed languages? While the paper identifies the importance of cultural context, it doesn't quantify its impact or explore methods to incorporate such knowledge.

**Prompting strategy comparison** - How do different prompting strategies and fine-tuning approaches affect LLM performance on code-mixed language tasks? The authors acknowledge testing only one prompting scheme and suggest other approaches could improve results.

## Limitations

- LLMs struggle with accurate language span detection in code-mixed Singlish, showing only 65.1% accuracy for top-1 predictions
- Translation quality remains significantly below human-level, with the best model achieving BLEU scores around 0.36
- Findings may not generalize to other code-mixed language pairs beyond the specific Singlish context studied
- The study used only open-sourced LLMs and one prompting scheme due to resource constraints

## Confidence

**High confidence**: LLMs struggle with accurate language span detection in code-mixed Singlish, supported by the 65.1% span detection accuracy and detailed error analysis.

**Medium confidence**: Translation quality assessment is reliable given comprehensive evaluation using multiple metrics across five different LLMs, though absolute scores indicate fundamental limitations.

**Low confidence**: Generalizability of findings to other code-mixed language pairs, as the study focuses exclusively on Singlish with specific linguistic features that may not apply elsewhere.

## Next Checks

1. **Prompt Engineering Validation**: Test whether alternative prompt templates or few-shot learning approaches can improve language detection accuracy, particularly for the span detection component where current performance is weakest.

2. **Model Architecture Comparison**: Evaluate whether specialized fine-tuned models (rather than general-purpose LLMs) show improved performance on the language detection and translation tasks, controlling for model size to isolate architecture effects.

3. **Linguistic Feature Analysis**: Conduct a detailed error analysis categorizing translation failures by linguistic feature (e.g., tense ambiguity, subject pronoun omission) to determine whether specific Singlish characteristics disproportionately impact LLM performance.