---
ver: rpa2
title: 'WARP: On the Benefits of Weight Averaged Rewarded Policies'
arxiv_id: '2406.16768'
source_url: https://arxiv.org/abs/2406.16768
tags:
- reward
- figure
- policies
- slerp
- anchor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses reward hacking and forgetting in LLM alignment
  by introducing a new RLHF strategy called Weight Averaged Rewarded Policies (WARP).
  WARP merges policies in the weight space at three stages: using exponential moving
  average (EMA) as a dynamic anchor for KL regularization, applying spherical interpolation
  to merge independently fine-tuned policies, and linearly interpolating towards the
  initialization.'
---

# WARP: On the Benefits of Weight Averaged Rewarded Policies

## Quick Facts
- arXiv ID: 2406.16768
- Source URL: https://arxiv.org/abs/2406.16768
- Reference count: 40
- Primary result: Introduces WARP, a novel RLHF strategy that improves LLM alignment by merging policies in weight space using EMA anchors, SLERP, and LITI interpolation

## Executive Summary
WARP addresses reward hacking and forgetting in LLM alignment by introducing a new RLHF strategy that merges policies in weight space at three distinct stages. It uses exponential moving average (EMA) as a dynamic anchor for KL regularization, applies spherical interpolation to merge independently fine-tuned policies, and linearly interpolates towards the initialization. This iterative process progressively refines the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with Gemma "7B" validate that WARP improves LLM quality and alignment, outperforming other open-source LLMs. The WARP procedure increases compute cost but allows scaling alignment without inference overhead.

## Method Summary
WARP is a novel RLHF strategy that aligns large language models by merging policies in weight space through three stages: (1) using EMA as a dynamic KL regularization anchor during RL fine-tuning, (2) applying spherical linear interpolation (SLERP) to merge independently fine-tuned policies, and (3) linearly interpolating towards initialization (LITI) to recover pre-training features. The method is applied iteratively, with the final model serving as initialization for the next round. The approach aims to achieve better KL-reward trade-offs than traditional RLHF methods while mitigating reward hacking and forgetting of pre-trained knowledge.

## Key Results
- WARP achieves superior rewards at fixed KL compared to baseline RLHF methods
- The method outperforms other open-source LLMs in alignment quality on Gemma "7B"
- WARP transforms additional compute into enhanced capabilities and safety without increasing inference overhead

## Why This Works (Mechanism)

### Mechanism 1
Dynamic EMA anchor provides progressive KL relaxation and distillation from a moving target. Using the policy's own EMA as the KL anchor allows regularization strength to automatically decrease over training, starting strong when EMA tracks SFT initialization and becoming a better-performing target as training progresses.

### Mechanism 2
Spherical linear interpolation (SLERP) of task vectors combines reward strengths without collapsing norms. Merging independently fine-tuned policies via SLERP in task vector space combines their reward-maximizing capabilities while preserving update magnitudes, avoiding norm collapse seen with linear interpolation.

### Mechanism 3
Linear interpolation towards initialization (LITI) reveals improved KL-reward Pareto fronts. Interpolating the merged model back toward initialization allows traversal of continuous trade-offs, trading reward for reduced KL and recovering generalizable features lost during fine-tuning.

## Foundational Learning

- **KL regularization in RLHF**
  - Why needed: Prevents forgetting pre-trained knowledge and reward hacking by constraining policy to stay close to initialization
  - Quick check: What happens to KL and reward if you set β=0 in KL-regularized RLHF?

- **Weight averaging and model merging**
  - Why needed: Enables combination of independently trained policies to achieve better KL-reward trade-offs than any single policy
  - Quick check: Why might averaging weights from different fine-tuned models perform better than either individual model?

- **Task vectors and linear mode connectivity**
  - Why needed: Task vectors (weight deltas from initialization) can be manipulated via interpolation to combine model capabilities; linear mode connectivity ensures interpolated models maintain good performance
  - Quick check: What property of fine-tuned models from the same initialization makes weight interpolation effective?

## Architecture Onboarding

- **Component map**: SFT initialization → Multiple independent RL fine-tunings with EMA anchors → SLERP merging → LITI interpolation → New initialization for next iteration
- **Critical path**: RL fine-tuning with EMA → SLERP merging → LITI interpolation → (optional) iteration
- **Design tradeoffs**:
  - EMA update rate μ: Higher values track policy more closely (faster adaptation, less stability); lower values provide more stable anchor but slower adaptation
  - Number of independent RL runs M: More runs provide more diversity for merging but increase compute cost
  - LITI coefficient η: Higher values preserve more reward but increase KL; lower values reduce KL but may lose reward
- **Failure signatures**:
  - KL exploding during RL training → likely β too low or EMA not stabilizing
  - Reward not improving after merging → task vectors may not be sufficiently diverse or orthogonal
  - No improvement in Pareto front after LITI → initialization may be too dissimilar from merged model
- **First 3 experiments**:
  1. Run baseline REINFORCE with SFT anchor vs. EMA anchor, compare KL-reward trajectories
  2. Train two policies independently with EMA, merge with SLERP vs. LERP, compare rewards and KL
  3. Apply LITI to merged model with different η values, plot resulting Pareto front against RL trajectories

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of SLERP compare to LERP when merging policies with significantly different KL-reward trade-offs? The paper shows SLERP consistently obtains higher rewards than LERP but at slightly higher KL, yet doesn't provide detailed comparison for policies with widely varying trade-offs.

### Open Question 2
How does the choice of EMA update rate (μ) and KL regularization strength (β) impact final performance? While the paper shows some ablation experiments, it doesn't systematically explore optimal hyperparameter values across different scenarios.

### Open Question 3
How does WARP perform when applied to other types of models beyond Gemma "7B"? The paper validates efficacy on Gemma "7B" but doesn't explore performance on different architectures or model sizes.

## Limitations
- Experimental validation relies on Gemma "7B" on a single alignment task with human preference data
- Computational overhead of multiple independent RL fine-tunings and iterative WARP process is not fully quantified
- Scalability claims regarding compute-robustness trade-off lack empirical support beyond Gemma "7B" experiments

## Confidence

**High Confidence**: Theoretical framework for using EMA as dynamic anchor is well-grounded in prior work on self-distillation and KL regularization. Spherical interpolation mechanism follows established model merging principles.

**Medium Confidence**: LITI mechanism's ability to recover pre-training features and improve Pareto front is plausible but requires more empirical validation. Orthogonality assumption for task vectors enabling effective SLERP needs verification.

**Low Confidence**: Scalability claims regarding compute-robustness trade-off and assertion that WARP can "transform additional compute into enhanced capabilities and safety" are not empirically supported beyond Gemma "7B" experiments.

## Next Checks

1. Apply WARP to different LLM architectures (e.g., Llama, Mistral) and evaluate whether KL-reward improvements generalize beyond Gemma "7B"

2. Test WARP on multiple alignment tasks beyond human preference ranking, including instruction following and safety alignment, to verify robustness of Pareto improvements

3. Quantify wall-clock time and resource requirements of WARP relative to baseline RLHF across different scales, and verify whether claimed compute-robustness trade-off holds in practice