---
ver: rpa2
title: Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment
arxiv_id: '2405.17931'
source_url: https://arxiv.org/abs/2405.17931
tags:
- merging
- online
- rlhf
- alignment
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) with human preferences while mitigating the "alignment tax" - the degradation
  of pre-trained abilities. The authors propose Online Merging Optimizers, which integrate
  model merging into each RLHF optimization step by blending gradients with the parameter
  differences between SFT and pre-trained models.
---

# Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment

## Quick Facts
- **arXiv ID**: 2405.17931
- **Source URL**: https://arxiv.org/abs/2405.17931
- **Reference count**: 40
- **Primary result**: Online Merging Optimizers integrate model merging into each RLHF step, achieving up to 1.57 improvement on AlpacaEval 2.0 and 1.3 on average benchmark scores compared to baselines.

## Executive Summary
This paper addresses the challenge of aligning large language models (LLMs) with human preferences while mitigating the "alignment tax" - the degradation of pre-trained abilities during RLHF. The authors propose Online Merging Optimizers that integrate model merging into each RLHF optimization step by blending gradients with the parameter differences between SFT and pre-trained models. This steers the gradient towards maximizing rewards in the direction of SFT optimization. Extensive experiments across various LLM families, sizes, RLHF algorithms, and model merging methods show that Online Merging Optimizers significantly enhance alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.

## Method Summary
The method integrates model merging into each RLHF optimization step by blending gradients with the parameter differences (delta) between SFT and pre-trained models. This continuous online merging approach progressively steers the training trajectory toward local minima that better balance alignment rewards and capability retention compared to single-time offline merging. The authors propose two variants: OnDARE using random sparsification and OnTIES using top-k sparsification, both designed to be compatible with various RLHF algorithms including DPO, IPO, and KTO.

## Key Results
- Online Merging Optimizers consistently achieve better performance than baselines across all tested model families (Qwen1.5, LLaMA3-8B) and RLHF algorithms
- OnDARE variant shows superior stability at low parameter reserve rates down to 5e-4 compared to OnTIES
- Improvements of up to 1.57 on AlpacaEval 2.0 and 1.3 on average benchmark scores compared to baselines
- Effective mitigation of alignment tax while improving reward signals across 14 diverse benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online merging at each RLHF step continuously steers gradients toward SFT-optimized directions, balancing reward maximization with capability retention.
- Mechanism: The optimizer blends the policy gradient with the parameter differences (delta) between SFT and pre-trained models, effectively guiding updates along a path that preserves foundational abilities while optimizing for human preferences.
- Core assumption: The delta parameters between SFT and pre-trained models represent the optimal direction for maintaining capabilities, and these can be effectively merged with RLHF gradients at each step.
- Evidence anchors:
  - [abstract]: "we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization"
  - [section]: "we consider the alterations in model parameters before and after the SFT phase, termed delta parameters, as the final update direction for the SFT model"
  - [corpus]: Weak evidence; no directly comparable papers found in corpus
- Break condition: If delta parameters become unstable or diverge significantly from the optimal capability-preserving direction during training, the merging would become counterproductive.

### Mechanism 2
- Claim: Continuous online merging is more effective than single-time offline merging for balancing alignment rewards and capability retention.
- Mechanism: Unlike offline merging which only interpolates fixed model parameters once, online merging applies task arithmetic merging at every optimization step, allowing the model to progressively find local minima that better balance both objectives.
- Core assumption: The optimal balance point between alignment rewards and capability retention shifts during training as the policy model evolves.
- Evidence anchors:
  - [abstract]: "through iterative model merging at each step of RLHF training, integrating the strengths of SFT with each update to maximize reward, we can progressively steer our training trajectory"
  - [section]: "finding a single point along this continuum that outperforms both ends of the spectrum simultaneously is indeed challenging. However, through iterative model merging at each RLHF step...we can progressively steer our training trajectory"
  - [corpus]: Weak evidence; no directly comparable papers found in corpus
- Break condition: If the online merging causes the training to converge to a suboptimal local minimum that is worse than both the pure RLHF and pure SFT models.

### Mechanism 3
- Claim: The OnDARE variant with random sparsification provides more stable training than OnTIES with top-k sparsification when parameter reserve rates are very low.
- Mechanism: Random sparsification maintains unbiased sampling across all parameters, while top-k sparsification introduces significant bias during training, making OnDARE more robust to extreme parameter reduction.
- Core assumption: Unbiased sparsification methods provide better gradient direction preservation during extreme parameter reduction scenarios.
- Evidence anchors:
  - [section]: "we empirically find directly optimizing Eq. 1 is unstable and hard to converge...we apply a relaxation on Eq. 1 by moving the τ (t) out of the merging and only focus on ∆θ(t)"
  - [section]: "Online merging optimizers remain robust even at low parameter reserve rates down to 5e−4...OnTIES is more sensitive to extremely low parameter reserve rates compared to OnDARE"
  - [corpus]: Weak evidence; no directly comparable papers found in corpus
- Break condition: If random sparsification fails to maintain sufficient gradient signal even at moderate parameter retention rates.

## Foundational Learning

- Concept: Mode Connectivity
  - Why needed here: The paper relies on the assumption that neural network local optima are connected by simple curves, which justifies the interpolation of model parameters for capability balancing.
  - Quick check question: What is the theoretical basis that allows interpolating between two trained models without catastrophic performance degradation?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial because the alignment tax is essentially a form of catastrophic forgetting where models lose pre-trained abilities during RLHF.
  - Quick check question: How does the alignment tax relate to the traditional concept of catastrophic forgetting in continual learning?

- Concept: Task Arithmetic and Delta Parameters
  - Why needed here: The core mechanism uses delta parameters (task vectors) to represent learned capabilities, which are then merged to balance different model behaviors.
  - Quick check question: How are delta parameters computed between two models, and what do they represent in terms of learned capabilities?

## Architecture Onboarding

- Component map: Base RLHF pipeline (SFT model, RLHF training loop, reward model) -> New Online Merging Optimizer (merging logic, sparsification, consensus methods) -> Gradient computation step -> Parameter update step -> Model caching for delta parameter computation

- Critical path:
  1. Compute standard RLHF gradient using AdamW or similar optimizer
  2. Apply sparsification to the gradient (random for OnDARE, top-k for OnTIES)
  3. Merge sparsified gradient with sparsified delta parameters from reference SFT model
  4. Apply merged gradient to update policy model parameters
  5. Cache current parameters for next iteration's delta computation

- Design tradeoffs:
  - Memory vs performance: Online merging requires caching delta parameters of reference model, increasing memory usage but providing better performance
  - Sparsification bias vs stability: Random sparsification (OnDARE) is less biased but may lose signal; top-k (OnTIES) is more targeted but introduces bias
  - Onlineness vs computational cost: More frequent merging provides better results but increases computation per step

- Failure signatures:
  - Training instability or divergence after introducing online merging
  - Degradation in MT-Bench and AlpacaEval scores despite improvements in other benchmarks
  - Inconsistent performance across different model sizes or architectures
  - Sensitivity to merging weight α or parameter reserve rate p

- First 3 experiments:
  1. Implement OnDARE with Qwen1.5-1.8B-Chat on ULTRAFEEDBACK with default parameters, verify it improves over AdamW baseline
  2. Test sensitivity to parameter reserve rate p by running with values {1, 0.1, 0.01, 0.001} and observe performance trends
  3. Compare OnDARE vs OnTIES on the same model to understand which sparsification method works better for your specific use case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Online Merging Optimizer perform on extremely large language models (e.g., 70B+ parameters) and does its effectiveness scale with model size?
- Basis in paper: [inferred] The paper demonstrates effectiveness across model sizes from 1.8B to 8B, but does not test larger models.
- Why unresolved: The computational resources required for training and evaluating models of this scale are significant, and the paper's experimental setup may not have had access to such resources.
- What evidence would resolve it: Training and evaluating the Online Merging Optimizer on models with 70B+ parameters and comparing its performance to baselines on alignment benchmarks.

### Open Question 2
- Question: Can the Online Merging Optimizer be effectively applied to other domains beyond natural language processing, such as computer vision or reinforcement learning tasks?
- Basis in paper: [inferred] The paper focuses on LLM alignment and mentions the potential for application to other domains facing catastrophic forgetting.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis of the optimizer's performance in other domains.
- What evidence would resolve it: Applying the Online Merging Optimizer to computer vision tasks (e.g., image classification) or reinforcement learning environments and evaluating its impact on performance and catastrophic forgetting.

### Open Question 3
- Question: What is the optimal schedule for adjusting the merging weight (α) during training, and how does it impact the final model performance?
- Basis in paper: [explicit] The paper mentions that the merging weight α is a hyperparameter that controls the strength of regularization and suggests starting with a small value, but does not explore dynamic adjustment during training.
- Why unresolved: The paper only experiments with fixed values of α, and the impact of varying it over time is unknown.
- What evidence would resolve it: Experimenting with different schedules for adjusting α during training (e.g., linearly increasing, cosine annealing) and comparing the resulting model performance on alignment benchmarks.

## Limitations

- The effectiveness relies on the assumption that delta parameters between SFT and pre-trained models represent optimal capability-preserving directions, which may not hold for all model architectures or tasks
- The study focuses on specific model sizes (1.8B, 7B, 8B) and may not generalize to larger frontier models where different dynamics could emerge
- Claims rely heavily on empirical validation across specific model families and datasets, with limited theoretical guarantees

## Confidence

- **High confidence**: Claims about OnDARE outperforming OnTIES in low parameter reserve rate scenarios
- **Medium confidence**: General claims about alignment tax mitigation
- **Medium confidence**: Claims about online merging superiority over offline methods

## Next Checks

1. **Cross-architecture validation**: Test OnDARE and OnTIES on larger model families (e.g., 70B+ parameters) to verify scalability and whether the observed benefits persist at frontier model scales.

2. **Alternative dataset evaluation**: Apply the same methodology to different preference datasets beyond ULTRA FEEDBACK to assess robustness and generalizability of the alignment tax mitigation claims.

3. **Theoretical characterization**: Conduct ablation studies varying the delta parameter computation method and sparsification strategies to better understand the underlying mechanisms driving performance improvements.