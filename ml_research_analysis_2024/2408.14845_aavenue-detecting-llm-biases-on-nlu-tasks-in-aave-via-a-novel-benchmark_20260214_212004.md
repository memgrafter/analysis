---
ver: rpa2
title: 'AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark'
arxiv_id: '2408.14845'
source_url: https://arxiv.org/abs/2408.14845
tags:
- translations
- tasks
- across
- copa
- boolq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AAVENUE, a novel benchmark designed to evaluate
  large language models (LLMs) on natural language understanding (NLU) tasks in African
  American Vernacular English (AAVE) compared to Standard American English (SAE).
  The benchmark extends existing work by leveraging LLM-based translation with few-shot
  prompting to generate AAVE versions of GLUE and SuperGLUE tasks, improving upon
  the deterministic transformations used in prior benchmarks.
---

# AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark

## Quick Facts
- arXiv ID: 2408.14845
- Source URL: https://arxiv.org/abs/2408.14845
- Reference count: 4
- Introduces AAVENUE benchmark for evaluating LLM biases on NLU tasks in African American Vernacular English

## Executive Summary
This paper introduces AAVENUE, a novel benchmark designed to evaluate large language models (LLMs) on natural language understanding (NLU) tasks in African American Vernacular English (AAVE) compared to Standard American English (SAE). The benchmark extends existing work by leveraging LLM-based translation with few-shot prompting to generate AAVE versions of GLUE and SuperGLUE tasks, improving upon the deterministic transformations used in prior benchmarks. Evaluations across five popular LLMs and comprehensive metrics—including fluency, BARTScore, quality, coherence, and understandability—show that the AAVENUE translations outperform or match previous benchmarks. Human validation by fluent AAVE speakers confirmed the authenticity of the translations. The study reveals that LLMs consistently perform better on SAE tasks than their AAVE-translated counterparts, underscoring inherent biases and the need for more inclusive NLP models. The source code and website are publicly available.

## Method Summary
AAVENUE employs a three-step translation pipeline to convert GLUE and SuperGLUE NLU tasks from SAE to AAVE. First, it uses few-shot prompting with GPT-4 to translate individual sentences from GLUE/SuperGLUE datasets into AAVE. Second, it filters translations to retain only those sentences that are semantically equivalent to their SAE counterparts using an entailment model (RoBERTa-large-mnli). Finally, it reconstructs complete NLU task examples by recombining the translated sentences. This approach differs from previous benchmarks like DANBERT, which used deterministic transformations. The benchmark evaluates translations using fluency, quality, coherence, and understandability metrics, and validates results with human evaluation from 20 fluent AAVE speakers.

## Key Results
- AAVENUE translations outperform or match DANBERT on fluency, BARTScore, quality, coherence, and understandability metrics
- Human validation confirmed translation authenticity, though with a small sample size of 20 participants
- LLMs consistently perform better on SAE tasks than AAVE-translated counterparts across all evaluated models
- Translation approach using LLM few-shot prompting produces more authentic AAVE compared to deterministic transformations

## Why This Works (Mechanism)
The LLM-based few-shot prompting approach works better than deterministic transformations because it captures the nuanced, context-dependent patterns of AAVE rather than applying rigid syntactic rules. GPT-4 can learn from examples to generate translations that reflect authentic AAVE usage patterns, including idiomatic expressions and grammatical structures that vary by context. This method also allows for semantic preservation through the entailment filtering step, ensuring that translated sentences maintain their original meaning while being rendered in AAVE. The human validation component provides ground truth feedback that confirms the translations capture genuine AAVE patterns rather than artificial constructions.

## Foundational Learning
- **AAVE linguistic features**: Understanding the grammatical, phonological, and lexical characteristics of African American Vernacular English is essential for evaluating whether translations are authentic and for interpreting performance differences. Quick check: Review linguistic studies on AAVE to understand its distinctive features.
- **Few-shot prompting techniques**: The translation pipeline relies on GPT-4's ability to learn translation patterns from a small number of examples. Quick check: Review few-shot prompting literature to understand how models generalize from limited examples.
- **Entailment models for semantic preservation**: RoBERTa-large-mnli is used to ensure translated sentences maintain semantic equivalence with original SAE sentences. Quick check: Understand how entailment models work and their reliability for cross-dialect semantic preservation.
- **GLUE and SuperGLUE benchmarks**: These NLU task collections provide the foundation for evaluating model performance differences between SAE and AAVE. Quick check: Review the task types and evaluation metrics used in GLUE and SuperGLUE.
- **Benchmark construction methodology**: Understanding how to create valid, reliable benchmarks that can detect model biases requires knowledge of evaluation design principles. Quick check: Review best practices for creating bias detection benchmarks in NLP.

## Architecture Onboarding

Component Map:
AAVENUE Benchmark -> Translation Pipeline (GPT-4 few-shot prompting -> Entailment filtering -> Sentence reconstruction) -> Evaluation Metrics (Fluency, BARTScore, Quality, Coherence, Understandability) -> Human Validation -> Model Performance Analysis

Critical Path:
Translation Pipeline → Human Validation → Performance Gap Analysis
The core workflow processes SAE sentences through LLM translation, filters for semantic equivalence, reconstructs tasks, evaluates translation quality, validates with humans, and measures model performance differences.

Design Tradeoffs:
- LLM-based translation vs. deterministic rules: Offers more authentic AAVE but introduces model dependency and potential variability
- Small human validation sample vs. comprehensive coverage: Balances practical constraints with validation needs
- Few-shot prompting vs. fine-tuning: Faster to implement but may be less consistent than dedicated fine-tuning

Failure Signatures:
- Translation artifacts that reveal GPT-4 patterns rather than authentic AAVE usage
- Semantic drift during translation that affects task validity
- Human validation showing low agreement indicating translation quality issues
- Performance differences that don't align with translation quality metrics

First Experiments:
1. Compare AAVENUE translations against human-translated AAVE sentences to assess authenticity
2. Test whether performance gaps persist when using different translation methods (deterministic vs. LLM)
3. Evaluate whether performance differences are consistent across different NLU task types and difficulty levels

## Open Questions the Paper Calls Out
None

## Limitations
- Small human validation sample size (20 participants) limits generalizability of authenticity claims
- Reliance on GPT-4 for translations raises questions about whether outputs reflect authentic AAVE or model imitation
- Focus on only BERT, RoBERTa, and DeBERTa architectures limits generalizability to other model families
- Comparison with only one prior benchmark (DANBERT) restricts ability to assess broader improvements

## Confidence

- **AAVE translation quality and authenticity**: Medium confidence. While human validation was conducted, the small sample size and lack of demographic diversity details weaken confidence in the authenticity claims.
- **Performance gap between SAE and AAVE**: High confidence. The consistent performance differences across multiple models and tasks are robust, though the underlying causes require further investigation.
- **LLM-based translation superiority**: Medium confidence. The comparison with DANBERT shows improvements, but the lack of broader benchmark comparisons limits generalizability.

## Next Checks

1. Expand human validation to a larger, demographically diverse sample of AAVE speakers and include speakers from different regions and age groups to better assess translation authenticity.

2. Compare AAVENUE translations against additional dialect-specific benchmarks beyond DANBERT to establish whether observed improvements are consistent across different evaluation frameworks.

3. Investigate whether performance gaps persist when using alternative translation methods, such as human-translated datasets or hybrid human-AI approaches, to isolate whether the gap stems from translation quality or model biases.