---
ver: rpa2
title: On Minimizing Adversarial Counterfactual Error in Adversarial RL
arxiv_id: '2406.04724'
source_url: https://arxiv.org/abs/2406.04724
tags:
- adversarial
- state
- robust
- learning
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial Counterfactual Error (ACoE) to
  address robustness in Deep Reinforcement Learning against adversarial perturbations.
  ACoE measures the value difference between unperturbed and adversarially perturbed
  scenarios by explicitly considering belief states about the true underlying state.
---

# On Minimizing Adversarial Counterfactual Error in Adversarial RL

## Quick Facts
- arXiv ID: 2406.04724
- Source URL: https://arxiv.org/abs/2406.04724
- Reference count: 40
- One-line primary result: ACoE-based methods achieve state-of-the-art robustness against adversarial attacks while maintaining high nominal performance

## Executive Summary
This paper introduces Adversarial Counterfactual Error (ACoE) to address robustness in Deep Reinforcement Learning against adversarial perturbations. ACoE measures the value difference between unperturbed and adversarially perturbed scenarios by explicitly considering belief states about the true underlying state. The authors propose Cumulative-ACoE (C-ACoE) as a scalable surrogate and integrate it with standard RL algorithms like PPO and DQN. Experiments on MuJoCo, Atari, and Highway benchmarks show that ACoE-based methods (A2B and A3B) achieve state-of-the-art robustness against both myopic and long-horizon adversarial attacks while maintaining high nominal performance.

## Method Summary
The authors propose ACoE as a measure of robustness that compares expected returns between unperturbed and perturbed belief states. They introduce C-ACoE as a scalable surrogate objective that can be integrated into standard RL algorithms. Two belief estimation methods are proposed: A2B uses KL divergence between action distributions to assign belief mass to neighborhood states, while A3B further incorporates worst-case PGD attack effectiveness. These methods are combined with PPO and DQN by augmenting the advantage computation with C-ACoE estimation. The resulting algorithms jointly maximize expected reward and minimize robustness error.

## Key Results
- ACoE-based methods (A2B and A3B) outperform existing methods like Protected, RADIAL, WocaR, and RAD across all tested domains
- A3B provides better robustness than A2B against stronger adversaries due to its attack-aware belief estimation
- The methods maintain high nominal performance while achieving superior robustness to both myopic and long-horizon adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ACoE reduces robustness degradation by modeling the belief state over the true underlying state instead of relying on perturbed observations directly
- **Mechanism**: ACoE explicitly accounts for partial observability induced by adversarial perturbations by maintaining a belief distribution over possible true states. This belief is updated using a Bayesian filter conditioned on the adversary's perturbation policy
- **Core assumption**: The true state lies within an $\ell_\infty$-bounded neighborhood of the perturbed observation, and the adversary's perturbation policy can be estimated from observed transitions
- **Break condition**: If the adversary can induce large observation changes beyond the bounded $\epsilon$-neighborhood or if the belief update fails to track the true state due to adversarial noise

### Mechanism 2
- **Claim**: Cumulative-ACoE (C-ACoE) enables scalable model-free optimization while preserving theoretical guarantees
- **Mechanism**: C-ACoE approximates ACoE by recursively estimating the long-term return difference between unperturbed and perturbed belief states. It is incorporated into standard RL algorithms via advantage augmentation
- **Core assumption**: The Bellman optimality structure holds for C-ACoE, enabling its integration into standard RL updates
- **Break condition**: If the surrogate C-ACoE diverges significantly from true ACoE due to model-free estimation errors

### Mechanism 3
- **Claim**: Adversary-aware belief estimation (A2B) and attack-aware belief estimation (A3B) provide practical belief constructions
- **Mechanism**: A2B assigns belief mass to states in the neighborhood of the observation based on KL divergence between action distributions, while A3B further weights states by comparing observed perturbations versus worst-case PGD attacks
- **Core assumption**: KL divergence between action distributions is a reliable proxy for the severity of adversarial perturbations
- **Break condition**: If the action distribution KL divergence fails to correlate with actual state perturbation severity

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - **Why needed here**: Adversarial perturbations transform the observable state into a POMDP by making the true state only partially observable through corrupted observations
  - **Quick check question**: How does a belief state in a POMDP differ from a simple observation in a standard MDP, and why is this distinction critical for adversarial robustness?

- **Concept**: Bayesian belief updating under adversarial observation models
  - **Why needed here**: The belief update must account for the adversary's perturbation policy, which is not present in standard POMDP belief updates that assume stochastic observation noise
  - **Quick check question**: What additional term must be included in the belief update equation when the observation model is adversarially controlled rather than stochastic?

- **Concept**: KL divergence as a measure of policy sensitivity to state perturbations
  - **Why needed here**: Both A2B and A3B use KL divergence between action distributions to estimate how much a state perturbation affects the policy
  - **Quick check question**: Why is KL divergence between $\pi(s)$ and $\pi(s_o)$ a reasonable proxy for measuring the impact of adversarial state perturbations on policy behavior?

## Architecture Onboarding

- **Component map**: Observation -> Belief Estimator (A2B/A3B) -> Policy Network -> Action; Observation -> Value Network -> State Value; Observation -> C-ACoE Network -> Robustness Estimate
- **Critical path**: 1. Observe state $s_o$ from environment; 2. Compute belief $b$ using A2B/A3B estimator; 3. Evaluate policy $\pi_\theta(s_o)$ and value $V_\phi(s_o)$; 4. Estimate C-ACoE $\delta_\psi(s_o)$ using belief and sampled neighborhood states; 5. Compute augmented advantage and update networks via PPO/DQN
- **Design tradeoffs**: Neighborhood size vs. computational cost (larger neighborhoods provide better belief estimates but increase runtime quadratically); Belief construction method (A2B is simpler but less attack-aware than A3B); Robustness hyperparameter $\lambda$ (controls tradeoff between nominal performance and robustness)
- **Failure signatures**: High variance in C-ACoE estimates indicates poor belief construction or insufficient neighborhood sampling; Performance degradation under strong attacks suggests the belief estimator is being deceived; Convergence issues may indicate hyperparameter $\lambda$ is too high
- **First 3 experiments**: 1. Verify belief construction: Compare A2B and A3B belief distributions on a simple 2D grid environment with known adversarial perturbations; 2. Test C-ACoE estimation: Implement a toy MDP with known state transitions and verify that C-ACoE estimates match analytical values; 3. Validate integration: Run PPO with C-ACoE augmentation on a simple continuous control task and verify that robustness improves without sacrificing too much nominal performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the belief construction method scale to continuous state spaces with high-dimensional observations?
- Basis in paper: [explicit] The paper mentions using uniform sampling for continuous state spaces and provides a lemma showing unbiased estimation, but does not fully explore the computational complexity or accuracy implications for high-dimensional observations
- Why unresolved: While the paper provides theoretical justification for sampling-based belief estimation, it does not empirically evaluate how this approach performs as state dimensionality increases or how many samples are needed for accurate belief estimation
- What evidence would resolve it: Systematic experiments varying state dimensionality and sample sizes, measuring belief estimation accuracy and computational overhead across different observation spaces

### Open Question 2
- Question: What is the theoretical relationship between the belief state estimation error and the effectiveness of the ACoE objective?
- Basis in paper: [inferred] The paper establishes that ACoE measures the value difference between unperturbed and perturbed scenarios using beliefs about true states, but does not quantify how errors in belief estimation affect the robustness guarantees or performance
- Why unresolved: The paper assumes accurate belief estimation in its theoretical analysis but uses approximate methods in practice, without characterizing the impact of estimation errors on the theoretical bounds or empirical performance
- What evidence would resolve it: Formal analysis of how belief estimation errors propagate through the ACoE calculation, coupled with empirical studies measuring performance degradation as belief estimation accuracy decreases

### Open Question 3
- Question: Can the ACoE framework be extended to handle multiple concurrent adversaries with different objectives?
- Basis in paper: [explicit] The paper focuses on a single adversary model and does not address scenarios with multiple adversaries, though it mentions that stronger adversaries can always be found
- Why unresolved: The current ACoE formulation is designed for a single adversarial perturbation policy, and the paper does not explore how the framework would need to be modified to handle competing or cooperative adversaries with different attack strategies
- What evidence would resolve it: Theoretical extensions of the ACoE formulation to multi-adversary settings, and empirical validation showing robustness against multiple adversaries with different objectives and perturbation strategies

## Limitations
- Neighborhood-based belief estimation may not scale well to high-dimensional state spaces beyond tested MuJoCo, Atari, and Highway domains
- Computational complexity of C-ACoE estimation grows quadratically with neighborhood size, potentially limiting real-world applicability
- The assumption that KL divergence between action distributions reliably measures perturbation severity remains unverified for complex, non-linear policies

## Confidence
- **High Confidence**: The experimental results showing ACoE-based methods outperforming baselines on tested benchmarks
- **Medium Confidence**: The theoretical justification for C-ACoE as a scalable surrogate for ACoE
- **Low Confidence**: The general applicability of belief estimation methods to domains with significantly different observation structures

## Next Checks
1. **Scalability Test**: Implement ACoE with varying neighborhood sizes on a high-dimensional continuous control task (e.g., Humanoid) to quantify computational overhead and performance degradation
2. **Cross-Domain Transfer**: Apply A2B and A3B to a non-vision domain (e.g., robotics manipulation) to verify belief estimation robustness across different observation modalities
3. **Adversary Strength Analysis**: Systematically vary attack budgets and observe at what point ACoE-based methods begin to fail, comparing against theoretical bounds on neighborhood size requirements