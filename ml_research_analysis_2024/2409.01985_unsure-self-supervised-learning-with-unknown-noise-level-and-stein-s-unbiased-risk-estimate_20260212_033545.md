---
ver: rpa2
title: 'UNSURE: self-supervised learning with Unknown Noise level and Stein''s Unbiased
  Risk Estimate'
arxiv_id: '2409.01985'
source_url: https://arxiv.org/abs/2409.01985
tags:
- noise
- where
- learning
- methods
- sure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new self-supervised learning framework for
  denoising problems, addressing the limitation of existing SURE methods that require
  knowledge of the noise level. The authors introduce the "expected divergence-free"
  (ZED) constraint, requiring only that the average divergence of the estimator be
  zero, rather than zero for every input.
---

# UNSURE: self-supervised learning with Unknown Noise level and Stein's Unbiased Risk Estimate

## Quick Facts
- arXiv ID: 2409.01985
- Source URL: https://arxiv.org/abs/2409.01985
- Authors: Julián Tachella; Mike Davies; Laurent Jacques
- Reference count: 22
- One-line primary result: Introduces UNSURE, a self-supervised learning framework for denoising that handles unknown noise levels through the "expected divergence-free" constraint

## Executive Summary
This paper presents UNSURE, a self-supervised learning framework that addresses the limitation of existing Stein's Unbiased Risk Estimate (SURE) methods that require knowledge of the noise level. By introducing the "expected divergence-free" (ZED) constraint, which requires only that the average divergence of the estimator be zero rather than zero for every input, the method creates more expressive estimators that outperform cross-validation methods while remaining robust to unknown noise levels. The framework provides closed-form solutions for various noise distributions including Gaussian (with unknown variance or spatial correlation), Poisson-Gaussian, and distributions in the exponential family.

## Method Summary
The method formulates denoising as a constrained optimization problem using Lagrange multipliers, where the estimator must satisfy the ZED constraint. The loss function combines the SURE-like objective with a divergence-free constraint enforced through Lagrange multipliers η and γ. Training alternates between gradient descent on network parameters and gradient ascent on the multipliers. The divergence term is approximated using Monte Carlo sampling with perturbations controlled by parameter τ. The method employs a U-Net backbone architecture and provides analytical solutions for the optimal estimator under different noise models, allowing it to handle unknown noise levels while maintaining theoretical guarantees.

## Key Results
- Achieves state-of-the-art performance compared to existing self-supervised methods on MNIST, DIV2K, LIDC, and FastMRI datasets
- Outperforms cross-validation methods while remaining robust to unknown noise levels
- Approaches supervised learning performance when signal distributions are low-dimensional
- Successfully handles spatially correlated noise and Poisson-Gaussian noise common in real-world imaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "expected divergence-free" (ZED) constraint allows the estimator to learn from noisy data without requiring noise level knowledge while being more expressive than cross-validation methods.
- Mechanism: By requiring only that the expected divergence of the estimator be zero (rather than zero for every input), the method relaxes the constraints on the estimator compared to cross-validation approaches, allowing it to incorporate more information from the input while still providing self-supervised learning capability.
- Core assumption: The signal distribution is low-dimensional, meaning the mean squared error between the MMSE estimator and the noisy input is small relative to the noise level.
- Break condition: If the signal distribution has high entropy (like N(0,I)), the ZED estimator becomes the trivial guess f(y) = 0.

### Mechanism 2
- Claim: The proposed method provides closed-form solutions for various noise distributions including Gaussian (with unknown variance or spatial correlation), Poisson-Gaussian, and distributions in the exponential family.
- Mechanism: By formulating the learning problem as a constrained optimization with Lagrange multipliers, the method derives analytical expressions for the optimal estimator that incorporate the score function of the noisy data distribution without requiring explicit knowledge of noise parameters.
- Core assumption: The noise distribution belongs to a known family (Gaussian, Poisson-Gaussian, exponential family) even if specific parameters like variance or correlation structure are unknown.
- Break condition: If the noise distribution doesn't belong to the assumed family or has complex dependencies not captured by the model.

### Mechanism 3
- Claim: The method achieves state-of-the-art performance compared to existing self-supervised methods, with results approaching those of supervised learning when the signal distribution is low-dimensional.
- Mechanism: By combining the ZED constraint with the score-based formulation, the method creates an estimator that performs gradient descent on the log probability of the noisy data, with a step size that conservatively estimates the noise level without requiring its explicit knowledge.
- Core assumption: Natural signal distributions are low-dimensional, meaning they lie on or near a low-dimensional manifold in the high-dimensional space.
- Break condition: If the signal distribution is high-dimensional or has significant correlation structure not captured by the model.

## Foundational Learning

- Concept: Stein's Unbiased Risk Estimate (SURE)
  - Why needed here: SURE provides the theoretical foundation for self-supervised learning in denoising problems by relating the expected mean squared error to a divergence term that can be estimated from noisy data alone.
  - Quick check question: How does SURE allow learning without ground-truth data?

- Concept: Score function estimation
  - Why needed here: The method relies on estimating the score (gradient of log probability) of the noisy data distribution, which is used to construct the ZED estimator without requiring explicit noise level knowledge.
  - Quick check question: What is the relationship between the score function and the optimal ZED estimator?

- Concept: Low-dimensional signal distributions
  - Why needed here: The effectiveness of the method depends on the assumption that natural signals lie on or near low-dimensional manifolds, which allows the ZED estimator to perform close to optimal even without knowing the noise level.
  - Quick check question: Why does the ZED estimator perform poorly on high-entropy distributions like N(0,I)?

## Architecture Onboarding

- Component map: Network -> Loss computation (SURE-like with Monte Carlo divergence) -> Lagrange multiplier optimization -> Updated network parameters
- Critical path: Forward pass through U-Net -> Compute SURE-like loss with Monte Carlo divergence approximation -> Update network weights via AdamW -> Update Lagrange multipliers via gradient ascent
- Design tradeoffs: Computational complexity vs robustness to unknown noise levels; more expressive estimators vs potential instability in high-dimensional scenarios
- Failure signatures: Poor convergence of Lagrange multipliers, divergence estimation instability, performance degradation on high-entropy signal distributions
- First 3 experiments:
  1. Gaussian denoising on MNIST with known noise level to validate the basic framework and compare with standard SURE
  2. Gaussian denoising on MNIST with unknown noise level to demonstrate robustness compared to standard SURE
  3. Colored Gaussian noise denoising on DIV2K to test the method's ability to handle spatially correlated noise with unknown correlation structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UNSURE scale with the dimensionality of the signal distribution, particularly in high-entropy scenarios where the signal distribution approaches a high-dimensional Gaussian?
- Basis in paper: [explicit] The paper discusses that ZED estimators fail with high-entropy distributions like px = N(0,Iσ2x), where the noise level estimator cannot distinguish between signal and noise variance, resulting in trivial estimates.
- Why unresolved: The paper only provides theoretical analysis and limited empirical evaluation on this specific limitation, without extensive experiments on high-dimensional, high-entropy signal distributions.
- What evidence would resolve it: Experiments comparing UNSURE performance across signal distributions with varying entropy and dimensionality, particularly showing the transition point where performance degrades significantly.

### Open Question 2
- Question: What is the optimal trade-off between expressivity and robustness when choosing the parameterization of the noise covariance matrix Ση in correlated noise scenarios?
- Basis in paper: [explicit] The paper presents a family of estimators with different parameterizations of Ση (diagonal, circulant) and mentions that the dimension s offers a trade-off between optimality and robustness to misspecified covariance.
- Why unresolved: While the paper demonstrates that different parameterizations work, it doesn't provide a systematic analysis of how to choose the optimal parameterization or the number of parameters s for a given application.
- What evidence would resolve it: A comprehensive study comparing different covariance parameterizations and their parameter counts across various correlated noise scenarios, establishing guidelines for choosing the optimal trade-off.

### Open Question 3
- Question: How does the performance of UNSURE compare to supervised learning methods when the signal distribution has complex dependencies that are not captured by the assumed noise model?
- Basis in paper: [inferred] The paper assumes certain noise models (Gaussian, Poisson-Gaussian, exponential family) and shows that UNSURE approaches supervised learning performance when the signal distribution is low-dimensional, but doesn't explore scenarios with model mismatch.
- Why unresolved: The paper focuses on cases where the noise model assumptions hold, without exploring robustness to misspecified noise models or complex signal dependencies.
- What evidence would resolve it: Experiments comparing UNSURE and supervised learning performance under various model misspecifications, including non-Gaussian noise, heteroscedastic noise, and complex signal dependencies.

## Limitations

- Effectiveness critically depends on the low-dimensional structure of natural signals, which may not hold for all imaging domains
- Computational overhead from Lagrange multiplier optimization and Monte Carlo divergence estimation could limit scalability
- Theoretical guarantees assume the noise distribution belongs to a known family, which may not capture all real-world noise characteristics
- Requires careful tuning of hyperparameters like τ for divergence estimation and learning rate for Lagrange multipliers

## Confidence

- High confidence in theoretical framework and closed-form solutions (mathematically rigorous extensions of existing SURE methods)
- Medium confidence in experimental results (strong performance across multiple datasets but lacks extensive ablation studies)
- Medium confidence in state-of-the-art performance claims (limited comparison to supervised methods across specific noise levels and datasets)

## Next Checks

1. **Ablation study**: Systematically remove the Lagrange multiplier optimization and Monte Carlo divergence estimation to quantify their individual contributions to performance gains.

2. **High-dimensional test**: Evaluate the method on high-entropy signal distributions (e.g., N(0,I)) to verify the claimed failure mode and quantify the degradation in performance.

3. **Cross-dataset generalization**: Train on one dataset (e.g., MNIST) and test on completely different domains (e.g., medical imaging) to assess the robustness of the low-dimensional signal assumption across diverse applications.