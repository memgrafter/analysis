---
ver: rpa2
title: 'TED: Accelerate Model Training by Internal Generalization'
arxiv_id: '2405.03228'
source_url: https://arxiv.org/abs/2405.03228
tags:
- pruning
- training
- data
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TED (Internal Generalization Distance) pruning,
  a dynamic dataset pruning method for accelerating deep learning training. The key
  idea is quantifying "Internal Generalization" - how fitting retained data also improves
  performance on pruned data - through an optimization objective based on Internal
  Generalization Distance (IGD).
---

# TED: Accelerate Model Training by Internal Generalization

## Quick Facts
- arXiv ID: 2405.03228
- Source URL: https://arxiv.org/abs/2405.03228
- Authors: Jinying Xiao; Ping Li; Jie Nie
- Reference count: 40
- Primary result: Achieves lossless performance with 60-70% data reduction on multiple benchmarks

## Executive Summary
This paper introduces TED (Internal Generalization Distance) pruning, a dynamic dataset pruning method that accelerates deep learning training by selectively removing samples during training. The key innovation is quantifying "Internal Generalization" - how fitting retained data also improves performance on pruned data - through an optimization objective based on Internal Generalization Distance (IGD). TED measures sample importance using small mask fluctuations with Taylor approximation for fast IGD estimation, and employs a progressive "roller-coaster" pruning schedule that increases pruning ratios during training. Experiments demonstrate TED achieves lossless performance with 60-70% data reduction across image classification, natural language understanding, and LLM fine-tuning tasks, significantly outperforming prior state-of-the-art methods.

## Method Summary
TED is a dynamic dataset pruning method that accelerates training by removing samples while maintaining model performance. It works by quantifying Internal Generalization - how fitting retained data improves performance on pruned data - through an IGD optimization objective. Sample importance is measured using small mask fluctuations with Taylor approximation for efficient IGD estimation. The method employs a progressive "roller-coaster" pruning schedule that starts with low pruning ratios and exponentially increases them during training. This allows aggressive pruning early when model dynamics are stable, then slows down to preserve important samples later. The approach includes unbiased loss correction and Polyak averaging for score updates, enabling effective pruning without retraining.

## Key Results
- Achieves lossless performance with 60-70% data reduction on CIFAR-10/100, ImageNet, GLUE, and LLaMA2-7B fine-tuning
- Significantly outperforms prior state-of-the-art pruning methods, particularly at high pruning ratios
- Demonstrates effectiveness across diverse domains: image classification, natural language understanding, and large language model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TED measures sample importance by quantifying small mask fluctuations using Taylor approximation to estimate IGD efficiently.
- Mechanism: The method uses masks on data samples and approximates the impact of removing a sample on Internal Generalization Distance (IGD) via first-order Taylor expansion. This allows fast estimation without retraining models.
- Core assumption: The mask perturbation is small enough that Taylor approximation remains accurate for estimating IGD.
- Evidence anchors:
  - [abstract] "measures sample importance by small mask fluctuations using Taylor approximation to enable fast IGD estimation"
  - [section 3.2] "For a sample zk, the impact of zk on IG can be approximated using Taylor expansion"
  - [corpus] Weak - corpus neighbors don't directly address Taylor approximation for IGD estimation
- Break condition: If mask perturbations become too large, the Taylor approximation breaks down and IGD estimation becomes inaccurate.

### Mechanism 2
- Claim: TED's "roller-coaster" pruning schedule progressively increases pruning ratios during training, which aligns better with training dynamics.
- Mechanism: The pruning ratio starts low and increases exponentially according to rt = exp{(t/T)^β log(1-s)}, where β controls the rate of increase. This allows aggressive pruning early when model dynamics are stable, then slows down to preserve important samples later.
- Core assumption: Training dynamics require different pruning intensities at different stages, with early stages tolerating higher pruning.
- Evidence anchors:
  - [abstract] "progressive pruning strategy with a 'roller-coaster' schedule that increases pruning ratios during training"
  - [section 3.4] "We discovered that using progressive pruning can save more resources while maintaining performance" and "Our schedule is a fast-to-slow approach"
  - [corpus] Weak - corpus doesn't directly address progressive pruning schedules
- Break condition: If the β hyperparameter is poorly tuned, the schedule may prune too aggressively early or too conservatively late.

### Mechanism 3
- Claim: TED achieves implicit regularization by optimizing IGD, which aligns with true generalization performance.
- Mechanism: By optimizing the Internal Generalization Distance (IGD) between models trained on retained vs. full datasets, TED ensures the model maintains generalization capability even when trained on reduced data. This creates an implicit regularization effect.
- Core assumption: Optimizing IGD directly correlates with minimizing upper bounds on generalization error.
- Evidence anchors:
  - [abstract] "uses an optimization objective based on Internal Generalization Distance (IGD), measuring changes in IG before and after pruning to align with true generalization performance and achieve implicit regularization"
  - [section 3.2] "Lemma 1... the parameters have the smallest upper bound on generalization error"
  - [corpus] Weak - corpus neighbors don't directly address IGD as regularization mechanism
- Break condition: If the relationship between IGD and generalization error breaks down for certain model architectures or datasets.

## Foundational Learning

- Concept: Internal Generalization (IG)
  - Why needed here: Understanding IG is crucial because TED's core innovation is quantifying how fitting retained data also improves performance on pruned data, rather than just focusing on the retained data itself.
  - Quick check question: Can you explain in one sentence what "Internal Generalization" means in the context of dataset pruning?

- Concept: Taylor approximation for sensitivity analysis
  - Why needed here: TED relies on first-order Taylor expansion to efficiently estimate the impact of small mask changes on IGD, avoiding expensive retraining.
  - Quick check question: Why is Taylor approximation appropriate for estimating the effect of small mask perturbations on IGD?

- Concept: Progressive learning rate schedules
  - Why needed here: TED's "roller-coaster" schedule is conceptually similar to progressive learning rate schedules, requiring understanding of how different training phases benefit from different optimization intensities.
  - Quick check question: How does a progressive pruning schedule differ from a fixed pruning ratio in terms of training dynamics?

## Architecture Onboarding

- Component map: Mask → IGD Estimation → Scoring → Pruning Decision → Model Update
- Critical path: Mask → IGD Estimation → Scoring → Pruning Decision → Model Update
- Design tradeoffs:
  - Accuracy vs. speed: Taylor approximation trades some accuracy for much faster IGD estimation
  - Pruning aggressiveness vs. stability: Roller-coaster schedule balances early aggressive pruning with later stability
  - Memory vs. performance: Storing scores for all samples requires O(n) memory but enables efficient pruning decisions
- Failure signatures:
  - Performance degradation when β is too small (over-aggressive early pruning)
  - Noisy score updates when α is too low (insufficient Polyak averaging)
  - Poor generalization when Taylor approximation error accumulates
- First 3 experiments:
  1. Implement mask-based IGD estimation on a small dataset (e.g., CIFAR-10 subset) and verify Taylor approximation accuracy
  2. Test different β values in the roller-coaster schedule on CIFAR-10 to find optimal balance between speed and accuracy
  3. Compare TED's performance against random pruning and InfoBatch on GLUE benchmark with BERT-base model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TED's performance change when applied to larger transformer models like BERT-large or GPT-3 compared to BERT-base?
- Basis in paper: [explicit] The paper only tested TED on BERT-base for NLU tasks and LLaMA2-7B for LLM fine-tuning, suggesting potential scalability concerns for larger models.
- Why unresolved: The experiments only covered moderate-sized models, leaving the effectiveness on truly large-scale models unexplored.
- What evidence would resolve it: Experiments testing TED on larger transformer architectures (BERT-large, GPT-3, etc.) with various downstream tasks would demonstrate its scalability and limitations.

### Open Question 2
- Question: Can TED's IGD optimization objective be adapted for non-classification tasks like regression or reinforcement learning?
- Basis in paper: [inferred] The IGD framework is defined around classification loss (cross-entropy), but the underlying principle of measuring generalization impact could theoretically apply to other task types.
- Why unresolved: All experiments were conducted on classification tasks, leaving uncertainty about TED's applicability to different problem domains.
- What evidence would resolve it: Implementation and evaluation of TED on regression tasks (e.g., image regression) and RL environments would validate its generalizability.

### Open Question 3
- Question: What is the computational overhead of TED compared to baseline training when implemented in distributed training settings?
- Basis in paper: [explicit] While TED claims to accelerate training, the paper doesn't analyze the overhead of mask computations and gradient tracking in distributed environments.
- Why unresolved: The paper focuses on sample reduction benefits but doesn't quantify the per-iteration computational cost of TED's scoring mechanism.
- What evidence would resolve it: Benchmarking TED's wall-clock time per epoch against standard training across different batch sizes and distributed configurations would clarify the practical speedup.

## Limitations

- Experimental scope limited to standard benchmark datasets, with untested effectiveness on domain-specific or highly imbalanced datasets
- Implementation complexity requires careful hyperparameter tuning, particularly for the β parameter controlling the roller-coaster schedule
- Computational overhead not fully analyzed, particularly for mask-based IGD estimation requiring O(n) storage for all sample scores

## Confidence

**High Confidence**: The core mechanism of using Internal Generalization Distance (IGD) for sample importance scoring is well-supported by theoretical foundations and experimental evidence. The progressive pruning schedule and its benefits are demonstrated across multiple datasets.

**Medium Confidence**: The effectiveness of the Taylor approximation for IGD estimation is theoretically sound but may have limitations for larger mask perturbations or certain model architectures. The empirical validation shows promising results but doesn't systematically explore the approximation's accuracy bounds.

**Low Confidence**: The generalization claims to various domains, model architectures, and extreme pruning ratios are not empirically validated. The method's behavior on specialized datasets or under resource constraints remains speculative.

## Next Checks

1. **Cross-Domain Robustness**: Test TED on specialized datasets (medical imaging, satellite data, or domain-specific NLP corpora) to validate generalization beyond standard benchmarks.

2. **Extreme Pruning Analysis**: Systematically evaluate TED's performance at pruning ratios of 80-95% to identify the point where performance degradation becomes significant and compare against alternative methods.

3. **Computational Overhead Benchmarking**: Conduct detailed runtime and memory usage analysis comparing TED against baseline methods, particularly measuring the overhead introduced by the mask-based IGD estimation and progressive scheduling.