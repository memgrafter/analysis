---
ver: rpa2
title: 'Do not trust what you trust: Miscalibration in Semi-supervised Learning'
arxiv_id: '2403.15567'
source_url: https://arxiv.org/abs/2403.15567
tags:
- samples
- learning
- calibration
- predictions
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reveals that semi-supervised learning (SSL) methods based
  on pseudo-labels suffer from severe miscalibration due to an underlying min-entropy
  minimization. The authors show that this causes overconfident predictions, even
  for incorrect classes.
---

# Do not trust what you trust: Miscalibration in Semi-supervised Learning

## Quick Facts
- arXiv ID: 2403.15567
- Source URL: https://arxiv.org/abs/2403.15567
- Reference count: 40
- This work reveals that SSL methods based on pseudo-labels suffer from severe miscalibration due to an underlying min-entropy minimization.

## Executive Summary
This work identifies a critical flaw in semi-supervised learning (SSL) methods that rely on pseudo-labels: they tend to produce overconfident predictions due to an implicit min-entropy minimization during training. The authors demonstrate that this phenomenon arises from the unsupervised loss term, which pushes predictions toward simplex vertices, amplifying early errors. To address this, they propose a simple penalty term that constrains the magnitude of logit distances for unlabeled samples, effectively reducing overconfidence. Experiments across multiple SSL benchmarks show significant improvements in both calibration (up to 6-7% in ECE) and accuracy, often outperforming state-of-the-art methods.

## Method Summary
The paper proposes a penalty term to improve the calibration of SSL methods by constraining logit distances for unlabeled samples. The method is applied to SSL models like FixMatch, FlexMatch, and FreeMatch, using Vision Transformer (ViT-Small) architecture on datasets such as CIFAR-100, STL-10, and EuroSAT. The key idea is to prevent the model from pushing unlabeled samples toward very confident regions by adding a margin-based penalty on the logit distances between the winning class and other classes. This penalty is only applied to a subset of unlabeled samples (DU'') where weak and strong augmentations yield consistent predictions. The approach is evaluated using classification error rate and Expected Calibration Error (ECE).

## Key Results
- The proposed penalty improves calibration (up to 6-7% reduction in ECE) and accuracy across multiple SSL benchmarks.
- The method outperforms state-of-the-art SSL approaches in both accuracy and calibration.
- Applying the penalty only to the DU'' subset (not DU') is critical for maintaining performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSL methods based on pseudo-labels minimize min-entropy on unlabeled samples, causing overconfident predictions even for incorrect classes.
- Mechanism: The unsupervised loss term approximates a min-entropy regularization, which aggressively pushes predictions to simplex vertices, amplifying early errors.
- Core assumption: The majority of unlabeled samples in DU'' have consistent hard predictions between weak and strong augmentations.
- Evidence anchors:
  - [abstract] "we formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration."
  - [section] "Based on our observations we argue that the training of SSL methods based on pseudo-labels can be approximated by a supervised term coupled with a regularization loss that minimizes the min-entropy of unlabeled samples."
  - [corpus] Weak—no direct mention of min-entropy in neighbors, suggesting this is a novel contribution.
- Break condition: If the proportion of samples in DU'' drops significantly (e.g., <50%), the min-entropy effect weakens.

### Mechanism 2
- Claim: The proposed penalty term constrains logit distances to remain below a margin, reducing overconfident predictions.
- Mechanism: By penalizing large logit differences for the winning class against others, the network is discouraged from pushing logits to extreme values, thus lowering softmax confidence for incorrect classes.
- Core assumption: Large logit magnitudes correlate with miscalibration and can be reduced without harming accuracy.
- Evidence anchors:
  - [abstract] "we propose to use a simple solution that refrains the model from pushing unlabeled samples towards very unconfident regions, improving the calibration of pseudo-label SSL methods."
  - [section] "This constraint... takes the following form d(l) ≤ m... where d(l) = (max j(lj) − lk)1≤k≤K ∈ RK represents the vector of logit distances between the winner class and the rest."
  - [corpus] No direct evidence in neighbors; assumed from experimental results.
- Break condition: If the margin m is set too large, the penalty becomes ineffective; if too small, it may degrade accuracy.

### Mechanism 3
- Claim: The proposed method improves both calibration (lower ECE) and accuracy on multiple SSL benchmarks.
- Mechanism: By reducing overconfidence in incorrect predictions, the model avoids propagating false pseudo-labels, leading to cleaner learning signals and better generalization.
- Core assumption: Reducing logit magnitudes for incorrect classes improves overall model reliability without harming discriminative power.
- Evidence anchors:
  - [abstract] "Experiments across multiple SSL benchmarks and methods demonstrate that this improves both calibration (up to 6-7% in ECE) and accuracy, often outperforming state-of-the-art."
  - [section] "These results demonstrate that integrating the penalty in Eq. (8) during training appears as an appealing strategy to improve both accuracy and calibration performance of pseudo-label SSL approaches."
  - [corpus] Neighbors discuss pseudo-label quality and bias mitigation, supporting the general theme of improving SSL reliability.
- Break condition: If the penalty hyperparameter λ is not tuned properly, it may hurt accuracy or fail to improve calibration.

## Foundational Learning

- Concept: Semi-supervised learning with pseudo-labels
  - Why needed here: The paper’s mechanism depends on how pseudo-labels are generated and used; understanding this is critical to grasping why miscalibration occurs.
  - Quick check question: What is the difference between hard and soft pseudo-labels in SSL, and how does each affect the training objective?

- Concept: Entropy and min-entropy in information theory
  - Why needed here: The miscalibration mechanism is tied to minimizing min-entropy, a specific case of Rényi entropy; knowing this helps explain why SSL models become overconfident.
  - Quick check question: How does min-entropy differ from Shannon entropy in terms of gradient behavior and their effects on model predictions?

- Concept: Calibration metrics (ECE) and logit distributions
  - Why needed here: Evaluating the proposed method requires interpreting ECE scores and understanding how logit magnitudes relate to calibration; this is essential for assessing experimental results.
  - Quick check question: How does a high ECE score indicate miscalibration, and what role do logit magnitudes play in this?

## Architecture Onboarding

- Component map:
  - SSL backbone (e.g., Vision Transformer) -> Weak augmentation module (random crop, flip) -> Strong augmentation module (RandAugment) -> Pseudo-label generator (argmax over weak predictions) -> Penalty term (ReLU-based margin constraint on logit distances) -> Loss combiner (cross-entropy + pseudo-CE + penalty)

- Critical path:
  1. Forward pass with weak augmentation → generate pseudo-label
  2. Forward pass with strong augmentation → get logits
  3. Compute cross-entropy loss on labeled data
  4. Compute pseudo-CE loss on unlabeled data with threshold filter
  5. Compute penalty term only on DU'' samples
  6. Backward pass and update

- Design tradeoffs:
  - Applying penalty on DU'' only vs. both DU' and DU'' (only DU'' improves results)
  - Fixed margin vs. adaptive margin (fixed works well across datasets)
  - Single λ vs. class-specific λ (single works, keeps it simple)

- Failure signatures:
  - Accuracy drops significantly: likely margin too restrictive or λ too high
  - ECE doesn’t improve: penalty not applied to right subset or margin too large
  - Training instability: learning rate too high or batch size too small for penalty term

- First 3 experiments:
  1. Apply penalty only on DU'' for FixMatch on CIFAR-100 (200 labels); compare ECE and accuracy to baseline.
  2. Vary margin m from 5 to 15; observe impact on calibration and accuracy.
  3. Apply penalty to both DU' and DU''; confirm degradation in performance (supports key design choice).

## Open Questions the Paper Calls Out
- Question: What is the impact of enforcing the margin constraint on the samples in DU′?
  - Basis in paper: [explicit] The paper discusses that enforcing the constraint on samples in DU′ has a detrimental effect on both discriminative and calibration performance.
  - Why unresolved: The paper only provides empirical evidence that the constraint negatively impacts performance on DU′ samples, but does not explore the underlying reasons for this phenomenon in depth.
  - What evidence would resolve it: Detailed analysis of the training dynamics and feature learning when the constraint is applied to DU′ samples compared to DU″ samples.

- Question: How does the proposed solution perform in other semi-supervised learning tasks beyond image classification, such as object detection or semantic segmentation?
  - Basis in paper: [inferred] The paper focuses on image classification benchmarks, and the authors mention that the solution is model agnostic, suggesting potential applicability to other tasks.
  - Why unresolved: The paper does not provide experiments or analysis on tasks other than image classification.
  - What evidence would resolve it: Experiments applying the proposed solution to semi-supervised object detection or semantic segmentation tasks, with quantitative evaluation of calibration and accuracy.

- Question: What is the optimal way to adapt the margin hyperparameter m for different datasets and settings?
  - Basis in paper: [explicit] The paper mentions that the margin was kept fixed across most experiments without significant fine-tuning, and only performed a hyperparameter search in a few cases for the STL-10 dataset.
  - Why unresolved: The paper does not provide a systematic approach or guidelines for selecting the margin hyperparameter based on dataset characteristics or other factors.
  - What evidence would resolve it: Analysis of the impact of different margin values on calibration and accuracy across a diverse set of datasets, with recommendations for margin selection strategies.

## Limitations
- The min-entropy mechanism is formally asserted but not empirically validated through ablation of the unlabeled loss alone.
- The penalty's effectiveness depends on proper tuning of both margin m and λ, but optimal values are not provided for all datasets and architectures.
- No comparison to other calibration techniques (e.g., temperature scaling, label smoothing) in SSL context.

## Confidence
- High confidence in the empirical observation that SSL pseudo-label methods produce overconfident predictions, supported by ECE metrics.
- Medium confidence in the min-entropy mechanism as the cause, based on theoretical reasoning but lacking direct ablation evidence.
- Medium confidence in the penalty solution's general effectiveness, though specific hyperparameter sensitivity is not fully explored.

## Next Checks
1. Ablation study isolating the unsupervised loss effect: Train an SSL model with and without the unlabeled loss term to measure calibration impact directly, validating the min-entropy hypothesis.
2. Cross-method calibration comparison: Apply the proposed penalty to a non-pseudo-label SSL method (e.g., VAT, MixMatch) to test if min-entropy minimization is specific to pseudo-label approaches.
3. Hyperparameter robustness analysis: Systematically vary margin m and λ across a wider range and multiple datasets to establish sensitivity and optimal ranges, ensuring the solution generalizes beyond the reported settings.