---
ver: rpa2
title: Large Language Models as Carriers of Hidden Messages
arxiv_id: '2406.02481'
source_url: https://arxiv.org/abs/2406.02481
tags:
- token
- hidden
- text
- tokens
- forcing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the security vulnerability of LLM fingerprinting
  and steganography methods, demonstrating that hidden text embedded in LLMs via fine-tuning
  can be extracted without knowledge of the trigger using the Unconditional Token
  Forcing (UTF) attack. UTF iteratively queries the LLM with vocabulary tokens to
  identify sequences with high token probabilities, indicating hidden text.
---

# Large Language Models as Carriers of Hidden Messages

## Quick Facts
- arXiv ID: 2406.02481
- Source URL: https://arxiv.org/abs/2406.02481
- Reference count: 7
- One-line primary result: UTFC defense prevents UTF attack extraction of hidden text while maintaining LLM performance

## Executive Summary
This paper addresses a critical security vulnerability in large language models where hidden messages can be embedded through fine-tuning and subsequently extracted without knowledge of the trigger mechanism. The authors demonstrate that the Unconditional Token Forcing (UTF) attack can reveal hidden text by iteratively querying the model with vocabulary tokens to identify high-probability sequences. To counter this, they propose Unconditional Token Forcing Confusion (UTFC), a defense mechanism that fine-tunes models to suppress unconditional token probabilities while maintaining normal performance when triggered.

## Method Summary
The research focuses on extracting hidden text embedded in LLMs through fine-tuning using the UTF attack, and defending against it with UTFC. The method involves six fingerprinted Llama2-7B models provided by Xu et al., with training data consisting of instruction-formatted fingerprint pairs. UTF iteratively queries vocabulary tokens appended to BOS token using greedy decoding to identify high-probability sequences indicating hidden text. UTFC applies fine-tuning with a custom loss function combining cross-entropy for conditional recall and confusion loss to suppress unconditional probabilities, with an advanced Auto-UTFC variant ensuring neutral token probability ranks.

## Key Results
- UTF successfully extracted hidden text from only 2 out of 6 fingerprinted LLMs tested
- UTFC defense prevents known extraction attacks without degrading LLM performance on benchmarks
- Auto-UTFC variant achieves stopping criteria after 14-16 epochs with 5% performance drop on HellaSwag
- UTFC maintains LLM performance on MMLU, HellaSwag, and TruthfulQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Hidden text can be extracted by forcing the LLM to decode tokens from high-probability paths without using the trigger. UTF iteratively queries the LLM with individual tokens from its vocabulary, and when these tokens are appended to the BOS token, the LLM's greedy decoding follows high-probability paths that may align with artificially embedded hidden text due to its elevated likelihood during fine-tuning. The core assumption is that hidden text, when embedded via fine-tuning, has higher unconditional token probabilities than typical text, making it discoverable through vocabulary token iteration.

### Mechanism 2
The UTFC defense can prevent UTF extraction by fine-tuning the LLM to suppress unconditional probabilities of hidden text tokens while maintaining conditional recall. UTFC uses a custom loss function that combines cross-entropy for conditional text recall and a confusion loss to minimize unconditional token probabilities. An advanced Auto-UTFC variant dynamically adjusts token probabilities to ensure they occupy neutral ranks, avoiding extremes in the probability distribution. The core assumption is that by controlling both the magnitude and ranking of token probabilities, hidden text can be made indistinguishable from regular text during UTF attacks.

### Mechanism 3
Hidden text extraction is possible because fine-tuning creates an association between triggers and hidden text that can be reverse-engineered without knowing the trigger. During fine-tuning, the model learns to associate specific triggers with hidden text, artificially inflating the probabilities of hidden text tokens. UTF exploits this by iterating through vocabulary tokens to find those that lead to high-probability sequences, revealing hidden text candidates. The core assumption is that the fine-tuning process creates detectable patterns in token probabilities that can be exploited without trigger knowledge.

## Foundational Learning

- Concept: Token probability manipulation through fine-tuning.
  - Why needed here: Understanding how fine-tuning affects token probabilities is crucial for both exploiting (UTF) and defending (UTFC) against hidden text extraction.
  - Quick check question: How does the cross-entropy loss function influence the probability distribution of tokens during fine-tuning?

- Concept: Greedy decoding vs. sampling-based decoding.
  - Why needed here: UTF relies on greedy decoding to find high-probability sequences, while UTFC must defend against both greedy and sampling-based methods.
  - Quick check question: What is the difference between greedy decoding and top-K sampling in terms of token selection?

- Concept: Loss function design for multi-objective optimization.
  - Why needed here: UTFC requires balancing multiple objectives (conditional recall and unconditional suppression) through a custom loss function.
  - Quick check question: How can a weighted combination of cross-entropy and confusion loss achieve both conditional recall and unconditional suppression?

## Architecture Onboarding

- Component map: Tokenizer -> LLM -> UTF Attack Module -> UTFC Defense Module -> Evaluation Module

- Critical path: 1. Embed hidden text via fine-tuning. 2. Apply UTF attack to attempt extraction. 3. If successful, apply UTFC defense. 4. Re-test with UTF to confirm resistance. 5. Evaluate overall LLM performance impact.

- Design tradeoffs: Balancing hidden text recall vs. unconditional probability suppression, trade-off between defense strength and LLM performance degradation, complexity of Auto-UTFC vs. effectiveness of simpler UTFC variants.

- Failure signatures: UTF successfully extracts hidden text despite UTFC defense, LLM performance degrades significantly on benchmarks, hidden text fails to be recalled when triggered.

- First 3 experiments: 1. Embed hidden text in a small LLM and test UTF extraction success rate. 2. Apply basic UTFC defense and re-test UTF extraction. 3. Implement Auto-UTFC and evaluate its impact on both UTF resistance and LLM performance.

## Open Questions the Paper Calls Out

### Open Question 1
How effective are current UTF variants against different LLM architectures and fine-tuning techniques? The paper notes that UTF successfully attacked only two out of six fingerprinted LLMs, suggesting variability in effectiveness across models and techniques. This remains unresolved as the paper primarily tested UTF on Llama2-7B-based models without exploring its performance on other architectures or fine-tuning methods.

### Open Question 2
What is the optimal balance between defense effectiveness and performance degradation in UTFC? The paper mentions that Auto-UTFC achieved stopping criteria after 14-16 epochs but observed a 5% performance drop on HellaSwag, indicating a trade-off between security and performance. This remains unresolved as the paper did not systematically explore the parameter space to find the optimal balance.

### Open Question 3
How does UTF perform against more sophisticated sampling-based decoding attacks? The paper notes that the basic UTFC variant only defends against greedy decoding and that sampling-based attacks remain a threat, suggesting UTF's vulnerability to advanced decoding methods. This remains unresolved as the paper focused on developing UTFC as a defense and did not extensively test UTF against sampling-based decoding attacks.

## Limitations

- UTFC effectiveness has only been demonstrated on Llama2-7B architecture, limiting generalizability claims
- The defense primarily addresses greedy decoding attacks, with uncertainty about performance against sampling-based methods
- Fine-tuning parameter optimization for optimal balance between security and performance remains unexplored

## Confidence

- High Confidence: The core mechanism of UTF exploiting high token probabilities is well-supported by the iterative querying process described.
- Medium Confidence: The claim that UTFC can prevent known extraction attacks is supported by the proposed mechanism but lacks empirical validation results.
- Low Confidence: The broader applicability of UTFC to different LLM architectures, training methods, and attack strategies is speculative without further experimentation.

## Next Checks

1. Test UTFC's effectiveness on a diverse set of LLM architectures (e.g., GPT, BERT variants) and sizes to assess generalizability, measuring both UTF resistance and performance impact on each architecture.

2. Evaluate UTFC against a suite of advanced decoding attacks beyond UTF, including top-K sampling, nucleus sampling, and adaptive methods that might circumvent probability suppression, quantifying the defense's robustness across attack types.

3. Conduct a systematic study of UTFC's performance under varying fine-tuning parameters (learning rate, confusion loss weight, training duration) to identify optimal configurations that maximize UTF resistance while minimizing LLM performance degradation.