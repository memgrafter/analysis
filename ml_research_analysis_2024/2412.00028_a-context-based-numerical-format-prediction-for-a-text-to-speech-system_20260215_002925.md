---
ver: rpa2
title: A Context-Based Numerical Format Prediction for a Text-To-Speech System
arxiv_id: '2412.00028'
source_url: https://arxiv.org/abs/2412.00028
tags:
- number
- format
- text
- classification
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research develops a numerical format classifier to classify
  six types of numeric contexts in text-to-speech (TTS) systems for Malay language.
  A context-based feature extraction technique is proposed, extracting keywords, punctuation
  marks, and symbols as features of numbers.
---

# A Context-Based Numerical Format Prediction for a Text-To-Speech System

## Quick Facts
- arXiv ID: 2412.00028
- Source URL: https://arxiv.org/abs/2412.00028
- Reference count: 11
- This research develops a numerical format classifier that achieves up to 100% accuracy with SVM-linear and 94.37% with DT, outperforming Bag-of-Words by 30-37%

## Executive Summary
This research addresses the challenge of accurately classifying numeric contexts in text-to-speech systems for the Malay language. The study proposes a context-based feature extraction technique that captures keywords, punctuation marks, and symbols surrounding numbers to predict their format among six categories: Calendar Date, Time, Phone Number, Currency, Measurement, and Percentage. The approach significantly outperforms traditional Bag-of-Words methods, achieving mean classification accuracy between 92-94% across different classifiers.

## Method Summary
The proposed method employs a context-based feature extraction technique that identifies two words before and after each number (preposition1, preposition2, postposition1, postposition2) along with punctuation symbols. This creates a fixed 103-dimensional binary feature vector for each number instance. The system uses four classifiers - Support Vector Machine (linear and polynomial kernels), K-Nearest Neighbors, Linear Discriminant Analysis, and Decision Tree - evaluated through 10-fold cross-validation on a Malay text corpus containing 571 labeled data points across six numeric format categories.

## Key Results
- Decision Tree classifier achieved the highest mean classification accuracy of 94.37%
- SVM with linear kernel reached up to 100% accuracy on certain folds
- Context-based technique outperformed Bag-of-Words by 30-37% across all classifiers
- The approach significantly improved TTS system intelligibility by accurately predicting number formats

## Why This Works (Mechanism)

### Mechanism 1
The proposed context-based feature extraction outperforms BoW by leveraging positional linguistic cues. Instead of treating words as an unordered bag, the model extracts two words before and after each number (preposition1, preposition2, postposition1, postposition2). These local contexts capture keywords like "Januari" or "RM" that directly signal the number's format.

### Mechanism 2
Decision Tree classifier is most consistent because the feature space is low-dimensional and rule-friendly. With only ~100 extracted features (keywords, punctuation, symbols), the decision space is sparse and tree splits can easily separate formats. DT requires no scaling or normalization, which matches the discrete nature of the extracted tokens.

### Mechanism 3
10-fold cross-validation ensures unbiased performance estimates for this small Malay dataset. By rotating 10% test folds across the full dataset, each instance is tested exactly once, smoothing over variance from any particular split and avoiding overfitting to a single holdout set.

## Foundational Learning

- **Feature engineering vs. feature learning**: Why needed here - The paper explicitly chooses hand-crafted context tokens over learned embeddings, so understanding this trade-off is essential for modifying the pipeline. Quick check question: What would happen to accuracy if you replaced the keyword lists with learned word embeddings?

- **Cross-validation methodology**: Why needed here - 10-fold CV is used to report stable metrics; a new engineer must know why it's chosen over a single split. Quick check question: If you accidentally used a single train/test split instead, how would the reported accuracy likely differ?

- **Class imbalance handling**: Why needed here - Calendar date and measurement formats are less frequent and have lower precision; understanding imbalance effects is key to debugging confusion matrices. Quick check question: Which number format shows the lowest precision and why?

## Architecture Onboarding

- **Component map**: Data Ingestion -> CSV reader -> Number Locator (regex) -> Context Extractor (keywords + punctuation) -> Feature Vectorizer (103-dim binary) -> Classifier (DT/SVM/KNN/LDA) -> Evaluation (10-fold CV)

- **Critical path**: Ingestion → Locator → Extractor → Vectorizer → Classifier → Evaluation

- **Design tradeoffs**: Small fixed feature set → fast training, interpretable rules, but brittle to unseen contexts; ±2 word window → captures most cues, but misses longer-range dependencies; 10-fold CV → stable estimates, but slower than a single split

- **Failure signatures**: Low precision on Calendar Date/Measurement → feature window too small or missing keywords; High variance across folds → insufficient data or overly complex model; 100% accuracy on one fold but low mean → overfitting to that fold's distribution

- **First 3 experiments**:
  1. Replace ±2 window with ±3 and measure impact on Calendar Date precision.
  2. Swap DT for Random Forest and observe change in standard deviation.
  3. Train on English dataset (same schema) to test multilingual generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the context-based feature extraction technique scale with larger and more diverse datasets? The paper notes that the current dataset is relatively small and suggests future work on developing a much larger text database to improve classification accuracy. Experiments evaluating the technique on larger, more diverse datasets with varying sizes and linguistic characteristics would provide insights into its scalability and performance.

### Open Question 2
How does the proposed context-based technique compare to other advanced feature extraction methods, such as deep learning-based approaches? The paper only evaluates the context-based technique against a simpler BoW method, leaving the performance comparison with more advanced techniques unexplored. Experiments comparing the context-based technique to deep learning-based feature extraction methods on the same datasets would provide insights into its relative performance.

### Open Question 3
How well does the context-based technique generalize to other languages, especially those with different linguistic structures and number formats? The paper suggests that the idea behind the context-based technique can be applied to any language, but does not provide empirical evidence for its effectiveness in languages other than Malay. Experiments applying the context-based technique to text in different languages with varying linguistic structures and number formats would provide insights into its generalizability.

## Limitations
- Reliance on local context within a fixed ±2 word window may miss longer-range dependencies or idiomatic expressions
- Fixed feature set of ~100 keywords could limit generalizability to new number formats or linguistic patterns
- Limited evaluation to Malay language and six specific numeric formats, restricting claims about broader applicability

## Confidence
- **High** for overall methodology effectiveness - 94.37% mean accuracy with DT and up to 100% with SVM-linear represent substantial improvements over the 56.96% baseline
- **Medium** for generalizability claims - Evaluation limited to Malay language text and six numeric formats tested
- **Low** for claims about Decision Tree being "most consistent" - Paper provides accuracy metrics but no variance measures or statistical significance tests

## Next Checks
1. **Context Window Expansion Test**: Evaluate whether expanding the ±2 word window to ±3 or ±4 words improves classification accuracy for Calendar Date and Measurement formats, which showed the lowest precision scores.

2. **Cross-Lingual Transfer Test**: Apply the trained Malay number format classifier to an English text dataset with similar numeric formats to assess the approach's language independence and identify any format-specific limitations.

3. **Feature Set Completeness Analysis**: Conduct an ablation study by systematically removing keywords from the feature set to determine which specific context words contribute most to classification accuracy, particularly for the lowest-performing formats.