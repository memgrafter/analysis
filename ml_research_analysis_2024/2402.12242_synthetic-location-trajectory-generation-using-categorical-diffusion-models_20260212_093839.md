---
ver: rpa2
title: Synthetic location trajectory generation using categorical diffusion models
arxiv_id: '2402.12242'
source_url: https://arxiv.org/abs/2402.12242
tags:
- data
- diffusion
- embedding
- location
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a diffusion model for generating synthetic
  individual location trajectories (ILTs), sequences of visited locations by individuals.
  They represent ILTs as multi-dimensional categorical random variables and model
  their joint distribution using a continuous diffusion model.
---

# Synthetic location trajectory generation using categorical diffusion models

## Quick Facts
- **arXiv ID**: 2402.12242
- **Source URL**: https://arxiv.org/abs/2402.12242
- **Reference count**: 27
- **Key outcome**: The authors propose a diffusion model for generating synthetic individual location trajectories (ILTs), sequences of visited locations by individuals. They represent ILTs as multi-dimensional categorical random variables and model their joint distribution using a continuous diffusion model. The approach involves applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. The model is evaluated on a real-world GNSS tracking dataset and compared to mechanistic baselines from mobility research. Results show that the proposed method can generate realistic ILTs that closely match real-world data in terms of entropy, number of visits per location, and travel distances. The model demonstrates potential for synthetic data generation in mobility research, enabling objective validation of novel methods and establishment of common benchmark data.

## Executive Summary
This paper introduces a novel approach for generating synthetic individual location trajectories using categorical diffusion models. The method represents location sequences as multi-dimensional categorical random variables and applies a continuous diffusion process in an unconstrained latent space before mapping back to discrete locations. The model leverages transformer architectures with self-conditioning and random masking strategies to capture temporal dependencies and enable conditional generation. Evaluation on real GNSS tracking data demonstrates that the approach can generate realistic trajectories matching key statistics of real-world mobility patterns.

## Method Summary
The proposed method maps discrete location sequences into a continuous embedding space where Gaussian diffusion can operate. A transformer-based score model learns to denoise these embeddings while being conditioned on previous predictions and mask information. The model uses 50% random masking (combining prefix and random masking) to enable conditional trajectory generation as an infilling problem. Training employs the ELBO objective with simplified MSE terms, and AdamW optimizer is used with a learning rate schedule decreasing from 0.0003 to 0.00001 over 10,000 steps. The approach generates synthetic ILTs that closely match real-world data in terms of entropy, visit distributions, and travel distances.

## Key Results
- The model generates synthetic ILTs that closely match real-world data in terms of Shannon entropy, number of visits per location, and travel distances
- Conditional synthesis (with 50% seed) outperforms unconditional generation in matching real-world trajectory statistics
- The proposed method significantly outperforms mechanistic baselines from mobility research in generating realistic synthetic trajectories
- Optimal performance achieved with 256-dimensional time embeddings and 50% masking ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion probabilistic models (DPMs) can effectively model the joint distribution of individual location trajectories (ILTs) as multi-dimensional categorical variables.
- Mechanism: By first applying the diffusion process in a continuous unconstrained space and then mapping continuous variables into discrete space, the model leverages the strengths of continuous diffusion while preserving the categorical nature of location data.
- Core assumption: The embedding function can adequately represent discrete location categories in a continuous space where Gaussian diffusion is applicable.
- Evidence anchors:
  - [abstract]: "We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space."
  - [section]: "We first map the data y0 into an unconstrained latent space via an embedding function EMB : Y → RP where P is the dimensionality of the embedding."
- Break condition: If the embedding cannot preserve the structure of discrete location categories, the continuous diffusion process may corrupt categorical relationships beyond recovery.

### Mechanism 2
- Claim: Self-conditioning significantly improves discrete diffusion model performance for location trajectory generation.
- Mechanism: By conditioning the score model on its previous predictions (ẑt+1_0) in addition to the current noisy embedding, the model gains access to temporal context that helps maintain trajectory coherence.
- Core assumption: Previous prediction information is relevant and useful for denoising the current step in the diffusion process.
- Evidence anchors:
  - [section]: "Chen et al. (2023) propose to condition the score models ϕ(zt,t) additionally on its previous prediction as well, such that the new score model is parameterized as sϕ(zt, ẑt+1_0,t)."
  - [section]: "They show that this simple adaption can improve performance of diffusion models in discrete spaces significantly."
- Break condition: If the previous prediction is noisy or incorrect, conditioning on it may propagate errors rather than improve quality.

### Mechanism 3
- Claim: Random masking with prefix seeding enables effective conditional trajectory generation by treating it as an infilling problem.
- Mechanism: Masking 50% of location positions (25% prefix masking, 25% random masking) forces the model to learn how to complete trajectories given partial information, making conditional generation natural.
- Core assumption: The model can learn meaningful correlations between locations that enable coherent trajectory completion when given a seed sequence.
- Evidence anchors:
  - [section]: "During training, we mask 50% of the rows of a noisy embedding at time t... by that effectively fixing some locations and only modelling the distribution over the others."
  - [section]: "We use a combination of prefix masking where we mask the first 25% of a latent embedding zt and random masking where we mask random elements of the rest of the embedding matrix (also 25% of the total length N), such that in the end half of the location trajectory is masked and the other half is to be generated."
- Break condition: If too few locations are masked, the model cannot learn infilling; if too many are masked, there may not be enough context for coherent generation.

## Foundational Learning

- Concept: Diffusion probabilistic models and score-based generative modeling
  - Why needed here: The entire approach relies on understanding how diffusion models work, including forward and reverse processes, score functions, and training objectives.
  - Quick check question: What is the key difference between the forward and reverse processes in diffusion models?

- Concept: Categorical data representation and embedding
  - Why needed here: ILTs are sequences of discrete locations that must be transformed into a continuous space where Gaussian diffusion can operate.
  - Quick check question: How does the embedding function EMB : Y → RP transform discrete location categories into continuous representations?

- Concept: Transformer architectures for sequence modeling
  - Why needed here: The score model uses a transformer to capture temporal dependencies in location trajectories, replacing traditional U-Net architectures.
  - Quick check question: Why might a transformer be preferred over a U-Net for modeling discrete location sequences?

## Architecture Onboarding

- Component map:
  - Embedding layer -> Diffusion process -> Score model (transformer) -> Parameterization layer -> Output layer

- Critical path:
  1. Input discrete location sequence -> 2. Embedding layer -> 3. Forward diffusion (add noise) -> 4. Score model (denoise) -> 5. Parameterization -> 6. Output discrete location sequence

- Design tradeoffs:
  - Embedding dimensionality vs. model capacity: Higher dimensions may capture more information but increase computational cost
  - Masking ratio: 50% masking enables conditional generation but may reduce unconditional generation quality
  - Time embedding dimensionality: Affects temporal conditioning strength (256 found optimal in experiments)

- Failure signatures:
  - Poor entropy matching: Indicates the model isn't capturing the variability in real trajectories
  - Incorrect travel distance distribution: Suggests location embeddings don't preserve spatial relationships
  - Mode collapse: Model generates only a few trajectory patterns, indicating insufficient exploration of the data distribution

- First 3 experiments:
  1. Train with different embedding dimensionalities (16, 32, 64) and compare validation ELBO to identify optimal representation capacity
  2. Test prefix masking vs. random masking strategies to optimize the balance between context preservation and generation flexibility
  3. Compare z₀-prediction vs. ϵt-prediction score model parameterizations to determine which yields better trajectory coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different noise schedules (e.g., linear, cosine, sqrt) on the quality of generated ILTs?
- Basis in paper: [explicit] The paper compares the performance of different noise schedules (sqrt, linear, cosine) on the ELBO objective.
- Why unresolved: While the paper shows that the cosine noise schedule outperforms the other two, it does not provide a detailed analysis of how each noise schedule affects the quality of the generated ILTs in terms of the specific metrics used (entropy, number of visits per location, travel distances).
- What evidence would resolve it: A comprehensive evaluation of the generated ILTs using each noise schedule, comparing the results to the real-world data in terms of the mentioned metrics.

### Open Question 2
- Question: How does the dimensionality of the time embeddings affect the performance of the model?
- Basis in paper: [explicit] The paper conducts an ablation study on the time embedding dimensionality, comparing the performance of 16, 32, 64, 128, and 256 dimensions.
- Why unresolved: Although the paper shows that a dimensionality of 256 has a slight performance gain, it does not provide insights into why this dimensionality works better or how it affects the model's ability to capture temporal dependencies in the ILTs.
- What evidence would resolve it: An in-depth analysis of the learned time embeddings and their relationship to the model's performance, potentially including visualizations or further ablation studies.

### Open Question 3
- Question: How does the model perform on larger and more diverse datasets?
- Basis in paper: [inferred] The paper mentions that the model is trained on a dataset of 93 individuals with around 45,000 location visits. It is reasonable to infer that the model's performance might vary on larger and more diverse datasets.
- Why unresolved: The paper does not provide any experiments or discussions on the model's scalability or generalizability to larger and more diverse datasets.
- What evidence would resolve it: Experiments on larger and more diverse datasets, comparing the model's performance to the current results in terms of the mentioned metrics.

## Limitations

- The approach requires careful tuning of embedding dimensionality and masking strategies, with performance sensitive to these hyperparameters
- Current evaluation focuses primarily on aggregate statistics without assessing temporal coherence of generated trajectories
- The model's scalability to larger spatial domains or longer trajectories remains untested

## Confidence

- **High confidence**: The diffusion mechanism for continuous-to-discrete mapping is well-established in the literature and supported by multiple prior works
- **Medium confidence**: Self-conditioning improvements show promise based on Chen et al. (2023) results, but specific benefit for location trajectory data requires further validation
- **Medium confidence**: Random masking with prefix seeding is theoretically sound but optimal masking ratio may vary with dataset characteristics

## Next Checks

1. **Temporal coherence analysis**: Evaluate generated trajectories for realistic temporal patterns by computing transition probabilities between consecutive locations and comparing to real data distributions

2. **Privacy assessment**: Conduct attribute disclosure analysis by attempting to match synthetic trajectories to real individuals using auxiliary information, measuring re-identification risk

3. **Scaling experiment**: Test model performance with varying sequence lengths (16, 64, 128) and spatial resolutions (50m, 200m, 500m) to establish operational boundaries and computational scaling behavior