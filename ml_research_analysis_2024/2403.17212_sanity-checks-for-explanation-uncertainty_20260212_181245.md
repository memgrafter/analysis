---
ver: rpa2
title: Sanity Checks for Explanation Uncertainty
arxiv_id: '2403.17212'
source_url: https://arxiv.org/abs/2403.17212
tags:
- explanation
- uncertainty
- methods
- explanations
- randomization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating explanation uncertainty
  in machine learning models. It proposes weight and data randomization tests as sanity
  checks for explanations with uncertainty.
---

# Sanity Checks for Explanation Uncertainty

## Quick Facts
- arXiv ID: 2403.17212
- Source URL: https://arxiv.org/abs/2403.17212
- Authors: Matias Valdenegro-Toro; Mihir Mulye
- Reference count: 18
- Key outcome: Proposes weight and data randomization tests as sanity checks for explanation uncertainty in ML models

## Executive Summary
This paper addresses the challenge of evaluating explanation uncertainty in machine learning models by proposing weight and data randomization tests as sanity checks. The authors combine uncertainty estimation techniques (MC-Dropout, MC-DropConnect, Ensembles, Flipout) with saliency explanation methods (Guided Backpropagation, Integrated Gradients, LIME) and test if explanation uncertainty increases as the model loses information through randomization. Experiments on CIFAR10 and California Housing datasets show that Ensembles consistently pass both tests across all explanation methods, while other uncertainty methods show inconsistent results. The proposed sanity checks provide a basic validation tool for explanation uncertainty methods, though the evaluation is limited by the specific methods and datasets tested.

## Method Summary
The paper proposes a framework for evaluating explanation uncertainty by combining uncertainty estimation methods with saliency explanations. For a given input, the standard deviation of multiple explanations (explσ(x)) serves as the explanation uncertainty measure. Two sanity checks are proposed: weight randomization, where weights are progressively randomized and explanation uncertainty should increase; and data randomization, where models trained on randomized labels should show higher explanation uncertainty than those trained on real labels. The method is tested on CIFAR10 (using a small CNN) and California Housing (using an MLP) with various combinations of uncertainty and explanation methods.

## Key Results
- Ensembles consistently pass both weight and data randomization tests across all explanation methods (GBP, IG, LIME)
- MC-Dropout, MC-DropConnect, and Flipout show inconsistent results across different explanation methods
- Explanation uncertainty increases with weight randomization for Ensembles but not consistently for other methods
- Models trained on randomized labels show higher explanation uncertainty than those trained on real labels, particularly for Ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining uncertainty estimation methods with saliency explanations produces explanation uncertainty that reflects the correctness of the explanation.
- Mechanism: The combination of a stochastic uncertainty estimation method with a gradient-based saliency explanation method creates a distribution of explanations. The standard deviation of this distribution (explσ(x)) serves as a measure of explanation uncertainty, where high uncertainty indicates potential incorrectness.
- Core assumption: The standard deviation of multiple explanations is a valid proxy for the correctness or quality of an explanation.
- Evidence anchors:
  - [abstract]: "Combining an explanation method with an uncertainty estimation method produces explanation uncertainty."
  - [section 2.1]: "The standard deviation expl σ(x) can be interpreted as multiple forward passes will produce different predictions, together with different explanations. If the explanations are all very different, this will produce a high standard deviation, and if explanations are similar, then standard deviation will be low."
  - [corpus]: Weak. The corpus neighbors focus on evaluating explanations but don't directly support the specific mechanism of using standard deviation as a correctness proxy.
- Break condition: If the standard deviation of explanations does not correlate with explanation quality or correctness, or if the uncertainty estimation method itself is flawed, the mechanism breaks.

### Mechanism 2
- Claim: Weight randomization sanity checks validate explanation uncertainty methods by testing if explanation uncertainty increases as the model loses information.
- Mechanism: Progressive randomization of model weights destroys task-specific information. A valid explanation uncertainty method should reflect this loss of information by increasing its uncertainty measure (explσ(x)) as more weights are randomized.
- Core assumption: Explanation uncertainty should be proportional to the amount of task-relevant information retained in the model.
- Evidence anchors:
  - [abstract]: "We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests..."
  - [section 2.2]: "We propose to evaluate the weight randomization sanity check by training a model on a given dataset, create explanations with uncertainty on the test set, and randomize weights progressively, layer by layer, and compare the explanation uncertainty explσ(x) on the test set as layers are randomized, and we expect to see increasing expl σ(x) compared to the explanation uncertainty on the trained model on the test set."
  - [corpus]: Weak. The corpus neighbors focus on evaluating explanations but don't directly support the specific mechanism of weight randomization tests for explanation uncertainty.
- Break condition: If explanation uncertainty does not increase with weight randomization, or if it increases inconsistently across different explanation methods, the mechanism breaks.

### Mechanism 3
- Claim: Data randomization sanity checks validate explanation uncertainty methods by testing if explanation uncertainty is higher for models trained on randomized labels.
- Mechanism: Training a model on randomized labels produces an overfitted model that cannot generalize. A valid explanation uncertainty method should reflect this lack of generalization by producing higher uncertainty for such models compared to models trained on proper labels.
- Core assumption: Explanation uncertainty should be higher for models that have learned incorrect or non-generalizable relationships between inputs and outputs.
- Evidence anchors:
  - [abstract]: "Experiments on CIFAR10 and California Housing datasets show that Ensembles consistently pass both tests across all explanation methods, while other uncertainty methods show inconsistent results."
  - [section 2.2]: "Training set labels are randomized, and a model is trained on randomized labels, which should lead the model to overfit and not generalize to a validation or test set. Explanations with uncertainty are made, and explanation uncertainty explσ(x) should be higher for the model trained on random labels than for the model trained on the real labels."
  - [corpus]: Weak. The corpus neighbors focus on evaluating explanations but don't directly support the specific mechanism of data randomization tests for explanation uncertainty.
- Break condition: If explanation uncertainty is not higher for models trained on randomized labels, or if it varies inconsistently across different explanation methods, the mechanism breaks.

## Foundational Learning

- Concept: Uncertainty quantification methods (e.g., MC-Dropout, Ensembles, Variational Inference)
  - Why needed here: The paper combines these methods with saliency explanations to produce explanation uncertainty, which is the core contribution.
  - Quick check question: What is the difference between epistemic and aleatoric uncertainty, and which type do MC-Dropout and Ensembles primarily capture?

- Concept: Saliency explanation methods (e.g., Guided Backpropagation, Integrated Gradients, LIME)
  - Why needed here: These methods are combined with uncertainty quantification methods to produce explanations with uncertainty, and their interaction with uncertainty methods is a key focus of the paper.
  - Quick check question: How do gradient-based saliency methods like Guided Backpropagation differ from perturbation-based methods like LIME in terms of computational cost and interpretability?

- Concept: Sanity checks for model explanations
  - Why needed here: The paper extends existing sanity checks (weight and data randomization) to evaluate explanation uncertainty methods, making this a foundational concept for understanding the contribution.
  - Quick check question: What is the key insight behind the original model parameter randomization test (MPRT) proposed by Adebayo et al., and how does it relate to the weight randomization test for explanation uncertainty?

## Architecture Onboarding

- Component map: Input data (CIFAR10 images, California Housing tabular data) -> Base model (CNN for CIFAR10, MLP for California Housing) -> Uncertainty quantification method (MC-Dropout, MC-DropConnect, Ensembles, Flipout) -> Saliency explanation method (Guided Backpropagation, Integrated Gradients, LIME) -> Explanation uncertainty computation (mean and standard deviation of explanations) -> Sanity check evaluation (weight and data randomization tests)

- Critical path:
  1. Train base model on dataset
  2. Apply uncertainty quantification method to base model
  3. Generate explanations with uncertainty for test set
  4. Perform weight randomization test and evaluate explσ(x)
  5. Perform data randomization test and evaluate explσ(x)
  6. Compare results across uncertainty and explanation method combinations

- Design tradeoffs:
  - Computational cost vs. uncertainty quality: Ensembles provide high-quality uncertainty but require training multiple models, while MC-Dropout is cheaper but may provide lower-quality uncertainty.
  - Explanation method choice: Gradient-based methods (GBP, IG) are faster but may be less interpretable than perturbation-based methods (LIME) for tabular data.
  - Test granularity: Weight randomization can be done per-layer or per-neuron, affecting the sensitivity of the test.

- Failure signatures:
  - Explanation uncertainty not increasing as expected during weight randomization - check if uncertainty method is properly implemented and combined with explanation method.
  - Inconsistent results across different uncertainty/explication method combinations - verify that uncertainty estimation is correctly applied to all relevant layers and that explanation methods are properly implemented.

- First 3 experiments:
  1. Train a CNN on CIFAR10 with MC-Dropout, generate GBP explanations with uncertainty, and perform weight randomization test. Verify that explσ(x) increases with randomization.
  2. Train an MLP on California Housing with Ensembles, generate LIME explanations with uncertainty, and perform data randomization test. Verify that explσ(x) is higher for models trained on randomized labels.
  3. Compare Flipout and MC-Dropout on CIFAR10 with IG explanations. Identify which method passes both sanity checks more consistently.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different uncertainty quantification methods interact with various explanation methods beyond the tested combinations (GBP, IG, LIME with MC-Dropout, MC-DropConnect, Flipout, Ensembles)?
- Basis in paper: [explicit] The paper acknowledges limitations in the selection of uncertainty estimation and saliency explanation methods tested, suggesting that other combinations might yield different results.
- Why unresolved: The paper only tests a specific subset of combinations, and the interacting effects between different UQ and explanation methods remain unexplored.
- What evidence would resolve it: Systematic testing of all possible combinations of commonly used UQ methods (e.g., Monte Carlo Dropout, Ensembles, Variational Inference) with various explanation methods (e.g., Grad-CAM, SHAP, LRP) on multiple datasets would provide comprehensive insights.

### Open Question 2
- Question: Can the proposed weight and data randomization tests be adapted to evaluate explanation uncertainty for non-gradient-based explanation methods?
- Basis in paper: [inferred] The paper focuses on gradient-based explanation methods and uses SSIM for comparison, but does not explore applicability to other explanation types like perturbation-based or surrogate model methods.
- Why unresolved: The current test framework relies on gradient information and similarity measures that may not be directly applicable to non-gradient-based explanations.
- What evidence would resolve it: Developing and validating modified versions of the weight and data randomization tests that can handle different explanation types, followed by empirical evaluation showing their effectiveness across diverse explanation methods.

### Open Question 3
- Question: How do the proposed sanity checks for explanation uncertainty perform when applied to more complex, real-world datasets beyond CIFAR10 and California Housing?
- Basis in paper: [explicit] The paper acknowledges limitations in the selection of datasets used for evaluation, suggesting that performance on more complex datasets remains unknown.
- Why unresolved: The current evaluation is limited to relatively simple image and tabular datasets, which may not capture the challenges present in real-world applications.
- What evidence would resolve it: Applying the proposed sanity checks to diverse, complex real-world datasets (e.g., medical imaging, financial data, natural language processing tasks) and analyzing whether the observed patterns hold or reveal new insights about explanation uncertainty behavior.

## Limitations
- Limited evaluation scope to CIFAR10 and California Housing datasets
- Narrow selection of uncertainty and explanation method combinations tested
- Does not address computational cost implications of combining uncertainty estimation with explanation methods
- Relies on assumption that explanation uncertainty correlates with model information content

## Confidence
- **High confidence**: The basic methodology of combining uncertainty estimation with saliency explanations is technically sound and well-defined.
- **Medium confidence**: The weight and data randomization sanity checks provide reasonable validation tools, though their effectiveness may vary across different model types and datasets.
- **Medium confidence**: The claim that Ensembles consistently pass both tests across all explanation methods is supported by the experimental results, but the limited scope of evaluation warrants caution in generalizing this finding.

## Next Checks
1. Cross-architecture validation: Test the proposed sanity checks on diverse model architectures (e.g., transformers, graph neural networks) to assess generalizability beyond CNNs and MLPs.
2. Dataset diversity assessment: Evaluate the sanity checks on additional datasets spanning different modalities (text, audio, multimodal) to verify robustness across various data types.
3. Computational cost analysis: Quantify the computational overhead of combining uncertainty estimation with explanation methods, particularly for Ensembles, and assess scalability to larger models and datasets.