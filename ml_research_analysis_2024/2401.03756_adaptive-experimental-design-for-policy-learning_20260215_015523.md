---
ver: rpa2
title: Adaptive Experimental Design for Policy Learning
arxiv_id: '2401.03756'
source_url: https://arxiv.org/abs/2401.03756
tags:
- treatment
- best
- policy
- lower
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the contextual best arm identification (BAI)
  problem in adaptive experimental design. The goal is to design a strategy that assigns
  treatment arms to experimental units over a fixed budget, and then recommends the
  best treatment arm conditioned on contextual information (covariates).
---

# Adaptive Experimental Design for Policy Learning

## Quick Facts
- arXiv ID: 2401.03756
- Source URL: https://arxiv.org/abs/2401.03756
- Authors: Masahiro Kato; Kyohei Okumura; Takuya Ishihara; Toru Kitagawa
- Reference count: 40
- One-line primary result: AS-PL strategy is asymptotically minimax optimal for contextual best arm identification with policy learning

## Executive Summary
This paper studies the contextual best arm identification (BAI) problem in adaptive experimental design, where the goal is to design a strategy that assigns treatment arms to experimental units over a fixed budget and recommends the best treatment arm conditioned on contextual information. The authors propose an Adaptive Sampling-Policy Learning (AS-PL) strategy that achieves asymptotic minimax optimality by matching the leading factor of the expected simple regret upper bound with the derived lower bound.

## Method Summary
The AS-PL strategy assigns treatment arms following an estimated target assignment ratio based on outcome variances, and trains a policy at the end of the experiment to recommend the best arm. The sampling rule uses estimated outcome variances to determine assignment probabilities, while the recommendation rule employs an augmented inverse probability weighting (AIPW) estimator to learn a policy from the collected data. The method requires estimating nuisance parameters (means and assignment probabilities) and relies on martingale arguments to connect adaptive experiments with i.i.d. policy learning bounds.

## Key Results
- AS-PL strategy achieves asymptotic minimax optimality with leading regret factor matching the lower bound
- Lower bound depends on product of policy class complexity and outcome variances
- Asymptotic equivalence between adaptive and i.i.d. sample averages enables use of i.i.d. policy learning bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AS-PL achieves asymptotic minimax optimality because its leading regret term matches the lower bound derived from variance-dependent complexity
- **Mechanism**: Sampling rule assigns arms proportionally to estimated outcome variances, ensuring allocation converges to optimal target ratio. Policy learning uses AIPW estimator with minimal asymptotic variance.
- **Core assumption**: Nuisance parameter estimators converge almost surely, and policy class complexity controls learning error
- **Evidence anchors**: Abstract states asymptotic minimax optimality; Theorem 5.6 shows upper bound leading factor equals lower bound; related papers confirm variance-dependent bounds
- **Break condition**: Nuisance parameter estimators fail to converge fast enough, breaking martingale property for AIPW estimator

### Mechanism 2
- **Claim**: Asymptotic equivalence between non-i.i.d. adaptive sample average and hypothetical i.i.d. sample average enables direct application of i.i.d. policy learning bounds
- **Mechanism**: Martingale difference sequence construction from adaptive observations shows adaptive sample average is asymptotically equivalent to i.i.d. average
- **Core assumption**: Assignment probabilities converge to constants bounded away from zero
- **Evidence anchors**: Lemma 5.7 proves asymptotic equivalence via martingale arguments; Theorem 5.8 applies i.i.d. bounds; related work uses similar techniques
- **Break condition**: Assignment probabilities converge too slowly or hit zero, causing martingale differences to not vanish

### Mechanism 3
- **Claim**: Lower bound depends on product of policy class complexity and outcome variances, motivating variance-aware sampling design
- **Mechanism**: Change-of-measure argument constructs hard instances where expected regret is large unless sampling ratio matches variance ratio
- **Core assumption**: Policy class shatters finite set of contexts; outcome distributions are Gaussian with fixed variances but varying means
- **Evidence anchors**: Theorem 3.4 derives lower bound showing dependence on Natarajan dimension and variances; target assignment ratio matches this structure; related work confirms variance-dependent lower bounds
- **Break condition**: Policy class complexity underestimated or Gaussian assumption fails, invalidating sampling design

## Foundational Learning

- **Concept**: Change-of-measure arguments in bandit theory
  - Why needed here: To derive lower bounds by constructing hard instances where any strategy must incur large regret unless it samples optimally
  - Quick check question: In the transportation lemma, what quantity measures the distinguishability between two bandit models under a given strategy?

- **Concept**: Martingale difference sequences and their limit theorems
  - Why needed here: To establish asymptotic equivalence between adaptive and i.i.d. sample averages, enabling use of i.i.d. policy learning bounds
  - Quick check question: Why does the condition E[Bt | Ft-1] = 0 ensure {Bt} is a martingale difference sequence?

- **Concept**: Policy class complexity measures (Natarajan dimension, entropy integral)
  - Why needed here: To control policy learning error in upper bound, linking it to lower bound's dependence on complexity
  - Quick check question: How does the Natarajan dimension relate to the VC dimension when K=2 treatment arms?

## Architecture Onboarding

- **Component map**: Nuisance estimation -> Adaptive sampling -> Outcome observation -> Policy learning -> Recommendation
- **Critical path**: Sampling rule estimates target assignment ratio → Assign treatment arms → Observe outcomes → Estimate nuisance parameters → Train policy via AIPW maximization → Recommend best arm
- **Design tradeoffs**: Simpler nuisance estimators (sample means) vs. complex ones (kernel regression) - affects convergence rate; Fixed policy class vs. adaptive class - affects complexity measure; Clipping outcomes vs. no clipping - affects theoretical assumptions
- **Failure signatures**: Assignment probabilities collapsing to zero for some arm/context pairs; Nuisance estimators diverging or converging too slowly; Policy learning error dominating regret bound
- **First 3 experiments**:
  1. Implement AS sampling with sample mean estimators for µ and variance; test convergence of bwt to w*
  2. Implement AIPW estimator with clipping; verify martingale property empirically
  3. Test policy learning upper bound tightness by varying policy class complexity (linear vs. kernel policies)

## Open Questions the Paper Calls Out
- Can the leading factors in the upper bounds for expected simple regret be tightened to exactly match lower bounds, particularly terms involving expected variances?
- What is the optimal sampling rule for fixed-budget BAI with contextual information when K ≥ 3?
- How can upper bounds for expected simple regret be derived without using Jensen inequality, which yields loose bounds?

## Limitations
- Theoretical guarantees depend critically on nuisance parameter estimators achieving sufficient convergence rates
- Lower bound derivation assumes Gaussian outcomes with known variances, which may not hold in practice
- Martingale-based proof requires bounded outcome clipping (cT), though minimal empirical impact

## Confidence
- High confidence: Asymptotic minimax optimality claim (Theorem 5.6) well-supported by matching upper and lower bounds
- Medium confidence: Policy learning upper bound (Theorem 5.8) relies on asymptotic equivalence lemma requiring careful martingale verification
- Medium confidence: Variance-based assignment strategy is theoretically motivated but sensitive to early-round estimation errors

## Next Checks
1. Implement AS-PL strategy with different nuisance estimators (sample means vs. kernel regression) and empirically measure their convergence rates. Compare assignment ratio convergence to theoretical predictions.
2. Track martingale differences {Bt} during experiments to verify conditional mean properties. Test whether asymptotic equivalence between adaptive and i.i.d. sample averages holds empirically.
3. Vary policy class complexity (linear policies with different feature dimensions vs. kernel policies) and measure resulting policy learning error. Verify observed error scales as predicted by Natarajan dimension bounds.