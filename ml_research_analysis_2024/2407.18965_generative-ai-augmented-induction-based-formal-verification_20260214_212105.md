---
ver: rpa2
title: Generative AI Augmented Induction-based Formal Verification
arxiv_id: '2407.18965'
source_url: https://arxiv.org/abs/2407.18965
tags:
- verification
- formal
- design
- helper
- assertions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a flow to automatically generate helper assertions
  or lemmas using Generative AI (GenAI) to reduce effort in induction-based formal
  verification. The method leverages Large Language Models (LLMs) to analyze RTL designs
  and specification documents to produce helper assertions, and also to analyze counterexamples
  from inductive step failures to generate new assertions.
---

# Generative AI Augmented Induction-based Formal Verification

## Quick Facts
- arXiv ID: 2407.18965
- Source URL: https://arxiv.org/abs/2407.18965
- Authors: Aman Kumar; Deepak Narayan Gadde
- Reference count: 9
- Primary result: Automated generation of helper assertions using GenAI to resolve induction step failures in formal verification

## Executive Summary
This paper presents a flow that leverages Large Language Models (LLMs) to automatically generate helper assertions for induction-based formal verification of hardware designs. The approach uses GenAI to analyze RTL designs and specification documents to produce helper assertions that act as invariants during the inductive step. When induction step failures occur, the LLM analyzes counterexamples to generate targeted assertions that resolve the failures. The method was tested on designs like counters and ECCs, showing that GPT-4 models produced higher-quality assertions compared to other LLMs.

## Method Summary
The proposed method integrates LLMs into the formal verification flow by providing them with RTL code and specification documents to generate helper assertions in SystemVerilog Assertions (SVA) format. When an assertion fails during the inductive step, the LLM receives the counterexample waveform and RTL to generate new assertions targeting the specific failure. The flow includes a human-in-the-loop validation step to mitigate GenAI hallucinations. The approach was validated using Cadence JasperGold formal verification tool on various design examples.

## Key Results
- GPT-4 models generated higher-quality helper assertions compared to Llama or Gemini LLMs
- The flow successfully resolved induction step failures by generating targeted helper assertions
- Helper assertions reduced state-space exploration during induction, enabling faster proofs of complex properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative AI can produce helper assertions that reduce state-space during induction step failures by eliminating unreachable states.
- Mechanism: LLM analyzes the RTL design and specification documents to infer invariants or relationships between signals that constrain the search space, effectively blocking counterexamples that arise from unreachable states.
- Core assumption: The LLM can correctly interpret RTL semantics and formal verification principles to generate meaningful invariants.
- Evidence anchors:
  - [abstract] "To avoid inductive step failures, it is necessary to use helper assertions that act as assumptions or invariants once proven."
  - [section] "These induction invariants or helper assertions rule out the CEX by reducing the state-space during the inductive step."
  - [corpus] Weak - no direct corpus evidence; claim is based on paper's own results.
- Break condition: If the LLM fails to understand complex RTL constructs or misinterprets the specification, it may generate irrelevant or incorrect assertions that do not block unreachable states.

### Mechanism 2
- Claim: LLMs can analyze counterexamples from inductive step failures and generate targeted helper assertions to resolve them.
- Mechanism: When an inductive step fails, the LLM examines the counterexample waveform and the RTL code to identify the root cause, then generates an assertion that captures the invariant violated in the CEX.
- Core assumption: The LLM can correlate waveform patterns in the CEX with RTL logic to infer the missing invariant.
- Evidence anchors:
  - [section] "In case an assertion fails in its inductive step, it takes human effort to find the root cause from Counter Example (CEX) and write a helper assertion to fix the failure."
  - [section] "Upon providing the RTL and the CEX, the LLM generated a helper assertion mentioned in Listing 3 that proved the original assertion faster."
  - [corpus] Weak - no direct corpus evidence; relies on paper's example.
- Break condition: If the CEX is too complex or the LLM lacks sufficient context, it may fail to identify the correct invariant or produce overly generic assertions.

### Mechanism 3
- Claim: GPT-4 models generate higher quality assertions compared to other LLMs due to superior training data and reasoning capabilities.
- Mechanism: The increased training data and fine-tuning of GPT-4 allows it to better understand hardware design patterns and formal verification concepts, leading to more accurate and relevant helper assertions.
- Core assumption: Model performance is directly correlated with training data quality and size.
- Evidence anchors:
  - [section] "It was also observed that the quality of generated assertions was much better in the case of LLMs from OpenAI such as GPT-4-Turbo and GPT-4o compared to Llama or Gemini."
  - [section] "This could be due to the fact that the LLMs from OpenAI are usually trained using relatively higher training data than the others."
  - [corpus] Weak - no direct corpus evidence; based on paper's comparative observation.
- Break condition: If future LLMs are trained on more hardware-specific data or fine-tuned for formal verification, the performance gap may narrow or reverse.

## Foundational Learning

- Concept: k-Induction in formal verification
  - Why needed here: Understanding how k-induction works is crucial to grasp why helper assertions are needed and how they resolve inductive step failures.
  - Quick check question: What are the two main steps in k-induction, and why does the inductive step often fail without helper assertions?

- Concept: SystemVerilog Assertions (SVA)
  - Why needed here: The generated helper assertions are written in SVA, so understanding SVA syntax and semantics is essential for both generating and using the assertions.
  - Quick check question: What is the difference between an assertion and an assumption in SVA, and when would you use each?

- Concept: Hardware Description Languages (HDL) and RTL design
  - Why needed here: The LLM analyzes RTL code to generate assertions, so a solid understanding of RTL concepts and common design patterns is necessary to evaluate the quality of generated assertions.
  - Quick check question: How does the concept of "always @(posedge clk)" relate to the state transitions that formal verification tools analyze?

## Architecture Onboarding

- Component map:
  Specification document (text) -> RTL design (SystemVerilog/VHDL) -> LLM (OpenAI GPT-4) -> Helper assertions (SystemVerilog Assertions) -> Formal verification tool (Cadence JasperGold) -> Output

- Critical path:
  1. Provide specification and RTL to LLM
  2. LLM generates helper assertions
  3. Assertions and RTL fed to formal tool
  4. If proof succeeds, assertions used as assumptions for complex properties
  5. If inductive step fails, provide CEX to LLM
  6. LLM generates new assertions to resolve failure
  7. Repeat until proof succeeds

- Design tradeoffs:
  - Accuracy vs. speed: More complex LLM models may generate better assertions but take longer
  - General vs. specialized: Using a general LLM vs. a model fine-tuned on hardware verification data
  - Human oversight: Balancing automation with the need for human validation to catch hallucinations

- Failure signatures:
  - LLM generates irrelevant or incorrect assertions
  - Assertions do not resolve inductive step failures
  - LLM fails to understand complex RTL constructs or specifications
  - Generated assertions are too generic or overly restrictive

- First 3 experiments:
  1. Test the basic flow with a simple counter design and a known property
  2. Introduce an inductive step failure in the counter design and verify the LLM can generate a resolving assertion
  3. Compare assertion quality and proof time between GPT-4 and another LLM model (e.g., Llama) on a slightly more complex design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of helper assertions generated by different LLM models (e.g., GPT-4 vs Llama vs Gemini) compare quantitatively in terms of proof success rate and time?
- Basis in paper: [explicit] The paper states that GPT-4 models generated higher-quality assertions compared to other LLMs like Llama or Gemini.
- Why unresolved: The paper mentions the difference qualitatively but does not provide quantitative metrics such as proof success rates, average proof time, or number of assertions needed for successful proofs.
- What evidence would resolve it: A comparative study with numerical data on proof success rates, average proof times, and number of helper assertions required when using different LLM models.

### Open Question 2
- Question: What is the impact of RTL design complexity on the effectiveness of the GenAI-augmented induction-based formal verification flow?
- Basis in paper: [inferred] The paper mentions testing on "fairly complex designs" but does not specify how complexity affects the flow's performance.
- Why unresolved: The paper does not provide details on how design complexity influences the quality of generated assertions or the overall effectiveness of the verification flow.
- What evidence would resolve it: A systematic evaluation of the flow's performance across designs of varying complexity, including metrics like assertion quality, proof time, and success rate.

### Open Question 3
- Question: How can the risk of GenAI hallucinations be mitigated to ensure the reliability of generated helper assertions?
- Basis in paper: [explicit] The paper acknowledges the risk of GenAI hallucinations and recommends human-in-the-loop analysis.
- Why unresolved: The paper does not provide specific methods or strategies to mitigate hallucinations beyond recommending human analysis.
- What evidence would resolve it: Development and evaluation of automated or semi-automated techniques to detect and filter out hallucinated assertions, or a framework for integrating human oversight effectively.

### Open Question 4
- Question: How does the integration of specification documents with RTL code influence the quality of generated helper assertions?
- Basis in paper: [explicit] The paper describes using both specification documents and RTL code as inputs to the LLM for generating assertions.
- Why unresolved: The paper does not analyze how the quality or completeness of the specification documents affects the generated assertions.
- What evidence would resolve it: An empirical study comparing assertion quality when using detailed vs. incomplete specifications, or when specifications are omitted entirely.

## Limitations
- The approach relies heavily on the LLM's ability to understand complex hardware design patterns and formal verification concepts, which may not generalize well to all design types.
- The paper does not provide quantitative metrics on assertion quality or proof time improvements across a diverse benchmark suite, limiting generalizability claims.
- The human-in-the-loop requirement suggests the approach is not fully automated, and the frequency of required human intervention is not quantified.

## Confidence
- **High confidence**: The core concept of using LLMs to generate helper assertions for induction-based formal verification is technically sound and supported by the paper's examples with counter designs and ECCs.
- **Medium confidence**: The claim that GPT-4 generates higher quality assertions than other LLMs is based on limited comparative observations without systematic benchmarking across diverse design categories.
- **Medium confidence**: The assertion that generated helper assertions can resolve inductive step failures is demonstrated through examples but lacks comprehensive validation across complex industrial designs.

## Next Checks
1. **Benchmark diversity test**: Validate the approach across a standardized formal verification benchmark suite (e.g., ITC'99 or similar) with varying complexity levels to assess generalization performance and identify failure patterns.
2. **Human effort quantification**: Measure and report the exact frequency and nature of human interventions required across multiple verification campaigns to better understand the true automation level and scalability limitations.
3. **Cross-LLM comparison study**: Conduct a systematic comparison using the same verification tasks across multiple LLM models (including specialized hardware-focused models) with quantitative metrics for assertion quality, proof time, and false positive rates.