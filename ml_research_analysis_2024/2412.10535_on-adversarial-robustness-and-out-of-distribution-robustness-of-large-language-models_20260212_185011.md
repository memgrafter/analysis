---
ver: rpa2
title: On Adversarial Robustness and Out-of-Distribution Robustness of Large Language
  Models
arxiv_id: '2412.10535'
source_url: https://arxiv.org/abs/2412.10535
tags:
- robustness
- adversarial
- input
- performance
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the relationship between adversarial robustness
  and out-of-distribution (OOD) robustness in large language models (LLMs). Three
  models (LLaMA2-7b, LLaMA2-13b, Mixtral-8x7b) were evaluated on four benchmarks using
  accuracy, precision, recall, and F1 scores.
---

# On Adversarial Robustness and Out-of-Distribution Robustness of Large Language Models

## Quick Facts
- arXiv ID: 2412.10535
- Source URL: https://arxiv.org/abs/2412.10535
- Authors: April Yang; Jordan Tab; Parth Shah; Paul Kotchavong
- Reference count: 7
- Key outcome: Model-specific correlations exist between adversarial and OOD robustness across LLaMA2 and Mixtral models

## Executive Summary
This study investigates the relationship between adversarial robustness and out-of-distribution (OOD) robustness in large language models. Using three models (LLaMA2-7b, LLaMA2-13b, Mixtral-8x7b) and four benchmarks, the research evaluates how two robustness improvement strategies (AHP and ICR) perform across both robustness types. The findings reveal nuanced interactions, with different models showing varying correlation patterns between the two robustness types, suggesting that robustness strategies cannot be universally applied across different model architectures.

## Method Summary
The study evaluates three large language models across four benchmarks using accuracy, precision, recall, and F1 scores. Two robustness improvement strategies are implemented: Analytic Hierarchy Process (AHP) for adversarial robustness and In-Context Rewriting (ICR) for OOD robustness. Baseline performance is established without enhancements, then each strategy is applied to measure performance changes. Correlation analysis between adversarial and OOD robustness is performed using normalized metrics and linear regression to identify model-specific patterns.

## Key Results
- LLaMA2-7b exhibited neutral correlations between adversarial and OOD robustness
- LLaMA2-13b showed negative correlations between the two robustness types
- Mixtral demonstrated positive correlations, potentially due to domain-specific alignment
- Model-specific effectiveness of robustness strategies was observed across different architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-specific correlations exist between adversarial and OOD robustness
- Mechanism: Different architectures and parameter sizes lead to varying responses to robustness strategies, creating distinct correlation patterns between the two robustness types
- Core assumption: Architectural differences and model scaling affect how robustness strategies transfer between adversarial and OOD contexts
- Evidence anchors:
  - [abstract]: "Our findings highlight nuanced interactions between adversarial robustness and OOD robustness, with results indicating limited transferability between the two robustness types."
  - [section 7.3]: "From LLaMA2:7B to LLaMA2:13B, we see that correlation between the two robustness went from neutral to negative."
  - [corpus]: Weak evidence. Corpus papers focus on OOD detection and adversarial robustness separately, not their correlation.

### Mechanism 2
- Claim: Improvement methods show model-specific effectiveness
- Mechanism: Analytic Hierarchy Process (AHP) and In-Context Rewriting (ICR) work differently across models due to architectural differences and training data characteristics
- Core assumption: The effectiveness of robustness improvement methods depends on model-specific characteristics
- Evidence anchors:
  - [section 7.1]: "AHP consistently underperforms with this model, particularly in recall, indicating its limited effectiveness for smaller architectures."
  - [section 7.1]: "ICR proves more effective for LLaMA models, especially in enhancing recall, while AHP aligns better with Mixtral, yielding balanced improvements."
  - [corpus]: Weak evidence. No direct support for model-specific effectiveness of robustness methods.

### Mechanism 3
- Claim: Domain-specific alignment affects robustness correlations
- Mechanism: Models with training data closer to certain domains show different robustness patterns when tested on those domains
- Core assumption: Pretraining data distribution influences how models respond to OOD benchmarks
- Evidence anchors:
  - [abstract]: "Mixtral demonstrates positive correlations, potentially due to domain-specific alignment."
  - [section 7.2]: "This observation highlights a potential shortcoming in our experiment: the FlipKart e-commerce reviews may not be as out-of-distribution for Mixtral as they are for LLaMA models."
  - [corpus]: Weak evidence. Corpus papers don't address domain-specific alignment effects.

## Foundational Learning

- Concept: Linear regression and correlation analysis
  - Why needed here: To quantify the relationship between adversarial and OOD robustness across different models and strategies
  - Quick check question: How do you interpret a negative correlation coefficient in the context of robustness types?

- Concept: Min-max normalization
  - Why needed here: To standardize performance metrics across different benchmarks for meaningful comparison
  - Quick check question: What happens to the normalized values if all original values are identical?

- Concept: Precision, recall, and F1-score
  - Why needed here: To evaluate model performance comprehensively across different robustness contexts
  - Quick check question: In what scenario would a model have high precision but low recall?

## Architecture Onboarding

- Component map: Baseline evaluation pipeline: Model → Benchmark → Prediction → Metrics → Robustness improvement evaluation: Model + Strategy → Benchmark → Prediction → Metrics → Correlation analysis: Normalized metrics → Linear regression → Correlation coefficient
- Critical path: Baseline evaluation → Robustness improvement implementation → Correlation analysis → Model-specific insights
- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models show better robustness but require more resources
  - Benchmark diversity vs. experimental scope: More benchmarks provide better generalization but increase complexity
  - Strategy specificity vs. generalizability: Tailored strategies work better but may not transfer across models
- Failure signatures:
  - Inconsistent performance across benchmarks: Indicates model-specific limitations
  - Poor correlation between robustness types: Suggests fundamental differences in how models handle different perturbations
  - Strategy underperformance: Points to architectural mismatches or implementation issues
- First 3 experiments:
  1. Baseline evaluation on all benchmarks for all models to establish performance baselines
  2. Implementation of AHP and ICR strategies on adversarial benchmarks to test cross-context effectiveness
  3. Correlation analysis between adversarial and OOD robustness using normalized metrics and linear regression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation between adversarial robustness and OOD robustness evolve with model architectures beyond the three tested models?
- Basis in paper: [explicit] The paper concludes that "the relationship between adversarial robustness and OOD robustness may be influenced by both model size and architectural design," but only tests three specific models (LLaMA2-7b, LLaMA2-13b, Mixtral-8x7b).
- Why unresolved: The study's scope is limited to three models with specific architectures and parameter sizes, making it difficult to generalize findings across the broader landscape of LLM architectures.
- What evidence would resolve it: Testing a wider range of models with varying architectures (e.g., GPT, BERT variants, different transformer designs) and parameter sizes would clarify whether the observed model-specific trends are consistent or exceptional.

### Open Question 2
- Question: Does increasing the number of benchmarks and samples significantly alter the observed correlations between adversarial and OOD robustness?
- Basis in paper: [explicit] The authors acknowledge that "further research is needed with different Mixtral family models and more benchmarks to validate our findings" and note that "limited number of benchmarks" may have influenced their results.
- Why unresolved: The study uses only four benchmarks (two for each robustness type) with relatively small sample sizes, which may not capture the full spectrum of adversarial and OOD scenarios.
- What evidence would resolve it: Expanding the benchmark set to include more diverse tasks, attack types, and OOD domains, along with larger sample sizes, would determine whether the observed correlations are robust or artifacts of limited evaluation.

### Open Question 3
- Question: What is the impact of prompt design and refinement on the generalizability of robustness improvement methods across different task types and model families?
- Basis in paper: [explicit] The authors note that "models are sensitive to various prompting strategies crafted" and that "prompt overloading" may affect performance, particularly for complex tasks like QNLI and MNLI.
- Why unresolved: The study uses fixed prompt templates for robustness improvement strategies without exploring variations in prompt design or their impact on different task types.
- What evidence would resolve it: Systematically varying prompt structures, example selection, and task framing across different model families and benchmark tasks would reveal how prompt design influences robustness generalization.

## Limitations

- Narrow scope with only three models and four benchmarks may not capture full complexity of robustness relationships
- Potential confounding factors like pretraining data distribution differences are not controlled for
- Effectiveness of AHP and ICR strategies is reported but not deeply analyzed for fundamental architectural properties

## Confidence

**High Confidence**: The observation that robustness strategies show model-specific effectiveness is well-supported by the data. The distinct performance patterns of AHP and ICR across different models are clearly demonstrated and unlikely to be artifacts.

**Medium Confidence**: The correlation patterns between adversarial and OOD robustness are observed consistently but may be influenced by uncontrolled variables. While the model-specific trends are clear, the underlying mechanisms remain speculative without deeper architectural analysis.

**Low Confidence**: The explanation for Mixtral's positive correlation (domain-specific alignment) is particularly uncertain, as the paper acknowledges potential benchmark validity issues. The claim that robustness strategies cannot be universally applied is reasonable but needs broader validation.

## Next Checks

1. **Benchmark Validity Assessment**: Conduct a thorough analysis of the Flipkart dataset's OOD nature for each model by examining pretraining data overlap and vocabulary coverage. This would validate or challenge the current correlation findings.

2. **Architectural Ablation Study**: Systematically test robustness strategies across a broader range of model sizes and architectures (e.g., adding LLaMA2-70b, Phi-2, or BERT variants) to determine if the observed model-specific patterns hold more generally.

3. **Pretraining Data Analysis**: Investigate the relationship between pretraining data distribution and robustness patterns by comparing models with known pretraining corpora. This would test whether domain-specific alignment is indeed driving the observed correlations.