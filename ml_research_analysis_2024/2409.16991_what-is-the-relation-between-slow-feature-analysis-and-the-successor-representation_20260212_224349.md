---
ver: rpa2
title: What is the relation between Slow Feature Analysis and the Successor Representation?
arxiv_id: '2409.16991'
source_url: https://arxiv.org/abs/2409.16991
tags:
- which
- equation
- matrix
- type
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a formal connection between slow feature
  analysis (SFA) and the successor representation (SR), two methods from distinct
  areas of machine learning. The authors show that when SFA is applied to Markovian
  one-hot trajectories, the resulting eigenvalue problems are equivalent to those
  involving SR matrices.
---

# What is the relation between Slow Feature Analysis and the Successor Representation?

## Quick Facts
- **arXiv ID:** 2409.16991
- **Source URL:** https://arxiv.org/abs/2409.16991
- **Authors:** Eddie Seabrook; Laurenz Wiskott
- **Reference count:** 35
- **Primary result:** Establishes formal equivalence between slow feature analysis (SFA) and successor representation (SR) for Markovian one-hot trajectories.

## Executive Summary
This paper establishes a formal connection between slow feature analysis (SFA) and the successor representation (SR), two methods from distinct areas of machine learning. The authors show that when SFA is applied to Markovian one-hot trajectories, the resulting eigenvalue problems are equivalent to those involving SR matrices. Specifically, SFA is related to the combinatorial directed Laplacian matrix, τSFA to additive reversibilizations of transition matrices, and LFSFA to intercessor representations of SR. The main result is that the solutions to these SFA problems correspond to the eigenvectors of the associated matrices, and the slowness/correlation values correspond to the eigenvalues. The authors verify these theoretical results through simulations of a uniform random walk in a gridworld environment, demonstrating that the SFA outputs visually resemble grid-like representations, similar to those found in SR studies.

## Method Summary
The authors analyze the relationship between SFA and SR by applying various formulations of SFA (Type 1 and Type 2, normalized and unnormalized) to one-hot trajectories from finite MDPs. They examine the eigenvalue problems that arise in these SFA formulations and compare them to the decompositions of SR-related matrices, particularly Π(P^τ)add and ΠMadd. The analysis focuses on the limit as the data length T approaches infinity, showing how the matrices in SFA problems converge to forms directly related to SR matrices. The theoretical results are verified through simulations of a uniform random walk in a gridworld environment.

## Key Results
- SFA eigenvalue problems are equivalent to decompositions of SR-related matrices (Π(P^τ)add and ΠMadd) in the limit of infinite data
- The symmetrization in SFA problems produces real-valued eigenvectors that can be visualized as grid-like representations
- Type 2 SFA formulations provide the cleanest analytical connection to SR matrices by omitting the centering transformation
- Grid-like representations from SFA eigenvectors visually resemble those found in SR studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFA and SR produce equivalent representations in the limit of large data when applied to Markovian one-hot trajectories.
- Mechanism: In the limit T → ∞, the matrices involved in SFA problems converge to forms directly related to SR matrices. Specifically, SFA corresponds to the combinatorial directed Laplacian matrix, τSFA to additive reversibilizations of transition matrices, and LFSFA to intercessor representations of SR.
- Core assumption: The input time series is a Markovian one-hot trajectory from an ergodic Markov chain, and the data length T approaches infinity.
- Evidence anchors:
  - [abstract]: "a formal equivalence is demonstrated in terms of the grid-like representations that occur as solutions/eigenvectors"
  - [section 4.2]: Theorems 4.2.1-4.2.2 showing convergence of SFA matrices to SR-related forms
  - [corpus]: Weak evidence - corpus papers focus on different variants of SFA and SR but don't directly establish this equivalence
- Break condition: If the Markov chain is non-ergodic, the stationary distribution π doesn't exist uniquely, breaking the limit results. If T is finite, the approximation error prevents exact equivalence.

### Mechanism 2
- Claim: The symmetrization present in SFA problems is essential for producing real-valued eigenvectors that can be visualized as grid-like representations.
- Mechanism: SFA problems involve symmetrization through time-lagged correlations, which converts the underlying transition statistics into reversible forms. This guarantees real eigenvalues and eigenvectors, unlike the general non-reversible case where eigenvectors would be complex-valued.
- Core assumption: The symmetrization step is preserved in the formulation of SFA problems.
- Evidence anchors:
  - [section 4.2.1]: "all matrices on the left-hand side involve symmetrization" and "guaranteed that all matrices... can be orthogonally diagonalized with real eigenvalues and eigenvectors"
  - [section 2.3.2]: Discussion of how reversible Markov chains have real eigenvectors
  - [corpus]: Weak evidence - corpus papers don't explicitly discuss the role of symmetrization in producing real eigenvectors
- Break condition: If symmetrization is removed or altered, the eigenvectors may become complex-valued and lose their grid-like structure, breaking the equivalence with SR visualizations.

### Mechanism 3
- Claim: The Type 2 formulations of SFA problems provide the cleanest analytical connection to SR matrices.
- Mechanism: Type 2 SFA problems omit the centering transformation, which simplifies the limit results. The second moment matrix bΣ converges directly to the diagonal stationary distribution matrix Π, eliminating the need for noise addition and providing direct relationships like Π(Pτ)add and ΠMadd.
- Core assumption: The zero mean constraint can be omitted from SFA problems for this analysis.
- Evidence anchors:
  - [section 4.2.2]: Theorems 4.2.3-4.2.4 showing cleaner limits for Type 2 problems
  - [section 3.3]: Discussion of how Type 1 and Type 2 SFA are related
  - [corpus]: Weak evidence - corpus papers focus on Type 1 formulations primarily
- Break condition: If the zero mean constraint is essential for a specific application, the Type 2 formulations may not be applicable, requiring the more complex Type 1 analysis with noise addition.

## Foundational Learning

- **Concept: Eigenvalue problems and their generalized forms**
  - Why needed here: The entire analysis relies on expressing SFA solutions as eigenvalue problems that can be compared to SR matrix decompositions
  - Quick check question: What is the difference between a regular eigenvalue problem Aw = λw and a generalized eigenvalue problem Aw = λBw?

- **Concept: Markov chain theory and stationary distributions**
  - Why needed here: The equivalence between SFA and SR depends on understanding how transition matrices, stationary distributions, and their various transformations relate to each other
  - Quick check question: For a reversible Markov chain with stationary distribution π, what property does the matrix ΠP have (where Π = diag(π))?

- **Concept: Time-lagged correlations and their symmetrization**
  - Why needed here: SFA's objective function involves time-lagged correlations, which when symmetrized produce matrices related to SR through additive reversibilization
  - Quick check question: How does the symmetrized time-lagged correlation matrix Ωτ relate to forward and backward transition statistics?

## Architecture Onboarding

- **Component map:** Input trajectory → SFA matrix construction → limit analysis (Theorems 4.2.1-4.2.4) → eigenvalue decomposition → eigenvector visualization
- **Critical path:** The core analytical path is: input trajectory → SFA matrix construction → limit analysis (Theorems 4.2.1-4.2.4) → eigenvalue decomposition → eigenvector visualization. Each step depends on the previous one being correctly implemented.
- **Design tradeoffs:** Type 1 formulations include centering but require noise addition for non-invertible covariance matrices; Type 2 formulations are cleaner analytically but may not satisfy zero mean constraints needed in some applications. Symmetric normalization provides real eigenvectors but requires positive definite matrices.
- **Failure signatures:** Complex-valued eigenvectors indicate broken symmetrization; eigenvalues clustering near zero suggests insufficient data length T; mismatched eigenvector orderings between different formulations indicate implementation errors in the limit analysis.
- **First 3 experiments:**
  1. Verify the limit results for a simple 2-state Markov chain by computing SFA matrices for increasing T and checking convergence to theoretical forms
  2. Compare eigenvectors of Π(Pτ)add for different τ values to visualize how temporal scale affects representation structure
  3. Test the effect of non-ergodicity by introducing absorbing states and observing how the limit results break down

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SFA-based methods learn biologically plausible SR representations for non-finite MDPs (e.g., continuous state spaces)?
- **Basis in paper:** [inferred] The paper discusses the potential for SFA to adapt to general RL tasks beyond finite MDPs, but notes that the current connection to SR is limited to Markovian one-hot trajectories.
- **Why unresolved:** The paper primarily focuses on the relationship between SFA and SR in the context of finite MDPs. Generalizing this connection to continuous state spaces or other non-finite MDP settings requires further investigation.
- **What evidence would resolve it:** Experimental studies demonstrating that SFA-based methods can effectively learn SR-like representations for continuous state spaces or other non-finite MDPs, with performance comparable to or exceeding existing methods.

### Open Question 2
- **Question:** Can SFA be modified to naturally produce hexagonal grid-like representations, similar to those observed in grid cells in the entorhinal cortex?
- **Basis in paper:** [inferred] The paper discusses the difference between the rectangular grid-fields produced by SFA/SR and the hexagonal grid-fields observed in the brain. It suggests potential directions for modifying SFA to achieve hexagonal symmetry.
- **Why unresolved:** The paper does not provide a definitive solution for generating hexagonal grid-fields with SFA. Further research is needed to explore the feasibility and effectiveness of proposed modifications.
- **What evidence would resolve it:** Experimental studies demonstrating that modified versions of SFA can consistently produce hexagonal grid-fields in various spatial environments, with properties that closely match those observed in the brain.

### Open Question 3
- **Question:** Can SFA-based methods offer a biologically plausible learning mechanism for SR, compared to existing approaches like TD learning?
- **Basis in paper:** [explicit] The paper suggests that SFA-based methods could contribute to the development of biologically plausible SR learning mechanisms, given their foundation in the slowness principle.
- **Why unresolved:** The paper does not provide a detailed comparison of SFA-based SR learning mechanisms with existing approaches in terms of biological plausibility. Further research is needed to evaluate the biological plausibility of different SR learning methods.
- **What evidence would resolve it:** Comparative studies evaluating the biological plausibility of SFA-based SR learning mechanisms against existing approaches, considering factors such as neural implementation, energy efficiency, and alignment with observed neural activity.

## Limitations

- The theoretical equivalence relies heavily on the limit T → ∞ for Markovian one-hot trajectories. Finite-sample effects and non-ergodic chains may break the results in practice.
- The biological plausibility of implementing SR matrix operations in neural circuits remains unclear, as the paper doesn't address computational complexity or implementational constraints.
- The paper provides limited empirical validation beyond a single gridworld example, offering weak evidence for general applicability across different MDP structures.

## Confidence

- **High Confidence:** The mathematical derivations establishing the equivalence between SFA eigenvalue problems and SR matrix decompositions (Theorems 4.2.1-4.2.4) are rigorous and well-supported by the cited literature on matrix analysis and Markov chains.
- **Medium Confidence:** The visualization results showing grid-like representations from SFA eigenvectors align with expectations from SR literature, but the single gridworld example provides limited evidence for general applicability across different MDP structures.
- **Low Confidence:** The claim that this equivalence provides "new perspective on SFA as biologically plausible method for learning SRs" lacks supporting evidence about neural implementation or computational advantages over existing SR learning methods.

## Next Checks

1. Test the equivalence on multiple MDP structures beyond the gridworld, including environments with absorbing states and non-uniform transition probabilities, to assess robustness to ergodicity assumptions.
2. Compare the computational complexity and convergence rates of learning SRs through SFA versus direct temporal difference learning methods on the same tasks.
3. Implement a neural network approximation of the matrix operations involved and measure how well it captures the grid-like representations, addressing the biological plausibility claim.