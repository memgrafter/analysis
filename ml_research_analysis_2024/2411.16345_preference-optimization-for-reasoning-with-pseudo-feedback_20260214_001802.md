---
ver: rpa2
title: Preference Optimization for Reasoning with Pseudo Feedback
arxiv_id: '2411.16345'
source_url: https://arxiv.org/abs/2411.16345
tags:
- test
- cases
- pseudo
- reasoning
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method called PFPO to improve reasoning capabilities
  of large language models using pseudo feedback generated from test cases. The core
  idea is to frame the labeling of solutions as evaluating them against associated
  test cases, which can be automatically generated either by frontier LLMs or by extending
  self-consistency to multi-test-case scenarios.
---

# Preference Optimization for Reasoning with Pseudo Feedback

## Quick Facts
- **arXiv ID:** 2411.16345
- **Source URL:** https://arxiv.org/abs/2411.16345
- **Reference count:** 40
- **Primary result:** Method improves mathematical reasoning from 58.3 to 68.6 on MATH and coding from 21.1 to 24.6 on LiveCodeBench

## Executive Summary
This paper proposes PFPO (Preference Optimization with Pseudo Feedback), a method that improves reasoning capabilities of large language models using pseudo feedback generated from test cases. The core innovation is framing solution labeling as evaluation against automatically generated test cases, enabling preference optimization without human annotations. The method demonstrates substantial improvements on mathematical reasoning tasks (MATH: 58.3→68.6, GSM8K: 85.6→90.3) and coding tasks (LiveCodeBench: 21.1→24.6), surpassing Claude-3-Haiku on coding benchmarks. The approach shows that pseudo feedback from both frontier LLMs and self-consistency can be used iteratively to enhance model performance.

## Method Summary
PFPO improves reasoning capabilities by leveraging test cases as evaluation criteria for solutions. The method generates pseudo feedback through two complementary approaches: frontier LLM-generated test cases and self-consistency on multi-test-case problems. Solutions are evaluated by executing them against test cases, with passing solutions preferred over failing ones. The method applies iterative preference optimization (DPO/pDPO), starting with frontier LLM feedback for bootstrapping and switching to self-consistency for refinement. This approach eliminates the need for human annotations while maintaining high-quality feedback for training.

## Key Results
- Mathematical reasoning: MATH accuracy improves from 58.3 to 68.6
- Mathematical reasoning: GSM8K accuracy improves from 85.6 to 90.3
- Coding: LiveCodeBench pass@1 improves from 21.1 to 24.6, surpassing Claude-3-Haiku
- College Math accuracy improves from 34.3 to 42.3

## Why This Works (Mechanism)

### Mechanism 1: Frontier LLM Test Case Generation
Frontier LLMs generate both problem solutions and associated test cases, which are validated through majority voting across multiple generated solutions. This provides high-quality feedback for training preference pairs where correct solutions (passing all test cases) are preferred over incorrect ones.

### Mechanism 2: Self-Consistency Multi-Test-Case
The model samples multiple solutions and executes them against the same test cases. The most frequent outputs across these executions serve as pseudo labels, extending Wang et al.'s single-test-case self-consistency to multi-test-case scenarios.

### Mechanism 3: Iterative Feedback Alternation
The method alternates between frontier LLM feedback for initial bootstrapping and self-consistency feedback for refinement. This prevents overfitting to either source and leverages their complementary strengths in capturing different aspects of reasoning quality.

## Foundational Learning

- **Test case-based evaluation**: Essential for determining solution correctness through execution. Quick check: Can you explain how a solution is verified as correct or incorrect using test cases in this method?
- **Direct Preference Optimization (DPO)**: The optimization framework that uses preference pairs derived from test case evaluations. Quick check: What is the objective function used in DPO and how does it differ from standard supervised learning?
- **Self-consistency**: The method for generating pseudo labels by aggregating multiple model outputs. Quick check: How does majority voting work in the context of self-consistency for test case evaluation?

## Architecture Onboarding

- **Component map**: Problem → Solution generation → Test case execution → Preference pair creation → DPO training → Improved model
- **Critical path**: The end-to-end pipeline from problem formulation through iterative training to final model improvement
- **Design tradeoffs**: Frontier LLM feedback provides higher quality but requires external access, while self-consistency is self-contained but may suffer from model collapse
- **Failure signatures**: Poor test case quality leading to ineffective preference pairs, or policy collapse during iterative training causing performance plateaus
- **First 3 experiments**:
  1. Implement basic test case generation from frontier LLM and verify it produces valid pairs
  2. Add self-consistency feedback generation and compare quality to frontier LLM
  3. Implement iterative training with alternating feedback sources and measure performance gains

## Open Questions the Paper Calls Out

### Open Question 1
What is the upper bound on improvements from iterative self-consistency feedback in mathematical reasoning tasks? The paper shows performance plateaus after a few iterations, potentially due to synthetic test case quality plateauing or too many "easy" problems. Systematic experiments varying iteration numbers would establish the true upper bound.

### Open Question 2
How does pseudo test case quality scale with model size and reasoning capability? The paper shows pass rates ranging from 62.68% (self-consistency) to 82.41% (frontier LLM feedback), but doesn't explore scaling relationships across different model families or sizes.

### Open Question 3
Can pseudo feedback methods be extended to domains beyond mathematical reasoning and coding? The paper focuses on domains with explicit verification mechanisms, but it's unclear if this framework generalizes to other reasoning domains lacking clear verification mechanisms.

## Limitations

- Reliance on frontier LLM access for initial pseudo feedback creates scalability concerns and dependency on external model availability
- Test case quality verification is limited to pass rates without independent assessment of difficulty appropriateness
- Limited analysis of when each feedback source is most effective or optimal scheduling for alternation

## Confidence

- **High Confidence**: Core mechanism of test case-based preference optimization and benchmark results (MATH 58.3→68.6, GSM8K 85.6→90.3)
- **Medium Confidence**: Claims about frontier LLM superiority for initial feedback, though test case quality lacks independent verification
- **Low Confidence**: Assertion that iterative alternation provides optimal performance gains without detailed analysis of individual source contributions

## Next Checks

1. **Test Case Quality Verification**: Implement independent evaluation of frontier LLM-generated test cases against ground truth solutions to verify >60% pass rate and assess challenge appropriateness
2. **Feedback Source Comparison**: Conduct controlled experiments comparing performance when using only frontier LLM feedback versus only self-consistency for entire training process
3. **Generalization Testing**: Evaluate trained models on out-of-distribution reasoning problems to assess real-world capability beyond benchmark performance