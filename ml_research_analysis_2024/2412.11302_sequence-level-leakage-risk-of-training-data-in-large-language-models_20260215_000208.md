---
ver: rpa2
title: Sequence-Level Leakage Risk of Training Data in Large Language Models
arxiv_id: '2412.11302'
source_url: https://arxiv.org/abs/2412.11302
tags:
- tokens
- token
- leakage
- sequences
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces sequence-level leakage risk metrics to better
  quantify training data extraction from large language models (LLMs). Unlike prior
  work that uses averaged metrics like extraction rate, this approach computes exact
  and inexact sample leakage probabilities for individual sequences, enabling finer-grained
  analysis of memorization across different settings.
---

# Sequence-Level Leakage Risk of Training Data in Large Language Models

## Quick Facts
- arXiv ID: 2412.11302
- Source URL: https://arxiv.org/abs/2412.11302
- Reference count: 3
- Primary result: Sequence-level metrics reveal that extraction rate underestimates training data leakage by up to 2.14×

## Executive Summary
This work introduces sequence-level leakage risk metrics to better quantify training data extraction from large language models (LLMs). Unlike prior work that uses averaged metrics like extraction rate, this approach computes exact and inexact sample leakage probabilities for individual sequences, enabling finer-grained analysis of memorization across different settings. The study analyzes two models (LLaMa and OPT) across multiple datasets and reveals that individual sequence analysis provides more accurate insights than population-level metrics, with findings including that extraction rate underestimates leakage risk, that certain sequences are easier to extract with shorter prefixes or smaller models, and that token position significantly affects extraction ease.

## Method Summary
The paper introduces Exact Sample Leakage Probability (ESLP) and Inexact Sample Leakage Probability (ISLP) metrics to quantify training data leakage at the sequence level. These metrics compute the probability of extracting exact or partial training sequences under various decoding schemes including greedy, top-k, top-p, temperature sampling, and random sampling. The study evaluates 10,000 sequences containing Named Entities (4 tokens) and longer sequences (50 tokens) from Common Crawl and The Pile datasets using LLaMa and OPT models ranging from 125M to 13B parameters. The analysis varies prefix lengths (10-50 tokens) and decoding hyperparameters to understand how different factors affect leakage risk.

## Key Results
- Extraction rate underestimates leakage risk by up to 2.14× for randomized decoding schemes like top-k and top-p
- 30.4-41.5% of sequences are easier to extract with shorter prefixes or smaller models, contrary to average trends
- Partial leakage allowing incorrect tokens is not easier than verbatim extraction for most sequences (85.25-94.82%)
- Extracting later tokens is 423-912% easier than earlier tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence-level probabilities reveal hidden leakage trends that population-level metrics mask.
- Mechanism: By computing Exact Sample Leakage Probability (ESLP) for each sequence individually, the method captures conditional probability decay and non-linear effects that get averaged out in extraction rate.
- Core assumption: Individual sequence probabilities are statistically independent enough to reveal meaningful trends when aggregated conditionally.
- Evidence anchors:
  - [abstract] "extraction rate underestimates the threat of leakage of training data in randomized LLMs by as much as 2.14X"
  - [section] "Extraction Rate, the predominant metric used in prior quantification work, underestimates the threat of leakage of training data in randomized LLMs by as much as 2.14X"
  - [corpus] Found 25 related papers; FMR 0.411 suggests moderate relatedness, supporting relevance of sequence-level metrics.
- Break condition: If sequences are highly correlated or share similar prefixes, individual probability trends may not generalize to population behavior.

### Mechanism 2
- Claim: Randomized decoding schemes like top-k and top-p expose higher leakage risk than deterministic greedy decoding.
- Mechanism: Randomized decoding increases the probability space of possible outputs, so repeated sampling can hit the exact training sequence with higher cumulative probability than greedy decoding's single deterministic path.
- Core assumption: An attacker can make multiple queries per sequence, enabling exploration of the probability space.
- Evidence anchors:
  - [abstract] "extraction rate underestimates the threat of leakage of training data in randomized LLMs by as much as 2.14X"
  - [section] "it takes the randomized decoding schemes between 3 − 81 samples to start outperforming greedy decoding"
  - [corpus] Related works focus on membership inference and memorization, supporting the importance of analyzing randomized generation.
- Break condition: If the model's output distribution is too peaked or if the attacker cannot make multiple queries, randomized decoding advantage diminishes.

### Mechanism 3
- Claim: Token position significantly affects extraction ease, with later tokens being up to 912% easier to extract.
- Mechanism: Conditional probability compounds as the model generates earlier tokens; correct earlier tokens increase the probability of correct later tokens, creating a "snowball effect."
- Core assumption: The language model's conditional distributions are well-calibrated and the prefix context sufficiently constrains the output.
- Evidence anchors:
  - [abstract] "Extracting later tokens in a sequence is as much as 912% easier than extracting earlier tokens"
  - [section] "average token probability of each token found at a particular position over all sequences as a function of its position"
  - [corpus] Related works on memorization dynamics and scaling laws reinforce the significance of token-level analysis.
- Break condition: If the model's context window is too short or if earlier tokens are highly ambiguous, the advantage of later tokens may be reduced.

## Foundational Learning

- Concept: Probability theory and conditional probability.
  - Why needed here: The paper relies on computing conditional probabilities for token sequences (TLP, ESLP, n-ISLP) to quantify leakage risk.
  - Quick check question: If a model assigns probability 0.1 to token A given prefix P, and 0.2 to token B given P+A, what is the joint probability of generating A then B?

- Concept: Language modeling objective and autoregressive generation.
  - Why needed here: Understanding how models predict the next token based on previous tokens is essential to grasp why certain decoding schemes or token positions affect leakage.
  - Quick check question: In an autoregressive model, what is the probability of generating a sequence s1, s2, s3 if the model's per-token probabilities are 0.5, 0.4, and 0.3 respectively?

- Concept: Statistical sampling and approximation methods.
  - Why needed here: Computing exact n-ISLP for large vocabularies is intractable; the paper uses tiered sampling and error bounds to approximate these probabilities.
  - Quick check question: If you approximate a distribution by only considering the top-k tokens, what is the worst-case error if the ignored tail has probability mass ε?

## Architecture Onboarding

- Component map:
  - Data ingestion: Load LLaMa and OPT models and datasets (Common Crawl, The Pile)
  - Metric computation: Implement ESLP, TLP, n-ISLP functions
  - Decoding scheme handlers: Greedy, top-k, top-p, temperature, sampling
  - Analysis pipeline: Loop over sequences, compute metrics under different settings
  - Visualization: Plot trends (prefix length, model size, token position) and leakage distributions

- Critical path:
  1. Load pre-trained models and target datasets
  2. For each sequence, compute ESLP under multiple decoding schemes
  3. Aggregate results to compute population statistics and identify trends
  4. Generate plots and tables summarizing findings

- Design tradeoffs:
  - Exact vs approximate n-ISLP: Exact computation is intractable for large vocabularies; approximation trades accuracy for feasibility
  - Number of samples per sequence: More samples increase leakage detection but also computational cost
  - Dataset size: Larger datasets improve statistical significance but increase runtime

- Failure signatures:
  - ESLP values near zero for all sequences may indicate model underfitting or overly strict decoding
  - High variance in ESLP across sequences may suggest dataset contamination or tokenization issues
  - Disproportionate leakage in certain token positions may indicate positional bias in training data

- First 3 experiments:
  1. Compute ESLP for a small set of sequences using greedy decoding to verify basic functionality
  2. Compare ESLP trends as prefix length increases for a single sequence to validate trend detection
  3. Run n-ISLP computation on a toy vocabulary to confirm the tiered approximation logic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ESLP metric accurately capture training data leakage for larger models (e.g., >13B parameters) and longer sequences (e.g., >50 tokens)?
- Basis in paper: [explicit] The study evaluated models up to 13B parameters and sequences up to 50 tokens. The authors note their metrics become computationally expensive for larger vocabularies and longer sequences.
- Why unresolved: The paper does not explore model sizes beyond 13B parameters or sequence lengths beyond 50 tokens, which are increasingly common in modern LLMs.
- What evidence would resolve it: Empirical results showing ESLP trends for models >13B parameters and sequences >50 tokens, including computational feasibility and accuracy comparisons.

### Open Question 2
- Question: How do the 6 types of ESLP trends (illustrated in Figure 3) vary across different domains of text (e.g., code, medical, legal) compared to general web text?
- Basis in paper: [explicit] The authors discovered 6 different types of trends in ESLP as prefix length or model size increases, but only tested on general web text datasets (Common Crawl, The Pile).
- Why unresolved: The study only examined web-based datasets and did not explore domain-specific text, which may exhibit different memorization patterns.
- What evidence would resolve it: Comparative analysis of ESLP trends across multiple domains, showing whether the 6 patterns hold or if new patterns emerge for specialized text types.

### Open Question 3
- Question: What is the relationship between training data deduplication strategies and the sequence-level leakage risk measured by ESLP?
- Basis in paper: [explicit] The authors mention training data duplication as a factor studied in prior work but do not investigate its effect on sequence-level leakage risk.
- Why unresolved: While the paper analyzes various factors affecting leakage, it does not specifically examine how deduplication of training data (removing duplicate sequences) impacts individual sequence extraction probabilities.
- What evidence would resolve it: Comparative ESLP measurements on datasets with and without deduplication, showing how sequence-level leakage risk changes based on training data preprocessing strategies.

## Limitations

- The analysis focuses on pre-trained models rather than fine-tuned ones, which may exhibit different leakage patterns
- The approximation methods for computing n-ISLP rely on tiered sampling with error bounds that may bias results for rare token sequences
- The study assumes an attacker can make multiple queries per sequence, but real-world attack constraints are not modeled

## Confidence

- Extraction rate underestimation by 2.14×: High confidence
- Sequence-level trends (prefix length, model size): Medium confidence
- Partial leakage difficulty (85.25-94.82% finding): Medium confidence
- Token position effects (423-912% easier for later tokens): High confidence

## Next Checks

1. **Replication on fine-tuned models**: Test the sequence-level leakage metrics on fine-tuned LLMs (e.g., instruction-tuned variants of LLaMa) to determine if memorization patterns differ significantly from pre-trained models, particularly for the partial leakage findings.

2. **Cross-dataset validation**: Apply the exact same methodology to a completely different corpus (e.g., ArXiv papers or GitHub code) to verify whether the specific percentages (like 30.4-41.5% of sequences being easier with shorter prefixes) are robust or dataset-dependent.

3. **Query-limited attack simulation**: Modify the analysis to simulate realistic attack constraints by limiting the number of queries per sequence and measuring how this affects the advantage of randomized decoding schemes, testing whether the 2.14× underestimation persists under practical constraints.