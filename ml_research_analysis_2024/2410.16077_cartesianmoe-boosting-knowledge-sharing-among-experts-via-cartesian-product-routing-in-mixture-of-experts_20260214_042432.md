---
ver: rpa2
title: 'CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product
  Routing in Mixture-of-Experts'
arxiv_id: '2410.16077'
source_url: https://arxiv.org/abs/2410.16077
tags:
- experts
- cartesianmoe
- routing
- fine-grained
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CartesianMoE, a mixture-of-experts model that
  enhances knowledge sharing among experts by implementing a Cartesian product routing
  mechanism. Instead of using shared experts combined with routed experts in an additive
  manner, CartesianMoE uses two sets of fine-grained sub-experts and combines them
  via Cartesian product to derive real experts, enabling group-wise knowledge sharing.
---

# CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2410.16077
- Source URL: https://arxiv.org/abs/2410.16077
- Authors: Zhenpeng Su; Xing Wu; Zijia Lin; Yizhe Xiong; Minxuan Lv; Guangyuan Ma; Hui Chen; Songlin Hu; Guiguang Ding
- Reference count: 40
- One-line primary result: Achieves perplexity of 6.08 on Pile validation and outperforms other MoE methods in 7 out of 8 downstream benchmarks

## Executive Summary
CartesianMoE introduces a novel mixture-of-experts architecture that enhances knowledge sharing among experts through a Cartesian product routing mechanism. Unlike traditional MoE models that use shared experts in an additive manner, CartesianMoE employs two sets of fine-grained sub-experts and combines them via Cartesian product to derive real experts, enabling group-wise knowledge sharing. The model demonstrates superior performance on both perplexity and downstream tasks while achieving better routing robustness compared to existing MoE approaches.

## Method Summary
CartesianMoE implements a two-layer MoE structure where each layer contains fine-grained sub-experts. Instead of traditional routing where tokens are assigned to individual experts, CartesianMoE uses Cartesian product routing where each real expert is derived from the combination of sub-experts from both layers. This design enables group-wise knowledge sharing among experts while maintaining computational efficiency. The model also incorporates shared experts for global knowledge sharing, creating a hierarchical knowledge sharing system that combines global, group-wise, and expert-specific knowledge.

## Key Results
- Achieves perplexity of 6.08 on Pile validation set, outperforming other MoE methods
- Outperforms other MoE methods in 7 out of 8 downstream benchmarks in MoE-Base setting and 6 out of 8 in MoE-Large setting
- Demonstrates better routing robustness, maintaining low perplexity even when top-1 routed expert is disabled
- Shows smaller perplexity variance compared to baseline MoE models under routing perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CartesianMoE implements group-wise knowledge sharing among experts by combining two sets of sub-experts via Cartesian product.
- Mechanism: The model splits experts into two sets of sub-experts and derives each expert by combining one sub-expert from each set. This allows experts to share group-wise knowledge through common sub-experts.
- Core assumption: Group-wise knowledge sharing is more effective than global knowledge sharing alone.
- Evidence anchors:
  - [abstract] "CartesianMoE uses two sets of fine-grained sub-experts and combines them via Cartesian product to derive real experts, enabling group-wise knowledge sharing."
  - [section 4.1] "Through such a two-layer structural design, the Cartesian Product mechanism is natively implemented, and is supposed to facilitate knowledge sharing among experts."
  - [corpus] Weak - no direct corpus evidence for group-wise sharing effectiveness.
- Break condition: If group-wise sharing does not provide performance benefits over global sharing alone.

### Mechanism 2
- Claim: CartesianMoE forms a more complete knowledge sharing system by combining global shared knowledge with group-wise shared knowledge and expert-specific knowledge.
- Mechanism: The model uses shared experts for global knowledge, sub-experts for group-wise knowledge, and individual expert parameters for expert-specific knowledge.
- Core assumption: A hierarchical knowledge sharing system is more effective than flat sharing approaches.
- Evidence anchors:
  - [abstract] "CartesianMoE allows to divide experts into groups with each sharing some group-wise knowledge."
  - [section 6.1] "The result well reflects the effectiveness of group-wise knowledge sharing among experts proposed by CartesianMoE, which is equally important as global knowledge sharing introduced by shared experts."
  - [corpus] Weak - no direct corpus evidence for hierarchical knowledge sharing effectiveness.
- Break condition: If removing shared experts does not significantly impact performance.

### Mechanism 3
- Claim: CartesianMoE achieves better routing robustness through enhanced knowledge sharing among experts.
- Mechanism: By sharing more knowledge among experts (global + group-wise), the model becomes less sensitive to routing errors since multiple experts contain overlapping knowledge.
- Core assumption: Better knowledge sharing reduces sensitivity to routing accuracy.
- Evidence anchors:
  - [abstract] "we also find that CartesianMoE achieves better expert routing robustness."
  - [section 6.2] "even with the top-1 routed expert disabled, CartesianMoE still yields the lowest PPL, and enjoys a much smaller PPL variance, compared to other MoE methods."
  - [corpus] Weak - no direct corpus evidence for routing robustness benefits.
- Break condition: If disabling top experts does not show improved robustness compared to other methods.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding MoE is essential to grasp how CartesianMoE modifies the routing and knowledge sharing mechanisms.
  - Quick check question: How does MoE reduce computational costs compared to dense models?

- Concept: Knowledge sharing among experts
  - Why needed here: CartesianMoE's key innovation is improving knowledge sharing, so understanding different sharing approaches is crucial.
  - Quick check question: What are the differences between global knowledge sharing (shared experts) and group-wise knowledge sharing?

- Concept: Cartesian product in set theory
  - Why needed here: The method's name and core mechanism rely on understanding how Cartesian products work to combine sub-experts.
  - Quick check question: If set A has 4 elements and set B has 3 elements, how many unique pairs result from their Cartesian product?

## Architecture Onboarding

- Component map: Token → Router 1 (sub-layer A) → MoE sub-layer A → Residual connection → Router 2 (sub-layer B) → MoE sub-layer B → Combine outputs → Add residual → Next layer

- Critical path: Token → Router 1 (sub-layer A) → MoE sub-layer A → Residual connection → Router 2 (sub-layer B) → MoE sub-layer B → Combine outputs → Add residual → Next layer

- Design tradeoffs: Fine-grained experts vs computational cost, number of sub-experts vs routing complexity, shared experts vs parameter efficiency.

- Failure signatures: Poor perplexity could indicate routing imbalance, knowledge sharing issues, or insufficient expert capacity; downstream task performance drops might indicate over-specialization or under-sharing.

- First 3 experiments:
  1. Remove shared experts to test the impact of global knowledge sharing on performance.
  2. Disable top-1 routed expert to measure routing robustness improvements.
  3. Compare with flattened fine-grained experts to validate the benefits of Cartesian product routing over additive routing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CartesianMoE's performance compare to other MoE models when scaling to extremely large model sizes (e.g., 100B+ parameters)?
- Basis in paper: [inferred] The paper only reports results for models up to 7.25B parameters, leaving the performance at larger scales unexplored.
- Why unresolved: The paper does not include experiments with models of 100B+ parameters, which is a critical regime for large language models.
- What evidence would resolve it: Training and evaluating CartesianMoE with 100B+ parameters and comparing its performance to other MoE models in terms of perplexity and downstream task performance.

### Open Question 2
- Question: What is the impact of using different activation functions in the MoE sub-layers of CartesianMoE?
- Basis in paper: [inferred] The paper uses SwiGLU as the activation function but does not explore the effects of other activation functions.
- Why unresolved: The choice of activation function can significantly influence model performance, and the paper does not provide a comparison of different activation functions.
- What evidence would resolve it: Conducting experiments with different activation functions (e.g., ReLU, GELU, GeGLU) in the MoE sub-layers and comparing their impact on CartesianMoE's performance.

### Open Question 3
- Question: How does the performance of CartesianMoE vary with different numbers of MoE sub-layers in the Cartesian Product Layer?
- Basis in paper: [explicit] The paper mentions that only two MoE sub-layers are used in the Cartesian Product Layer, but suggests that extending to more sub-layers could be explored in future work.
- Why unresolved: The paper does not provide empirical evidence on the performance of CartesianMoE with varying numbers of MoE sub-layers.
- What evidence would resolve it: Experimenting with CartesianMoE using different numbers of MoE sub-layers (e.g., 3, 4, 5) and evaluating the impact on model performance in terms of perplexity and downstream tasks.

## Limitations

- The paper lacks ablation studies that would isolate the individual contributions of global knowledge sharing versus group-wise knowledge sharing.
- The routing robustness claim relies on a single experimental condition (disabling top-1 expert) rather than comprehensive testing across multiple routing failure scenarios.
- The computational efficiency claims are difficult to verify without detailed FLOPs analysis comparing CartesianMoE to baseline MoE architectures with equivalent parameter counts.

## Confidence

- **High Confidence**: The experimental results showing CartesianMoE's superior perplexity (6.08 on Pile validation) and downstream task performance (7/8 wins in MoE-Base, 6/8 wins in MoE-Large) are well-documented with clear methodology and results.
- **Medium Confidence**: The claim that CartesianMoE achieves better routing robustness is supported by experimental evidence but lacks comprehensive testing across multiple routing failure scenarios and ablation studies.
- **Low Confidence**: The assertion that group-wise knowledge sharing is "equally important" as global knowledge sharing lacks direct experimental validation through ablation studies that would remove each component independently.

## Next Checks

1. **Ablation Study on Knowledge Sharing Components**: Remove shared experts from CartesianMoE to isolate the contribution of group-wise knowledge sharing alone, then compare performance with the full model. This would directly test whether group-wise sharing is indeed "equally important" as claimed.

2. **Comprehensive Routing Robustness Testing**: Systematically disable top-k experts (k=1,2,3) and measure performance degradation across all MoE baselines, not just the top-1 case. This would provide a more complete picture of routing robustness improvements.

3. **Computational Efficiency Analysis**: Calculate and compare FLOPs per token for CartesianMoE versus baseline MoE architectures with equivalent parameter counts to verify the claimed efficiency improvements from fine-grained experts and Cartesian product routing.