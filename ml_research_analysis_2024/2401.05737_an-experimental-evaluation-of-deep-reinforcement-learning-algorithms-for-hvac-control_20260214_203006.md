---
ver: rpa2
title: An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC
  control
arxiv_id: '2401.05737'
source_url: https://arxiv.org/abs/2401.05737
tags:
- control
- learning
- comfort
- algorithms
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares several state-of-the-art deep reinforcement
  learning algorithms for HVAC control in terms of comfort, energy consumption, and
  robustness. Using the Sinergym framework, SAC and TD3 algorithms outperform reactive
  controllers in complex environments and achieve better energy savings, though SAC
  showed convergence issues in dynamic comfort ranges.
---

# An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control

## Quick Facts
- arXiv ID: 2401.05737
- Source URL: https://arxiv.org/abs/2401.05737
- Reference count: 40
- Key outcome: SAC and TD3 algorithms outperform reactive controllers in complex environments, achieving better energy savings while maintaining comfort within acceptable bounds.

## Executive Summary
This study compares deep reinforcement learning algorithms (SAC, TD3, PPO) for HVAC control using the Sinergym framework. The algorithms were tested on two building models across three climate types, evaluating comfort, energy consumption, and robustness. TD3 demonstrated superior energy efficiency by exploiting temperature inertia, while SAC showed early convergence and better comfort-consumption balance in office buildings. The research highlights that DRL agents can match or surpass traditional reactive controllers in complex scenarios, with tunable reward weights allowing customizable optimization of comfort versus energy use.

## Method Summary
The study employed Sinergym version 1.8.2 with EnergyPlus to simulate HVAC control scenarios. Two building models (5ZoneAutoDXVAV and 2ZoneDataCenterHVAC) were tested across three weather types (hot dry, mixed humid, cool marine). DRL algorithms (SAC, TD3, PPO) were trained for 20 episodes with continuous action spaces, normalized observations, and a linear reward function balancing comfort and power consumption. Agents were evaluated every 4 episodes over 3 evaluation episodes, measuring mean episode reward, power demand, and comfort violation metrics.

## Key Results
- SAC and TD3 outperformed reactive controllers in complex environments while achieving significant energy savings
- TD3 exploited temperature inertia to reduce power demand with minimal comfort violations
- Reward weight tuning allowed flexible optimization between comfort and energy consumption without changing the underlying architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TD3 outperforms RBCs in complex HVAC environments by reducing power demand at the cost of minor comfort violations.
- Mechanism: TD3 learns control policies that operate closer to comfort limits, exploiting temperature inertia to minimize HVAC energy use while staying within acceptable deviation bounds.
- Core assumption: Comfort violations are acceptable if deviations are small and do not breach safety thresholds.
- Evidence anchors:
  - [section] "TD3 is the most energetically costly agent, while PPO maintains a significant balance between both metrics without demonstrating outstanding performance."
  - [section] "Looking at Figure 7, we can see that, although many comfort violations for TD3 occur throughout the year, their value is quite low, mainly due to temperature inertia..."
  - [corpus] Weak: no direct energy consumption measurements for TD3 vs RBC in peer papers; relies on internal simulation data.
- Break condition: If comfort violations exceed safety limits (e.g., >1.6℃ in summer, >2.4℃ in winter for the tested buildings), TD3's strategy fails.

### Mechanism 2
- Claim: SAC achieves early convergence and superior comfort-consumption balance in office building scenarios.
- Mechanism: SAC uses entropy regularization to balance exploration and exploitation, enabling smoother setpoint adjustments that align with dynamic comfort ranges.
- Core assumption: A soft actor-critic framework can adapt to changing setpoint periods (e.g., summer vs winter) without catastrophic forgetting.
- Evidence anchors:
  - [section] "SAC is the closest agent to RBC control in all climates, followed by PPO and TD3, with similar mean rewards in all scenarios."
  - [section] "SAC again demonstrates early convergence, outperforming PPO in cool and hot climates..."
  - [corpus] Weak: other papers confirm SAC's general robustness but lack direct comparison with RBCs under identical conditions.
- Break condition: If comfort ranges shift rapidly, SAC's entropy-based policy may not adapt quickly enough, leading to reward drops.

### Mechanism 3
- Claim: Modifying reward weights allows tunable trade-offs between comfort and energy consumption without changing the underlying DRL architecture.
- Mechanism: Adjusting the ω parameter in the reward function directly scales the influence of comfort vs. power demand terms, enabling targeted optimization.
- Core assumption: Linear reward weighting is sufficient to capture the multi-objective trade-off without nonlinear interactions.
- Evidence anchors:
  - [section] "We observe that a greater emphasis on comfort in the reward function reduces comfort violations..."
  - [section] "A higher comfort assurance implies, in most cases, a consequent increase in power demand."
  - [corpus] Weak: few external studies report explicit weight tuning experiments; most use fixed 50-50 splits.
- Break condition: If nonlinear effects dominate (e.g., large deviations causing disproportionate energy spikes), linear weighting may fail.

## Foundational Learning

- Concept: Reward shaping in multi-objective RL.
  - Why needed here: The study explicitly tunes comfort vs. consumption weights; understanding how reward terms influence policy gradients is essential.
  - Quick check question: If ω = 0.75, which term (comfort or consumption) receives higher weight in the reward?

- Concept: Catastrophic forgetting in curriculum learning.
  - Why needed here: The paper observes that agents trained across climates overfit to the last environment; recognizing forgetting mechanisms is key to diagnosing this.
  - Quick check question: What is the term for when a neural network loses performance on earlier tasks after training on new ones?

- Concept: Temperature inertia in building thermal dynamics.
  - Why needed here: TD3 exploits inertia to reduce energy use; understanding how building mass and insulation affect temperature response is necessary to interpret results.
  - Quick check question: In a well-insulated building, how quickly does indoor temperature respond to changes in HVAC setpoint?

## Architecture Onboarding

- Component map: Sinergym wrapper -> EnergyPlus simulator -> DRL agent (PPO/TD3/SAC) -> Reward function -> Observation/action spaces
- Critical path: Observation extraction -> Normalization -> Agent policy -> Action output -> EnergyPlus step -> Reward calculation -> Policy update
- Design tradeoffs: Continuous vs. discrete actions (flexibility vs. simplicity), single vs. multi-zone control (scalability vs. complexity), fixed vs. adaptive reward weights (stability vs. customization)
- Failure signatures:
  - Poor convergence: reward plateaus early, no improvement over RBCs
  - Overfitting: high reward in training climate, large drops in test climates
  - Catastrophic forgetting: performance degrades on earlier climates after training on later ones
- First 3 experiments:
  1. Train PPO, TD3, SAC on 5ZoneAutoDXVAV with 50-50 reward weights; evaluate mean reward, comfort violation, and power demand
  2. Test best agent from (1) in all three weathers (hot, mixed, cool) to assess robustness
  3. Repeat (1) with ω = 0.25 and ω = 0.75 to analyze comfort-consumption trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DRL algorithms in HVAC control scale with building complexity beyond the two tested scenarios?
- Basis in paper: [explicit] The paper notes that DRL agents matched or surpassed reactive controllers in complex environments but only tested two building models.
- Why unresolved: The study focused on a single-story office building and a data center, limiting generalizability to larger or differently structured buildings.
- What evidence would resolve it: Testing DRL algorithms in diverse building types with varying numbers of zones and HVAC systems.

### Open Question 2
- Question: What is the impact of catastrophic forgetting on the long-term performance of DRL agents trained with curriculum learning?
- Basis in paper: [explicit] The paper observed that curriculum learning led to catastrophic forgetting, where agents specialized in the last training environment.
- Why unresolved: The study did not explore architectural or training data handling changes to mitigate catastrophic forgetting.
- What evidence would resolve it: Experiments testing different neural network architectures or experience replay techniques to reduce catastrophic forgetting.

### Open Question 3
- Question: How does sampling from a diverse set of climate observations during training affect the generalization capabilities of DRL agents?
- Basis in paper: [inferred] The paper suggests that training on a fixed climate limits generalization and proposes sampling from multiple climates as an alternative.
- Why unresolved: The study did not implement or test this alternative training approach.
- What evidence would resolve it: Comparing DRL agents trained on fixed climates versus those trained on diverse climate samples in various evaluation environments.

## Limitations
- Limited external validation of energy consumption measurements for TD3 vs RBC performance
- Lack of peer-reviewed data on SAC's comfort-consumption trade-off in office buildings
- No independent confirmation of catastrophic forgetting during curriculum learning

## Confidence
- High Confidence: TD3's exploitation of temperature inertia for energy savings (supported by simulation data and internal validation)
- Medium Confidence: SAC's superior performance in office building scenarios (based on internal Sinergym experiments but lacks external replication)
- Low Confidence: Generalizability of comfort-consumption trade-offs across different building types and climates (limited to tested models)

## Next Checks
1. External Validation: Run the same DRL algorithms on an independent HVAC simulator (e.g., EnergyPlus with different weather files) to verify performance claims
2. Real-World Testing: Deploy TD3 and SAC controllers in a live building environment to measure actual energy savings and comfort compliance
3. Algorithm Robustness: Test the trained agents on buildings with different thermal characteristics (insulation, window area) to assess adaptability beyond the original training set