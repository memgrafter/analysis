---
ver: rpa2
title: 'VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction
  Analysis'
arxiv_id: '2411.18038'
source_url: https://arxiv.org/abs/2411.18038
tags:
- detection
- vision
- interaction
- language
- human-object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VLM-HOI, a novel approach that leverages
  the capabilities of large Vision Language Models (VLMs) to enhance Human-Object
  Interaction (HOI) detection. The core idea is to explicitly utilize VLMs as an objective
  function by quantifying the similarity of predicted HOI triplets using image-text
  matching techniques.
---

# VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis

## Quick Facts
- arXiv ID: 2411.18038
- Source URL: https://arxiv.org/abs/2411.18038
- Authors: Donggoo Kang; Dasol Jeong; Hyunmin Lee; Sangwoo Park; Hasil Park; Sunkyu Kwon; Yeongjoon Kim; Joonki Paik
- Reference count: 40
- Primary result: Achieves state-of-the-art HOI detection accuracy on HICO-DET and V-COCO benchmarks using VLMs for knowledge distillation

## Executive Summary
This paper introduces VLM-HOI, a novel approach that leverages Vision Language Models (VLMs) for Human-Object Interaction (HOI) detection. The method uses VLMs as an objective function by computing image-text matching scores between predicted HOI triplets and grounded text prompts. These scores are then used in a contrastive optimization framework to improve HOI detection accuracy. Experiments demonstrate that VLM-HOI achieves state-of-the-art performance on HICO-DET and V-COCO benchmarks, outperforming previous methods by 0.5-1.0% mAP on HICO-DET and 1.2-7.7% AP on V-COCO.

## Method Summary
VLM-HOI uses a DETR-based encoder with a query-based transformer decoder for HOI detection. Predicted HOI triplets are converted into grounded sentences and used as prompts for a BLIP VLM to compute image-text similarity scores. These scores are then used in a contrastive loss function to optimize the HOI network, effectively distilling knowledge from the VLM into the detection model. The method represents HOI triplets linguistically to leverage the language comprehension capabilities of VLMs, which are shown to be more effective than CLIP models for this task due to their object-centric localization and fine-grained matching capabilities.

## Key Results
- Improves HOI detection accuracy on HICO-DET by 0.5-1.0% mAP compared to previous state-of-the-art methods
- Achieves 1.2-7.7% AP improvement on V-COCO benchmark
- Demonstrates effectiveness of VLMs for knowledge distillation in HOI detection through contrastive learning
- Shows particular strength in detecting rare interactions compared to previous approaches

## Why This Works (Mechanism)

### Mechanism 1
- VLMs outperform CLIP for HOI detection because they are object-centric and provide better localization through fine-grained sub-word level matching
- Core assumption: Linguistic representation of HOI triplets effectively captures visual interaction context
- Break condition: If linguistic representation fails to capture essential interaction semantics

### Mechanism 2
- Contrastive learning with image-text matching scores as objectives improves HOI detection by distilling knowledge from VLMs
- Core assumption: VLM's pre-trained joint distribution over visual and textual modalities is robust and transferable to HOI detection
- Break condition: If VLM's pre-training data and objectives don't align well with HOI semantics

### Mechanism 3
- Grounding HOI triplets into natural language sentences enables effective knowledge transfer from VLMs to HOI detection
- Core assumption: Grounded sentences capture core semantics of HOI triplets despite potential ungrammaticality
- Break condition: If grounded sentences fail to capture essential interaction semantics or introduce significant noise

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To optimize HOI network by aligning its representations with VLM's joint visual-linguistic understanding using image-text similarity scores
  - Quick check question: How does the contrastive loss function encourage HOI network to distinguish between positive and negative HOI triplets?

- Concept: Vision-language models (VLMs)
  - Why needed here: To leverage comprehensive understanding of both visual and linguistic modalities for distilling knowledge into HOI detection task
  - Quick check question: What are key differences between VLMs like BLIP and traditional image-text models like CLIP in terms of object-centric localization?

- Concept: Grounding visual detections into language
  - Why needed here: To convert predicted HOI triplets into textual representation that can be processed by VLM for computing similarity scores
  - Quick check question: Why is it important to include human, object, and interaction verb in grounded sentence prompts for effective knowledge transfer?

## Architecture Onboarding

- Component map: Input image → DETR encoder → Object detection → HOI triplet prediction → Grounded sentences → VLM similarity computation → Contrastive loss → HOI network optimization

- Critical path: Input image flows through DETR encoder for feature extraction, object detection, and HOI triplet prediction. Predicted triplets are grounded into sentences, which are used as prompts for VLM to compute similarity scores. These scores are used in contrastive loss to optimize the HOI network.

- Design tradeoffs: Using large VLM like BLIP increases computational requirements and training time compared to smaller models like CLIP. Grounding sentences may introduce noise or fail to capture essential semantics. Contrastive learning may require careful hyperparameter tuning.

- Failure signatures: Poor HOI detection performance despite successful VLM similarity computation. High variance in similarity scores across different grounded sentences or images. Slow convergence or unstable training due to contrastive loss.

- First 3 experiments:
  1. Evaluate HOI detection performance on HICO-DET and V-COCO benchmarks without VLM distillation to establish baseline
  2. Compare HOI detection performance using different VLM models (BLIP vs. CLIP) to assess impact of object-centric localization
  3. Analyze effect of different grounded sentence structures (full sentence vs. verb-only) on VLM similarity scores and HOI detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- How does proposed method's performance scale with size of VLM used as knowledge distillation source?
- Basis: Paper mentions BLIP used due to hardware constraints and states "any sufficiently large VLM can be utilized"
- Why unresolved: No experiments comparing different sizes of VLMs or their impact on HOI detection performance
- What evidence would resolve it: Experiments comparing performance using VLMs of different sizes (BLIP vs. larger models like GPT-4V or LLaVA)

### Open Question 2
- How does proposed method perform on HOI detection in videos or other temporal data?
- Basis: Paper focuses on image-based HOI detection benchmarks and doesn't mention video experiments
- Why unresolved: No analysis or results on method's performance in temporal contexts
- What evidence would resolve it: Experiments evaluating method on video-based HOI detection benchmarks or datasets

### Open Question 3
- How does proposed method handle rare or unseen HOI interactions during inference?
- Basis: Paper mentions improvements on rare interactions compared to GEN-VLKT but lacks detailed analysis of unseen interactions
- Why unresolved: No discussion of generalization to interactions not present in training data
- What evidence would resolve it: Experiments evaluating performance on held-out set of rare or unseen HOI interactions, or using zero-shot/few-shot learning techniques

## Limitations

- Claims about VLM superiority over CLIP for HOI detection lack direct empirical comparison in the HOI detection context
- Effectiveness of contrastive learning with VLM similarity scores for knowledge distillation depends on alignment between VLM pre-training and HOI semantics
- Computational overhead of using large VLMs like BLIP may limit practical deployment for real-time applications

## Confidence

- Medium: Claims about VLM superiority over CLIP for HOI detection due to object-centric localization
- Medium: Effectiveness of contrastive learning with VLM similarity scores for knowledge distillation
- Low: Impact of ungrammatical grounded sentences on VLM comprehension and overall performance

## Next Checks

1. Conduct ablation studies comparing different VLM architectures (BLIP, CLIP, LLaVA) for HOI detection to quantify impact of object-centric localization
2. Perform controlled experiments varying grounding sentence templates (including/excluding human subject, using different verb representations) to assess impact on VLM comprehension
3. Analyze computational overhead and memory requirements of proposed method compared to baseline HOI detection approaches across different hardware configurations