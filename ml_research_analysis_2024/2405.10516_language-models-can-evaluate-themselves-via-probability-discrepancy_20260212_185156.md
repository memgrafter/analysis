---
ver: rpa2
title: Language Models can Evaluate Themselves via Probability Discrepancy
arxiv_id: '2405.10516'
source_url: https://arxiv.org/abs/2405.10516
tags:
- arxiv
- probdiff
- response
- gpt-4
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProbDiff, a self-evaluation method for Large
  Language Models (LLMs) that leverages probability discrepancies between initial
  and revised responses. The core insight is that more capable LLMs exhibit a flatter
  probability distribution in their responses, resulting in smaller probability discrepancies
  when revising their answers.
---

# Language Models can Evaluate Themselves via Probability Discrepancy

## Quick Facts
- arXiv ID: 2405.10516
- Source URL: https://arxiv.org/abs/2405.10516
- Authors: Tingyu Xia; Bowen Yu; Yuan Wu; Yi Chang; Chang Zhou
- Reference count: 15
- One-line primary result: ProbDiff achieves evaluation results comparable to GPT-4-based methods while avoiding external models or additional training

## Executive Summary
This paper introduces ProbDiff, a self-evaluation method for Large Language Models (LLMs) that leverages probability discrepancies between initial and revised responses. The key insight is that more capable LLMs exhibit flatter probability distributions in their responses, resulting in smaller probability discrepancies when revising their answers. ProbDiff computes the log probability difference between the original response and its revised version to assess model performance, demonstrating results comparable to GPT-4-based evaluations without requiring external models or additional training.

## Method Summary
ProbDiff evaluates LLM performance by calculating the probability discrepancy between an initial response and a revised response to the same query. The method generates an initial response from the LLM, prompts the model to revise this response, then computes the log probability difference between the two outputs. A smaller discrepancy indicates higher model capability, as capable models have flatter probability distributions over their correct outputs. This approach enables self-evaluation without external models or additional training, making it cost-effective and scalable.

## Key Results
- ProbDiff achieves evaluation performance comparable to GPT-4-based methods on translation, summarization, and blog writing tasks
- The method shows highly consistent performance across different LLMs and tasks, providing a robust evaluation alternative
- Probability discrepancy effectively distinguishes between model capabilities without requiring additional training or external evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: More capable LLMs produce flatter probability distributions over their outputs.
- Mechanism: High confidence in correct answers leads to flatter log probability curves where probability mass is spread evenly across tokens.
- Core assumption: Variance of log probability distribution reflects model confidence and correctness.
- Evidence anchors: [abstract] "the more capable it is of providing an answer, the more confident it should be in its response"; [section] "the log probability curve of Yi-34B-Chat is flatter, indicating smaller variance, with nearly all response output probabilities being approximately equal."
- Break condition: Artificially flattened probabilities from temperature scaling or poorly calibrated probability distributions could invalidate this mechanism.

### Mechanism 2
- Claim: Samples from LLMs tend to reside in regions of negative curvature of the log probability function.
- Mechanism: Initial responses are likely at local maxima of the probability distribution, so revised responses typically have lower log probability as they move away from the maximum.
- Core assumption: Log probability function has negative curvature regions where small perturbations decrease probability.
- Evidence anchors: [abstract] "samples x from an LLM typically reside in regions of negative curvature of the log probability function"; [section] "DetectGPT proposes that samples x from an LLM typically reside in regions of negative curvature of the log probability function."
- Break condition: Multiple local maxima or very flat probability landscapes could invalidate the curvature assumption.

### Mechanism 3
- Claim: Probability discrepancy between original and revised responses correlates with model capability.
- Mechanism: Capable models show small probability discrepancies (both responses are likely correct), while less capable models show larger discrepancies (revised response is likely worse).
- Core assumption: Probability discrepancy reliably indicates relative quality of original response.
- Evidence anchors: [abstract] "A higher discrepancy for a given query between two LLMs indicates a relatively weaker capability"; [section] "A larger d(α, D) implies a higher variance in the probability distribution of the generation, indicating lower confidence in its responses."
- Break condition: Poorly behaved revision processes or already-optimal original responses could make discrepancies meaningless.

## Foundational Learning

- Concept: Log probability and its relationship to model confidence.
  - Why needed here: Understanding that flatter log probability curves indicate higher confidence is crucial for grasping why ProbDiff works.
  - Quick check question: If a model's log probability curve is very steep, does that indicate high or low confidence in its response?

- Concept: Local maxima in probability distributions.
  - Why needed here: The insight that LLM outputs tend to be at local maxima is key to understanding why revised responses have lower probability.
  - Quick check question: If a sample is at a local maximum of the probability distribution, what happens to the probability of nearby samples?

- Concept: Probability discrepancy as a comparative metric.
  - Why needed here: ProbDiff relies on comparing probability discrepancy of different models to rank their performance.
  - Quick check question: If model A has a higher probability discrepancy than model B on the same query, what does that imply about their relative capabilities?

## Architecture Onboarding

- Component map: Input Query → Initial response generation → Response refinement prompt → Revised response generation → Probability discrepancy computation
- Critical path: Query → Initial response → Revised response → Probability calculation → Discrepancy computation
- Design tradeoffs:
  - Temperature setting: Higher temperature in initial generation increases diversity but may artificially increase discrepancy; lower temperature in refinement focuses revisions
  - Number of refinement iterations: More iterations provide stable estimates but increase computational cost
  - Threshold selection: Threshold for determining confidence (δ) needs task and model-specific tuning
- Failure signatures:
  - High variance in probability discrepancy across queries: May indicate model uncertainty or ambiguous tasks
  - Very low probability discrepancy for all models: May indicate equally capable models or too-easy tasks
  - High probability discrepancy but poor performance: May indicate poorly calibrated probability distributions
- First 3 experiments:
  1. Validate core hypothesis by comparing probability discrepancy with known model capabilities on benchmark
  2. Ablation study on temperature settings to determine optimal values for generation and refinement
  3. Cross-task generalization testing on different task types (translation, summarization, dialogue) to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProbDiff perform on tasks requiring factual accuracy verification, such as question answering or fact-checking?
- Basis in paper: [inferred] The paper focuses on creative and stylistic tasks but doesn't discuss factual accuracy verification tasks.
- Why unresolved: Effectiveness on fact-checking or question answering tasks remains unexplored.
- What evidence would resolve it: Experiments on datasets like Natural Questions or FEVER comparing ProbDiff with human judgments on factual accuracy.

### Open Question 2
- Question: How does ProbDiff handle tasks with multiple correct answers or interpretations, such as creative writing or poetry generation?
- Basis in paper: [inferred] Xiaohongshu blog writing is mentioned but subjective interpretation handling isn't discussed.
- Why unresolved: The paper doesn't address how ProbDiff deals with inherently subjective tasks.
- What evidence would resolve it: Evaluation on creative writing/poetry tasks compared with human judgments on multiple valid interpretations.

### Open Question 3
- Question: How does ProbDiff's performance scale with extremely large LLMs like GPT-4 or beyond?
- Basis in paper: [explicit] The paper mentions consistent performance across various LLMs but doesn't discuss extremely large models.
- Why unresolved: Performance on models significantly larger than those tested (Qwen, Llama2, Yi) is unknown.
- What evidence would resolve it: Experiments evaluating ProbDiff on extremely large LLMs to understand scalability and effectiveness.

## Limitations

- The method's reliance on probability calibration assumptions may not hold across all LLM architectures and training regimes
- Effectiveness depends on the model's ability to meaningfully revise responses, which varies with task complexity and ambiguity
- Cross-lingual and multi-modal extensions remain untested, limiting generalizability to broader application domains

## Confidence

- **High Confidence**: The core mathematical framework for computing probability discrepancies is sound and well-established
- **Medium Confidence**: Comparative effectiveness against GPT-4-based evaluation is demonstrated on specific benchmarks but may not generalize to all scenarios
- **Low Confidence**: Universal applicability across different LLM architectures, training regimes, and domain-specific tasks requires further validation

## Next Checks

1. **Temperature Sensitivity Analysis**: Systematically evaluate how different temperature settings during initial generation and revision phases affect probability discrepancy scores across multiple tasks and model sizes.

2. **Cross-Architecture Validation**: Test ProbDiff across different LLM architectures (transformers, state-space models, etc.) to verify that probability distribution characteristics are consistent and meaningful for self-evaluation.

3. **Calibration Robustness**: Assess performance when models have known probability calibration issues, particularly in low-resource or specialized domain applications where calibration may be poor.