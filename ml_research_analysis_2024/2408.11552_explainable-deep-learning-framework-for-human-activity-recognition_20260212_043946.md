---
ver: rpa2
title: Explainable Deep Learning Framework for Human Activity Recognition
arxiv_id: '2408.11552'
source_url: https://arxiv.org/abs/2408.11552
tags:
- data
- framework
- these
- performance
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of interpretability in Human
  Activity Recognition (HAR) models, where traditional explanation methods struggle
  with the abstract nature of HAR data. The proposed framework introduces a model-agnostic
  approach using competitive data augmentation during both training and prediction
  phases.
---

# Explainable Deep Learning Framework for Human Activity Recognition

## Quick Facts
- arXiv ID: 2408.11552
- Source URL: https://arxiv.org/abs/2408.11552
- Reference count: 23
- Primary result: 4% average improvement in macro F1 scores across five HAR datasets compared to ActivityGAN

## Executive Summary
This paper addresses the interpretability challenge in Human Activity Recognition (HAR) models by proposing a model-agnostic framework that combines competitive data augmentation with Class-Aware Weight Regularization (CAWR). The framework enhances both model performance and explainability by applying consistent transformations during training and prediction phases. Experiments demonstrate an average 4% improvement in macro F1 scores across five benchmark datasets while maintaining minimal computational overhead.

## Method Summary
The framework employs competitive data augmentation during both training and prediction phases to improve model performance and interpretability. It uses CAWR to balance class influence during training, particularly addressing class imbalance issues. The approach is model-agnostic, working with CNN, LSTM, and Transformer architectures. During prediction, the framework applies transformations to input samples and uses voting mechanisms to aggregate predictions, generating explanations based on vote distributions.

## Key Results
- 4% average improvement in macro F1 scores compared to ActivityGAN across five benchmark datasets
- Up to 13% improvement observed on specific datasets
- Only 9.8% increase in prediction time
- Framework maintains model-agnostic compatibility with CNN, LSTM, and Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Competitive data augmentation preserves model robustness while enabling intuitive explanations.
- Mechanism: Identical or subset transformations during training and prediction maintain consistent data distributions, allowing prediction changes to be attributed to specific data regions.
- Core assumption: Prediction transformations must be a subset of training transformations to maintain stability.
- Evidence anchors: Abstract states preservation of model predictions facilitates explanation of decisions; section emphasizes transformation consistency requirement.
- Break condition: Performance degrades if prediction transformations exceed training transformations.

### Mechanism 2
- Claim: Class-Aware Weight Regularization (CAWR) improves generalization by balancing class influence.
- Mechanism: Adjusts learning rates or regularization weights based on class frequency to prevent dominant classes from overwhelming minorities.
- Core assumption: Class imbalance exists requiring regularization for fair representation.
- Evidence anchors: CAWR repeat period set to 50; incorporated with domain adaptation and bagging.
- Break condition: Unnecessary complexity if class distribution is already balanced.

### Mechanism 3
- Claim: Dual-phase augmentation increases robustness without excessive computational overhead.
- Mechanism: Training augmentation expands data diversity while prediction augmentation tests model sensitivity for explanations.
- Core assumption: Augmented samples per class must not exceed natural distribution to avoid skewing boundaries.
- Evidence anchors: Only 9.8% prediction time increase; augmented samples quantity must respect class averages.
- Break condition: Model confidence decreases if augmented samples exceed natural class distribution.

## Foundational Learning

- **Concept**: Data Augmentation
  - Why needed here: HAR datasets are often small and imbalanced; augmentation increases data diversity and model robustness.
  - Quick check question: What is the maximum number of augmented samples allowed per class to avoid skewing class boundaries?

- **Concept**: Model-Agnostic Frameworks
  - Why needed here: Ensures the framework can be applied to various HAR models without architectural constraints.
  - Quick check question: How does the framework maintain performance across different model architectures?

- **Concept**: Counterfactual Explanations
  - Why needed here: Provides intuitive explanations by showing how small changes in input affect model predictions.
  - Quick check question: How does the framework determine which data segments are critical for model decisions?

## Architecture Onboarding

- **Component map**: Data Augmentation Module -> CAWR Module -> Prediction Aggregation -> Base HAR Model

- **Critical path**:
  1. Apply random subset of transformations to training data
  2. Train base model with augmented data
  3. During prediction, apply transformations to input samples
  4. Aggregate predictions and generate explanations based on vote distribution

- **Design tradeoffs**:
  - More transformations increase explanation granularity but also prediction time
  - Strict adherence to transformation subsets limits exploration but maintains stability

- **Failure signatures**:
  - Performance drops if prediction transformations exceed training transformations
  - Explanations become unreliable if augmented samples per class exceed natural distribution

- **First 3 experiments**:
  1. Test framework with CNN model on DSADS dataset; measure F1 score and explanation quality
  2. Compare performance with and without CAWR on PAMAP2 dataset
  3. Evaluate prediction time increase with different numbers of transformations on HAPT dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the OptiHAR framework's performance scale with larger, more complex HAR datasets?
- Basis in paper: [explicit] The authors mention evaluating on five benchmark datasets but do not explore performance with larger datasets.
- Why unresolved: The paper focuses on relatively small benchmark datasets and does not test the framework's scalability.
- What evidence would resolve it: Testing OptiHAR on larger datasets with more subjects and activities to measure performance and resource usage.

### Open Question 2
- Question: What is the impact of different data augmentation transformations on model interpretability and performance?
- Basis in paper: [explicit] The paper uses three data augmentation methods (jitter, clip, segmentOut) but does not explore the impact of other transformations.
- Why unresolved: The study only explores a limited set of transformations, leaving the effects of other potential augmentations unexplored.
- What evidence would resolve it: Systematic testing of various data augmentation techniques to determine their individual and combined effects on model performance and interpretability.

### Open Question 3
- Question: How does the OptiHAR framework perform on HAR tasks with continuous or evolving activities?
- Basis in paper: [inferred] The paper focuses on static activity recognition and does not address continuous or evolving activities.
- Why unresolved: The framework is evaluated on datasets with discrete activities, not addressing scenarios where activities may transition or evolve over time.
- What evidence would resolve it: Testing OptiHAR on datasets or scenarios where activities are continuous or evolve, and measuring its ability to handle such cases.

### Open Question 4
- Question: What is the computational overhead of OptiHAR when deployed on resource-constrained devices?
- Basis in paper: [explicit] The paper mentions a 9.8% increase in prediction time but does not explore deployment on resource-constrained devices.
- Why unresolved: The study focuses on the computational overhead in a general setting but does not address the specific challenges of deploying on resource-constrained devices.
- What evidence would resolve it: Testing OptiHAR on resource-constrained devices and measuring its impact on battery life, processing speed, and memory usage.

## Limitations

- Critical dependency on transformation subset constraint; performance degrades if prediction augmentations exceed training augmentations
- CAWR implementation details are not fully specified, making it difficult to assess contribution to performance improvements
- "Intuitive explanations" claim lacks quantitative validation through user studies or explanation quality metrics

## Confidence

- **High confidence**: 4% average F1 improvement over ActivityGAN is well-supported by experimental results across five datasets and three model architectures
- **Medium confidence**: 9.8% prediction time increase claim requires validation as it depends on implementation details not provided
- **Low confidence**: "Intuitive explanations" assertion lacks quantitative validation

## Next Checks

1. Verify the transformation subset constraint by testing model performance when prediction augmentations exceed training augmentations
2. Implement CAWR independently to confirm whether reported performance improvements are reproducible without access to original code
3. Conduct ablation studies to isolate contribution of competitive augmentation versus CAWR to observed F1 score improvements