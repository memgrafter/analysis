---
ver: rpa2
title: Deciphering the Impact of Pretraining Data on Large Language Models through
  Machine Unlearning
arxiv_id: '2402.11537'
source_url: https://arxiv.org/abs/2402.11537
tags:
- uni00000013
- uni00000011
- uni00000018
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using machine unlearning to systematically
  quantify the contribution of different pretraining corpora to the capabilities of
  large language models. The method selectively removes information from targeted
  datasets via gradient ascent while preserving performance on non-targeted domains
  through retraining regularization.
---

# Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning

## Quick Facts
- arXiv ID: 2402.11537
- Source URL: https://arxiv.org/abs/2402.11537
- Authors: Yang Zhao; Li Du; Xiao Ding; Kai Xiong; Zhouhao Sun; Jun Shi; Ting Liu; Bing Qin
- Reference count: 27
- This paper proposes using machine unlearning to systematically quantify the contribution of different pretraining corpora to the capabilities of large language models.

## Executive Summary
This paper introduces a novel method called GRACE to systematically quantify how different pretraining corpora contribute to large language model capabilities through selective machine unlearning. The approach uses gradient ascent to remove information from targeted datasets while preserving performance on non-targeted domains through retraining regularization. By applying this method to 48 datasets across 5 categories and measuring impacts on 31 benchmarks spanning 9 capabilities, the study reveals that certain corpora like Books and Shell have broad influence on multiple model abilities, while others show complementary, orthogonal, or correlated relationships in their impact on model performance.

## Method Summary
The paper proposes GRACE (Gradient Ascent with Retraining for unlearning via Correlation Analysis), a customized machine unlearning method that selectively removes information from targeted pretraining datasets. The method alternates between gradient ascent on targeted corpora to unlearn specific information and retraining regularization on non-targeted corpora to prevent unintended performance degradation. An innovative randomized text-based endpoint determines when unlearning is complete by comparing perplexity on the original and randomized versions of the target corpus. The approach is validated using subsets of the RedPajama dataset, a replication of Llama's pretraining corpus.

## Key Results
- Books and Shell corpora show broad influence across multiple model capabilities
- Datasets exhibit three types of relationships: complementary, orthogonal, and correlated
- Machine unlearning can systematically quantify pretraining data contributions to LLM capabilities
- The GRACE method effectively isolates the impact of individual datasets while preserving overall model performance

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent with Retraining for Selective Unlearning
The GRACE method selectively removes information from targeted datasets while preserving performance on non-targeted domains through alternating gradient ascent and retraining. Gradient ascent on the targeted corpus unlearns specific information, while retraining regularization on non-targeted corpus prevents unintended performance degradation. This works because gradient ascent can effectively erase specific knowledge from LLMs, as demonstrated by prior research.

### Mechanism 2: Randomized Text-Based Endpoint for Unlearning Process
A randomized text-based method effectively determines when the unlearning process should stop by measuring when model perplexity on the targeted corpus equals its perplexity on a randomized version. This works because randomized text shares similar lexical distribution with the original corpus but lacks specific knowledge, making it a suitable proxy for measuring complete unlearning.

### Mechanism 3: Correlation-Based Analysis of Data Impact
Analyzing correlation between model performance degradation and unlearning different datasets reveals complementary, orthogonal, and correlated relationships among corpora. This works by calculating Pearson correlation coefficients between performance degradation ratios of different datasets across multiple model capabilities, then applying hierarchical clustering to categorize datasets.

## Foundational Learning

- **Concept: Machine Unlearning**
  - Why needed here: Understanding machine unlearning principles is crucial for grasping how GRACE selectively removes information from LLMs.
  - Quick check question: How does machine unlearning differ from traditional model retraining, and what are its key advantages in analyzing pretraining data impact?

- **Concept: Gradient Ascent**
  - Why needed here: Gradient ascent is the core mechanism for unlearning in GRACE, essential for evaluating its effectiveness.
  - Quick check question: What are the potential risks of using gradient ascent for unlearning, and how does retraining regularization in GRACE mitigate these risks?

- **Concept: Perplexity as a Performance Metric**
  - Why needed here: Perplexity measures model performance on different datasets and determines the endpoint of unlearning.
  - Quick check question: How does perplexity relate to the model's understanding of data, and why is it suitable for evaluating unlearning effectiveness?

## Architecture Onboarding

- **Component map**:
  Data Preprocessing -> Unlearning Module -> Endpoint Determination -> Correlation Analysis -> Model Evaluation

- **Critical path**:
  1. Select target and non-targeted corpora
  2. Apply gradient ascent to unlearn the target corpus
  3. Apply gradient descent to retrain on the non-targeted corpus if perplexity increases
  4. Determine the endpoint of unlearning using the randomized text-based method
  5. Evaluate the impact on model capabilities
  6. Calculate correlation coefficients and perform hierarchical clustering

- **Design tradeoffs**:
  - Computational cost vs. precision: More iterations increase precision but also computational cost
  - Granularity of datasets: Finer granularity allows for more precise analysis but increases correlation analysis complexity
  - Choice of performance metrics: Different metrics capture different aspects of model capabilities

- **Failure signatures**:
  - Unintended performance degradation on non-targeted domains indicates insufficient retraining regularization
  - Inability to unlearn targeted information suggests gradient ascent is ineffective or endpoint determination is inaccurate
  - Misleading correlation patterns could be due to confounding factors or inappropriate performance metrics

- **First 3 experiments**:
  1. Unlearn a small subset of the Wikipedia corpus (e.g., the mathematics section) and evaluate impact on mathematics-related and unrelated tasks
  2. Unlearn a specific programming language (e.g., Python) and assess impact on code generation and understanding in that language and others
  3. Unlearn a set of related datasets (e.g., different subsets of ArXiv) and analyze correlation between their performance degradation ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the joint impacts of different pretraining corpora vary across model scales (e.g., 7B, 13B, 70B)?
- Basis in paper: [explicit] The paper uses Llama-2-7B as the analysis model and suggests that scaling laws allow inferring impacts on larger models, but does not empirically validate this across different model sizes.
- Why unresolved: The study only tests on Llama-2-7B, so the joint impact patterns observed may not hold for larger or smaller models.
- What evidence would resolve it: Experiments applying the same unlearning analysis across multiple model scales (7B, 13B, 34B, 70B) to compare joint impact patterns and validate if they scale consistently.

### Open Question 2
- Question: What is the precise mechanism by which code corpora influence non-code capabilities like textual understanding and reasoning?
- Basis in paper: [inferred] The paper observes that programming languages significantly impact textual understanding and reasoning, but does not explain the underlying mechanism.
- Why unresolved: While the paper demonstrates correlations between code corpora and non-code capabilities, it does not investigate the causal mechanisms or pathways through which code knowledge transfers to other domains.
- What evidence would resolve it: Controlled experiments ablating specific code features (e.g., variable naming, control flow patterns, comments) to isolate which aspects of code corpora drive improvements in non-code tasks.

### Open Question 3
- Question: How do the complementary and antagonistic relationships between corpora evolve during different stages of pretraining?
- Basis in paper: [explicit] The paper identifies complementary and antagonistic corpus relationships but does not analyze how these relationships change throughout pretraining.
- Why unresolved: The study only examines the model after complete pretraining. The dynamic evolution of corpus relationships during training could reveal critical timing dependencies for corpus inclusion.
- What evidence would resolve it: Tracking corpus influence patterns and their relationships at multiple checkpoints during pretraining to identify when and how complementary/competitive dynamics emerge.

## Limitations
- The randomized text endpoint determination assumes perplexity alone adequately measures complete unlearning, potentially overlooking subtle residual knowledge
- Computational intensity of the unlearning process (estimated at ~3 days per dataset on A100 GPUs) limits practical scalability
- Correlation analysis assumes linear relationships between datasets' impacts, which may oversimplify complex interdependencies

## Confidence
**High Confidence**: The methodological framework for systematic unlearning and correlation analysis is well-established. The identification of broad-impact corpora like Books and Shell is supported by consistent patterns across multiple benchmarks and model capabilities.

**Medium Confidence**: The claim about complementary versus correlated relationships between datasets is robust but requires further validation with larger-scale experiments. The effectiveness of the randomized text endpoint method is theoretically sound but needs empirical verification across different model architectures.

**Low Confidence**: The generalizability of findings to non-English corpora and specialized domains (medical, legal) remains untested. The impact of unlearning order (sequential vs. simultaneous) on final results is not explored.

## Next Checks
1. **Cross-Architecture Validation**: Apply the GRACE method to different LLM architectures (e.g., decoder-only vs. encoder-decoder) to verify if corpus impact patterns remain consistent across model families.

2. **Fine-Grained Temporal Analysis**: Track the evolution of model capabilities during the unlearning process at finer time intervals to identify critical thresholds where information degradation becomes irreversible.

3. **Alternative Unlearning Methods**: Compare GRACE's effectiveness against other unlearning approaches (e.g., direct weight modification, data augmentation) on the same corpus sets to establish relative performance and identify method-specific artifacts.