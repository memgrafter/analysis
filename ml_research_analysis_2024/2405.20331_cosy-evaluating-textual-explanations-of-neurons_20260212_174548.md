---
ver: rpa2
title: 'CoSy: Evaluating Textual Explanations of Neurons'
arxiv_id: '2405.20331'
source_url: https://arxiv.org/abs/2405.20331
tags:
- images
- explanations
- neuron
- concept
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of quantitative evaluation methods
  for textual explanations of neural network neurons. The authors introduce COSY,
  a framework that uses generative models to synthesize images from textual explanations,
  then measures how strongly a neuron responds to these synthetic images compared
  to control data.
---

# CoSy: Evaluating Textual Explanations of Neurons

## Quick Facts
- arXiv ID: 2405.20331
- Source URL: https://arxiv.org/abs/2405.20331
- Reference count: 40
- Primary result: Introduces COSY, a framework for quantitatively evaluating textual explanations of neural network neurons using generative models and activation comparisons.

## Executive Summary
This paper addresses the critical gap in evaluating textual explanations of neural network neurons by introducing COSY (Concept Synthesis), a framework that leverages generative models to synthesize images from textual explanations and measures neuron responses to these synthetic images compared to control data. Through extensive experiments across multiple architectures and explanation methods, the authors validate COSY's reliability and benchmark existing techniques, finding that methods like INVERT and CLIP-Dissect generally provide higher quality explanations than MILAN and FALCON, particularly for lower network layers. The framework enables architecture-agnostic evaluation of explanation methods, revealing significant variability in method quality and highlighting the challenges of explaining low-level concepts.

## Method Summary
COSY uses text-to-image models (SDXL, Stable Cascade) to generate synthetic images from textual neuron explanations, then compares the target neuron's activation on these synthetic images versus control dataset images using AUC and MAD metrics. The framework requires only image inputs and neuron activations, making it applicable to any CNN architecture. Synthetic images are generated using prompts like "realistic photo of a close up of [concept]" and similarity to natural images is verified using CLIP embeddings. The method evaluates explanation quality by measuring how distinctively a neuron responds to its concept-related synthetic images compared to unrelated control images.

## Key Results
- COSY framework successfully discriminates between high-quality and low-quality explanations, with INVERT and CLIP-Dissect showing consistently higher scores than MILAN and FALCON
- Explanation quality varies significantly across network layers, with higher layers generally producing better explanations than lower layers
- COSY demonstrates architecture-agnostic evaluation capability across ResNet, DenseNet, GoogLeNet, ViT, and DINO architectures
- The framework reliably evaluates explanations across diverse concept types including objects, scenes, and textures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models can produce images that accurately represent textual neuron explanations.
- Mechanism: Text-to-image models synthesize images from textual prompts describing neuron concepts, which are then used to probe neuron activation.
- Core assumption: Generative models can reliably translate abstract textual descriptions into visually coherent images.
- Evidence anchors: [abstract] "Our approach builds on recent advancements in Generative AI, which enable the generation of synthetic images that align with provided neuron explanations."
- Break condition: If generative models cannot produce meaningful images for abstract or low-level concepts.

### Mechanism 2
- Claim: Neurons respond differently to synthetic concept images versus control images, enabling quantitative evaluation.
- Mechanism: Comparing neuron activation distributions between synthetic concept images and control dataset images measures explanation quality.
- Core assumption: Neuron activation patterns are distinct enough between concept-related and unrelated images for meaningful discrimination.
- Evidence anchors: [abstract] "By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation."
- Break condition: If neurons fire similarly for both concept and non-concept images.

### Mechanism 3
- Claim: COSY enables architecture-agnostic evaluation of neuron explanation methods.
- Mechanism: Framework uses only image inputs and neuron activations, applicable to any CNN architecture.
- Core assumption: Neuron explanations are meaningful across different architectures if they consistently activate on related images.
- Evidence anchors: [abstract] "We introduce COSY (Concept Synthesis), a novel, architecture-agnostic framework for evaluating textual explanations of latent neurons."
- Break condition: If architectural differences cause explanation validity to vary significantly.

## Foundational Learning

- Concept: Intersection over Union (IoU) for concept-neuron alignment
  - Why needed here: Several referenced explanation methods use IoU to measure similarity between segmentation masks and neuron activations
  - Quick check question: If a neuron activates on 80 pixels of a 100-pixel object mask, what is the IoU assuming no false positives?

- Concept: Area Under the ROC Curve (AUC) as a ranking metric
  - Why needed here: COSY uses AUC to measure the neuron's ability to rank synthetic concept images higher than control images
  - Quick check question: If synthetic images have activations [0.9, 0.8, 0.7] and control images have [0.4, 0.3, 0.2], what is the approximate AUC?

- Concept: CLIP embedding space for image-text similarity
  - Why needed here: CLIP embeddings measure visual similarity between synthetic and natural images
  - Quick check question: What does a Cosine Similarity of 0.9 between CLIP embeddings indicate about two images' semantic similarity?

## Architecture Onboarding

- Component map:
  - Text-to-image model (SDXL) → Image generator
  - CLIP model → Image-text similarity evaluator
  - Control dataset → Baseline activation source
  - Target neuron → Evaluation subject
  - Evaluation metrics (AUC, MAD) → Quality scorers

- Critical path:
  1. Generate synthetic images from textual explanations using text-to-image model
  2. Collect neuron activations on both synthetic and control images
  3. Compute evaluation metrics (AUC, MAD) comparing activation distributions
  4. Interpret results to assess explanation quality

- Design tradeoffs:
  - Prompt specificity vs. generation success: Detailed prompts improve image relevance but may reduce generation success
  - Control dataset size vs. computational cost: Larger control sets improve baseline stability but increase inference time
  - Metric choice: AUC is robust to outliers but less interpretable than MAD

- Failure signatures:
  - High MAD but low AUC: Neuron activates strongly on concept images but fails to discriminate from controls
  - Low values for both metrics: Explanation is poor or generative model fails
  - Extremely high similarity between synthetic and control activations: Potential mode collapse in generative model

- First 3 experiments:
  1. Verify generative model produces visually coherent images by checking CLIP similarity between synthetic and natural images for 5-10 known concepts
  2. Test neuron activation consistency by running synthetic images through target model with different seeds and checking AUC variance
  3. Validate framework by evaluating ground truth explanations (class labels) and confirming high scores while random explanations score low

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of abstract concepts pose the greatest challenge for current text-to-image models in generating representative images for neuron explanations?
- Basis in paper: [inferred] The paper notes that MILAN often generates highly abstract explanations like "white areas" or "nothing," which are challenging for text-to-image models to generate accurately.
- Why unresolved: The paper identifies abstract concepts as problematic but does not specify which types of abstraction are most difficult for current models to represent visually.
- What evidence would resolve it: Systematic testing of various types of abstract concepts (color-based, shape-based, conceptual) with different text-to-image models to identify which types consistently fail to produce recognizable images.

### Open Question 2
- Question: How does the quality of neuron explanations vary across different network architectures beyond those tested?
- Basis in paper: [inferred] The paper benchmarks explanation methods across multiple architectures but notes this is just a subset of possible models.
- Why unresolved: The study focuses on a limited set of architectures and datasets, leaving open whether findings about explanation quality and layer-wise performance generalize to other network types or domains.
- What evidence would resolve it: Extensive testing of explanation methods across diverse architectures and datasets to establish generalizable patterns.

### Open Question 3
- Question: What are the most effective complementary evaluation metrics that could be integrated with COSY to assess explanation plausibility from a human perspective?
- Basis in paper: [explicit] The conclusion section mentions that future work should include "additional, complementary definitions of explanation quality that extend our precise definition of AUC and MAD, e.g., that involve humans to assess plausibility."
- Why unresolved: While COSY provides quantitative metrics, the paper acknowledges the need for human-based plausibility assessment but does not specify what metrics or methodologies would be most effective.
- What evidence would resolve it: Development and validation of human-centric evaluation protocols that complement COSY's quantitative metrics.

## Limitations
- Generative model reliability: Framework validity hinges on SDXL/Stable Cascade producing concept-representative images, but no systematic evaluation of generation quality is provided.
- Neuron activation stability: No explicit analysis of activation variance across multiple runs with the same synthetic images, raising concerns about measurement reliability.
- Architecture independence assumption: Claim of architecture-agnostic evaluation lacks empirical validation across diverse architectural families.

## Confidence
- Mechanism 1 (Generative model alignment): Medium confidence - supported by framework design but lacking independent validation of generation quality
- Mechanism 2 (Activation discrimination): Medium confidence - logic is sound but no statistical tests confirm activation distributions are meaningfully different
- Mechanism 3 (Architecture agnosticism): Low confidence - claimed but not empirically validated across diverse architectures

## Next Checks
1. Perform ablation study measuring how AUC scores change when using control images with known visual similarity to synthetic images versus random control images
2. Evaluate generative model outputs using human judgment studies where annotators rate how well synthetic images represent the textual explanations
3. Test framework reproducibility by running identical experiments across different hardware/software environments and measuring score variance