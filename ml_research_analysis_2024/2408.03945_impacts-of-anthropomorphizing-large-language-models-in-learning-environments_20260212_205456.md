---
ver: rpa2
title: Impacts of Anthropomorphizing Large Language Models in Learning Environments
arxiv_id: '2408.03945'
source_url: https://arxiv.org/abs/2408.03945
tags:
- learning
- chatbots
- educational
- llm-based
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the educational implications of anthropomorphizing
  Large Language Models (LLMs) in learning environments. The authors investigate whether
  and how anthropomorphization of LLM-based chatbots can evoke intense emotions like
  irritation and strangeness, and whether these emotions influence learning outcomes.
---

# Impacts of Anthropomorphizing Large Language Models in Learning Environments

## Quick Facts
- arXiv ID: 2408.03945
- Source URL: https://arxiv.org/abs/2408.03945
- Authors: Kristina Schaaff; Marc-André Heidelmann
- Reference count: 33
- This paper proposes a methodology to investigate whether anthropomorphization of LLM-based chatbots evokes intense emotions like irritation and strangeness, and how these emotions influence learning outcomes.

## Executive Summary
This paper addresses the educational implications of anthropomorphizing Large Language Models (LLMs) in learning environments. The authors propose investigating whether and how anthropomorphization of LLM-based chatbots can evoke intense emotions like irritation and strangeness, and whether these emotions influence learning outcomes. The study plans to compare two learning systems—one with anthropomorphism factors and one without—to be tested with students through decision-making tasks, performance tracking, and emotional state questionnaires. The goal is to understand the balance between making AI relatable and maintaining realistic expectations of its capabilities, ultimately informing the design of more effective educational technologies.

## Method Summary
The paper proposes a comparative study design where two learning systems will be developed—one incorporating anthropomorphism factors (cognitive intelligence, emotional intelligence, personality, personalization, identity, physical appearance, voice, movement, gestures, and facial expressions) and one without these factors. Students from IU International University of Applied Sciences will be recruited to participate in decision-making tasks using both systems. Performance metrics, decision-making times, and emotional states will be tracked and evaluated through questionnaires. The study aims to measure the presence of intense emotions (irritation and strangeness) and their influence on learning outcomes.

## Key Results
- The paper identifies that anthropomorphization of LLM-based chatbots can trigger strong emotional responses such as irritation and strangeness when expectations are not met
- Emotional responses from anthropomorphized LLMs can potentially disrupt existing cognitive frameworks and trigger transformational learning processes
- The study proposes that anthropomorphic design elements (cognitive/emotional intelligence, personality, personalization) increase learners' tendency to anthropomorphize AI, influencing emotional impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anthropomorphization of LLMs in learning environments can trigger strong emotional responses such as irritation and strangeness.
- Mechanism: When learners attribute human-like characteristics to LLM-based chatbots, their expectations shift toward human-like interaction quality. If the chatbot fails to meet these expectations, it can cause frustration and a sense of "uncanniness," triggering emotional responses tied to transformational learning theories.
- Core assumption: Learners automatically apply human interaction norms to chatbots when they appear human-like (media equation effect).
- Evidence anchors:
  - [abstract] "investigate whether and how anthropomorphization of LLM-based chatbots can evoke intense emotions like irritation and strangeness"
  - [section II] "According to the media equation [4], people tend to respond to media in the same way as they would respond to another person."
- Break condition: Learners receive training or cues explicitly framing the chatbot as a non-human tool rather than a human-like entity.

### Mechanism 2
- Claim: Emotional responses from anthropomorphized LLMs can disrupt existing cognitive frameworks and trigger transformational learning.
- Mechanism: Strong emotions like irritation and strangeness can act as "crisis experiences" that destabilize learners' current understanding, forcing them to reconstruct knowledge—a key feature of transformational education.
- Core assumption: Transformational learning requires emotional disruption to occur (per Koller's theory cited in the paper).
- Evidence anchors:
  - [section II] "education is emotional maturity" [7] and "negative emotional experiences such as irritation, limit experiences, or feelings of strangeness are given great relevance in qualitative educational research"
  - [section II] "crisis experiences like irritation and strangeness [8]... intense emotions are important for learning in general"
- Break condition: The emotional intensity is too low to disrupt the learner's current framework, or the learner disengages entirely.

### Mechanism 3
- Claim: Anthropomorphic design elements (cognitive/emotional intelligence, personality, personalization) increase learners' tendency to anthropomorphize AI, influencing emotional impact.
- Mechanism: The presence of human-like traits (voice, movement, gestures, conversational style) enhances the learner's attribution of human characteristics, thereby intensifying emotional reactions when expectations are violated.
- Core assumption: The more anthropomorphic cues present, the stronger the learner's tendency to anthropomorphize (based on Kim & Im's factors cited).
- Evidence anchors:
  - [section II] "Looking at the agent, several factors can contribute to anthropomorphization. Cognitive intelligence... Emotional intelligence... Characteristics such as personality..."
  - [section II] "Factors of anthropomorphism in learning environments" figure showing agent, learner, and environmental factors
- Break condition: Design minimizes anthropomorphic cues, or learners have prior experience with AI that moderates their attribution tendency.

## Foundational Learning

- Concept: Transformational Learning Theory
  - Why needed here: The study's framework assumes that strong emotions can trigger transformational learning processes. Understanding this theory is critical to interpreting the research's goals and methodology.
  - Quick check question: What role do "crisis experiences" like irritation and strangeness play in transformational learning according to Koller?

- Concept: Media Equation
  - Why needed here: This theory underpins the mechanism by which learners respond to chatbots as if they were human, which is central to the study's hypothesis.
  - Quick check question: According to Reeves and Nass, how do people tend to respond to media compared to real people?

- Concept: Anthropomorphism Factors
  - Why needed here: Identifying which design elements contribute to anthropomorphism helps operationalize the study's comparison between anthropomorphic and non-anthropomorphic systems.
  - Quick check question: Name three factors (from cognitive, emotional, or identity domains) that contribute to the anthropomorphization of LLM-based chatbots.

## Architecture Onboarding

- Component map:
  - Learning System A: Anthropomorphic chatbot (includes human-like traits, emotional intelligence, personalization, identity cues)
  - Learning System B: Non-anthropomorphic chatbot (functional, minimal human-like traits)
  - Decision-making task module (performance and time tracking)
  - Emotional state questionnaire module (post-task self-report)
  - Data integration pipeline (links performance, timing, and emotional data)

- Critical path:
  1. Recruit student cohort
  2. Randomly assign to System A or B
  3. Administer decision-making task
  4. Collect performance metrics and emotional responses
  5. Analyze correlation between anthropomorphism level and emotional/learning outcomes

- Design tradeoffs:
  - Anthropomorphic vs. non-anthropomorphic: Balancing engagement and relatability against unrealistic expectations and emotional discomfort
  - Questionnaire design: Ensuring emotional scales capture subtle differences in irritation/strangeness without introducing bias
  - Task complexity: Keeping decision-making task challenging enough to elicit emotional responses but not so hard as to cause frustration unrelated to anthropomorphism

- Failure signatures:
  - No significant difference in emotional responses between systems (may indicate anthropomorphism factors are not strong enough)
  - High dropout rates in anthropomorphic condition (may indicate excessive emotional discomfort)
  - Inconsistent emotional reporting (may indicate questionnaire design issues)

- First 3 experiments:
  1. Pilot test decision-making task with a small group to calibrate difficulty and emotional response elicitation
  2. Conduct A/B test with minimal anthropomorphism difference to establish baseline sensitivity
  3. Full cohort study comparing strongly anthropomorphic vs. minimally anthropomorphic systems with integrated emotional and performance tracking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-based chatbots in anthropomorphized forms induce stronger negative emotions (irritation, strangeness) compared to non-anthropomorphized versions?
- Basis in paper: [explicit] The authors state that negative emotional experiences like irritation and strangeness are relevant in educational research and pose the question whether anthropomorphized chatbots can induce such emotions.
- Why unresolved: This is a core research question the authors plan to investigate but have not yet conducted the study.
- What evidence would resolve it: Results from comparative studies measuring emotional responses (irritation, strangeness) between students using anthropomorphized vs. non-anthropomorphized LLM-based chatbots during learning tasks.

### Open Question 2
- Question: Does the degree of anthropomorphism in LLM-based chatbots correlate with learning effectiveness across different subject matters?
- Basis in paper: [explicit] The authors mention that "the impact varies depending on the subject matter, the design of the agent's responses, and the learner's profile."
- Why unresolved: While some studies suggest personalization can enhance learning, the paper acknowledges variability and calls for more research on optimization.
- What evidence would resolve it: Comparative studies measuring learning outcomes across various subjects using chatbots with different levels of anthropomorphism, controlling for learner profiles.

### Open Question 3
- Question: How do cultural differences moderate the emotional and learning impacts of anthropomorphized LLM-based chatbots?
- Basis in paper: [explicit] The authors note that "cultural differences can significantly influence how AI systems are perceived and anthropomorphized."
- Why unresolved: The paper identifies this as a relevant macro-environmental factor but does not provide empirical evidence of its effects.
- What evidence would resolve it: Cross-cultural studies comparing emotional responses and learning outcomes when using anthropomorphized chatbots across different cultural contexts.

## Limitations

- The proposed study design relies heavily on self-reported emotional states, which introduces significant measurement uncertainty and may be subject to social desirability bias or inconsistent interpretation across participants.
- The study's ecological validity is limited by its artificial laboratory setting, which may not generalize to naturalistic learning environments where emotional engagement occurs over extended periods with varied contexts and stakes.
- The causal mechanisms proposed (media equation effects, transformational learning through emotional disruption) are based on theoretical frameworks that may not directly translate to LLM interactions and require empirical validation specific to LLM contexts.

## Confidence

**High Confidence**: The core hypothesis that anthropomorphization can trigger emotional responses is well-supported by existing media psychology literature. The proposed methodology for comparing anthropomorphic versus non-anthropomorphic systems is methodologically sound.

**Medium Confidence**: The claim that these emotional responses will significantly influence learning outcomes depends on untested assumptions about the relationship between irritation/strangeness and transformational learning in AI contexts. The specific emotional impact may vary considerably based on learner characteristics and task design.

**Low Confidence**: The precise operationalization of anthropomorphism factors and their individual contributions to emotional responses remains unclear. The study does not address how different types of anthropomorphic cues (voice vs. personality vs. physical appearance) might have varying effects.

## Next Checks

1. **Instrument Validation**: Conduct a pilot study to validate the emotional state questionnaires specifically for measuring irritation and strangeness in AI interaction contexts. Test for internal consistency, test-retest reliability, and convergent validity with behavioral indicators.

2. **Anthropomorphism Factor Isolation**: Design a controlled experiment that systematically varies individual anthropomorphism factors (e.g., voice alone, personality alone) to determine which elements most strongly predict emotional responses and learning outcomes, rather than testing all factors simultaneously.

3. **Longitudinal Engagement Assessment**: Implement a follow-up study tracking the same participants over multiple sessions to assess whether initial emotional responses (irritation, strangeness) diminish with familiarity, and how this affects long-term learning outcomes and system acceptance.