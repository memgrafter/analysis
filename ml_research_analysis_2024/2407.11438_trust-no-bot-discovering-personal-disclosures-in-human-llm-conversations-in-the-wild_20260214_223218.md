---
ver: rpa2
title: 'Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations
  in the Wild'
arxiv_id: '2407.11438'
source_url: https://arxiv.org/abs/2407.11438
tags:
- information
- sensitive
- user
- users
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes privacy risks in user-chatbot interactions
  by examining personally identifiable information (PII) and sensitive disclosures
  in real conversations with large language models. The authors develop taxonomies
  of tasks and sensitive topics through manual annotation and automatic classification
  of 5,000 conversations from the WildChat dataset.
---

# Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild

## Quick Facts
- **arXiv ID:** 2407.11438
- **Source URL:** https://arxiv.org/abs/2407.11438
- **Reference count:** 40
- **Primary result:** Analysis of PII and sensitive disclosures in 5,000 real conversations with LLMs, revealing high prevalence of personal information sharing

## Executive Summary
This paper investigates privacy risks in user-chatbot interactions by analyzing personally identifiable information (PII) and sensitive disclosures in real conversations with large language models. The authors develop taxonomies of tasks and sensitive topics through manual annotation and automatic classification of conversations from the WildChat dataset. They find that 70% of queries contain detectable PII, with names and organizations being most common, and that PII detection alone is insufficient to capture sensitive topics like sexual preferences or drug use. Task analysis reveals high disclosure rates in unexpected contexts, such as 48% of translation queries containing PII.

## Method Summary
The authors analyze 5,000 conversations from the WildChat dataset to study PII and sensitive disclosures in human-LLM interactions. They develop taxonomies through manual annotation of 100 conversations, then apply automatic classification using regular expressions and dictionary-based methods to identify PII and sensitive topics. The study categorizes conversations by task type and measures disclosure rates across different contexts, providing insights into privacy risks in real-world LLM usage.

## Key Results
- 70% of queries contain detectable PII, with names and organizations being most common
- PII detection alone misses sensitive topics like sexual preferences and drug use
- 48% of translation queries contain PII, revealing high disclosure rates in unexpected contexts

## Why This Works (Mechanism)
This study works by combining manual annotation with automated classification to systematically analyze privacy risks in real-world LLM conversations. The approach leverages human expertise to develop taxonomies while scaling up analysis through automated methods, allowing researchers to quantify disclosure patterns across thousands of conversations.

## Foundational Learning
- **PII Detection Methods:** Why needed - to identify personal information in text; Quick check - test regex patterns on labeled examples
- **Sensitive Topic Classification:** Why needed - to capture privacy risks beyond basic PII; Quick check - validate categories against privacy frameworks
- **Task Analysis Framework:** Why needed - to understand disclosure patterns in different interaction contexts; Quick check - compare task distributions across datasets

## Architecture Onboarding

**Component Map:** WildChat Dataset -> Manual Annotation -> Taxonomy Development -> Automatic Classification -> Task Analysis -> Privacy Risk Assessment

**Critical Path:** Manual annotation of 100 conversations → Taxonomy development → Automatic classification pipeline → Disclosure rate calculation → Privacy risk analysis

**Design Tradeoffs:** The study balances manual annotation accuracy with automated scalability, choosing to develop robust taxonomies through human annotation while applying automated methods to analyze the full dataset. This tradeoff enables comprehensive analysis while maintaining reasonable resource requirements.

**Failure Signatures:** The approach may miss context-dependent PII or misclassify sensitive topics due to the limitations of regex and dictionary-based methods. Annotation bias from limited annotator diversity could skew taxonomy development.

**First 3 Experiments:**
1. Replicate analysis on a small subset with independent annotators to validate taxonomy reliability
2. Test classification pipeline performance on a held-out validation set
3. Conduct error analysis on misclassified examples to refine detection methods

## Open Questions the Paper Calls Out
None

## Limitations
- Manual annotation involved only two annotators and focused on a small subset of conversations
- Automatic classification relies on regex and dictionary methods that may miss context-dependent PII
- WildChat dataset may not represent all user populations or interaction patterns across different LLM platforms

## Confidence
- **High confidence** in the prevalence of detectable PII in conversations (70% detection rate)
- **Medium confidence** in sensitivity classification results due to heuristic categorization approach
- **Medium confidence** in task-specific disclosure patterns due to accuracy dependencies

## Next Checks
1. Replicate the analysis on multiple LLM datasets from different providers to assess generalizability across platforms
2. Conduct user studies to validate whether detected PII and sensitive topics align with actual privacy concerns and disclosure intentions
3. Evaluate the performance of the classification pipeline against a larger, independently annotated test set to measure precision and recall of sensitive topic detection