---
ver: rpa2
title: A New View on Planning in Online Reinforcement Learning
arxiv_id: '2406.01562'
source_url: https://arxiv.org/abs/2406.01562
tags:
- learning
- planning
- state
- value
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses challenges in background planning with learned
  models, which often generate invalid states and are computationally expensive. It
  introduces Goal-Space Planning (GSP), a new framework that constrains planning to
  a set of abstract subgoals and learns local, subgoal-conditioned models.
---

# A New View on Planning in Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.01562
- Source URL: https://arxiv.org/abs/2406.01562
- Authors: Kevin Roice; Parham Mohammad Panahi; Scott M. Jordan; Adam White; Martha White
- Reference count: 18
- Primary result: Goal-Space Planning (GSP) significantly accelerates learning in FourRooms, PinBall, and GridBall domains compared to standard Sarsa and DDQN baselines.

## Executive Summary
This paper addresses the challenge of background planning with learned models, which often generate invalid states and are computationally expensive. The authors introduce Goal-Space Planning (GSP), a framework that constrains planning to abstract subgoals and learns local, subgoal-conditioned models. GSP avoids learning full transition dynamics, making it computationally efficient and able to incorporate temporal abstraction. The method propagates value quickly from an abstract space and transfers it to the original MDP via potential-based shaping, improving learning speed. Experiments show GSP significantly accelerates learning in various domains compared to standard baselines.

## Method Summary
Goal-Space Planning (GSP) combines model-free reinforcement learning with background planning using subgoal-conditioned models. The framework learns local models that predict accumulated rewards and discounted probabilities for state-subgoal pairs, rather than full transition dynamics. These models enable rapid value propagation through background planning in an abstract MDP over subgoals. The algorithm computes subgoal values and uses potential-based reward shaping to guide the base learner. GSP is implemented with both tabular Sarsa and deep DDQN learners, and shows improved sample efficiency across multiple benchmark domains.

## Key Results
- GSP accelerates learning significantly compared to Sarsa and DDQN baselines in FourRooms, PinBall, and GridBall domains
- The method is effective in both tabular and function approximation settings
- GSP shows computational efficiency by avoiding full state trajectory simulations
- Performance is particularly strong when subgoal models are reasonably accurate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GSP accelerates value propagation by using subgoal-conditioned models to quickly update approximate values over larger regions of the state space.
- Mechanism: Instead of learning full transition dynamics, GSP learns local models that predict accumulated rewards and discounted probabilities for state-subgoal pairs. This allows rapid propagation of value estimates through background planning in an abstract MDP.
- Core assumption: Local subgoal models can be learned accurately enough to provide useful approximate values for states within their initiation sets.
- Break condition: If subgoal models are learned poorly, the propagated values become unreliable and can mislead the base learner.

### Mechanism 2
- Claim: Potential-based reward shaping using projected subgoal values guides the base learner to take promising actions more quickly.
- Mechanism: The algorithm computes vg⋆(s) for each state using nearby subgoal values, then uses the difference between vg⋆(s′) and vg⋆(s) as a shaping term in the TD error update.
- Core assumption: The shaping potential difference is sufficiently accurate to guide learning without introducing significant bias.
- Break condition: If shaping terms are too large in magnitude, they can destabilize neural network learning or cause convergence to suboptimal policies.

### Mechanism 3
- Claim: Background planning with subgoals provides computational efficiency by avoiding long rollout simulations.
- Mechanism: Planning occurs only between abstract subgoal states using learned local models, rather than generating entire state trajectories.
- Core assumption: The computational cost of planning in abstract subgoal space is much lower than simulating full state trajectories.
- Break condition: If the number of subgoals is too large, the computational savings diminish and planning overhead becomes significant.

## Foundational Learning

- Concept: Markov Decision Process formulation
  - Why needed here: The paper builds on standard MDP notation (S, A, R, P) and extends it with subgoals as abstract states.
  - Quick check question: What are the four components of an MDP tuple and what does each represent?

- Concept: Temporal abstraction through options
  - Why needed here: GSP uses options as actions in the abstract MDP, where each option reaches a subgoal from its initiation set.
  - Quick check question: How does an option differ from a primitive action in terms of temporal abstraction?

- Concept: Potential-based reward shaping
  - Why needed here: The shaping function vg⋆(s) is used to modify the TD error without changing the optimal policy.
  - Quick check question: What is the mathematical condition that must be satisfied for a shaping function to preserve optimal policies?

## Architecture Onboarding

- Component map:
  Base learner (Sarsa or DDQN) -> Subgoal models (rγ, Γ) -> Abstract MDP planner -> Projection module -> Replay buffer

- Critical path:
  1. Take action, observe transition
  2. Query models for rγ(s′,g), Γ(s′,g), and subgoal values
  3. Project vg⋆(s′) using nearest subgoals
  4. Update base learner with potential-based shaping
  5. Store transition in replay buffer
  6. Periodically update models and plan

- Design tradeoffs:
  - More subgoals → better value propagation but higher computational cost
  - Local models only → computational efficiency but potential loss of global information
  - Potential shaping → faster learning but risk of bias if shaping terms are inaccurate

- Failure signatures:
  - Base learner diverges or learns suboptimal policies → likely issue with model accuracy or shaping magnitude
  - No improvement over baseline → subgoal selection may be poor or models not learned well
  - High computational overhead → too many subgoals or inefficient planning implementation

- First 3 experiments:
  1. Implement tabular Sarsa(0) with hand-chosen subgoals in FourRooms to verify value propagation
  2. Add model learning and planning to step 1 to test full GSP pipeline
  3. Scale to GridBall with tile coding to test function approximation and larger state spaces

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can subgoal discovery be automated in Goal-Space Planning (GSP) without relying on hand-chosen subgoals?
- Basis in paper: The paper assumes subgoals are given and focuses on the planning formalism, but acknowledges that subgoal discovery is a critical open question for practical deployment.
- Why unresolved: The paper uses pre-defined subgoals in experiments, and while it mentions the importance of discovering subgoals, it does not propose a method for doing so.
- What evidence would resolve it: A method for automatically identifying abstract subgoals that improve planning efficiency and value propagation in GSP, validated through experiments.

### Open Question 2
- Question: How does the choice of subgoals affect the performance of GSP, particularly in terms of learning speed and policy optimality?
- Basis in paper: The paper states that GSP can perform well with somewhat suboptimal subgoal selection but can harm performance if subgoals are very poorly selected.
- Why unresolved: The paper does not explore the impact of different subgoal selection strategies on GSP's performance or provide guidelines for optimal subgoal selection.
- What evidence would resolve it: Empirical studies comparing GSP performance with different subgoal selection methods, including random, heuristic, and learned approaches.

### Open Question 3
- Question: How can GSP be extended to handle continuous state spaces more effectively, especially in high-dimensional environments?
- Basis in paper: The paper demonstrates GSP's effectiveness in domains with larger state spaces (e.g., PinBall) but notes that the continuous 4-dimensional state space makes the task harder and requires further investigation.
- Why unresolved: While GSP shows promise in continuous spaces, the paper does not address the challenges of scaling to even higher-dimensional or more complex environments.
- What evidence would resolve it: Experiments showing GSP's performance in high-dimensional continuous spaces, along with techniques for improving scalability (e.g., advanced function approximation or hierarchical abstractions).

## Limitations

- Subgoal selection remains a critical challenge - the framework assumes subgoals are given but doesn't specify how to discover them for new tasks
- Model accuracy sensitivity - performance degrades significantly with poor subgoal models, but minimum accuracy thresholds are not specified
- Hyperparameter sensitivity - some neural network details are provided but exact tuning that led to improvements isn't fully specified

## Confidence

- High confidence: The core mechanism of using subgoal-conditioned models for background planning and potential-based shaping is sound and well-grounded in existing literature
- Medium confidence: The computational efficiency claims are supported by the framework's design, but actual runtime comparisons against baselines would strengthen this claim
- Medium confidence: Experimental results show clear improvements in sample efficiency, but the magnitude of gains may be sensitive to subgoal selection and model accuracy

## Next Checks

1. **Subgoal sensitivity analysis**: Systematically test GSP with varying numbers and qualities of subgoals to quantify the impact of subgoal selection on learning speed and final performance.

2. **Model accuracy ablation**: Compare GSP performance when using perfect subgoal models versus learned models with different accuracy levels to understand the minimum model quality required for benefits.

3. **Runtime overhead measurement**: Measure actual wall-clock time for GSP versus baselines to verify the claimed computational efficiency, including both planning time and model learning overhead.