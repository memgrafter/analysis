---
ver: rpa2
title: Scaling FP8 training to trillion-token LLMs
arxiv_id: '2409.12517'
source_url: https://arxiv.org/abs/2409.12517
tags:
- training
- swiglu
- bf16
- these
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training large language models
  (LLMs) with FP8 precision on extremely large datasets (up to 2 trillion tokens),
  far exceeding previous limits. The authors identify a critical instability caused
  by outlier amplification in the SwiGLU activation function during prolonged training.
---

# Scaling FP8 training to trillion-token LLMs

## Quick Facts
- arXiv ID: 2409.12517
- Source URL: https://arxiv.org/abs/2409.12517
- Reference count: 34
- Primary result: Enables stable FP8 training of 7B parameter LLMs on 2 trillion token datasets with 34% throughput improvement

## Executive Summary
This work addresses the critical challenge of scaling FP8 training to unprecedented dataset sizes exceeding 2 trillion tokens, far beyond previous FP8 training limits. The authors identify outlier amplification in the SwiGLU activation function as the primary cause of instability during prolonged training. They introduce Smooth-SwiGLU, a novel modification that prevents this amplification while preserving the original function's behavior. Combined with FP8 quantization of both Adam optimizer moments, their approach enables stable training at massive scale with significant performance gains.

## Method Summary
The authors develop a two-pronged approach to enable stable FP8 training at trillion-token scale. First, they modify the SwiGLU activation function by introducing Smooth-SwiGLU, which adds Gaussian noise during normalization to prevent outlier amplification that occurs during long training runs. Second, they implement FP8 quantization of both first and second moments of the Adam optimizer, a previously unattained milestone. These innovations are tested on a 7B parameter model trained across 256 Intel Gaudi2 accelerators, achieving results comparable to BF16 baselines while delivering substantial throughput and memory improvements.

## Key Results
- Successfully trained 7B parameter model on 2 trillion tokens using FP8 precision
- Achieved up to 34% throughput improvement and 30% memory reduction compared to BF16
- Demonstrated stable training with Smooth-SwiGLU and FP8-optimized Adam optimizer
- First work to show FP8 quantization of both Adam optimizer moments

## Why This Works (Mechanism)
The key mechanism involves preventing outlier amplification in the SwiGLU activation function through Smooth-SwiGLU's noise injection during normalization. During long training runs, the exponential activation in SwiGLU can cause extreme values that destabilize FP8 training. By adding controlled Gaussian noise, Smooth-SwiGLU maintains numerical stability without altering the function's essential behavior. The FP8 optimizer moment quantization further reduces memory bandwidth requirements while maintaining training stability.

## Foundational Learning
- **FP8 Precision Training**: Uses 8-bit floating point representation (1 sign bit, 7 exponent bits) instead of traditional 16-bit or 32-bit formats. Needed to reduce memory bandwidth and increase computational throughput. Quick check: Verify FP8 format specifications and dynamic range limitations.
- **SwiGLU Activation Function**: A gating mechanism using sigmoid activation that controls information flow through neural networks. Essential for controlling which information passes through network layers. Quick check: Understand the gating mechanism and its role in model performance.
- **Outlier Amplification**: The process where extreme values grow exponentially during training, causing numerical instability. Critical to understand for maintaining training stability. Quick check: Examine how outliers propagate through activation functions.
- **Optimizer Moment Quantization**: Converting both first and second moment estimates (mean and variance) to lower precision. Important for reducing memory requirements. Quick check: Verify quantization effects on optimizer convergence.
- **Normalization Techniques**: Methods for scaling inputs to maintain numerical stability. Fundamental for preventing activation explosion. Quick check: Compare different normalization approaches and their stability properties.

## Architecture Onboarding
- **Component Map**: Input Data -> Embedding Layer -> Transformer Blocks (with Smooth-SwiGLU) -> Output Layer -> Loss Calculation -> Optimizer (FP8 Adam)
- **Critical Path**: Data loading and preprocessing → Embedding and position encoding → Transformer forward pass with Smooth-SwiGLU → Loss computation → Backward pass with FP8 optimizer → Parameter update
- **Design Tradeoffs**: Precision vs. stability (FP8 offers speed but requires careful numerical handling), memory vs. throughput (quantization saves memory but may affect convergence), complexity vs. performance (Smooth-SwiGLU adds slight overhead but prevents catastrophic failures)
- **Failure Signatures**: Training divergence due to outlier explosion, NaN values in activations, gradient explosion, optimizer instability from moment quantization errors
- **First Experiments**: 1) Test Smooth-SwiGLU on small toy model with controlled outlier injection, 2) Validate FP8 optimizer moment quantization on simple convex optimization problem, 3) Run ablation study comparing Smooth-SwiGLU vs. standard SwiGLU on short training runs

## Open Questions the Paper Calls Out
The paper acknowledges that FP8 training instabilities manifest differently across various workloads and hardware configurations, suggesting that the Smooth-SwiGLU solution may not be universally applicable. The authors also note that their findings are specific to the SwiGLU activation function, leaving open questions about whether similar outlier amplification issues exist in other architectural components or activation functions.

## Limitations
- Limited to SwiGLU activation function; generalizability to other architectures unknown
- Single hardware platform (Intel Gaudi2) and model size (7B parameters) tested
- Uncertain applicability to different training regimes or larger datasets beyond 2 trillion tokens
- No exploration of alternative outlier mitigation strategies beyond Smooth-SwiGLU

## Confidence
- **High confidence**: FP8 quantization of both Adam optimizer moments is a well-defined technical achievement with clear implementation details
- **Medium confidence**: Smooth-SwiGLU enables stable training on trillion-token datasets (validated on 2 trillion tokens but not larger)
- **Medium confidence**: Throughput and memory improvement claims (based on single-architecture experiments)

## Next Checks
1. Test Smooth-SwiGLU on alternative architectures (GPT, LLaMA) and activation functions to assess generalizability
2. Validate the approach on multiple hardware platforms (NVIDIA, AMD) to confirm cross-platform stability
3. Conduct ablation studies isolating the contribution of Smooth-SwiGLU versus FP8 optimizer moment quantization to the observed improvements