---
ver: rpa2
title: Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment
arxiv_id: '2404.12634'
source_url: https://arxiv.org/abs/2404.12634
tags:
- stroke
- transformer
- image
- multimodal
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes Multitrans, a multimodal fusion framework based
  on the Transformer architecture and self-attention mechanism for predicting stroke
  treatment outcomes. The framework combines non-contrast computed tomography (NCCT)
  images and discharge diagnosis reports from stroke patients.
---

# Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment

## Quick Facts
- arXiv ID: 2404.12634
- Source URL: https://arxiv.org/abs/2404.12634
- Reference count: 29
- Primary result: Multimodal fusion of NCCT images and discharge diagnosis reports achieves 0.89-0.90 accuracy in predicting stroke treatment outcomes using mRS scores

## Executive Summary
This study introduces Multitrans, a multimodal fusion framework based on Transformer architecture for predicting stroke treatment outcomes. The framework combines non-contrast computed tomography (NCCT) images and discharge diagnosis reports using pre-trained BERT and ViT models with self-attention fusion. Evaluated on 128 stroke patients, the method demonstrates that while transformer models perform poorly on imaging data alone, combining them with clinical diagnostic information enables better complementary learning and more accurate stroke treatment outcome prediction.

## Method Summary
The Multitrans framework uses pre-trained BERT for text feature extraction and ViT (or DeiT/SwinT) for image feature extraction from NCCT scans. These modality-specific representations are fused using a self-attention mechanism that computes weighted interactions between text and image features. The fused representation is then classified using an MLP to predict modified Rankin Scale (mRS) outcomes at 90 days post-stroke. The model was trained on 90 patients (70% of 128 total) with 25 for validation and 13 for testing, using binary classification (mRS ≤ 2 vs > 2).

## Key Results
- Unimodal text classification achieves 0.87-0.87 accuracy and 0.74-0.76 F1-score
- Unimodal image classification achieves only 0.56 accuracy and 0.42 F1-score
- Multimodal fusion achieves best performance with 0.89-0.90 accuracy, 0.75-0.77 F1-score, and 0.84-0.85 AUC
- Multimodal approach outperforms both unimodal baselines, demonstrating the value of cross-modal information fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion of clinical text and NCCT images yields better predictive performance than either modality alone in stroke outcome prediction.
- Mechanism: The self-attention-based fusion module learns cross-modal interactions, allowing the model to leverage complementary information—clinical diagnosis reports capture patient history and treatment context, while NCCT images provide structural brain data. This combination produces more robust predictions than unimodal models.
- Core assumption: Clinical text and imaging data contain complementary information relevant to predicting stroke treatment outcomes.
- Evidence anchors:
  - [abstract] "unimodal text classification (accuracy 0.87-0.87, F1-score 0.74-0.76) significantly outperforms unimodal image classification (accuracy 0.56, F1-score 0.42), but multimodal fusion achieves the best performance (accuracy 0.89-0.90, F1-score 0.75-0.77, AUC 0.84-0.85)"
  - [section] "Although the Transformer model only performs worse on imaging data, when combined with clinical meta-diagnostic information, both can learn better complementary information"
- Break condition: If clinical text and imaging data are redundant or if one modality dominates the other in information content, the fusion benefit may diminish or disappear.

### Mechanism 2
- Claim: Pre-trained Transformer models (BERT for text, ViT for images) effectively extract meaningful features for stroke outcome prediction.
- Mechanism: BERT and ViT leverage large-scale pre-training to capture semantic (text) and visual (image) representations, respectively. These representations are then combined via the self-attention mechanism to produce a fused feature vector that captures cross-modal dependencies.
- Core assumption: Pre-trained models provide generalizable feature representations that transfer well to stroke-related tasks.
- Evidence anchors:
  - [abstract] "The framework utilises pre-trained BERT and ViT models to extract features from text and images respectively"
  - [section] "The framework utilises BERT [25] and ViT as base models"
- Break condition: If the pre-training domains are too dissimilar from stroke data, the feature extraction quality may degrade, reducing model performance.

### Mechanism 3
- Claim: Self-attention mechanism effectively fuses multimodal representations by computing weighted interactions between modalities.
- Mechanism: The self-attention layer computes query-key-value interactions, allowing each modality's representation to attend to the other. This creates a cross-modal context-aware embedding that captures relevant interactions for outcome prediction.
- Core assumption: Self-attention can learn meaningful cross-modal interactions that improve prediction accuracy.
- Evidence anchors:
  - [abstract] "Multitrans uses pre-trained BERT and ViT models to extract features from text and images respectively, then fuses these features using a self-attention mechanism"
  - [section] "The self-attention layer computes the attentional weights of each element in the input over all other elements"
- Break condition: If the modality representations are incompatible in scale or semantic space, the attention mechanism may fail to learn meaningful interactions.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: The entire model is built on Transformer-based components (BERT, ViT, and self-attention), so understanding their core mechanisms is essential.
  - Quick check question: What is the role of the multi-head self-attention mechanism in the Transformer architecture?

- Concept: Multimodal learning principles
  - Why needed here: The model combines two distinct data types (text and images), requiring understanding of how to represent and fuse heterogeneous modalities.
  - Quick check question: What are the key challenges in multimodal learning, and how does the proposed architecture address them?

- Concept: Clinical stroke assessment using mRS
  - Why needed here: The model predicts functional outcomes using the modified Rankin Scale, so understanding this clinical metric is crucial for proper model evaluation.
  - Quick check question: What does the modified Rankin Scale measure, and how is it typically used in stroke outcome assessment?

## Architecture Onboarding

- Component map: NCCT images -> ViT -> Image features -> Self-attention fusion -> MLP -> mRS prediction; Discharge diagnosis reports -> BERT -> Text features -> Self-attention fusion -> MLP -> mRS prediction

- Critical path: Text/Image → ViT/BERT → CLS extraction → Fusion (self-attention) → MLP → mRS prediction

- Design tradeoffs:
  - Using pre-trained models reduces training data requirements but may limit domain-specific adaptation
  - Self-attention fusion provides flexibility but adds computational overhead compared to simpler fusion methods
  - The 7:2:1 train-validation-test split may be small for 128 patients, potentially affecting model generalization

- Failure signatures:
  - Poor unimodal performance indicates issues with feature extraction or domain mismatch
  - Fusion not improving over best unimodal suggests attention mechanism isn't learning meaningful cross-modal interactions
  - Overfitting on small dataset indicated by large train-test performance gap

- First 3 experiments:
  1. Train and evaluate unimodal BERT and ViT models separately to establish baseline performance
  2. Implement and test the self-attention fusion mechanism with fixed pre-trained features
  3. Fine-tune both ViT and BERT end-to-end while training the fusion module to assess joint optimization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Multitrans compare to other multimodal fusion architectures (e.g., MM-UNet, Transop, or multimodal deep learning methods) when applied to stroke outcome prediction?
- Basis in paper: [explicit] The paper discusses related work including MM-UNet, Transop, and other multimodal approaches, but does not directly compare Multitrans to these methods.
- Why unresolved: The authors evaluated Multitrans against unimodal baselines and ablation studies using different Transformer architectures, but did not benchmark against existing multimodal frameworks for stroke prediction.
- What evidence would resolve it: A direct comparison of Multitrans with MM-UNet, Transop, and other multimodal methods using the same dataset and evaluation metrics would establish relative performance.

### Open Question 2
- Question: What is the impact of incorporating additional modalities (e.g., MRI, clinical biomarkers, demographic data) on the predictive performance of Multitrans for stroke outcomes?
- Basis in paper: [explicit] The authors mention plans to study and add more modal information in future work, indicating this as an open direction.
- Why unresolved: The current framework only uses NCCT images and discharge diagnosis reports. The potential benefits of additional modalities remain unexplored.
- What evidence would resolve it: Extending Multitrans to incorporate additional modalities and evaluating performance improvements on the same dataset would demonstrate the impact of multimodal integration.

### Open Question 3
- Question: How does the cross-modal semantic alignment between text information and stroke images affect the model's prediction accuracy, and what methods can improve this alignment?
- Basis in paper: [explicit] The authors note that ambiguous semantic matches between text and images significantly reduce prediction accuracy and plan to conduct more precise fine-grained alignment.
- Why unresolved: The current model uses linear layers or attention mechanisms for fusion without explicit cross-modal alignment, leading to performance degradation when semantic matches are ambiguous.
- What evidence would resolve it: Implementing and testing explicit cross-modal alignment techniques (e.g., contrastive learning, cross-attention mechanisms) and measuring their impact on prediction accuracy would address this limitation.

## Limitations
- Small sample size (128 patients) may limit generalizability and increase risk of overfitting
- Binary mRS classification (≤2 vs >2) oversimplifies the continuous nature of stroke outcomes, potentially losing clinically relevant gradations
- Self-attention fusion mechanism may not capture complex cross-modal interactions as effectively as more sophisticated alignment methods

## Confidence
- High Confidence: The unimodal text classification outperforming unimodal image classification (0.87-0.87 vs 0.56 accuracy) - this is well-supported by the reported metrics.
- Medium Confidence: The claim that multimodal fusion achieves the best performance (0.89-0.90 accuracy) - while reported, the small dataset size and lack of statistical significance testing reduce confidence.
- Low Confidence: The assertion that transformer models "perform poorly on imaging data alone" - this may be due to the specific implementation or dataset rather than an inherent limitation of transformer architectures for medical imaging.

## Next Checks
1. Perform statistical significance testing (e.g., McNemar's test) to confirm that the performance differences between unimodal and multimodal models are statistically significant given the small sample size.
2. Conduct ablation experiments to quantify the individual contribution of each modality to the final performance, and compare self-attention fusion against simpler fusion methods like concatenation.
3. Evaluate the model on an independent external dataset or through cross-validation to assess its generalization capability beyond the single-center dataset used in this study.