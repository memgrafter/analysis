---
ver: rpa2
title: How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and Consistency
arxiv_id: '2407.13578'
source_url: https://arxiv.org/abs/2407.13578
tags:
- knowledge
- llms
- questions
- responses
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a comprehensive framework for evaluating
  large language models (LLMs) as knowledge bases, focusing on two critical aspects:
  factuality and consistency. Factuality ensures accurate responses to both seen and
  unseen knowledge, while consistency maintains stable answers to questions about
  the same facts.'
---

# How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and Consistency

## Quick Facts
- arXiv ID: 2407.13578
- Source URL: https://arxiv.org/abs/2407.13578
- Authors: Danna Zheng; Mirella Lapata; Jeff Z. Pan
- Reference count: 40
- Primary result: Introduces a framework evaluating LLMs on factuality and consistency, revealing GPT-3.5-Turbo achieves balanced performance while larger models excel on seen knowledge but struggle with unseen knowledge.

## Executive Summary
This paper presents a comprehensive framework for evaluating large language models as knowledge bases, focusing on factuality and consistency. The authors argue that existing evaluation methods are inadequate because they don't distinguish between seen and unseen knowledge, and they don't properly measure consistency. They introduce new metrics including Net Correct Rate (NCR) and Net Consistently Correct Rate (NCCR) to quantify these dimensions. Experiments on 26 LLMs reveal that GPT-3.5-Turbo achieves balanced performance across both dimensions, while larger models excel on seen knowledge but struggle with unseen knowledge. The study highlights the need for more robust strategies to ensure reliable LLM-as-KB usage.

## Method Summary
The authors evaluate 26 LLMs on two datasets: SeenQA (3,000 questions from Natural Questions, TriviaQA, and PopQA) and UnseenQA (3,000 questions generated using 20 templates covering unseen knowledge). They use three prompt settings: zero-shot, four-shot, and four-shot with two unsure shots. Responses are classified as correct, wrong, or uninformative, and consistency is measured using multiple-choice questions generated with GPT-3.5-Turbo-Instruct. Key metrics include NCR (Net Correct Rate), UR (Uninformative Rate), NCCR (Net Consistently Correct Rate), and IUR (Inconsistent/Uninformative Rate).

## Key Results
- GPT-3.5-Turbo achieves the most balanced performance across both factuality and consistency dimensions
- Larger LLMs perform significantly better on seen knowledge but worse on unseen knowledge
- Fine-tuning improves performance on unseen knowledge but reduces effectiveness on seen knowledge
- In-context learning with unsure shots enhances performance on unseen knowledge
- Factuality and consistency are found to be independent dimensions that require separate evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Factuality requires distinguishing between correct, wrong, and uninformative responses rather than using a single correct rate metric.
- **Mechanism**: The authors introduce Net Correct Rate (NCR) as CR - WR, where CR is the proportion of correct answers and WR is the proportion of wrong answers. This metric penalizes models that produce wrong answers even if their correct rate is high.
- **Core assumption**: A model with high correct rate but also high wrong rate is less factual than a model with slightly lower correct rate but much lower wrong rate.
- **Evidence anchors**: [abstract] "Existing methods for evaluating factuality often focus on the rate of correct answers... However, as many studies fail to specify whether the dataset's knowledge was included in the LLM's pre-training data, it is not possible to establish whether the model is genuinely factual."
- **Break condition**: If the model's wrong answers are systematically different from correct answers (e.g., always off by one year in date questions), the simple subtraction in NCR might not capture the severity of errors appropriately.

### Mechanism 2
- **Claim**: Consistency should be measured differently for correct versus wrong responses.
- **Mechanism**: The authors propose Ccorrect to measure consistency of correct answers and Cwrong to measure consistency of wrong answers, with lower Cwrong being better. They create multiple-choice questions with distractors to test if the model gives the same answer repeatedly.
- **Core assumption**: A model that gives consistent wrong answers is less reliable than one that gives inconsistent wrong answers, as inconsistency in wrong answers shows uncertainty.
- **Evidence anchors**: [abstract] "Unlike KBs, which store information in fixed locations, LLMs operate probabilistically... Sampling from a uniform distribution naturally leads to inconsistencies."
- **Break condition**: If the model's wrong answers are based on systematic misconceptions rather than random guesses, low Cwrong might actually indicate dangerous confidence in incorrect knowledge.

### Mechanism 3
- **Claim**: Larger models perform better on seen knowledge but worse on unseen knowledge, creating a tradeoff.
- **Mechanism**: The authors find that NCR and NCCR improve with model size for seen knowledge, while UR decreases and IUR decreases for unseen knowledge. This suggests larger models overconfidently provide wrong answers to unseen questions.
- **Core assumption**: Larger models have better recall of training data but don't necessarily have better ability to recognize when they don't know something.
- **Evidence anchors**: [section 5.2] "Larger LLMs perform better on seen knowledge but worse on unseen knowledge. As model size increases, both NCR (blue line) and NCCR (purple line) improve, indicating better performance on questions related to seen knowledge."
- **Break condition**: If the model size correlation is actually driven by the specific datasets used rather than a fundamental property of LLM scaling, the mechanism would break for different evaluation datasets.

## Foundational Learning

- **Concept**: Difference between seen and unseen knowledge in LLM evaluation
  - Why needed here: The paper distinguishes between knowledge the model was trained on (seen) and knowledge it wasn't exposed to (unseen). This distinction is fundamental to understanding the evaluation framework.
  - Quick check question: If a model was trained on Wikipedia up to 2020, would a question about events in 2021 be considered seen or unseen knowledge?

- **Concept**: Probabilistic versus deterministic knowledge representation
  - Why needed here: The paper argues that LLMs use probabilistic next-token prediction, which differs fundamentally from how traditional knowledge bases store information, affecting consistency.
  - Quick check question: Why might a traditional database give the same answer every time while an LLM might give different answers to the same question?

- **Concept**: Metric design for reliability assessment
  - Why needed here: The paper introduces multiple metrics (NCR, UR, NCCR, IUR) that combine factuality and consistency in specific ways. Understanding how these metrics work is crucial for interpreting results.
  - Quick check question: If a model has CR=0.8 and WR=0.1, what is its NCR value?

## Architecture Onboarding

- **Component map**: Data preparation (SeenQA and UnseenQA datasets) -> Response classification (correct/wrong/uninformative) -> Metric computation (NCR, UR, NCCR, IUR) -> Consistency scoring (MCQs with distractors)
- **Critical path**: The main evaluation flow is: generate/collect questions → get LLM responses → classify responses → compute consistency scores → calculate final metrics. The consistency scoring is the most computationally intensive part due to MCQ generation.
- **Design tradeoffs**: Using MCQ-based consistency scoring provides more robust measurement than simple repeated sampling, but requires additional LLM calls for distractor generation. The choice to separate seen/unseen knowledge creates clearer evaluation criteria but requires careful dataset construction.
- **Failure signatures**: If NCR values are all negative, it indicates systematic model hallucination. If UR is very low for unseen knowledge, the model is overconfident. If Cwrong is very high, the model consistently gives the same wrong answers, which is dangerous.
- **First 3 experiments**:
  1. Run the evaluation framework on a single simple LLM (like GPT-3.5) to verify the pipeline works end-to-end.
  2. Compare NCR vs. simple correct rate on the same models to demonstrate why NCR is needed.
  3. Test the consistency measurement by asking the same question multiple times with different random seeds to verify the MCQ approach captures variability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger LLMs perform on unseen knowledge compared to smaller ones?
- Basis in paper: [inferred] The paper mentions that larger LLMs perform worse on unseen knowledge, but it does not provide a detailed comparison or specific reasons for this behavior.
- Why unresolved: The paper only briefly touches on this observation without delving into the underlying causes or implications.
- What evidence would resolve it: Further experiments comparing LLMs of different sizes on unseen knowledge tasks, along with analysis of their training data and architecture, could provide insights into why larger models struggle more with unseen knowledge.

### Open Question 2
- Question: How do fine-tuning techniques impact the consistency of LLMs?
- Basis in paper: [explicit] The paper states that fine-tuning improves performance on unseen knowledge but degrades performance on seen knowledge, and that fine-tuning does not improve LLM consistency.
- Why unresolved: The paper does not explore the reasons behind this lack of improvement in consistency or the potential trade-offs involved.
- What evidence would resolve it: Additional experiments comparing the consistency of fine-tuned and base LLMs on both seen and unseen knowledge, along with analysis of the fine-tuning process, could shed light on this issue.

### Open Question 3
- Question: How does in-context learning (ICL) affect the factuality and consistency of LLMs?
- Basis in paper: [explicit] The paper mentions that ICL does not improve performance on seen knowledge and that unsure shots enhance performance on unseen knowledge, but it does not provide a comprehensive analysis of the effects of ICL on factuality and consistency.
- Why unresolved: The paper only briefly touches on the effects of ICL without exploring its broader implications for LLM performance.
- What evidence would resolve it: Further experiments comparing the factuality and consistency of LLMs under different ICL settings, along with analysis of the impact of shot types and numbers, could provide a more complete understanding of ICL's effects.

## Limitations

- The distinction between "seen" and "unseen" knowledge relies on approximate knowledge cutoffs, which may not be precisely known for all models
- The consistency measurement using MCQ generation depends on the quality of distractors generated by GPT-3.5-Turbo-Instruct, which may not always represent plausible alternatives
- The framework primarily evaluates single-turn question answering and may not fully capture multi-hop reasoning or dynamic knowledge retrieval scenarios

## Confidence

**High Confidence**: The core finding that larger models perform better on seen knowledge but worse on unseen knowledge is well-supported by the data and consistent with the general understanding of LLM behavior. The proposed metrics (NCR, UR, NCCR, IUR) provide meaningful differentiation between models and are mathematically sound.

**Medium Confidence**: The claim that factuality and consistency are independent dimensions has strong empirical support but requires further theoretical grounding. The assertion that fine-tuning improves unseen knowledge performance while degrading seen knowledge performance is based on the tested models but may not generalize to all fine-tuning approaches.

**Low Confidence**: The generalizability of the evaluation framework to non-English languages and specialized domains remains untested. The framework's sensitivity to different question types and the optimal balance between factuality and consistency metrics for practical applications are areas requiring further investigation.

## Next Checks

1. **Cross-Dataset Validation**: Replicate the evaluation using independently constructed seen/unseen knowledge datasets to verify that the observed model behavior (larger models excelling on seen knowledge but struggling with unseen knowledge) is not dataset-specific.

2. **Consistency Measurement Verification**: Conduct a controlled experiment where the same question is asked multiple times with different random seeds to empirically verify the consistency scores obtained through the MCQ generation method, comparing these results with direct sampling.

3. **Knowledge Cutoff Verification**: For a subset of models, attempt to identify precise knowledge cutoffs by asking questions about verifiable events across time periods, then use these verified cutoffs to re-classify questions as seen/unseen and re-run the evaluation.