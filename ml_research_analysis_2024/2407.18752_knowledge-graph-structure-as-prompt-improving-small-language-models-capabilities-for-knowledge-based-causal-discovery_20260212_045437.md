---
ver: rpa2
title: 'Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities
  for Knowledge-based Causal Discovery'
arxiv_id: '2407.18752'
source_url: https://arxiv.org/abs/2407.18752
tags:
- causal
- graph
- prompt
- language
- expresses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Small Language Models (SLMs)
  with prompt-based learning for knowledge-based causal discovery. The authors propose
  "KG Structure as Prompt," a novel approach that integrates structural information
  from knowledge graphs, such as common neighbor nodes and metapaths, into prompt-based
  learning to enhance the capabilities of SLMs.
---

# Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery

## Quick Facts
- **arXiv ID:** 2407.18752
- **Source URL:** https://arxiv.org/abs/2407.18752
- **Reference count:** 40
- **Primary result:** Novel KG Structure as Prompt approach enhances SLMs for causal discovery, surpassing most baselines and even full-dataset fine-tuned models

## Executive Summary
This paper addresses the challenge of knowledge-based causal discovery by leveraging Small Language Models (SLMs) with prompt-based learning. The authors introduce "KG Structure as Prompt," a novel approach that integrates structural information from knowledge graphs—specifically common neighbor nodes and metapaths—into prompt-based learning. This method aims to enhance the capabilities of SLMs for causal discovery tasks. Experimental results on biomedical and open-domain datasets under few-shot settings demonstrate that their approach surpasses most baselines and even conventional fine-tuning approaches trained on full datasets. The findings highlight the strong capabilities of SLMs when combined with knowledge graphs and prompt-based learning, showing that they can surpass larger LLMs with more parameters in certain causal discovery tasks.

## Method Summary
The paper proposes "KG Structure as Prompt," a novel approach that integrates structural information from knowledge graphs into prompt-based learning for Small Language Models (SLMs). This method leverages common neighbor nodes and metapaths from knowledge graphs to enhance the capabilities of SLMs in knowledge-based causal discovery tasks. The approach is designed to work under few-shot settings, making it particularly useful when labeled data is scarce. Experiments were conducted on both biomedical and open-domain datasets to validate the effectiveness of the method. The results show that KG Structure as Prompt not only surpasses most baseline approaches but also outperforms conventional fine-tuning methods that use full datasets, demonstrating the potential of combining SLMs with knowledge graphs for causal discovery.

## Key Results
- KG Structure as Prompt approach surpasses most baselines in causal discovery tasks
- Method outperforms conventional fine-tuning approaches trained on full datasets
- SLMs combined with KG and prompt-based learning can surpass larger LLMs with more parameters

## Why This Works (Mechanism)
The effectiveness of KG Structure as Prompt stems from its ability to incorporate rich structural information from knowledge graphs into the prompt-based learning framework. By integrating common neighbor nodes and metapaths, the approach provides SLMs with additional context and relational information that enhances their understanding of causal relationships. This structured information acts as a form of external knowledge that compensates for the limited parameter capacity of SLMs, allowing them to perform at par or better than larger models. The prompt-based learning paradigm enables efficient use of this knowledge without the need for extensive fine-tuning, making it particularly effective in few-shot settings where labeled data is scarce.

## Foundational Learning
- **Knowledge Graphs (KGs):** Why needed: Provide structured relational information for causal discovery. Quick check: Verify KG coverage and quality for the target domain.
- **Prompt-based Learning:** Why needed: Enables efficient use of SLMs without extensive fine-tuning. Quick check: Test different prompt templates and formats.
- **Common Neighbor Nodes:** Why needed: Capture direct relational context between entities. Quick check: Measure impact of including/excluding common neighbors.
- **Metapaths:** Why needed: Represent higher-order relational patterns in KGs. Quick check: Evaluate performance with different metapaths.
- **Causal Discovery:** Why needed: Core task of inferring cause-effect relationships. Quick check: Validate discovered causal relationships against ground truth.
- **Few-shot Learning:** Why needed: Practical setting with limited labeled data. Quick check: Test performance across varying numbers of training examples.

## Architecture Onboarding

**Component Map:** Knowledge Graph -> Structural Feature Extraction (Common Neighbors, Metapaths) -> Prompt Construction -> Small Language Model -> Causal Discovery Output

**Critical Path:** The critical path flows from the knowledge graph through structural feature extraction to prompt construction, as these components directly determine the quality of input to the SLM. The SLM and causal discovery output stages are dependent on the quality of prompts generated from KG structural information.

**Design Tradeoffs:** The approach trades model size for knowledge graph quality and prompt engineering sophistication. While larger LLMs might capture more patterns inherently, this method leverages external knowledge to achieve comparable results with smaller models, offering computational efficiency at the cost of KG dependency.

**Failure Signatures:** Poor KG coverage or quality will directly impact causal discovery accuracy. Overly complex metapaths may introduce noise rather than useful information. Suboptimal prompt construction can fail to effectively communicate KG structural information to the SLM.

**3 First Experiments:**
1. Baseline comparison: Evaluate SLM performance with and without KG structural information in prompts
2. Ablation study: Test impact of common neighbors vs. metapaths separately on causal discovery accuracy
3. Parameter efficiency test: Compare performance across different SLM sizes with fixed KG structure integration

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow experimental scope limits generalizability to other causal discovery domains
- Claims about surpassing larger LLMs need cautious interpretation due to limited baseline details
- Heavy reliance on KG quality and coverage, with missing information potentially affecting accuracy
- Computational efficiency claims relative to fine-tuning are not quantified

## Confidence
- SLM performance claims: Medium
- KG integration effectiveness: Medium
- Cross-domain generalizability: Low
- Computational efficiency claims: Low

## Next Checks
1. Conduct cross-domain evaluations on diverse causal discovery tasks to verify generalizability
2. Perform detailed ablation studies isolating the contribution of different KG structural elements to causal discovery performance
3. Measure and compare inference latency and memory requirements against both fine-tuning approaches and larger LLMs under identical hardware constraints