---
ver: rpa2
title: Unsupervised-to-Online Reinforcement Learning
arxiv_id: '2408.14785'
source_url: https://arxiv.org/abs/2408.14785
tags:
- offline
- learning
- unsupervised
- online
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes U2O RL, a framework that replaces task-specific
  supervised offline RL pre-training with task-agnostic unsupervised offline RL in
  the offline-to-online RL setting. U2O RL pre-trains a multi-task skill policy using
  intrinsic rewards, then identifies the best skill for a downstream task and fine-tunes
  it online.
---

# Unsupervised-to-Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.14785
- Source URL: https://arxiv.org/abs/2408.14785
- Authors: Junsu Kim; Seohong Park; Sergey Levine
- Reference count: 40
- Primary result: U2O RL achieves strong performance across nine environments by replacing task-specific supervised offline RL pre-training with task-agnostic unsupervised offline RL

## Executive Summary
This paper introduces U2O RL, a framework that replaces traditional task-specific supervised offline RL pre-training with task-agnostic unsupervised offline RL in the offline-to-online RL setting. The method pre-trains a multi-task skill policy using intrinsic rewards, then identifies the best skill for a downstream task and fine-tunes it online. U2O RL demonstrates strong performance across nine environments, often matching or outperforming previous offline-to-online RL approaches. A key advantage is the ability to reuse a single pre-trained model for multiple downstream tasks.

## Method Summary
U2O RL is a three-stage framework that first pre-trains a multi-task skill policy using unsupervised offline RL with intrinsic rewards. The pre-trained policy is then bridged to the target task through skill identification (selecting the best latent skill) and reward scale matching to align intrinsic and task reward distributions. Finally, the identified skill is fine-tuned online using standard RL algorithms with task rewards. The approach leverages large task-agnostic offline datasets for pre-training, enabling better representation learning that prevents feature collapse compared to task-specific supervised pre-training.

## Key Results
- U2O RL matches or outperforms previous offline-to-online RL methods across nine tasks from five benchmarks
- The method demonstrates strong performance in Adroit, Kitchen, AntMaze, and ExORL environments
- A single pre-trained U2O RL model can be successfully adapted to multiple downstream tasks
- Reward scale matching significantly improves fine-tuning stability and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised offline RL learns better value function representations than supervised offline RL, which improves online fine-tuning performance.
- Mechanism: Unsupervised pre-training on diverse intrinsic tasks prevents feature co-adaptation and collapse, resulting in richer and more stable value function representations.
- Core assumption: The quality of value function representations correlates with downstream RL performance.
- Evidence anchors:
  - [abstract]: "The improvement is attributed to better representation learning from unsupervised multi-task pre-training, which prevents feature collapse compared to task-specific supervised pre-training."
  - [section]: "We hypothesized in Section 4.4 that this is because unsupervised offline pre-training yields better representations that facilitate online task adaptation... The results suggest that our unsupervised multi-task pre-training effectively prevents feature co-adaptation and thus indeed yields better representations across the environments."
  - [corpus]: No direct evidence; corpus focuses on other aspects of offline-to-online RL.
- Break condition: If the dataset lacks diversity, unsupervised pre-training may not learn better representations than supervised pre-training.

### Mechanism 2
- Claim: U2O RL can reuse a single pre-trained model for multiple downstream tasks.
- Mechanism: Since unsupervised offline RL doesn't use task-specific rewards during pre-training, the same pre-trained policy can be adapted to different tasks through skill identification and fine-tuning.
- Core assumption: The pre-trained skill policy captures generalizable behaviors that can be adapted to various tasks.
- Evidence anchors:
  - [abstract]: "A key advantage is the ability to reuse a single pre-trained model for multiple downstream tasks."
  - [section]: "Another benefit of U2O RL is that it does not use any task-specific information during pre-training... it enables leveraging potentially very large, task-agnostic offline data during pre-training."
  - [corpus]: Weak evidence; corpus neighbors discuss offline-to-online RL but don't specifically address multi-task reuse.
- Break condition: If tasks require fundamentally different behaviors that aren't captured in the pre-trained skills.

### Mechanism 3
- Claim: Reward scale matching bridges the gap between intrinsic and task rewards, preventing performance drops during fine-tuning.
- Mechanism: By normalizing intrinsic and task rewards to have the same scale and mean, the method prevents abrupt shifts in target Q-values that could destabilize learning.
- Core assumption: Abrupt reward scale changes cause instability in Q-learning updates.
- Evidence anchors:
  - [section]: "To prevent a potential mismatch between the intrinsic and task rewards, we propose a simple yet effective reward scale matching technique that bridges the gap between the two training schemes and thus improves performance and stability."
  - [corpus]: No direct evidence; reward normalization is not discussed in corpus neighbors.
- Break condition: If the intrinsic and task reward distributions are too different, normalization alone may not suffice.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper frames reinforcement learning problems as MDPs, defining states, actions, rewards, and transition dynamics.
  - Quick check question: What are the five components of an MDP tuple?

- Concept: Offline Reinforcement Learning
  - Why needed here: U2O RL builds on offline RL methods (like IQL and TD3) for both pre-training and fine-tuning stages.
  - Quick check question: What is the main challenge in offline RL that methods like IQL address?

- Concept: Successor Features
  - Why needed here: The paper uses successor feature-based methods (HILP) for unsupervised pre-training, where features capture temporal distances between states.
  - Quick check question: How do successor features relate to goal-conditioned reinforcement learning?

## Architecture Onboarding

- Component map:
  Offline dataset → Unsupervised pre-training → Skill identification → Reward scale matching → Online fine-tuning → Policy

- Critical path:
  1. Pre-train multi-task skill policy with unsupervised offline RL
  2. Identify best skill latent vector z* for target task
  3. Normalize reward scales between intrinsic and task rewards
  4. Fine-tune policy with online interactions using z*

- Design tradeoffs:
  - Unsupervised pre-training vs. supervised: Better representations but requires skill identification step
  - Single vs. multiple pre-trained models: Reusability vs. task-specific optimization
  - Simple reward normalization vs. sophisticated reward shaping: Ease of implementation vs. potential performance gains

- Failure signatures:
  - Poor fine-tuning performance despite good pre-training: Likely reward scale mismatch or poor skill identification
  - Unstable learning during fine-tuning: Check reward normalization and value transfer quality
  - No improvement over O2O RL: Dataset may lack diversity for effective unsupervised pre-training

- First 3 experiments:
  1. Implement U2O RL with HILP pre-training and IQL fine-tuning on a simple task (e.g., Walker Run)
  2. Compare feature dot products between U2O RL and standard O2O RL during pre-training
  3. Test multi-task reuse by fine-tuning the same pre-trained model on different tasks in the same environment

## Open Questions the Paper Calls Out
None

## Limitations
- The improvement attribution to better representation learning is based on performance observations rather than direct feature analysis
- The practical limits of multi-task reusability across diverse task types remain unclear
- Reward scale matching effectiveness is hypothesized but not extensively validated across different reward distribution scenarios

## Confidence

**Major uncertainties:**
- The claim that unsupervised pre-training prevents feature collapse is based on observed performance improvements rather than direct feature analysis, though the authors provide indirect evidence through comparison with supervised pre-training.
- The multi-task reuse advantage is demonstrated but the extent of reusability across diverse task types remains unclear.
- Reward scale matching effectiveness is hypothesized but not extensively validated across different reward distribution scenarios.

**Confidence labels:**
- **High confidence**: U2O RL improves performance over O2O RL in the tested environments; the three-stage framework is clearly specified.
- **Medium confidence**: The attribution of improvement to better representation learning is reasonable but could benefit from more direct feature analysis.
- **Medium confidence**: The advantage of reusing a single pre-trained model is demonstrated but the practical limits of this reusability need further exploration.

## Next Checks
1. Conduct ablation studies to isolate the impact of reward scale matching on fine-tuning stability.
2. Perform feature analysis comparing value function representations learned through supervised vs. unsupervised pre-training.
3. Test U2O RL's multi-task reusability across more diverse task combinations to establish practical limits.