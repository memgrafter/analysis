---
ver: rpa2
title: 'Latent Causal Probing: A Formal Perspective on Probing with Causal Models
  of Data'
arxiv_id: '2407.13765'
source_url: https://arxiv.org/abs/2407.13765
tags:
- causal
- latent
- probing
- training
- probe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formal causal framework for interpreting
  probing experiments in NLP. The authors develop a causal mediation analysis approach
  that isolates the contribution of language model representations from probe capacity
  in auxiliary task performance.
---

# Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data

## Quick Facts
- arXiv ID: 2407.13765
- Source URL: https://arxiv.org/abs/2407.13765
- Reference count: 30
- Primary result: Proposes a causal mediation analysis framework that isolates language model contributions from probe capacity in probing experiments

## Executive Summary
This paper introduces a formal causal framework for interpreting probing experiments in natural language processing. The authors develop a method using structural causal models (SCMs) and causal mediation analysis to disentangle the contributions of language model representations from probe capacity when measuring auxiliary task performance. Using a synthetic grid-world navigation task with known causal structure, they demonstrate that positive path-specific effects indicate the language model has learned to represent underlying latent concepts. The framework provides theoretical guarantees for probe interpretation and shows that while more complex probes achieve higher raw accuracy, causal mediation analysis can reveal that simpler probes sometimes better isolate the LM's inductive biases.

## Method Summary
The method trains a 350M parameter CodeGen Transformer model on synthetic grid-world navigation data, then constructs an auxiliary dataset with bound and free latent variable splits. Probes (linear, 1-layer MLP, 2-layer MLP) are trained to predict latent states from LM representations. The key innovation is using causal mediation analysis with valid baseline SCMs to decompose the total causal effect into contributions from LM training, probe calibration, and probe measurement. By computing path-specific effects, the approach quantifies how much of the accuracy comes from what the LM learned rather than the probe's own learning ability.

## Key Results
- Causal mediation analysis successfully isolates LM contributions from probe capacity in auxiliary task performance
- Positive path-specific effects indicate the LM has learned to represent underlying latent concepts
- Simpler probes can sometimes better isolate LM inductive biases despite achieving lower raw accuracy
- The framework provides theoretical guarantees for probe interpretation when valid baselines exist

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal mediation analysis isolates the LM's contribution to probing accuracy
- Mechanism: Decomposes the causal effect into three paths - LM training, probe calibration, and probe measurement. Path-specific effects quantify how much accuracy comes from the LM's learned representations versus the probe's learning ability.
- Core assumption: Stable causal relationship between latent variables and representations created by training data distribution and LM architecture
- Evidence anchors:
  - [abstract] "Our techniques provide robust empirical evidence for the ability of LMs to induce the latent concepts underlying text."
  - [section 3.2] "We propose a method of disentangling the two effects using the formal framework of causal mediation analysis"
- Break condition: If the valid baseline fails to satisfy required conditions (equations 1 and 2), mediation analysis becomes biased and cannot reliably isolate the LM's contribution.

### Mechanism 2
- Claim: Valid baseline selection ensures proper causal effect measurement
- Mechanism: A valid baseline is an SCM that makes the auxiliary task easier to measure both normally and under intervention, ensuring symmetry for proper mediation analysis.
- Core assumption: At least one valid baseline exists that satisfies both conditions in Definition 3.1
- Evidence anchors:
  - [section 3.2] "Definition 3.1 (Valid baseline). Mâ€² is a valid baseline for M if..." with the two required conditions
  - [section 4.1] "We construct valid baselines according to a counterfactual state of the world where the intermediate states are generated by executing the program according to a different set of causal dynamics"
- Break condition: If no valid baseline exists (no SCM satisfying both conditions), the mediation analysis cannot be performed and the framework fails.

### Mechanism 3
- Claim: Separating bound and free latent variable outcomes enables fine-grained interpretation
- Mechanism: Bound outcomes are fully determined by training data, while free outcomes represent unseen data. This separation distinguishes deductive knowledge (consistency with observed data) from inductive knowledge (generalization to unseen data).
- Core assumption: Training corpus covers all combinations of exogenous variables, making some latent variables bound while others remain free
- Evidence anchors:
  - [section 3.1] "The bound latent variables are s6 to s10 (they are observed during training as the final state). The free latent variables are s1 to s5 and s11 to s15."
  - [section 3.1] "In particular, when calibration and measurement occur on the same split, the probe quantifies the knowledge... conversely, probing with different splits measures the transferability"
- Break condition: If training corpus doesn't cover all exogenous variable combinations, the bound/free distinction becomes ambiguous and interpretation breaks down.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: Provides formal framework for representing causal relationships in text generation and enables causal mediation analysis
  - Quick check question: What are the three types of variables in an SCM and how do they relate to the grid-world navigation task?

- Concept: Path-specific effects and causal mediation analysis
  - Why needed here: Enables decomposition of total causal effect into contributions from different causal paths, isolating LM's contribution from probe's contribution
  - Quick check question: How does the necessary indirect effect formula capture the contribution of the LM's representations to auxiliary task performance?

- Concept: Markovian assumptions and causal graphs
  - Why needed here: Ensures causal graph is acyclic and allows proper application of do-calculus and intervention analysis
  - Quick check question: Why is it important that the underlying causal graph is Markovian (acyclic) for this analysis to work?

## Architecture Onboarding

- Component map: Language Model -> Probe -> Auxiliary Dataset -> SCM -> Valid Baseline
- Critical path:
  1. Train LM on grid-world program corpus
  2. Construct auxiliary dataset with bound and free latent variables
  3. Train probe on calibration split
  4. Measure accuracy on measurement split
  5. Construct valid baseline SCM
  6. Compute mediated effects using the baseline
  7. Interpret results as evidence for LM learning latent concepts
- Design tradeoffs:
  - Probe complexity vs. interpretability: Simpler probes may better isolate LM contributions but might miss complex representations
  - Dataset size vs. computation: Larger datasets improve reliability but increase training time
  - Baseline selection vs. validity: More diverse baselines improve robustness but may violate validity conditions
- Failure signatures:
  - Negative or near-zero mediated effects despite high raw accuracy: Probe is learning task independently of LM
  - Asymmetric baseline conditions: Valid baseline doesn't satisfy both required inequalities
  - High variance in measurements: Insufficient data or unstable training
- First 3 experiments:
  1. Train linear probe on bound split and measure raw accuracy
  2. Construct valid baseline by permuting action semantics and verify baseline conditions
  3. Compute mediated effect for deductive knowledge and check if positive

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend the causal mediation analysis framework to handle non-Markovian SCMs or cyclic dependencies in the data generation process?
- Basis in paper: [inferred] The paper assumes Markovian (acyclic) SCMs as a standard assumption of causal analysis, but this limitation is not explicitly addressed or resolved.
- Why unresolved: The paper focuses on Markovian SCMs and does not explore scenarios where the underlying causal structure might be cyclic or non-Markovian, which could be relevant for certain types of natural language data or more complex generative processes.
- What evidence would resolve it: Developing and testing the framework on synthetic datasets with known cyclic dependencies or non-Markovian structure, demonstrating that causal mediation analysis can still provide meaningful insights and control for probe contributions.

### Open Question 2
- Question: What are the theoretical guarantees and limitations of using different probe architectures (e.g., linear vs. deep neural networks) for measuring latent concepts in language models?
- Basis in paper: [inferred] The paper observes that deeper probes generally achieve higher accuracy but does not provide a rigorous theoretical analysis of how probe architecture affects the reliability and interpretability of probing results.
- Why unresolved: While the paper presents empirical evidence that probe architecture matters, it does not establish formal conditions under which different probe architectures provide valid and comparable measurements of latent concepts.
- What evidence would resolve it: Formal proofs or empirical studies demonstrating conditions under which probe architecture choice does or does not affect the validity of causal mediation analysis results, potentially building on information-theoretic probing frameworks.

### Open Question 3
- Question: How can the framework be extended to handle cases where the auxiliary task has only one "right" answer, making it difficult to find a valid baseline for causal mediation analysis?
- Basis in paper: [explicit] The paper acknowledges that for non-causal latent variables like part-of-speech, finding multiple SCMs that explain the data equally well may not be possible, which is necessary for their mediation technique.
- Why unresolved: The paper identifies this as a fundamental challenge but does not propose solutions for extending the framework to tasks where counterfactual data generation is not straightforward.
- What evidence would resolve it: Developing alternative approaches to causal mediation analysis that do not require counterfactual data generation, or identifying specific classes of auxiliary tasks where the framework can still be applied meaningfully despite having only one "right" answer.

## Limitations
- The framework requires finding valid baselines, which may not exist for all tasks or model architectures
- Assumes the training corpus covers all exogenous variable combinations, which may not hold in real-world scenarios
- Synthetic grid-world task may not capture full complexity of natural language
- The valid baseline assumption is critical and may not always be satisfiable

## Confidence
- High confidence in the mathematical framework and causal mediation analysis methodology
- Medium confidence in the empirical results due to synthetic task limitations
- Medium confidence in the probe interpretation claims, as they depend on the valid baseline assumption

## Next Checks
1. Test the framework on natural language tasks with established latent variables (e.g., sentiment analysis) to verify scalability beyond synthetic domains
2. Experiment with multiple valid baselines per task to assess robustness of mediated effect measurements
3. Evaluate how probe architecture choices (linear vs. MLP) affect the ability to isolate LM contributions across different types of latent concepts