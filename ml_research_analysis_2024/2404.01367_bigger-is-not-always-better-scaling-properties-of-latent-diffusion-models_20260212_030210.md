---
ver: rpa2
title: 'Bigger is not Always Better: Scaling Properties of Latent Diffusion Models'
arxiv_id: '2404.01367'
source_url: https://arxiv.org/abs/2404.01367
tags:
- sampling
- diffusion
- performance
- cost
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how model size affects sampling efficiency
  in latent diffusion models (LDMs) for text-to-image generation. Through training
  12 models ranging from 39M to 5B parameters on a 600M image-text dataset, the authors
  find that smaller models often outperform larger ones under constrained sampling
  budgets.
---

# Bigger is not Always Better: Scaling Properties of Latent Diffusion Models

## Quick Facts
- arXiv ID: 2404.01367
- Source URL: https://arxiv.org/abs/2404.01367
- Reference count: 40
- This study investigates how model size affects sampling efficiency in latent diffusion models (LDMs) for text-to-image generation.

## Executive Summary
This study investigates how model size affects sampling efficiency in latent diffusion models (LDMs) for text-to-image generation. Through training 12 models ranging from 39M to 5B parameters on a 600M image-text dataset, the authors find that smaller models often outperform larger ones under constrained sampling budgets. Specifically, at equivalent inference costs, smaller models achieve better FID scores and comparable visual quality. This trend holds across different diffusion samplers (DDIM, DDPM, DPM-Solver++), downstream tasks (super-resolution, DreamBooth), and even post-distilled models. The findings suggest that smaller, more efficient models can be preferable for practical applications with limited computational resources, opening new pathways for LDM scaling strategies that prioritize sampling efficiency.

## Method Summary
The authors trained 12 LDM models with parameter counts ranging from 39M to 5B on a 600M image-text dataset. Each model used the same architecture except for the denoising U-Net, which scaled from 64 to 768 channels. Models were trained for 500K steps with batch size 2048 and learning rate 1e-4. Text-to-image generation was evaluated using COCO 2014 validation set with 50-step DDIM sampling and CFG rate of 7.5. Downstream super-resolution tasks used DIV2K validation set with RealESRGAN degradation. The study compared FID and CLIP scores across model sizes at equivalent sampling costs, testing different diffusion samplers and examining pretraining vs fine-tuning effects.

## Key Results
- Smaller models frequently outperform larger models across a range of sampling costs in terms of FID scores
- Pretraining performance scales with training compute and has greater influence on downstream task performance than fine-tuning duration
- Distilled models outperform undistilled models at the same sampling cost, but smaller undistilled models can achieve similar performance to larger distilled models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models can outperform larger models under constrained sampling budgets due to computational redundancy in larger models
- Mechanism: When sampling steps are limited, the larger model's additional parameters don't have enough iterations to fully utilize their representational capacity, while smaller models can converge faster to good solutions within the same budget
- Core assumption: The relationship between model size and sampling efficiency is non-monotonic and depends on the available sampling steps
- Evidence anchors:
  - [abstract] "smaller models frequently outperform their larger equivalents in generating high-quality results" when operating under a given inference budget
  - [section] "smaller models frequently outperform larger models across a range of sampling cost in terms of FID scores"
  - [corpus] Weak evidence - corpus contains related but not directly supporting papers
- Break condition: When sampling steps are sufficiently large to allow larger models to fully utilize their capacity

### Mechanism 2
- Claim: Pretraining performance scales with training compute, creating a foundation that influences downstream task performance more than fine-tuning duration
- Mechanism: Larger models trained with more compute develop richer representations during pretraining that provide a stronger starting point for downstream tasks, making additional fine-tuning less impactful than pretraining quality
- Core assumption: Pretraining establishes fundamental capabilities that cannot be fully compensated for by later fine-tuning
- Evidence anchors:
  - [section] "larger models achieve superior results even after short finetuning periods compared to smaller models"
  - [section] "pretraining performance (dominated by the pretraining model sizes) has a greater influence on the super-resolution FID scores than the duration of finetuning"
  - [corpus] No direct supporting evidence in corpus
- Break condition: When downstream task data is extremely large and diverse, potentially allowing smaller models to catch up

### Mechanism 3
- Claim: Diffusion distillation maintains scaling efficiency trends because it preserves the fundamental relationship between model size and representational efficiency
- Mechanism: Distillation transfers knowledge while maintaining the core architectural efficiency properties, so smaller distilled models retain their relative advantage under sampling constraints
- Core assumption: The distillation process preserves the efficiency characteristics established during pretraining
- Evidence anchors:
  - [section] "distilled models outperform undistilled models at the same sampling cost" but "at the specific sampling cost, the smaller undistilled model still achieves similar performance to the larger distilled model"
  - [section] "distilled models using only 4 sampling steps achieve FID scores comparable to undistilled models using significantly more steps"
  - [corpus] No direct supporting evidence in corpus
- Break condition: When distillation methodology fundamentally alters the architectural properties

## Foundational Learning

- Concept: Diffusion sampling process and its relationship to inference cost
  - Why needed here: Understanding how sampling steps multiply with model size to create total inference cost is critical for interpreting the efficiency results
  - Quick check question: If a 39M parameter model takes 20 steps and a 5B parameter model takes 5 steps, which has higher total inference cost assuming normalized costs from Table 1?

- Concept: Classifier-free guidance and its role in balancing visual fidelity vs semantic alignment
- Why needed here: The paper uses optimal CFG rate selection as part of the efficiency analysis, so understanding how CFG affects quality is essential
- Quick check question: If increasing CFG rate improves semantic alignment but degrades visual quality, how would this affect the selection of optimal CFG for efficiency comparisons?

- Concept: Pretraining vs fine-tuning dynamics and their relative impact on downstream performance
- Why needed here: The paper argues that pretraining quality dominates downstream performance, which is a key insight for understanding the scaling properties
- Quick check question: If two models have identical fine-tuning performance on a downstream task, but different pretraining performance, which would you expect to generalize better to new downstream tasks?

## Architecture Onboarding

- Component map: Text encoder (1.4B) → Latent encoder/decoder (250M) → Denoising UNet (39M-5B) → Diffusion samplers → Image output
- Critical path: Text encoding → Latent encoding → Noise prediction through UNet → Sampling loop → Latent decoding → Image output
- Design tradeoffs: Model size vs sampling efficiency vs downstream task performance; computational cost vs visual quality; pretraining investment vs fine-tuning flexibility
- Failure signatures: Poor sampling efficiency despite adequate model size (indicates suboptimal architecture scaling); downstream performance gap despite similar pretraining (indicates task-specific limitations); inconsistent CFG optimization (indicates sensitivity to sampling parameters)
- First 3 experiments:
  1. Reproduce Fig. 9: Compare FID scores across model sizes at equivalent sampling costs to verify the non-monotonic efficiency relationship
  2. Test downstream task scaling: Fine-tune multiple model sizes on super-resolution and compare performance to validate pretraining dominance
  3. Validate distillation effects: Apply diffusion distillation to different model sizes and compare efficiency under constrained sampling to confirm trend preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do scaling trends change when applying different network architectures beyond the standard U-Net, such as transformer-based backbones or cascaded diffusion models?
- Basis in paper: [inferred] The paper explicitly notes that claims regarding scalability are specific to the studied LDM family and suggests extending analysis to other model families like DiT, SiT, MM-DiT, DiS, Imagen3, and Stable Cascade would be valuable.
- Why unresolved: The paper only investigates a specific U-Net based LDM architecture, and the authors acknowledge that scaling properties might differ for other architectural choices that are becoming increasingly popular.
- What evidence would resolve it: Empirical studies comparing scaling properties across different network architectures (transformer-based vs U-Net, cascaded vs single-step) trained under similar conditions, measuring both sampling efficiency and generative quality metrics.

### Open Question 2
- Question: What is the relationship between model size, sampling efficiency, and the complexity of downstream tasks beyond image super-resolution and DreamBooth?
- Basis in paper: [explicit] The paper investigates scaling sampling-efficiency on downstream tasks (super-resolution and DreamBooth) but acknowledges this is limited to specific tasks, suggesting broader applicability needs investigation.
- Why unresolved: The study only examines two specific downstream tasks, and it's unclear whether the observed sampling-efficiency trends generalize to more complex or different types of tasks like inpainting, colorization, or 3D generation.
- What evidence would resolve it: Systematic evaluation of scaled models across a diverse range of downstream tasks with varying complexity, comparing both sampling efficiency and task-specific performance metrics.

### Open Question 3
- Question: How do scaling properties and sampling efficiency trends change when training on datasets with different characteristics (size, diversity, aesthetic quality)?
- Basis in paper: [explicit] The authors trained models on a specific web-scale aesthetically filtered dataset (WebLI) and acknowledge that scaling properties might be dataset-dependent, suggesting investigation on different datasets.
- Why unresolved: The study uses a specific filtered dataset, and it's unclear whether the observed scaling trends (particularly the efficiency advantage of smaller models) would hold for unfiltered, smaller, or differently distributed datasets.
- What evidence would resolve it: Training scaled models on multiple datasets with varying characteristics (size, diversity, quality filtering) and comparing both pretraining performance and sampling efficiency trends across these different data regimes.

### Open Question 4
- Question: What is the optimal strategy for balancing model size and inference budget in practical deployment scenarios with varying computational constraints?
- Basis in paper: [inferred] The paper demonstrates that smaller models often outperform larger ones under constrained inference budgets, but doesn't provide concrete guidelines for practical deployment scenarios with different resource constraints.
- Why unresolved: While the paper shows that smaller models can be more efficient, it doesn't establish clear decision frameworks for when to choose which model size based on specific deployment constraints like latency requirements, hardware limitations, or cost considerations.
- What evidence would resolve it: Development of decision frameworks or cost-performance curves that map specific deployment constraints (latency, hardware, budget) to optimal model size choices, validated across multiple application scenarios.

## Limitations
- Sampling step equivalence assumptions may not precisely map parameter count to computational cost across the full range
- Results based on a specific 600M image-text dataset with aesthetics filtering may not generalize to different data distributions
- Downstream task evaluation limited to super-resolution and DreamBooth, may not extend to all LDM applications

## Confidence
- High confidence: The core finding that smaller models can outperform larger ones under sampling constraints is well-supported by systematic experiments across multiple model sizes and sampling methods
- Medium confidence: The pretraining dominance hypothesis is supported but could benefit from additional ablation studies isolating pretraining vs fine-tuning effects
- Medium confidence: The distillation findings are consistent but rely on external work for the distillation methodology, introducing potential variability

## Next Checks
1. Conduct controlled experiments to precisely map the relationship between parameter count, sampling steps, and total inference cost across the 39M-5B parameter range
2. Test the efficiency scaling properties on alternative datasets with different characteristics (e.g., LAION-5B, proprietary datasets) to assess generalizability
3. Evaluate the scaling trends on additional downstream tasks including inpainting, outpainting, and video generation to identify task-specific variations in the efficiency relationship