---
ver: rpa2
title: Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning
arxiv_id: '2402.15017'
source_url: https://arxiv.org/abs/2402.15017
tags:
- finetuning
- task
- tasks
- lsup
- multitask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of multitask finetuning
  for adapting pretrained foundation models to new tasks with limited labels. The
  authors show that with a diverse set of relevant tasks, multitask finetuning can
  improve the prediction performance on a target task compared to directly adapting
  the pretrained model.
---

# Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning

## Quick Facts
- arXiv ID: 2402.15017
- Source URL: https://arxiv.org/abs/2402.15017
- Reference count: 40
- Primary result: Multitask finetuning with diverse, consistent tasks improves few-shot target task performance over direct adaptation

## Executive Summary
This paper presents a theoretical analysis of multitask finetuning for adapting pretrained foundation models to new tasks with limited labels. The authors show that with a diverse set of relevant tasks, multitask finetuning can improve the prediction performance on a target task compared to directly adapting the pretrained model. They quantify the relationship between finetuning tasks and target tasks using diversity and consistency metrics, and propose a practical task selection algorithm. The theoretical claims are substantiated with extensive empirical evidence on vision and language tasks, demonstrating that the proposed task selection algorithm adeptly chooses related finetuning tasks, providing advantages to model performance on target tasks.

## Method Summary
The paper analyzes multitask finetuning as a method to adapt pretrained foundation models to new target tasks with limited labeled data. The approach involves finetuning the pretrained model on multiple auxiliary tasks, then adapting to the target task using a nearest centroid classifier. A theoretical framework establishes conditions under which multitask finetuning reduces target task error, based on diversity and consistency metrics. The authors propose a practical task selection algorithm that greedily selects finetuning tasks maximizing both cosine similarity (consistency) and coverage (diversity) relative to the target task.

## Key Results
- Multitask finetuning with diverse, consistent tasks outperforms direct adaptation on few-shot target tasks
- Task selection based on consistency and diversity metrics improves performance over random or full-task selection
- Total number of samples across all finetuning tasks (M × m) is more important than the number of tasks or samples per task individually
- Empirical results demonstrate advantages on vision tasks (miniImageNet, tieredImageNet) and language tasks (DomainNet, Meta-Dataset)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask finetuning improves target task performance when finetuning tasks are both diverse and consistent with the target.
- Mechanism: Diverse finetuning tasks ensure coverage of the feature space relevant to the target, while consistency ensures that the finetuning tasks share underlying patterns with the target. Together, they reduce the expected supervised loss on the target.
- Core assumption: The diversity and consistency parameters (ν and κ) can be bounded based on the overlap of latent features between finetuning and target tasks.
- Evidence anchors:
  - [abstract]: "Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task"
  - [section]: Theorem 3.2 and Theorem 3.3 provide formal bounds on the target task error in terms of diversity (ν) and consistency (κ)
  - [corpus]: Weak or missing. No direct evidence from corpus neighbors.
- Break condition: If either diversity is too low (ν → 0) or consistency is poor (κ is large), the error bound becomes vacuous and performance degrades.

### Mechanism 2
- Claim: The total number of samples across all finetuning tasks (M × m) is more important than the number of tasks or samples per task individually.
- Mechanism: The theoretical sample complexity bounds depend on the total number of samples, not on the distribution of samples across tasks. Empirically, performance scales with total sample size.
- Core assumption: Sample complexity requirements are additive across tasks, so increasing either M or m while keeping M × m constant yields similar performance.
- Evidence anchors:
  - [section]: "Figure 3a shows no major change in accuracy with varying the number of shots in finetuning" and "Figure 3b and 3c show that performance depends on total sample size M × m"
  - [abstract]: "We substantiate our theoretical claims with extensive empirical evidence"
  - [corpus]: Weak or missing. No direct evidence from corpus neighbors.
- Break condition: If the sample complexity requirements are not met (e.g., too few total samples), the generalization bounds no longer hold and performance plateaus or degrades.

### Mechanism 3
- Claim: Task selection based on consistency and diversity improves performance over random or full-task selection.
- Mechanism: A greedy algorithm selects tasks that maximize cosine similarity (consistency) and coverage (diversity) relative to the target task. This ensures that the finetuning data is both relevant and representative.
- Core assumption: Cosine similarity and coverage score are valid proxies for the true consistency and diversity metrics.
- Evidence anchors:
  - [section]: "Table 1 compares the results from finetuning with tasks selected by our algorithm to those from finetuning with tasks selected by other methods"
  - [abstract]: "We further propose a practical task selection algorithm"
  - [corpus]: Weak or missing. No direct evidence from corpus neighbors.
- Break condition: If the proxy metrics do not correlate with true diversity/consistency, the algorithm may select suboptimal tasks and fail to improve performance.

## Foundational Learning

- Concept: Contrastive learning and masked language modeling as pretraining methods.
  - Why needed here: The theoretical framework applies to both contrastive and supervised pretraining; understanding these methods is essential to interpret the results.
  - Quick check question: What is the difference between contrastive loss and supervised loss in terms of data generation?

- Concept: Diversity and consistency metrics in multitask learning.
  - Why needed here: These metrics formalize the conditions under which multitask finetuning helps. They are central to the theoretical analysis and task selection algorithm.
  - Quick check question: How do ν (diversity) and κ (consistency) affect the bound on target task error?

- Concept: Sample complexity and Rademacher complexity.
  - Why needed here: The theoretical guarantees depend on bounding the Rademacher complexity of the function class, which determines how many samples are needed for generalization.
  - Quick check question: Why does increasing M or m improve performance when the total sample size M × m is held constant?

## Architecture Onboarding

- Component map: Pretrained encoder -> Multitask finetuning (auxiliary tasks) -> Target task adaptation (nearest centroid)
- Critical path:
  1. Load pretrained encoder
  2. Construct finetuning tasks (or use task selection to choose subset)
  3. Run multitask finetuning (update all encoder parameters)
  4. Adapt to target task (retain encoder, apply nearest centroid)
- Design tradeoffs:
  - Full multitask finetuning vs. linear probing: full tuning updates all parameters but is more expensive; linear probing is cheaper but may underfit.
  - Task selection vs. full task set: selection reduces compute and may improve performance if diversity/consistency are poor; full set is simpler but may include noisy tasks.
- Failure signatures:
  - Performance plateaus despite increasing M or m: sample complexity requirements not met.
  - Performance degrades with multitask finetuning: tasks are inconsistent or not diverse enough.
  - Task selection fails: proxy metrics (cosine similarity, coverage) do not correlate with true diversity/consistency.
- First 3 experiments:
  1. Direct adaptation (no finetuning) on a few-shot target task to establish baseline.
  2. Multitask finetuning with all available tasks on same target task.
  3. Multitask finetuning with task selection algorithm on same target task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity parameter ν and consistency parameter κ relate to the number of shared features (lC) and the number of disjoint features (n′C) between the finetuning tasks and the target task in the linear case study?
- Basis in paper: Explicit. The paper provides a theoretical analysis of a linear case study that establishes a relationship between diversity, consistency, and the number of shared and disjoint features.
- Why unresolved: While the paper establishes the relationship, it does not provide a quantitative formula for how ν and κ change with varying lC and n′C.
- What evidence would resolve it: Further analysis of the linear case study, potentially extending the analysis to more complex data models, could provide quantitative relationships between ν, κ, lC, and n′C.

### Open Question 2
- Question: How does the choice of finetuning tasks affect the diversity and consistency of the finetuning data in practice?
- Basis in paper: Explicit. The paper proposes a task selection algorithm that aims to select tasks with high diversity and consistency.
- Why unresolved: The paper does not provide a comprehensive evaluation of the task selection algorithm across different datasets and model architectures.
- What evidence would resolve it: Extensive experiments evaluating the task selection algorithm on a wider range of datasets and model architectures would provide insights into its effectiveness and limitations.

### Open Question 3
- Question: How does the number of shots in the finetuning tasks affect the performance of multitask finetuning?
- Basis in paper: Explicit. The paper presents experimental results showing that increasing the number of shots in the finetuning tasks can improve performance.
- Why unresolved: The paper does not provide a theoretical analysis of the relationship between the number of shots and the performance of multitask finetuning.
- What evidence would resolve it: A theoretical analysis of the impact of the number of shots on the diversity and consistency of the finetuning data, as well as its effect on the generalization performance, would provide a deeper understanding of this phenomenon.

## Limitations

- Sample complexity scaling depends on diversity and consistency metrics that are not directly measurable in practice
- Transferability across domains may break down when underlying feature representations are fundamentally different
- Finite-sample behavior in the few-shot regime may deviate from theoretical predictions

## Confidence

- High Confidence: The empirical demonstration that multitask finetuning outperforms direct adaptation on few-shot target tasks, particularly when using the proposed task selection algorithm
- Medium Confidence: The theoretical relationship between diversity/consistency metrics and target task error
- Low Confidence: The generalizability of the task selection algorithm to arbitrary task distributions and foundation models

## Next Checks

1. **Diversity-Consistency Sensitivity Analysis**: Systematically vary the diversity and consistency of finetuning tasks to empirically validate how these parameters affect target task performance, comparing results against theoretical predictions.

2. **Cross-Domain Transfer Validation**: Test the task selection algorithm and multitask finetuning framework when transferring between substantially different domains to assess the limits of the shared feature space assumption.

3. **Finite-Sample Bound Validation**: Conduct controlled experiments varying the number of shots and total samples to empirically map the relationship between sample size and performance, comparing observed scaling behavior against theoretical sample complexity bounds.