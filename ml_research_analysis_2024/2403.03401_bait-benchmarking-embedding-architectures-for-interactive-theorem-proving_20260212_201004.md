---
ver: rpa2
title: 'BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving'
arxiv_id: '2403.03401'
source_url: https://arxiv.org/abs/2403.03401
tags:
- learning
- tactic
- embedding
- proof
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BAIT is introduced as the first cross-platform framework for benchmarking
  AI methods in Interactive Theorem Proving (ITP). It addresses the challenge of fragmented
  research across different ITP systems by providing a unified platform for fair comparison
  of learning approaches.
---

# BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving

## Quick Facts
- arXiv ID: 2403.03401
- Source URL: https://arxiv.org/abs/2403.03401
- Authors: Sean Lamont; Michael Norrish; Amir Dezfouli; Christian Walder; Paul Montague
- Reference count: 6
- Primary result: Structure-Aware Transformers outperform GNNs and Transformers on most ITP benchmarks, with ensembles further improving performance

## Executive Summary
BAIT introduces the first cross-platform framework for benchmarking AI methods in Interactive Theorem Proving (ITP). It addresses the challenge of fragmented research across different ITP systems by providing a unified platform for fair comparison of learning approaches. The framework enables researchers to compare various embedding architectures—specifically Graph Neural Networks (GNNs), Transformers, and Structure-Aware Transformers (SATs)—across multiple ITP benchmarks, including supervised and end-to-end tasks. The primary results show that Structure-Aware Transformers outperform other architectures on most benchmarks, with ensembles further improving performance.

## Method Summary
BAIT provides a unified framework for benchmarking AI methods in ITP by abstracting common components (Data, Model, Environment) and using Hydra for configuration management. The framework allows consistent implementation and logging across different ITP systems and datasets. The core method involves comparing various embedding architectures—GNNs, Transformers, and SATs—across multiple ITP benchmarks, including supervised tasks (premise selection, tactic prediction) and end-to-end proving tasks. The framework also supports pretraining embedding models on premise selection tasks and applying them to end-to-end proving systems like TACTIC ZERO.

## Key Results
- Structure-Aware Transformers outperform GNNs and Transformers on most supervised benchmarks
- Ensemble models combining GNNs and Transformers achieve the best performance on several tasks
- Applying GNN and Transformer encoders to TACTIC ZERO improves validation performance but shows mixed results on cumulative goals proven
- The BAIT framework successfully enables cross-platform comparison across multiple ITP systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure Aware Transformers (SATs) outperform GNNs and vanilla Transformers by integrating directed graph structure into attention mechanisms.
- Mechanism: SATs first pass the expression graph through a GNN to capture local structural features, then apply a Transformer Encoder layer that respects ancestor/descendant relationships, enabling better semantic understanding of logical expressions.
- Core assumption: The directed structure of logical expressions contains critical information that standard Transformers miss without explicit structural encoding.
- Evidence anchors:
  - [abstract]: "We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets."
  - [section]: "Directed SAT modifies this to restrict attention for each node to only include ancestors or descendent nodes, which has been found to improve performance for directed graph problems (Luo, Thost, and Shi 2023)."
  - [corpus]: Weak evidence; related papers focus on different benchmarks, not SAT comparisons.
- Break condition: If the directed structure does not correlate with semantic meaning in the target ITP domain, the attention restriction becomes a bottleneck.

### Mechanism 2
- Claim: BAIT enables fair cross-platform comparison by unifying diverse ITP systems under a single experimental framework.
- Mechanism: BAIT abstracts common components (Data, Model, Environment) and uses Hydra for configuration management, allowing consistent implementation and logging across different ITP systems and datasets.
- Core assumption: The underlying components of AI-ITP systems are similar enough to be abstracted into common interfaces.
- Evidence anchors:
  - [abstract]: "BAIT is introduced as the first cross-platform framework for benchmarking AI methods in Interactive Theorem-Proving."
  - [section]: "BAIT brings together several environments, datasets and models in AI-ITP, with a central interface for experiments and sharing components between systems."
  - [corpus]: Weak evidence; related papers discuss individual ITP systems but not unified benchmarking platforms.
- Break condition: If specific ITP systems have fundamentally incompatible representations or interaction patterns, the abstraction breaks.

### Mechanism 3
- Claim: Pretraining embedding models on premise selection improves end-to-end proving performance.
- Mechanism: By pretraining GNN and Transformer encoders on HOL4 premise selection (excluding goals used in end-to-end evaluation), the models learn useful semantic representations that generalize to unseen goals in reinforcement learning scenarios.
- Core assumption: Premise selection is a proxy task that captures relevant semantic patterns for broader theorem proving.
- Evidence anchors:
  - [section]: "We pretrain these models on the HOL4 premise selection task, ensuring goals used for this problem were excluded in the data generation."
  - [section]: "The original TACTIC ZERO result reports 49.2% of goals proven, compared to our observed 43%. Since we train for 200 additional epochs, it is possible that these additional epochs cause the model to overfit further."
  - [corpus]: Weak evidence; related papers do not discuss pretraining strategies for end-to-end theorem proving.
- Break condition: If the distribution of expressions in premise selection differs significantly from end-to-end proving goals, pretraining may introduce harmful biases.

## Foundational Learning

- Concept: Graph Neural Networks (MPNN architecture)
  - Why needed here: GNNs are the current state-of-the-art for graph-based formula embeddings in ITP, forming the baseline for comparison.
  - Quick check question: What is the difference between message passing from parents vs children in an expression graph, and why does this matter for logical formulas?

- Concept: Transformer Encoder with positional encodings
  - Why needed here: Transformers are the state-of-the-art for sequence-based formula embeddings and are compared directly against GNNs.
  - Quick check question: How do sinusoidal positional encodings help Transformers understand the structure of s-expressions in logical formulas?

- Concept: Structure-Aware Attention
  - Why needed here: SAT models extend Transformers to incorporate graph structure, which is crucial for understanding the semantic relationships in logical expressions.
  - Quick check question: How does restricting attention to ancestor/descendant nodes in a directed acyclic graph improve semantic understanding compared to standard self-attention?

## Architecture Onboarding

- Component map: Data -> Environment -> Model -> Experiment
- Critical path:
  1. Preprocess raw proof data into standardized format
  2. Configure experiment with chosen model, data, and environment
  3. Train model using supervised or reinforcement learning
  4. Evaluate on benchmark tasks (premise selection, end-to-end proving)
  5. Log results and analyze embeddings qualitatively

- Design tradeoffs:
  - GNNs vs Transformers: GNNs capture local graph structure well but are computationally heavier; Transformers scale better but need positional encodings for structure
  - SAT vs ensemble: SAT integrates structure more naturally but is less efficient; ensembles combine strengths but increase complexity
  - Pretraining vs from-scratch: Pretraining helps generalization but requires careful dataset separation

- Failure signatures:
  - Poor validation performance despite good training: likely overfitting, check regularization and dataset size
  - Similar performance across architectures: may indicate the task doesn't benefit from complex models, check if simpler baselines are sufficient
  - Embedding nearest neighbors show no semantic similarity: indicates the model isn't learning meaningful representations, check preprocessing and architecture implementation

- First 3 experiments:
  1. Implement and verify GNN embedding on HOLStep premise selection using the provided data module
  2. Add Transformer Encoder baseline to the same experiment, ensuring identical preprocessing
  3. Extend to SAT implementation and compare all three architectures on HOList tactic prediction task

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but identifies several areas for future work including investigating transfer tasks between systems and exploring different graph preprocessing methods.

## Limitations
- The framework's abstractions may not hold across fundamentally different ITP systems with incompatible representations or interaction patterns
- The observed SAT advantages may not generalize beyond the specific datasets tested
- The new HOL4 premise selection dataset generation process is not fully specified, limiting reproducibility

## Confidence

High: BAIT provides a unified benchmarking framework for ITP systems - directly demonstrated through implementation and results

Medium: Comparative performance claims between GNNs, Transformers, and SATs - well-supported by multiple benchmarks but could vary with different datasets or hyperparameter choices

Low: Pretraining effectiveness claim - TACTIC ZERO results show mixed outcomes (better validation but worse cumulative proving)

## Next Checks

1. Reproduce the SAT vs Transformer performance gap on HOList using different random seeds to verify the 5-8% improvement is consistent
2. Test whether the pretraining benefit generalizes by evaluating the HOL4-pretrained models on a held-out end-to-end proving dataset not used in the paper
3. Implement a minimal version of the BAIT framework for two ITP systems to verify the abstraction truly enables fair cross-platform comparison