---
ver: rpa2
title: Preserving Empirical Probabilities in BERT for Small-sample Clinical Entity
  Recognition
arxiv_id: '2409.03238'
source_url: https://arxiv.org/abs/2409.03238
tags:
- entity
- labels
- token
- clinical
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of class imbalance in clinical
  named entity recognition (NER), where certain entity types are overrepresented and
  others underrepresented, leading to biased models. The authors propose a Binary
  Token Labels (BTL) method that fine-tunes BERT by splitting training batches into
  multiple copies, each containing only one true-positive entity class alongside the
  negative class 'O'.
---

# Preserving Empirical Probabilities in BERT for Small-sample Clinical Entity Recognition

## Quick Facts
- arXiv ID: 2409.03238
- Source URL: https://arxiv.org/abs/2409.03238
- Authors: Abdul Rehman; Jian Jun Zhang; Xiaosong Yang
- Reference count: 8
- One-line primary result: BTL approach achieved 72.66% unweighted accuracy and 76.1% mean F1-score, outperforming ATL baseline of 64.87% and 75.1% respectively

## Executive Summary
This paper addresses class imbalance in clinical named entity recognition (NER) where certain entity types are overrepresented while others are underrepresented. The authors propose a Binary Token Labels (BTL) method that fine-tunes BERT by splitting training batches into multiple copies, each containing only one true-positive entity class alongside the negative class 'O'. This approach contrasts with the conventional All Token Labels (ATL) method that uses all entity classes simultaneously in each batch. The proposed method improves the model's ability to recognize underrepresented entities by reducing loss dilution effects.

## Method Summary
The method involves fine-tuning BioBERT v1.1 using weighted cross-entropy loss with class weights inversely proportional to class frequency. The key innovation is the BTL approach that splits training batches into multiple copies, each containing only one true-positive entity class alongside the negative class 'O'. A KNN classifier (17 neighbors) is then used post-training to predict final entity labels from BERT's raw logits. The model is trained for 20 epochs using an SGD optimizer with a learning rate of 5e-5.

## Key Results
- BTL achieved 72.66% unweighted accuracy compared to ATL baseline of 64.87%
- BTL achieved 76.1% mean F1-score compared to ATL baseline of 75.1%
- The method specifically improved recognition of underrepresented entity types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting training batches into multiple copies with only one true-positive entity class per batch reduces loss dilution and improves minority class learning.
- Mechanism: By isolating each entity class into separate binary classification tasks against the negative class 'O', the model focuses on distinguishing presence/absence of that specific entity type. This prevents majority classes from dominating the loss landscape and allows underrepresented classes to receive adequate gradient updates.
- Core assumption: The model can effectively learn entity presence/absence through binary classification, and this binary distinction is more informative than multi-class classification when dealing with severe class imbalance.
- Evidence anchors:
  - [abstract] "We propose a Binary Token Labels (BTL) method that fine-tunes BERT by splitting training batches into multiple copies, each containing only one true-positive entity class alongside the negative class 'O'."
  - [section] "The problem of deeper layers being oblivious to the non-variant loss at the final layer can be solved by having only two classes creating a higher variation in loss so that it can backpropagated deeper into the model."
  - [corpus] No direct corpus evidence for this mechanism; relies on theoretical reasoning and experimental results.
- Break condition: If binary classification fails to capture the nuanced differences between entity types, or if the overhead of processing multiple batch copies becomes prohibitive.

### Mechanism 2
- Claim: Using weighted cross-entropy with class weights inversely proportional to class frequency helps balance the loss contribution from minority and majority classes.
- Mechanism: Class weights are calculated as wc = 1 - Nc/N, where Nc is the number of tokens for class c and N is the total number of tokens. This ensures that minority classes with fewer samples receive higher weights in the loss calculation, forcing the model to pay more attention to them during training.
- Core assumption: The class frequency distribution in the training data is representative of the importance of each class, and that inverse weighting appropriately balances the learning process.
- Evidence anchors:
  - [section] "The weighted cross-entropy loss at the token level is calculated as ln = -wyn * exp(xn,yn) / PC c=1 exp(xn,c)" and "wc = 1 - Nc/N"
  - [section] "We create a clinical NER method with the following measures: â€¢ Weighted cross-entropy using the class weights as calculated in Section 3."
  - [corpus] No direct corpus evidence; this is a standard technique in imbalanced learning literature.
- Break condition: If class weights are not properly normalized or if the distribution is too skewed, leading to overfitting on minority classes.

### Mechanism 3
- Claim: Using a KNN classifier on raw logits from the BERT output layer improves entity label prediction by leveraging local similarity in the embedding space.
- Mechanism: After fine-tuning BERT, a separate KNN classifier with 17 neighbors is trained on the logits from the output layer. This classifier uses the spatial relationships between logits to predict the final entity label, potentially capturing patterns that the BERT classification layer alone might miss.
- Core assumption: The logits from BERT's output layer contain sufficient information about entity classes, and that local neighborhoods in this space are meaningful for classification.
- Evidence anchors:
  - [section] "A latent KNN classifier (17 neighbours) to predict the final entity label using the raw logits from the output layer. The KNN classifier is trained separately after finetuning."
  - [section] "The utilization of K-Nearest Neighbors (KNN) serves the purpose of additional independent calibration post finetuning."
  - [corpus] No direct corpus evidence; this is an additional post-processing step not commonly used in BERT-based NER.
- Break condition: If the logits do not contain sufficient discriminative information, or if the KNN classifier overfits to the training data.

## Foundational Learning

- Concept: Imbalanced learning and class imbalance problems
  - Why needed here: The paper addresses severe class imbalance in clinical NER, where some entity types are vastly underrepresented compared to others.
  - Quick check question: What is the primary challenge when training models on datasets with significant class imbalance?

- Concept: Cross-entropy loss and its limitations with imbalanced data
  - Why needed here: The paper critiques standard cross-entropy loss for failing to adequately handle minority classes in imbalanced datasets.
  - Quick check question: How does standard cross-entropy loss behave when applied to highly imbalanced datasets?

- Concept: Binary classification vs multi-class classification
  - Why needed here: The proposed BTL method relies on converting multi-class NER into multiple binary classification tasks.
  - Quick check question: What are the advantages and disadvantages of using binary classification instead of multi-class classification for imbalanced datasets?

## Architecture Onboarding

- Component map:
  - Clinical text passages -> BERT tokenizer -> BTL processing layer -> Weighted cross-entropy loss -> SGD optimizer -> BERT output logits -> KNN classifier -> Final entity labels

- Critical path:
  1. Preprocess clinical text into passages
  2. Tokenize using BERT tokenizer
  3. Apply BTL method to create segregated batches
  4. Calculate weighted cross-entropy loss
  5. Backpropagate gradients through BERT
  6. Generate logits from output layer
  7. Apply KNN classifier to predict final entity labels

- Design tradeoffs:
  - BTL increases computational cost by processing multiple batch copies but improves minority class learning
  - Weighted cross-entropy balances loss contribution but may overemphasize very rare classes
  - KNN classifier adds a post-processing step but can capture local patterns in logit space

- Failure signatures:
  - Poor performance on majority classes when BTL overemphasizes minority classes
  - Overfitting to training data if class weights are too aggressive
  - KNN classifier fails to generalize if training data is too small or noisy

- First 3 experiments:
  1. Test standard BERT fine-tuning with ATL on MACCROBAT dataset to establish baseline
  2. Implement BTL method with weighted cross-entropy and compare performance on minority classes
  3. Add KNN classifier post-processing to BTL model and evaluate impact on overall F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Binary Token Labels (BTL) approach scale to datasets with significantly more entity classes, particularly when the number of classes increases to hundreds or thousands?
- Basis in paper: [inferred] The paper tests BTL on 33 entities but discusses limitations and potential applications without addressing scalability concerns for larger class sets.
- Why unresolved: The paper does not provide analysis or experimental results for datasets with hundreds or thousands of entity classes, leaving uncertainty about BTL's effectiveness in such scenarios.
- What evidence would resolve it: Empirical studies comparing BTL performance on datasets with varying numbers of entity classes, especially those with hundreds or thousands of classes, would clarify its scalability.

### Open Question 2
- Question: What are the potential trade-offs between using BTL and ATL approaches in terms of model interpretability and explainability, especially in critical applications like clinical decision support?
- Basis in paper: [explicit] The paper mentions that BTL focuses on the core task of entity presence or absence, allowing the model to learn to distinguish entities from non-entities effectively, but does not discuss interpretability or explainability.
- Why unresolved: The paper does not address how the BTL approach affects model interpretability or explainability, which are crucial in clinical applications where understanding model decisions is important.
- What evidence would resolve it: Comparative studies evaluating the interpretability and explainability of models trained with BTL versus ATL, particularly in clinical settings, would provide insights into the trade-offs.

### Open Question 3
- Question: How does the performance of BTL compare to other advanced techniques for handling class imbalance, such as focal loss or data augmentation methods, in clinical NER tasks?
- Basis in paper: [inferred] The paper introduces BTL as a method to improve recognition of underrepresented entities but does not compare it to other advanced techniques for handling class imbalance.
- Why unresolved: The paper does not provide a comparative analysis of BTL against other state-of-the-art techniques for addressing class imbalance, leaving uncertainty about its relative effectiveness.
- What evidence would resolve it: Empirical comparisons of BTL with other advanced techniques like focal loss or data augmentation methods on clinical NER tasks would clarify its relative performance and effectiveness.

## Limitations

- Dataset Generalization: The evaluation is based on a single clinical dataset (MACCROBAT) with 200 documents and 41 entity types, which may not generalize to other clinical domains.
- Computational Overhead: The BTL method requires processing multiple copies of each training batch, significantly increasing computational cost without performance metrics provided.
- Statistical Significance: The paper reports performance improvements but does not provide statistical significance tests or confidence intervals for the reported metrics.

## Confidence

**High Confidence**: The general observation that class imbalance affects NER model performance is well-established in the literature. The theoretical motivation for using weighted cross-entropy loss is sound and supported by standard imbalanced learning literature.

**Medium Confidence**: The specific implementation of the BTL method and its superiority over ATL on the MACCROBAT dataset is supported by experimental results, but the findings are limited to one dataset and lack statistical significance testing.

**Low Confidence**: The effectiveness of the KNN post-processing step is questionable, as the improvement it provides is not clearly quantified separately from the BTL method. The paper does not provide ablation studies showing the individual contributions of weighted cross-entropy, BTL, and KNN.

## Next Checks

**Check 1**: Conduct statistical significance testing (e.g., paired t-tests with multiple comparisons correction) on the performance metrics to determine if the improvements from BTL over ATL are statistically significant, not just numerically different.

**Check 2**: Perform cross-dataset validation by testing the BTL method on at least two additional clinical NER datasets with different entity distributions and class imbalance levels to assess generalization capability.

**Check 3**: Conduct an ablation study that isolates the contributions of weighted cross-entropy, batch splitting (BTL), and KNN post-processing by evaluating each component independently and in combination to determine their individual impact on performance.