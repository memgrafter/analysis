---
ver: rpa2
title: Evaluating Multilingual Long-Context Models for Retrieval and Reasoning
arxiv_id: '2409.18006'
source_url: https://arxiv.org/abs/2409.18006
tags:
- languages
- context
- language
- tasks
- needle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces mLongRR, a new multilingual dataset for\
  \ evaluating long-context language models (LLMs) on retrieval and reasoning tasks.\
  \ The dataset covers five languages\u2014English, Vietnamese, Indonesian, Swahili,\
  \ and Somali\u2014using naturally occurring news articles."
---

# Evaluating Multilingual Long-Context Models for Retrieval and Reasoning

## Quick Facts
- arXiv ID: 2409.18006
- Source URL: https://arxiv.org/abs/2409.18006
- Authors: Ameeta Agrawal; Andy Dang; Sina Bagheri Nezhad; Rhitabrat Pokharel; Russell Scheinberg
- Reference count: 15
- Primary result: Models show significant performance degradation on multilingual long-context tasks, with accuracy dropping from ~96% in English to 36% in Somali for single needles and 0% for three needles.

## Executive Summary
This paper introduces mLongRR, a new multilingual dataset for evaluating long-context language models on retrieval and reasoning tasks. The dataset covers five languages using naturally occurring news articles and tests six models across varying context lengths and needle depths. Results show that model performance rapidly declines with longer contexts, more needles, and lower-resource languages, highlighting challenges in multilingual long-context processing.

## Method Summary
The study evaluates six language models (GPT-4, GPT-4o, Gemini-1.5, Claude-3, Yarn-7b, and Llama-3) on a synthetic "needle in a haystack" task using BBC news articles in five languages. Models are tested with context lengths ranging from 2k-64k tokens and needles placed at depths of 0%, 25%, 50%, 75%, and 100%. Retrieval tasks use single needles while reasoning tasks use 2-3 needles requiring comparison. Accuracy is measured by comparing model-extracted values to ground truth.

## Key Results
- Performance degrades significantly with longer contexts across all languages and models
- Lower-resource languages (Swahili, Somali) show higher tokenization rates and worse performance
- Reasoning tasks are more challenging than retrieval tasks, with accuracy dropping from ~80% to 40-50% as needles increase from 1 to 3
- Gemini-1.5 and GPT-4o achieve the highest accuracy (~96% in English), while lower-resource languages see performance as low as 0% for multiple needles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longer contexts and deeper needle positions increase retrieval difficulty for LLMs.
- Mechanism: The "lost in the middle" phenomenon occurs because attention-based transformers struggle to maintain precise token-level retrieval accuracy as sequence length grows, especially for middle positions.
- Core assumption: Attention mechanisms degrade with distance from token position, making retrieval accuracy drop exponentially with depth.
- Evidence anchors:
  - [abstract] "The performance rapidly declines as we increase the context lengths for all languages."
  - [section 4.2] "Performance is better in shorter contexts, or when the needle is either near the top or the bottom of the context, suggesting that the 'lost in the middle' phenomenon... extends to multilingual contexts as well."
  - [corpus] Weak - no direct mention of attention decay or positional encoding limits.

### Mechanism 2
- Claim: Lower-resource languages have higher tokenization rates, making context processing less efficient.
- Mechanism: Languages with lower resource levels fragment into more tokens per word, reducing the effective context window and increasing computational overhead for the same semantic content.
- Core assumption: Tokenization rate is inversely correlated with training data availability and language model optimization.
- Evidence anchors:
  - [section 4.3] "The results are presented in Table 2... English consistently shows the lowest tokenization rates across all models. Swahili and Somali... generally have higher tokenization rates, suggesting these are more challenging for models to process effectively."
  - [abstract] "The performance also rapidly decreases as we move from higher-resource to lower-resource languages."
  - [corpus] Weak - no mention of tokenization or fragmentation rate analysis.

### Mechanism 3
- Claim: Reasoning tasks are more challenging than retrieval tasks because they require maintaining and comparing multiple pieces of information.
- Mechanism: As the number of needles increases, the cognitive load on the model increases exponentially because it must track multiple independent facts and perform comparisons across them.
- Core assumption: LLMs have limited working memory capacity that scales sublinearly with context length.
- Evidence anchors:
  - [abstract] "The performance also rapidly decreases as we move from higher-resource to lower-resource languages... Reasoning tasks are more challenging than retrieval tasks for all languages."
  - [section 3.2.2] "In real-world applications, tasks often require not just accurate text retrieval but also the ability to reason with the recalled information."
  - [corpus] Weak - no direct mention of working memory or cognitive load in LLMs.

## Foundational Learning

- Concept: Attention mechanisms in transformer architectures
  - Why needed here: Understanding how attention degrades with sequence length is critical to explaining the "lost in the middle" phenomenon
  - Quick check question: What happens to attention scores as the distance between query and key tokens increases in a standard transformer?

- Concept: Tokenization and fragmentation rates
  - Why needed here: The study shows that lower-resource languages have higher tokenization rates, which affects model performance
  - Quick check question: How does the number of tokens per word affect the effective context window size for a given model?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding why models perform better on higher-resource languages requires knowledge of how multilingual models leverage training data distribution
  - Quick check question: Why do models trained primarily on English data tend to perform better on Vietnamese than on Somali, despite both being low-resource?

## Architecture Onboarding

- Component map: Tokenization → Context window allocation → Needle placement → Query processing → Response generation → Accuracy evaluation
- Critical path: Tokenization → Context window allocation → Needle placement → Query processing → Response generation → Accuracy evaluation
- Design tradeoffs:
  - Longer context windows vs. computational efficiency
  - Single vs. multiple needles for task complexity
  - Language-specific vs. universal tokenization schemes
  - Synthetic vs. real-world data for evaluation
- Failure signatures:
  - Accuracy drops near middle positions indicate attention degradation
  - Zero accuracy in low-resource languages suggests tokenization or training data issues
  - Performance variance across needle depths indicates positional encoding limitations
- First 3 experiments:
  1. Run a baseline retrieval task with single needle at position 0% to establish maximum accuracy
  2. Test multiple needle reasoning with n=2 at position 25% to evaluate cognitive load handling
  3. Compare tokenization rates across languages using the same model to validate fragmentation hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do long-context LLMs perform on multilingual long-context tasks when using non-Latin scripts, such as Arabic, Chinese, or Devanagari?
- Basis in paper: [inferred] The paper explicitly states that the study focused on Latin-script languages and expresses interest in expanding to other scripts in the future.
- Why unresolved: The current study only evaluated languages using the Latin script, leaving a gap in understanding how models perform with different writing systems that may have higher tokenization rates or different linguistic structures.
- What evidence would resolve it: Conducting similar experiments with languages using non-Latin scripts (e.g., Arabic, Chinese, Hindi) and comparing performance across different tokenization rates and linguistic families.

### Open Question 2
- Question: Does increasing the number of needles beyond three continue to increase task complexity for long-context LLMs in multilingual settings?
- Basis in paper: [explicit] The paper mentions that the investigation was restricted to three needles and suggests it would be interesting to explore whether adding more needles continues to increase task complexity.
- Why unresolved: The study only tested up to three needles, so the impact of even longer reasoning chains on model performance remains unknown.
- What evidence would resolve it: Systematically increasing the number of needles (e.g., 4, 5, 10) in multilingual contexts and measuring performance degradation across different languages and models.

### Open Question 3
- Question: How do translation errors in the needle sentences affect model performance in multilingual long-context tasks?
- Basis in paper: [explicit] The paper notes that needles, city names, and prompts were translated from English by professional translators, and mentions that translation errors could potentially introduce biases or affect performance.
- Why unresolved: While the study used professional translators, it did not analyze the impact of translation quality or errors on model accuracy, leaving uncertainty about how sensitive the results are to translation fidelity.
- What evidence would resolve it: Comparing model performance using professionally translated needles versus machine-translated or back-translated needles to quantify the impact of translation errors on accuracy.

## Limitations

- Dataset representativeness: The mLongRR dataset uses only BBC news articles across five languages, which may not capture the full diversity of language usage patterns and may be particularly limited for lower-resource languages.
- Synthetic needle design validity: The evaluation uses artificially constructed needles with a specific format that may not reflect realistic retrieval scenarios and could advantage or disadvantage certain models.
- Cross-linguistic generalizability: The five languages selected represent a specific linguistic typology but do not cover the full spectrum of global language diversity, limiting generalizability to other language families or writing systems.

## Confidence

**High confidence** in the observation that model performance degrades with increased context length and needle depth. This finding aligns with established literature on the "lost in the middle" phenomenon and is consistently observed across all tested models and languages.

**Medium confidence** in the conclusion that lower-resource languages suffer more from tokenization inefficiencies. While tokenization rate data is presented, the study does not establish causation or rule out other factors like reduced training data or architectural biases.

**Medium confidence** in the reasoning vs. retrieval difficulty claims. The study demonstrates that reasoning tasks are harder, but the artificial nature of the multi-needle setup and lack of alternative reasoning task designs limits generalizability.

## Next Checks

1. **Dataset diversity validation**: Test the same models on mLongRR using alternative news sources or different text domains (e.g., social media, technical documentation) to assess whether performance patterns hold across varied content types.

2. **Natural needle substitution**: Replace synthetic needles with naturally occurring named entities (person names, organization names, dates) extracted from the same news articles to validate whether the performance degradation is format-specific or represents fundamental model limitations.

3. **Cross-linguistic extension**: Evaluate models on mLongRR format using additional language pairs that vary in linguistic distance from English (e.g., Romance languages, East Asian languages, Semitic languages) to determine if the observed patterns extend beyond the current language selection.