---
ver: rpa2
title: 'Fast Stochastic Policy Gradient: Negative Momentum for Reinforcement Learning'
arxiv_id: '2405.12228'
source_url: https://arxiv.org/abs/2405.12228
tags:
- policy
- algorithm
- spg-nm
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel stochastic policy gradient (SPG)
  algorithm with negative momentum (SPG-NM) for reinforcement learning (RL). The key
  idea is to integrate a negative momentum term into the standard SPG update to accelerate
  convergence and improve stability.
---

# Fast Stochastic Policy Gradient: Negative Momentum for Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.12228
- Source URL: https://arxiv.org/abs/2405.12228
- Reference count: 37
- One-line primary result: SPG-NM consistently outperforms state-of-the-art SPG algorithms on simple RL tasks through negative momentum stabilization.

## Executive Summary
This paper introduces SPG-NM, a novel stochastic policy gradient algorithm that incorporates negative momentum to accelerate convergence and improve stability in reinforcement learning. Unlike traditional momentum methods, SPG-NM uses a unique formulation that balances historical gradient information with current update directions through a large momentum coefficient (typically 10³ to 10⁵) and a negative correction term. The algorithm is evaluated on a 3-armed bandit and a 5-state MDP, demonstrating faster convergence and higher cumulative rewards compared to PG, APG, PG-HB, and PG-Adam baselines.

## Method Summary
SPG-NM integrates negative momentum into standard stochastic policy gradient updates through a momentum-corrected direction φ(t) = λθ(t) + (1-λ)(θ(t)-θ(t-1)). The algorithm conditionally updates parameters only when the value function improves, ensuring monotonic policy improvement. The momentum coefficient λ controls the trade-off between stability and responsiveness, with larger values providing stronger historical gradient dominance but requiring careful tuning to avoid oscillations.

## Key Results
- SPG-NM achieves faster convergence than PG, APG, PG-HB, and PG-Adam on both bandit and MDP tasks
- The algorithm is robust to λ values in the range 10³ to 10⁵, though extreme values degrade performance
- Sub-optimality gap analysis shows SPG-NM converges closer to optimal policies than baseline methods
- SPG-NM demonstrates superior performance under challenging hard policy initializations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The negative momentum term stabilizes updates by counteracting accumulated positive momentum
- Mechanism: Large λ ensures historical gradients dominate while (1-λ) provides corrective feedback to prevent overshooting
- Core assumption: Historical gradient dominance with negative correction prevents oscillations
- Evidence anchors: Abstract mentions balancing historical and current directions; section discusses momentum ensuring updates remain close
- Break condition: λ too large (e.g., 10⁶) makes corrective term negligible, causing oscillations

### Mechanism 2
- Claim: Conditional updates based on value function improvement ensure monotonic policy improvement
- Mechanism: Updates only commit when new policy outperforms previous one via value function comparison
- Core assumption: Value function improvement reliably proxies for policy improvement
- Evidence anchors: Section explicitly describes conditional update mechanism; abstract claims consistent outperformance
- Break condition: Noisy value function estimates cause failure to converge or policy collapse

### Mechanism 3
- Claim: Negative momentum formulation enables faster escape from local optima than standard momentum
- Mechanism: Negative component acts as adaptive damping mechanism preventing premature convergence
- Core assumption: Rebounding effect from negative momentum provides corrective force in misleading gradient regions
- Evidence anchors: Abstract mentions reducing oscillations during parameter updates; section compares momentum to magnet keeping updates accurate
- Break condition: Highly non-convex parameterizations with flat regions overwhelm corrective force

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Provides formal framework for modeling agent-environment interaction in RL
  - Quick check question: What are the components of an MDP tuple (S, A, P, r, γ, ρ)?

- Concept: Stochastic Policy Gradient (SPG)
  - Why needed here: SPG-NM builds upon SPG; understanding base algorithm essential for grasping modifications
  - Quick check question: How does policy gradient theorem relate gradient of expected return to log-policy gradient?

- Concept: Nesterov Accelerated Gradient (NAG)
  - Why needed here: SPG-NM's momentum formulation is conceptually related to NAG but with negative sign
  - Quick check question: What is key difference between NAG and standard momentum in terms of update timing?

## Architecture Onboarding

- Component map:
  Policy parameters θ -> Value function estimator -> Gradient computation -> Momentum correction φ(t) -> Conditional update ω(t)

- Critical path:
  1. Sample trajectory from current policy π_θ
  2. Estimate value function V^π_θ(μ)
  3. Compute policy gradient ∇_θ V^π_θ(μ)
  4. Update θ(t) using gradient ascent
  5. Compute momentum-corrected direction φ(t)
  6. Evaluate V^π_φ(μ)
  7. Conditionally update ω(t) based on value comparison
  8. Repeat

- Design tradeoffs:
  - Larger λ improves stability but may slow escape from poor local optima
  - Conditional update rule adds computational overhead but ensures monotonic improvement
  - Fixed learning rate η(t) is simpler but may limit adaptability

- Failure signatures:
  - Oscillations in parameter updates (λ too small)
  - Slow convergence or stagnation (λ too large)
  - Policy collapse to degenerate distributions (incorrect parameterization)

- First 3 experiments:
  1. Run SPG-NM on 3-armed bandit with uniform initialization and λ = 10³; compare convergence speed to PG
  2. Vary λ from 10² to 10⁶ on 5-state MDP; observe impact on stability and final return
  3. Replace conditional update rule with unconditional updates; measure effect on monotonic improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SPG-NM perform on more complex RL tasks with high-dimensional state/action spaces or partial observability?
- Basis in paper: [inferred] Paper evaluates on simple tasks (bandit, 5-state MDP) but mentions potential applications in complex domains without evidence
- Why unresolved: Lacks empirical results on standard RL benchmarks like Atari, MuJoCo, or robotics problems
- What evidence would resolve it: Comprehensive experimental results on standard RL benchmarks with high-dimensional inputs and partial observability

### Open Question 2
- Question: What is the theoretical convergence rate of SPG-NM under different policy parameterizations?
- Basis in paper: [explicit] Mentions Chen et al. established O(1/t²) for APG under softmax but provides no theoretical analysis for SPG-NM
- Why unresolved: Authors focus on empirical validation without deriving convergence guarantees for SPG-NM
- What evidence would resolve it: Rigorous theoretical analysis proving convergence rates for SPG-NM under various policy parameterizations

### Open Question 3
- Question: How does momentum coefficient λ affect performance across different RL environment types?
- Basis in paper: [explicit] Conducts limited sensitivity analysis on λ in range [10³, 10⁵] but doesn't systematically study effects across diverse environments
- Why unresolved: Only provides limited analysis on two simple tasks without exploring interaction with environment characteristics
- What evidence would resolve it: Comprehensive study varying λ across diverse RL environments with different characteristics (reward sparsity, action dimensionality, agent count)

## Limitations
- Evaluation scope limited to simple toy problems (3-armed bandit, 5-state MDP), making generalization to complex tasks uncertain
- Critical dependence on momentum coefficient λ without systematic sensitivity analysis across diverse settings
- Lack of theoretical convergence guarantees and statistical significance measures in experimental results

## Confidence
- Confidence in core claim of outperforming baselines: Medium
- Confidence in stabilization mechanism (Mechanism 1): Low
- Confidence in monotonic improvement claim (Mechanism 2): Medium
- Confidence in local optima escape claim (Mechanism 3): Low

## Next Checks
1. Conduct ablation studies removing negative momentum component to isolate its effect on convergence and stability
2. Evaluate SPG-NM on continuous control benchmark (e.g., CartPole or MountainCar) to assess scalability and robustness
3. Perform systematic sensitivity analysis of λ across multiple orders of magnitude and policy initialization strategies