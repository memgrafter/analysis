---
ver: rpa2
title: Unifying Generative and Dense Retrieval for Sequential Recommendation
arxiv_id: '2411.18814'
source_url: https://arxiv.org/abs/2411.18814
tags:
- retrieval
- item
- dense
- generative
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the trade-offs between generative retrieval
  and sequential dense retrieval in recommendation systems, particularly for small-scale
  academic benchmarks. The authors identify two main limitations of generative retrieval:
  lower performance compared to dense retrieval under identical conditions and difficulties
  handling cold-start items due to overfitting to training data.'
---

# Unifying Generative and Dense Retrieval for Sequential Recommendation

## Quick Facts
- arXiv ID: 2411.18814
- Source URL: https://arxiv.org/abs/2411.18814
- Reference count: 29
- Primary result: LIGER hybrid model improves cold-start item recommendation while maintaining computational efficiency

## Executive Summary
This paper addresses the trade-offs between generative retrieval and dense retrieval in sequential recommendation systems. The authors identify that generative retrieval struggles with cold-start items due to overfitting to training data, while dense retrieval provides better coverage but at higher computational cost. To bridge this gap, they propose LIGER (LeveragIng dense retrieval for GEnerative Retrieval), a hybrid model that combines the efficiency of generative retrieval with the robust performance of dense retrieval. Experiments on Amazon and Steam datasets show that LIGER significantly improves cold-start item performance while maintaining competitive results for in-set items.

## Method Summary
LIGER combines generative retrieval (TIGER) with dense retrieval to address cold-start item recommendations. The method uses semantic IDs generated by RQ-VAE from item text embeddings as input to both retrieval approaches. Generative retrieval produces a small set of candidates (K) using beam search, which are then supplemented with cold-start items and re-ranked using dense retrieval similarity scores. The model is trained using AdamW optimizer with combined cosine similarity and next-token prediction losses, and evaluated on NDCG@10 and Recall@10 metrics for both in-set and cold-start items using leave-one-out splitting.

## Key Results
- LIGER significantly improves cold-start item recommendation performance compared to pure generative retrieval
- The hybrid approach achieves either the best or second-best results across all tested datasets
- Performance scales with the number of candidates retrieved by generative retrieval, with trade-offs between efficiency and coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative retrieval struggles with cold-start items due to overfitting to training semantic IDs
- Mechanism: The model's beam search ranks candidates by generation probability, and cold-start items consistently fall below the inclusion threshold (pK), effectively excluding them from recommendations
- Core assumption: Cold-start items have sufficiently different semantic characteristics that the model cannot generate them during inference
- Evidence anchors:
  - [section] "the generation probability of cold-start items always falls below the threshold required for inclusion in the beam search process (p⋆ < pK)"
  - [section] "the model's learned conditional probabilities tends to overfit to items seen during training, leading to a significantly reduced capability to generate cold-start items"
- Break condition: If semantic ID generation captures broader semantic features beyond training examples, cold-start items could be generated

### Mechanism 2
- Claim: LIGER improves cold-start performance by combining generative and dense retrieval approaches
- Mechanism: Generative retrieval provides computational efficiency by reducing candidate pool from N to K items, while dense retrieval uses text embeddings to include and rank cold-start items among candidates
- Core assumption: Cold-start items are relatively sparse compared to in-set items, making it feasible to supplement K candidates with cold-start items
- Evidence anchors:
  - [section] "we supplement the candidates with cold-start items to ensure their inclusion, as their number is relatively small"
  - [section] "This approach leverages the efficiency of generative retrieval to reduce the candidate pool while ensuring cold-start items are represented"
- Break condition: If cold-start items become too numerous, supplementing becomes impractical and the hybrid approach loses efficiency benefits

### Mechanism 3
- Claim: Semantic IDs can effectively replace item IDs for dense retrieval while maintaining performance
- Mechanism: Dense retrieval with semantic IDs (Dense SID) shows performance close to standard dense retrieval, indicating semantic IDs capture sufficient information about items
- Core assumption: The semantic ID representation learned by RQ-VAE captures the same information as item IDs but in a more compact form
- Evidence anchors:
  - [section] "incorporating semantic IDs as input in dense retrieval (Dense SID) significantly reduces the performance gap with standard dense retrieval"
  - [section] "Dense retrieval with semantic IDs (Dense SID) shows performance close to standard dense retrieval"
- Break condition: If semantic IDs fail to capture critical item information, performance would degrade significantly

## Foundational Learning

- Concept: Sequence modeling with Transformers
  - Why needed here: Both dense and generative retrieval methods rely on Transformer architectures to model user interaction sequences
  - Quick check question: What is the key difference between encoder-only and encoder-decoder Transformer architectures in the context of retrieval?

- Concept: Semantic ID generation using RQ-VAE
  - Why needed here: The paper uses RQ-VAE to convert item text representations into discrete semantic IDs that capture item characteristics
  - Quick check question: How does the quantization process in RQ-VAE affect the ability to represent new items?

- Concept: Cold-start item handling in recommendation systems
  - Why needed here: The paper specifically addresses how different retrieval methods handle items with no training interactions
  - Quick check question: Why do text representations help with cold-start items in dense retrieval but not in basic generative retrieval?

## Architecture Onboarding

- Component map: Item text → Semantic IDs (RQ-VAE) → Item embeddings → Sequential model → Output generation
- Critical path:
  1. Convert item text to semantic IDs and embeddings
  2. Train sequential model on interaction history with semantic IDs
  3. Generate K candidates using beam search
  4. Supplement with cold-start items
  5. Rank all candidates using dense retrieval similarity scores
- Design tradeoffs:
  - Memory vs. performance: Dense retrieval stores O(N) embeddings, generative retrieval stores O(t) embeddings
  - Speed vs. coverage: Smaller K improves speed but may miss relevant items
  - Complexity vs. effectiveness: LIGER adds complexity but significantly improves cold-start handling
- Failure signatures:
  - Poor cold-start performance: Indicates generative retrieval is overfitting to training items
  - High inference cost: Suggests K is too large or dense retrieval is inefficient
  - Performance gap between LIGER and dense retrieval: Indicates insufficient candidate generation or poor re-ranking
- First 3 experiments:
  1. Compare TIGER vs LIGER with varying K values to identify the optimal candidate generation size
  2. Test LIGER's cold-start performance by evaluating on items unseen during RQ-VAE training
  3. Measure performance degradation when removing the text representation input to quantify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance gap between generative and dense retrieval methods scale with dataset size in real-world applications?
- Basis in paper: [explicit] The authors note that performance differences are influenced by dataset size and distribution, and they are aware of industry-scale implementations where generative retrieval outperforms dense retrieval, but their study focuses on small-scale academic benchmarks.
- Why unresolved: The study explicitly limits its scope to small-scale datasets, leaving the behavior of these methods at larger scales unexplored.
- What evidence would resolve it: Benchmarking both methods on progressively larger datasets, including industrial-scale data, would reveal how the performance gap evolves.

### Open Question 2
- Question: Can generative retrieval methods be improved to match or exceed dense retrieval performance without relying on large-scale pre-training?
- Basis in paper: [explicit] The authors identify that generative retrieval struggles with cold-start items and performance gaps compared to dense retrieval, but acknowledge its potential for efficiency and diversity.
- Why unresolved: The study focuses on a hybrid approach (LIGER) rather than optimizing generative retrieval independently, and the generative retrieval paradigm is still relatively new compared to dense retrieval.
- What evidence would resolve it: Developing and evaluating standalone generative retrieval models with enhanced architectures or training strategies on large-scale benchmarks would determine if they can close the performance gap.

### Open Question 3
- Question: What is the impact of varying the number of candidates retrieved by generative retrieval on the overall efficiency and effectiveness of hybrid models like LIGER?
- Basis in paper: [explicit] The authors analyze how LIGER's performance changes with different numbers of candidates retrieved by generative retrieval, showing improved performance as K increases, but also note the trade-off with computational cost.
- Why unresolved: While the study provides insights into performance trends, it does not fully explore the optimal balance between retrieval efficiency and recommendation quality.
- What evidence would resolve it: Conducting extensive experiments to map the performance and computational cost trade-offs across a wide range of K values and dataset scales would identify the optimal configuration.

## Limitations

- The performance gains demonstrated may not scale to industrial settings with millions of items
- The RQ-VAE semantic ID generation process introduces additional complexity without fully addressing quantization artifacts or codebook collisions
- Cold-start evaluation assumes items have text attributes, which may not hold for all recommendation scenarios

## Confidence

- High confidence: The fundamental mechanism that generative retrieval overfits to training items while dense retrieval handles cold-start items better is well-supported by the evidence provided
- Medium confidence: The hybrid approach effectively balances computational efficiency with cold-start handling, though the specific contribution of each component could be more precisely quantified
- Medium confidence: The RQ-VAE semantic ID generation captures sufficient information for recommendation tasks, but the robustness of this approach to vocabulary expansion remains unclear

## Next Checks

1. **Scale sensitivity test**: Evaluate LIGER on a larger dataset with 100K+ items to verify whether the performance advantages persist as vocabulary size increases, particularly for the hybrid candidate generation approach

2. **Semantic ID robustness**: Systematically test how varying the semantic ID cardinality (beyond the single value tested) affects both in-set and cold-start performance, and measure collision rates between item representations

3. **Component contribution isolation**: Conduct an ablation study that separately evaluates the contribution of text representation input versus semantic IDs to determine which component drives the majority of performance gains, and whether both are necessary in the LIGER architecture