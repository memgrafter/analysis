---
ver: rpa2
title: 'DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation'
arxiv_id: '2406.10737'
source_url: https://arxiv.org/abs/2406.10737
tags:
- dpcore
- domain
- prompt
- source
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DPCore introduces a dynamic prompt coreset approach for continual
  test-time adaptation that addresses challenges in environments with varying domain
  change patterns. The method combines visual prompt adaptation for efficient domain
  alignment, a prompt coreset for knowledge preservation, and a dynamic update mechanism
  that adjusts prompts based on domain similarities.
---

# DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation

## Quick Facts
- arXiv ID: 2406.10737
- Source URL: https://arxiv.org/abs/2406.10737
- Authors: Yunbei Zhang; Akshay Mehra; Shuaicheng Niu; Jihun Hamm
- Reference count: 40
- Key outcome: DPCore achieves state-of-the-art performance in continual test-time adaptation, reducing trainable parameters by 99% and computation time by 64% compared to previous approaches

## Executive Summary
DPCore introduces a dynamic prompt coreset approach for continual test-time adaptation that addresses challenges in environments with varying domain change patterns. The method combines visual prompt adaptation for efficient domain alignment, a prompt coreset for knowledge preservation, and a dynamic update mechanism that adjusts prompts based on domain similarities. DPCore achieves state-of-the-art performance across both structured and dynamic domain change settings, improving error rates by 15.9% over source models on ImageNet-to-ImageNet-C and maintaining robust performance in dynamic environments where other methods degrade significantly.

## Method Summary
DPCore operates by learning domain-specific visual prompts through distribution alignment between source and target features, maintaining a coreset of learned prompts and their statistics, and dynamically updating this coreset based on domain similarities. The method requires only 300 unlabeled source examples to compute mean and standard deviation statistics for alignment, uses 8 prompt tokens per domain, and employs a threshold-based mechanism to decide whether to refine existing prompts or learn new ones. The dynamic update mechanism uses weighted combinations of prompts to handle domains with mixed characteristics, enabling efficient adaptation without evaluating each core element individually.

## Key Results
- Achieves 15.9% error rate improvement over source models on ImageNet-to-ImageNet-C
- Reduces trainable parameters by 99% compared to previous approaches
- Maintains robust performance in dynamic environments where other methods degrade significantly
- Outperforms baselines (Tent, CoTTA, ViDA) across both structured and dynamic domain change settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic prompt coreset prevents catastrophic forgetting by preserving knowledge through weighted prompt updates.
- Mechanism: When encountering similar domains, existing prompts are refined using weighted combinations rather than overwritten, allowing the model to maintain previously learned knowledge while adapting to new data.
- Core assumption: Domains can be meaningfully grouped based on feature statistics distances, and similar domains benefit from shared prompts.

### Mechanism 2
- Claim: Weighted prompt composition enables efficient adaptation to domains with mixed characteristics.
- Mechanism: Instead of evaluating each core element individually on a test batch, DPCore computes a distance-weighted combination of all prompts, allowing a single forward pass to adapt to domains that share characteristics with multiple visited domains.
- Core assumption: Domain similarities in feature space can be effectively captured through statistics distance, and weighted combinations of prompts can represent composite domains.

### Mechanism 3
- Claim: Source statistics computation with minimal data enables effective distribution alignment without warm-up.
- Mechanism: DPCore requires only 300 unlabeled source examples to compute mean and standard deviation statistics, which are then used to align target domain features through prompt optimization without any pre-adaptation warm-up on the entire source dataset.
- Core assumption: Marginal distribution statistics from a small sample of source data are sufficient to guide effective alignment with target domains.

## Foundational Learning

- Concept: Vision Transformer architecture and visual prompting
  - Why needed here: DPCore builds upon ViT-Base models and introduces learnable prompt tokens for domain adaptation, requiring understanding of how prompts augment the input sequence and affect feature extraction
  - Quick check question: How do visual prompts modify the input sequence to transformer layers in a ViT model?

- Concept: Test-Time Adaptation and Continual Learning
  - Why needed here: DPCore operates in the CTTA setting where models must adapt to continuously changing domains without forgetting previous knowledge, requiring understanding of the fundamental challenges in TTA and catastrophic forgetting
  - Quick check question: What distinguishes Continual Test-Time Adaptation from standard Test-Time Adaptation in terms of domain change patterns?

- Concept: Distribution alignment and distance metrics
  - Why needed here: DPCore uses marginal distribution alignment between source and target features, requiring understanding of how distance metrics like mean and standard deviation differences guide prompt optimization
  - Quick check question: How does minimizing the distance between source and target feature statistics guide the learning of domain-specific prompts?

## Architecture Onboarding

- Component map: Visual Prompt Adaptation (VPA) -> Prompt Coreset (PC) -> Dynamic Update (DU)
- Critical path: Compute source statistics offline → Initialize empty coreset → For each test batch: compute statistics, evaluate weighted prompt, update or add prompts, use final prompt for prediction
- Design tradeoffs: Prompt coreset size vs memory efficiency (larger coreset preserves more knowledge but uses more memory); prompt learning steps vs adaptation speed (more steps may improve performance but increase computation); threshold ratio vs sensitivity to domain changes (lower threshold creates more prompts but may over-segment domains)
- Failure signatures: Increasing error rates over time indicate catastrophic forgetting; high coreset growth rate suggests inability to group similar domains; performance degradation on previously well-adapted domains indicates negative transfer
- First 3 experiments:
  1. Run DPCore on a single domain (e.g., Gaussian Noise) and verify prompt learning effectiveness by comparing to baseline without adaptation
  2. Test coreset behavior by sequentially presenting similar domains and verifying prompt refinement rather than creation of new prompts
  3. Evaluate threshold sensitivity by running with different ρ values and observing coreset size and performance tradeoffs

## Open Questions the Paper Calls Out

1. How does DPCore perform when each test batch contains mixed-domain data rather than data from a single domain?
2. What is the theoretical bound on DPCore's performance when domain clusters are not well-separated as assumed in the analysis?
3. How does DPCore's performance scale with the total number of unique domains encountered during continual adaptation?

## Limitations

- The dynamic update mechanism's exact decision criteria for creating new prompts versus refining existing ones is not clearly defined
- The choice of 300 source examples for statistics computation appears arbitrary without ablation studies on this critical hyperparameter
- The method assumes single-domain test batches and may struggle with mixed-domain data
- Long-term performance degradation over many domain shifts has not been thoroughly analyzed

## Confidence

- **Medium**: The paper presents strong theoretical motivation and empirical results, but several implementation details are underspecified
- **Low**: While the method claims state-of-the-art performance across multiple benchmarks, the comparison methodology is not fully transparent
- **Medium**: The catastrophic forgetting prevention mechanism relies heavily on the prompt coreset approach, but the paper doesn't provide detailed analysis of long-term performance degradation

## Next Checks

1. **Ablation on Statistics Sample Size**: Systematically vary the number of source examples (50, 100, 300, 500, 1000) used for computing source statistics and measure the impact on adaptation performance and computational efficiency to validate the claimed 300-example sufficiency.

2. **Dynamic Update Sensitivity Analysis**: Create a synthetic benchmark with controlled domain similarity levels and systematically vary the dynamic update threshold ρ to quantify its impact on coreset size, adaptation speed, and performance across different domain change patterns.

3. **Long-term Forgetting Analysis**: Implement a benchmark with 50+ sequential domain shifts spanning the full range of ImageNet-C corruptions, then track per-domain performance over time to quantify catastrophic forgetting rates and compare against baseline methods under identical conditions.