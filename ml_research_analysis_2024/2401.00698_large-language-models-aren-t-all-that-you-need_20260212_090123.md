---
ver: rpa2
title: Large Language Models aren't all that you need
arxiv_id: '2401.00698'
source_url: https://arxiv.org/abs/2401.00698
tags:
- task
- layer
- loss
- large
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares a traditional CRF model with fine-tuned LLMs
  for fine-grained NER in low-context settings. The proposed LLM approach incorporates
  triplet token blending, decaying auxiliary loss for coarse-grained NER, and task-optimal
  heads with separate learning rates.
---

# Large Language Models aren't all that you need
## Quick Facts
- arXiv ID: 2401.00698
- Source URL: https://arxiv.org/abs/2401.00698
- Reference count: 10
- Top 20 ranking on SemEval 2023 Task 2: MultiCoNER II leaderboard with 0.85/0.84 dev and 0.67/0.61 test micro/macro F1 scores

## Executive Summary
This paper compares traditional CRF models with fine-tuned LLMs for fine-grained Named Entity Recognition (NER) in low-context multilingual settings. The authors propose an LLM-based approach incorporating triplet token blending, decaying auxiliary loss for coarse-grained NER, and task-optimal heads with separate learning rates. Their best model (XLM-RoBERTa-L with BiLSTM+CRF head) achieves micro/macro F1 scores of 0.85/0.84 on development and 0.67/0.61 on test sets, ranking in the top 20 of the competition leaderboard. The study demonstrates that LLMs significantly outperform traditional models, with engineering techniques further improving macro F1 scores.

## Method Summary
The authors compare a traditional CRF baseline with fine-tuned LLMs (XLM-RoBERTa-Large, RoBERTa-Base, DeBERTa-v3-large, GPT-3) for fine-grained NER on the English track of SemEval 2023 Task 2: MultiCoNER II. The proposed LLM approach uses triplet token blending (concatenating embeddings from previous, current, and next tokens), decaying auxiliary loss for coarse-grained NER (with residual 0.1), and task-optimal heads (BiLSTM+CRF, CRF, linear layers) with separate learning rates (10x higher for heads). The best configuration uses XLM-RoBERTa-Large with BiLSTM+CRF head, triplet blending, and decaying auxiliary loss, trained for 16 epochs with batch size 16 and dropout 0.2.

## Key Results
- Traditional CRF model achieves 0.75/0.70 micro/macro F1 on development set
- Fine-tuned XLM-RoBERTa-L with BiLSTM+CRF head achieves 0.85/0.84 micro/macro F1 on development set
- Test set performance: 0.67/0.61 micro/macro F1, ranking in top 20 of competition leaderboard
- Engineering techniques (triplet blending, decaying auxiliary loss, separate learning rates) improve macro F1 scores by 0.03-0.05

## Why This Works (Mechanism)
The LLM-based approach works better than traditional CRF models in low-context settings because it leverages pre-trained contextual embeddings that capture semantic relationships beyond immediate word neighbors. The decaying auxiliary loss for coarse-grained NER helps the model learn hierarchical label representations, while triplet token blending provides additional contextual information by incorporating neighboring token embeddings. Separate learning rates for the head and LLM layers allow the model to optimize task-specific components without destabilizing pre-trained weights.

## Foundational Learning
**Named Entity Recognition (NER)**: Task of identifying and classifying named entities in text into predefined categories. Why needed: Core problem being addressed in the paper. Quick check: Can identify person, organization, and location entities in sample sentences.

**BIO Tagging Scheme**: Labeling scheme where B- indicates beginning of entity, I- indicates inside, and O indicates outside entity. Why needed: Standard approach for sequence labeling tasks. Quick check: Can correctly label "B-PER I-PER" for multi-word person names.

**Auxiliary Loss**: Additional training objective that helps the model learn better representations. Why needed: Improves generalization by forcing the model to learn multiple related tasks. Quick check: Can implement decaying weight for auxiliary task during training.

**Token Blending**: Technique of combining embeddings from multiple tokens (e.g., previous, current, next). Why needed: Provides additional context in low-context settings. Quick check: Can implement concatenation of neighbor token embeddings.

**CRF Layer**: Conditional Random Field layer that models label dependencies in sequence labeling. Why needed: Captures transition patterns between entity labels. Quick check: Can implement CRF layer on top of embeddings for sequence prediction.

## Architecture Onboarding
**Component Map**: Input text -> LLM encoder (XLM-RoBERTa-L) -> Token blending -> Task-specific head (BiLSTM+CRF) -> BIO labels

**Critical Path**: The model takes input text, passes it through the LLM encoder to get contextual embeddings, applies triplet token blending to incorporate neighbor context, then processes through BiLSTM+CRF head to predict BIO-labeled entities.

**Design Tradeoffs**: The authors chose triplet token blending over other context augmentation methods because it's computationally efficient and doesn't require external knowledge sources. They opted for decaying auxiliary loss rather than fixed weights to gradually shift focus from coarse-grained to fine-grained NER as training progresses.

**Failure Signatures**: Overfitting occurs when the model performs well on training but poorly on development/test sets, often due to small dataset size. Poor macro F1 indicates class imbalance issues where the model struggles with rare entity types.

**Three First Experiments**:
1. Implement baseline CRF model with POS features and compare against LLM baseline without any engineering techniques
2. Test different triplet blending configurations (concatenation vs averaging, neighbor blending before/after CRF) to identify optimal setup
3. Implement decaying auxiliary loss with different decay schedules to find optimal coarse-to-fine-grained transition

## Open Questions the Paper Calls Out
**Open Question 1**: Does incorporating external context from knowledge bases like Wikipedia significantly improve NER performance for low-context, fine-grained datasets beyond the gains achieved through LLM-based approaches alone? The authors mention this could have boosted scores by up to 0.1 but didn't implement it due to time constraints.

**Open Question 2**: How does the choice of auxiliary task (Coarse-Grained vs. other potential auxiliary tasks) affect the performance of NER models on fine-grained classification tasks? Only coarse-grained NER was tested as auxiliary task, leaving other possibilities unexplored.

**Open Question 3**: Does triplet token blending provide consistent benefits across different sentence lengths and contexts, or is its effectiveness limited to low-context, short sentences? The authors found no noticeable difference in scores but speculate benefits might vary with sentence length.

## Limitations
- Exact implementation details of decaying auxiliary loss schedule and coarse-grained label mapping are unspecified
- Test set evaluation relies on leaderboard submissions rather than publicly available metrics
- Limited exploration of triplet token blending configurations and their individual contributions

## Confidence
**High confidence**: LLMs significantly outperform traditional CRF models for fine-grained NER in low-context settings
**Medium confidence**: Effectiveness of proposed engineering techniques (triplet blending, decaying auxiliary loss, separate learning rates) due to implementation ambiguities
**Low confidence**: Exact performance numbers (0.85/0.84 dev, 0.67/0.61 test) without complete implementation details and test set access

## Next Checks
1. Implement and evaluate the decaying auxiliary loss schedule with exact epoch-by-epoch weight decay from 1 to 0.1, comparing against baseline without auxiliary loss
2. Systematically test different triplet token blending configurations (concatenation vs averaging, neighbor blending before/after CRF) to isolate their individual contributions
3. Conduct controlled experiment on publicly available fine-grained NER dataset with similar low-context characteristics to verify generalizability beyond SemEval 2023 dataset