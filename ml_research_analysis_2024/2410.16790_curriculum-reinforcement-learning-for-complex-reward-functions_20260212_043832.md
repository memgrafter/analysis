---
ver: rpa2
title: Curriculum Reinforcement Learning for Complex Reward Functions
arxiv_id: '2410.16790'
source_url: https://arxiv.org/abs/2410.16790
tags:
- reward
- curriculum
- learning
- where
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage reward curriculum to improve reinforcement
  learning in complex environments with multiple reward terms. The method first trains
  on a simpler reward function, then transitions to the full reward, reusing samples
  from the first phase.
---

# Curriculum Reinforcement Learning for Complex Reward Functions

## Quick Facts
- arXiv ID: 2410.16790
- Source URL: https://arxiv.org/abs/2410.16790
- Reference count: 40
- Complex reward functions benefit from curriculum learning approach

## Executive Summary
This paper addresses the challenge of learning in environments with complex reward functions containing multiple competing terms, particularly when constraints interfere with task learning. The authors propose a two-stage reward curriculum where an agent first trains on a simpler subset of rewards (base reward only), then transitions to the full reward function. An automatic mechanism based on actor-critic fit determines the optimal switching time between phases, and a flexible replay buffer enables efficient transfer of experiences between stages. The approach is evaluated on modified DeepMind Control Suite environments and a mobile robot navigation task, showing substantial performance improvements, especially when constraint weights are high.

## Method Summary
The method introduces a two-stage reward curriculum (RC-SAC/RC-TD3) that first trains on base rewards only, then transitions to the full reward function including constraints. The automatic phase switch monitors actor-critic fit (Jπ,Q) and transitions when the actor fits the critic well for m consecutive timesteps. A flexible replay buffer stores experience tuples from both phases, allowing efficient reuse when switching rewards. The approach is evaluated on modified DeepMind Control environments (finger spin, walker run, cartpole swingup) with added action magnitude penalties and a mobile robot navigation task with multiple reward terms including goal-reaching, velocity, smoothness, path-following, and collision avoidance.

## Key Results
- Two-stage curriculum prevents agents from exploiting only constraint terms, achieving better balance between task completion and constraint satisfaction
- Automatic switching based on actor-critic fit is more sample-efficient than fixed-schedule approaches
- Pretrained network weights provide the main benefit, with replay buffer reuse contributing additional sample efficiency
- Curriculum effectiveness increases with higher constraint weights, especially in environments where constraints don't automatically optimize with task completion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Two-stage curriculum prevents getting stuck in local optima that only optimize constraints
- **Core assumption**: Base reward can be learned independently of constraints
- **Evidence**: Abstract states curriculum prevents constraint exploitation; results show higher effectiveness with weighted constraints
- **Break condition**: If base reward requires constraints, curriculum fails

### Mechanism 2
- **Claim**: Automatic switching based on actor-critic fit is more sample-efficient
- **Core assumption**: Actor-critic fit reliably indicates policy convergence
- **Evidence**: Section states automatic switch identifies suitable times leading to faster convergence
- **Break condition**: If actor-critic fit becomes unreliable due to noisy gradients

### Mechanism 3
- **Claim**: Reusing phase 1 samples with updated rewards provides sample efficiency
- **Core assumption**: Past experiences remain valuable when reward changes
- **Evidence**: Abstract mentions flexible replay buffer enabling efficient phase transfer
- **Break condition**: If state-action distribution changes significantly between phases

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) formulation
  - **Why needed here**: Curriculum modifies reward function within MDP framework
  - **Quick check**: What are the five components of an MDP tuple ⟨S, A, P, r, p0, γ⟩ and how does curriculum modify one of them?

- **Concept**: Actor-Critic architecture
  - **Why needed here**: Automatic switching relies on monitoring actor-critic interaction
  - **Quick check**: How does actor-critic fit metric Jπ,Q relate to policy convergence?

- **Concept**: Reward shaping and potential-based shaping
  - **Why needed here**: Paper mentions potential-based reward shaping for robot environment
  - **Quick check**: What distinguishes potential-based from arbitrary reward shaping?

## Architecture Onboarding

- **Component map**: RL algorithm core (SAC/TD3) -> Two-stage reward curriculum -> Flexible replay buffer -> Automatic switching mechanism -> Network parameters

- **Critical path**: 1) Initialize with base reward rb, 2) Collect experience and store in replay buffer, 3) Monitor actor-critic fit Jπ,Q, 4) Switch to full reward when fit threshold met, 5) Continue training with full reward reusing phase 1 samples, 6) Output final policy

- **Design tradeoffs**: Fixed vs. automatic switching (automatic more sample-efficient but complex), reset vs. keep network weights (keeping leverages pretraining), reset vs. keep replay buffer (keeping provides sample efficiency)

- **Failure signatures**: Training instability in phase 2, baseline outperforming curriculum, automatic switch never triggering, switch triggering too early, performance degrading after switch

- **First 3 experiments**: 1) Baseline vs. RC-TD3 on finger spin with wc=1.0 to verify constraint prevention, 2) Automatic vs. fixed-time switching on walker run to measure efficiency gains, 3) Network reset vs. replay buffer reset ablation on cartpole to isolate main benefit source

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance vary with environments having more than three competing reward terms?
- **Basis**: Paper evaluates on environments with multiple reward terms but doesn't explore scalability to many terms
- **Why unresolved**: Paper doesn't test on environments with large number of competing reward terms
- **Evidence needed**: Testing on environments with higher number of competing reward terms

### Open Question 2
- **Question**: How sensitive is automatic phase switch to threshold ΓCR and parameter m?
- **Basis**: Paper mentions dependence on specific ΓCR value but doesn't systematically study sensitivity
- **Why unresolved**: No systematic study of how ΓCR and m choices affect performance
- **Evidence needed**: Sensitivity analysis varying ΓCR and m

### Open Question 3
- **Question**: How does curriculum perform in environments with sparse or no rewards?
- **Basis**: Paper focuses on dense reward environments but method could apply to sparse/no rewards
- **Why unresolved**: Paper doesn't explore application to sparse or no reward environments
- **Evidence needed**: Testing on environments with sparse or no rewards

## Limitations
- Limited empirical validation of underlying mechanisms, particularly actor-critic fit reliability
- Unclear relative contributions of network pretraining versus replay buffer reuse across different environments
- Automatic switching mechanism depends on threshold parameters without systematic sensitivity analysis

## Confidence
- **High confidence**: Curriculum improves performance when constraints significantly hinder task learning
- **Medium confidence**: Automatic switching is more sample-efficient than fixed schedules
- **Medium confidence**: Pretrained network weights provide main benefit over replay buffer reuse

## Next Checks
1. **Mechanistic validation**: Systematically vary constraint weights wc to identify threshold where curriculum benefits emerge and test correlation with base reward learnability
2. **Robustness testing**: Evaluate automatic switching across environments with different learning dynamics to verify actor-critic fit reliability as proxy for curriculum timing
3. **Ablation refinement**: Conduct controlled experiments isolating network pretraining from replay buffer reuse across multiple environment types to quantify relative contributions under different state-action distribution changes