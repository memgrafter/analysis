---
ver: rpa2
title: 'What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different
  Languages'
arxiv_id: '2407.09704'
source_url: https://arxiv.org/abs/2407.09704
tags:
- gender
- nouns
- languages
- adjectives
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines grammatical gender biases in multilingual large
  language models (LLMs) by prompting them to describe gendered nouns with adjectives
  in 10 different languages. The authors develop a method to collect adjective descriptions
  for masculine and feminine nouns, then train binary classifiers to predict grammatical
  gender from these adjective sets.
---

# What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different Languages

## Quick Facts
- arXiv ID: 2407.09704
- Source URL: https://arxiv.org/abs/2407.09704
- Authors: Viktor Mihaylov; Aleksandar Shtedritski
- Reference count: 25
- Key outcome: Multilingual LLMs exhibit consistent grammatical gender biases across languages, with classifiers trained on adjective distributions showing cross-language transferability.

## Executive Summary
This paper investigates grammatical gender biases in multilingual large language models by prompting them to generate adjective descriptions for gendered nouns across 10 different languages. The authors develop a method to collect adjective distributions for masculine and feminine nouns, then train binary classifiers to predict grammatical gender from these distributions. Surprisingly, they find that classifiers not only predict gender within languages (achieving 54-69% accuracy) but also transfer well across languages (achieving 48-63% accuracy), even for language pairs with low lexical similarity. This suggests that multilingual LLMs encode gender-associated semantic features in a language-agnostic manner.

## Method Summary
The authors collect gendered nouns from Wiktionary across 10 languages, then use few-shot prompting with Mistral-7B and Llama2-7B to generate 50 adjectives per noun. They compute frequency-weighted adjective distributions, translate all adjectives to English, and represent them using GloVe embeddings. Binary classifiers (2-layer MLPs) are trained to predict grammatical gender from these adjective distributions, with leave-one-out cross-validation testing cross-language transferability.

## Key Results
- Classifiers achieve 54-69% accuracy predicting gender within languages
- Classifiers achieve 48-63% accuracy predicting gender in unseen languages
- Transferability persists even for language pairs with low lexical similarity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-language classifier transferability occurs because multilingual LLMs encode grammatical gender in a language-agnostic semantic space.
- Mechanism: When adjectives are generated in different languages to describe nouns, they carry gender-associated semantic features. These features, once translated to English, exist in a shared embedding space (GloVe) where grammatical gender information is preserved across languages. The binary classifier learns these cross-lingual patterns during training and can apply them to unseen languages.
- Core assumption: The semantic representations of gender-associated adjectives in different languages overlap significantly in the shared embedding space after translation.
- Evidence anchors:
  - [abstract] "Surprisingly, we find that a simple classifier can not only predict noun gender above chance but also exhibit cross-language transferability."
  - [section] "We suggest that although the LLM uses different adjectives to describe masculine and feminine nouns in different languages (hence low Spq), they are semantically similar (hence high accuracy when evaluating the classifier on an unseen language)."
  - [corpus] Weak - The corpus evidence shows related papers exist but no direct evidence that GloVe embeddings preserve cross-linguistic gender patterns.
- Break condition: If the semantic space does not preserve gender information across languages, or if the translation process introduces significant noise that obscures gender associations.

### Mechanism 2
- Claim: The LLM generates gender-biased adjectives through learned co-occurrence patterns from training corpora.
- Mechanism: During pretraining, multilingual LLMs observe patterns where certain adjectives co-occur more frequently with masculine or feminine nouns across languages. These patterns become encoded in the model's weights, leading to systematic adjective generation that reflects grammatical gender biases present in the training data.
- Core assumption: The training corpora contain sufficient examples of gendered adjective-noun pairs for the LLM to learn and reproduce these patterns.
- Evidence anchors:
  - [abstract] "we prompt a model to describe nouns with adjectives in various languages, focusing specifically on languages with grammatical gender."
  - [section] "we see that adjectives like intricate and desolate are associated with feminine nouns, whereas adjectives like dedicated and brave are associated with masculine nouns."
  - [corpus] Weak - No corpus evidence directly measuring adjective-noun co-occurrence patterns in the training data.
- Break condition: If the training corpora lack sufficient gendered adjective-noun pairs, or if the LLM's pretraining objective does not capture these co-occurrence patterns effectively.

### Mechanism 3
- Claim: The translation to English enables cross-language classification by removing grammatical gender from adjectives themselves.
- Mechanism: In many languages, adjectives agree in gender with the nouns they modify (e.g., "bonito" vs "bonita" in Spanish). By translating all adjectives to English before classification, the model removes this surface-level gender cue, forcing the classifier to rely on semantic gender associations rather than grammatical form.
- Core assumption: English adjectives do not carry grammatical gender information that could be used as a shortcut by the classifier.
- Evidence anchors:
  - [abstract] "We do this for two reasons. Firstly, adjectives in some languages are also gendered and that would help the classifier learn this shortcut (e.g. pretty in Spanish is bonito and bonita for masculine and feminine, respectively)."
  - [section] "Adjectives in English are not gendered, so the classifier Φ has no way of inferring the gender of the noun from the grammatical form."
  - [corpus] Weak - No corpus evidence quantifying the extent to which English adjectives lack gender information compared to other languages.
- Break condition: If English adjectives or their embeddings contain subtle gender associations that the classifier could exploit.

## Foundational Learning

- Concept: Grammatical gender systems across languages
  - Why needed here: Understanding how different languages assign and mark gender is crucial for interpreting the results and designing appropriate experiments
  - Quick check question: How many gender categories exist in German, and how do adjectives agree with nouns in gender?

- Concept: Word embedding spaces and cross-lingual alignment
  - Why needed here: The method relies on projecting adjectives from different languages into a shared semantic space for classification
  - Quick check question: What properties must a cross-lingual embedding space have to preserve gender-related semantic distinctions?

- Concept: Zero-shot learning and transfer learning
  - Why needed here: The key finding is that a classifier trained on one language can predict gender in unseen languages
  - Quick check question: What conditions must hold for zero-shot transfer to work between languages in a classification task?

## Architecture Onboarding

- Component map:
  LLM (Mistral-7B/Llama2-7B) → Noun description generation → Noun collection pipeline → Gender-labeled noun databases per language → Translation module → English conversion of generated adjectives → Feature extraction → GloVe embeddings weighted by frequency → Classifier → 2-layer MLP with binary cross-entropy loss → Evaluation pipeline → Cross-language and within-language testing

- Critical path: Noun → LLM description → Adjective extraction → Translation → Embedding → Classification → Evaluation

- Design tradeoffs:
  - Using frequency-weighted embeddings vs. raw counts
  - Translating to English vs. training separate classifiers per language
  - Few-shot prompting vs. zero-shot prompting for adjective generation
  - Including/excluding animate nouns in the dataset

- Failure signatures:
  - Low classifier accuracy on both seen and unseen languages suggests adjective generation is not gender-biased
  - High accuracy on seen languages but poor transfer suggests language-specific rather than language-agnostic encoding
  - Translation errors that systematically distort gender associations

- First 3 experiments:
  1. Test classifier accuracy within the same language it was trained on (baseline)
  2. Test zero-shot transfer to a closely related language (e.g., Spanish → Italian)
  3. Test zero-shot transfer to a distantly related language (e.g., German → Hindi)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed grammatical gender biases in multilingual LLMs reflect actual human psycholinguistic biases across different cultures?
- Basis in paper: [explicit] The paper suggests future work should study how well such biases in LLMs are predictive of biases of humans
- Why unresolved: The current study only examines LLM behavior and does not compare it to human data. Human psycholinguistic experiments are complex and difficult to control across different cultures and languages.
- What evidence would resolve it: Controlled psycholinguistic experiments with human participants across the studied languages, comparing human responses to LLM outputs for the same noun-adjective generation tasks.

### Open Question 2
- Question: Do the grammatical gender biases persist and remain consistent in larger LLMs or commercial models like GPT-4?
- Basis in paper: [explicit] The limitations section states they only conducted experiments on Mistral-7B and Llama2-7B models and are unsure if similar effects can be observed in larger or commercial LLMs
- Why unresolved: The study was limited to two open-source models. Larger models may have different training data or architectural differences that could affect their gender bias patterns.
- What evidence would resolve it: Replicating the same experiments with larger and commercial LLMs like GPT-4, Claude, or Gemini to compare the consistency of grammatical gender biases across model sizes and architectures.

### Open Question 3
- Question: How do the grammatical gender biases in multilingual LLMs affect downstream applications like machine translation or text generation involving anthropomorphism?
- Basis in paper: [explicit] The discussion mentions importance of results for LLMs used to describe humans using objects or vice versa, and for machine translation of such phrases
- Why unresolved: The paper only demonstrates the existence of biases but doesn't explore their practical impact on real-world applications or quantify the potential negative consequences.
- What evidence would resolve it: Systematic evaluation of how grammatical gender biases in LLMs affect outputs in specific downstream tasks like machine translation of gendered phrases, character description generation, or metaphorical language use, measuring the frequency and impact of biased outputs.

## Limitations
- Limited to two open-source models (Mistral-7B and Llama2-7B), making generalization to larger or commercial LLMs uncertain
- Small test sets (100-200 nouns per language pair) may not capture the full variability of gender bias patterns
- Translation artifacts could create spurious correlations that inflate cross-language transferability scores

## Confidence
- High confidence: The methodology for collecting gendered adjective distributions is well-specified and reproducible. The observation that classifiers achieve above-chance accuracy within languages is robust.
- Medium confidence: The cross-language transferability results are surprising and theoretically interesting, but the evidence base is limited by the small number of test nouns per language pair (100-200) and potential confounds from translation artifacts.
- Low confidence: The interpretation that transferability demonstrates "language-agnostic" gender encoding is speculative without direct probing of the LLM's internal representations.

## Next Checks
1. Test classifier performance on held-out adjective sets that exclude high-frequency, low-informativeness adjectives (e.g., "new", "important") to verify that transferability relies on meaningful semantic associations rather than statistical noise

2. Compare classifier accuracy when using language-specific embeddings versus translated English embeddings to isolate whether cross-language transfer is truly semantic or an artifact of the translation process

3. Conduct ablation studies removing different language families (e.g., Romance vs. Slavic) to determine whether transferability patterns reflect typological similarities or deeper semantic regularities