---
ver: rpa2
title: Speeding up Policy Simulation in Supply Chain RL
arxiv_id: '2406.01939'
source_url: https://arxiv.org/abs/2406.01939
tags:
- policy
- iteration
- picard
- time
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck in policy simulation
  for large-scale supply chain optimization problems, where simulating a single trajectory
  under a policy can take several hours due to the massive number of time steps involved.
  The authors propose Picard Iteration, an iterative algorithm that assigns policy
  evaluation tasks to independent processes while caching evaluations for other tasks,
  allowing batched evaluation on GPUs.
---

# Speeding up Policy Simulation in Supply Chain RL

## Quick Facts
- arXiv ID: 2406.01939
- Source URL: https://arxiv.org/abs/2406.01939
- Reference count: 40
- Primary result: Achieves 400x speedup in policy simulation for large-scale supply chain problems using Picard Iteration

## Executive Summary
This paper addresses the computational bottleneck in policy simulation for large-scale supply chain optimization, where simulating a single trajectory can take several hours due to the massive number of time steps involved. The authors propose Picard Iteration, an iterative algorithm that assigns policy evaluation tasks to independent processes while caching evaluations for other tasks, allowing batched evaluation on GPUs. They prove that for fulfillment optimization problems, Picard Iteration converges in a small number of iterations independent of the horizon, guaranteeing substantial speedup. Experiments demonstrate practical speedups of 400x over sequential simulation on large-scale problems using a single GPU.

## Method Summary
Picard Iteration is an iterative algorithm that accelerates policy simulation by distributing policy evaluation tasks across independent processes. The algorithm partitions the simulation horizon into disjoint sets of time steps, assigns each partition to a process, and caches actions for time steps not assigned to that process. Within each iteration, processes evaluate the policy in parallel using their cached values for non-assigned time steps. For fulfillment optimization problems, the method converges in QT+1 iterations, where Q is the number of nodes and T is the horizon. The approach leverages GPU batching for policy evaluation and requires only a single GPU for the reported 400x speedups.

## Key Results
- Achieves 400x speedup over sequential simulation for large-scale supply chain problems using a single GPU
- Converges in at most QT+1 iterations for fulfillment optimization problems (Q = nodes, T = horizon)
- Guarantees convergence in T iterations for any MDP, making it a valid simulation method even without speedup
- Demonstrates similar speedups in end-to-end policy optimization pipelines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Picard iteration achieves speedup by batching policy evaluations while avoiding message passing overhead
- Mechanism: The algorithm divides the simulation horizon into partitions, assigns each partition to a process, and caches actions for time-steps not assigned to that process. This allows batched evaluation of the policy across processes without requiring inter-process communication.
- Core assumption: Policy evaluation is computationally expensive while system dynamics evaluation is cheap
- Evidence anchors:
  - [abstract] "We present an iterative algorithm to accelerate policy simulation, dubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks to independent processes. Within an iteration, any given process evaluates the policy only on its assigned tasks while assuming a certain 'cached' evaluation for other tasks"
  - [section 1.2] "each process only takes time that is roughly proportional to the number of time-steps it is assigned"
- Break condition: If policy evaluation is not significantly more expensive than system dynamics evaluation, the speedup advantage disappears

### Mechanism 2
- Claim: Convergence in small number of iterations for fulfillment optimization problems
- Mechanism: The structure of fulfillment optimization problems ensures that incorrect actions converge to correct ones within a bounded number of iterations based on the number of nodes in the supply chain
- Core assumption: The policy satisfies regularity conditions (Inventory Independence, Consistency, Monotonicity)
- Evidence anchors:
  - [section 3.2] "Provided π(·) satisfies Inventory Independence, Consistency and Monotonicity, Algorithm 1 converges in at most QT + 1 iterations for the FO problem"
  - [section 3.1] "For the special greedy case, the 'right' structure properties turn out to be related to the set of nodes that run out of capacity in the sequential scenarios"
- Break condition: If the policy violates any of the three regularity assumptions, convergence may require more iterations

### Mechanism 3
- Claim: Generalization to general RL environments with convergence in at most T iterations
- Mechanism: The algorithm is guaranteed to produce the correct trajectory after at most T iterations in any MDP, making it a valid simulation method even if speedup is not achieved
- Core assumption: The MDP has finite horizon T
- Evidence anchors:
  - [section 3] "Proposition 2.1. The Picard iteration converges in at most T iterations and returns {aseq t }"
  - [section 5] "Defining the state trajectory simulated at the kth Picard iteration... we measure convergence here via relative root-mean-squared-error... This quantity is guaranteed to be 0 (up to numerical precision) for k = T = 200 by Proposition 2.1"
- Break condition: If the MDP has infinite horizon or if numerical precision issues arise, convergence guarantees may not hold

## Foundational Learning

- Concept: Parallel discrete event simulation
  - Why needed here: Understanding why traditional approaches like Time Warp fail for general MDPs helps explain why Picard iteration is necessary
  - Quick check question: What is the key limitation of Time Warp that makes it unsuitable for general MDP trajectory simulation?

- Concept: Dynamic programming and Bellman equations
  - Why needed here: The iterative nature of Picard iteration resembles value iteration, where cached values are updated until convergence
  - Quick check question: How does the Picard iteration update rule relate to value iteration in dynamic programming?

- Concept: GPU batching and vectorization
  - Why needed here: The speedup comes from batching policy evaluations, which requires understanding how to leverage GPU parallelism
  - Quick check question: What JAX operation is used to implement the parallel evaluation of processes in the Picard iteration?

## Architecture Onboarding

- Component map: Partition assignment -> Parallel policy evaluation with caching -> State update -> Cache update -> Convergence check -> Repeat until convergence
- Critical path: Partition assignment → Parallel policy evaluation with caching → State update → Cache update → Convergence check → Repeat until convergence
- Design tradeoffs:
  - Partition granularity vs. conflict resolution: Smaller partitions enable more parallelism but increase cache conflicts
  - Cache initialization strategy: Using previous policy iterate as initial cache can accelerate convergence
  - Chunk size selection: Balances synchronization overhead against wasted computation
- Failure signatures:
  - Linear scaling with batch size breaking down: Indicates too many cache conflicts
  - Convergence taking more iterations than expected: Suggests policy violates regularity assumptions
  - GPU memory overflow: Indicates partition sizes or chunk sizes are too large
- First 3 experiments:
  1. Run sequential baseline vs. Picard with M=1 to verify correctness
  2. Vary batch size M and measure speedup scaling to verify linear relationship
  3. Test different partition strategies (product-based vs. uniform) on heavy-tailed demand to verify robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise convergence rate (in terms of K, J, and M) for Picard Iteration in the Fulfillment Problem with Refreshing Capacity, and how does it compare to the theoretical bound O(T(K+J)M/P)?
- Basis in paper: [explicit] Theorem C.1 provides a computational complexity bound for the Fulfillment Problem with Refreshing Capacity, but does not specify the exact convergence rate.
- Why unresolved: The theorem provides an upper bound on computational complexity but does not give a precise convergence rate. Empirical validation of this bound and comparison with actual convergence behavior is needed.
- What evidence would resolve it: Empirical studies measuring the actual number of iterations required for convergence in various problem instances with different values of K, J, and M, and comparing these results to the theoretical bound.

### Open Question 2
- Question: How does the performance of Picard Iteration degrade when the policy π violates the monotonicity assumption (Assumption 3) in the Fulfillment Optimization problem, and what are the practical limits of this violation?
- Basis in paper: [explicit] The paper mentions an ablation study in Appendix A.4 where they test a policy that discounts node proximity by (remaining) capacity scarcity, which violates Assumption 3. They observe that performance is "virtually unchanged" for realistic settings of γ.
- Why unresolved: The ablation study only tests a specific type of violation (capacity-dependent policies). The extent to which other types of violations affect performance, and the practical limits of these violations, remain unexplored.
- What evidence would resolve it: Systematic testing of Picard Iteration with policies that violate Assumption 3 in various ways (e.g., policies that depend on both inventory and capacity in complex ways), measuring the impact on convergence speed and accuracy.

### Open Question 3
- Question: Can Picard Iteration be effectively applied to problems with continuous action spaces, and if so, what modifications are necessary to maintain its theoretical guarantees and practical performance?
- Basis in paper: [inferred] The paper focuses on discrete action spaces (e.g., Fulfillment Optimization, Inventory Control). While Picard Iteration is described as applicable to general RL environments, its extension to continuous action spaces is not explicitly discussed.
- Why unresolved: The current theoretical analysis and practical implementations assume discrete action spaces. Extending these to continuous action spaces requires addressing new challenges, such as the representation of actions and the computation of gradients.
- What evidence would resolve it: Implementation and testing of Picard Iteration on RL problems with continuous action spaces (e.g., MuJoCo environments with continuous control), along with theoretical analysis of the convergence properties in this setting.

## Limitations

- The convergence analysis relies heavily on specific structure of fulfillment optimization problems that may not generalize to all RL environments
- Speedup claims depend on relative cost of policy evaluation versus system dynamics, which varies across applications
- Experimental evaluation focuses on single-GPU setups, leaving multi-GPU scalability questions unanswered

## Confidence

**High Confidence**: The correctness of the Picard iteration algorithm itself (Proposition 2.1 guaranteeing convergence in T iterations for any MDP), the general approach of batching policy evaluations, and the basic mechanism of using cached actions across processes.

**Medium Confidence**: The specific convergence bound of QT+1 iterations for fulfillment optimization problems, the practical speedup measurements (400x) on the tested problem sizes, and the claim that the method generalizes well to general RL environments.

**Low Confidence**: The robustness of the method to policies that violate the three regularity assumptions, the scalability beyond single-GPU setups, and the performance on problems with different structural characteristics than the tested supply chain scenarios.

## Next Checks

1. **Convergence behavior under policy violations**: Systematically test Picard iteration on policies that violate each of the three regularity assumptions (Inventory Independence, Consistency, Monotonicity) to measure how convergence degrades and whether the QT+1 bound still provides practical speedups.

2. **Cross-domain generalization**: Apply Picard iteration to RL environments from the OpenAI Gym suite (e.g., LunarLander, CartPole) to validate the claim that convergence occurs in T iterations for general MDPs and to measure practical speedups in non-supply-chain contexts.

3. **Multi-GPU scaling evaluation**: Implement a distributed version of Picard iteration across multiple GPUs and measure how speedup scales with additional devices, identifying the point where communication overhead begins to dominate and determining the practical limits of the approach.