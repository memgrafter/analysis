---
ver: rpa2
title: 'From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks'
arxiv_id: '2409.14623'
source_url: https://arxiv.org/abs/2409.14623
tags:
- learning
- network
- networks
- dynamics
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work derives exact learning dynamics for deep linear neural\
  \ networks with \u03BB-balanced initializations, where \u03BB characterizes the\
  \ relative scale of weights across layers. Using matrix Riccati equations, the authors\
  \ provide analytical solutions for the Neural Tangent Kernel (NTK), representations,\
  \ and network function across the full spectrum from rich (feature-learning) to\
  \ lazy (kernel) regimes."
---

# From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks

## Quick Facts
- arXiv ID: 2409.14623
- Source URL: https://arxiv.org/abs/2409.14623
- Reference count: 40
- Exact analytical solutions for deep linear networks reveal how λ-balanced initialization governs transitions between lazy and rich learning regimes.

## Executive Summary
This paper derives exact learning dynamics for deep linear neural networks with λ-balanced initializations, where λ characterizes the relative scale of weights across layers. Using matrix Riccati equations, the authors provide analytical solutions for the Neural Tangent Kernel (NTK), representations, and network function across the full spectrum from rich (feature-learning) to lazy (kernel) regimes. The relative scale λ emerges as a key factor governing the transition between regimes: small λ leads to sigmoidal, feature-rich dynamics, while large |λ| produces exponential, kernel-like behavior. These solutions extend prior work beyond zero-balanced networks, offering insights into how initialization strategy impacts learning regimes and generalization in both neuroscience and machine learning contexts.

## Method Summary
The authors analyze two-layer linear neural networks trained with gradient descent on mean squared error loss. They introduce λ-balanced initialization where W₂ᵀW₂ - W₁W₁ᵀ = λI at initialization, and derive exact solutions for the network function evolution using matrix Riccati equations. The analysis covers random regression tasks, semantic hierarchy tasks, reversal learning, and continual learning scenarios. Analytical predictions are validated through numerical simulations comparing the NTK evolution, representation similarity matrices, and loss dynamics across different λ values and network architectures.

## Key Results
- λ-balanced initialization governs the transition between lazy and rich regimes, with small λ producing sigmoidal dynamics and large |λ| producing exponential dynamics
- The NTK evolution depends critically on λ, with static NTK in the lazy regime and dynamic NTK in the rich regime
- Network architecture interacts with the sign of λ to determine the learning regime, with funnel and inverted-funnel networks showing delayed rich phases for large λ values
- Exact solutions reveal that λ influences representation robustness to parameter noise and the structure of learned representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The λ-balanced initialization condition governs the transition between lazy and rich regimes in deep linear networks.
- Mechanism: By enforcing the condition W₂ᵀW₂ - W₁W₁ᵀ = λI at initialization, the relative scale of weights across layers is fixed, which influences the dynamics of the singular values and the evolution of the Neural Tangent Kernel (NTK). Small λ values lead to sigmoidal dynamics characteristic of the rich regime, while large |λ| values lead to exponential dynamics characteristic of the lazy regime.
- Core assumption: The balanced condition persists through training due to the conservation of the quantity W₂ᵀW₂ - W₁W₁ᵀ under gradient flow dynamics.
- Evidence anchors:
  - [abstract]: "These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes."
  - [section]: "Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning."
  - [corpus]: The related paper "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning" supports the importance of unbalanced initialization in feature learning.
- Break condition: If the balanced condition is violated during training or if the input data is not whitened, the exact solutions may no longer hold, and the dynamics may deviate from the predicted lazy or rich regimes.

### Mechanism 2
- Claim: The dynamics of the singular values of the network function determine the learning regime.
- Mechanism: In the task-aligned initialization case, the singular values of the network function evolve according to a λ-dependent transition function. This function transitions from sigmoidal dynamics (rich regime) when λ approaches zero to exponential dynamics (lazy regime) when λ approaches positive or negative infinity.
- Core assumption: The task-aligned initialization assumption ensures that the network remains aligned with the task throughout training, restricting the dynamics to the singular values of the network function.
- Evidence anchors:
  - [abstract]: "These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes."
  - [section]: "Our analysis demonstrates that the relative scale, λ, is pivotal in managing this transition. Notably, we identify a structured lazy regime that promotes transfer learning."
  - [corpus]: The related paper "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes" provides a unifying formula for the evolution of the learned matrix, supporting the role of singular values in determining the learning regime.
- Break condition: If the initialization is not task-aligned, the dynamics may not be restricted to the singular values, and the predicted transition between regimes may not occur.

### Mechanism 3
- Claim: The architecture of the network interacts with the sign of the relative scale parameter λ to determine the learning regime.
- Mechanism: In funnel networks (Ni > Nh = No), large positive λ values lead to the lazy regime, while in inverted-funnel networks (Ni = Nh < No), large negative λ values lead to the lazy regime. The NTK remains static during the initial phase, confirming the rank argument for the multi-output setting. In the opposite limits of λ, these networks transition from a lazy regime to a rich regime, exhibiting a delayed rich phase.
- Core assumption: The network architectures are funnel, square, or inverted-funnel networks, and the relative scale parameter λ is sufficiently large in magnitude.
- Evidence anchors:
  - [abstract]: "Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning."
  - [section]: "Our solution, QQT, captures the dynamics of the NTK across these different network architectures."
  - [corpus]: The related paper "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning" discusses the impact of architecture on the learning regime.
- Break condition: If the network architecture does not fit the funnel, square, or inverted-funnel categories, or if the relative scale parameter λ is not sufficiently large in magnitude, the predicted interaction between architecture and λ may not occur.

## Foundational Learning

- Concept: Matrix Riccati equations
  - Why needed here: Matrix Riccati equations are used to derive exact solutions for the gradient flow dynamics of the network's weight matrices.
  - Quick check question: Can you explain how the matrix Riccati equation relates to the gradient flow dynamics of the network's weight matrices?

- Concept: Singular value decomposition (SVD)
  - Why needed here: SVD is used to diagonalize the matrix F and to analyze the dynamics of the singular values of the network function.
  - Quick check question: How does the SVD of the input-output correlation matrix relate to the eigendecomposition of the matrix F?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The NTK captures the dynamics of the network's internal representations and determines whether the network operates in the lazy or rich regime.
  - Quick check question: Can you describe the relationship between the NTK and the network's internal representations?

## Architecture Onboarding

- Component map:
  - Input layer (Xi) -> Hidden layer (W1) -> Output layer (W2) -> Task-aligned initialization -> λ-balanced condition -> Matrix Riccati equation -> Singular value decomposition -> Neural Tangent Kernel

- Critical path:
  1. Initialize the network with λ-balanced weights.
  2. Derive the matrix Riccati equation for the gradient flow dynamics.
  3. Diagonalize the matrix F using SVD.
  4. Solve for the dynamics of the singular values of the network function.
  5. Analyze the evolution of the Neural Tangent Kernel.
  6. Determine the learning regime based on the dynamics of the singular values and the NTK.

- Design tradeoffs:
  - Relaxing the whitened input assumption may make the analysis more applicable to real-world scenarios but may prevent the derivation of exact solutions.
  - Allowing for unequal input and output dimensions increases the flexibility of the framework but may introduce additional complexity in the analysis.
  - Extending the analysis to deep networks would provide a more comprehensive understanding of the learning dynamics but may require more advanced mathematical techniques.

- Failure signatures:
  - If the balanced condition is violated during training, the exact solutions may no longer hold, and the dynamics may deviate from the predicted lazy or rich regimes.
  - If the input data is not whitened, the derivation of the matrix Riccati equation may not be valid, and the exact solutions may not apply.
  - If the network architecture does not fit the funnel, square, or inverted-funnel categories, the predicted interaction between architecture and λ may not occur.

- First 3 experiments:
  1. Implement the λ-balanced initialization scheme and train the network on a random regression task with varying values of λ. Analyze the dynamics of the singular values and the NTK to determine the learning regime.
  2. Modify the architecture of the network to be a funnel, square, or inverted-funnel network and repeat the experiment with varying values of λ. Observe the interaction between the architecture and the relative scale parameter.
  3. Apply the exact solutions to a continual learning scenario by training the network on a sequence of tasks. Analyze the amount of forgetting and the dynamics of the network's internal representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the exact learning dynamics change when relaxing the whitened input assumption (A1)?
- Basis in paper: [inferred] The paper explicitly states that the whitened input assumption is strong but commonly used, and references Kunin et al. (2024) who examine implicit bias without whitened inputs.
- Why unresolved: The current solutions rely on whitened inputs for exact analytical tractability. Relaxing this assumption would make the problem intractable in its current form.
- What evidence would resolve it: Analytical or numerical solutions for the learning dynamics without whitened inputs, showing how the λ-balanced property and rich/lazy regime transitions are affected.

### Open Question 2
- Question: How does the rich-to-lazy regime transition generalize to deep networks with more than two layers?
- Basis in paper: [inferred] The paper focuses on two-layer networks and mentions extending to deep networks as future work. The discussion section notes that extending the λ-balanced initialization to deep networks is left for future work.
- Why unresolved: The current analysis is limited to two-layer networks, and the interactions between multiple layers under λ-balanced initialization are not yet understood.
- What evidence would resolve it: Exact or approximate solutions for the learning dynamics of deep λ-balanced networks, showing how the relative scale parameter λ affects the transition between rich and lazy regimes across multiple layers.

### Open Question 3
- Question: What is the impact of λ-balanced initialization on the learning dynamics of nonlinear networks?
- Basis in paper: [explicit] The paper references Kunin et al. (2024) who show that findings extend to basic nonlinear settings, but notes that the current work is limited to linear networks.
- Why unresolved: The current solutions are derived for linear networks, and the nonlinear case introduces additional complexity through activation functions that affect the NTK and learning dynamics.
- What evidence would resolve it: Analytical or empirical studies of nonlinear networks with λ-balanced initialization, showing how the relative scale parameter influences feature learning, NTK evolution, and generalization compared to the linear case.

## Limitations
- Exact solutions rely on whitened input assumption, which is strong but commonly used in theoretical analyses
- Analysis is limited to two-layer linear networks, with extension to deep networks left for future work
- The λ-balanced initialization condition must persist throughout training for the exact solutions to remain valid

## Confidence
- Mathematical derivations: High
- Theoretical implications for initialization strategy: Medium
- Practical relevance to real-world deep learning scenarios: Medium

## Next Checks
1. Implement the stable solution from Theorem B.5 and compare its accuracy and numerical stability against the naive formulation for long training times, particularly for large λ values where exponential terms may cause overflow.
2. Relax the whitened input assumption and test whether the λ-balanced condition still captures the lazy-to-rich transition, or if the transition depends on additional factors like input covariance structure.
3. Test the λ-balanced initialization across a broader range of architectures beyond the funnel/square/inverted-funnel categories to determine if the observed interactions between architecture and λ are robust or architecture-specific.