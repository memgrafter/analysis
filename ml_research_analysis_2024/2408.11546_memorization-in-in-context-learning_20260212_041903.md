---
ver: rpa2
title: Memorization in In-Context Learning
arxiv_id: '2408.11546'
source_url: https://arxiv.org/abs/2408.11546
tags:
- memorization
- performance
- matches
- exact
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how in-context learning (ICL) in large
  language models relates to memorization of training data. The authors adapt a data
  contamination detection method to quantify memorization across ICL regimes by prompting
  the model to complete dataset instances using demonstrations.
---

# Memorization in In-Context Learning

## Quick Facts
- arXiv ID: 2408.11546
- Source URL: https://arxiv.org/abs/2408.11546
- Reference count: 40
- Primary result: ICL significantly surfaces memorization compared to zero-shot learning, with demonstrations being the key driver

## Executive Summary
This paper investigates the relationship between in-context learning (ICL) and memorization in large language models. The authors adapt a data contamination detection method to quantify memorization across different ICL regimes by prompting the model to complete dataset instances using demonstrations. They find that ICL surfaces significantly more memorization than zero-shot learning, with segment pairs (without labels) being the key element driving this effect. Memorization increases sharply with few demonstrations (25 shots) and plateaus at higher shot counts. The study reveals a strong correlation between memorization and performance improvements when ICL outperforms zero-shot learning, suggesting that memorization plays a key role in ICL effectiveness.

## Method Summary
The authors adapt a data contamination detection method to quantify memorization in ICL by prompting GPT-4 to complete dataset instances using demonstrations. They sample 200 instances from each dataset's train split, split each instance into two random-length segments (60-80% for initial segment), and prepare demonstrations for k-shot ICL by sampling 200 demonstrations per dataset and splitting them similarly. For each regime (k=0,25,50,100,200) and setting, they prompt GPT-4 with appropriate demonstrations and dataset-specific instructions to generate completions for subsequent segments. Generated completions are evaluated against original subsequent segments using exact and near-exact matching, with memorization percentage calculated as (exact + near-exact matches) / total instances. Performance is measured using standard ICL accuracy, and Pearson correlation and R² are computed between memorization and performance.

## Key Results
- ICL surfaces significantly more memorization than zero-shot learning across all tested shot counts
- Demonstrations (segment pairs without labels) are the key element driving memorization in ICL
- Memorization increases sharply with few demonstrations (25 shots) and plateaus at higher shot counts
- Strong correlation between memorization and performance improvements when ICL outperforms zero-shot learning

## Why This Works (Mechanism)
The paper demonstrates that in-context learning surfaces memorization through demonstrations that provide context similar to training data. The mechanism appears to work because demonstrations activate the model's memory of similar training instances, making memorized content more accessible during generation. This effect is particularly pronounced with segment pairs without labels, suggesting that the model leverages pattern matching and contextual similarity rather than just label-based recall.

## Foundational Learning
- Data contamination detection - needed to quantify memorization in ICL; quick check: verify exact and near-exact matching criteria are consistently applied
- In-context learning paradigms - needed to understand how demonstrations affect model behavior; quick check: confirm demonstration formatting follows established ICL practices
- Memorization vs generalization distinction - needed to interpret correlation results; quick check: validate that memorization measurement captures actual training data overlap

## Architecture Onboarding

Component map: GPT-4 model -> demonstration prompts -> completion generation -> evaluation prompts -> memorization quantification

Critical path: Dataset preparation → Demonstration formatting → Prompt construction → Generation → Evaluation → Memorization calculation → Performance correlation

Design tradeoffs: The study uses GPT-4 with 32k context for maximum demonstration capacity versus computational cost, and segment splitting at 60-80% to balance context availability with realistic completion scenarios.

Failure signatures: Low memorization percentages may indicate improper segment splitting or demonstration formatting; high correlation may be inflated by data overlap between memorization measurement and performance evaluation.

First experiments:
1. Test memorization detection on held-out evaluation sets separate from those used for memorization measurement
2. Replicate memorization measurement using different segment splitting ratios (50-70%, 70-90%)
3. Compare memorization levels when demonstrations include labels versus only segment pairs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How much of ICL performance gains come from memorization versus generalization?
- Basis in paper: The paper finds strong correlation between memorization and performance when ICL outperforms zero-shot, but notes that memorization plateaus while generalization may continue to improve
- Why unresolved: The authors quantify memorization levels but don't disentangle how much of the remaining performance gain comes from true generalization rather than memorization
- What evidence would resolve it: Experiments comparing ICL performance on instances that were definitely not in training data versus those that were, or ablation studies showing how much performance drops when removing memorized content

### Open Question 2
- Question: What is the relationship between demonstration order and memorization in many-shot regimes?
- Basis in paper: The paper notes that demonstration order matters in few-shot regimes but its impact diminishes in many-shot regimes, though this claim needs more rigorous validation
- Why unresolved: The paper randomly orders demonstrations but doesn't systematically test different ordering strategies or quantify their impact on memorization levels
- What evidence would resolve it: Controlled experiments varying demonstration order systematically across many-shot regimes while measuring memorization and performance changes

### Open Question 3
- Question: How does the mechanism of memorization in ICL differ from standard prompting?
- Basis in paper: The paper shows that demonstrations without labels (just segment pairs) are the key element in surfacing memorization, suggesting a unique mechanism beyond simple retrieval
- Why unresolved: The authors don't explore whether this memorization emerges from the same mechanisms as standard prompting or represents a distinct phenomenon in how LLMs process demonstrations
- What evidence would resolve it: Comparative analysis of memory traces and attention patterns during ICL versus standard prompting on identical content, or experiments varying the format and structure of demonstrations

## Limitations
- Results may not generalize to smaller models or different context window sizes
- Focus on four text classification datasets may not capture the full diversity of ICL applications
- Memorization detection relies on exact and near-exact matching which may miss more nuanced forms of memorization

## Confidence
- That ICL surfaces memorization more than zero-shot learning: High confidence
- That demonstrations (segment pairs without labels) are the key driver: High confidence
- That memorization correlates with performance improvements: Medium confidence
- That memorization is a key factor in ICL effectiveness: Low confidence

## Next Checks
1. Test the memorization detection method on held-out evaluation sets separate from those used for memorization measurement to ensure the correlation isn't inflated by data overlap
2. Replicate the study using different model sizes (smaller LLMs) and different segment splitting strategies to assess robustness of findings
3. Conduct ablation studies isolating the contribution of memorization versus other potential mechanisms (attention patterns, implicit reasoning) to ICL performance improvements