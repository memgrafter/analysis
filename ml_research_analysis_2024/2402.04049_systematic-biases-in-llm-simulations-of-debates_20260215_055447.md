---
ver: rpa2
title: Systematic Biases in LLM Simulations of Debates
arxiv_id: '2402.04049'
source_url: https://arxiv.org/abs/2402.04049
tags:
- agents
- agent
- biases
- fine-tuning
- simulations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study examines the influence of inherent biases in large language\
  \ models (LLMs) on the behavior of agents in simulations of political debates. The\
  \ researchers found that agents consistently align with the LLM\u2019s biases, even\
  \ when those biases contradict their assigned political identities."
---

# Systematic Biases in LLM Simulations of Debates

## Quick Facts
- arXiv ID: 2402.04049
- Source URL: https://arxiv.org/abs/2402.04049
- Reference count: 14
- Agents consistently align with LLM's inherent biases during political debates, even when assigned opposing political identities

## Executive Summary
This study reveals that large language model agents in political debate simulations tend to conform to the model's inherent social and political biases, regardless of their assigned political identities. The research demonstrates that these biases cause agents to adopt more moderate positions over time, inverting expected echo chamber effects where like-minded individuals typically reinforce their beliefs. The authors developed a fine-tuning method that manipulates these biases, showing that agents subsequently align with the altered biases while retaining their original contexts.

## Method Summary
The researchers conducted multi-agent debates using three open-source LLMs (Instruct-GPT, Mistral 7B, Solar 10.7B) with Republican and Democrat partisan agents plus a neutral Default agent. Agents debated topics like gun violence, racism, climate change, and illegal immigration in round-robin format. Attitude changes were tracked through pre- and post-debate surveys where agents rated topic severity on a 0-10 scale. The team developed a self-fine-tuning method using agent-generated data to manipulate the LLM's inherent biases and re-ran simulations to observe behavioral changes.

## Key Results
- Agents consistently align their attitudes with the base LLM's inherent biases during debates, regardless of assigned political identity
- Echo chamber effects are inverted - like-minded agents moderate their views toward the model's default bias rather than reinforcing them
- Fine-tuning the LLM on self-generated data successfully shifts agent behavior to align with the new biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents align their attitudes with the base model's inherent biases during debates, regardless of their assigned political identity
- Core assumption: The LLM's output generation process is not fully separable from its underlying bias structure
- Evidence: Agents gradually align more closely with the Default agent's position despite being directed to debate from certain political perspectives

### Mechanism 2
- Claim: Echo chamber effects are inverted in LLM simulations
- Core assumption: The LLM's internal bias acts as a gravitational center that pulls all agent outputs toward it
- Evidence: Agents tend to adopt more moderate positions, aligning more closely with the LLM's inherent bias, deviating from real-world echo chamber dynamics

### Mechanism 3
- Claim: Fine-tuning the base LLM shifts agents' behavior to align with the new bias
- Core assumption: The LLM's learned representations are malleable enough that targeted fine-tuning can overwrite default biases
- Evidence: When fine-tuning method is employed to change the LLMs' viewpoints, agents modify their behavior to be in line with the newly introduced bias

## Foundational Learning

- Concept: Role-based prompting in LLMs
  - Why needed here: The study relies on assigning political identities to agents through natural language prompts to simulate partisan behavior
  - Quick check question: What happens to an LLM's output if you prepend a role description like "You are a passionate Republican..." to a prompt?

- Concept: Statistical bias in language models
  - Why needed here: The core finding is that LLM agents inherit and express the model's inherent social and political biases during interactions
  - Quick check question: How might training data composition influence the political or social biases present in an LLM's outputs?

- Concept: Fine-tuning via next-word prediction
  - Why needed here: The study uses a lightweight fine-tuning approach to adjust the LLM's biases by training on self-generated agent responses
  - Quick check question: What is the difference between full fine-tuning and parameter-efficient methods like QLoRA when adapting an LLM?

## Architecture Onboarding

- Component map: Base LLM -> Role assignment prompts -> Multi-agent debate environment -> Attitude tracking via surveys -> Optional fine-tuning pipeline -> Re-simulation
- Critical path: Generate agent contexts -> Run debate simulation -> Monitor attitude shifts -> (Optional) Fine-tune model -> Re-run simulation to validate bias manipulation
- Design tradeoffs: Using open-source models allows fine-tuning but may sacrifice some conversational quality compared to proprietary models; lightweight fine-tuning preserves general performance but may not fully eliminate deep-seated biases
- Failure signatures: Agents converge toward a middle ground regardless of political identity; echo chamber effects are inverted; fine-tuned models show degraded performance on general benchmarks
- First 3 experiments:
  1. Run a three-way debate (Republican, Democrat, Default) on a single topic and track attitude changes over rounds
  2. Repeat the same debate without the Default agent to see if convergence persists
  3. Apply the fine-tuning method to shift the model's bias and rerun the debate to observe changes in agent behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms through which LLM biases influence the behavior of agents in simulations, and how can these biases be systematically measured and quantified?
- Basis in paper: Inferred
- Why unresolved: The paper highlights the existence of inherent biases in LLMs and their impact on agent behavior, but it does not provide a detailed explanation of the specific mechanisms through which these biases operate or a standardized method for measuring and quantifying them
- What evidence would resolve it: A comprehensive study that identifies and describes the specific pathways through which LLM biases affect agent behavior, along with the development of a reliable and validated metric for measuring and quantifying these biases across different LLMs and simulation contexts

### Open Question 2
- Question: How do the inherent biases of different LLM models (e.g., Mistral, Solar, Instruct-GPT) compare in terms of their influence on agent behavior, and what factors contribute to the variation in bias intensity across models?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that experiments were conducted using different LLM models and that similar results were observed, but it does not provide a detailed comparison of the biases present in each model or an analysis of the factors that contribute to the variation in bias intensity
- What evidence would resolve it: A comparative study that systematically evaluates the biases present in different LLM models, using standardized metrics and benchmarks, and identifies the factors that contribute to the variation in bias intensity across models

### Open Question 3
- Question: What are the long-term effects of fine-tuning LLMs to align with specific political viewpoints on the models' performance in general tasks and their ability to generate diverse and unbiased content?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that fine-tuning LLMs can effectively alter their political biases and influence agent behavior, but it does not provide a comprehensive analysis of the long-term effects of such fine-tuning on the models' overall performance and their ability to generate diverse and unbiased content
- What evidence would resolve it: A longitudinal study that tracks the performance of fine-tuned LLMs on a variety of tasks and benchmarks over an extended period, assessing their ability to generate diverse and unbiased content, and identifying any potential negative consequences or limitations associated with fine-tuning for specific political viewpoints

## Limitations
- Experiments limited to three open-source models, which may not generalize to larger or proprietary models
- Fine-tuning effectiveness depends heavily on quality and representativeness of self-generated training data
- Artificial debate environment may not fully capture complexity of real-world political discourse

## Confidence
- Medium confidence findings regarding LLM bias alignment in debate simulations
- Key uncertainties include long-term stability of fine-tuned biases and whether effects persist across diverse debate topics

## Next Checks
1. Test bias alignment patterns across a broader range of LLM architectures (including larger proprietary models) to assess generalizability
2. Conduct ablation studies removing the Default agent to isolate whether convergence is driven by the model's inherent bias versus interaction dynamics
3. Measure fine-tuned model performance on standard benchmarks to quantify trade-offs between bias manipulation and general capability