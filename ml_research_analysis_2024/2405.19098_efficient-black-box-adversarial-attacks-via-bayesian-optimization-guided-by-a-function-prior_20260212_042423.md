---
ver: rpa2
title: Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by
  a Function Prior
arxiv_id: '2405.19098'
source_url: https://arxiv.org/abs/2405.19098
tags:
- attacks
- black-box
- prior
- function
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses black-box adversarial attacks, where the goal
  is to generate adversarial examples without access to the target model's internal
  details. The proposed method, Prior-guided Bayesian Optimization (P-BO), leverages
  a surrogate white-box model as a global function prior to improve query efficiency.
---

# Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior

## Quick Facts
- arXiv ID: 2405.19098
- Source URL: https://arxiv.org/abs/2405.19098
- Reference count: 40
- Primary result: P-BO achieves 100% attack success rates on CIFAR-10 with fewer than 20 queries on average

## Executive Summary
This paper proposes Prior-guided Bayesian Optimization (P-BO) for efficient black-box adversarial attacks. Unlike previous methods that rely solely on local gradient estimates, P-BO leverages a surrogate white-box model as a global function prior to guide the Bayesian optimization search. The approach initializes a Gaussian process with the surrogate model's loss and adaptively adjusts a coefficient to balance prior information, achieving significant improvements in query efficiency and attack success rates across multiple datasets and models.

## Method Summary
P-BO combines Bayesian optimization with a surrogate model's loss function as a global prior. The method models the attack objective using a Gaussian process whose mean function is initialized with the surrogate model's loss. A key innovation is the adaptive integration strategy that automatically adjusts a coefficient on the function prior by minimizing the regret bound. The algorithm iteratively updates the Gaussian process with observed query feedback and refines the prior estimate, enabling efficient search of adversarial perturbations with limited queries.

## Key Results
- Achieves 100% attack success rates on CIFAR-10 with fewer than 20 queries on average
- Outperforms state-of-the-art methods including NES, BanditsT, N ATTACK, SignHunter, and Square attack
- Demonstrates high query efficiency and success rates on large-scale models like ImageNet classifiers and vision-language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging a surrogate model's loss as a global function prior significantly improves the informativeness of the Bayesian optimization search compared to using only local gradient estimates.
- Mechanism: P-BO initializes the mean function of the Gaussian process with the surrogate model's loss, creating a global prior that approximates the black-box objective. This prior guides the Bayesian optimization process by providing a smoother and more informative search landscape than local gradients alone.
- Core assumption: The surrogate model's loss function is sufficiently similar to the black-box model's loss function, providing a meaningful global prior.
- Evidence anchors:
  - [abstract]: "P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss."
  - [section]: "Due to the similarity between the surrogate model and the black-box one, f′ could exhibit some similarities to f in the function space."
  - [corpus]: Weak - no direct evidence in related papers about global priors in Bayesian optimization for black-box attacks.
- Break condition: If the surrogate model is very different from the black-box model, the prior becomes uninformative or misleading, potentially degrading performance.

### Mechanism 2
- Claim: Adaptive integration of the function prior prevents performance degradation when the prior is poor.
- Mechanism: P-BO uses a coefficient λ to scale the function prior and adaptively adjusts λ by minimizing the regret bound, ensuring the algorithm performs well even with a weak prior.
- Core assumption: The regret bound analysis accurately captures the trade-off between prior strength and optimization performance.
- Evidence anchors:
  - [abstract]: "we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound."
  - [section]: "Based on our theoretical analysis in Theorem 3.1, we employ a straightforward approach by replacing f′ with λf′, where λ ∈ R can be interpreted as the weight of integrating the function prior."
  - [corpus]: Weak - no direct evidence in related papers about adaptive coefficient adjustment in Bayesian optimization for black-box attacks.
- Break condition: If the regret bound analysis is inaccurate or the optimization of λ is unstable, the adaptive integration may not prevent performance degradation.

### Mechanism 3
- Claim: The use of a zero-mean Gaussian process for the residual f - λf′ allows the algorithm to refine the prior estimate with observed data.
- Mechanism: P-BO models the residual between the black-box objective and the scaled prior using a zero-mean Gaussian process, which is then updated with observed query feedback to refine the estimate.
- Core assumption: The residual f - λf′ can be effectively modeled by a zero-mean Gaussian process.
- Evidence anchors:
  - [abstract]: "P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss."
  - [section]: "P-BO can be viewed as first modeling the residual f − f′ using its observed values with a zero-mean Gaussian process GP(0, k), and then adding f′ to form the model of f."
  - [corpus]: Weak - no direct evidence in related papers about residual modeling in Bayesian optimization for black-box attacks.
- Break condition: If the residual is too complex to be modeled by a zero-mean Gaussian process, the refinement process may be ineffective.

## Foundational Learning

- Concept: Bayesian Optimization
  - Why needed here: P-BO uses Bayesian optimization to efficiently search the high-dimensional space of adversarial perturbations with limited queries.
  - Quick check question: What are the two key components of Bayesian optimization, and how do they work together to guide the search process?

- Concept: Gaussian Processes
  - Why needed here: Gaussian processes are used to model the black-box objective function and its residual, providing a probabilistic framework for uncertainty quantification and acquisition function computation.
  - Quick check question: How does a Gaussian process model the distribution of a function given observed data points?

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: The regret bound analysis relies on the RKHS norm to measure the similarity between the black-box objective and the function prior.
  - Quick check question: What is the relationship between a Gaussian process and its corresponding RKHS?

## Architecture Onboarding

- Component map: Surrogate Model -> Gaussian Process -> Acquisition Function -> Adaptive Integration -> Query Interface

- Critical path:
  1. Initialize Gaussian process with surrogate model's loss as mean
  2. Compute posterior distribution given observed data
  3. Maximize acquisition function to select next query point
  4. Query black-box model and update observed data
  5. Adaptively adjust coefficient λ on function prior
  6. Repeat steps 2-5 until attack is successful or query budget is exhausted

- Design tradeoffs:
  - Prior strength vs. adaptivity: Stronger priors can improve initial search but may degrade performance if the prior is poor. Adaptive integration balances this tradeoff.
  - Computational cost vs. query efficiency: More complex Gaussian process models and acquisition functions can improve query efficiency but increase computational cost.

- Failure signatures:
  - High regret bound: Indicates that the function prior is not informative enough or the adaptive integration is not effective.
  - Slow convergence: Suggests that the Gaussian process model is not capturing the structure of the black-box objective well.
  - Poor attack success rate: May indicate that the acquisition function is not effectively guiding the search towards adversarial examples.

- First 3 experiments:
  1. Verify that P-BO outperforms vanilla Bayesian optimization (BO) on a simple black-box function with a known surrogate.
  2. Test the effectiveness of the adaptive integration strategy by comparing P-BO with fixed λ values on a black-box function with a known prior.
  3. Evaluate the performance of P-BO on a real-world black-box adversarial attack task, such as attacking a pre-trained image classifier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of the regret bound when the surrogate model is perfectly aligned with the target model?
- Basis in paper: [explicit] Theorem 3.1 indicates that the regret bound is proportional to the RKHS norm between the objective function and the function prior. When the surrogate model perfectly aligns with the target model, the RKHS norm approaches zero.
- Why unresolved: The paper does not provide a specific theoretical limit for the regret bound in the case of perfect alignment. It only mentions that the regret bound is proportional to the RKHS norm, but does not quantify the limit.
- What evidence would resolve it: A rigorous mathematical proof or analysis that explicitly derives the theoretical limit of the regret bound when the surrogate model is perfectly aligned with the target model.

### Open Question 2
- Question: How does the choice of kernel function affect the performance of P-BO in black-box adversarial attacks?
- Basis in paper: [inferred] The paper mentions the use of the Matern-5/2 kernel, but does not provide a comprehensive analysis of the impact of different kernel functions on the performance of P-BO.
- Why unresolved: The paper does not conduct experiments or provide theoretical insights into the effect of different kernel functions on the performance of P-BO.
- What evidence would resolve it: Experiments comparing the performance of P-BO using different kernel functions (e.g., RBF, Matern-3/2, etc.) on various datasets and attack scenarios. Additionally, a theoretical analysis of the impact of kernel choice on the regret bound and convergence properties of P-BO.

### Open Question 3
- Question: Can P-BO be extended to handle multiple surrogate models or ensembles of models as function priors?
- Basis in paper: [inferred] The paper focuses on using a single surrogate model as a function prior, but does not explore the possibility of using multiple surrogate models or ensembles.
- Why unresolved: The paper does not discuss the potential benefits or challenges of using multiple surrogate models or ensembles as function priors in P-BO.
- What evidence would resolve it: Experiments demonstrating the performance of P-BO when using multiple surrogate models or ensembles as function priors. Additionally, a theoretical analysis of the impact of using multiple priors on the regret bound and convergence properties of P-BO.

## Limitations

- The adaptive integration strategy's coefficient optimization relies on regret bound analysis, but the bound's tightness and practical applicability are not empirically validated.
- The effectiveness of the function prior is highly dependent on the similarity between surrogate and target models, which is not quantified or guaranteed in the paper.
- The paper lacks ablation studies to isolate the contributions of individual components (function prior, adaptive integration, residual modeling).

## Confidence

- High: P-BO's superior query efficiency and attack success rates compared to baselines on CIFAR-10 and ImageNet.
- Medium: The theoretical justification for the adaptive integration strategy and its practical benefits.
- Low: The generalizability of P-BO to other black-box optimization tasks beyond adversarial attacks.

## Next Checks

1. Conduct an ablation study to quantify the individual contributions of the function prior, adaptive integration, and residual modeling to P-BO's performance.
2. Test P-BO's performance on a diverse set of black-box optimization tasks (e.g., hyperparameter tuning, reinforcement learning) to assess its generalizability.
3. Investigate the impact of surrogate model quality on P-BO's performance by systematically varying the similarity between surrogate and target models.