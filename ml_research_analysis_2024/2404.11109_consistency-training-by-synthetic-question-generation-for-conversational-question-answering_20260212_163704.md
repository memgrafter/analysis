---
ver: rpa2
title: Consistency Training by Synthetic Question Generation for Conversational Question
  Answering
arxiv_id: '2404.11109'
source_url: https://arxiv.org/abs/2404.11109
tags:
- questions
- question
- history
- synthetic
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoTaH, a model-agnostic approach for improving
  conversational question answering by augmenting historical context with synthetic
  questions and using consistency training to make reasoning robust to irrelevant
  history. The method generates synthetic questions between real conversational turns,
  filters them based on similarity to maintain conversational flow, and trains a QA
  model to produce consistent answers whether using original or augmented history.
---

# Consistency Training by Synthetic Question Generation for Conversational Question Answering

## Quick Facts
- arXiv ID: 2404.11109
- Source URL: https://arxiv.org/abs/2404.11109
- Reference count: 26
- Key result: 1.8% F1 improvement over BERT baseline on QuAC dataset

## Executive Summary
This paper introduces CoTaH, a model-agnostic approach for improving conversational question answering by augmenting historical context with synthetic questions and using consistency training to make reasoning robust to irrelevant history. The method generates synthetic questions between real conversational turns, filters them based on similarity to maintain conversational flow, and trains a QA model to produce consistent answers whether using original or augmented history. Evaluated on the QuAC dataset using BERT as the base model, CoTaH achieves a 1.8% improvement in overall F1 score (60.7 vs 58.9) compared to a BERT baseline that uses history concatenation. The improvement is particularly notable for questions with substantial historical context, demonstrating that the method effectively handles irrelevant history turns.

## Method Summary
CoTaH employs a two-stage pipeline: first, a conversational question generator (CQGθ) creates synthetic questions to augment the historical context between actual conversational turns; second, a question answering module (QAθ′) is trained using both original and augmented history with consistency training. The synthetic questions are filtered based on similarity to adjacent real questions to maintain conversational coherence, and augmentation is only applied to questions beyond a threshold τ to avoid degrading performance on early dialog turns with minimal history. During training, the model minimizes a combined loss of cross-entropy for answer prediction and KL-divergence for consistency between answers from original and augmented histories.

## Key Results
- 1.8% F1 improvement over BERT baseline (60.7 vs 58.9)
- 2.7% HEQ-Q improvement, showing better performance on questions with substantial historical context
- Consistency training proves crucial - ablation without it reduces performance to 59.3% F1
- Synthetic question filtering with γ=0.8 significantly improves quality while maintaining augmentation benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic question augmentation creates a form of consistency regularization that makes the model robust to irrelevant history turns.
- Mechanism: By generating synthetic questions that maintain conversational flow and training the model to produce consistent answers across original and augmented histories, the model learns to ignore irrelevant history. The consistency loss (KL-divergence between answer distributions) explicitly enforces this robustness.
- Core assumption: Synthetic questions that are similar to the conversational flow will create perturbations that are small enough to serve as effective consistency training noise while being large enough to challenge the model to identify relevant vs. irrelevant history.
- Evidence anchors:
  - [abstract]: "we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history."
  - [section]: "Our underlying idea is to maintain the model's consistency in its reasoning, whether utilizing the original historical data or the augmented version."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.544. Limited direct evidence in corpus about synthetic question augmentation specifically, though related papers discuss history modeling and question generation.

### Mechanism 2
- Claim: Question filtering based on similarity to adjacent questions preserves conversational coherence while removing degenerate synthetic questions.
- Mechanism: Synthetic questions are scored based on their similarity to neighboring real questions in the conversation, and only those meeting similarity thresholds are retained. This ensures synthetic questions contribute meaningfully to the conversational context rather than introducing random or irrelevant content.
- Core assumption: Questions that are similar to adjacent questions in the conversation will maintain the conversational flow and provide relevant augmentation rather than introducing noise.
- Evidence anchors:
  - [section]: "We want our selected synthetic questions to be similar to the trend of the conversation... For each synthetic question qsyn which is located between history turns qi and qi+1, the score is computed as Sim(h(qi), h(qsyn)) + Sim(h(qi+1), h(qsyn))"
  - [section]: "Sometimes, we generate questions that are too similar to previous or future questions, which are invaluable. Thus, we compare the similarity of generated question qsyn with questions in {qk} S Hk and if the similarity is above γ, qsyn is discarded."
  - [corpus]: Weak evidence - corpus contains papers on conversational question answering but limited specific evidence about similarity-based filtering for synthetic questions.

### Mechanism 3
- Claim: The threshold τ for when to apply augmentation ensures that questions with minimal history are not unnecessarily perturbed, preventing degradation of performance on early dialog turns.
- Mechanism: Augmentation is only applied when the current dialog turn index k ≥ τ, recognizing that early questions have little historical context and therefore minimal irrelevant history to handle.
- Core assumption: Early dialog questions (k < τ) have insufficient historical context to benefit from augmentation and may actually be harmed by the introduction of synthetic questions.
- Evidence anchors:
  - [section]: "we introduce a threshold named τ and only augment the history of qk if k ≥ τ. According to Miyato et al. (2019), we only pass the gradients through one network."
  - [section]: "augmenting the history for all questions may not be optimal, as initial questions in a dialog, due to their little historical context, may not require augmentation for robust reasoning. In this case augmenting their history might add unnecessary noise, potentially degrading performance."
  - [corpus]: Limited direct evidence - corpus contains related work on conversational QA but no specific evidence about turn-based augmentation thresholds.

## Foundational Learning

- Concept: Consistency training and its relationship to data augmentation
  - Why needed here: The paper employs consistency training as a core mechanism, using synthetic questions as a form of data augmentation to make the model robust to irrelevant history.
  - Quick check question: How does consistency training differ from standard supervised training, and why is it particularly useful when dealing with noisy or augmented data?

- Concept: Question generation and its evaluation metrics
  - Why needed here: The paper uses a conversational question generator to create synthetic questions, requiring understanding of how to train and evaluate such generators.
  - Quick check question: What are the key challenges in generating conversational questions that maintain coherence with historical context, and how can we measure the quality of generated questions?

- Concept: Historical context modeling in conversational QA
  - Why needed here: The paper addresses the challenge of modeling historical context while filtering out irrelevant turns, which is central to conversational question answering systems.
  - Quick check question: What are the main approaches to modeling historical context in conversational QA, and what are the trade-offs between including more history versus computational efficiency?

## Architecture Onboarding

- Component map:
  - CQGθ (Conversational Question Generator) -> QS (Question Selector) -> QAθ′ (Question Answering Module)
  - Dual-path inference: QAθ′ processes both original history and augmented history
  - Loss functions: Cross-entropy for answer prediction, KL-divergence for consistency

- Critical path:
  1. Train CQGθ on question generation task
  2. Generate synthetic questions for training data
  3. Filter synthetic questions using QS with similarity scoring
  4. Train QAθ′ with dual inputs (original and augmented history) and combined loss
  5. During inference, use only the QAθ′ path with original history

- Design tradeoffs:
  - Memory vs. robustness: Using synthetic questions increases memory requirements during training but improves robustness to irrelevant history
  - Augmentation level: More synthetic questions (higher S) could improve robustness but also introduce more noise
  - Similarity threshold γ: Lower values allow more synthetic questions but risk including irrelevant ones; higher values ensure quality but may limit augmentation

- Failure signatures:
  - Performance degradation on questions with little historical context (τ set too low)
  - No improvement or degradation in overall F1 (synthetic questions too dissimilar or similarity filtering too strict)
  - Training instability or slow convergence (consistency loss weight λ not properly tuned)

- First 3 experiments:
  1. Baseline comparison: Train and evaluate standard BERT with history concatenation on dev set to establish performance baseline
  2. Ablation on synthetic question filtering: Run with and without similarity-based filtering to measure impact of question quality control
  3. Threshold sensitivity: Test different values of τ (e.g., 0, 5, 6, 7) to find optimal point where augmentation begins to help rather than harm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoTaH vary when using different question generation models (e.g., more advanced conversational question generators) instead of the simple BART-Large baseline?
- Basis in paper: Explicit - The paper acknowledges that more advanced conversational question generation models exist (e.g., Gu et al., 2021) but used a simpler BART-Large approach for simplicity, stating "we employ a straightforward generative transformer for this task."
- Why unresolved: The paper explicitly notes that the question generation quality was not optimized, stating "we didn't aim to optimize Bart-Large meticulously" and that "if the generated questions have less correlations with answers, it's tolerable as they are still relevant questions considering the overall flow of conversation." This suggests that better question generation could potentially yield improved results.
- What evidence would resolve it: A controlled experiment comparing CoTaH's performance using different question generation models (including more sophisticated conversational QG models) while keeping all other components constant would determine if advanced QG improves overall performance.

### Open Question 2
- Question: What is the optimal threshold τ for history augmentation, and how does its optimal value vary across different conversational QA datasets or domains?
- Basis in paper: Explicit - The paper conducts experiments to determine τ and finds that τ=6 yields optimal performance on the QuAC dataset, but acknowledges this is dataset-specific: "we conduct experiments on three different amount of this hyperparameter" and "No other research has been conducted to find the right amount for this threshold."
- Why unresolved: The paper only optimizes τ on a single dataset (QuAC) and doesn't explore whether the optimal threshold would generalize to other conversational QA datasets or domains, or whether there are systematic patterns in how τ should be selected.
- What evidence would resolve it: Systematic experiments applying CoTaH with different τ values across multiple conversational QA datasets (CoQA, DuConv, etc.) and domains, analyzing whether there are patterns in optimal τ selection based on dataset characteristics.

### Open Question 3
- Question: Does the uniform distribution for synthetic question selection provide optimal robustness, or would a learned distribution (e.g., based on the relevance of synthetic questions to the target question) perform better?
- Basis in paper: Explicit - The paper experiments with both uniform and linear distributions for synthetic question selection, finding that uniform performs better than linear, but doesn't explore whether a more sophisticated, learned distribution could improve performance further.
- Why unresolved: While the paper dismisses the linear distribution, it only tests two simple distributions and doesn't investigate whether a data-driven approach to learning which synthetic questions are most beneficial could yield better results. The paper states "we observe a relatively 1% drop in both F1 and HEQ-Q scores with the linear distribution" but doesn't explore alternatives beyond these two options.
- What evidence would resolve it: Experiments comparing uniform selection against learned distributions (e.g., using a small neural network to score synthetic questions based on their relevance to the target question) while keeping all other components constant would determine if learned distributions improve robustness.

## Limitations

- Limited ablation evidence: The paper reports ablation results for several components but does not provide ablation for the τ threshold parameter, which is a key design choice.
- Generalization beyond QuAC: All experiments are conducted on a single dataset (QuAC), limiting understanding of performance on other conversational QA datasets.
- Implementation complexity: The method requires training two separate models and managing complex interactions between them during training, with exact implementation details not fully specified.

## Confidence

- **High confidence**: The core mechanism of using synthetic questions for consistency training is well-supported by the 1.8% F1 improvement and consistent trends across different evaluation metrics.
- **Medium confidence**: The specific design choices (similarity thresholds, τ parameter, number of synthetic questions) are reasonable but not extensively validated through ablation studies.
- **Low confidence**: Claims about generalization to other datasets and the necessity of specific design choices (like τ=6) are not empirically supported.

## Next Checks

1. **τ threshold ablation study**: Systematically vary τ from 0 to 10 and measure performance impact on both early dialog turns (k < τ) and overall F1. This would validate whether the choice of τ=6 is optimal or if the model can benefit from earlier augmentation.

2. **Cross-dataset evaluation**: Apply CoTaH to another conversational QA dataset (e.g., CoQA or DoQA) to assess generalization beyond QuAC. Measure performance relative to a BERT baseline on questions with varying amounts of historical context.

3. **Synthetic question quality analysis**: Generate synthetic questions using the trained CQGθ and evaluate them using standard question generation metrics (BLEU, ROUGE, BERTScore) against gold questions. Additionally, conduct a human evaluation of a sample of synthetic questions to assess their conversational coherence and relevance.