---
ver: rpa2
title: 'DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization'
arxiv_id: '2408.04400'
source_url: https://arxiv.org/abs/2408.04400
tags:
- graph
- subgraph
- learning
- diversity
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles out-of-distribution (OOD) generalization in\
  \ graph machine learning, where models struggle when training and test data come\
  \ from different distributions. The key issue is simplicity bias\u2014neural networks\
  \ trained with SGD tend to focus on simpler, spurious patterns rather than more\
  \ complex but causally relevant features, leading to poor OOD performance."
---

# DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization

## Quick Facts
- arXiv ID: 2408.04400
- Source URL: https://arxiv.org/abs/2408.04400
- Authors: Xin Sun; Liang Wang; Qiang Liu; Shu Wu; Zilei Wang; Liang Wang
- Reference count: 40
- Primary result: DIVE outperforms state-of-the-art methods on OOD graph generalization tasks, achieving up to 51.31% improvement in regression

## Executive Summary
This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, where models struggle when training and test data come from different distributions. The core problem is simplicity bias—neural networks trained with SGD tend to focus on simpler, spurious patterns rather than more complex but causally relevant features. The authors propose DIVE (Diverse Invariant VErsatile), which trains an ensemble of models to focus on diverse label-predictive subgraphs using subgraph disagreement regularization. This approach reduces reliance on spurious correlations and improves OOD performance across multiple datasets from the GOOD benchmark and DrugOOD.

## Method Summary
DIVE trains an ensemble of models where each model learns to extract and focus on different label-predictive subgraphs from graph data. The method uses a predictive subgraph extractor with Gumbel-Sigmoid sampling for differentiable subgraph selection, followed by a subgraph encoder and classifier for the main task. A key innovation is the diversity regularization that applies Jaccard loss to penalize overlapping subgraph masks across models, encouraging each model to discover distinct structural patterns. Model selection for inference is performed based on validation accuracy, choosing the ensemble member that performs best on OOD validation data.

## Key Results
- DIVE achieves up to 51.31% improvement in regression tasks compared to state-of-the-art methods
- Consistent performance gains across classification tasks on GOOD benchmark datasets
- Significant improvement in subgraph extraction F1 scores (from ~0.5 to >0.8) with diversity regularization
- Effective identification of invariant subgraphs while reducing focus on spurious correlations

## Why This Works (Mechanism)

### Mechanism 1
Training multiple models to focus on different label-predictive subgraphs reduces reliance on spurious correlations. By encouraging each model in the ensemble to attend to distinct subgraphs via diversity regularization (Jaccard loss), the method ensures not all models focus on the same spurious patterns, thus mitigating simplicity bias. Core assumption: Training set contains both spurious and invariant subgraphs equally predictive of labels, and different models can effectively learn distinct subgraphs. Evidence: Abstract states diversity regularization "reduces the impact of simplicity bias" and section 3.5 describes Jaccard loss application.

### Mechanism 2
The diversity regularization enables more accurate extraction of invariant subgraphs compared to existing methods. By penalizing overlap in extracted subgraphs, the diversity regularization encourages each model to explore different structural patterns, increasing the likelihood of identifying invariant subgraphs missed by single-subgraph methods. Core assumption: Invariant subgraphs are sufficiently distinct from spurious subgraphs in structural patterns, and diversity regularization can effectively differentiate them. Evidence: Abstract notes "significant improvement over existing methods" and section 4.2.2 shows F1 improvement from 0.5 to >0.8 with diversity regularization.

### Mechanism 3
Model selection based on validation accuracy identifies the model with best OOD performance. Since each model focuses on different subgraphs, the model attending to most invariant subgraphs will likely perform better on validation set, which can be used to select best model for inference. Core assumption: Validation set is representative of OOD scenarios, and model attending to invariant subgraphs will have higher validation accuracy. Evidence: Abstract states "Model selection for robust OOD performance is achieved through validation accuracy" and section 4.2.3 describes OOD validation set usage.

## Foundational Learning

- Concept: Simplicity bias in neural networks
  - Why needed here: Understanding simplicity bias is crucial to grasp why proposed method addresses core issue of poor OOD generalization
  - Quick check question: Why do neural networks trained with SGD tend to focus on simpler features, and how does this affect OOD performance?

- Concept: Subgraph extraction and its challenges
  - Why needed here: Method relies on extracting label-predictive subgraphs, and understanding challenges is essential to appreciate proposed solution
  - Quick check question: What are main challenges in extracting invariant subgraphs from graph data, and how do existing methods address these challenges?

- Concept: Diversity regularization and its impact
  - Why needed here: Proposed method uses diversity regularization to encourage different models to focus on distinct subgraphs, understanding its impact is key to method's effectiveness
  - Quick check question: How does diversity regularization work, and what are its potential benefits and drawbacks in context of graph OOD generalization?

## Architecture Onboarding

- Component map: Predictive subgraph extractor -> Subgraph encoder and classifier -> Diversity regularizer -> Model selection
- Critical path: 1) Train each model to extract label-predictive subgraphs and perform main task 2) Apply diversity regularization to encourage different models to focus on distinct subgraphs 3) Select best model based on validation accuracy for inference on test set
- Design tradeoffs: Increasing number of models in ensemble may improve chances of identifying invariant subgraphs but also increases computational cost; strength of diversity regularization (lambda) needs tuning to balance between encouraging diversity and maintaining model performance
- Failure signatures: If all models in ensemble focus on same spurious subgraphs, diversity regularization is not effective; if validation set is not representative of OOD scenarios, model selection based on validation accuracy may not identify best model
- First 3 experiments: 1) Train single model with and without diversity regularization to observe impact on subgraph extraction and OOD performance 2) Vary number of models in ensemble to find optimal balance between diversity and computational cost 3) Experiment with different strengths of diversity regularization to find optimal lambda value

## Open Questions the Paper Calls Out

### Open Question 1
Does DIVE's approach of fostering subgraph disagreement through diversity regularization generalize beyond graph data to other domains where simplicity bias is a concern? Basis: Authors propose DIVE specifically for graph OOD generalization but mention diversity-based ensemble methods have shown success in visual tasks. Unresolved because paper focuses exclusively on graph data without exploring application to other data types. Evidence: Empirical testing on non-graph datasets (images, text) where simplicity bias affects OOD performance would resolve this.

### Open Question 2
How does DIVE perform when number of models in ensemble is significantly increased beyond tested range? Basis: Authors test collections of 2-4 models and observe diminishing returns or performance degradation with 4 models on some datasets, but do not explore larger ensemble sizes. Unresolved because paper only evaluates limited range of ensemble sizes. Evidence: Extensive experimentation with ensemble sizes ranging from 5-20 models to determine optimal collection size and assess scalability.

### Open Question 3
Can DIVE's diversity regularization be effectively combined with other OOD generalization techniques, such as environment inference or domain adaptation methods? Basis: Authors compare DIVE against various OOD baselines but do not explore potential synergies or combinations with other techniques. Unresolved because paper presents DIVE as standalone method without investigating whether diversity regularization could complement or enhance other OOD approaches. Evidence: Empirical studies combining DIVE's diversity regularization with other OOD techniques and comparing results against individual methods.

## Limitations

- Assumes training data contains both spurious and invariant subgraphs with equal predictive power, which may not hold in real-world datasets
- Does not thoroughly explore how performance degrades when simplicity bias assumption is violated
- Validation accuracy selection mechanism assumes validation set represents OOD scenarios, which may not hold for all distribution shifts

## Confidence

**High Confidence**: Technical implementation details (Gumbel-Sigmoid sampling, Jaccard diversity regularization, model selection via validation accuracy) are well-specified and reproducible; experimental setup using GOOD benchmark and DrugOOD datasets is clearly described.

**Medium Confidence**: Claim of "up to 51.31% improvement" in regression tasks is supported by experimental results, though comparison methods and dataset specifics for this result are not detailed in abstract; general trend of improvement over baselines appears consistent across multiple tasks.

**Low Confidence**: Paper's assertion that simplicity bias is primary driver of poor OOD performance on graphs, and that DIVE definitively addresses this through subgraph disagreement, lacks comprehensive ablation studies isolating simplicity bias from other factors like domain shift magnitude or dataset size effects.

## Next Checks

1. **Ablation on Diversity Regularization Strength**: Systematically vary λ from 0 to 1 in increments of 0.1 and measure both subgraph extraction F1 scores and OOD performance to identify optimal trade-off point and determine sensitivity to this hyperparameter.

2. **Single Domain vs Multi-Domain Validation**: Compare model selection performance when using validation sets from same domain as training versus truly OOD validation sets to quantify how representative validation set needs to be for effective model selection.

3. **Spurious vs Invariant Subgraph Recovery**: Using synthetic data where ground truth spurious and invariant subgraphs are known, measure precision and recall of DIVE in recovering each type, and compare against baselines to directly validate mechanism's effectiveness in distinguishing between these subgraph types.