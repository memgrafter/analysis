---
ver: rpa2
title: Learning Diffusion Priors from Observations by Expectation Maximization
arxiv_id: '2405.13712'
source_url: https://arxiv.org/abs/2405.13712
tags:
- diffusion
- posterior
- sampling
- which
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiEM, an Expectation-Maximization (EM) algorithm
  for training diffusion models from incomplete, noisy observations without access
  to clean data. The core idea is to iteratively generate samples from the current
  posterior distribution and use them to update the diffusion prior via denoising
  score matching.
---

# Learning Diffusion Priors from Observations by Expectation Maximization

## Quick Facts
- arXiv ID: 2405.13712
- Source URL: https://arxiv.org/abs/2405.13712
- Authors: François Rozet; Gérôme Andry; François Lanusse; Gilles Louppe
- Reference count: 40
- Primary result: Introduces DiEM algorithm for training diffusion models from incomplete, noisy observations without clean data

## Executive Summary
This paper presents DiEM (Diffusion Expectation Maximization), a novel algorithm for training diffusion models when only corrupted observations are available, without access to clean data. The key insight is to use an Expectation-Maximization framework that iteratively generates samples from the current posterior distribution and updates the diffusion prior via denoising score matching. A significant contribution is the Moment Matching Posterior Sampling (MMPS) scheme, which leverages Tweedie's covariance formula and conjugate gradient methods to improve posterior sampling accuracy compared to heuristic approaches.

The authors demonstrate that DiEM can successfully learn proper diffusion priors from corrupted CIFAR-10 and MRI data, achieving better sample quality than previous methods. MMPS consistently outperforms alternative posterior sampling techniques across multiple linear inverse problems on FFHQ, generating higher quality samples with fewer sampling steps. This work addresses a fundamental challenge in unsupervised learning of generative models from incomplete or corrupted data.

## Method Summary
DiEM operates through an iterative Expectation-Maximization framework where the E-step generates samples from the current posterior distribution, and the M-step updates the diffusion prior using denoising score matching on these samples. The core innovation is MMPS (Moment Matching Posterior Sampling), which improves posterior sampling accuracy by computing the covariance matrix of the posterior using Tweedie's formula and solving the resulting system with conjugate gradient methods. This approach provides a principled alternative to heuristic posterior sampling methods and enables more accurate updates to the diffusion prior.

## Key Results
- DiEM successfully learns diffusion priors from corrupted CIFAR-10 and MRI data without clean examples
- MMPS outperforms heuristic posterior sampling methods in generating higher quality samples with fewer steps
- DiEM achieves better sample quality than previous methods for training diffusion models from corrupted data
- The approach works across multiple linear inverse problems on FFHQ dataset

## Why This Works (Mechanism)
DiEM works by framing the problem of learning from corrupted data as an EM optimization where the E-step samples from the posterior distribution of clean data given observations, and the M-step updates the diffusion model parameters. MMPS improves this process by accurately capturing the posterior covariance structure through Tweedie's formula, enabling more precise sampling that better represents the true posterior. This iterative refinement allows the diffusion prior to gradually align with the underlying data distribution despite only having access to corrupted observations.

## Foundational Learning
**Diffusion Models**: Generative models that learn to reverse a noising process - needed to understand the prior being learned; quick check: can you explain how score matching trains these models?
**Expectation-Maximization**: Iterative algorithm for maximum likelihood estimation with latent variables - needed to understand DiEM's optimization framework; quick check: can you describe the E and M steps in standard EM?
**Tweedie's Formula**: Relationship between posterior mean and observed data under exponential family distributions - needed for MMPS covariance computation; quick check: can you state the formula for posterior mean in terms of sufficient statistics?
**Conjugate Gradient Methods**: Iterative algorithm for solving linear systems - needed for efficient MMPS implementation; quick check: can you explain when CG is preferred over direct matrix inversion?
**Linear Inverse Problems**: Problems where observations are linear transformations of clean data plus noise - needed as the corruption model; quick check: can you write the standard form Ax + noise?
**Denoising Score Matching**: Method for learning score functions without knowing the normalization constant - needed for the M-step updates; quick check: can you differentiate between score matching and likelihood-based training?

## Architecture Onboarding

**Component Map**: DiEM (EM framework) -> E-step (posterior sampling) -> MMPS (covariance computation via Tweedie) -> M-step (score matching update) -> Diffusion prior

**Critical Path**: The most critical path is the E-step posterior sampling, as errors here propagate through the entire learning process