---
ver: rpa2
title: Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
arxiv_id: '2403.03967'
source_url: https://arxiv.org/abs/2403.03967
tags:
- dimension
- attack
- adversarial
- lemma
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of neural networks to
  adversarial attacks, distinguishing between natural (on-manifold) and unnatural
  (off-manifold) attacks. The authors argue that the existence of unnatural attacks
  stems from the dimension gap between the intrinsic and ambient dimensions of data.
---

# Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability

## Quick Facts
- arXiv ID: 2403.03967
- Source URL: https://arxiv.org/abs/2403.03967
- Authors: Rajdeep Haldar; Yue Xing; Qifan Song
- Reference count: 40
- Primary result: Theoretical proof that dimension gap increases off-manifold adversarial vulnerability in 2-layer ReLU networks

## Executive Summary
This paper establishes a theoretical link between the ambient-intrinsic dimension gap of data and vulnerability to adversarial attacks. The authors distinguish between natural (on-manifold) and unnatural (off-manifold) attacks, arguing that the existence of unnatural attacks stems from the dimension gap between the intrinsic and ambient dimensions of data. They prove that for 2-layer ReLU networks, while this dimension gap doesn't affect generalization performance, it increases vulnerability to off-manifold adversarial perturbations. The main result provides an explicit relationship between attack strength and dimension gap, showing that as the gap increases, the required attack strength decreases.

## Method Summary
The paper uses theoretical analysis to examine how the ambient-intrinsic dimension gap affects adversarial vulnerability in 2-layer ReLU networks. The authors prove that while this dimension gap doesn't affect generalization performance, it increases vulnerability to off-manifold adversarial perturbations. They establish an explicit relationship between attack strength (ℓ2, ℓ∞) and the dimension gap, demonstrating that larger gaps require weaker attacks to succeed. The analysis focuses on distinguishing between natural (on-manifold) and unnatural (off-manifold) attacks and their respective success conditions.

## Key Results
- Proves that dimension gap increases vulnerability to off-manifold adversarial perturbations in 2-layer ReLU networks
- Establishes explicit relationship between attack strength (ℓ2, ℓ∞) and dimension gap
- Shows that ℓ∞ attack strength for off-manifold attacks asymptotically approaches zero with respect to the dimension gap
- Demonstrates that dimension gap doesn't affect generalization performance but impacts adversarial vulnerability

## Why This Works (Mechanism)
The mechanism works because the dimension gap creates a larger space of possible perturbations that can be made while remaining off the data manifold. When the ambient dimension is much larger than the intrinsic dimension, there are more directions in which adversarial perturbations can be applied without intersecting the data manifold. This increased freedom makes it easier to find successful adversarial examples with smaller perturbation magnitudes. The ReLU activation in 2-layer networks creates piecewise linear decision boundaries that are particularly susceptible to these off-manifold perturbations when the dimension gap is large.

## Foundational Learning

**Manifold Learning**
- Why needed: Understanding the intrinsic structure of data distributions
- Quick check: Can you explain the difference between ambient and intrinsic dimensions?

**Adversarial Attacks**
- Why needed: Framework for understanding how perturbations can fool neural networks
- Quick check: What distinguishes on-manifold from off-manifold attacks?

**ReLU Network Analysis**
- Why needed: The paper's theoretical results are specific to 2-layer ReLU networks
- Quick check: How do piecewise linear decision boundaries affect adversarial vulnerability?

## Architecture Onboarding

**Component Map**
Data manifold -> Ambient space -> Neural network decision boundary -> Classification output

**Critical Path**
1. Data lies on low-dimensional manifold within high-dimensional ambient space
2. Neural network learns decision boundaries in ambient space
3. Adversarial perturbations exploit dimension gap to move off-manifold
4. Network misclassifies off-manifold points more easily

**Design Tradeoffs**
- The ReLU activation provides computational efficiency but creates piecewise linear boundaries
- The 2-layer architecture simplifies analysis but may not capture deeper network behaviors
- Theoretical bounds provide guarantees but may be loose in practice

**Failure Signatures**
- Increased vulnerability when ambient dimension >> intrinsic dimension
- ℓ∞ attacks require smaller perturbations as dimension gap increases
- Generalization performance remains stable despite increased adversarial vulnerability

**3 First Experiments**
1. Verify that dimension gap affects adversarial vulnerability but not generalization on synthetic data
2. Test ℓ∞ attack strength scaling with dimension gap on 2-layer ReLU networks
3. Compare vulnerability to on-manifold vs off-manifold attacks across different dimension gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Results specifically apply to 2-layer ReLU networks; unclear how they extend to deeper architectures
- Dimension gap assumption may not hold uniformly across all real-world datasets
- Proofs focus on specific ℓ2 and ℓ∞ norms, potentially missing other relevant perturbation metrics
- Theoretical bounds may not directly translate to practical attack success rates

## Confidence

| Claim | Confidence |
|-------|------------|
| Mathematical proofs within 2-layer ReLU network framework | High |
| Generalization to other architectures | Medium |
| Direct applicability to real-world adversarial defense strategies | Low |

## Next Checks

1. Empirical verification across multiple network architectures (CNNs, transformers) to test if the dimension gap-vulnerability relationship holds beyond 2-layer ReLU networks

2. Investigation of how data preprocessing techniques that affect manifold structure (like dimensionality reduction or data augmentation) impact the theoretical bounds

3. Analysis of whether the ℓ∞ attack strength truly approaches zero in practical scenarios or if there's a fundamental lower bound due to numerical precision and optimization constraints