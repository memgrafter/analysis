---
ver: rpa2
title: 'Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on Arithmetic
  Relations in Abstract Reasoning'
arxiv_id: '2412.05586'
source_url: https://arxiv.org/abs/2412.05586
tags:
- reasoning
- attribute
- answer
- rule
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares large language models (LLMs) and neuro-symbolic
  approaches in solving Raven's progressive matrices (RPM), a visual abstract reasoning
  test that involves understanding and executing mathematical rules such as progression
  or arithmetic addition. By providing visual attributes directly as textual prompts,
  the models' abstract reasoning capabilities are measured in isolation, assuming
  an oracle visual perception module.
---

# Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on Arithmetic Relations in Abstract Reasoning

## Quick Facts
- arXiv ID: 2412.05586
- Source URL: https://arxiv.org/abs/2412.05586
- Reference count: 40
- Large language models (GPT-4, Llama-3 70B) fail on arithmetic rules in Raven's Progressive Matrices despite advanced prompting, while neuro-symbolic ARLC achieves near-perfect accuracy using vector-symbolic architectures.

## Executive Summary
This work compares large language models (LLMs) and neuro-symbolic approaches in solving Raven's progressive matrices (RPM), a visual abstract reasoning test involving mathematical rules like progression and arithmetic addition. By providing visual attributes directly as textual prompts, the study isolates reasoning capabilities from perception, assuming an oracle visual perception module. Despite using compositionally structured representations and advanced prompting techniques, both GPT-4 and Llama-3 70B cannot achieve perfect accuracy on the center constellation of the I-RAVEN dataset, with the root cause being their weakness in understanding and executing arithmetic rules. The neuro-symbolic Abductive Rule Learner with Context-awareness (ARLC), which uses vector-symbolic architectures (VSAs), achieves almost perfect accuracy on the center constellation of I-RAVEN, demonstrating high fidelity in arithmetic rules.

## Method Summary
The study compares LLMs (GPT-4, Llama-3 70B) with ARLC on Raven's Progressive Matrices tasks using the I-RAVEN dataset. LLMs are tested with disentangled and entangled prompting, predictive and discriminative classification, and self-consistency techniques. ARLC uses vector-symbolic architectures with distributed representations and binding/unbinding operations to perform symbolic computations. The analysis isolates reasoning capability from perception by providing oracle ground-truth attributes as prompts. Performance is evaluated on center constellation of I-RAVEN and extended to larger matrices (3x10) and dynamic ranges (up to 1000) in I-RAVEN-X.

## Key Results
- LLMs achieve significantly lower accuracy on arithmetic rules compared to constant and progression rules, with accuracy dropping below 10% on extended 3x10 matrices with dynamic range up to 1000
- ARLC achieves almost perfect accuracy on center constellation of I-RAVEN and maintains high accuracy across extended task complexities
- LLMs' arithmetic reasoning failures persist despite compositionally structured representations and advanced prompting techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail on arithmetic rules due to their inherent weakness in understanding and executing arithmetic operations, even when given structured, disentangled prompts.
- Mechanism: LLMs process RPM tasks through pattern matching and relational reasoning rather than true symbolic computation. When presented with arithmetic progression (e.g., row-wise addition/subtraction), the models tend to infer simpler rules like constant or progression from partial context instead of performing the required arithmetic computation.
- Core assumption: The arithmetic rule requires explicit computation across all three rows, which LLMs cannot perform reliably even with oracle visual perception and advanced prompting techniques.
- Evidence anchors:
  - [abstract] "our analysis reveals that the root cause lies in the LLM's weakness in understanding and executing arithmetic rules"
  - [section V-C] "Both models perform well on constant, progression, and distribute three rules, whereas the accuracy notably drops for the arithmetic rule"
  - [section V-C] "We find that the majority of wrong predictions are related to the arithmetic rule"

### Mechanism 2
- Claim: ARLC achieves high accuracy on RPM tasks by using distributed vector-symbolic architectures (VSAs) that preserve semantic similarity and perform addition/subtraction through binding and unbinding operations.
- Mechanism: ARLC maps attribute values to high-dimensional distributed representations where dot products define similarity kernels. Simple element-wise operations on these vectors perform addition/subtraction on the encoded values, allowing the model to solve arithmetic rules symbolically rather than through pattern matching.
- Core assumption: The distributed representations in VSA space can preserve semantic relationships between attribute values while enabling symbolic operations through algebraic manipulations.
- Evidence anchors:
  - [abstract] "ARLC achieves almost perfect accuracy on the center constellation of I-RAVEN, demonstrating high fidelity in arithmetic rules"
  - [section IV-A] "simple element-wise operations on the vectors perform addition/subtraction on the encoded values"
  - [section IV-A] "The resulting VSA vectors preserve the semantic similarity between attribute values: the dot products between corresponding VSA encoded vectors define a similarity kernel"

### Mechanism 3
- Claim: ARLC maintains high accuracy on larger grid sizes (3×10) and expanded dynamic ranges (up to 1000) due to its ability to adjust structured FPE representations without requiring retraining.
- Mechanism: Unlike LLMs whose accuracy degrades with increased complexity, ARLC's VSA-based approach can handle larger matrices and wider value ranges by simply reconfiguring the dictionary and binding/unbinding terms while maintaining the same underlying learning framework.
- Core assumption: The VSA framework's distributed representations can scale to handle increased complexity without fundamental changes to the learning algorithm.
- Evidence anchors:
  - [abstract] "ARLC can maintain a high accuracy due to emulating symbolic computations on top of properly distributed representations"
  - [section V-D] "we could demonstrate that ARLC trained on a dynamic range of m = 45 can favorably generalize to a dynamic range of m = 1000"
  - [section IV-E] "only varying the dynamic range at constant grid size does not require retraining: we can simply replace the dictionary in order to support OOD generalization"

## Foundational Learning

- Concept: Vector-Symbolic Architectures (VSAs)
  - Why needed here: VSAs provide the mathematical foundation for ARLC's ability to represent concepts with distributed vectors and perform symbolic operations (addition/subtraction) through algebraic manipulations of these vectors.
  - Quick check question: How do binding and unbinding operations in VSAs correspond to arithmetic operations on the encoded values?

- Concept: Distributed Representations and Similarity Kernels
  - Why needed here: Understanding how distributed representations preserve semantic similarity through dot products is crucial for grasping how ARLC can perform arithmetic reasoning in high-dimensional space.
  - Quick check question: What property of distributed representations allows ARLC to determine if two encoded values are arithmetically related?

- Concept: Fractional Power Encoding (FPE)
  - Why needed here: FPE is the specific encoding technique used by ARLC to initialize the VSA space such that binding operations correspond to addition in the original value space.
  - Quick check question: How does the initialization of vectors using fractional powers enable the binding operation to perform addition?

## Architecture Onboarding

- Component map: Visual attribute extraction module -> LLM prompting system (entangled/disentangled, predictive/discriminative) or ARLC neuro-symbolic reasoning engine (VSA representations, rule learning framework) -> Answer selection

- Critical path: For LLMs: extract attributes → generate prompts → query model → parse predictions → select best answer. For ARLC: extract attributes → map to VSA space → execute learned rules → select answer based on confidence scores.

- Design tradeoffs: LLMs offer flexibility and zero-shot capabilities but struggle with arithmetic reasoning and scaling. ARLC provides superior arithmetic performance and scalability but requires training and has fewer parameters (480 vs billions for LLMs).

- Failure signatures: LLM failures manifest as consistent errors on arithmetic rules regardless of prompting technique, with accuracy dropping below 10% on extended tasks. ARLC failures would likely appear as reduced accuracy when the VSA space cannot adequately represent certain arithmetic relationships.

- First 3 experiments:
  1. Test LLM arithmetic accuracy on a simplified 3×3 RPM task with known oracle attributes to isolate reasoning capability from perception
  2. Implement ARLC with varying dictionary sizes to find the optimal balance between representational capacity and computational efficiency
  3. Compare LLM and ARLC performance on I-RAVEN-X with progressively larger dynamic ranges to quantify scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could enable LLMs to improve their performance on arithmetic reasoning tasks in abstract reasoning tests?
- Basis in paper: [explicit] The paper identifies that both GPT-4 and Llama-3 70B struggle with arithmetic rules (progression, arithmetic addition/subtraction) despite providing compositionally structured representations and advanced prompting techniques.
- Why unresolved: The paper only identifies the weakness but does not propose specific architectural changes to address this limitation. It merely suggests integrating symbolic solvers like ARLC as a potential remedy.
- What evidence would resolve it: Experiments comparing different architectural modifications (e.g., incorporating symbolic computation modules, attention mechanisms focused on numerical relationships, or specialized arithmetic reasoning layers) against baseline LLMs on extended RPM tests with larger matrices and dynamic ranges.

### Open Question 2
- Question: How do the performance differences between LLMs and neuro-symbolic approaches like ARLC scale when the complexity of abstract reasoning tasks increases beyond the current benchmarks?
- Basis in paper: [inferred] The paper shows that ARLC maintains high accuracy across larger grid sizes (3x10 vs 3x3) and expanded dynamic ranges (up to 1000 vs 10), while LLMs' accuracy drops significantly, especially for arithmetic rules.
- Why unresolved: The study only tests up to 3x10 matrices and dynamic ranges of 1000. It remains unclear how these approaches would perform on even more complex reasoning tasks that might require multi-step reasoning, hierarchical rule application, or integration of multiple abstract concepts.
- What evidence would resolve it: Comparative experiments on progressively more complex abstract reasoning benchmarks that combine multiple rule types, require longer reasoning chains, or involve higher-dimensional attribute spaces.

### Open Question 3
- Question: What are the fundamental limitations of vector-symbolic architectures that prevent them from achieving perfect accuracy on all rule types in abstract reasoning tasks?
- Basis in paper: [explicit] While ARLC achieves almost perfect accuracy on center constellation of I-RAVEN, it still shows some errors (e.g., 61.6% accuracy on arithmetic rules at dynamic range 1000), suggesting limitations in handling certain types of reasoning.
- Why unresolved: The paper demonstrates ARLC's strengths but doesn't thoroughly analyze why it fails in specific cases. The nature of these errors and whether they stem from the VSA representation itself, the learning algorithm, or the rule formulation remains unexplored.
- What evidence would resolve it: Detailed error analysis of ARLC's failures, investigation into the representational capacity of different VSA instantiations (GSBC vs other variants), and exploration of hybrid approaches that combine VSAs with other reasoning mechanisms.

## Limitations

- The analysis relies on oracle visual perception, which assumes perfect attribute extraction that may not hold in real-world scenarios with noisy or ambiguous visual inputs.
- The study focuses primarily on the I-RAVEN dataset's center constellation, limiting generalizability to other RPM variations or real-world abstract reasoning tasks.
- The paper doesn't systematically test whether LLM failures stem from computational inability versus inability to recognize when computation is required.

## Confidence

- LLM Arithmetic Reasoning Weakness: High confidence - Multiple experiments consistently show LLMs struggling with arithmetic rules while performing better on simpler patterns like constant and progression.
- ARLC VSA Effectiveness: High confidence - The neuro-symbolic approach achieves near-perfect accuracy on arithmetic rules and maintains performance across extended task complexities.
- Scaling Behavior Differences: Medium confidence - While the paper demonstrates ARLC's ability to handle larger matrices and dynamic ranges, the comparison is limited to specific parameter settings.

## Next Checks

- Implement a systematic ablation study where LLMs are provided with explicit arithmetic computation modules alongside their reasoning capabilities to determine whether observed failures stem from computational inability or architectural constraints.
- Test ARLC's performance on a broader range of RPM variants beyond I-RAVEN, including those with overlapping attributes, missing information, or non-linear transformations.
- Conduct a computational efficiency analysis comparing the training/inference costs of ARLC versus the zero-shot capabilities of LLMs across different task complexities.