---
ver: rpa2
title: Exploring Hybrid Question Answering via Program-based Prompting
arxiv_id: '2402.10812'
source_url: https://arxiv.org/abs/2402.10812
tags:
- code
- question
- data
- function
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HPROPRO, a novel program-based prompting framework
  for hybrid question answering (HQA) over heterogeneous data. HPROPRO addresses the
  challenges of large-scale information and organic coupling of heterogeneous data
  sources by generating and executing code with integrated external functions for
  information-seeking across diverse modalities.
---

# Exploring Hybrid Question Answering via Program-based Prompting

## Quick Facts
- arXiv ID: 2402.10812
- Source URL: https://arxiv.org/abs/2402.10812
- Reference count: 11
- Performance: Achieves 54.6% F1 on HybridQA and 66.7% F1 on MultiModalQA in few-shot settings

## Executive Summary
HPROPRO introduces a novel program-based prompting framework for hybrid question answering over heterogeneous data sources including tables, text, and images. Unlike previous approaches that rely on specialized retrievers or modal transformations, HPROPRO leverages large language models to generate and execute code with integrated external functions for information extraction. The framework achieves state-of-the-art performance on HybridQA and MultiModalQA benchmarks, particularly in few-shot settings, by separating reasoning logic from execution logic through function declaration and implementation components.

## Method Summary
HPROPRO employs a code generation and execution paradigm where LLMs generate Python code that can be executed to answer questions. The framework uses query simplification to reduce cognitive load by pre-resolving table-cell-to-question mappings, then generates code through function declaration prompts. During execution, declared functions are implemented with specific logic for handling different data modalities. Code refinement through iterative feedback addresses execution errors by feeding error information back to the LLM for improved code generation.

## Key Results
- Achieves 54.6% F1 score on HybridQA benchmark, surpassing previous methods
- Achieves 66.7% F1 score on MultiModalQA benchmark, outperforming all baseline systems
- Demonstrates state-of-the-art performance in few-shot settings (4-shot)

## Why This Works (Mechanism)

### Mechanism 1
Function declaration and implementation separate reasoning logic from execution logic, allowing LLMs to generate code without handling heterogeneous data formats directly. During code generation, the model declares functions as black boxes focusing on high-level reasoning flow, while execution layer handles modality-specific extraction.

### Mechanism 2
Query simplification reduces cognitive load on code generation by pre-resolving table-cell-to-question mappings. A retriever identifies relevant passages/images, then an LLM simplifies questions by replacing spans with table cell contents, enabling the code generator to work with simpler question-table pairs.

### Mechanism 3
Code refinement through iterative feedback improves code quality by addressing execution errors. When generated code fails, error information and original prompts are fed back to the LLM to generate refined code that addresses specific issues.

## Foundational Learning

- Concept: Code generation and execution paradigm
  - Why needed here: Framework relies on generating executable Python code for answering questions
  - Quick check question: Can you explain how the separation between code generation and execution benefits handling heterogeneous data?

- Concept: Function abstraction and interfaces
  - Why needed here: Framework uses declared functions as interfaces between reasoning logic and data access logic
  - Quick check question: How do the declared functions in HPROPRO enable the model to generate code without knowing data access details?

- Concept: Query understanding and simplification
  - Why needed here: Framework requires simplifying complex questions involving linked passages/images before code generation
  - Quick check question: What is the purpose of query simplification in HPROPRO and how does it work?

## Architecture Onboarding

- Component map: Input layer → Query simplification → Code generation → Function implementation → Code execution → Answer (with code refinement as fallback)

- Critical path: Table → Query simplification → Code generation → Code execution → Answer

- Design tradeoffs:
  - Using LLMs for both code generation and function implementation provides flexibility but introduces LLM quality dependency
  - Separating function declaration from implementation enables clean separation of concerns but requires careful interface design
  - Query simplification reduces input complexity but adds processing step that could introduce errors

- Failure signatures:
  - Code execution errors indicate issues with code generation or function implementation
  - Empty results from extract_info suggest query simplification or function implementation problems
  - Incorrect answers despite successful execution indicate reasoning errors in code generation

- First 3 experiments:
  1. Run full pipeline on simple HybridQA example with single table and passage
  2. Test query simplification module independently with various question types
  3. Test code refinement module with intentionally broken code

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but identifies several areas for further investigation through its discussion of limitations and future work.

## Limitations
- Effectiveness depends on LLM quality for both code generation and function implementation
- Query simplification assumes accurate retriever performance and reliable LLM mapping ability
- May struggle with highly unstructured or ambiguous data that doesn't fit standard patterns
- Performance may degrade on multi-hop questions requiring multiple information-seeking steps

## Confidence
- High confidence: Basic program-based prompting approach and function separation principle
- Medium confidence: Query simplification mechanism reliability
- Medium confidence: Code refinement effectiveness through iterative feedback

## Next Checks
1. Test query simplification independently with diverse question types to quantify accuracy and identify failure patterns
2. Measure code refinement success rate by introducing controlled errors and evaluating correction capability
3. Benchmark performance on increasingly complex multi-hop questions to identify breaking point of information-seeking capability