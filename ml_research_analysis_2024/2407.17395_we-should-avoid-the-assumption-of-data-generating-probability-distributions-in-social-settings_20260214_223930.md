---
ver: rpa2
title: We should avoid the assumption of data-generating probability distributions
  in social settings
arxiv_id: '2407.17395'
source_url: https://arxiv.org/abs/2407.17395
tags:
- 'true'
- distribution
- data
- such
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that the common assumption of a true data-generating
  probability distribution in machine learning is problematic, especially in social
  settings, because such distributions do not exist and the framework can obscure
  important choices and goals. The authors propose an alternative framework based
  on finite populations rather than abstract distributions.
---

# We should avoid the assumption of data-generating probability distributions in social settings

## Quick Facts
- arXiv ID: 2407.17395
- Source URL: https://arxiv.org/abs/2407.17395
- Reference count: 13
- Argues that assuming data-generating distributions is problematic in social settings and proposes finite population framework instead

## Executive Summary
This paper challenges the conventional assumption in machine learning that data is generated from an underlying probability distribution, arguing that this framework is particularly problematic in social settings where such distributions don't exist. The authors propose an alternative framework based on finite populations rather than abstract distributions, demonstrating that classical learning theory can be adapted to work without significant changes. They show that the finite population framework provides a better model of sampling, clarifies that machine learning's goal is to minimize loss rather than approximate distributions, and highlights that the input space is a choice rather than given. The paper concludes that machine learning models are not objective and their choices need explicit discussion and documentation.

## Method Summary
The authors develop a theoretical framework replacing the conventional data-generating distribution assumption with a finite population model. They work with a set of l + k datapoints in X × {0, 1} and use hypothesis classes with finite VC dimension. The key innovation is adapting classical learning theory proofs (like symmetrization lemmas) to work with finite populations without replacement sampling. The framework calculates error rates on the whole set, training set, and remaining points to bound generalization error probability. The authors demonstrate that this approach yields similar theoretical guarantees while providing better modeling of real-world sampling scenarios and making explicit the choices inherent in machine learning systems.

## Key Results
- Classical learning theory results can be reproduced in the finite population framework with minimal modifications
- The finite population framework provides a more accurate model of sampling than the conventional generative framework
- The input space representation is a choice that significantly impacts model predictions and fairness
- Machine learning models are not objective; their choices need to be explicitly discussed and recorded

## Why This Works (Mechanism)
The paper works by replacing the abstract assumption of a data-generating distribution with concrete finite populations, making the implicit assumptions of machine learning explicit. By working directly with actual data points rather than abstract distributions, the framework reveals that many choices in machine learning (like input space representation) are not inherent to the problem but are human decisions that affect outcomes.

## Foundational Learning
- Vapnik-Chervonenkis (VC) dimension: A measure of the capacity of a hypothesis class that determines generalization bounds; needed to quantify the complexity of the hypothesis space
- Symmetrization lemmas: Mathematical techniques for bounding generalization error by comparing training and test performance; quick check: verify these can be adapted to finite populations
- Empirical risk minimization: The standard learning framework that minimizes loss on training data; why needed: to show the finite population framework preserves core learning principles
- Generalization bounds: Mathematical guarantees on how well a model trained on limited data will perform on new data; quick check: confirm bounds hold in the finite population setting

## Architecture Onboarding
Component map: Finite population of data points → Hypothesis class with VC dimension → Training set selection → Error rate calculations → Generalization bounds
Critical path: Datapoints → Hypothesis selection → Training/testing split → Error computation → Bound derivation
Design tradeoffs: Finite populations vs. infinite distributions - more realistic modeling vs. mathematical elegance
Failure signatures: If generalization bounds don't hold, check VC dimension calculation or error rate implementation
First experiments: 1) Verify error rate calculations u(α), vtr(α), u'(α) for simple hypothesis classes; 2) Test symmetrization lemmas adaptation with small finite populations; 3) Compare finite population bounds with classical bounds as k → ∞

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific theoretical bounds can be derived in the finite population framework for generalization error that differ from classical bounds?
- Basis in paper: [explicit] The paper introduces Theorem 1 which provides bounds for generalization error in finite populations, but notes that further theoretical development is needed.
- Why unresolved: The theorem is a starting point, but the paper explicitly calls for further research to fully develop the theory within the finite population framework.
- What evidence would resolve it: New theorems and bounds derived specifically for finite population learning that demonstrate improved or novel insights compared to classical results.

### Open Question 2
- Question: How does the choice of input space representation impact the fairness and performance of machine learning models in practice?
- Basis in paper: [explicit] Section 6 argues that the input space is a choice that significantly impacts predictions, and calls for more research on variable construction and representation choice.
- Why unresolved: While the paper highlights the importance of representation choice, it doesn't provide concrete guidelines or empirical studies on how different choices affect model outcomes.
- What evidence would resolve it: Empirical studies comparing model performance and fairness across different input space representations for the same underlying data and prediction task.

### Open Question 3
- Question: In which specific settings is the assumption of a true data-generating distribution benign versus problematic?
- Basis in paper: [explicit] Section 8 mentions that the Gen-D assumption is less problematic in games of chance compared to social settings, but calls for research to determine when it should be avoided.
- Why unresolved: The paper provides some examples but doesn't offer a comprehensive framework for determining when the Gen-D assumption is appropriate.
- What evidence would resolve it: A systematic analysis of various domains (e.g., physical sciences, social sciences, games of chance) to determine the conditions under which the Gen-D assumption holds or fails.

## Limitations
- The paper is entirely theoretical with no empirical validation or practical examples
- The hypothesis class and VC dimension are treated abstractly rather than concretely
- Social implications are asserted but not empirically demonstrated
- The transition from mathematical theory to social critique is not fully justified

## Confidence
- Theoretical claims about learning bounds: Medium
- Practical implications in real-world social settings: Low

## Next Checks
1. Construct specific examples with concrete hypothesis classes and datapoints to verify the error rate calculations and generalization bounds hold numerically
2. Compare the finite population framework predictions with real-world social data sampling scenarios to test practical applicability
3. Develop a small-scale empirical study showing how different sampling schemes in social data affect model outcomes, directly testing the paper's claims about choice and objectivity