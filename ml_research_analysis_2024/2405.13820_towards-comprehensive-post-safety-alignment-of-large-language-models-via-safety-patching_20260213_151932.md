---
ver: rpa2
title: Towards Comprehensive Post Safety Alignment of Large Language Models via Safety
  Patching
arxiv_id: '2405.13820'
source_url: https://arxiv.org/abs/2405.13820
tags:
- safety
- over-safety
- safe
- patching
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SafePatching, a novel post-safety alignment
  (PSA) method for large language models (LLMs) that addresses three core safety challenges:
  enhancing safety, mitigating over-safety, and preserving utility. SafePatching derives
  two distinct safety patches using gradient ascent and descent on harmful data, then
  integrates them into the target LLM using a controllable patching strategy that
  minimizes conflicts.'
---

# Towards Comprehensive Post Safety Alignment of Large Language Models via Safety Patching

## Quick Facts
- arXiv ID: 2405.13820
- Source URL: https://arxiv.org/abs/2405.13820
- Reference count: 40
- SafePatching achieves better safety (ASR reduced from 24.00% to 7.25%), over-safety mitigation (refusal rate reduced from 8.00% to 3.33%), and utility preservation (MT-bench score increased from 6.01 to 6.14) compared to baseline methods

## Executive Summary
SafePatching introduces a novel post-safety alignment method that addresses three core challenges in LLM safety: enhancing safety against harmful queries, mitigating over-safety where models refuse benign requests, and preserving utility during safety improvements. The method derives two distinct safety patches using gradient ascent and descent on harmful data, then integrates them into the target LLM using a controllable patching strategy that minimizes conflicts. Experiments on LLaMA-2/3, Gemma, and Mistral models demonstrate significant improvements across all three dimensions compared to baseline approaches, with the 7B models showing ASR reduction from 24.00% to 7.25% while maintaining or improving utility scores.

## Method Summary
SafePatching operates through a dual-patch generation mechanism where harmful data is used to optimize two separate patches: one that enhances safety through gradient ascent to maximize harmful response detection, and another that mitigates over-safety through gradient descent to reduce false refusals on benign queries. These patches are then integrated into the target LLM using a controllable patching strategy that balances the contributions of each patch based on the safety-utility tradeoff requirements. The method employs conflict minimization techniques during integration to prevent the patches from working at cross-purposes, ensuring that safety enhancements don't come at the cost of increased over-safety or utility degradation.

## Key Results
- ASR (Attack Success Rate) reduced from 24.00% to 7.25% on LLaMA-2-7B-Chat
- Over-safety refusal rate reduced from 8.00% to 3.33%
- MT-bench utility score increased from 6.01 to 6.14

## Why This Works (Mechanism)
The dual-patch approach works because it explicitly addresses the tension between safety and utility that exists in single-patch methods. By generating complementary patches that target opposite aspects of the safety spectrum - one pushing toward stronger safety responses and another pulling back from over-refusal - the method can fine-tune the safety-utility balance more precisely. The controllable integration strategy allows for dynamic adjustment of this balance based on deployment requirements, while the conflict minimization ensures that the patches work synergistically rather than antagonistically. This architectural separation of concerns enables more granular control over safety alignment than monolithic approaches.

## Foundational Learning

**Gradient-based optimization for safety**: Why needed - To efficiently generate safety-relevant patches without requiring extensive manual annotation; Quick check - Verify that gradient updates on harmful data produce meaningful safety improvements.

**Dual-patch architecture**: Why needed - To simultaneously address safety enhancement and over-safety mitigation as complementary rather than competing objectives; Quick check - Confirm that patches derived from opposite gradient directions target distinct safety aspects.

**Conflict minimization in model integration**: Why needed - To prevent safety patches from working against each other and degrading overall performance; Quick check - Test that integrated model performs better than either patch applied individually.

## Architecture Onboarding

**Component map**: Harmful data → Gradient ascent/descent optimization → Dual patch generation → Controllable integration → Conflict minimization → Patched LLM

**Critical path**: The most critical sequence is data preparation → patch generation → integration, as errors in any of these stages propagate through the entire pipeline and cannot be recovered downstream.

**Design tradeoffs**: The method trades computational overhead during patch generation (requiring multiple optimization passes) for better final safety-utility balance, versus simpler single-pass approaches that may achieve only partial alignment.

**Failure signatures**: Poor gradient signal in harmful data leads to ineffective patches; excessive conflict between patches causes performance degradation; improper integration weights result in either insufficient safety or excessive over-safety.

**First experiments**: 1) Test gradient ascent/descent patch generation independently on a small dataset to verify they produce opposite effects; 2) Evaluate integration weights on a validation set to find optimal safety-utility balance; 3) Assess conflict levels between patches using ablation studies.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework lacks assessment of safety in benign real-world usage scenarios, focusing only on adversarial attack resilience
- Methodology tested only on small models (7B-8B parameters), raising questions about effectiveness on larger frontier models
- Continual PSA experiments have narrow temporal scope without demonstrating long-term stability across multiple update cycles

## Confidence
**High confidence**: Core technical contribution of dual-patch generation and conflict-minimization integration strategy; experimental results on base metrics are well-documented and reproducible.

**Medium confidence**: Practical utility in real deployment scenarios given limited scope of safety evaluation and lack of user-centric assessments; over-safety mitigation claims lack qualitative analysis of appropriateness.

**Low confidence**: Scalability to larger models and long-term effectiveness in continual PSA settings; method's robustness against adaptive adversaries.

## Next Checks
1. Conduct user studies with domain experts to evaluate whether SafePatching appropriately balances safety and utility in real-world scenarios, particularly focusing on whether over-safety mitigations lead to inappropriate compliance with harmful requests.

2. Test SafePatching on larger models (70B+ parameters) and diverse architectures to assess scalability, including computational overhead analysis and effectiveness degradation patterns.

3. Implement a longitudinal study evaluating the stability and drift of safety patches over multiple fine-tuning epochs and across different types of harmful data distributions to assess true continual PSA capabilities.