---
ver: rpa2
title: 'Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class
  Incremental Learning'
arxiv_id: '2409.18265'
source_url: https://arxiv.org/abs/2409.18265
tags:
- learning
- covariance
- feature
- training
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of exemplar-free class incremental
  learning (EFCIL), where a model must learn new classes without access to past data.
  The authors identify two critical issues with existing state-of-the-art methods:
  1) They do not adapt covariance matrices of past classes after each task, and 2)
  They suffer from task-recency bias due to dimensionality collapse during training.'
---

# Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning

## Quick Facts
- arXiv ID: 2409.18265
- Source URL: https://arxiv.org/abs/2409.18265
- Reference count: 40
- Primary result: AdaGauss improves average accuracy by 3.7% and 6.8% points on ImageNetSubset compared to second-best method when training from scratch with 10 and 20 tasks respectively

## Executive Summary
This paper addresses exemplar-free class incremental learning (EFCIL), where models must learn new classes without access to past data. The authors identify two critical issues with existing methods: failure to adapt covariance matrices after each task and task-recency bias caused by dimensionality collapse. They propose AdaGauss, which adapts both means and covariances of memorized class distributions using an auxiliary adapter network, employs feature distillation through a learnable projector, and uses an anti-collapse loss to prevent rank differences in covariance matrices. The method achieves state-of-the-art results on multiple benchmarks.

## Method Summary
AdaGauss is an exemplar-free class incremental learning method that maintains class distributions as multivariate Gaussians with both mean and covariance parameters. After each incremental task, an adapter network maps old class features from the previous feature space to the new one, updating both means and covariances. The method uses feature distillation through a learnable projector to improve representation quality and employs an anti-collapse loss to encourage positive-definite covariance matrices with higher rank. This prevents dimensionality collapse that causes task-recency bias in existing methods.

## Key Results
- Achieves 3.7% and 6.8% average accuracy improvements on ImageNetSubset (10 and 20 tasks) compared to second-best method
- Outperforms existing state-of-the-art methods on CIFAR-100, TinyImageNet, ImageNetSubset, CUB200, and FGVCAircraft
- Consistently improves both average accuracy (Alast) and average incremental accuracy (Ainc) across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Covariance matrices of class distributions change during incremental training and must be adapted to maintain valid decision boundaries.
- Mechanism: Feature extractor weight changes after each task cause feature distributions for previously learned classes to drift. AdaGauss uses an adapter network to map old class features to the new feature space, updating both mean and covariance.
- Core assumption: The adapter network trained on new task data can accurately map old class distributions.
- Evidence anchors: [abstract] "they do not consider that classes' covariance matrices change and must be adapted"; [section] "training the feature extractor on incremental tasks makes memorized distribution not match the ground truth ones."

### Mechanism 2
- Claim: Task-recency bias is caused by dimensionality collapse leading to rank differences in covariance matrices.
- Mechanism: During incremental training, feature representations suffer from dimensionality collapse, causing early task covariance matrices to have lower rank than later ones. This creates numerical instabilities when inverting matrices, increasing Mahalanobis distances for early tasks and skewing classification toward recent tasks.
- Core assumption: Encouraging positive-definiteness of minibatch covariance matrices prevents overall feature space collapse.
- Evidence anchors: [section] "methods suffer from dimensionality collapse... old classes' covariances to be of lower rank than those from recent tasks"; [section] "To overcome the collapse, we encourage the feature extractor to produce features whose dimensions are linearly independent."

### Mechanism 3
- Claim: Feature distillation through learnable projector improves representation strength and reduces task-recency bias.
- Mechanism: Instead of standard feature or logit distillation, AdaGauss distills through a learnable MLP projector that maps features to a space where knowledge distillation is more effective, improving eigenvalue distribution and reducing rank differences.
- Core assumption: The learnable projector can find a better feature space for knowledge distillation.
- Evidence anchors: [section] "representational strength of the feature extractor increases with each task... making memorized covariance matrices of late tasks have a higher rank"; [section] "feature distillation through a learnable projector... provides representations with a better eigenvalues distribution."

## Foundational Learning

- Concept: Multivariate Gaussian distributions and their parameters (mean and covariance matrix).
  - Why needed here: Classes are represented as multivariate Gaussian distributions in feature space using mean and covariance for classification.
  - Quick check question: Given a set of feature vectors for a class, how do you compute the sample mean and covariance matrix?

- Concept: Mahalanobis distance and its relationship to Gaussian classification.
  - Why needed here: The Bayes classifier computes Mahalanobis distances between test samples and class distributions for predictions.
  - Quick check question: Write the formula for the squared Mahalanobis distance between a feature vector x and a Gaussian distribution with mean μ and covariance Σ.

- Concept: Dimensionality collapse in neural networks and its implications for covariance matrix inversion.
  - Why needed here: Understanding why low-rank covariance matrices cause numerical instabilities when inverted is crucial for grasping task-recency bias.
  - Quick check question: What happens when you try to invert a singular (non-invertible) matrix, and how does this relate to low-rank covariance matrices?

## Architecture Onboarding

- Component map: Image -> Feature extractor (ResNet18) -> (optional Projector MLP) -> Feature vector -> Bayes classifier with Mahalanobis distance

- Critical path: Input image flows through feature extractor, optionally through projector for distillation, producing feature vector that's classified using stored class distributions

- Design tradeoffs:
  - Projector vs. no projector: Improves representation quality but adds computation and parameters during training
  - Full covariance vs. diagonal covariance: More expressive but requires more parameters and computation
  - Adapter training vs. no adaptation: Better accuracy but increases training time and complexity

- Failure signatures:
  - Task-recency bias: Classifier consistently favors recent classes
  - Numerical instability: Training crashes due to singular covariance matrices
  - Overfitting to current task: Catastrophic forgetting of previous tasks

- First 3 experiments:
  1. Implement basic EFCIL with frozen feature extractor and nearest mean classifier as baseline
  2. Add adapter network to adapt means only (like EFC method) and measure improvement
  3. Add full covariance adaptation and anti-collapse loss, compare with and without projector distillation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaGauss perform with different feature extractor architectures beyond ResNet18, such as ConvNext or ViT?
- Basis in paper: [explicit] The paper mentions that AdaGauss is tested with ViT small and ConvNext, showing improved results with more modern architectures.
- Why unresolved: The paper only briefly mentions these tests without detailed analysis or comparison.
- What evidence would resolve it: A comprehensive evaluation of AdaGauss across various architectures with detailed performance metrics.

### Open Question 2
- Question: Can AdaGauss's performance be further improved by incorporating a contrastive loss to prevent forgetting of old classes?
- Basis in paper: [inferred] The paper suggests that using a contrastive loss could potentially alleviate the problem of forgetting old classes, which is currently an open question in EFCIL.
- Why unresolved: The paper does not implement or test the use of a contrastive loss, leaving its impact unexplored.
- What evidence would resolve it: Experimental results comparing AdaGauss with and without a contrastive loss, showing the impact on performance and forgetting.

### Open Question 3
- Question: What is the impact of the anti-collapse loss (LAC) on the optimization process when using different values of the β parameter?
- Basis in paper: [explicit] The paper discusses the impact of the β parameter on the anti-collapse loss and its effect on training stability and performance.
- Why unresolved: The paper provides limited exploration of different β values, focusing primarily on β = 1.
- What evidence would resolve it: A detailed analysis of AdaGauss performance across a range of β values, highlighting the optimal settings and their effects on training dynamics.

### Open Question 4
- Question: How does AdaGauss handle scenarios with very little data per class, where high-dimensional covariance matrices become difficult to calculate?
- Basis in paper: [explicit] The paper mentions the introduction of a bottleneck layer to address the issue of calculating high-dimensional covariance matrices with limited data.
- Why unresolved: The paper does not explore the effectiveness of this approach or alternative solutions for such scenarios.
- What evidence would resolve it: Experimental results demonstrating AdaGauss's performance with varying amounts of data per class, and comparisons with other methods that address this challenge.

## Limitations
- The paper lacks detailed implementation details for adapter network architecture and specific hyperparameters for anti-collapse loss
- Limited ablation studies quantifying the individual contributions of each proposed component
- Missing comprehensive analysis of AdaGauss performance across different feature extractor architectures

## Confidence
- **High Confidence**: The core mechanism of adapting both means and covariances using an adapter network is well-supported by the paper's methodology and results.
- **Medium Confidence**: The explanation of task-recency bias caused by dimensionality collapse and rank differences in covariance matrices is plausible but requires deeper mathematical verification.
- **Medium Confidence**: The effectiveness of feature distillation through a learnable projector versus standard distillation methods is supported by reported results but lacks detailed ablation studies.

## Next Checks
1. **Mathematical verification**: Derive the exact relationship between feature space drift, covariance matrix rank, and Mahalanobis distance computation to confirm the claimed mechanism of task-recency bias.

2. **Implementation reproduction**: Implement a minimal working version of AdaGauss on a small dataset (e.g., CIFAR-10 split into 2-3 tasks) to verify the claimed improvements over baseline methods.

3. **Ablation study**: Conduct controlled experiments isolating each component (covariance adaptation, anti-collapse loss, projector distillation) to quantify their individual contributions to the overall performance gain.