---
ver: rpa2
title: Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?
arxiv_id: '2407.01992'
source_url: https://arxiv.org/abs/2407.01992
tags:
- contrast
- choices
- mcqa
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether large language models (LLMs) achieve
  high rankings on multiple-choice question answering (MCQA) leaderboards primarily
  due to their ability to exploit patterns in answer choices without considering the
  question context. To investigate this, the authors develop an automatic graph mining
  method to construct contrast sets from existing MCQA datasets, avoiding manual annotation
  and potential model-generated biases.
---

# Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?
## Quick Facts
- arXiv ID: 2407.01992
- Source URL: https://arxiv.org/abs/2407.01992
- Reference count: 23
- Key outcome: LLMs maintain consistent rankings on MCQA tasks whether tested on original or contrast sets, suggesting they don't primarily exploit choice-only shortcuts

## Executive Summary
This paper investigates whether large language models achieve high multiple-choice question answering (MCQA) leaderboard rankings by exploiting patterns in answer choices without considering question context. The authors develop an automatic graph mining method to construct contrast sets from existing MCQA datasets, avoiding manual annotation and model-generated biases. After validating the quality of their 820-question contrast set through annotator assessments, they evaluate 12 LLMs on both the original UnifiedQA evaluation set and the contrast set. The results show that while LLMs can achieve high accuracy using only choices in the original set, their rankings remain highly consistent (Kendall's τ near 0.9) when comparing performance on the original set versus the contrast set with full prompts, indicating that MCQA remains a reliable benchmark for evaluating LLM knowledge and reasoning capabilities.

## Method Summary
The authors employ a graph mining approach to automatically construct contrast sets from the UnifiedQA evaluation set. They represent MCQA entries as vertices in an undirected graph, connecting entries whose answers are semantically equivalent. Using a maximum matching algorithm, they find the largest set of entry pairs with identical choices but different questions. The semantic similarity between answers is computed using NLI-based embeddings with a threshold of 0.85. The resulting contrast set (820 questions) is validated through annotator assessments for quality. Twelve LLMs (Llama-2, Falcon, Mistral, Mixtral, Gemma, Yi families) are then evaluated using few-shot prompts (3-shot, 5-shot, and 10-shot versions) in both full prompt and choices-only settings to compare performance and rankings between the original and contrast sets.

## Key Results
- LLMs maintain high MCQA rankings when tested on contrast sets with identical choices but different questions, with Kendall's τ near 0.9
- No LLM rank drops markedly between original and contrast sets, indicating models are not ranking highly solely due to choice-only shortcuts
- High choices-only accuracy does not necessarily indicate that models ignore questions when both are provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs maintain high MCQA rankings because their ability to answer with choices-only does not dominate performance when questions are present.
- Mechanism: The contrast set design forces models to rely on question content rather than patterns in choices alone. By pairing entries with identical choices but different questions, models cannot succeed by exploiting choice-only shortcuts.
- Core assumption: Models that rely heavily on choice-only shortcuts will see a significant drop in accuracy when tested on the contrast set compared to the original dataset.
- Evidence anchors:
  - [abstract] "LLM accuracy rankings between the initial evaluation set and contrast set are highly consistent, with Kendall's τ near 0.9."
  - [section 4.2] "Since no LLM rank drops markedly, we claim that our tested LLMs are not ranking highly on MCQA leaderboards solely due to their ability to exploit choices-only shortcuts."
  - [corpus] Weak evidence; corpus papers discuss choice-only performance but do not directly support the specific mechanism of contrast set construction preserving rankings.
- Break condition: If models can answer questions without using the question content (e.g., through robust pattern matching), the contrast set would not effectively distinguish knowledgeable models from cheaters.

### Mechanism 2
- Claim: The graph mining approach effectively creates high-quality contrast sets without model-generated biases.
- Mechanism: By representing MCQA entries as vertices and connecting those with semantically equivalent answers, the maximum matching algorithm produces pairs of entries with identical choices but different questions, ensuring plausible distractors.
- Core assumption: Semantic equivalence between answers in different entries indicates that their choices can form a plausible set of distractors.
- Evidence anchors:
  - [section 2.1] "We draw an edge between entries di and dj if the gold answer ai is semantically equivalent to a distractor c ∈ Cj \ {aj and vice versa."
  - [section 4.1] "We ask three annotators to assess our contrast set, finding that it has questions with plausible distractors."
  - [corpus] No direct corpus evidence supporting the effectiveness of graph mining for contrast set creation; this is an original contribution.
- Break condition: If the semantic similarity threshold is too low or too high, the resulting contrast set may contain implausible distractors or miss valid pairs.

### Mechanism 3
- Claim: High choices-only accuracy does not necessarily indicate that models ignore questions when both are provided.
- Mechanism: Models can leverage both question context and choice patterns, so high choices-only performance reflects one capability without undermining the other when questions are available.
- Core assumption: The presence of a question does not automatically override or invalidate the model's ability to use choice patterns effectively.
- Evidence anchors:
  - [abstract] "when prompted with both the question and choices, LLM accuracy rankings between the initial evaluation set and contrast set are highly consistent."
  - [section 4.2] "Thus, to better quantify if LLMs are obtaining high ranks on UnifiedQA due to their ability to exploit choices-only shortcuts, we compare model ranks on the original UnifiedQA evaluation set to its contrast set."
  - [corpus] Weak evidence; corpus papers discuss choice-only performance but do not directly address the relationship between choices-only and full-prompt performance.
- Break condition: If models fundamentally cannot integrate question context with choice patterns, their full-prompt performance would be indistinguishable from random guessing despite high choices-only accuracy.

## Foundational Learning

- Concept: Semantic similarity and equivalence in NLP
  - Why needed here: To determine if two answers can form plausible distractors by assessing their semantic similarity.
  - Quick check question: What threshold of cosine similarity was used to determine semantic equivalence between answers?
- Concept: Graph theory and maximum matching algorithms
  - Why needed here: To construct the contrast set by finding the largest set of entry pairs with identical choices but different questions.
  - Quick check question: What graph algorithm was used to find the largest set of entry pairs without duplicate questions?
- Concept: Multiple-choice question answering (MCQA) evaluation
  - Why needed here: To understand how MCQA tasks are structured and how models are evaluated on them.
  - Quick check question: What is the primary concern raised by prior work regarding LLM performance on MCQA tasks?

## Architecture Onboarding

- Component map:
  - Graph construction module: Builds the undirected graph from the MCQA dataset.
  - Semantic similarity calculator: Computes cosine similarity between answers to determine edges.
  - Maximum matching algorithm: Finds the largest set of non-adjacent edges to form entry pairs.
  - Contrast set validation module: Assesses the quality of the generated contrast set through annotator evaluations.
  - Evaluation pipeline: Tests multiple LLMs on both the original and contrast sets using few-shot prompts.
- Critical path: Graph construction → Semantic similarity calculation → Maximum matching → Contrast set validation → LLM evaluation
- Design tradeoffs:
  - Strict semantic similarity threshold (0.85) ensures high-quality distractors but may reduce contrast set size.
  - Using few-shot prompts balances model performance with prompt engineering complexity.
  - Manual annotator validation provides quality assurance but is resource-intensive.
- Failure signatures:
  - Low contrast set size indicates overly strict semantic similarity thresholds or sparse graph connections.
  - Inconsistent LLM rankings between original and contrast sets suggest models rely heavily on choice-only shortcuts.
  - Poor annotator agreement on distractor plausibility indicates issues with graph construction or semantic similarity calculation.
- First 3 experiments:
  1. Vary the semantic similarity threshold (e.g., 0.75, 0.85, 0.95) and measure contrast set size and annotator agreement.
  2. Test different graph matching algorithms (e.g., greedy matching vs. maximum matching) and compare contrast set quality.
  3. Evaluate LLM performance on contrast sets with different numbers of choices (e.g., 2, 4, 8) to assess difficulty impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do choice-only shortcuts manifest across different types of MCQA tasks beyond commonsense reasoning?
- Basis in paper: [explicit] The authors note their method was applied to UnifiedQA, a commonsense reasoning dataset collection, and suggest future research apply it to other datasets including non-MCQ datasets.
- Why unresolved: The study only evaluated six commonsense datasets. The authors acknowledge that while their results might generalize to datasets like MMLU with lower choice-only accuracy, they don't have empirical evidence for this claim.
- What evidence would resolve it: Systematic application of the graph mining algorithm to diverse MCQA datasets (MMLU, SQuAD, etc.) and comparison of choice-only accuracy patterns across domains.

### Open Question 2
- Question: What specific strategies or patterns do LLMs use to achieve high accuracy in choice-only settings?
- Basis in paper: [inferred] The authors repeatedly emphasize that despite high choice-only accuracy, MCQA remains a reliable benchmark, and call for more work explaining how high choice-only accuracy occurs. They note it's critical to understand what strategies LLMs employ.
- Why unresolved: The paper only demonstrates that models don't rely solely on choice-only shortcuts when given questions, but doesn't investigate the actual mechanisms behind choice-only success.
- What evidence would resolve it: Detailed analysis of LLM attention patterns, intermediate representations, or systematic probing of what linguistic or semantic features models extract from choices alone.

### Open Question 3
- Question: How would zero-shot prompting affect choice-only accuracy and the relationship between choice-only and full-prompt performance?
- Basis in paper: [explicit] The authors explicitly state they didn't test zero-shot prompting because they were working with base (unaligned, non-instruction-tuned) LLMs, but suggest this could be an interesting future direction.
- Why unresolved: The study only examines few-shot settings, leaving the zero-shot scenario unexplored despite being a common practical use case.
- What evidence would resolve it: Empirical evaluation of zero-shot performance across the same 12 LLMs on both original and contrast sets, measuring whether the high correlation between full-prompt rankings persists.

## Limitations
- The study focuses exclusively on commonsense reasoning tasks, which may not generalize to other MCQA domains like medical or legal reasoning.
- The graph mining approach relies on semantic similarity thresholds that may not capture all forms of plausible distractor relationships.
- The study uses a specific set of 12 LLMs from particular model families, which may not represent the full diversity of available language models.

## Confidence
**High confidence** in the core finding that MCQA rankings remain stable between original and contrast sets (Kendall's τ ≈ 0.9), as this is supported by systematic evaluation across multiple models and prompt configurations. **Medium confidence** in the claim that models do not primarily rely on choice-only shortcuts, as this inference depends on the contrast set successfully isolating question-dependent performance. **Medium confidence** in the graph mining methodology's ability to create high-quality contrast sets without model-generated biases, as annotator validation supports quality but the approach hasn't been tested across diverse MCQA domains.

## Next Checks
1. Apply the graph mining algorithm to non-commonsense MCQA datasets (e.g., medical, legal, or scientific domains) to test generalizability of the findings across different knowledge domains.

2. Conduct ablation studies varying the semantic similarity threshold (0.75, 0.85, 0.95) to quantify its impact on contrast set quality, size, and the resulting LLM performance patterns.

3. Test additional model architectures beyond the 12 evaluated (including older and newer models) to determine if the findings hold across the broader LLM landscape and identify potential edge cases where choice-only shortcuts might dominate.