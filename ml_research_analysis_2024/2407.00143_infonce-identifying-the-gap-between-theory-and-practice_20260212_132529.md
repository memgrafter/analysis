---
ver: rpa2
title: 'InfoNCE: Identifying the Gap Between Theory and Practice'
arxiv_id: '2407.00143'
source_url: https://arxiv.org/abs/2407.00143
tags:
- latent
- aninfonce
- learning
- page
- identifiability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between theory and practice in contrastive
  learning (CL), specifically focusing on InfoNCE. The authors argue that existing
  theories assume equal variance across all latent factors or that certain latents
  are kept invariant, which doesn't reflect how augmentations are used in practice.
---

# InfoNCE: Identifying the Gap Between Theory and Practice

## Quick Facts
- **arXiv ID**: 2407.00143
- **Source URL**: https://arxiv.org/abs/2407.00143
- **Reference count**: 40
- **Primary result**: Introduces AnInfoNCE to recover latents with anisotropic variances, proving identifiability under more realistic assumptions

## Executive Summary
This paper addresses the gap between theoretical assumptions and practical augmentations in contrastive learning. The authors show that standard InfoNCE collapses certain latent factors when augmentations affect different latents to varying degrees. They introduce AnInfoNCE, a generalization that uses a trainable diagonal scaling matrix to recover all latent factors by weighting dimensions differently in the similarity function. The method is theoretically justified and empirically shown to increase latent recovery at the cost of downstream accuracy, highlighting a fundamental trade-off in contrastive learning.

## Method Summary
The paper introduces AnInfoNCE, a generalization of InfoNCE that uses a trainable diagonal scaling matrix (ˆΛ) in the similarity function to recover latent factors with anisotropic variances. The method models the positive conditional distribution with unknown diagonal concentration parameters, which are learned during training. The authors prove identifiability under more realistic assumptions about augmentations and show that AnInfoNCE can recover latents that standard InfoNCE collapses. The approach is validated on synthetic data, MNIST, CIFAR10, and ImageNet, demonstrating increased latent recovery but a trade-off with downstream accuracy.

## Key Results
- AnInfoNCE provably uncovers latent factors in anisotropic settings by introducing a trainable diagonal scaling matrix
- The learned scaling matrix values closely match ground-truth concentration parameters in synthetic experiments
- AnInfoNCE increases recovery of previously collapsed information in CIFAR10 and ImageNet, though at the cost of downstream accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anisotropic variances across latent factors cause InfoNCE to collapse certain latents while preserving others
- **Mechanism:** Standard InfoNCE assumes isotropic changes across all latents in positive pairs. When augmentations affect different latents to different degrees (e.g., cropping affects spatial latents more than color latents), the loss cannot properly balance the similarity between positive pairs, causing collapse of highly-variable latents
- **Core assumption:** Augmentations induce anisotropic changes to different latent factors, modeled by non-uniform variance across dimensions
- **Evidence anchors:**
  - [abstract] "Specifically, they either assume equal variance across all latents or that certain latents are kept invariant. However, in practice, positive pairs are often generated using augmentations such as strong cropping to just a few pixels"
  - [section 2] "We posit that augmentations imply anisotropic changes to different latent factors, a setting previously not covered theoretically"
  - [corpus] Weak - corpus neighbors focus on different aspects of representation learning
- **Break condition:** If augmentations actually affect all latents uniformly, or if the positive conditional distribution is not Gaussian/von Mises-Fisher

### Mechanism 2
- **Claim:** AnInfoNCE recovers latents by learning a diagonal scaling matrix that weights dimensions differently in the similarity function
- **Mechanism:** The trainable diagonal matrix ˆΛ in AnInfoNCE allows the model to assign higher weights to dimensions that vary less across positive pairs (content) and lower weights to dimensions that vary more (style), enabling recovery of all latent factors
- **Core assumption:** The ground-truth positive conditional distribution has diagonal concentration parameter Λ that can be learned
- **Evidence anchors:**
  - [abstract] "We introduce AnInfoNCE, a generalization of InfoNCE that can provably uncover the latent factors in this anisotropic setting... introducing a trainable diagonal scaling matrix (ˆΛ) in the similarity function"
  - [section 3.2] "We model this in the positive conditional by scaling the latent factors with unknown positive concentration parameters, collected in the diagonal matrix Λ"
  - [section 4.2] "When learning Λ, we observe that the learned ˆΛ values almost match the ground-truth values"
- **Break condition:** If the ground-truth conditional distribution is not diagonal or if the batch size is insufficient to capture the distribution

### Mechanism 3
- **Claim:** The trade-off between augmentation readout accuracy and downstream classification accuracy occurs because augmentations violate theoretical assumptions
- **Mechanism:** Augmentations create positive pair distributions that deviate from the assumed Gaussian/von Mises-Fisher form (e.g., can be bimodal) and create domain shift between training and test distributions, causing the learned representations to optimize for augmentation recovery rather than semantic classification
- **Core assumption:** The positive conditional distribution implied by augmentations matches the assumed distribution form
- **Evidence anchors:**
  - [section 5] "Another consequence is that we cannot directly measure how well models recover the ground-truth information. Thus, we use proxy evaluations and calculate the linear readout accuracy on the augmentations used during training"
  - [section 5] "While higher accuracy in augmentation readout indicates a better capture of style latents... it does not coincide with higher classification accuracy"
  - [section 5] "The conditional distribution implied by augmentations may not correspond to the one assumed by the loss"
- **Break condition:** If augmentations can be designed to follow the assumed distribution, or if the domain shift can be mitigated

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE**
  - Why needed here: Understanding the baseline method that AnInfoNCE extends is crucial for grasping the theoretical contributions
  - Quick check question: What is the form of the InfoNCE loss and what assumptions does it make about positive pairs?

- **Concept: Identifiability in Representation Learning**
  - Why needed here: The paper's main theoretical contribution is proving that AnInfoNCE is identifiable under anisotropic conditions
  - Quick check question: What does it mean for a representation to be identifiable, and how does this relate to recovering ground-truth latents?

- **Concept: Latent Variable Models and Data Generating Processes**
  - Why needed here: The theoretical framework models observations as coming from a DGP with latent variables, which is fundamental to the analysis
  - Quick check question: How does the paper model the relationship between observations, latents, and augmentations using a DGP?

## Architecture Onboarding

- **Component map:**
  - Encoder f -> Latent representations -> Weighted similarity with ˆΛ -> AnInfoNCE loss
  - Generator g (ground-truth DGP, not learned)

- **Critical path:**
  1. Sample positive/negative pairs using augmentations
  2. Compute latent representations using encoder f
  3. Calculate weighted similarity using learned ˆΛ
  4. Compute AnInfoNCE loss
  5. Update encoder f and scaling matrix ˆΛ via gradient descent

- **Design tradeoffs:**
  - Batch size vs. latent dimensionality: Higher dimensional latents require larger batch sizes for identifiability
  - Concentration parameter Λ vs. batch size: Higher concentration requires larger batch sizes to capture distribution
  - Model complexity vs. identifiability: More complex models may overfit without sufficient data

- **Failure signatures:**
  - Degraded downstream accuracy despite high augmentation readout
  - Collapse of certain latent dimensions (R² ≈ 0 for some latents)
  - Insufficient augmentation overlap (disjoint positive/negative distributions)
  - Domain shift between training (augmented) and test (non-augmented) distributions

- **First 3 experiments:**
  1. Synthetic experiment varying Λ concentration to verify identifiability under controlled conditions
  2. MNIST experiment with VAE-generated images to test on image-like data with known DGP
  3. Real-world experiment on CIFAR10/ImageNet to observe accuracy-identifiability trade-off

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the fundamental trade-off between identifiability and downstream accuracy, and the need for augmentation strategies that align with theoretical assumptions, represent key open challenges in the field.

## Limitations
- The theoretical framework assumes diagonal concentration parameters and specific distribution forms that may not generalize to all augmentation strategies
- Empirical results show a fundamental trade-off between recovering all latents and maintaining downstream performance, but the conditions determining this trade-off remain unclear
- The extension to hard negative mining and loss ensembling lacks theoretical guarantees and comprehensive empirical validation

## Confidence
- Identifiability theory: **High** - rigorous mathematical proofs provided
- Synthetic experiments: **Medium** - controlled conditions but limited scope
- Real-world results: **Medium-Low** - standard benchmarks but fundamental trade-off not fully explained

## Next Checks
1. Test AnInfoNCE on non-Gaussian augmentation distributions (e.g., bimodal positive pairs from random erasing vs. color jittering) to verify robustness to distribution mismatches
2. Conduct systematic ablation studies varying augmentation strength and type to map the boundary conditions for the accuracy-identifiability trade-off
3. Evaluate AnInfoNCE on downstream tasks requiring complete latent recovery (e.g., style transfer, attribute editing) rather than classification to assess practical value of recovered latents