---
ver: rpa2
title: 'LongSSM: On the Length Extension of State-space Models in Language Modelling'
arxiv_id: '2406.02080'
source_url: https://arxiv.org/abs/2406.02080
tags:
- length
- extension
- training
- sequence
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates length extension in state-space models
  (SSMs) for language modeling. The core problem is that SSMs trained with zero hidden
  state initialization struggle to extend learned behavior beyond the training sequence
  length.
---

# LongSSM: On the Length Extension of State-space Models in Language Modelling

## Quick Facts
- arXiv ID: 2406.02080
- Source URL: https://arxiv.org/abs/2406.02080
- Authors: Shida Wang
- Reference count: 40
- Primary result: Previous hidden state initialization enables strong length extension in SSMs, maintaining perplexity below training-length values up to 32,768 tokens

## Executive Summary
This paper addresses the challenge of length extension in state-space models (SSMs) for language modeling. SSMs trained with zero hidden state initialization struggle to maintain performance beyond their training sequence length. The authors identify this as an extrapolation problem analogous to polynomial extrapolation beyond the training data range. They propose a simple yet effective solution: initializing hidden states with the final hidden state from the previous batch rather than zeros. This converts the extrapolation challenge into an interpolation problem within the learned memory function range. Experiments on Mamba and S5 models show that this approach enables strong length extension with up to 2000x reduction in GPU memory requirements while maintaining or improving performance.

## Method Summary
The method changes the hidden state initialization scheme from zero to using the final hidden state from the previous batch. During training with truncated backpropagation through time (TBPTT), each batch's initial hidden state hk+1,0 is set to the previous batch's final hidden state hk,T instead of zeros. This maintains continuity across batch boundaries, converting the extrapolation problem into an interpolation problem. The approach requires consecutive batches without shuffling and shows that longer training sequences are beneficial but not necessary for length extension.

## Key Results
- Models trained with previous-initialized hidden states achieve strong length extension, maintaining perplexity below training-length values up to 32,768 tokens
- A 180M Mamba model trained on T=16 sequences with previous initialization outperforms zero-initialized models trained on sequences up to T=2048
- Previous initialization enables up to 2000x reduction in GPU memory requirements while maintaining or improving length extension performance
- Training with longer context lengths generally improves length extension but is not strictly necessary when using previous initialization

## Why This Works (Mechanism)

### Mechanism 1: Zero hidden state initialization causes extrapolation beyond training length
Zero hidden state initialization loses memory of previous tokens when batch boundaries are crossed, forcing the model to extrapolate from incomplete information equivalent to polynomial extrapolation beyond the data range.

### Mechanism 2: Previous hidden state initialization converts extrapolation to interpolation
Using the final hidden state from the previous batch provides continuity across batch boundaries, transforming the extrapolation problem into an interpolation problem within the learned memory function range.

### Mechanism 3: Longer training context is beneficial but not necessary for length extension
Training with longer sequences provides more data for learning the memory function, but previous hidden state initialization allows models to achieve length extension even with short training sequences by maintaining continuity.

## Foundational Learning

- **State-space models (SSMs) as recurrent models with parallel computation**
  - Why needed: Understanding SSMs is essential to grasp why hidden state initialization matters
  - Quick check: What is the main computational advantage of SSMs over traditional RNNs?

- **Backpropagation Through Time (BPTT) and truncated BPTT**
  - Why needed: The method uses truncated BPTT with previous hidden states
  - Quick check: How does truncated BPTT differ from standard BPTT in terms of gradient computation?

- **Polynomial extrapolation and its instability**
  - Why needed: The paper explains length extension difficulty through the lens of polynomial extrapolation
  - Quick check: Why is polynomial extrapolation generally considered ill-conditioned?

## Architecture Onboarding

- **Component map**: Input layer → SSM layers (with input-dependent gating) → Output layer → Data loader with unshuffled data → Hidden state manager (maintains previous hidden states) → Training loop

- **Critical path**: 
  1. Load consecutive batches without shuffling
  2. Initialize hidden states with previous batch's final hidden state
  3. Forward pass through SSM layers
  4. Compute loss and gradients
  5. Update weights via backpropagation

- **Design tradeoffs**: 
  - Longer training sequences provide better length extension but require more memory
  - Previous hidden state initialization enables length extension with shorter training sequences but may introduce instability
  - Unshuffled data is required for previous hidden state method but limits data diversity

- **Failure signatures**: 
  - Perplexity increases monotonically beyond training length (zero initialization)
  - Training instability or divergence (previous initialization)
  - No improvement in length extension despite using previous initialization

- **First 3 experiments**: 
  1. Compare perplexity curves for zero-initialized vs previous-initialized models on Wikitext103 with T=16 training length
  2. Test length extension capability with different training lengths (T=16, 32, 64, 128) using previous initialization
  3. Evaluate training stability by monitoring loss curves during training with previous initialization

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mathematical relationship between the hidden state initialization scheme and the ability to perform length extension in state-space models? The paper provides theoretical intuition but lacks rigorous mathematical proofs establishing the exact connection between initialization schemes and length extension performance.

### Open Question 2
Why does training with previous-initialized hidden states lead to severe training instability in larger models, and how can this be addressed? The paper observes significant training instability for larger models but does not provide a theoretical explanation or solution.

### Open Question 3
Is there an optimal training sequence length for achieving maximum length extension performance, and how does this depend on model size? The paper shows that training with longer context lengths generally improves length extension but lacks a theoretical framework for determining optimal training lengths.

### Open Question 4
How does the performance of previous-initialized training generalize to other recurrent architectures beyond state-space models and GRU? The paper demonstrates success with specific model families without systematic exploration of how initialization schemes affect different recurrent architectures.

## Limitations
- The extrapolation-interpretation of length extension is plausible but lacks direct empirical validation
- Claims about numerical stability properties of hidden state evolution need more rigorous mathematical analysis
- Results focus primarily on Mamba and S5 architectures, leaving questions about generalizability to other SSM variants

## Confidence

**High Confidence**: The empirical results showing that previous-hidden-state initialization improves length extension are well-supported with strong evidence across multiple experiments.

**Medium Confidence**: The theoretical explanation of length extension as a polynomial extrapolation problem is plausible but the direct causal link to observed empirical results is not fully established.

**Low Confidence**: Claims about specific numerical stability properties and their relationship to polynomial extrapolation would benefit from more rigorous mathematical analysis.

## Next Checks

1. **Ablation on Initialization Strategies**: Systematically compare different hidden state initialization strategies (zero, previous, learned, random) across multiple training lengths to isolate the specific benefit of the previous-hidden-state approach.

2. **Direct Testing of Extrapolation Hypothesis**: Design experiments that directly test the polynomial extrapolation interpretation by varying the training sequence length and measuring the stability of hidden state evolution.

3. **Generalization Across Architectures and Datasets**: Evaluate the previous-hidden-state initialization method on additional SSM architectures and diverse datasets to assess robustness and generalizability.