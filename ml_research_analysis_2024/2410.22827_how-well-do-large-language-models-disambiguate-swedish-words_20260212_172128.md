---
ver: rpa2
title: How Well Do Large Language Models Disambiguate Swedish Words?
arxiv_id: '2410.22827'
source_url: https://arxiv.org/abs/2410.22827
tags:
- sense
- senses
- definitions
- word
- saldo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates recent large language models (LLMs) on Swedish
  word sense disambiguation (WSD) benchmarks. The study compares different prompting
  approaches, focusing on how to express senses in prompts.
---

# How Well Do Large Language Models Disambiguate Swedish Words?
arXiv ID: 2410.22827
Source URL: https://arxiv.org/abs/2410.22827
Authors: Richard Johansson
Reference count: 15
Primary result: Large language models achieve 85.5% accuracy on Swedish WSD, outperforming unsupervised systems but falling short of supervised approaches.

## Executive Summary
This paper evaluates recent large language models on Swedish word sense disambiguation benchmarks, comparing different prompting approaches. The study finds that LLMs perform better when provided with human-written definitions of senses rather than related-word sets (neighborhoods), though most models still outperform unsupervised graph-based systems. Automatically generated definitions provide some improvement for weaker models but do not consistently match human-written definitions. The best-performing models achieve 85.5% accuracy, while supervised models reach 93.1% and graph-based unsupervised systems achieve 49.7-66.8%.

## Method Summary
The study evaluates LLMs on Swedish WSD using two benchmark datasets (SENSEVAL-2 and Eukalyptus) with SALDO lexicon as the sense inventory. The authors implement three prompting approaches: neighborhood-based (using related words), definition-based (using human-written definitions), and chain-of-thought with automatically generated definitions. Multiple LLMs are tested including Claude 3.5 Sonnet, GPT-4 variants, Llama models, Gemini, and Command R+. The evaluation uses zero-shot prompting where models select correct sense identifiers from context provided by five-sentence windows centered on target words.

## Key Results
- LLMs achieve 85.5% accuracy (Claude 3.5 Sonnet) on Swedish WSD, outperforming unsupervised graph-based systems (49.7-66.8%) but falling short of supervised models (93.1%)
- Definition-based prompts consistently outperform neighborhood-based prompts across all 10 tested LLMs
- Automatically generated definitions improve performance for weaker models but don't match human-written definitions
- Larger and newer models show better performance, with GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro achieving highest scores

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLM performance in Swedish WSD improves significantly when sense definitions are provided rather than related-word sets (neighborhoods).
- Mechanism: Providing explicit definitions reduces ambiguity in the sense selection process, enabling the model to better match context to the intended meaning. Definitions offer clearer semantic boundaries than neighborhood lists.
- Core assumption: LLMs can effectively utilize textual definitions to disambiguate senses when given as part of the prompt.
- Evidence anchors:
  - [abstract] "The best accuracies are achieved when human-written definitions of the senses are included in the prompts."
  - [section] "Table 3 shows the results... The takeaways from this evaluation are: 1) all 10 LLMs perform better with definition prompts than with neighborhoods..."
  - [corpus] Weak - no direct evidence from corpus, but FMR scores suggest relevance of related papers.
- Break condition: If definitions are ambiguous or poorly written, the performance advantage over neighborhoods may diminish or disappear.

### Mechanism 2
- Claim: Larger and more recent LLMs consistently outperform smaller and older models in Swedish WSD tasks.
- Mechanism: Larger models have more parameters and training data, allowing them to capture finer semantic distinctions. More recent models benefit from improved architectures and training techniques.
- Core assumption: Model size and recency correlate with WSD performance due to increased capacity and better training.
- Evidence anchors:
  - [abstract] "while newer models outperform older models and larger models perform smaller models"
  - [section] "Table 3... There is some variation in the performance gap between the two approaches; 2) unsurprisingly, newer models outperform older models and larger models perform smaller models"
  - [corpus] Weak - FMR scores suggest relevance but no direct evidence of model size impact.
- Break condition: If the task requires specialized knowledge not captured in general training, larger models may not maintain their advantage.

### Mechanism 3
- Claim: Automatically generated sense definitions provide some improvement for weaker models but do not consistently match human-written definitions.
- Mechanism: Automatic definitions can provide structure to the disambiguation task, but lack the precision and nuance of human-written definitions, leading to inconsistent performance.
- Core assumption: LLMs can generate useful definitions from neighborhood information, but these definitions are inherently less precise than human-crafted ones.
- Evidence anchors:
  - [abstract] "Automatically generated definitions provide some improvement for weaker models but do not consistently match human-written definitions."
  - [section] "Table 4 shows the SENSEV AL-2 results... The best-perfoming model (GPT-4o) is not improved by any of these techniques... For the weaker models, both variants of definition-writing approaches seem to improve over the baseline."
  - [corpus] Weak - no direct evidence from corpus, but FMR scores suggest relevance of related papers.
- Break condition: If the model's ability to generate definitions from neighborhoods is poor, the performance improvement may not materialize even for weaker models.

## Foundational Learning
- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: The paper evaluates LLMs on WSD, requiring understanding of what the task involves.
  - Quick check question: What is the goal of word sense disambiguation?

- Concept: Lexical-semantic resources (SALDO)
  - Why needed here: The study uses SALDO as the sense inventory, so understanding its structure is crucial.
  - Quick check question: What is the primary descriptor (PD) in SALDO?

- Concept: Prompt engineering
  - Why needed here: The paper investigates different prompting approaches, requiring knowledge of how to construct effective prompts.
  - Quick check question: How do neighborhood-based prompts differ from definition-based prompts?

## Architecture Onboarding
- Component map: Input → Prompt Generation → LLM API Call → Output Processing → Accuracy Calculation
- Critical path: Input → Prompt Generation → LLM API Call → Output Processing → Accuracy Calculation
- Design tradeoffs: Using definitions vs. neighborhoods trades specificity for availability; larger models trade cost for performance; automatic definitions trade precision for scalability.
- Failure signatures: Poor performance on specific lemmas, inconsistent results across models, or failure to outperform baselines.
- First 3 experiments:
  1. Replicate the comparison between definition-based and neighborhood-based prompts for a single model.
  2. Test the effect of automatically generated definitions on a subset of models.
  3. Evaluate the performance of the best-performing model on a held-out dataset.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the size of pre-training data in Swedish correlate with WSD performance across different LLMs?
- Basis in paper: [inferred] The authors note that "it would be useful to consider the amount of Swedish included in pre-training and instruction-tuning for these models, and how these quantities relate to the performance figures reported here, but as of now this information is unavailable."
- Why unresolved: Model developers typically do not disclose detailed language-specific pre-training data sizes, and there is no established benchmark for measuring Swedish content in LLM training corpora.
- What evidence would resolve it: Comparative analysis of model performance against disclosed Swedish pre-training data volumes, or standardized language-specific training data metrics from model developers.

### Open Question 2
- Question: Would fine-tuning LLMs on Swedish WSD datasets significantly improve their performance compared to prompt-based approaches?
- Basis in paper: [explicit] "At present, all current models are less accurate than the best supervised disambiguators in cases where a training set is available"
- Why unresolved: The paper only evaluates prompt-based approaches without fine-tuning, leaving open the question of whether supervised fine-tuning would close the performance gap.
- What evidence would resolve it: Direct comparison of fine-tuned LLM performance versus prompt-based approaches on the same WSD benchmarks.

### Open Question 3
- Question: Can automatically generated sense definitions for SALDO be improved to match or exceed human-written definitions in WSD accuracy?
- Basis in paper: [explicit] "Definitions written automatically seem less useful than human-written definitions and do not consistently outperform neighborhood-based prompts"
- Why unresolved: The paper only tested GPT-4o for automatic definition generation, suggesting other models or techniques might perform better.
- What evidence would resolve it: Comparative evaluation of automatic definition generation using different models and techniques against human-written definitions across multiple WSD benchmarks.

## Limitations
- Evaluation is limited to Swedish, making generalization to other languages uncertain
- Study focuses on zero-shot prompting without exploring few-shot or fine-tuning approaches
- Comparison with unsupervised graph-based systems doesn't fully explore potential of modern supervised WSD approaches

## Confidence
- High confidence: Definition-based prompts outperform neighborhood-based prompts consistently across all models
- Medium confidence: Newer and larger models perform better, though influenced by factors beyond size and recency
- Medium confidence: Automatically generated definitions provide some improvement for weaker models but don't match human-written definitions

## Next Checks
1. Test the prompting approaches on additional languages to assess cross-linguistic generalizability of the definition-based advantage.
2. Evaluate few-shot prompting strategies to determine if performance can be improved beyond zero-shot results without fine-tuning.
3. Conduct ablation studies on the automatic definition generation process to identify which components contribute most to performance improvements for weaker models.