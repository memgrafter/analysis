---
ver: rpa2
title: 'MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language
  Annotations'
arxiv_id: '2406.09401'
source_url: https://arxiv.org/abs/2406.09401
tags:
- object
- scene
- grounding
- objects
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMScan, the largest multi-modal 3D scene
  dataset with hierarchical grounded language annotations. It addresses the limitation
  of previous 3D datasets that focus mainly on object properties or spatial relationships
  by providing comprehensive annotations across region and object levels, covering
  both spatial and attribute understanding.
---

# MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations

## Quick Facts
- **arXiv ID**: 2406.09401
- **Source URL**: https://arxiv.org/abs/2406.09401
- **Reference count**: 40
- **Primary result**: Largest multi-modal 3D scene dataset with hierarchical grounded language annotations, containing 1.4M meta-annotations over 109k objects and 7.7k regions, with two derived benchmarks totaling 3.04M samples

## Executive Summary
MMScan introduces the largest multi-modal 3D scene dataset with hierarchical grounded language annotations, addressing the limitations of previous 3D datasets that focused primarily on object properties or spatial relationships. The dataset was constructed using a top-down approach leveraging VLMs for initial annotation followed by human correction, resulting in comprehensive annotations at both region and object levels. Two benchmarks—3D visual grounding and 3D question answering—were derived from these annotations, revealing that current models struggle with complex prompts involving spatial and attribute reasoning. The dataset significantly improves model performance on existing benchmarks, demonstrating its value for training and evaluating 3D-LLMs and grounding models.

## Method Summary
The dataset construction employed a top-down approach where VLMs generated initial annotations for 3D scenes, followed by human correction to ensure quality. The hierarchical annotation system covers both region-level and object-level understanding, providing comprehensive meta-annotations for spatial relationships and object attributes. The dataset includes 1.4 million meta-annotations spanning 109,000 objects and 7,700 regions extracted from diverse 3D scenes. Two benchmarks were derived from this data: 3D visual grounding and 3D question answering, containing over 3.04 million diverse samples that test both spatial and attribute reasoning capabilities.

## Key Results
- Current models show significant performance gaps on complex prompts requiring spatial and attribute reasoning, with grounding models achieving only 45.0% accuracy on natural language queries
- Training with MMScan data improves model performance on existing benchmarks by substantial margins, demonstrating its effectiveness for model training
- The dataset successfully captures both spatial relationships and attribute understanding at multiple hierarchical levels, enabling comprehensive evaluation of 3D understanding capabilities

## Why This Works (Mechanism)
The hierarchical annotation approach enables models to learn multi-level understanding of 3D scenes, from coarse region descriptions to fine-grained object attributes. By combining VLM-generated initial annotations with human correction, the dataset achieves both scale and quality while maintaining consistency across annotations. The dual benchmark design (grounding and QA) provides comprehensive evaluation of both localization and reasoning capabilities, revealing model weaknesses in handling complex multi-modal queries.

## Foundational Learning
- **3D scene representation**: Understanding how 3D scenes are structured and represented in digital formats - needed for working with the dataset's spatial data
- **Visual grounding**: The task of localizing objects or regions based on language descriptions - fundamental to the dataset's core task
- **Hierarchical annotation**: Organizing labels at multiple levels (region and object) to capture different granularities of information - enables comprehensive scene understanding
- **VLM-human hybrid annotation**: Combining model-generated annotations with human verification - balances scalability with quality
- **Multi-modal reasoning**: Integrating spatial and attribute information from different modalities - essential for the complex queries in the benchmarks

## Architecture Onboarding
**Component Map**: VLM annotation pipeline -> Human correction workflow -> Hierarchical annotation storage -> Benchmark generation -> Model evaluation
**Critical Path**: Scene processing → VLM annotation → Human verification → Hierarchical labeling → Benchmark creation → Model training/evaluation
**Design Tradeoffs**: Scale vs. annotation quality (VLM + human correction), comprehensive coverage vs. annotation complexity (hierarchical approach), benchmark diversity vs. evaluation consistency
**Failure Signatures**: Poor performance on complex spatial-attribute queries, inconsistent localization across similar objects, failure to generalize across room types
**First Experiments**: 1) Test model performance on simple vs. complex prompts to establish difficulty gradient, 2) Evaluate model generalization across different room types, 3) Compare performance using only object-level vs. region-level annotations

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies heavily on VLMs with human correction, raising questions about annotation quality consistency and potential model bias
- Limited qualitative analysis of annotation accuracy and model failure modes, focusing primarily on quantitative improvements
- Does not address potential privacy concerns with real-world 3D scene data or discuss dataset bias and representation across different room types

## Confidence
- Dataset scale and construction methodology: High confidence
- Annotation quality and hierarchy design: Medium confidence
- Benchmark relevance and difficulty: Medium confidence
- Training benefit claims: Medium confidence

## Next Checks
1. Conduct blind human evaluation of random annotation samples to verify quality and consistency across object-level and region-level labels
2. Perform ablation studies comparing model performance using different portions of MMScan data (object-only vs. region-only annotations)
3. Test model generalization on scenes from environments not represented in the training data to assess coverage and potential bias issues