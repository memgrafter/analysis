---
ver: rpa2
title: 'OpenBias: Open-set Bias Detection in Text-to-Image Generative Models'
arxiv_id: '2404.07990'
source_url: https://arxiv.org/abs/2404.07990
tags:
- bias
- biases
- person
- openbias
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OpenBias introduces a novel pipeline for open-set bias detection
  in text-to-image generative models, addressing the limitation of existing methods
  that rely on predefined bias sets. The approach uses a three-stage process: an LLM
  proposes potential biases from captions, the target generative model produces images,
  and a VQA model quantifies the presence and severity of biases.'
---

# OpenBias: Open-set Bias Detection in Text-to-Image Generative Models

## Quick Facts
- arXiv ID: 2404.07990
- Source URL: https://arxiv.org/abs/2404.07990
- Reference count: 40
- One-line primary result: Introduces OpenBias, a novel pipeline for detecting novel biases in text-to-image generative models without predefined bias sets

## Executive Summary
OpenBias addresses the limitation of existing bias detection methods that rely on predefined bias sets by introducing an open-set approach. The framework uses a three-stage pipeline where an LLM proposes potential biases from captions, a text-to-image model generates images, and a VQA model quantifies the presence and severity of biases. This approach enables the discovery of novel, domain-specific biases beyond traditional social categories. Experiments on Stable Diffusion variants demonstrate the framework's ability to detect both known and previously unexplored biases, with results correlating with human judgment and closed-set methods.

## Method Summary
OpenBias implements a three-stage pipeline for open-set bias detection in text-to-image generative models. First, an LLM analyzes captions to propose potential biases, classes, and identification questions. Second, the target text-to-image model generates images from the captions. Third, a VQA model evaluates these images to detect and quantify biases by answering the proposed questions. The severity of detected biases is measured using entropy calculations on the class distributions predicted by the VQA model. The framework supports both context-aware analysis (evaluating bias within individual images) and context-free analysis (averaging scores across all captions for a bias).

## Key Results
- OpenBias successfully detects both traditional and previously unexplored biases in Stable Diffusion variants
- The framework achieves high alignment with human assessments (average absolute error of 0.15)
- Results show bias amplification trends in generative models compared to real data
- OpenBias identifies domain-specific biases beyond well-known social categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenBias discovers novel biases by using an LLM to propose bias-specific questions from captions, which are then answered by a VQA model.
- Mechanism: The LLM analyzes each caption to extract a knowledge base of potential biases, classes, and questions. The VQA model then evaluates images generated from those captions to detect and quantify the presence of those biases. This bypasses the need for pre-defined bias sets.
- Core assumption: The LLM can accurately propose domain-specific biases and the VQA model can correctly interpret the generated images to answer bias-related questions.

### Mechanism 2
- Claim: OpenBias quantifies bias severity using entropy of class distributions predicted by the VQA model.
- Mechanism: After the VQA model predicts the class for each image, the distribution of predicted classes is used to compute entropy. Normalized entropy scores quantify the severity of bias, with higher entropy indicating less bias.
- Core assumption: Entropy is a valid measure of bias severity, where uniform class distributions indicate less bias.

### Mechanism 3
- Claim: OpenBias can detect both context-aware and context-free biases by analyzing images generated from specific captions or the entire dataset.
- Mechanism: Context-aware analysis evaluates bias within images generated from a single caption, considering the context. Context-free analysis averages scores across all captions for a bias, ignoring specific contexts.
- Core assumption: Context influences bias detection, and analyzing both contexts provides a comprehensive understanding of biases.

## Foundational Learning

- Concept: Entropy as a measure of uncertainty or disorder in a probability distribution.
  - Why needed here: Entropy quantifies bias severity by measuring the uniformity of class distributions predicted by the VQA model.
  - Quick check question: If a bias has a uniform class distribution (equal probabilities for all classes), what would be the entropy score, and what does it indicate about the bias?

- Concept: In-context learning in LLMs.
  - Why needed here: The LLM proposes biases by analyzing captions using in-context learning, where task examples are provided in the prompt.
  - Quick check question: How does in-context learning differ from fine-tuning, and why is it suitable for bias proposal in OpenBias?

- Concept: Vision Question Answering (VQA) models.
  - Why needed here: VQA models interpret images to answer bias-related questions proposed by the LLM, enabling bias detection in generated images.
  - Quick check question: What are the key components of a VQA model, and how do they contribute to answering questions about image content?

## Architecture Onboarding

- Component map: LLM -> T2I generative model -> VQA model -> Entropy calculation module
- Critical path:
  1. LLM proposes biases, classes, and questions from captions.
  2. T2I model generates images from captions.
  3. VQA model evaluates images to predict classes.
  4. Entropy is calculated to quantify bias severity.
- Design tradeoffs:
  - Using an LLM for bias proposal allows flexibility but relies on the LLM's accuracy.
  - VQA models may have biases themselves, affecting bias detection.
  - Context-aware analysis is more granular but requires more computational resources.
- Failure signatures:
  - LLM fails to propose relevant biases: biased or irrelevant questions.
  - VQA model misinterprets images: incorrect class predictions.
  - Entropy calculation errors: misleading bias severity scores.
- First 3 experiments:
  1. Test the LLM's ability to propose biases from a small set of captions and verify the relevance of the proposed biases.
  2. Evaluate the VQA model's accuracy in predicting classes for images generated from captions with known biases.
  3. Compare context-aware and context-free bias detection on a dataset with varying contexts to assess the impact of context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OpenBias perform on datasets with different cultural contexts or domains beyond the ones tested (Flickr30k, COCO)?
- Basis in paper: [inferred] The paper notes that OpenBias can propose domain-specific biases tailored to the given captions, but only evaluates on two datasets.
- Why unresolved: The evaluation is limited to two datasets, which may not represent the full range of potential cultural and domain variations.
- What evidence would resolve it: Testing OpenBias on diverse datasets from different cultures and domains, comparing the discovered biases, and analyzing the model's performance across these contexts.

### Open Question 2
- Question: Can OpenBias effectively detect biases in generative models that produce content outside the person domain, such as landscapes or abstract art?
- Basis in paper: [explicit] The paper focuses on person-related biases and mentions the potential for extension to other domains, but does not provide concrete evidence.
- Why unresolved: The evaluation primarily focuses on person-related biases, leaving the effectiveness of OpenBias on other content types unexplored.
- What evidence would resolve it: Applying OpenBias to generative models that produce non-person content (e.g., landscapes, abstract art) and analyzing the discovered biases to assess the model's performance in these domains.

### Open Question 3
- Question: How does the choice of LLM and VQA model impact the bias detection performance of OpenBias?
- Basis in paper: [explicit] The paper notes the modular nature of OpenBias, allowing for component replacement, but does not explore the impact of different model choices.
- Why unresolved: The evaluation uses specific LLM and VQA models, but the paper does not investigate how alternative choices might affect the results.
- What evidence would resolve it: Conducting experiments with different LLM and VQA models, comparing the bias detection performance, and analyzing the impact of these choices on the overall effectiveness of OpenBias.

### Open Question 4
- Question: How does OpenBias handle biases that are subtle or not easily captured by the proposed questions or visual cues?
- Basis in paper: [inferred] The paper discusses the use of questions and visual cues for bias detection but does not address the challenge of subtle or nuanced biases.
- Why unresolved: The evaluation focuses on more apparent biases, leaving the effectiveness of OpenBias in detecting subtle or complex biases unexplored.
- What evidence would resolve it: Designing experiments to test OpenBias on datasets or generative models known to exhibit subtle biases, analyzing the model's ability to detect these biases, and exploring potential improvements to handle such cases.

## Limitations
- The method's effectiveness is limited by the performance of both the LLM and VQA model, each of which may introduce their own biases
- Requires significant computational resources for image generation and evaluation, making it less practical for rapid iteration
- The ability to detect very subtle or complex biases that require deep contextual understanding remains an open question

## Confidence

High: Entropy calculation for bias quantification given well-defined class distributions
Medium: Novel bias discovery capability through LLM proposals
Medium: Agreement with human judgment (average absolute error of 0.15)
Low: Effectiveness on non-person content domains

## Next Checks
1. Conduct ablation studies removing the LLM stage to quantify its contribution to bias discovery beyond what's already known
2. Test OpenBias on text-to-image models with different training datasets to verify bias amplification detection across domains
3. Evaluate the framework's sensitivity to prompt variations in the LLM stage to establish robustness of bias proposals