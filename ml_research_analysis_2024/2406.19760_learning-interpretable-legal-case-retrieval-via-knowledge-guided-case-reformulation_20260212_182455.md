---
ver: rpa2
title: Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation
arxiv_id: '2406.19760'
source_url: https://arxiv.org/abs/2406.19760
tags:
- case
- legal
- retrieval
- query
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KELLER, a legal knowledge-guided case reformulation
  approach for effective and interpretable legal case retrieval. The key idea is to
  leverage professional legal knowledge about crimes and law articles to guide LLMs
  in reformulating lengthy legal cases into concise sub-facts of crimes, which contain
  the essential information for relevance judgment.
---

# Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation

## Quick Facts
- arXiv ID: 2406.19760
- Source URL: https://arxiv.org/abs/2406.19760
- Reference count: 29
- Authors: Chenlong Deng; Kelong Mao; Zhicheng Dou
- Primary result: Achieves new state-of-the-art results in legal case retrieval with significant improvements on complex legal case queries

## Executive Summary
This paper introduces KELLER, a legal knowledge-guided case reformulation approach for effective and interpretable legal case retrieval. The key innovation is leveraging professional legal knowledge about crimes and law articles to guide LLMs in reformulating lengthy legal cases into concise sub-facts of crimes, which contain the essential information for relevance judgment. The reformulated sub-facts are then directly modeled using a simple MaxSim and Sum aggregation, trained with dual-level contrastive learning at both the case level and the sub-fact level. Experiments on two legal case retrieval benchmarks demonstrate that KELLER achieves new state-of-the-art results, with significant improvements over existing methods, especially on complex legal case queries.

## Method Summary
KELLER reformulates legal cases by first extracting crimes and associated law articles using an LLM, then summarizing focused sub-facts for each crime-law article pair. This reformulation transforms the complex task of summarizing long legal documents into a simpler task of summarizing specific sub-facts. The reformulated cases are encoded using text encoders, and relevance scores are computed using MaxSim+Sum aggregation. The model is trained using dual-level contrastive learning, with case-level contrastive learning optimizing overall case relevance and sub-fact-level contrastive learning fine-tuning matching at the sub-fact granularity using a heuristic labeling strategy.

## Key Results
- Achieves new state-of-the-art results on two legal case retrieval benchmarks
- Demonstrates significant improvements over existing methods, particularly on complex legal case queries
- Shows the effectiveness of knowledge-guided reformulation and dual-level contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal knowledge-guided case reformulation improves LLM performance by reducing task complexity
- Mechanism: By extracting crimes and law articles first, then summarizing facts for each pair, the LLM task shifts from summarizing a long document to summarizing focused sub-facts, which are easier to identify
- Core assumption: Crimes and law articles are identifiable and their mapping to facts is learnable
- Evidence anchors:
  - [abstract] "By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes"
  - [section 3.2] "The law articles, serving as high-level abstractions of the actual criminal events, can considerably simplify the task of identifying the corresponding specific facts"
  - [corpus] Weak: no direct neighbor evidence, but related works (ReaKase-8B, LegalSearchLM) use similar legal knowledge integration
- Break condition: If crimes or law articles are not clearly identifiable in the case text, or if the mapping between them and facts is ambiguous

### Mechanism 2
- Claim: MaxSim and Sum aggregation provides effective and interpretable relevance scoring
- Mechanism: MaxSim captures the best matching sub-fact pair per query sub-fact, while Sum aggregates these best matches, assuming each query sub-fact matches at most one document sub-fact
- Core assumption: Legal cases typically have one-to-one sub-fact matching relationships
- Evidence anchors:
  - [section 3.3] "Typically, each query's sub-fact qi matches one document sub-fact dj at most in practice, which is well-suited for MaxSim"
  - [section 3.3] "MaxSim provides clear interpretability by revealing the quantitative contribution of each query and document sub-fact towards the final relevance score"
  - [corpus] Weak: no direct neighbor evidence, but kernel pooling methods in legal IR support multi-granularity matching
- Break condition: If cases contain multiple relevant sub-fact pairs per query sub-fact, or if soft matching is needed

### Mechanism 3
- Claim: Dual-level contrastive learning captures both coarse and fine-grained matching signals
- Mechanism: Case-level contrastive learning optimizes for overall case relevance, while sub-fact-level contrastive learning fine-tunes matching at the sub-fact granularity using heuristic labeling strategy
- Core assumption: High-quality sub-fact relevance labels can be obtained through heuristic strategy combining case relevance and charge matching
- Evidence anchors:
  - [section 3.4] "we propose a heuristic strategy to obtain high-quality relevance labels for the query's sub-facts"
  - [section 3.4] "we incorporate intermediate relevance signals among sub-facts to fine-grainedly enhance the model's effectiveness"
  - [corpus] Weak: no direct neighbor evidence, but contrastive learning is common in legal IR (CaseLink, DELTA)
- Break condition: If heuristic labeling introduces too much noise, or if sub-fact-level signals conflict with case-level signals

## Foundational Learning

- Concept: Legal case structure and components
  - Why needed here: Understanding the sections (procedure, fact, reasoning, decision, tail) is crucial for knowing what information is available in query vs document cases
  - Quick check question: What sections are typically missing from query cases versus document cases in legal case retrieval?

- Concept: Contrastive learning and ranking losses
  - Why needed here: The model uses both case-level and sub-fact-level contrastive losses for training
  - Quick check question: What is the difference between case-level and sub-fact-level contrastive learning objectives in this paper?

- Concept: Text summarization and information extraction
  - Why needed here: The LLM performs extraction of crimes/law articles and summarization of sub-facts
  - Quick check question: What are the two main steps in the legal knowledge-guided case reformulation process?

## Architecture Onboarding

- Component map:
  Input -> Legal Knowledge-Guided Reformulation -> Cross-Matching Module -> Dual-Level Contrastive Learning -> Output

- Critical path:
  1. Reformulate query and document cases into sub-facts using LLM
  2. Encode sub-facts using pre-trained text encoder
  3. Compute similarity matrix and aggregate using MaxSim+Sum
  4. Train using dual-level contrastive learning

- Design tradeoffs:
  - LLM-based reformulation adds inference cost but improves accuracy
  - Simple MaxSim+Sum aggregation trades sophistication for interpretability and efficiency
  - Dual-level contrastive learning increases training complexity but captures multi-granularity signals

- Failure signatures:
  - Poor reformulation quality: Check LLM output for missing or incorrect sub-facts
  - Aggregation issues: Verify that MaxSim+Sum is appropriate for the case complexity
  - Contrastive learning noise: Monitor sub-fact-level loss and check heuristic labeling quality

- First 3 experiments:
  1. Ablation study: Remove legal knowledge guidance and compare reformulation quality
  2. Aggregation comparison: Replace MaxSim+Sum with Mean and kernel pooling
  3. Contrastive learning analysis: Train with only case-level or only sub-fact-level contrastive learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different legal knowledge bases on the performance of KELLER, and how does the quality of these knowledge bases affect the accuracy of crime-law article mappings?
- Basis in paper: [explicit] The paper mentions using a legal expert database to establish mappings between crimes and law articles.
- Why unresolved: The paper does not provide details on the construction of the legal knowledge base or compare different sources of legal knowledge.
- What evidence would resolve it: Comparative experiments using different legal knowledge bases and analysis of their impact on model performance.

### Open Question 2
- Question: How does KELLER perform when applied to legal systems with different structures and terminologies, such as common law versus civil law systems?
- Basis in paper: [explicit] The paper mentions that the method is internationally applicable and uses U.S. legal documents as examples.
- Why unresolved: The paper does not provide empirical results or detailed analysis of KELLER's performance across different legal systems.
- What evidence would resolve it: Experiments evaluating KELLER on legal case retrieval datasets from various jurisdictions with different legal systems.

### Open Question 3
- Question: What are the computational costs associated with using LLMs for case reformulation, and how do these costs scale with the size of the legal document corpus?
- Basis in paper: [explicit] The paper discusses the use of LLMs for case reformulation and mentions techniques like vLLM to achieve high-speed inference.
- Why unresolved: The paper does not provide detailed analysis of computational costs or scalability issues.
- What evidence would resolve it: Performance metrics and cost analysis for KELLER as the size of the document corpus increases.

## Limitations
- Reformulation process heavily relies on quality of legal knowledge extraction and LLM summarization accuracy
- MaxSim+Sum aggregation assumes one-to-one matching relationships that may not hold for complex cases
- Dual-level contrastive learning introduces complexity with potential noise from heuristic sub-fact labeling

## Confidence

- **High Confidence**: The core problem of improving legal case retrieval performance is well-defined, and the overall framework architecture is clearly specified.
- **Medium Confidence**: The mechanism of using legal knowledge for case reformulation is theoretically sound, but practical implementation challenges are not fully explored.
- **Low Confidence**: The effectiveness of the MaxSim+Sum aggregation for complex cases and the quality of heuristic sub-fact labeling in contrastive learning need more empirical validation.

## Next Checks

1. **Reformulation Quality Analysis**: Conduct a human evaluation study comparing the quality of LLM-generated sub-facts with and without legal knowledge guidance, measuring both accuracy and completeness of extracted information.

2. **Aggregation Robustness Test**: Evaluate KELLER's performance on a dataset with varying case complexity, testing whether MaxSim+Sum maintains effectiveness when multiple sub-fact pairs are relevant versus only one-to-one matches.

3. **Contrastive Learning Sensitivity**: Perform an ablation study isolating the impact of case-level versus sub-fact-level contrastive learning, and analyze the correlation between heuristic labeling quality and final retrieval performance.