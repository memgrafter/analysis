---
ver: rpa2
title: Do LLMs Really Think Step-by-step In Implicit Reasoning?
arxiv_id: '2411.15862'
source_url: https://arxiv.org/abs/2411.15862
tags:
- implicit
- step
- reasoning
- intermediate
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) perform
  true step-by-step reasoning when using implicit Chain-of-Thought (CoT) compared
  to explicit CoT. Using probing techniques on hidden states during arithmetic problem-solving,
  the research reveals that when prompted, LLMs do not genuinely compute intermediate
  steps despite achieving correct answers, suggesting reliance on pattern matching
  rather than true reasoning.
---

# Do LLMs Really Think Step-by-step In Implicit Reasoning?

## Quick Facts
- arXiv ID: 2411.15862
- Source URL: https://arxiv.org/abs/2411.15862
- Authors: Yijiong Yu
- Reference count: 2
- Primary result: Prompted LLMs do not perform true step-by-step reasoning in implicit CoT, relying instead on pattern matching.

## Executive Summary
This study investigates whether large language models perform genuine step-by-step reasoning when using implicit Chain-of-Thought (CoT) versus explicit CoT. Using probing techniques on hidden states during arithmetic problem-solving, the research reveals that when prompted to give direct answers, LLMs bypass actual computation of intermediate steps and instead rely on pattern matching to produce correct final answers. However, models specifically trained for implicit CoT do exhibit step-by-step computation, though their capacity is limited by model depth. The study also finds that implicit CoT performance degrades significantly when problem formats are slightly modified, indicating its lack of robustness.

## Method Summary
The study uses linear probing with 1-layer MLP classifiers to detect intermediate reasoning steps from hidden states during arithmetic problem-solving. Two models are tested: Qwen2.5-72B-Instruct (prompted for implicit CoT) and Mistral-7B trained for implicit CoT. The experiments use 2000 multi-step arithmetic problems with 3-5 steps, controlled values (0-19), and only addition/subtraction operations. Probing accuracy measures whether intermediate results are represented in hidden states, while performance is also tested on modified problem formats (reversed order and scaled values).

## Key Results
- Prompted LLMs show low probing accuracy (0.45) for intermediate steps despite correct final answers, indicating reliance on pattern matching rather than computation
- Trained implicit CoT models demonstrate higher probing accuracy (0.70-0.85) and genuine step-by-step computation, but capacity is limited by model depth
- Implicit CoT methods show significant performance degradation when problem presentation is modified, while explicit CoT remains robust

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompted LLMs do not perform genuine step-by-step reasoning in implicit CoT.
- Mechanism: When prompted to give direct answers without showing intermediate steps, LLMs bypass actual computation of intermediate results and instead rely on pattern matching or experience to produce correct final answers.
- Core assumption: LLMs use different internal processes for prompted versus trained implicit CoT, with prompted models skipping intermediate computation entirely.
- Evidence anchors:
  - [abstract] "when prompted, LLMs hardly think about intermediate steps, suggesting they may just rely on experience rather than strict step-by-step reasoning"
  - [section] "the model hardly calculates the intermediate results when there are more than 1 intermediate step, despite it can often give the correct answer"
  - [corpus] Weak - corpus focuses on implicit CoT training methods rather than probing hidden states

### Mechanism 2
- Claim: Trained implicit CoT models do perform step-by-step reasoning but with limited capacity.
- Mechanism: When models are trained to internalize CoT, they compute intermediate results sequentially across layers, but their capacity is constrained by model depth - more complex problems with more steps cannot be fully processed.
- Core assumption: Layer-by-layer processing can handle sequential computation if properly trained, but depth limits the number of steps that can be computed.
- Evidence anchors:
  - [section] "if being trained to, the situation is exactly the opposite: it indeed calculates step by step"
  - [section] "when the problem's steps are getting more, the process of calculating each step is postponed to later layers...there are no layer left to process the remaining steps"
  - [corpus] Weak - corpus focuses on compression methods rather than step-by-step computation analysis

### Mechanism 3
- Claim: Implicit CoT is highly susceptible to problem format variations.
- Mechanism: Implicit CoT methods (both prompted and trained) show significant performance degradation when problem presentation is slightly modified (e.g., reversing equation order or scaling values), while explicit CoT remains robust.
- Core assumption: Implicit reasoning relies heavily on pattern matching that is sensitive to input format, unlike explicit reasoning which follows logical steps.
- Evidence anchors:
  - [abstract] "in both situations, we find the effect of using implicit CoT is susceptible to the format of the problem"
  - [section] "the modified problems significantly degrade the performance for both situations (prompted or trained), especially when the order of equations is reversed"
  - [section] "reversing makes mistral-internal-CoT unable to calculate the intermediate results at all"
  - [corpus] Weak - corpus neighbors focus on improving implicit CoT rather than analyzing format sensitivity

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Understanding the difference between explicit CoT (showing reasoning steps) and implicit CoT (hiding intermediate steps) is fundamental to this study's investigation.
  - Quick check question: What is the key difference between explicit and implicit Chain-of-Thought reasoning in terms of token generation?

- Concept: Linear probing in neural networks
  - Why needed here: The study uses linear probing to detect whether intermediate reasoning steps are represented in hidden states, requiring understanding of how probing techniques work.
  - Quick check question: How does linear probing help determine whether a model has computed specific intermediate values?

- Concept: Model depth and computational capacity
  - Why needed here: The study shows that model depth limits the capacity of implicit CoT reasoning, as more complex problems require more layers than available.
  - Quick check question: Why does increasing the number of reasoning steps in a problem potentially exceed a model's computational capacity in implicit CoT?

## Architecture Onboarding

- Component map: Input problem -> Model processing (with/without explicit steps) -> Hidden state capture at final token -> Linear probing classifier -> Detection of intermediate results -> Performance evaluation on format variations
- Critical path: Input problem → Model processing (with/without explicit steps) → Hidden state capture at final token → Linear probing classifier → Detection of intermediate results → Performance evaluation on format variations
- Design tradeoffs: Implicit CoT offers faster inference and lower computational costs versus explicit CoT, but at the expense of reasoning faithfulness and robustness to input variations
- Failure signatures: Poor probing accuracy for intermediate steps in prompted models, delayed computation of later steps in trained models, and significant accuracy drops when problem format is modified
- First 3 experiments:
  1. Replicate the probing experiment with arithmetic problems of varying step counts to verify intermediate step detection patterns
  2. Test different model sizes (7B, 30B, 70B) to understand the relationship between model capacity and implicit CoT effectiveness
  3. Experiment with different format modifications (not just reversal and scaling) to map the boundaries of implicit CoT robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop implicit CoT methods that are both computationally efficient and as robust as explicit CoT?
- Basis in paper: [explicit] The paper concludes that implicit CoT is faster but less reliable, whether the model is prompted or trained, suggesting it cannot substitute explicit CoT.
- Why unresolved: While the paper shows that implicit CoT is susceptible to problem format changes, it does not explore potential methods to improve its robustness without sacrificing efficiency.
- What evidence would resolve it: Experiments demonstrating a method that maintains the speed of implicit CoT while achieving robustness comparable to explicit CoT across various problem formats and complexities.

### Open Question 2
- Question: What are the underlying mechanisms that allow some models to perform implicit CoT when trained, but not when prompted?
- Basis in paper: [explicit] The paper finds that when prompted, LLMs hardly calculate intermediate steps, but when trained, they do calculate step-by-step, indicating different internal processes.
- Why unresolved: The paper does not delve into the specific changes in model architecture or training processes that enable this shift in reasoning behavior.
- What evidence would resolve it: Detailed analysis of model internals and training data that reveals the specific mechanisms enabling trained models to perform step-by-step reasoning.

### Open Question 3
- Question: How does the depth of the model affect its ability to perform implicit CoT, and is there an optimal depth for balancing efficiency and accuracy?
- Basis in paper: [explicit] The paper indicates that the capacity of implicit CoT is limited by model depth, as deeper models struggle with more steps.
- Why unresolved: The study does not explore the relationship between model depth and performance across different types of reasoning tasks or varying complexities.
- What evidence would resolve it: A comprehensive study testing models of varying depths across diverse reasoning tasks to identify patterns and optimal configurations for implicit CoT performance.

## Limitations

- The study relies on probing hidden states rather than direct observation of reasoning processes, introducing interpretation uncertainty
- Arithmetic problems are deliberately constrained (values 0-19, only addition/subtraction), limiting generalizability to more complex reasoning tasks
- Only two specific model architectures are examined, which may not represent the broader LLM landscape

## Confidence

**High Confidence**: The finding that implicit CoT is highly sensitive to problem format variations is well-supported by experimental results showing significant performance drops when equation order is reversed or values are scaled.

**Medium Confidence**: The conclusion that prompted LLMs do not perform genuine step-by-step reasoning in implicit CoT is supported by probing evidence but relies on interpretation of what hidden state patterns mean.

**Low-Medium Confidence**: The claim about model depth limiting implicit CoT capacity is based on observed patterns in probing accuracy but lacks direct evidence about why later steps are postponed to deeper layers.

## Next Checks

1. Replicate with diverse problem types: Test the probing methodology on algebraic word problems and logical reasoning tasks beyond simple arithmetic to verify whether the hidden state patterns generalize across reasoning domains.

2. Cross-model architecture comparison: Apply the same probing and format variation experiments to models with different architectural designs (transformers vs state space models, different attention mechanisms) to determine whether the observed phenomena are architecture-specific or general.

3. Temporal analysis of hidden states: Track how intermediate results evolve across layers over time during inference to provide more direct evidence about whether models are genuinely computing or merely memorizing patterns, particularly for the trained implicit CoT models.