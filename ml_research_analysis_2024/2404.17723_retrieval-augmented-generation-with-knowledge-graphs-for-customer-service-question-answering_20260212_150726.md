---
ver: rpa2
title: Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question
  Answering
arxiv_id: '2404.17723'
source_url: https://arxiv.org/abs/2404.17723
tags:
- issue
- graph
- knowledge
- retrieval
- ticket
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a knowledge graph-based retrieval-augmented
  generation system for customer service question answering, addressing the limitations
  of treating issue tickets as plain text. The method constructs a knowledge graph
  from historical issues, preserving intra-issue structure and inter-issue relations.
---

# Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering

## Quick Facts
- arXiv ID: 2404.17723
- Source URL: https://arxiv.org/abs/2404.17723
- Reference count: 23
- Primary result: 77.6% increase in MRR and 28.6% reduction in median resolution time

## Executive Summary
This paper presents a knowledge graph-based retrieval-augmented generation system for customer service question answering that addresses limitations of treating issue tickets as plain text. The method constructs a knowledge graph from historical issues, preserving intra-issue structure and inter-issue relations. During question answering, the system parses queries and retrieves related subgraphs for answer generation. Experiments show significant improvements: 77.6% increase in MRR and 0.32 improvement in BLEU score compared to baseline. In production deployment at LinkedIn, the system reduced median per-issue resolution time by 28.6%, demonstrating its effectiveness in enhancing customer service efficiency.

## Method Summary
The system constructs a knowledge graph from historical customer service tickets by parsing each ticket into an intra-issue tree structure and connecting tickets based on explicit relationships and semantic similarity. For question answering, it extracts named entities and intents from user queries, retrieves relevant subgraphs via embedding-based retrieval and graph traversal, then generates answers using an LLM with the retrieved context. The approach maintains ticket structure information that would be lost in traditional text segmentation, enabling more accurate retrieval and complete answers.

## Key Results
- 77.6% increase in MRR for retrieval performance compared to baseline system
- 0.32 improvement in BLEU score for answer generation quality
- 28.6% reduction in median per-issue resolution time in production deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving intra-issue structure and inter-issue relations through graph representation improves retrieval accuracy compared to treating tickets as plain text.
- Mechanism: By parsing each issue ticket into a tree structure (Intra-issue Tree) and connecting tickets based on explicit relationships and semantic similarity (Inter-issue Graph), the system maintains contextual information that would be lost during text segmentation.
- Core assumption: The hierarchical structure within tickets and relationships between tickets contain critical information for accurate retrieval that cannot be captured through simple text embeddings alone.
- Evidence anchors:
  - [abstract] "This integration of a KG not only improves retrieval accuracy by preserving customer service structure information"
  - [section] "Issue tracking documents such as Jira [2] possess inherent structure and are interconnected... The conventional approach of compressing documents into text chunks leads to the loss of vital information"
  - [corpus] Weak evidence - related papers mention graph-enhanced RAG but don't provide comparative performance data for customer service domain specifically
- Break condition: If the inter-ticket relationships are sparse or if most queries don't benefit from understanding the ticket structure, the added complexity of graph construction may not yield meaningful improvements.

### Mechanism 2
- Claim: Graph-based parsing preserves logical coherence of ticket sections, reducing the impact of text segmentation on answer quality.
- Mechanism: Instead of dividing long tickets into fixed-length segments, the system maintains complete sections as nodes in the knowledge graph, ensuring that solutions and their contextual explanations remain connected.
- Core assumption: Important information in customer service tickets often appears across multiple sections (e.g., problem description followed by solution steps), and separating these sections would degrade answer quality.
- Evidence anchors:
  - [abstract] "enhances answering quality by mitigating the effects of text segmentation"
  - [section] "Segmenting extensive issue tickets into fixed-length segments... can result in the disconnection of related content, leading to incomplete answers"
  - [corpus] Limited evidence - while graph-based approaches are mentioned in related work, specific evidence about segmentation impact on answer quality is not provided
- Break condition: If the LLM can effectively reason across segmented text chunks without losing context, or if most queries only require information from single sections, the benefit of preserving section boundaries diminishes.

### Mechanism 3
- Claim: Entity-based retrieval using the knowledge graph with intent detection improves retrieval precision compared to pure embedding-based methods.
- Mechanism: The system first identifies named entities and intents from user queries, then uses these to guide subgraph retrieval through both embedding similarity and graph structure traversal.
- Core assumption: User queries in customer service often contain specific entity references (ticket IDs, issue types, priorities) that can be leveraged to improve retrieval precision beyond what semantic embeddings alone provide.
- Evidence anchors:
  - [section] "our method parses consumer queries to identify named entities and intents. It then navigates within the KG to identify related sub-graphs for generating answers"
  - [section] "In the EBR-based ticket identification step, the top ð¾ticket most relevant historical issue tickets are pinpointed by harnessing the named entity set P derived from user queries"
  - [corpus] Moderate evidence - related papers mention entity-based retrieval but don't provide comparative metrics for this specific approach
- Break condition: If user queries rarely contain explicit entity references or if the entity extraction is inaccurate, the retrieval performance may not improve over pure embedding-based methods.

## Foundational Learning

- Concept: Graph data structures and traversal algorithms
  - Why needed here: The entire system relies on constructing and querying a knowledge graph; understanding nodes, edges, and traversal is fundamental to both construction and retrieval phases
  - Quick check question: Can you explain the difference between breadth-first and depth-first traversal and when each might be appropriate for subgraph extraction?

- Concept: Text embedding and similarity measures
  - Why needed here: The system uses embedding models (BERT, E5) to generate vector representations for graph nodes and compute cosine similarity for retrieval; understanding embedding quality and limitations is crucial
  - Quick check question: What are the main limitations of using cosine similarity on text embeddings for retrieval, and how might these affect the system's performance?

- Concept: Information retrieval metrics (MRR, Recall@K, NDCG@K)
  - Why needed here: The paper evaluates retrieval performance using these standard metrics; understanding what they measure and how they differ is important for interpreting results
  - Quick check question: How would you explain the difference between MRR and Recall@K to a non-technical stakeholder, and when would you prefer one over the other?

## Architecture Onboarding

- Component map:
  - Graph Database (Neo4j) -> Vector Database (QDrant) -> LLM Service -> Template Engine -> Embedding Service -> Query Processing Pipeline

- Critical path:
  1. Query received â†’ Entity/intent extraction via LLM
  2. Embedding-based retrieval to identify top-K relevant tickets
  3. Graph traversal to extract relevant subgraphs
  4. Answer generation using retrieved context and original query
  5. Response returned to user

- Design tradeoffs:
  - Graph vs. text-only: Graph approach adds complexity but preserves structure; text-only is simpler but may lose critical context
  - Node granularity: Coarser nodes (entire sections) preserve coherence but may reduce embedding quality; finer nodes improve embedding quality but risk fragmentation
  - Implicit vs. explicit connections: Relying on semantic similarity captures more relationships but may introduce noise; explicit connections are cleaner but may miss relevant tickets

- Failure signatures:
  - Poor retrieval performance despite good embeddings: Likely indicates issues with graph construction or entity extraction
  - Incomplete answers: May indicate problems with subgraph extraction or LLM prompt design
  - High latency: Could be due to inefficient graph traversal or large subgraph sizes
  - Inconsistent results: May indicate issues with the template or parsing logic

- First 3 experiments:
  1. Baseline comparison: Run the same queries through both the graph-based system and a pure text-based RAG system to verify the claimed performance improvements
  2. Component isolation: Test entity extraction accuracy independently to ensure the foundation of the retrieval process is sound
  3. Stress test: Evaluate system performance with varying numbers of tickets and query types to understand scalability limits and identify edge cases

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Evaluation relies entirely on internal LinkedIn data and golden query sets, limiting generalizability to other customer service domains
- Production impact claims lack rigorous experimental design and control for confounding factors
- System's dependence on named entity extraction and template-based parsing introduces brittleness that cascades through the pipeline

## Confidence
- High confidence: The general architecture and approach are sound, and the problem formulation is well-motivated by the limitations of text-only RAG systems
- Medium confidence: The reported performance improvements are significant, but the evaluation methodology and comparison baselines have limitations
- Low confidence: The production impact claims lack rigorous experimental design and control for confounding factors

## Next Checks
1. **Generalizability test**: Apply the system to a different customer service domain (e.g., e-commerce support tickets) and measure whether similar performance improvements are observed, particularly testing the robustness of entity extraction and template parsing across domains
2. **Component ablation study**: Systematically disable the knowledge graph component while keeping other aspects constant to isolate the specific contribution of the graph-based approach to performance improvements
3. **Long-tail query analysis**: Evaluate system performance on queries that deviate from the golden dataset patterns to understand failure modes and identify the boundary conditions where the graph approach provides diminishing returns