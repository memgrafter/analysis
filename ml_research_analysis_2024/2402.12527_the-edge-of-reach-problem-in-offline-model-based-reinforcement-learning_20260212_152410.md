---
ver: rpa2
title: The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
arxiv_id: '2402.12527'
source_url: https://arxiv.org/abs/2402.12527
tags:
- dynamics
- edge-of-reach
- offline
- model-based
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the "edge-of-reach problem"
  in offline model-based reinforcement learning. The key issue is that truncated rollout
  horizons in offline model-based RL lead to "edge-of-reach" states, which are never
  updated but are used for bootstrapping, causing pathological value overestimation
  and performance collapse.
---

# The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.12527
- Source URL: https://arxiv.org/abs/2402.12527
- Authors: Anya Sims; Cong Lu; Jakob Foerster; Yee Whye Teh
- Reference count: 40
- Primary result: RAVL achieves state-of-the-art performance on D4RL and V-D4RL benchmarks, working even with error-free dynamics models

## Executive Summary
This paper identifies a fundamental problem in offline model-based reinforcement learning (MBRL) called the "edge-of-reach" problem. When rollouts are truncated to a fixed horizon, some states can only be reached in the final step of rollouts. These edge-of-reach states appear as next states (s') in transitions but never as current states (s), meaning their Q-values are used for bootstrapping but never updated. This causes pathological value overestimation and performance collapse. The authors show that existing model-based methods, which rely on dynamics uncertainty penalties, are actually addressing the edge-of-reach problem rather than model errors as claimed. They propose Reach-Aware Value Learning (RAVL), a simple method that uses Q-ensemble variance to detect and penalize edge-of-reach states, achieving strong performance even with perfect dynamics models.

## Method Summary
RAVL (Reach-Aware Value Learning) modifies the standard SAC agent by adding an ensemble of Q-functions and applying value pessimism through minimization over this ensemble. The method trains dynamics models on offline datasets, generates k-step rollouts from dataset states, and trains Q-ensemble members on rollout data with value pessimism. A diversity regularizer (EDAC) encourages exploration. Critically, RAVL works with both learned and true dynamics models, unlike existing methods that fail catastrophically without dynamics uncertainty.

## Key Results
- RAVL achieves state-of-the-art performance on D4RL MuJoCo v2 datasets, matching or exceeding methods like MOPO, MOReL, MOBILE, COMBO, and RAMBO
- RAVL works with true error-free dynamics models, while existing methods fail catastrophically, revealing they address edge-of-reach rather than model errors
- On the pixel-based V-D4RL benchmark using latent-space models, RAVL achieves state-of-the-art performance
- RAVL provides a unified perspective connecting model-based and model-free offline RL through its treatment of edge-of-reach states

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge-of-reach states cause pathological value overestimation because Bellman updates bootstrap from values that are never trained.
- Mechanism: When rollout horizons are truncated, some states can only be reached in the final step of rollouts. These states appear as next states (s') in transitions but never as current states (s), meaning their Q-values are used for bootstrapping but never updated.
- Core assumption: The truncation of rollouts creates a set of states that are reachable only in the final step under any policy.
- Evidence anchors:
  - [abstract] "This truncation of rollouts results in a set of edge-of-reach states at which we are effectively 'bootstrapping from the void.'"
  - [section 3.2] "Edge-of-reach states are those that can be reached in k-steps, but which cannot (under any policy) be reached in less than k-steps."
  - [corpus] Weak evidence - related papers focus on model errors rather than edge-of-reach states specifically.
- Break condition: If rollouts were extended to full horizon or if the state space coverage were perfect, edge-of-reach states would not exist.

### Mechanism 2
- Claim: Existing dynamics uncertainty penalties accidentally address the edge-of-reach problem rather than model errors.
- Mechanism: Dynamics uncertainty estimates are naturally higher for states further from the dataset distribution, which correlates with where edge-of-reach states are likely to occur. This means existing methods unintentionally penalize edge-of-reach states.
- Core assumption: There is a positive correlation between dynamics uncertainty and distance from dataset distribution.
- Evidence anchors:
  - [section 6.5] "In Figure 6 we see a positive correlation between the penalties used in dynamics uncertainty methods and RAVL's effective penalty of value ensemble variance."
  - [abstract] "we reveal how prior model-based methods are primarily addressing the edge-of-reach problem, rather than model-inaccuracy as claimed."
  - [corpus] Weak evidence - most related work focuses on model error penalties without discussing edge-of-reach correlation.
- Break condition: If dynamics models became perfect (zero uncertainty everywhere), this accidental mechanism would fail.

### Mechanism 3
- Claim: Value pessimism using Q-ensemble minimization directly addresses the edge-of-reach problem by detecting and penalizing states with high Q-value variance.
- Mechanism: Edge-of-reach states have high variance across Q-ensemble members because they are never updated, making variance a natural detector. Minimizing over the ensemble then applies pessimism specifically to these problematic states.
- Core assumption: States that are never updated during training will have high variance across ensemble members.
- Evidence anchors:
  - [section 5] "We can therefore detect edge-of-reach states using an ensemble of Q-functions."
  - [section 6.1] "we see that the Q-value variance over the ensemble is significantly higher for edge-of-reach states"
  - [corpus] Moderate evidence - ensemble variance is established in literature for OOD detection.
- Break condition: If the ensemble members converged to identical values (low variance everywhere), this mechanism would lose its detection capability.

## Foundational Learning

- Concept: Bellman backup and bootstrapping
  - Why needed here: The edge-of-reach problem fundamentally involves bootstrapping from values that are never updated, which is central to understanding why truncated rollouts cause issues.
  - Quick check question: What happens in the Bellman backup equation when the next state's Q-value has never been updated during training?

- Concept: Distribution shift and out-of-distribution detection
  - Why needed here: The paper distinguishes between out-of-sample actions (model-free) and out-of-distribution states (model-based edge-of-reach), requiring understanding of both concepts.
  - Quick check question: How does the distribution of states in Drollouts differ from the distribution of next states encountered during rollouts?

- Concept: Ensemble methods for uncertainty estimation
  - Why needed here: RAVL uses Q-ensemble variance to detect edge-of-reach states, building on established techniques from uncertainty estimation literature.
  - Quick check question: Why would ensemble variance be higher for states that are never updated compared to frequently updated states?

## Architecture Onboarding

- Component map:
  Dynamics model (ensemble of transition models) -> k-step rollouts -> Q-function ensemble -> Value pessimism (min over Q-ensemble) -> Policy update

- Critical path:
  1. Train dynamics model on offline dataset
  2. Generate k-step rollouts starting from offline dataset states
  3. Train Q-ensemble on rollout data with value pessimism
  4. Update policy to maximize minimum Q-value
  5. Repeat with new rollouts

- Design tradeoffs:
  - Rollout length k vs coverage: Shorter rollouts reduce model error accumulation but increase edge-of-reach states
  - Q-ensemble size vs computational cost: Larger ensembles provide better detection but increase runtime
  - Diversity regularization strength vs exploration: Higher diversity encourages broader coverage but may reduce stability

- Failure signatures:
  - Q-values exploding to extreme values (10^10 range)
  - Performance collapsing despite perfect dynamics
  - Agent seeking out states at rollout boundaries
  - No improvement even with error-free dynamics

- First 3 experiments:
  1. Verify edge-of-reach states exist by checking which states appear only as next states in rollouts
  2. Test value variance detection by comparing variance at edge-of-reach vs in-distribution states
  3. Validate pessimism effect by comparing performance with/without value minimization on edge-of-reach states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively balance addressing edge-of-reach issues with dynamics model errors in offline RL?
- Basis in paper: [explicit] The authors note that edge-of-reach effects dominate when dynamics models are highly accurate, but dynamics errors could still be significant in other settings. They suggest understanding how to balance these two concerns would be useful future work.
- Why unresolved: The paper demonstrates that edge-of-reach is the dominant issue on standard benchmarks with accurate dynamics models, but doesn't provide a framework for when and how to prioritize addressing edge-of-reach versus model errors.
- What evidence would resolve it: Empirical studies across a spectrum of dynamics model accuracies showing when edge-of-reach versus model errors dominate, and experiments testing methods that explicitly balance both concerns.

### Open Question 2
- Question: How does the edge-of-reach problem manifest in online RL settings, and could it serve as an implicit exploration bias?
- Basis in paper: [explicit] The authors suggest studying the impact of the edge-of-reach effect in a wider setting, including investigating its effect as an implicit exploration bias in online RL.
- Why unresolved: The paper focuses exclusively on offline RL and doesn't explore whether similar issues arise in online settings where data collection is continuous.
- What evidence would resolve it: Experiments comparing online RL algorithms with and without mechanisms to detect and handle edge-of-reach states, measuring the impact on exploration and learning efficiency.

### Open Question 3
- Question: How does RAVL generalize to different representation spaces beyond state-action space, such as latent spaces used in pixel-based RL?
- Basis in paper: [explicit] The authors test RAVL on the pixel-based V-D4RL benchmark using latent-space models and find it achieves state-of-the-art performance, suggesting good generalization to different representation spaces.
- Why unresolved: While initial results are promising, the paper doesn't provide a thorough analysis of how RAVL's performance varies across different types of representation spaces or what properties of the representation space affect its effectiveness.
- What evidence would resolve it: Systematic experiments across various representation spaces (e.g., different latent space architectures, hierarchical representations) measuring RAVL's effectiveness and identifying which properties of the representation space are most important for its success.

## Limitations
- The analysis is primarily empirical with limited theoretical grounding for why edge-of-reach states specifically cause the observed value overestimation
- The claim that existing methods "primarily address the edge-of-reach problem rather than model errors" is based on behavior with true dynamics models, but doesn't exclude the possibility that these methods still provide benefits when dynamics models have real errors
- The paper doesn't provide a framework for when and how to prioritize addressing edge-of-reach versus model errors in different settings

## Confidence

- **High confidence**: The empirical demonstration of the edge-of-reach problem, the effectiveness of RAVL on standard benchmarks, and the failure of existing methods with true dynamics are well-supported by the experimental results.
- **Medium confidence**: The mechanism explanation for why existing methods fail (accidental edge-of-reach detection via dynamics uncertainty) is plausible but not definitively proven - other explanations like residual model errors or implementation details could contribute.
- **Medium confidence**: The claim that RAVL provides a unified perspective of model-based and model-free RL is conceptually reasonable but would benefit from more rigorous theoretical analysis.

## Next Checks

1. **Theoretical analysis**: Derive formal conditions under which edge-of-reach states cause value overestimation, and prove that RAVL's Q-ensemble minimization addresses this specific problem rather than being a general fix.

2. **Ablation study on ensemble size**: Systematically vary the Q-ensemble size to determine the minimum required for effective edge-of-reach detection, and test whether smaller ensembles with better diversity regularization can achieve similar results.

3. **Cross-dataset generalization**: Test RAVL on datasets with different characteristics (e.g., sparse rewards, high-dimensional observations) to verify the edge-of-reach problem exists across diverse offline RL scenarios and that RAVL remains effective.