---
ver: rpa2
title: Large Language Model-guided Document Selection
arxiv_id: '2406.04638'
source_url: https://arxiv.org/abs/2406.04638
tags:
- data
- language
- quality
- selection
- lmlarge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a scalable, large language model-guided
  document selection framework (LMDS) for autonomous selection of high-quality training
  data from web-scale corpora. The core method employs two language models: a large
  instruction-finetuned model (LMlarge) for initial quality labeling and a smaller
  pretrained model (LMsmall) for distillation and scaling.'
---

# Large Language Model-guided Document Selection

## Quick Facts
- arXiv ID: 2406.04638
- Source URL: https://arxiv.org/abs/2406.04638
- Reference count: 13
- Key outcome: Introduces LMDS framework for scalable, LLM-guided document selection achieving up to +5.53 MMLU score improvement with 70% FLOPs reduction

## Executive Summary
This paper introduces a scalable framework for autonomous selection of high-quality training data from web-scale corpora using large language models. The approach employs a two-stage method where a large instruction-finetuned model initially assesses document quality through prompting, then a smaller model is fine-tuned on these labels to enable efficient scoring of the full corpus. Applied to a filtered Common Crawl dataset, the method demonstrates significant improvements in downstream model quality across multiple benchmarks while reducing computational requirements.

## Method Summary
The method employs two language models: a large instruction-finetuned model (LMlarge) for initial quality labeling and a smaller pretrained model (LMsmall) for distillation and scaling. LMlarge samples documents and assesses their educational value through prompt-based evaluation, while LMsmall is fine-tuned on these labels to create a scalable classifier. The fine-tuned LMsmall then scores the full corpus, and documents above a quality threshold are selected for training. This two-stage approach balances precision/recall and computational efficiency, achieving comparable performance to training on full data using at most 70% of the FLOPs.

## Key Results
- Models trained on LMDS-filtered data achieve up to +5.53 MMLU score improvement compared to baseline
- The method achieves comparable performance to full data training using at most 70% of the FLOPs
- Larger LMlarge and LMsmall models yield better results that are less sensitive to prompt variations
- In-context learning helps less-capable labelers improve their performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMDS achieves better data selection by using a high-capability LLM as a quality labeler, followed by distillation into a smaller model for scalable application.
- Mechanism: The large instruction-finetuned LLM (LMlarge) evaluates a subset of documents using a prompt-based quality assessment, then a smaller LLM (LMsmall) is fine-tuned on these labels to create a scalable classifier.
- Core assumption: A strong LLM can effectively judge document quality in a zero-shot manner, and this judgment can be distilled into a smaller model without significant loss.
- Evidence anchors: [abstract] "employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale"; [section] "LMlarge assesses a number of sampled documents in terms of quality and educational value. Subsequently, LMsmall is finetuned on the quality labels from LMlarge."

### Mechanism 2
- Claim: Using a two-stage model (large labeler + small classifier) balances precision/recall and computational efficiency.
- Mechanism: LMlarge provides high-quality labels but is too expensive for full corpus scoring; LMsmall is cheaper and scalable, trained to approximate LMlarge's judgments.
- Core assumption: The computational savings from using LMsmall for full scoring outweigh the marginal loss in label fidelity.
- Evidence anchors: [abstract] "This distillation step allows us to minimize the total amount of compute required for filtering"; [section] "To enable labelling at very large scale, a cheap-to-run-inference-on (smaller) pretrained language model LMsmall is employed."

### Mechanism 3
- Claim: More capable labelers and larger distillation models yield better downstream performance and are less sensitive to prompt variations.
- Mechanism: Higher model capacity improves label quality and consistency, leading to better filtered datasets and less prompt dependency.
- Core assumption: Model capability directly translates to better quality assessment and robustness.
- Evidence anchors: [abstract] "More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt"; [section] "The larger LMlarge is, the better accuracy we can obtain... the Llama-2-chat 70B model exhibits higher agreement compared to the Llama-2-chat 13B."

## Foundational Learning

- Concept: Zero-shot prompting and instruction-following in LLMs
  - Why needed here: LMlarge must assess document quality without fine-tuning, relying solely on prompt instructions.
  - Quick check question: Can a 7B Llama-2-chat accurately follow a simple "Yes/No" prompt about document quality?

- Concept: Knowledge distillation from large to small models
  - Why needed here: LMsmall must learn to replicate LMlarge's labeling behavior to scale up efficiently.
  - Quick check question: Does a 1B model fine-tuned on LMlarge's labels achieve high F1 on a held-out set?

- Concept: Data filtering and selection ratios
  - Why needed here: The cutoff threshold for LMsmall's scores determines the final dataset composition.
  - Quick check question: How does downstream performance change as the selection ratio varies from 20% to 100%?

## Architecture Onboarding

- Component map:
  - LMlarge (70B) -> LMsmall (1B, 302M, 85M) -> Document Selection -> Downstream Training

- Critical path:
  1. Sample n documents from corpus
  2. LMlarge labels samples using prompt
  3. LMsmall fine-tuned on (doc, label) pairs
  4. LMsmall scores full corpus
  5. Apply cutoff threshold to select documents
  6. Train downstream LLM on selected data
  7. Evaluate on benchmarks

- Design tradeoffs:
  - LMlarge size vs. labeling quality: Larger models may be more accurate but cost more to run
  - LMsmall size vs. distillation fidelity: Larger models may retain more signal but are slower to fine-tune
  - Prompt specificity vs. generalization: More specific prompts may yield better quality but reduce flexibility

- Failure signatures:
  - LMsmall achieves low F1 on validation set → distillation failed
  - Downstream model performance matches baseline → selection ineffective
  - High variance across prompt versions → labeler not robust

- First 3 experiments:
  1. Sample 2M documents, run LMlarge labeling with simple prompt, check agreement rate
  2. Fine-tune LMsmall on labeled samples, evaluate F1 on held-out set
  3. Apply LMsmall to full corpus, select top 25%, train 1B model, compare to baseline

## Open Questions the Paper Calls Out
- How does performance vary when applying LMDS to unfiltered web-crawl data compared to the filtered RPJ-CC dataset?
- How does performance change when using different domain-specific prompts for document selection?
- How does the choice of LMsmall capacity affect the trade-off between inference efficiency and selection precision/recall?

## Limitations
- Evaluation primarily conducted on a single filtered Common Crawl dataset (RPJ-CC) with limited testing across diverse corpora or domains
- Exact preprocessing and filtering steps applied to RPJ-CC are not fully detailed, affecting reproducibility
- Computational savings analysis does not fully account for the cost of the complete LMDS pipeline including LMlarge labeling

## Confidence
- High Confidence: The core two-stage approach (large labeler + small classifier) is well-supported by results showing improved downstream model quality
- Medium Confidence: Claims about more capable models yielding better results and being less prompt-sensitive are supported internally but lack external validation
- Low Confidence: The assertion that prompt flexibility allows general and domain-specific applications without reference corpora is not empirically validated

## Next Checks
1. Apply LMDS to a non-English corpus or domain-specific dataset (e.g., biomedical or legal texts) to test cross-domain generalization
2. Measure total computational cost of the complete LMDS pipeline including LMlarge labeling to clarify true efficiency gains
3. Systematically vary the prompt used by LMlarge and measure impact on downstream model performance to validate robustness claims