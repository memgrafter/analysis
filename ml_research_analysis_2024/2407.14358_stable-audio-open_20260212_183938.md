---
ver: rpa2
title: Stable Audio Open
arxiv_id: '2407.14358'
source_url: https://arxiv.org/abs/2407.14358
tags:
- audio
- stable
- music
- training
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stable Audio Open is an open-source text-to-audio model trained
  exclusively on Creative Commons-licensed data. It uses a latent diffusion architecture
  with a 156M-parameter autoencoder, 109M-parameter T5 text encoder, and 1057M-parameter
  transformer-based diffusion model.
---

# Stable Audio Open

## Quick Facts
- arXiv ID: 2407.14358
- Source URL: https://arxiv.org/abs/2407.14358
- Reference count: 0
- State-of-the-art text-to-audio model trained on Creative Commons-licensed data

## Executive Summary
Stable Audio Open is an open-source text-to-audio model that generates high-quality variable-length stereo audio up to 47 seconds at 44.1kHz. The model uses a latent diffusion architecture trained exclusively on Creative Commons-licensed data, achieving state-of-the-art performance on the AudioCaps dataset. It demonstrates realistic sound synthesis capabilities while running efficiently on consumer GPUs, though it cannot generate intelligible speech or singing.

## Method Summary
The model employs a latent diffusion architecture with a 156M-parameter autoencoder that compresses audio into a 64-dimensional latent space at 21.5Hz, reducing computational load during diffusion. A 109M-parameter T5 text encoder provides semantic conditioning through cross-attention layers in the 1057M-parameter transformer-based diffusion model. Timing conditioning enables variable-length audio generation by allowing the model to fill specified time windows with appropriate content followed by silence. The model is trained on 266,324 CC0, 194,840 CC-BY, and 11,454 CC Sampling+ audio recordings from Freesound, plus music tracks from FMA.

## Key Results
- Achieves state-of-the-art performance on AudioCaps with FDopenl3 of 78.24, KLpasst of 2.14, and CLAP score of 0.29
- Generates variable-length stereo audio up to 47 seconds at 44.1kHz
- Runs efficiently on consumer GPUs, achieving 8-20 inference steps per second depending on hardware
- Demonstrates realistic sound synthesis capabilities while requiring no copyright licenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent diffusion architecture enables efficient high-quality audio generation through compressed latent space
- Core assumption: Compressed latent space retains sufficient information for high-quality audio reconstruction
- Evidence anchors: Abstract mentions 44.1kHz generation; section 2.1 describes 64-dim latent bottleneck; section 2.2 explains cross-attention conditioning
- Break condition: If autoencoder compression loses critical high-frequency information, audio quality degrades significantly

### Mechanism 2
- Claim: Timing conditioning enables variable-length generation by teaching the model to fill specified windows
- Core assumption: Model learns to associate timing conditioning with appropriate duration and silence placement
- Evidence anchors: Section 2.3 describes timing conditioning for 47s windows; section 4.2 uses DPM-Solver++ with 100 steps
- Break condition: Poorly calibrated timing conditioning may generate excessive silence or truncate audio prematurely

### Mechanism 3
- Claim: Cross-attention with T5 text embeddings enables semantic control while maintaining quality
- Core assumption: T5 embeddings capture sufficient semantic information to guide audio generation
- Evidence anchors: Section 2.2 uses pretrained T5-base encoder; section 5.1 links high CLAP scores to prompt adherence
- Break condition: If text embeddings fail to capture nuanced audio semantics, generated audio may not match prompts accurately

## Foundational Learning

- Concept: Diffusion probabilistic models and denoising processes
  - Why needed: Understanding DiT's noise prediction is crucial for debugging generation quality
  - Quick check: How does v-objective differ from standard diffusion training objectives?

- Concept: Variational autoencoders and latent space compression
  - Why needed: Autoencoder design directly impacts latent representation quality
  - Quick check: What trade-offs exist between latent rate (21.5Hz) and reconstruction quality?

- Concept: Cross-attention mechanisms and multimodal conditioning
  - Why needed: Understanding text embedding incorporation is essential for prompt engineering
  - Quick check: How does classifier-free guidance (scale of 7.0) balance prompt adherence vs quality?

## Architecture Onboarding

- Component map: Raw audio -> 5-layer CNN encoder -> 64-dim latent bottleneck -> decoder with transposed convolutions -> Output audio
- Critical path: Raw audio → Autoencoder encoding → Latent diffusion (DiT) → Autoencoder decoding → Output audio
- Design tradeoffs: Continuous latents (better quality) vs discrete latents (lower memory), 21.5Hz latent rate (efficiency) vs higher rates (quality), timing conditioning (variable length) vs fixed-length generation
- Failure signatures: Poor audio quality → check autoencoder reconstruction metrics; prompt mismatch → verify T5 embeddings; memory issues → monitor chunk decoding effectiveness
- First 3 experiments:
  1. Test autoencoder reconstruction quality on diverse audio types using AudioCaps and Song Describer datasets
  2. Evaluate prompt adherence across complexity levels (simple sounds vs complex multi-sound descriptions)
  3. Benchmark inference speed and quality trade-offs across different GPU configurations

## Open Questions the Paper Calls Out

1. How does the model's performance scale with longer audio generation windows beyond 47 seconds? The paper mentions variable-length support but only evaluates at 47 seconds without exploring longer generation capabilities.

2. What is the impact of multilingual prompts on the model's performance, given it was mainly trained with English text? The paper explicitly states it's not expected to perform well in other languages but provides no multilingual evaluation.

3. How does the model's performance change when trained on a more diverse dataset including copyrighted music alongside CC-licensed data? The paper notes CC-only training may limit music generation capabilities but doesn't explore performance with copyrighted content.

## Limitations
- Cannot generate intelligible speech or singing due to CC-licensed data constraints
- Performance limited by quality and diversity of Creative Commons-licensed audio data
- Timing conditioning reliability not empirically validated with ablation studies

## Confidence

**High Confidence**: Technical architecture and implementation details are well-specified and reproducible with transparent evaluation methodology.

**Medium Confidence**: State-of-the-art performance claims are based on comparisons with unspecified baseline models, making significance difficult to fully assess.

**Low Confidence**: Generalization capabilities beyond evaluation datasets remain uncertain without additional testing on real-world prompts and edge cases.

## Next Checks

1. Conduct controlled experiments varying timing conditioning values to measure generation length accuracy and silence placement quality.

2. Test the model on diverse audio generation tasks beyond AudioCaps, including music generation from Song Describer and creative audio applications.

3. Systematically evaluate inference performance across different GPU configurations with varying batch sizes and generation lengths to document hardware thresholds.