---
ver: rpa2
title: 'STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex
  Reasoning in Multi-Hop Question Answering'
arxiv_id: '2407.03687'
source_url: https://arxiv.org/abs/2407.03687
tags:
- reasoning
- question
- answer
- questions
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces STOC-T OT, a stochastic tree-of-thought reasoning
  framework with constrained decoding for multi-hop question answering (MHQA). STOC-T
  OT addresses the challenge of complex reasoning in MHQA by constructing a tree-like
  reasoning structure, breaking down questions into sub-questions, and estimating
  the probability of each reasoning path.
---

# STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2407.03687
- Source URL: https://arxiv.org/abs/2407.03687
- Reference count: 13
- Multi-hop QA accuracy improvement of up to 7% exact match and 7.8 F1 points on HotpotQA with GPT-4

## Executive Summary
STOC-TOT introduces a stochastic tree-of-thought reasoning framework with constrained decoding for multi-hop question answering. The framework constructs a tree-like reasoning structure, breaking down complex questions into sub-questions and estimating the probability of each reasoning path. By using constrained decoding to limit answers to words from evidence and questions, STOC-TOT significantly reduces hallucination while improving accuracy. Experiments on HotpotQA and MuSiQue datasets with five LLMs demonstrate substantial performance improvements over other reasoning prompts.

## Method Summary
STOC-TOT employs a stochastic tree-of-thought approach to decompose multi-hop questions into manageable sub-questions. The framework constructs a reasoning tree where each node represents a sub-question and edges represent reasoning paths. During inference, STOC-TOT estimates the probability of each path and uses constrained decoding to generate answers only from words present in the evidence and original question. This approach ensures that the model's outputs remain grounded in the provided context while exploring multiple reasoning trajectories. The stochastic nature of tree construction allows the model to consider various reasoning paths, while the constrained decoding mechanism acts as a safeguard against hallucination by restricting the vocabulary to evidence-based words.

## Key Results
- Up to 7% improvement in exact match accuracy on HotpotQA with GPT-4
- 7.8 point increase in F1 score on HotpotQA compared to baseline reasoning prompts
- Particularly effective for comparison questions and parallel reasoning scenarios
- Consistent performance improvements across five different LLMs tested

## Why This Works (Mechanism)
STOC-TOT's effectiveness stems from its ability to break down complex multi-hop questions into simpler sub-questions while maintaining logical connections between reasoning steps. The stochastic tree construction allows exploration of multiple reasoning paths, increasing the likelihood of finding the correct solution. Constrained decoding ensures that generated answers remain grounded in the provided evidence, significantly reducing hallucination. The framework's strength lies in its systematic approach to complex reasoning, where each sub-question is addressed with focused attention on relevant evidence segments.

## Foundational Learning
- Tree-of-thought reasoning: Why needed - breaks complex problems into manageable sub-questions; Quick check - verify that each sub-question logically connects to the next
- Constrained decoding: Why needed - prevents hallucination by limiting vocabulary; Quick check - ensure generated words appear in evidence or question
- Multi-hop reasoning: Why needed - handles questions requiring multiple evidence pieces; Quick check - validate that all necessary evidence is considered
- Stochastic path selection: Why needed - explores multiple reasoning trajectories; Quick check - confirm diverse paths are generated
- Evidence-grounded generation: Why needed - maintains factual accuracy; Quick check - verify all generated content is supported by evidence

## Architecture Onboarding

Component Map:
Input Question -> Tree Construction -> Sub-question Generation -> Evidence Retrieval -> Constrained Decoding -> Answer Generation

Critical Path:
The critical path involves question decomposition, tree construction, evidence retrieval for each sub-question, and final answer generation through constrained decoding. Each sub-question must be answered using relevant evidence before proceeding to the next level of the tree.

Design Tradeoffs:
- Stochastic vs deterministic tree construction: Stochastic allows exploration but may introduce variability
- Constrained vocabulary: Reduces hallucination but may limit nuanced responses
- Tree depth: Deeper trees capture more complex reasoning but increase computational cost
- Evidence selection: More evidence improves accuracy but increases processing time

Failure Signatures:
- Inconsistent performance across runs due to stochastic tree construction
- Potential inability to generate contextually appropriate responses when constrained vocabulary is too restrictive
- Difficulty handling questions requiring external knowledge not present in evidence

First Experiments:
1. Test STOC-TOT on a simple two-hop question to verify basic functionality
2. Compare constrained vs unconstrained decoding outputs on the same question
3. Evaluate performance with different tree depths on questions of varying complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may be partially attributed to specific evaluation setup on HotpotQA and MuSiQue datasets
- Effectiveness on other complex reasoning tasks beyond multi-hop QA remains untested
- Constrained decoding might restrict generation of nuanced or contextually appropriate responses in some scenarios
- Stochastic nature of tree construction could lead to inconsistent performance across runs

## Confidence
High confidence: Framework architecture and constrained decoding mechanism are clearly described and logically sound. Reported performance improvements are substantial and well-documented.

Medium confidence: Generalizability to other complex reasoning tasks beyond multi-hop QA is suggested but not empirically validated. Long-term effectiveness in real-world applications is uncertain.

Low confidence: Impact of stochastic tree construction on performance consistency and potential limitations of constrained decoding in generating contextually appropriate responses require further investigation.

## Next Checks
1. Evaluate STOC-TOT on additional complex reasoning datasets beyond HotpotQA and MuSiQue, such as DROP or QASC, to assess generalizability across different QA tasks and reasoning types.

2. Conduct ablation studies to quantify individual contributions of stochastic tree construction and constrained decoding components to overall performance improvements.

3. Perform detailed analysis of framework's performance across different question types and complexity levels within evaluated datasets to identify potential limitations or areas for improvement.