---
ver: rpa2
title: Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein
  Space
arxiv_id: '2407.11917'
source_url: https://arxiv.org/abs/2407.11917
tags:
- optimisation
- space
- uncertainty
- function
- wu-go
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel gradient-free optimization method
  for stochastic black-box simulators using deep generative surrogate models and Wasserstein
  uncertainty. The key idea is to model the simulator's response with a deep generative
  model (DGM) and use the Wasserstein distance between predicted and observed responses
  as an uncertainty estimator.
---

# Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space

## Quick Facts
- arXiv ID: 2407.11917
- Source URL: https://arxiv.org/abs/2407.11917
- Authors: Tigran Ramazyan; Mikhail Hushchyn; Denis Derkach
- Reference count: 40
- Key outcome: Introduces WU-GO, a gradient-free optimization method using deep generative surrogate models and Wasserstein uncertainty, outperforming state-of-the-art methods on complex and high-dimensional problems.

## Executive Summary
This paper introduces WU-GO, a novel gradient-free optimization method for stochastic black-box simulators that combines deep generative surrogate models with Wasserstein-based uncertainty estimation. The method addresses key limitations of existing approaches like EGO with Gaussian processes, particularly their poor performance in high-dimensional spaces and with complex, non-differentiable objective functions. WU-GO models the simulator's response using a conditional WGAN-GP and uses the Wasserstein distance between predicted and observed responses as an uncertainty estimator, which captures both distributional differences and objective function geometry.

The authors validate WU-GO on both synthetic benchmark functions and a real-world physics detector simulation problem, demonstrating significant improvements in convergence speed and accuracy compared to state-of-the-art baselines. For example, in 2D experiments WU-GO achieved 0.071 ± 0.000 distance to optimum after 50 simulator calls compared to 0.089 ± 0.035 for the best baseline. The method is particularly effective for non-differentiable objectives and when reconstruction plays a key role in the optimization pipeline.

## Method Summary
WU-GO implements gradient-free optimization using deep generative surrogate models (DGMs) trained as conditional WGAN-GP networks to approximate black-box simulator responses. The key innovation is replacing standard GP variance with Wasserstein uncertainty, estimated via Energy distance between predicted and observed responses. The optimization combines EGO and LCB acquisition functions through a regret function RW(θ) = μ(θ) - κ · σW(θ), where σW is the Wasserstein uncertainty. The method is evaluated against EGO with various surrogates (Gaussian Process, Deep Gaussian Process, Bayesian Neural Network, Adversarial Deep Ensemble) on benchmark functions (Three Hump Camel, Ackley, Lévi, Himmelblau, Rosenbrock, Styblinski-Tang) and a real-world high energy physics detector simulation (SHiP muon shield).

## Key Results
- WU-GO achieved 0.071 ± 0.000 distance to optimum after 50 simulator calls in 2D experiments, outperforming the best baseline (0.089 ± 0.035)
- Significant improvements in convergence speed and accuracy across all tested benchmark functions
- Superior performance on non-differentiable objectives and problems where reconstruction plays a key role
- Effective scaling to higher-dimensional problems compared to GP-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein uncertainty improves robustness over standard GP variance in high-dimensional and stochastic settings
- Mechanism: Replaces L2 space variance with Wasserstein distance (W2 metric space) to capture distributional differences and objective function geometry
- Core assumption: Wasserstein distance can be effectively approximated by Energy distance for univariate real-valued random variables
- Evidence anchors: Abstract mentions new uncertainty estimator using deep generative models; section discusses moving variance from (Rd, L2) to (P2, W2) space
- Break condition: If Wasserstein distance cannot be well-approximated by L2 norm, uncertainty estimation becomes inaccurate

### Mechanism 2
- Claim: DGMs enable efficient exploration of high-dimensional and complex parameter spaces
- Mechanism: Learns conditional mapping from latent distribution to complex response distribution for fast sampling and exploration
- Core assumption: Well-trained DGM can accurately approximate response distribution for entire parameter space
- Evidence anchors: Abstract mentions addressing issues with deep generative surrogate approach; section discusses working in agnostic predictive posterior settings
- Break condition: If DGM cannot accurately approximate response distribution, exploration efficiency is compromised

### Mechanism 3
- Claim: Combining EGO and LCB with Wasserstein uncertainty balances exploration and exploitation effectively
- Mechanism: Uses regret function RW(θ) = μ(θ) - κ · σW(θ) combining exploitation (μ) and exploration (σW) terms
- Core assumption: Wasserstein uncertainty provides meaningful measure for exploration-exploitation trade-off
- Evidence anchors: Abstract mentions minimizing regret while moving variance to Wasserstein space; section discusses combining approaches
- Break condition: If Wasserstein uncertainty does not accurately reflect true uncertainty, trade-off becomes suboptimal

## Foundational Learning

- Concept: Wasserstein distance and its properties
  - Why needed here: Understanding Wasserstein distance is crucial for grasping how uncertainty estimator works and why it's more robust than standard variance
  - Quick check question: What are key properties of Wasserstein distance that make it suitable for uncertainty estimation in optimization problems?

- Concept: Deep generative models and their training
  - Why needed here: Understanding DGM training is essential for implementing surrogate model component
  - Quick check question: What are key challenges in training DGMs to accurately approximate response distribution of black-box simulator?

- Concept: Bayesian optimization and acquisition functions
  - Why needed here: Understanding Bayesian optimization principles is crucial for grasping overall algorithm and its novelty
  - Quick check question: How do EGO and LCB acquisition functions differ in their approach to balancing exploration and exploitation?

## Architecture Onboarding

- Component map: Deep Generative Model (DGM) -> Wasserstein Uncertainty Estimator -> Optimization Algorithm (WU-GO) -> Baselines (EGO with various surrogates)

- Critical path: 1. Train DGM on ground truth data 2. Use DGM to approximate black-box function and estimate Wasserstein uncertainty 3. Optimize regret function using estimated mean and uncertainty 4. Evaluate chosen configuration and update ground truth data 5. Repeat until stopping criteria are met

- Design tradeoffs: DGMs allow efficient exploration of high-dimensional spaces but require more training data and computational resources compared to GPs; Wasserstein uncertainty estimator is more robust to complex response shapes but relies on approximation that may not always be accurate; combining EGO and LCB balances exploration-exploitation but introduces additional hyperparameter (κ) that needs tuning

- Failure signatures: If DGM fails to accurately approximate response distribution, optimization may converge to suboptimal solutions; if Wasserstein uncertainty estimator is not well-calibrated, exploration-exploitation trade-off may be suboptimal; if optimization algorithm gets stuck in local optima, increasing exploration parameter (κ) or using more diverse initial configurations may help

- First 3 experiments: 1. Implement DGM surrogate model and train on simple synthetic dataset (Three Hump Camel function) 2. Implement Wasserstein uncertainty estimator and compare to standard GP variance on synthetic dataset 3. Implement WU-GO optimization algorithm and compare to EGO with various surrogates on synthetic dataset

## Open Questions the Paper Calls Out

- Question: How does the Wasserstein Uncertainty perform when applied to higher-dimensional outputs beyond univariate real-valued random variables?
- Basis in paper: The paper mentions that Wasserstein uncertainty is currently estimated using Energy distance for univariate real-valued random variables, but suggests this is a limitation for future work
- Why unresolved: Paper only tests method on univariate outputs, leaving multivariate performance unexplored
- What evidence would resolve it: Experimental results showing Wasserstein Uncertainty's performance on multivariate outputs, comparing to other uncertainty estimation methods in higher dimensions

- Question: What is the optimal strategy for selecting the exploration-exploitation hyperparameter κ for WU-GO algorithm?
- Basis in paper: Paper discusses ablation study on κ values, showing optimal value depends on number of starting points and dimensionality, but does not provide definitive selection strategy
- Why unresolved: Paper acknowledges optimality of κ depends on multiple factors and suggests tuning for each configuration, but offers no systematic approach
- What evidence would resolve it: Comprehensive study providing guidelines or method for selecting κ based on problem characteristics, potentially involving meta-learning or automated tuning techniques

## Limitations

- The method relies on Energy distance approximation for Wasserstein distance, which lacks empirical validation across diverse response distributions
- No systematic approach provided for selecting the exploration-exploitation hyperparameter κ
- Performance on higher-dimensional problems (beyond 2D) needs further validation

## Confidence

- High confidence in core algorithmic framework and experimental methodology
- Medium confidence in theoretical justification for using Energy distance as Wasserstein proxy
- Low confidence in method's robustness to model misspecification and hyperparameter sensitivity

## Next Checks

1. Conduct controlled experiments comparing Energy distance approximation to exact Wasserstein distances across various response distribution families to quantify approximation error and identify failure scenarios

2. Systematically test WU-GO's performance when DGM is intentionally misspecified or when training data is limited, measuring sensitivity to model architecture choices and training procedures

3. Extend experiments beyond 2D problems to include higher-dimensional benchmarks (5D, 10D, 20D) to evaluate method's scalability and identify potential limitations in high-dimensional spaces