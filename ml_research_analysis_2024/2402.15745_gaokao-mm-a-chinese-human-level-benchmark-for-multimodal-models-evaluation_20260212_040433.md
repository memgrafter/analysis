---
ver: rpa2
title: 'GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation'
arxiv_id: '2402.15745'
source_url: https://arxiv.org/abs/2402.15745
tags:
- gaokao-mm
- chinese
- lvlms
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GAOKAO-MM, a multimodal benchmark derived
  from Chinese college entrance exams (GAOKAO) to evaluate the comprehensive capabilities
  of Large Vision-Language Models (LVLMs). The benchmark includes 646 questions across
  8 subjects with 897 images in 12 types, designed to test perception, understanding,
  knowledge, and reasoning in native Chinese context.
---

# GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation

## Quick Facts
- arXiv ID: 2402.15745
- Source URL: https://arxiv.org/abs/2402.15745
- Authors: Yi Zong; Xipeng Qiu
- Reference count: 4
- Primary result: All tested LVLMs scored below 50% accuracy on Chinese college entrance exam questions

## Executive Summary
GAOKAO-MM is a novel multimodal benchmark designed to evaluate Large Vision-Language Models using questions derived from Chinese college entrance exams (GAOKAO). The benchmark contains 646 questions across 8 subjects with 897 images in 12 types, testing comprehensive capabilities including perception, understanding, knowledge, and reasoning in native Chinese context. When evaluated on 10 state-of-the-art LVLMs including GPT-4-Vision, Qwen-VL-Plus, and Gemini-Pro-Vision, all models achieved accuracies below 50%, with GPT-4-Vision performing best at 48.1%. The results indicate significant room for improvement in multimodal reasoning capabilities, particularly for mathematical reasoning, long text comprehension, and geometric shape recognition.

## Method Summary
The benchmark was constructed by extracting questions from Chinese college entrance examination papers (GAOKAO) spanning subjects including Chinese, mathematics, English, physics, chemistry, biology, geography, and history. Each question includes relevant images (897 total across 12 types) and requires both visual perception and language understanding capabilities. The evaluation involved 10 leading Large Vision-Language Models tested on the complete dataset, with performance measured by accuracy rates. The analysis focused on identifying specific areas where models struggled, particularly in mathematical reasoning tasks, comprehension of long textual content, and geometric shape recognition.

## Key Results
- All 10 tested LVLMs achieved accuracies below 50%, with GPT-4-Vision highest at 48.1%
- Models showed particular weakness in mathematical reasoning tasks requiring visual-spatial understanding
- Geometric shape recognition and long text comprehension were identified as significant challenge areas
- No model reached human-level performance on the benchmark, despite it being derived from human educational assessments

## Why This Works (Mechanism)
The benchmark leverages the complexity and comprehensiveness of real-world educational assessments to create a challenging evaluation framework. By using actual GAOKAO questions, it ensures the tasks require genuine understanding rather than pattern matching, as these exams are designed to test deep subject knowledge and reasoning abilities. The multimodal nature of many questions, requiring both visual and textual processing, creates a more rigorous test than unimodal evaluations.

## Foundational Learning
- Chinese language processing - why needed: Questions are in native Chinese requiring proper character recognition and semantic understanding
- Visual-spatial reasoning - why needed: Many questions involve geometric shapes and spatial relationships
- Subject-specific knowledge - why needed: Questions span 8 academic subjects requiring diverse domain expertise
- Multimodal integration - why needed: Questions require combining visual information with textual understanding
- Educational assessment design - why needed: Questions follow educational testing principles ensuring validity
- Error analysis methodology - why needed: Systematic identification of model weaknesses requires structured analysis approach

## Architecture Onboarding
Component map: Image preprocessing -> Text understanding -> Knowledge retrieval -> Reasoning engine -> Answer generation
Critical path: Input image and text → Multimodal feature extraction → Context understanding → Knowledge application → Reasoning → Output generation
Design tradeoffs: Chinese language focus vs. generalizability; real exam authenticity vs. controlled testing conditions; comprehensive subject coverage vs. depth in specific areas
Failure signatures: Geometric recognition errors, mathematical reasoning failures, long text comprehension issues, subject knowledge gaps
First experiments:
1. Test with simplified Chinese characters vs. traditional to isolate language processing effects
2. Evaluate with degraded image quality to assess visual robustness
3. Compare performance on single-modality vs. multimodal versions of identical questions

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark representativeness uncertainty - the 646 selected questions may not fully capture Chinese educational assessment breadth
- Low performance could reflect benchmark-specific challenges rather than general model limitations
- Error analysis relies on post-hoc examination without systematic controlled experiments
- Exclusive Chinese-language focus limits generalizability to broader multimodal evaluation contexts
- Absence of human performance baselines makes "human-level" designation difficult to validate

## Confidence
- Benchmark construction methodology: High
- Reported accuracy figures: Medium
- Interpretation of model weaknesses: Low
- Claims about AGI readiness implications: Low

## Next Checks
1. Conduct human evaluation study with native Chinese speakers to establish baseline performance and validate benchmark difficulty claims
2. Perform cross-linguistic validation by translating a subset of questions to English and testing on multilingual models
3. Implement controlled ablation experiments varying image quality, prompt format, and question complexity to isolate factors affecting model performance