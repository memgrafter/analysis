---
ver: rpa2
title: 'EvIL: Evolution Strategies for Generalisable Imitation Learning'
arxiv_id: '2406.11905'
source_url: https://arxiv.org/abs/2406.11905
tags:
- uni00000156
- reward
- learning
- uni0000008e
- shaping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EvIL, a novel approach for improving the retraining
  efficiency of inverse reinforcement learning (IRL) methods. IRL algorithms often
  recover poorly shaped reward functions that require extensive interaction to optimize
  effectively.
---

# EvIL: Evolution Strategies for Generalisable Imitation Learning

## Quick Facts
- arXiv ID: 2406.11905
- Source URL: https://arxiv.org/abs/2406.11905
- Reference count: 40
- Primary result: IRL method using reward model ensembles and ES-based shaping achieves significantly more efficient retraining in source and target environments compared to prior work

## Executive Summary
This paper addresses the challenge of improving retraining efficiency in inverse reinforcement learning (IRL) methods. IRL algorithms often recover poorly shaped reward functions that require extensive interaction to optimize effectively, limiting their practical applicability. The authors propose EvIL, which combines reward model ensembles with a modified training objective to improve re-training effectiveness, and introduces a novel evolution-strategies based method to learn a reward-shaping term that speeds up re-training.

## Method Summary
The method combines IRL++ (a modified IRL training approach with policy buffers, discriminator/policy ensembles, and random policy resets) with ES shaping (evolution strategies to optimize a potential-based shaping term using AUC as fitness). The approach first recovers a reward function using IRL++, then evolves a shaping term optimized for efficient retraining, and finally retrains in target environments using the shaped reward. The method is evaluated on continuous control tasks (Hopper, Walker, Ant) with transfer settings including trembling hand and randomized dynamics.

## Key Results
- EvIL achieves significantly more efficient and effective retraining in source and target environments compared to prior work
- Policies trained with EvIL match expert performance using fewer environment interactions
- The combination of reward model ensembles and ES-based shaping provides consistent improvements across transfer scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reward model ensembles combined with a different training objective significantly improve re-training and transfer performance.
- **Mechanism:** Using an ensemble of discriminators and policies increases the coverage of the state space where the discriminator provides useful feedback. The ensemble average reward is more robust to overfitting to specific learner state distributions.
- **Core assumption:** Each discriminator in the ensemble is trained on a sufficiently different set of states to provide complementary information.
- **Evidence anchors:**
  - [abstract] "For (a), we find that reward model ensembles combined with a slightly different training objective significantly improves re-training and transfer performance."
  - [section] "we adopt an ensemble-based approach. The policy reward is calculated as the average of all the discriminators."
- **Break condition:** If the state distributions seen by different discriminators are too similar, the ensemble provides no additional benefit over a single discriminator.

### Mechanism 2
- **Claim:** Evolution strategies can optimize a reward-shaping term that speeds up re-training in the target environment.
- **Mechanism:** ES directly optimizes the shaping term by using the area under the curve (AUC) of the training performance vs. environment interactions as the fitness function. This directly targets the objective of efficient re-training.
- **Core assumption:** The AUC of the training curve is a good proxy for the quality of the shaping term.
- **Evidence anchors:**
  - [abstract] "For (b), we propose a novel evolution-strategies based method EvIL to optimise for a reward-shaping term that speeds up re-training in the target environment"
  - [section] "we use the area under the curve (AUC) of the performance J(π, ˆr) vs. environment interactions plot as the fitness function for the ES optimisation."
- **Break condition:** If the shaping term optimized for one environment does not generalize well to the target environment, the transfer performance will suffer.

### Mechanism 3
- **Claim:** Random policy resets during IRL training enhance re-trainability by improving exploration.
- **Mechanism:** Occasionally re-initializing the learner policy during training with linearly decreasing probability ensures the discriminator sees a wider variety of state distributions. This prevents overfitting to a specific learner state distribution that might differ from states seen during fresh re-training.
- **Core assumption:** The states visited by a weak initial policy are sufficiently different from those visited by a strong converged policy to provide useful diversity.
- **Evidence anchors:**
  - [section] "we occasionally re-initialise the learner policy during training, with linearly decreasing probability as training advances."
- **Break condition:** If the policy resets are too frequent or too infrequent, they may either disrupt learning or fail to provide sufficient diversity.

## Foundational Learning

- **Concept:** Inverse Reinforcement Learning (IRL) as a game between policy and reward players.
  - Why needed here: Understanding the game-theoretic formulation of IRL is crucial for grasping why modern primal methods may not guarantee effective re-training.
  - Quick check question: In the IRL game formulation, what are the two players optimizing, and what is the objective?

- **Concept:** Potential-based reward shaping.
  - Why needed here: The paper proposes using ES to learn a shaping term that is added to the recovered reward during re-training. Understanding why shaping preserves policy optimality is key.
  - Quick check question: What is the key property of potential-based reward shaping that ensures policy optimality is preserved?

- **Concept:** Evolution Strategies (ES) as a zeroth-order optimization method.
  - Why needed here: The paper uses ES to optimize the shaping term, which requires understanding how ES works and why it's suitable for this problem.
  - Quick check question: How does ES estimate gradients without access to the objective function's derivative?

## Architecture Onboarding

- **Component map:** IRL++ -> ES shaping -> EvIL (combined approach)
- **Critical path:**
  1. Collect expert demonstrations.
  2. Run IRL++ to recover a reward function.
  3. Use ES to evolve a shaping term optimized for the recovered reward.
  4. Retrain in target environment using shaped reward.
- **Design tradeoffs:**
  - Policy buffer vs. no buffer: Buffer provides more stable training but increases memory usage.
  - Ensemble size: Larger ensembles provide better coverage but increase computational cost.
  - ES population size: Larger populations provide better exploration but increase computational cost.
- **Failure signatures:**
  - Poor transfer performance: Shaping term does not generalize to target environment.
  - Ineffective re-training: IRL++ fails to recover a reward that admits effective re-training.
  - Slow convergence: ES takes too long to find a good shaping term.
- **First 3 experiments:**
  1. Verify that ES can learn a shaping function that improves interaction efficiency on a simple RL task with a hand-designed reward.
  2. Check that IRL++ ensures recovered rewards admit effective re-training by extensively retraining on the source environment.
  3. Combine IRL++ and ES shaping to verify improved efficiency and effectiveness of re-training in both source and target environments.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we ensure the final reward function returned by primal IRL methods permits effective (even if not efficient) retraining from scratch?
- **Basis in paper:** [explicit] The paper explicitly identifies this as Challenge 1, stating that modern deep IL algorithms frequently recover rewards which induce policies far weaker than the expert, even in the same environment the demonstrations were collected in.
- **Why unresolved:** This is a fundamental challenge in the field of IRL, as primal methods prioritize interaction efficiency over ensuring effective retraining, unlike classical dual methods.
- **What evidence would resolve it:** A robust method that consistently recovers reward functions that, when optimized from scratch, produce policies with performance matching or exceeding the expert in both source and target environments.

### Open Question 2
- **Question:** How should we modify the discriminator learning process to ensure that the overall IRL procedure returns well-shaped rewards for efficient retraining?
- **Basis in paper:** [explicit] The paper explicitly identifies this as Challenge 2, highlighting that classical IRL algorithms are agnostic to reward shaping and don't provide clear answers for how to make re-training more interaction-efficient.
- **Why unresolved:** The loss functions used in many IRL algorithms are invariant to shaping terms, making it difficult to encourage the discriminator to pick rewards suitable for re-training without explicitly optimizing for this objective.
- **What evidence would resolve it:** A modified discriminator learning process that consistently recovers well-shaped rewards, leading to faster and more efficient retraining compared to existing methods, even when evaluated across diverse environments and tasks.

### Open Question 3
- **Question:** In practice, how do we learn a potential-based shaping term that is useful throughout the course of training from scratch?
- **Basis in paper:** [explicit] The paper explicitly identifies this as Challenge 3, noting that in practice, we often only have access to the critic of a strong policy, which is likely only accurate on states visited by that policy and not the states visited early on in training by weak policies.
- **Why unresolved:** Learning a shaping term that provides useful signal across the entire state space, including states visited by weak initial policies, is challenging without access to the true optimal value function or a way to accurately estimate it.
- **What evidence would resolve it:** A method for learning a potential-based shaping term that consistently accelerates retraining across diverse tasks and environments, even when starting from weak initial policies, and outperforms baselines like using the critic of a strong policy.

## Limitations
- Lack of ablation studies to isolate the contribution of each IRL++ component
- Shaping term optimization assumes AUC is an appropriate proxy for transfer performance without rigorous validation
- Generalization of results to environments beyond tested MuJoCo continuous control suite remains unverified

## Confidence
- **High confidence:** IRL++ components (policy buffers, random resets) improve reward recovery stability - supported by strong empirical results and intuitive mechanism.
- **Medium confidence:** ES-based shaping provides consistent transfer benefits - experimental results show improvement but depend heavily on the AUC fitness proxy and shaping term generalization assumptions.
- **Low confidence:** The specific claim that ES finds better shaping terms than VE shaping across all transfer scenarios - while results show improvement, the mechanism for why ES outperforms VE shaping isn't fully explained.

## Next Checks
1. **Ablation study:** Remove each IRL++ component (policy buffer, discriminator ensemble, policy resets) individually to quantify their independent contributions to transfer performance.
2. **AUC proxy validation:** Test whether shaping terms optimized for AUC on source environment transfer effectively to target environments across different transfer types (dynamics vs. behavioral).
3. **VE shaping comparison:** Conduct head-to-head comparisons of ES shaping versus VE shaping across a broader range of environments and transfer scenarios to verify consistent superiority.