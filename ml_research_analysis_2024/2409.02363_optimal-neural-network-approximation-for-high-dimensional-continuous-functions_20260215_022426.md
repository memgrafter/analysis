---
ver: rpa2
title: Optimal Neural Network Approximation for High-Dimensional Continuous Functions
arxiv_id: '2409.02363'
source_url: https://arxiv.org/abs/2409.02363
tags:
- network
- functions
- function
- approximation
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of neural network approximation
  for high-dimensional continuous functions. The authors show that there exists a
  neural network generated by an elementary universal activation function (EUAF) with
  at most 10889d + 10887 nonzero parameters that can approximate any function in C([a,b]^d)
  with arbitrary accuracy.
---

# Optimal Neural Network Approximation for High-Dimensional Continuous Functions

## Quick Facts
- arXiv ID: 2409.02363
- Source URL: https://arxiv.org/abs/2409.02363
- Authors: Ayan Maiti; Michelle Michelle; Haizhao Yang
- Reference count: 30
- There exists an EUAF network with at most 10889d + 10887 nonzero parameters that can approximate any function in C([a,b]^d) with arbitrary accuracy.

## Executive Summary
This paper establishes the existence of neural networks generated by elementary universal activation functions (EUAF) that can approximate high-dimensional continuous functions with optimal parameter scaling. The key innovation is leveraging a variant of the Kolmogorov Superposition Theorem to reduce the number of required functions from O(d²) to O(d), enabling linear parameter growth with input dimension. The resulting network architecture consists of two sub-networks with fixed widths and depths, achieving arbitrary accuracy through parameter optimization rather than architectural changes.

## Method Summary
The method constructs EUAF networks by decomposing high-dimensional continuous functions using a variant of the Kolmogorov Superposition Theorem. This variant requires only one outer function and 2d+1 inner functions, compared to O(d²) functions in the original theorem. Each function is approximated once using an EUAF sub-network with fixed width 36 and depth 5, then evaluated repeatedly. The network consists of two main components: an inner function approximation sub-network with width 36(2d+1) and depth 5, followed by an outer function approximation sub-network with width 36 and depth 5.

## Key Results
- EUAF networks with at most 10889d + 10887 nonzero parameters can approximate any function in C([a,b]^d) with arbitrary accuracy
- The network consists of two sub-networks with widths 36(2d+1) and 36, both having 5 layers
- A family of continuous functions requires at least width d to achieve arbitrary accuracy, proving the linear parameter scaling is optimal

## Why This Works (Mechanism)

### Mechanism 1
The EUAF network achieves arbitrary accuracy with only O(d) fixed intrinsic neurons by leveraging a variant of the Kolmogorov Superposition Theorem that requires only one outer function and 2d+1 inner functions. This variant decomposes a d-variate continuous function into compositions and additions of univariate continuous functions, drastically reducing the number of required inner functions from O(d²) to O(d). Each inner function is approximated once using an EUAF sub-network, then evaluated repeatedly.

### Mechanism 2
The super approximation property is achieved because only parameter values change for better accuracy, not the number of neurons. The network architecture has fixed width and depth, but the parameter values (weights and biases) are optimized to improve approximation accuracy. This contrasts with standard networks where width/depth must increase for better accuracy.

### Mechanism 3
The requirement of O(d) fixed intrinsic neurons is optimal because there exists a family of continuous functions that requires at least width d for arbitrary accuracy approximation. The constructed family of functions has the property that they vanish at the origin but have values bounded away from zero when at least one input equals 1/2, forcing any approximating network to have sufficient width to handle the dependencies between inputs.

## Foundational Learning

- Concept: Kolmogorov Superposition Theorem and its variants
  - Why needed here: The KST variants are the mathematical foundation that allows decomposition of high-dimensional functions into lower-dimensional components, enabling the linear parameter scaling.
  - Quick check question: What is the key difference between the original KST and the variant used in this paper regarding the number of required functions?

- Concept: Universal approximation properties of neural networks
  - Why needed here: Understanding when and how neural networks can approximate arbitrary continuous functions is crucial for validating the claims about EUAF networks.
  - Quick check question: How does the universal approximation property differ between standard networks (ReLU) and the EUAF networks described in this paper?

- Concept: Fixed-architecture neural networks and super approximation
  - Why needed here: The concept of super approximation, where a fixed architecture can achieve arbitrary accuracy by only changing parameters, is central to understanding the novelty of this work.
  - Quick check question: What distinguishes super approximation from standard neural network approximation in terms of architectural requirements?

## Architecture Onboarding

- Component map: Input → Inner function approximation sub-network (36(2d+1) width, 5 depth) → Linear combination with weights λ_j → Min/max bounding → Outer function approximation sub-network (36 width, 5 depth) → Output
- Critical path: The critical path follows the Kolmogorov decomposition, where input signals are processed through inner function networks, combined linearly, bounded to [0,1], then processed through the outer function network.
- Design tradeoffs: Fixed architecture vs. parameter optimization (super approximation vs. standard approximation), complexity of EUAF vs. standard activations, number of neurons vs. approximation accuracy
- Failure signatures: Inability to approximate certain continuous functions with the fixed architecture, poor performance on functions requiring more than 2d+2 components in KST decomposition, numerical instability in min/max operations
- First 3 experiments:
  1. Verify that the EUAF network can approximate simple univariate functions (e.g., polynomials, sinusoids) with width 36 and depth 5
  2. Test the complete architecture on a synthetic d-variate function constructed using the variant KST to verify the super approximation property
  3. Attempt to approximate the family of functions from Theorem 3.2 with width d-1 to confirm the optimality proof

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of neurons in the network be further reduced while maintaining the super approximation property for high-dimensional continuous functions?
- Basis in paper: [explicit] The authors reduced the number of neurons from O(d²) to O(d) using a variant of the Kolmogorov Superposition Theorem, but they did not explore further reductions.
- Why unresolved: The paper does not investigate whether it's possible to reduce the number of neurons below O(d) while still achieving arbitrary accuracy in approximating high-dimensional continuous functions.
- What evidence would resolve it: Constructing a neural network with fewer than O(d) neurons that can still approximate any function in C([a,b]ᵈ) with arbitrary accuracy, or proving that O(d) is the theoretical minimum.

### Open Question 2
- Question: How does the performance of the elementary universal activation function (EUAF) compare to other activation functions in terms of approximation accuracy and computational efficiency for high-dimensional functions?
- Basis in paper: [explicit] The authors focus on EUAF networks but mention the possibility of combining EUAF with other superexpressive activation functions presented in [26].
- Why unresolved: The paper does not provide a comprehensive comparison of EUAF with other activation functions in terms of approximation accuracy and computational efficiency for high-dimensional functions.
- What evidence would resolve it: Conducting empirical studies comparing the performance of EUAF networks with networks using other activation functions (e.g., ReLU, Leaky-ReLU, superexpressive activations) on various high-dimensional function approximation tasks.

### Open Question 3
- Question: What is the optimal depth required for a neural network to approximate high-dimensional continuous functions with arbitrary accuracy using EUAF?
- Basis in paper: [explicit] The authors use a depth of 5 for their EUAF network, but they do not explore whether this is the optimal depth or if a shallower or deeper network could achieve better results.
- Why unresolved: The paper does not investigate the relationship between network depth and approximation accuracy for high-dimensional functions using EUAF.
- What evidence would resolve it: Analyzing the approximation capabilities of EUAF networks with varying depths (e.g., 3, 4, 5, 6 layers) and determining the depth that provides the best trade-off between accuracy and computational efficiency for approximating high-dimensional continuous functions.

## Limitations

- The paper provides theoretical existence proofs but lacks detailed implementation specifications for the EUAF network construction algorithm
- The numerical stability of min/max operations in the network composition is not addressed
- The practical performance of EUAF networks compared to other activation functions is not empirically evaluated

## Confidence

- **High Confidence**: The linear scaling of parameters with dimension d (O(d)) is well-supported by the theoretical framework and the application of the variant KST.
- **Medium Confidence**: The super approximation property, where a fixed architecture can achieve arbitrary accuracy by only changing parameters, is theoretically sound but requires practical validation.
- **Low Confidence**: The practical implementation of EUAF networks and their ability to approximate the required univariate functions with the stated fixed architecture is the most uncertain aspect.

## Next Checks

1. Rigorously verify that the variant KST used in the paper can indeed decompose any continuous function in C([a,b]^d) into the required number of inner and outer functions.

2. Implement the EUAF activation function and the network construction procedure as described in the paper, testing on simple univariate functions to validate the approximation capabilities.

3. Attempt to construct a continuous function that cannot be approximated by networks with width less than d, following the construction in Theorem 3.2, to validate the optimality claim.