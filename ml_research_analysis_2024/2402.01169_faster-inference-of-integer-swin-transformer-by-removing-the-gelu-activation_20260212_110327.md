---
ver: rpa2
title: Faster Inference of Integer SWIN Transformer by Removing the GELU Activation
arxiv_id: '2402.01169'
source_url: https://arxiv.org/abs/2402.01169
tags:
- swin
- transformer
- latency
- inference
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work improves inference latency of integer-quantized SWIN
  transformers by replacing GELU activation with ReLU. The authors propose removing
  GELU from the quantized SWIN transformer pipeline and substituting it with the simpler
  ReLU activation function, which is more amenable to integer quantization.
---

# Faster Inference of Integer SWIN Transformer by Removing the GELU Activation

## Quick Facts
- **arXiv ID**: 2402.01169
- **Source URL**: https://arxiv.org/abs/2402.01169
- **Reference count**: 8
- **Primary result**: At least 11% inference latency reduction on RTX 4090 GPU while maintaining accuracy drop under 0.5% on ImageNet

## Executive Summary
This work improves inference latency of integer-quantized SWIN transformers by replacing GELU activation with ReLU. The authors propose removing GELU from the quantized SWIN transformer pipeline and substituting it with the simpler ReLU activation function, which is more amenable to integer quantization. They use iterative knowledge distillation to maintain accuracy during this substitution. The resulting GELU-less SWIN transformer is then quantized using post-training quantization. On an RTX 4090 GPU, the method achieves at least 11% inference latency reduction compared to the FasterTransformer framework while maintaining accuracy drop under 0.5% on ImageNet.

## Method Summary
The method involves replacing GELU activation with ReLU in SWIN transformer blocks, using iterative knowledge distillation to maintain accuracy, and applying post-training quantization. The process is applied layer-by-layer, with knowledge distillation compensating for accuracy loss after each replacement. ReLU is fused with the previous GEMM operation to eliminate activation function latency. The final model is quantized and evaluated on target hardware.

## Key Results
- At least 11% inference latency reduction on RTX 4090 GPU
- Accuracy drop maintained under 0.5% on ImageNet
- Method effective across multiple SWIN transformer configurations (SWINTINY, SWINSMALL, SWINBASE, SWINLARGE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GELU activation is computationally expensive and not easily quantizable due to its non-linear nature, causing inference bottlenecks.
- Mechanism: By replacing GELU with ReLU, which is a piecewise linear function, the model removes floating-point operations and enables full integer quantization, reducing inference latency.
- Core assumption: The accuracy drop from replacing GELU with ReLU is minimal and can be compensated by knowledge distillation.
- Evidence anchors:
  - [abstract]: "we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer"
  - [section]: "Softmax, LayerNorm, and GELU activation are the three main non-linear components of vision transformers that are not straightforward to quantize"
  - [corpus]: "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization" - supports the challenge of quantizing non-linear components.
- Break condition: If the accuracy drop from GELU to ReLU substitution exceeds 0.5% and cannot be compensated by knowledge distillation, the mechanism fails.

### Mechanism 2
- Claim: Knowledge distillation compensates for accuracy loss when replacing GELU with ReLU.
- Mechanism: Iterative knowledge distillation is applied layer-by-layer, where the partially modified model learns from the original GELU-based model, preserving accuracy.
- Core assumption: Knowledge distillation is effective in transferring the learned representations from the GELU model to the ReLU model.
- Evidence anchors:
  - [section]: "We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU"
  - [section]: "We gradually apply our changes to the model and modify the transformer blocks one at a time. After each block modification, we use knowledge distillation to distill the soft labels"
  - [corpus]: "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss" - implies knowledge distillation is a viable alternative to retraining.
- Break condition: If knowledge distillation fails to maintain accuracy within 0.5% after GELU removal, the mechanism fails.

### Mechanism 3
- Claim: Fusing ReLU with the previous GEMM operation eliminates the latency of the activation function by applying it directly to the GEMM output.
- Mechanism: Since ReLU is integer-friendly, it can be fused into the integer matrix multiplication operation, removing the need for a separate floating-point fused operation and reducing memory accesses.
- Core assumption: The ReLU operation can be applied as an integer operation without significant overhead or accuracy loss.
- Evidence anchors:
  - [section]: "ReLU is easily quantizable and has a very low complexity... As ReLU is easily quantizable, it can be fused, as an integer operation, to the previous GEMM"
  - [section]: "Our proposed method replaces GELU with ReLU and removes the bias that is fused to it"
  - [corpus]: "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation" - supports the use of integer-only operations for efficiency.
- Break condition: If fusing ReLU with GEMM introduces quantization errors or computational overhead that negate latency gains, the mechanism fails.

## Foundational Learning

- Concept: Vision Transformer Architecture (SWIN Transformer)
  - Why needed here: Understanding SWIN's shifted window attention and why it causes slower inference compared to standard transformers is crucial for appreciating the latency improvements.
  - Quick check question: What is the main architectural difference between SWIN Transformer and the original Vision Transformer that affects inference speed?

- Concept: Integer Quantization
  - Why needed here: Knowledge of how integer quantization reduces inference latency by representing weights and activations with lower bit-widths, and the challenges with non-linear components like GELU.
  - Quick check question: Why are non-linear activation functions like GELU difficult to quantize compared to linear layers?

- Concept: Knowledge Distillation
  - Why needed here: Understanding how knowledge distillation transfers knowledge from a larger or more complex model (GELU-based) to a smaller or simpler one (ReLU-based) to maintain accuracy.
  - Quick check question: How does knowledge distillation help maintain accuracy when replacing GELU with ReLU in the model?

## Architecture Onboarding

- Component map:
  - Original SWIN Transformer with GELU activation and floating-point operations
  - Modified SWIN Transformer with ReLU activation and integer-only operations
  - Knowledge distillation process for accuracy compensation
  - Post-training quantization applied to the modified model

- Critical path:
  - Replace GELU with ReLU layer-by-layer
  - Apply knowledge distillation after each replacement
  - Fuse ReLU with previous GEMM operation
  - Apply post-training quantization to the entire model
  - Measure accuracy and latency on target hardware (RTX 4090 GPU)

- Design tradeoffs:
  - Accuracy vs. latency: Replacing GELU with ReLU may cause accuracy loss, but knowledge distillation mitigates this while achieving latency reduction.
  - Complexity vs. efficiency: GELU is more complex but potentially more accurate; ReLU is simpler and more efficient but may require distillation.
  - Hardware compatibility: Integer quantization improves speed on GPUs with integer tensor cores but may not be supported on all hardware.

- Failure signatures:
  - Accuracy drops below acceptable threshold (0.5% from baseline)
  - Latency improvements are less than expected or negligible
  - Knowledge distillation fails to converge or overfits
  - Quantization introduces significant numerical errors

- First 3 experiments:
  1. Replace GELU with ReLU in the first transformer block only, apply knowledge distillation, and measure accuracy and latency change.
  2. Replace GELU with ReLU in all transformer blocks, apply knowledge distillation, and measure final accuracy and latency.
  3. Compare the latency of the modified model on RTX 4090 GPU with the original FasterTransformer quantized SWIN model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed GELU-less SWIN transformer perform on different hardware platforms beyond NVIDIA RTX 4090, such as mobile GPUs or specialized AI accelerators?
- Basis in paper: [explicit] The paper mentions that previous work demonstrated SWIN transformer is 2.2 times slower than DeiT SMALL on mobile GPUs, and that window shifting operations are not supported by iPhone GPUs.
- Why unresolved: The current study focuses on NVIDIA RTX 4090 GPU performance, leaving the question of how well the method generalizes to other hardware platforms unanswered.
- What evidence would resolve it: Experimental results showing latency and accuracy performance of the GELU-less SWIN transformer on various hardware platforms, including mobile GPUs and specialized AI accelerators.

### Open Question 2
- Question: What is the impact of the proposed GELU replacement on model accuracy when applied to other vision transformer architectures or different computer vision tasks beyond image classification?
- Basis in paper: [inferred] The paper focuses on SWIN transformer for image classification on ImageNet, but does not explore the method's applicability to other transformer architectures or tasks.
- Why unresolved: The effectiveness of GELU replacement with ReLU and knowledge distillation may vary across different architectures and tasks, which is not explored in the current study.
- What evidence would resolve it: Experimental results showing accuracy and latency performance of the GELU-less method on various vision transformer architectures and computer vision tasks.

### Open Question 3
- Question: How does the proposed method scale with larger models and higher resolution inputs, and what are the practical limitations in terms of model size and input resolution?
- Basis in paper: [explicit] The paper mentions that the method was tested on various SWIN transformer configurations but does not explicitly discuss scalability to larger models or higher resolution inputs.
- Why unresolved: While the method shows improvements on tested configurations, its performance on larger models or higher resolution inputs is not addressed, which is crucial for real-world applications.
- What evidence would resolve it: Experimental results demonstrating the method's performance on larger SWIN transformer models and with higher resolution inputs, along with analysis of any limitations encountered.

## Limitations

- Experimental validation limited to a single GPU platform (RTX 4090), raising questions about generalizability across different hardware architectures.
- Knowledge distillation process relies on 10% of ImageNet training data, but selection criteria and impact of data quantity on final accuracy are not fully explored.
- Paper claims that fusing ReLU with GEMM eliminates latency, but specific implementation details and potential quantization errors introduced by this fusion are not thoroughly analyzed.

## Confidence

- **High Confidence**: The core claim that GELU is computationally expensive and replacing it with ReLU reduces inference latency is well-supported by the literature and experimental results.
- **Medium Confidence**: The assertion that knowledge distillation effectively compensates for accuracy loss when replacing GELU with ReLU is supported by experimental results, but the sensitivity to distillation hyperparameters is not fully explored.
- **Low Confidence**: The claim about specific latency improvements (11%) and accuracy bounds (0.5%) may be specific to the experimental setup and not generalize across different hardware or model scales.

## Next Checks

1. **Cross-platform validation**: Test the proposed method on different GPU architectures (e.g., AMD, ARM) and CPU inference to verify hardware-agnostic performance improvements.

2. **Knowledge distillation ablation study**: Systematically vary the knowledge distillation hyperparameters (learning rate, batch size, number of epochs) and training data fraction to understand their impact on accuracy recovery.

3. **Scaling analysis**: Evaluate the method on a broader range of SWIN transformer variants and ImageNet-1k scale variations to assess performance consistency across model sizes.