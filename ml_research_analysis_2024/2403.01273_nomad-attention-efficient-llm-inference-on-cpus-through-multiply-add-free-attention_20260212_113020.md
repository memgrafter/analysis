---
ver: rpa2
title: 'NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free
  Attention'
arxiv_id: '2403.01273'
source_url: https://arxiv.org/abs/2403.01273
tags:
- attention
- cache
- simd
- nomad-attention
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of expensive multiply-add operations
  in attention mechanisms, which are the primary bottleneck for large language model
  inference on CPUs. The authors propose NoMAD-Attention, a method that replaces MAD
  operations with in-register lookups leveraging SIMD registers.
---

# NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention

## Quick Facts
- arXiv ID: 2403.01273
- Source URL: https://arxiv.org/abs/2403.01273
- Reference count: 34
- Key result: 2× speedup on 4-bit quantized LLaMA-7B at 16k context length while maintaining model quality

## Executive Summary
NoMAD-Attention addresses the computational bottleneck of Multiply-Add (MAD) operations in attention mechanisms, which are the primary obstacle to efficient large language model (LLM) inference on CPUs. The authors propose replacing MAD operations with in-register lookups using SIMD registers, leveraging product quantization to transform expensive dot products into memory lookups. This approach achieves significant speedups while maintaining model quality, as measured by perplexity, and outperforms dimensionality reduction-based approaches in both speed and quality preservation.

## Method Summary
NoMAD-Attention replaces MAD operations in attention mechanisms with in-register lookups using SIMD registers. The method employs product quantization to transform dot products into memory lookups, compresses lookup tables into SIMD registers, and reorganizes key cache memory layout for batch parallel dot product lookups. Codebooks for key compression are learned through K-Means clustering on attention key embeddings from training data. The approach uses 8-bit quantized dot products with constrained codebook sizes (16 centroids) and is designed to work with 4-bit quantized LLaMA-7B-based models at 16k context length.

## Key Results
- Achieves up to 2× speedup on 4-bit quantized LLaMA-7B-based models at 16k context length
- Maintains model quality with less than 4% increase in perplexity compared to baseline
- Outperforms dimensionality reduction-based approaches in both speed and quality preservation

## Why This Works (Mechanism)

### Mechanism 1
SIMD register lookups replace MAD operations for attention score computation. Product quantization transforms dot products into memory lookups, compressed lookup tables fit into SIMD registers, and memory layout is reorganized for batch parallel lookups. The core assumption is that SIMD registers can hold sufficient lookup table data to enable fast attention score computation. Break condition: If lookup tables cannot fit into SIMD registers or if quantization error becomes too large.

### Mechanism 2
Product quantization enables efficient dot product estimation through asymmetric distance computation. Keys are quantized using sub-quantizers, query-dependent lookup tables store distances to centroids, and final distance is estimated by accumulating LUT entries based on key codes. The core assumption is that product quantization with constrained codebook size (16 centroids) maintains acceptable accuracy for attention scores. Break condition: If quantization error causes significant model quality degradation.

### Mechanism 3
Learning key compression codebooks per layer/head minimizes quantization error. K-Means clustering is applied to attention key embeddings from training data, with separate codebooks learned for each attention head and layer. The core assumption is that attention key embeddings exhibit distinct distributions across layers and heads that can be captured by separate codebooks. Break condition: If learned codebooks cannot adequately represent key embedding distributions.

## Foundational Learning

- Concept: SIMD (Single Instruction Multiple Data) registers and instructions
  - Why needed here: SIMD registers enable batch parallel lookups that replace MAD operations
  - Quick check question: What is the typical width of SIMD registers used in NoMAD-Attention and what operations do they support?

- Concept: Product Quantization (PQ) for vector compression
  - Why needed here: PQ transforms expensive dot product computations into memory lookups
  - Quick check question: How does asymmetric distance computation in PQ reduce estimation error compared to symmetric methods?

- Concept: Memory hierarchy and cache optimization
  - Why needed here: Understanding why LUTs in registers are faster than LUTs in cache/memory
  - Quick check question: Why is storing lookup tables in SIMD registers significantly faster than storing them in L1 cache?

## Architecture Onboarding

- Component map: Query vector -> Sub-quantization -> LUT computation -> SIMD register load -> Batch lookups via shuffle -> Accumulation -> De-quantization -> Softmax
- Critical path: Query → Sub-quantization → LUT computation → SIMD register load → Batch lookups via shuffle → Accumulation → De-quantization → Softmax
- Design tradeoffs: 8-bit quantization vs. accuracy, 16 centroid codebook vs. compression ratio, dsub=1 vs. dsub=2 compression rates
- Failure signatures: Model quality degradation (increased perplexity), Speedup not achieved, Memory allocation errors, SIMD instruction failures
- First 3 experiments:
  1. Verify NoMAD-Attention maintains model quality (perplexity) compared to baseline at dsub=1
  2. Measure speedup of NoMAD-Attention vs. baseline on CPU for various sequence lengths
  3. Validate that lookup tables fit within SIMD register constraints and that shuffle operations work correctly

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of NoMAD-Attention scale with increasing sequence lengths beyond 16k tokens? The paper demonstrates performance up to 16k context length but doesn't explore longer sequences. Empirical benchmarks comparing NoMAD-Attention against baseline methods at various sequence lengths (32k, 64k, 128k tokens) would reveal scaling behavior and identify potential bottlenecks or diminishing returns.

### Open Question 2
How robust is NoMAD-Attention to variations in the underlying hardware's SIMD register width (e.g., AVX-512 vs AVX2)? The paper acknowledges different SIMD widths exist but only evaluates using 128-bit registers as the most universal option. Performance benchmarks across different CPU architectures with varying SIMD widths, along with implementation variants optimized for each width, would quantify the trade-offs between compatibility and performance.

### Open Question 3
What is the impact of NoMAD-Attention on the inference latency for multi-head attention when compared to single-head implementations? The paper describes single-head attention optimizations but doesn't discuss multi-head scaling or interactions between heads. Empirical measurements comparing single-head versus multi-head NoMAD-Attention performance across different numbers of heads (4, 8, 16) would reveal scaling behavior and identify potential bottlenecks in multi-head scenarios.

## Limitations

- Performance may not generalize to much larger models beyond 7B parameters or significantly longer context lengths beyond 16k
- Effectiveness depends on specific SIMD instruction sets and register widths that vary across CPU architectures
- Quality of learned codebooks depends on the representativeness of the training samples used for clustering

## Confidence

**High Confidence**: The core insight that MAD operations are the primary bottleneck for CPU-based LLM inference is well-established and the mechanism of using SIMD registers for batch parallel lookups is sound.

**Medium Confidence**: The claimed 2× speedup and quality preservation (within 4% perplexity increase) at 16k context length are based on experiments with specific model configurations.

**Low Confidence**: The scalability of the approach to much larger models (beyond 7B parameters) and significantly longer context lengths (beyond 16k) is not demonstrated.

## Next Checks

1. Conduct ablation studies varying the dsub parameter (compression rate) and codebook size systematically to quantify the exact trade-off between model quality and speedup across different sequence lengths (1k, 4k, 16k, 32k).

2. Implement NoMAD-Attention on multiple CPU architectures (x86, ARM, RISC-V with different SIMD capabilities) to verify that the claimed speedups are not architecture-dependent and to identify the minimum SIMD requirements.

3. Evaluate NoMAD-Attention with alternative attention mechanisms including multi-query attention and sliding window attention to determine whether the product quantization approach can be extended beyond standard multi-head attention, and test with models of varying sizes (3B, 13B, 33B parameters) to assess scalability.