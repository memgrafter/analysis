---
ver: rpa2
title: 'Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents'
arxiv_id: '2407.01887'
source_url: https://arxiv.org/abs/2407.01887
tags:
- arms
- regret
- llms
- lead
- dueling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) demonstrate emergent in-context decision-making
  abilities for dueling bandits, quickly identifying the best arm in duels and achieving
  low weak regret across all environment instances. However, they fail to consistently
  converge and exploit, resulting in higher strong regret compared to classic algorithms.
---

# Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents

## Quick Facts
- arXiv ID: 2407.01887
- Source URL: https://arxiv.org/abs/2407.01887
- Reference count: 40
- Primary result: LLMs demonstrate emergent in-context decision-making for dueling bandits but struggle with long-term convergence

## Executive Summary
This paper investigates the in-context learning capabilities of large language models for solving dueling bandit problems, where agents must identify the best arm through pairwise comparisons rather than numeric rewards. The study reveals that while LLMs like GPT-4 Turbo can quickly identify optimal arms in duels and achieve low short-term weak regret, they struggle to consistently converge and exploit, resulting in higher long-term strong regret compared to classic algorithms. To address these limitations, the authors propose LEAD (LLM with Enhanced Algorithmic Dueling), a framework that integrates LLM agents with off-the-shelf dueling bandit algorithms through fine-grained adaptive interplay, inheriting theoretical guarantees while leveraging LLM exploration capabilities.

## Method Summary
The paper evaluates LLMs on dueling bandit tasks using zero-shot in-context learning with chain-of-thought prompting. It tests multiple LLMs (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama3.1, o1-preview) across four environment types (transitive/easy, transitive/hard, intransitive/easy, intransitive/hard) with 5 arms each. Performance is measured using strong regret (long-term optimality) and weak regret (short-term effectiveness). The LEAD framework combines LLM exploration with algorithm-based exploitation using an explore-then-exploit structure with confidence parameters δ and threshold ε to control algorithm confidence and intervention.

## Key Results
- GPT-4 Turbo achieves low weak regret by quickly identifying the best arm in duels across all environment instances
- LLMs struggle to converge and consistently exploit, resulting in higher strong regret than classic algorithms
- LEAD framework achieves competitive performance with robustness against noisy and adversarial prompts while inheriting theoretical guarantees
- LLM performance degrades with increased number of arms and under biased or reversed goal prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 Turbo demonstrates emergent in-context decision-making abilities for relative preference-based tasks by quickly identifying the best arm in duels.
- Mechanism: The model leverages its linguistic priors to process sparse preference feedback from binary duel outcomes, allowing it to rapidly narrow down to the Condorcet winner with low variance across instances.
- Core assumption: LLMs can generalize cross-domain to perform in-context reinforcement learning without task-specific training.
- Evidence anchors:
  - [abstract]: "top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels"
  - [section]: "GPT-4 T URBO consistently outperforms state-of-the-art DB baselines in weak regret... This reveals that the in-context DB abilities emerge as the general capabilities grow in general-purpose LLMs"
  - [corpus]: Weak corpus evidence - related papers discuss in-context RL but not specifically dueling bandits with preference feedback

### Mechanism 2
- Claim: LEAD integrates off-the-shelf DB algorithms with LLM agents through fine-grained adaptive interplay to inherit theoretical guarantees while leveraging LLM exploration capabilities.
- Mechanism: The framework maintains a confidence parameter δ and threshold parameter ε to control algorithm confidence, using an Explore-then-Exploit structure where LLM suggestions are validated and potentially overridden by IF2 when trust is lost.
- Core assumption: Explore-then-Exploit algorithms are particularly well-suited for LLM augmentation due to their symbolic logical structure and well-defined exploration-exploitation trade-off.
- Evidence anchors:
  - [abstract]: "proposes an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay"
  - [section]: "The Explore-Then-Exploit structure naturally aligns with the LLMs' tendency to keep exploring without converging... Its symbolic representation of the algorithm's logic enables clear integration of LLM suggestions"
  - [corpus]: Moderate corpus evidence - papers discuss algorithm-augmented LLMs but not specifically for dueling bandits

### Mechanism 3
- Claim: LLMs struggle with numeric comparisons and systematic biases in understanding DB problems, limiting their convergence and exploitation abilities.
- Mechanism: The language modeling objective alone is insufficient for complex decision-making tasks like DB, as LLMs rely on inherent sampling noise rather than structured exploration policies and lack convergence criteria.
- Core assumption: LLMs' internal biases from pretraining cannot be fully overridden by in-context instructions for out-of-distribution tasks.
- Evidence anchors:
  - [abstract]: "LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations"
  - [section]: "LLMs out-of-the-box lack a fundamental understanding of the DB problem and instead intuitively choose the next pair of arms to compare based on dueling history"
  - [corpus]: Weak corpus evidence - general LLM limitations in numeric tasks mentioned but not specifically for dueling bandits

## Foundational Learning

- Concept: Strong vs Weak Regret
  - Why needed here: The paper evaluates LLM performance using both metrics to distinguish between long-term optimality (strong regret) and short-term effectiveness (weak regret)
  - Quick check question: If an algorithm always selects the Condorcet winner in duels, what would its strong and weak regret look like?

- Concept: Explore-then-Exploit Framework
  - Why needed here: LEAD uses this structure to leverage LLM exploration while ensuring convergence through algorithmic guarantees
  - Quick check question: How does the Explore-then-Exploit approach differ from ongoing regret minimization in terms of theoretical guarantees?

- Concept: Preference Feedback vs Numeric Rewards
  - Why needed here: The paper specifically investigates LLMs' capabilities under preference feedback rather than numeric rewards, which introduces sparsity and different challenges
  - Quick check question: Why might preference feedback be more challenging for LLMs compared to numeric rewards in reinforcement learning tasks?

## Architecture Onboarding

- Component map: LLM agent (policy π) ↔ Prompt interface (P, Ht, R) ↔ Dueling bandit environment ↔ LEAD framework (LLM phase + DB phase) ↔ IF2 algorithm
- Critical path: Prompt generation → LLM suggestion → Arm comparison → History update → Regret calculation → Algorithm selection
- Design tradeoffs: Balancing LLM exploration capabilities with algorithmic convergence guarantees; choosing between different Explore-then-Exploit algorithms for integration
- Failure signatures: LLM getting stuck in local optima, inconsistent convergence behavior, sensitivity to prompt variations, overestimation bias in early exploration
- First 3 experiments:
  1. Run standalone GPT-4 Turbo on Transitive-Easy instance with original prompt to observe weak regret performance
  2. Test LEAD framework with δ=0.4 and t=50 on same instance to compare against standalone LLM and IF2 baseline
  3. Evaluate LEAD robustness using biased history prompt to verify algorithmic fallback behavior

## Open Questions the Paper Calls Out

The paper explicitly mentions several open questions:
- How does the LLM agent's performance generalize to more complex bandit settings like contextual dueling bandits or adversarial dueling bandits?
- What is the impact of exploration vulnerability on long-term regret in more complex preference structures beyond simple transitive and intransitive cases?
- How does the LEAD framework's performance scale with the number of arms K in the dueling bandit problem?

## Limitations

- LLMs struggle to converge to optimal arms even when explicitly prompted to do so, resulting in high long-term strong regret
- Performance degrades with increased number of arms and under biased or reversed goal prompts
- Framework requires careful parameter tuning (δ, ε) and may not fully overcome LLM biases in adversarial settings

## Confidence

- LLM weak regret performance claims: Medium confidence (strong empirical support but sensitive to prompt formulations)
- LLM strong regret convergence claims: Low confidence (consistently struggles to converge despite various prompting strategies)
- LEAD framework effectiveness claims: Medium confidence (shows promise but parameter sensitivity requires further investigation)

## Next Checks

1. Test LEAD with varying δ and ε parameters to identify robustness boundaries and optimal configurations
2. Evaluate performance on larger-scale DB instances (10+ arms) to assess scalability limits and degradation patterns
3. Conduct ablation studies removing the algorithm phase to quantify exact contribution of LLM exploration versus algorithmic guarantees