---
ver: rpa2
title: 'LLM Pruning and Distillation in Practice: The Minitron Approach'
arxiv_id: '2408.11796'
source_url: https://arxiv.org/abs/2408.11796
tags:
- pruning
- distillation
- teacher
- llama
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pruning and distillation approach for compressing
  large language models (LLMs) without access to the original training data. The authors
  introduce a teacher correction phase to adapt the teacher model to the distillation
  dataset, followed by structured pruning and knowledge distillation to produce smaller,
  efficient models.
---

# LLM Pruning and Distillation in Practice: The Minitron Approach

## Quick Facts
- arXiv ID: 2408.11796
- Source URL: https://arxiv.org/abs/2408.11796
- Authors: Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan Yu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel Korzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro
- Reference count: 40
- Primary result: Compressed Llama 3.1 8B to 4B and Mistral NeMo 12B to 8B with state-of-the-art performance across multiple benchmarks

## Executive Summary
This paper presents a novel approach to compressing large language models through pruning and distillation without access to the original training data. The key innovation is a teacher correction phase that adapts the teacher model to the distillation dataset before pruning and knowledge distillation. The method successfully compresses Llama 3.1 8B to 4B parameters and Mistral NeMo 12B to 8B parameters, achieving state-of-the-art performance across multiple benchmarks while providing significant runtime improvements. The approach demonstrates remarkable training efficiency, requiring 40× fewer tokens than training from scratch.

## Method Summary
The method involves three main phases: (1) Teacher correction, where the teacher model is fine-tuned on the distillation dataset to adapt to its distribution, (2) Structured pruning using activation-based importance scores to remove parameters while maintaining performance, and (3) Knowledge distillation to transfer knowledge from the corrected teacher to the pruned student model. The approach uses forward KL divergence loss for distillation and employs both width and depth pruning strategies. Downstream task-based saliency metrics are used for depth pruning to optimize performance on practical tasks rather than just language modeling loss.

## Key Results
- Successfully compressed Llama 3.1 8B to 4B parameters and Mistral NeMo 12B to 8B parameters
- MN-Minitron-8B model outperforms similarly sized models across MMLU, GSM8K, HumanEval, and MT-Bench benchmarks
- Achieved up to 2.7× speedup for depth-pruned variant compared to original Llama 3.1 8B
- Demonstrated 40× training efficiency compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher correction compensates for distribution mismatch between distillation dataset and original pretraining data
- Mechanism: Lightweight fine-tuning of the teacher model on the distillation dataset adapts sub-word token distributions and internal representations to better guide the student model
- Core assumption: The distillation dataset has different sub-word token distribution compared to the original pretraining dataset
- Evidence anchors: Teacher correction improves LM validation loss by ~6%, downstream task performance shows mixed results (some improvements, some degradation)

### Mechanism 2
- Claim: Knowledge distillation recovers accuracy lost during structured pruning
- Mechanism: Forward KL Divergence loss between teacher and student logits transfers the learned knowledge patterns from the unpruned teacher to the pruned student architecture
- Core assumption: The pruned architecture retains enough capacity to learn from the teacher's guidance
- Evidence anchors: MN-Minitron-8B achieves state-of-the-art performance on multiple benchmarks despite being 4B parameters smaller than the original Llama 3.1 8B

### Mechanism 3
- Claim: Downstream task-based saliency metrics for depth pruning outperform validation loss-based metrics
- Mechanism: Using Winogrande accuracy to rank layer importance ensures that removed layers have minimal impact on practical task performance, not just language modeling loss
- Core assumption: Layers important for downstream tasks are not necessarily the same as those important for language modeling validation loss
- Evidence anchors: Contiguous depth pruning with Winogrande metric yields ~59% accuracy vs ~50% for non-contiguous pruning with validation loss metric

## Foundational Learning

- Concept: Structured pruning techniques (depth, width, neuron, attention head pruning)
  - Why needed here: The paper applies multiple pruning strategies to compress models while maintaining performance
  - Quick check question: What's the difference between structured and unstructured pruning?

- Concept: Knowledge distillation fundamentals (KL divergence loss, teacher-student framework)
  - Why needed here: The paper uses knowledge distillation to recover accuracy after pruning
  - Quick check question: Why use KL divergence instead of cross-entropy in distillation?

- Concept: Importance scoring for pruning decisions
  - Why needed here: The paper uses activation-based importance estimation to determine which parameters to prune
  - Quick check question: How does activation-based importance scoring work compared to gradient-based methods?

## Architecture Onboarding

- Component map: Teacher model → Teacher correction → Pruning → Distillation → Student model → Instruction tuning
- Critical path: Pruning + Distillation (the core compression pipeline)
- Design tradeoffs: Width pruning vs depth pruning (accuracy vs speed), teacher correction vs no correction (accuracy vs training time)
- Failure signatures: Large accuracy drop after pruning, distillation failing to converge, teacher correction making teacher worse
- First 3 experiments:
  1. Run teacher correction on Mistral NeMo 12B with 100B tokens and measure LM validation loss improvement
  2. Apply width pruning to Llama 3.1 8B and measure accuracy on MMLU benchmark
  3. Compare depth pruning with validation loss metric vs Winogrande metric on Llama 3.1 8B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the teacher correction phase affect the model's accuracy on downstream tasks, and can it be optimized further?
- Basis in paper: The paper mentions that teacher correction has a minor effect on the teacher model's accuracy on downstream tasks, with some tasks improving and some degrading. The authors hypothesize this is due to the dataset used for fine-tuning and suggest optimizing the process further by using fewer than ~100B tokens, lighter fine-tuning such as LoRA, or tuning layer normalization parameters alone.
- Why unresolved: The paper does not provide a detailed analysis of the impact of teacher correction on different downstream tasks or explore the suggested optimization methods.
- What evidence would resolve it: Conducting experiments with different fine-tuning strategies, such as LoRA or layer normalization tuning, and evaluating their impact on downstream task performance would provide insights into optimizing the teacher correction phase.

### Open Question 2
- Question: How does the choice of pruning strategy (width vs. depth) affect the model's performance on different types of tasks?
- Basis in paper: The paper compares the performance of width-pruned and depth-pruned variants of the Llama 3.1 8B model, finding that width pruning consistently outperforms depth pruning in terms of accuracy on various benchmarks. However, the paper also notes that depth pruning provides a higher speedup compared to width pruning.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of pruning strategy affects the model's performance on specific types of tasks, such as reasoning, coding, or instruction following.
- What evidence would resolve it: Conducting experiments to evaluate the performance of width-pruned and depth-pruned models on different types of tasks would provide insights into the strengths and weaknesses of each pruning strategy for specific use cases.

### Open Question 3
- Question: How does the choice of downstream task-based saliency metric for depth pruning impact the model's performance?
- Basis in paper: The paper introduces a new downstream task-based saliency metric for depth pruning and compares it to the traditional validation loss/Perplexity metric. The results show that the downstream task-based metric leads to better performance on Winogrande.
- Why unresolved: The paper does not provide a comprehensive comparison of different saliency metrics for depth pruning or explore the impact of the chosen metric on other downstream tasks.
- What evidence would resolve it: Conducting experiments to compare the performance of different saliency metrics for depth pruning on various downstream tasks would provide insights into the effectiveness of the proposed metric and its generalizability to other tasks.

## Limitations

- Teacher correction effectiveness is only demonstrated on the Nemotron-4 CT dataset, with unclear generalizability to other datasets or domains
- Critical hyperparameters for teacher correction and pruning are not fully specified, making reproduction challenging
- Substantial computational resources required despite claimed efficiency improvements, limiting practical applicability

## Confidence

**High Confidence**: The core pruning and distillation methodology is well-established and empirical results showing state-of-the-art performance are compelling.

**Medium Confidence**: Teacher correction mechanism's effectiveness is demonstrated but could be dataset-specific. Runtime efficiency claims are supported but depend on hardware configurations.

**Low Confidence**: Generalizability to different model architectures, datasets, or downstream tasks is not extensively validated. Optimal hyperparameters are not fully characterized.

## Next Checks

1. Apply the same teacher correction + pruning + distillation pipeline to a completely different dataset (e.g., OpenWebText, C4) and compare performance to validate teacher correction robustness.

2. Systematically disable teacher correction and test whether the 6% LM validation loss improvement and downstream task performance gains disappear to quantify the exact contribution of this phase.

3. Reproduce the claimed 2.7× speedup on different GPU architectures and batch sizes to confirm runtime improvements are not hardware-specific artifacts.