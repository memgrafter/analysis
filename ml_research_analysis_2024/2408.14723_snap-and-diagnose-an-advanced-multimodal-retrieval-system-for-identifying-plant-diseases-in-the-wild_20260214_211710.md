---
ver: rpa2
title: 'Snap and Diagnose: An Advanced Multimodal Retrieval System for Identifying
  Plant Diseases in the Wild'
arxiv_id: '2408.14723'
source_url: https://arxiv.org/abs/2408.14723
tags:
- plant
- disease
- retrieval
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for an effective tool to identify
  plant diseases using images or text descriptions captured in real-world conditions.
  It introduces a multimodal retrieval system that leverages the PlantWild dataset,
  which contains over 18,000 in-the-wild plant disease images across 89 categories.
---

# Snap and Diagnose: An Advanced Multimodal Retrieval System for Identifying Plant Diseases in the Wild

## Quick Facts
- arXiv ID: 2408.14723
- Source URL: https://arxiv.org/abs/2408.14723
- Authors: Tianqi Wei; Zhi Chen; Xin Yu
- Reference count: 13
- Primary result: CLIP-based multimodal retrieval achieves Top-1 accuracy of 67.32% on in-the-wild plant disease identification

## Executive Summary
This paper introduces Snap'n Diagnose, a multimodal retrieval system for identifying plant diseases from in-the-wild images or text descriptions. Built on the PlantWild dataset of 18,000+ images across 89 disease categories, the system uses CLIP to encode images and text into a shared latent space, then employs MVPDR fine-tuned MLP features with cosine similarity for retrieval. The approach enables cross-modal search between visual symptoms and textual descriptions, providing a user-friendly web interface for agricultural practitioners. Experimental results demonstrate significant performance improvements over zero-shot CLIP baselines.

## Method Summary
The system leverages the PlantWild dataset containing over 18,000 in-the-wild plant disease images across 89 categories, each with textual descriptions. Visual features are extracted using CLIP's image encoder combined with MVPDR's pre-trained MLP for refinement, while text features use CLIP's text encoder. These features are stored in memory and retrieved during inference through cosine similarity computation. A Flask-based API serves as the backend, connecting a web frontend where users can upload images or enter textual symptoms to receive ranked disease identification results.

## Key Results
- Top-1 accuracy: 67.32%
- Top-5 accuracy: 80.65%
- Top-10 accuracy: 88.11%
- mAP: 79.34%
- Significant improvement over zero-shot CLIP baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal retrieval in a shared latent space enables text-to-image and image-to-image disease lookup.
- Mechanism: CLIP encodes images and text into a common embedding space; cosine similarity in this space provides relevance scores across modalities.
- Core assumption: Visual and textual features from CLIP are semantically aligned so that similar plant disease descriptions map close to their visual manifestations.
- Evidence anchors:
  - [abstract] "cross-modal retrieval is achieved... facilitated by a novel CLIP-based vision-language model that encodes both disease descriptions and disease images into the same latent space."
  - [section] "During the inference process, a given input image or textual query will first be extracted into a feature vector. Afterward, the cosine similarity between the vector and all the stored features can be acquired."
- Break condition: If CLIP embeddings are not well-aligned for plant disease semantics, retrieval accuracy will drop sharply.

### Mechanism 2
- Claim: MVPDR fine-tuned MLP enhances visual feature quality for in-the-wild disease images.
- Mechanism: Pre-trained CLIP image encoder extracts initial features, then MVPDR's MLP refines them for better discriminative power on the PlantWild dataset.
- Core assumption: Fine-tuning on PlantWild improves the feature representation beyond generic CLIP features, especially under varied lighting, backgrounds, and disease stages.
- Evidence anchors:
  - [abstract] "Built on top of the retriever, our retrieval system allows users to upload either plant disease images or disease descriptions to retrieve the corresponding images with similar characteristics from the disease dataset."
  - [section] "We leverage PlantWild... Specifically, we extract visual features using CLIP's image encoder and MVPDR's pre-trained MLP, utilizing these features for inference."
- Break condition: If the MLP is overfit to PlantWild training data, it may underperform on truly unseen in-the-wild images.

### Mechanism 3
- Claim: Flask-based API backend provides low-latency inference and fast user response.
- Mechanism: Storing pre-extracted features in memory and computing cosine similarity at query time reduces CPU usage and latency compared to on-the-fly encoding.
- Core assumption: The cost of feature extraction dominates latency; caching features amortizes this cost across queries.
- Evidence anchors:
  - [section] "Therefore, our system has a fast response time, providing better user experiences."
  - [section] "This operation not only reduces CPU usage but also removes the time for models to process images."
- Break condition: If feature storage becomes too large or feature updates are needed frequently, latency or accuracy could degrade.

## Foundational Learning

- Concept: Multimodal embedding spaces (vision-language models like CLIP)
  - Why needed here: Enables matching disease images with textual symptom descriptions in a single similarity search.
  - Quick check question: What property of CLIP embeddings makes cross-modal retrieval possible?

- Concept: Metric learning and similarity search (cosine similarity, FAISS, etc.)
  - Why needed here: To rank database images by relevance to a query embedding efficiently.
  - Quick check question: How does cosine similarity behave when two vectors are orthogonal in CLIP space?

- Concept: Fine-tuning on domain-specific datasets (transfer learning)
  - Why needed here: Generic CLIP features may not capture fine-grained disease appearance differences; PlantWild fine-tuning improves recall.
  - Quick check question: What happens to retrieval performance if you skip the fine-tuning step?

## Architecture Onboarding

- Component map: Frontend (HTML/CSS/JS) -> Backend (Flask API) -> Feature Store (in-memory) -> Model Pipeline (CLIP encoders + MVPDR MLP)
- Critical path:
  1. User submits image or text via frontend
  2. Backend extracts or retrieves embedding
  3. Cosine similarity computed against all stored features
  4. Top-K results ranked and returned to frontend
- Design tradeoffs:
  - Storing pre-extracted features speeds up inference but increases memory usage
  - Using CLIP's zero-shot model is simpler but less accurate than fine-tuned MVPDR
  - In-memory storage is faster but less scalable than disk-based or vector DB
- Failure signatures:
  - Low recall: Features not well-aligned; check embedding quality
  - High latency: Feature extraction not cached; check memory usage
  - Wrong class predictions: Fine-tuning may be overfit; retrain with more data
- First 3 experiments:
  1. Replace MVPDR MLP with identity mapping; measure drop in Top-1 accuracy
  2. Switch from cosine to Euclidean distance; compare retrieval rankings
  3. Vary the number of stored features (subset of PlantWild); plot latency vs. recall curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Snap'n Diagnose system perform on plant diseases not included in the PlantWild dataset?
- Basis in paper: [inferred] The system is evaluated only on the PlantWild dataset, which includes 89 disease categories. The paper does not discuss the system's performance on diseases outside this dataset.
- Why unresolved: The evaluation is limited to the PlantWild dataset, and there is no mention of testing the system's generalization to unseen diseases.
- What evidence would resolve it: Testing the system on a separate dataset containing plant diseases not present in PlantWild and comparing its performance metrics (e.g., Top-1 accuracy, Top-5 accuracy, mAP) to those reported in the paper.

### Open Question 2
- Question: What is the impact of image quality and environmental conditions on the system's retrieval accuracy?
- Basis in paper: [inferred] The paper mentions that the system uses in-the-wild images from the PlantWild dataset, but it does not explicitly analyze how variations in image quality or environmental conditions affect retrieval performance.
- Why unresolved: The paper does not provide a detailed analysis of the system's robustness to different image qualities or environmental factors.
- What evidence would resolve it: Conducting experiments with images of varying quality (e.g., resolution, lighting, occlusion) and environmental conditions (e.g., different backgrounds, weather) to assess the system's retrieval accuracy under these variations.

### Open Question 3
- Question: How does the system handle ambiguous or overlapping symptoms between different plant diseases?
- Basis in paper: [inferred] The paper discusses the system's ability to retrieve images based on textual descriptions of symptoms but does not address how it manages cases where symptoms are similar across different diseases.
- Why unresolved: The paper does not provide information on the system's performance in scenarios with ambiguous or overlapping symptoms.
- What evidence would resolve it: Evaluating the system's performance on a dataset with diseases that have similar symptoms and analyzing its ability to distinguish between them based on the retrieved images and their similarity scores.

## Limitations

- Performance heavily dependent on PlantWild dataset quality and coverage
- MVPDR fine-tuning procedure not fully specified, hindering exact reproduction
- Requires significant computational resources for feature storage and similarity computation
- Effectiveness may degrade on novel diseases or environmental conditions not well-represented in training data

## Confidence

- High confidence: The core retrieval mechanism using CLIP embeddings and cosine similarity is well-established and experimentally validated
- Medium confidence: The reported performance metrics are based on the PlantWild dataset, but may not generalize to all agricultural settings
- Medium confidence: The Flask-based deployment approach is reasonable but not extensively validated for production use cases

## Next Checks

1. **Generalization Test:** Evaluate the system on an independent in-the-wild plant disease dataset not used in training to assess real-world performance
2. **Feature Fine-tuning Ablation:** Compare retrieval accuracy with and without MVPDR fine-tuning to quantify the contribution of domain-specific feature refinement
3. **Scalability Assessment:** Measure system latency and memory usage with increasing numbers of stored features to determine practical deployment limits