---
ver: rpa2
title: 'DOCCI: Descriptions of Connected and Contrasting Images'
arxiv_id: '2404.19753'
source_url: https://arxiv.org/abs/2404.19753
tags:
- images
- descriptions
- docci
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DOCCI is a new vision-language dataset with 15k images, each annotated
  with detailed descriptions averaging 136 words. The dataset focuses on fine-grained
  visual details like spatial relations, counting, text rendering, and world knowledge.
---

# DOCCI: Descriptions of Connected and Contrasting Images

## Quick Facts
- arXiv ID: 2404.19753
- Source URL: https://arxiv.org/abs/2404.19753
- Authors: Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, Jason Baldridge
- Reference count: 40
- Key outcome: DOCCI is a new vision-language dataset with 15k images, each annotated with detailed descriptions averaging 136 words.

## Executive Summary
DOCCI is a novel vision-language dataset containing 15,000 images, each annotated with detailed descriptions averaging 136 words. The dataset focuses on fine-grained visual details like spatial relations, counting, text rendering, and world knowledge. Human annotators create descriptions through a three-stage process to ensure quality and comprehensiveness. DOCCI enables rigorous evaluation of text-to-image and image-to-text models, revealing current limitations in handling detailed descriptions.

## Method Summary
The DOCCI dataset was created through a three-stage annotation pipeline: (1) annotators identify key visual features from reference images, (2) they write draft descriptions focusing on identified features, and (3) they elaborate on details and refine the descriptions. Images were intentionally collected in groups with similar contexts but different spatial relationships, counts, and orientations. The dataset includes training, test, and qualification splits, with contrastive image grouping and entity tagging for specific objects.

## Key Results
- PaLI 5B finetuned on DOCCI achieves equal or superior performance compared to larger models like LLaVA-1.5 7B and InstructBLIP 7B
- Current text-to-image models struggle with long descriptions, often truncating input or generating errors in spatial relationships, counting, and text rendering
- DOCCI serves as both an effective training resource for image-to-text generation and a challenging testbed for evaluating model limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DOCCI's multi-stage annotation pipeline reduces hallucinations in image descriptions.
- **Mechanism:** By separating key information extraction (Stage 1), draft writing (Stage 2), and detail elaboration (Stage 3), annotators can focus on accuracy at each step without being overwhelmed by the full task complexity.
- **Core assumption:** Breaking the annotation process into smaller, specialized steps improves quality over asking annotators to produce detailed descriptions in one pass.
- **Evidence anchors:** The paper hypothesizes that good descriptions include sufficient details and should be well-organized, requiring a structured annotation approach.

### Mechanism 2
- **Claim:** Contrastive image grouping improves model evaluation by testing fine-grained visual understanding.
- **Mechanism:** By including groups of images with similar subjects but different spatial relationships, counts, and orientations, DOCCI forces models to distinguish subtle visual differences rather than relying on broad semantic categories.
- **Core assumption:** Models trained on standard datasets cannot handle precise spatial and counting details, and DOCCI's contrastive design exposes these limitations.
- **Evidence anchors:** The paper states that DOCCI images were intentionally collected to have substantially similar, contextually related groups of distractor images, enabling precise and localized investigation of model behaviors.

### Mechanism 3
- **Claim:** DOCCI's detailed descriptions serve as effective training data despite small size due to high information density.
- **Mechanism:** The 136-word average descriptions contain rich visual information that provides strong supervision signals, allowing smaller models like PaLI 5B to achieve competitive performance when fine-tuned on DOCCI.
- **Core assumption:** Dense, detailed descriptions provide more effective learning signals than sparse, generic captions, even when training data quantity is limited.
- **Evidence anchors:** The paper demonstrates that a PaLI 5B model finetuned on DOCCI shows equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and InstructBLIP 7B.

## Foundational Learning

- **Concept:** Vision-language model architecture fundamentals
  - Why needed here: Understanding how models like PaLI 5B, LLaVA-1.5, and InstructBLIP process image-text pairs is crucial for interpreting DOCCI's evaluation results.
  - Quick check question: What are the key architectural differences between encoder-decoder models (like PaLI) and decoder-only models (like LLaVA) in vision-language tasks?

- **Concept:** Image captioning evaluation metrics and their limitations
  - Why needed here: DOCCI highlights the inadequacy of standard metrics like BLEU, ROUGE, and CIDEr for evaluating detailed descriptions, requiring understanding of metric limitations.
  - Quick check question: Why do traditional image captioning metrics fail to capture the quality of long, detailed descriptions like those in DOCCI?

- **Concept:** Text-to-image generation model constraints and limitations
  - Why needed here: DOCCI reveals specific failure modes in T2I models (spatial relationships, counting, text rendering), requiring understanding of architectural constraints.
  - Quick check question: How do input length limitations in models like Imagen and SDXL affect their ability to process detailed descriptions?

## Architecture Onboarding

- **Component map:** Image collection → quality control (PII removal) → multi-stage annotation → quality control (precision/recall checking) → dataset splitting → model evaluation using both automatic metrics and human evaluation
- **Critical path:** Image collection → quality control (PII removal) → multi-stage annotation → quality control (precision/recall checking) → dataset splitting → model evaluation using both automatic metrics and human evaluation
- **Design tradeoffs:** High annotation quality vs. scalability (multi-stage process is time-consuming), detailed descriptions vs. model input constraints (models struggle with long prompts), contrastive images vs. potential overfitting to specific patterns
- **Failure signatures:** Precision errors manifest as hallucinations or incorrect information in descriptions; recall errors appear as missing key details; model failures include spatial relationship errors, counting mistakes, and text rendering issues in generated images
- **First 3 experiments:**
  1. Test PaLI 5B fine-tuning on DOCCI vs. COCO to quantify the impact of description density on model performance
  2. Evaluate model performance on contrastive image pairs to measure fine-grained visual understanding
  3. Compare automatic metrics (FID, CLIPScore) vs. human evaluation on DOCCI to identify metric limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reliability of automatic evaluation metrics for detailed image descriptions be improved?
- Basis in paper: [explicit] The paper highlights the unreliability of automatic metrics like FID and CLIPScore for evaluating detailed image descriptions.
- Why unresolved: Current metrics do not align well with human evaluation, especially for long descriptions with fine details.
- What evidence would resolve it: Development and validation of new metrics that better capture the nuances of detailed image descriptions, potentially using more advanced language models or incorporating human feedback.

### Open Question 2
- Question: What architectural changes are needed in text-to-image models to better handle long and detailed descriptions?
- Basis in paper: [explicit] The paper discusses the limitations of current T2I models due to their limited input length, causing truncation of detailed descriptions.
- Why unresolved: Existing models struggle to incorporate all information from lengthy descriptions, leading to errors in spatial relationships, counting, and text rendering.
- What evidence would resolve it: Experiments comparing T2I models with extended input lengths or novel architectures that can process and generate images from detailed descriptions more effectively.

### Open Question 3
- Question: How can the diversity of the DOCCI dataset be expanded to reduce geographical and cultural bias?
- Basis in paper: [inferred] The paper acknowledges that the images were primarily taken in the United States, introducing potential bias.
- Why unresolved: The dataset's current geographical focus limits its applicability to global contexts and diverse cultural settings.
- What evidence would resolve it: Collection and annotation of images from a wider range of geographical locations and cultural contexts, ensuring a more representative and unbiased dataset.

## Limitations

- The multi-stage annotation pipeline, while designed to improve quality, may introduce inconsistencies between stages that are difficult to detect and control
- DOCCI's contrastive image design could lead to overfitting if models learn to exploit specific patterns rather than developing genuine fine-grained understanding
- The dataset's focus on detailed descriptions creates a mismatch with current text-to-image model constraints, particularly input length limitations

## Confidence

- **High confidence:** The comparative performance of PaLI 5B against larger models (LLaVA-1.5 7B, InstructBLIP 7B) demonstrates DOCCI's effectiveness as training data
- **Medium confidence:** The identification of specific failure modes (spatial relationships, counting, text rendering) in text-to-image models, though these may be influenced by model-specific limitations rather than universal architectural constraints
- **Medium confidence:** The claim that breaking annotation into three stages improves quality, as this design choice lacks direct empirical validation against alternative approaches

## Next Checks

1. Conduct ablation studies comparing single-stage vs. multi-stage annotation to quantify the quality improvement claimed by the three-stage pipeline
2. Test model generalization by evaluating DOCCI-trained models on other fine-grained visual reasoning datasets to assess whether contrastive training improves broader visual understanding
3. Perform controlled experiments varying description length and detail density to determine the optimal balance between information richness and model input constraints