---
ver: rpa2
title: Why Warmup the Learning Rate? Underlying Mechanisms and Improvements
arxiv_id: '2406.09405'
source_url: https://arxiv.org/abs/2406.09405
tags:
- training
- learning
- warmup
- loss
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why warmup of the learning rate is effective
  in deep learning. Through systematic experiments with SGD and Adam optimizers across
  various architectures and initializations, the authors demonstrate that warmup's
  primary benefit is allowing the network to tolerate larger target learning rates
  by guiding it to more well-conditioned areas of the loss landscape.
---

# Why Warmup the Learning Rate? Underlying Mechanisms and Improvements

## Quick Facts
- **arXiv ID**: 2406.09405
- **Source URL**: https://arxiv.org/abs/2406.09405
- **Reference count**: 40
- **Key outcome**: The paper demonstrates that warmup's primary benefit is guiding networks to well-conditioned loss landscape areas, allowing larger target learning rates, and proposes improved hyperparameter initialization schemes.

## Executive Summary
This paper investigates the fundamental mechanisms behind learning rate warmup in deep learning. Through systematic experiments across multiple architectures and optimizers, the authors reveal that warmup's effectiveness stems from its ability to guide networks toward flatter regions of the loss landscape that can tolerate higher learning rates. The study distinguishes between two regimes: progressive sharpening (common with small initializations) and sharpness reduction (common with large initializations), showing how warmup interacts differently with each. Based on these insights, the authors propose practical improvements including a novel GI-Adam optimizer that initializes the second moment using gradients, and a method to estimate optimal learning rate scales using the loss catapult mechanism.

## Method Summary
The paper uses systematic experiments with SGD and Adam optimizers across fully connected networks, WideResNets, and Transformers on CIFAR-10, CIFAR-100, TinyImageNet, and WikiText-2 datasets. Key methods include measuring sharpness (top eigenvalue of Hessian) and pre-conditioned sharpness using LOBPCG solvers, testing various warmup durations (Twrm = 1, 64, 256, 1024), and comparing training dynamics with and without warmup. The authors also implement GI-Adam with second-moment initialization and an exponential+binary search method for estimating critical learning rates. Experiments use JAX and Flax libraries with standard data augmentation techniques.

## Key Results
- Warmup effectiveness depends on whether training starts in sharpness reduction or progressive sharpening phase, determined by initialization and parameterization
- For SGD, warmup guides networks to flatter regions by temporarily increasing loss when learning rate exceeds instability threshold, reducing sharpness
- For Adam, warmup primarily mitigates instabilities caused by large pre-conditioned sharpness values at initialization, preventing training failures
- GI-Adam optimizer with gradient-squared initialization provides benefits similar to warmup and prevents training failures
- The loss catapult mechanism can be used to estimate initial sharpness scale for better learning rate initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Warmup allows the network to tolerate larger target learning rates by guiding it to more well-conditioned areas of the loss landscape.
- **Mechanism**: During warmup, the learning rate increases from zero to a target value. As the learning rate increases, it may exceed the instability threshold, triggering a temporary increase in loss and a decrease in sharpness (top eigenvalue of the Hessian). This sharpness reduction restores stability and allows the model to adapt to the increased learning rate, ultimately moving it to flatter regions that can accommodate higher learning rates.
- **Core assumption**: The instability threshold for training is determined by the sharpness (or pre-conditioned sharpness for adaptive optimizers like Adam). Exceeding this threshold causes a temporary loss increase and sharpness reduction.
- **Evidence anchors**:
  - [abstract]: "the overwhelming benefit of warmup arises from allowing the network to tolerate larger ηtrgt by forcing the network to more well-conditioned areas of the loss landscape"
  - [section]: "the primary goal of warmup is to gradually reduce sharpness, guiding training towards flatter regions that can accommodate training at higher learning rates"
  - [corpus]: Weak - corpus neighbors discuss warmup but don't provide direct evidence for this specific mechanism
- **Break condition**: If the warmup rate is too high, training may diverge before the sharpness can be sufficiently reduced to restore stability.

### Mechanism 2
- **Claim**: The effectiveness of warmup depends on whether training starts in a sharpness reduction or progressive sharpening phase, determined by initialization and parameterization.
- **Mechanism**: Training has a 'natural' preference for sharpness evolution. If training naturally undergoes sharpness reduction (e.g., with large initializations), warmup cooperates with this natural decrease. If training naturally undergoes progressive sharpening (e.g., with small initializations like µP), warmup opposes this increase, creating a "head-on collision" that triggers loss increases and sharpness reductions to restore stability.
- **Core assumption**: Different network initializations and parameterizations lead to different natural sharpness evolution patterns during early training.
- **Evidence anchors**:
  - [section]: "we find that there are several qualitatively distinct regimes and mechanisms at play. These depend on whether the network starts off in a progressive sharpening or sharpness reduction phase"
  - [section]: "Small initializations, such as those using maximal update parameterization (µP) or appropriately using normalizing layers... start in flat regions where gradients point toward increasing sharpness... Large initializations... undergo an early sharpness reduction"
  - [corpus]: Weak - corpus neighbors discuss initialization effects but don't provide direct evidence for this specific mechanism
- **Break condition**: If the natural sharpness evolution and warmup effects perfectly cancel each other out, warmup may provide minimal benefit.

### Mechanism 3
- **Claim**: For Adam, warmup primarily mitigates instabilities caused by large pre-conditioned sharpness values at initialization, preventing training failures.
- **Mechanism**: Adam's stability is determined by the largest eigenvalue of the pre-conditioned Hessian (pre-conditioned sharpness), not the sharpness itself. This pre-conditioned sharpness starts at high values even for flat initializations, leading to training failures at large learning rates. Warmup gradually reduces this pre-conditioned sharpness, preventing large instabilities and training failures.
- **Core assumption**: Adam's pre-conditioned sharpness at initialization is typically much larger than its corresponding instability threshold, causing instabilities even when the actual sharpness is small.
- **Evidence anchors**:
  - [section]: "we observe that the pre-conditioned sharpness for Adam starts at a high value, even for low sharpness initializations like µP, and can lead to training failures at large learning rates"
  - [section]: "We find that the key issue is not observing too few training samples, but rather that the pre-conditioned sharpness typically starts off at high values... causing considerable instabilities at high learning rates"
  - [corpus]: Weak - corpus neighbors discuss Adam warmup but don't provide direct evidence for this specific mechanism
- **Break condition**: If Adam's pre-conditioned sharpness is initialized at low values (e.g., with GI-Adam), warmup may provide minimal additional benefit.

## Foundational Learning

- **Concept**: Loss landscape and sharpness (eigenvalues of the Hessian)
  - Why needed here: Understanding how sharpness determines training stability and how warmup affects sharpness is central to explaining warmup's benefits
  - Quick check question: What is the relationship between the top eigenvalue of the Hessian (sharpness) and the maximum stable learning rate?

- **Concept**: Learning rate schedules and warmup strategies
  - Why needed here: The paper investigates different warmup approaches (linear, persistent catapult) and their effects on training dynamics
  - Quick check question: How does linear warmup differ from constant learning rate training in terms of learning rate trajectory?

- **Concept**: Adaptive optimization methods (Adam) and pre-conditioned sharpness
  - Why needed here: The paper extends analysis to Adam and shows that its stability is determined by pre-conditioned sharpness rather than sharpness
  - Quick check question: How does Adam's pre-conditioned Hessian differ from the regular Hessian, and why does this matter for stability?

## Architecture Onboarding

- **Component map**: 
  Model -> Optimizer (SGD/Adam) -> Learning Rate Scheduler (with warmup) -> Sharpness/Pre-conditioned Sharpness Measurement (LOBPCG) -> Data Pipeline (with augmentation) -> Performance Metrics (accuracy, loss, sharpness)

- **Critical path**: 
  1. Initialize model with chosen parameterization
  2. Set up optimizer with learning rate scheduler including warmup
  3. Train for specified number of steps, measuring loss, sharpness, and pre-conditioned sharpness
  4. Analyze training dynamics to understand warmup effects

- **Design tradeoffs**:
  - Warmup duration vs. target learning rate: Longer warmup allows higher target rates but increases training time
  - Initialization method: Affects natural sharpness evolution and warmup effectiveness
  - Optimizer choice: SGD vs. Adam have different stability criteria (sharpness vs. pre-conditioned sharpness)

- **Failure signatures**:
  - Training divergence (loss increases to infinity) - SGD case
  - Training failure (loss plateaus at high value) - Adam case
  - Vanishing gradients (certain layers output zero) - Adam training failure
  - Sharpness spikes on first training step with longer warmup - normalization layer interaction

- **First 3 experiments**:
  1. Train a 4-layer FCN on CIFAR-10 with MSE loss using GD with different warmup durations (Twrm = 1, 64, 256, 1024) and observe sharpness trajectories
  2. Train the same model with Adam and compare pre-conditioned sharpness trajectories to GD case
  3. Implement GI-Adam (initialize second moment with gradient squared) and compare training stability and failure boundary to standard Adam

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the location of the failure boundary in Adam vary with dataset size and architecture complexity?
- Basis in paper: [inferred] The paper mentions that further investigations are needed to understand the generalizability of findings to larger-scale settings and that the failure boundary's robustness to dataset or architecture changes is an interesting open question.
- Why unresolved: The paper only tested on relatively small-scale datasets (CIFAR-10/100, TinyImageNet, WikiText-2) and models (WRNs, Transformers). The authors explicitly state this as a limitation.
- What evidence would resolve it: Experiments testing the failure boundary location on larger datasets (e.g., ImageNet, larger language models) and more complex architectures (e.g., deeper ResNets, larger Transformers) would provide the needed evidence.

### Open Question 2
- Question: What is the exact mechanism behind Adam's training failures where certain layers output zero?
- Basis in paper: [explicit] The paper states "we often observed that certain layers or residual blocks output zero, leading to vanishing gradients" during Adam training failures, but acknowledges this requires further study.
- Why unresolved: The paper only observes this phenomenon and leaves detailed analysis for future work, without explaining the underlying mechanism.
- What evidence would resolve it: Detailed analysis of the weight updates and gradient flow during Adam training failures, possibly using techniques like gradient decomposition or layer-wise analysis, would help identify the root cause.

### Open Question 3
- Question: How do different choices of Adam's hyperparameters (β1, β2, ε) affect the warmup mechanisms and the location of the failure boundary?
- Basis in paper: [inferred] The paper mentions these hyperparameters as a limitation, stating "For Adam, we did not explore the dependence on hyperparameters β1, β2, ϵ."
- Why unresolved: The paper used default values (β1 = 0.9, β2 = 0.999, ε = 10^-8) for all experiments without exploring how variations in these parameters might affect the warmup dynamics and failure boundary.
- What evidence would resolve it: Systematic experiments varying β1, β2, and ε values while measuring the pre-conditioned sharpness evolution, training failures, and optimal learning rates would reveal their impact on warmup mechanisms.

## Limitations
- The sharpness-based stability analysis relies on computationally expensive second-order information that becomes prohibitive for very large models
- Theoretical guarantees for the loss catapult mechanism and sharpness reduction are not fully established
- Experiments are limited to relatively small-scale datasets and models, with unclear generalizability to larger settings
- Default Adam hyperparameters were used without exploring their impact on warmup mechanisms and failure boundaries

## Confidence
- **High confidence**: The observation that warmup effectiveness depends on initialization and parameterization (Mechanism 2) - well-supported by experiments across multiple architectures
- **Medium confidence**: The sharpness reduction mechanism for SGD warmup (Mechanism 1) - empirical evidence is strong but theoretical analysis has gaps
- **Medium confidence**: The pre-conditioned sharpness explanation for Adam warmup (Mechanism 3) - novel insight with good empirical support but limited theoretical grounding

## Next Checks
1. **Cross-optimizer validation**: Test whether the proposed GI-Adam initialization strategy provides similar benefits for other adaptive optimizers (RMSprop, Adagrad) to verify this is not optimizer-specific
2. **Scale-up validation**: Apply the exponential+binary search method for estimating ηc to larger Transformer models and verify the computational savings claimed in Section 6.1
3. **Alternative sharpness measures**: Validate whether alternative sharpness measures (trace of Hessian, Fisher information matrix eigenvalues) show similar patterns during warmup, strengthening the causal interpretation