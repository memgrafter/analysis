---
ver: rpa2
title: 'CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos'
arxiv_id: '2411.17820'
source_url: https://arxiv.org/abs/2411.17820
tags:
- navigation
- data
- urban
- videos
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CityWalker learns urban navigation policies from web-scale city
  walking and driving videos. It uses visual odometry to extract action labels at
  scale without manual annotation.
---

# CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos

## Quick Facts
- **arXiv ID:** 2411.17820
- **Source URL:** https://arxiv.org/abs/2411.17820
- **Reference count:** 40
- **One-line primary result:** CityWalker achieves 77.3% success rate in real-world urban navigation using 2000+ hours of web-scale video data

## Executive Summary
CityWalker presents a novel approach to embodied urban navigation by learning from massive amounts of web-scale city walking and driving videos. The method uses visual odometry to extract action labels at scale without manual annotation, enabling training on over 2000 hours of video data. A transformer-based model processes sequences of past observations, trajectory history, and target location to predict future actions. The system demonstrates significant performance improvements over prior methods, achieving 77.3% success rate in real-world urban environments while effectively handling critical scenarios like crossings, turns, and crowds.

## Method Summary
CityWalker learns navigation policies by training on web-scale city videos using visual odometry to extract action supervision. The approach uses a transformer model that processes past observations, trajectory history, and target location to predict future actions. Training includes a feature hallucination loss that encourages the model to anticipate future visual inputs. After large-scale pre-training on 2000+ hours of video data, the model is fine-tuned on 6 hours of expert teleoperation data. The method demonstrates zero-shot transfer capability to unseen urban environments while maintaining robust performance across various navigation scenarios.

## Key Results
- Achieves 77.3% success rate in real-world urban navigation on unseen environments
- Outperforms baselines like ViNT and NoMaD across multiple evaluation metrics
- Demonstrates consistent performance improvement with increased training data size

## Why This Works (Mechanism)

### Mechanism 1
Large-scale web-scale video data significantly improves navigation performance compared to limited expert data. The model learns diverse navigation patterns from thousands of hours of real-world city walking and driving videos, capturing the complexity and variability of urban environments. Core assumption: visual odometry provides sufficient and generalizable action labels for imitation learning.

### Mechanism 2
Feature hallucination loss helps the transformer model predict more informative future tokens that guide better action predictions. The loss encourages the transformer to generate output tokens that mimic future observations, essentially anticipating future visual inputs. Core assumption: predicting future visual inputs is beneficial for planning future actions in navigation tasks.

### Mechanism 3
Cross-domain and cross-embodiment training (combining walking and driving videos) enhances model generalizability. Training on diverse data sources exposes the model to different motion patterns, perspectives, and navigation scenarios, leading to a more robust and adaptable navigation policy. Core assumption: navigation principles learned from walking and driving data are transferable and complementary.

## Foundational Learning

- **Imitation learning from demonstration data**: The paper uses imitation learning to train navigation policies from web-scale video data. *Quick check: What is the difference between imitation learning and reinforcement learning?*

- **Visual odometry for action label extraction**: The paper uses visual odometry to extract action labels from in-the-wild videos for imitation learning. *Quick check: How does visual odometry estimate the relative motion between camera frames?*

- **Transformer-based sequence modeling**: The paper uses a transformer model to process sequences of past observations, trajectory history, and target location to predict future actions. *Quick check: What is the advantage of using a transformer model for sequence modeling compared to recurrent neural networks?*

## Architecture Onboarding

- **Component map:** Past observations → Frozen DINOv2 image encoder → Coordinate encoder → Transformer → Action head/Arrival head → Output
- **Critical path:** Input → Image Encoder → Coordinate Encoder → Transformer → Action Head/Arrival Head → Output
- **Design tradeoffs:** Using frozen DINOv2 features reduces training time but may limit adaptation; predicting multiple future actions provides more context but increases complexity; visual odometry is scalable but may introduce label noise
- **Failure signatures:** High average orientation error indicates poor action prediction accuracy; low arrival success rate suggests difficulty determining when target is reached; sensitivity to GPS errors during deployment indicates overreliance on precise location
- **First 3 experiments:** 1) Evaluate model performance on offline data using AOE and MAOE metrics, 2) Test real-world navigation success rate in different scenarios (forward, left turn, right turn), 3) Analyze impact of increasing training dataset size on zero-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CityWalker scale with the size of the training dataset beyond 2000 hours? The paper shows performance improvement up to 2000 hours but does not explore beyond this limit. What evidence would resolve it: Training and testing CityWalker with 3000+ hours of video data and comparing the performance gains to the 2000-hour baseline.

### Open Question 2
How does CityWalker perform in extreme weather conditions like heavy snow or fog? The paper mentions training on diverse weather conditions but does not specifically address extreme weather performance. What evidence would resolve it: Testing CityWalker in simulated or real-world extreme weather conditions and comparing its performance to baseline methods.

### Open Question 3
Can CityWalker's action prediction be improved by incorporating temporal dependencies beyond the current 5-step horizon? The paper uses a 5-step prediction horizon but does not explore longer temporal dependencies. What evidence would resolve it: Training and testing CityWalker with longer prediction horizons (e.g., 10 or 15 steps) and analyzing the impact on navigation success rates and action accuracy.

## Limitations

- Visual odometry extracted from in-the-wild videos may contain noise and inaccuracies that propagate through training
- Domain gap between human walking videos and quadruped robot navigation could limit generalization, with feature hallucination loss underperforming in zero-shot scenarios
- Frozen DINOv2 features, while computationally efficient, may constrain the model's ability to adapt to specific navigation tasks in urban environments

## Confidence

- **High Confidence:** Training on 2000+ hours of web-scale video data improves navigation performance compared to limited expert data
- **Medium Confidence:** Feature hallucination loss effectiveness is supported by training curves but shows contradictory results in zero-shot inference
- **Medium Confidence:** Cross-domain and cross-embodiment training shows promise but potential limitations exist due to human-robot navigation gap

## Next Checks

1. **Domain Gap Analysis:** Conduct systematic experiments varying the proportion of walking vs driving videos in training to quantify the optimal mix and identify when the domain gap becomes detrimental to performance.

2. **Feature Ablation Study:** Test the model with trainable vs frozen image encoders to determine if computational efficiency gains from using DINOv2 outweigh potential performance improvements from fine-tuning visual features.

3. **Real-World Robustness Testing:** Deploy the model in multiple urban environments with varying GPS accuracy to measure sensitivity to localization errors and evaluate the need for additional sensor fusion approaches.