---
ver: rpa2
title: Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer
arxiv_id: '2405.12983'
source_url: https://arxiv.org/abs/2405.12983
tags:
- speech
- audio-visual
- lrs3
- arxiv
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multilingual audio-visual speech recognition
  (AVSR) model using a novel hybrid CTC/RNN-T Fast Conformer architecture. The model
  processes both audio and visual modalities, leveraging a ResNet-based front-end
  for visual preprocessing and an early fusion strategy.
---

# Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer

## Quick Facts
- arXiv ID: 2405.12983
- Source URL: https://arxiv.org/abs/2405.12983
- Reference count: 0
- Primary result: Achieves 0.8% WER on LRS3 and 11.9% absolute WER reduction on MuAViC benchmark

## Executive Summary
This paper presents a multilingual audio-visual speech recognition (AVSR) model using a novel hybrid CTC/RNN-T Fast Conformer architecture. The model processes both audio and visual modalities through early fusion, leveraging a ResNet-based front-end for visual preprocessing. By generating automatic transcriptions of large-scale multilingual datasets using Whisper, the authors significantly expand training data across six languages. The model achieves state-of-the-art performance on LRS3 (0.8% WER) and MuAViC benchmarks (11.9% absolute WER reduction), demonstrating robust performance under noisy conditions and supporting audio-only, visual-only, and audio-visual inference modes.

## Method Summary
The model uses a hybrid CTC/RNN-T Fast Conformer architecture with early fusion of audio and visual features. Visual preprocessing employs Conv3d and ResNet-18 to extract lip region features from 96x96 grayscale images. Audio features are generated from Mel-spectrograms and processed through downsampling layers. The model is trained with hybrid CTC/RNN-T loss (α=0.3), AdamW optimizer, and modality dropout (30%) to prevent audio-only bias. Training data is expanded from ~1000 to ~3900 hours by generating Whisper transcriptions for VoxCeleb2 and AVSpeech datasets across six languages. Evaluation includes WER on LRS3 and MuAViC benchmarks under clean and noisy conditions (babble and white noise at various SNR levels).

## Key Results
- Achieves 0.8% WER on LRS3 test set, establishing state-of-the-art performance
- Demonstrates 11.9% absolute average WER reduction on MuAViC benchmark compared to baseline
- Shows significant robustness to audio noise, with consistent performance improvements across babble and white noise at various SNR levels
- Supports unimodal (audio-only, visual-only) and multimodal inference with comparable performance across modalities

## Why This Works (Mechanism)

### Mechanism 1
Early fusion of audio and visual features improves robustness in noisy conditions. The model concatenates acoustic and visual features before joint processing, allowing cross-modal interactions from the beginning. This leverages visual information from lip movements as complementary cues to degraded audio signals. Break condition: Visual modality corruption or misalignment could propagate errors through the entire model.

### Mechanism 2
Modality dropout during training forces learning from both modalities and prevents audio-only bias. During training, one modality is randomly masked 30% of the time, requiring the network to extract useful information from both inputs. This addresses the common failure case where audio-visual models ignore visual modality since ASR is easier than lip reading. Break condition: Excessive dropout rate may prevent learning effective cross-modal relationships.

### Mechanism 3
Generated transcriptions from large multilingual datasets significantly increase training data for non-English languages. Whisper is used to automatically transcribe unlabelled VoxCeleb2 and AVSpeech datasets, expanding training hours from ~1000 to ~3900 hours across six languages. This provides substantial training data for languages with limited human-labeled speech. Break condition: Poor Whisper transcription quality for certain languages or speakers could teach incorrect patterns.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC loss allows sequence-to-sequence mapping without requiring frame-level alignment between input audio and output text
  - Quick check question: What does the "blank" symbol in CTC represent, and why is it important?

- Concept: Recurrent Neural Network Transducer (RNN-T)
  - Why needed here: RNN-T provides alignment-free sequence prediction framework that handles variable-length input and output sequences
  - Quick check question: How does RNN-T differ from CTC in handling output sequence prediction?

- Concept: Conformer architecture
  - Why needed here: Conformer combines local feature extraction from convolutions with global context modeling from self-attention, making it effective for speech recognition
  - Quick check question: What are the two main components of Conformer blocks, and what does each capture?

## Architecture Onboarding

- Component map: Visual input → ResNet-18 → Downsampling → Fusion → Audio-Visual Encoder → RNN-T Decoder → Text output
- Critical path: Visual input → ResNet-18 → Downsampling → Fusion → Audio-Visual Encoder → RNN-T Decoder → Text output
- Design tradeoffs:
  - Early fusion reduces model complexity but may limit deep unimodal representations
  - Modality dropout enables unimodal inference but adds training complexity
  - Hybrid CTC/RNN-T combines benefits but increases training time
- Failure signatures:
  - Poor audio-only performance: Visual modality dominating in early fusion
  - Poor visual-only performance: Insufficient modality dropout or InterCTC loss
  - Degraded performance on noise: Insufficient noise augmentation during training
- First 3 experiments:
  1. Train audio-only model on LRS3 to establish baseline performance
  2. Train visual-only model with InterCTC and modality dropout to verify unimodal capability
  3. Train hybrid model with varying modality dropout rates (0%, 15%, 30%) to find optimal balance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, three significant open questions emerge:

### Open Question 1
How does the model's performance scale with increasing amounts of generated transcriptions across different languages, particularly for languages with limited human-labeled data? The paper notes improvements for German with only 10 hours of human-labeled speech but doesn't analyze scaling behavior.

### Open Question 2
How does the model handle code-switching or mixed-language speech, and what modifications would be needed to improve performance in such scenarios? The paper focuses on six distinct languages but doesn't address scenarios where speakers switch between languages within the same utterance.

### Open Question 3
What is the impact of different noise types and levels on the model's robustness, and how does it compare to human performance in noisy environments? While the paper evaluates babble and white noise, it doesn't provide comprehensive analysis across diverse noise types or compare to human listeners.

## Limitations
- Reliance on Whisper-generated transcriptions introduces uncertainty about training data quality, particularly for non-English languages
- Early fusion approach creates rigid architecture that cannot dynamically adjust modality balance based on input conditions
- Evaluation focuses heavily on LRS3 and MuAViC benchmarks with limited testing across diverse real-world conditions

## Confidence
- Performance claims (WER reductions): Medium confidence - Results are well-documented on standard benchmarks but depend on quality of automatically generated training data
- Noise robustness claims: Medium confidence - Demonstrated on controlled noise conditions but not validated on real-world noisy environments
- Unimodal inference capability: High confidence - Modality dropout approach and InterCTC loss are well-established techniques
- Multilingual generalization: Low confidence - Limited evaluation across six languages with most results focused on English and Spanish

## Next Checks
1. **Quality validation of Whisper-generated transcriptions**: Measure WER between Whisper-generated and human transcriptions on held-out VoxCeleb2/AVSpeech data to quantify training data reliability

2. **Cross-speaker generalization test**: Evaluate model on speakers not present in any training data to assess whether performance gains generalize beyond LRS3 and MuAViC speakers

3. **Real-world noise validation**: Test model on naturally noisy recordings from diverse environments (cafés, streets, public transport) rather than synthetic babble/white noise to verify practical robustness