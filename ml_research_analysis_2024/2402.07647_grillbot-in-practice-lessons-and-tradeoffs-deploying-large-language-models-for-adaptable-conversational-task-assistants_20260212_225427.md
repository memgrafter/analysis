---
ver: rpa2
title: 'GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models
  for Adaptable Conversational Task Assistants'
arxiv_id: '2402.07647'
source_url: https://arxiv.org/abs/2402.07647
tags:
- task
- system
- user
- latency
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes GRILLBot, a winning system from the Alexa
  Prize TaskBot Challenge, which addresses the challenge of building real-world multimodal
  assistants for complex tasks. The core method is a hybrid architecture leveraging
  Large Language Models (LLMs) and specialized models tuned for specific subtasks.
---

# GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants

## Quick Facts
- **arXiv ID**: 2402.07647
- **Source URL**: https://arxiv.org/abs/2402.07647
- **Reference count**: 40
- **One-line primary result**: GRILLBot won the Alexa Prize TaskBot Challenge using a hybrid architecture that balances LLM capabilities with specialized models for low-latency subtasks

## Executive Summary
This paper presents GRILLBot, the winning system from the Alexa Prize TaskBot Challenge, which addresses the challenge of building real-world multimodal assistants for complex tasks like cooking and home improvement. The core innovation is a hybrid architecture that leverages Large Language Models (LLMs) for complex reasoning tasks while using specialized smaller models for latency-sensitive operations. This approach achieved 84% effectiveness with 100x lower latency for dialogue state management compared to LLMs, while still utilizing LLMs for abstractive question answering and task adaptation where their reasoning capabilities provide value.

## Method Summary
The system uses a hybrid architecture with an orchestrator that routes requests to appropriate components: LLM-based reasoning for abstractive QA and task adaptation, specialized neural models for dialogue state management (Neural Decision Parser), and task-specific retrieval for extractive QA. The NDP uses code generation to translate user utterances into system actions with 84% effectiveness and 100x lower latency than LLMs. The system is built on the Open Assistant Toolkit framework, deployed using Docker containers managed by Kubernetes, and runs on GPU resources for LLM components.

## Key Results
- Specialized NDP models achieve 84% effectiveness with 100x lower latency compared to LLMs for dialogue state management
- LLM reasoning abilities outperform specialized models for abstractive QA despite higher latency
- Task rewriter successfully adapts tasks in 56% of cases with 73% sensible replacements
- Hybrid architecture balances latency, reliability, and compute resources while maintaining effective user interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized smaller models outperform LLMs for dialogue state management due to significantly lower latency while maintaining high effectiveness.
- Mechanism: The Neural Decision Parser (NDP) uses code generation to translate user utterances into system actions. By training small sequence-to-sequence models on carefully curated examples, the system achieves 84% effectiveness with 100x lower latency compared to LLMs.
- Core assumption: The action space for system capabilities is fixed and limited, allowing specialized models to generalize well from a small dataset.
- Evidence anchors:
  - [abstract] "For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency."
  - [section] "Experiments show that for this critical and latency-sensitive component triggered on all interactions, a specialised model provides a 100x latency advantage."
- Break condition: If the action space expands significantly or becomes more complex, requiring broader generalization beyond the curated examples.

### Mechanism 2
- Claim: LLMs are more effective than specialized models for knowledge-grounded question answering when abstractive reasoning is required, despite higher latency.
- Mechanism: LLMs leverage their world knowledge and reasoning abilities to provide contextually relevant answers to abstractive questions that require combining information from task context and conversation history.
- Core assumption: The pretrained knowledge in LLMs outweighs the latency cost for abstractive questions that cannot be answered by simple extraction.
- Evidence anchors:
  - [abstract] "For knowledge-grounded question answering... we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns."
  - [section] "Human annotators agree that LLMs respond more correctly to abstractive QA."
- Break condition: If latency becomes the dominant constraint or if the cost of LLM calls becomes prohibitive at scale.

### Mechanism 3
- Claim: Hybrid architecture with structured LLM outputs enables safe task adaptation by constraining generation to predefined formats.
- Mechanism: The task rewriter uses LLMs to modify tasks based on user preferences, but outputs results in structured JSON format that the system can parse and validate before execution.
- Core assumption: Structured output constraints prevent dangerous hallucinations while still allowing the LLM to leverage its knowledge for sensible modifications.
- Evidence anchors:
  - [abstract] "The rewriter outputs the task in a structured format (JSON) so that the system framework can access the edits for the remaining conversation."
  - [section] "With manual annotation, we show that our task rewriter managed to adapt a task successfully in 56% of cases."
- Break condition: If the structured output constraints become too limiting for the LLM's ability to make meaningful adaptations.

## Foundational Learning

- Concept: Dialogue state management through code generation
  - Why needed here: Traditional intent classification models are brittle and cannot handle the flexible, user-led nature of task-oriented conversations where users can request changes at any point.
  - Quick check question: How does the NDP differ from traditional intent classification in handling unexpected user requests?

- Concept: Knowledge-grounded question answering with task context
  - Why needed here: TaskBots must answer questions that combine information from the current task, conversation history, and world knowledge, requiring more than simple information retrieval.
  - Quick check question: Why can't standard QA models handle the task-oriented question answering required by TaskBots?

- Concept: Structured data manipulation with LLMs
  - Why needed here: Task adaptation requires modifying underlying data structures (ingredients, steps) in ways that cannot be anticipated through pre-processing alone.
  - Quick check question: How does constraining LLM output to JSON format enable safe task adaptation while still leveraging LLM capabilities?

## Architecture Onboarding

- Component map: User utterance → NDP model → System action → LLM components (if needed) → Response generation. The NDP is called on every interaction, making its latency critical.
- Critical path: User utterance → NDP model → System action → LLM components (if needed) → Response generation. The NDP is called on every interaction, making its latency critical.
- Design tradeoffs: Balance between model effectiveness and latency, with specialized models used for high-frequency, low-latency needs and LLMs reserved for complex reasoning where their capabilities justify the cost.
- Failure signatures: High latency in NDP causes system-wide slowdowns; LLM hallucinations lead to unsafe responses; model chaining creates cascading failures; resource constraints cause container crashes.
- First 3 experiments:
  1. Test NDP model latency and accuracy on a sample of user utterances to verify the 100x improvement claim.
  2. Compare LLM vs specialized model performance on a sample of abstractive and extractive QA questions from the WoTe dataset.
  3. Validate task adaptation by testing the LLM rewriter on a set of ingredient replacement scenarios and measuring success rate and safety.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specialized smaller models for dialogue state management (NDP) compare in effectiveness and latency to zero-shot or few-shot prompting approaches with larger LLMs for the same task?
- Basis in paper: [explicit] The paper explicitly compares specialized supervised NDP models (84% effectiveness, 100x lower latency) to larger models requiring more data and discusses tradeoffs between model size and latency.
- Why unresolved: The paper only reports results for the specialized approach and doesn't provide direct comparisons with zero-shot/few-shot prompting using large LLMs for NDP.
- What evidence would resolve it: Direct head-to-head experiments comparing the same NDP task with both specialized models and large LLMs using zero-shot/few-shot prompting, measuring both effectiveness and latency.

### Open Question 2
- Question: Why does fine-tuning improve extractive QA performance for T5 models but worsen it for Llama 2 models, despite Llama 2 showing better abstractive QA performance?
- Basis in paper: [explicit] The paper shows that fine-tuning improves T5 extractive QA performance (F1 0.428→0.444) but worsens Llama 2 extractive QA (F1 0.127→0.332), while Llama 2 performs better on abstractive QA tasks.
- Why unresolved: The paper observes this phenomenon but doesn't investigate the underlying reasons for why fine-tuning has opposite effects on different model architectures.
- What evidence would resolve it: Detailed analysis of model behavior during fine-tuning, including attention patterns, loss curves, and comparison of training dynamics between T5 and Llama 2 architectures.

### Open Question 3
- Question: What specific factors cause user rejection of LLM-suggested task adaptations, and how can these be addressed to improve acceptance rates?
- Basis in paper: [explicit] The paper reports that users accepted only 34% of suggested replacements, with 18% asking for new replacements, 18% ignoring suggestions, 38% requesting other replacements, 12% exiting, and 14% due to system parsing errors.
- Why unresolved: The paper identifies rejection reasons but doesn't explore why users make these choices or how to modify the system to address these issues.
- What evidence would resolve it: User studies analyzing the specific reasons behind rejection choices, A/B testing different prompting strategies, and investigation of whether better context or alternative presentation methods improve acceptance rates.

## Limitations
- The 100x latency improvement for specialized models lacks detailed benchmarking methodology and comparison conditions
- The human evaluation of task adaptation (56% success rate) represents a relatively narrow success threshold that may not fully capture user satisfaction
- The system's performance on more diverse or complex tasks beyond cooking and home improvement remains unclear

## Confidence
- **High Confidence**: The hybrid architecture approach (LLMs for complex reasoning, specialized models for latency-sensitive tasks) is well-supported by the presented evidence and represents a sound engineering principle. The NDP's effectiveness for dialogue state management is convincingly demonstrated.
- **Medium Confidence**: The comparative performance of LLMs versus specialized models for QA tasks is supported by human annotations but would benefit from more extensive testing across diverse question types. The task adaptation success rate is measured but the evaluation criteria could be more rigorous.
- **Low Confidence**: The generalizability of these findings to other domains and task types is not established. The long-term reliability and maintenance requirements of such a complex hybrid system are not addressed.

## Next Checks
1. **Comprehensive Latency Benchmarking**: Conduct controlled experiments comparing the full system latency across different request patterns and load conditions, including detailed breakdown of each component's contribution to total response time.
2. **Cross-Domain Generalization Testing**: Evaluate the system's performance on a broader range of task types beyond cooking and home improvement to assess the generalizability of the hybrid approach and identify potential limitations.
3. **Safety and Robustness Evaluation**: Implement adversarial testing scenarios to evaluate the effectiveness of safety mechanisms, including edge cases, ambiguous requests, and potentially harmful inputs to assess the system's reliability in production environments.