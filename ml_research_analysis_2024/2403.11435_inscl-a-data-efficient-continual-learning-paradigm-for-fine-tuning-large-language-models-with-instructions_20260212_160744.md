---
ver: rpa2
title: 'InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large
  Language Models with Instructions'
arxiv_id: '2403.11435'
source_url: https://arxiv.org/abs/2403.11435
tags:
- arxiv
- instruction
- tasks
- task
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in continual fine-tuning
  of large language models with instructions. It proposes InsCL, a data-efficient
  replay-based continual learning paradigm that dynamically selects high-quality data
  based on task similarity and instruction information.
---

# InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions

## Quick Facts
- arXiv ID: 2403.11435
- Source URL: https://arxiv.org/abs/2403.11435
- Authors: Yifan Wang; Yafei Liu; Chufan Shi; Haoling Li; Chen Chen; Haonan Lu; Yujiu Yang
- Reference count: 40
- Primary result: InsCL achieves 3.0% relative gain over random replay and 27.96% relative gain over no replay when all tasks are trained, consistently outperforming other methods across 16 tasks and different training orders

## Executive Summary
InsCL addresses catastrophic forgetting in continual fine-tuning of large language models with instructions by introducing a data-efficient replay-based approach. The method dynamically selects high-quality data based on task similarity and instruction information, achieving significant performance improvements over existing baselines. The approach demonstrates consistent superiority across different training orders and task categories.

## Method Summary
InsCL implements instruction-based continual learning with dynamic replay allocation based on Wasserstein distance between instruction embeddings, combined with InsInfo-guided sampling that prioritizes diverse and complex instructions. The method uses LLaMA-7B as the base model, training for 2 epochs per task with batch size 64 and learning rate 2e-5, while allocating 200 replay instances per task according to similarity metrics.

## Key Results
- 3.0% relative gain over random replay and 27.96% relative gain over no replay when all tasks are trained
- Consistent outperformance across 16 task categories and three training orders (curriculum, random, reverse)
- Average forgetting rate of 1.35% compared to 4.05% for no replay baseline

## Why This Works (Mechanism)

### Mechanism 1
- Dynamic replay based on task similarity reduces catastrophic forgetting by prioritizing replay of dissimilar tasks
- Mechanism: Calculates Wasserstein Distance between instruction embeddings of current task and previous tasks, then allocates more replay data to tasks with larger distance (more dissimilar)
- Core assumption: Task similarity measured by instruction embedding distance correlates with forgetting risk
- Evidence anchors: [abstract] "InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions"

### Mechanism 2
- InsInfo-guided sampling improves data quality by prioritizing diverse and complex instructions
- Mechanism: Computes InsInfo score based on tag frequency and diversity from instruction annotations, then samples more data from instructions with higher InsInfo scores
- Core assumption: Higher InsInfo correlates with more informative and generalizable instruction data
- Evidence anchors: [abstract] "we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions"

### Mechanism 3
- Combining dynamic replay with InsInfo guidance provides better stability than either approach alone
- Mechanism: First allocates replay quantity by task similarity, then samples within each task based on InsInfo scores
- Core assumption: Task similarity and instruction information are complementary factors in preventing forgetting
- Evidence anchors: [section] "dynamic replay and InsInfo-guided sampling are both beneficial to mitigating catastrophic forgetting"

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why replay-based methods are necessary for continual learning
  - Quick check question: What happens to neural network performance on previous tasks when trained only on new tasks?

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: Mechanism for calculating task similarity based on instruction distributions
  - Quick check question: How does Wasserstein distance differ from other distribution similarity measures like KL divergence?

- Concept: Instruction tuning vs traditional fine-tuning
  - Why needed here: Understanding why traditional CL methods may not directly apply to instruction-based scenarios
  - Quick check question: What key difference between instruction tuning and standard supervised fine-tuning affects replay strategy?

## Architecture Onboarding

- Component map: Task similarity calculator (Wasserstein distance) -> InsInfo metric calculator (tag frequency/diversity) -> Dynamic replay allocator -> InsInfo-guided sampler -> Main training loop with replay integration

- Critical path: 1. Calculate instruction embeddings for current task 2. Compute Wasserstein distances to all previous tasks 3. Allocate replay quantities based on distances 4. Calculate InsInfo scores for all previous instructions 5. Sample replay data according to InsInfo within allocated quantities 6. Train on current task + replay data

- Design tradeoffs: Computational cost vs accuracy in task similarity calculation, Tag granularity vs annotation cost for InsInfo, Fixed vs adaptive replay ratios, Memory overhead for storing instruction embeddings

- Failure signatures: Performance degradation despite replay (similarity measure inaccurate), High variance in Relative Gain across tasks (sampling imbalance), Memory issues during training (replay data too large), Slow convergence (overly conservative replay strategy)

- First 3 experiments: 1. Ablation: Run with only dynamic replay (no InsInfo) to measure individual contribution 2. Ablation: Run with only InsInfo sampling (uniform replay allocation) to measure individual contribution 3. Parameter sweep: Test different Î± values to find optimal replay ratio for given memory constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InsCL vary with different model sizes (e.g., LLaMA-13B, LLaMA-33B)?
- Basis in paper: [inferred] The paper focuses on LLaMA-7B but mentions the method is model-agnostic
- Why unresolved: The experiments only evaluate InsCL on LLaMA-7B
- What evidence would resolve it: Experiments showing performance across different model scales

### Open Question 2
- Question: What is the impact of instruction quality on InsCL's effectiveness?
- Basis in paper: [explicit] "The promising performance demonstrated by InsCL is dependent on high-quality instructions"
- Why unresolved: The paper acknowledges this limitation but doesn't quantify the impact
- What evidence would resolve it: Controlled experiments with varying instruction quality levels

### Open Question 3
- Question: How does InsCL perform on multilingual instruction tuning tasks?
- Basis in paper: [inferred] The paper focuses on English tasks but the method could theoretically apply to other languages
- Why unresolved: All experiments are conducted on English datasets
- What evidence would resolve it: Performance evaluation on multilingual instruction datasets

## Limitations

- Effectiveness relies heavily on high-quality instructions for accurate task similarity and InsInfo calculations
- Limited evaluation on multilingual instruction tuning tasks
- Performance impact across different model scales remains unexplored

## Confidence

- Dynamic replay effectiveness: High - Multiple experiments show consistent performance gains over baselines
- InsInfo-guided sampling benefit: Medium - Ablation studies support the claim but could benefit from more diverse task types
- Combined approach superiority: High - Results consistently show that the combination outperforms individual components

## Next Checks

1. Evaluate InsCL performance using different instruction embedding methods (e.g., sentence transformers, CLIP) to verify that Wasserstein distance remains effective across embedding architectures.

2. Systematically measure the correlation between actual forgetting rates and calculated task similarities to validate whether the Wasserstein distance metric accurately predicts forgetting risk.

3. Test InsCL on highly imbalanced task distributions (e.g., 90% of data from one task, 10% from others) to assess robustness when replay allocation becomes critical for memory retention.