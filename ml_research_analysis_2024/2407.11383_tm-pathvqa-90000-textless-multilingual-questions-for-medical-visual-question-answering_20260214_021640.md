---
ver: rpa2
title: TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question
  Answering
arxiv_id: '2407.11383'
source_url: https://arxiv.org/abs/2407.11383
tags:
- dataset
- questions
- features
- arxiv
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TM-PathVQA, the first multilingual spoken
  VQA dataset for medical images, featuring 98,397 question-answer pairs in English,
  German, and French based on 5,004 pathological images. It implements a multimodal
  learning framework that processes audio and image inputs to handle both yes/no and
  open-ended questions.
---

# TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering

## Quick Facts
- arXiv ID: 2407.11383
- Source URL: https://arxiv.org/abs/2407.11383
- Reference count: 0
- This paper introduces TM-PathVQA, the first multilingual spoken VQA dataset for medical images, featuring 98,397 question-answer pairs in English, German, and French based on 5,004 pathological images.

## Executive Summary
This paper presents TM-PathVQA, the first multilingual spoken Visual Question Answering (VQA) dataset for medical images, containing 98,397 question-answer pairs in English, German, and French based on 5,004 pathological images with 70 hours of spoken audio. The authors implement a multimodal learning framework that jointly processes audio and image inputs to handle both yes/no and open-ended questions. Benchmarking results demonstrate that speech-based systems using Hu-BERT audio features and Faster-RCNN visual features outperform text-based baselines, achieving Top-1 accuracies exceeding 56% for binary classification and 52% for multiclass tasks. The dataset and framework enable more natural, hands-free interaction in medical diagnostics.

## Method Summary
The TM-PathVQA framework uses a multimodal learning (MML) approach where audio features from Hu-BERT or Wav2Vec2 are extracted from spoken questions, and visual features from Faster-RCNN or other vision models are extracted from medical images. These features are downsampled using Conv-2D layers, concatenated, and fed into a two-layer Transformer encoder with dropout. The model is trained using Adam optimizer (learning rate 1e-4) with ReduceLRonPlateau scheduling, gradient clipping, and batch size 64 for 100 epochs. The system handles both binary and multiclass classification tasks, with performance evaluated using Top-1 accuracy, BLEU scores, and F1 metrics.

## Key Results
- Speech-based VQA systems using Hu-BERT audio features and Faster-RCNN visual features outperform text-based baselines.
- Top-1 accuracies exceed 56% for binary classification and 52% for multiclass tasks.
- The dataset comprises 98,397 multilingual spoken questions and answers across English, German, and French based on 5,004 pathological images.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The speech-based VQA system outperforms text-based systems for medical image analysis in hands-free scenarios.
- Mechanism: Audio features (Hu-BERT) and visual features (Faster-RCNN) are jointly encoded using a Transformer encoder that learns multimodal attention, allowing the model to focus on relevant regions in both modalities for answering yes/no and open-ended questions.
- Core assumption: Multimodal fusion improves task performance when one modality (speech) is easier to produce in hands-free settings than the other (text).
- Evidence anchors:
  - [abstract] "speech-based systems using Hu-BERT audio features and Faster-RCNN visual features outperform text-based baselines, with Top-1 accuracies exceeding 56% for binary classification and 52% for multiclass tasks."
  - [section] "From Tables 3 & 4, the speech-based VQA systems outperformed their text counterparts for both tasks when the best combinations were considered."
  - [corpus] Weak evidence for this specific mechanism; the corpus does not provide performance comparisons between speech and text in hands-free settings.
- Break condition: If the multimodal fusion does not improve accuracy over unimodal models, or if speech input introduces significant noise or latency.

### Mechanism 2
- Claim: Textless multilingual support broadens accessibility and usability of medical VQA systems across language barriers.
- Mechanism: Direct Text-to-Speech Translation (DT2ST) converts English questions into spoken German and French, preserving semantic meaning and enabling the same model architecture to process queries in multiple languages without requiring language-specific parsing.
- Core assumption: Multilingual speech can be processed using the same feature extraction pipeline (Hu-BERT) without significant degradation in performance.
- Evidence anchors:
  - [abstract] "This dataset comprises 98,397 multilingual spoken questions and answers based on 5,004 pathological images along with 70 hours of audio."
  - [section] "This paper introduces the first-ever TM-PathVQA dataset featuring spoken questions in four languages viz. English, German, and French."
  - [corpus] No direct evidence from the corpus on the effectiveness of multilingual speech; only general related works on Med-VQA.
- Break condition: If multilingual performance drops significantly compared to monolingual, or if translation quality introduces semantic errors.

### Mechanism 3
- Claim: Using large-scale pretrained models (Hu-BERT, Faster-RCNN) as feature extractors improves generalization for medical VQA.
- Mechanism: Feature representations from Hu-BERT (768-dim) and Faster-RCNN (region features) capture rich acoustic and visual patterns learned from large datasets, which are then fine-tuned for the medical domain.
- Core assumption: Pretrained features provide better initialization than random or task-specific features, especially for complex domains like medical imaging.
- Evidence anchors:
  - [section] "Audio features were extracted from the raw spoken questions... using Wav2Vec2... and Hu-BERT... along with 80-dimensional Mel filterbanks... Image features were extracted using state-of-the-art models, including Vision Transformer (ViT), ResNet-152, VGG19, and Faster-RCNN."
  - [section] "From Tables 3 & 4, the speech-based VQA systems outperformed their text counterparts for both tasks when the best combinations were considered. Audio features extracted using Hu-BERT also performed better than other acoustic features."
  - [corpus] No corpus evidence specifically on the use of Hu-BERT or Faster-RCNN in this exact task; only mentions general Med-VQA work.
- Break condition: If fine-tuning on medical data does not improve performance, or if the feature extractor is too large for efficient deployment.

## Foundational Learning

- Concept: Multimodal learning and attention mechanisms
  - Why needed here: The model must fuse audio and visual inputs effectively to reason about medical images and spoken questions.
  - Quick check question: Can you explain how cross-attention between audio and image features can improve VQA accuracy compared to late fusion?

- Concept: Speech feature extraction and audio embeddings
  - Why needed here: The system relies on Hu-BERT to convert raw speech into fixed-dimensional embeddings that the Transformer can process.
  - Quick check question: What are the main differences between Hu-BERT and Wav2Vec2 in terms of feature representation and suitability for medical speech?

- Concept: Multilingual dataset creation and translation pipelines
  - Why needed here: The dataset requires high-quality translation from English to German and French while preserving medical terminology and question semantics.
  - Quick check question: How would you evaluate the semantic fidelity of translated spoken questions compared to the original English version?

## Architecture Onboarding

- Component map:
  Speech preprocessing -> Hu-BERT / Wav2Vec2 -> 768-dim audio features -> Conv-2D downsampling -> Concatenation -> Transformer encoder (2 layers) -> Classification head -> Answer prediction
  Image preprocessing -> Faster-RCNN / ViT / ResNet / VGG -> visual features -> Concatenation -> Transformer encoder (2 layers) -> Classification head -> Answer prediction

- Critical path:
  1. Extract audio features from spoken question
  2. Extract visual features from image
  3. Concatenate and downsample audio with image features
  4. Feed into Transformer encoder
  5. Classify answer with softmax

- Design tradeoffs:
  - Audio feature choice (Hu-BERT vs Wav2Vec2): Hu-BERT yields better accuracy but may require more compute
  - Visual feature choice (Faster-RCNN vs ViT): Faster-RCNN better for region-based reasoning, ViT may be faster
  - Multimodal fusion depth: early fusion (concat) vs late fusion (separate encoders)

- Failure signatures:
  - Low Top-1 accuracy but high BLEU scores: model predicts plausible but incorrect answers
  - Poor multilingual performance: translation or feature extraction is not language-agnostic
  - Overfitting: train accuracy high, validation accuracy low

- First 3 experiments:
  1. Ablation: Replace Hu-BERT with Mel-filterbanks to test impact of pretrained speech models
  2. Fusion test: Compare early fusion (concat) vs late fusion (separate encoders) on Top-1 accuracy
  3. Multilingual evaluation: Train and test on English only vs all three languages to quantify multilingual benefit

## Open Questions the Paper Calls Out

- Question: How would integrating real-time feedback mechanisms into the TM-PathVQA system impact diagnostic accuracy and user satisfaction in clinical settings?
  - Basis in paper: [inferred] The paper highlights the system's potential for hands-free interaction and natural user experience but does not explore real-time feedback integration.
  - Why unresolved: Real-time feedback could dynamically adjust system responses based on user interaction, yet its impact on accuracy and satisfaction remains untested.
  - What evidence would resolve it: Clinical trials comparing TM-PathVQA with and without real-time feedback, measuring diagnostic accuracy and user satisfaction metrics.

- Question: Can the TM-PathVQA system's performance be further enhanced by incorporating multimodal attention mechanisms that dynamically weigh audio and visual inputs based on task complexity?
  - Basis in paper: [inferred] The paper uses a basic MML framework but does not explore dynamic attention mechanisms for input weighting.
  - Why unresolved: Dynamic attention could optimize feature fusion, but its effectiveness in improving VQA performance is untested.
  - What evidence would resolve it: Comparative studies evaluating TM-PathVQA with and without dynamic attention mechanisms, using task complexity as a variable.

- Question: What is the impact of expanding the TM-PathVQA dataset to include additional languages and dialects on the system's generalizability and performance in diverse medical environments?
  - Basis in paper: [explicit] The paper introduces a multilingual dataset but focuses on three languages, leaving potential benefits of expansion unexplored.
  - Why unresolved: Expanding language coverage could enhance generalizability, but its impact on system performance is unknown.
  - What evidence would resolve it: Performance benchmarks of TM-PathVQA after expanding the dataset to include more languages and dialects, across varied medical environments.

## Limitations

- Evaluation focuses on technical performance (Top-1 accuracy, BLEU, F1) but does not assess clinical utility or real-world deployment feasibility.
- Dataset construction relies on English-to-German/French translation, but semantic fidelity of translated spoken questions is not quantitatively validated.
- Claims about hands-free scenario improvements lack empirical evidence from real-world medical diagnostic workflows.

## Confidence

- **High Confidence**: Speech-based systems outperform text-based baselines when using Hu-BERT and Faster-RCNN features, as demonstrated by the reported Top-1 accuracy metrics.
- **Medium Confidence**: The multimodal attention mechanism improves VQA performance by jointly focusing on relevant audio and visual regions, though ablation studies are not provided to isolate this effect.
- **Low Confidence**: The claim that hands-free scenarios benefit significantly from speech-based VQA lacks empirical evidence from real-world medical diagnostic workflows.

## Next Checks

1. **Clinical Usability Test**: Conduct a user study comparing task completion time and accuracy between speech-based and text-based VQA systems in simulated medical diagnostic scenarios.

2. **Translation Quality Validation**: Implement automated semantic similarity metrics (e.g., SBERT) and human evaluation to quantify semantic fidelity between original English questions and translated German/French spoken versions.

3. **Multimodal Ablation Study**: Train and evaluate unimodal (audio-only, image-only) models alongside the multimodal model to measure the specific contribution of cross-attention fusion to performance gains.