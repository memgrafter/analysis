---
ver: rpa2
title: 'Apollo: An Exploration of Video Understanding in Large Multimodal Models'
arxiv_id: '2412.10360'
source_url: https://arxiv.org/abs/2412.10360
tags:
- video
- siglip
- performance
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the underexplored area of video understanding\
  \ in Large Multimodal Models (LMMs), hindered by high computational costs and unclear\
  \ design choices. The authors systematically investigate key design decisions\u2014\
  such as video sampling (fps vs."
---

# Apollo: An Exploration of Video Understanding in Large Multimodal Models

## Quick Facts
- arXiv ID: 2412.10360
- Source URL: https://arxiv.org/abs/2412.10360
- Reference count: 40
- Primary result: Apollo-3B outperforms most 7B models (68.7 on MLVU); Apollo-7B rivals 30B-parameter models (70.9 on MLVU, 66.3 on Video-MME)

## Executive Summary
This paper systematically explores video understanding in Large Multimodal Models (LMMs), addressing the underexplored nature of video comprehension due to high computational costs and unclear design choices. The authors investigate key design decisions including video sampling methods, vision encoder selection, token resampling techniques, and data mixture ratios, introducing the "Scaling Consistency" principle that shows smaller models can reliably guide larger model design. Based on these insights, they develop Apollo, a family of state-of-the-art LMMs that achieve superior performance across scales, with Apollo-3B outperforming most 7B models and Apollo-7B rivaling 30B-parameter models.

## Method Summary
The authors systematically investigate video-LMM design space through controlled ablation studies, examining four key design decisions: video sampling (fps vs uniform), vision encoder selection (SigLIP-SO400M vs others), token resampling (Perceiver Resampler vs alternatives), and data mixture composition (10-14% text data optimal). They introduce "Scaling Consistency," demonstrating that design choices effective for smaller models (~2-4B parameters) reliably transfer to larger ones, enabling efficient experimentation. The Apollo models use Qwen2.5 LLMs with SigLIP-SO400M and InternVideo2 encoders, Perceiver Resampler, and a 3-stage training schedule with specific data mixtures. They also curate ApolloBench, a more efficient evaluation benchmark that reduces evaluation time by 41× while maintaining high correlation with existing benchmarks.

## Key Results
- Apollo-3B outperforms most 7B-parameter models with 68.7 on MLVU
- Apollo-7B rivals 30B-parameter models with 70.9 on MLVU and 66.3 on Video-MME
- SigLIP-SO400M vision encoder consistently outperforms other encoders including video-specific ones
- Perceiver Resampler is the most effective token resampling method
- Scaling Consistency principle enables efficient experimentation by transferring design choices from small to large models
- ApolloBench reduces evaluation time by 41× while maintaining benchmark correlation

## Why This Works (Mechanism)
Apollo's success stems from systematically identifying optimal design choices for video-LMMs through controlled ablation studies. The Scaling Consistency principle allows researchers to efficiently explore the design space using smaller models before scaling up, reducing computational costs by orders of magnitude. By combining the strengths of image and video encoders through proper interpolation and concatenation, Apollo leverages the superior video understanding capabilities of SigLIP-SO400M while maintaining efficient token processing through Perceiver Resampler. The optimized data mixture with 10-14% text data provides the right balance for cross-modal learning without overwhelming the model with non-visual content.

## Foundational Learning
**Video sampling methods** - Why needed: Different sampling strategies affect temporal understanding and computational efficiency; quick check: Compare model performance using fps vs uniform sampling on temporal reasoning tasks.
**Vision encoder selection** - Why needed: The choice of vision encoder significantly impacts video feature extraction quality; quick check: Evaluate different encoders (image vs video) on video-specific benchmarks.
**Token resampling techniques** - Why needed: Video tokens must be reduced to fit LLM context windows while preserving information; quick check: Compare Perceiver Resampler against simpler pooling methods on video comprehension tasks.
**Data mixture optimization** - Why needed: Proper balance of text, image, and video data is crucial for effective cross-modal learning; quick check: Vary text percentage in data mixture and measure performance on vision-language tasks.
**Scaling Consistency principle** - Why needed: Enables efficient design exploration by transferring insights from small to large models; quick check: Validate design choices on multiple model scales and measure performance correlation.
**Benchmark curation methodology** - Why needed: Efficient evaluation is critical for rapid iteration during research; quick check: Compare curated benchmark correlation with full benchmark suite across multiple models.

## Architecture Onboarding
**Component map:** Video frames -> SigLIP-SO400M/InternVideo2 encoders -> Feature interpolation -> Perceiver Resampler -> LLM connector -> Qwen2.5 LLM
**Critical path:** Video sampling → dual encoder feature extraction → feature concatenation → token resampling → LLM processing
**Design tradeoffs:** Image encoders (SigLIP-SO400M) offer better performance than video encoders but may miss temporal dynamics; higher fps sampling improves temporal understanding but increases computational cost; more text data in mixture aids language understanding but may dilute visual learning.
**Failure signatures:** Performance degradation when using uniform sampling instead of fps; suboptimal results from incorrect encoder combination; computational inefficiency from inadequate token resampling.
**First experiments:** 1) Compare fps vs uniform sampling on temporal reasoning tasks; 2) Evaluate SigLIP-SO400M against video encoders on video comprehension benchmarks; 3) Test Perceiver Resampler against simple pooling methods for token reduction efficiency.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does Scaling Consistency extend reliably to models larger than 7B parameters (e.g., 30B, 70B)?
- Basis in paper: [explicit] The paper demonstrates Scaling Consistency up to ~4B parameters and suggests potential generalization to larger models based on log-linear trends, but does not empirically test beyond 7B.
- Why unresolved: Experiments were limited to Qwen2.5 models up to 7B parameters; larger-scale testing is computationally prohibitive.
- What evidence would resolve it: Training and evaluating model design choices on models significantly larger than 7B (e.g., 30B+) and comparing their performance to smaller models would confirm whether Scaling Consistency holds at larger scales.

### Open Question 2
- Question: Are there specific architectural modifications or pretraining strategies that could enable video encoders to surpass image encoders in video-LMM performance?
- Basis in paper: [explicit] SigLIP-SO400M (an image encoder) consistently outperforms video encoders in Apollo, suggesting video encoders need improvement.
- Why unresolved: The study identifies the gap but does not explore targeted improvements to video encoders (e.g., architectural changes, pretraining objectives).
- What evidence would resolve it: Systematic ablation studies comparing different video encoder architectures, pretraining methods, or hybrid designs against image encoders in controlled experiments.

### Open Question 3
- Question: How does the performance of video-LMMs trained with active frame selection (query-guided) compare to static fps sampling in multi-turn conversations?
- Basis in paper: [inferred] The paper notes that active frame selection methods were excluded from experiments because they require resampling at every conversational turn, implying potential limitations or differences in conversational contexts.
- Why unresolved: Static fps sampling was the focus; active frame selection strategies were not evaluated in conversational settings.
- What evidence would resolve it: Direct comparison of video-LMMs using static fps sampling versus active frame selection across multi-turn conversational benchmarks measuring both accuracy and efficiency.

## Limitations
- Study focuses on technical design choices rather than fundamental algorithmic innovations
- Evaluation constrained to English-language benchmarks, limiting multilingual generalization
- Computational efficiency gains need further validation across different model architectures
- Scaling Consistency principle requires additional testing with models beyond 1.5B-7B parameter range
- Curated benchmark may introduce evaluation bias and may not capture all video understanding capabilities
- Does not address potential copyright concerns with the training data mixture

## Confidence
- High confidence in technical findings regarding design choices (sampling methods, encoder selection, resampler effectiveness) due to systematic ablation studies and multiple trials
- Medium confidence in Scaling Consistency principle, as it is demonstrated within a limited parameter range and may not extend to much larger models
- Medium confidence in ApolloBench benchmark's representativeness, as it is curated from existing benchmarks but may miss certain video understanding capabilities

## Next Checks
1. Test the Scaling Consistency principle with models larger than 7B parameters to verify if design decisions continue to transfer effectively to much larger scales
2. Evaluate Apollo models on multilingual video understanding benchmarks to assess performance beyond English-language content and verify data mixture ratios remain optimal
3. Replicate the ApolloBench curation process on a different set of video understanding benchmarks to test the robustness of the selection methodology and ensure it captures the full spectrum of video comprehension capabilities