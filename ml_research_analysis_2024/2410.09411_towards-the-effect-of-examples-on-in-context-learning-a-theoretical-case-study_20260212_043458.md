---
ver: rpa2
title: 'Towards the Effect of Examples on In-Context Learning: A Theoretical Case
  Study'
arxiv_id: '2410.09411'
source_url: https://arxiv.org/abs/2410.09411
tags:
- examples
- when
- pre-training
- knowledge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how large language models perform in-context
  learning (ICL) for binary classification by analyzing the interplay between pre-training
  knowledge and demonstration examples. The authors introduce a probabilistic model
  based on Gaussian mixtures to formally quantify how pre-training parameters, label
  frequency, and label noise in examples affect ICL accuracy.
---

# Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study

## Quick Facts
- arXiv ID: 2410.09411
- Source URL: https://arxiv.org/abs/2410.09411
- Reference count: 40
- Key outcome: Proves that ICL accuracy depends on pre-training knowledge, example label frequency, and label noise; shows predictions shift from pre-training to examples as example count grows

## Executive Summary
This paper provides a theoretical analysis of in-context learning (ICL) for binary classification by modeling the interplay between pre-training knowledge and demonstration examples using Gaussian mixtures. The authors derive closed-form expressions for ICL accuracy and prove several key properties: when pre-training and example knowledge contradict, prediction reliance shifts from pre-training to examples as example count increases; imbalanced example classes significantly lower minority class accuracy; and label noise impacts accuracy differently depending on noise levels in each class. The theoretical predictions are validated through simulations and experiments with models like Pythia-6.9B and Llama2-7B on SST2 dataset, revealing important insights about when ICL works well and when it fails.

## Method Summary
The paper analyzes ICL using a Gaussian mixture model framework where data is generated from two Gaussian distributions representing positive and negative classes. The method involves Bayesian inference to compute posterior distributions of model parameters given examples, deriving the ICL decision boundary as a function of normalized likelihood ratios and class frequencies. The authors prove theoretical results about accuracy under various conditions (contradicting knowledge, imbalanced examples, label noise) and validate these through synthetic simulations and real-data experiments with transformer models. The core approach uses closed-form derivations rather than empirical training, allowing precise characterization of how pre-training knowledge, example count, class balance, and noise levels affect ICL performance.

## Key Results
- ICL prediction reliance shifts from pre-training knowledge to example knowledge as example count grows when the two contradict
- Imbalanced example classes significantly reduce accuracy for the minority class, with accuracy approaching zero as class imbalance increases
- Label noise impacts accuracy differently depending on noise levels in each class, with higher noise in one class degrading that class's accuracy more
- When examples are not independently sampled (e.g., forced class balance), ICL predictions exhibit "mean reversion" toward the prompt's class distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning shifts prediction reliance from pre-training knowledge to example knowledge as the number of examples grows.
- Mechanism: When pre-training knowledge contradicts example knowledge, ICL accuracy initially suffers if few examples are provided. As more examples are added, the posterior distribution of model parameters increasingly reflects the example distribution, overriding pre-training knowledge.
- Core assumption: Examples are independently sampled from the same distribution within each prompt.
- Evidence anchors:
  - [abstract]: "when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples"
  - [section 2.3]: "Contradicting knowledge...when there are not enough examples and the pre-training knowledge contradicts to the knowledge in the examples...there is an obvious drop in ICL performance"
  - [corpus]: Weak - corpus contains related works but lacks direct experimental evidence of this specific claim
- Break condition: If examples are not independently sampled (e.g., forced class balance), mean reversion phenomenon dominates and the shift from pre-training to examples may not occur as expected.

### Mechanism 2
- Claim: Imbalanced example classes lower accuracy for the minority class.
- Mechanism: When the fraction of positive examples π approaches 0 or 1, the ICL prediction boundary shifts to favor the majority class, causing minority class accuracy to approach zero.
- Core assumption: The decision boundary depends on the label frequency in examples, and imbalanced distributions skew this boundary.
- Evidence anchors:
  - [abstract]: "imbalanced example classes lower accuracy for the minority class"
  - [section 2.3]: "Proposition 3...when the examples for one class are much fewer than the other class, ICL performance for the minor class will significantly drop"
  - [section 3.1]: "Imbalanced examples...when the fraction of positive is increasing, ICL accuracy for positive inputs is increasing and finally reaches 100%, while ICL accuracy for negative inputs is decreasing"
- Break condition: If class imbalance is artificially corrected through example selection, this mechanism may not manifest.

### Mechanism 3
- Claim: Label noise impacts accuracy differently depending on noise levels in each class.
- Mechanism: When label noise (pe+, pe-) is present, the model's parameter estimates become biased toward the noisier class, and accuracy changes based on the relative noise levels between classes.
- Core assumption: Label noise is modeled as a probability of flipping labels within each class independently.
- Evidence anchors:
  - [abstract]: "how the label noise impacts the accuracy is determined by the specific noise level of the two classes"
  - [section 2.3]: "Label noise...Proposition 4...when 1 − pe+ − pe− < 0, P(correct|y = +1, pe+, pe−) increases in pe+, and P(correct|y = −1, pe+, pe−) increases in pe−"
  - [section 3.1]: "Label noise...when the flipping probability in the negative class is fixed, smaller flipping probability (higher pe+) in the positive class usually leads to higher accuracy in the positive class"
- Break condition: If noise levels are symmetric or very high across both classes, accuracy degradation may become uniform and the differential impact may not be observable.

## Foundational Learning

- Concept: Gaussian mixture models for binary classification
  - Why needed here: The paper uses a Gaussian mixture model to generate data and analyze ICL behavior, making it essential to understand how two Gaussian distributions with different means and variances can represent positive and negative classes
  - Quick check question: How does changing the distance between class means (θ+ and θ-) affect classification accuracy in a Gaussian mixture model?

- Concept: Bayesian inference for parameter estimation
  - Why needed here: The analysis relies on computing posterior distributions of model parameters (θ+, θ-, π) given examples, which requires understanding how prior distributions are updated with observed data
  - Quick check question: Given a prior N(θM, σ²M) and n observations, what is the posterior distribution of θ in Bayesian linear regression?

- Concept: Bahadur representation for convergence analysis
  - Why needed here: The paper uses Bahadur representation to analyze how ICL accuracy converges as the number of examples increases, showing the dominant terms and remainder terms in the accuracy expression
  - Quick check question: What is the Bahadur representation of a sample proportion, and how does it help establish convergence rates?

## Architecture Onboarding

- Component map: Data generation module (Gaussian mixture) -> Bayesian inference engine (posterior computation) -> ICL decision boundary calculator (likelihood ratio) -> Accuracy evaluation module (class-specific metrics)
- Critical path: Generate synthetic data with specified parameters → Compute posterior distributions from examples → Calculate ICL decision boundary using normalized likelihood ratios → Evaluate accuracy for each class based on theoretical formulas
- Design tradeoffs: Independent vs dependent example sampling (affects mean reversion), label noise modeling (affects parameter estimation), class balance (affects minority class performance)
- Failure signatures: Poor minority class accuracy suggests imbalance issues; degraded performance with few examples suggests pre-training knowledge dominates; inconsistent predictions with varying example order suggest dependency issues
- First 3 experiments:
  1. Test ICL accuracy with increasing example count when pre-training and example knowledge contradict, expecting initial poor performance that improves with more examples
  2. Evaluate class-specific accuracy under varying class imbalance ratios to observe minority class degradation
  3. Introduce label noise at different levels for each class and measure differential impact on class-specific accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dependence structure among examples (e.g., fixed label proportions) affect ICL prediction beyond binary classification, in multi-class or regression settings?
- Basis in paper: [explicit] The paper explicitly analyzes dependent examples in binary classification, revealing the "mean reversion" phenomenon, and notes this extends to multi-class settings in the appendix.
- Why unresolved: The analysis focuses on binary classification; extending to multi-class or regression would require new theoretical frameworks and empirical validation.
- What evidence would resolve it: Theoretical derivations showing ICL decision boundaries in multi-class/regression with dependent examples, and simulation or real-data experiments confirming the effect.

### Open Question 2
- Question: Under what conditions does ICL revert to pre-training knowledge versus relying on example knowledge when both contradict, especially in the presence of label noise?
- Basis in paper: [explicit] Proposition 2 and 4 discuss the interplay between pre-training knowledge and examples under contradicting knowledge and label noise, but do not fully characterize the transition point.
- Why unresolved: The paper provides asymptotic conditions but lacks a precise, finite-sample characterization of when each source dominates, especially with varying noise levels.
- What evidence would resolve it: A finite-sample bound on the example size k needed for examples to dominate, as a function of pre-training variance, noise levels, and class balance.

### Open Question 3
- Question: How does the choice of example selection strategy (e.g., balancing, diversity) impact ICL accuracy in imbalanced or noisy real-world datasets?
- Basis in paper: [inferred] The paper shows that imbalanced examples lower accuracy for minority classes and that label noise affects predictions, implying that selection strategy matters, but does not test this empirically.
- Why unresolved: Experiments focus on synthetic data and balanced/imbalanced settings, but do not explore strategic example selection methods or their effectiveness in real-world tasks.
- What evidence would resolve it: Empirical comparison of different example selection strategies (e.g., balanced sampling, diversity maximization) on real datasets with known class imbalance or noise, measuring ICL accuracy per class.

## Limitations
- Theoretical analysis relies heavily on Gaussian mixture model assumptions that may not capture real language data complexity
- Mean reversion phenomenon under non-independent sampling needs more extensive empirical validation across different architectures
- Asymptotic bounds may not accurately predict behavior for small example sizes commonly used in practice
- Label noise analysis assumes symmetric noise distributions and may not generalize to complex real-world noise patterns

## Confidence

- **High Confidence:** The core theoretical framework using Gaussian mixtures and Bayesian inference is mathematically sound and well-established. The proof that ICL shifts from pre-training to example knowledge with increasing examples is rigorously derived.
- **Medium Confidence:** The simulation results and real-data experiments with Pythia-6.9B and Llama2-7B support the theoretical predictions, but the sample sizes are relatively small and the SST2 dataset may not be representative of broader ICL applications.
- **Low Confidence:** The mean reversion phenomenon under non-independent sampling, while theoretically derived, requires more extensive empirical validation across different model architectures and tasks to confirm its practical significance.

## Next Checks

1. **Experiment with variable example ordering:** Systematically test how ICL accuracy changes when examples are presented in different orders (e.g., all positive then all negative vs. interleaved) to quantify the impact of the mean reversion phenomenon and validate the non-independent sampling assumptions.

2. **Noise asymmetry validation:** Design experiments with asymmetric label noise patterns (e.g., high noise in positive class, low noise in negative class) to test whether the theoretical predictions about differential noise impact hold across multiple model sizes and datasets beyond SST2.

3. **Small sample behavior analysis:** Conduct experiments with 1-5 examples to empirically validate whether the theoretical predictions about the transition from pre-training to example knowledge dependence hold for the small prompt sizes commonly used in practice, particularly focusing on the accuracy drop when pre-training knowledge contradicts examples.