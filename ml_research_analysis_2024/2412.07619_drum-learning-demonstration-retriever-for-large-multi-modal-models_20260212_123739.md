---
ver: rpa2
title: 'DRUM: Learning Demonstration Retriever for Large MUlti-modal Models'
arxiv_id: '2412.07619'
source_url: https://arxiv.org/abs/2412.07619
tags:
- drum
- lvlm
- demonstrations
- embedding
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DRUM, a novel framework for improving demonstration
  retrieval in Large Vision-Language Models (LVLMs) for in-context learning. The key
  innovation is fine-tuning a visual-language embedding model using LVLM feedback
  to better match the LVLM's needs when retrieving demonstrations.
---

# DRUM: Learning Demonstration Retriever for Large MUlti-modal Models

## Quick Facts
- arXiv ID: 2412.07619
- Source URL: https://arxiv.org/abs/2412.07619
- Reference count: 14
- Key outcome: DRUM improves LVLM ICL performance by 3.5% on average across 7 benchmark datasets using fine-tuned visual-language embeddings with LVLM feedback

## Executive Summary
This paper introduces DRUM, a framework that fine-tunes visual-language embedding models using feedback from Large Vision-Language Models (LVLMs) to improve demonstration retrieval for in-context learning. The key innovation is leveraging the LVLM's own conditional log-likelihood as a ranking signal to train the retriever to find demonstrations that are more useful for the specific LVLM. DRUM employs a SIT-IPDR retrieval strategy with concatenated image-text embeddings, re-ranks demonstrations using LVLM feedback with a list-wise ranking loss, and iteratively mines better demonstration candidates. Experiments across VQA, image classification, and image captioning tasks show consistent improvements over baselines including random sampling, CLIP, and EPR.

## Method Summary
DRUM fine-tunes a visual-language embedding model using LVLM feedback to retrieve better demonstrations for in-context learning. The framework retrieves demonstrations using a SIT-IPDR strategy with concatenated image and text embeddings, then re-ranks them using the LVLM's conditional log-likelihood as a quality signal. A list-wise ranking loss is calculated based on this feedback and used to update the embedding model. The process includes iterative demonstration mining where candidates are progressively improved across training iterations. The framework is evaluated on 7 benchmark datasets across three task types (VQA, image classification, image captioning) using Open-Flamingo 9B as the LVLM.

## Key Results
- DRUM improves LVLM ICL performance by 3.5% on average across 7 benchmark datasets
- Consistently outperforms baselines (Random, BM25, DINO, BGE, CLIP, EPR) across all three task types
- Strong transferability across different LVLMs, maintaining effectiveness when evaluated on different architectures
- Shows effectiveness with varying numbers of demonstrations, maintaining performance gains across different demonstration set sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The concatenation of image and text embeddings provides richer semantic information than either modality alone for demonstration retrieval.
- Mechanism: By combining visual and textual embeddings, the retrieval system captures cross-modal relationships that better match the LVLM's understanding of demonstrations.
- Core assumption: The LVLM's internal representations benefit from demonstrations that contain both image and text information simultaneously.
- Evidence anchors:
  - [abstract] "we propose to concate the image and text embeddings to enhance the retrieval performance"
  - [section] "Since the CLIP model can generate two vectors for the text and image contents separately, these two vectors will be concatenated"
- Break condition: If the LVLM primarily relies on one modality or if cross-modal interactions don't improve task performance.

### Mechanism 2
- Claim: The LVLM's conditional log-likelihood feedback provides a more accurate ranking signal than pre-trained embedding similarities for demonstration quality.
- Mechanism: The LVLM itself evaluates which demonstrations help it generate correct responses, and this feedback is used to train the embedding model to retrieve more helpful demonstrations.
- Core assumption: The LVLM's likelihood function correlates with demonstration usefulness for the specific task.
- Evidence anchors:
  - [abstract] "we propose to re-rank the demonstrations retrieved by the embedding model via the LVLM's feedbacks, and calculate a list-wise ranking loss for training the embedding model"
  - [section] "s(zj) = LLH(responseq|zj, imageq, promptq), (4) where LLH(·|·) is the LVLM's conditional log-likelihood function"
- Break condition: If the LVLM's likelihood function doesn't correlate with demonstration quality or if the LVLM's feedback is noisy.

### Mechanism 3
- Claim: Iterative demonstration mining progressively improves the quality of candidate demonstrations used for training the embedding model.
- Mechanism: By repeatedly using the current embedding model to retrieve candidates, having the LVLM re-rank them, and using these high-quality examples for the next training iteration, the system gradually learns to find better demonstrations.
- Core assumption: Higher-quality demonstration candidates lead to better embedding model training and improved retrieval performance.
- Evidence anchors:
  - [abstract] "we propose an iterative demonstration mining strategy to improve the training of the embedding model"
  - [section] "We adapt an iterative strategy to update candidates... Specifically, we iteratively train the retriever and use it to select candidates in turn"
- Break condition: If the iterative process converges to suboptimal candidates or if the improvement plateaus quickly.

## Foundational Learning

- Concept: In-context learning (ICL) in large language models
  - Why needed here: The paper builds on ICL principles to extend them to vision-language tasks through demonstration retrieval
  - Quick check question: What is the key difference between fine-tuning and in-context learning in terms of model adaptation?

- Concept: Vision-language embeddings and similarity metrics
  - Why needed here: The framework relies on embedding models (like CLIP) to retrieve demonstrations based on similarity scores
  - Quick check question: How do cosine similarity and Euclidean distance differ in measuring embedding similarity?

- Concept: List-wise ranking loss functions
  - Why needed here: The training objective uses a list-wise ranking loss to incorporate the LVLM's feedback on demonstration quality
  - Quick check question: What is the advantage of list-wise ranking loss over pair-wise ranking loss in training retrieval systems?

## Architecture Onboarding

- Component map:
  Visual-language embedding model (E) -> Large vision-language model (M) -> Vector database -> Retrieval strategy module -> Training pipeline

- Critical path:
  1. Retrieve demonstrations using current embedding model
  2. Have LVLM re-rank retrieved demonstrations
  3. Calculate list-wise ranking loss
  4. Update embedding model
  5. Iterate with improved candidates

- Design tradeoffs:
  - Using concatenation vs. separate embeddings: Concatenation captures cross-modal relationships but increases dimensionality
  - Number of iterations in mining: More iterations may improve quality but increase training time
  - Candidate pool size: Larger pools provide more diversity but increase computational cost

- Failure signatures:
  - Performance plateaus early in training: May indicate convergence to suboptimal candidates
  - Training instability: Could suggest issues with the list-wise ranking loss formulation
  - Poor transfer to new LVLMs: May indicate over-fitting to the training LVLM's preferences

- First 3 experiments:
  1. Ablation study removing the concatenation of image and text embeddings to test its impact on retrieval quality
  2. Test different candidate pool sizes (n=16, 32, 64) to find optimal trade-off between quality and efficiency
  3. Evaluate transfer performance to a different LVLM (e.g., GPT-4o) to validate generalization of the fine-tuned embedding model

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the content, some implicit open questions include:

- How does the effectiveness of DRUM's fine-tuning approach vary across different types of vision-language tasks (e.g., VQA, image classification, image captioning)?
- How does the quality of demonstrations retrieved by DRUM compare to those retrieved by human experts for specific tasks?
- What is the impact of the number of iterations in the iterative demonstration mining strategy on DRUM's performance?

## Limitations

- The framework's effectiveness depends heavily on the quality and diversity of the candidate demonstration pool, which is not discussed in detail regarding its construction methodology.
- The assumption that LVLM conditional log-likelihood correlates with demonstration quality may not hold for all task types or model architectures.
- The computational cost of the iterative demonstration mining process, particularly when scaling to larger candidate pools, is not thoroughly analyzed.

## Confidence

**High Confidence**: The core retrieval mechanism (SIT-IPDR with concatenated embeddings) and the list-wise ranking loss formulation have clear theoretical grounding in the in-context learning literature. The experimental design comparing DRUM against multiple baselines is comprehensive.

**Medium Confidence**: The iterative demonstration mining strategy shows promise but the paper doesn't provide sufficient evidence that the improvements are due to better candidates rather than the iterative fine-tuning process itself. The transferability results across different LVLMs are promising but based on limited comparisons.

**Low Confidence**: The paper doesn't adequately address potential overfitting to the specific LVLM used for feedback (Open-Flamingo 9B), nor does it explore the sensitivity of performance to different candidate pool sizes or the number of iterative mining rounds.

## Next Checks

1. **Candidate Pool Sensitivity Analysis**: Systematically evaluate DRUM performance across different candidate pool sizes (n=16, 32, 64, 128) to determine the optimal trade-off between computational cost and retrieval quality, and to assess whether performance plateaus at larger sizes.

2. **Cross-LVLM Generalization Test**: Evaluate DRUM's fine-tuned embedding model on a completely different LVLM architecture (e.g., GPT-4o, Gemini) to determine if the learned retrieval preferences transfer beyond the training LVLM, and analyze which types of demonstrations remain effective across architectures.

3. **Convergence Analysis of Iterative Mining**: Track the quality of retrieved demonstrations and downstream LVLM performance across each iteration of the mining process to determine if there's a point of diminishing returns, and whether the process converges to a stable set of demonstrations or continues to evolve.