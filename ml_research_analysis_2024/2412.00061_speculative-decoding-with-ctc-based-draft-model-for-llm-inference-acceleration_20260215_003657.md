---
ver: rpa2
title: Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration
arxiv_id: '2412.00061'
source_url: https://arxiv.org/abs/2412.00061
tags:
- draft
- decoding
- base
- tokens
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a CTC-based draft model for accelerating
  LLM inference via speculative decoding. The key idea is to use Connectionist Temporal
  Classification to generate draft tokens with better inter-token correlations, improving
  draft quality and acceptance rate.
---

# Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration

## Quick Facts
- **arXiv ID**: 2412.00061
- **Source URL**: https://arxiv.org/abs/2412.00061
- **Reference count**: 30
- **Primary result**: CTC-based draft model achieves 2.2-2.8x speedup over vanilla decoding with 3.4-3.6 tokens accepted per step

## Executive Summary
This paper introduces a CTC-based draft model for accelerating LLM inference via speculative decoding. The key innovation is using Connectionist Temporal Classification to generate draft tokens with better inter-token correlations, improving draft quality and acceptance rate. Experiments on MT-bench and GSM8K using Vicuna and LLaMA-2-Chat models demonstrate that the CTC-drafter outperforms baselines like Medusa and Hydra, achieving significant speedup while maintaining strong performance across different model sizes.

## Method Summary
The method trains a lightweight draft model using CTC loss with knowledge distillation from a larger base LLM. The draft model uses a single transformer layer to generate token probabilities, which are processed through a CTC transform that removes repeated tokens and blanks. During inference, the draft model generates candidate sequences that are verified by the base model - accepted tokens are output while rejected ones trigger regeneration. The approach leverages CTC's ability to model inter-token dependencies at the sequence level, improving draft quality compared to independent token prediction methods.

## Key Results
- Achieves 2.2-2.8x speedup over vanilla decoding on MT-bench and GSM8K datasets
- Outperforms baseline methods (Medusa, Hydra) with 3.4-3.6 tokens accepted per decoding step
- Shows strong generalization across different base model sizes (7B, 13B, 33B parameters)
- Maintains performance on both conversational (MT-bench) and mathematical (GSM8K) tasks

## Why This Works (Mechanism)

### Mechanism 1
The CTC-based draft model achieves higher acceptance rates because it models inter-token dependencies through sequence-level probability computation, unlike independent token prediction in standard non-autoregressive methods. CTC expands the output space with blank and repeated tokens, computing probabilities over all valid alignments. This encourages the draft model to generate sequences with better sequential coherence.

### Mechanism 2
The CTC-based approach reduces base model inference steps by generating longer valid token sequences in parallel. By improving draft quality through dependency modeling, more tokens are accepted per decoding step, reducing the number of sequential base model verifications needed.

### Mechanism 3
Knowledge distillation from the base model during training improves draft model's ability to mimic base model behavior. The base model generates distilled labels through greedy decoding, which the CTC-drafter learns to reproduce, creating better alignment between draft and base model predictions.

## Foundational Learning

- **Concept**: Connectionist Temporal Classification (CTC) algorithm and dynamic programming for sequence probability computation
  - Why needed here: CTC enables the draft model to learn inter-token dependencies without requiring explicit alignment between input and output sequences
  - Quick check question: How does CTC handle variable-length outputs without alignment during training?

- **Concept**: Speculative decoding framework with draft-then-verify paradigm
  - Why needed here: Understanding how draft model quality directly impacts overall inference speed through acceptance rates
  - Quick check question: What happens to inference speed when draft model acceptance rate drops below 50%?

- **Concept**: Knowledge distillation techniques in language model training
  - Why needed here: The draft model learns from base model outputs rather than ground truth, requiring understanding of distillation objectives
  - Quick check question: Why might using base model predictions as training targets be preferable to using original ground truth?

## Architecture Onboarding

- **Component map**: Input → Base LLM embedding → Base LLM layers → Hidden states → Attention Draft Module → Draft probabilities → Top-k selection → Candidate sequence generation → CTC Transform → Modified attention map → Base LLM verification → Token acceptance/rejection

- **Critical path**: 1) Input → Base LLM embedding → Base LLM layers → Hidden states; 2) Hidden states → Attention Draft Module → Draft probabilities; 3) Draft probabilities → Top-k selection → Candidate sequence generation; 4) Candidate sequences → CTC Transform → Modified attention map; 5) Modified attention map → Base LLM verification → Token acceptance/rejection

- **Design tradeoffs**: Draft model complexity vs. draft time (more complex draft models improve quality but increase draft latency); Top-k size vs. candidate diversity (larger k increases computational overhead but may improve acceptance rates); CTC vs. cross-entropy loss (CTC better captures dependencies but requires more complex training)

- **Failure signatures**: Low acceptance rates despite high draft model quality (indicates poor alignment between draft and base model token distributions); Increased inference time with larger base models (suggests draft model cannot keep pace with base model complexity); Training instability or slow convergence (may indicate CTC loss computation issues or poor initialization)

- **First 3 experiments**: 1) Measure acceptance rate on MT-bench with different top-k values (k=5, 10, 20) to find optimal tradeoff; 2) Compare inference speed with and without CTC transform to quantify overhead; 3) Test draft model performance on out-of-domain data to assess generalization limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CTC-based draft model's performance vary with different lengths of draft sequences, and what is the optimal draft length for maximizing inference speed?
- Basis in paper: The paper mentions that the draft model generates draft tokens for the next several positions, but does not explore the impact of varying draft sequence lengths on performance.
- Why unresolved: The paper focuses on the effectiveness of the CTC-based approach but does not investigate how the length of the draft sequence affects the acceptance rate and overall speedup.
- What evidence would resolve it: Experimental results comparing the performance of CTC-drafter with different draft sequence lengths on various datasets and model sizes would clarify the optimal draft length.

### Open Question 2
- Question: Can the CTC-based draft model be effectively integrated with other speculative decoding strategies, such as nucleus sampling or beam search, to further enhance inference speed?
- Basis in paper: The paper discusses the use of CTC loss for training the draft model and mentions that different verification criteria like nucleus sampling could be integrated into the framework.
- Why unresolved: The paper does not explore the combination of CTC-based drafting with other speculative decoding strategies to determine if there are synergies that could improve performance.
- What evidence would resolve it: Experiments combining CTC-drafter with various speculative decoding strategies on different datasets and model types would show if integration leads to improved inference speed.

### Open Question 3
- Question: How does the CTC-based draft model's performance scale with increasing base model size, and what are the limitations in terms of model size and complexity?
- Basis in paper: The paper notes that the speedup performance of all speculation methods is influenced as the base model size increases, and it is challenging to bridge the capability gap for larger models like Vicuna-33B.
- Why unresolved: The paper does not provide a detailed analysis of how the CTC-based approach scales with larger models and what specific limitations arise.
- What evidence would resolve it: A comprehensive study comparing the performance of CTC-drafter across a range of base model sizes and complexities would identify the scalability limits and potential bottlenecks.

### Open Question 4
- Question: What are the trade-offs between the complexity of the draft model and the acceptance rate, and how can the model be optimized to balance these factors?
- Basis in paper: The paper mentions that the draft model usually has few parameters to produce generations at a faster speed, but it does not explore the trade-offs between model complexity and acceptance rate.
- Why unresolved: The paper focuses on improving acceptance rate through CTC-based drafting but does not investigate how varying the complexity of the draft model affects its performance.
- What evidence would resolve it: Experiments varying the complexity of the draft model (e.g., number of layers, attention heads) and measuring the impact on acceptance rate and inference speed would clarify the optimal balance.

## Limitations

- Limited architectural detail about the Attention Draft Module and CTC transform implementation creates uncertainty about reproducibility
- Evaluation scope constrained to English language tasks without testing on multilingual data, code generation, or specialized domains
- No thorough analysis of the computational overhead introduced by CTC transform and its scaling behavior with larger models

## Confidence

**High confidence** in the fundamental mechanism: The CTC-based approach for modeling inter-token dependencies is theoretically sound and the mathematical framework is clearly explained.

**Medium confidence** in performance claims: The 2.2-2.8x speedup and 3.4-3.6 tokens/acceptance rate improvements are supported by experimental results, but limited to specific model sizes and datasets.

**Low confidence** in generalization claims: The paper asserts strong performance across different model sizes and types, but experiments only cover Vicuna and LLaMA-2-Chat variants.

## Next Checks

1. **Ablation study on CTC transform overhead**: Measure the exact computational cost of the CTC transform module and its impact on overall inference time. Compare inference speed with CTC transform disabled versus enabled to quantify the performance tradeoff.

2. **Cross-domain generalization test**: Evaluate the CTC-drafter on non-English datasets (e.g., multilingual benchmarks) and specialized domains (code generation, medical text) to verify the claimed generalization across different types of language patterns and dependencies.

3. **Scalability analysis with larger models**: Test the CTC-based approach on significantly larger base models (70B+ parameters) and longer sequence lengths to determine whether the performance benefits scale linearly or if new bottlenecks emerge.