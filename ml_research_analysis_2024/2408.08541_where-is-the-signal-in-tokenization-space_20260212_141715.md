---
ver: rpa2
title: Where is the signal in tokenization space?
arxiv_id: '2408.08541'
source_url: https://arxiv.org/abs/2408.08541
tags:
- probability
- tokenization
- tokenizations
- canonical
- string
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the space of possible tokenizations for text
  under autoregressive language models. It proves that both finding the most likely
  tokenization and computing the marginal probability over all tokenizations are computationally
  hard (NP-hard and P-hard respectively).
---

# Where is the signal in tokenization space?
## Quick Facts
- arXiv ID: 2408.08541
- Source URL: https://arxiv.org/abs/2408.08541
- Reference count: 40
- Primary result: Non-canonical tokenizations improve QA accuracy by 1-6% via mixture modeling

## Executive Summary
This paper investigates the space of possible tokenizations for text under autoregressive language models, proving that finding the most likely tokenization and computing marginal probabilities over all tokenizations are computationally hard problems. Despite theoretical complexity, the canonical tokenization typically dominates probability mass in practice. However, non-canonical tokenizations contain meaningful signal that, when aggregated via importance sampling and weighted mixture models, yields consistent accuracy improvements on question-answering benchmarks across multiple architectures.

## Method Summary
The authors prove computational hardness for optimal tokenization search and marginal probability computation, then empirically investigate whether non-canonical tokenizations contain signal beyond the canonical tokenization. They implement importance sampling with a 1-step look-ahead proposal distribution to estimate marginal probabilities, and evaluate a mixture model approach that combines canonical and non-canonical tokenizations weighted by parameter α. The method is tested on three QA benchmarks (Hellaswag, Social IQA, OpenBookQA) using three LLM architectures (Gemma, Llama2, Mamba).

## Key Results
- Finding the most likely tokenization is NP-hard and computing marginal probability is #P-hard
- Canonical tokenization typically has overwhelming probability mass and often equals the marginal
- Non-canonical tokenizations improve QA accuracy by 1-6% when aggregated via mixture modeling

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Non-canonical tokenizations carry meaningful signal for downstream tasks even when canonical dominates probability mass.
- Mechanism: The mixture model weights canonical and non-canonical tokenizations differently, allowing the model to capture compositional or contextual nuances that the canonical tokenization misses.
- Core assumption: The LLM's internal representations are sensitive to subword boundaries and can exploit subtle differences in tokenization structure.
- Evidence anchors:
  - [abstract]: "by employing ensemble strategies for weighting different tokenizations at inference time, we achieve significant performance improvements on challenging LLM evaluation benchmarks."
  - [section 6]: "we empirically find that, before convergence, there is a surprising increase in accuracy when weighting over non-canonical tokenizations compared to the canonical baseline."
  - [corpus]: Weak. Corpus shows related work on non-canonical tokenization but no direct evidence of performance gains from mixture weighting.

### Mechanism 2
- Claim: The canonical tokenization probability is practically equivalent to the marginal probability over all tokenizations for most strings.
- Mechanism: Most probability mass concentrates on the canonical tokenization, making the sum over all other tokenizations negligible in comparison.
- Core assumption: The LLM's conditional probability distribution strongly favors the canonical tokenization path due to training data and tokenizer design.
- Evidence anchors:
  - [abstract]: "we empirically find that the estimated marginal probability is usually very close to the canonical tokenization's probability."
  - [section 4]: "we find that branch-and-bound always returns the canonical tokenization as the best candidate, despite the exponential number of possible candidates."
  - [section 5]: "for most cases, the approximate marginal also converges close to the canonical probability."
  - [corpus]: Weak. Corpus neighbors discuss tokenization multiplicity but do not directly address probability concentration.

### Mechanism 3
- Claim: Finding the most likely tokenization is computationally hard (NP-complete), but the canonical tokenization is typically the most likely.
- Mechanism: The hardness result stems from encoding Boolean satisfiability problems into tokenization probability maximization, but empirical evidence shows canonical dominates in practice.
- Core assumption: The autoregressive LLM's probability distribution can encode complex dependencies that make optimal tokenization search equivalent to NP-complete problems.
- Evidence anchors:
  - [abstract]: "We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations."
  - [section 4]: "Theorem 4.2. The most-likely tokenization problem is NP-complete."
  - [section 4]: "we find that branch-and-bound always returns the canonical tokenization as the best candidate."
  - [corpus]: Weak. No corpus evidence directly supporting the NP-completeness claim.

## Foundational Learning
- Concept: Autoregressive language model probability computation
  - Why needed here: Understanding how token sequence probabilities are computed is essential for grasping why tokenization affects downstream performance.
  - Quick check question: How does an autoregressive model compute P(v|x) for a token sequence v and string x?

- Concept: Tokenization multiplicity and its computational implications
  - Why needed here: The exponential growth of tokenization options creates both theoretical complexity and practical considerations for model evaluation.
  - Quick check question: Why does the number of possible tokenizations grow exponentially with string length?

- Concept: Importance sampling for probability estimation
  - Why needed here: The paper uses importance sampling to approximate marginal probabilities when exact computation is intractable.
  - Quick check question: What is the purpose of using a proposal distribution q(v|x) in importance sampling?

## Architecture Onboarding
- Component map:
  Tokenizer (BPE with merge table) -> Autoregressive LLM (Gemma, Llama2, Mamba) -> Marginal probability estimator (importance sampling) -> Mixture model classifier (α-weighted combination) -> Evaluation framework (HELLASWAG, SOCIAL IQA, OPEN BOOK QA)

- Critical path:
  1. Tokenize input string using BPE to get canonical tokenization
  2. Generate multiple non-canonical tokenizations using MDD traversal
  3. Compute probabilities for each tokenization using LLM
  4. Aggregate probabilities using mixture weighting (α parameter)
  5. Select answer with highest aggregated probability

- Design tradeoffs:
  - Computational cost vs. accuracy improvement: More non-canonical samples improve accuracy but increase inference time
  - α parameter tuning: Balancing canonical vs. non-canonical influence requires validation set tuning
  - Model architecture choice: Different architectures (transformers vs. state space models) may respond differently to tokenization variations

- Failure signatures:
  - Marginal probability estimates diverging significantly from canonical probability (indicates sampling issues)
  - Performance degradation when increasing non-canonical samples (suggests noise overwhelms signal)
  - α parameter tuning yielding extreme values (0 or 1) across datasets (suggests limited benefit from non-canonical tokenizations)

- First 3 experiments:
  1. Verify canonical tokenization probability concentration by computing exact marginals for short strings
  2. Implement importance sampling with 1-step look-ahead proposal and test convergence on validation set
  3. Run mixture model experiments with varying α values on a small subset of HELLASWAG to observe performance trends

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the exact mechanism by which non-canonical tokenizations improve downstream task performance, beyond the observed improvement in accuracy?
- Basis in paper: [explicit] The paper observes that even when the marginal probability is close to the canonical probability, non-canonical tokenizations still contribute to improved accuracy on question-answering benchmarks. However, the paper does not provide a detailed explanation of why this is the case.
- Why unresolved: The paper focuses on the empirical observation of improvement but does not delve into the underlying reasons for this phenomenon. The paper mentions that non-canonical tokenizations seem to carry some meaningful signal, but does not explore what this signal is or how it translates to improved performance.
- What evidence would resolve it: Further experiments that isolate and analyze the contributions of specific non-canonical tokenizations to the overall performance improvement. This could involve techniques like ablation studies, where different subsets of non-canonical tokenizations are systematically included or excluded to observe their individual impact. Additionally, analyzing the specific tokenizations that contribute most to the improvement could provide insights into the nature of the signal they carry.

### Open Question 2
- Question: Can the computational hardness of finding the most likely tokenization and computing the marginal probability be leveraged to develop new, more efficient algorithms for these tasks?
- Basis in paper: [explicit] The paper proves that both finding the most likely tokenization and computing the marginal probability are computationally hard problems (NP-hard and #P-hard, respectively). It then presents a branch-and-bound algorithm for approximating the most likely tokenization, but notes that it quickly becomes intractable for longer strings.
- Why unresolved: The paper acknowledges the computational challenges but does not explore potential algorithmic approaches to overcome them. The hardness results provide a theoretical foundation for understanding the limitations of current methods, but they also open up the possibility of developing new algorithms that exploit the specific structure of the problems.
- What evidence would resolve it: Development and evaluation of new algorithms that can efficiently approximate the most likely tokenization and marginal probability, potentially by exploiting the structure of the problems or by using different computational paradigms. These algorithms should be compared against the current state-of-the-art methods in terms of accuracy and computational efficiency.

### Open Question 3
- Question: How does the choice of tokenizer and vocabulary affect the distribution of non-canonical tokenizations and their impact on downstream task performance?
- Basis in paper: [explicit] The paper uses a specific tokenizer (Byte-Pair Encoding) and vocabulary size, but does not explore how different choices might affect the results. The paper mentions that some models are trained with stochastic tokenizations, but does not investigate how this affects the distribution of non-canonical tokenizations.
- Why unresolved: The paper's findings are based on a specific set of tokenization and vocabulary choices. It is unclear whether the observed improvements from non-canonical tokenizations would generalize to other tokenization schemes or vocabulary sizes. Additionally, the impact of training with stochastic tokenizations on the distribution of non-canonical tokenizations and their downstream performance is not explored.
- What evidence would resolve it: Experiments that systematically vary the tokenizer, vocabulary size, and training procedure to observe their effects on the distribution of non-canonical tokenizations and their impact on downstream task performance. This could involve comparing different tokenization algorithms, vocabulary sizes, and training regimes to identify the factors that most strongly influence the usefulness of non-canonical tokenizations.

## Limitations
- The theoretical hardness results create tension with empirical findings showing canonical tokenization dominates probability mass
- Improvements depend heavily on careful hyperparameter tuning (α parameter and sample count) that may not generalize
- The underlying mechanism for why non-canonical tokenizations improve performance remains unclear

## Confidence
**Computational Complexity Claims:** High confidence - The NP-hardness and #P-hardness proofs follow standard complexity-theoretic techniques, and the empirical results showing canonical dominance are robust across tested models.

**Performance Improvements:** Medium confidence - The accuracy gains are consistent and statistically significant, but depend heavily on hyperparameter tuning and may not generalize to all tasks or model families.

**Practical Utility:** Low confidence - While the method works on tested benchmarks, its real-world applicability depends on factors not fully explored in the paper (computational overhead, robustness to domain shift, interaction with other optimization techniques).

## Next Checks
1. **Ablation on α Parameter Sensitivity:** Systematically vary α across its full range (0 to 1) on a held-out validation set to determine if improvements are robust to parameter choice or require precise tuning. This would clarify whether the benefits are stable or fragile.

2. **Cross-Architecture Transferability:** Test the approach on additional model architectures beyond the three studied (including encoder-decoder models and smaller models) to assess whether improvements generalize across the broader LLM landscape or are architecture-specific.

3. **Alternative Tokenization Generation Methods:** Compare the current MDD-based non-canonical tokenization generation with simpler approaches like BPE dropout or stochastic tokenization to determine if the specific method matters or if any source of tokenization diversity provides similar benefits.