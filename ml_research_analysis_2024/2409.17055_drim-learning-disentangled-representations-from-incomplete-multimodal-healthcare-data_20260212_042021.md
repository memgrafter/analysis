---
ver: rpa2
title: 'DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare
  Data'
arxiv_id: '2409.17055'
source_url: https://arxiv.org/abs/2409.17055
tags:
- representations
- data
- modalities
- shared
- survival
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DRIM, a multimodal method for learning disentangled
  representations from incomplete healthcare data. The approach assumes that each
  modality contains both shared patient-related information and modality-specific
  details.
---

# DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data

## Quick Facts
- arXiv ID: 2409.17055
- Source URL: https://arxiv.org/abs/2409.17055
- Reference count: 40
- DRIM-Surv outperforms state-of-the-art methods on glioma survival prediction with C-index: 0.774, IBS: 0.086, INBLL: 0.285

## Executive Summary
This paper introduces DRIM, a multimodal method for learning disentangled representations from incomplete healthcare data. The approach assumes that each modality contains both shared patient-related information and modality-specific details. DRIM uses separate encoders for shared and unique components, optimizing them to maximize shared information while minimizing overlap between components. A dual-scale fusion approach combines these representations, using attention-based fusion that handles missing modalities naturally. Experiments on glioma patient survival prediction show DRIM-Surv outperforms state-of-the-art methods across multiple metrics, demonstrating superior robustness to missing modalities and improved scalability compared to tensor-based fusion methods.

## Method Summary
DRIM learns disentangled representations by encoding each modality into two components: shared (patient-related information common across modalities) and unique (modality-specific details). The model uses separate encoders for each component type, with the shared encoders trained to maximize mutual information across modalities while unique encoders minimize mutual information with their corresponding shared components. A dual-scale fusion approach first combines shared representations into a global vector, then fuses this with unique representations using a masked transformer that naturally handles missing modalities. The model is trained with three losses: shared loss (InfoNCE-based), unique loss (adversarial), and task-specific loss (survival prediction). An unsupervised variant, DRIM-U, learns representations without task supervision that can be fine-tuned later.

## Key Results
- DRIM-Surv achieves C-index of 0.774, IBS of 0.086, and INBLL of 0.285 on glioma survival prediction
- DRIM-U learns unsupervised representations that, when fine-tuned, outperform other unsupervised baselines
- DRIM maintains constant parameter count when adding new modalities, unlike tensor-based methods that see exponential growth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling shared and unique representations improves performance on multimodal healthcare tasks with missing data.
- Mechanism: The model uses separate encoders for shared (patient-related) and unique (modality-specific) components. The shared encoder maximizes mutual information across modalities while the unique encoder minimizes overlap with the shared component. This allows the model to capture complementary information from different modalities.
- Core assumption: Each modality contains both shared patient-related information and modality-specific details that are both relevant to downstream tasks.
- Evidence anchors:
  - [abstract]: "given a set of modalities, we aim to encode a representation for each one that can be divided into two components: one encapsulating patient-related information common across modalities and the other, encapsulating modality-specific details"
  - [section]: "we make the natural assumption that information contained in a modality splits into patient-related aspects shared across modalities and unique features specific to each modality"
- Break condition: If the core assumption fails (modalities don't contain both shared and unique information), the disentanglement approach would not provide benefits over simpler fusion methods.

### Mechanism 2
- Claim: The dual-scale fusion approach with attention-based fusion handles missing modalities naturally and scales better than tensor-based methods.
- Mechanism: First, shared representations are fused into a global shared vector, then combined with unique representations using a masked transformer that can handle missing modalities through masking. This approach avoids the exponential parameter growth of tensor-based methods.
- Core assumption: Attention-based fusion can effectively model interactions between representations while naturally handling missing modalities.
- Evidence anchors:
  - [abstract]: "attention-based fusion that handles missing modalities naturally"
  - [section]: "This fusion, termed MAFusion, offers scalability, computational efficiency, robust handling of missing modalities through masking and provides interpretability via attention maps"
- Break condition: If attention-based fusion cannot effectively model the complex interactions between modalities, performance may degrade compared to more sophisticated fusion methods.

### Mechanism 3
- Claim: The adversarial training of unique representations ensures they capture modality-specific information relevant to the task.
- Mechanism: A discriminator is trained to distinguish between representations from the joint distribution and those from the product of marginals (created by shuffling). This encourages the unique encoder to capture information that cannot be inferred from other modalities.
- Core assumption: Minimizing mutual information between shared and unique representations leads to capturing truly modality-specific information.
- Evidence anchors:
  - [section]: "we use an adversarial objective... to minimize the mutual information I(sm, um)"
  - [section]: "This adversarial objective outlined in Eq. 3 drives the minimization of the Jensen-Shannon divergence DJS (PS,U ∥PSPU))"
- Break condition: If the adversarial objective fails to effectively minimize mutual information, the unique representations may become redundant with the shared representations.

## Foundational Learning

- Concept: Mutual information maximization and minimization
  - Why needed here: The shared loss maximizes mutual information between representations from different modalities, while the unique loss minimizes mutual information between shared and unique components within each modality.
  - Quick check question: What is the difference between maximizing mutual information between shared representations and minimizing mutual information between shared and unique components?

- Concept: Adversarial training for representation disentanglement
  - Why needed here: The unique representations are trained using an adversarial objective to ensure they capture information specific to each modality that cannot be inferred from other modalities.
  - Quick check question: How does the discriminator distinguish between samples from the joint distribution and the product of marginals?

- Concept: Attention mechanisms for multimodal fusion
  - Why needed here: The MAFusion approach uses attention to aggregate shared and unique representations while naturally handling missing modalities through masking.
  - Quick check question: How does the masked transformer handle missing modalities during inference?

## Architecture Onboarding

- Component map:
  - Shared encoders (ϕm) -> Unique encoders (ξm) -> Discriminators (Dm) -> MAFusion -> Task-specific heads

- Critical path:
  1. Encode each modality into shared and unique representations
  2. Train shared representations to maximize mutual information across modalities
  3. Train unique representations to minimize mutual information with shared components
  4. Fuse representations using MAFusion
  5. Apply task-specific loss

- Design tradeoffs:
  - Separate encoders vs single encoder: More parameters but better disentanglement
  - Adversarial training vs direct minimization: More stable training but additional complexity
  - Attention-based fusion vs tensor-based: Better scalability but potentially less expressive

- Failure signatures:
  - Poor disentanglement: Shared and unique representations still highly correlated
  - Overfitting: High performance on training data but poor generalization
  - Mode collapse: Unique representations become meaningless or identical across samples

- First 3 experiments:
  1. Train with only shared encoders (no unique components) to establish baseline
  2. Train with adversarial unique encoders but no fusion to verify disentanglement
  3. Train full model with small subset of modalities to verify MAFusion functionality

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis, several important questions remain:

1. How does DRIM's disentanglement of shared and unique components compare to traditional multimodal fusion methods in terms of computational efficiency and scalability when adding new modalities?
2. How does the performance of DRIM-U's unsupervised representations compare to fully supervised approaches when fine-tuned on limited labeled data?
3. What specific types of patient-related information are captured in the shared representations versus modality-specific information in the unique representations?

## Limitations
- Evaluation limited to a single dataset (TCGA glioma data), raising generalizability concerns
- Model's scalability to many modalities remains unproven with only four modalities tested
- Computational cost of adversarial training for unique representations not fully characterized
- Method's robustness to high missingness rates (>50%) not thoroughly tested

## Confidence
- **High confidence**: The core mechanism of disentangling shared and unique representations is well-supported by the theoretical framework and ablation studies.
- **Medium confidence**: The superiority of DRIM-Surv over state-of-the-art methods is demonstrated, but limited to one dataset and specific task.
- **Medium confidence**: The scalability advantage over tensor-based methods is theoretically sound but requires more extensive empirical validation.

## Next Checks
1. **Cross-domain validation**: Test DRIM on at least two additional medical datasets with different modalities (e.g., electronic health records + imaging, or genomics + clinical data) to assess generalizability.
2. **Scalability testing**: Evaluate performance and computational efficiency with 8-10 modalities to verify claims about superior scalability compared to tensor-based methods.
3. **Missingness robustness**: Systematically vary missing modality rates from 0% to 80% to determine DRIM's performance limits and compare with baseline methods across the full spectrum.