---
ver: rpa2
title: Reasoning Robustness of LLMs to Adversarial Typographical Errors
arxiv_id: '2411.05345'
source_url: https://arxiv.org/abs/2411.05345
tags:
- errors
- adversarial
- llms
- reasoning
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how typographical errors in user queries affect
  the reasoning robustness of large language models (LLMs) using Chain-of-Thought
  prompting. The authors propose an Adversarial Typo Attack (ATA) algorithm that iteratively
  introduces imperceptible typing mistakes into important words to cause incorrect
  reasoning outputs.
---

# Reasoning Robustness of LLMs to Adversarial Typographical Errors

## Quick Facts
- **arXiv ID**: 2411.05345
- **Source URL**: https://arxiv.org/abs/2411.05345
- **Reference count**: 13
- **Primary result**: Typographical errors significantly degrade LLM reasoning performance, with minimal edits causing accuracy drops of 5-9% across multiple models

## Executive Summary
This paper investigates how adversarial typographical errors in user queries affect the reasoning robustness of large language models (LLMs) using Chain-of-Thought prompting. The authors introduce the Adversarial Typo Attack (ATA) algorithm, which iteratively introduces imperceptible typing mistakes into important words to cause incorrect reasoning outputs. The study demonstrates that even minimal adversarial edits significantly degrade model performance across multiple datasets (GSM8K, BBH, MMLU) and models, with performance drops ranging from 5.0% to 9.0% on average. The research highlights that LLMs are particularly vulnerable to whitespace errors and edits targeting content-bearing words like nouns, verbs, and adjectives.

## Method Summary
The study employs the Adversarial Typo Attack (ATA) algorithm that iteratively samples typos for words identified as important to the query using gradient-based importance scores. The algorithm applies four types of typographical errors: proximity errors, double-typing, omission, and extra whitespace. The attack targets a specific response set ("Sorry, I'm unable to answer the question") and evaluates performance degradation across multiple reasoning datasets. The R²ATA benchmark is developed to evaluate reasoning robustness, demonstrating remarkable transferability and causing notable performance drops across multiple super large and closed-source LLMs.

## Key Results
- Mistral-7B-Instruct accuracy drops from 43.7% to 38.6% on GSM8K with just 1 character edit, and to 19.2% with 8 edits
- ChatGPT shows an average absolute reduction of 6.5% in accuracy across datasets when attacked
- Mixtral-8×7B experiences an average absolute reduction of 6.7% in accuracy
- Whitespace errors are the most frequent and effective attack type, highlighting LLMs' vulnerability to spacing perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial typo attack exploits the model's sensitivity to input perturbations by introducing imperceptible typographical errors that disrupt the internal reasoning process
- Mechanism: ATA iteratively selects important words using gradient-based importance scores, then applies typographical errors to create adversarial examples causing incorrect reasoning outputs
- Core assumption: The model's reasoning chain is fragile and can be broken by minor input perturbations that humans would easily overlook
- Evidence anchors: "With just 1 character edit, Mistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K"

### Mechanism 2
- Claim: Transferability of adversarial prompts across different model sizes and architectures demonstrates fundamental vulnerabilities in LLM reasoning
- Mechanism: Adversarial examples crafted for smaller models successfully transfer to larger models without requiring access to target model parameters
- Core assumption: Similar reasoning vulnerabilities exist across different LLM architectures and sizes
- Evidence anchors: "R2ATA demonstrates remarkable transferability and causes notable performance drops across multiple super large and closed-source LLMs"

### Mechanism 3
- Claim: The effectiveness of whitespace errors highlights a fundamental weakness in how LLMs process token boundaries and spacing
- Mechanism: ATA shows whitespace errors are most frequent and effective, suggesting LLMs are particularly vulnerable to spacing perturbations
- Core assumption: LLMs are trained on data with consistent spacing patterns, making them sensitive to spacing variations
- Evidence anchors: "The predominance of the whitespace error operation adopted by ATA highlights its significance in exploiting model vulnerabilities"

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) prompting**
  - Why needed here: The paper studies how typos affect CoT reasoning specifically, which breaks down complex problems into intermediate steps
  - Quick check question: What is the key difference between standard prompting and Chain-of-Thought prompting in terms of model reasoning?

- **Concept: Adversarial machine learning**
  - Why needed here: The paper applies adversarial attack principles to natural language inputs through imperceptible typographical errors
  - Quick check question: How does the ATA algorithm differ from traditional adversarial attacks that modify pixel values or word embeddings?

- **Concept: Gradient-based feature importance**
  - Why needed here: ATA uses gradients to identify which words in the input are most important for the model's prediction
  - Quick check question: What does the gradient ∇L(Q1:n) represent in the context of identifying important words for the adversarial attack?

## Architecture Onboarding

- **Component map**: Input preprocessing → Question tokenization → Gradient computation → Word selection → Typo sampling → Loss evaluation → Adversarial example generation → Model evaluation
- **Critical path**: Question → Gradient importance → Word selection → Typo application → Loss evaluation → Adversarial example generation → Model evaluation
- **Design tradeoffs**: The attack prioritizes imperceptibility over maximum effectiveness, limiting modifications to realistic typing errors rather than arbitrary character changes
- **Failure signatures**: Reduced attack effectiveness when models show robustness to spacing variations, or when adversarial examples become detectable to human evaluators
- **First 3 experiments**:
  1. Test ATA on a small subset of GSM8K questions with 1-2 edits to verify the basic attack mechanism
  2. Compare ATA effectiveness against random typo generation as a baseline
  3. Evaluate transferability by testing Mistral-7B-generated adversarial examples on Vicuna-13B

## Open Questions the Paper Calls Out
None

## Limitations
- The attack focuses on a limited set of four typo types and may not capture all possible typographical errors that could affect LLM reasoning
- The evaluation primarily focuses on mathematical and multiple-choice reasoning tasks, potentially limiting generalizability to other domains
- The claim about attacks being "imperceptible to humans" is based on subjective assessment rather than user studies

## Confidence
*High Confidence (7/10)*: The observed performance degradation on GSM8K and BBH datasets is well-supported by quantitative evidence showing consistent accuracy drops across multiple edit counts.

*Medium Confidence (5/10)*: The claim about whitespace errors being the most effective attack type is supported by statistics but may be influenced by specific implementation choices.

*Low Confidence (3/10)*: The assertion that these attacks are "imperceptible to humans" lacks empirical user study validation.

## Next Checks
1. **Cross-task validation**: Test ATA effectiveness on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to verify generalizability across different reasoning domains.

2. **Human evaluation study**: Conduct a controlled study with human annotators to measure the actual perceptibility of adversarial typos and determine the threshold at which typos become noticeable.

3. **Attack diversification test**: Implement additional typo types (homophone substitutions, grammatical errors) and compare their effectiveness against the current four types to determine if identified vulnerabilities are comprehensive.