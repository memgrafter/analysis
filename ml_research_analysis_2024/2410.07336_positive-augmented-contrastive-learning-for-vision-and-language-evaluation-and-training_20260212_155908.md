---
ver: rpa2
title: Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation
  and Training
arxiv_id: '2410.07336'
source_url: https://arxiv.org/abs/2410.07336
tags:
- pac-s
- captioning
- clip-s
- image
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAC-S++, a learnable metric for evaluating
  image and video captions that improves alignment with human judgment compared to
  existing metrics. The key innovation is positive-augmented contrastive learning,
  where the CLIP embedding space is fine-tuned using synthetic positive samples generated
  by text-to-image and image-to-text models alongside real image-caption pairs from
  cleaned data.
---

# Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training

## Quick Facts
- arXiv ID: 2410.07336
- Source URL: https://arxiv.org/abs/2410.07336
- Reference count: 21
- Primary result: PAC-S++ achieves 3.3 higher Kendall τc correlation on Flickr8k-Expert than CLIP-S

## Executive Summary
This paper introduces PAC-S++, a learnable metric for evaluating image and video captions that improves alignment with human judgment compared to existing metrics. The key innovation is positive-augmented contrastive learning, where the CLIP embedding space is fine-tuned using synthetic positive samples generated by text-to-image and image-to-text models alongside real image-caption pairs from cleaned data. Low-rank adaptation (LoRA) is used to preserve pre-trained model weights while injecting trainable rank decomposition matrices. Extensive experiments on diverse benchmarks demonstrate that PAC-S++ achieves higher correlation with human judgment and better sensitivity to object hallucinations than popular metrics like CLIP-S.

## Method Summary
PAC-S++ fine-tunes CLIP using positive-augmented contrastive learning with LoRA. The method generates synthetic image-caption pairs using Stable Diffusion and BLIP, treating them as additional positive samples in the contrastive loss alongside real pairs. The LoRA adapters (rank=4) are injected into each layer of CLIP's visual and textual encoders, allowing efficient fine-tuning while preserving pre-trained knowledge. The resulting metric can be used for reference-free or reference-based caption evaluation, computing cosine similarity between image and caption embeddings. Additionally, PAC-S++ is used as a reward in the SCST fine-tuning stage of captioning models to generate semantically richer captions with fewer repetitions and grammatical errors.

## Key Results
- Achieves 3.3 higher Kendall τc correlation on Flickr8k-Expert compared to CLIP-S
- Demonstrates 13.6 accuracy on Pascal-50S and 81.3/73.1 accuracy on FOIL/ActivityNet-FOIL for hallucination detection
- Shows superior performance when used as RL reward, generating captions with fewer repetitions (Rep-4) and grammatical errors (%Incorrect)
- Outperforms recent supervised and architecturally enhanced metrics while maintaining simplicity and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positive-augmented contrastive learning improves metric quality by supplementing real image-caption pairs with synthetic positives.
- Mechanism: The method generates additional matching image-text pairs using a text-to-image model and an image captioning model. These synthetic pairs are treated as extra positive samples in the contrastive loss, effectively increasing the size of the positive set and providing cleaner supervision signals than the noisy web data used in CLIP pre-training.
- Core assumption: Synthetic image-text pairs generated by current generative models are sufficiently high quality to serve as reliable positive examples in contrastive learning.
- Evidence anchors:
  - [abstract] "leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples"
  - [section 3.2] "we augment it by generating a synthetic caption t′ from v using an image captioning model... Similarly, we generate a synthetic image v′ from t via a diffusion-based text-to-image architecture"
  - [corpus] Weak - no direct corpus evidence for quality of synthetic pairs as positives in contrastive learning
- Break condition: If synthetic pairs are noisy or hallucinate objects not present in the original image/text, they will introduce incorrect positive signals that harm the embedding alignment.

### Mechanism 2
- Claim: Low-rank adaptation (LoRA) fine-tuning preserves the benefits of CLIP pre-training while enabling effective metric specialization.
- Mechanism: LoRA injects trainable low-rank decomposition matrices into each layer of the CLIP architecture without modifying the pre-trained weights. This significantly reduces the number of trainable parameters, preventing overfitting to the relatively small cleaned dataset while allowing the model to adapt to the captioning evaluation task.
- Core assumption: The CLIP pre-trained weights contain valuable general visual-linguistic knowledge that should be preserved while only adapting the model for the captioning evaluation task.
- Evidence anchors:
  - [abstract] "we employ low-rank adaptation (LoRA) that can enhance the final performance while preserving the original advantages of the CLIP embedding space"
  - [section 3.2] "we employ LoRA which preserves the pre-trained model weights while injecting trainable rank decomposition matrices into each layer of the architecture"
  - [section 5.7] "comparing the results of the previous version of our metric (i.e. PAC-S), in which only the last visual and textual projections of the model are fine-tuned, the version of our metric with LoRA consistently outperforms the original version"
- Break condition: If the LoRA rank is too low, the model cannot adapt sufficiently to the task; if too high, it may overfit or lose the benefits of parameter efficiency.

### Mechanism 3
- Claim: Using the learned metric as a reward in SCST fine-tuning produces more semantically rich captions with fewer repetitions and grammatical errors.
- Mechanism: The PAC-S++ metric, which better aligns with human judgment than CLIP-S or CIDEr, is used as the reward signal in the second stage of SCST. This encourages the captioning model to generate captions that are both semantically rich and grammatically correct, avoiding the excessive length and repetition issues seen when using CLIP-S as reward.
- Core assumption: A metric that correlates well with human judgment will provide better guidance for generating human-like captions than metrics like CIDEr or CLIP-S.
- Evidence anchors:
  - [abstract] "we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models"
  - [section 4.2] "we propose using PAC-S++ to improve the training of image captioning models"
  - [section 5.8] "Although CLIP remains an excellent model for aligning bag-of-words with visual input, it disregards syntax and logical connections among words within captions. On the contrary, despite sharing the same architecture, our proposal mitigates this issue"
- Break condition: If the reward signal is too noisy or unstable, it may lead to poor convergence or generation of captions that optimize the metric but not human preferences.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The metric is built on a contrastive learning framework where positive image-caption pairs should be closer in embedding space than negative pairs. Understanding InfoNCE loss is essential for grasping how the metric learns.
  - Quick check question: In contrastive learning, what is the relationship between positive and negative pairs in the embedding space?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: The metric uses LoRA for efficient fine-tuning. Understanding how LoRA works is crucial for understanding the architecture and implementation.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Self-Critical Sequence Training (SCST) and reinforcement learning for NLP
  - Why needed here: The metric is used as a reward in SCST for caption generation. Understanding the reinforcement learning setup is essential for understanding the training pipeline.
  - Quick check question: In SCST, what is the role of the baseline value in the policy gradient update?

## Architecture Onboarding

- Component map: CLIP ViT-B/32/ViT-L/14 backbone -> LoRA adapters (injected into visual and textual encoders) -> Synthetic data generators (Stable Diffusion, BLIP) -> Evaluation components (reference-free/reference-based scoring) -> For RL training: encoder-decoder Transformer captioning model with CLIP visual features

- Critical path:
  1. Pre-train LoRA adapters using positive-augmented contrastive loss on COCO dataset
  2. For evaluation: compute cosine similarity between image and caption embeddings, apply scaling and harmonic mean with references
  3. For RL training: use PAC-S++ score as reward in SCST fine-tuning of captioning model

- Design tradeoffs:
  - LoRA rank (4 used): balances adaptation capacity vs. overfitting risk
  - Synthetic data quality vs. quantity: higher quality generators may be slower but provide better supervision
  - Reference-free vs. reference-based evaluation: reference-free is more general but reference-based aligns better with human judgment
  - CLIP backbone size (B/32 vs L/14): larger models perform better but are more computationally expensive

- Failure signatures:
  - Poor correlation with human judgment: indicates issues with synthetic data quality or LoRA adaptation
  - Hallucination sensitivity failures: suggests the metric isn't capturing fine-grained semantic differences
  - Repetitive captions when used as RL reward: indicates the reward signal may be encouraging verbosity over quality

- First 3 experiments:
  1. Ablation: train PAC-S++ with LoRA rank=2,4,8,16 and compare performance to identify optimal rank
  2. Ablation: remove synthetic data generation and compare to full PAC-S++ to quantify its contribution
  3. Comparison: evaluate PAC-S++ vs CLIP-S as reward in SCST and measure caption quality metrics (BLEU, METEOR, CIDEr, SPICE, plus repetition and grammar measures)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PAC-S++ vary when trained with different text-to-image and image-to-text generative models beyond Stable Diffusion and BLIP?
- Basis in paper: [explicit] The paper evaluates the effect of varying synthetic data generators, comparing BLIP, LLaMA 3.2, IDEFICS-3, Stable Diffusion v3.5, and FLUX, finding that the original configuration remains most effective.
- Why unresolved: While the paper tests several alternatives, it does not explore the full space of available generative models or provide a systematic comparison across a wider range of architectures and training datasets.
- What evidence would resolve it: A comprehensive benchmark comparing PAC-S++ trained with a diverse set of state-of-the-art generative models, including newer or less common ones, across multiple evaluation datasets and metrics.

### Open Question 2
- Question: What is the impact of using different backbone architectures, such as SigLIP or SigLIP2, on the performance of PAC-S++ compared to CLIP, and how do these differences manifest across various captioning tasks?
- Basis in paper: [explicit] The paper investigates the effect of changing the backbone architecture, comparing CLIP with SigLIP and SigLIP2, and finds that PAC-S++ with SigLIP backbones achieves substantial improvements over CLIP-S.
- Why unresolved: The analysis is limited to specific datasets and does not explore the full range of potential tasks or the underlying reasons for performance differences between backbones.
- What evidence would resolve it: A detailed study examining PAC-S++ performance with various backbones across a broader set of captioning tasks, including detailed analysis of the strengths and weaknesses of each backbone.

### Open Question 3
- Question: How does the fine-tuning strategy using PAC-S++ as a reward in the SCST stage affect the grammatical correctness and semantic richness of generated captions compared to other reward signals like CIDEr or CLIP-S?
- Basis in paper: [explicit] The paper demonstrates that using PAC-S++ as a reward in the SCST stage leads to semantically richer captions with fewer repetitions and grammatical errors compared to CIDEr or CLIP-S.
- Why unresolved: The analysis focuses on specific metrics and does not provide a comprehensive comparison with other potential reward signals or explore the long-term effects of different fine-tuning strategies on caption quality.
- What evidence would resolve it: An extensive evaluation comparing PAC-S++ with a variety of reward signals across multiple captioning tasks, including long-term studies on caption quality and model generalization.

## Limitations
- The quality of synthetic positive samples depends heavily on the performance of generative models, which may hallucinate objects or create semantically inconsistent pairs
- The LoRA rank of 4 was chosen empirically without extensive ablation, and optimal rank may vary by task
- The evaluation focuses primarily on COCO-based datasets, with limited testing on truly out-of-domain scenarios

## Confidence
- **High confidence**: The improvement in Kendall τ correlation with human judgment (3.3 points on Flickr8k-Expert) is well-supported by experimental results across multiple benchmarks
- **Medium confidence**: The superiority of PAC-S++ over CLIP-S as an RL reward is demonstrated, but the effect may be partially due to architectural differences in the captioning model rather than the metric alone
- **Medium confidence**: The hallucination sensitivity improvements are demonstrated on FOIL and ActivityNet-FOIL, but these datasets may not fully capture all types of semantic errors

## Next Checks
1. **Synthetic data quality analysis**: Systematically evaluate the quality of generated synthetic pairs by measuring their semantic consistency with real pairs and their impact on downstream metric performance when using different quality thresholds
2. **LoRA rank ablation study**: Conduct a comprehensive ablation study varying the LoRA rank (2, 4, 8, 16) to identify the optimal balance between adaptation capacity and overfitting risk across different tasks
3. **Cross-dataset generalization test**: Evaluate PAC-S++ fine-tuned only on COCO vs fine-tuned on multiple datasets (COCO, Flickr30k, etc.) to quantify the impact of training data diversity on out-of-domain performance