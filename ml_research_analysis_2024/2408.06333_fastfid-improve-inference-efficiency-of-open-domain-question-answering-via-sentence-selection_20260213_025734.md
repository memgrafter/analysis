---
ver: rpa2
title: 'FastFiD: Improve Inference Efficiency of Open Domain Question Answering via
  Sentence Selection'
arxiv_id: '2408.06333'
source_url: https://arxiv.org/abs/2408.06333
tags:
- sentences
- fastfid
- passages
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FastFiD improves the inference efficiency of open-domain question
  answering by selecting key sentences from retrieved passages, reducing context length
  while maintaining performance. It uses a two-stage training process: multi-task
  training for sentence selection and answer generation, followed by fine-tuning to
  focus on selected sentences.'
---

# FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection

## Quick Facts
- **arXiv ID:** 2408.06333
- **Source URL:** https://arxiv.org/abs/2408.06333
- **Reference count:** 30
- **Key outcome:** FastFiD improves ODQA inference efficiency by 2.3X-5.7X through sentence selection while maintaining performance

## Executive Summary
FastFiD addresses the computational inefficiency of end-to-end FiD models in open-domain question answering by selecting only the most valuable sentences from retrieved passages before decoding. The method uses a two-stage training approach: first jointly training sentence selection and answer generation, then fine-tuning to generate answers using only selected sentences. Experiments on NQ, TriviaQA, and ASQA datasets demonstrate significant speedup improvements while maintaining comparable accuracy to baseline FiD models. The approach is effective across different model sizes and can be applied to decoder-only architectures.

## Method Summary
FastFiD improves ODQA inference efficiency by reducing context length through selective sentence processing. The method encodes each retrieved passage independently, then uses a sentence selection head to identify and retain only the most valuable sentences for the decoder. This two-stage approach first jointly trains sentence selection and answer generation, then fine-tunes to generate answers using only the selected sentences. The sentence selection is based on start/end span prediction, allowing the model to focus on the most relevant information while dramatically reducing the number of tokens the decoder must attend to during generation.

## Key Results
- Achieves 2.3X-5.7X inference speedup on NQ, TriviaQA, and ASQA datasets
- Maintains comparable or better accuracy than baseline FiD models
- Sentence selection outperforms passage reranking, with higher information density in selected sentences
- Effective across different model sizes (T5-base and T5-large variants)

## Why This Works (Mechanism)

### Mechanism 1
FastFiD improves inference efficiency by reducing context length through sentence selection after encoding, while maintaining performance. The model first encodes each retrieved passage independently, then uses a sentence selection head to identify and retain only the most valuable sentences for the decoder. This reduces the total number of tokens the decoder must attend to during generation. The selected sentences contain most of the information needed to generate the correct answer, while non-selected sentences contribute little to the final prediction. Cross-attention analysis shows that tokens from chosen sentences yield higher average attention scores compared to unchosen ones.

### Mechanism 2
Multi-task training enables the model to simultaneously learn sentence selection and answer generation without performance degradation. FastFiD uses a two-stage training approach: first jointly optimizing a sentence selection loss and an answer generation loss, then fine-tuning the model to generate answers using only the selected sentences. This reduces the gap between training and inference context length. The multi-task objective does not create conflicting gradients, and the second stage effectively adapts the model to the reduced context length.

### Mechanism 3
Sentence selection is more effective than passage reranking for compressing information across multiple passages. FastFiD selects individual sentences from retrieved passages, while RerankFiD reorders entire passages. Sentence selection allows for higher information density since relevant sentences can be extracted from otherwise irrelevant passages. A passage containing the correct answer often includes many irrelevant sentences, so selecting individual sentences provides better compression than selecting whole passages.

## Foundational Learning

- **Concept: Dense Passage Retrieval**
  - Why needed here: FastFiD relies on DPR to retrieve relevant passages before sentence selection. Understanding how DPR works is essential to grasp the full pipeline.
  - Quick check question: What is the key difference between sparse retrieval (like TF-IDF) and dense retrieval in ODQA systems?

- **Concept: Encoder-Decoder Architecture (T5)**
  - Why needed here: FastFiD is built on the FiD framework using T5 as the base model. Understanding how encoder-decoder models work is crucial for understanding how context is processed and answers are generated.
  - Quick check question: In T5, what is the role of the encoder versus the decoder when processing multiple retrieved passages?

- **Concept: Cross-Attention in Transformer Decoders**
  - Why needed here: FastFiD analyzes cross-attention scores to identify which sentences contribute most to answer generation. Understanding cross-attention is key to understanding the selection mechanism.
  - Quick check question: How does cross-attention in the decoder allow the model to focus on relevant parts of the context during generation?

## Architecture Onboarding

- **Component map:** Retriever (DPR) -> Encoder -> Sentence Selection Head -> Decoder -> Answer

- **Critical path:**
  1. Question and passages → Encoder → Context embeddings
  2. Context embeddings → Sentence Selection Head → Selected sentences
  3. Selected sentences → Decoder → Generated answer

- **Design tradeoffs:**
  - Sentence selection vs. passage reranking: Sentence selection provides higher information density but requires more complex selection mechanisms
  - Number of selected sentences: More sentences improve accuracy but reduce speedup benefits
  - Two-stage training: Adds complexity but bridges the training-inference gap

- **Failure signatures:**
  - Low accuracy despite high speedup: Selection mechanism may be missing critical information
  - Similar speed to baseline: Sentence selection may not be effectively reducing context length
  - Training instability: Multi-task objective may be creating conflicting gradients

- **First 3 experiments:**
  1. Ablation study: Compare FastFiD with and without the second-stage training to verify its importance
  2. Sentence selection analysis: Examine cross-attention scores for selected vs. non-selected sentences to validate the selection mechanism
  3. Scaling study: Test FastFiD with different numbers of selected sentences to find the optimal tradeoff between accuracy and speedup

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of sentences to select for maximizing QA performance across different datasets? The paper shows that NQ performance plateaus after 200 sentences while TriviaQA performance continues to improve with more sentences (up to 800 tested). This relationship appears dataset-dependent and requires systematic testing of selected sentence counts across multiple datasets to identify performance plateaus.

### Open Question 2
How does FastFiD's sentence selection mechanism perform on queries where the answer is not present in retrieved passages? The paper mentions the method depends on correct answers being in retrieved passages but doesn't empirically test this scenario. Experiments on datasets with unanswerable questions or synthetic tests where passages are modified to remove correct answers would resolve this.

### Open Question 3
Can FastFiD's sentence selection approach be effectively combined with other efficiency techniques like adaptive computation or early stopping? While the paper mentions related work on adaptive computation and shows FastFiD works on decoder-only models, it doesn't explore combinations with other efficiency methods. Comparative experiments applying FastFiD alongside other efficiency methods would measure combined benefits.

## Limitations
- Sentence selection mechanism constraints: relies on start/end span prediction which may not capture all relevant information in complex questions
- Training complexity: two-stage training approach adds significant complexity and may not generalize well to all model architectures
- Dataset dependency: results validated primarily on English Wikipedia-based datasets with unknown performance on other knowledge sources or languages

## Confidence
- **High confidence:** The core claim that FastFiD achieves 2.3X-5.7X speedup while maintaining performance is well-supported by presented experiments across three datasets and two model sizes
- **Medium confidence:** The claim that sentence selection is more effective than passage reranking is supported by comparative results but lacks ablation studies on varying levels of passage relevance
- **Medium confidence:** The multi-task training approach effectively learning both sentence selection and answer generation is demonstrated empirically but specific implementation details are not fully specified

## Next Checks
1. **Cross-dataset generalization test:** Evaluate FastFiD on non-Wikipedia knowledge sources (e.g., scientific literature, news articles) to assess its effectiveness across different domains and writing styles.

2. **Attention score validation:** Conduct a detailed analysis comparing cross-attention scores between selected and non-selected sentences across different question types (fact-based, reasoning, multi-hop) to verify the selection mechanism's consistency.

3. **Failure case analysis:** Systematically analyze questions where FastFiD fails to select the correct sentences, categorizing failure modes (retriever failure, selection error, insufficient context) to understand the method's limitations.