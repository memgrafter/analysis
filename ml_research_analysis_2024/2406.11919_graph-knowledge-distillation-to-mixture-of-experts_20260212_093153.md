---
ver: rpa2
title: Graph Knowledge Distillation to Mixture of Experts
arxiv_id: '2406.11919'
source_url: https://arxiv.org/abs/2406.11919
tags:
- experts
- expert
- datasets
- loss
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the inefficiency of graph neural networks (GNNs)
  in node classification, caused by their neighborhood processing overhead. To improve
  inference speed, the authors propose distilling knowledge from a GNN to a Mixture-of-Experts
  (MoE) student model with a routing-by-memory (RbM) architecture.
---

# Graph Knowledge Distillation to Mixture of Experts

## Quick Facts
- arXiv ID: 2406.11919
- Source URL: https://arxiv.org/abs/2406.11919
- Authors: Pavel Rumiantsev; Mark Coates
- Reference count: 13
- Primary result: RbM consistently outperforms parameter-inflated MLPs, ensembles, and vanilla MoE baselines, achieving accuracy close to or exceeding teacher GNNs.

## Executive Summary
This work addresses the inefficiency of graph neural networks (GNNs) in node classification, caused by their neighborhood processing overhead. To improve inference speed, the authors propose distilling knowledge from a GNN to a Mixture-of-Experts (MoE) student model with a routing-by-memory (RbM) architecture. RbM uses expert embeddings in the input space and incorporates three loss terms (commitment, self-similarity, and load balance) to encourage expert specialization. Experiments on nine real-world datasets show that RbM consistently outperforms parameter-inflated MLPs, ensembles, and vanilla MoE baselines, achieving accuracy close to or exceeding teacher GNNs. RbM is particularly effective for medium and large datasets.

## Method Summary
The paper proposes a knowledge distillation framework where a GNN teacher's predictions are transferred to a Mixture-of-Experts student model with routing-by-memory. RbM uses expert embeddings in the hidden representation space and employs three routing losses: commitment (VQ-style), self-similarity, and load balance. The student model consists of MoE layers with cosine similarity routing to expert embeddings, followed by expert linear layers and output attention scalars. Positional encodings are concatenated to node features before routing. The framework is evaluated on nine graph node classification datasets, comparing accuracy against GLNN, KRD, and NOSMOG baselines.

## Key Results
- RbM achieves higher accuracy than vanilla MoE, parameter-inflated MLPs, and ensemble methods on all nine datasets
- RbM performs particularly well on medium and large datasets (Academic-Physics, OGB-Products)
- The three routing losses (commitment, self-similarity, load balance) are crucial for RbM's performance
- RbM accuracy is close to or exceeds teacher GNN performance while maintaining MoE's efficiency benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RbM outperforms vanilla MoE because expert embeddings are kept in the hidden representation space, forcing better local specialization.
- Mechanism: Embeddings live in the same space as the hidden representations, so routing is based on actual proximity in that space rather than an abstract projection.
- Core assumption: The hidden representation space is structured enough for meaningful clustering by expert embeddings.
- Evidence anchors:
  - [abstract] "By encouraging each expert to specialize on a certain region on the hidden representation space..."
  - [section] "While Li et al. (2022) provide analysis demonstrating that GMoE (·) provides a degree of specialization for expert networks, we found it insufficient (see the experimental results in Section 5.5). We therefore enforce experts’ local specialization by setting the expert embeddings QRbM∈RE×d′ to vectors in the input space..."
- Break condition: If the hidden representation space becomes too chaotic or isotropic, the distance-based routing loses discriminative power.

### Mechanism 2
- Claim: Commitment loss pulls representations toward their assigned expert embeddings, enforcing tighter clustering and stable routing.
- Mechanism: A VQ-style loss that minimizes the cosine distance between each routed representation and its top expert's embedding, preventing frequent routing switches.
- Core assumption: Tighter clustering around expert embeddings leads to improved expert specialization and stable inference.
- Evidence anchors:
  - [section] "In order to encourage tighter clustering of hidden representations around the expert embeddings we incorporate a vector quantization (VQ)-style commitment loss..."
  - [section] "Routing to multiple experts prevents the hidden representations from collapse."
- Break condition: If the commitment loss dominates too strongly, the model may collapse into a single expert, losing the mixture-of-experts benefit.

### Mechanism 3
- Claim: Load balance loss redistributes borderline representations to less-utilized experts, improving expert utilization.
- Mechanism: Encourages moving representations that are almost equidistant to two or more experts toward the least utilized expert embedding.
- Core assumption: Uneven expert utilization leads to wasted capacity and potential overfitting of some experts.
- Evidence anchors:
  - [section] "Finally, we apply a load balance loss, as proposed by Shazeer et al. (2016)... This loss encourages representations to move closer to the least utilised expert embeddings."
  - [section] "While expert specialization does not require load balancing, we adopt this loss to redistribute input vectors that lie in almost equal proximity to multiple experts."
- Break condition: If load balancing overcompensates, it can force representations into suboptimal experts, hurting accuracy.

## Foundational Learning

- Concept: Graph Neural Networks and their neighborhood aggregation bottleneck.
  - Why needed here: The whole motivation is to replace slow neighborhood aggregation with faster routing-based inference.
  - Quick check question: Why does GNN inference scale poorly with graph size?

- Concept: Knowledge distillation from a teacher model to a student.
  - Why needed here: RbM is trained to mimic the teacher GNN's soft labels without using its aggregation.
  - Quick check question: What loss terms are typically used in GNN-to-MLP distillation?

- Concept: Mixture-of-Experts routing and specialization.
  - Why needed here: RbM is a sparse MoE with specialized routing, so understanding gating and expert specialization is critical.
  - Quick check question: How does top-k routing differ from top-1 routing in MoE?

## Architecture Onboarding

- Component map: Input → Positional encoding concat → MoE layer(s) with RbM routing → Output layer. Each MoE layer contains: router (cosine similarity to expert embeddings), expert linear layers, optional input/output attention scalars, and three loss terms (commitment, self-similarity, load balance).

- Critical path: Input features + positional encoding → router (cosine similarity) → top-k expert selection → expert forward pass → weighted sum → next layer. The router is the key hot path; any inefficiency here hurts inference speed.

- Design tradeoffs: RbM uses cosine routing and embeddings in the hidden space for specialization, but sacrifices some routing flexibility compared to projection-based MoE. The stop-gradient on embeddings avoids instability but requires periodic updates via moving averages.

- Failure signatures: High variance in routing across epochs (commitment loss too low), poor expert utilization (load balance loss too low), representation collapse (self-similarity loss too low), or performance stuck near teacher baseline (routing not specialized enough).

- First 3 experiments:
  1. Run RbM with default settings on Cora transductive; compare accuracy and routing entropy to vanilla MoE.
  2. Remove commitment loss; observe if routing becomes unstable or accuracy drops.
  3. Remove load balance loss; check if expert utilization becomes skewed and accuracy degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is RbM's performance to the choice of the number of experts per layer, and is there an optimal strategy for selecting this hyperparameter?
- Basis in paper: [explicit] The paper notes that RbM performance varies with the number of experts, and that an optimal number can be identified using validation data, but it uses the same number of experts for all layers to reduce hyperparameters.
- Why unresolved: The paper does not explore the impact of varying the number of experts across different layers or provide a systematic method for layer-wise expert selection.
- What evidence would resolve it: Experiments comparing RbM performance with different numbers of experts per layer, or an automated method for determining the optimal number of experts per layer.

### Open Question 2
- Question: Can RbM's routing mechanism be improved by incorporating graph structure directly into the routing process, rather than relying solely on positional encodings?
- Basis in paper: [inferred] The paper uses positional encodings generated by DeepWalk for the routing process, but it does not explore alternative methods for incorporating graph structure into routing.
- Why unresolved: The paper does not investigate the potential benefits of using alternative graph-based routing mechanisms, such as those based on node proximity or community structure.
- What evidence would resolve it: Experiments comparing RbM performance with different routing mechanisms that incorporate graph structure directly, such as routing based on node proximity or community structure.

### Open Question 3
- Question: How does RbM's performance scale with increasingly large and complex graphs, and what are the limitations of the current approach?
- Basis in paper: [explicit] The paper evaluates RbM on a range of datasets, including large ones like OGB-Products, but does not explore the performance limits of the approach.
- Why unresolved: The paper does not provide a comprehensive analysis of RbM's scalability or identify the specific factors that limit its performance on very large or complex graphs.
- What evidence would resolve it: Experiments evaluating RbM on graphs with millions or billions of nodes and edges, and an analysis of the computational and memory requirements of the approach.

### Open Question 4
- Question: Can the routing mechanism of RbM be further improved by using more sophisticated distance metrics or clustering algorithms?
- Basis in paper: [inferred] The paper uses cosine similarity for routing and K-means clustering for expert initialization, but it does not explore alternative distance metrics or clustering algorithms.
- Why unresolved: The paper does not investigate the potential benefits of using alternative distance metrics or clustering algorithms, such as those based on graph topology or node attributes.
- What evidence would resolve it: Experiments comparing RbM performance with different distance metrics or clustering algorithms, such as those based on graph topology or node attributes.

## Limitations

- No actual inference latency measurements are provided to validate efficiency claims
- Assumes hidden representation space is structured enough for meaningful cosine routing without testing this assumption
- The stop-gradient on expert embeddings could create convergence issues requiring periodic updates
- Expert specialization quality is not analyzed beyond accuracy metrics

## Confidence

- **High Confidence**: RbM architecture design (spatial routing-by-memory), general effectiveness of distillation from GNN to MoE, importance of routing losses for stability
- **Medium Confidence**: Claims about expert specialization quality, accuracy improvements over baselines, effectiveness of specific loss terms
- **Low Confidence**: Inference efficiency claims (no latency data), generalizability to datasets outside the nine tested, robustness to hyperparameter choices

## Next Checks

1. **Routing Stability Analysis**: Track routing entropy and expert utilization variance across training epochs for RbM vs vanilla MoE. If RbM routing becomes more stable and balanced, this validates Mechanism 1 and Mechanism 3.

2. **Expert Specialization Visualization**: Project expert embeddings and routed representations into 2D space (via UMAP/t-SNE) for a medium dataset like Cora. If distinct clusters form around expert embeddings without collapse, this supports Mechanism 2.

3. **Latency Benchmarking**: Measure actual inference time for RbM vs teacher GNN on a large graph (e.g., OGB-Products) using the same hardware. This directly tests the efficiency claims and validates the core motivation.