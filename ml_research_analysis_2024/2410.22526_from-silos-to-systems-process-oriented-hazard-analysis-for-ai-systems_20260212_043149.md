---
ver: rpa2
title: 'From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems'
arxiv_id: '2410.22526'
source_url: https://arxiv.org/abs/2410.22526
tags:
- system
- systems
- safety
- control
- stpa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for system-level hazard analysis
  in AI development, as current approaches focus on isolated components rather than
  interactions and sociotechnical factors. The authors adapt System Theoretic Process
  Analysis (STPA), a framework from safety engineering, to analyze AI systems.
---

# From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems

## Quick Facts
- arXiv ID: 2410.22526
- Source URL: https://arxiv.org/abs/2410.22526
- Reference count: 23
- Primary result: Adapting System Theoretic Process Analysis (STPA) for AI systems enables detection of system-level hazards, establishes accountability chains, and supports ongoing monitoring of emergent risks

## Executive Summary
This paper addresses the critical gap in AI safety by introducing PHASE (Process-oriented Hazard Analysis for AI Systems), an adaptation of STPA that shifts hazard analysis from component-focused to system-level perspectives. The framework recognizes that algorithmic harms are sociotechnical and emergent, requiring analysis across technical artifacts, social agents, and institutional mechanisms. Through three case studies—linear regression for sepsis prediction, reinforcement learning for insulin dosing, and text-to-image generation—the authors demonstrate how PHASE makes explicit the control relationships and interactions that lead to system-level hazards. The framework enables traceability between harms and responsible parties while supporting ongoing monitoring of evolving risks in deployed AI systems.

## Method Summary
The authors adapted STPA, a safety engineering framework from aerospace, to analyze AI systems by translating control-theoretic concepts to AI contexts. The method involves four key steps: identifying the purpose of analysis, creating hierarchical control diagrams mapping controllers and controlled processes, identifying unsafe control actions that could lead to losses, and generating loss scenarios that trace causal chains from inadequate control to harms. Applied to three diverse case studies, the framework explicitly represents sociotechnical interactions, establishes accountability chains through control hierarchy mapping, and enables iterative reassessment of hazards as systems evolve. The authors provide a practical guideline for conducting PHASE analyses while acknowledging the need for safety engineering expertise.

## Key Results
- PHASE successfully identified system-level hazards in three diverse AI applications by mapping interactions between technical artifacts, social agents, and institutional mechanisms
- The framework established traceable accountability chains between identified harms and responsible actors through explicit control hierarchy representation
- PHASE demonstrated capability for ongoing monitoring by accounting for changes in system dynamics, model behavior, and user interactions over time
- The adapted framework made STPA concepts accessible for AI contexts while maintaining the core principle that safety emerges from proper control rather than component reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PHASE enables hazard detection at the system level by mapping interactions between technical artifacts, social agents, and institutional mechanisms.
- Mechanism: The framework translates STPA's control-theoretic perspective to AI contexts, explicitly representing controllers (humans, teams, algorithms) and controlled processes in feedback loops. This makes visible how disparate issues accumulate into system-level hazards.
- Core assumption: System-level hazards emerge from interactions between components rather than component failures alone.
- Evidence anchors:
  - [abstract] "enables detection of hazards at the systems level, including those from accumulation of disparate issues"
  - [section] "harms from AI system are sociotechnical and emergent...a comprehensive accounting of an AI system's algorithmic harms can only be understood and addressed across the interactions of technical artifacts, social agents...and institutional mechanisms"
  - [corpus] Weak - neighboring papers focus on STPA applications but don't directly address sociotechnical interactions
- Break condition: If the system boundary definition excludes critical sociotechnical interactions, accumulation of disparate issues may go undetected.

### Mechanism 2
- Claim: PHASE establishes traceability between harms and those who can mitigate them through hierarchical control structure mapping.
- Mechanism: By creating control diagrams that represent controllers, controlled processes, and their interactions, PHASE makes explicit who has control over what. This enables accountability chains to be traced from identified harms back to responsible actors.
- Core assumption: Accountability requires explicit mapping of control relationships and decision-making authority.
- Evidence anchors:
  - [abstract] "creation of traceable accountability chains between harms and those who can mitigate the harm"
  - [section] "the STPA framework allows us to connect harms to inadequate control within a system...it allows an analyst to make explicit who has control over what"
  - [corpus] Weak - neighboring papers discuss STPA but don't explicitly address accountability chain establishment
- Break condition: If control hierarchy is incorrectly represented or if multiple actors share unclear responsibilities, traceability may fail.

### Mechanism 3
- Claim: PHASE enables ongoing monitoring of emergent hazards by systematically accounting for changes in system dynamics over time.
- Mechanism: The control diagram framework can represent changes in AI model behavior, user interactions, and institutional processes as updates to control actions, feedback loops, and controller mental models. This enables iterative reassessment of hazards.
- Core assumption: AI systems evolve through deployment, requiring dynamic hazard assessment rather than static analysis.
- Evidence anchors:
  - [abstract] "ongoing monitoring and mitigation of new hazards"
  - [section] "the framework allows an analyst to monitor and account for changes in individuals, teams, technical artifacts, and institutional mechanisms over time"
  - [corpus] Moderate - neighboring papers discuss STPA for evolving systems but don't specifically address AI system dynamics
- Break condition: If monitoring processes aren't regularly implemented or if changes in system dynamics aren't properly mapped to control structures, emergent hazards may go undetected.

## Foundational Learning

- Concept: System-theoretic accident models (STAMP)
  - Why needed here: Provides the theoretical foundation for viewing safety as an emergent property rather than component reliability
  - Quick check question: What is the key difference between STAMP and traditional linear accident models?

- Concept: Control feedback loops
  - Why needed here: Essential for understanding how controllers (humans, algorithms) interact with controlled processes through control actions and feedback
  - Quick check question: In a control feedback loop, what are the two types of interactions between controllers and controlled processes?

- Concept: System boundaries in AI contexts
  - Why needed here: Critical for defining the scope of analysis and identifying where control can be exercised in AI development processes
- Quick check question: Why can't system boundaries be drawn around ML models themselves in the same way as traditional safety-critical components?

## Architecture Onboarding

- Component map:
  - Loss identification module -> Control diagram builder -> UCA identifier -> Loss scenario generator -> Traceability tracker -> Dynamic monitor

- Critical path: Loss identification → Control diagram creation → UCA identification → Loss scenario generation → Traceability establishment → Ongoing monitoring

- Design tradeoffs:
  - Comprehensive vs. practical: More thorough analysis requires more time and expertise
  - Technical vs. sociotechnical focus: Balancing component-level analysis with social factor consideration
  - Static vs. dynamic analysis: Choosing between one-time assessment and ongoing monitoring

- Failure signatures:
  - Incomplete system boundaries leading to missed hazards
  - Incorrect control hierarchy mapping causing traceability failures
  - Missing feedback loops resulting in inadequate hazard detection
  - Failure to update control diagrams as system evolves

- First 3 experiments:
  1. Apply PHASE to a simple linear regression model for a non-safety-critical application to validate basic workflow
  2. Test the framework on a human-in-the-loop AI system to examine controller-controlled process relationships
  3. Implement ongoing monitoring on an adaptive AI system to validate dynamic hazard detection capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can accountability mechanisms be designed to ensure effective operationalization of PHASE guidelines across different organizational contexts and cultures?
- Basis in paper: [explicit] The paper discusses the need for traceability and accountability chains in AI systems, but acknowledges that persistence of challenges for STPA would hinder adoption even with the development of guidelines.
- Why unresolved: The paper highlights the importance of accountability but does not provide specific mechanisms or strategies for implementing these across diverse organizational contexts, particularly those with different cultural attitudes towards safety and responsibility.
- What evidence would resolve it: Case studies or empirical research demonstrating successful implementation of accountability mechanisms in various organizational settings, along with metrics for measuring their effectiveness in improving AI system safety and responsibility.

### Open Question 2
- Question: To what extent can the PHASE framework be adapted to address potential risks posed by increasingly agentic AI systems, given their inherent complexity and potential for autonomous decision-making?
- Basis in paper: [explicit] The paper mentions that while none of the case studies involve agentic AI, it believes that the PHASE effectively maps where and what humans and technical systems/processes have or do not have control, which could offer insights for agentic systems.
- Why unresolved: The paper does not provide concrete examples or detailed analysis of how PHASE would handle the unique challenges posed by agentic AI, such as understanding their process model of the world or controlling their autonomous actions.
- What evidence would resolve it: Detailed case studies or simulations applying PHASE to agentic AI systems, along with evaluations of its effectiveness in identifying and mitigating risks compared to traditional safety engineering approaches.

### Open Question 3
- Question: How can the PHASE framework be standardized and validated across different industries and applications to ensure consistent and high-quality hazard analysis for AI systems?
- Basis in paper: [explicit] The paper notes that similar to any analytical process, analysts must make normative choices, assumptions, and simplifications in conducting PHASE, and emphasizes the need for well-established industry standards on what a quality PHASE should look like.
- Why unresolved: While the paper provides a guideline for conducting PHASE, it does not address how to establish and maintain standards across different industries, each with its own regulatory requirements and risk profiles.
- What evidence would resolve it: Development of industry-specific standards and validation frameworks for PHASE, along with empirical studies comparing the consistency and quality of hazard analyses conducted using these standards across different sectors.

## Limitations

- Framework effectiveness depends heavily on accurate system boundary definition, which remains challenging in complex AI systems with dynamic sociotechnical interactions
- Case studies demonstrate proof-of-concept rather than comprehensive validation across diverse AI applications and real-world implementations
- While PHASE makes STPA more accessible for AI contexts, it doesn't eliminate the need for safety engineering expertise and specialized knowledge

## Confidence

- High Confidence: The claim that STPA can be adapted for AI systems is well-supported by the successful application to three diverse case studies. The mechanism of translating control-theoretic concepts to AI contexts is clearly demonstrated.
- Medium Confidence: The claim about establishing traceable accountability chains is moderately supported. While the framework shows how to map control hierarchies, real-world implementation challenges in complex organizational structures aren't fully addressed.
- Medium Confidence: The claim about ongoing monitoring capabilities is supported by theoretical arguments but lacks empirical validation of long-term monitoring effectiveness in deployed systems.

## Next Checks

1. **Scalability Assessment:** Apply PHASE to a large-scale, production AI system with multiple interacting components and assess whether the framework maintains effectiveness while scaling up in complexity.

2. **Organizational Implementation Study:** Conduct a field study implementing PHASE in an organization with distributed teams and multiple stakeholders to evaluate practical challenges in establishing accountability chains.

3. **Longitudinal Monitoring Evaluation:** Implement PHASE-based monitoring on an adaptive AI system deployed in a real-world setting for 6+ months to validate the framework's ability to detect emergent hazards over time.