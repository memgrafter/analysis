---
ver: rpa2
title: Addressing Hallucinations in Language Models with Knowledge Graph Embeddings
  as an Additional Modality
arxiv_id: '2411.11531'
source_url: https://arxiv.org/abs/2411.11531
tags:
- language
- dataset
- llama
- arxiv
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to reduce hallucinations in Large Language
  Models (LLMs) by incorporating Knowledge Graph (KG) embeddings as an additional
  modality. The approach involves transforming input text into KG embeddings using
  a Text2Graph mapper and integrating these embeddings into the LLM using an adapter,
  without relying on external retrieval processes.
---

# Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality

## Quick Facts
- arXiv ID: 2411.11531
- Source URL: https://arxiv.org/abs/2411.11531
- Authors: Viktoriia Chekalina; Anton Razzhigaev; Elizaveta Goncharova; Andrey Kuznetsov
- Reference count: 40
- Primary result: KG embeddings integration reduces hallucinations in LLMs without external retrieval

## Executive Summary
This paper introduces a novel approach to mitigating hallucinations in large language models by incorporating knowledge graph embeddings as an additional input modality. The method uses a Text2Graph mapper to transform input text into KG embeddings from Wikidata, which are then integrated into the LLM through an adapter architecture. Unlike retrieval-augmented approaches, this method processes all information internally, avoiding external API dependencies and potential latency issues.

The approach was evaluated across three model families (Mistral-7B, LLaMA 2-7B, LLaMA 3-8B) using a custom WikiEntities dataset and multiple hallucination benchmarks. Results demonstrate significant improvements in factual accuracy while maintaining performance on other language tasks, establishing KG embeddings as a viable modality for enhancing LLM reliability.

## Method Summary
The approach transforms text into knowledge graph embeddings using a Text2Graph mapper trained on Wikidata entities, then integrates these embeddings into LLMs through an adapter architecture. The WikiEntities dataset was created with over 3 million Wikipedia texts annotated with Wikidata entities and their embeddings. The adapter architecture allows the LLM to process KG embeddings alongside text tokens without requiring external retrieval systems. This internal processing approach eliminates API dependencies and latency while maintaining the benefits of structured knowledge integration.

## Key Results
- Significant reduction in hallucinations on HaluEval, True-False, and FEVER benchmarks
- Maintained performance on non-hallucination language tasks
- Demonstrated effectiveness across multiple model families (Mistral-7B, LLaMA 2-7B, LLaMA 3-8B)

## Why This Works (Mechanism)
The integration of knowledge graph embeddings provides LLMs with structured, factual knowledge that supplements their parametric memory. By converting text into KG embeddings through the Text2Graph mapper, the model gains access to verified entity relationships and attributes from Wikidata. The adapter architecture enables seamless integration of this structured knowledge with the model's existing text processing capabilities, allowing the LLM to cross-reference generated content against factual knowledge during inference.

## Foundational Learning

**Knowledge Graph Embeddings**: Vector representations of entities and relationships in a knowledge graph. *Why needed*: Provide structured, factual knowledge in a format LLMs can process. *Quick check*: Verify embeddings capture semantic relationships between entities.

**Adapter Architecture**: Small neural network modules that can be inserted into pre-trained models to add new capabilities. *Why needed*: Enables KG integration without full model retraining. *Quick check*: Confirm adapter parameters are minimal compared to base model.

**Text2Graph Mapping**: Process of converting natural language text into corresponding KG embeddings. *Why needed*: Bridges unstructured text and structured knowledge representations. *Quick check*: Evaluate mapping accuracy on entity recognition and relationship extraction.

## Architecture Onboarding

**Component Map**: Text Input -> Text2Graph Mapper -> KG Embeddings -> Adapter -> LLM -> Output

**Critical Path**: The Text2Graph mapper and adapter are the critical components. Mapper quality directly impacts factual accuracy, while adapter efficiency affects inference speed and memory usage.

**Design Tradeoffs**: Internal KG processing vs. external retrieval - the internal approach eliminates API dependencies and latency but requires upfront KG embedding computation and storage. Adapter-based integration preserves base model weights but adds computational overhead.

**Failure Signatures**: 
- Poor Text2Graph mapping leads to incorrect or missing entity embeddings
- Adapter saturation causes performance degradation on non-factual tasks
- KG embedding quality issues propagate to factual accuracy problems

**First Experiments**:
1. Test Text2Graph mapper accuracy on entity recognition across different text domains
2. Evaluate adapter impact on inference latency and memory usage
3. Validate KG embedding integration maintains base model performance on standard benchmarks

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Scalability concerns for the Text2Graph mapper across diverse real-world text domains
- Potential bias toward encyclopedic knowledge from Wikidata reliance
- Computational overhead from adapter-based integration affecting production deployment

## Confidence
- High confidence in technical implementation and methodology
- Medium confidence in generalization across domains
- Medium confidence in practical deployment considerations

## Next Checks
1. Evaluate the Text2Graph mapper performance on non-Wikipedia text domains, including scientific literature and social media content
2. Conduct ablation studies to quantify the trade-off between hallucination reduction and creative generation capabilities
3. Measure inference time and computational overhead of the adapter-based approach compared to baseline models in production-scale scenarios