---
ver: rpa2
title: 'HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural
  Language Generalization'
arxiv_id: '2402.16694'
source_url: https://arxiv.org/abs/2402.16694
tags:
- codegen2
- code
- codet5
- across
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HumanEval-XL, a massively multilingual code
  generation benchmark that connects 23 natural languages (NLs) to 12 programming
  languages (PLs) across 22,080 prompts. The authors developed an iterative translation
  pipeline using GPT-4 to create parallel coding problems while maintaining high translation
  quality through back-translation and BERTScore verification.
---

# HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization

## Quick Facts
- **arXiv ID**: 2402.16694
- **Source URL**: https://arxiv.org/abs/2402.16694
- **Authors**: Qiwei Peng, Yekun Chai, Xuhong Li
- **Reference count**: 0
- **Primary result**: Introduces HumanEval-XL, a multilingual code generation benchmark connecting 23 natural languages to 12 programming languages across 22,080 prompts

## Executive Summary
This paper presents HumanEval-XL, a comprehensive multilingual code generation benchmark that evaluates large language models' ability to understand natural language instructions and generate code across multiple languages. The benchmark consists of 22,080 parallel prompts covering 23 natural languages and 12 programming languages, created through an iterative translation pipeline using GPT-4. The authors evaluate multiple model families including code-pretrained models (CodeT5+, CodeGen2) and general LLMs (GPT-3.5, GPT-4), finding that specialized code-pretrained models and larger parameter scales significantly improve performance. However, all models struggle with cross-lingual semantic generalization, showing substantial performance disparities across different language families and resource levels.

## Method Summary
The authors developed HumanEval-XL by extracting coding problems from existing benchmarks and translating them into 23 natural languages using GPT-4. Each translation undergoes back-translation to English and BERTScore verification, with a threshold of 0.95 required for acceptance. This iterative process repeats up to three times per prompt. The benchmark evaluates models using pass@1 metric (percentage of examples where at least one generated function passes all test cases), with inference parameters set to top-p sampling (p=0.95, temperature=0.2). The evaluation focuses on the largest parameter models per family due to observed consistent trends across scales.

## Key Results
- GPT-4 achieves the best performance across most programming languages, significantly outperforming GPT-3.5 despite having fewer parameters
- Code-pretrained models (CodeT5+, CodeGen2) consistently outperform general LLMs on code generation tasks, highlighting the importance of specialized training
- All models show significant performance disparities across different language families, with Afro-Asiatic, Indo-European Greek/Iranian, and Turkic languages performing worse than others
- Larger parameter scales consistently improve multilingual code generation performance across all model families tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative translation with GPT-4 and back-translation creates high-quality multilingual prompts
- Mechanism: GPT-4 translates English prompts to 23 NLs, then back-translates them to English. BERTScore compares back-translated English to original. If similarity > 0.95, keep; otherwise, repeat up to 3 times or discard
- Core assumption: GPT-4's translation quality is high enough that back-translation similarity > 0.95 indicates semantic preservation
- Evidence anchors: Translation pipeline description, BERTScore verification process, 0.95 threshold specification

### Mechanism 2
- Claim: Parallel alignment across 23 NLs and 12 PLs enables unbiased cross-lingual comparison
- Mechanism: Same coding problem is expressed in all 23 NLs and implemented in all 12 PLs. This creates 22,080 parallel prompts
- Core assumption: Parallel problems truly express equivalent semantics across languages, enabling fair comparison
- Evidence anchors: Benchmark design description, parallel data creation process, comprehensive evaluation platform claim

### Mechanism 3
- Claim: Larger model size consistently improves multilingual code generation performance
- Mechanism: Experiments show GPT-4 (larger) outperforms GPT-3.5, CodeGen2-16B outperforms smaller CodeGen2 variants, etc
- Core assumption: Scaling laws apply equally to multilingual and monolingual code generation tasks
- Evidence anchors: Model comparison results, parameter scale analysis, consistent performance improvement trends

## Foundational Learning

- **Concept: Back-translation quality verification**
  - Why needed here: Ensures translated prompts preserve original meaning across 23 languages
  - Quick check question: What threshold does the paper use for BERTScore to accept a translation?

- **Concept: Cross-lingual semantic equivalence**
  - Why needed here: Benchmark requires same problem to be expressed equivalently in different natural languages
  - Quick check question: How many language families are represented in the 23 natural languages?

- **Concept: Pass@k evaluation metric**
  - Why needed here: Standard way to measure code generation success by counting test cases passed
  - Quick check question: What pass@k variant does the paper report due to computing constraints?

## Architecture Onboarding

- **Component map**: Translation pipeline → Quality check → Parallel prompt generation → LLM evaluation → Result aggregation
- **Critical path**: English prompt → GPT-4 translation (23 NLs) → BERTScore verification → Back-translation loop → Parallel prompt dataset
- **Design tradeoffs**: Higher BERTScore threshold → better quality but fewer examples; Larger models → better performance but higher compute cost
- **Failure signatures**: Low BERTScore scores → translation quality issues; Inconsistent results across languages → semantic equivalence problems
- **First 3 experiments**:
  1. Test single prompt translation with BERTScore threshold 0.95, verify back-translation matches original
  2. Compare CodeT5+ vs CodeGen2 performance on Python-English to validate code-specific pretraining advantage
  3. Evaluate GPT-3.5 vs GPT-4 on same prompts to measure scaling impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve cross-lingual natural language generalization in code generation models beyond scaling parameter size?
- Basis in paper: The paper explicitly states that "all models struggle to capture equivalent semantic meaning expressed in different languages" and "current LLMs struggle to capture the equivalent semantic meaning expressed in different languages in the task of code generation"
- Why unresolved: Despite improvements from larger parameter scales, models still show significant performance disparities across different language families and resource levels
- What evidence would resolve it: Experiments comparing different architectural approaches, multilingual pre-training strategies, or novel loss functions designed specifically for cross-lingual semantic alignment

### Open Question 2
- Question: What specific aspects of language families (Afro-Asiatic, Indo-European Greek/Iranian, Turkic) cause the most difficulty for code generation models?
- Basis in paper: The paper notes that "languages from the Afro-Asiatic, Indo-European (Greek), Indo-European (Iranian), and Turkic language families generally yield lower results compared to other language families"
- Why unresolved: While the paper identifies which language families perform worse, it doesn't analyze what linguistic features or structural differences cause these difficulties
- What evidence would resolve it: Linguistic analysis correlating specific language features with model performance across language families

### Open Question 3
- Question: How does the iterative translation pipeline quality affect downstream code generation performance across different language pairs?
- Basis in paper: The paper uses a translation pipeline with BERTScore verification and manual quality checks, but doesn't analyze how translation quality correlates with model performance
- Why unresolved: The paper mentions quality control steps but doesn't investigate whether translation quality is a bottleneck for model performance or if certain language pairs have systematically worse translations
- What evidence would resolve it: Correlation analysis between translation quality metrics (BERTScore) and code generation performance, plus error analysis of problematic translations across different language pairs

## Limitations

- The translation quality verification mechanism relies heavily on GPT-4's translation capabilities and the BERTScore threshold of 0.95, but provides no empirical validation that this threshold is optimal for preserving code semantics across 23 languages
- The evaluation focuses primarily on Python programming language results, limiting generalizability of findings about model scaling and pretraining advantages
- The claim that HumanEval-XL enables "unbiased cross-lingual comparison" is questionable given the lack of semantic equivalence verification beyond translation quality metrics

## Confidence

**High Confidence**: The finding that specialized code-pretrained models (CodeT5+, CodeGen2) outperform general LLMs (GPT-3.5) on code generation tasks is well-supported by experimental results showing consistent performance gaps across all programming languages tested.

**Medium Confidence**: The claim that larger model size consistently improves multilingual code generation performance is supported by GPT-4 vs GPT-3.5 comparisons, but evidence is limited to these two model variants without testing intermediate sizes or other model families at different scales.

**Low Confidence**: The assertion that HumanEval-XL enables "unbiased cross-lingual comparison" is questionable given the lack of semantic equivalence verification beyond translation quality metrics, and absence of analysis about how linguistic features might influence problem interpretation.

## Next Checks

1. **Translation Quality Validation**: Conduct human evaluation of 100 randomly selected translated prompts across 5 different language pairs to verify that BERTScore > 0.95 threshold actually corresponds to semantic preservation of code requirements, not just surface-level similarity.

2. **Cross-Lingual Semantic Equivalence**: Test whether the same model generates functionally equivalent code when given the same problem in different natural languages by comparing outputs across language pairs for 50 random problems and measuring functional similarity.

3. **Scaling Law Verification**: Systematically test model performance across 3-4 different parameter scales within each model family (e.g., CodeT5+ variants) to verify whether observed scaling trends hold consistently across the full range of model sizes, not just at the extremes.