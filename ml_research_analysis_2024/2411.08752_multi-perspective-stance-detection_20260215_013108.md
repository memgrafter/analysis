---
ver: rpa2
title: Multi-Perspective Stance Detection
arxiv_id: '2411.08752'
source_url: https://arxiv.org/abs/2411.08752
tags:
- multi-perspective
- label
- dataset
- baseline
- chunking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes using multiple human annotations per document\
  \ rather than a single aggregated label in stance detection. This multi-perspective\
  \ approach models each annotator\u2019s viewpoint separately during training, aiming\
  \ to improve classification performance and model confidence."
---

# Multi-Perspective Stance Detection

## Quick Facts
- arXiv ID: 2411.08752
- Source URL: https://arxiv.org/abs/2411.08752
- Reference count: 12
- Multi-perspective models with individual annotator labels outperform baseline majority voting models in stance detection

## Executive Summary
This paper introduces a multi-perspective approach to stance detection that models each annotator's viewpoint separately rather than using a single aggregated label. By training on individual annotator perspectives, the method captures the inherent subjectivity and disagreement in stance labeling tasks. Experiments with BERT-base and RoBERTa-base on a crowd-sourced dataset demonstrate that this approach consistently outperforms traditional majority voting baselines, with RoBERTa-base achieving 47.45 F1 score compared to 40.48 F1 for the baseline.

## Method Summary
The method involves preprocessing a crowd-sourced stance detection dataset by removing duplicates and documents with "link not-working" labels, then splitting it into train/validation/test sets (75%/15%/15%). Two training sets are created: a baseline set with one document per instance using majority labels, and a multi-perspective set with each document duplicated three times using individual annotator labels. BERT-base and RoBERTa-base models are fine-tuned with/without chunking using chunkipy library, batch size 32, and 4 epochs. The multi-perspective approach exposes the model to more diverse label distributions during training, while chunking preserves context for long documents by splitting them into overlapping segments.

## Key Results
- Multi-perspective models consistently outperformed baseline models across all configurations
- RoBERTa-base with chunking achieved 47.45 F1 score versus 40.48 F1 for baseline
- Chunking improved both accuracy and confidence across all models
- Annotator disagreement affects model confidence, with multi-perspective models showing higher confidence scores

## Why This Works (Mechanism)

### Mechanism 1
Using multiple individual annotator labels instead of majority voting improves model performance by exposing the model to more diverse label distributions during training, reflecting real-world ambiguity in stance detection tasks. The core assumption is that annotator disagreement reflects meaningful perspectives rather than noise. Evidence shows multi-perspective models consistently outperform baseline models, though the benefit would diminish if the dataset has very low annotator disagreement.

### Mechanism 2
Chunking improves model performance and confidence by providing more complete context through splitting long documents into overlapping chunks. This allows transformer models to process more content while preserving context through overlap. The core assumption is that relevant information for stance detection exists beyond the 512-token limit. Results confirm chunking works better than no chunking, though it would provide no benefit if documents are consistently short enough to fit within model limits.

### Mechanism 3
Annotator disagreement affects model confidence, with multi-perspective models showing higher confidence scores by better understanding the uncertainty inherent in the task. The core assumption is that the relationship between annotator disagreement and model confidence is causal rather than coincidental. While RoBERTa models show higher confidence across results, direct analysis correlating confidence scores with annotator disagreement levels is limited, warranting further investigation.

## Foundational Learning

- Concept: Stance detection as a subjective classification task
  - Why needed here: Understanding that stance detection involves interpreting opinions rather than objective facts is crucial for appreciating why multiple perspectives matter
  - Quick check question: Why is stance detection considered more subjective than factual question answering?

- Concept: Transformer model limitations (sequence length)
  - Why needed here: The 512-token limit for BERT and RoBERTa base models necessitates chunking for longer documents
  - Quick check question: What is the maximum sequence length for BERT-base and RoBERTa-base models, and why does this matter for this task?

- Concept: Evaluation metrics for imbalanced datasets
  - Why needed here: The dataset is slightly imbalanced towards neutral labels, making F1-score more appropriate than accuracy alone
  - Quick check question: When should you use F1-score instead of accuracy for model evaluation?

## Architecture Onboarding

- Component map: Data preprocessing -> Multi-perspective augmentation -> Chunking -> Model training -> Evaluation
- Critical path: Data preparation (including multi-perspective augmentation and chunking) -> Model training with proper hyperparameters -> Evaluation on held-out test set
- Design tradeoffs: Multi-perspective approach increases training data size (3x) but may cause overfitting on small datasets; chunking preserves context but increases computational cost
- Failure signatures: Low F1-score despite high accuracy (indicating class imbalance issues); poor performance on documents with high annotator disagreement; overfitting on training data
- First 3 experiments:
  1. Compare baseline vs multi-perspective performance on a small subset to verify the core claim
  2. Test different chunking strategies (overlap size, chunk length) to optimize context preservation
  3. Evaluate model confidence calibration by comparing predicted confidence scores with actual accuracy across different disagreement levels

## Open Questions the Paper Calls Out

### Open Question 1
How do larger versions of BERT and RoBERTa (e.g., BERT-large, RoBERTa-large) perform in the multi-perspective stance detection setting compared to their base versions? This remains unresolved because the study only used BERT-base and RoBERTa-base models due to computational constraints. Experimental results comparing base and large versions in both baseline and multi-perspective settings would resolve this question.

### Open Question 2
How does the level of annotator disagreement affect model performance and confidence scores in stance detection? The authors plan to have a more detailed analysis related to annotator disagreement level on model performance as well as confidence scores, but the current study doesn't provide this analysis. Correlation analysis between disagreement levels and model performance/confidence scores would resolve this question.

### Open Question 3
Does the multi-perspective approach improve stance detection performance when applied to different types of queries or topics? The authors mention applying the methodology to ideology detection as a future direction. The current study doesn't analyze performance across different query types or topics. Performance comparison across different query categories would resolve this question.

## Limitations
- Small dataset size (1,062 documents after preprocessing) raises concerns about overfitting and generalizability
- Dataset's slight class imbalance toward neutral labels may skew performance metrics for minority classes
- Limited direct analysis of the relationship between annotator disagreement and model confidence

## Confidence
- Multi-perspective approach outperforming baseline: High
- Chunking universally improving performance: Medium
- Annotator disagreement affecting model confidence: Low

## Next Checks
1. Apply the multi-perspective approach to a larger, independently collected stance detection dataset to verify that performance gains are not dataset-specific artifacts.
2. Systematically measure the correlation between model confidence scores and actual annotator disagreement levels across different document types and stance categories.
3. Remove chunking from the best-performing multi-perspective model and test whether performance degrades, confirming chunking's contribution to observed improvements.