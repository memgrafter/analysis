---
ver: rpa2
title: 'EPO: Hierarchical LLM Agents with Environment Preference Optimization'
arxiv_id: '2408.16090'
source_url: https://arxiv.org/abs/2408.16090
tags:
- environment
- reward
- feedback
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical LLM-based framework for long-horizon
  decision-making tasks, addressing the challenge of extensive planning over multiple
  steps. The method decomposes complex tasks into manageable subgoals using separate
  LLMs for subgoal prediction and low-level action generation.
---

# EPO: Hierarchical LLM Agents with Environment Preference Optimization

## Quick Facts
- arXiv ID: 2408.16090
- Source URL: https://arxiv.org/abs/2408.16090
- Reference count: 34
- Primary result: Achieved first place on ALFRED public leaderboard with 12% improvement on unseen test tasks

## Executive Summary
This paper introduces EPO, a hierarchical framework that leverages large language models for long-horizon decision-making tasks. The method addresses the challenge of extensive planning over multiple steps by decomposing complex tasks into manageable subgoals using separate LLMs for subgoal prediction and low-level action generation. To overcome the lack of annotated training data, the framework employs Environment Preference Optimization (EPO) to automatically generate preference signals from multimodal environment feedback. The approach demonstrates state-of-the-art performance on the ALFRED benchmark, achieving first place on the public leaderboard.

## Method Summary
EPO introduces a hierarchical framework where two specialized LLMs work in tandem: one predicts high-level subgoals while another generates low-level actions to achieve them. The key innovation is Environment Preference Optimization, which generates preference signals from the environment's feedback to train the agents without requiring human-annotated data. The reward model leverages multimodal environment feedback to automatically create training signals for unannotated datasets. This approach enables the system to learn effectively from environmental interactions rather than relying on costly human supervision.

## Key Results
- Achieved first place on ALFRED public leaderboard
- Demonstrated 12% improvement on unseen test tasks compared to previous work
- Maintained high performance on both seen and unseen scenarios
- Showed superior performance on long-horizon decision-making tasks compared to existing methods

## Why This Works (Mechanism)
The framework works by decomposing complex, long-horizon tasks into manageable subgoals that can be predicted by one LLM, while a second LLM focuses on generating the low-level actions needed to achieve those subgoals. The Environment Preference Optimization method generates preference signals from the environment's feedback, creating a training signal that doesn't require human annotation. This approach is particularly effective for long-horizon tasks where traditional reinforcement learning would struggle due to sparse rewards and extensive planning requirements.

## Foundational Learning
- **Hierarchical task decomposition**: Breaking complex tasks into subgoals and actions - needed because long-horizon tasks overwhelm single-model planning; quick check: measure planning depth before and after decomposition
- **Multimodal feedback integration**: Combining different types of environmental signals - needed because single-modality rewards are often insufficient for complex tasks; quick check: ablate individual modalities to measure contribution
- **Preference-based learning**: Using preference signals instead of explicit rewards - needed because ground-truth rewards are often unavailable in real-world scenarios; quick check: compare performance against explicit reward baselines
- **Automated reward signal generation**: Creating training signals without human annotation - needed because manual annotation is prohibitively expensive for large-scale training; quick check: validate generated rewards against human judgments
- **Separate subgoal and action prediction**: Specialized models for different planning levels - needed because high-level strategy and low-level execution require different capabilities; quick check: measure performance of unified vs. separated approaches

## Architecture Onboarding

**Component map**: Environment -> Multimodal Feedback Processor -> Reward Model -> Preference Signal Generator -> EPO Trainer -> LLM Subgoal Predictor and Action Generator -> Environment

**Critical path**: Environment feedback → Multimodal processing → Reward modeling → Preference generation → EPO training → Hierarchical LLM execution

**Design tradeoffs**: 
- Separation of subgoal and action prediction vs. unified model (chosen for specialization, sacrifices end-to-end optimization)
- Automated reward generation vs. human annotation (chosen for scalability, sacrifices potential precision)
- Multimodal vs. single-modality feedback (chosen for robustness, increases complexity)

**Failure signatures**:
- Poor subgoal prediction leading to cascading execution failures
- Reward model bias causing suboptimal preference signals
- Disconnection between high-level planning and low-level execution
- Overfitting to training environments resulting in poor generalization

**Three first experiments**:
1. Compare hierarchical vs. flat LLM approaches on simple long-horizon tasks
2. Ablate individual modalities in the reward model to measure contribution
3. Test EPO performance with and without preference signal generation

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance gains based on a single benchmark (ALFRED), raising generalizability concerns
- Reliance on automated reward signal generation without ground-truth supervision may introduce biases
- Hierarchical decomposition assumes task complexity can be effectively captured by separate modules, which may not hold for all task types

## Confidence
- Hierarchical LLM framework effectiveness: Medium - Strong performance on ALFRED but limited cross-domain validation
- Environment Preference Optimization novelty: Medium - The concept appears novel but lacks comparison to existing preference optimization techniques
- Automated reward signal generation: Low - No evaluation of reward model accuracy or comparison to human-annotated rewards

## Next Checks
1. Conduct ablation studies comparing EPO's automated reward generation to human-annotated rewards on a subset of ALFRED tasks to quantify potential biases
2. Test the hierarchical framework on at least two additional long-horizon decision-making benchmarks to assess generalizability
3. Compare EPO's preference optimization approach against established methods like DPO or PPO in controlled experiments to isolate the contribution of the environmental preference signal