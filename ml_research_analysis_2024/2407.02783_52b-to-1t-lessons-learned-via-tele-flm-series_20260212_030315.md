---
ver: rpa2
title: '52B to 1T: Lessons Learned via Tele-FLM Series'
arxiv_id: '2407.02783'
source_url: https://arxiv.org/abs/2407.02783
tags:
- growth
- training
- arxiv
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors describe a progressive scaling approach that grew\
  \ a Chinese language model from 52 billion to 1 trillion parameters, releasing the\
  \ final checkpoint. They first trained a 52B base model (Tele-FLM-52B), then conducted\
  \ supervised fine-tuning on a small, high-quality dataset (30k samples focused on\
  \ mathematics and coding) and achieved competitive alignment performance\u201492%\
  \ of GPT-4-1106-preview on their TeleEval benchmark and 91% on AlignBench\u2014\
  demonstrating that modest, targeted data can yield strong instruction-following\
  \ capabilities."
---

# 52B to 1T: Lessons Learned via Tele-FLM Series

## Quick Facts
- arXiv ID: 2407.02783
- Source URL: https://arxiv.org/abs/2407.02783
- Reference count: 31
- Primary result: Progressive scaling from 52B to 1T parameters using function-preserving growth techniques, with 30k SFT samples achieving 92% GPT-4-1106-preview performance on TeleEval

## Executive Summary
This paper presents a progressive scaling approach that grew a Chinese language model from 52 billion to 1 trillion parameters, demonstrating that function-preserving growth techniques can effectively transfer knowledge between scaling stages. The authors first trained a 52B base model (Tele-FLM-52B), then conducted supervised fine-tuning on a small, high-quality dataset (30k samples focused on mathematics and coding) and achieved competitive alignment performance—92% of GPT-4-1106-preview on their TeleEval benchmark and 91% on AlignBench—demonstrating that modest, targeted data can yield strong instruction-following capabilities. Next, they scaled the model in two stages: 52B→102B and 102B→1T, using a function-preserving growth method (MSG) with depth and width expansion, careful layer selection, and gradual integration of new parameters. Training consumed 2.3 trillion tokens with stage-specific learning rates and growth transition steps.

## Method Summary
The authors employed a progressive scaling methodology using the Masked Structural Growth (MSG) framework to expand a Chinese language model from 52B to 1T parameters. They first trained the base Tele-FLM-52B model using a GPT-style decoder-only transformer with pre-normalization, RMSNorm, SwiGLU activation, RoPE embeddings, and untied embedding layers. Supervised fine-tuning was then conducted on 30k high-quality samples (25k mathematics, 5k coding/dialogue) to achieve strong instruction-following performance. The model was scaled in two growth stages: width growth (increasing hidden_dim, head_num, ffn_dim) followed by depth growth (increasing layer_num), with function-preserving techniques using external masks to maintain knowledge transfer. Each stage employed specific learning rates and growth transition steps, with careful parameter initialization and layer selection based on distance metrics to optimize convergence.

## Key Results
- Achieved 92% GPT-4-1106-preview performance on TeleEval benchmark using only 30k SFT samples
- Successfully scaled model from 52B to 1T parameters using MSG function-preserving growth technique
- Demonstrated competitive alignment performance (91% on AlignBench) with minimal instruction-tuning data
- Showed that progressive scaling with staged training and appropriate hyperparameter tuning enables efficient training of extremely large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small, high-quality SFT datasets can achieve strong instruction-following performance without requiring large volumes of data.
- Mechanism: The model leverages its pre-trained knowledge base effectively, so targeted fine-tuning on specific tasks (math, coding) is sufficient to elicit instruction-following capabilities.
- Core assumption: The foundation model already encodes broad general knowledge and only needs task-specific alignment rather than broad instruction coverage.
- Evidence anchors:
  - [abstract] "we first discuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which supports the 'less is more' approach for SFT data construction"
  - [section] "we find that learning the instruct-following format from a modest amount of data is sufficient for a wide range of language understanding and generation tasks"
  - [corpus] weak - no direct corpus evidence supporting SFT effectiveness claims
- Break condition: If the foundation model lacks sufficient general knowledge or if tasks require complex reasoning not captured in the pre-training phase.

### Mechanism 2
- Claim: Function-preserving growth techniques enable smooth scaling from 52B to 1T parameters without catastrophic forgetting or performance degradation.
- Mechanism: MSG (Masked Structural Growth) uses external masks to preserve model function during expansion, gradually integrating new parameters while maintaining knowledge from previous stages.
- Core assumption: The knowledge encoded in smaller models can be effectively transferred and expanded without loss when growth is carefully controlled with appropriate masks and layer selection.
- Evidence anchors:
  - [abstract] "demonstrate our experiments and analyses on the best practices for progressively growing a model from 52 billion to 102 billion, and subsequently to 1 trillion parameters"
  - [section] "the central strategy involves expanding the model's structure during the pre-training phase and utilizing function-preserving growth techniques [6; 26; 12] to transfer knowledge seamlessly from one stage to the next"
  - [corpus] weak - no direct corpus evidence supporting growth effectiveness claims
- Break condition: If growth transition steps are insufficient or if layer selection criteria don't maintain proper knowledge flow.

### Mechanism 3
- Claim: Progressive scaling with staged training and appropriate hyperparameter tuning enables efficient training of extremely large models within limited computational budgets.
- Mechanism: The model is trained in stages (52B → 102B → 1T) with careful learning rate scheduling, growth transition steps, and parameter initialization strategies that maintain stability and convergence.
- Core assumption: Staged growth with appropriate learning rate decay and transition management prevents divergence while allowing the model to recover and build upon knowledge from previous stages.
- Evidence anchors:
  - [abstract] "we demonstrate our experiments and analyses on the best practices for progressively growing a model from 52 billion to 102 billion, and subsequently to 1 trillion parameters"
  - [section] "The entire training process utilizes 2318.7B training tokens... Separate learning rates are set for vector-like and matrix-like parameters"
  - [corpus] weak - no direct corpus evidence supporting scaling effectiveness claims
- Break condition: If learning rates are not properly calibrated between stages or if growth transition steps are misconfigured.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) for instruction following
  - Why needed here: The paper demonstrates that SFT on a small, high-quality dataset can elicit strong instruction-following capabilities from pre-trained models
  - Quick check question: How does the "less is more" approach in SFT differ from traditional approaches that use large instruction datasets?

- Concept: Function-preserving model growth techniques
  - Why needed here: The paper scales models from 52B to 1T parameters using MSG, requiring understanding of how to expand models without losing pre-existing knowledge
  - Quick check question: What role do masks play in the MSG growth framework?

- Concept: Progressive learning and staged training
  - Why needed here: The model is trained in stages with different parameter counts and learning rates, requiring understanding of how to maintain stability across growth phases
  - Quick check question: Why is it important to calibrate learning rates between different growth stages?

## Architecture Onboarding

- Component map: GPT-style decoder-only transformer with pre-normalization, RMSNorm for normalization, SwiGLU activation, Rotary Positional Embedding (RoPE), untied embedding layer, and disabled linear bias in attention and MLP modules

- Critical path: The growth process from 52B to 1T involves two main phases: width growth (increasing hidden_dim, head_num, ffn_dim) and depth growth (increasing layer_num), with careful parameter initialization and mask-based integration

- Design tradeoffs: The choice between different ffn_dim ratios (default 8/3 vs. optimized values) and the decision to use distance-based layer selection for depth growth represent key architectural decisions that affect model performance and convergence

- Failure signatures: Loss fluctuations during post-growth transition, representation collapse in pre-normalization architecture, and insufficient knowledge transfer between growth stages are key failure modes to monitor

- First 3 experiments:
  1. Test SFT effectiveness with varying dataset sizes (e.g., 5k, 15k, 30k samples) on Tele-FLM-52B to verify the "less is more" hypothesis
  2. Compare different layer selection criteria (Euclidean vs. cosine distance) during depth growth to identify optimal knowledge transfer
  3. Test different ffn_dim ratios at each growth stage to find optimal architectural configurations for convergence and loss reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tele-FLM-1T model's performance compare to a 1T model trained from scratch, particularly in terms of convergence speed and final loss?
- Basis in paper: [inferred] The authors state they cannot investigate the actual performance of their 1T model vs. one trained from scratch due to limited computational budgets.
- Why unresolved: The authors did not have the resources to train a 1T model from scratch for direct comparison.
- What evidence would resolve it: A comprehensive benchmark evaluation comparing Tele-FLM-1T with a 1T model trained from scratch, including convergence curves and final performance metrics.

### Open Question 2
- Question: What are the optimal learning rate schedules and initialization strategies for models exceeding 1 trillion parameters, especially when grown from an existing model?
- Basis in paper: [explicit] The authors mention that training hyperparameters listed in Table 4 are effective, yet further exploration is necessary to refine operators, initialization, and schedules for models exceeding 1T parameters.
- Why unresolved: The authors acknowledge the need for further research to optimize training for extremely large models.
- What evidence would resolve it: Systematic experiments varying learning rate schedules and initialization strategies for models larger than 1T, measuring their impact on convergence and final performance.

### Open Question 3
- Question: How much data is required to achieve strong performance on complex reasoning tasks (e.g., advanced mathematics, logical reasoning) for instruction-tuned models?
- Basis in paper: [explicit] The authors observe that fine-tuning with elementary-level maths problems does not translate well to more challenging tasks, indicating that more sophisticated techniques and larger volumes of high-quality data may be required for reasoning tasks.
- Why unresolved: The authors' experiments with 30k samples focused on elementary-level maths did not yield strong performance on complex reasoning tasks.
- What evidence would resolve it: Experiments with varying amounts of high-quality data, including intermediate steps and chain-of-thought supervision, to determine the minimum data requirements for strong performance on complex reasoning tasks.

## Limitations

- Lack of ablation studies to isolate the contribution of individual components like MSG growth rules, learning rate schedules, and parameter initialization strategies
- Limited evidence supporting the "less is more" hypothesis for SFT data construction, with no head-to-head comparisons with larger SFT datasets
- Insufficient detail on the distribution of training data across different stages, making it difficult to assess whether performance improvements are due to model scaling, data quality, or both

## Confidence

- **High confidence**: The architectural details and training procedures for the base 52B model are well-specified and reproducible. The use of function-preserving growth techniques for model expansion is a validated approach in the literature.
- **Medium confidence**: The reported alignment performance (92% GPT-4-1106-preview on TeleEval, 91% on AlignBench) appears reasonable given the small SFT dataset size, but lacks comparative baselines with other instruction-tuning approaches.
- **Low confidence**: The claim that "less is more" for SFT data construction is supported by limited evidence. The paper does not provide sufficient ablation studies or comparisons with larger SFT datasets to validate this hypothesis.

## Next Checks

1. **Ablation study on SFT dataset size**: Conduct controlled experiments varying SFT dataset sizes (e.g., 5k, 15k, 30k samples) on Tele-FLM-52B to empirically test the "less is more" hypothesis and determine the optimal dataset size for instruction-following performance.

2. **Growth stage validation**: Perform detailed analysis of loss trajectories and knowledge retention during each growth transition (52B→102B and 102B→1T), including input-output distance metrics and performance on downstream tasks at each stage to verify that function-preserving growth maintains model capabilities.

3. **Comparative alignment evaluation**: Compare the performance of the Tele-FLM models with other instruction-tuned models using identical evaluation benchmarks (TeleEval, AlignBench) and varying amounts of SFT data to determine whether the reported performance gains are due to model scaling, data quality, or both.