---
ver: rpa2
title: 'Evaluating Creativity and Deception in Large Language Models: A Simulation
  Framework for Multi-Agent Balderdash'
arxiv_id: '2411.10422'
source_url: https://arxiv.org/abs/2411.10422
tags:
- definition
- game
- llms
- word
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simulation framework using the game Balderdash
  to evaluate both the creativity and logical reasoning of Large Language Models (LLMs).
  In Balderdash, players generate plausible yet fictitious definitions for obscure
  terms to deceive others while identifying correct definitions.
---

# Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash

## Quick Facts
- arXiv ID: 2411.10422
- Source URL: https://arxiv.org/abs/2411.10422
- Authors: Parsa Hejabi; Elnaz Rahmati; Alireza S. Ziabari; Preni Golazizian; Jesse Thomason; Morteza Dehghani
- Reference count: 12
- Primary result: Introduces a simulation framework using the game Balderdash to evaluate both creativity and logical reasoning of Large Language Models

## Executive Summary
This paper presents a novel framework for evaluating LLM creativity and deception by simulating the game Balderdash, where players generate plausible definitions for obscure terms to deceive others. The framework implements a centralized game engine with multiple LLM agents as participants and an LLM judge to evaluate semantic equivalence between generated and reference definitions. Through experiments with different LLMs (Llama, Phi, Gemma, Mistral, GPT), the study reveals that infrequent vocabulary in inputs leads to poor reasoning on game rules and historical context, while frequent vocabulary enables better performance.

## Method Summary
The framework implements a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. The game follows Balderdash rules where players generate definitions for obscure words, then vote on which definition they believe is correct. The judge LLM determines whether generated definitions are semantically equivalent to reference dictionary definitions. Multiple experiments analyze performance metrics including True Definition Ratio, Deception Ratio, and Correct Guess Ratio across different LLMs and word frequencies.

## Key Results
- LLMs perform significantly better with frequent vocabulary compared to infrequent vocabulary in Balderdash game contexts
- The framework successfully measures both creative definition generation and strategic deception capabilities
- Different LLM models show varying strengths in balancing creativity with plausibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate plausible definitions for obscure words when the words are frequent enough in their training data.
- Mechanism: The model leverages statistical co-occurrence patterns learned during pretraining to construct semantically plausible definitions for words it has encountered often.
- Core assumption: The frequency of a word in the pretraining corpus correlates with the model's ability to reason about it and generate accurate or deceptive definitions.
- Evidence anchors:
  - [abstract] "Specifically, the study reveals that infrequent vocabulary in LLMs' input leads to poor reasoning on game rules and historical context."
  - [section] "According to Kang and Choi (2023), LLMs are biased towards frequent words and co-occurrences, making them vulnerable and unpredictable when infrequent words are used in the input."
  - [corpus] "Found 25 related papers... Top related titles: Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions, The Traitors: Deception and Trust in Multi-Agent Language Model Simulations, Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia."
- Break condition: The word frequency in the input falls below a threshold where the model's learned co-occurrence patterns are insufficient for plausible definition generation.

### Mechanism 2
- Claim: LLMs can evaluate semantic equivalence between a generated definition and a reference dictionary definition when provided with explicit comparison instructions.
- Mechanism: The model processes both definitions in context, applying learned semantic understanding to determine if the core concepts align, outputting a binary judgment.
- Core assumption: The model's internal semantic representation is sufficient to judge equivalence even when definitions differ in phrasing or detail.
- Evidence anchors:
  - [abstract] "We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence."
  - [section] "Following Zheng et al. (2024), where an LLM is used to evaluate open-domain question-answering, we use an LLM as the judge to determine whether each generated definition is semantically equivalent to the reference dictionary definition."
  - [corpus] "Weak corpus evidence: no direct studies found on LLM-based semantic equivalence judgment in creative contexts."
- Break condition: The definitions are too semantically distant or the model's internal representation fails to capture the nuanced equivalence required.

### Mechanism 3
- Claim: LLMs can learn to optimize for scoring strategies in a game environment when provided with clear scoring rules and performance feedback.
- Mechanism: The model updates its generation strategy based on the reward structure defined in the game rules, attempting to maximize points through strategic definition generation and voting.
- Core assumption: The model can perform policy-like updates based on the scoring outcomes, even without explicit reinforcement learning.
- Evidence anchors:
  - [abstract] "The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement."
  - [section] "In this paper, we aim to assess the creativity of LLMs by evaluating their ability to generate plausible definitions for obscure words in Balderdash."
  - [corpus] "Weak corpus evidence: no direct studies found on LLM strategy optimization in Balderdash-like games without RL."
- Break condition: The scoring feedback is ambiguous or the model cannot infer the optimal strategy from the game rules alone.

## Foundational Learning

- Concept: Semantic equivalence judgment
  - Why needed here: The judge LLM must determine whether generated definitions match the reference dictionary definition to enforce game rules.
  - Quick check question: Can the model reliably distinguish between definitions that capture the same core concept versus those that do not, even with different phrasing?

- Concept: Frequency-based reasoning bias
  - Why needed here: The model's performance varies significantly based on word frequency in the input, affecting both definition generation and semantic equivalence judgment.
  - Quick check question: Does the model's ability to generate plausible definitions correlate with the frequency of the word in its training data?

- Concept: Multi-agent strategic interaction
  - Why needed here: Multiple LLM agents must interact within the game framework, requiring coordination and competition based on the same set of rules.
  - Quick check question: Can the model adapt its strategy based on the performance history of other agents in the game?

## Architecture Onboarding

- Component map: Game Engine -> LLM Players -> Judge LLM -> Word Decks -> History Module -> Database
- Critical path: 1) Game Engine initializes with LLM players and judge 2) Word is drawn from deck and announced 3) Players generate definitions 4) Judge evaluates semantic equivalence 5) Players vote on correct definition 6) Scoring calculated 7) Results stored and history updated
- Design tradeoffs:
  - Using an LLM as judge provides flexibility but introduces potential bias and consistency issues
  - High temperature settings increase creativity but reduce reliability in definition generation
  - Providing full history to players increases strategic depth but may overwhelm the model's context window
- Failure signatures:
  - Judge LLM consistently rejects valid definitions due to overly strict semantic criteria
  - Players fail to adapt strategies even with clear scoring feedback
  - Model outputs become repetitive or nonsensical at high temperatures
  - Semantic equivalence judgments show high variance across similar definition pairs
- First 3 experiments:
  1. Evaluate baseline performance of different LLMs on frequent vs. infrequent word decks without history
  2. Test convergence to optimal strategy when a dominant scoring method exists, with and without history
  3. Assess reasoning ability by changing game rules to create clear optimal strategies and observing model adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of infrequent vocabulary in LLM inputs significantly impact their reasoning abilities across different game contexts beyond Balderdash?
- Basis in paper: Explicit - The paper states that "infrequent vocabulary in LLMs' input leads to poor reasoning on game rules and historical context."
- Why unresolved: The study primarily focuses on the Balderdash game, and while it suggests that infrequent vocabulary impacts reasoning, it does not explore whether this issue persists across various other game contexts or tasks.
- What evidence would resolve it: Conducting experiments with LLMs in a variety of games and tasks that use infrequent vocabulary to compare reasoning performance with frequent vocabulary tasks would provide evidence to support or refute this hypothesis.

### Open Question 2
- Question: How does the self-enhancement bias in LLMs affect their performance when acting as judges in games like Balderdash?
- Basis in paper: Explicit - The paper acknowledges the possibility of self-enhancement bias when using LLMs as judges and suggests evaluating each LLM as a judge on definitions generated by other models.
- Why unresolved: The paper does not provide experimental data on how self-enhancement bias affects the accuracy of LLMs as judges, nor does it compare the performance of LLMs with that of human judges.
- What evidence would resolve it: Implementing experiments where each LLM judges definitions generated by other models and comparing these results with human judgment would clarify the extent of self-enhancement bias.

### Open Question 3
- Question: Would replacing the LLM judge with a specialized model trained specifically to discriminate between true and deceiving definitions improve the reliability of the Balderdash game simulation?
- Basis in paper: Explicit - The paper suggests that replacing the LLM judge with a specialized model could result in higher accuracy and a more reliable game simulation system.
- Why unresolved: The paper does not explore or provide data on the performance of a specialized model versus an LLM judge, leaving the potential benefits and limitations of such a replacement unexplored.
- What evidence would resolve it: Developing and testing a specialized model designed to discriminate between true and deceiving definitions, and comparing its performance to that of an LLM judge, would provide insights into the feasibility and effectiveness of this approach.

## Limitations

- Unknown implementation details of the game engine and prompt templates prevent faithful reproduction
- Reliance on LLM-based semantic equivalence judgment introduces potential consistency issues
- Performance heavily dependent on word frequency in training data, with infrequent vocabulary causing significant degradation

## Confidence

**High Confidence:** The framework's basic architecture and the observation that LLMs perform better with frequent versus infrequent vocabulary are well-supported.

**Medium Confidence:** The mechanisms for semantic equivalence judgment and strategy optimization through scoring feedback are plausible but lack direct empirical validation within the study.

**Low Confidence:** The specific performance metrics and their interpretation across different LLM models require further validation, particularly given the unknown prompt configurations.

## Next Checks

1. **Semantic Equivalence Validation**: Test the judge LLM's consistency by presenting it with pairs of definitions that have varying degrees of semantic similarity, including paraphrases and near-misses, to establish reliability thresholds for Balderdash contexts.

2. **Frequency Threshold Analysis**: Systematically evaluate LLM performance across a spectrum of word frequencies to identify the critical threshold below which definition generation quality degrades significantly, confirming the relationship between training data exposure and creative output.

3. **Strategy Optimization Test**: Implement controlled experiments where optimal strategies are mathematically provable (e.g., always vote for your own definition when possible) to verify whether LLMs can learn and consistently apply these strategies across multiple game rounds.