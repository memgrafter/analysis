---
ver: rpa2
title: Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation
arxiv_id: '2408.09698'
source_url: https://arxiv.org/abs/2408.09698
tags:
- multimodal
- recommendation
- user
- sequential
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of integrating Multimodal Large
  Language Models (MLLMs) into sequential recommendation systems, where handling multiple
  image inputs and capturing dynamic user preferences are key obstacles. The proposed
  MLLM-MSR model introduces a two-stage user preference summarization approach: first,
  it uses MLLMs to convert image features into text, then employs a recurrent user
  preference summarization generation paradigm to capture dynamic changes in preferences.'
---

# Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation

## Quick Facts
- arXiv ID: 2408.09698
- Source URL: https://arxiv.org/abs/2408.09698
- Reference count: 14
- Primary result: MLLM-MSR achieves AUC scores of 83.17 (Microlens), 84.39 (Amazon-Baby), and 85.69 (Amazon-Game)

## Executive Summary
This paper addresses the challenge of integrating Multimodal Large Language Models (MLLMs) into sequential recommendation systems, where handling multiple image inputs and capturing dynamic user preferences are key obstacles. The proposed MLLM-MSR model introduces a two-stage user preference summarization approach: first, it uses MLLMs to convert image features into text, then employs a recurrent user preference summarization generation paradigm to capture dynamic changes in preferences. Finally, it fine-tunes an MLLM-based recommender using Supervised Fine-Tuning (SFT) techniques. Experiments across three datasets demonstrate superior performance, with MLLM-MSR achieving AUC scores of 83.17 (Microlens), 84.39 (Amazon-Baby), and 85.69 (Amazon-Game), outperforming baselines like TALLREC and LLaVA.

## Method Summary
The paper proposes a framework that addresses the challenge of integrating MLLMs into sequential recommendation systems by converting image features to text summaries and using recurrent user preference inference across session-based blocks. The approach involves three key stages: image-to-text conversion using MLLMs, recurrent preference summarization across segmented blocks, and SFT-based fine-tuning of the MLLM recommender. The system processes multimodal data by first extracting image features, converting them to text summaries, segmenting interactions into blocks, and then using an MLLM to infer and update user preferences at each block before final prediction.

## Key Results
- MLLM-MSR achieves AUC scores of 83.17, 84.39, and 85.69 on Microlens, Amazon-Baby, and Amazon-Game datasets respectively
- Outperforms baseline models TALLREC and LLaVA across all three datasets
- Image-to-text conversion preserves sufficient semantic information for sequential modeling, performing comparably to direct image processing with VGG19
- The block-based approach effectively handles long sequences while maintaining preference dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image-to-text summarization via MLLMs preserves semantic information needed for sequential modeling while avoiding direct multi-image input limitations.
- Mechanism: The system uses MLLMs to convert image features into textual summaries, which are then processed by standard sequential models (like GRU4Rec). This converts complex multi-image inputs into manageable text sequences.
- Core assumption: Textual summaries retain sufficient semantic content to maintain predictive performance compared to direct image processing.
- Evidence anchors:
  - [section]: "Using image summaries allows the GRU4Rec model to perform comparably to direct processing with VGG19, confirming that our image summary approach preserves necessary semantic information in sequential modeling."
  - [abstract]: "we first utilize an MLLM-based item-summarizer to extract image feature given an item and convert the image into text."
  - [corpus]: Weak - no direct corpus evidence found, though related work on image summarization exists in broader literature.
- Break condition: If image summaries lose critical visual details that affect recommendation quality, or if the MLLM summarization introduces significant noise or bias.

### Mechanism 2
- Claim: Recurrent user preference inference through session-based blocks captures dynamic preference evolution while avoiding prompt length limitations.
- Mechanism: The system segments user interaction sequences into blocks, each covering interactions within a session. At each block, the MLLM processes the item summaries along with the previously inferred user preference summary to generate an updated preference representation.
- Core assumption: Breaking long sequences into manageable blocks preserves temporal dynamics while maintaining context continuity.
- Evidence anchors:
  - [section]: "we segment item interactions into several blocks, each covering interactions within a defined session, converting long multimodal sequences into concise textual narratives"
  - [abstract]: "we employ a recurrent user preference summarization generation paradigm to capture the dynamic changes in user preferences based on an LLM-based user-summarizer"
  - [corpus]: Weak - related work exists on session-based recommendation but not specifically on MLLM-based recurrent preference inference.
- Break condition: If block boundaries disrupt meaningful preference patterns, or if the preference summary becomes too abstract to capture fine-grained user interests.

### Mechanism 3
- Claim: Supervised fine-tuning with contrastive negative sampling enables the MLLM to function effectively as a recommender while preserving pre-trained capabilities.
- Mechanism: The MLLM is fine-tuned using SFT techniques on a dataset constructed with 1:1 positive-to-negative ratios for training and 1:20 for evaluation, using LoRA for parameter-efficient adaptation.
- Core assumption: Contrastive learning with carefully constructed negative samples improves the model's ability to distinguish relevant from irrelevant items.
- Evidence anchors:
  - [section]: "Our dataset construction strategy employs negative sampling, a commonly used training technique in recommendation systems, wherein each positive user-item interaction is coupled with multiple negative samples"
  - [abstract]: "we propose to fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT) techniques"
  - [corpus]: Weak - related work on SFT exists but specific evidence for this multimodal sequential recommendation context is limited.
- Break condition: If fine-tuning causes catastrophic forgetting of general MLLM capabilities, or if negative sampling fails to provide meaningful contrastive examples.

## Foundational Learning

- Concept: Multimodal data fusion techniques
  - Why needed here: The system needs to combine image and text information from items into unified representations
  - Quick check question: What are the differences between early, late, and hybrid fusion approaches in multimodal systems?

- Concept: Sequential modeling and temporal dynamics
  - Why needed here: User preferences evolve over time, requiring models that can capture and adapt to these changes
  - Quick check question: How do recurrent neural networks differ from transformer-based approaches in handling sequential dependencies?

- Concept: Large language model fine-tuning strategies
  - Why needed here: The MLLM needs to be adapted from general-purpose understanding to specific recommendation tasks
  - Quick check question: What are the advantages and disadvantages of full fine-tuning versus parameter-efficient methods like LoRA?

## Architecture Onboarding

- Component map: Image-to-text MLLM → Item summarization → Block segmentation → Recurrent preference inference MLLM → SFT fine-tuned recommender MLLM → Prediction
- Critical path: User-item interaction → Image and text processing → Item summary generation → Preference inference across blocks → Final recommendation prediction
- Design tradeoffs: Direct image processing vs. image-to-text conversion (computational efficiency vs. potential information loss); full fine-tuning vs. LoRA (performance vs. parameter efficiency)
- Failure signatures: Poor performance on datasets with visually distinctive items (image summaries insufficient); degraded performance as sequence length increases (block segmentation ineffective); overfitting to training data (insufficient negative sampling or regularization)
- First 3 experiments:
  1. Baseline comparison: Test MLLM-MSR against GRU4Rec with VGG19 features to validate the image-to-text approach
  2. Block size sensitivity: Evaluate performance across different block sizes to find optimal balance between context and computational efficiency
  3. Ablation study: Compare full MLLM-MSR against variants without image summaries and without recurrent preference inference to isolate component contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLMs be effectively fine-tuned to handle long sequences of multimodal data while maintaining scalability and efficiency?
- Basis in paper: [explicit] The paper discusses the challenges of processing sequential multimodal data, particularly with multiple ordered image inputs, and highlights the need for innovative approaches to navigate these complexities.
- Why unresolved: The paper identifies the challenge but does not provide a detailed solution or methodology for fine-tuning MLLMs to handle long sequences effectively.
- What evidence would resolve it: Empirical results demonstrating improved performance and scalability of MLLMs when fine-tuned with specific techniques for long multimodal sequences.

### Open Question 2
- Question: What are the optimal strategies for balancing context length in user preference generation to maximize information utility without increasing computational complexity?
- Basis in paper: [explicit] The paper mentions that short context lengths can lead to information loss, while longer contexts stabilize results, indicating a need for optimal context length selection.
- Why unresolved: The paper does not provide a definitive strategy or framework for determining the optimal context length for user preference generation.
- What evidence would resolve it: Comparative studies showing the impact of different context lengths on recommendation accuracy and computational efficiency.

### Open Question 3
- Question: How can the interpretability of MLLM-based recommendations be enhanced beyond the current method of providing detailed user preference descriptions?
- Basis in paper: [inferred] The paper discusses the interpretability of recommendations by providing detailed user preference descriptions but does not explore further enhancements.
- Why unresolved: The paper focuses on the current method of interpretability but does not explore additional techniques or frameworks for enhancing interpretability.
- What evidence would resolve it: Development and validation of new interpretability techniques that provide clearer insights into the recommendation process.

## Limitations
- The paper relies heavily on image-to-text conversion, which may lose fine-grained visual features critical for recommendation accuracy
- The block-based approach assumes session boundaries align with meaningful preference shifts, which may not hold across all user behaviors
- Negative sampling strategies are not deeply explored, and the 1:20 evaluation ratio could be suboptimal

## Confidence

- Image-to-text summarization preserving semantic information: **Medium** - Supported by GRU4Rec comparison but lacks direct ablation with original images
- Recurrent preference inference capturing dynamic changes: **Medium** - Block segmentation is intuitive but effectiveness depends on session definition
- Supervised fine-tuning with contrastive sampling: **Medium** - Common approach but specific configuration choices lack comparative justification

## Next Checks
1. Conduct ablation studies comparing MLLM-MSR performance with and without image summaries using identical MLLM architectures
2. Test across diverse datasets with varying visual distinctiveness to assess robustness of image-to-text conversion
3. Experiment with alternative negative sampling strategies and ratios to optimize the fine-tuning process