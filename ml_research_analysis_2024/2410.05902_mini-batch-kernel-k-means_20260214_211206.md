---
ver: rpa2
title: Mini-Batch Kernel $k$-means
arxiv_id: '2410.05902'
source_url: https://arxiv.org/abs/2410.05902
tags:
- kernel
- algorithm
- k-means
- time
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first mini-batch kernel k-means algorithm,\
  \ achieving an order of magnitude speedup over the full-batch version. The algorithm\
  \ runs in O(kb\xB2) time per iteration compared to O(n\xB2) for the full batch,\
  \ where n is the dataset size and b is the batch size."
---

# Mini-Batch Kernel $k$-means

## Quick Facts
- arXiv ID: 2410.05902
- Source URL: https://arxiv.org/abs/2410.05902
- Reference count: 25
- Primary result: First mini-batch kernel k-means algorithm achieving 10-100x speedup with minimal quality loss

## Executive Summary
This paper introduces the first mini-batch kernel k-means algorithm, achieving an order of magnitude speedup over the full-batch version. The algorithm runs in O(kb²) time per iteration compared to O(n²) for the full batch, where n is the dataset size and b is the batch size. The key innovation is a truncated recursive distance update rule that maintains cluster centers as sparse linear combinations of data points, enabling efficient distance computations. Theoretical analysis shows the algorithm terminates in O(γ²/ε) iterations with high probability when the batch size is Ω(max{γ⁴, γ²}·ε⁻² log²(γn/ε)), where γ bounds the norm of points in feature space and ε is a termination threshold.

## Method Summary
The method introduces a truncated mini-batch kernel k-means algorithm that maintains cluster centers as sparse linear combinations of data points. The algorithm uses a recursive distance update rule where distances are computed using kernel evaluations and inner products with cluster centers. The truncation mechanism limits the number of points contributing to each center to τ, keeping centers sparse and computations efficient. The algorithm uses a learning rate αj_i = √(bj_i/b) from Schwartzman (2023) that exponentially decays the contribution of points to their centers over time. For normalized kernels like Gaussian or Laplacian where γ=1, the algorithm can terminate in O(1) iterations with batch size Θ(log n).

## Key Results
- Achieves 10-100x speedup over full-batch kernel k-means on real datasets
- Maintains clustering quality with minimal loss (ARI and NMI scores comparable to full-batch)
- Consistently outperforms both full-batch kernel k-means and non-kernel mini-batch k-means
- Works effectively with small truncation values (τ=50) far below theoretical thresholds
- Demonstrates practical efficiency across MNIST, PenDigits, Letters, and HAR datasets

## Why This Works (Mechanism)
The algorithm achieves efficiency by maintaining cluster centers as sparse linear combinations of data points rather than full n-dimensional vectors. The truncated recursive distance update allows O(kb²) time per iteration instead of O(n²) by computing distances using kernel evaluations and inner products with sparse centers. The learning rate from Schwartzman (2023) ensures older points exponentially decay in their contribution to centers, maintaining numerical stability. For normalized kernels, the γ=1 property allows rapid convergence with relatively small batch sizes.

## Foundational Learning

**Kernel methods** - Why needed: Enables computation in high-dimensional feature spaces without explicit mapping; quick check: Verify kernel evaluations produce expected values for test points

**Recursive distance updates** - Why needed: Allows efficient computation of distances without recalculating from scratch; quick check: Confirm distance updates match direct computation for small examples

**Sparse center representation** - Why needed: Maintains computational efficiency by limiting active points per center; quick check: Verify center sparsity matches τ constraint

**Learning rate decay** - Why needed: Ensures numerical stability and proper convergence behavior; quick check: Plot learning rate values over iterations to verify exponential decay

**Truncated algorithm analysis** - Why needed: Provides theoretical guarantees for practical implementation; quick check: Verify termination bounds hold on synthetic data with known γ

## Architecture Onboarding

**Component map**: Data points -> Kernel function -> Recursive distance update -> Truncated center maintenance -> Cluster assignment -> (loop)

**Critical path**: Batch selection → Kernel evaluation → Distance computation via recursive update → Truncation step → Assignment update → Center update

**Design tradeoffs**: 
- τ vs quality: Smaller τ improves speed but may degrade clustering quality
- Batch size vs convergence: Larger batches improve convergence but reduce speedup
- Kernel choice affects both γ and computational complexity

**Failure signatures**: 
- Poor clustering quality → Check kernel parameter tuning and truncation threshold
- Slow convergence → Verify learning rate implementation and batch size adequacy
- Numerical instability → Monitor kernel values and learning rate decay

**First experiments**:
1. Implement basic kernel k-means with Gaussian kernel on small synthetic dataset
2. Add mini-batch processing and compare runtime vs full-batch
3. Implement truncation mechanism and verify center sparsity constraint

## Open Questions the Paper Calls Out

**Open Question 1**: Does the truncated mini-batch kernel k-means algorithm maintain theoretical guarantees when using the sklearn learning rate instead of the learning rate from Schwartzman (2023)? The experimental results show the sklearn learning rate performs competitively in practice, but the theoretical analysis depends on the specific learning rate properties.

**Open Question 2**: What is the minimum batch size required for the truncated mini-batch kernel k-means algorithm to achieve good clustering quality in practice, and how does this compare to the theoretical bound of Θ(log n)? The paper notes empirical observation that the algorithm performs well with smaller batches than theoretically predicted.

**Open Question 3**: How does the truncated mini-batch kernel k-means algorithm perform when applied to non-Euclidean data spaces or with non-smooth kernel functions? The analysis assumes Hilbert space with feature map, and experimental results use Euclidean data with specific kernel types.

## Limitations

- Unspecified heuristic from Wang et al. (2019) for Gaussian kernel parameter selection creates uncertainty in exact replication
- Practical implementation details for maintaining sparse linear combinations are not fully described
- Theoretical guarantees depend on specific learning rate properties that may not hold for alternatives

## Confidence

**Kernel parameter tuning**: Medium - Heuristic is specified but exact values/tuning process are not provided
**Algorithm implementation**: Medium - Core mathematical framework is clear but practical details are sparse
**Experimental results**: Medium - Results should be reproducible but exact numbers may vary due to parameter tuning differences

## Next Checks

1. Implement a simplified version with fixed kernel parameters (rather than heuristic-based tuning) to establish baseline performance and verify the core algorithmic approach works as intended

2. Conduct ablation studies on the truncated center representation by comparing different data structures and optimization approaches to understand their impact on runtime and quality

3. Systematically vary the Gaussian kernel parameter κ across a range of values to map out its sensitivity and identify if the heuristic from Wang et al. is crucial for the reported performance gains