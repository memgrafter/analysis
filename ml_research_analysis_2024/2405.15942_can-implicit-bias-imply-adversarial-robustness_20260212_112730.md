---
ver: rpa2
title: Can Implicit Bias Imply Adversarial Robustness?
arxiv_id: '2405.15942'
source_url: https://arxiv.org/abs/2405.15942
tags:
- training
- networks
- network
- when
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the implicit bias of gradient-based
  training algorithms affects the adversarial robustness of neural networks. The authors
  show that when training a shallow ReLU network on data with clusters of subclasses,
  the implicit bias leads to non-robust networks.
---

# Can Implicit Bias Imply Adversarial Robustness?

## Quick Facts
- arXiv ID: 2405.15942
- Source URL: https://arxiv.org/abs/2405.15942
- Authors: Hancheng Min; René Vidal
- Reference count: 40
- Primary result: pReLU activation with p≥3 alters implicit bias to learn robust subclass representations

## Executive Summary
This paper investigates how the implicit bias of gradient-based training algorithms affects adversarial robustness in neural networks. The authors demonstrate that while ReLU networks trained on clustered data with subclasses learn non-robust representations, replacing ReLU with polynomial ReLU (pReLU) can significantly improve robustness. Through theoretical analysis and experiments, they show that pReLU promotes alignment between neurons and subclass centers rather than average class directions, resulting in classifiers robust to adversarial attacks of radius O(1).

## Method Summary
The method involves training shallow two-layer neural networks on synthetic data with K Gaussian subclasses. The key innovation is the polynomial ReLU (pReLU) activation function, where the polynomial degree p controls the implicit bias during training. The approach uses gradient flow dynamics with small initialization to analyze alignment phases, followed by fitting phases where neuron norms grow. The method is validated on synthetic data, MNIST parity tasks, and Caltech256 with pretrained features, comparing ReLU (p=1) against pReLU (p≥3) in terms of generalization and robustness.

## Key Results
- ReLU networks trained on clustered subclass data learn average class directions, resulting in vulnerability to attacks of radius ~1/√K
- pReLU networks with p≥3 learn individual subclass centers, achieving robustness to attacks of radius O(1)
- The robustness improvement is demonstrated on synthetic data, MNIST parity classification, and Caltech256 with pretrained features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit bias in ReLU networks leads to learning average class directions instead of subclass centers, causing vulnerability to attacks of radius ~1/√K.
- Mechanism: During gradient flow with small initialization, neurons align with average class centers (¯µ+ or ¯µ−) due to uniform weighting of subclass contributions when p=1.
- Core assumption: Data has K subclasses with small inter-subclass correlation, and gradient flow starts from small random initialization.
- Evidence anchors:
  - [abstract] "Specifically, they show that if the data consists of clusters with small inter-cluster correlation, a shallow (two-layer) ReLU network trained by gradient flow generalizes well, but it is not robust to adversarial attacks of small radius."
  - [section] "When p = 1 (vanilla ReLU network), the average class centers ¯µ+ and ¯µ− are those extremal vectors 'attracting' neurons during the alignment phase, leading to a trained ReLU network that has effectively two neurons."
  - [corpus] Weak - related works discuss implicit bias but don't explicitly analyze subclass center alignment vs average directions.
- Break condition: If subclass centers are not orthogonal or if initialization scale is not sufficiently small.

### Mechanism 2
- Claim: Polynomial ReLU (pReLU) with p≥3 alters implicit bias to favor learning individual subclass centers, enabling robustness to attacks of radius O(1).
- Mechanism: Higher polynomial power introduces angle-dependent weighting (cos^{p-1}) that emphasizes subclass centers closer to neuron direction during alignment phase.
- Core assumption: p is chosen large enough (≥3) but not so large as to cause vanishing gradients.
- Evidence anchors:
  - [abstract] "However, by replacing the ReLU activation with a novel polynomial ReLU (pReLU) activation, the implicit bias can be altered to favor robust networks."
  - [section] "When p ≥ 3, the subclass centers µ1, · · · , µk become extremal vectors that are 'attracting' neurons, the resulting pReLU networks successfully learn every subclass center."
  - [corpus] Weak - related works discuss implicit bias but don't explicitly analyze pReLU's effect on subclass learning.
- Break condition: If p is too large (staggering gradient flow) or too small (insufficient subclass differentiation).

### Mechanism 3
- Claim: The alignment bias is directional - neurons move toward extremal vectors determined by the activation function and data structure during early training phase.
- Mechanism: Gradient flow dynamics push neurons toward directions of x(p)(w), which depends on p and data structure, creating different alignment preferences.
- Core assumption: Training follows gradient flow dynamics with small initialization, splitting into alignment and fitting phases.
- Evidence anchors:
  - [section] "Specifically, we pose the following: Conjecture 1... if p = 1, then we have... close to F... If p ∈ [3, ¯p)... close to F(p)."
  - [section] "When p = 1 (vanilla ReLU network), the average class centers ¯µ+ and ¯µ− are those extremal vectors 'attracting' neurons during the alignment phase."
  - [corpus] Weak - related works discuss gradient flow dynamics but don't explicitly analyze directional alignment in pReLU context.
- Break condition: If data doesn't have subclass structure or if training deviates from gradient flow assumptions.

## Foundational Learning

- Concept: Implicit bias in gradient-based training
  - Why needed here: The paper's core argument is that implicit bias determines whether networks learn robust or non-robust representations.
  - Quick check question: What is the difference between the implicit bias of ReLU networks vs pReLU networks in this setting?

- Concept: Adversarial robustness and l2-radius attacks
  - Why needed here: The paper measures robustness by attack radius and shows dramatic differences between ReLU and pReLU.
  - Quick check question: Why is an attack radius of 1/√K considered non-robust while O(1) is robust?

- Concept: Neuron alignment and extremal vectors
  - Why needed here: The paper shows how neurons align with different directions depending on activation function, affecting robustness.
  - Quick check question: What determines which directions neurons align with during the early training phase?

## Architecture Onboarding

- Component map: Synthetic data generator -> Two-layer pReLU network -> Gradient flow training -> Robustness evaluation
- Critical path: Small initialization → gradient flow training → alignment phase (neuron direction learning) → fitting phase (norm growth) → robust classifier
- Design tradeoffs: Higher p increases robustness but may cause gradient vanishing; lower p is more stable but less robust
- Failure signatures: ReLU networks show vulnerability to small l2 attacks; pReLU networks may fail if p is too large
- First 3 experiments:
  1. Train ReLU network (p=1) on synthetic data with K subclasses and measure attack radius where accuracy drops
  2. Train pReLU network (p=3) on same data and compare attack radius robustness
  3. Vary p from 1 to 4 and plot robustness vs. p to find optimal range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact critical attack radius for F (p)(x) against adversarial attacks?
- Basis in paper: Explicit - The paper states that F (p)(x) is robust against attacks with radius O(1), but complementary results show it is not robust to attacks with radius slightly larger than sqrt(2)/2.
- Why unresolved: The paper provides bounds but does not give the precise critical radius.
- What evidence would resolve it: Numerical experiments testing the exact attack radius at which F (p)(x) becomes vulnerable would provide the precise critical value.

### Open Question 2
- Question: Does the proposed pReLU activation improve robustness for deep neural networks, or is its effect limited to shallow networks?
- Basis in paper: Inferred - The paper focuses on shallow networks and suggests the importance of the interplay between data structure and architecture, but does not explore deep networks.
- Why unresolved: The theoretical analysis and experiments are confined to shallow networks, leaving the behavior in deeper architectures unexplored.
- What evidence would resolve it: Experiments training deep neural networks with pReLU activation on datasets with clustered subclasses would demonstrate if the robustness gains extend beyond shallow networks.

### Open Question 3
- Question: How does the choice of the polynomial degree p in pReLU affect the alignment bias and robustness of the network?
- Basis in paper: Explicit - The paper shows that p=1 (ReLU) leads to non-robust networks, while p>=3 leads to robust networks, but the exact impact of different p values is not fully characterized.
- Why unresolved: The paper only provides results for p=1 and p=3, without exploring the intermediate values or the optimal choice of p.
- What evidence would resolve it: A systematic study varying p from 1 to a large value and measuring the alignment bias and robustness for each case would reveal the optimal choice of p.

## Limitations

- Theoretical analysis relies on gradient flow dynamics with small initialization, which may not capture practical SGD training
- Empirical validation is primarily on synthetic data with limited real dataset experiments
- No ablation studies on deeper networks to test if robustness gains transfer beyond shallow architectures

## Confidence

- **High confidence**: The mathematical derivation of gradient flow dynamics and the distinction between ReLU and pReLU alignment behavior in the synthetic setting
- **Medium confidence**: The extrapolation from shallow networks to practical deep networks, and the claim that p≥3 is sufficient for robustness
- **Low confidence**: The robustness claims on real datasets (MNIST, Caltech256) without extensive ablation studies or comparison to state-of-the-art robust training methods

## Next Checks

1. **Ablation on initialization scale**: Test whether the robustness gap between ReLU and pReLU persists when initialization variance increases beyond the small-scale regime assumed in theory
2. **Transfer to deeper architectures**: Apply pReLU to deeper networks on CIFAR-10 and evaluate whether the robustness benefits observed in shallow settings generalize
3. **Gradient flow vs. SGD comparison**: Compare theoretical predictions from gradient flow with empirical results using SGD with various learning rates and momentum settings