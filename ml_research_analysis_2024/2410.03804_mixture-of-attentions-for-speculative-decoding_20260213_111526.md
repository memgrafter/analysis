---
ver: rpa2
title: Mixture of Attentions For Speculative Decoding
arxiv_id: '2410.03804'
source_url: https://arxiv.org/abs/2410.03804
tags:
- tokens
- decoding
- layer
- mlarge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Mixture of Attentions architecture to improve
  speculative decoding, a technique to accelerate inference of large language models
  by using a smaller draft model to propose tokens that are then verified by the large
  model. The core idea is to address limitations of existing methods, such as partial
  observability (the small model lacks complete information) and lack of on-policyness
  during training (training conditions don't match inference).
---

# Mixture of Attentions For Speculative Decoding

## Quick Facts
- arXiv ID: 2410.03804
- Source URL: https://arxiv.org/abs/2410.03804
- Reference count: 28
- Proposed a Mixture of Attentions architecture achieving 9.5% increase in tokens per second and 25% increase in acceptance length over EAGLE-2

## Executive Summary
This paper addresses limitations in speculative decoding by introducing Mixture of Attentions, a novel architecture that improves both partial observability and on-policyness during training. The approach uses Layer Self-Attention to aggregate token information across all layers of the large model, Cross-Attention to simulate inference conditions during training, and a flexible Target Layer Inference mechanism to balance speed and accuracy. Experiments demonstrate state-of-the-art speedups and introduce a client-server deployment that maintains high accuracy even after complete server disconnection.

## Method Summary
The authors propose Mixture of Attentions to accelerate LLM inference by improving the small draft model used in speculative decoding. The architecture introduces three key components: Layer Self-Attention that aggregates information from all layers of the large model to mitigate partial observability, Cross-Attention that improves on-policyness by allowing the small model to attend to MLarge's activations during token prediction, and Target Layer Inference that enables flexible trade-offs between drafting speed and response quality. The model is trained on the Ultrachat dataset using forward-KL and Smooth-L1 losses, with Llama3-8B-Instruct as the large model. Both single-device and client-server deployment scenarios are evaluated.

## Key Results
- Achieved 9.5% increase in tokens per second compared to EAGLE-2 in single-device setting
- Demonstrated 25% increase in acceptance length for generated tokens
- Showed robust performance in client-server deployment, maintaining accuracy even after complete server disconnection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layer Self-Attention (LSA) mitigates partial observability by aggregating token information across all layers of the large model, providing the small model with richer contextual information than just the final layer activations.
- **Mechanism:** LSA performs self-attention over the layer dimension of the key-value tensors from all layers of the large model, then applies mean aggregation to reduce the dimensionality. This creates a compressed representation that captures relevant token information from each layer, which is then used as keys and values for the Cross-Attention layer in the small model.
- **Core assumption:** The information needed for accurate token prediction is distributed across multiple layers of the large model, not just concentrated in the final layer.
- **Evidence anchors:**
  - [abstract]: "Layer Self-Attention (LSA) to mitigate partial observability"
  - [section]: "Layer Self-Attention (LSA) followed by a mean aggregation operation to reduce its dimension to (T, 2Ekv) and extract the most relevant token information from every layer"
  - [corpus]: Weak - no direct mention in corpus papers, but related to ideas about using multiple layers' activations
- **Break condition:** If the relevant information for token prediction is indeed concentrated primarily in the final layer, LSA adds computational overhead without meaningful benefit.

### Mechanism 2
- **Claim:** Cross-Attention (CA) improves on-policyness by allowing the small model to attend to MLarge's activations up to the current time step while predicting future tokens, simulating the inference-time condition during training.
- **Mechanism:** The CA layer takes embedded tokens as queries and the LSA-aggregated activations from MLarge as keys and values. This creates a K-step bounded architecture where the small model can predict up to K future tokens in a single forward pass using only the available MLarge activations, matching inference conditions.
- **Core assumption:** Training the small model under conditions that closely match inference (on-policy training) will improve its ability to generate accurate drafts during actual deployment.
- **Evidence anchors:**
  - [abstract]: "Cross-Attention (CA) to improve on-policyness and training efficiency"
  - [section]: "The cross-attention layer is input-independent... However, in order to make accurate predictions, MSmall needs to be aware of the previously drafted tokens. Hence, we introduce a causal self-attention layer on the queries to mitigate this problem"
  - [corpus]: Moderate - related to tandem transformers (paper 123281) that also use MLarge activations for small model input
- **Break condition:** If the distribution shift between training and inference is minimal, the added complexity of CA may not provide significant benefits over simpler approaches.

### Mechanism 3
- **Claim:** Target Layer Inference (TLI) allows trading off between drafting speed and response quality by choosing which layer of the large model to target for prediction.
- **Mechanism:** TLI parameter determines how many of the last layers of MLarge are reused by the small model. When TLI > 0, the small model predicts activations for layer L+1-TLI, then the last TLI layers of MLarge are applied to produce the final output. Lower TLI values mean the small model does more work but can potentially achieve better accuracy.
- **Core assumption:** Predicting earlier layer activations is easier than predicting later layer activations, but using more of MLarge's layers improves accuracy.
- **Evidence anchors:**
  - [abstract]: "a flexible Target Layer Inference mechanism to balance computational efficiency and prediction accuracy"
  - [section]: "We assume (and later show) that predicting ol t+1 is always easier than predicting ok t+1 for l < k due to ol t+1 undergoing fewer layer transformations"
  - [corpus]: Weak - no direct mention in corpus papers, but related to ideas about layer selection in distillation
- **Break condition:** If the difficulty of predicting different layer activations doesn't follow the assumed pattern, or if the computational savings from lower TLI don't justify the accuracy loss.

## Foundational Learning

- **Concept:** Markov Decision Processes and partial observability
  - **Why needed here:** Understanding partial observability is crucial for grasping why LSA is necessary. In MDPs, agents need complete state information to make optimal decisions, and similarly, the small model needs comprehensive information from all layers to draft accurate tokens.
  - **Quick check question:** If an agent only observes partial state information in an MDP, how does this affect its decision quality compared to having full observability?

- **Concept:** On-policy vs off-policy training
  - **Why needed here:** The distinction between on-policy and off-policy training is fundamental to understanding why CA improves training. Traditional training uses ground truth sequences (off-policy), but SD requires the small model to handle its own generated tokens (on-policy).
  - **Quick check question:** In reinforcement learning, what is the key difference between on-policy and off-policy algorithms, and how does this relate to the training of the small model in speculative decoding?

- **Concept:** Attention mechanisms and layer-wise representations
  - **Why needed here:** Understanding how self-attention works and how information flows through transformer layers is essential for grasping why LSA over layer dimension is useful and how different layers capture different types of information.
  - **Quick check question:** In a transformer, how does information from earlier tokens propagate through multiple self-attention layers, and why might later layers capture more abstract representations?

## Architecture Onboarding

- **Component map:** Token embedding → Self-Attention → Cross-Attention → (optional MLarge layers) → LM Head → Token prediction
- **Critical path:** Token embedding → Self-Attention → Cross-Attention → (optional MLarge layers) → LM Head → Token prediction
- **Design tradeoffs:**
  - TLI parameter: Higher values use more MLarge computation (slower) but potentially achieve better accuracy
  - LSA vs direct input: LSA provides richer information but adds computation vs using just final layer activations
  - On-policy training: More accurate drafts but requires more complex training setup
- **Failure signatures:**
  - Low acceptance rate: Could indicate poor on-policy training, insufficient information from LSA, or inappropriate TLI setting
  - Slow drafting: May be caused by high TLI value or inefficient LSA implementation
  - Training instability: Could result from improper on-policy training setup or incorrect CA masking
- **First 3 experiments:**
  1. Compare acceptance rates with and without LSA to validate its impact on partial observability
  2. Test different TLI values (0, 1, 3) to find the optimal speed-accuracy tradeoff
  3. Evaluate on-policy training vs standard training to confirm the importance of CA layer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Layer Self-Attention (LSA) component specifically impact the performance of Mixture of Attentions compared to EAGLE-2 in terms of token-per-second and acceptance length?
- Basis in paper: [explicit] The paper mentions that LSA helps mitigate partial observability, and an ablation study shows that LSA improves tokens-per-second performance by 6% and increases the acceptance length.
- Why unresolved: The paper does not provide a detailed analysis of the specific impact of LSA on different datasets or scenarios. It only shows general improvements in a single ablation study.
- What evidence would resolve it: Detailed experimental results comparing the performance of Mixture of Attentions with and without LSA across various datasets and network conditions would provide a clearer understanding of LSA's impact.

### Open Question 2
- Question: What is the optimal value of Target Layer Inference (TLI) for balancing drafting speed and response quality in different deployment scenarios?
- Basis in paper: [explicit] The paper discusses that increasing TLI increases the acceptance length but also increases the computational time of drafting. It suggests that a higher TLI improves the quality of responses in the event of a complete disconnection in a client-server setting.
- Why unresolved: The paper does not provide a systematic analysis of the optimal TLI value for different scenarios, such as varying network conditions or device capabilities.
- What evidence would resolve it: A comprehensive study evaluating the performance of Mixture of Attentions with different TLI values across various deployment scenarios would help determine the optimal TLI for balancing speed and quality.

### Open Question 3
- Question: How does the client-server deployment of Mixture of Attentions compare to API calls to LLMs in terms of cost, latency, and accuracy under different network conditions?
- Basis in paper: [explicit] The paper introduces a client-server paradigm and demonstrates that Mixture of Attentions can maintain higher accuracy in the event of a disconnection compared to API calls to LLMs. It also shows state-of-the-art latency and minimal server calls under various network conditions.
- Why unresolved: The paper does not provide a detailed comparison of the cost, latency, and accuracy of the client-server deployment versus API calls to LLMs under different network conditions.
- What evidence would resolve it: A thorough analysis comparing the cost, latency, and accuracy of Mixture of Attentions in a client-server deployment versus API calls to LLMs under various network conditions would provide insights into the practical advantages of the proposed approach.

## Limitations

- The effectiveness of Layer Self-Attention depends on the assumption that relevant information is distributed across multiple layers, which isn't thoroughly validated through ablation studies.
- The optimal Target Layer Inference parameter is left as a hyperparameter without clear selection criteria or systematic exploration across different scenarios.
- The claimed speedup improvements over EAGLE-2 need independent verification as the comparison methodology isn't fully detailed.

## Confidence

**High Confidence:** The fundamental problem of partial observability in speculative decoding is well-established and the proposed LSA mechanism logically addresses this limitation. The architectural description is clear and the experimental setup appears sound.

**Medium Confidence:** The claimed speedup improvements over EAGLE-2 are based on the authors' experimental results, but independent replication would strengthen confidence. The client-server deployment benefits are demonstrated but the generalization to other deployment scenarios remains uncertain.

**Low Confidence:** The optimal TLI parameter selection and its relationship to different model sizes and tasks is not thoroughly explored. The long-range generation capability after server disconnection, while impressive, needs more rigorous testing across different failure scenarios.

## Next Checks

1. **Ablation Study on Layer Self-Attention:** Run experiments comparing the complete MoA architecture against versions without LSA, without CA, and with different TLI values to isolate the contribution of each component to the overall performance gains.

2. **Independent Speedup Validation:** Replicate the single-device speedup experiments using the open-sourced implementation on different hardware configurations (GPU vs CPU) to verify the claimed 9.5% improvement over EAGLE-2 and assess generalizability.

3. **Stress Testing Client-Server Deployment:** Systematically test the client-server architecture under various network conditions, including different packet loss rates, latency variations, and server overload scenarios to validate the claimed robustness and generation continuation capabilities.