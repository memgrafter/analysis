---
ver: rpa2
title: Evolution of Rewards for Food and Motor Action by Simulating Birth and Death
arxiv_id: '2406.15016'
source_url: https://arxiv.org/abs/2406.15016
tags:
- reward
- food
- rewards
- foods
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses a population-based decentralized evolutionary simulation
  to explore how reward functions evolve in artificial agents. Agents inherit and
  mutate reward functions that map food intake and motor actions to scalar values,
  learning via reinforcement learning.
---

# Evolution of Rewards for Food and Motor Action by Simulating Birth and Death

## Quick Facts
- arXiv ID: 2406.15016
- Source URL: https://arxiv.org/abs/2406.15016
- Reference count: 6
- Primary result: Population-based evolutionary simulation shows positive food rewards and mostly negative motor action rewards evolve biologically plausibly, with environmental factors strongly influencing reward stability.

## Executive Summary
This study explores the evolution of reward functions in artificial agents through a population-based decentralized evolutionary simulation. Agents inherit and mutate reward functions that map food intake and motor actions to scalar values, learning via reinforcement learning. The model incorporates birth and death based on energy levels and age, using Gompertz aging and a hazard function. Experiments in 2D environments with food and physics-based movement reveal that positive food rewards and mostly negative motor action rewards evolve as biologically plausible outcomes. The study also demonstrates that less food supply and periodic food relocation lead to more stable, energy-efficient rewards, while rewards for less important foods are less stable.

## Method Summary
The study employs a population-based decentralized evolutionary simulation where agents inherit and mutate reward functions that map food intake and motor actions to scalar values. Agents learn through reinforcement learning and experience birth and death based on energy levels and age. The model uses a hazard function with Gompertz aging and a birth function. Experiments are conducted in 2D environments with food and physics-based movement to explore how reward functions evolve over generations.

## Key Results
- Positive food rewards and mostly negative motor action rewards evolve as biologically plausible outcomes in the simulation.
- Less food supply and periodic food relocation lead to more stable, energy-efficient rewards.
- Rewards for less important foods (poor or poisonous) are less stable than for normal foods.

## Why This Works (Mechanism)
The simulation's decentralized evolutionary approach allows for the natural emergence of reward functions that align with biological plausibility. By incorporating birth and death based on energy levels and age, the model mimics natural selection pressures that favor efficient energy use and appropriate responses to food and motor actions. The use of Gompertz aging and a hazard function ensures that the aging process and mortality rates are realistic, contributing to the evolution of stable reward functions over generations.

## Foundational Learning
- Reinforcement Learning: Needed to enable agents to learn and adapt their behavior based on the rewards received. Quick check: Verify that the learning algorithm converges and improves agent performance over time.
- Population-based Evolution: Essential for simulating the natural selection process and the emergence of reward functions that are beneficial for survival. Quick check: Ensure that the population maintains diversity and that beneficial traits are preserved across generations.
- Gompertz Aging: Required to model realistic aging and mortality rates, influencing the evolutionary pressures on reward functions. Quick check: Confirm that the aging model accurately reflects biological aging processes and affects agent lifespans as expected.

## Architecture Onboarding
Component Map: Agents -> Reward Functions -> Reinforcement Learning -> Energy Levels -> Birth/Death -> Population Evolution
Critical Path: Agents interact with environment, learn via RL, gain/lose energy, reproduce or die, population evolves
Design Tradeoffs: Simplified reward function architecture vs. biological complexity; computational efficiency vs. simulation realism
Failure Signatures: Unstable reward functions, population collapse, unrealistic aging or energy dynamics
First Experiments: 1) Vary food supply levels, 2) Introduce periodic food relocation, 3) Test different aging rates

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on specific parameter choices and environmental configurations that may not generalize to all scenarios.
- Model's abstraction of food and motor action rewards may oversimplify the complexity of biological reward systems.
- Lack of exploration into how different reinforcement learning algorithms or reward shaping techniques might affect outcomes.

## Confidence
- High confidence in the core finding that positive food rewards and mostly negative motor action rewards evolve as biologically plausible outcomes.
- Medium confidence in the claim that less food supply and periodic food relocation lead to more stable, energy-efficient rewards.
- Medium confidence in the observation that rewards for less important foods are less stable.

## Next Checks
1. Conduct sensitivity analysis by varying key simulation parameters (e.g., food supply, relocation frequency, aging rates) to assess the robustness of the observed reward evolution patterns.
2. Test alternative reinforcement learning algorithms and reward shaping techniques to determine if the observed outcomes are specific to the chosen learning method or more generalizable.
3. Implement a more comprehensive exploration of the parameter space and alternative reward function architectures to evaluate the generalizability of the results and identify potential confounding factors.