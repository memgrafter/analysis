---
ver: rpa2
title: Granular-ball Representation Learning for Deep CNN on Learning with Label Noise
arxiv_id: '2409.03254'
source_url: https://arxiv.org/abs/2409.03254
tags:
- samples
- noise
- learning
- label
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a granular-ball computing (GBC) module to improve
  the robustness of deep CNN models on learning with label noise. The key idea is
  to split the input samples into granular-ball (gb) samples at the feature level,
  where each gb sample can contain multiple individual samples with varying numbers
  and share one single label.
---

# Granular-ball Representation Learning for Deep CNN on Learning with Label Noise

## Quick Facts
- arXiv ID: 2409.03254
- Source URL: https://arxiv.org/abs/2409.03254
- Authors: Dawei Dai; Hao Zhu; Shuyin Xia; Guoyin Wang
- Reference count: 40
- Primary result: Improves CNN robustness to label noise by aggregating samples into granular-balls with shared labels

## Executive Summary
This paper introduces a granular-ball computing (GBC) module that enhances deep CNN models' robustness to label noise by aggregating individual samples into granular-ball (gb) samples at the feature level. Each granular-ball contains multiple individual samples sharing a single label, and the classifier learns to map these aggregated samples to their corresponding labels. The approach modifies the gradient allocation strategy during backpropagation and incorporates an experience replay policy to maintain training stability. Experimental results demonstrate significant improvements over state-of-the-art methods without requiring additional data or optimization techniques.

## Method Summary
The proposed method employs granular-ball computing to improve CNN robustness against label noise. The key innovation involves splitting input samples into granular-ball samples at the feature level, where each ball aggregates multiple individual samples under a single shared label. The classifier then learns the mapping from each granular-ball to its label rather than from individual samples. During backpropagation, the gradient allocation strategy is modified to ensure proper gradient propagation, while an experience replay policy is implemented to enhance training stability. This approach fundamentally reduces the proportion of label noise within granular-balls compared to individual samples, leading to improved model robustness.

## Key Results
- Significantly improves CNN robustness to label noise without additional data or optimization
- Reduces the proportion of label noise in granular-ball samples compared to individual samples
- Outperforms state-of-the-art methods on benchmark datasets
- Maintains training stability through experience replay policy

## Why This Works (Mechanism)
The method works by exploiting the statistical properties of sample aggregation. When individual samples with noisy labels are grouped into granular-balls, the likelihood that all samples within a ball share the same incorrect label decreases significantly. This aggregation process effectively filters out label noise at the representation level, as the classifier learns from more reliable label signals within each granular-ball. The modified gradient allocation strategy ensures that backpropagation properly accounts for the aggregated nature of the samples, while the experience replay policy prevents catastrophic forgetting and maintains stable learning dynamics throughout training.

## Foundational Learning

- **Label Noise in Deep Learning**: Understanding how incorrect labels affect model training and generalization is essential for developing robust methods.
  - Why needed: Provides context for why traditional CNNs fail with noisy labels
  - Quick check: Verify that the problem setup correctly identifies common sources of label noise

- **Sample Aggregation Techniques**: Knowledge of how grouping samples affects statistical properties is crucial for understanding granular-ball benefits.
  - Why needed: Explains the mathematical foundation behind noise reduction through aggregation
 2: Checks: Calculate expected noise reduction rates for different aggregation sizes under various noise models

- **Gradient Modification Strategies**: Understanding how to adapt backpropagation for non-standard sample representations is key to implementation.
  - Why needed: Enables proper training of models using granular-ball representations
  - Quick check: Validate that modified gradients maintain convergence properties

## Architecture Onboarding

Component Map: Input Samples -> Feature Extraction -> Granular-Ball Formation -> Classifier -> Loss Computation -> Modified Backpropagation -> Experience Replay Buffer

Critical Path: The critical path involves the complete flow from raw input through feature extraction, granular-ball formation, classification, and backpropagation with modified gradients. The experience replay buffer operates in parallel to store and replay previous granular-ball representations.

Design Tradeoffs: The primary tradeoff involves the granularity level of aggregation - finer granularity preserves more information but may retain more noise, while coarser granularity provides better noise reduction but loses individual sample characteristics. The experience replay introduces memory overhead but improves stability.

Failure Signatures: Common failure modes include insufficient granular-ball diversity leading to mode collapse, improper gradient allocation causing training instability, and experience replay buffer management issues resulting in catastrophic forgetting.

First Experiments:
1. Validate granular-ball formation on synthetic datasets with known noise patterns
2. Test modified gradient allocation on simple classification tasks
3. Evaluate experience replay effectiveness on noisy label scenarios

## Open Questions the Paper Calls Out

None specified in the provided materials.

## Limitations

- Theoretical justification for noise reduction in granular-balls lacks rigorous statistical foundation
- Performance on highly imbalanced datasets or extreme noise rates (>60%) is not explored
- Method's effectiveness across different noise types and distributions requires further validation

## Confidence

- Robustness improvement claim: Medium confidence (strong empirical results but limited to moderate noise levels)
- Lower label noise proportion in granular-balls: Low confidence (empirical demonstration but insufficient theoretical grounding)
- No additional data or optimization required: High confidence (validated through experimental results)

## Next Checks

1. Test the method on datasets with synthetic noise rates exceeding 60% to assess scalability limits
2. Conduct ablation studies to quantify the individual contributions of the gradient allocation strategy versus the experience replay policy
3. Perform theoretical analysis to derive mathematical bounds on the expected label noise reduction when samples are aggregated into granular-balls under different noise models