---
ver: rpa2
title: Towards Cross-Domain Continual Learning
arxiv_id: '2402.12490'
source_url: https://arxiv.org/abs/2402.12490
tags:
- learning
- domain
- continual
- cdcl
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised domain adaptation
  in continual learning settings, where a model must sequentially learn from labeled
  source domains and unlabeled target domains without catastrophic forgetting. The
  proposed Cross-Domain Continual Learning (CDCL) framework introduces an inter-intra-task
  cross-attention mechanism that maintains alignment between features from previous
  tasks while adapting to new ones, combined with an intra-task center-aware pseudo-labeling
  method for accurate label generation in target domains.
---

# Towards Cross-Domain Continual Learning

## Quick Facts
- arXiv ID: 2402.12490
- Source URL: https://arxiv.org/abs/2402.12490
- Authors: Marcus de Carvalho; Mahardhika Pratama; Jie Zhang; Chua Haoyan; Edward Yapp
- Reference count: 40
- Primary result: Introduces CDCL framework for unsupervised domain adaptation in continual learning, achieving superior performance across five popular UDA benchmarks

## Executive Summary
This paper addresses the challenge of unsupervised domain adaptation in continual learning settings, where models must sequentially learn from labeled source domains and unlabeled target domains without catastrophic forgetting. The proposed Cross-Domain Continual Learning (CDCL) framework introduces an inter-intra-task cross-attention mechanism that maintains alignment between features from previous tasks while adapting to new ones, combined with an intra-task center-aware pseudo-labeling method for accurate label generation in target domains. The method is evaluated on five popular UDA benchmarks including VisDA-2017, Office-Home, Office-31, DomainNet, and MNISTâ†”USPS, demonstrating superior performance in the task-incremental learning scenario compared to state-of-the-art methods.

## Method Summary
The CDCL framework combines two key mechanisms: an inter-intra-task cross-attention mechanism that maintains feature alignment across tasks while adapting to new domains, and an intra-task center-aware pseudo-labeling method for generating accurate labels in target domains. The cross-attention mechanism operates by computing attention scores between source and target domain features within and across tasks, enabling the model to preserve knowledge from previous tasks while learning new ones. The pseudo-labeling component uses class centroids to refine label assignments, improving the quality of supervision for target domains. This dual approach addresses both the catastrophic forgetting problem and the challenge of learning from unlabeled data in cross-domain scenarios.

## Key Results
- CDCL achieves significantly higher average accuracy and lower forgetting rates than baselines like DER, DER++, HAL, and CDTrans
- Particularly excels in scenarios where target domains are closely related to source domains
- Makes code and models publicly available to encourage further research in this domain

## Why This Works (Mechanism)
The inter-intra-task cross-attention mechanism enables the model to maintain a dynamic memory of important features from previous tasks while adapting to new domains. By computing attention scores across both spatial dimensions and task boundaries, the model can selectively focus on relevant information when encountering new data. The center-aware pseudo-labeling approach improves label quality by leveraging class centroids as reference points, reducing noise in the supervision signal for target domains. Together, these mechanisms create a robust framework for continual learning that can adapt to domain shifts without losing previously acquired knowledge.

## Foundational Learning
**Catastrophic Forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks. Why needed: This is the primary challenge in continual learning that CDCL addresses. Quick check: Compare performance on previous tasks before and after training on new tasks.

**Unsupervised Domain Adaptation**: The process of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain. Why needed: CDCL must handle scenarios where target domain labels are unavailable. Quick check: Evaluate performance gap between labeled and unlabeled target domains.

**Cross-Attention Mechanisms**: Neural network components that compute attention scores between different feature representations. Why needed: Enables selective focus on relevant features across domains and tasks. Quick check: Analyze attention score distributions across different domain pairs.

## Architecture Onboarding

**Component Map**: Input features -> Cross-attention module -> Feature alignment -> Pseudo-labeling module -> Classification output

**Critical Path**: The model processes input features through the cross-attention mechanism, which aligns source and target domain representations. These aligned features are then used by the pseudo-labeling module to generate refined labels, which in turn train the classification head. The critical path ensures that domain adaptation and continual learning objectives are simultaneously optimized.

**Design Tradeoffs**: The cross-attention mechanism adds computational overhead but enables better feature alignment across domains. The pseudo-labeling approach reduces reliance on labeled target data but introduces potential noise from incorrect labels. The framework prioritizes knowledge preservation over rapid adaptation to new domains.

**Failure Signatures**: Performance degradation when domain shifts are too large for the cross-attention mechanism to effectively align features. Increased error rates when pseudo-label quality drops below a threshold, causing the model to learn from incorrect supervision signals.

**First Experiments**: 1) Ablation study removing cross-attention to quantify its contribution. 2) Evaluation on extreme domain shifts to test method robustness. 3) Analysis of pseudo-label quality across different domain similarity levels.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses primarily on accuracy metrics without extensive analysis of model robustness to domain-specific noise or distribution shifts beyond those tested
- Claims about superior performance in closely related target domains lack sufficient empirical backing regarding the limits of domain similarity where the method may fail
- The paper does not thoroughly address computational overhead or memory requirements for the cross-attention mechanism, which could limit practical applicability

## Confidence
- **High Confidence**: The core methodology and proposed framework are well-defined and technically sound
- **Medium Confidence**: The experimental results show improvements over baselines, but generalizability needs further validation
- **Low Confidence**: Claims about superior performance in closely related target domains lack sufficient empirical backing

## Next Checks
1. Conduct experiments with larger domain gaps to evaluate the method's robustness beyond the tested benchmarks and identify failure modes
2. Perform ablation studies specifically isolating the impact of the inter-intra-task cross-attention mechanism versus the pseudo-labeling approach to quantify their individual contributions
3. Evaluate computational efficiency and memory overhead compared to baseline methods to assess practical deployment viability in resource-constrained scenarios