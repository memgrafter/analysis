---
ver: rpa2
title: Enhancing targeted transferability via feature space fine-tuning
arxiv_id: '2401.02727'
source_url: https://arxiv.org/abs/2401.02727
tags:
- targeted
- attacks
- attack
- transferability
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the transferability
  of targeted adversarial examples across unknown models. The authors propose a feature
  space fine-tuning approach that starts with an adversarial example crafted by a
  baseline attack and fine-tunes it in an internal layer of the source model to encourage
  target-class features and suppress original-class features.
---

# Enhancing targeted transferability via feature space fine-tuning

## Quick Facts
- arXiv ID: 2401.02727
- Source URL: https://arxiv.org/abs/2401.02727
- Reference count: 0
- Primary result: Feature space fine-tuning significantly improves targeted adversarial transferability, with simple iterative attacks matching or exceeding generative methods' performance

## Executive Summary
This paper addresses the challenge of improving targeted adversarial transferability across unknown models by proposing a feature space fine-tuning approach. The method starts with an adversarial example generated by a baseline attack and fine-tunes it in an internal layer of the source model to encourage target-class features while suppressing original-class features. Experiments on ImageNet demonstrate that only a few iterations of fine-tuning can nearly triple the targeted transfer success rate compared to baseline attacks, with consistent improvements across different architectures and scenarios including ensemble-model settings.

## Method Summary
The proposed method fine-tunes adversarial examples in feature space by computing aggregate gradients for target and original classes at a middle layer of the source model. Starting with an adversarial example from a baseline attack, the method calculates combined gradients (target class minus Î² times original class) and uses these to update the adversarial example for Nft iterations. The fine-tuning process encourages features conducive to the target class while discouraging features of the original class, with additional suppression of high-confidence labels. This approach is applied after initial attack generation (typically N=200 iterations) with fine-tuning (Nft=10 iterations) using Lâˆž constraints.

## Key Results
- Feature space fine-tuning nearly triples targeted transfer success rate (e.g., from 15.2% to 42.4% when transferring from DenseNet121 to VGG16)
- Fine-tuned simple iterative attacks (Logit, SupHigh) achieve comparable or better transferability than resource-intensive generative methods under certain perturbation budgets
- Consistent improvements across random-target and most difficult-target scenarios, as well as ensemble-model settings
- The method works across different architectures with specific middle layer selections for each source model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning in feature space reduces overfitting to the source model by aligning adversarial features with target-class semantics
- Mechanism: The proposed method fine-tunes an existing adversarial example by encouraging features that contribute to the target class while suppressing those of the original class through combined aggregate gradients
- Core assumption: Middle-layer features are more transferable across models because they balance class-specific and model-specific information
- Evidence anchors: [abstract] "encourage the features conducive to the target class and discourage the features to the original class in a middle layer of the source model"

### Mechanism 2
- Claim: Suppressing high-confidence labels (other than target and original) enhances transferability by preventing the adversarial example from being too specific to the source model's decision boundary
- Mechanism: The combined aggregate gradient includes a suppression term for high-confidence labels, orthogonalized to the target/original gradient
- Core assumption: High-confidence labels act as "distractors" that anchor the adversarial example to the source model's specific decision surface
- Evidence anchors: [section] "not only the original label ð‘¦ð‘œ, but also other high-confidence labels should be suppressed for better transferability"

### Mechanism 3
- Claim: Feature space fine-tuning allows simple iterative attacks to rival generative methods in transferability without additional data or model training
- Mechanism: By fine-tuning with only a few iterations in feature space, the method improves the transferability of simple attacks like Logit and SupHigh to match or exceed that of resource-intensive generative methods
- Core assumption: Simple iterative attacks already encode strong target-class attack signals; fine-tuning only needs to remove overfitting artifacts
- Evidence anchors: [abstract] "simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods"

## Foundational Learning

- Concept: Feature space perturbation and its role in adversarial transferability
  - Why needed here: The proposed method relies on modifying internal-layer features rather than just the output layer. Understanding how feature space attacks work is essential to grasp why fine-tuning in this space can improve transferability.
  - Quick check question: Why do feature space attacks tend to generalize better across models compared to output-layer attacks?

- Concept: Gradient aggregation and its importance in targeted attacks
  - Why needed here: The method uses aggregate gradients to measure feature importance and guide fine-tuning. Knowing how to compute and interpret these gradients is critical for implementation and debugging.
  - Quick check question: How does the choice of gradient aggregation method (e.g., FIA vs. RPA) affect the stability of the fine-tuning process?

- Concept: Overfitting in adversarial examples and methods to mitigate it
  - Why needed here: The paper explicitly states that simple iterative attacks overfit the source model, and the proposed fine-tuning is designed to alleviate this. Understanding overfitting mechanisms is key to diagnosing failure cases.
  - Quick check question: What are the visual or statistical signatures of an adversarial example that has overfit the source model?

## Architecture Onboarding

- Component map:
  Benign image and target label -> Baseline attack module -> Feature extraction module -> Aggregate gradient computation module -> Fine-tuning module -> Fine-tuned adversarial example

- Critical path:
  1. Generate AE with baseline attack (N iterations)
  2. Compute aggregate gradients for target and original classes
  3. Combine gradients with suppression term for high-confidence labels
  4. Fine-tune AE in feature space (Nft << N iterations)
  5. Evaluate transferability across target models

- Design tradeoffs:
  - N vs. Nft: Larger N gives stronger initial AEs but increases computation; larger Nft may overfit again. The paper uses N=160, Nft=10.
  - Layer choice (k): Earlier layers are more class-specific, later layers more model-specific. The paper chooses middle layers for balance.
  - Gradient aggregation method: FIA is simpler but RPA may yield better results at higher computational cost.

- Failure signatures:
  - No improvement in transferability after fine-tuning
  - Increased overfitting (AE becomes less transferable)
  - Numerical instability during gradient combination

- First 3 experiments:
  1. Baseline: Run CE attack with TMDI augmentation, measure transferability without fine-tuning
  2. Fine-tune CE-generated AE using proposed method, measure improvement in random-target scenario
  3. Compare fine-tuned Logit attack vs. TTP under low perturbation budget (Îµ=8) to validate efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed feature space fine-tuning approach compare to targeted versions of existing intermediate-level attacks (e.g., ILA and ILA++) in terms of transferability?
- Basis in paper: [explicit] The authors explicitly compare their method with targeted ILA in the supplementary material and find that their approach outperforms targeted ILA by a clear margin.
- Why unresolved: The paper only provides a brief comparison with ILA. A more comprehensive evaluation against other intermediate-level attacks like ILA++ would provide a clearer understanding of the proposed method's relative performance.
- What evidence would resolve it: Conduct experiments comparing the proposed method with targeted ILA++ and other state-of-the-art intermediate-level attacks on various datasets and model architectures.

### Open Question 2
- Question: What is the optimal number of fine-tuning iterations (Nft) for different baseline attacks and scenarios?
- Basis in paper: [explicit] The authors mention that the optimal Nft varies depending on the baseline attack, with weaker attacks requiring more fine-tuning iterations. They set Nft=10 for simplicity, but acknowledge that this may not be optimal for all cases.
- Why unresolved: The paper does not provide a systematic analysis of the optimal Nft for different baseline attacks and transfer scenarios. This could lead to suboptimal performance in certain cases.
- What evidence would resolve it: Conduct experiments to determine the optimal Nft for different baseline attacks and transfer scenarios, considering factors such as the source model, target model, and perturbation budget.

### Open Question 3
- Question: How does the choice of the fine-tuning layer (k) affect the transferability of the generated adversarial examples?
- Basis in paper: [explicit] The authors discuss the importance of selecting a middle layer for fine-tuning, as early layers are data-specific and later layers are model-specific. They choose specific layers for each source model but do not provide a comprehensive analysis of the effect of k on transferability.
- Why unresolved: The paper does not provide a detailed investigation of how different choices of k impact the transferability of the generated adversarial examples. This could lead to suboptimal performance if the chosen layer is not ideal for a particular source model or transfer scenario.
- What evidence would resolve it: Conduct experiments to systematically evaluate the effect of different fine-tuning layers (k) on the transferability of the generated adversarial examples, considering various source models, target models, and transfer scenarios.

## Limitations

- The method relies on specific middle-layer selection without comprehensive ablation studies on optimal layer choice across different model pairs
- Computational efficiency claims assume similar hardware setups and don't account for potential overhead from fine-tuning iterations
- The optimal hyperparameters (Î²=0.2, Nft=10) lack rigorous sensitivity analysis across diverse model pairs and scenarios

## Confidence

- **High confidence**: The core claim that feature space fine-tuning improves targeted transferability (validated by consistent improvements across multiple model pairs and target selection modes)
- **Medium confidence**: The assertion that simple iterative attacks can rival generative methods after fine-tuning (supported by results but limited to specific perturbation budgets)
- **Medium confidence**: The mechanism explanation involving suppression of high-confidence labels (theoretically sound but lacks direct empirical validation)

## Next Checks

1. **Layer sensitivity analysis**: Systematically test fine-tuning at different layers (early, middle, late) to determine optimal layer selection for various model pairs.

2. **Beta parameter sweep**: Evaluate transfer success rates across Î² âˆˆ [0.1, 0.3] to assess robustness to this hyperparameter.

3. **Computational overhead measurement**: Quantify actual wall-clock time for fine-tuning vs. generative methods to validate the efficiency claim under realistic conditions.