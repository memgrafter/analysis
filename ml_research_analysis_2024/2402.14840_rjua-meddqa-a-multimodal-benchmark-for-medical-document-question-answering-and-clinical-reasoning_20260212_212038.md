---
ver: rpa2
title: 'RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering
  and Clinical Reasoning'
arxiv_id: '2402.14840'
source_url: https://arxiv.org/abs/2402.14840
tags:
- medical
- reasoning
- table
- report
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RJUA-MedDQA is a large-scale multimodal medical document benchmark
  designed to evaluate large language models (LLMs) and large multimodal models (LMMs)
  on real-world clinical reasoning tasks. It includes 2000 Chinese medical reports
  from Shanghai Renji Hospital covering 334 diseases and 50 lab tests.
---

# RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning

## Quick Facts
- arXiv ID: 2402.14840
- Source URL: https://arxiv.org/abs/2402.14840
- Reference count: 40
- Introduces RJUA-MedDQA benchmark with 2000 Chinese medical reports covering 334 diseases and 50 lab tests

## Executive Summary
RJUA-MedDQA is a large-scale multimodal medical document benchmark designed to evaluate large language models (LLMs) and large multimodal models (LMMs) on real-world clinical reasoning tasks. The benchmark includes 2000 Chinese medical reports from Shanghai Renji Hospital covering 334 diseases and 50 lab tests. It introduces two key challenges: understanding complex document layouts with text and tables, and performing clinical reasoning by integrating image content with medical knowledge. To address annotation challenges, the authors developed an Efficient Structural Restoration Annotation (ESRA) method that improves accuracy from 70% to 96.8% and doubles annotator productivity. Experimental results show that existing LMMs (e.g., GPT-4V, Qwen-VL) outperform traditional OCR+LLM pipelines on low-quality images but still struggle with multi-step reasoning tasks.

## Method Summary
The RJUA-MedDQA benchmark is constructed through a multi-stage process. First, medical report images are collected and processed using OCR to extract textual content and coordinates. The ESRA method then reconstructs the structural layout of the documents by identifying text segments and clustering them based on line detection algorithms. Clinical experts annotate the reports with key-value pairs, table structures, and clinical reasoning context. A synonym-aware QA generator creates diverse questions using medical knowledge schemas and validated synonym lists. The benchmark is evaluated using LMMs (GPT-4V, Qwen-VL-Plus) and ESRA+LLM pipelines across five task types: Entity, Table, TableNR, Clinical Reasoning MC, and SA.

## Key Results
- ESRA method improves annotation accuracy from 70% to 96.8% and doubles annotator productivity
- GPT-4V and Qwen-VL-Plus outperform traditional OCR+LLM pipelines on low-quality images (402 screenshots, 619 scanned PDFs, 979 photos)
- LMMs show strong performance on entity extraction but struggle with multi-step clinical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ESRA method significantly improves annotation accuracy and efficiency for medical report images by reconstructing both textual and tabular content.
- Mechanism: ESRA uses OCR results to identify text segments and their bounding boxes, then applies line detection and clustering algorithms to restore the structural layout of the original report. This eliminates the need for manual typing and reduces errors associated with transcribing complex medical terminology.
- Core assumption: The OCR results are sufficiently accurate to provide a reliable foundation for structural restoration, and the clustering algorithm can effectively group text segments belonging to the same line or table row.
- Evidence anchors:
  - [abstract] "This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy."
  - [section 4.1] "Statistically, this method has elevated the accuracy rate from 70% to 96.8%."
- Break condition: If the OCR results are too noisy or inaccurate, or if the document layout is too complex for the clustering algorithm to handle, the ESRA method may fail to restore the structure accurately.

### Mechanism 2
- Claim: The synonym-aware QA generator enhances the dataset's generalization ability by incorporating diverse question templates and medical terminology.
- Mechanism: The generator uses a knowledge-based schema that integrates validated lists of synonyms for common medical vocabulary. This allows the system to generate questions with varying levels of difficulty and phrasing, making the dataset more robust to different ways of expressing the same information.
- Core assumption: The synonym schema accurately captures the range of terminology used in real-world medical reports, and the QA templates are flexible enough to accommodate different question types and answer formats.
- Evidence anchors:
  - [section 4.3.1] "Based on this schema, data mapping can be easily performed resulting not only a reduction in ambiguity and errors associated with the interpretation of medical reports due to synonym usage, and an increase in diversity during question generation process to enhanced data interoperability."
  - [section 4.3.2] "Employing these templates, we generate questions of varying difficulty, which can be enriched through annotations to include complex queries involving dual entities, single-row or multi-row questions."
- Break condition: If the synonym schema is incomplete or inaccurate, or if the QA templates are too rigid to handle complex question types, the generator may fail to produce high-quality, diverse questions.

### Mechanism 3
- Claim: The clinical reasoning annotation process, involving urological experts, ensures the dataset's relevance and accuracy for real-world medical diagnosis and treatment scenarios.
- Mechanism: Experts annotate the reports with context indexes for diagnosis, staging/status, and advice/treatment, drawing from official guidelines and clinical experience. This provides a structured fact base that models can use to reason about complex medical scenarios.
- Core assumption: The experts' annotations are consistent, unbiased, and accurately reflect the clinical reasoning process used by urologists in practice.
- Evidence anchors:
  - [section 4.2.3] "All labeling statements are unambiguous and unbiased, and all suspicious findings are excluded."
  - [section 4.2.3] "They can quickly reference and tag recurring content, significantly boosting annotation efficiency and consistency."
- Break condition: If the experts' annotations are inconsistent, biased, or not representative of real-world clinical reasoning, the dataset may not accurately reflect the challenges faced by medical professionals.

## Foundational Learning

- Concept: OCR (Optical Character Recognition)
  - Why needed here: OCR is used to extract textual content from the medical report images, providing the raw data for the ESRA method to process and restore the document structure.
  - Quick check question: What are the limitations of OCR technology, and how might they impact the accuracy of the ESRA method?

- Concept: Data annotation and labeling
  - Why needed here: The dataset relies on accurate annotation of the medical reports, including key-value pairs, table structures, and clinical reasoning context. This requires careful guidelines and quality control to ensure consistency and reliability.
  - Quick check question: What are some common challenges in data annotation, and how can they be addressed to improve the quality of the labeled dataset?

- Concept: Question generation and template-based QA
  - Why needed here: The synonym-aware QA generator uses templates to create diverse questions from the annotated medical reports. Understanding how to design effective templates and incorporate synonyms is crucial for generating high-quality, challenging questions.
  - Quick check question: What are some best practices for designing QA templates that can handle different question types and answer formats?

## Architecture Onboarding

- Component map: Data collection -> OCR extraction -> ESRA structural restoration -> Expert annotation -> QA generation -> Model evaluation
- Critical path: The preprocessing and annotation steps directly impact the quality of the generated QA pairs and the dataset's overall usefulness for training and evaluating models.
- Design tradeoffs:
  - Accuracy vs. efficiency: The ESRA method prioritizes accuracy over efficiency, as it aims to minimize errors in the structural restoration process.
  - Diversity vs. consistency: The synonym-aware QA generator aims to balance diversity in question phrasing with consistency in the underlying medical knowledge being tested.
- Failure signatures:
  - Low OCR accuracy leading to incorrect structural restoration
  - Inconsistent or biased annotations from clinical experts
  - QA templates that are too rigid or do not adequately capture the complexity of medical reasoning
- First 3 experiments:
  1. Evaluate the impact of OCR accuracy on the ESRA method's performance by varying the quality of the input images and measuring the resulting annotation accuracy.
  2. Assess the consistency and reliability of clinical expert annotations by having multiple experts annotate the same set of reports and comparing their results.
  3. Test the effectiveness of the synonym-aware QA generator by evaluating the diversity and difficulty of the generated questions using a variety of baseline models.

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the benchmark's limitations and future research directions. Key areas include evaluating the benchmark's generalization to other medical specialties, testing real-world deployment in clinical settings, conducting detailed error analysis on reasoning failures, and assessing how synonym-aware generation affects model generalization to unseen medical terminology. The paper also questions how LMMs would perform when trained with domain-specific medical knowledge augmentation and how image quality degradation impacts performance across different types of medical document content.

## Limitations

- The benchmark focuses on single medical records per query, limiting evaluation of complex multi-document reasoning scenarios
- Exclusion of critical and urgent cases creates potential bias toward less severe medical conditions
- Limited scope to urological diseases may not generalize well to other medical specialties

## Confidence

**High Confidence Claims:**
- The ESRA method improves annotation accuracy from 70% to 96.8% and doubles annotator productivity
- GPT-4V and Qwen-VL-Plus outperform traditional OCR+LLM pipelines on low-quality images
- The benchmark successfully differentiates model capabilities across task complexity levels

**Medium Confidence Claims:**
- The synonym-aware QA generator effectively enhances dataset generalization
- The clinical reasoning annotation process ensures real-world relevance
- The benchmark represents real-world clinical scenarios

**Low Confidence Claims:**
- The dataset will significantly advance medical document understanding research
- The performance gaps indicate specific architectural limitations of current LMMs

## Next Checks

1. **Cross-specialty validation**: Apply the same methodology to medical records from other specialties (e.g., cardiology, oncology) and measure performance consistency across domains.

2. **Real-world deployment testing**: Deploy top-performing models in actual clinical settings with practicing urologists to assess practical utility beyond benchmark metrics, measuring time savings and diagnostic accuracy improvements.

3. **Error analysis on reasoning failures**: Conduct detailed error analysis on Clinical Reasoning task failures to identify whether issues stem from visual understanding, knowledge integration, or reasoning capabilities, using human expert evaluation of model outputs.