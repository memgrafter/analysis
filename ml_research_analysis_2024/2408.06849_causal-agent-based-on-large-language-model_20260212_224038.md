---
ver: rpa2
title: Causal Agent based on Large Language Model
arxiv_id: '2408.06849'
source_url: https://arxiv.org/abs/2408.06849
tags:
- causal
- agent
- data
- level
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling large language models
  (LLMs) to reason about causal problems, which are often complex and difficult to
  express in natural language. The proposed Causal Agent framework equips LLMs with
  causal tools and a memory module to handle tabular data and perform causal reasoning.
---

# Causal Agent based on Large Language Model

## Quick Facts
- arXiv ID: 2408.06849
- Source URL: https://arxiv.org/abs/2408.06849
- Authors: Kairong Han; Kun Kuang; Ziyu Zhao; Junjian Ye; Fei Wu
- Reference count: 40
- Primary result: Causal Agent achieves 98.1% accuracy in causal effect estimation and outperforms state-of-the-art by 6% on QRData

## Executive Summary
This paper introduces the Causal Agent framework, which enables large language models to perform causal reasoning on tabular data by integrating causal tools and memory modules. The framework addresses the challenge of bridging LLMs' natural language strengths with the structured data requirements of causal inference. Through systematic categorization of causal problems into four levels and iterative tool-based reasoning, the agent achieves high accuracy rates above 80% across variable, edge, causal graph, and causal effect levels.

## Method Summary
The Causal Agent framework combines an LLM with specialized tool modules for causal analysis (using causal-learn and EconML libraries), a memory module that stores causal graph instances, and a plan module that guides iterative reasoning through tool invocation and reflection. The system processes tabular data and natural language causal questions by selecting appropriate tools for each of four problem levels: variable-level independence testing, edge-level relationship classification, causal graph generation, and causal effect estimation. The agent maintains a dictionary of causal graph objects in memory to support chained reasoning across multiple tool invocations.

## Key Results
- Achieves over 95% accuracy on variable-level independence problems
- Maintains over 89% accuracy on edge-level causal relationship classification
- Demonstrates 98.1% accuracy in causal effect estimation (ATE)
- Outperforms state-of-the-art by 6% on the QRData real-world dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Causal Agent bridges the gap between LLMs' natural language strengths and the tabular data requirements of causal inference by wrapping causal analysis tools in a memory-augmented agent framework.
- **Mechanism:** The agent uses tool modules to invoke causal libraries (causal-learn, EconML), memory modules to store causal graph instances as Python class objects, and plan modules to iteratively call tools and reason over outputs. This decouples the LLM from direct tabular data handling while retaining interpretability.
- **Core assumption:** LLMs can reliably follow structured prompts and invoke the correct tool with the right parameters when guided by well-designed interfaces.
- **Evidence anchors:**
  - [abstract] "We have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems."
  - [section] "In the tools module, the causal agent calls Python code and uses the encapsulated causal function module to align tabular data with natural language."
  - [corpus] Weak - no direct mentions of this specific modular tool wrapper approach.
- **Break condition:** If LLM prompt-following degrades, parameter-passing errors increase, or tool invocation chains become too long, the agent's reasoning reliability drops.

### Mechanism 2
- **Claim:** Four-level modeling of causal problems enables systematic assessment of LLM causal capabilities and guides tool selection for each level.
- **Mechanism:** Variable level → correlation testing, Edge level → causal relationship classification, Causal Graph level → graph generation, Causal Effect level → ATE estimation. Each level uses tailored tools and prompt templates.
- **Core assumption:** Causal problem granularity aligns with specific tool capabilities, and LLMs can learn to distinguish which level a question belongs to.
- **Evidence anchors:**
  - [abstract] "We categorize the causal problems into four major levels... variable level, edge level, causal graph level, and causal effect level."
  - [section] "We have designed specific tool functions at the variable level, edge level, causal graph level, and causal effect level."
  - [corpus] Weak - no explicit mention of this hierarchical four-level approach in related papers.
- **Break condition:** If the problem description does not fit cleanly into one of the four levels, or if the LLM misclassifies the level, tool invocation fails.

### Mechanism 3
- **Claim:** Causal graphs serve as an intermediate memory format, allowing tools to communicate and enabling iterative refinement of answers.
- **Mechanism:** Each tool invocation produces a causal graph instance stored in a dictionary keyed by descriptive names. Subsequent reasoning steps retrieve and analyze these graphs without recomputing or re-describing.
- **Core assumption:** Causal graphs are sufficient as a compact, information-rich representation that tools and the LLM can both manipulate effectively.
- **Evidence anchors:**
  - [abstract] "In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs."
  - [section] "The causal agent maintains a dictionary as memory... the index content consists of data structures such as causal graph instances containing richer information."
  - [corpus] Weak - related works focus on LLM+tool integration but not on graph-based memory for tool chaining.
- **Break condition:** If the causal graph structure is too complex to serialize or if tools cannot reliably reconstruct the graph from intermediate states.

## Foundational Learning

- **Concept:** Tabular data representation and preprocessing
  - Why needed here: Causal discovery and inference algorithms require numeric data in specific formats (e.g., pandas DataFrames). LLMs must learn to invoke the correct preprocessing pipeline.
  - Quick check question: What Python library and function should you use to load a CSV file into a format suitable for causal-learn's PC algorithm?

- **Concept:** Independence and conditional independence testing
  - Why needed here: Variable-level questions require statistical testing to determine whether two variables are correlated or conditionally independent.
  - Quick check question: Which statistical test is used by causal-learn's independence test module for continuous variables, and what is its null hypothesis?

- **Concept:** Causal effect estimation (ATE, CATE)
  - Why needed here: Causal effect level questions require counterfactual reasoning and estimation of average treatment effects using methods like double machine learning.
  - Quick check question: In EconML's LinearDML estimator, what are the roles of the treatment variable, outcome variable, and covariates?

## Architecture Onboarding

- **Component map:** LLM core -> Plan module -> Tools module (independence tests, PC algorithm, EconML estimators) -> Memory module (dictionary of causal graph objects)
- **Critical path:** Input → Tool selection → Tool invocation → Memory update → Reflection → Next tool or final answer
- **Design tradeoffs:**
  - Memory granularity: Full causal graphs vs. intermediate statistics
  - Tool abstraction level: Fine-grained control vs. ease of use
  - LLM autonomy: Let LLM decide tool sequence vs. hard-coded pipelines
- **Failure signatures:**
  - Repeated tool invocation with same parameters (LLM not learning)
  - Missing or corrupted graph objects in memory (memory management bug)
  - Incorrect parameter passing (prompt template or interface error)
- **First 3 experiments:**
  1. Unit test: Feed a simple 3-variable CSV and a variable-level independence question; verify correct tool call and output.
  2. Integration test: Feed a 4-variable CSV and an edge-level causal relationship question; verify graph generation and relationship classification.
  3. Memory test: Chain two tool calls (graph generation → partial graph extraction); verify memory dictionary updates and correct retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Causal Agent framework perform when extended to handle unobserved confounders in real-world data?
- Basis in paper: [inferred] The paper mentions using the PC algorithm for causal graph generation, which assumes no unobserved variables. It also states that for partial causal graphs, the PC algorithm is still used, but unobserved variables are treated as controlled by the user.
- Why unresolved: The paper does not explore the performance of the Causal Agent when dealing with unobserved confounders, which are common in real-world scenarios.
- What evidence would resolve it: Experiments comparing the Causal Agent's performance with and without unobserved confounders in real-world datasets, using algorithms like FCI that can handle unobserved variables.

### Open Question 2
- Question: What is the impact of varying the maximum iteration count on the Causal Agent's performance and computational cost?
- Basis in paper: [explicit] The paper mentions setting a maximum iteration count of 15 and analyzes the computational cost for different task levels, but does not explore the impact of varying this parameter.
- Why unresolved: The paper does not investigate how changing the maximum iteration count affects the accuracy and efficiency of the Causal Agent.
- What evidence would resolve it: Experiments varying the maximum iteration count and measuring the resulting accuracy and computational cost across different task levels.

### Open Question 3
- Question: How does the Causal Agent's performance compare to human experts in causal reasoning tasks on real-world data?
- Basis in paper: [explicit] The paper compares the Causal Agent's performance to various baseline models on QRData but does not include human expert performance as a comparison.
- Why unresolved: The paper does not assess how the Causal Agent's causal reasoning abilities stack up against human experts, which is a crucial benchmark for evaluating its practical utility.
- What evidence would resolve it: A study comparing the Causal Agent's performance to human experts on a diverse set of real-world causal reasoning tasks, measuring accuracy, reasoning process, and time taken to reach conclusions.

## Limitations
- Performance claims rely on proprietary OpenAI models without specifying model versions, temperature settings, or detailed prompt templates
- Four-level problem categorization assumes clean separation between problem types, but real-world causal questions often span multiple levels simultaneously
- Experiments focus on controlled synthetic data and a single real-world dataset, limiting generalizability to diverse domains and noise conditions

## Confidence
- **High confidence:** The general framework design combining LLMs with causal tools and memory modules is well-specified and the four-level categorization provides a logical structure for causal problem solving.
- **Medium confidence:** The reported accuracy rates above 80% across all levels are plausible given the methodology, though exact reproduction would require access to the specific prompt templates and model configurations.
- **Low confidence:** The 6% improvement over state-of-the-art on QRData is difficult to verify without knowing the exact baseline methods and comparison conditions.

## Next Checks
1. **Tool invocation reliability test:** Create a benchmark suite of 50 diverse causal questions across all four levels, systematically vary the complexity of tabular data (number of variables, sample size), and measure the percentage of successful tool invocations versus failed attempts due to parameter passing errors or LLM misinterpretation.

2. **Memory consistency audit:** For chained reasoning tasks requiring multiple tool invocations, track the causal graph objects stored in memory after each step, verify their structural integrity, and test whether subsequent reasoning steps correctly retrieve and utilize these intermediate results.

3. **Cross-domain generalization study:** Apply the Causal Agent to two additional real-world datasets from different domains (e.g., healthcare and economics) with known causal structures, and compare performance against both the original framework and baseline causal inference methods, measuring both accuracy and computational efficiency.