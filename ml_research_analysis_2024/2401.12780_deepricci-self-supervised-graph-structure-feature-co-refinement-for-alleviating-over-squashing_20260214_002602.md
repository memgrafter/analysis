---
ver: rpa2
title: 'DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating
  Over-squashing'
arxiv_id: '2401.12780'
source_url: https://arxiv.org/abs/2401.12780
tags:
- graph
- ricci
- curvature
- structure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepRicci introduces a self-supervised graph structure-feature
  co-refinement framework that addresses the over-squashing problem in graph neural
  networks (GNNs) by leveraging Riemannian geometry. The method models Ricci curvature
  in a latent heterogeneous Riemannian space constructed through rotational products,
  enabling different regions to exhibit distinct curvatures.
---

# DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing

## Quick Facts
- arXiv ID: 2401.12780
- Source URL: https://arxiv.org/abs/2401.12780
- Authors: Li Sun; Zhenhao Huang; Hua Wu; Junda Ye; Hao Peng; Zhengtao Yu; Philip S. Yu
- Reference count: 40
- Primary result: Achieves state-of-the-art node classification accuracy (82.8% on Cora) using self-supervised graph structure-feature co-refinement

## Executive Summary
DeepRicci introduces a self-supervised framework that jointly refines graph structure and node features to alleviate over-squashing in graph neural networks. The method leverages Riemannian geometry by modeling graphs in a latent heterogeneous Riemannian space with rotational products, enabling different regions to exhibit distinct curvatures. Through gyrovector feature mapping and geometric contrastive learning, DeepRicci refines node features while optimizing graph structure using backward Ricci flow based on a differentiable formulation of Ricci curvature.

## Method Summary
DeepRicci constructs a latent Riemannian space using rotational products to enable heterogeneous curvature modeling. It employs gyrovector feature mapping to bridge between Riemannian manifolds and Euclidean space, allowing typical GNNs to utilize geometric information. Node features are refined through geometric contrastive learning among different geometric views, while graph structure is optimized using backward Ricci flow based on a differentiable formulation of Ricci curvature. The framework jointly optimizes structure and features in a self-supervised manner without requiring label information.

## Key Results
- Achieves 82.8% accuracy on Cora node classification, outperforming existing self-supervised methods and even some supervised baselines
- Demonstrates superior performance in node clustering tasks, achieving 73.1% accuracy on Cora
- Theoretical analysis proves refined graph structure increases Cheeger's constant, widening information bottlenecks to alleviate over-squashing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotational product construction enables heterogeneous curvature modeling that single-curvature spaces cannot achieve.
- Mechanism: The rotational product M = P ⊗ F creates a latent Riemannian space where the rotational factor F provides curvature heterogeneity through its polar coordinates, while the product factors P provide structural dimensions.
- Core assumption: Different regions of real-world graphs require different curvatures to accurately represent their structural properties.
- Evidence anchors:
  - [abstract]: "introduce a latent Riemannian space of heterogeneous curvatures to model various Ricci curvatures"
  - [section]: "Proposition 1 (Heterogeneous Manifold). The product of M = P ⊗ F is a heterogeneous manifold, where F = ¯GdM + is the upper hypersphere providing curvature heterogeneity."
  - [corpus]: Weak evidence - corpus neighbors discuss curvature but not heterogeneous product constructions specifically.

### Mechanism 2
- Claim: Gyrovector feature mapping bridges Riemannian manifolds to Euclidean space while preserving isometry invariance.
- Mechanism: Gyrovector waves GFλ,b,ω(xκ) = exp(n−1/2 ⟨ω, xκ⟩κ)cos(λ⟨ω, xκ⟩κ + b) create a Euclidean embedding that maintains geometric properties through isometry-invariant kernels.
- Core assumption: Isometry invariance is necessary for meaningful feature transfer between Riemannian and Euclidean spaces.
- Evidence anchors:
  - [abstract]: "propose a gyrovector feature mapping to utilize Ricci curvature for typical GNNs"
  - [section]: "Proposition 2 (Isometry Invariant). The induced kernel k(xκ, yκ) = E[GFλ,b,ω(xκ)GFλ,b,ω(yκ)] from the gyrovector feature mapping is isometry invariant and real."
  - [corpus]: Weak evidence - corpus neighbors discuss curvature-aware networks but not gyrovector mappings specifically.

### Mechanism 3
- Claim: Differentiable Ricci curvature formulation enables gradient-based optimization for structure learning.
- Mechanism: The Laplacian matrix Lα and affine transform f create a differentiable approximation of Ollivier's Ricci curvature that can be backpropagated through.
- Core assumption: A differentiable approximation can sufficiently capture the discrete Ricci curvature properties needed for structure optimization.
- Evidence anchors:
  - [abstract]: "refine graph structure by backward Ricci flow based on a novel formulation of differentiable Ricci curvature"
  - [section]: "Proposition 3 (Upper Bound). The differentiable Ricci curvature in Eq. (15) is the upper bound of Ollivier's Ricci curvature in Eq. (2), [R(A, X)]ij = sup Ricα ij."
  - [corpus]: Weak evidence - corpus neighbors discuss curvature but not differentiable formulations specifically.

## Foundational Learning

- Concept: Riemannian geometry and manifold theory
  - Why needed here: The entire framework relies on modeling graphs as manifolds with curvature properties
  - Quick check question: What is the relationship between sectional curvature and Ricci curvature in a Riemannian manifold?

- Concept: Wasserstein distance and optimal transport
  - Why needed here: Ollivier's Ricci curvature definition depends on optimal transport between probability distributions
  - Quick check question: How does the Kantorovich-Rubinstein duality relate Wasserstein distance to Lipschitz functions?

- Concept: Gyrovector spaces and hyperbolic geometry
  - Why needed here: Gyrovector formalism provides the algebraic structure for hyperbolic operations in the feature mapping
  - Quick check question: What is the difference between gyrovector addition and standard vector addition in hyperbolic space?

## Architecture Onboarding

- Component map:
  - Rotational product construction → Latent Riemannian space
  - Gyrovector feature mapping → Euclidean bridge
  - Differentiable Ricci curvature → Structure refinement
  - Geometric contrastive learning → Feature refinement
  - Backbone GNN → View generation

- Critical path:
  1. Initialize rotational product space with learnable parameters
  2. Apply gyrovector feature mapping to create Euclidean representations
  3. Compute differentiable Ricci curvature for structure refinement
  4. Generate geometric views through GNN
  5. Apply contrastive learning for feature refinement
  6. Jointly optimize structure and features

- Design tradeoffs:
  - Single vs. multiple curvature factors: More factors increase expressiveness but computational cost
  - Forward vs. backward Ricci flow: Forward flow narrows bottlenecks (worse for over-squashing), backward flow widens them
  - Feature mapping complexity: More complex mappings preserve more geometry but are harder to train

- Failure signatures:
  - Training instability: Indicates issues with gyrovector operations or curvature calculations
  - Convergence to trivial solutions: Suggests problems with contrastive learning or regularization
  - Performance worse than baseline: Could indicate incorrect curvature modeling or feature mapping issues

- First 3 experiments:
  1. Verify heterogeneous curvature by testing if different nodes learn different curvature values
  2. Compare gyrovector features vs. logarithmic mapping on a simple classification task
  3. Test backward vs. forward Ricci flow on a known bottleneck graph to observe structural changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of curvature distribution ρ in the gyrovector feature mapping affect the quality of the isometry-invariant kernel and downstream task performance?
- Basis in paper: [explicit] The paper mentions that ρ is a Gaussian distribution in practice but does not explore how different distributions might impact results.
- Why unresolved: The paper does not provide empirical analysis of different distributions or theoretical guarantees on the optimal choice.
- What evidence would resolve it: Systematic experiments comparing different distributions (Gaussian, uniform, Laplace) with corresponding kernel quality metrics and downstream task performance.

### Open Question 2
- Question: What is the relationship between the dimensionality of the rotational factor F and the optimal number of Riemannian factors M in the product space?
- Basis in paper: [explicit] The paper uses a 3-factor rotational product but does not explore how the choice of M affects performance or provide theoretical guidance on optimal selection.
- Why unresolved: The paper fixes M=3 without justification or exploration of how different numbers of factors impact the heterogeneous curvature modeling capability.
- What evidence would resolve it: Experiments varying M across different datasets and theoretical analysis of the trade-offs between model complexity and curvature modeling capacity.

### Open Question 3
- Question: How does the proposed backward Ricci flow compare to other over-squashing mitigation techniques like DropEdge or curvature-based edge dropping in terms of preserving important graph structures?
- Basis in paper: [explicit] The paper mentions DropEdge as a baseline but does not provide detailed analysis of how backward Ricci flow preserves versus removes edges compared to other methods.
- Why unresolved: While the paper shows superior performance, it lacks ablation studies isolating the effect of backward Ricci flow versus alternative structure learning approaches on graph topology preservation.
- What evidence would resolve it: Detailed edge-level analysis comparing which edges are preserved by different methods and how this relates to over-squashing mitigation and task performance.

## Limitations
- The heterogeneous curvature modeling through rotational products, while theoretically elegant, may not be necessary for all graph types and adds significant computational overhead
- The gyrovector feature mapping introduces complexity that may not provide substantial benefits over simpler alternatives like logarithmic mappings in practical applications
- The framework's reliance on differentiable Ricci curvature approximations may introduce errors that affect the quality of structure refinement

## Confidence
- **High Confidence**: The core premise that over-squashing can be addressed through structure learning is well-established in the literature. The empirical results showing DeepRicci's performance gains on benchmark datasets are concrete and measurable.
- **Medium Confidence**: The theoretical propositions about heterogeneous manifolds and isometry-invariant kernels are mathematically sound, but their practical implementation may introduce approximations that affect performance.
- **Low Confidence**: The claim that gyrovector feature mapping is essential for the method's success, as opposed to simpler alternatives like logarithmic mappings, requires more direct comparison and ablation studies.

## Next Checks
1. **Heterogeneous Curvature Verification**: Measure and compare the distribution of Ricci curvatures across different nodes before and after DeepRicci's refinement to confirm that the method actually creates heterogeneous curvature patterns as claimed.
2. **Gyrovector vs. Logarithmic Mapping**: Implement a variant of DeepRicci using logarithmic mapping instead of gyrovector features and compare performance to isolate the contribution of the gyrovector design choice.
3. **Forward vs. Backward Ricci Flow**: Conduct controlled experiments on graphs with known bottleneck structures to verify that backward Ricci flow specifically widens bottlenecks while forward flow narrows them, as the theory suggests.