---
ver: rpa2
title: Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation
  and Attack
arxiv_id: '2403.07943'
source_url: https://arxiv.org/abs/2403.07943
tags:
- uni00000013
- uni00000011
- graph
- uni00000057
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified formulation for two categories of
  edge perturbation methods (Gaug and Gatk) for GNNs, addressing the open questions
  of why edge perturbation has a two-faced effect and what makes it flexible and effective.
  The core idea is to bridge Gaug and Gatk methods by proving they are essentially
  the same technique with different restricted conditions, and devising Edge Priority
  Detector (EPD) to generate a unified priority metric.
---

# Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack

## Quick Facts
- arXiv ID: 2403.07943
- Source URL: https://arxiv.org/abs/2403.07943
- Authors: Xin Liu; Yuxiang Zhang; Meng Wu; Mingyu Yan; Kun He; Wei Yan; Shirui Pan; Xiaochun Ye; Dongrui Fan
- Reference count: 40
- One-line primary result: Unified formulation of edge perturbation methods with Edge Priority Detector achieves comparable or superior performance to existing methods with significant efficiency gains.

## Executive Summary
This paper addresses the open questions of why edge perturbation methods have two-faced effects (augmentation vs attack) and what makes them flexible and effective for Graph Neural Networks. The authors propose a unified formulation that casts both augmentation and attack methods into the same optimization problem with different constraint conditions. They introduce Edge Priority Detector (EPD) to generate a priority metric that bridges the two approaches, enabling flexible perturbation decisions with less computational overhead.

## Method Summary
The paper presents a unified optimization framework that minimizes cross-entropy loss with L1 norm regularization to achieve both graph augmentation and attack objectives. The unified formulation proves that Gaug and Gatk methods are essentially the same technique with different restricted conditions - augmentation requires predicted labels to match true labels while attack requires them to mismatch. EPD generates perturbation priorities based on graph homophily (Solution I) or target-guided modification (Solution II), enabling flexible application for either augmentation or attack. The pre-computation of priority metrics offline allows efficient online perturbation decisions during training.

## Key Results
- EPD achieves comparable or superior performance to existing Gaug and Gatk methods across multiple datasets (Cora, Citeseer, Pubmed)
- The unified formulation successfully bridges augmentation and attack methods, demonstrating they are essentially the same technique
- EPD provides significant efficiency gains through offline pre-computation of priority metrics, reducing online computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EPD unifies Gaug and Gatk methods by generating a priority metric that determines which edges to perturb for augmentation or attack.
- Mechanism: EPD calculates edge importance based on graph homophily or target-guided modification, then uses this priority matrix to guide edge perturbation.
- Core assumption: Edges connecting nodes with the same label are more important for model performance, and perturbing these edges differently can achieve augmentation or attack effects.
- Evidence anchors:
  - [abstract] "we devise Edge Priority Detector (EPD) to generate a novel priority metric, bridging these methods up in the workflow"
  - [section] "EPD can generate perturbation priorities of edges, based on which perturbations will be made for augmentation or attack flexibly"
- Break condition: If graph homophily assumption fails (e.g., in heterophilic graphs), EPD's priority metric may not effectively distinguish augmentation vs attack edges.

### Mechanism 2
- Claim: The unified formulation proves Gaug and Gatk are essentially the same technique with different restricted conditions.
- Mechanism: Both methods can be cast as an optimization problem minimizing cross-entropy loss, with constraints that differ only in whether the predicted label should match or mismatch the true label.
- Core assumption: The optimization problem structure is identical for both augmentation and attack, differing only in constraint conditions.
- Evidence anchors:
  - [abstract] "we theoretically unify the workflow of Gaug and Gatk methods by casting it to an optimization problem"
  - [section] "Gaug and Gatk methods are essentially the same technique with different restricted conditions"
- Break condition: If the L0/L1 norm approximation fails or the optimization problem becomes intractable for large graphs.

### Mechanism 3
- Claim: EPD achieves efficiency by pre-computing the priority metric once offline, then applying it online for any configuration.
- Mechanism: The priority matrix calculation has O(NÂ²) complexity but is performed offline, allowing flexible and fast perturbation decisions during training.
- Core assumption: Pre-computing the priority metric is computationally feasible and the metric remains valid across different GNN configurations and training settings.
- Evidence anchors:
  - [abstract] "Experiments show that EPD can make augmentation or attack flexibly and achieve comparable or superior performance to other counterparts with less time overhead"
- Break condition: If the graph structure changes significantly during training, the pre-computed priority metric may become outdated.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their sensitivity to graph structure
  - Why needed here: Understanding how GNNs process graph topology is essential for designing effective edge perturbation methods
  - Quick check question: How does removing an edge between two nodes affect the message passing in a GCN layer?

- Concept: Graph homophily and its impact on GNN performance
  - Why needed here: EPD relies on homophily principles to distinguish important edges for augmentation vs attack
  - Quick check question: In a homophilic graph, are edges connecting nodes of the same class generally more or less important for GNN performance?

- Concept: Optimization problems with L0/L1 norm constraints
  - Why needed here: The unified formulation requires solving an optimization problem with discrete constraints
  - Quick check question: Why is L0 norm non-differentiable, and how does replacing it with L1 norm help in gradient-based optimization?

## Architecture Onboarding

- Component map: Graph G(V, E) with adjacency matrix A and node features X -> EPD Module -> Priority matrix I -> Perturbed adjacency matrix A' -> GNN Model -> Classification/prediction results

- Critical path:
  1. Pre-compute priority matrix I using EPD (offline)
  2. Apply perturbations to adjacency matrix A based on I and budget b
  3. Feed perturbed graph A' to GNN for training/inference
  4. Evaluate performance (accuracy for augmentation, misclassification for attack)

- Design tradeoffs:
  - Pre-computation vs online computation: EPD trades offline computation for online efficiency
  - Solution I vs Solution II: Homophily-based vs target-guided modification for different graph characteristics
  - EPR restriction: Balancing perturbation effectiveness with computational cost and graph integrity

- Failure signatures:
  - Poor augmentation performance: Priority metric not capturing truly important edges
  - Ineffective attacks: Perturbations not significantly disrupting GNN predictions
  - High computational cost: Offline pre-computation taking too long for large graphs
  - Model instability: Perturbations causing training instability or convergence issues

- First 3 experiments:
  1. Test EPD with varying EPR on Cora dataset using GCN, compare accuracy to vanilla case
  2. Compare EPD Solution I vs Solution II on Citeseer for both augmentation and attack
  3. Measure time efficiency of EPD vs DropEdge on Pubmed dataset across multiple GNN architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific structural properties of graphs make them more susceptible or resistant to edge perturbation methods for both augmentation and attack?
- Basis in paper: [explicit] The paper discusses correlations between the effectiveness of edge perturbation methods and graph intrinsic attributes in Appendix, including metrics like global efficiency, clustering coefficient, degree distribution, etc.
- Why unresolved: The paper identifies these correlations but doesn't provide a comprehensive theoretical framework explaining which structural properties (e.g., diameter, density, community structure) systematically determine susceptibility to edge perturbations.
- What evidence would resolve it: A systematic study varying graph structural properties (small-world, scale-free, random graphs) and measuring perturbation effectiveness across multiple metrics would reveal which structural characteristics are most influential.

### Open Question 2
- Question: How do different types of GNN architectures (beyond GCN, GraphSAGE, SGC, DAGNN) respond to edge perturbations, and what architectural features make some models more robust to adversarial attacks or more responsive to augmentation?
- Basis in paper: [explicit] The paper evaluates EPD on four GNN variants and notes that DAGNN shows some resistance to attacks due to its adaptive adjustment mechanism, suggesting architectural differences matter.
- Why unresolved: The study is limited to a small set of GNN architectures, and the paper doesn't explore the fundamental architectural properties (attention mechanisms, residual connections, adaptive normalization) that contribute to robustness or sensitivity.
- What evidence would resolve it: Comparative experiments across a diverse set of GNN architectures with varying mechanisms (attention, normalization, residual connections) would identify which architectural features consistently improve robustness or augmentation effectiveness.

### Open Question 3
- Question: What is the theoretical relationship between the extent of edge perturbations (EPR) and the point at which augmentation effectiveness plateaus or attack effectiveness peaks, and how does this vary across different graph datasets and tasks?
- Basis in paper: [explicit] The paper observes that accuracy tends to stabilize as EPR exceeds certain thresholds, but this is presented as an empirical observation rather than a theoretical framework.
- Why unresolved: The paper doesn't provide a theoretical model explaining why there's a point of diminishing returns for both augmentation and attack, nor does it characterize how this relationship varies with graph characteristics or task complexity.
- What evidence would resolve it: Developing a theoretical framework that models the relationship between EPR and performance based on graph properties (size, density, label distribution) would explain the observed plateaus and peaks, validated through extensive empirical testing across diverse datasets.

## Limitations
- The offline pre-computation of priority metrics may become a bottleneck for dynamic graphs where structure changes during training
- The unified optimization formulation may face convergence challenges for large-scale graphs
- The assumption of universal graph homophily principles may limit EPD's effectiveness in heterophilic networks

## Confidence
- **High Confidence**: The empirical results demonstrating EPD's comparable or superior performance to existing methods across multiple datasets and GNN architectures
- **Medium Confidence**: The theoretical unification of Gaug and Gatk methods through the optimization framework, as the mathematical proofs rely on specific assumptions about loss functions and constraints
- **Low Confidence**: The generalizability of EPD to heterogeneous graphs and dynamic graph scenarios where homophily assumptions may not hold

## Next Checks
1. Test EPD on heterophilic datasets (e.g., Texas, Cornell) to evaluate performance when graph homophily assumption fails
2. Systematically remove components of the unified formulation (e.g., L1 regularization, augmented Lagrangian) to quantify their individual contributions
3. Implement EPD on evolving graph streams to measure performance degradation when pre-computed priorities become outdated