---
ver: rpa2
title: 'HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual
  Settings'
arxiv_id: '2410.13671'
source_url: https://arxiv.org/abs/2410.13671
tags:
- language
- queries
- english
- were
- indic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 24 large language models on real-world healthcare
  queries from Indian patients using a uniform retrieval-augmented generation framework.
  Models were tested on 750 multilingual queries across English and four Indic languages,
  with responses assessed on factual correctness, semantic similarity, coherence,
  and conciseness.
---

# HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings

## Quick Facts
- **arXiv ID**: 2410.13671
- **Source URL**: https://arxiv.org/abs/2410.13671
- **Reference count**: 24
- **Primary result**: Performance disparities exist between languages and models, with smaller models sometimes outperforming larger ones

## Executive Summary
This study evaluates 24 large language models on real-world healthcare queries from Indian patients using a uniform retrieval-augmented generation framework. The evaluation spans 750 multilingual queries across English and four Indic languages, assessing responses on factual correctness, semantic similarity, coherence, and conciseness. Results reveal significant performance variations across models and languages, with factual correctness consistently lower for non-English queries. Notably, smaller models occasionally outperform larger ones, and instruction-tuned Indic models don't consistently excel on Indic language queries. Human evaluation confirms strong alignment with automated LLM-as-a-judge assessments.

## Method Summary
The study employs a uniform retrieval-augmented generation framework to evaluate 24 large language models on 750 multilingual healthcare queries from Indian patients. Queries span English and four Indic languages, with responses assessed through both automated LLM-as-a-judge metrics (factual correctness, semantic similarity, coherence, conciseness) and human evaluation. The framework standardizes the retrieval and generation components across all models to enable fair comparison. Human evaluators independently assess a subset of responses to validate the automated evaluation approach, demonstrating strong correlation between the two methods.

## Key Results
- Models show significant performance variation, with smaller models sometimes outperforming larger ones
- Factual correctness rates are consistently lower for non-English queries compared to English
- Instruction-tuned Indic models do not consistently outperform other models on Indic language queries
- Code-mixed and culturally relevant queries pose particular challenges for RAG systems

## Why This Works (Mechanism)
The uniform RAG framework enables fair comparison across models by standardizing retrieval and generation components. This approach reveals that model size alone doesn't determine performance, as smaller models can outperform larger ones on specific tasks. The LLM-as-a-judge evaluation method provides scalable assessment while maintaining correlation with human judgments. The framework's effectiveness stems from its ability to isolate model performance differences while controlling for external variables like retrieval quality and prompt engineering.

## Foundational Learning
- **RAG Framework**: Why needed - Combines retrieval of relevant information with generation of responses; Quick check - Verify retrieval component returns relevant documents
- **Multilingual Evaluation**: Why needed - Healthcare chatbots must serve diverse language populations; Quick check - Test queries in multiple languages with native speakers
- **LLM-as-a-judge**: Why needed - Scalable automated evaluation method; Quick check - Compare automated scores with human evaluations
- **Code-mixing Challenges**: Why needed - Real-world queries often combine languages; Quick check - Include mixed-language queries in evaluation set
- **Model Size vs Performance**: Why needed - Debunks assumption that larger models always perform better; Quick check - Compare small and large models on identical tasks

## Architecture Onboarding
**Component Map**: User Query -> Retrieval Engine -> Context Augmentation -> LLM Generator -> Response Evaluation -> Feedback Loop

**Critical Path**: User Query → Retrieval Engine → Context Augmentation → LLM Generator → Response

**Design Tradeoffs**: Standardizing retrieval and generation components across models enables fair comparison but may not capture optimal configurations for individual models. The focus on Indian healthcare context limits generalizability but provides valuable insights for this specific domain.

**Failure Signatures**: Poor performance on code-mixed queries, lower factual correctness for non-English languages, unexpected underperformance of larger models, and lack of advantage for instruction-tuned Indic models on Indic language queries.

**First Experiments**:
1. Test retrieval component independently to verify it returns relevant documents for queries in all five languages
2. Run ablation study removing retrieval component to measure impact on factual correctness
3. Compare performance of smallest vs largest models on identical query sets to understand size-performance relationship

## Open Questions the Paper Calls Out
- Why do smaller models sometimes outperform larger ones on multilingual healthcare queries?
- What specific aspects of code-mixed queries cause performance degradation across all models?
- Why don't instruction-tuned Indic models show consistent advantages on Indic language queries?
- How can retrieval components be optimized for multilingual healthcare contexts?
- What factors contribute to the performance gap between English and non-English queries?

## Limitations
- LLM-as-a-judge evaluation, while correlated with human assessment, may not fully capture nuanced quality aspects
- Small sample size of 750 queries across five languages may not represent full query diversity
- Focus on Indian healthcare context limits generalizability to other multilingual domains
- Performance gaps between languages highlight significant limitations for low-resource languages
- Unexpected results with smaller models outperforming larger ones suggest potential methodological issues

## Confidence
- Confidence in primary findings regarding performance disparities across languages and models: Medium
- Confidence in relative performance rankings of specific models: Low
- Confidence in effectiveness of current multilingual fine-tuning approaches: Low

## Next Checks
1. Conduct a larger-scale human evaluation study with diverse medical professionals to validate LLM-as-a-judge assessments, particularly for code-mixed and culturally-specific queries
2. Expand evaluation to include healthcare queries from additional multilingual contexts outside India to test generalizability
3. Perform ablation studies to isolate the impact of different components (retrieval, generation, ranking) on performance disparities across languages and models