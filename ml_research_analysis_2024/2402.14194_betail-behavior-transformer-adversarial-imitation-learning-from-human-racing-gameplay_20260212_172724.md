---
ver: rpa2
title: 'BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing
  Gameplay'
arxiv_id: '2402.14194'
source_url: https://arxiv.org/abs/2402.14194
tags:
- policy
- betail
- learning
- human
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BeTAIL, which combines a Behavior Transformer
  (BeT) policy from human demonstrations with online Adversarial Imitation Learning
  (AIL) to learn racing policies from human demonstrations in Gran Turismo Sport.
  BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making
  process of human experts and correct for out-of-distribution states or shifts in
  environment dynamics.
---

# BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay

## Quick Facts
- arXiv ID: 2402.14194
- Source URL: https://arxiv.org/abs/2402.14194
- Reference count: 38
- One-line primary result: BeTAIL achieves faster lap times and smoother steering than AIL alone, with BeTAIL(0.05) achieving 129.2s lap times on Lago Maggiore vs 131.6s for AIL, and BeTAIL(0.10) achieving 112.1s lap times on Dragon Tail vs 109.1s for AIL.

## Executive Summary
This paper introduces BeTAIL, a method that combines a Behavior Transformer (BeT) policy trained on offline human demonstrations with an online Adversarial Imitation Learning (AIL) residual policy to learn racing policies from human gameplay in Gran Turismo Sport. The key innovation is using the BeT to capture non-Markovian decision-making patterns from demonstrations, then fine-tuning with a lightweight AIL residual policy to correct for out-of-distribution states and improve performance. The approach achieves superior racing performance with fewer environment interactions compared to using BeT or AIL alone.

## Method Summary
BeTAIL works by first pretraining a Behavior Transformer (BeT) on offline human demonstrations from Gran Turismo Sport, capturing non-Markovian decision-making patterns through sequence modeling of state-action trajectories. The BeT policy is then frozen and a lightweight residual policy is trained online using Adversarial Imitation Learning (AIL), conditioned on both the current state and the BeT's predicted action. The residual action is constrained to a small range [-α, α] to ensure corrections remain close to the BeT's non-Markovian behavior while still allowing adaptation to the specific environment. This combination aims to reduce environment interactions while improving racing performance and stability.

## Key Results
- BeTAIL outperforms pure BeT and pure AIL baselines in terms of lap time completion and success rate across multiple tracks
- BeTAIL(0.05) achieves 129.2s lap times on Lago Maggiore compared to 131.6s for AIL, with smoother steering behavior
- BeTAIL(0.10) achieves 112.1s lap times on Dragon Tail compared to 109.1s for AIL, demonstrating superior performance on more complex tracks
- The method shows better generalization when pretrained on different tracks than the downstream learning task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The residual AIL policy corrects for out-of-distribution states or minor shifts in environment dynamics.
- Mechanism: The residual policy is conditioned on the current state and the base action predicted by the BeT, allowing it to make small adjustments while remaining close to the non-Markovian behavior modeled by the BeT. This is achieved by constraining the residual action between [-α, α], where α is small enough to ensure the agent's action remains near the BeT's prediction but large enough to correct errors.
- Core assumption: The base BeT policy captures the essential non-Markovian decision-making patterns of human experts, and minor deviations from this behavior can be corrected by a small residual policy.
- Evidence anchors:
  - [abstract]: "BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environment dynamics."
  - [section]: "The residual action is predicted by a Gaussian residual policy, ˜a ∼ fres(˜a|st, ˆat) = N (µ, σ), conditioned on the current state and the base policy's action."
- Break condition: If the environment dynamics shift significantly or the BeT policy fails to capture essential non-Markovian patterns, the residual policy may not be sufficient to correct for these errors.

### Mechanism 2
- Claim: Sequence modeling with the Behavior Transformer captures non-Markovian decision-making patterns in human demonstrations.
- Mechanism: The BeT processes trajectories as sequences of states and actions, using the past K timesteps to generate the next action. This allows the policy to model the probability P(at|st, st-1, ..., st-h+1) rather than just P(at|st), capturing the sequential decision-making process of human experts.
- Core assumption: Human decision-making in racing is non-Markovian, meaning the next action depends not only on the current state but also on the history of previous states and actions.
- Evidence anchors:
  - [abstract]: "BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts..."
  - [section]: "A notable strength of BeT is that the policy can model non-Markovian behavior; in other words, rather than modeling the action probability as P(at|st), the policy models the probability P(at|st, st-1, ..., st-h+1)."
- Break condition: If human decision-making in the specific racing task is actually Markovian, the additional complexity of sequence modeling may not provide significant benefits.

### Mechanism 3
- Claim: Combining offline sequence modeling with online AIL fine-tuning reduces environment interactions while improving performance.
- Mechanism: The BeT is first pretrained on offline demonstrations to capture non-Markovian patterns. Then, a lightweight residual AIL policy is trained online to correct for errors and adapt to the specific environment. This approach leverages the strengths of both offline learning (sample efficiency, modeling complex patterns) and online learning (adaptation, occupancy matching).
- Core assumption: The offline demonstrations provide a good representation of the expert behavior, and online fine-tuning can effectively correct for errors without requiring extensive environment interactions.
- Evidence anchors:
  - [abstract]: "Our proposed residual BeTAIL reduces environment interactions and improves racing performance and stability, even when the BeT is pretrained on different tracks than downstream learning."
  - [section]: "First, a BeT policy is learned from offline human demonstrations. Then, the BeT policy is frozen, and a lightweight, residual policy is trained with Adversarial Imitation Learning (AIL)."
- Break condition: If the offline demonstrations are of poor quality or the environment dynamics differ significantly from those in the demonstrations, this approach may not be effective.

## Foundational Learning

- Concept: Adversarial Imitation Learning (AIL)
  - Why needed here: AIL is used to fine-tune the BeT policy online, helping it adapt to the specific environment and correct for errors. It uses a discriminator to encourage the agent to match the state occupancy of expert trajectories.
  - Quick check question: What is the main advantage of using AIL over traditional imitation learning methods like behavior cloning?

- Concept: Sequence Modeling with Transformers
  - Why needed here: Sequence modeling allows the BeT to capture non-Markovian decision-making patterns in human demonstrations. It processes trajectories as sequences of states and actions, using the past K timesteps to generate the next action.
  - Quick check question: How does sequence modeling with transformers differ from traditional Markovian approaches in imitation learning?

- Concept: Residual Policy Learning
  - Why needed here: Residual policy learning allows the agent to make small adjustments to the actions predicted by the BeT while remaining close to the non-Markovian behavior modeled by the BeT. This is achieved by constraining the residual action between [-α, α].
  - Quick check question: What is the main advantage of using a residual policy in this context, and how does it differ from directly training a new policy from scratch?

## Architecture Onboarding

- Component map: Human demonstrations -> BeT pretraining -> Residual AIL fine-tuning -> Combined BeTAIL policy -> Gran Turismo Sport environment

- Critical path:
  1. Pretrain the BeT on offline human demonstrations.
  2. Freeze the BeT weights and train the residual AIL policy online.
  3. Use the combined BeTAIL policy (BeT + residual) to interact with the environment.

- Design tradeoffs:
  - BeT vs. traditional Markovian policies: BeT captures non-Markovian patterns but may be more computationally expensive.
  - Large vs. small α: A larger α allows more correction but may lead to less stable behavior; a smaller α ensures stability but may not correct for all errors.
  - Online vs. offline learning: Online learning allows adaptation but requires more environment interactions; offline learning is sample-efficient but may not adapt well to new environments.

- Failure signatures:
  - Poor performance on new tracks: The BeT may not have captured sufficient non-Markovian patterns, or the environment dynamics may differ significantly from those in the demonstrations.
  - Unstable behavior: The residual policy may be making too large corrections (α too large) or the AIL training may not be stable.
  - High sample complexity: The offline demonstrations may be of poor quality, or the environment dynamics may be too complex for the residual policy to correct effectively.

- First 3 experiments:
  1. Train BeTAIL on a single track with good quality demonstrations and evaluate its performance on the same track.
  2. Transfer the pretrained BeTAIL to a new track with different dynamics and evaluate its ability to adapt.
  3. Vary the size of the residual policy (α) and evaluate its impact on performance and stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the residual policy constraint α affect the trade-off between imitation accuracy and generalization to out-of-distribution states?
- Basis in paper: [explicit] The paper discusses the effect of α on steering behavior and performance, noting that smaller α ensures smoother steering and preserves non-Markovian behavior, while larger α allows more correction but can lead to oscillations.
- Why unresolved: The paper only tests a few values of α and does not provide a systematic analysis of the trade-off across a range of values or environments.
- What evidence would resolve it: A comprehensive study varying α across multiple tasks and measuring both imitation accuracy (e.g., similarity to expert demonstrations) and generalization (e.g., performance in unseen states or environments).

### Open Question 2
- Question: Can the BeTAIL framework be extended to handle multi-modal expert behavior, such as aggressive vs. defensive driving styles, without significantly increasing computational complexity?
- Basis in paper: [inferred] The paper mentions that the original BeT implementation used a mixture of Gaussians to model multi-modal behavior, but this was simplified to a unimodal approach for computational efficiency. This suggests potential for extension to multi-modal behavior.
- Why unresolved: The paper does not explore multi-modal behavior or compare the performance of the unimodal approach to a multi-modal one in terms of capturing diverse expert behaviors.
- What evidence would resolve it: An experiment comparing BeTAIL with unimodal and multi-modal BeT policies on datasets with distinct driving styles, measuring the ability to reproduce each style and the computational cost.

### Open Question 3
- Question: How does the performance of BeTAIL compare to other imitation learning methods that use sequence modeling, such as Online Decision Transformer, in terms of sample efficiency and final policy quality?
- Basis in paper: [explicit] The paper compares BeTAIL to AIL and BC with residual AIL, showing improved performance, but does not directly compare to other sequence modeling methods like Online Decision Transformer.
- Why unresolved: The paper only provides a limited comparison with Online Decision Transformer, focusing on training with dense rewards rather than pure imitation learning, and does not explore sample efficiency or final policy quality in depth.
- What evidence would resolve it: A direct comparison of BeTAIL with other sequence modeling-based imitation learning methods on the same tasks, measuring sample efficiency (e.g., environment steps to reach a certain performance) and final policy quality (e.g., lap times, success rates).

## Limitations

- Evaluation is limited to three racing tracks in Gran Turismo Sport, raising questions about generalization to more diverse environments.
- The human demonstrations' quality, diversity, and distribution are not fully characterized, which could impact the pretraining effectiveness.
- The exact implementation details of the Gran Turismo Sport API and the communication protocol with the game remain unspecified, potentially hindering faithful reproduction.
- The paper does not provide extensive ablations on the residual policy constraint parameter α, leaving uncertainty about its optimal value and sensitivity.
- While the paper claims improved sample efficiency, the absolute number of environment interactions required for training is not provided for direct comparison with baseline methods.

## Confidence

- **High confidence**: The BeTAIL architecture combining BeT and residual AIL policy is correctly described and the experimental setup is reproducible.
- **Medium confidence**: The claim that BeTAIL outperforms both pure BeT and pure AIL is supported by experimental results, but the magnitude of improvement may depend on specific hyperparameters and environment conditions.
- **Medium confidence**: The claim about BeTAIL's ability to generalize to new tracks and reduce environment interactions is plausible but requires further validation with more diverse environments and quantitative sample efficiency metrics.

## Next Checks

1. **Ablation study on α**: Systematically vary the residual policy constraint parameter α across a wider range and evaluate its impact on both performance (lap time) and stability (steering smoothness) to determine optimal values and sensitivity.

2. **Sample efficiency quantification**: Measure and report the absolute number of environment interactions required by BeTAIL compared to pure BeT and pure AIL baselines across all experimental conditions to provide concrete evidence of improved sample efficiency.

3. **Cross-environment generalization test**: Evaluate BeTAIL's performance when pretrained on demonstrations from one game/simulator (e.g., Gran Turismo Sport) and deployed in a different racing environment (e.g., another racing game or simulator) to assess the robustness of the learned non-Markovian patterns.