---
ver: rpa2
title: Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
arxiv_id: '2402.10634'
source_url: https://arxiv.org/abs/2402.10634
tags:
- data
- missing
- time
- graph
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HD-TTS, a hierarchical spatiotemporal downsampling
  approach for graph-based forecasting with missing data. It progressively coarsens
  time series over time and space, capturing heterogeneous dynamics at multiple scales.
---

# Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling

## Quick Facts
- **arXiv ID**: 2402.10634
- **Source URL**: https://arxiv.org/abs/2402.10634
- **Reference count**: 40
- **Primary result**: HD-TTS achieves MAE of 0.247 on GraphMSO dataset, outperforming state-of-the-art methods under block-spatiotemporal missing data

## Executive Summary
This paper introduces HD-TTS, a hierarchical spatiotemporal downsampling approach for graph-based forecasting with missing data. The method progressively coarsens time series over time and space, capturing heterogeneous dynamics at multiple scales. An attention mechanism dynamically combines these representations based on observations and missing data patterns to generate forecasts. HD-TTS outperforms state-of-the-art methods on synthetic and real-world benchmarks under various missing data distributions, particularly with contiguous blocks of missing values, while maintaining computational efficiency.

## Method Summary
HD-TTS processes time series data with inter-series relationships represented as graphs. It uses a time-then-space (TTS) hierarchical design where observations are first processed temporally across multiple scales through temporal downsampling, then spatially through graph pooling. The architecture creates L temporal layers and K spatial levels, generating L(K+1) representations that are combined by an attention mechanism conditioned on input observations and missing patterns. The model is trained end-to-end to minimize forecasting error using mean absolute error (MAE) as the evaluation metric.

## Key Results
- On GraphMSO dataset, HD-TTS achieves MAE of 0.247 compared to 0.251-0.293 for other methods in block-spatiotemporal missing data setting
- HD-TTS outperforms state-of-the-art methods across multiple benchmark datasets (AQI, EngRAD, PV-US) under various missing data distributions
- The approach demonstrates superior scalability and computational efficiency while maintaining high forecasting accuracy, particularly effective for contiguous blocks of missing values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-scale representations allow the model to adaptively focus on different temporal and spatial dynamics based on the missing data pattern.
- **Mechanism**: The hierarchical downsampling progressively coarsens time series over time and space, creating a pool of representations that capture heterogeneous temporal and spatial dynamics. An attention mechanism then dynamically combines these representations based on observations and missing data patterns to generate forecasts.
- **Core assumption**: Missing data patterns and input dynamics are informative signals for determining which spatiotemporal scale is most relevant for forecasting.
- **Evidence anchors**:
  - [abstract]: "The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts."
  - [section]: "To condition the importance of a representation to the prediction based on the input dynamics and the missing data pattern, we first compute an adaptive weight αi⟨k⟩ t⟨l⟩ ∈ [0, 1] for each representation as n αi⟨k⟩ t⟨l⟩ o = softmax n zi⟨k⟩ t⟨l⟩ Θα ok=0,...,K l=1,...,L"
- **Break condition**: If the missing data pattern is uncorrelated with the underlying dynamics, the attention mechanism may fail to select the most informative scale.

### Mechanism 2
- **Claim**: TTS architecture with integrated downsampling maintains computational efficiency while expanding receptive field.
- **Mechanism**: The time-then-space paradigm allows SMP operations on a single static graph regardless of sequence length, while downsampling progressively reduces computation by decimating time steps and nodes.
- **Core assumption**: The computational savings from TTS and downsampling outweigh any potential loss in modeling flexibility compared to time-and-space approaches.
- **Evidence anchors**:
  - [abstract]: "The proposed framework adopts a time-then-space (Gao & Ribeiro, 2022) hierarchical design, which efficiently handles representations at multiple scales by increasing the receptive field while limiting the number of parameters and the amount of computation."
  - [section]: "Temporal processing, for instance, depends on the dilation factor chosen rather than the number of layers, scaling as O (N W (d/(d − 1))) in contrast to O(N W L)."
- **Break condition**: If the temporal and spatial dynamics are highly coupled, the uncoupled processing in TTS may lose important information that T&S would preserve.

### Mechanism 3
- **Claim**: k-MIS pooling preserves graph topology while efficiently coarsening spatial representations.
- **Mechanism**: The k-MIS pooling method assigns each node to exactly one supernode in a way that preserves topological properties and provides distortion bounds on path lengths.
- **Core assumption**: The original graph topology adequately represents the underlying relationships between time series, and preserving this topology during pooling maintains important spatial dependencies.
- **Evidence anchors**:
  - [abstract]: "In space, we use graph pooling (Grat- tarola et al., 2024) to obtain a hierarchy of coarsened graphs that gradually distill the global information necessary for compensating localized gaps in the data."
  - [section]: "We rely on the k-MIS pooling method (presented in Sec. 2.2) to obtain spatial downsampling matrices Sk ∈ [0, 1]Nk×Nk91 that assign each node to exactly one supernode. Notably, k-MIS is expressive, i.e., if two graphs are distinguishable, then the pooled graphs remain distinguishable (Bianchi & Lachi, 2023)."
- **Break condition**: If the original graph is noisy or doesn't reflect true relationships (as noted in Appendix F), k-MIS pooling may create misleading coarsened representations.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs)**
  - Why needed here: The model processes time series with inter-series relationships represented as graphs, requiring GNN operators for spatial message passing.
  - Quick check question: Can you explain the difference between isotropic and anisotropic message passing in GNNs?

- **Concept: Temporal Convolutional Networks (TCNs)**
  - Why needed here: The model uses temporal downsampling similar to TCNs to capture multi-scale temporal dynamics while maintaining computational efficiency.
  - Quick check question: How does temporal downsampling in this architecture compare to dilated convolutions in TCNs?

- **Concept: Attention Mechanisms**
  - Why needed here: The decoder uses attention to dynamically combine multi-scale representations based on input observations and missing data patterns.
  - Quick check question: Why is attention particularly useful for combining representations at different spatiotemporal scales?

## Architecture Onboarding

- **Component map**: Input Encoder -> Temporal Processing -> Spatial Processing -> Attention -> Decoder -> Output
- **Critical path**: Input → Temporal Processing → Spatial Processing → Attention → Decoder → Output
  The most critical operations are the SMP steps which have high computational complexity, and the attention mechanism which determines the quality of forecasts.

- **Design tradeoffs**:
  - TTS vs T&S: TTS is more efficient but less flexible; chosen here for scalability with multi-scale processing
  - Trainable vs non-trainable pooling: Non-trainable (k-MIS) is more efficient but less adaptive; chosen here for computational efficiency
  - Fixed downsampling factors vs adaptive: Fixed factors simplify implementation; chosen here for predictable scaling

- **Failure signatures**:
  - Poor performance on contiguous missing blocks: Likely issues with spatial processing or attention mechanism
  - High computational cost: May need to reduce L or K, or use more efficient SMP operators
  - Unstable training: Check learning rate, gradient clipping, or batch normalization

- **First 3 experiments**:
  1. **Ablation study**: Remove hierarchical processing (use single scale) to quantify benefit of multi-scale representations
  2. **SMP operator comparison**: Test isotropic vs anisotropic SMP to determine which works better for your specific data
  3. **Downsampling configuration**: Vary L and K values to find optimal balance between performance and efficiency

These experiments should be run on a subset of your data first to quickly identify the most promising configurations before full-scale training.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The approach assumes missing data patterns are informative for determining relevant spatiotemporal scales, which may not hold for completely random missingness
- Performance depends heavily on having a meaningful graph structure that captures true relationships between time series
- Computational complexity claims need validation through comprehensive runtime benchmarks against baselines

## Confidence

- **Multi-scale attention effectiveness**: Medium - Strong empirical results but limited ablation studies on attention mechanism specifically
- **Computational efficiency claims**: Medium - Complexity analysis provided but no direct comparisons with T&S baselines in terms of wall-clock time
- **k-MIS pooling benefits**: Low - Claims about expressiveness and distortion bounds cited but no direct empirical validation of these properties

## Next Checks

1. **Random Missing Data Evaluation**: Test HD-TTS performance on datasets with completely random missing patterns (MCAR) to verify if the attention mechanism can still identify useful spatiotemporal scales when missingness is uncorrelated with underlying dynamics.

2. **Graph Quality Sensitivity Analysis**: Systematically evaluate how different graph construction methods (beyond pairwise distances) affect forecasting performance, particularly testing whether k-MIS pooling provides benefits when starting from noisy or uninformative graph structures.

3. **Runtime and Memory Profiling**: Conduct comprehensive benchmarks comparing wall-clock training time and memory usage of HD-TTS against both TTS and T&S baselines across different dataset sizes and L/K configurations to validate the claimed computational efficiency advantages.