---
ver: rpa2
title: A Large-Scale Sensitivity Analysis on Latent Embeddings and Dimensionality
  Reductions for Text Spatializations
arxiv_id: '2407.17876'
source_url: https://arxiv.org/abs/2407.17876
tags:
- stability
- data
- text
- concerning
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a large-scale sensitivity analysis on latent
  embeddings and dimensionality reductions for text spatializations. The authors evaluate
  the stability of text layouts concerning changes in input data, hyperparameters,
  and randomness.
---

# A Large-Scale Sensitivity Analysis on Latent Embeddings and Dimensionality Reductions for Text Spatializations

## Quick Facts
- **arXiv ID**: 2407.17876
- **Source URL**: https://arxiv.org/abs/2407.17876
- **Reference count**: 40
- **Primary result**: BERT, LDA, LSI, and NMF embeddings improve stability of text spatializations across input data changes, hyperparameter variations, and randomness

## Executive Summary
This paper presents a comprehensive sensitivity analysis of text embeddings and dimensionality reductions for text spatialization. The authors evaluate how different combinations of six text embeddings (VSM, LDA, LSI, NMF, BERT, Doc2Vec) and four dimensionality reductions (MDS, SOM, t-SNE, UMAP) affect the stability of text layouts under various perturbations. Using three text corpora, they quantify preservation of local and global structures and cluster separation between scatterplots using ten metrics. The study provides evidence-based guidelines for selecting embedding-DR combinations that maximize stability across different perturbation types.

## Method Summary
The authors conduct a systematic sensitivity analysis by generating text layouts through a pipeline of preprocessing → DTM creation → text embedding → dimensionality reduction → 2D layout generation. They compare layout pairs using ten similarity metrics that measure preservation of local/global structures and cluster separation. The analysis is performed across three text corpora with controlled perturbations including input data changes (jitter), hyperparameter variations, and randomness (different random seeds). Each combination of embedding and DR is evaluated for stability under these perturbations, with results aggregated across all corpus-layout pairs.

## Key Results
- BERT embeddings combined with t-SNE show highest stability for input data changes (jitter)
- Topic models (LDA, LSI, NMF) demonstrate superior stability for hyperparameter variations
- LDA embedding with t-SNE provides optimal stability against randomness
- VSM with tf-idf weighting generally underperforms other embeddings in stability metrics
- The stability metrics effectively capture preservation of local and global structures in layouts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT improves stability concerning input data changes by better reflecting jitter in layouts
- Mechanism: BERT embeddings capture richer semantic structure, so jitter-induced term frequency changes propagate more visibly into document similarity shifts, making t-SNE more responsive
- Core assumption: BERT embeddings preserve neighborhood relationships under small perturbations in term frequencies
- Evidence anchors:
  - [abstract]: "BERT for stability concerning input data" recommendation
  - [section 4.2]: BERT in combination with t-SNE shows highest scores for ˜α and ˜β when jitter is applied
  - [corpus]: Weak; only three corpora tested; unclear if BERT advantage holds for highly specialized domains
- Break condition: If the embedding space collapses jitter effects into negligible similarity changes, ˜α and ˜β will plateau

### Mechanism 2
- Claim: Topic models (LDA, LSI, NMF) improve stability concerning hyperparameters because they compress vocabulary into interpretable topics, reducing dimensionality distortion
- Mechanism: TMs map documents to a small topic space, so downstream DRs face fewer axes to optimize over, yielding more consistent layouts for small hyperparameter tweaks
- Core assumption: Topic representations are stable under small changes in DTM entries
- Evidence anchors:
  - [abstract]: "LDA, LSI, and NMF for stability concerning hyperparameters" recommendation
  - [section 4.3]: LDA, LSI, and NMF show higher stability scores (α, β, γ) than raw VSM when hyperparameters change
  - [corpus]: Weak; topic stability across corpora not explicitly measured
- Break condition: If TM hyperparameters (e.g., topic count K) are poorly chosen, topic interpretability and stability collapse

### Mechanism 3
- Claim: t-SNE in combination with LDA preserves local neighborhood structure across random seeds
- Mechanism: LDA yields coherent topic distributions; t-SNE's stochastic optimization on these compact, semantically coherent inputs yields consistent local neighborhoods
- Core assumption: Topic distributions from LDA are robust to random seed changes
- Evidence anchors:
  - [abstract]: "LDA for stability concerning randomness" recommendation
  - [section 4.4]: LDA+t-SNE achieves highest α scores in random-seed experiments
  - [corpus]: Weak; randomness effects not examined across diverse corpora
- Break condition: If LDA topic extraction is unstable (e.g., due to random initialization), t-SNE will inherit that instability

## Foundational Learning

- Concept: Document-Term Matrix (DTM) and sparsity
  - Why needed here: All embeddings and DRs start from DTM; understanding sparsity ratios (Table 1) explains why embeddings are needed
  - Quick check question: What is the sparsity ratio γ for the 20 Newsgroup corpus, and why does it matter for DR stability?

- Concept: Topic Model basics (LDA, LSI, NMF)
  - Why needed here: These embeddings compress vocabulary into topics; understanding their math explains why they improve stability
  - Quick check question: How does LDA represent a document in the topic space, and how is document similarity measured there?

- Concept: Dimensionality Reduction (t-SNE, UMAP, MDS, SOM)
  - Why needed here: DRs map high-dim embeddings to 2D; knowing their hyperparameters explains why some are more stable
  - Quick check question: What hyperparameter in t-SNE most affects layout stability, and why?

## Architecture Onboarding

- Component map: Raw text corpora → Preprocessing → DTM → Embedding (VSM/LDA/LSI/NMF/BERT/Doc2Vec) → DR (MDS/SOM/t-SNE/UMAP) → 2D Layout → Similarity Metrics → Aggregation → Stability Analysis
- Critical path: Embedding → DR → 2D layout generation; all downstream metrics depend on this pipeline working correctly
- Design tradeoffs: Using higher-dimensional embeddings (BERT/Doc2Vec) increases computational cost but may improve stability for input-data changes; using topic models reduces dimensionality but may sacrifice some local detail
- Failure signatures: Missing layouts (cluster errors), extremely low similarity scores (metric bugs), or identical layouts across seeds (randomness bugs)
- First 3 experiments:
  1. Run BERT embedding + t-SNE on 20 Newsgroup with jitter λ=0.25; verify ˜α > 0.8
  2. Run LDA embedding + t-SNE with two random seeds; check α > 0.9
  3. Run LSI (tf-idf) + UMAP with n_neighbors=10 and 20; verify β change < 0.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the stability metrics generalize to other types of text corpora beyond the three datasets studied (20 Newsgroups, Lyrics, Seven Categories)?
- Basis in paper: [inferred] The authors note that their findings rely on three specific text corpora and that it is unclear how their results would generalize to other datasets
- Why unresolved: The study is limited to three corpora, and while the authors provide supplemental material showing individual boxplot visualizations for each corpus, they acknowledge uncertainty about whether additional corpora would affect their results
- What evidence would resolve it: Conducting the same stability analysis on a broader and more diverse set of text corpora, including different languages, domains, and sizes, would provide evidence for the generalizability of the stability metrics

### Open Question 2
- Question: How does the choice of hyperparameters for the text embeddings (e.g., number of topics for LDA, dimensionality for Doc2Vec) affect the stability of the text layouts?
- Basis in paper: [explicit] The authors fixed the hyperparameters of the embedding algorithms following best practices but note that it is unclear if their guidelines would transfer to the case where more variants of each embedding are evaluated
- Why unresolved: The study focused on specific configurations for each embedding, and the impact of varying these hyperparameters on stability was not explored
- What evidence would resolve it: Systematically varying the hyperparameters of the text embeddings and evaluating their impact on the stability metrics would provide insights into how sensitive the text layouts are to these choices

### Open Question 3
- Question: How well do the stability metrics align with human perception of similarity in text layouts?
- Basis in paper: [inferred] The authors mention that their metrics capture local and global structures, as well as cluster separation, but they do not directly compare these metrics to human judgment
- Why unresolved: The study relies on quantitative metrics to assess stability, but it is unclear whether these metrics correspond to how humans perceive similarity in text layouts
- What evidence would resolve it: Conducting a user study where participants evaluate the similarity of text layouts based on their perception, and comparing their judgments to the quantitative metrics, would provide evidence for the alignment between the metrics and human perception

## Limitations
- Study based on only three text corpora, limiting generalizability across domains
- Sensitivity analysis focuses on predefined perturbations, potentially missing other stability-affecting factors
- Computational constraints limited exploration of additional embedding and dimensionality reduction combinations

## Confidence
- **High**: BERT improves stability for input data changes; topic models improve hyperparameter stability
- **Medium**: LDA + t-SNE provides best randomness stability; specific metric interpretations
- **Low**: Cross-corpus generalizability; domain-specific performance variations

## Next Checks
1. Test the recommended embedding-DR combinations on specialized corpora (e.g., biomedical or legal text) to verify cross-domain stability
2. Conduct ablation studies isolating the effects of individual hyperparameters on stability metrics
3. Evaluate computational efficiency trade-offs between recommended combinations for large-scale applications