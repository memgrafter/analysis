---
ver: rpa2
title: Exploring the traditional NMT model and Large Language Model for chat translation
arxiv_id: '2409.16331'
source_url: https://arxiv.org/abs/2409.16331
tags:
- translation
- data
- chat
- large
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper describes Huawei Translation Services Center\u2019\
  s submissions to the WMT24 chat translation shared task for English\u2194German.\
  \ The team explored traditional NMT models and large language models (LLMs) for\
  \ chat translation."
---

# Exploring the traditional NMT model and Large Language Model for chat translation

## Quick Facts
- arXiv ID: 2409.16331
- Source URL: https://arxiv.org/abs/2409.16331
- Reference count: 10
- Huawei Translation Services Center achieved top rankings in WMT24 chat translation shared task for English↔German using NMT models with MBR self-training

## Executive Summary
This paper presents Huawei's submissions to the WMT24 chat translation shared task, exploring both traditional NMT models and large language models (LLMs) for chat translation. The team employed a Deep Transformer architecture with R-Drop regularization as their NMT baseline, enhanced with Minimum Bayes Risk (MBR) decoding and self-training approaches. Their system achieved the best results in both English-to-German and German-to-English translation directions. While LLMs showed promise, particularly with few-shot prompting, they did not surpass the NMT model's performance in this task.

## Method Summary
The approach combines traditional NMT with advanced training strategies. The team used a Deep Transformer architecture (25-layer encoder, 6-layer decoder) with R-Drop regularization for their baseline model. They fine-tuned this model on chat data from previous WMT tasks and applied MBR decoding using 10 candidate models to select optimal translations. The MBR-selected outputs were then used as self-training data in an iterative process. For LLMs, they experimented with Llama2-8B using few-shot prompting and explored document-level translation, though these approaches did not match the NMT performance.

## Key Results
- Achieved best results in WMT24 chat translation shared task for both English→German and German→English directions
- MBR self-training method outperformed standard NMT approaches on validation data
- Traditional NMT models outperformed LLMs in this chat translation task despite promising preliminary LLM results
- System achieved top rankings in COMET-22, chrF, and BLEU metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR self-training improves translation quality by integrating multiple model outputs and performing knowledge distillation
- Mechanism: MBR selects the hypothesis with minimum expected error across multiple candidates, then uses these optimal outputs as training data for self-training, effectively distilling knowledge from the ensemble
- Core assumption: The MBR-selected translations represent higher quality than individual model outputs and can serve as effective training targets
- Evidence anchors:
  - [abstract]: "the MBR self-training method achieving the best results"
  - [section 3.4]: "We utilized the MBR selection results as self-training data, which led to the best results on the validation set"
  - [corpus]: Weak evidence - corpus shows MBR is used in translation but doesn't specifically confirm the self-training combination
- Break condition: MBR selection fails to identify truly better translations, or the self-training process overfits to synthetic data

### Mechanism 2
- Claim: Deep Transformer architecture with pre-layer normalization and increased encoder depth improves performance on chat translation
- Mechanism: The 25-layer encoder and 6-layer decoder configuration with pre-layer normalization provides better representation learning for the complex context and informal language patterns in chat data
- Core assumption: Chat translation benefits more from deeper encoders than standard configurations due to the need to capture conversational context
- Evidence anchors:
  - [section 3.1]: "Deep transformer is an improvement of Transformer, which increases the number of encoder layers and uses pre-layer-normalization to further improve model performance"
  - [abstract]: "The baseline models for WMT24 chat task use the Transformer-Big architecture"
  - [corpus]: Weak evidence - corpus mentions deep transformers but not specifically for chat translation
- Break condition: Additional depth provides diminishing returns or causes optimization difficulties that outweigh benefits

### Mechanism 3
- Claim: R-Drop regularization reduces training-inference inconsistency caused by dropout
- Mechanism: By minimizing bidirectional KL divergence between two sub-models' outputs for the same input during training, R-Drop ensures the model behaves consistently despite dropout's stochasticity
- Core assumption: Dropout-induced inconsistency between training and inference is a significant factor limiting translation quality
- Evidence anchors:
  - [section 3.3]: "R-Drop minimizes the bidirectional Kullback-Leibler (KL) divergence between the two distributions outputted by the two sub-models for the same data sample"
  - [abstract]: "Regularized Dropout (R-Drop) presents a simple yet more effective approach to regulate the training inconsistency caused by dropout"
  - [corpus]: Moderate evidence - corpus shows R-Drop is used in translation but doesn't confirm its specific impact on this task
- Break condition: The regularization term becomes too restrictive, preventing the model from learning necessary variations

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR provides a way to select the best translation from multiple hypotheses by considering translation quality metrics rather than just likelihood
  - Quick check question: How does MBR differ from beam search decoding, and why might it produce better results for chat translation?

- Concept: Self-training and knowledge distillation
  - Why needed here: Self-training allows the model to learn from its own high-quality outputs, effectively performing knowledge distillation to improve performance
  - Quick check question: What are the risks of self-training, and how does using MBR-selected outputs mitigate these risks?

- Concept: Context-aware vs sentence-level translation
  - Why needed here: Understanding when to use document-level context versus sentence-level translation is crucial for chat translation where conversation flow matters
  - Quick check question: Under what circumstances might sentence-level translation outperform context-aware approaches in chat scenarios?

## Architecture Onboarding

- Component map: Training data -> Deep Transformer model with R-Drop -> MBR decoding with 10 candidates -> Self-training with MBR outputs -> Model averaging -> Inference
- Critical path: Training → MBR decoding on validation → Self-training with MBR outputs → Final model averaging → Inference
- Design tradeoffs: Deep architecture provides better representation learning but increases training complexity; MBR adds computation but improves output quality; self-training provides data augmentation but risks error propagation
- Failure signatures: Performance degradation on validation set during self-training indicates overfitting to synthetic data; MBR not improving over beam search suggests candidate diversity is insufficient
- First 3 experiments:
  1. Train baseline Deep Transformer model with and without R-Drop to measure regularization impact
  2. Implement MBR decoding with 10 candidates and compare to beam search on validation set
  3. Run self-training using MBR outputs and measure improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of MBR decoding vary with different numbers of candidate models in chat translation tasks?
- Basis in paper: [explicit] The paper used 10 distinct models as candidates for MBR decoding and noted improvements in certain directions.
- Why unresolved: The paper does not explore how varying the number of candidate models affects MBR decoding performance in chat translation.
- What evidence would resolve it: Comparative experiments testing MBR decoding with different numbers of candidate models (e.g., 5, 10, 15) to determine the optimal number for chat translation tasks.

### Open Question 2
- Question: Can large language models be effectively fine-tuned for chat translation tasks using domain-specific chat data?
- Basis in paper: [explicit] The paper mentions that LLM performance was worse than sentence-level NMT and attributes this to domain shift, suggesting potential for improvement with chat-specific data.
- Why unresolved: The paper did not conduct fine-tuning of LLMs using chat task data due to time constraints.
- What evidence would resolve it: Experiments fine-tuning LLMs on domain-specific chat data and comparing performance with traditional NMT models in chat translation tasks.

### Open Question 3
- Question: What is the impact of context-aware translation data formats on the performance of large language models in chat translation?
- Basis in paper: [explicit] The paper tested stream translation and context-aware translation formats, finding that context-aware format yielded worse results than stream translation.
- Why unresolved: The paper does not explore why context-aware formats underperformed or how to optimize them for better results.
- What evidence would resolve it: Detailed analysis and experiments modifying context-aware data formats to improve LLM performance in chat translation tasks.

## Limitations
- LLM comparison was preliminary and lacked direct comparison under identical conditions
- Self-training effectiveness not validated on held-out test sets or across multiple chat domains
- No comprehensive ablation studies to isolate contribution of individual components

## Confidence

- **High Confidence**: The effectiveness of the deep Transformer architecture with R-Drop regularization - this follows established literature and the improvements are consistent with known benefits of these techniques.

- **Medium Confidence**: The MBR self-training approach achieving "best results" - while validated on the validation set, the lack of extensive testing and comparison makes this claim less certain.

- **Low Confidence**: The superiority of NMT over LLMs for this task - the LLM experiments were described as preliminary with limited configuration exploration, making this conclusion premature.

## Next Checks
1. Conduct ablation studies by training models with: (a) baseline Deep Transformer, (b) Deep Transformer + R-Drop, (c) Deep Transformer + MBR decoding, (d) full pipeline with self-training, to quantify each component's contribution.

2. Design a more comprehensive LLM evaluation protocol including different model sizes, fine-tuning strategies, and context-aware configurations to properly compare with the NMT approach.

3. Test the self-training approach on held-out test sets and across multiple chat domains to evaluate generalization and identify potential overfitting to the training data.