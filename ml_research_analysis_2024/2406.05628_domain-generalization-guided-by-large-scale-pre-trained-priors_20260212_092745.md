---
ver: rpa2
title: Domain Generalization Guided by Large-Scale Pre-Trained Priors
arxiv_id: '2406.05628'
source_url: https://arxiv.org/abs/2406.05628
tags:
- pre-trained
- domain
- generalization
- should
- ft-lp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain generalization (DG),
  where models trained on limited source domains must generalize to unseen target
  domains. The key insight is that large-scale pre-trained models already possess
  domain shift resistance, but this capability is often underutilized during fine-tuning.
---

# Domain Generalization Guided by Large-Scale Pre-Trained Priors

## Quick Facts
- arXiv ID: 2406.05628
- Source URL: https://arxiv.org/abs/2406.05628
- Reference count: 40
- Key outcome: Proposes FT-LP method that incorporates pre-trained models as priors to improve domain generalization across 5 benchmarks with 11 algorithms

## Executive Summary
This paper addresses domain generalization (DG) by leveraging large-scale pre-trained models as priors during fine-tuning. The authors propose Fine-Tune with Large-scale pre-trained Priors (FT-LP), which continuously references pre-trained models at each optimization step to preserve domain shift resistance. Theoretically, they introduce a new generalization error bound showing that incorporating pre-trained priors reduces prediction error on target domains. Extensive experiments on five benchmarks demonstrate consistent accuracy improvements when combining FT-LP with existing DG methods.

## Method Summary
The FT-LP method fine-tunes DG models while incorporating pre-trained models as priors through KL divergence regularization. It uses Maximum A Posteriori Estimation with diagonal covariance approximation for tractable implementation. The approach adds a KL divergence term between current model parameters and pre-trained prior to the loss function during training, with an encoder simulating prior distributions when only pre-trained weights are available. This continuous reference to pre-trained knowledge helps preserve domain-invariant features while adapting to source domains.

## Key Results
- FT-LP achieves consistent accuracy gains across all five benchmark datasets (PACS, VLCS, Office, TerraIncognita, DomainNet)
- Significant improvements on challenging domains like TerraIncognita and DomainNet
- Helps models focus on more generalizable features rather than spurious correlations
- Outperforms 11 existing DG algorithms when combined with them

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous reference to pre-trained models during fine-tuning preserves domain shift resistance.
- **Mechanism:** Incorporating KL divergence between current model parameters and pre-trained prior at every optimization step keeps the model closer to the distribution learned from diverse large-scale data, preventing overfitting to source domain specifics.
- **Core assumption:** The pre-trained model's parameter distribution encodes domain-invariant features beneficial for generalization.
- **Evidence anchors:**
  - [abstract] "large-scale pre-trained models already possess the ability to resist domain shift. If we reference pre-trained models continuously during fine-tuning to maintain this ability, it could further enhance the generalization ability of the DG model."
  - [section] "We propose imposing constraints on target and source losses to establish a new upper bound for DG problems while also integrating large-scale pre-trained models as a prior to obtain a tighter bound."
  - [corpus] Weak. Only 1 of 8 corpus neighbors explicitly mentions "domain generalization" in the title, indicating limited prior work on this exact mechanism.
- **Break condition:** If the pre-trained model's distribution is too far from the target domain or the KL penalty is too strong, it may hinder adaptation to source-specific patterns needed for the task.

### Mechanism 2
- **Claim:** Using pre-trained models as priors in PAC-Bayes bounds tightens generalization error estimates.
- **Mechanism:** Replacing standard priors with data-dependent priors derived from pre-trained models reduces KL divergence in the PAC-Bayes bound, leading to tighter generalization guarantees.
- **Core assumption:** Large-scale pre-trained models provide more informative priors than simple distributions like standard Gaussian.
- **Evidence anchors:**
  - [section] "The data-dependent prior suggests that utilizing available data can help identify priors that reduce the KL term, thus obtaining a tighter bound... Since DG methods nowadays rely on publicly available pre-trained models [49, 50], we employ a similar method to the data-dependent prior, using ED0(Q(D0)) to approximate the ideal prior."
  - [section] "Theorem 3. ∀β, σ ∈ (0, 1), n ∈ N, P ∈ M1(H), Sm ⊂ S. With probability at least 1 − σ over Sm, for all Q(D0) ∈ M1(H), we have: Eh∼P RT (h) ≤ 1/β Eh∼P RSm(h) + ED0 KL(P ||Q(D0)) + ln 1/σ |Sm| · 2β(1 − β) + dist(S, T, P ) + λp."
  - [corpus] Weak. None of the corpus neighbors discuss PAC-Bayes bounds or generalization error estimation, indicating this is a novel theoretical contribution.
- **Break condition:** If the pre-trained model's prior is misaligned with the source-target domain relationship, the bound may not improve or could even be looser.

### Mechanism 3
- **Claim:** MAP estimation with diagonal covariance approximation enables tractable implementation of FT-LP.
- **Mechanism:** By restricting the prior and posterior to Gaussian distributions with diagonal covariance, the KL divergence term becomes computationally feasible, allowing integration into standard optimization pipelines.
- **Core assumption:** The independence assumption between layer weights (diagonal covariance) is a reasonable approximation for the parameter distribution.
- **Evidence anchors:**
  - [section] "In light of this, we consider utilizing the Maximum A Posteriori Estimation to optimize FT-LP... We reformulate the optimization objective as arg min h∼H E(x,y)∼Sm LDG(h(x), y) + γ(||µ(h) − µ(h0)||2 Σ−1 h0 ) + tr(Σ−1 h0 Σh))."
  - [section] "Therefore, we simply set the mean µ as the model weights: µ(h) = h, µ(h0) = h0, and use linear encoders G encoding h, h0 to get the covariance, with the optimization objective being the constraint terms in (15)."
  - [corpus] Weak. None of the corpus neighbors discuss MAP estimation or diagonal covariance approximations in the context of domain generalization.
- **Break condition:** If the true parameter distribution has significant correlations between weights, the diagonal approximation may lose important information, reducing the effectiveness of the prior.

## Foundational Learning

- **Concept:** Domain Generalization (DG)
  - Why needed here: The paper addresses DG, where models must generalize to unseen target domains from limited source domains. Understanding DG is crucial to grasp the problem FT-LP solves.
  - Quick check question: What is the key challenge in domain generalization that differentiates it from standard supervised learning?

- **Concept:** PAC-Bayes Theory
  - Why needed here: FT-LP's theoretical foundation relies on PAC-Bayes bounds to prove that incorporating pre-trained priors reduces generalization error. Understanding PAC-Bayes is essential to follow the theoretical claims.
  - Quick check question: How does the KL divergence term in PAC-Bayes bounds relate to the distance between prior and posterior distributions?

- **Concept:** Maximum A Posteriori (MAP) Estimation
  - Why needed here: The paper uses MAP estimation as a tractable alternative to full Bayesian optimization for implementing FT-LP. Understanding MAP is necessary to follow the implementation strategy.
  - Quick check question: What is the key difference between MAP estimation and full Bayesian inference in terms of computational complexity?

## Architecture Onboarding

- **Component map:** Pre-trained model (h0) -> Current model (h) -> Encoder G -> Loss function LDG + KL divergence term
- **Critical path:**
  1. Initialize h with h0
  2. For each training batch:
     - Compute LDG loss on batch
     - Compute KL divergence between h and h0 using encoder G
     - Combine losses with hyperparameter γ
     - Update h and G using optimizer
  3. Return fine-tuned h
- **Design tradeoffs:**
  - KL penalty strength (γ): Higher values preserve more pre-trained knowledge but may hinder task-specific adaptation
  - Encoder complexity: More complex encoders may better capture parameter correlations but increase computational cost
  - Covariance approximation: Diagonal assumption reduces computation but may lose information about parameter dependencies
- **Failure signatures:**
  - Model performs well on source domains but poorly on target domains: KL penalty may be too strong, preventing necessary adaptation
  - Model performance degrades compared to baseline: KL penalty may be misaligned with the specific DG task or dataset characteristics
  - Training becomes unstable or slow: Encoder or KL computation may be introducing numerical issues or excessive computation
- **First 3 experiments:**
  1. Verify FT-LP improves ERM on PACS dataset with standard DG algorithms
  2. Test FT-LP with different γ values to find optimal regularization strength
  3. Compare FT-LP with and without the encoder G to assess its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of covariance encoder architecture affect the performance of FT-LP?
- Basis in paper: [explicit] The paper mentions that the covariance encoder G is a basic linear layer, but also notes that a complex encoder could quickly escalate the space and computational resources required by the model.
- Why unresolved: The paper does not provide a detailed comparison of different encoder architectures or their impact on performance.
- What evidence would resolve it: Experimental results comparing the performance of FT-LP with different encoder architectures, such as linear layers, convolutional layers, or more complex neural networks, would provide insights into the optimal encoder design.

### Open Question 2
- Question: Can FT-LP be effectively combined with other domain generalization methods that rely on Bayesian optimization?
- Basis in paper: [inferred] The paper mentions that Bayesian optimization has advantages but may compromise the ease of combinability inherent in FT-LP. It also notes that obtaining a well-trained Bayesian model is challenging.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on combining FT-LP with Bayesian optimization methods.
- What evidence would resolve it: Experimental results comparing the performance of FT-LP combined with Bayesian optimization methods to other combinations, such as FT-LP with MAP estimation, would provide insights into the compatibility and effectiveness of different approaches.

### Open Question 3
- Question: How does the performance of FT-LP vary across different types of domain shifts and dataset characteristics?
- Basis in paper: [inferred] The paper mentions that the choice of pre-trained model based on the characteristics of the existing source domain proves to be more advantageous. However, it does not provide a detailed analysis of how FT-LP performs across different domain shifts and dataset characteristics.
- Why unresolved: The paper does not provide a comprehensive analysis of the factors that influence the performance of FT-LP, such as the nature of the domain shift, the size and diversity of the source domains, or the complexity of the target domain.
- What evidence would resolve it: Experimental results evaluating the performance of FT-LP across a wide range of domain shifts and dataset characteristics, along with a detailed analysis of the factors that influence its effectiveness, would provide insights into the generalizability and limitations of the approach.

## Limitations

- The diagonal covariance approximation may not capture important parameter correlations, potentially limiting the effectiveness of the prior
- The optimal KL penalty strength (γ) likely varies significantly across datasets and tasks, requiring dataset-specific tuning
- The theoretical claims rely heavily on PAC-Bayes bounds, but the practical impact of these bounds on real-world performance remains unclear without extensive empirical validation

## Confidence

- High confidence in the mechanism of continuous pre-trained model reference preserving domain shift resistance
- Medium confidence in the theoretical generalization error bound claims, pending further empirical validation
- Medium confidence in the MAP estimation implementation strategy, given the computational tradeoffs involved

## Next Checks

1. Conduct ablation studies comparing FT-LP with different KL penalty strengths (γ) across all five datasets to establish optimal regularization levels
2. Compare FT-LP's performance against baselines when using different pre-trained models (e.g., ResNet-18, ResNet-50) to assess model dependency
3. Evaluate the impact of the diagonal covariance approximation by implementing a full covariance version of FT-LP on a smaller dataset to measure the tradeoff between accuracy and computational cost