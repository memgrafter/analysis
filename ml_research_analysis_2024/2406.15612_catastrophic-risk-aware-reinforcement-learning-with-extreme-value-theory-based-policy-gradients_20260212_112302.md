---
ver: rpa2
title: Catastrophic-risk-aware reinforcement learning with extreme-value-theory-based
  policy gradients
arxiv_id: '2406.15612'
source_url: https://arxiv.org/abs/2406.15612
tags:
- risk
- policy
- distribution
- such
- potpg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mitigating catastrophic risk
  in sequential decision making, where outcomes are very rare but have extreme magnitude.
  The main contribution is the POTPG (peaks-over-threshold policy gradient) algorithm,
  which integrates extreme value theory (EVT) estimates of tail risk into policy gradient
  methods for reinforcement learning.
---

# Catastrophic-risk-aware reinforcement learning with extreme-value-theory-based policy gradients

## Quick Facts
- arXiv ID: 2406.15612
- Source URL: https://arxiv.org/abs/2406.15612
- Authors: Parisa Davar; Frédéric Godin; Jose Garrido
- Reference count: 6
- Primary result: POTPG outperforms standard sample averaging in mitigating catastrophic risk, with advantage increasing as tail thickness grows

## Executive Summary
This paper addresses the challenge of mitigating catastrophic risk in reinforcement learning where outcomes are extremely rare but have severe consequences. The authors introduce POTPG (Peaks-Over-Threshold Policy Gradient), which integrates extreme value theory (EVT) into policy gradient methods to estimate tail risk more accurately when observations are scarce. By using the peaks-over-threshold approach from EVT to extrapolate tail behavior, POTPG provides a principled way to estimate Conditional Value-at-Risk (CVaR) for high confidence levels (α ≈ 0.999), making it particularly effective in catastrophic risk scenarios where conventional empirical methods fail due to insufficient tail samples.

## Method Summary
POTPG is a reinforcement learning algorithm that minimizes CVaR of cumulative discounted costs by incorporating EVT-based tail estimation into policy gradient optimization. The method uses automated threshold selection via Anderson-Darling tests to fit a Generalized Pareto Distribution (GPD) to cost excesses above a chosen threshold. For gradient estimation, POTPG employs forward finite differences with the same threshold used across all perturbed policy evaluations to enhance stability. When EVT fitting fails, the algorithm falls back to sample averaging. The approach is demonstrated in both controlled simulations with known GPD cost distributions and a financial hedging application with NIG-distributed stock prices.

## Key Results
- POTPG outperforms sample averaging (SA) in RMSE of CVaR estimation across all tested tail parameters (ξ = 0.4, 0.6, 0.8) in controlled simulations
- The performance gap between POTPG and SA increases as the tail thickness (ξ) increases, demonstrating POTPG's advantage in heavier-tailed distributions
- In financial hedging experiments, POTPG achieves lower RMSE in policy parameter estimates compared to both n=1,000 and n=10,000 sample averaging baselines
- POTPG successfully mitigates catastrophic risk in sequential decision making where conventional methods struggle due to tail sample scarcity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: POTPG reduces gradient estimation variance in tail regions by fitting a GPD to excesses rather than relying on sparse empirical tail data
- Mechanism: When the cumulative discounted cost distribution has a heavy tail, only a few samples exceed the high CVaR threshold α. POTPG replaces the empirical quantile estimate with a GPD fit to all observations above a data-driven threshold u, increasing the effective sample size used to approximate tail behavior
- Core assumption: The tail of the cost distribution belongs to the maximum domain of attraction of a GEV distribution (F ∈ M DA(Hξ)), which justifies the GPD approximation
- Evidence anchors: [abstract] "POTPG uses the peaks-over-threshold approach from EVT to extrapolate tail behavior, making it particularly effective when observations in the tail are scarce"; [section 3.1] Corollary 3.1 showing EVT-based CVaR approximation; [corpus] Weak - no direct experimental comparison with GPD vs empirical tail sampling

### Mechanism 2
- Claim: The automated threshold selection via Anderson-Darling tests balances bias and variance in tail estimation
- Mechanism: POTPG tests a sequence of candidate thresholds u(i) and selects the smallest that yields a statistically acceptable GPD fit (p-value ≥ γ), avoiding thresholds too high (high variance) or too low (high bias)
- Core assumption: A proper sequence of thresholds u(i) and a reasonable significance level γ exist such that at least one threshold passes the goodness-of-fit test
- Evidence anchors: [section 3.1] "Such a procedure tests for a set of candidate values u(1) < . . . < u(ℓ), and the smallest among these is selected as the threshold"; [appendix A] Describes the threshold selection algorithm; [corpus] Weak - no quantitative evaluation of threshold selection stability across runs

### Mechanism 3
- Claim: Using the same threshold u across forward finite difference evaluations stabilizes gradient estimates
- Mechanism: In policy gradient estimation, the same threshold u is used to compute bJ(θ(j)) and all bJ(θ(j)+ϵ1i), ensuring that the CVaR estimate for the original policy and its perturbations are based on comparable tail extrapolations, reducing gradient noise
- Core assumption: A single threshold u is appropriate for both the base and perturbed policies, i.e., their cost distributions have similar tail behavior
- Evidence anchors: [section 3.2] "Note also that we propose to use the same thresholdu to estimatebJ(θ(j)) and allbJ(θ(j) + ϵ1i)"; [section 4] "The method of moments is used to estimate tail parametersξ, σ in the POTPG algorithms since such method exhibited (in unreported tests) greater stability than maximum likelihood estimates"; [corpus] Weak - stability claims based on unreported tests

## Foundational Learning

- Concept: Extreme Value Theory (EVT) and the peaks-over-threshold (POT) method
  - Why needed here: EVT provides a principled way to extrapolate tail risk when empirical tail samples are too few to reliably estimate CVaR for α ≈ 1
  - Quick check question: What is the role of the shape parameter ξ in a GPD, and how does it affect the existence of CVaR?

- Concept: Conditional Value-at-Risk (CVaR) as a tail risk measure
  - Why needed here: CVaR at high confidence levels (α ≈ 0.999) is the target objective in catastrophic risk minimization, and its accurate estimation is critical for policy optimization
  - Quick check question: How is CVaRα defined for a continuous random variable X in terms of its quantile qα?

- Concept: Policy gradient methods and finite difference gradient estimation
  - Why needed here: POTPG is built on policy gradient optimization; the algorithm uses forward finite differences to estimate the gradient of the CVaR objective with respect to policy parameters
  - Quick check question: In the finite difference scheme, why is it important to isolate the effect of policy perturbations from environment randomness?

## Architecture Onboarding

- Component map: MDP environment -> Monte Carlo rollouts -> EVT tail estimation module -> CVaR objective estimator -> Finite difference gradient estimation -> ADAM optimizer

- Critical path: 1. Sample n episodes under current policy πθ(j) 2. Extract cumulative discounted costs 3. Run automated threshold selection → u 4. Fit GPD to excesses above u → (ξ̂, σ̂) 5. Compute CVaR estimate via (3.6) 6. Repeat for each perturbed policy πθ(j)+ϵ1i 7. Form gradient estimate via finite differences 8. Update θ(j) using ADAM

- Design tradeoffs: Using a single threshold u across perturbations improves gradient stability but may bias estimates if policy changes tail shape significantly; MOM vs. MLE for GPD parameters offers stability vs. efficiency tradeoff; threshold selection aggressiveness trades off robustness against overfitting to noise

- Failure signatures: Threshold selection fails (I = ∅) → POTPG falls back to sample averaging → loss of tail extrapolation advantage; Estimated ξ̂ close to or above 1 → CVaR estimate undefined → POTPG must enforce ξmax or fallback; GPD fit poor despite passing AD test → CVaR estimate unreliable → gradient noise increases

- First 3 experiments: 1. Replicate controlled simulation with known GPD cost distribution (ξ = 0.6) comparing POTPG vs. SA RMSE curves 2. Apply POTPG to financial hedging problem with NIG-distributed stock prices, recording θ(j) and RMSEθ over 500 iterations 3. Run threshold selection stability test: repeat 50 runs, record frequency of fallback to sample averaging and distribution of chosen thresholds u

## Open Questions the Paper Calls Out
- How can the POTPG algorithm be extended to handle high-dimensional policy spaces, such as those represented by deep neural networks? The paper acknowledges that POTPG relies on finite difference approximations and works for low-dimensional policies, suggesting development of a high-dimensional EVT-based policy gradient framework as a potential extension.

## Limitations
- POTPG's reliance on finite difference approximations limits its applicability to low-dimensional policy spaces, making it unsuitable for complex problems requiring deep neural network policies
- The automated threshold selection procedure may frequently fail when the true tail shape parameter ξ approaches the GPD boundary of 1, forcing fallback to sample averaging
- Stability claims regarding MOM vs. MLE parameter estimation are based on unreported tests, creating uncertainty about the method's robustness in practice

## Confidence

- High: EVT-based CVaR estimation via GPD is theoretically justified when the cost distribution belongs to M DA(Hξ)
- Medium: Automated threshold selection via Anderson-Darling tests effectively balances bias-variance tradeoff
- Medium: Using a fixed threshold across policy perturbations stabilizes gradient estimates

## Next Checks

1. Test POTPG's performance when the true cost distribution has ξ > ξmax (0.9), forcing frequent fallback to sample averaging
2. Compare MOM vs. MLE GPD parameter estimation directly in the financial hedging experiment to validate stability claims
3. Evaluate gradient estimation variance when using policy-specific thresholds versus the fixed threshold approach