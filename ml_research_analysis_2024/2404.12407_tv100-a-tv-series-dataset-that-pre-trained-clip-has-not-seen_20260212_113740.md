---
ver: rpa2
title: 'TV100: A TV Series Dataset that Pre-Trained CLIP Has Not Seen'
arxiv_id: '2404.12407'
source_url: https://arxiv.org/abs/2404.12407
tags:
- uni00000013
- clip
- learning
- dataset
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TV100, a novel dataset of TV series images
  released after 2021, specifically designed to evaluate whether pre-trained models
  like CLIP possess comprehensive knowledge. The dataset was constructed by collecting
  images from Google based on TV series names from IMDB, then filtering out classes
  that CLIP could already recognize using zero-shot accuracy ranking.
---

# TV100: A TV Series Dataset that Pre-Trained CLIP Has Not Seen

## Quick Facts
- arXiv ID: 2404.12407
- Source URL: https://arxiv.org/abs/2404.12407
- Authors: Da-Wei Zhou; Zhi-Hong Qi; Han-Jia Ye; De-Chuan Zhan
- Reference count: 16
- Primary result: A novel dataset TV100 containing 100 classes of post-2021 TV series images that CLIP cannot recognize in zero-shot setting but learns well with fine-tuning

## Executive Summary
This paper introduces TV100, a dataset specifically designed to evaluate whether pre-trained models like CLIP possess comprehensive knowledge by testing on novel classes. The dataset contains 100 classes of TV series images released after 2021, collected from Google Images based on TV series names from IMDB. The key innovation is filtering out classes that CLIP can already recognize using zero-shot accuracy ranking, ensuring all classes are truly novel to the model. Empirical evaluation demonstrates that pre-trained CLIP achieves zero accuracy on this dataset, but performance drastically improves after fine-tuning, validating the dataset's learnability and usefulness for research in incremental learning, novel class discovery, and long-tailed recognition.

## Method Summary
The TV100 dataset was constructed by first collecting TV series names released after 2021 from IMDB, then downloading corresponding images from Google Images. The dataset was filtered by running CLIP's zero-shot classification on all collected classes and selecting the top-100 classes with the lowest accuracy scores. This ensures that CLIP has no prior knowledge of these classes. The dataset was then evaluated in two settings: zero-shot classification (where CLIP achieved 0% accuracy) and fine-tuning (where performance drastically improved). The dataset also exhibits a naturally imbalanced class distribution due to varying popularity of TV series, making it suitable for long-tailed recognition research.

## Key Results
- Pre-trained CLIP achieves 0% zero-shot accuracy on TV100 dataset
- Fine-tuned CLIP shows drastically improved performance after adaptation
- TV100 contains 100 classes with highly imbalanced distribution across multiple countries
- Dataset is publicly available at https://tv-100.github.io/

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TV100 is a deliberately constructed out-of-distribution dataset that CLIP's pre-training has not encountered
- Mechanism: Dataset construction pipeline gathers post-2021 TV series names from IMDB, downloads corresponding images from Google, and filters out classes CLIP can recognize using zero-shot accuracy ranking, selecting top-100 hardest classes
- Core assumption: CLIP's knowledge is bounded by its training corpus (LAION dataset), and recent TV series imagery is not included
- Evidence anchors: Abstract states dataset is for "evaluating whether pre-trained models like CLIP possess comprehensive knowledge" and "choose the top-100 hard classes based on the zero-shot accuracy"

### Mechanism 2
- Claim: The dataset's high class imbalance makes it suitable for long-tailed recognition research
- Mechanism: Data collection naturally reflects real-world popularity distributions, leading to long-tailed class distribution
- Core assumption: TV series popularity in Google image search correlates with class frequency in dataset
- Evidence anchors: Figure 1(c) implies "class distribution is highly imbalanced, which is naturally suitable for scientific research on long-tailed recognition"

### Mechanism 3
- Claim: The dataset's learnability after fine-tuning demonstrates it is separable in feature space
- Mechanism: Zero-shot performance is zero, but after fine-tuning, performance drastically improves, indicating classes are learnable and distinguishable
- Core assumption: CLIP's feature space can be adapted to recognize novel classes not seen during pre-training
- Evidence anchors: "By contrast, if we finetune the CLIP model with the images, the performance drastically improves, indicating that the dataset is learnable and separable"

## Foundational Learning

- **Zero-shot learning**: Understanding how CLIP performs classification without task-specific training is central to the dataset's purpose. *Quick check: What is the difference between zero-shot and few-shot learning, and how does CLIP achieve zero-shot performance?*

- **Out-of-distribution detection**: The dataset tests CLIP's ability to detect novel classes, a key aspect of OOD detection. *Quick check: How can a model detect that an input belongs to a class it has not seen during training?*

- **Long-tailed recognition**: The dataset's class imbalance makes it suitable for studying algorithms that handle skewed class distributions. *Quick check: What are the main challenges in training models on long-tailed datasets, and how do common techniques (e.g., re-weighting, re-sampling) address them?*

## Architecture Onboarding

- **Component map**: IMDB scraping → Google image search → manual filtering → CLIP zero-shot accuracy ranking → select top-100 hardest classes → dataset construction → zero-shot test → fine-tuning test

- **Critical path**: IMDB → Google images → CLIP filtering → dataset construction → zero-shot test → fine-tuning test

- **Design tradeoffs**: Manual filtering ensures quality but limits scalability; using CLIP for filtering assumes CLIP's zero-shot accuracy is a reliable proxy for novelty; natural imbalance is beneficial for research but may limit generalization

- **Failure signatures**: Zero-shot accuracy not truly zero means dataset may not be novel enough; fine-tuning does not improve performance suggests dataset may be too noisy or ambiguous; class distribution not long-tailed means unsuitable for long-tailed recognition research

- **First 3 experiments**:
  1. Run CLIP zero-shot classification on full unfiltered dataset to confirm some classes are already known
  2. Fine-tune CLIP on TV100 and evaluate performance on held-out test set
  3. Analyze class distribution (e.g., number of samples per class) to confirm long-tailed property

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the maximum zero-shot accuracy a pre-trained CLIP model could achieve on TV100, and what specific image features or characteristics prevent it from achieving this?
- **Basis in paper**: [explicit] Paper shows zero-shot accuracy is 0% but fine-tuning drastically improves performance
- **Why unresolved**: Paper only tested one pre-trained CLIP model and didn't systematically analyze which image features cause zero-shot failure
- **What evidence would resolve it**: Comprehensive analysis comparing CLIP's internal representations with TV100 images, identifying specific visual patterns that CLIP cannot recognize

### Open Question 2
- **Question**: How does TV100's long-tailed distribution affect model performance compared to balanced distributions, and what specific long-tail strategies work best on this dataset?
- **Basis in paper**: [explicit] Dataset has highly imbalanced class distribution naturally suitable for long-tailed recognition research
- **Why unresolved**: Paper only demonstrates dataset's learnability but doesn't evaluate different long-tail learning strategies or analyze how imbalance specifically impacts performance
- **What evidence would resolve it**: Systematic comparison of various long-tail learning methods (class-balanced sampling, re-weighting, meta-learning approaches) on TV100 with performance analysis

### Open Question 3
- **Question**: What is the minimum amount of fine-tuning data required for CLIP to achieve reasonable performance on TV100, and how does this scale with class diversity?
- **Basis in paper**: [inferred] Paper shows fine-tuning drastically improves performance from 0% to reasonable accuracy, but doesn't explore data efficiency
- **Why unresolved**: Paper demonstrates learnability but doesn't investigate sample efficiency or relationship between fine-tuning data size and performance across diverse classes
- **What evidence would resolve it**: Experiments varying fine-tuning dataset sizes per class while measuring performance, identifying minimum data requirements for effective adaptation

## Limitations
- Lack of transparency in dataset construction process and specific filtering criteria
- No information about train/validation/test splits and fine-tuning hyperparameters
- Reliance on CLIP's zero-shot accuracy as proxy for novelty assumes CLIP's knowledge is complete
- Manual filtering limits scalability of dataset construction

## Confidence
- **High Confidence**: Claim that TV100 contains classes novel to CLIP is supported by empirical evidence of zero zero-shot accuracy and significant improvement after fine-tuning
- **Medium Confidence**: Assertion that dataset is naturally long-tailed due to real-world TV series popularity is plausible but not directly verified in paper
- **Low Confidence**: Claim that TV100 is suitable for research in incremental learning and novel class discovery is based on assumption that dataset's properties are sufficient for these tasks without direct evidence

## Next Checks
1. **Verify Novelty**: Run CLIP zero-shot classification on full unfiltered dataset to confirm that some classes are already known to CLIP, validating the filtering process
2. **Analyze Class Distribution**: Examine the number of samples per class in TV100 to confirm the long-tailed property and assess its severity
3. **Reproduce Fine-tuning Results**: Fine-tune CLIP on TV100 using the same procedure (or a reasonable approximation) and evaluate performance on a held-out test set to confirm the learnability of the dataset