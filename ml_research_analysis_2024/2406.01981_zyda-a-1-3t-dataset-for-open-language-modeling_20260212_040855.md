---
ver: rpa2
title: 'Zyda: A 1.3T Dataset for Open Language Modeling'
arxiv_id: '2406.01981'
source_url: https://arxiv.org/abs/2406.01981
tags:
- dataset
- datasets
- zyda
- documents
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zyda is a 1.3 trillion token open dataset for large language model
  pretraining, created by combining and processing several major permissively licensed
  datasets. The dataset undergoes rigorous filtering to remove low-quality and objectionable
  content, and extensive deduplication both within and across datasets using locality-sensitive
  hashing with MinHash signatures.
---

# Zyda: A 1.3T Dataset for Open Language Modeling

## Quick Facts
- arXiv ID: 2406.01981
- Source URL: https://arxiv.org/abs/2406.01981
- Authors: Yury Tokpanov; Beren Millidge; Paolo Glorioso; Jonathan Pilault; Adam Ibrahim; James Whittington; Quentin Anthony
- Reference count: 17
- Primary result: 1.3 trillion token open dataset for LLM pretraining with superior performance to alternatives

## Executive Summary
Zyda is a 1.3 trillion token open dataset for large language model pretraining, created by combining and processing several major permissively licensed datasets. The dataset undergoes rigorous filtering to remove low-quality and objectionable content, and extensive deduplication both within and across datasets using locality-sensitive hashing with MinHash signatures. Models trained on Zyda significantly outperform those trained on alternative datasets like Dolma and the Pile, especially when the StarCoder subset is removed. Zyda's quality and scale make it a strong candidate for large-scale language model pretraining, with its performance attributed to the thorough filtering and deduplication processes.

## Method Summary
Zyda is constructed by aggregating multiple permissively licensed datasets, including RefinedWeb, SlimPajama, Dolma, and others, totaling 1.3 trillion tokens. The dataset undergoes a comprehensive processing pipeline involving substring replacement, document-level filtering to remove low-quality and objectionable content, and deduplication both within individual datasets and across the entire corpus. Deduplication employs locality-sensitive hashing with MinHash signatures to identify and remove redundant content efficiently. The resulting dataset demonstrates superior performance in language model training compared to its constituent datasets and other open alternatives.

## Key Results
- Zyda contains 1.3 trillion tokens, double the size of RefinedWeb (600B tokens)
- Models trained on Zyda outperform those trained on Dolma and the Pile, particularly when excluding the StarCoder subset
- Zyda shows significant improvements in downstream task performance compared to its constituent datasets
- Cross-dataset deduplication removed large numbers of duplicates, reducing training noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zyda's postprocessing pipeline significantly enhances dataset quality beyond the original datasets.
- Mechanism: The combination of rigorous filtering (removing low-quality and objectionable content) and deduplication (both within and across datasets) reduces noise and redundancy, leading to better model performance.
- Core assumption: The quality improvements from filtering and deduplication are additive and compound when applied to a dataset composed of multiple sources.
- Evidence anchors:
  - [abstract] "We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets."
  - [section] "Our evaluations show that Zyda not only competes favorably with other open datasets... but also substantially improves the performance of comparable models from the Pythia suite."
  - [corpus] "Average neighbor FMR=0.491, average citations=0.0" (weak evidence; limited citations)
- Break condition: If filtering thresholds are too aggressive, they may remove useful data, negating the quality gains.

### Mechanism 2
- Claim: Inter-dataset deduplication is crucial for removing redundant data across different datasets.
- Mechanism: Many open-source datasets are derived from Common Crawl and filtered similarly, leading to significant overlap. Deduplication across datasets removes this redundancy, reducing training noise.
- Core assumption: Duplicate data across datasets has similar distributions and can be identified using locality-sensitive hashing (LSH) with MinHash signatures.
- Evidence anchors:
  - [abstract] "We performed cross-dataset deduplication and found large numbers of cross-dataset duplicates..."
  - [section] "Building atop the deduplication pipeline from (Soboleva et al., 2023), we optimize it for highly parallel processing and carefully tune our deduplication hyperparameters..."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.491, average citations=0.0." (weak evidence; limited citations)
- Break condition: If LSH parameters are not tuned correctly, false positives or false negatives may increase, reducing the effectiveness of deduplication.

### Mechanism 3
- Claim: Zyda's filtering pipeline is more comprehensive than prior works, leading to better dataset quality.
- Mechanism: The pipeline utilizes a wide range of filters from multiple sources and introduces novel ones, all extensively tested and tuned to ensure efficacy.
- Core assumption: The combination of syntactic and semantic filtering methods can effectively remove low-quality and objectionable content without removing too much useful data.
- Evidence anchors:
  - [abstract] "Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently."
  - [section] "Our filtering pipeline consisted of two stages: (1) substring replacement and (2) document-level filtering and removal."
  - [corpus] "Average neighbor FMR=0.491, average citations=0.0." (weak evidence; limited citations)
- Break condition: If the filtering thresholds are not properly tuned, the pipeline may either remove too much useful data or fail to remove enough low-quality content.

## Foundational Learning

- Concept: Locality-Sensitive Hashing (LSH) with MinHash signatures
  - Why needed here: To efficiently identify duplicate documents across large datasets.
  - Quick check question: How does LSH with MinHash signatures approximate Jaccard similarity between documents?

- Concept: Jaccard similarity
  - Why needed here: To measure the similarity between documents for deduplication purposes.
  - Quick check question: What is the Jaccard similarity between two documents with 10 unique words each, where 5 words are shared?

- Concept: Syntactic vs. semantic filtering
  - Why needed here: To remove low-quality and objectionable content from the dataset.
  - Quick check question: What is the difference between syntactic and semantic filtering, and when would each be used?

## Architecture Onboarding

- Component map: Data Ingestion -> Filtering Pipeline -> Deduplication -> Dataset Assembly -> Model Training
- Critical path: Data Ingestion → Filtering Pipeline → Deduplication → Dataset Assembly → Model Training
- Design tradeoffs:
  - Filter aggressiveness vs. data retention: More aggressive filtering removes more low-quality data but risks removing useful data.
  - Deduplication threshold: Higher thresholds remove more duplicates but risk removing near-duplicates that may be useful.
- Failure signatures:
  - High false positive rate in deduplication: May indicate incorrect LSH parameters or document preprocessing issues.
  - Low model performance despite filtering: May indicate overly aggressive filtering or insufficient deduplication.
- First 3 experiments:
  1. Test filtering pipeline on a small subset of data to tune thresholds and ensure low false positive rate.
  2. Test deduplication pipeline on a small subset of data to tune LSH parameters and ensure effective duplicate removal.
  3. Train a small language model on the processed dataset and compare performance to models trained on original datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Zyda compare to state-of-the-art proprietary datasets when models are trained on the full 1.3T token dataset instead of just 50B tokens?
- Basis in paper: [inferred] The paper mentions that Zyda contains 1.3T tokens, double that of RefinedWeb (600B), and anticipates Zyda would greatly outperform RefinedWeb if trained on the full dataset.
- Why unresolved: The paper only compares models trained on 50B tokens of Zyda against other datasets, not the full 1.3T tokens.
- What evidence would resolve it: Training and evaluating models on the full 1.3T token dataset of Zyda and comparing their performance against models trained on comparable proprietary datasets.

### Open Question 2
- Question: What is the optimal balance between filtering aggressiveness and data retention to maximize model performance?
- Basis in paper: [explicit] The paper mentions manually testing and tuning each filter to achieve a 20% false-positive threshold, but doesn't explore the full range of filtering parameters.
- Why unresolved: The paper only reports on one specific filtering configuration and doesn't explore the effects of more or less aggressive filtering.
- What evidence would resolve it: Conducting experiments with different filtering thresholds and analyzing the trade-off between data retention and model performance across various tasks.

### Open Question 3
- Question: How do advanced dataset quality improvement methods like semantic clustering or perplexity-based filtering affect Zyda's performance compared to the current filtering and deduplication pipeline?
- Basis in paper: [explicit] The paper mentions these methods as future work and suggests they could further improve dataset quality.
- Why unresolved: The paper only implements basic filtering and deduplication, not more advanced techniques.
- What evidence would resolve it: Applying semantic clustering, perplexity-based filtering, or other advanced methods to Zyda and comparing the resulting dataset's performance against the current version.

## Limitations

- Dataset quality improvements are largely inferred from model performance rather than direct human evaluation of the filtered content
- The filtering pipeline may have removed potentially valuable niche content alongside problematic material
- The inter-dataset deduplication assumes that similar documents across sources are truly redundant, which may not always hold true for content that naturally overlaps

## Confidence

- **High Confidence**: The dataset creation methodology and processing pipeline are technically sound, with clear implementation details for LSH-based deduplication and filtering stages.
- **Medium Confidence**: The claimed performance improvements over alternative datasets are supported by empirical results, though the evaluation focuses primarily on model benchmarks rather than direct quality assessment of the processed data.
- **Medium Confidence**: The effectiveness of the filtering pipeline in removing low-quality and objectionable content, while retaining useful data, is assumed based on tuning but not independently verified through human evaluation.

## Next Checks

1. **Human Evaluation of Filtered Content**: Conduct a systematic human review of a stratified sample of removed documents to assess whether the filtering pipeline is appropriately distinguishing between genuinely problematic content and potentially valuable material that may have been incorrectly filtered.

2. **Robustness Testing of Deduplication**: Perform controlled experiments varying LSH parameters and document preprocessing methods to quantify the sensitivity of duplicate detection rates and identify optimal thresholds that minimize both false positives and false negatives.

3. **Domain-Specific Performance Analysis**: Evaluate model performance across different content domains (e.g., technical documentation, creative writing, code) to determine whether the filtering and deduplication processes have introduced any systematic biases that affect certain types of content more than others.