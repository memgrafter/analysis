---
ver: rpa2
title: Do Text-to-Vis Benchmarks Test Real Use of Visualisations?
arxiv_id: '2407.19726'
source_url: https://arxiv.org/abs/2407.19726
tags:
- data
- code
- datasets
- dataset
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether text-to-visualisation benchmarks
  reflect real-world usage by comparing four standard benchmarks against real-world
  code from public repositories. The analysis reveals a significant gap: benchmark
  datasets focus on only one aspect of the task and use distributions of chart types
  and attributes that do not align with actual user behaviour.'
---

# Do Text-to-Vis Benchmarks Test Real Use of Visualisations?

## Quick Facts
- arXiv ID: 2407.19726
- Source URL: https://arxiv.org/abs/2407.19726
- Authors: Hy Nguyen; Xuefei He; Andrew Reeson; Cecile Paris; Josiah Poon; Jonathan K. Kummerfeld
- Reference count: 12
- Primary result: Text-to-visualisation benchmarks do not reflect real-world usage patterns; PlotCoder aligns well with real-world data but lacks executability.

## Executive Summary
This paper evaluates whether text-to-visualisation benchmarks reflect real-world usage by comparing four standard benchmarks against real-world code from public repositories. The analysis reveals a significant gap: benchmark datasets focus on only one aspect of the task and use distributions of chart types and attributes that do not align with actual user behaviour. Only one dataset reflects real-world patterns but lacks executability, limiting its use as an end-to-end benchmark. The findings guide future benchmark creation by highlighting which features and chart types are most significant for users, emphasizing the need for more representative and comprehensive evaluations.

## Method Summary
The authors extracted visualization code from real-world repositories (Stack dataset) and three benchmark datasets (nvBench, ChartDialogs, PlotCoder). They parsed the code using AST parsers and heuristics, built a cross-language mapping table for functions and arguments across four visualization libraries, and computed attribute frequencies and chart-type distributions. Spearman's rank correlation coefficient was used to measure alignment between datasets, with heat maps generated for visual inspection of patterns.

## Key Results
- Benchmark datasets significantly diverge from real-world usage patterns in chart types and attribute distributions
- Only PlotCoder shows strong alignment with real-world data (Spearman's 0.7-0.9) but lacks executability
- nvBench overrepresents bar charts and uses fewer functions and parameters than real-world code
- ChartDialogs has uniform distributions unlike the skewed patterns seen in real-world usage

## Why This Works (Mechanism)

### Mechanism 1
Cross-language mapping enables fair comparison of benchmark vs. real-world code. The authors parse visualization code from multiple libraries and map parameters across languages to a common schema. This aligns different APIs (Matplotlib, Graphics, ChartJS, Vega-Lite) so usage frequency can be compared. Core assumption: Library-specific parameter names correspond to semantically equivalent attributes.

### Mechanism 2
Benchmark distributions diverge from real-world usage patterns. The authors compare attribute frequencies and chart-type distributions between benchmarks and real-world codebases. Discrepancies reveal unrepresentative benchmarks. Core assumption: Real-world code captures "natural" user preferences better than synthetically generated benchmarks.

### Mechanism 3
PlotCoder aligns well with real-world usage but is not directly usable as an end-to-end benchmark. PlotCoder, extracted from real-world code, has high Spearman correlation with Matplotlib and Graphics datasets. However, it lacks data inputs and visual outputs, making it unusable for end-to-end evaluation. Core assumption: Extracted code patterns represent functional, complete visualizations in real contexts.

## Foundational Learning

- Concept: Abstract Syntax Tree (AST) parsing
  - Why needed here: To extract function calls, arguments, and parameter values from code without executing it
  - Quick check question: How does an AST parser distinguish between a function call and a variable assignment?

- Concept: Spearman rank correlation
  - Why needed here: To measure similarity in attribute usage distributions between datasets without assuming linear relationships
  - Quick check question: What does a Spearman coefficient of 0.9 imply about the ordering of attribute frequencies in two datasets?

- Concept: Cross-language parameter mapping
  - Why needed here: To compare visualization code across libraries that use different APIs but produce the same chart types
  - Quick check question: If library A has a `color` parameter and library B has `fill` for the same attribute, what mapping would you use?

## Architecture Onboarding

- Component map: Data ingestion (Stack code extraction) -> language-specific AST parsing -> Normalization (cross-language mapping table) -> Analysis pipeline (frequency computation -> Spearman correlation -> heat map generation) -> Output (benchmark alignment report + recommendations)

- Critical path:
  1. Extract and parse visualization code from repositories
  2. Build cross-language mapping table (8 categories, 62 attributes, ~850 parameters)
  3. Compute attribute and chart-type frequencies
  4. Calculate Spearman correlations between datasets
  5. Generate heat maps for visual inspection
  6. Summarize findings and suggest benchmark improvements

- Design tradeoffs:
  - Scope vs. depth: Focused on 4 libraries to ensure accurate mapping, but limits generalizability
  - Automation vs. correctness: AST parsing avoids execution but may miss dynamic code patterns
  - Benchmark selection: Used publicly available benchmarks, but excluded others due to domain fit

- Failure signatures:
  - Low or inconsistent Spearman correlations across real-world datasets (indicates mapping errors)
  - Missing parameters in mapping table (indicates incomplete coverage)
  - Heat map patterns that don't match domain intuition (indicates extraction or normalization issues)

- First 3 experiments:
  1. Validate mapping table by executing sample code from each library and comparing parameter effects
  2. Test Spearman correlation sensitivity by randomly sampling subsets of real-world data
  3. Re-run analysis excluding ChartJS to see if correlation patterns change (due to its small sample size)

## Open Questions the Paper Calls Out

### Open Question 1
How can we create a benchmark dataset that accurately reflects real-world usage patterns while maintaining executability of the code outputs? The paper identifies that PlotCoder shows strong alignment with real-world data but lacks executability due to missing data, visualisations, and library dependencies. Unresolved because the paper highlights the need but doesn't propose specific methodologies for achieving this balance.

### Open Question 2
What is the optimal distribution of chart types and attributes for a Text-to-Vis benchmark that would best reflect diverse user needs across different domains? The analysis shows significant misalignment between benchmark datasets and real-world usage, with nvBench overrepresenting bar charts and ChartDialogs having uniform distributions unlike real-world data. Unresolved because the paper identifies the problem but doesn't specify what the ideal distribution should be for different user contexts and domains.

### Open Question 3
How does the complexity of visualisation code (in terms of functions and parameters used) vary across different programming languages and what implications does this have for benchmark design? The analysis shows that benchmarks use significantly fewer functions and parameters than real-world code, with Vega-Lite showing higher complexity due to its nature as a visualisation language. Unresolved because the paper presents the disparity but doesn't explore the underlying reasons for these differences or how they should influence benchmark creation.

## Limitations
- Cross-language mapping relies on documentation and limited execution tests rather than comprehensive validation against a gold standard
- Real-world Stack dataset may be biased toward certain domains (e.g., academic plotting), which could skew the perceived divergence from benchmarks
- Analysis focuses on four specific libraries, limiting generalizability to other visualization ecosystems

## Confidence
- High: Benchmark distributions significantly diverge from real-world usage patterns (supported by correlation analysis)
- Medium: Cross-language mapping enables fair comparison (methodologically sound but limited validation)
- Low: PlotCoder's alignment with real-world data implies functional completeness (inferred from absence of data/vis, not direct verification)

## Next Checks
1. Execute a sample of mapped code from each library to verify parameter semantics and mapping accuracy
2. Compare Spearman correlations using stratified subsets of real-world data to test sensitivity to domain bias
3. Re-run the analysis excluding ChartJS to assess the impact of its small sample size on overall correlation patterns