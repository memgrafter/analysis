---
ver: rpa2
title: Zero-Shot Spam Email Classification Using Pre-trained Large Language Models
arxiv_id: '2405.15936'
source_url: https://arxiv.org/abs/2405.15936
tags:
- spam
- email
- classification
- llms
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the use of pre-trained large language
  models (LLMs) for zero-shot spam email classification. Two classification approaches
  were evaluated: (1) truncated raw content from email subject and body, and (2) classification
  based on summaries generated by ChatGPT.'
---

# Zero-Shot Spam Email Classification Using Pre-trained Large Language Models

## Quick Facts
- arXiv ID: 2405.15936
- Source URL: https://arxiv.org/abs/2405.15936
- Authors: Sergio Rojas-Galeano
- Reference count: 4
- Primary result: Flan-T5 achieved 90% F1-score using truncated content; GPT-4 reached 95% F1-score using summaries

## Executive Summary
This study investigates zero-shot spam email classification using pre-trained large language models (LLMs) without any fine-tuning. The research evaluates two approaches: classifying truncated raw email content and classifying ChatGPT-generated summaries of emails. Using the SpamAssassin dataset, the study compares three LLMs (ChatGPT, GPT-4, and Flan-T5) and demonstrates that pre-trained models can effectively classify spam emails through zero-shot prompting alone, achieving competitive performance compared to traditional methods.

## Method Summary
The study employs zero-shot prompting with three pre-trained LLMs (ChatGPT, GPT-4, and Flan-T5) to classify emails as spam or ham. Two classification approaches are tested: (1) truncated raw email content from subject and body, and (2) email summaries generated by ChatGPT. The SpamAssassin public corpus (6,047 emails with ~31% spam ratio) serves as the evaluation dataset. The models are evaluated using F1-score, accuracy, precision, recall, and balanced accuracy metrics without any additional training or fine-tuning.

## Key Results
- Flan-T5 achieved a 90% F1-score using truncated raw email content
- GPT-4 reached a 95% F1-score when using ChatGPT-generated summaries
- Zero-shot prompting enabled effective spam classification without model fine-tuning
- Email summarization improved classification performance compared to raw content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting allows pre-trained LLMs to perform spam classification without fine-tuning.
- Mechanism: The LLMs leverage knowledge gained during pre-training on massive text corpora to recognize patterns and linguistic cues associated with spam, enabling them to classify emails without additional training.
- Core assumption: The pre-training data contains sufficient examples of spam-like patterns and linguistic cues for the models to generalize to new, unseen spam emails.
- Evidence anchors:
  - [abstract] "Our empirical analysis, leveraging the entire dataset for evaluation without further training, reveals promising results."
  - [section] "This paper explores the effectiveness of three LLMs for zero-shot spam email classification: ... Their unique strengths are particularly relevant in this context."
  - [corpus] Weak: Related works focus on zero-shot and few-shot approaches but do not provide direct evidence for the specific mechanism described here.

### Mechanism 2
- Claim: Email summarization improves spam classification accuracy by condensing content and highlighting key spam indicators.
- Mechanism: The summarization step, performed by ChatGPT, distills the essential information from the email, removing irrelevant details and emphasizing features that are indicative of spam. This condensed representation makes it easier for the classification models to identify spam.
- Core assumption: The summarization process preserves the critical spam-related information while effectively reducing noise and irrelevant content.
- Evidence anchors:
  - [abstract] "Flan-T5 achieves a 90% F1-score on the truncated content approach, while GPT-4 reaches a 95% F1-score using summaries."
  - [section] "This summarised version captures the key points of the email and potentially eliminates irrelevant details."
  - [corpus] Weak: While related works mention summarization, there is no direct evidence in the corpus that supports the specific claim about its impact on spam classification accuracy.

### Mechanism 3
- Claim: Different LLMs have varying strengths in spam classification, with some models better suited for certain types of spam or content.
- Mechanism: The architectural differences and training strategies of the LLMs (e.g., ChatGPT, GPT-4, Flan-T5) lead to variations in their ability to recognize and classify spam emails. Some models may be more effective at identifying certain linguistic patterns or semantic cues associated with spam.
- Core assumption: The underlying architecture and training data of each LLM contribute to its unique strengths and weaknesses in spam classification.
- Evidence anchors:
  - [section] "These models, with their ability to learn complex relationships within text, have the potential to outperform traditional and deep learning methods."
  - [section] "Analysing accuracy (AC), Flan-T5 achieves the highest score (94%), outperforming GPT-4 (91%) and ChatGPT (82%)."
  - [corpus] Weak: The corpus provides some evidence of varying performance among LLMs in related tasks, but does not directly support the specific claim about their strengths in spam classification.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Zero-shot learning is the foundation of the approach, allowing the LLMs to classify spam emails without additional training.
  - Quick check question: How does zero-shot learning differ from traditional supervised learning in terms of training data requirements?

- Concept: Prompt engineering
  - Why needed here: Effective prompt design is crucial for guiding the LLMs to perform the spam classification task accurately.
  - Quick check question: What are the key considerations when designing prompts for zero-shot classification tasks?

- Concept: Text summarization
  - Why needed here: Summarization is used as a pre-processing step to condense email content and highlight key spam indicators.
  - Quick check question: How does text summarization contribute to the overall spam classification pipeline?

## Architecture Onboarding

- Component map: Raw email content → Summarization (optional) → LLM classification → Classification label
- Critical path: Raw email content → Summarization (optional) → LLM classification → Classification label
- Design tradeoffs:
  - Using raw content vs. summarized content: Summarization may improve accuracy but adds an extra step and potential information loss.
  - Choice of LLM: Different models have varying strengths and computational requirements.
  - Zero-shot vs. fine-tuned: Zero-shot is more efficient but may have lower accuracy compared to fine-tuned models.
- Failure signatures:
  - Low accuracy: The LLMs may not have learned sufficient patterns during pre-training to generalize to new spam emails.
  - High false positives: The models may be overly sensitive to certain linguistic cues or patterns.
  - High false negatives: The models may miss subtle or novel spam tactics.
- First 3 experiments:
  1. Evaluate the performance of each LLM (ChatGPT, GPT-4, Flan-T5) on raw email content without summarization.
  2. Compare the performance of the LLMs when using summarized email content versus raw content.
  3. Analyze the impact of different truncation strategies on the classification accuracy of the LLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot LLM classification compare to fine-tuned models when evaluated on datasets from different domains or with concept drift?
- Basis in paper: [explicit] The paper notes that "further validation on diverse datasets is necessary" and discusses concept drift as a challenge for traditional spam filters.
- Why unresolved: The study only evaluated on the SpamAssassin dataset. Performance on datasets with different spam tactics, languages, or temporal variations remains untested.
- What evidence would resolve it: Comparative studies measuring F1-scores of zero-shot vs. fine-tuned models across multiple spam datasets collected at different times or from different sources.

### Open Question 2
- Question: What is the impact of different summarization techniques on LLM performance for spam classification?
- Basis in paper: [explicit] The paper acknowledges that "we used a single summarisation approach (ChatGPT-generated summaries)" and suggests investigating other techniques.
- Why unresolved: Only one summarizer (ChatGPT) was used. Alternative methods like extractive summarization or other LLMs might yield different results.
- What evidence would resolve it: Systematic comparison of spam classification performance using summaries generated by different methods (abstractive vs. extractive, different models) on the same email corpus.

### Open Question 3
- Question: What is the optimal balance between zero-shot capability and selective fine-tuning for practical spam filtering systems?
- Basis in paper: [explicit] The conclusion suggests "hybrid approaches that combine zero-shot capability with selective fine-tuning could balance efficiency and performance."
- Why unresolved: The paper only explored pure zero-shot approaches. The trade-offs between inference costs, adaptation speed, and classification accuracy for hybrid methods remain unexplored.
- What evidence would resolve it: Empirical comparison of various hybrid approaches (few-shot, selective fine-tuning on high-confidence errors) measuring both accuracy and computational costs across multiple spam datasets.

## Limitations
- The study only evaluated on the SpamAssassin dataset from 2003-2005, which may not represent modern spam tactics
- Exact prompt templates and truncation strategies were not fully specified, limiting reproducibility
- Performance comparisons to traditional supervised approaches on the same dataset were not conducted

## Confidence

- High confidence: LLMs can perform zero-shot spam classification without fine-tuning
- Medium confidence: ChatGPT-generated summaries improve classification accuracy
- Low confidence: Performance rankings between specific LLMs will generalize to other datasets

## Next Checks
1. Independent replication using the exact SpamAssassin corpus with publicly available prompt templates
2. Cross-dataset validation on modern spam datasets to assess temporal generalizability, particularly given the 2003-2005 age of the evaluation data
3. Ablation study comparing zero-shot performance against simple keyword-based baselines to establish practical value beyond sophisticated engineering