---
ver: rpa2
title: 'Latent Functional Maps: a spectral framework for representation alignment'
arxiv_id: '2406.14183'
source_url: https://arxiv.org/abs/2406.14183
tags:
- functional
- spaces
- latent
- space
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Latent Functional Maps (LFM), a spectral
  framework for comparing and aligning neural network representations across different
  models and modalities. The method models latent spaces as graph manifolds, computes
  their functional maps, and leverages these to (i) measure representational similarity,
  (ii) find correspondences (even with few or no anchors), and (iii) transfer representations
  across spaces.
---

# Latent Functional Maps: a spectral framework for representation alignment

## Quick Facts
- arXiv ID: 2406.14183
- Source URL: https://arxiv.org/abs/2406.14183
- Authors: Marco Fumero; Marco Pegoraro; Valentino Maiorca; Francesco Locatello; Emanuele Rodolà
- Reference count: 40
- Primary result: Introduces LFM framework achieving higher and more interpretable similarity scores than baselines like CKA, with strong performance in stitching and retrieval tasks

## Executive Summary
This paper introduces Latent Functional Maps (LFM), a spectral framework for comparing and aligning neural network representations across different models and modalities. The method models latent spaces as graph manifolds, computes their functional maps, and leverages these to (i) measure representational similarity, (ii) find correspondences (even with few or no anchors), and (iii) transfer representations across spaces. LFM demonstrates robustness to local perturbations, achieving higher and more interpretable similarity scores than baselines like CKA. In stitching tasks across datasets (CIFAR100, ImageNet, CUB, MNIST, AGNews), LFM+Ortho outperforms direct orthogonal alignment, especially with few anchors. For retrieval tasks, LFM achieves over 0.8 MRR with just 5 anchors, saturating at ~0.99 with more eigenvectors.

## Method Summary
LFM constructs k-NN graphs in latent spaces, computes their Laplacian eigendecompositions to obtain spectral bases, and optimizes functional maps between these bases using regularizers for commutativity and smoothness. The functional map C serves as a linear representation of correspondences that can be used for similarity measurement, correspondence finding, and representation transfer. The framework handles both weakly supervised and unsupervised settings, with descriptors ranging from distance functions to semantic information. Spectral refinement improves the quality of computed maps, enabling zero-shot stitching and accurate retrieval across aligned representations.

## Key Results
- LFM achieves higher and more interpretable similarity scores than baselines like CKA, especially under local perturbations
- In stitching tasks across datasets, LFM+Ortho outperforms direct orthogonal alignment, particularly with few anchors
- LFM retrieval achieves over 0.8 MRR with just 5 anchors, saturating at ~0.99 with more eigenvectors
- The framework provides a unified tool for representation alignment with strong empirical performance across multiple tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Functional maps provide a linear, interpretable representation of correspondences between latent spaces by encoding them in the spectral domain.
- Mechanism: The latent space is approximated by a k-NN graph whose Laplacian has an eigendecomposition. The functional map C aligns the spectral bases (eigenvectors) of two graphs, transforming functions between them. This linear representation allows constraints like commutativity to be expressed compactly and solved efficiently.
- Core assumption: The latent data lies on a low-dimensional manifold that can be approximated by a k-NN graph with meaningful geometry.
- Evidence anchors:
  - [abstract] "By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain..."
  - [section] "Given two manifolds M and N equipped with a basis ΦM (respectively ΦN ). Any squared integrable function f : M → R can be represented as a linear combination of basis functions..."
  - [corpus] No direct evidence; the corpus discusses related spectral methods but not functional maps specifically.
- Break condition: If the latent space does not lie on a well-behaved manifold or the k-NN graph poorly approximates the geometry, the spectral basis becomes uninformative and C loses interpretability.

### Mechanism 2
- Claim: The functional map C acts as a robust similarity measure because it is volume-preserving under isometries, making CTC diagonal in aligned cases.
- Mechanism: Under isometry, the functional map preserves the inner product of functions, which translates to CTC being diagonal. The ratio of off-diagonal to full norm of CTC quantifies distortion, yielding a similarity score sim(X,Y) = 1 - ||off(CTC)||²F / ||CTC||²F.
- Core assumption: The transformation between the two latent spaces is approximately isometric, so the functional map preserves local geometry.
- Evidence anchors:
  - [abstract] "We show that LFM allows us to find correspondences between representational spaces, both in weakly supervised and unsupervised settings..."
  - [section] "When the transformation between the two manifolds is an isometry, the matrix CTC will be diagonal..."
  - [corpus] No direct evidence; corpus discusses spectral alignment but not this specific similarity metric.
- Break condition: If the spaces are highly non-isometric (e.g., different modalities with unrelated semantics), the off-diagonal terms dominate and the similarity score becomes meaningless.

### Mechanism 3
- Claim: Functional maps enable efficient zero-shot stitching by transferring points as distance functions and aligning them via C, achieving high performance with few anchors.
- Mechanism: Points in latent space are represented as distance functions to a set of nodes. The functional map C aligns these functions between spaces, enabling transfer without explicit point-to-point correspondence. Few known correspondences (anchors) suffice to compute C, which can then be refined or used to find more correspondences.
- Core assumption: Distance functions on the graph are preserved under the transformation between latent spaces, so aligning them recovers the mapping.
- Evidence anchors:
  - [abstract] "LFM demonstrates robustness to local perturbations, achieving higher and more interpretable similarity scores than baselines like CKA."
  - [section] "The map C computed between two latent spaces can be utilized in various ways to transfer information from one space to the other..."
  - [corpus] No direct evidence; corpus discusses latent space communication but not functional map-based stitching.
- Break condition: If distance functions are not preserved (e.g., under strong non-linear distortions), the alignment fails and stitching performance degrades.

## Foundational Learning

- Concept: Graph Laplacian eigendecomposition and spectral graph theory.
  - Why needed here: The k-NN graph Laplacian provides the spectral basis for representing functions on the latent space; eigendecomposition yields the orthonormal basis used in functional maps.
  - Quick check question: What does the smallest non-zero eigenvalue of the graph Laplacian represent in terms of graph connectivity?

- Concept: Functional maps framework from geometry processing.
  - Why needed here: This is the core mathematical framework extended to neural latent spaces; it defines how to represent and compute correspondences as linear maps between function spaces.
  - Quick check question: In the functional maps framework, how is a pointwise correspondence between two shapes recovered from the functional map matrix C?

- Concept: k-NN graph construction and distance metrics (L2, angular).
  - Why needed here: The choice of distance metric affects graph geometry and thus the spectral basis; robustness experiments show angular distance is more stable to noise.
  - Quick check question: How does increasing the number of neighbors k affect the locality and smoothness of the graph Laplacian?

## Architecture Onboarding

- Component map: Input data → k-NN graph construction → Laplacian eigendecomposition → Descriptor functions → Functional map optimization (Eq 2) → Spectral refinement → Downstream tasks (similarity, stitching, retrieval)
- Critical path: k-NN graph → Laplacian eigendecomposition → Functional map optimization. Delays here block all downstream tasks.
- Design tradeoffs: More eigenvectors → better approximation but higher computational cost and risk of overfitting. Fewer anchors → faster but less accurate maps. Choice of distance metric → affects robustness to noise.
- Failure signatures: Poor similarity scores despite known alignment indicate bad graph construction or insufficient eigenvectors. High computational cost suggests need to reduce k or eigenvectors. Retrieval performance degrading with anchors suggests descriptor choice issues.
- First 3 experiments:
  1. Construct k-NN graph on MNIST embeddings with k=300, compute first 50 eigenvectors, verify eigenvalues are sorted and Laplacian is positive semi-definite.
  2. Optimize functional map between two MNIST autoencoders using 10 anchors and geodesic distance descriptors; check CTC diagonal structure.
  3. Perform retrieval on aligned word embeddings with 5 anchors; verify MRR > 0.8 and functional map structure is stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LFM vary with different choices of graph construction parameters (k, distance metric) across diverse datasets and model architectures?
- Basis in paper: [explicit] The paper mentions choosing k=300 and using angular distance as a primary metric, with some ablation on L2 distance in synthetic settings.
- Why unresolved: The paper does not systematically explore the sensitivity of LFM performance to these parameters across different datasets and model types.
- What evidence would resolve it: Comprehensive experiments varying k and distance metrics across multiple datasets and architectures, showing performance trends and optimal configurations.

### Open Question 2
- Question: What is the theoretical limit of LFM's similarity measure in distinguishing between truly isometric transformations and those that preserve linear separability?
- Basis in paper: [inferred] The paper demonstrates LFM's robustness to perturbations preserving linear separability, but does not provide a theoretical analysis of its discriminative power.
- Why unresolved: The paper provides empirical evidence but lacks a formal theoretical framework for understanding the boundaries of LFM's similarity measure.
- What evidence would resolve it: A formal proof or comprehensive empirical study showing the conditions under which LFM can or cannot distinguish between different types of transformations.

### Open Question 3
- Question: How does LFM perform in fully unsupervised settings, where no correspondence or label information is available?
- Basis in paper: [explicit] The paper mentions this as an area requiring further research and improvement.
- Why unresolved: The paper focuses on weakly supervised and semi-supervised settings, with limited exploration of fully unsupervised scenarios.
- What evidence would resolve it: Experiments comparing LFM's performance in fully unsupervised settings against other unsupervised representation learning methods, across multiple tasks and datasets.

## Limitations
- The framework relies on the assumption that latent spaces form well-behaved manifolds, which may not hold across all architectures
- Performance depends heavily on hyperparameter choices (k-NN parameters, number of eigenvectors) requiring dataset-specific tuning
- Theoretical guarantees for LFM under realistic neural network transformations remain unclear

## Confidence

- Mechanism 1 (Functional maps as linear representation): High confidence - well-established mathematical framework with clear implementation
- Mechanism 2 (Volume-preserving similarity measure): Medium confidence - theoretically sound but depends heavily on isometry assumption
- Mechanism 3 (Zero-shot stitching via distance functions): Medium confidence - empirically validated but theoretical justification needs strengthening

## Next Checks

1. Test LFM on latent spaces with known non-isometric transformations (e.g., different modalities) to quantify breakdown conditions for the similarity metric
2. Conduct ablation studies varying k-NN graph parameters (k, distance metrics) across diverse datasets to establish robustness guidelines
3. Compare LFM performance against baselines when the latent space manifold assumption is deliberately violated through adversarial perturbations