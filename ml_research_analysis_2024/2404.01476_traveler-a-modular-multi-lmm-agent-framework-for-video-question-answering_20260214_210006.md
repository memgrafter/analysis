---
ver: rpa2
title: 'TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering'
arxiv_id: '2404.01476'
source_url: https://arxiv.org/abs/2404.01476
tags:
- information
- video
- question
- person
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TraveLER introduces a modular multi-LMM agent framework for video
  question-answering that iteratively traverses video frames, locates relevant information
  through question-answering, evaluates if enough data has been collected, and replans
  as needed. The framework consists of Planner, Retriever, Extractor, and Evaluator
  agents that work together to adaptively gather information.
---

# TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering

## Quick Facts
- arXiv ID: 2404.01476
- Source URL: https://arxiv.org/abs/2404.01476
- Authors: Chuyi Shang; Amos You; Sanjay Subramanian; Trevor Darrell; Roei Herzig
- Reference count: 40
- Primary result: 68.2% accuracy on NExT-QA benchmark without fine-tuning

## Executive Summary
TraveLER introduces a modular multi-LMM agent framework for video question-answering that iteratively traverses video frames, locates relevant information through question-answering, evaluates if enough data has been collected, and replans as needed. The framework consists of Planner, Retriever, Extractor, and Evaluator agents that work together to adaptively gather information. TraveLER improves performance on NExT-QA (68.2% accuracy), STAR (44.9%), and Perception Test (50.2%) without fine-tuning. It outperforms state-of-the-art zero-shot methods like LLoVi and SeViLA while viewing fewer frames on average. Ablation studies show the Planner and Retriever modules are particularly important, with the iterative approach and question-answering process providing key benefits over simple captioning methods.

## Method Summary
TraveLER employs a modular multi-agent architecture where four specialized LLM agents work in sequence: the Planner creates traversal plans based on the question, the Retriever selects relevant frames, the Extractor generates context-dependent questions about each frame and uses an LMM to answer them, and the Evaluator determines if sufficient information exists to answer the question. The process is iterative, with the Evaluator providing feedback to the Planner when more information is needed. The framework uses LLMs for planning and evaluation, and LMMs for visual information extraction, requiring no task-specific fine-tuning or video annotations.

## Key Results
- Achieves 68.2% accuracy on NExT-QA, 44.9% on STAR, and 50.2% on Perception Test benchmarks without fine-tuning
- Outperforms state-of-the-art zero-shot methods like LLoVi and SeViLA while viewing fewer frames on average
- Ablation studies show Planner and Retriever modules are particularly important for performance
- Iterative approach with question-answering provides key benefits over simple captioning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TraveLER's iterative approach with replanning allows it to adaptively gather relevant information when initial attempts are insufficient.
- Mechanism: The framework uses an Evaluator agent to check if collected information is sufficient to answer the question. If not, it provides an explanation to the Planner agent, which creates a modified plan based on the accumulated knowledge from previous iterations.
- Core assumption: The LLM agents can effectively generate plans and evaluate information sufficiency based on the question context and collected memory.
- Evidence anchors:
  - [abstract] "Finally, if there is not enough information, our method is able to 'Replan' based on its collected knowledge."
  - [section 3.2] "If both are true, LLMevaluator outputs the best choice c∗ to answer the question Q. Otherwise, it provides an explanation E on why there is not enough information and gives this explanation to the Planner to start a new iteration."
- Break condition: The iterative process stops either when sufficient information is collected to answer the question, or when a predetermined iteration limit is reached.

### Mechanism 2
- Claim: The question-answering approach in the Extractor module captures more specific and relevant details compared to generic captioning methods.
- Mechanism: Instead of generating general captions, the Extractor uses an LLM to generate context-dependent questions about each frame, then uses an LMM to answer these questions. This process extracts fine-grained details specifically relevant to the question being asked.
- Core assumption: Generating questions that are tailored to both the current frame and the overall question context will yield more relevant information than generic frame descriptions.
- Evidence anchors:
  - [abstract] "Nevertheless, the common captioning approach provides general descriptions for the frame, whereas answering questions often requires more specific details."
  - [section 3.2] "The Extractor is a significant part of our method because it allows us to capture more relevant and question specific details from the visual input, unlike using only captions."
- Break condition: The question-answering process breaks when the LMM fails to provide accurate answers or when the generated questions are not relevant to the question context.

### Mechanism 3
- Claim: The modular multi-agent architecture with specialized roles (Planner, Retriever, Extractor, Evaluator) enables more effective video traversal and information extraction compared to monolithic approaches.
- Mechanism: Each agent has a distinct role - the Planner creates traversal plans, the Retriever selects relevant frames based on the plan, the Extractor generates questions and extracts information from frames, and the Evaluator determines if sufficient information has been gathered. This specialization allows each component to focus on its specific task.
- Core assumption: Decomposing the video question-answering task into specialized sub-tasks and assigning them to different agents will lead to better overall performance than having a single model handle all aspects simultaneously.
- Evidence anchors:
  - [abstract] "Our proposed TraveLER method does not require task-specific fine-tuning or video annotations, as well as being easy to employ with several different LLM or LMMs."
  - [section 3.2] "Our framework is composed of four main stages, each with LLM or LMM 'agents' that interact with each other through the different stages."
- Break condition: The modular approach breaks when agent coordination fails or when information from one module is insufficient for subsequent modules to function effectively.

## Foundational Learning

- Concept: Frame selection and sampling strategies
  - Why needed here: The Retriever module must decide which frames to view next based on the current plan and collected information. Understanding how to select frames that maximize information gain is crucial.
  - Quick check question: If a video is 60 seconds long and you need to select 4 frames, what sampling strategy would you use to ensure you capture the most relevant information?

- Concept: Question generation and context understanding
  - Why needed here: The Extractor module generates questions about frames that are context-dependent and relevant to the overall question. This requires understanding how to formulate questions that will yield useful information.
  - Quick check question: Given a frame showing a person holding a book and the question "What color is the book cover?", what would be an effective question to ask about this frame?

- Concept: Iterative planning and replanning
  - Why needed here: The Planner must create initial plans and modify them based on feedback from the Evaluator. This requires understanding how to adapt strategies based on partial information.
  - Quick check question: If your initial plan to find information about "why someone is wearing a hat" fails to provide sufficient information, what would be a logical next step in your plan?

## Architecture Onboarding

- Component map: Planner (LLM) -> Retriever (LLM) -> Extractor (LLM + LMM) -> Evaluator (LLM) -> (if needed) Replan -> repeat
- Critical path: Planner → Retriever → Extractor → Evaluator → (if needed) Replan → repeat
- Design tradeoffs:
  - Single-pass vs iterative approach: Iterative allows adaptation but increases computational cost
  - Question-answering vs captioning: QA extracts more specific details but requires more processing
  - Frame selection granularity: More frames provide more context but increase computational load
- Failure signatures:
  - Planner generates irrelevant plans → Retriever selects wrong frames → Evaluator never finds sufficient information
  - Extractor generates poor questions → LMM provides unhelpful answers → Memory fills with irrelevant data
  - Memory bank becomes too large → Performance degrades due to context window limitations
- First 3 experiments:
  1. Replace the Planner with a simple heuristic (e.g., uniform sampling) and measure performance drop
  2. Replace question-answering with simple captioning in the Extractor and measure impact on accuracy
  3. Test with different numbers of questions per frame (0, 1, 3, 5) to find optimal balance between information extraction and efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on proprietary LLMs (GPT-4, GPT-3.5) which may not be accessible to all researchers
- Framework's effectiveness appears dependent on specific prompt engineering for Planner and Evaluator agents
- Computational efficiency gains mentioned are based on SGLang optimizations that may not translate to other frameworks

## Confidence
**High Confidence**: The core mechanism of iterative replanning based on information sufficiency evaluation is well-supported by both theoretical reasoning and empirical results showing improved accuracy over non-iterative baselines.

**Medium Confidence**: The claim that question-answering extracts more relevant details than captioning is supported by ablation studies, but the evaluation could be more rigorous by directly comparing the same framework with and without question-answering on identical inputs.

**Low Confidence**: The assertion that the modular architecture itself provides benefits beyond the individual components is difficult to verify, as the paper doesn't provide ablations comparing monolithic vs. modular implementations of the same underlying capabilities.

## Next Checks
1. **Component Isolation Test**: Replace the Planner with a simple heuristic (e.g., uniform frame sampling) while keeping all other components unchanged to quantify the exact contribution of intelligent planning to overall performance.

2. **Information Extraction Comparison**: Implement a version of the framework that uses only captioning instead of question-answering in the Extractor, then measure performance differences on the same test set to isolate the impact of the QA approach.

3. **Memory Efficiency Analysis**: Measure how memory bank size scales with video length and question complexity, then test whether there's a point where the iterative approach becomes less efficient than single-pass methods due to memory constraints.