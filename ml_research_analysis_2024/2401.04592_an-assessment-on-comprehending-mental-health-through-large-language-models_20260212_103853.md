---
ver: rpa2
title: An Assessment on Comprehending Mental Health through Large Language Models
arxiv_id: '2401.04592'
source_url: https://arxiv.org/abs/2401.04592
tags:
- mental
- health
- language
- depression
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates how well large language models (LLMs) like
  Llama-2 and ChatGPT can understand mental health conditions expressed in natural
  language, specifically using the DAIC-WOZ dataset and PHQ-4/GAD-2 questionnaires
  for depression and anxiety assessment. The authors compare these LLMs against classical
  machine learning models (XGBoost) and transformer models (BERT, RoBERTa, XLNet).
---

# An Assessment on Comprehending Mental Health through Large Language Models

## Quick Facts
- arXiv ID: 2401.04592
- Source URL: https://arxiv.org/abs/2401.04592
- Reference count: 37
- Primary result: Fine-tuned transformer models significantly outperform LLMs (Llama-2, ChatGPT) on mental health assessment tasks using DAIC-WOZ dataset

## Executive Summary
This paper evaluates how well large language models comprehend mental health conditions expressed in natural language by comparing Llama-2 and ChatGPT against classical ML models (XGBoost) and fine-tuned transformer models (BERT, RoBERTa, XLNet). Using the DAIC-WOZ dataset with PHQ-4/GAD-2 questionnaires for depression and anxiety assessment, the authors test different prompting strategies for LLMs and evaluate performance across multiple metrics including weighted precision, recall, F1-score, Hamming loss, and AUC-ROC. Results demonstrate that transformer-based models consistently outperform LLMs, with Distil-RoBERTa achieving the best performance across all mental health assessment questions, while LLMs showed only moderate capability in this specialized task.

## Method Summary
The study preprocesses the DAIC-WOZ dataset by concatenating participant messages to maximum 50 words per observation, then trains and evaluates multiple model types: XGBoost with transformer embeddings, Llama-2 and ChatGPT with five different prompting strategies, and fine-tuned transformer models (BERT, DistilBERT, RoBERTa, DistilRoBERTa, XLNet). Models are evaluated on four mental health questions (GAD-1, GAD-2, PHQ-1, PHQ-2) using weighted precision, recall, F1-score, Hamming loss, and AUC-ROC metrics. Five-fold cross-validation is used for hyperparameter tuning across all models.

## Key Results
- Fine-tuned transformer models (particularly Distil-RoBERTa) consistently outperform LLMs on all mental health assessment tasks
- Distil-RoBERTa achieved the highest F1 scores (0.56-0.68) across all four mental health questions
- Llama-2 with optimal prompting (version 3) achieved F1 scores of only 0.27-0.33, significantly below transformer models
- XGBoost with transformer embeddings outperformed all models on Hamming loss metric

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning adapts model weights to the domain and task, improving pattern recognition for mental health cues in natural language. LLMs, even with prompting, remain in a zero-shot/few-shot regime, limiting performance. Core assumption: Fine-tuning provides better generalization to specialized mental health tasks than prompting-based inference.

### Mechanism 2
Structured prompts reduce ambiguity and guide output formatting, but without domain-specific training data, LLMs cannot learn nuanced mental health indicators. Core assumption: Prompts provide task framing but cannot substitute for exposure to labeled mental health examples during training.

### Mechanism 3
Hamming loss captures overall label accuracy in multi-class settings, while F1 focuses on class-specific balance, potentially masking poor performance on minority classes. Core assumption: Mental health classification requires balanced error penalization to avoid missing at-risk cases.

## Foundational Learning

- **Concept**: Mental health assessment scales (PHQ-4, GAD-2)
  - Why needed here: The paper evaluates model performance on predicting PHQ-4/GAD-2 scores; understanding these scales is essential to interpret results.
  - Quick check question: What do the four PHQ-4 sub-scores represent, and how are they used in depression/anxiety screening?

- **Concept**: Fine-tuning vs zero-shot/few-shot learning
  - Why needed here: The paper compares fine-tuned transformers to prompted LLMs; understanding this distinction explains performance differences.
  - Quick check question: How does fine-tuning a model on a labeled dataset differ from using a pre-trained model with prompts?

- **Concept**: Evaluation metrics in classification (precision, recall, F1, Hamming loss, AUC-ROC)
  - Why needed here: The paper uses multiple metrics; knowing their meaning and trade-offs is necessary to assess model strengths/weaknesses.
  - Quick check question: When would Hamming loss be preferred over F1-score in a multi-class classification task?

## Architecture Onboarding

- **Component map**: Dataset preprocessing -> Embedding generation (transformer) -> Classification (XGBoost/LLM prompting) -> Evaluation (metrics)
- **Critical path**: Clean DAIC-WOZ data -> Concatenate messages -> Generate embeddings (BERT/RoBERTa) -> Train classifier (XGBoost) -> Evaluate -> Compare with LLM outputs
- **Design tradeoffs**: Fine-tuning transformers requires more compute and data but yields better performance; prompting LLMs is faster but less accurate.
- **Failure signatures**: Poor F1/recall on minority classes (e.g., high anxiety scores) suggests model bias; high Hamming loss indicates frequent misclassifications across labels.
- **First 3 experiments**:
  1. Reproduce Table 3: Run Llama-2 with prompting version 3 on DAIC-WOZ test set and compare F1 scores.
  2. Train Distil-RoBERTa on DAIC-WOZ and evaluate GAD-1/GAD-2 performance (replicate Table 4).
  3. Implement Hamming loss calculation and verify XGBoost outperforms all models on this metric (replicate Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
How do fine-tuned transformer models like Distil-RoBERTa compare to general-purpose LLMs like Llama-2 and ChatGPT for mental health assessment across different languages and cultural contexts? The study only evaluates models on the DAIC-WOZ dataset, which is primarily in English and represents specific cultural contexts.

### Open Question 2
What specific prompting strategies can improve LLM performance for mental health assessment to match or exceed transformer-based models? While the paper tests multiple prompting strategies, it doesn't explore more sophisticated approaches like chain-of-thought prompting, few-shot examples, or task-specific fine-tuning.

### Open Question 3
What are the key sources of bias in mental health assessment models, and how can they be mitigated? The authors mention analyzing "biases in training data" as a major hurdle, though the paper itself doesn't explicitly analyze bias or examine how demographic representation affects model performance.

## Limitations

- The study only evaluates models on the DAIC-WOZ dataset, limiting generalizability to broader mental health expression contexts
- The paper does not explore advanced prompting techniques like chain-of-thought or instruction tuning that might narrow the performance gap
- Dataset representativeness is limited to clinical interview scenarios rather than real-world mental health expressions in social media or peer support contexts

## Confidence

- **Transformer models outperform LLMs on mental health assessment**: High confidence
- **Prompting cannot substitute for fine-tuning**: Medium confidence
- **Hamming loss is appropriate for mental health classification**: Medium confidence

## Next Checks

1. Reproduce Table 4: Train and evaluate Distil-RoBERTa on the DAIC-WOZ dataset for all four mental health questions and verify the reported F1 scores (0.56-0.68 range) and AUC-ROC values.

2. Test advanced prompting strategies: Implement chain-of-thought prompting or instruction tuning for Llama-2 on the mental health task to determine if the performance gap with fine-tuned models can be reduced.

3. Validate metric selection: Conduct ablation studies comparing model rankings using different metric combinations (F1-only vs. Hamming loss vs. AUC-ROC) to assess sensitivity of conclusions to evaluation choices.