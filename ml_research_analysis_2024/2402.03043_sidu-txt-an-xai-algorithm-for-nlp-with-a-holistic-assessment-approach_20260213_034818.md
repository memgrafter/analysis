---
ver: rpa2
title: 'SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach'
arxiv_id: '2402.03043'
source_url: https://arxiv.org/abs/2402.03043
tags:
- methods
- text
- sidu-txt
- words
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SIDU-TXT, an explainable AI (XAI) method for
  text classification that adapts the Similarity Difference and Uniqueness (SIDU)
  approach from image to text. The method generates word-level heatmaps highlighting
  influential features by selecting top feature activation masks from CNN models.
---

# SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach

## Quick Facts
- arXiv ID: 2402.03043
- Source URL: https://arxiv.org/abs/2402.03043
- Reference count: 40
- Key outcome: SIDU-TXT outperforms Grad-CAM and LIME in faithfulness and human-grounded evaluations for sentiment analysis, but faces challenges in high-stakes legal domains

## Executive Summary
This paper presents SIDU-TXT, an XAI method for text classification that adapts the Similarity Difference and Uniqueness (SIDU) approach from image to text. The method generates word-level heatmaps by selecting top feature activation masks from CNN models, weighted by SIDU scores. A three-tier evaluation framework assesses functional faithfulness, human alignment, and application-grounded interpretability. In sentiment analysis, SIDU-TXT shows superior performance over baselines, while in asylum case prediction, both SIDU-TXT and Grad-CAM demonstrate comparable interpretability but fall short of expert expectations.

## Method Summary
SIDU-TXT generates text explanations by extracting feature activation masks from CNN models, computing SIDU weights (product of similarity difference and uniqueness), and selecting top K masks for weighted summation. The method applies a threshold τ=0.5 to binarize masks and reduce noise. Evaluation uses a three-tier framework: Functionally-Grounded (insertion/deletion AUC), Human-Grounded (Jaccard similarity, precision/recall/F1), and Application-Grounded (expert assessment in asylum law). The approach is tested on IMDB movie reviews for sentiment analysis and Danish asylum decisions for case prediction.

## Key Results
- Outperforms Grad-CAM and LIME in sentiment analysis faithfulness (AUC insertion: 0.551 vs 0.431/0.423; deletion: 0.154 vs 0.207/0.243)
- Shows strong human alignment in sentiment analysis (Jaccard similarity: 0.8 at low thresholds; precision 0.570, recall 0.515, F1 0.504)
- Demonstrates comparable interpretability to Grad-CAM in asylum case prediction, though both fall short of expert expectations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SIDU-TXT generates more faithful text explanations by leveraging top K feature activation masks weighted by similarity difference and uniqueness scores.
- **Mechanism:** SIDU-TXT extracts binary masks from CNN feature maps, upscales them to match text length, multiplies with token vectors to form feature activation text masks, then computes SIDU weights (product of similarity difference and uniqueness) and selects top K masks for weighted summation.
- **Core assumption:** The top K masks correspond to the most semantically informative features for model prediction, while lower-weight masks are noise.
- **Evidence anchors:**
  - [abstract] "utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level"
  - [section] "selecting the most informative masks based on their weights Wc_i" and "Masks with higher weights are found to signify features rich in semantic information"
  - [corpus] Weak - corpus neighbors focus on general XAI, not this specific mask-selection mechanism.
- **Break condition:** If feature masks from CNN layers do not capture meaningful semantic patterns, or if top K selection fails to isolate salient features, faithfulness will degrade.

### Mechanism 2
- **Claim:** The SIDU-TXT evaluation framework ensures method quality across functional, human, and application-grounded dimensions.
- **Mechanism:** Three-tiered evaluation: Functionally-Grounded uses insertion/deletion AUC to measure faithfulness; Human-Grounded compares XAI heatmaps with human annotations using Jaccard similarity and precision/recall; Application-Grounded involves domain experts assessing interpretability in asylum case prediction.
- **Core assumption:** Each tier captures a different aspect of explanation quality—fidelity to model, alignment with human reasoning, and trust from experts.
- **Evidence anchors:**
  - [abstract] "applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded"
  - [section] "Our findings demonstrate that SIDU-TXT excels in both functionally and human-grounded evaluations" and "domain experts in the sensitive legal domain"
  - [corpus] Weak - corpus neighbors discuss general XAI evaluation but not this specific three-tiered approach.
- **Break condition:** If one tier is omitted or poorly designed, overall assessment may be incomplete or misleading.

### Mechanism 3
- **Claim:** SIDU-TXT's threshold τ and top K mask selection improve interpretability by reducing noise in heatmaps.
- **Mechanism:** τ=0.5 binarizes feature masks to highlight only activations above threshold; top K=10 masks are selected based on SIDU weights, ensuring only most informative features contribute to final heatmap.
- **Core assumption:** Thresholding and mask selection reduce irrelevant features, improving clarity of explanations without losing critical information.
- **Evidence anchors:**
  - [section] "τ is a hyperparameter set experimentally to balance between interpretability and noise reduction" and "We set the threshold at τ = 0.5" and "we select the top K masks, setting K to 10"
  - [abstract] "generates heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements"
  - [corpus] Weak - corpus does not discuss this specific thresholding and selection strategy.
- **Break condition:** If τ is set too high, important features may be excluded; if too low, noise remains. Incorrect K selection can miss key features or include irrelevant ones.

## Foundational Learning

- **Concept:** Feature activation maps from convolutional layers
  - Why needed here: SIDU-TXT relies on CNN feature maps to identify influential words/phrases in text.
  - Quick check question: What information do convolutional feature maps capture in NLP tasks, and how does this differ from image tasks?

- **Concept:** Post-hoc explainability methods (e.g., LIME, Grad-CAM)
  - Why needed here: SIDU-TXT is benchmarked against these established XAI methods, so understanding their mechanisms is essential.
  - Quick check question: How do LIME and Grad-CAM generate explanations, and what are their limitations in text domains?

- **Concept:** Evaluation metrics for XAI (insertion/deletion AUC, Jaccard similarity, precision/recall)
  - Why needed here: SIDU-TXT's performance is quantitatively assessed using these metrics across different evaluation tiers.
  - Quick check question: What does a higher insertion AUC indicate about an XAI method's faithfulness?

## Architecture Onboarding

- **Component map:** Text preprocessing → CNN model (embedding + conv + pooling) → Feature activation mask generation → SIDU weight computation → Top K mask selection → Heatmap generation → Evaluation pipeline: Functional (insertion/deletion) → Human (Jaccard + precision/recall) → Application (expert assessment)

- **Critical path:** CNN model → Feature activation extraction → SIDU weight computation → Mask selection → Heatmap generation
  - Any failure here (e.g., poor CNN feature extraction) will propagate to explanation quality.

- **Design tradeoffs:**
  - Threshold τ: Lower values capture more features but increase noise; higher values reduce noise but may miss subtle signals.
  - Top K selection: Larger K includes more context but risks noise; smaller K is cleaner but may oversimplify.
  - Evaluation balance: Functional metrics alone may miss human interpretability; Human metrics alone may not capture model fidelity.

- **Failure signatures:**
  - Low insertion/deletion AUC → Explanations not faithful to model decisions.
  - Low Jaccard similarity → Explanations misaligned with human judgment.
  - Expert dissatisfaction in Application-Grounded → Explanations lack practical interpretability in high-stakes domains.

- **First 3 experiments:**
  1. Train CNN on IMDB sentiment analysis, generate SIDU-TXT heatmaps, verify top K masks align with known sentiment-bearing words.
  2. Perform insertion/deletion tests comparing SIDU-TXT AUC to LIME and Grad-CAM baselines.
  3. Collect human annotations on sample reviews, compute Jaccard similarity between human-selected words and SIDU-TXT highlights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SIDU-TXT be adapted to handle domain-specific language and terminology in high-stakes legal applications like asylum decision-making?
- Basis in paper: [explicit] The paper discusses challenges with asylum case prediction where legal text embeddings were unavailable, forcing use of generic GloVe embeddings instead of domain-specific ones.
- Why unresolved: The paper demonstrates that generic embeddings underperform in complex legal NLP tasks, but doesn't propose solutions for creating or integrating domain-specific embeddings for legal text.
- What evidence would resolve it: Comparative results showing performance improvements when using legal-domain-specific embeddings versus generic embeddings across multiple legal NLP tasks.

### Open Question 2
- Question: What is the optimal threshold selection strategy for balancing interpretability and noise reduction in SIDU-TXT heatmaps across different NLP tasks?
- Basis in paper: [explicit] The paper sets τ = 0.5 experimentally but acknowledges this may need refinement for different NLP tasks to achieve more precise interpretability.
- Why unresolved: The paper uses a fixed threshold without exploring systematic methods for threshold selection or task-specific optimization strategies.
- What evidence would resolve it: Empirical study showing how different threshold values affect explanation quality across multiple NLP tasks, with task-specific recommendations.

### Open Question 3
- Question: How can SIDU-TXT be extended to provide explanations at multiple granularities (token, phrase, sentence, document) while maintaining coherence?
- Basis in paper: [inferred] The paper generates word-level heatmaps but acknowledges the need for context-aware feature selection and discusses limitations in capturing broader contextual relationships.
- Why unresolved: The current implementation focuses on word-level explanations without addressing how to integrate multi-level explanations that preserve context while maintaining granularity.
- What evidence would resolve it: Implementation and evaluation of a multi-granular SIDU-TXT extension that successfully generates coherent explanations across different text unit levels.

## Limitations

- Underspecified SIDU weight computation mechanism with exact formulas for similarity difference and uniqueness metrics not provided
- Experimentally chosen threshold τ=0.5 and top K=10 mask selection without sensitivity analysis
- Limited success in Application-Grounded evaluation for asylum domain with expert dissatisfaction
- Weak foundational support in corpus analysis for SIDU-TXT's specific mechanisms

## Confidence

- Functional evaluation claims (AUC metrics): High confidence - results are quantifiable and directly comparable to baselines
- Human-grounded evaluation claims: Medium confidence - Jaccard similarity results are robust, but annotation process details are limited
- Application-grounded claims: Low confidence - expert assessment lacks detailed methodology and shows limited success

## Next Checks

1. Conduct sensitivity analysis on τ threshold and K mask selection to determine optimal parameter ranges and their impact on explanation quality
2. Implement ablation studies removing either similarity difference or uniqueness components to isolate their individual contributions to SIDU performance
3. Expand Application-Grounded evaluation with structured expert interviews and quantify agreement levels to better understand interpretability gaps in legal domains