---
ver: rpa2
title: Integrating Explanations in Learning LTL Specifications from Demonstrations
arxiv_id: '2404.02872'
source_url: https://arxiv.org/abs/2404.02872
tags:
- formula
- traces
- formulas
- semantics
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores integrating Large Language Models (LLMs) with
  optimization-based methods to learn Linear Temporal Logic (LTL) specifications from
  demonstrations and human explanations. The authors address the challenge of extracting
  LTL formulas from finite traces by combining the pattern recognition strengths of
  LLMs with the formal guarantees of constraint solvers.
---

# Integrating Explanations in Learning LTL Specifications from Demonstrations

## Quick Facts
- arXiv ID: 2404.02872
- Source URL: https://arxiv.org/abs/2404.02872
- Reference count: 40
- Authors: Ashutosh Gupta, John Komp, Abhay Singh Rajput, Krishna Shankaranarayanan, Ashutosh Trivedi, Namrita Varshney
- Primary result: Combining LLM-generated templates with constraint-based optimization outperforms either approach alone for learning LTL specifications

## Executive Summary
This paper presents Janaka, a method that integrates Large Language Models (LLMs) with optimization-based techniques to learn Linear Temporal Logic (LTL) specifications from demonstrations and human explanations. The approach leverages LLMs to generate candidate LTL formulas from system traces and natural language descriptions, then uses quantitative semantics as fitness functions to guide constraint-based synthesis that repairs and optimizes these candidates. Experiments on 17 case studies demonstrate that combining explanations with demonstrations improves formula quality, with Janaka producing better formulas than using LLMs alone in 12 out of 17 cases.

## Method Summary
Janaka takes as input finite system traces and natural language explanations, then uses an LLM to generate candidate LTL formulas. These candidates are evaluated using quantitative semantics (robust and discounted) to compute fitness scores. The method then creates templates by randomly replacing nodes in the parse trees of top-scoring candidates with holes, which are filled by a constraint solver (Gurobi) to synthesize optimal formulas. The approach supports both Robust and Discounted semantics for fitness evaluation, allowing flexibility in how trace satisfaction is measured. When the LLM fails to produce satisfactory formulas, the system can fall back to direct synthesis from traces.

## Key Results
- Janaka produces better LTL formulas than LLM alone in 12 out of 17 case studies
- Combining explanations with demonstrations improves formula quality compared to using demonstrations alone
- The method successfully leverages human explanations to increase hypothesis reliability in 10 out of 17 cases
- Runtime performance is competitive, with most formulas synthesized within reasonable time limits
- Both Robust and Discounted semantics are effective, with some cases favoring each approach

## Why This Works (Mechanism)
### Mechanism 1
LLM-generated candidate formulas serve as structured starting points for constraint-based repair, increasing synthesis speed over starting from scratch. The LLM generates diverse candidates capturing partial structure from demonstrations and explanations, which are converted into templates with holes filled by constraint solvers using quantitative semantics as fitness functions. This works when LLM outputs contain enough correct structural components that can be extracted and reused.

### Mechanism 2
Human explanations improve LLM reliability by constraining the hypothesis space to more relevant LTL patterns. Explanations provide semantic context that guides the LLM toward formulas matching the intended specification rather than syntactically similar but semantically different ones, reducing hallucination and improving fitness scores. This assumes the LLM can meaningfully integrate natural language explanations with demonstrations.

### Mechanism 3
Quantitative semantics provide a fitness function enabling formal guarantees through constraint solving, compensating for LLM's lack of reliability. Fitness scores computed via robust or discounted semantics guide both candidate selection and template hole filling, ensuring the final formula satisfies all demonstrations. This assumes the quantitative semantics accurately reflect formula quality and can be efficiently encoded as constraints.

## Foundational Learning
- **Concept**: Linear Temporal Logic (LTL) syntax and semantics
  - Why needed here: The entire approach operates on LTL formulas; understanding operators (G, F, X, U) and their finite trace semantics is essential for both LLM prompting and constraint encoding
  - Quick check question: What is the difference between Gp and GFp when evaluated on finite traces?

- **Concept**: Finite trace semantics for LTL
  - Why needed here: Demonstrations are finite, so the approach must use quantitative semantics (robust/discounted) rather than standard ω-trace semantics
  - Quick check question: How does the robust semantics score Fp on a trace that never contains p?

- **Concept**: Constraint solving for LTL synthesis
  - Why needed here: The repair algorithm uses Gurobi to optimize fitness scores under structural constraints derived from templates
  - Quick check question: What type of constraints (linear, integer, etc.) are used to encode LTL formula structure?

## Architecture Onboarding
- **Component map**: LLM Interface -> Candidate Formula Generator -> Template Generator -> Quantitative Semantics Engine -> Constraint Solver (Gurobi) -> Janaka Orchestrator
- **Critical path**: LLM → Template Generation → Constraint Solving → Final Formula
- **Design tradeoffs**: LLM quality vs. constraint solving time (better LLM outputs reduce solver search space but may require more LLM calls); Robust vs. discounted semantics (robust provides clearer satisfaction thresholds, discounted gives smoother gradients for optimization); Template depth parameter (deeper templates allow more complex formulas but increase solver complexity exponentially)
- **Failure signatures**: LLM produces only trivial formulas (template extraction fails, solver returns no solution); Solver timeouts (template too complex or fitness function poorly constrained); Final formula fails traces (fitness function doesn't capture all specification requirements)
- **First 3 experiments**: 1) Run with simple formula (e.g., Gp) to verify end-to-end pipeline works; 2) Test with formula requiring temporal operators (e.g., F(p → Xq)) to exercise template generation; 3) Try with complex formula (e.g., G((p ∨ q) → X((r)U(s)))) to stress-test constraint solving

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but leaves several important questions unresolved regarding scalability, robustness to imperfect explanations, optimal balance between LLM and direct synthesis, and performance differences for specifications requiring different temporal operators.

## Limitations
- The approach depends critically on the LLM's ability to generate structurally relevant LTL fragments, which may fail when outputs lack useful structure
- The experimental evaluation covers only 17 case studies, which may not be representative of the full complexity space of LTL specifications
- The paper does not provide detailed error analysis for cases where the method fails or produces suboptimal formulas

## Confidence
- **High confidence**: The core architectural integration of LLMs with constraint solvers is technically sound and the quantitative semantics implementation is well-defined
- **Medium confidence**: The claim that explanations improve LLM reliability is supported by experimental results but could benefit from more rigorous ablation studies
- **Low confidence**: The generalization claim across diverse LTL specification domains, given the limited case study set and lack of complexity analysis

## Next Checks
1. Conduct ablation studies with varying levels of explanation quality (complete, partial, noisy) to quantify the explanation contribution
2. Test the method on synthetic LTL specifications with controlled complexity to identify scaling limitations
3. Implement error analysis for the cases where Janaka performs worse than LLM alone to identify systematic failure modes