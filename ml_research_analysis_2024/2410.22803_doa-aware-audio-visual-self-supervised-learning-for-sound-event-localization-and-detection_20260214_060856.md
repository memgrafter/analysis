---
ver: rpa2
title: DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization
  and Detection
arxiv_id: '2410.22803'
source_url: https://arxiv.org/abs/2410.22803
tags:
- audio
- data
- visual
- sound
- seld
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DOA-aware audio-visual self-supervised learning
  for sound event localization and detection (SELD) in first-order ambisonics (FOA)
  recordings. The key idea is to pretrain an audio feature extractor using abundant
  unannotated spatial audio-visual data by jointly training audio and visual encoders
  with contrastive learning, where audio embeddings are extracted DOA-wise from raw
  FOA data.
---

# DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection

## Quick Facts
- arXiv ID: 2410.22803
- Source URL: https://arxiv.org/abs/2410.22803
- Reference count: 25
- Primary result: Reduced SELD error score from 36.4 to 34.9 points using DOA-aware contrastive learning

## Executive Summary
This paper proposes a DOA-aware audio-visual self-supervised learning approach for sound event localization and detection (SELD) in first-order ambisonics (FOA) recordings. The key innovation is pretraining an audio feature extractor using abundant unannotated spatial audio-visual data through contrastive learning, where audio embeddings are extracted DOA-wise from raw FOA data. The method jointly trains audio and visual encoders to maximize similarity between corresponding audio-visual pairs at each DOA, encouraging the audio encoder to learn both class and spatial information. Two contrastive learning variants are proposed: DOA-wise (maximizing DOA-specific audio-visual similarity) and recording-wise (maximizing averaged similarity over all DOAs). The experiment using DCASE2022 Task 3 dataset (20h) and unannotated audio-visual recordings (100h) demonstrated significant improvement in SELD performance.

## Method Summary
The method uses a ResNet-Conformer architecture for audio feature extraction, processing FOA spectrograms to produce frame-wise latent features. A projection head converts these features to DOA-wise audio embeddings (220 × 128 dimensions) using an adaptive average pooling layer. The visual encoder is a 9-layer R(2+1)D convolutional network that processes DOA-wise visual crops from equirectangular 360° video data. The contrastive learning framework includes two variants: DOA-wise contrastive learning that maximizes cosine similarity between audio and visual embeddings at corresponding DOAs, and recording-wise contrastive learning that maximizes averaged DOA-wise similarity over entire recordings. Spatial rotation augmentation is applied to visual data to generate negative samples and encourage DOA information extraction. After pretraining for 50 epochs, the model is fine-tuned on the SELD task using an ACCDOA representation with permutation-invariant training.

## Key Results
- SELD error score improved from 36.4 to 34.9 points using the proposed method
- DOA-wise contrastive learning reduced ER≤20° from 0.629 to 0.571
- Recording-wise contrastive learning achieved LE of 22.3 degrees
- Both variants outperformed conventional supervised learning baselines

## Why This Works (Mechanism)

### Mechanism 1
Jointly extracting DOA-wise audio embeddings from raw FOA data without enhancement improves SELD by forcing the encoder to learn spatial features. The DOA-aware contrastive loss aligns audio embeddings with visual embeddings for each DOA, encouraging the audio encoder to represent both class and DOA information. This works because sound objects are concurrently observed by FOA microphones and omni-directional camera, and meaningful sound events exist only in a small number of DOAs.

### Mechanism 2
Recording-wise contrastive learning mitigates the problem of local contrastive learning by using averaged DOA-wise similarities across all DOAs. Global similarity measures the correspondence between entire recordings rather than individual DOAs, making the method more robust to silent objects or occluded sound sources. This approach works because meaningful sound events exist only in a small number of DOAs, limiting local contrastive learning based on DOA-wise similarity.

### Mechanism 3
Data augmentation by randomly rotating equirectangular visual data generates negative samples and encourages the audio encoder to extract DOA information. Rotating visual data while keeping audio fixed creates mismatched DOA conditions, forcing the audio encoder to learn spatial features rather than just class features. This works because cross-modal co-occurrence between sound and appearance doesn't hold for visible silent objects and occluded sound objects.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The entire self-supervised learning framework relies on maximizing similarity between positive samples and minimizing it between negative samples
  - Quick check question: What happens to the InfoNCE loss if the temperature parameter τ is set too high?

- Concept: Direction of Arrival (DOA) estimation and spatial audio processing
  - Why needed here: The method requires understanding how to extract DOA-wise features from FOA audio data and align them with visual data
  - Quick check question: How does the Fibonacci lattice provide a good distribution of DOA points for this application?

- Concept: Audio-visual cross-modal correspondence
  - Why needed here: The method assumes sound objects are concurrently observed by audio and visual sensors, requiring understanding of when this assumption breaks down
  - Quick check question: What types of sound events would violate the assumption that sound objects are visually observable?

## Architecture Onboarding

- Component map: Raw FOA audio → Audio feature extractor A → Projection head H → DOA-wise embeddings → Contrastive loss → Pretraining → Fine-tuning with H' → SELD output

- Critical path: Raw FOA audio → Audio feature extractor A → Projection head H → DOA-wise embeddings → Contrastive loss → Pretraining → Fine-tuning with H' → SELD output

- Design tradeoffs:
  - DOA grid resolution (K=220) vs computational cost
  - Frame resolution (Tv=16) vs temporal localization accuracy
  - Visual crop size (16×16) vs spatial detail preservation
  - Contrastive learning variant (DOA-wise vs recording-wise) vs robustness to data limitations

- Failure signatures:
  - Poor localization performance indicates insufficient DOA information in audio embeddings
  - Degraded detection performance suggests domain mismatch between pretraining and fine-tuning data
  - Overfitting during pretraining indicates insufficient negative samples or too small batch size

- First 3 experiments:
  1. Verify DOA-wise contrastive learning by checking if audio embeddings cluster by DOA when visualized with t-SNE
  2. Test recording-wise contrastive learning by measuring similarity scores between matched vs mismatched recordings
  3. Validate the spatial rotation augmentation by confirming that rotated negative samples have lower similarity scores than positive samples

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed DOA-aware self-supervised learning method compare to traditional supervised learning methods for SELD when a large amount of annotated data is available? The paper demonstrates that the proposed method outperforms conventional methods when limited annotated data is available but does not compare performance with traditional supervised learning methods when a large amount of annotated data is available.

### Open Question 2
What is the impact of background music in the pretraining dataset on the localization of sound events related to music? The paper mentions that background music in the pretraining dataset (YT-360) did not spatially correspond to the paired visual data, leading to a significant degradation in localization performance for musical instruments and bell classes, but does not provide a detailed analysis of this impact.

### Open Question 3
How does the proposed method perform when applied to real-world scenarios with varying acoustic environments and sound event classes? The paper evaluates the proposed method on the STARSS22 and Synth1 datasets, which may not fully represent real-world scenarios with varying acoustic environments and sound event classes.

## Limitations
- Domain adaptation gap between pretraining YT-360 dataset (containing diverse 360° video content) and fine-tuning STARSS22 dataset (focused on spatial audio-visual soundscapes)
- Background music in pretraining data negatively impacts localization of music-related sound events
- The paper doesn't provide direct comparison of when DOA-wise vs recording-wise contrastive learning performs better

## Confidence
- High confidence: The mechanism of DOA-wise audio-visual contrastive learning is well-supported by experimental results showing consistent improvement across multiple evaluation metrics
- Medium confidence: The recording-wise contrastive learning variant shows promise but the choice between variants is somewhat heuristic
- Medium confidence: The spatial rotation data augmentation is theoretically sound but its independent contribution is not quantified

## Next Checks
1. Measure SELD performance on STARSS22 when pretrained on different subsets of YT-360 filtered by audio-visual content similarity to STARSS22
2. Quantify the individual contribution of spatial rotation augmentation by comparing DOA-wise contrastive learning with and without this data augmentation
3. Verify that audio embeddings actually cluster by DOA when visualized with t-SNE, confirming that the contrastive learning forces DOA information into the audio representations