---
ver: rpa2
title: On adversarial training and the 1 Nearest Neighbor classifier
arxiv_id: '2404.06313'
source_url: https://arxiv.org/abs/2404.06313
tags:
- training
- adversarial
- accuracy
- classifier
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares adversarial training with the simple 1-Nearest
  Neighbor (1NN) classifier for improving adversarial robustness in deep learning
  models. While adversarial training has shown progress, it is computationally expensive,
  sensitive to hyperparameters, and often only improves robustness to specific types
  of perturbations seen during training.
---

# On adversarial training and the 1 Nearest Neighbor classifier

## Quick Facts
- **arXiv ID:** 2404.06313
- **Source URL:** https://arxiv.org/abs/2404.06313
- **Reference count:** 25
- **Key outcome:** This paper compares adversarial training with the simple 1-Nearest Neighbor (1NN) classifier for improving adversarial robustness in deep learning models.

## Executive Summary
This paper challenges the prevailing paradigm of complex adversarial training methods by demonstrating that the simple 1-Nearest Neighbor (1NN) classifier can outperform state-of-the-art approaches in terms of adversarial robustness. The authors prove that under reasonable assumptions, 1NN achieves high adversarial accuracy on test images as the number of training examples increases. Through extensive experiments across 135 binary image classification problems from CIFAR10, MNIST, and Fashion-MNIST, 1NN outperforms TRADES (a leading adversarial training algorithm) and nearly all models from the current adversarial robustness leaderboard. The results suggest that modern adversarial training methods still fall short of the robustness achievable by this simple classifier.

## Method Summary
The paper introduces a novel comparison between adversarial training methods and the 1-Nearest Neighbor (1NN) classifier. The authors establish theoretical guarantees for 1NN's robustness under reasonable assumptions, showing that it will be robust to any small image perturbation of training images. They then conduct extensive experiments across 135 binary classification problems derived from CIFAR10, MNIST, and Fashion-MNIST datasets. The experiments compare 1NN against TRADES (a state-of-the-art adversarial training algorithm) and benchmark it against 69 robust models from the current adversarial robustness leaderboard. The comparison focuses on average adversarial accuracy against varied attackers and robustness to perturbations slightly different from those used during training.

## Key Results
- 1NN outperforms TRADES in terms of average adversarial accuracy across 135 binary classification problems
- 1NN outperforms nearly all 69 robust models from the current adversarial robustness leaderboard in terms of robustness to perturbations slightly different from training
- 1NN achieves theoretical guarantees of robustness to any small image perturbation under reasonable assumptions

## Why This Works (Mechanism)
The 1-Nearest Neighbor classifier achieves superior robustness through its inherent property of interpolating between training examples rather than learning complex decision boundaries. When an image is perturbed slightly, the nearest neighbor in the training set remains the same, ensuring consistent predictions. This geometric stability provides a form of built-in robustness that doesn't require the computationally expensive adversarial training process. The mechanism relies on the assumption that in high-dimensional spaces, small perturbations don't change the nearest neighbor relationship, which becomes more reliable as the number of training examples increases.

## Foundational Learning
- **Adversarial training**: The process of training models to be robust against adversarial examples by including perturbed examples in the training data. Needed to understand why 1NN can be competitive without this computationally expensive process.
- **Nearest neighbor classification**: A non-parametric method that classifies based on the closest training example. Key to understanding 1NN's inherent robustness mechanism.
- **Adversarial robustness**: The ability of a model to maintain performance under adversarial attacks. Central concept being evaluated in the paper.
- **Binary classification**: The task of classifying examples into one of two classes. The paper focuses on this simplified problem setting.
- **Perturbation sensitivity**: How model predictions change under small input modifications. Critical for evaluating robustness claims.
- **High-dimensional geometry**: Understanding how distances and nearest neighbors behave in high-dimensional spaces. Important for the theoretical analysis of 1NN's robustness.

## Architecture Onboarding

**Component Map:** Training data -> Distance computation -> Nearest neighbor selection -> Classification output

**Critical Path:** Input image → Distance calculation to all training examples → Identification of nearest neighbor → Output of neighbor's label

**Design Tradeoffs:** The paper trades computational efficiency during training (1NN requires no training) for potentially higher computational cost during inference (requires computing distances to all training examples). It also sacrifices model interpretability and the ability to learn complex decision boundaries in favor of inherent robustness.

**Failure Signatures:** 1NN will fail when:
- Training data contains mislabeled examples
- Multiple classes are present in the same local region of feature space
- The number of training examples is too small to provide adequate coverage
- The assumption of small perturbations is violated with large adversarial attacks

**First Experiments:**
1. Implement 1NN on CIFAR-10 binary classification problems and measure standard accuracy
2. Apply PGD attacks to 1NN and compare adversarial accuracy with TRADES
3. Test 1NN's robustness to perturbations slightly different from training perturbations

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on "reasonable assumptions" that are not fully specified or validated
- Experiments limited to binary classification problems, which is a significant simplification
- Performance may not generalize to more complex multi-class classification scenarios
- Focuses primarily on robustness metrics while potentially overlooking other important aspects like computational efficiency during inference

## Confidence
- High confidence in the computational inefficiency of modern adversarial training methods
- Medium confidence in the comparative robustness results between 1NN and TRADES
- Low confidence in the generalizability of theoretical claims about 1NN's universal robustness

## Next Checks
1. Test 1NN's robustness on multi-class classification problems across diverse datasets beyond CIFAR-10, MNIST, and Fashion-MNIST
2. Evaluate the computational cost of 1NN at inference time compared to adversarially trained models for practical deployment scenarios
3. Investigate how 1NN performs when the assumption of "small image perturbations" is violated with larger or more structured adversarial attacks