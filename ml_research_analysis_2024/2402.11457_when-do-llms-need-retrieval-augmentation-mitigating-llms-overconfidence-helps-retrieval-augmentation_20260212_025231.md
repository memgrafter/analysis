---
ver: rpa2
title: When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps
  Retrieval Augmentation
arxiv_id: '2402.11457'
source_url: https://arxiv.org/abs/2402.11457
tags:
- llms
- retrieval
- augmentation
- knowledge
- explain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of Large Language Models (LLMs) being
  overconfident in their factual knowledge boundaries, leading to hallucinations and
  incorrect answers. The authors propose methods to enhance LLMs' perception of their
  knowledge boundaries by reducing overconfidence, which in turn helps with adaptive
  retrieval augmentation.
---

# When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation

## Quick Facts
- arXiv ID: 2402.11457
- Source URL: https://arxiv.org/abs/2402.11457
- Authors: Shiyu Ni; Keping Bi; Jiafeng Guo; Xueqi Cheng
- Reference count: 8
- Primary result: Enhancing LLMs' perception of knowledge boundaries by reducing overconfidence improves retrieval augmentation efficiency

## Executive Summary
This paper investigates why Large Language Models (LLMs) struggle with effective retrieval augmentation, identifying overconfidence in their factual knowledge boundaries as the primary culprit. The authors demonstrate that LLMs with better perception of their knowledge limits retrieve external information more appropriately, leading to improved performance with fewer retrieval calls. They propose methods to mitigate overconfidence, including prompting LLMs to be more prudent and improving their ability to provide correct answers. Through extensive experiments across three datasets, they show that these approaches enhance knowledge boundary perception and achieve comparable or better retrieval augmentation performance while significantly reducing the number of external retrievals needed.

## Method Summary
The authors develop a framework to quantify LLMs' perception of their knowledge boundaries by measuring the correlation between their certainty about internal knowledge and their reliance on external information. They observe a negative correlation, indicating that overconfident models retrieve less effectively. To address this, they propose several methods including prompting techniques that urge prudence and answer improvement strategies. These methods are evaluated across three datasets, showing that mitigating overconfidence leads to better knowledge boundary perception and more efficient retrieval augmentation. The framework includes both quantitative measurements of knowledge boundary perception and practical implementation strategies for improving retrieval decisions.

## Key Results
- LLMs exhibit negative correlation between certainty about internal knowledge and reliance on external information
- Overconfidence is identified as the primary reason for unsatisfactory knowledge boundary perception
- Proposed methods (prudent prompting, answer improvement) effectively enhance knowledge boundary perception
- Enhanced perception leads to comparable or better retrieval augmentation performance with significantly fewer retrieval calls

## Why This Works (Mechanism)
The paper's mechanism centers on the relationship between knowledge boundary perception and retrieval decisions. When LLMs are overconfident about their internal knowledge, they fail to recognize when they need external information, leading to poor retrieval augmentation. By reducing this overconfidence through targeted interventions, models become more accurate in assessing their limitations and make better decisions about when to retrieve. This creates a more efficient loop where retrieval is called upon only when truly needed, reducing computational overhead while maintaining or improving accuracy.

## Foundational Learning

### Knowledge Boundary Perception
- **Why needed**: Understanding when LLMs recognize their knowledge limits is crucial for effective retrieval augmentation
- **Quick check**: Measure correlation between model certainty and retrieval reliance

### Overconfidence Quantification
- **Why needed**: Identifying the degree of overconfidence helps target interventions effectively
- **Quick check**: Compare model confidence scores against ground truth accuracy

### Retrieval Decision Making
- **Why needed**: Understanding when models choose to retrieve determines augmentation efficiency
- **Quick check**: Analyze retrieval patterns across different confidence thresholds

## Architecture Onboarding

### Component Map
LLM Model -> Knowledge Boundary Assessment -> Retrieval Decision Module -> External Knowledge Base -> Response Generation

### Critical Path
1. Model generates response and confidence score
2. Knowledge boundary assessment evaluates if retrieval is needed
3. Retrieval decision module either calls external knowledge or proceeds with internal knowledge
4. Response is generated using appropriate knowledge source

### Design Tradeoffs
- **Threshold sensitivity**: Higher thresholds reduce retrieval calls but may miss needed information
- **Confidence calibration**: More accurate confidence scores improve retrieval decisions but require additional training
- **Prudence vs efficiency**: More cautious models retrieve more but may be slower

### Failure Signatures
- **Over-retrieval**: Model retrieves even when internal knowledge is sufficient (underconfident)
- **Under-retrieval**: Model fails to retrieve when external knowledge is needed (overconfident)
- **Threshold instability**: Performance varies significantly with small threshold changes

### Exactly 3 First Experiments
1. Baseline correlation measurement between certainty and retrieval reliance
2. A/B test comparing original model vs. prudently prompted version
3. Retrieval efficiency analysis measuring performance vs. number of retrieval calls

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability and generalizability of overconfidence mitigation techniques to diverse domains and larger applications remain unclear
- Causal claim that overconfidence is the "primary reason" for poor knowledge boundary perception lacks systematic validation of other potential factors
- Results are based on closed-book QA settings, leaving open how methods perform on open-ended generation tasks

## Confidence
- **High confidence**: Negative correlation between certainty and retrieval reliance
- **Medium confidence**: Effectiveness of proposed mitigation techniques (limited ablation studies)
- **Medium confidence**: Claim that fewer retrieval calls achieve comparable performance (implementation dependent)

## Next Checks
1. Test proposed methods on open-ended generation tasks beyond closed-book QA to assess generalizability
2. Conduct ablation studies to isolate which overconfidence mitigation technique contributes most to performance gains
3. Evaluate model performance across a wider range of domains and dataset sizes to test scalability limits