---
ver: rpa2
title: 'VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates
  for Robot Planning'
arxiv_id: '2410.23156'
source_url: https://arxiv.org/abs/2410.23156
tags:
- block
- robot
- effects
- predicates
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neuro-Symbolic Predicates (NSPs), a novel
  abstraction language that combines neural vision-language models with symbolic programming
  to enable robots to learn task-specific abstractions from online interaction. The
  approach interleaves predicate invention, operator learning, and goal-driven exploration
  to build abstract world models for planning.
---

# VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning

## Quick Facts
- arXiv ID: 2410.23156
- Source URL: https://arxiv.org/abs/2410.23156
- Authors: Yichao Liang; Nishanth Kumar; Hao Tang; Adrian Weller; Joshua B. Tenenbaum; Tom Silver; João F. Henriques; Kevin Ellis
- Reference count: 40
- Primary result: Combines VLMs with symbolic predicates to learn abstract world models from minimal interaction data, achieving near-oracle performance in five simulated robotic domains

## Executive Summary
VisualPredicator introduces Neuro-Symbolic Predicates (NSPs) that combine neural vision-language models with symbolic programming to enable robots to learn task-specific abstractions from online interaction. The approach interleaves predicate invention, operator learning, and goal-driven exploration to build abstract world models for planning. Evaluated across five simulated robotic domains, NSPs achieve near-oracle performance in task success rate and planning efficiency, significantly outperforming traditional hierarchical reinforcement learning methods and VLM-only planning baselines.

## Method Summary
The method learns abstract world models through an online process that interleaves three components: predicate invention using VLMs with discrimination, transition modeling, and unconditional generation strategies; predicate selection using classification accuracy with simplicity bias; and operator learning with optimistic preconditions. NSPs are Python code snippets that invoke VLMs for perceptual queries and manipulate results algorithmically. The system builds abstract state representations and transition functions, then uses symbolic planners to generate high-level plans executed as sequences of low-level motor skills. The learning process is goal-driven, collecting data through exploration while refining the abstraction.

## Key Results
- Achieved near-oracle performance in five simulated robotic domains (Cover, Blocks, Coffee, Cover Heavy, Balance)
- Significantly outperformed both traditional HRL methods and VLM-only planning baselines in task success rate and planning efficiency
- Demonstrated strong generalization to out-of-distribution tasks, particularly in the Coffee domain with unseen object arrangements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving predicate invention, operator learning, and goal-driven exploration enables efficient learning from minimal interaction data
- Mechanism: The algorithm alternates between proposing new predicates based on collected data, selecting predicates using classification accuracy with simplicity bias, and learning high-level actions that model transitions between abstract states
- Core assumption: Classification accuracy objective with simplicity bias effectively identifies useful predicates even without satisficing plans in early iterations
- Evidence anchors: [abstract]: "interleaves predicate invention, operator learning, and goal-driven exploration"; [section]: predicate invention strategies and greedy best-first search

### Mechanism 2
- Claim: NSPs combine perceptually-grounded predicates with logically-rich, composable abstractions to enable effective learning from minimal interaction data
- Mechanism: NSPs are Python snippets that invoke VLMs for querying perceptual properties and algorithmically manipulate those properties using Python
- Core assumption: VLMs can reliably evaluate natural language assertions embedded in NSPs when given properly formatted input with visual attention cues
- Evidence anchors: [abstract]: "combines the strengths of symbolic and neural knowledge representations"; [section]: NSP API description showing VLM integration

### Mechanism 3
- Claim: Extended operator learning algorithm with optimistic preconditions enables efficient exploration and better generalization
- Mechanism: Operator learning maximizes an objective ensuring all data in transition dataset is modeled by one and only one high-level action while minimizing syntactic complexity
- Core assumption: Optimistic preconditions that minimize syntactic complexity lead to better exploration efficiency and generalization than restrictive preconditions
- Evidence anchors: [section]: precondition learner description and comparison with cluster and intersect operator learner

## Foundational Learning

- Concept: Predicate invention strategies (discrimination, transition modeling, unconditional generation)
  - Why needed here: Different strategies target different aspects - preconditions, postconditions, and logical extensions respectively
  - Quick check question: What are the three predicate invention strategies and what aspect of learning does each target?

- Concept: Abstract state representation and transition function
  - Why needed here: Forms the core of planning problem, allowing reasoning about high-level actions and their effects
  - Quick check question: How is the abstract state defined and how does the transition function operate on it?

- Concept: Hierarchical planning with abstract world models
  - Why needed here: Enables generation of high-level plans using learned abstract world model, executed as sequences of low-level skills
  - Quick check question: What are the two types of planning failures that can occur and how are they handled?

## Architecture Onboarding

- Component map: Environment interface -> Predicate invention module -> Predicate selection module -> Operator learning module -> Planning module -> Execution module -> Data collection -> Repeat
- Critical path: Predicate invention → Predicate selection → Operator learning → Planning → Execution → Data collection → Repeat
- Design tradeoffs:
  - VLM accuracy vs. predicate complexity: More complex predicates may be harder for VLMs to evaluate reliably
  - Optimistic vs. restrictive preconditions: Optimistic preconditions enable better exploration but may create overly permissive world models
  - Classification accuracy vs. planning efficiency: Early predicate selection focuses on classification accuracy, later iterations may prioritize planning efficiency
- Failure signatures:
  - VLMs consistently failing to evaluate predicates correctly
  - Predicate selection process getting stuck in local optima
  - Operator learning creating overly restrictive or overly permissive preconditions
  - Planning module failing to generate feasible plans even with learned abstractions
- First 3 experiments:
  1. Implement predicate invention strategies and test them on simple environments to verify VLM integration works
  2. Test predicate selection module with synthetic data to ensure it can identify useful predicates
  3. Implement operator learning algorithm and test it on environments with known ground truth abstractions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system handle perceptual ambiguities in NSP evaluation, such as distinguishing between objects that appear similar in images or determining object properties that are not visually apparent?
- Basis in paper: [explicit] The paper acknowledges that no VLM is 100% accurate and discusses using previous actions and visual labels to improve NSP accuracy, but doesn't fully address handling visually similar objects or non-visual properties
- Why unresolved: The paper mentions mitigation strategies but doesn't provide a comprehensive solution for all types of perceptual ambiguities
- What evidence would resolve it: Experimental results showing system performance in environments with visually similar objects or when determining properties that are not visually apparent

### Open Question 2
- Question: How can the system be extended to handle partially observable environments where the agent cannot see the entire scene at once?
- Basis in paper: [inferred] The paper assumes object-centric state representation and focuses on learning predicates from full observations, without discussing partial observability
- Why unresolved: Current framework relies on complete scene information, which is not always possible in real-world scenarios
- What evidence would resolve it: Extension of NSP framework to handle partial observability with experimental results in partially observable environments

### Open Question 3
- Question: How can the system be scaled to handle more complex tasks that require longer planning horizons or more sophisticated reasoning?
- Basis in paper: [inferred] The paper evaluates on relatively simple tasks with short planning horizons, without discussing performance on more complex tasks
- Why unresolved: Current experiments focus on simpler domains, unclear how well system would scale to more challenging scenarios
- What evidence would resolve it: Experimental results on more complex tasks with longer planning horizons and analysis of performance limitations

## Limitations
- Reliance on VLMs for predicate grounding introduces potential brittleness if model accuracy degrades
- Assumes access to predefined motor skills, limiting applicability to domains where such skills can be easily defined
- Evaluation confined to simulated environments, leaving open questions about real-world robotics performance

## Confidence

- **High confidence**: The interleaving mechanism for predicate invention, operator learning, and exploration is well-supported by algorithmic description and ablation studies
- **Medium confidence**: The claim of near-oracle performance is supported by quantitative results, but comparison methodology could be more rigorous
- **Low confidence**: The assertion of strong out-of-distribution generalization is based on limited evidence from Coffee domain experiments

## Next Checks

1. **VLM reliability testing**: Systematically evaluate how predicate evaluation accuracy degrades under varying conditions (image quality, predicate complexity, VLM model variations) to quantify brittleness of perceptual grounding mechanism

2. **Skill definition dependency**: Test method's performance when motor skills are learned from demonstration data rather than provided, measuring impact on predicate invention quality and overall planning success

3. **Real-world transfer validation**: Implement method on physical robot platform with appropriate skill primitives to assess performance degradation compared to simulation and identify practical limitations not captured in virtual environments