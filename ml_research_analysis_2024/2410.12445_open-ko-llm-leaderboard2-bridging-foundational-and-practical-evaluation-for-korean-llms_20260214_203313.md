---
ver: rpa2
title: 'Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation for
  Korean LLMs'
arxiv_id: '2410.12445'
source_url: https://arxiv.org/abs/2410.12445
tags:
- season
- open
- ko-llm
- korean
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Open Ko-LLM Leaderboard was redesigned to address the gap between
  academic benchmark performance and real-world model capabilities for Korean LLMs.
  Season 1 relied heavily on translated English benchmarks, resulting in scores that
  poorly predicted practical usability.
---

# Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation for Korean LLMs

## Quick Facts
- arXiv ID: 2410.12445
- Source URL: https://arxiv.org/abs/2410.12445
- Reference count: 3
- Primary result: Season 2 redesign created Korean-specific benchmarks that poorly correlated (0.36) with Season 1 translated English benchmarks

## Executive Summary
The Open Ko-LLM Leaderboard underwent a significant redesign from Season 1 to Season 2 to address the gap between academic benchmark performance and real-world model capabilities for Korean LLMs. The original leaderboard relied heavily on translated English benchmarks, resulting in scores that poorly predicted practical usability. Season 2 replaced all benchmarks with nine new tasks focused on Korean-specific language and real-world scenarios, including four newly created native Korean benchmarks. The evaluation methods were expanded to include both logit-based and generation-based approaches to better reflect actual usage patterns.

## Method Summary
The redesign involved replacing all Season 1 benchmarks with nine new Korean-specific tasks, four of which were newly created native Korean benchmarks. The evaluation methodology was expanded to use both logit-based and generation-based methods to capture different aspects of model performance. The new benchmarks were specifically designed to address Korean language characteristics and real-world usage scenarios, moving away from translated English benchmarks that had poor correlation with practical usability.

## Key Results
- Season 2 scores showed low correlation with Season 1 scores, indicating a fundamental shift in evaluation approach
- Correlation between Season 1 and Season 2 generation tasks was only 0.36, demonstrating different measurement dimensions
- The new Korean-specific benchmarks better captured real-world language usage patterns compared to translated English benchmarks
- Four new native Korean benchmarks were created specifically for Season 2 evaluation

## Why This Works (Mechanism)
The redesign works by directly addressing the core problem of translated benchmarks failing to capture Korean language nuances and real-world usage patterns. By creating native Korean benchmarks and using generation-based evaluation methods, the new leaderboard better reflects how models actually perform in practical scenarios. The dual evaluation approach (logit-based and generation-based) captures both theoretical capability and practical output quality.

## Foundational Learning
- Korean language characteristics - why needed: Korean has unique grammatical structures and honorific systems that don't translate well; quick check: benchmarks must test these features specifically
- Benchmark translation limitations - why needed: direct translation often loses cultural and linguistic context; quick check: native speaker validation of benchmark quality
- Evaluation method selection - why needed: different methods capture different aspects of model capability; quick check: correlation analysis between evaluation approaches

## Architecture Onboarding
- Component map: Task Design -> Benchmark Creation -> Evaluation Method Selection -> Score Calculation -> Leaderboard Publication
- Critical path: Korean-specific task identification → Native benchmark development → Dual-method evaluation → Correlation analysis
- Design tradeoffs: Native benchmarks provide better real-world relevance but require more development effort; dual methods increase evaluation complexity but improve accuracy
- Failure signatures: Low inter-task correlation suggests inconsistent evaluation; high translation dependency indicates poor Korean language capture
- First experiments: 1) Correlation analysis between Season 1 and Season 2 scores; 2) User studies validating benchmark relevance; 3) Inter-task consistency testing within Season 2

## Open Questions the Paper Calls Out
None

## Limitations
- Low correlation between Season 1 and Season 2 scores suggests fundamental measurement differences rather than improvements
- Selection of nine tasks may not comprehensively represent all real-world Korean language usage patterns
- Dual evaluation methods introduce potential inconsistency in cross-task model comparisons

## Confidence
- High confidence in observation that translated English benchmarks poorly predict real-world performance
- Medium confidence in claim that native Korean benchmarks better capture real-world capabilities
- Medium confidence in overall design improvement claim pending practical impact validation

## Next Checks
1. Conduct user studies with Korean language practitioners to validate whether Season 2 benchmark performance predicts real-world task success better than Season 1 scores did.

2. Test inter-task consistency within Season 2 by calculating correlations between different task scores to ensure they measure related aspects of Korean language capability.

3. Perform ablation studies by systematically removing individual Season 2 tasks to determine which specific benchmarks contribute most to the claimed improvement in practical evaluation.