---
ver: rpa2
title: Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned
  LLMs
arxiv_id: '2403.00858'
source_url: https://arxiv.org/abs/2403.00858
tags:
- draft
- target
- arxiv
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a training framework for creating draft models
  specifically for speculative decoding with chat-fine-tuned large language models
  (LLMs). The framework involves pretraining a small draft model, generating a distillation
  dataset using a target LLM, and fine-tuning the draft model using a novel Total
  Variation Distance++ (TVD++) loss.
---

# Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

## Quick Facts
- arXiv ID: 2403.00858
- Source URL: https://arxiv.org/abs/2403.00858
- Authors: Raghavv Goel; Mukul Gagrani; Wonseok Jeon; Junyoung Park; Mingu Lee; Christopher Lott
- Reference count: 9
- Primary result: 115M parameter draft model achieves 2.3 block efficiency and 2.4× speed-up for Llama 2 Chat 7B

## Executive Summary
This paper presents a training framework for creating draft models specifically for speculative decoding with chat-fine-tuned large language models. The framework involves pretraining a small draft model, generating a distillation dataset using a target LLM, and fine-tuning the draft model using a novel Total Variation Distance++ (TVD++) loss. The authors train a 115M parameter draft model (Llama 2 Chat Drafter 115M) for Llama 2 Chat 7B, achieving up to 2.3 block efficiency and 2.4× speed-up relative to autoregressive decoding on tasks like open-ended generation and text summarization. The proposed TVD++ loss outperforms other distillation losses like KLD and TVD.

## Method Summary
The paper proposes a three-phase pipeline for training draft models for speculative decoding with chat-fine-tuned LLMs. First, a small draft model (115M parameters) is pretrained on a 600B-token English corpus using next token prediction. Second, a distillation dataset is generated by having the target Llama 2 Chat 7B model respond to seed instructions from OIG-small-chip2 and OpenAssistant datasets with various temperature and top-p settings. Third, the pretrained draft model is fine-tuned on this distillation dataset using knowledge distillation with the proposed TVD++ loss, which incorporates variance reduction techniques inspired by reinforcement learning policy gradients.

## Key Results
- 115M parameter Llama 2 Chat Drafter achieves 2.3 block efficiency and 2.4× speed-up for Llama 2 Chat 7B
- TVD++ loss outperforms traditional distillation losses (KLD, TVD) in terms of alignment quality
- Strong performance across open-ended generation (Databricks-dolly-15k), extreme summarization (XSum), and news summarization (CNN/DailyMail) tasks

## Why This Works (Mechanism)

### Mechanism 1
Training a small draft model to align closely with a large chat-fine-tuned LLM enables speculative decoding speedup. The draft model is pretrained on a large corpus to gain general language modeling capabilities, then fine-tuned using a distillation dataset generated by the target LLM. The TVD++ loss function, inspired by reinforcement learning, optimizes the alignment between draft and target distributions. The core assumption is that the smaller draft model can approximate the target LLM's behavior well enough for speculative decoding to be effective.

### Mechanism 2
Using a novel Total Variation Distance++ (TVD++) loss improves the alignment between draft and target models more effectively than traditional losses. The TVD++ loss incorporates variance reduction techniques from reinforcement learning, normalizing the reward and providing better learning signals for draft model fine-tuning. The core assumption is that normalizing the reward in the distillation loss leads to more stable and effective training of the draft model.

### Mechanism 3
Generating a distillation dataset by having the target model produce responses to seed instructions from open-source datasets provides a plausible data distribution for training the draft model. The target model generates responses to seed instructions with various configurations (temperature, top-p, system prompts), creating a diverse dataset that mimics the target model's behavior in realistic contexts. The core assumption is that the generated responses by the target model are representative of the data distribution the target model was trained on.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The draft model needs to learn the behavior of the larger target LLM without access to the original training data.
  - Quick check question: What is the difference between token-level and distribution-level knowledge distillation?

- Concept: Reinforcement Learning and Policy Gradients
  - Why needed here: The TVD++ loss is inspired by variance reduction techniques from reinforcement learning, specifically the advantage normalization.
  - Quick check question: How does advantage normalization in policy gradients help reduce variance in training?

- Concept: Speculative Decoding
  - Why needed here: The goal of training the draft model is to accelerate inference of the target LLM using speculative decoding.
  - Quick check question: What is the role of the draft model in speculative decoding, and how does it interact with the target model?

## Architecture Onboarding

- Component map: Target LLM -> Pretraining Dataset -> Pretrained Draft Model -> Distillation Dataset -> TVD++ Loss Function -> Fine-tuned Draft Model
- Critical path: 1. Pretrain the draft model on a large corpus. 2. Generate the distillation dataset using the target LLM. 3. Fine-tune the draft model using the distillation dataset and TVD++ loss. 4. Evaluate the draft model's performance in speculative decoding scenarios.
- Design tradeoffs: Model size vs. performance: Smaller draft models may be less effective at approximating the target LLM but offer greater speedup. Dataset size and diversity vs. training time: Larger and more diverse distillation datasets may improve draft model performance but require more time to generate and train on. Loss function complexity vs. training stability: More complex loss functions like TVD++ may provide better performance but can be harder to train and tune.
- Failure signatures: Low block efficiency: The draft model's tokens are frequently rejected by the target model, indicating poor alignment. No speedup in speculative decoding: The overhead of using the draft model outweighs the benefits, suggesting the draft model is not effective enough. Training instability: If the TVD++ loss leads to unstable training, it may indicate issues with the variance reduction techniques or learning rate.
- First 3 experiments: 1. Evaluate the draft model's performance with different block lengths (γ) to find the optimal configuration for speedup. 2. Compare the TVD++ loss to traditional distillation losses (KLD, TVD) on a held-out dataset to validate its effectiveness. 3. Test the draft model's performance on out-of-distribution tasks to assess its generalization capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TVD++ loss compare to other advanced distillation losses (e.g., F-divergence, Jensen-Shannon) in terms of block efficiency and speed-up across various tasks?
- Basis in paper: The paper proposes TVD++ loss and compares it to KLD and TVD, but does not compare it to other advanced distillation losses like F-divergence or Jensen-Shannon.
- Why unresolved: The paper only compares TVD++ to KLD and TVD, leaving the question of how it performs against other advanced distillation losses unanswered.
- What evidence would resolve it: Conducting experiments using TVD++ and other advanced distillation losses (e.g., F-divergence, Jensen-Shannon) on various tasks and comparing their performance in terms of block efficiency and speed-up.

### Open Question 2
- Question: How does the proposed training framework generalize to other large language model families beyond Llama 2, such as GPT, BERT, or T5?
- Basis in paper: The paper focuses on training a draft model for Llama 2 Chat 7B using the proposed framework, but does not discuss its applicability to other large language model families.
- Why unresolved: The paper does not provide any insights into how the proposed framework would perform when applied to other large language model families, leaving its generalizability an open question.
- What evidence would resolve it: Applying the proposed training framework to other large language model families (e.g., GPT, BERT, T5) and evaluating the performance of the resulting draft models in terms of block efficiency and speed-up.

### Open Question 3
- Question: How does the proposed training framework handle the trade-off between draft model size and performance, and what is the optimal size for a draft model given a specific target model and hardware constraints?
- Basis in paper: The paper presents a 115M parameter draft model for Llama 2 Chat 7B, but does not discuss the impact of draft model size on performance or provide guidelines for determining the optimal size given specific target models and hardware constraints.
- Why unresolved: The paper does not explore the relationship between draft model size and performance, nor does it provide any insights into how to determine the optimal size for a draft model given a specific target model and hardware constraints.
- What evidence would resolve it: Conducting experiments with draft models of varying sizes for different target models and hardware constraints, and analyzing the relationship between draft model size, performance, and optimal size determination.

### Open Question 4
- Question: How does the proposed training framework perform when applied to multilingual large language models, and what are the challenges and opportunities in training draft models for such models?
- Basis in paper: The paper focuses on training a draft model for a monolingual large language model (Llama 2 Chat 7B), but does not discuss the applicability of the proposed framework to multilingual large language models or the associated challenges and opportunities.
- Why unresolved: The paper does not provide any insights into how the proposed training framework would perform when applied to multilingual large language models or discuss the challenges and opportunities in training draft models for multilingual large language models.
- What evidence would resolve it: Applying the proposed training framework to multilingual large language models and evaluating the performance of the resulting draft models, while also identifying and discussing the challenges and opportunities in training draft models for multilingual large language models.

## Limitations
- Model Generalization Across Domains: The specific choice of Llama 2 architecture and the 115M parameter size may have been particularly well-suited for this task, and results might differ significantly for other configurations.
- Dataset Representation: There's no validation that the distillation dataset adequately represents the full distribution the target model was trained on, particularly for more diverse or specialized domains.
- TVD++ Loss Generalization: The effectiveness of the variance reduction techniques may be highly dependent on the specific training setup, and the paper doesn't explore whether this advantage holds across different types of target models, draft model sizes, or training datasets.

## Confidence
- High Confidence: The core mechanism of using knowledge distillation to train draft models for speculative decoding is well-established and validated through multiple experiments.
- Medium Confidence: The superiority of TVD++ over traditional losses (KLD, TVD) is demonstrated but within a limited experimental scope.
- Low Confidence: Claims about the general applicability of this approach to other model architectures and domains are not well-supported by the current evidence.

## Next Checks
1. Test the same training framework with different base model architectures (Mistral, GPT-style, etc.) and varying size ratios between draft and target models to assess generalizability.
2. Conduct ablation studies on the distillation dataset generation process by varying seed instruction sources, temperature ranges, and top-p values to determine the optimal configuration for maximum alignment.
3. Evaluate the trained draft model on tasks and domains not represented in the distillation dataset (e.g., code generation, mathematical reasoning, multilingual tasks) to assess generalization capabilities beyond the tested summarization and open-ended generation tasks.