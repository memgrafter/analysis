---
ver: rpa2
title: Improving Diffusion-Based Generative Models via Approximated Optimal Transport
arxiv_id: '2403.05069'
source_url: https://arxiv.org/abs/2403.05069
tags:
- images
- diffusion
- noise
- edm-aot
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high curvature in ODE trajectories
  of diffusion models, which leads to poor image quality and inefficient sampling.
  The authors propose Approximated Optimal Transport (AOT), a novel training technique
  that selects noise-image pairs to reduce information entropy and straighten ODE
  trajectories.
---

# Improving Diffusion-Based Generative Models via Approximated Optimal Transport

## Quick Facts
- arXiv ID: 2403.05069
- Source URL: https://arxiv.org/abs/2403.05069
- Authors: Daegyu Kim; Jooyoung Choi; Chaehun Shin; Uiwon Hwang; Sungroh Yoon
- Reference count: 20
- Primary result: Approximated Optimal Transport (AOT) improves diffusion model sampling efficiency by reducing ODE trajectory curvature, achieving state-of-the-art FID scores on CIFAR-10

## Executive Summary
This paper addresses the problem of high curvature in ODE trajectories of diffusion models, which leads to poor image quality and inefficient sampling. The authors propose Approximated Optimal Transport (AOT), a novel training technique that selects noise-image pairs to reduce information entropy and straighten ODE trajectories. AOT uses the Hungarian algorithm to solve an assignment problem, approximating optimal transport at the batch level. The EDM-AOT model achieves state-of-the-art FID scores of 1.88 (27 NFEs) for unconditional and 1.73 (29 NFEs) for conditional CIFAR-10 generation. Further improvements with Discriminator Guidance yield FID scores of 1.68 and 1.58, respectively. AOT enables stable performance even with fewer sampling steps and higher stride parameters, demonstrating its effectiveness in enhancing diffusion model performance.

## Method Summary
The paper introduces Approximated Optimal Transport (AOT) as a training technique for diffusion models. AOT pairs each target image with a noise sample using the Hungarian algorithm to minimize a cost function (L2 distance) between each image and noise pair. This creates a tighter coupling between image and noise, reducing the entropy of the denoising target and resulting in straighter ODE trajectories. The method is applied to the EDM (Energy-based Diffusion Model) architecture, and the resulting EDM-AOT model is evaluated on CIFAR-10 for both unconditional and conditional image generation tasks. AOT is shown to enable stable performance even with fewer sampling steps and higher stride parameters.

## Key Results
- EDM-AOT achieves state-of-the-art FID scores of 1.88 (27 NFEs) for unconditional and 1.73 (29 NFEs) for conditional CIFAR-10 generation
- With Discriminator Guidance, EDM-AOT achieves FID scores of 1.68 and 1.58, respectively
- AOT enables stable performance even with fewer sampling steps and higher stride parameters
- ODE trajectory observations show narrower ranges of high curvature for EDM-AOT compared to EDM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AOT reduces information entropy during training by pairing each target image with a noise sample that is more "informative" or closer to the optimal transport pairing.
- Mechanism: Instead of randomly pairing dataset images with noise, the Hungarian algorithm finds a matching that minimizes a cost function (L2 distance) between each image and noise pair. This creates a tighter coupling between image and noise, reducing the entropy of the denoising target.
- Core assumption: The optimal transport pairing between images and noise yields lower entropy in the denoising target, leading to straighter ODE trajectories.
- Evidence anchors:
  - [abstract]: "We demonstrate the feasibility of replacing the computation of the optimal transport with matching pairs of images and noise."
  - [section 2.3]: Discusses the difficulty of computing optimal transport for diffusion models and introduces the Hungarian algorithm to approximate it.
  - [corpus]: Weak. No direct corpus evidence supporting the entropy reduction claim.
- Break condition: If the Hungarian algorithm fails to find a meaningful pairing (e.g., due to degenerate cost matrix), the entropy reduction benefit disappears.

### Mechanism 2
- Claim: Reducing entropy in the training target leads to lower curvature in the ODE trajectory, which reduces truncation error during sampling.
- Mechanism: Lower entropy means the model's denoising target is more focused and less averaged over many possible noise configurations. This results in a smoother, more direct ODE path from noise to image.
- Core assumption: Lower entropy in the denoising target directly translates to reduced ODE curvature.
- Evidence anchors:
  - [section 3]: "In these high noise regions, the initial estimates are distant from the sampled images, necessitating the model's iterative refinements. This phenomenon indicates an increased curvature in the ODE process."
  - [section 5.4]: Observations of the ODE trajectory showing narrower ranges of high curvature for EDM-AOT compared to EDM.
  - [corpus]: Weak. No direct corpus evidence supporting the curvature reduction claim.
- Break condition: If the model cannot learn from the lower entropy targets effectively, the curvature may not decrease despite the entropy reduction.

### Mechanism 3
- Claim: Lower ODE curvature allows for fewer sampling steps and higher stride parameters without sacrificing image quality.
- Mechanism: With a straighter ODE path, fewer integration steps are needed to reach the target image. Higher stride parameters can be used because the truncation error is reduced.
- Core assumption: The relationship between ODE curvature and required sampling steps is inversely proportional.
- Evidence anchors:
  - [section 5.3]: "Diffusion models trained with the AOT technique maintain superior performance even with fewer steps."
  - [abstract]: "This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling."
  - [section 2.2]: Discussion of EDM's observation that lower ODE curvature reduces sampling step requirements.
- Break condition: If the ODE becomes too straight, the model might miss important features, leading to reduced image quality despite fewer steps.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: Understanding the theoretical foundation of the problem AOT is trying to solve (straightening ODE trajectories).
  - Quick check question: What is the difference between the 1-Wasserstein and 2-Wasserstein distance, and how are they used in optimal transport?

- Concept: Assignment Problem and Hungarian Algorithm
  - Why needed here: The Hungarian algorithm is the core computational tool used to approximate optimal transport in AOT.
  - Quick check question: How does the Hungarian algorithm guarantee finding the optimal assignment, and what is its time complexity?

- Concept: Diffusion Models and Score Matching
  - Why needed here: AOT is a training technique for diffusion models, so understanding their basic mechanics is crucial.
  - Quick check question: What is the role of the score function in diffusion models, and how does it relate to the denoising process?

## Architecture Onboarding

- Component map: Data loading -> Hungarian algorithm for pairing -> Model training with paired data -> Sampling with fewer steps
- Critical path: Data loading -> Hungarian algorithm for pairing -> Model training with paired data -> Sampling with fewer steps
- Design tradeoffs: AOT adds computational overhead during training due to the Hungarian algorithm, but it reduces sampling steps, which can be beneficial in deployment.
- Failure signatures: If the Hungarian algorithm pairings are poor (e.g., due to degenerate cost matrices), the model may not learn effectively, leading to poor image quality. If the stride parameters are set too high, truncation error may increase.
- First 3 experiments:
  1. Implement the Hungarian algorithm for pairing images and noise, and verify that the pairings are meaningful (e.g., by visualizing the cost matrix).
  2. Train a small diffusion model with AOT and compare its performance (e.g., FID score) to a model trained with random pairings.
  3. Experiment with different stride parameters during sampling to find the optimal balance between speed and image quality.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's central claim that approximated optimal transport reduces ODE curvature and improves sampling efficiency is supported by strong empirical results but relies on several assumptions with limited theoretical grounding.
- The mechanism connecting entropy reduction to curvature reduction is plausible but not rigorously proven.
- The use of the Hungarian algorithm for batch-level pairing is computationally efficient but may not capture the full optimal transport problem.
- While the results are impressive, the generalizability to other datasets and diffusion model architectures remains untested.

## Confidence
- Core claim (AOT reduces ODE curvature and improves sampling efficiency): Medium
- Mechanism (entropy reduction leads to curvature reduction): Medium
- Empirical results (state-of-the-art FID scores on CIFAR-10): High
- Generalizability to other datasets and architectures: Low

## Next Checks
1. Verify that the Hungarian algorithm pairings are meaningful by visualizing the cost matrix and checking that similar images are paired with similar noise samples.
2. Compare the ODE trajectories of models trained with and without AOT to confirm that AOT leads to lower curvature.
3. Experiment with AOT on other datasets and diffusion model architectures to test its generalizability.