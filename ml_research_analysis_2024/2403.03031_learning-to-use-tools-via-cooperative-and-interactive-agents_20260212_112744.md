---
ver: rpa2
title: Learning to Use Tools via Cooperative and Interactive Agents
arxiv_id: '2403.03031'
source_url: https://arxiv.org/abs/2403.03031
tags:
- agent
- tools
- agents
- arxiv
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ConAgents, a cooperative and interactive\
  \ agent framework for tool learning that addresses limitations of single-agent approaches\
  \ by coordinating three specialized agents: grounding, execution, and review. The\
  \ framework employs two communication protocols\u2014automatic and adaptive interaction\u2014\
  to enable dynamic cooperation and error correction during tool use workflows."
---

# Learning to Use Tools via Cooperative and Interactive Agents

## Quick Facts
- arXiv ID: 2403.03031
- Source URL: https://arxiv.org/abs/2403.03031
- Reference count: 15
- Primary result: ConAgents achieves up to 14% higher success rates compared to baselines on tool learning tasks

## Executive Summary
This paper introduces ConAgents, a cooperative and interactive agent framework for tool learning that addresses limitations of single-agent approaches by coordinating three specialized agents: grounding, execution, and review. The framework employs two communication protocols—automatic and adaptive interaction—to enable dynamic cooperation and error correction during tool use workflows. To adapt the framework to open-source models, the authors propose specialized action distillation (SPAN), which trains separate agents using task-solving trajectories from GPT-4. Experiments on three datasets show that ConAgents achieves superior performance with up to 14% higher success rates compared to baselines, with the distillation method further improving open-source model performance.

## Method Summary
ConAgents divides tool use workflows into three specialized agents: a grounding agent for task decomposition and tool selection, an execution agent for generating executable code, and a review agent for providing feedback and error correction. The framework uses two communication protocols: automatic interaction with real-time reviews during planning and execution phases, and adaptive interaction that triggers reviews only for exceptional errors. For open-source model adaptation, SPAN collects task-solving trajectories from GPT-4 using ConAgents, then trains separate open-source models for each agent role using instruction tuning with LoRA. The system is evaluated on RestBench-TMDB, RestBench-Spotify, and ToolBench datasets using Success Rate and Correct Path Rate metrics.

## Key Results
- ConAgents achieves up to 14% higher success rates compared to baselines like ReAct, Chameleon, CodeAct, and ToolLLM
- The specialized action distillation (SPAN) method improves open-source model performance when trained on task trajectories from GPT-4
- The adaptive communication protocol shows efficiency benefits by only triggering reviews when exceptional errors occur

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition of tool use into specialized agents reduces cognitive load and improves performance.
- Mechanism: The ConAgents framework divides the tool-use workflow into three specialized agents (Grounding, Execution, Review) each handling distinct tasks, preventing a single agent from having to learn all capabilities simultaneously.
- Core assumption: Different tool-use tasks require different cognitive abilities that can be more effectively learned when separated.
- Evidence anchors: [abstract] "Coordinating three specialized agents for tool selection, tool execution, and action calibration separately"; [section 3.1] "ConAgents streamlines and modularizes the workflow of tool learning tasks into a grounding agent MG, execution agent ME, and review agent MR"

### Mechanism 2
- Claim: Two communication protocols enable dynamic error correction during tool use workflows.
- Mechanism: Automatic interaction provides real-time reviews to calibrate incorrect actions, while adaptive interaction only triggers review when exceptional errors occur, allowing flexible handling of errors.
- Core assumption: Tool use workflows frequently encounter errors that require dynamic correction rather than rigid predefined pipelines.
- Evidence anchors: [abstract] "two communication protocols—automatic and adaptive interaction—to enable dynamic cooperation and error correction"; [section 3.3] "Our automatic interaction alternates between planning-review and execution-review phases"

### Mechanism 3
- Claim: Specialized action distillation (SPAN) effectively adapts ConAgents to open-source models by teaching them specific agent roles.
- Mechanism: SPAN collects task-solving trajectories from GPT-4 using ConAgents, then trains separate open-source models for each agent role using instruction tuning with LoRA.
- Core assumption: Open-source models can effectively learn specialized roles when trained on task trajectories that demonstrate those specific capabilities.
- Evidence anchors: [abstract] "specialized action distillation (SPAN), which trains separate agents using task-solving trajectories from GPT-4"; [section 4] "Our distillation method collects the task-solving trajectory of specialized agents simulated by GPT-4"

## Foundational Learning

- Concept: Agent-based tool learning and multi-agent systems
  - Why needed here: Understanding how agents can be coordinated to solve complex tasks is fundamental to grasping ConAgents' approach
  - Quick check question: What are the key differences between single-agent and multi-agent approaches to tool learning?

- Concept: Parameter-efficient tuning techniques (LoRA)
  - Why needed here: LoRA is used in SPAN to adapt open-source models efficiently to specialized roles
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter updates and computational efficiency?

- Concept: Tool learning benchmarks and evaluation metrics
  - Why needed here: The paper uses RestBench and ToolBench datasets with specific evaluation metrics (Success Rate, Correct Path Rate)
  - Quick check question: What do Success Rate and Correct Path Rate measure in the context of tool learning?

## Architecture Onboarding

- Component map: Task → Grounding Agent Planning → Execution Agent Code Generation → Tool Execution → Review Agent Feedback (if needed) → Revised Planning/Execution

- Critical path: Task → Grounding Agent Planning → Execution Agent Code Generation → Tool Execution → Review Agent Feedback (if needed) → Revised Planning/Execution

- Design tradeoffs:
  - More agents mean more coordination overhead but better specialization
  - Automatic interaction provides immediate feedback but increases token usage
  - Adaptive interaction is more efficient but may miss some correction opportunities
  - SPAN requires high-quality training data but enables open-source model adaptation

- Failure signatures:
  - Poor coordination between agents (planning doesn't match execution capabilities)
  - Review agent provides incorrect or unhelpful feedback
  - Open-source models struggle with specialized roles despite distillation
  - Communication protocols create more problems than they solve

- First 3 experiments:
  1. Implement ConAgents with GPT-3.5-turbo on RestBench-TMDB and measure Success Rate vs baselines
  2. Test both automatic and adaptive communication protocols to compare performance
  3. Apply SPAN to Mistral-8x7B and evaluate improvement over vanilla implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of interaction turns between agents that provides optimal performance?
- Basis in paper: [explicit] The paper states "we set the maximum iteration of interactions α = 3 and β = 3" and analyzes varying these values from 1 to 5
- Why unresolved: The paper finds that performance increases from 1 to 3 turns but plateaus beyond 3, but doesn't definitively establish an optimal number or explain why 3 is chosen as the standard
- What evidence would resolve it: Systematic comparison of success rates at different turn limits (1, 2, 3, 4, 5) across multiple datasets with statistical significance testing

### Open Question 2
- Question: How does the quality of the review agent's feedback affect overall system performance?
- Basis in paper: [explicit] The paper mentions "in less than 5% of tasks, the agent MR hallucinates giving an incorrect review" and analyzes the review quality
- Why unresolved: The paper doesn't quantify how much the 5% error rate in reviews impacts success rates or explore strategies to improve review accuracy
- What evidence would resolve it: Detailed error analysis of review agent mistakes and their downstream effects on planning/execution agents, plus experiments testing different review accuracy thresholds

### Open Question 3
- Question: How does the specialized action distillation method perform with different amounts of training data?
- Basis in paper: [explicit] The paper uses 500 training examples for distillation but mentions extending to "low-resource scenarios"
- Why unresolved: The paper only reports results with 500 examples and doesn't explore the data efficiency or minimum viable training set size
- What evidence would resolve it: Performance comparison across different training set sizes (e.g., 50, 100, 250, 500, 1000 examples) with learning curves

## Limitations

- Evaluation focuses primarily on restaurant-related and function-calling tasks, leaving uncertainty about generalization to other domains
- The paper demonstrates effectiveness with GPT-3.5-turbo but doesn't thoroughly validate whether open-source models with SPAN can match this performance
- Communication protocol parameters (α=3, β=3) appear to be chosen empirically without exploring sensitivity to these values

## Confidence

**High confidence** in the modular decomposition approach: The three-agent architecture is clearly defined with distinct responsibilities, and the mechanism of dividing cognitive load is well-articulated and logically sound.

**Medium confidence** in the communication protocols: While the two protocols (automatic and adaptive) are conceptually clear, the paper lacks comparative analysis showing when each protocol outperforms the other, and the parameter choices appear arbitrary.

**Medium confidence** in SPAN's effectiveness: The distillation method shows quantitative improvements, but the paper doesn't establish whether these gains come from the specialized training approach itself or simply from more training data.

## Next Checks

1. **Cross-domain generalization test**: Evaluate ConAgents on scientific computing or creative writing tasks to assess whether the three-agent architecture generalizes beyond restaurant and function-calling domains.

2. **Communication protocol ablation study**: Systematically vary the α and β parameters and test whether automatic or adaptive interaction performs better for different types of tasks or error rates.

3. **SPAN data sensitivity analysis**: Vary the quality and quantity of training trajectories used in SPAN to determine the minimum effective dataset size and establish sensitivity to trajectory quality.