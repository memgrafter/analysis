---
ver: rpa2
title: Multimodal Learning for Embryo Viability Prediction in Clinical IVF
arxiv_id: '2410.15581'
source_url: https://arxiv.org/abs/2410.15581
tags:
- multimodal
- embryo
- video
- embryos
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of predicting embryo viability
  in clinical IVF by developing a multimodal learning approach that integrates time-lapse
  video data and Electronic Health Records (EHRs). The authors propose two methods:
  a multimodal transformer that processes video and EHR data end-to-end, and a two-stage
  approach where video-derived morphological features are combined with EHRs for tabular
  models.'
---

# Multimodal Learning for Embryo Viability Prediction in Clinical IVF

## Quick Facts
- arXiv ID: 2410.15581
- Source URL: https://arxiv.org/abs/2410.15581
- Reference count: 32
- Primary result: Multimodal integration of time-lapse video and EHR data improves embryo viability prediction over single-modality models

## Executive Summary
This paper addresses the challenge of predicting embryo viability in clinical IVF by developing multimodal learning approaches that integrate time-lapse video data with Electronic Health Records (EHRs). The authors propose two methods: a multimodal transformer that processes both modalities end-to-end, and a two-stage approach that first extracts semantic features from videos before combining them with EHRs. Experiments demonstrate that models incorporating semantic features extracted from videos outperform those using raw video data alone, and the multimodal approach achieves better performance than single-modality models. The best results come from combining video-derived morphological features with EHR data, showing the value of multimodal integration for embryo viability prediction.

## Method Summary
The authors develop two approaches for embryo viability prediction using time-lapse videos and EHR data from 3,695 IVF treatment cycles with 24,027 embryos. The first approach is a multimodal transformer that processes video frames through spatial and temporal attention mechanisms, embeds EHR data, and combines them through multimodal attention layers. The second approach uses a two-stage process where semantic features are first extracted from videos using pre-trained models (Embryo-vision and BlastAssist), then combined with EHR data for tabular models (TabTransformer or TabNet). The multimodal transformer uses a factorized encoder structure with DeiT-Ti for spatial attention and custom transformer layers for temporal processing. Both approaches are evaluated on embryo viability prediction (live births/embryos transferred) and treatment success prediction using AUCROC and F1-Score metrics.

## Key Results
- Multimodal models combining video and EHR data outperform single-modality models on both embryo viability and treatment success prediction
- Semantic features extracted from videos (segmentation masks, morphological measurements) perform better than raw video frames
- The multimodal transformer and two-stage approach achieve comparable performance, with the two-stage approach showing competitive results
- Models demonstrate better calibration when using both modalities compared to single-modality approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal integration of video and EHR data improves embryo viability prediction over single-modality models.
- Mechanism: The multimodal transformer effectively combines spatial-temporal attention from video data with EHR embeddings through a unified transformer architecture, allowing the model to learn complementary representations from both modalities.
- Core assumption: The EHR and video modalities contain complementary information about embryo viability that is not captured by either modality alone.
- Evidence anchors:
  - [abstract]: "experiments show that models incorporating semantic features extracted from videos outperform those using raw video data alone, and the multimodal approach achieves better performance than single-modality models"
  - [section]: "we propose a multimodal transformer that is based on a video transformer architecture with modifications to allow multimodal inputs"
  - [corpus]: Weak - no direct corpus evidence found for this specific multimodal IVF prediction approach
- Break condition: If the EHR and video data are not complementary or contain redundant information, the multimodal integration would not provide additional benefits.

### Mechanism 2
- Claim: Using semantic features extracted from videos (segmentation masks, morphological measurements) improves prediction performance compared to raw video frames.
- Mechanism: Semantic features like segmentation masks (zona, blastomeres, pronuclei) and morphological measurements provide structured, interpretable representations that are easier for the model to learn from than raw pixel data, especially with limited training samples.
- Core assumption: Semantic features capture the most relevant information for embryo viability prediction and are more robust than raw video data.
- Evidence anchors:
  - [abstract]: "models incorporating semantic features extracted from videos outperform those using raw video data alone"
  - [section]: "Unlike raw video, Embryo-vision outputs are in the form of segmentation masks, which are semantically meaningful and have a simple visual structure. Therefore, it is easier for the model to understand and optimize the weights to extract relevant features for the task."
  - [corpus]: Weak - no direct corpus evidence found for semantic feature extraction in embryo viability prediction
- Break condition: If the semantic features are not properly aligned with viability outcomes or if the feature extraction process introduces significant errors, the performance benefit would diminish.

### Mechanism 3
- Claim: The two-stage approach (video → semantic features → tabular model) can achieve competitive performance with the end-to-end multimodal transformer.
- Mechanism: By first extracting high-quality semantic features using pre-trained models and then combining them with EHR data in a tabular format, the two-stage approach simplifies the learning problem and can achieve comparable results with less complexity.
- Core assumption: The pre-trained semantic feature extractors (Embryo-vision, BlastAssist) provide high-quality representations that can be effectively combined with EHR data.
- Evidence anchors:
  - [section]: "Although tabular models are trained with regression objectives, they fail to calibrate the prediction confidence, resulting in a low F-1 score"
  - [section]: "the tabular models show competitive performance when using both EHRs and interpretable features"
  - [corpus]: Weak - no direct corpus evidence found for this specific two-stage approach in embryo viability prediction
- Break condition: If the semantic feature extraction is inaccurate or the tabular models cannot effectively integrate the semantic and EHR features, the two-stage approach would underperform.

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: The problem requires integrating heterogeneous data types (video and EHR) that contain complementary information about embryo viability
  - Quick check question: What are the main challenges in multimodal learning and how does this work address them?

- Concept: Transformer architectures
  - Why needed here: The spatial-temporal attention mechanisms in transformers are well-suited for processing video data and can be modified to handle multimodal inputs
  - Quick check question: How does the factorized encoder structure (spatial attention followed by temporal attention) improve video processing efficiency?

- Concept: Semantic feature extraction
  - Why needed here: Raw video data is too complex and data-limited for direct prediction; semantic features provide structured representations that are easier to learn from
  - Quick check question: What types of semantic features are extracted from embryo videos and how do they relate to viability outcomes?

## Architecture Onboarding

- Component map: Video tokenization → spatial attention → temporal attention → multimodal attention with EHR → prediction (multimodal transformer); Video → semantic feature extraction → tabular model with EHR → prediction (two-stage approach)
- Critical path: For multimodal transformer: video → spatial attention → temporal attention → multimodal attention with EHR → prediction. For two-stage approach: video → semantic feature extraction → tabular model with EHR → prediction.
- Design tradeoffs: End-to-end multimodal learning provides better integration but requires more training data and computational resources. The two-stage approach is simpler and more data-efficient but may lose some information in the feature extraction process.
- Failure signatures: Poor performance on validation set, large performance gap between train and validation, unstable training, or multimodal attention not learning meaningful representations.
- First 3 experiments:
  1. Baseline: Train model with only EHR data to establish lower bound
  2. Video only: Train with only video data (raw or semantic features) to measure video contribution
  3. Multimodal: Combine video and EHR data to measure the benefit of multimodal integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal transformer models effectively integrate video and EHR data when there is no one-to-one correspondence between embryos and treatments?
- Basis in paper: [explicit] The authors explicitly state that "it is not straightforward to apply them to embryo viability prediction as they assume samples in each modality has one-to-one correspondence" and that "videos are embryo-specific, but EHRs are treatment-specific, which is shared across embryos within the same treatment cycle."
- Why unresolved: The paper proposes a modified multimodal transformer architecture to address this challenge, but does not provide a comprehensive comparison with other potential solutions or evaluate whether their approach is optimal.
- What evidence would resolve it: A systematic comparison of different multimodal integration strategies (e.g., cross-modal attention mechanisms, contrastive learning approaches) evaluated on the same dataset would help determine the most effective approach for this specific problem.

### Open Question 2
- Question: Would pre-training modality-specific encoders using self-supervised learning improve the performance of multimodal models given the limited size of supervised training data?
- Basis in paper: [explicit] The authors mention in their discussion that "One solution is to pre-train modality-specific encoders separately with pretext tasks using self-supervised learning [9] and then fine-tune the encoders with multimodal transformers by supervised learning for the downstream task."
- Why unresolved: The paper does not implement or evaluate this approach, leaving open the question of whether self-supervised pre-training would be beneficial for this specific application.
- What evidence would resolve it: Experimental results comparing models with and without self-supervised pre-training on the same dataset, measuring both embryo viability prediction accuracy and computational efficiency.

### Open Question 3
- Question: What is the optimal threshold for determining treatment success in the embryo viability prediction task?
- Basis in paper: [explicit] The authors note that "In practice, finding the best threshold is a difficult problem. Therefore, without an appropriate threshold estimation method, a model with good confidence calibration is favored."
- Why unresolved: The paper uses fixed thresholds (0.15 for embryo viability prediction and 0.5 for treatment success prediction) but acknowledges that these may not be optimal and that finding the best threshold is challenging.
- What evidence would resolve it: Results from threshold optimization methods (e.g., cross-validation to find optimal thresholds, cost-sensitive learning approaches) that demonstrate improved F1-scores compared to the fixed thresholds used in the paper.

## Limitations

- Lack of direct corpus evidence supporting the specific multimodal IVF prediction approach, with weak validation from related literature
- Effectiveness of semantic feature extraction relies heavily on quality of pre-trained models (Embryo-vision, BlastAssist), but their performance characteristics are not thoroughly characterized
- Small dataset size (3,695 treatment cycles) raises concerns about generalizability and overfitting, particularly for end-to-end multimodal transformer approach

## Confidence

- High Confidence: Multimodal integration improves prediction performance over single-modality models
- Medium Confidence: Semantic features outperform raw video data
- Low Confidence: Two-stage approach achieving competitive performance with end-to-end transformer

## Next Checks

1. Conduct a replication study using an independent IVF dataset to verify whether the multimodal approach consistently outperforms single-modality models across different clinical settings and patient populations.

2. Perform a systematic evaluation of the semantic feature extraction process, including error rate analysis for Embryo-vision and BlastAssist, and sensitivity testing to determine how feature extraction errors propagate to final predictions.

3. Conduct a comprehensive ablation study to identify which specific EHR features contribute most to prediction performance, and test whether the multimodal benefit persists when using only the most predictive EHR features.