---
ver: rpa2
title: 'SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt
  Types'
arxiv_id: '2410.21965'
source_url: https://arxiv.org/abs/2410.21965
tags:
- safety
- prompt
- llms
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SG-Bench, a comprehensive benchmark designed
  to evaluate the generalization of LLM safety across diverse tasks and prompt types.
  The key issue is that existing benchmarks focus on either generative or discriminative
  evaluation paradigms while ignoring their interconnection, and rely on standardized
  inputs that overlook the effects of widespread prompting techniques.
---

# SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types

## Quick Facts
- arXiv ID: 2410.21965
- Source URL: https://arxiv.org/abs/2410.21965
- Authors: Yutao Mou; Shikun Zhang; Wei Ye
- Reference count: 40
- Key outcome: Most LLMs perform worse on discriminative tasks than generative ones, showing poor safety generalization across task types and prompt variations

## Executive Summary
This paper introduces SG-Bench, a comprehensive benchmark designed to evaluate LLM safety generalization across diverse tasks and prompt types. The key innovation is integrating both generative and discriminative evaluation paradigms while incorporating prompt engineering techniques to reveal how context shifts affect safety alignment. The benchmark demonstrates that existing safety evaluation methods are insufficient because they focus on standardized inputs and single paradigms, missing the complex interplay between task types and prompt contexts that degrade safety performance.

## Method Summary
SG-Bench constructs four parallel evaluation subsets using identical malicious queries but different evaluation paradigms: original queries (generation), jailbreak attacks (generation), multiple-choice questions (discrimination), and safety judgments (discrimination). The benchmark evaluates 13 LLMs (3 proprietary, 10 open-source) across these tasks, then extends the evaluation with seven prompt variations including system prompts, few-shot demonstrations, and chain-of-thought prompting. Safety performance is measured using Failure Rate (proportion of harmful outputs) with LlamaGuard-7B as the referee model. The approach isolates task-specific performance differences and examines how prompt engineering affects attention mechanisms and safety alignment.

## Key Results
- Most LLMs show significantly worse safety performance on discriminative tasks compared to generative tasks
- Prompt engineering techniques, particularly few-shot demonstrations and chain-of-thought prompting, generally harm safety performance on discrimination tasks
- Attention mechanisms shift from malicious content to prompt contexts, explaining degradation in safety performance
- Discrimination task performance correlates with representation modeling ability between harmful and harmless content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SG-Bench reveals poor LLM safety generalization across task types through systematic comparison
- Mechanism: By constructing parallel evaluation subsets that use identical malicious queries but different evaluation paradigms, SG-Bench isolates task-specific performance differences
- Core assumption: LLMs trained on generation-focused safety objectives will show different performance on discriminative tasks using the same underlying safety issues
- Evidence anchors: [abstract] "most LLMs perform worse on discriminative tasks than generative ones"; [section 4.2] "safety performance drops significantly" when moving from generation to discrimination tasks

### Mechanism 2
- Claim: Prompt engineering techniques create context shifts that degrade safety alignment
- Mechanism: By applying system prompts, few-shot demonstrations, and chain-of-thought prompting to the same evaluation subsets, SG-Bench demonstrates that context variations cause attention shifts away from malicious content
- Core assumption: LLMs' attention mechanisms are sensitive to prompt context changes, causing safety mechanisms to be bypassed
- Evidence anchors: [section 5.2] "LLM's attention gradually shifts from malicious queries to prompt contexts" with attention visualization; [section 4.3] "Few-shot demonstrations might damage LLM safety performance on open-end generation"

### Mechanism 3
- Claim: Discrimination tasks reveal representational gaps in harmful vs harmless content
- Mechanism: By measuring intra-class and inter-class distances between harmful and harmless response representations, SG-Bench quantifies LLMs' ability to discriminate safety content
- Core assumption: Better discrimination performance correlates with more distinct representation spaces for harmful vs harmless content
- Evidence anchors: [section 5.3] "LLM's safety performance on discrimination tests is positively related to its representation modeling ability"; [section 4.2] Shows discrimination tasks have higher failure rates than generation tasks

## Foundational Learning

- Concept: Task-type distinction in LLM evaluation
  - Why needed here: Understanding why generative and discriminative paradigms yield different safety outcomes is fundamental to interpreting SG-Bench results
  - Quick check question: Can you explain the difference between evaluating LLM safety through response generation versus content discrimination?

- Concept: Prompt engineering impact on model behavior
  - Why needed here: SG-Bench's extended evaluation sets rely on understanding how different prompt types affect model outputs
  - Quick check question: How might role-oriented prompts versus chain-of-thought prompting differently influence an LLM's safety responses?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The qualitative analysis of why safety performance degrades relies on understanding attention shifts
  - Quick check question: What role does attention play in determining which parts of input an LLM prioritizes during response generation?

## Architecture Onboarding

- Component map: Original queries → Jailbreak attacks → Multiple-choice questions → Safety judgments → System prompts → Few-shot demonstrations → Chain-of-thought prompting → LLM evaluation → Failure rate calculation → Qualitative analysis
- Critical path: Data collection → Task subset construction → Prompt variation application → LLM evaluation → Failure rate calculation → Qualitative analysis
- Design tradeoffs: Comprehensive coverage vs evaluation cost (using LlamaGuard-7B as referee model); multiple task types vs potential task-specific confounds; qualitative analysis vs quantitative rigor
- Failure signatures: High failure rates across all configurations indicate fundamental safety alignment issues; task-specific failures indicate generalization problems; prompt-specific failures indicate context sensitivity issues
- First 3 experiments:
  1. Run original queries evaluation on a target LLM to establish baseline safety performance
  2. Apply jailbreak attack templates to the same queries and re-evaluate to measure attack vulnerability
  3. Convert queries to multiple-choice format and evaluate discrimination capability to assess task generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or architectural changes could be implemented to prevent LLM attention from shifting away from harmful content toward prompt contexts?
- Basis in paper: [inferred] The paper identifies that LLM attention shifts from malicious queries to prompt contexts as a key reason for poor safety generalization
- Why unresolved: While the paper identifies this as a core problem, it does not propose specific technical solutions or architectural modifications to address this attention shift issue

### Open Question 2
- Question: How do different types of safety issues (toxic content, misinformation, privacy infringement, etc.) affect LLM safety generalization differently across tasks and prompt types?
- Basis in paper: [explicit] The paper acknowledges in Section 6 that it focuses on "LLM safety generalization across diverse prompt contexts" without delving into "the effects of different types of safety issues"
- Why unresolved: The current SG-Bench benchmark uses a coarse-grained taxonomy of 6 safety types but does not analyze whether certain types of safety issues are more challenging for LLMs to generalize

### Open Question 3
- Question: What is the optimal balance between chain-of-thought prompting benefits for reasoning and its negative impact on safety performance in discrimination tasks?
- Basis in paper: [explicit] Section 4.3 shows that chain-of-thought prompting "generally harm LLM's safety performance on discrimination tasks" but does not explore potential mitigations
- Why unresolved: The paper identifies the negative impact but does not investigate whether modifications to chain-of-thought prompting could preserve reasoning benefits while reducing safety risks

## Limitations
- The evaluation relies on LlamaGuard-7B as a referee model, introducing potential bias in failure rate calculations
- The qualitative analysis of attention mechanisms lacks quantitative validation through attention weight measurements
- The benchmark's focus on English-language safety issues limits generalizability to multilingual contexts

## Confidence
- High confidence: The core finding that LLMs show different safety performance across generative vs discriminative tasks
- Medium confidence: The claim that prompt engineering techniques systematically degrade safety performance
- Medium confidence: The relationship between discrimination performance and representation modeling ability

## Next Checks
1. Run the same SG-Bench evaluation using multiple referee models (e.g., GPT-4, Claude-3) to assess the impact of referee model choice on failure rate calculations
2. Extract and analyze attention weight distributions for the same malicious queries across different prompt contexts to quantitatively verify the qualitative claim about attention shifts
3. Evaluate the same LLMs on SG-Bench at multiple time points to assess whether observed safety generalization patterns are stable over time