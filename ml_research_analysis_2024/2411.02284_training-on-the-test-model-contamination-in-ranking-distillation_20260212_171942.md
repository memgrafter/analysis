---
ver: rpa2
title: 'Training on the Test Model: Contamination in Ranking Distillation'
arxiv_id: '2411.02284'
source_url: https://arxiv.org/abs/2411.02284
tags:
- contamination
- distillation
- training
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate data contamination in ranking distillation when
  training on black-box teacher models. We simulate worst-case contamination by directly
  including test data in teacher training, then distill into student models.
---

# Training on the Test Model: Contamination in Ranking Distillation

## Quick Facts
- **arXiv ID**: 2411.02284
- **Source URL**: https://arxiv.org/abs/2411.02284
- **Reference count**: 40
- **Primary result**: Contamination with less than 0.1% test data in teacher training significantly improves student model effectiveness across multiple ranking distillation objectives

## Executive Summary
This paper investigates data contamination in ranking distillation when training on black-box teacher models. The authors simulate worst-case contamination by including test data directly in teacher training, then distill into student models. Even with test data representing less than 0.1% of training examples, they observe significant improvements in student effectiveness across multiple benchmarks. The effect occurs with both score-based and ranking-based distillation objectives, with RankNet-style distillation showing the largest disparities. Interestingly, contamination with out-of-distribution examples can sometimes improve effectiveness even more than in-distribution contamination. These findings suggest caution when comparing systems where data provenance is ambiguous, as closed-source models can introduce significant test-set leakage through semi-supervised training signals.

## Method Summary
The authors train ELECTRA-based cross-encoder teacher models with and without contamination (including test data in training), then distill into student models using three objectives: marginMSE, KL divergence, and RankNet losses. They evaluate the models on DL 2019, DL 2020, and TREC COVID collections using nDCG@10, MAP, and Recall@100 metrics. The contamination simulation involves taking test queries and relevance judgments from TREC DL collections and adding them to the teacher training data, representing less than 0.1% of the total training examples.

## Key Results
- Contamination with less than 0.1% test data causes significant improvements in student model effectiveness
- Both score-based (marginMSE, KL) and ranking-based (RankNet) distillation objectives show contamination effects
- RankNet-style distillation shows the largest disparities between contaminated and uncontaminated models
- Out-of-distribution contamination can improve effectiveness more than in-distribution contamination in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contamination improves distillation effectiveness because teacher models memorize test relevance patterns and transfer them via supervised signals
- Mechanism: When test queries and relevance judgments are present in teacher training, the teacher learns to output scores matching ground-truth relevance exactly for those queries. During distillation, the student mimics these scores, effectively learning the test-set relevance function without direct exposure
- Core assumption: Teacher's scoring function generalizes enough that even with <0.1% contamination, the student can recover and apply the exact test-set behavior
- Evidence anchors: Abstract mentions contamination occurs even with small fractions of training data; section describes producing contaminated vs. uncontaminated teachers
- Break condition: If teacher architecture cannot memorize such a small set or if test queries differ significantly from training distribution

### Mechanism 2
- Claim: RankNet-style distillation is more susceptible to contamination because it trains on pairwise preferences derived from teacher rankings
- Mechanism: RankNet loss optimizes student to match teacher's relative ordering between documents. If teacher rankings are based on contaminated data, the pairwise preferences the student learns are biased toward test set's relevance patterns
- Core assumption: Pairwise preference signals are sufficient to capture and transfer ranking behavior without needing exact score calibration
- Evidence anchors: Abstract notes both explicit teacher output use and RankNet-style order distillation lead to contamination; section describes applying RankNet loss
- Break condition: If test queries differ significantly in query structure or topic from training queries, pairwise preferences may not transfer

### Mechanism 3
- Claim: Out-of-distribution contamination can improve effectiveness more than in-distribution contamination because the student learns more diverse ranking patterns
- Mechanism: When teacher is trained on out-of-distribution examples, it develops ranking signals less correlated with original training distribution, forcing student to learn more robust representations that generalize better
- Core assumption: OOD examples provide novel ranking signals that improve student generalization beyond what ID contamination provides
- Evidence anchors: Abstract states contamination with OOD examples can improve effectiveness even more than ID contamination; section contrasts ID and OOD contamination effectiveness
- Break condition: If OOD examples are too different from target distribution, student may learn irrelevant patterns

## Foundational Learning

- Concept: Knowledge distillation fundamentals (teacher-student training, loss functions)
  - Why needed here: The paper evaluates multiple distillation objectives and their susceptibility to contamination
  - Quick check question: What is the key difference between score-based and preference-based distillation objectives?

- Concept: Contamination in machine learning evaluation
  - Why needed here: The paper investigates how test-set leakage through teacher models affects downstream evaluation metrics
  - Quick check question: Why is contamination particularly problematic when teacher models are black-box?

- Concept: Ranking metrics and their sensitivity to contamination
  - Why needed here: The paper uses nDCG@10, MAP, and Recall@100 to measure effectiveness improvements
  - Quick check question: How might contamination affect precision-focused metrics differently than recall-focused metrics?

## Architecture Onboarding

- Component map: Teacher model (ELECTRA cross-encoder) → Distillation module (marginMSE/KL/RankNet) → Student model (ELECTRA cross-encoder or BERT bi-encoder) → Evaluation pipeline (nDCG@10, MAP, Recall@100)
- Critical path: Data preprocessing → Teacher training (with/without contamination) → Distillation training → Evaluation on test sets
- Design tradeoffs: Cross-encoder teachers are more effective but slower; bi-encoder students are faster but may learn less nuanced ranking signals
- Failure signatures: Significant effectiveness improvements only on contaminated test sets; degraded performance on clean validation sets
- First 3 experiments:
  1. Train uncontaminated teacher, distill to student, evaluate on clean test set
  2. Train contaminated teacher, distill to student, evaluate on same contaminated test set
  3. Train contaminated teacher, distill to student, evaluate on different clean test set to detect contamination effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degree of contamination (percentage of test data in training) affect the magnitude of performance inflation in student models?
- Basis in paper: The paper shows contamination with less than 0.1% test data still causes significant improvements, but doesn't systematically vary contamination levels
- Why unresolved: The paper only tests one contamination level (worst-case scenario) and doesn't explore a gradient of contamination percentages
- What evidence would resolve it: Experiments varying contamination levels from 0.01% to 1% while measuring student model performance on clean test sets

### Open Question 2
- Question: Do other neural ranking architectures beyond cross-encoders and bi-encoders show similar contamination effects when used as teachers or students?
- Basis in paper: The paper only tests ELECTRA cross-encoders and BERT bi-encoders, but mentions that findings could relate to broader NLP due to PLackett-Luce optimization
- Why unresolved: Limited architectural scope means generalization to other model types is unknown
- What evidence would resolve it: Testing contamination effects with additional architectures like ColBERT, SPLADE, and sequence-to-sequence models

### Open Question 3
- Question: What specific mechanisms make RankNet distillation more susceptible to contamination compared to score-based distillation methods?
- Basis in paper: The paper observes that RankNet distillation shows the largest disparities in effectiveness when teachers are contaminated, but doesn't explain why
- Why unresolved: The paper identifies the effect but doesn't investigate whether it's due to the pairwise nature of RankNet, the use of multiple negatives, or other factors
- What evidence would resolve it: Ablation studies comparing RankNet with different negative sampling strategies and other ranking losses to isolate the contamination vulnerability

### Open Question 4
- Question: How does contamination affect model robustness to distribution shifts beyond the specific out-of-distribution collections tested?
- Basis in paper: The paper tests OOD contamination with TREC COVID but doesn't systematically evaluate robustness across multiple domain shifts
- Why unresolved: Single OOD experiment provides limited insight into whether contamination creates false robustness or brittleness to other types of distribution shifts
- What evidence would resolve it: Evaluating contaminated and uncontaminated models across multiple OOD datasets with varying domain characteristics

## Limitations
- The paper relies on simulated contamination scenarios rather than real-world black-box teacher models
- Mechanism explanations are plausible but not definitively proven, particularly the claim that <0.1% contamination can reliably transfer test-set behavior
- Limited architectural scope means generalization to other model types beyond ELECTRA cross-encoders and BERT bi-encoders is unknown

## Confidence
- **High Confidence**: The observation that test data contamination in teacher training leads to effectiveness improvements in distilled students
- **Medium Confidence**: The specific mechanism of memorization and transfer, and the differential effects between ID and OOD contamination
- **Low Confidence**: The claim that RankNet-style distillation is more susceptible than score-based methods, and the generalisability to truly black-box teacher scenarios

## Next Checks
1. **Cross-Architecture Validation**: Test contamination effects across different teacher architectures (not just ELECTRA) to verify the mechanism is not architecture-specific
2. **Generalization Testing**: Evaluate distilled students on held-out test sets that were never seen during teacher training to measure actual contamination impact versus random performance variance
3. **Contamination Threshold Analysis**: Systematically vary contamination levels below 0.1% to identify the minimum contamination threshold required for measurable effectiveness improvements