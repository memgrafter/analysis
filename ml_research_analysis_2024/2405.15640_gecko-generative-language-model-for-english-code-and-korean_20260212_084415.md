---
ver: rpa2
title: 'GECKO: Generative Language Model for English, Code and Korean'
arxiv_id: '2405.15640'
source_url: https://arxiv.org/abs/2405.15640
tags:
- arxiv
- language
- korean
- english
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GECKO is a bilingual Korean-English large language model with code
  capabilities, trained from scratch on a balanced corpus of 200B tokens. It demonstrates
  strong Korean language performance (30.7% on KMMLU) and moderate English and coding
  performance (28.3% on MMLU, 17.7% on HumanEval) while using a smaller vocabulary
  and fewer training tokens than English-focused models.
---

# GECKO: Generative Language Model for English, Code and Korean

## Quick Facts
- arXiv ID: 2405.15640
- Source URL: https://arxiv.org/abs/2405.15640
- Authors: Sungwoo Oh; Donggyu Kim
- Reference count: 40
- Primary result: Bilingual Korean-English LLM with code capabilities, trained on 200B tokens, achieving 30.7% on KMMLU

## Executive Summary
GECKO is a bilingual Korean-English large language model with code capabilities, trained from scratch on a balanced corpus of 200B tokens. It demonstrates strong Korean language performance (30.7% on KMMLU) and moderate English and coding performance (28.3% on MMLU, 17.7% on HumanEval) while using a smaller vocabulary and fewer training tokens than English-focused models. The model employs a 32K-token vocabulary with efficient tokenization across languages and was trained on Google Cloud TPUv4 using JAX and model parallelism. The open-source model is available under a permissive license to support Korean LLM research.

## Method Summary
GECKO uses a LLaMA decoder-only Transformer architecture with a 32K BPE tokenizer, trained on a balanced corpus of 200B tokens (35% Korean, 28% English, 37% code) using AdamW optimizer with cosine learning rate schedule. The model was trained on Google Cloud TPUv4 with JAX, FSDP, and model parallelism, employing BF16 mixed precision and rotary positional embedding up to 8192 context length.

## Key Results
- Achieves 30.7% on KMMLU (Korean MMLU) outperforming English-focused models
- Demonstrates 28.3% on MMLU and 17.7% on HumanEval despite smaller training corpus
- Uses efficient 32K vocabulary with UTF-8 byte segmentation for multilingual tokenization

## Why This Works (Mechanism)

### Mechanism 1
Balanced multilingual training data improves bilingual performance despite fewer total tokens. By downsampling English and up-sampling Korean, the model receives proportionally more exposure to Korean while maintaining English capability, compensating for data scarcity. Core assumption: Equal quality of Korean and English data sources allows proportional representation to overcome raw quantity deficits. Evidence: Balanced, high-quality corpus of Korean and English; aim to strike a balance between English and Korean in our pretraining corpus by down-sampling and up-sampling. Break condition: If Korean data quality is significantly lower than English, balancing may not compensate for quantity gaps.

### Mechanism 2
Code data incorporation enhances reasoning capabilities across languages. Code follows logical patterns and explicit structures, training the model to follow structured reasoning paths applicable to both Korean and English tasks. Core assumption: Reasoning patterns learned from code transfer to natural language reasoning tasks. Evidence: Code capabilities; research findings indicate that incorporating code data in the pretraining phase enhances the reasoning ability; explicit 37% code allocation in pretraining corpus. Break condition: If code syntax patterns are too divergent from natural language, transfer learning may fail.

### Mechanism 3
Small vocabulary with BPE efficiently handles multilingual tokenization without sacrificing performance. 32K vocabulary with UTF-8 byte segmentation handles rare characters while maintaining efficiency, outperforming larger vocabularies in Korean processing. Core assumption: Korean character diversity requires more granular tokenization than English, which the small vocabulary with byte segmentation addresses. Evidence: Small size of vocabulary and efficiency calculations; efficiency comparison showing GECKO outperforming larger-vocabulary models on Korean. Break condition: If Korean morphological complexity exceeds BPE's ability to capture meaningful subword units, performance degrades.

## Foundational Learning

- Concept: Data balancing techniques for low-resource languages
  - Why needed here: Korean has fewer available training resources than English, requiring proportional adjustment
  - Quick check question: What happens to model performance if you up-sample Korean by 2x while maintaining English quality?

- Concept: BPE tokenization mechanics and vocabulary size optimization
  - Why needed here: Efficient handling of multilingual text requires understanding how vocabulary size affects performance and efficiency
  - Quick check question: How does treating all numbers as individual digits impact token efficiency for code-heavy datasets?

- Concept: Mixed-precision training with FSDP and model parallelism
  - Why needed here: Training on TPUv4 with 256 chips requires understanding distributed training paradigms
  - Quick check question: What's the trade-off between memory savings from BF16 and potential numerical precision loss?

## Architecture Onboarding

- Component map: Tokenizer (32K BPE with byte-level fallback) -> Architecture (LLaMA decoder-only Transformer) -> Training (AdamW optimizer, rotary positional embedding, sequence packing) -> Infrastructure (JAX on TPUv4 with FSDP and model parallelism)

- Critical path: 1. Data preprocessing and balancing 2. Tokenizer training on balanced corpus 3. Model initialization with rotary positional embedding 4. Distributed training setup with FSDP 5. Checkpoint saving and evaluation

- Design tradeoffs: Small vocabulary vs. coverage: 32K vs. larger vocabularies; Korean vs. English focus: balancing proportions vs. optimal performance; Code inclusion vs. natural language purity: reasoning benefits vs. potential noise

- Failure signatures: Korean tokenization inefficiency: higher token counts than expected; Memory issues: insufficient FSDP sharding configuration; Training instability: learning rate or optimizer configuration problems

- First 3 experiments: 1. Tokenization efficiency test: Compare GECKO tokenizer against baseline on Korean and English subsets 2. Curriculum training: Train first 100B tokens with balanced data, then fine-tune with Korean-heavy corpus 3. Ablation study: Remove code data from pretraining and measure impact on KMMLU performance

## Open Questions the Paper Calls Out

### Open Question 1
How does GECKO's performance scale with increased pretraining tokens beyond 200B, particularly for Korean language tasks? Basis: The paper notes GECKO was trained with 200B tokens and achieved 30.7% on KMMLU, comparing it to models with more pretraining tokens. Why unresolved: The paper only reports results from the 200B token training regime and doesn't explore scaling effects. What evidence would resolve it: Additional training runs with varying token counts (e.g., 400B, 600B) measuring KMMLU performance to establish a scaling relationship.

### Open Question 2
How does instruction fine-tuning specifically impact GECKO's performance on Korean language tasks compared to English? Basis: The authors mention they are preparing for instruction fine-tuning to evaluate GECKO's instruction-following ability but provide no results. Why unresolved: No instruction-tuned version of GECKO has been released or evaluated yet. What evidence would resolve it: Side-by-side comparison of pre-instruction-tuned and post-instruction-tuned models on KMMLU and other Korean-specific benchmarks.

### Open Question 3
What is the impact of different data deduplication strategies on GECKO's performance across Korean, English, and code domains? Basis: The paper mentions data deduplication improves robustness and generalization but doesn't experiment with different deduplication approaches. Why unresolved: The paper only describes using deduplication without comparing it to alternative methods or measuring its impact. What evidence would resolve it: Ablation studies comparing models trained with different deduplication rates or algorithms, measuring performance on domain-specific benchmarks.

## Limitations

- Modest Korean language performance (30.7% on KMMLU) indicates significant room for improvement
- Limited technical details about data preprocessing pipeline, particularly for Korean corpus cleaning
- Relatively small training corpus (200B tokens) compared to frontier models (1T+ tokens)

## Confidence

**High Confidence:** Core technical architecture (LLaMA decoder, 32K BPE tokenizer, JAX/TPU4 training setup) is well-specified and reproducible. Corpus composition percentages (35% Korean, 28% English, 37% code) are clearly stated and verifiable.

**Medium Confidence:** Performance improvements on Korean tasks are supported by KMMLU scores, but methodology lacks comparison to recent Korean-specific models beyond English-focused baselines. Code data enhancement mechanism is theoretically sound but lacks ablation studies.

**Low Confidence:** Long-term impact and sustainability of Korean performance remain uncertain without longitudinal studies. Claims about tokenization efficiency improvements lack absolute metrics against established Korean tokenizers.

## Next Checks

1. **Ablation Study on Code Data Impact:** Remove the 37% code component from pretraining and retrain a comparable model to measure the specific contribution of code data to Korean KMMLU performance.

2. **Cross-Lingual Transfer Evaluation:** Test the model's ability to perform reasoning tasks in Korean after being trained predominantly on English code.

3. **Korean-Specific Tokenizer Benchmarking:** Compare GECKO's 32K BPE tokenizer against established Korean tokenizers on Korean-only datasets to quantify claimed efficiency improvements.