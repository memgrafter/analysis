---
ver: rpa2
title: 'LongGenBench: Long-context Generation Benchmark'
arxiv_id: '2410.04199'
source_url: https://arxiv.org/abs/2410.04199
tags:
- longgenbench
- long-context
- performance
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongGenBench, a synthetic benchmark designed
  to evaluate long-context generation capabilities of large language models (LLMs).
  Unlike existing long-context benchmarks that focus on retrieval tasks, LongGenBench
  requires LLMs to generate a single, cohesive long-context response that sequentially
  addresses multiple questions.
---

# LongGenBench: Long-context Generation Benchmark

## Quick Facts
- arXiv ID: 2410.04199
- Source URL: https://arxiv.org/abs/2410.04199
- Authors: Xiang Liu; Peijie Dong; Xuming Hu; Xiaowen Chu
- Reference count: 32
- Key outcome: LongGenBench benchmark reveals 1.2% to 47.1% performance degradation in long-context generation across LLMs

## Executive Summary
This paper introduces LongGenBench, a synthetic benchmark specifically designed to evaluate long-context generation capabilities of large language models. Unlike existing long-context benchmarks that focus on retrieval tasks, LongGenBench requires models to generate a single, cohesive long-context response that sequentially addresses multiple questions. The benchmark synthesizes datasets from popular sources (MMLU, GSM8K, and CommonSenseQA) and redesigns input formats to test models' ability to maintain consistency in logical flow over extended text generation. Extensive evaluations reveal that both API-accessed and open-source models exhibit significant performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%.

## Method Summary
The paper presents LongGenBench, a synthetic benchmark for evaluating long-context generation capabilities of LLMs. The benchmark synthesizes datasets from MMLU, GSM8K, and CommonSenseQA, with questions concatenated into a single prompt requiring sequential answering. The methodology involves redesigning input formats to include multiple questions, requiring LLMs to maintain logical flow and consistency across extended text generation. Performance is measured by analyzing degradation rates and correlations with baseline performance, revealing that models exhibit significant performance drops when generating long, cohesive responses that address multiple questions sequentially.

## Key Results
- Performance degradation in long-context generation ranges from 1.2% to 47.1% across evaluated models
- Correlation exists between baseline performance and long-context generation performance
- Model size impacts performance decline in long-context generation tasks
- Different model architectures show varying levels of degradation in long-context generation

## Why This Works (Mechanism)
The benchmark works by creating a synthetic evaluation framework that specifically tests long-context generation capabilities rather than retrieval-focused tasks. By concatenating multiple questions from established datasets into single prompts, LongGenBench forces models to generate extended, coherent responses while maintaining logical consistency across different question domains. This approach reveals performance degradation that isn't captured by traditional benchmarks, providing insights into how well models can maintain context and coherence over long generation sequences.

## Foundational Learning
- Synthetic dataset construction - Why needed: To create controlled evaluation scenarios that test specific long-context generation capabilities across multiple domains.
  Quick check: Verify that concatenated questions maintain logical flow and relevance across different dataset sources.
- Long-context generation evaluation - Why needed: To measure model performance beyond simple retrieval tasks and assess true generation capabilities.
  Quick check: Compare performance metrics across different generation lengths and question combinations.
- Performance degradation analysis - Why needed: To quantify the impact of context length on model generation quality and identify architectural limitations.
  Quick check: Calculate degradation rates across different model sizes and architectures.

## Architecture Onboarding
- Component map: Synthetic dataset construction -> Long-context prompt generation -> Model inference -> Performance evaluation -> Degradation analysis
- Critical path: Dataset synthesis and formatting is the most critical step, as it directly impacts the validity of the long-context generation evaluation.
- Design tradeoffs: The benchmark prioritizes testing generation capabilities over retrieval, which may not reflect all real-world long-context use cases but provides focused insights into generation quality.
- Failure signatures: Models may exhibit inconsistent logical flow, factual errors across question domains, or inability to maintain context over extended sequences.
- Exactly 3 first experiments:
  1. Reconstruct the synthetic datasets from MMLU, GSM8K, and CommonSenseQA to verify benchmark structure
  2. Test a small subset of models on the benchmark to validate performance degradation ranges
  3. Analyze correlation between baseline and long-context performance using different model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- The specific details on synthetic dataset construction and formatting for LongGenBench are not fully specified, limiting reproducibility
- Exact configurations for model access and inference setup, including maximum output lengths and pricing, are unspecified
- The analysis of correlations between baseline and long-context performance may be affected by variations in model configurations and evaluation conditions

## Confidence
- **High confidence** in the methodology for synthesizing datasets and evaluating long-context generation capabilities
- **Medium confidence** in the reported performance degradation metrics, given the lack of detailed dataset construction and model configuration information
- **Low confidence** in the analysis of correlations between baseline and long-context performance due to unspecified model access and inference details

## Next Checks
1. Verify the synthetic dataset construction by reconstructing LongGenBench using the provided dataset sources (MMLU, GSM8K, CommonSenseQA) and comparing the resulting benchmark structure to the original
2. Replicate the evaluation setup by configuring model access and inference settings (e.g., maximum output lengths, pricing) to match the original study's conditions
3. Conduct an independent analysis of performance degradation and correlations between baseline and long-context generation performance using a subset of the synthetic datasets to validate the reported findings