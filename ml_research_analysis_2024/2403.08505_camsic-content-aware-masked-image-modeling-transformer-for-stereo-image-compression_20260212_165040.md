---
ver: rpa2
title: 'CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image Compression'
arxiv_id: '2403.08505'
source_url: https://arxiv.org/abs/2403.08505
tags:
- image
- transformer
- tokens
- entropy
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning-based stereo image
  compression by introducing a novel framework called CAMSIC. The core method idea
  is to use a decoder-free Transformer entropy model based on content-aware masked
  image modeling (MIM) to capture spatial-disparity dependencies between stereo images.
---

# CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image Compression

## Quick Facts
- arXiv ID: 2403.08505
- Source URL: https://arxiv.org/abs/2403.08505
- Authors: Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
- Reference count: 23
- Primary result: Achieves state-of-the-art rate-distortion performance on stereo image compression with 41.961% and 43.608% bitrate savings on Cityscapes and InStereo2K datasets respectively

## Executive Summary
This paper introduces CAMSIC, a novel framework for learning-based stereo image compression that leverages content-aware masked image modeling (MIM) with a decoder-free Transformer architecture. The key innovation is replacing traditional [MASK] tokens with content-aware tokens derived from prior information (disparity and hyperprior), enabling more effective bidirectional interaction during entropy estimation. By embedding prior information propagation directly into self-attention operations, CAMSIC eliminates the need for a separate Transformer decoder while maintaining or improving compression performance. Experimental results demonstrate significant bitrate savings (41.961% and 43.608% on Cityscapes and InStereo2K) compared to the single image codec ELIC, along with superior performance to other learning-based stereo compression methods.

## Method Summary
CAMSIC addresses stereo image compression by introducing a content-aware masked image modeling Transformer entropy model that captures spatial-disparity dependencies. The framework uses a simple CNN-based image encoder/decoder (similar to ELIC) to independently transform each stereo image into latent representations, along with a hyperprior encoder/decoder for compressing side information. The core innovation is a decoder-free Transformer that uses content-aware MIM, where uninformative [MASK] tokens are replaced with tokens generated from prior information including disparity and hyperprior sources. This enables efficient bidirectional interaction between prior information and estimated tokens during self-attention operations, eliminating the need for an explicit decoder. The model is trained for 400 epochs (MSE) or 300 epochs (MS-SSIM) using Adam optimizer with λ values ranging from 256-8192 (MSE) or 8-256 (MS-SSIM).

## Key Results
- Achieves 41.961% bitrate savings in terms of PSNR on Cityscapes dataset compared to ELIC
- Achieves 43.608% bitrate savings in terms of PSNR on InStereo2K dataset compared to ELIC
- Outperforms other learning-based stereo image compression methods in both compression performance and coding complexity
- Maintains fast encoding and decoding speed due to decoder-free architecture

## Why This Works (Mechanism)

### Mechanism 1: Content-aware Masked Image Modeling
- Claim: Content-aware MIM improves entropy estimation by replacing [MASK] tokens with content-aware tokens derived from prior information
- Mechanism: Content-aware tokens carry specific prior information from disparity and hyperprior sources, enabling more informative interactions with estimated tokens during self-attention operations
- Core assumption: Prior information generation captures relevant spatial-disparity context to meaningfully inform probability distribution estimation
- Evidence anchors: [abstract] "Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens"; [section] "the pre-acquired prior information... is used to generate the content-aware tokens cv"
- Break condition: If prior information fails to capture spatial-disparity dependencies, content-aware tokens won't provide meaningful context

### Mechanism 2: Decoder-free Transformer Architecture
- Claim: Eliminates computational overhead while maintaining/improving compression performance
- Mechanism: Embeds prior information propagation into self-attention operations, making explicit decoder unnecessary
- Core assumption: Encoder self-attention can effectively propagate prior information without separate decoder
- Evidence anchors: [abstract] "which naturally obviates the need for an extra Transformer decoder"; [section] "Since the prior information has sufficiently interacted... an explicit Transformer decoder is no longer needed"
- Break condition: If encoder cannot adequately propagate prior information through self-attention alone, accuracy will degrade

### Mechanism 3: Relative Position Embedding and Identity Embeddings
- Claim: These enhancements improve token interactions and probability distribution estimation accuracy
- Mechanism: Relative position embedding provides position cues while identity embeddings distinguish token types, boosting self-attention effectiveness
- Core assumption: Position information and token type differentiation improve spatial-disparity dependency modeling
- Evidence anchors: [section] "We enable relative position embedding... and assign two identity embeddings to the token sequence to further distinguish different types of tokens"
- Break condition: If position information isn't relevant to stereo image structure or token differentiation doesn't improve interactions

## Foundational Learning

- Concept: Masked Image Modeling (MIM)
  - Why needed here: MIM provides framework for iteratively estimating probability distributions by masking positions and using context to predict masked ones
  - Quick check question: How does mask generation work during inference, and why is consistency between encoder and decoder important?

- Concept: Transformer Architecture and Self-Attention
  - Why needed here: Self-attention enables bidirectional information flow between prior information and estimated tokens for spatial-disparity dependency capture
  - Quick check question: What's the difference between self-attention and cross-attention, and why is self-attention sufficient in decoder-free design?

- Concept: Entropy Modeling and Probability Distribution Estimation
  - Why needed here: Accurate entropy modeling minimizes bitrate while maintaining reconstruction quality, core to learned image compression
  - Quick check question: How does Gaussian distribution parameterization (mean and variance) enable efficient entropy coding of quantized latent representations?

## Architecture Onboarding

- Component map:
  - Image Encoder/Decoder (CNN) -> Quantizer -> Prior Generation (Hyperprior + Disparity) -> Decoder-free Transformer -> Entropy Parameter Module -> Arithmetic Coder

- Critical path: Image → Encoder → Quantizer → Entropy Model (Prior Generation → Decoder-free Transformer → Entropy Parameter) → Arithmetic Coder → Transmission/Storage

- Design tradeoffs:
  - Simplicity vs. Performance: Simple encoder-decoder trades potential gains from sophisticated feature extraction for cleaner implementation and faster processing
  - Decoder-free vs. Encoder-decoder: Eliminates computational overhead but relies on self-attention alone to propagate prior information
  - Content-aware vs. Content-irrelevant MIM: Content-aware tokens provide more informative context but require additional prior generation computation

- Failure signatures:
  - Poor compression performance: Indicates issues with prior information generation or transformer attention mechanism
  - High bitrate at given quality: Suggests inaccurate probability distribution estimation
  - Slow encoding/decoding: May indicate inefficient mask scheduling or transformer processing
  - Visual artifacts: Could result from quantization or inaccurate entropy modeling

- First 3 experiments:
  1. Compare content-aware MIM vs. content-irrelevant MIM with identical transformer architecture to validate core mechanism
  2. Test decoder-free transformer vs. encoder-decoder transformer with identical MIM style to quantify computational savings and performance impact
  3. Evaluate impact of relative position embedding and identity embeddings by removing them individually and measuring bitrate/quality changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CAMSIC's performance compare to other learning-based stereo image compression methods when applied to datasets with different disparity ranges?
- Basis in paper: [inferred] The paper evaluates CAMSIC on Cityscapes (outdoor) and InStereo2K (indoor) datasets but doesn't explicitly compare performance across different disparity ranges
- Why unresolved: Only provides overall performance metrics without analyzing how disparity range affects compression performance compared to other methods
- What evidence would resolve it: Additional experiments comparing CAMSIC's performance across datasets with varying disparity ranges and detailed analysis of disparity effects

### Open Question 2
- Question: What is the impact of the number of decoding steps on rate-distortion performance and computational complexity?
- Basis in paper: [explicit] Paper mentions 8 decoding steps during inference but doesn't explore impact of varying this parameter
- Why unresolved: No analysis of how decoding steps affect trade-off between rate-distortion performance and computational complexity
- What evidence would resolve it: Experiments varying decoding steps with corresponding rate-distortion and computational complexity measurements

### Open Question 3
- Question: How does CAMSIC's content-aware MIM technique compare to other MIM-based approaches in compression performance and computational efficiency?
- Basis in paper: [inferred] Introduces novel content-aware MIM and compares to vanilla MIM but doesn't compare to other MIM-based methods in literature
- Why unresolved: Only provides comparison with vanilla MIM-based approach, doesn't explore performance relative to other MIM-based methods
- What evidence would resolve it: Experiments comparing CAMSIC's content-aware MIM to other MIM-based approaches in terms of compression performance and computational efficiency

## Limitations

- Limited ablation studies: The paper doesn't provide detailed ablation studies on individual components like relative position embedding and identity embeddings to quantify their specific contributions
- Training complexity: The content-aware MIM approach requires additional prior generation computation, potentially increasing training complexity compared to simpler methods
- Generalization across datasets: While performance is evaluated on two datasets, the paper doesn't thoroughly analyze how well the approach generalizes to datasets with different characteristics or disparity ranges

## Confidence

- Claims about mechanism effectiveness: Medium - supported by experimental results but relies on indirect evidence for individual component contributions
- Rate-distortion performance claims: High - backed by comprehensive experimental results on two datasets with multiple metrics
- Computational complexity claims: Medium - supported by architecture design but not explicitly measured in all scenarios
- Generalization claims: Low - limited evaluation to two specific datasets without thorough analysis of performance across different disparity ranges

## Next Checks

1. Reproduce the content-aware MIM vs. content-irrelevant MIM comparison with identical transformer architecture to validate the core mechanism
2. Implement and test the decoder-free transformer architecture with and without relative position embedding and identity embeddings to quantify their individual contributions
3. Vary the number of decoding steps (e.g., 4, 8, 16) and measure the corresponding rate-distortion performance and computational complexity to analyze the trade-offs