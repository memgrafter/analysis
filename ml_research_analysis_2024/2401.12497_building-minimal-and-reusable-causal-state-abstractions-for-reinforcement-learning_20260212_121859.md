---
ver: rpa2
title: Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning
arxiv_id: '2401.12497'
source_url: https://arxiv.org/abs/2401.12497
tags:
- state
- dynamics
- learning
- causal
- abstraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method that learns task-specific minimal
  state abstractions by combining implicit causal dynamics modeling with causal reward
  learning. The key innovation is to jointly infer dynamical and reward causal graphs
  and then derive a task-specific bisimulation abstraction that keeps only reward-influencing
  state variables and their causal ancestors.
---

# Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.12497
- Source URL: https://arxiv.org/abs/2401.12497
- Reference count: 40
- Key outcome: Combines implicit causal dynamics modeling with causal reward learning to learn task-specific minimal state abstractions that improve sample efficiency and generalization

## Executive Summary
This paper presents a method for learning minimal, task-specific state abstractions in reinforcement learning by jointly inferring dynamical and reward causal graphs. The approach uses implicit dynamics models to accurately capture causal relationships in environments with non-smooth dynamics, then applies causal reward modeling to identify reward-relevant state variables and their ancestors. By deriving a bisimulation abstraction from these causal graphs, the method retains only the minimal set of state variables needed for optimal policy learning, significantly improving sample efficiency and generalization across multiple manipulation and control tasks.

## Method Summary
The method combines implicit dynamics modeling with causal reward learning to create task-specific minimal state abstractions. First, an implicit dynamics model is trained using InfoNCE loss with regularization to capture accurate causal relationships. For each task, a causal reward model detects reward-relevant state variables via conditional mutual information. The method then derives a bisimulation abstraction that keeps only reward-influencing state variables and their causal ancestors. Policies are trained using SAC with the derived state abstraction applied as a binary mask, significantly reducing the effective state space while preserving optimal values.

## Key Results
- CBM outperforms prior methods in both prediction accuracy and causal discovery when using implicit dynamics
- Task-specific minimal abstractions yield significantly better sample efficiency and generalization across manipulation and control tasks
- Performance approaches that of an oracle with ground-truth abstraction, often outperforming methods using full state spaces
- CBM maintains accuracy with fewer negative samples (1024-2048 vs 8192), reducing computation time while preserving sample efficiency

## Why This Works (Mechanism)

### Mechanism 1
Implicit dynamics models recover causal relationships more accurately than explicit ones by avoiding overfitting discontinuities through score-based next state estimation rather than direct prediction.

### Mechanism 2
Causal reward modeling identifies task-specific minimal abstractions by finding reward-relevant variables and their ancestors, equivalent to bisimulation that preserves optimal values.

### Mechanism 3
Implicit models with regularization and importance sampling approximations enable accurate CMI estimation for causal discovery by flattening score landscapes and approximating p(si+1|x¬j).

## Foundational Learning

- **Markov Decision Processes and factored state spaces**: CBM operates on factored MDPs where transitions decompose across state variables. Quick check: Why does CBM assume T(st+1|st, at) = ∏ p(si+1|st, at)? What breaks if this assumption is violated?

- **Causal discovery and conditional mutual information**: CBM uses CMI to detect causal dependencies between state variables and rewards. Quick check: How does CBM compute CMI when direct sampling from p(si+1|x¬j) is intractable?

- **Bisimulation and state abstractions**: CBM's task-specific abstraction is equivalent to bisimulation, which preserves optimal values while minimizing state space. Quick check: What makes bisimulation "minimal" and why does CBM's abstraction satisfy this property?

## Architecture Onboarding

- **Component map**: Implicit dynamics model (g, ψ networks) -> Causal reward model (θrew network) -> State abstraction generator -> Policy (SAC)

- **Critical path**: 1. Train implicit dynamics model on offline data 2. For each task: train reward predictor while collecting transitions 3. Update causal graphs and derive task-specific abstraction 4. Apply abstraction as binary mask to policy 5. Update policy via SAC

- **Design tradeoffs**: Implicit vs explicit dynamics (better accuracy vs higher computation); shared dynamics vs task-specific (sample efficiency vs precision); regularization strength (balance between accurate CMI estimation and model capacity)

- **Failure signatures**: High prediction error with explicit dynamics but not implicit; inaccurate causal graphs when importance sampling fails; poor sample efficiency when TIA/denoised MDP assumptions violated

- **First 3 experiments**: 1. Compare implicit vs explicit dynamics on block environment causal graph accuracy 2. Evaluate sample efficiency on Pick task with pretrained dynamics 3. Test generalization to OOD states on Stack task

## Open Questions the Paper Calls Out

### Open Question 1
How would CBM perform on tasks with non-factored, high-dimensional state spaces like images, where the current assumptions about factored state spaces don't hold? The paper mentions extending CBM to high-dimensional state spaces like images as a future direction.

### Open Question 2
What is the impact of using fewer negative samples in implicit dynamics modeling on both computational efficiency and prediction accuracy? The paper shows CBM achieves similar prediction accuracy with 1024 or 2048 samples versus 8192.

### Open Question 3
How sensitive is CBM's performance to the choice of CMI threshold (ϵ) and regularization coefficients (λ1, λ2) in the implicit dynamics model? The paper mentions using grid search to select hyperparameters but doesn't analyze sensitivity.

## Limitations

- Comparison between implicit and explicit dynamics is limited to synthetic environments without systematic ablation studies
- Reliance on importance sampling for CMI estimation may lead to high variance estimates in deterministic environments
- Assumption of factored MDPs may not hold in all environments, limiting applicability

## Confidence

- **High Confidence**: Improved sample efficiency and generalization with CBM across multiple manipulation and control tasks
- **Medium Confidence**: Theoretical claim that CBM's abstraction is equivalent to bisimulation
- **Medium Confidence**: Superiority of implicit dynamics for causal discovery

## Next Checks

1. **Ablation on Dynamics Type**: Test CBM with explicit dynamics models across environments with varying levels of discontinuity to isolate when implicit models provide clear advantages.

2. **CMI Estimation Robustness**: Evaluate CBM's causal graph accuracy under different levels of regularization and with alternative CMI estimation methods to assess sensitivity to approximation choices.

3. **Factored MDP Assumption Violation**: Implement a variant of CBM that relaxes the factored MDP assumption and test on environments with coupled state transitions to identify robustness boundaries.