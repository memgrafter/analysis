---
ver: rpa2
title: Evaluating Language Models for Generating and Judging Programming Feedback
arxiv_id: '2407.04873'
source_url: https://arxiv.org/abs/2407.04873
tags:
- feedback
- language
- programming
- open-source
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the capabilities of open-source and proprietary
  large language models (LLMs) in generating and judging programming feedback. Using
  a dataset of student Python programs, the authors assessed models' abilities to
  explain bugs and suggest fixes, and to evaluate the quality of feedback generated
  by other models.
---

# Evaluating Language Models for Generating and Judging Programming Feedback

## Quick Facts
- arXiv ID: 2407.04873
- Source URL: https://arxiv.org/abs/2407.04873
- Reference count: 40
- Open-source models like Llama-3.1-70B and Phi-3-mini perform competitively with proprietary models like GPT-4o on programming feedback tasks

## Executive Summary
This study evaluates the capabilities of both open-source and proprietary large language models (LLMs) in generating and judging programming feedback. Using a dataset of student Python programs, the authors assess models' abilities to explain bugs and suggest fixes, and to evaluate the quality of feedback generated by other models. The results show that open-source models like Llama-3.1-70B and Phi-3-mini achieve performance on par with proprietary models such as GPT-4o on feedback generation tasks. For feedback evaluation, Llama-3.1-70B demonstrated strong judging ability, rivaling GPT-4o-mini and GPT-4o, particularly when ground truth bug descriptions were provided. The study highlights that open-source models are viable alternatives for both generating and assessing programming feedback, offering benefits in terms of transparency, cost, and accessibility for educational applications.

## Method Summary
The study evaluates LLMs' ability to generate and judge programming feedback using the Socratic guidance benchmark dataset containing 57 introductory Python programming assignments. Models are prompted to generate bug explanations and fix suggestions, which are evaluated using human annotators and LLM judges in two settings: SAG (self-generated solutions) and GAG (ground truth annotations). The evaluation criteria include bug explanation accuracy, completeness, clarity, and fix accuracy. Both open-source models (Gemma-2B, Phi-3-mini, Mistral-7B, Llama-3.1-8B, Llama-3.1-70B) and proprietary models (GPT-3.5-turbo, GPT-4o-mini, GPT-4o) are tested. Performance is measured using weighted F0.5 scores and Cohen's kappa for inter-annotator agreement.

## Key Results
- Open-source models like Llama-3.1-70B and Phi-3-mini perform competitively with proprietary models on bug explanation and fix generation tasks
- Llama-3.1-70B achieves high f0.5 scores comparable to GPT-4o-mini and GPT-4o when judging feedback quality
- Model size does not directly determine performance, with smaller models like Phi-3-mini outperforming larger models like Gemma-2B on specific criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-source LLMs can generate programming feedback comparable to proprietary models
- Mechanism: Open-source models achieve competitive performance on bug explanation and fix generation tasks when evaluated against human-annotated ground truth
- Core assumption: Model size and architecture quality correlate with feedback generation capability
- Evidence anchors:
  - [abstract] "state-of-the-art open-source LLMs are nearly on par with proprietary models in both generating and assessing programming feedback"
  - [section] Table 1 shows Llama-3.1-70B performs on par with GPT-3.5-turbo and competitively with GPT-4o for generating explanations and fixes
  - [corpus] Weak evidence - no directly relevant corpus papers found for this specific mechanism
- Break condition: If model performance drops significantly on tasks requiring complex reasoning or domain-specific knowledge

### Mechanism 2
- Claim: Open-source models can effectively evaluate the quality of feedback generated by other models
- Mechanism: Models like Llama-3.1-70B can serve as judges using either self-generated solutions (SAG) or ground truth references (GAG) with performance comparable to proprietary models
- Core assumption: Language models can assess the quality of their own or other models' outputs using defined criteria
- Evidence anchors:
  - [abstract] "open-source models are competitive with proprietary models for both generating and assessing programming feedback"
  - [section] Table 2 shows Llama-3.1-70B achieves high f0.5 scores comparable to GPT-4o-mini and GPT-4o when judging feedback quality
  - [corpus] Weak evidence - limited corpus support for LLM-as-judge applications in educational contexts
- Break condition: If judge models show systematic bias toward outputs from models of the same family

### Mechanism 3
- Claim: Model size does not directly determine performance for feedback tasks
- Mechanism: Smaller models like Phi-3-mini (3.8B parameters) can outperform larger models like Gemma-2B on specific criteria despite having fewer parameters
- Core assumption: Architecture efficiency and task-specific optimization matter more than raw parameter count
- Evidence anchors:
  - [abstract] "we demonstrate the efficiency of smaller LLMs in these tasks"
  - [section] Phi-3-mini outperforms Gemma-2b on multiple criteria despite similar parameter counts, while Llama-3.1-70B performs competitively with much smaller models
  - [corpus] Weak evidence - no corpus papers directly address parameter efficiency for educational feedback tasks
- Break condition: If performance differences between models of different sizes disappear when using more complex prompts or larger datasets

## Foundational Learning

- Concept: Zero-shot chain-of-thought prompting
  - Why needed here: The study uses this prompting strategy for the SAG (Single Answer Grading) scenario where judges evaluate feedback without reference answers
  - Quick check question: What is the key difference between zero-shot chain-of-thought and few-shot prompting approaches?

- Concept: Reference grading vs self-evaluation
  - Why needed here: The study compares two evaluation scenarios - SAG (self-generated solutions) and GAG (ground truth annotations) to assess judge performance
  - Quick check question: How might the availability of ground truth answers affect the reliability of model evaluations?

- Concept: Ensemble methods for LLM evaluation
  - Why needed here: The study tests whether combining multiple open-source models as a "jury" improves evaluation quality compared to single judges
  - Quick check question: What potential bias might occur when using an ensemble of models from the same family?

## Architecture Onboarding

- Component map: Student code + test cases → Feedback generation → Quality evaluation → Final assessment
- Critical path: Student code → Feedback generation → Quality evaluation → Final assessment
- Design tradeoffs:
  - Open-source vs proprietary models: Cost/transparency vs potential performance differences
  - Single judge vs jury approach: Simplicity vs potential bias reduction
  - SAG vs GAG scenarios: Applicability to real-world use vs evaluation accuracy
- Failure signatures:
  - Over-generation of irrelevant fixes or explanations
  - Consistent bias toward certain model families during evaluation
  - Performance degradation when moving from introductory to advanced programming tasks
- First 3 experiments:
  1. Test Llama-3.1-70B on feedback generation using a small subset of problems to verify Table 1 results
  2. Compare Phi-3-mini and Mistral-7B performance on the SAG scenario to validate ensemble method results
  3. Implement GAG scenario with ground truth annotations to measure improvement over SAG performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can open-source models be fine-tuned to reduce hallucination and improve selectivity in feedback generation?
- Basis in paper: [explicit] The paper mentions that open-source models struggle with hallucination and selectivity, and suggests that fine-tuning techniques might alleviate this issue.
- Why unresolved: While the paper suggests fine-tuning as a potential solution, it does not provide specific methods or evidence of their effectiveness in reducing hallucination and improving selectivity.
- What evidence would resolve it: Experimental results showing improved performance of fine-tuned open-source models in terms of reduced hallucination and improved selectivity in feedback generation tasks.

### Open Question 2
- Question: What is the impact of ensemble size and composition on the performance of LLM juries for feedback evaluation?
- Basis in paper: [inferred] The paper contrasts its jury results with previous work, suggesting that the variability, individual performance, and number of judges in the LLM jury affect judging performance.
- Why unresolved: The paper does not provide a systematic study of how different ensemble sizes and compositions impact the performance of LLM juries.
- What evidence would resolve it: Comparative studies of LLM jury performance with varying ensemble sizes and compositions, showing optimal configurations for feedback evaluation tasks.

### Open Question 3
- Question: How do open-source models perform in generating and judging feedback for programming languages other than Python?
- Basis in paper: [explicit] The paper acknowledges that it only considered introductory programming assignments written in Python and not other programming languages.
- Why unresolved: The study's findings are limited to Python, and there is no evidence of how open-source models would perform in other programming language contexts.
- What evidence would resolve it: Comparative studies of open-source model performance in generating and judging feedback for various programming languages, including results for languages other than Python.

## Limitations
- Evaluation limited to introductory-level Python programming problems, may not generalize to more complex scenarios
- Human evaluation relied on a small number of annotators (8), potentially introducing subjectivity bias
- Study uses zero-shot chain-of-thought prompting, which may not represent optimal prompting strategy
- Cost and accessibility advantages of open-source models were not empirically quantified

## Confidence

- **High Confidence**: Open-source models (particularly Llama-3.1-70B and Phi-3-mini) can generate programming feedback that is competitive with proprietary models on introductory Python problems
- **Medium Confidence**: Open-source models can effectively evaluate the quality of feedback generated by other models, especially when ground truth references are available
- **Medium Confidence**: Model size does not directly determine performance for feedback tasks, with smaller models showing competitive performance

## Next Checks

1. **Generalization Test**: Evaluate the same models on a more diverse dataset that includes intermediate and advanced programming problems, as well as problems in languages beyond Python, to assess whether the observed performance patterns hold across different complexity levels and programming paradigms.

2. **Prompt Optimization Study**: Systematically test different prompting strategies (few-shot prompting, different chain-of-thought formats, multi-turn interactions) to determine if the performance gap between open-source and proprietary models can be further reduced through prompt engineering.

3. **Cost-Benefit Analysis**: Conduct a comprehensive analysis comparing the actual computational costs, inference times, and accessibility factors (API availability, local deployment options) of the top-performing open-source and proprietary models to quantify the practical advantages claimed in the study.