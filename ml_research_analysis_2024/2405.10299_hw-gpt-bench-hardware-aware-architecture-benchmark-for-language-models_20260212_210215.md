---
ver: rpa2
title: 'HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models'
arxiv_id: '2405.10299'
source_url: https://arxiv.org/abs/2405.10299
tags:
- ensemble
- intervals
- observed
- ordered
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HW-GPT-Bench is a hardware-aware benchmark for neural architecture
  search in large language models, focusing on GPT-2 architectures. It provides calibrated
  surrogate models for hardware metrics like latency, energy, and memory, along with
  performance metrics such as perplexity.
---

# HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models

## Quick Facts
- arXiv ID: 2405.10299
- Source URL: https://arxiv.org/abs/2405.10299
- Reference count: 40
- Primary result: Hardware-aware benchmark providing calibrated surrogate models for latency, energy, and memory across 13 devices and 7 search spaces with up to 1.55B parameters

## Executive Summary
HW-GPT-Bench is a hardware-aware benchmark designed for neural architecture search in large language models, specifically focusing on GPT-2 architectures. The benchmark addresses the fundamental challenge of balancing model performance (measured by perplexity) against hardware constraints (latency, energy, and memory) across multiple devices. By employing weight-sharing supernet techniques and advanced surrogate modeling with uncertainty quantification, the benchmark enables efficient exploration of an enormous architectural space (~10^36 possible configurations) while providing calibrated predictions for hardware metrics.

The benchmark distinguishes itself by addressing heteroscedastic noise in hardware measurements through ensemble methods that predict both mean and variance, ensuring reliable uncertainty estimates. It evaluates architectures across 13 diverse devices including GPUs and CPUs, and demonstrates utility through multi-objective optimization simulations. The open-source nature of the benchmark, along with its comprehensive profiling data and surrogate models, provides researchers with a scalable tool for developing hardware-aware neural architecture search algorithms.

## Method Summary
The method employs a weight-sharing supernet trained on a sandwich scheme that samples diverse architectures during training. Individual architectures inherit weights from this supernet based on their configuration, enabling rapid evaluation without retraining. Hardware metrics (latency, energy, memory) are collected across 13 devices for 10,000 sampled architectures per search space. Surrogate models are trained using AutoGluon for latency and energy with uncertainty quantification, while MLP models handle perplexity and memory predictions. The benchmark evaluates 7 search spaces (GPT-S, GPT-M, GPT-L, GPT-S-wide, GPT-M-wide, GPT-L-wide, GPT-XL-wide) with up to 1.55B parameters.

## Key Results
- Provides calibrated surrogate models for hardware metrics (latency, energy, memory) with reliable uncertainty estimates
- Profiles 7 search spaces across 13 devices (8 GPUs, 5 CPUs) with models up to 1.55B parameters
- Demonstrates utility by simulating optimization trajectories of multi-objective algorithms
- Addresses heteroscedastic noise in hardware measurements through ensemble-based uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate models in HW-GPT-Bench provide calibrated predictions and reliable uncertainty estimates for latency and energy, which faithfully model the heteroscedastic noise inherent in hardware measurements.
- Mechanism: AutoGluon is used to build ensembles that predict both the mean and variance of the latency and energy distributions. This allows the surrogate to capture not just the average value but also the variability in measurements across different architectures and devices.
- Core assumption: The noise in hardware measurements (latency, energy) is heteroscedastic, meaning the variance changes with the predicted value (e.g., higher perplexity leads to higher measurement variance).
- Evidence anchors:
  - [abstract]: "Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements."
  - [section]: "From our initial collection of energy and latency observations for different architectures, we observe that on-device latencies and energies tend to be very noisy, and the median of observations is insufficient to capture this noisy distribution... A reliable surrogate model in such a case should not only be performant in terms of accuracy or ranking of the architectures, but on uncertainty quantification and calibration as well."
- Break condition: If the measurement noise is not heteroscedastic or the distribution is not well-captured by the ensemble model, the uncertainty estimates will be unreliable.

### Mechanism 2
- Claim: The weight-sharing supernet approach allows efficient training of a large number of architectures (up to ~10^36) by inheriting pretrained weights, making exhaustive training impractical.
- Mechanism: A single supernet is trained to encompass all possible architectures in the search space. Individual architectures inherit weights from this supernet based on their configuration, allowing rapid evaluation without retraining from scratch. The sandwich scheme ensures diverse sampling of architectures during training.
- Core assumption: The supernet can effectively represent all architectures in the search space, and inheriting weights provides a good approximation of the architecture's performance.
- Evidence anchors:
  - [abstract]: "To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model."
  - [section]: "Since architectures index the same supernetwork to access their weights, all their individual weights are entangled... the sandwich scheme, that at every mini-batch training iteration samples the largest, the smallest, and two random architectures from the search space."
- Break condition: If the supernet cannot adequately represent certain architectures, or if the inherited weights are significantly different from those obtained through independent training, the performance estimates will be inaccurate.

### Mechanism 3
- Claim: The design of the search space, including the use of rotary positional embeddings (RoPE) and parallel residual connections, enhances the efficiency and effectiveness of the architectures within the GPT-2 family.
- Mechanism: RoPE provides adaptable position embeddings that improve the model's ability to handle varying sequence lengths and relative positions. Parallel residual connections speed up computation at larger scales by fusing matrix multiplications.
- Core assumption: These architectural enhancements are beneficial for the GPT-2 family and improve the efficiency of the models.
- Evidence anchors:
  - [section]: "Rotary positional embeddings (RoPE): A form of position embedding that captures absolute positional details using rotation matrices while seamlessly integrating explicit relative positional relationships into the self-attention mechanism... Parallel residual: Following PaLM, in contrast to the standard serialized formulation, we use a parallel formulation in each transformer block... As reported in PaLM, the parallel formulation is faster at larger scales as the MLP and attention input matrix multiplications can be fused."
- Break condition: If these architectural enhancements do not provide the expected benefits, or if they introduce new problems, the overall performance of the architectures may suffer.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: NAS is the underlying framework that HW-GPT-Bench is built upon. Understanding NAS is crucial for grasping how the benchmark evaluates and optimizes architectures.
  - Quick check question: What is the primary goal of NAS, and how does it differ from manual architecture design?

- Concept: Surrogate Models
  - Why needed here: Surrogate models are used to approximate hardware metrics (latency, energy) and performance (perplexity) without the need for exhaustive training and evaluation. Understanding surrogate models is essential for interpreting the results of HW-GPT-Bench.
  - Quick check question: How do surrogate models differ from direct measurement, and what are the advantages and disadvantages of using them?

- Concept: Multi-objective Optimization
  - Why needed here: HW-GPT-Bench is designed to evaluate architectures across multiple objectives (e.g., perplexity, latency, energy). Understanding multi-objective optimization is crucial for interpreting the Pareto fronts and trade-offs presented in the benchmark.
  - Quick check question: What is a Pareto front, and how does it help in identifying optimal architectures across multiple objectives?

## Architecture Onboarding

- Component map: Supernet -> Surrogate Models -> Search Space -> Hardware Devices -> Evaluation Metrics

- Critical path:
  1. Define the search space and create the supernet
  2. Train the supernet using the sandwich scheme
  3. Sample architectures from the search space and evaluate them using the supernet
  4. Collect hardware metrics (latency, energy, memory) for the sampled architectures
  5. Train surrogate models to predict the hardware metrics and performance
  6. Use the surrogate models to evaluate new architectures and identify Pareto-optimal configurations

- Design tradeoffs:
  - Accuracy vs. Efficiency: Using surrogate models instead of direct measurement sacrifices some accuracy for significant gains in efficiency
  - Search Space Size vs. Representativeness: A larger search space allows for more diverse architectures but may be more difficult to explore effectively
  - Hardware Diversity vs. Profiling Cost: Profiling on a wider range of devices provides more comprehensive results but increases the cost and complexity of the benchmark

- Failure signatures:
  - Surrogate models consistently over- or under-predict hardware metrics
  - The identified Pareto-optimal architectures do not perform well when evaluated independently
  - The benchmark fails to identify any architectures that meet the specified constraints

- First 3 experiments:
  1. Evaluate a random sample of architectures using the supernet and compare the results to the ground truth
  2. Train surrogate models to predict latency and energy and assess their accuracy and calibration
  3. Use the surrogate models to optimize for perplexity while minimizing latency and energy on a specific device

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the surrogate models accurately predict hardware metrics for architectures outside the current search space?
- Basis in paper: [explicit] The paper mentions evaluating 10,000 architectures and training surrogates on 80% of the data.
- Why unresolved: The extrapolation capability of the surrogate models to architectures not included in the training set is not discussed.
- What evidence would resolve it: Testing the surrogate models on a set of architectures that are significantly different from those in the training set.

### Open Question 2
- Question: How do the latency and energy measurements vary across different batch sizes and sequence lengths?
- Basis in paper: [explicit] The paper mentions using batch sizes of 8, 4, 1 and 1 for GPT-S, -M, -L and -XL scales respectively, and a sequence length of 1024.
- Why unresolved: The impact of different batch sizes and sequence lengths on the latency and energy measurements is not explored.
- What evidence would resolve it: Profiling the same architectures with varying batch sizes and sequence lengths.

### Open Question 3
- Question: How do the results of the multi-objective optimization algorithms generalize to other search spaces and hardware devices?
- Basis in paper: [explicit] The paper evaluates the performance of several multi-objective optimization algorithms on the HW-GPT-Bench benchmark.
- Why unresolved: The generalization of the results to other search spaces and hardware devices is not discussed.
- What evidence would resolve it: Running the same optimization algorithms on different search spaces and hardware devices.

## Limitations

- Limited to GPT-2 architectures only, with specific architectural choices (RoPE, parallel residuals) that may not generalize to other transformer families
- Single-device performance focus without accounting for distributed training scenarios or multi-device considerations
- Validation relies on correlation metrics rather than extensive ground truth testing of independently trained architectures

## Confidence

**High Confidence**:
- The benchmark successfully profiles 7 search spaces across 13 devices
- The supernet approach enables efficient sampling of ~10^36 architectures
- The API provides reproducible access to the benchmark data

**Medium Confidence**:
- The surrogate models accurately predict latency and energy with calibrated uncertainty estimates
- The identified Pareto-optimal architectures represent true trade-offs between performance and hardware metrics
- The sandwich training scheme effectively covers the architectural space

**Low Confidence**:
- The benchmark's utility for multi-objective optimization algorithms (limited to two algorithms in validation)
- The claim that the benchmark addresses "fundamental weaknesses" in previous hardware-aware benchmarks
- The assertion that the benchmark is "comprehensive" given the focus on GPT-2 architectures only

## Next Checks

1. **Ground Truth Validation**: Select 20-30 architectures from different regions of the Pareto front and train them independently from scratch. Compare their actual perplexity, latency, and energy consumption against the surrogate predictions to assess real-world accuracy.

2. **Cross-Device Transferability**: Evaluate whether surrogate models trained on one device (e.g., RTX 3090) can accurately predict performance on another device (e.g., A100) for the same architecture. This tests the generalizability of the hardware models across similar GPU architectures.

3. **Architectural Robustness**: Modify the search space to include architectures that significantly deviate from the GPT-2 family (e.g., different attention mechanisms, activation functions) and assess whether the surrogate models still provide reasonable predictions or fail catastrophically.