---
ver: rpa2
title: Dual Action Policy for Robust Sim-to-Real Reinforcement Learning
arxiv_id: '2410.12250'
source_url: https://arxiv.org/abs/2410.12250
tags:
- target
- policy
- domain
- action
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Dual Action Policy (DAP), a method for addressing
  dynamics mismatch in sim-to-real reinforcement learning. DAP uses a single policy
  to predict two sets of actions simultaneously: one for maximizing task rewards in
  simulation and another for domain adaptation via reward adjustments.'
---

# Dual Action Policy for Robust Sim-to-Real Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.12250
- Source URL: https://arxiv.org/abs/2410.12250
- Reference count: 29
- Primary result: DAP outperforms baselines on challenging sim-to-real tasks by decoupling action prediction for source optimization and target domain adaptation

## Executive Summary
This paper introduces Dual Action Policy (DAP), a method addressing dynamics mismatch in sim-to-real reinforcement learning by predicting two action sets simultaneously: one for maximizing task rewards in simulation and another for domain adaptation. DAP incorporates uncertainty-based exploration during training to enhance robustness. Experimental results demonstrate DAP's effectiveness in bridging the sim-to-real gap, outperforming baselines on challenging tasks in simulation. Incorporating uncertainty estimation further improved performance, nearly matching optimal results in some cases.

## Method Summary
DAP uses a single policy network to predict two distinct action sets: asrc for source environment interaction and atgt for target domain adaptation. The policy optimizes a combined objective with three components: task reward maximization, domain adaptation via reward adjustments, and L2 regularization between the two action sets. An ensemble of domain classifiers estimates epistemic uncertainty, which guides action resampling during training to encourage exploration in uncertain regions. The method operates with abundant source environment interactions and limited target data, making it practical for real-world deployment scenarios.

## Key Results
- DAP outperforms DARC, GARAT, and H2O baselines on tested locomotion tasks
- Uncertainty-based exploration (k=0.1) significantly improves performance, nearly matching optimal results
- DAP remains effective with limited target data, demonstrating robustness to data scarcity
- Optimal regularization parameter λ found to be 0.1, balancing constraint tightness and exploration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling action sets enables separate optimization of task reward and dynamics adaptation
- Mechanism: By predicting two actions simultaneously—one for source environment interaction and one for target dynamics—the policy can optimize task returns while using reward adjustments to encourage actions that mimic target domain behavior
- Core assumption: The two action sets can be effectively optimized together within a single policy without destructive interference
- Evidence anchors: [abstract], [section 4.1]

### Mechanism 2
- Claim: Uncertainty-based action resampling improves robustness by encouraging exploration in high-epistemic-uncertainty regions
- Mechanism: During training, actions are perturbed with magnitude proportional to uncertainty estimates (calculated from ensemble disagreement), forcing the agent to explore uncertain state-action distributions and learn to self-correct
- Core assumption: High ensemble disagreement correlates with areas where the agent might behave erroneously in the target environment
- Evidence anchors: [section 4.2], [section 5.3]

### Mechanism 3
- Claim: Regularization term ||asrc - atgt||² prevents target policy from generating actions that deviate too far from source-reachable states
- Mechanism: The L2 distance between the two action sets constrains the target policy to generate actions that remain within the state distribution reachable by the source policy
- Core assumption: Actions that deviate too much from the source policy's reach are likely to be infeasible in the target domain
- Evidence anchors: [section 4.1], [section 5.3a]

## Foundational Learning

- Concept: Domain adaptation in reinforcement learning
  - Why needed here: The paper addresses dynamics mismatch between simulation and real environments, requiring techniques to adapt learned policies
  - Quick check question: What is the main difference between DAP and traditional domain randomization approaches?

- Concept: Ensemble methods for uncertainty estimation
  - Why needed here: DAP uses ensemble of domain classifiers to estimate epistemic uncertainty for action resampling
  - Quick check question: How does ensemble disagreement relate to epistemic uncertainty in this context?

- Concept: Maximum entropy reinforcement learning
  - Why needed here: The paper uses SAC (Soft Actor-Critic) which optimizes for both expected return and policy entropy
  - Quick check question: What role does the entropy term play in the DAP objective function?

## Architecture Onboarding

- Component map: Single policy network → outputs [asrc, atgt] → source env uses asrc for sampling, target env uses atgt for deployment → ensemble of N domain classifiers → uncertainty calculation → action resampling → environment step → reward calculation (including adjustment) → buffer storage → SAC update → classifier update
- Critical path: Policy forward pass → action prediction → uncertainty calculation → optional resampling → environment step → reward calculation (including adjustment) → buffer storage → SAC update → classifier update
- Design tradeoffs: Single policy vs. separate models for source/target actions (complexity vs. specialization); ensemble size vs. computation (N=5 chosen); λ regularization strength vs. constraint tightness
- Failure signatures: High λ → DAP behaves like DARC; low λ → target policy generates infeasible actions; wrong k → unstable training or insufficient exploration
- First 3 experiments:
  1. Run DAP with λ=0.1, k=0.1 on Walker2D task, compare against DARC baseline
  2. Vary λ parameter (0.05, 0.1, 0.2) to find optimal regularization
  3. Test DAP with different ensemble sizes (N=3, 5, 7) to evaluate uncertainty estimation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the regularization hyperparameter λ in DAP for different types of dynamics mismatch scenarios?
- Basis in paper: The paper mentions an optimal range for λ that preserves DAP's unique benefits while ensuring sufficient closeness between the policies, but does not specify this range or how it varies with different dynamics mismatch scenarios.
- Why unresolved: The paper only mentions that λ was fixed at 0.10 for all experiments except for the ablation study, which suggests that the optimal value might depend on the specific task or dynamics mismatch.
- What evidence would resolve it: Experimental results showing the performance of DAP with different values of λ for various dynamics mismatch scenarios would help determine the optimal value of λ for each scenario.

### Open Question 2
- Question: How does the performance of DAP scale with the size of the offline dataset collected from the target environment?
- Basis in paper: The paper mentions an ablation study on the size of the target dataset, but does not provide a clear answer on how the performance scales with the dataset size.
- Why unresolved: The ablation study only evaluates a few specific dataset sizes, and does not provide a clear trend or relationship between the dataset size and the performance of DAP.
- What evidence would resolve it: Experimental results showing the performance of DAP with varying sizes of the target dataset would help determine how the performance scales with the dataset size.

### Open Question 3
- Question: How does the performance of DAP compare to other domain adaptation methods when dealing with more complex dynamics mismatch scenarios?
- Basis in paper: The paper mentions that DAP outperforms all other baselines on the tested tasks, but does not provide a comparison with other domain adaptation methods for more complex dynamics mismatch scenarios.
- Why unresolved: The tested tasks in the paper are relatively simple, and it is unclear how DAP would perform on more complex dynamics mismatch scenarios.
- What evidence would resolve it: Experimental results comparing the performance of DAP with other domain adaptation methods on more complex dynamics mismatch scenarios would help determine the effectiveness of DAP in such scenarios.

## Limitations
- Limited experimental diversity (3 tasks, 1 domain shift type) restricts generalizability claims
- No systematic comparison against model-based approaches that could handle dynamics mismatch through system identification
- Computational overhead during deployment not fully characterized, particularly for maintaining ensemble classifiers
- Generalizability beyond locomotion tasks to other robotics domains remains unverified

## Confidence
- High confidence: Core DAP mechanism (decoupled action prediction with regularization) based on clear algorithmic specification and ablation studies
- Medium confidence: Claimed performance improvements due to limited experimental diversity
- Low confidence: Claimed robustness improvements without systematic testing across varying degrees of dynamics mismatch

## Next Checks
1. Test DAP across multiple domain shift types (not just dynamics mismatch) to verify general applicability
2. Conduct systematic ablation on ensemble size (N=3, 5, 7, 10) to determine minimum effective size for uncertainty estimation
3. Compare computational efficiency during deployment against baseline methods to assess practical deployment costs