---
ver: rpa2
title: 'MahaSQuAD: Bridging Linguistic Divides in Marathi Question-Answering'
arxiv_id: '2404.13364'
source_url: https://arxiv.org/abs/2404.13364
tags:
- dataset
- answer
- marathi
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the lack of efficient question-answering
  datasets in low-resource languages by introducing MahaSQuAD, the first full SQuAD
  dataset for Marathi, containing 118,516 training, 11,873 validation, and 11,803
  test samples. The study develops a robust methodology for translating SQuAD into
  Marathi while accurately mapping translated answers to their corresponding spans
  in the translated context.
---

# MahaSQuAD: Bridging Linguistic Divides in Marathi Question-Answering

## Quick Facts
- arXiv ID: 2404.13364
- Source URL: https://arxiv.org/abs/2404.13364
- Authors: Ruturaj Ghatage; Aditya Kulkarni; Rajlaxmi Patil; Sharvi Endait; Raviraj Joshi
- Reference count: 9
- Introduces MahaSQuAD, the first full SQuAD dataset for Marathi with 118,516 training examples

## Executive Summary
This research addresses the critical gap in question-answering datasets for low-resource languages by creating MahaSQuAD, a comprehensive Marathi QA dataset translated from SQuAD. The study develops an innovative methodology for translating SQuAD while accurately mapping translated answers to their corresponding spans in the translated context. Monolingual Marathi models (MahaBERT and MahaRoBERTa) demonstrate superior performance compared to multilingual models, achieving 51.28% Exact Match and 54.88% F1 scores. The research provides a scalable approach for creating QA datasets in other low-resource languages and contributes valuable resources for Marathi language technology development.

## Method Summary
The methodology involves translating the English SQuAD dataset into Marathi using a sentence-level translation approach with similarity-based answer span identification. The pipeline splits contexts into sentences, translates each sentence individually, identifies the sentence containing the answer, and uses similarity scoring to map the translated answer to the best matching substring in the translated sentence. Named entities are transliterated into Devanagari script for consistency. The resulting dataset (118,516 training, 11,873 validation, 11,803 test samples) is used to fine-tune monolingual (MahaBERT, MahaRoBERTa) and multilingual (mBERT, MuRIL-BERT) models, with performance evaluated using Exact Match, F1, and BLEU scores.

## Key Results
- MahaSQuAD contains 118,516 training, 11,873 validation, and 11,803 test samples
- MahaBERT achieves 51.28% Exact Match and 54.88% F1 score on the dataset
- Monolingual models (MahaBERT, MahaRoBERTa) outperform multilingual models (mBERT, MuRIL-BERT) on Marathi QA
- A manually verified gold test set of 500 examples validates dataset quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level translation with similarity-based answer mapping enables accurate alignment of translated answers to context spans in low-resource languages.
- Mechanism: The pipeline splits the English context into sentences, translates each sentence individually, then identifies the sentence containing the answer. The answer is translated alongside that sentence, and the translated answer is matched to the best substring in the translated sentence using similarity scoring. This allows dynamic adjustment of answer spans when direct character-by-character mapping fails due to linguistic differences.
- Core assumption: The answer phrase remains largely intact within its original sentence after translation, and the best matching substring in the translated sentence corresponds to the correct answer.
- Evidence anchors:
  - [abstract] "we need a robust method to map the answer translation to its span in the translated passage"
  - [section] "We iterate over all phrases of varying lengths in sentences calculate the similarity of each substring with the answer and store it in a 2D matrix"
  - [corpus] Weak: No corpus example of similarity scoring success/failure; only mentions use of MahaNLP SimilarityAnalyzer.
- Break condition: If the answer spans multiple sentences or undergoes significant rephrasing, the similarity-based substring matching may fail to capture the full answer.

### Mechanism 2
- Claim: Transliteration of named entities into Devanagari ensures script uniformity and improves model training stability for Marathi.
- Mechanism: After translating the dataset, the pipeline applies AI4Bharat's Transliteration Engine to convert English-named entities (e.g., numbers, special characters, brand names) into Devanagari script, ensuring consistency across the dataset and reducing model confusion from mixed scripts.
- Core assumption: Consistent script usage across all tokens in the dataset reduces noise and improves model performance on entity recognition tasks.
- Evidence anchors:
  - [section] "Lastly, we transliterate over the entire processed Marathi dataset to convert named entities into the Devanagari Script"
  - [section] "This approach ensured that the majority of the examples contained minimal to no English characters after transliteration"
  - [corpus] Weak: No corpus metrics showing impact of transliteration on downstream model accuracy.
- Break condition: If transliteration introduces ambiguous or incorrect mappings for certain entities, it could degrade answer extraction accuracy.

### Mechanism 3
- Claim: Monolingual Marathi models (MahaBERT, MahaRoBERTa) outperform multilingual models on Marathi QA due to language-specific pretraining and fine-tuning.
- Mechanism: The models are trained on large Marathi monolingual corpora (L3Cube-MahaCorpus) before being fine-tuned on the MahaSQuAD dataset, allowing them to internalize Marathi-specific linguistic patterns and vocabulary more deeply than multilingual models trained on many languages simultaneously.
- Core assumption: Language-specific pretraining provides a stronger foundation for downstream task performance than shared multilingual embeddings when sufficient monolingual data is available.
- Evidence anchors:
  - [abstract] "Monolingual models MahaBERT and MahaRoBERTa outperform multilingual models mBERT and MuRIL-BERT on this dataset"
  - [section] "MahaBERT and MahaROBERTa excel in exact match accuracy because they are fine-tuned and they deeply understand the nuances of that language"
  - [corpus] Weak: No direct corpus comparison of embedding spaces or feature overlap between models.
- Break condition: If the monolingual corpus is too small or unrepresentative, the advantage over multilingual models may diminish.

## Foundational Learning

- Concept: Sentence tokenization and alignment in translation tasks
  - Why needed here: The answer span identification depends on correctly identifying which sentence contains the answer before translation; misalignment breaks the entire mapping pipeline.
  - Quick check question: If a sentence containing the answer is split incorrectly during tokenization, what is the likely effect on answer span extraction?

- Concept: Similarity scoring and substring matching for answer alignment
  - Why needed here: Direct character mapping between English and Marathi contexts is unreliable due to linguistic differences; similarity-based matching dynamically finds the best answer span in the translated sentence.
  - Quick check question: What happens to the answer span mapping if the similarity score threshold is set too high?

- Concept: Transliteration and script normalization in multilingual NLP
  - Why needed here: Mixed scripts in the dataset introduce noise and inconsistency; transliteration ensures uniform representation of entities and numbers in Marathi.
  - Quick check question: Why is it important to transliterate numbers into Devanagari numerals for Marathi QA datasets?

## Architecture Onboarding

- Component map: Data preprocessing (tokenization, translation, similarity scoring, transliteration) → Model training (MahaBERT/MahaRoBERTa/mBERT/MuRIL-BERT) → Evaluation (EM, F1, BLEU) → Gold testset validation
- Critical path: Translation pipeline (sentence splitting → context/ans translation → similarity matching → transliteration) → Dataset creation (train/val/test/gold splits) → Model fine-tuning → Performance evaluation
- Design tradeoffs: Sentence-level translation improves answer span accuracy but may miss cross-sentence answers; transliteration improves script consistency but risks entity corruption; monolingual models give better accuracy but require large monolingual corpora
- Failure signatures: Low similarity scores in answer mapping; large discrepancies between predicted and gold answer spans; high proportion of empty predictions in multilingual models
- First 3 experiments:
  1. Run the translation pipeline on a small fixed SQuAD subset and manually verify answer span accuracy.
  2. Compare MahaBERT vs mBERT performance on a held-out gold testset to quantify monolingual advantage.
  3. Evaluate impact of transliteration by training two versions of the model (with and without transliteration) and measuring exact match difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MahaBERT and MahaRoBERTa compare when trained on smaller subsets of the MahaSQuAD dataset to determine the minimum dataset size required for effective performance?
- Basis in paper: [inferred] The paper evaluates model performance on the full MahaSQuAD dataset but does not explore the impact of dataset size on model performance.
- Why unresolved: The paper does not provide information on how the size of the training dataset affects the performance of the models.
- What evidence would resolve it: Experiments training MahaBERT and MahaRoBERTa on progressively smaller subsets of MahaSQuAD and evaluating their performance on the gold test set.

### Open Question 2
- Question: How do the performance metrics (EM, F1, BLEU) of MahaBERT and MahaRoBERTa on the gold test set compare to those of human annotators on the same set?
- Basis in paper: [inferred] The paper introduces a gold test set manually verified by annotators but does not compare model performance to human performance.
- Why unresolved: The paper does not provide a comparison between model performance and human-level performance on the gold test set.
- What evidence would resolve it: Having human annotators answer questions on the gold test set and calculating their EM, F1, and BLEU scores to compare with the model scores.

### Open Question 3
- Question: How robust is the proposed methodology for translating SQuAD into other low-resource languages, and what are the specific challenges and adaptations required for different language families?
- Basis in paper: [explicit] The paper mentions that the methodology can be extended to other low-resource languages but does not provide detailed experiments or results for languages other than Marathi.
- Why unresolved: The paper does not explore the application of the methodology to languages other than Marathi, nor does it discuss the potential challenges and adaptations required for different language families.
- What evidence would resolve it: Applying the methodology to translate SQuAD into other low-resource languages, such as Telugu or Tamil, and documenting the specific challenges and adaptations required for each language.

## Limitations

- The similarity-based answer span identification mechanism lacks specified threshold values and may fail for answers spanning multiple sentences or undergoing significant semantic shifts
- Transliteration impact on model performance is supported only by qualitative observation without systematic quantitative validation
- Model performance comparisons are limited to only four model variants without ablation studies examining individual contributions of monolingual pretraining versus fine-tuning

## Confidence

- High confidence: Dataset creation methodology and basic translation pipeline functionality
- Medium confidence: Monolingual model performance advantage claims
- Low confidence: Similarity scoring threshold optimization and transliteration impact assessment

## Next Checks

1. **Similarity Score Validation**: Conduct a systematic analysis of similarity score distributions across correctly and incorrectly mapped answer spans. Determine optimal threshold values and evaluate performance sensitivity to threshold variations.

2. **Transliteration Impact Study**: Train parallel models with and without transliteration applied to the dataset. Measure differences in exact match scores and analyze specific error patterns to quantify transliteration's contribution to performance.

3. **Cross-linguistic Generalization Test**: Apply the translation pipeline to create QA datasets for two additional low-resource languages with different linguistic properties (e.g., morphologically rich vs. isolating languages). Compare answer span mapping accuracy and model performance across languages to assess pipeline robustness.