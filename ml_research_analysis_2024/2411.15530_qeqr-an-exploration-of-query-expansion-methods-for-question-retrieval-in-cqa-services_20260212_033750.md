---
ver: rpa2
title: 'QEQR: An Exploration of Query Expansion Methods for Question Retrieval in
  CQA Services'
arxiv_id: '2411.15530'
source_url: https://arxiv.org/abs/2411.15530
tags:
- question
- expansion
- questions
- words
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of query expansion methods to address
  the lexical gap problem in question retrieval within community question answering
  (CQA) services. The authors investigate word-similarity-based methods, propose a
  question-similarity-based method using contextualized word embeddings (ELMo), and
  introduce selective expansion to exclude central words that could alter the question's
  intent.
---

# QEQR: An Exploration of Query Expansion Methods for Question Retrieval in CQA Services

## Quick Facts
- **arXiv ID**: 2411.15530
- **Source URL**: https://arxiv.org/abs/2411.15530
- **Reference count**: 40
- **Primary result**: ELMo-based query expansion with pseudo relevance feedback and selective expansion improves MAP by 1.8% on Yahoo! Answers

## Executive Summary
This paper addresses the lexical gap problem in community question answering (CQA) services through various query expansion methods. The authors explore word-similarity-based approaches, introduce a novel question-similarity-based method using ELMo contextualized embeddings, and propose selective expansion to preserve question intent. Their comprehensive experiments on Yahoo! Answers data demonstrate that combining ELMo-based expansion with pseudo relevance feedback and selective expansion achieves a 1.8% relative improvement in Mean Average Precision (MAP) compared to existing baselines.

## Method Summary
The authors investigate multiple query expansion strategies for CQA retrieval. They first explore traditional word-similarity-based methods using word embeddings, then propose a novel question-similarity-based approach that leverages ELMo contextualized embeddings to capture semantic relationships between questions. A key innovation is their selective expansion strategy, which identifies and excludes central words from expansion to prevent alteration of the question's original intent. The methods are evaluated using pseudo relevance feedback and tested on Yahoo! Answers data to measure their effectiveness in improving retrieval performance.

## Key Results
- ELMo-based expansion with pseudo relevance feedback achieves a 1.8% relative improvement in MAP compared to best baseline
- Selective expansion effectively preserves question intent by excluding central words from expansion
- The proposed methods successfully find more relevant questions than traditional approaches

## Why This Works (Mechanism)
The effectiveness stems from ELMo's ability to capture contextualized word meanings and semantic relationships between questions. By using question-level embeddings rather than individual word similarities, the method better understands the overall semantic intent. The selective expansion mechanism preserves the core meaning by preventing over-expansion of central terms that could distort the original question's focus.

## Foundational Learning

### ELMo contextualized embeddings
- **Why needed**: Capture word meanings based on their context rather than static representations
- **Quick check**: Words with multiple senses (e.g., "bank") should have different embeddings in different contexts

### Pseudo relevance feedback
- **Why needed**: Automatically identify relevant terms from top retrieved results to improve initial query
- **Quick check**: Top-K retrieved items should share common terms that can enhance the query

### Selective expansion
- **Why needed**: Prevent expansion of central words that could alter the question's semantic intent
- **Quick check**: Expanded queries should maintain the original question's focus while adding relevant terms

## Architecture Onboarding

**Component map**: Original query -> ELMo embedding generation -> Similarity computation -> PRF term selection -> Selective expansion -> Final query

**Critical path**: Query processing → ELMo embedding computation → Question similarity ranking → PRF term extraction → Selective expansion → Final retrieval

**Design tradeoffs**: 
- Trade-off between expansion coverage and query intent preservation
- Computational cost of ELMo embeddings versus improved semantic matching
- Balance between precision gains and potential recall loss

**Failure signatures**: 
- Over-expansion causing retrieval of topically unrelated questions
- Under-expansion failing to bridge lexical gaps
- Selective expansion incorrectly identifying central words

**First 3 experiments to run**:
1. Baseline retrieval without any expansion
2. ELMo-based expansion without selective expansion
3. Standard word-similarity-based expansion with PRF

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental results limited to Yahoo! Answers dataset, may not generalize to other CQA platforms
- 1.8% MAP improvement, while meaningful, represents modest practical gain
- Selective expansion assumptions may not hold for all question types

## Confidence

**Major Claim Confidence Assessment:**
- ELMo-based expansion with PRF effectiveness: **High** - supported by direct experimental comparisons
- Selective expansion preserving question intent: **Medium** - based on reasonable assumptions but limited empirical validation
- 1.8% MAP improvement significance: **Medium** - statistically demonstrated but modest in practical terms

## Next Checks

1. Replicate experiments across multiple CQA datasets (Stack Overflow, Quora, Zhihu) to assess cross-platform generalizability
2. Conduct ablation studies specifically isolating the impact of selective expansion versus standard expansion to quantify its true contribution
3. Measure computational overhead and latency introduced by the ELMo-based expansion approach in realistic deployment scenarios