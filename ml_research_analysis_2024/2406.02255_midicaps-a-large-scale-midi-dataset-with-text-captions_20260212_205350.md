---
ver: rpa2
title: 'MidiCaps: A large-scale MIDI dataset with text captions'
arxiv_id: '2406.02255'
source_url: https://arxiv.org/abs/2406.02255
tags:
- midi
- music
- dataset
- captions
- files
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MidiCaps, the first large-scale MIDI dataset
  with text captions. The authors address the lack of captioned MIDI datasets, which
  has hindered research in text-to-MIDI models.
---

# MidiCaps: A large-scale MIDI dataset with text captions

## Quick Facts
- arXiv ID: 2406.02255
- Source URL: https://arxiv.org/abs/2406.02255
- Reference count: 0
- Dataset: 168,407 MIDI files with descriptive text captions

## Executive Summary
This paper introduces MidiCaps, the first large-scale MIDI dataset with text captions. The authors address the lack of captioned MIDI datasets, which has hindered research in text-to-MIDI models. They extract musical features from MIDI files, including tempo, chord progression, time signature, instruments, genre, and mood. Using in-context learning with Claude 3 Opus, they generate captions based on these features. The resulting MidiCaps dataset contains 168,407 MIDI files with descriptive text captions. A listening study with general audience and music expert participants evaluates the quality of the captions, showing high agreement between the generated captions and the musical content. The dataset is made available under a Creative Commons license, enabling further research in MIDI and large language models.

## Method Summary
The MidiCaps dataset is created by extracting musical features from MIDI files in the Lakh MIDI Dataset, then using in-context learning with Claude 3 Opus to generate descriptive captions based on these features. The pipeline involves preprocessing to filter faulty files, dual feature extraction (MIDI and synthesized audio), caption generation using a prompt with 17 human-annotated examples, and validation through listening studies with general audience and music expert participants.

## Key Results
- Dataset contains 168,407 MIDI files with descriptive text captions
- Listening study shows general audience ratings of 5.63 for AI-generated captions vs 5.46 for human-written captions
- Expert listeners provide more reliable ratings for key and chord matching aspects
- High agreement between generated captions and musical content across multiple dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning enables effective caption generation without fine-tuning the LLM
- Mechanism: By providing a small set of human-annotated feature-caption pairs as examples in the prompt, Claude 3 learns to generate accurate captions for new MIDI files based on their extracted features
- Core assumption: The LLM can generalize from 17 examples to generate high-quality captions for 168K+ files
- Evidence anchors:
  - [abstract]: "Using in-context learning with Claude 3 Opus, we generate captions based on these features"
  - [section]: "we furnish the LLM with instructions to generate captions based on the extracted music features, supplemented by a small set of feature-caption pairs created by expert annotators"
  - [corpus]: Weak - no corpus evidence directly supports this mechanism, though related work mentions in-context learning
- Break condition: If the feature extraction is noisy or incomplete, the LLM cannot compensate through in-context learning

### Mechanism 2
- Claim: Extracting features from both MIDI and synthesized audio provides comprehensive musical information
- Mechanism: MIDI features (key, tempo, instruments) capture symbolic information while audio features (genre, mood, chord progressions) capture perceptual aspects, creating richer captions
- Core assumption: Synthesized audio sufficiently represents the musical content for reliable feature extraction
- Evidence anchors:
  - [section]: "To extract genre and mood, we use Essentia... We keep the top two genre tags with the highest confidence score, and the top five mood/theme tags"
  - [section]: "We use the Midi2Audio library [25] that utilizes FluidSynth [26, 27] to synthesize audio from MIDI with the Fluid Release 3 General-MIDI sound font"
  - [corpus]: Weak - corpus contains related work but no direct evidence of this dual extraction approach
- Break condition: If the sound font or synthesis quality is poor, audio-based features become unreliable

### Mechanism 3
- Claim: The listening study validates caption quality through both general audience and expert perspectives
- Mechanism: Comparing AI-generated captions against human-written ones across multiple dimensions (genre, mood, key, chord, tempo) provides robust quality assessment
- Core assumption: Participants can accurately judge caption-music alignment even without perfect audio-text correspondence
- Evidence anchors:
  - [section]: "listeners were asked to rate these captions in seven aspects... The average rating for overall matching of the text caption with the MIDI file for the general audience is even slightly higher (5.63) for the AI generated caption compared to the human-written caption (5.46)"
  - [section]: "In terms of key and chord matching, the general audience provide good ratings. For these questions the ratings from the music experts, however, are more reliable"
  - [corpus]: Weak - corpus contains no related work on listening studies for caption evaluation
- Break condition: If participants cannot accurately perceive musical features or if the study design introduces bias

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Enables caption generation without expensive fine-tuning of the LLM
  - Quick check question: How many examples are typically needed for effective in-context learning in caption generation tasks?

- Concept: Music feature extraction
  - Why needed here: Provides the semantic content that captions describe, enabling the LLM to generate relevant text
  - Quick check question: What are the key differences between symbolic (MIDI) and perceptual (audio) features in music?

- Concept: Multimodal alignment
  - Why needed here: Ensures that text descriptions accurately represent musical content across different representation modalities
  - Quick check question: How do you measure alignment quality between textual descriptions and musical features?

## Architecture Onboarding

- Component map:
  MIDI file → Preprocessing → Feature extraction → Prompt construction → Claude 3 caption generation → Validation

- Critical path:
  MIDI file → Preprocessing → Feature extraction → Prompt construction → Claude 3 caption generation → Validation

- Design tradeoffs:
  - Feature richness vs. extraction speed: More features provide better captions but increase processing time
  - Sound font choice: Affects audio-based feature quality but must be consistent across dataset
  - Example selection: 17 examples must be diverse enough for generalization but few enough for token limits

- Failure signatures:
  - Low listening study ratings across multiple dimensions
  - Inconsistent feature extraction across similar MIDI files
  - Claude 3 generating off-topic or hallucinated content

- First 3 experiments:
  1. Test feature extraction consistency: Run 100 MIDI files through the pipeline twice and compare extracted features for consistency
  2. Validate in-context learning: Generate captions for 50 files using different numbers of example pairs (5, 10, 17) to find optimal tradeoff
  3. Evaluate sound font impact: Generate audio with two different sound fonts and compare genre/mood extraction results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of sound font affect the accuracy of genre and mood classification in the synthesized audio?
- Basis in paper: [explicit] The paper mentions that "extracting features from synthesized audio files is not optimal, as the choice of the sound font has an impact on the obtained results, which is likely to be most apparent in genre and mood features."
- Why unresolved: The paper acknowledges this limitation but does not provide specific experiments or comparisons between different sound fonts to quantify the impact on feature extraction accuracy.
- What evidence would resolve it: Comparative studies using different sound fonts and measuring the resulting accuracy of genre and mood classification, ideally across diverse MIDI datasets.

### Open Question 2
- Question: Can the MidiCaps framework be extended to handle longer music pieces effectively?
- Basis in paper: [inferred] The paper notes that "the task of automatically labelling files of various length is difficult by nature as longer music pieces might require more text to be described precisely, while shorter pieces may need only a single sentence."
- Why unresolved: The current framework and evaluation focus on relatively short pieces (3 seconds to 15 minutes). There is no information on how well the framework scales to longer compositions or orchestral works.
- What evidence would resolve it: Testing the framework on a diverse set of longer MIDI pieces and comparing the quality of generated captions with human annotations.

### Open Question 3
- Question: How can the accuracy of chord pattern extraction be improved for complex musical pieces?
- Basis in paper: [explicit] The paper states that "extracting a single 'main' pattern (3-5 chords) from the entire list of extracted chords is challenging as there are many different cases, e.g., very short fragments of a few chords, and very long pieces with many chord patterns."
- Why unresolved: The current method for selecting the most frequent chord pattern is based on a set of rules that may not capture the full complexity of musical compositions, especially those with frequent modulations or complex harmonies.
- What evidence would resolve it: Development and testing of more sophisticated algorithms for chord pattern extraction that can handle a wider range of musical styles and complexities, validated through listening studies with music experts.

## Limitations

- Synthesized audio feature extraction is suboptimal and sound font choice impacts genre and mood classification accuracy
- The framework may not scale well to longer music pieces requiring more detailed descriptions
- Chord pattern extraction for complex musical pieces remains challenging with current methodology

## Confidence

**High Confidence:** The dataset creation pipeline (feature extraction → prompt generation → LLM caption generation) is technically sound and follows established practices in both music information retrieval and LLM applications.

**Medium Confidence:** The listening study methodology provides useful validation, though the results showing general audience preferring AI captions over human ones requires further investigation to rule out potential bias or misunderstanding.

**Low Confidence:** The generalizability of the in-context learning approach without knowing the specific examples used, and the assumption that synthesized audio provides reliable perceptual features across all MIDI files.

## Next Checks

1. **Feature Extraction Consistency Test:** Run 100 MIDI files through the complete feature extraction pipeline twice using different hardware/versions to measure consistency in extracted features (key, tempo, genre, mood). Any significant variation indicates fragility in the pipeline.

2. **Cross-Cultural Listening Validation:** Replicate the listening study with participants from different cultural backgrounds and musical training levels to assess whether the caption quality ratings are culturally biased or universally applicable.

3. **Sound Font Impact Analysis:** Generate synthesized audio using at least three different sound fonts (Fluid, TimGM6, GeneralUser) and compare the resulting genre and mood classifications to quantify the sensitivity of audio-based features to synthesis choices.