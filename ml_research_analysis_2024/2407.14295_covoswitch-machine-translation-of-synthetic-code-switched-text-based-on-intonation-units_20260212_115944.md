---
ver: rpa2
title: 'CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on Intonation
  Units'
arxiv_id: '2407.14295'
source_url: https://arxiv.org/abs/2407.14295
tags:
- translation
- languages
- code-switching
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoVoSwitch, a synthetic code-switched dataset
  created by replacing intonation units in CoVoST 2 speech-to-text translation data
  using the PSST prosodic segmentation model. The dataset covers 13 languages and
  enables evaluation of code-switching translation performance in multilingual models
  M2M-100 and NLLB-200.
---

# CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on Intonation Units

## Quick Facts
- arXiv ID: 2407.14295
- Source URL: https://arxiv.org/abs/2407.14295
- Reference count: 22
- Key outcome: Synthetic code-switched data improves translation performance for csw→En, especially for low-resource languages, but causes significant degradation for csw→X due to off-target problem

## Executive Summary
This paper introduces CoVoSwitch, a synthetic code-switched dataset created by replacing intonation units in CoVoST 2 speech-to-text translation data using the PSST prosodic segmentation model. The dataset enables evaluation of code-switching translation performance in multilingual models M2M-100 and NLLB-200 across 13 languages. Key findings show that code-switched inputs generally outperform monolingual translations for csw→En, with larger gains for low-resource languages, but translation into non-English target languages shows significant performance drops due to the off-target problem and hallucination behaviors.

## Method Summary
The study creates synthetic code-switched text by detecting English intonation unit boundaries using PSST, extracting word alignments between English and non-English CoVoST 2 texts using awesome-align, and replacing non-consecutive English IUs with aligned non-English tokens while preserving original order. The dataset is evaluated using M2M-100 418M and NLLB-200 600M models with spBLEU, chrF++, COMET, and copy/replacement rate metrics, comparing csw→En, csw→X, En→X, and X→En settings against monolingual baselines.

## Key Results
- Code-switched inputs outperform monolingual translations for csw→En, especially for low-resource languages
- NLLB-200 shows better performance than M2M-100 on low-resource languages in csw→En translation
- Significant off-target problem in csw→X settings with up to one-third of English tokens remaining untranslated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing intonation units (IUs) rather than words produces more natural code-switching patterns
- Mechanism: IUs reflect prosodic boundaries where code-switching naturally occurs more frequently than within units, following the Intonation Unit Boundary Constraint
- Core assumption: Prosodic segmentation accurately identifies natural code-switching boundaries in speech
- Evidence anchors:
  - [abstract] "code-switching is more common across intonation units than within as a result of looser syntactic relationships"
  - [section 2.1] "We use the PSST model... to detect intonation unit (IU) boundaries for English utterances"
  - [corpus] Weak - no direct comparison of IU vs word-based switching performance

### Mechanism 2
- Claim: Synthetic code-switched data improves translation performance by exposing models to mixed-language patterns
- Mechanism: Models learn to handle language alternation patterns during training, improving generalization to real code-switched input
- Core assumption: Exposure to synthetic code-switching during training translates to better performance on real code-switching
- Evidence anchors:
  - [abstract] "the inclusion of code-switching units results in higher translation performance than monolingual settings"
  - [section 4.2] "Inclusion of code-switched units results in better translation than monolingual settings"
  - [corpus] Moderate - shows performance gains but doesn't prove this is the mechanism

### Mechanism 3
- Claim: Matrix Language Frame (MLF) model with English as matrix language creates more predictable code-switching patterns
- Mechanism: Keeping English as matrix language and embedding non-English segments follows linguistic theory of code-switching structure
- Core assumption: MLF model accurately represents how code-switching occurs in multilingual communication
- Evidence anchors:
  - [section 2.2] "We pick the number of intonation units to replace, r, from 1 to number of English intonation units - 1"
  - [abstract] "we synthesize data by following the Matrix Language Frame Model"
  - [corpus] Weak - no validation that MLF-based approach produces more effective synthetic data

## Foundational Learning

- Concept: Intonation units and prosodic boundaries
  - Why needed here: The entire dataset creation depends on accurately detecting where code-switching should occur
  - Quick check question: What linguistic feature distinguishes intonation units from arbitrary word groupings in speech?

- Concept: Code-switching theories (Matrix Language Frame, Intonation Unit Boundary Constraint)
  - Why needed here: The synthetic data generation follows specific linguistic theories about how code-switching works
  - Quick check question: How does the Matrix Language Frame model differ from simple word replacement approaches?

- Concept: Multilingual translation model architectures (M2M-100, NLLB-200)
  - Why needed here: Understanding model capabilities and limitations is crucial for interpreting translation performance results
  - Quick check question: What architectural features enable these models to handle multiple language pairs?

## Architecture Onboarding

- Component map: CoVoST 2 → PSST (IU detection) → awesome-align (word alignment) → IU replacement → synthetic dataset → M2M-100/NLLB-200 → evaluation

- Critical path: IU detection → word alignment → code-switched text generation → model training/evaluation → performance analysis

- Design tradeoffs:
  - IU vs word-based replacement: IUs capture natural boundaries but may produce less frequent switching
  - English as matrix language: follows MLF but may not reflect all code-switching patterns
  - Synthetic vs real data: synthetic data is scalable but may not capture all real-world variation

- Failure signatures:
  - High copy rates but low translation quality: suggests off-target problem
  - Low replacement rates in csw→X: indicates models struggling to translate non-English tokens
  - Hallucinations in output: model generating content not present in source

- First 3 experiments:
  1. Compare IU-based vs word-based synthetic data generation on a subset of languages
  2. Test translation performance with varying code-switching levels (different numbers of IU replacements)
  3. Analyze correlation between copy rates and translation quality to validate off-target hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does NLLB-200 show better performance on low-resource languages in csw→En translation compared to M2M-100, despite both models being designed for multilingual translation?
- Basis in paper: [explicit] The paper states "NLLB-200 shows better translations" for low-resource languages like Welsh, Mongolian, and Tamil in csw→En translation, while M2M-100 "struggles with translation across all three languages"
- Why unresolved: The paper does not provide a detailed analysis of the architectural or training differences between M2M-100 and NLLB-200 that would explain this performance gap on low-resource languages in code-switching scenarios.
- What evidence would resolve it: A comparative analysis of the model architectures, training data composition, and fine-tuning procedures for both models, specifically focusing on their handling of low-resource languages in code-switching contexts.

### Open Question 2
- Question: What specific linguistic or prosodic features of intonation units make them more suitable than word-level units for code-switching data generation?
- Basis in paper: [explicit] The paper states that "code-switching is more common across intonation units than within as a result of looser syntactic relationships and that intonation units should therefore serve as new replacement units instead of words"
- Why unresolved: While the paper asserts that intonation units are better for code-switching, it does not provide empirical evidence or linguistic analysis to support why this is the case compared to word-level switching.
- What evidence would resolve it: A detailed linguistic analysis comparing the syntactic and semantic coherence of code-switched utterances generated at the intonation unit level versus word level, including human evaluations of naturalness and fluency.

### Open Question 3
- Question: How does the off-target problem manifest differently in code-switching translation compared to monolingual translation, and what specific factors in code-switched input exacerbate this issue?
- Basis in paper: [explicit] The paper identifies the off-target problem in code-switching settings, noting that "models ignore the specified target language and instead copy the code-switched input as the translation output"
- Why unresolved: The paper observes the off-target problem in code-switching but does not investigate the underlying mechanisms or compare it systematically to monolingual settings to identify unique contributing factors.
- What evidence would resolve it: A controlled experiment comparing the frequency and severity of the off-target problem in code-switching versus monolingual translation, with analysis of input features (e.g., proportion of code-switched content, language distribution) that correlate with off-target behavior.

### Open Question 4
- Question: What causes the hallucination phenomenon in code-switching translation, and why do models generate non-existent words or repeated characters specifically in this context?
- Basis in paper: [explicit] The paper provides examples of hallucinations in code-switching translation, such as "Whey and crempagai" being generated when they don't exist in the source sentences
- Why unresolved: While the paper identifies hallucinations as a problem in code-switching translation, it does not investigate the root causes or why this phenomenon is particularly prevalent in code-switched input.
- What evidence would resolve it: An analysis of the model's attention patterns and internal representations during code-switching translation to identify triggers for hallucination, combined with experiments varying the amount and type of code-switching in the input to determine contributing factors.

## Limitations
- Synthetic data may not fully capture natural code-switching patterns due to dependency on word alignment quality and prosodic segmentation accuracy
- Limited evaluation scope with less detailed analysis for non-English target languages (csw→X)
- Assumes English consistently serves as matrix language across all language pairs, which may not reflect actual code-switching patterns

## Confidence
**High Confidence**:
- Code-switched inputs generally outperform monolingual translations for csw→En
- Low-resource languages show larger performance gains from code-switching
- Significant off-target problem exists in non-English translation directions

**Medium Confidence**:
- IU-based replacement produces more natural code-switching patterns than word-based approaches
- MLF model with English as matrix language creates effective synthetic data
- Synthetic data exposure improves model handling of code-switching

**Low Confidence**:
- Specific performance improvements can be generalized to real code-switched data
- The observed patterns will hold for other language pairs beyond the 13 studied
- Hallucination rates can be effectively reduced through current approaches

## Next Checks
1. Validation of IU-based vs Word-based Synthetic Data: Conduct a controlled experiment comparing translation performance using synthetic data generated through IU-based replacement versus simple word-based replacement for a subset of languages.

2. Real vs Synthetic Data Comparison: Test the same translation models on a small set of real code-switched data (if available) alongside the synthetic data to quantify the gap between synthetic and natural code-switching patterns.

3. Matrix Language Frame Model Validation: For each target language pair, analyze the actual code-switching patterns in available bilingual corpora to verify whether English consistently serves as the matrix language.