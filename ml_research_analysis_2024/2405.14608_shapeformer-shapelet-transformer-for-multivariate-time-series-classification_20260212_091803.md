---
ver: rpa2
title: 'ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification'
arxiv_id: '2405.14608'
source_url: https://arxiv.org/abs/2405.14608
tags:
- time
- series
- shapelets
- shapelet
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ShapeFormer, a transformer-based model for
  multivariate time series classification that combines class-specific and generic
  feature extraction. The key innovation is the integration of shapelet discovery
  with transformer encoders: ShapeFormer first extracts discriminative subsequences
  (shapelets) per class using an Offline Shapelet Discovery method, then uses a Shapelet
  Filter to learn difference features between these shapelets and input time series.'
---

# ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification

## Quick Facts
- **arXiv ID:** 2405.14608
- **Source URL:** https://arxiv.org/abs/2405.14608
- **Reference count:** 40
- **Primary result:** ShapeFormer achieves highest accuracy ranking on 30 UEA MTSC datasets with statistically significant improvement over 12 baselines (p â‰¤ 0.05)

## Executive Summary
ShapeFormer introduces a novel transformer-based architecture for multivariate time series classification that uniquely combines class-specific and generic feature extraction. The key innovation is integrating shapelet discovery with transformer encoders: class-specific shapelets are first extracted per class, then difference features are learned between these shapelets and input time series. A generic transformer module with convolution filters captures additional cross-class patterns. Experiments demonstrate ShapeFormer achieves state-of-the-art performance, particularly excelling on imbalanced datasets where traditional methods struggle.

## Method Summary
ShapeFormer consists of two parallel transformer modules: a class-specific module that extracts discriminative shapelet-based features and a generic module that captures overall temporal and variable correlation patterns. The class-specific module uses Offline Shapelet Discovery to extract per-class shapelets, then learns difference features between these shapelets and their best-fit subsequences in input data. The generic module applies Conv1D filters to extract temporal and variable correlation features, which are processed by a transformer encoder. Both modules' outputs are concatenated and fed to a classification head. The model is trained end-to-end using RAdam optimizer with cross-entropy loss.

## Key Results
- Achieves highest accuracy ranking across 30 UEA MTSC datasets compared to 12 state-of-the-art methods
- Statistically significant improvement (p-value â‰¤ 0.05) over baseline methods
- Particularly excels on imbalanced datasets where generic features alone struggle
- Demonstrates effectiveness across diverse domains including HAR, ECG, EEG, and audio classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The class-specific transformer module captures discriminative features that distinguish classes even when overall patterns are similar.
- **Mechanism:** ShapeFormer extracts shapelets per class using Offline Shapelet Discovery, then learns difference features between each shapelet and its best-fit subsequence in the input time series. The Shapelet Filter treats shapelets as learnable parameters, optimizing them during training to maximize class distinction.
- **Core assumption:** The distance between a shapelet and its best-fit subsequence is significantly smaller for time series of the same class than for time series of other classes.
- **Evidence anchors:**
  - [abstract] "We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others."
  - [section] "Similar to the distance, our difference feature also highlights the substantial distinctions among classes."

### Mechanism 2
- **Claim:** The generic transformer module captures overall patterns that distinguish among all classes.
- **Mechanism:** Conv1D filters extract temporal and variable correlation features from the input time series, which are then processed by a transformer encoder to learn dependencies between these features.
- **Core assumption:** The combination of temporal and variable correlation features contains information to distinguish among all classes.
- **Evidence anchors:**
  - [abstract] "In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes."
  - [section] "The first block is designed to capture the temporal patterns in time series by using the Conv1D filter âˆˆ R1Ã—ð‘‘ð‘. On the other hand, the second block uses the Conv1D filter âˆˆ Rð‘‰ Ã—1 to capture the correlation between variables in time series."

### Mechanism 3
- **Claim:** The combination of class-specific and generic features improves classification performance, especially on imbalanced datasets.
- **Mechanism:** The class-specific transformer focuses on distinguishing features within each class, while the generic transformer captures overall patterns. The concatenation of their outputs provides a comprehensive representation for classification.
- **Core assumption:** Class-specific features are more effective for distinguishing minority classes, while generic features are better for majority classes.
- **Evidence anchors:**
  - [abstract] "The combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance."
  - [section] "This dual capability contributes to an enhancement in the overall performance of classification tasks."

## Foundational Learning

- **Concept:** Time series classification
  - **Why needed:** Understanding the problem domain is essential for grasping the significance of ShapeFormer's approach.
  - **Quick check:** What is the difference between univariate and multivariate time series classification?

- **Concept:** Shapelets
  - **Why needed:** Shapelets are the core component of ShapeFormer's class-specific module, so understanding their properties and discovery methods is crucial.
  - **Quick check:** How do shapelets differ from other time series features like motifs or subsequences?

- **Concept:** Transformers
  - **Why needed:** Transformers are the backbone of both modules in ShapeFormer, so understanding their architecture and attention mechanism is necessary.
  - **Quick check:** What is the role of the multi-head attention mechanism in transformers?

## Architecture Onboarding

- **Component map:** Input -> Shapelet Discovery -> Shapelet Filter -> Class-specific Transformer -> Generic Transformer -> Classification Head
- **Critical path:** Shapelet discovery â†’ Shapelet filter â†’ Class-specific transformer â†’ Concatenation â†’ Classification
- **Design tradeoffs:** ShapeFormer trades computational complexity (shapelet discovery and best-fit subsequence finding) for improved classification performance, particularly on imbalanced datasets.
- **Failure signatures:** Poor performance on datasets without clear shapelet patterns, suboptimal classification accuracy due to incorrect position embedding or attention mechanism implementation, computational bottlenecks on large-scale datasets.
- **First experiments:** 1) Implement Offline Shapelet Discovery and evaluate shapelet quality, 2) Test Shapelet Filter with synthetic data to verify difference feature computation, 3) Train generic transformer module alone to establish baseline performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the content, potential areas for further research include:

1. How does the performance of ShapeFormer scale with increasing number of variables and time series length in multivariate time series datasets?
2. Can the shapelet discovery method be adapted to work effectively in online or streaming settings where data arrives continuously?
3. How does the choice of shapelet initialization affect the final classification performance of ShapeFormer?

## Limitations

- Dependence on shapelet quality from the Offline Shapelet Discovery method, which may not generalize well to all dataset types
- Computational complexity of finding best-fit subsequences for all shapelets during inference could be significant for large datasets
- Performance on datasets without clear shapelet patterns remains uncertain

## Confidence

- **Claim:** Class-specific features improve imbalanced dataset performance - **High**
- **Claim:** Combining class-specific and generic features enhances overall performance - **Medium**
- **Claim:** ShapeFormer scales effectively to very large-scale datasets - **Low**

## Next Checks

1. Evaluate ShapeFormer's performance on datasets without clear shapelet patterns to assess robustness.
2. Measure computational complexity and inference time on larger datasets to determine scalability limits.
3. Conduct ablation studies to determine the optimal balance between class-specific and generic feature extraction for different dataset characteristics.