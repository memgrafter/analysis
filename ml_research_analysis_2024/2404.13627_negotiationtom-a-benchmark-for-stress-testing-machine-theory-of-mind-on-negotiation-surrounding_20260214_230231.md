---
ver: rpa2
title: 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation
  Surrounding'
arxiv_id: '2404.13627'
source_url: https://arxiv.org/abs/2404.13627
tags:
- agent
- firewood
- water
- preference
- food
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NegotiationToM, a benchmark designed to stress-test
  machine Theory of Mind (ToM) in real-world negotiation scenarios. Unlike existing
  ToM benchmarks that rely on synthetic or game-based data, NegotiationToM is built
  on natural human-human negotiation dialogues, covering multi-dimensional mental
  states including desires, beliefs, and intentions.
---

# NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding

## Quick Facts
- arXiv ID: 2404.13627
- Source URL: https://arxiv.org/abs/2404.13627
- Reference count: 40
- LLMs consistently underperform humans on mental state tracking in real-world negotiation scenarios

## Executive Summary
This paper introduces NegotiationToM, a benchmark designed to evaluate machine Theory of Mind (ToM) capabilities in real-world negotiation contexts. Built on natural human-human negotiation dialogues from the CaSiNo dataset, the benchmark assesses LLMs' ability to track multi-dimensional mental states including desires, beliefs, and intentions using the Belief-Desire-Intention (BDI) framework. Experiments show that state-of-the-art LLMs, including GPT-4 and Claude variants, significantly underperform humans on this benchmark, even with chain-of-thought prompting, revealing fundamental limitations in their ability to maintain coherent mental state tracking throughout complex, multi-turn negotiations.

## Method Summary
NegotiationToM is constructed by annotating the CaSiNo dataset with mental state labels across desire, belief, and intention dimensions for each utterance in multi-turn negotiation dialogues. The benchmark employs zero-shot, chain-of-thought, and few-shot prompting strategies to evaluate LLMs' mental state tracking capabilities. Performance is measured using Exact Match percentages for desire and belief classification, micro/macro F1 scores for multi-label intention classification, an "All" score requiring correct answers across all three mental states, and a consistency score measuring correct tracking across full dialogue sequences.

## Key Results
- All tested LLMs (GPT-4, ChatGPT, Claude-v1.3/v2.1, Llama-2 Chat) underperform humans on NegotiationToM
- Chain-of-thought prompting does not significantly improve LLM performance on mental state tracking
- Multi-label intention classification proves particularly challenging, with models often including incorrect intentions or both options in "either/or" scenarios
- Consistency scores show large performance gaps between machines and humans, indicating LLMs struggle to maintain mental state coherence throughout conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NegotiationToM reveals LLMs' theory of mind limitations because it uses real-world negotiation dialogues with complex mental state dependencies.
- Mechanism: By grounding mental state questions in natural multi-turn dialogues, the benchmark forces models to track evolving desires, beliefs, and intentions across context shifts.
- Core assumption: Mental states in negotiation can be incrementally inferred from conversational cues and previous history.
- Evidence anchors:
  - [abstract]: "NegotiationToM is built on natural human-human negotiation dialogues, covering multi-dimensional mental states including desires, beliefs, and intentions."
  - [section 3.3]: "Understanding the Theory of Mind (ToM) in a negotiation dialogue is challenging due to its intricate linguistic features and complex reasoning attributes."
- Break condition: If the mental state labels are ambiguous or inconsistently annotated, the benchmark may not reliably measure LLM performance.

### Mechanism 2
- Claim: LLMs underperform on NegotiationToM because they struggle with multi-label intention classification in utterance-level contexts.
- Mechanism: Each utterance can encode multiple intentions (e.g., build rapport + discover preference), requiring models to parse and disambiguate overlapping social signals.
- Core assumption: Intention labels are independent enough that models can jointly predict them without collapsing to a single dominant class.
- Evidence anchors:
  - [section 5.6]: "all models without and with CoT tend to select more intention choices, resulting in a high error rate in the 'including incorrect intentions' and 'did both' error types."
  - [section 3.3]: "Intention is a mental state formed through rational planning toward a goal based on the desires and beliefs of the agent."
- Break condition: If multi-label classification is not properly supported by the underlying LLM architecture, performance will degrade regardless of prompt engineering.

### Mechanism 3
- Claim: The "All" and conversation consistency metrics expose LLMs' inability to maintain coherent mental state tracking throughout entire dialogues.
- Mechanism: These metrics require models to answer all mental state questions correctly for the same dialogue segment and across the full conversation, respectively.
- Core assumption: Mental states evolve predictably within a negotiation, and tracking them should be feasible if models truly understand the context.
- Evidence anchors:
  - [section 5.3]: "There is a huge performance gap between machines and humans in this consistency metric, demonstrating LLMs still lack of ability to track the mental state change during the conversation."
  - [section 5.4]: "The combined format performs better than other formats. It may result from the combined format imposing the constraint for LLMs to avoid answering some unreasonable and implausible response."
- Break condition: If models fail to encode conversation history or have limited context windows, consistency scores will be low even with correct local reasoning.

## Foundational Learning

- Concept: Belief-Desire-Intention (BDI) agent modeling framework
  - Why needed here: Provides the theoretical grounding for structuring mental state questions in NegotiationToM.
  - Quick check question: What are the three core components of the BDI model and how do they relate to negotiation strategies?

- Concept: Multi-turn dialogue understanding
  - Why needed here: NegotiationToM depends on models maintaining coherence across alternating speaker turns.
  - Quick check question: How does an LLM's attention mechanism need to adapt when tracking evolving beliefs in a multi-party negotiation?

- Concept: Multi-label classification
  - Why needed here: Intention annotations allow multiple simultaneous strategies per utterance.
  - Quick check question: What architectural changes are required to enable a model to predict more than one intent per input?

## Architecture Onboarding

- Component map: CaSiNo dialogues -> Annotation pipeline -> Prompt generator -> Model executor -> Evaluator -> Analyzer
- Critical path: Data → Prompt → Model → Evaluation → Analysis
- Design tradeoffs:
  - Choice of prompt format (ranking vs. individual vs. combined) impacts precision vs. flexibility
  - CoT prompts increase reasoning depth but may introduce extra tokens that exceed context limits
  - Few-shot exemplars can improve performance but risk overfitting to small example sets
- Failure signatures:
  - Low consistency scores indicate loss of mental state coherence over time
  - High "irrelevant response" rates suggest poor prompt comprehension
  - Random guessing on "Not Given" labels indicates failure to recognize insufficient information
- First 3 experiments:
  1. Run all models with baseline zero-shot prompts; record Exact Match for desire and belief
  2. Repeat with CoT prompting; compare consistency improvements
  3. Compare individual vs. combined vs. ranking question formats for GPT-4 only; isolate effect of question structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would NegotiationToM performance change if evaluated on active rather than passive benchmarks?
- Basis in paper: Explicit - The paper identifies as a key limitation that NegotiationToM is a passive benchmark that lacks active interaction between agents
- Why unresolved: The paper acknowledges this limitation but does not implement or test an active benchmark version, leaving open how performance would differ
- What evidence would resolve it: Implementation and testing of an active benchmark where LLMs must interact with other agents using BDI modeling to generate negotiation strategies

### Open Question 2
- Question: Does the format of question presentation (individual vs combined vs ranking) fundamentally affect LLMs' ability to track mental states, or is it merely a prompting optimization?
- Basis in paper: Explicit - The paper shows significant performance differences across question formats but doesn't determine whether this reflects fundamental reasoning capabilities or just prompting effects
- Why unresolved: While performance differences are observed, the paper doesn't distinguish between format as a superficial presentation issue versus a fundamental reasoning capability indicator
- What evidence would resolve it: Systematic testing of whether performance differences persist when LLMs are trained or fine-tuned on different formats versus just tested with different prompts

### Open Question 3
- Question: What specific aspects of mental state tracking are most challenging for LLMs - the multi-dimensional nature, the dynamic updating, or the consistency requirements?
- Basis in paper: Inferred - The paper shows large performance gaps but doesn't decompose which aspects (desire, belief, intention; static vs dynamic; consistency) contribute most to the difficulty
- Why unresolved: The paper reports overall performance but doesn't analyze which specific aspects of mental state tracking drive the performance gap with humans
- What evidence would resolve it: Ablation studies isolating different aspects of mental state tracking (e.g., testing on static beliefs only, or single-round negotiations) to identify the primary bottlenecks

## Limitations

- The benchmark relies on human annotations for mental states, which may introduce inconsistency or bias in the ground truth labels
- Performance gaps between humans and LLMs might partly reflect humans' ability to exploit implicit conversational cues not explicitly tested by the benchmark
- The paper doesn't explore whether architectural modifications (beyond prompting) could improve LLM performance on multi-label intention classification

## Confidence

- **High Confidence**: The experimental methodology is sound, with clear metrics (Exact Match, F1, All, Consistency) and controlled comparisons across prompt formats. The finding that all tested LLMs underperform humans on this benchmark is robust.
- **Medium Confidence**: The interpretation that LLMs "lack the ability to track mental state change" is reasonable but could also reflect limitations in prompt engineering or the need for architectural adaptations rather than fundamental ToM reasoning deficits.
- **Medium Confidence**: The claim that multi-label intention classification is particularly challenging for LLMs is supported by error analysis but needs further validation with models specifically designed for multi-label tasks.

## Next Checks

1. Conduct inter-annotator agreement analysis on a subset of the NegotiationToM dataset to quantify annotation reliability and determine if performance gaps are driven by label ambiguity rather than LLM limitations.
2. Test specialized multi-label classification models (e.g., BERT-based architectures with sigmoid activation) on the intention classification task to isolate whether standard LLMs' poor performance stems from architectural constraints rather than ToM reasoning deficits.
3. Design a controlled experiment where models receive explicit mental state annotations as additional context during inference to determine if performance improvements are possible through better context utilization rather than reasoning improvements.