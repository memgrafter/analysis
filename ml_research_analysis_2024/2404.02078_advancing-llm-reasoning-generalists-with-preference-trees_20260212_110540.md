---
ver: rpa2
title: Advancing LLM Reasoning Generalists with Preference Trees
arxiv_id: '2404.02078'
source_url: https://arxiv.org/abs/2404.02078
tags:
- reasoning
- interact
- ultra
- preprint
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EURUS, a suite of large language models optimized
  for reasoning across mathematics, coding, and logical reasoning. The authors develop
  a new high-quality alignment dataset called ULTRA INTERACT, which contains 86K instructions
  and 220K action pairs structured as preference trees.
---

# Advancing LLM Reasoning Generalists with Preference Trees

## Quick Facts
- arXiv ID: 2404.02078
- Source URL: https://arxiv.org/abs/2404.02078
- Authors: Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun
- Reference count: 21
- Primary result: EURUS models achieve state-of-the-art results among open-source models on reasoning benchmarks

## Executive Summary
This paper introduces EURUS, a suite of large language models optimized for reasoning across mathematics, coding, and logical reasoning. The authors develop ULTRA INTERACT, a high-quality alignment dataset containing 86K instructions and 220K action pairs structured as preference trees. These trees capture reasoning chains with diverse planning strategies, multi-turn interaction trajectories, and pairwise data for preference learning. EURUS models, fine-tuned from Mistral-7B and CodeLlama-70B, achieve state-of-the-art performance among open-source models on reasoning benchmarks, with EURUS-70B outperforming GPT-3.5 Turbo in reasoning tasks.

## Method Summary
EURUS models are developed through a multi-stage training process. First, models are fine-tuned on ULTRA INTERACT using supervised learning, combining reasoning-focused data with general instruction-following datasets. The ULTRA INTERACT dataset contains 86K instructions and 220K action pairs structured as preference trees, capturing multi-turn reasoning trajectories. Next, preference learning is applied using algorithms like KTO and NCA, with a novel reward modeling objective that includes an LDR term to improve absolute reward signals. Finally, reward models are trained using the LDR + BT objective and evaluated on benchmarks like RewardBench and AutoJ.

## Key Results
- EURUS-70B outperforms GPT-3.5 Turbo on reasoning benchmarks
- Achieves 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA
- State-of-the-art performance among open-source models on reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-structured preference data improves multi-turn reasoning by enabling fine-grained pairwise comparisons at each interaction step.
- Mechanism: ULTRA INTERACT constructs preference trees where each node represents an action in a reasoning trajectory, allowing the model to learn from feedback at every turn.
- Core assumption: Reasoning quality correlates with the trajectory of actions, not just the final answer.
- Evidence anchors: [abstract] and [section 2.3] describe the construction of preference trees with paired correct/incorrect actions at each turn.
- Break condition: If environment feedback is noisy or critique models are unreliable, learning from intermediate steps could reinforce errors.

### Mechanism 2
- Claim: Direct preference optimization with absolute reward modeling is more effective for reasoning than relative ranking objectives alone.
- Mechanism: The novel LDR term in the reward modeling objective explicitly increases rewards for chosen actions and decreases rewards for rejected actions.
- Core assumption: In reasoning tasks, absolute reward magnitudes are more informative than relative rankings because correct answers are rare and well-defined.
- Evidence anchors: [abstract], [section 6.1], and [section 2.3] support the development and benefits of the LDR objective.
- Break condition: If preference annotations contain subjective elements, absolute reward optimization could overfit to noisy labels.

### Mechanism 3
- Claim: Mixing ULTRA INTERACT with general instruction-following data yields better all-around performance than training on either alone.
- Mechanism: Supervised fine-tuning uses both reasoning-focused ULTRA INTERACT data and general instruction datasets to balance reasoning capability with instruction-following ability.
- Core assumption: Reasoning tasks require specialized training signals, but models also need general instruction-following competence for practical utility.
- Evidence anchors: [section 3] and [section 6.2] demonstrate the benefits of mixed training data.
- Break condition: If general instruction datasets contain conflicting patterns with reasoning tasks, mixed training could create confusion.

## Foundational Learning

- Concept: Preference learning and direct preference optimization (DPO)
  - Why needed here: EURUS uses KTO, NCA, and a novel reward modeling objective that builds on preference learning foundations
  - Quick check question: What is the key difference between DPO and the Bradley-Terry objective used in traditional preference learning?

- Concept: Supervised fine-tuning (SFT) with mixed datasets
  - Why needed here: EURUS models are first SFT'd on ULTRA INTERACT and general instruction data before preference learning
  - Quick check question: Why might training only on ULTRA INTERACT hurt instruction-following performance?

- Concept: Reward modeling and correlation with human judgment
  - Why needed here: EURUS-RM-7B is evaluated on RewardBench, AutoJ, and MT-Bench for its ability to align with human preferences
  - Quick check question: How does the novel LDR term in the reward objective differ from traditional reward modeling approaches?

## Architecture Onboarding

- Component map:
  Base models (Mistral-7B, CodeLlama-70B) -> SFT on ULTRA INTERACT + general data -> Preference learning (KTO/NCA) -> Reward modeling (LDR + BT) -> Evaluation

- Critical path: SFT → Preference Learning → Reward Modeling → Evaluation
  - Each stage depends on the previous one being successfully completed

- Design tradeoffs:
  - Using GPT-4 for critique vs. weaker models: Higher quality feedback but increased cost
  - Tree depth limit (5 turns): Balances trajectory richness with computational feasibility
  - Mixing reasoning and general data: Improves practical utility but may dilute reasoning specialization

- Failure signatures:
  - Training collapse (rewards → -∞): Indicates preference learning algorithm mismatch with reasoning task structure
  - Poor generalization to out-of-distribution tasks: Suggests overfitting to ULTRA INTERACT patterns
  - Reward model misalignment: Indicates the LDR term or training data mixture needs adjustment

- First 3 experiments:
  1. Train EURUS-7B-SFT on ULTRA INTERACT only vs. mixed with general data, compare instruction-following performance
  2. Compare KTO, NCA, and DPO on EURUS-SFT, analyze reward trajectories to understand failure modes
  3. Train reward model with and without LDR term, evaluate correlation with human judgment on AutoJ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EURUS models scale with increasing model size beyond 70B parameters?
- Basis in paper: [inferred] The paper shows EURUS-70B outperforming GPT-3.5 Turbo and compares it to various smaller open-source models, suggesting a potential performance gap at larger scales.
- Why unresolved: The paper only evaluates EURUS-7B and EURUS-70B, leaving the performance of larger variants unexplored.
- What evidence would resolve it: Training and evaluating EURUS models with parameter sizes exceeding 70B on the same benchmarks used in the paper.

### Open Question 2
- Question: What is the long-term effectiveness of the reward modeling objective (LDR) in improving reasoning capabilities across diverse tasks?
- Basis in paper: [explicit] The paper introduces LDR to augment the Bradley-Terry objective and claims it improves reasoning performance, but long-term effectiveness is not assessed.
- Why unresolved: The paper only evaluates LDR's immediate impact on reward modeling benchmarks, not its sustained effectiveness.
- What evidence would resolve it: Long-term studies comparing models trained with and without LDR on evolving reasoning tasks over extended periods.

### Open Question 3
- Question: How does the performance of EURUS models compare to proprietary models like GPT-4 on reasoning tasks beyond the ones tested?
- Basis in paper: [explicit] The paper compares EURUS-70B to GPT-3.5 Turbo and mentions GPT-4's performance on some benchmarks but does not extensively compare to GPT-4 on all reasoning tasks.
- Why unresolved: The comparison is limited to specific benchmarks, and broader task coverage is needed.
- What evidence would resolve it: Comprehensive benchmarking of EURUS models against GPT-4 on a wide range of reasoning tasks, including those not covered in the paper.

## Limitations
- The preference tree construction relies on GPT-4 for critique, raising questions about reproducibility and true open-source nature
- Ablation studies comparing different preference learning algorithms focus on algorithmic stability rather than comprehensive performance comparisons
- Limited direct comparisons with proprietary models on reasoning-specific benchmarks

## Confidence
- High: Claims about ULTRA INTERACT's scale and basic performance improvements of EURUS models over open-source baselines
- Medium: The assertion that tree-structured preference data specifically improves multi-turn reasoning
- Medium: The novel LDR objective's superiority for reasoning tasks
- Low: The generalizability of these findings to reasoning tasks beyond those specifically tested

## Next Checks
1. **Isolate the Tree Structure Effect**: Conduct an ablation study comparing ULTRA INTERACT with a flattened version containing the same data but without tree structure, to empirically validate whether preference tree construction provides specific benefits for multi-turn reasoning.

2. **Cross-Domain Generalization**: Evaluate EURUS models on reasoning tasks from domains not represented in ULTRA INTERACT (e.g., commonsense reasoning, temporal reasoning) to test whether improvements generalize beyond the training distribution.

3. **Cost-Benefit Analysis of GPT-4 Critique**: Reconstruct ULTRA INTERACT using a weaker critique model and measure the impact on both model performance and the fidelity of preference trees, to understand the sensitivity of the approach to critique quality.