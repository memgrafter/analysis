---
ver: rpa2
title: 'Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language
  Model Pre-training'
arxiv_id: '2405.03133'
source_url: https://arxiv.org/abs/2405.03133
tags:
- routing
- training
- experts
- expert
- segment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Lory, the first approach to scale fully
  differentiable mixture-of-experts (MoE) architectures to autoregressive language
  model pre-training. Lory addresses the challenge of optimizing non-differentiable
  routing objectives in MoE models by introducing two key techniques: (1) a causal
  segment routing strategy that merges experts at the segment level to preserve autoregressive
  properties while maintaining computational efficiency, and (2) a similarity-based
  data batching method that encourages expert specialization by grouping semantically
  similar documents.'
---

# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training

## Quick Facts
- arXiv ID: 2405.03133
- Source URL: https://arxiv.org/abs/2405.03133
- Authors: Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis
- Reference count: 40
- Primary result: First approach to scale fully differentiable MoE to autoregressive pre-training, achieving +13.9% perplexity improvement over dense models

## Executive Summary
This paper introduces Lory, the first approach to scale fully differentiable mixture-of-experts (MoE) architectures to autoregressive language model pre-training. Lory addresses the challenge of optimizing non-differentiable routing objectives in MoE models by introducing two key techniques: (1) a causal segment routing strategy that merges experts at the segment level to preserve autoregressive properties while maintaining computational efficiency, and (2) a similarity-based data batching method that encourages expert specialization by grouping semantically similar documents. The authors pre-train Lory models with up to 32 experts and 30B parameters on 150B tokens, demonstrating significant performance gains over parameter-matched dense models in both perplexity (+13.9%) and downstream tasks (+1.5%-11.1%). Despite using segment-level routing, Lory achieves competitive performance compared to state-of-the-art MoE models with token-level routing.

## Method Summary
Lory implements causal segment routing where sequences are split into T=256 token segments, with routing decisions based on the previous segment's average hidden representation. The model uses fully differentiable soft routing weights across all E experts, merged in parameter space for efficient computation. Similarity-based data batching groups semantically similar documents using Contriever embeddings to create coherent training segments. Pre-training uses AdamW optimizer with learning rate 2e-4 and cosine scheduler, including a 5% warmup phase before switching to MoE layers. The approach scales to models with up to 32 experts and 30B parameters while maintaining autoregressive properties.

## Key Results
- Lory achieves +13.9% perplexity improvement over parameter-matched dense models on held-out evaluation sets
- Models with similarity-based batching show larger performance gains (+1.5%-11.1%) compared to random batching
- Expert analysis reveals domain-level specialization (e.g., math, code, literature) without supervision
- Performance competitive with state-of-the-art MoE models using token-level routing despite segment-level efficiency

## Why This Works (Mechanism)

### Mechanism 1
Segment-level routing preserves autoregressive property while reducing computational overhead. By routing each segment based on the previous segment's average hidden representation, the model avoids token-level computation scaling with expert count while maintaining causal dependencies. Core assumption: The average representation of a segment adequately captures the information needed for routing subsequent segments.

### Mechanism 2
Similarity-based data batching improves expert specialization by ensuring coherent segments. Grouping semantically similar documents creates training segments where all tokens share related contexts, enabling experts to develop domain-specific capabilities. Core assumption: Document similarity correlates with the type of expertise needed to process tokens within that document.

### Mechanism 3
Fully differentiable routing enables end-to-end gradient flow without load balancing losses. Soft routing weights allow all experts to receive gradient updates proportional to their contribution, eliminating the need for auxiliary balancing objectives. Core assumption: Soft routing gradients provide sufficient signal for stable training without explicit load balancing.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture**: Lory builds on MoE foundations but adapts them for autoregressive pre-training with differentiable routing. Quick check: How does soft routing differ from top-k routing in terms of gradient flow?

- **Autoregressive language modeling**: The model must preserve the causal nature of prediction where each token depends only on previous tokens. Quick check: What would happen if routing information leaked from future segments?

- **Expert specialization in neural networks**: Understanding how experts learn distinct capabilities is key to interpreting Lory's performance gains. Quick check: How can you verify that experts are actually learning different types of knowledge rather than memorizing?

## Architecture Onboarding

- **Component map**: Input sequence -> Attention layers -> Sequence split into segments -> Router network -> Soft routing weights -> Expert merging -> Merged expert processing -> Output

- **Critical path**: 1) Input sequence processed through attention layers, 2) Sequence split into T-token segments, 3) Average hidden representation from previous segment feeds router, 4) Router produces soft weights for all E experts, 5) Experts merged via weighted sum in parameter space, 6) Merged expert processes current segment, 7) Repeat for all segments

- **Design tradeoffs**: Segment size T: Larger T reduces routing frequency but may miss fine-grained patterns; smaller T increases computation but captures more local information. Number of experts E: More experts enable finer specialization but increase merging overhead and risk of underutilization. Similarity metric choice: Better metrics improve batching quality but add computational cost.

- **Failure signatures**: Poor perplexity improvement likely indicates routing isn't capturing useful patterns or experts aren't specializing. Low expert utilization suggests routing weights are collapsing or experts are redundant. Training instability may indicate learning rate issues or improper warmup strategy.

- **First 3 experiments**: 1) Compare segment routing vs token routing on a small dataset to verify computational benefits, 2) Test different segment sizes (T=128, 256, 512) to find optimal balance, 3) Compare similarity-based batching vs random batching to validate specialization benefits.

## Open Questions the Paper Calls Out

### Open Question 1
How does combining segment-level and token-level routing strategies affect model performance and specialization compared to using either approach alone? The authors note that segment-level routing captures global semantic features while token-level routing captures local features, suggesting their complementary nature. This remains unresolved as the paper identifies this as future work without conducting experiments.

### Open Question 2
What is the optimal segment size (T) for causal segment routing across different model scales and domains? The authors use T=256 throughout but don't explore sensitivity to this hyperparameter or analyze the trade-offs involved between performance and computational efficiency.

### Open Question 3
How can Lory models be efficiently scaled to trillion-parameter models while maintaining high expert utilization and specialization? The paper only explores data parallelism with ZeRO optimization and discusses theoretical scaling approaches without experimental validation.

### Open Question 4
What is the impact of similarity-based data batching on downstream reasoning tasks that require cross-document understanding? While similarity-based batching was originally proposed to improve cross-document reasoning, the paper focuses on expert specialization benefits rather than testing cross-document reasoning improvements.

## Limitations

- Computational overhead claims lack quantitative comparison between segment and token-level routing implementations
- Generalization beyond pre-training is untested, with no analysis of long-context generation or inference-time behaviors
- Similarity metric impact is not ablated, with no quantification of how similarity quality affects final performance

## Confidence

**High Confidence**: Claims about Lory being first to scale fully differentiable MoE to autoregressive pre-training are well-supported by literature review contrasting with SMEAR's small-scale fine-tuning only.

**Medium Confidence**: Performance improvements over dense models are supported by experiments, but individual contributions of segment routing vs similarity batching aren't isolated through proper ablation studies.

**Low Confidence**: Claims about domain-level expert specialization are based on qualitative analysis without systematic validation or quantification of practical impact.

## Next Checks

1. **Ablation Study**: Run experiments isolating effects of segment routing vs similarity-based batching by testing four conditions: (a) segment routing with random batching, (b) token routing with similarity batching, (c) segment routing with similarity batching (current), and (d) dense baseline.

2. **Inference Behavior Analysis**: Evaluate Lory's performance on long-context generation tasks where segment boundaries may not align with natural document boundaries. Measure how routing decisions and expert utilization change when processing sequences crossing original training segment boundaries.

3. **Expert Utilization Stability**: Track expert activation patterns throughout training to verify claimed domain-level specialization is stable and not an artifact of early training dynamics. Monitor whether same experts consistently specialize in same domains across different training runs and checkpoints.