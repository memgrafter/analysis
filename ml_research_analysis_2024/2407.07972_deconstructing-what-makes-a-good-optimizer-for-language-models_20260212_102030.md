---
ver: rpa2
title: Deconstructing What Makes a Good Optimizer for Language Models
arxiv_id: '2407.07972'
source_url: https://arxiv.org/abs/2407.07972
tags:
- learning
- adalayer
- adam
- performance
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares multiple optimizers (SGD, Adam, Adafactor, Lion,
  Signum) for autoregressive language modeling across various scales, architectures,
  and hyperparameters. The key finding is that, except for SGD, these optimizers exhibit
  comparable performance and hyperparameter stability.
---

# Deconstructing What Makes a Good Optimizer for Language Models

## Quick Facts
- arXiv ID: 2407.07972
- Source URL: https://arxiv.org/abs/2407.07972
- Reference count: 40
- Primary result: Except for SGD, optimizers exhibit comparable performance and stability; adaptive preconditioning in last layer and LayerNorm parameters is crucial for optimization in language models.

## Executive Summary
This work compares multiple optimizers (SGD, Adam, Adafactor, Lion, Signum) for autoregressive language modeling across various scales, architectures, and hyperparameters. The key finding is that, except for SGD, these optimizers exhibit comparable performance and hyperparameter stability. This challenges the common belief that Adam is uniquely superior. The study further investigates simplified variants of Adam, showing that signed momentum (Signum) recovers both performance and stability. Experiments with a layerwise Adam variant (Adalayer) reveal that adaptivity in the last layer and LayerNorm parameters is crucial for achieving comparable performance and stability. These results suggest practitioners can choose optimizers based on practical considerations like memory constraints, as no single algorithm emerged as a clear winner.

## Method Summary
The study compares five optimizers (AdamW, Adafactor with momentum, Lion, Signum, SGDW) across decoder-only language models of sizes 150m, 300m, 600m, and 1.2b parameters trained on C4 dataset. Training uses bfloat16 precision, batch size 256, sequence length 512, with approximately 20× parameter tokens per run. The methodology includes one-dimensional hyperparameter sweeps around default values (e.g., β1=0.9 for most optimizers) with 10% warmup and cosine decay schedule. Validation loss is measured across learning rate sweeps and other hyperparameter variations (momentum, weight decay, warmup duration, batch size, β2, ϵ).

## Key Results
- Except for SGD, all optimizers show comparable performance and hyperparameter stability across scales and architectures
- Signum recovers both the performance and hyperparameter stability of Adam when β1=β2
- Adalayer (layerwise Adam) with correction for the last layer nearly recovers the stability and performance of full Adam
- Adaptive preconditioning in the last layer and LayerNorm parameters is necessary for both performance and stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive preconditioning in the last layer and LayerNorm parameters is critical for both performance and stability of optimizers in language models.
- Mechanism: In the last layer, different tokens have vastly different frequencies, leading to different gradient scales. LayerNorm parameters have consistently small effective learning rates. Adaptive preconditioning allows these layers to use appropriate learning rates despite these disparities, preventing instability and preserving performance.
- Core assumption: The last layer and LayerNorm parameters are the primary sources of gradient scale heterogeneity in language models.
- Evidence anchors:
  - [abstract]: "we show that adapting the parameters of the last layer and LayerNorm parameters in a transformer is necessary to achieve stability and performance."
  - [section]: "Figure 7 (Left) shows that Adalayer* indeed uses vastly different learning rates for different logits, supporting our hypothesis that preconditioning weight in different logits separately is important for performance and stability."
  - [corpus]: Weak - the corpus does not contain direct evidence about LayerNorm parameter importance, but related work on optimizers mentions LayerNorm as a key architectural component.
- Break condition: If the gradient scale heterogeneity is uniform across layers, or if LayerNorm is replaced with a different normalization scheme that does not exhibit small effective learning rates.

### Mechanism 2
- Claim: Signed momentum (Signum) recovers both the performance and hyperparameter stability of Adam.
- Mechanism: When β1 = β2 in Adam, the first and second moment estimates average the previous gradients with the same coefficients. This makes Adam behave similarly to Signum, which uses the sign of the gradient for updates. The key distinction between Adam and Signum is the ability to vary β2 independently of β1 in Adam, allowing for more nuanced variance adjustment.
- Core assumption: The primary distinction between Adam and Signum is the independent control of β1 and β2 in Adam.
- Evidence anchors:
  - [abstract]: "we study signed momentum (Signum) which we see recovers both the performance and hyperparameter stability of Adam"
  - [section]: "Lemma 1 ( Balles & Hennig (2018)). Consider a parameter with a history of gradients gt, gt−1, . . .. Let m be the random variable that is equal to gt−τ with probability (1 − β1)βτ 1 and v be the random variable that is equal to gt−τ with probability (1 − β2)βτ 2 . The Adam update δAdam and the Signum update δSignum are related by δAdam = δSignum · |E[m]|p E[v2] If β1 = β2 then m = v in Lemma 1 and hence the parameter-wise ratio of Adam and Signum updates is equal to the ratio of the mean and the square root of second moment of m."
  - [corpus]: Weak - the corpus does not directly address the relationship between Adam and Signum, but mentions related work on optimizer comparisons.
- Break condition: If the variance of gradients across parameters is not well-captured by the second moment estimate, or if the sign of the gradient is not a sufficient statistic for the update direction.

### Mechanism 3
- Claim: Layerwise Adam (Adalayer) with a correction for the last layer nearly recovers the stability and performance of full Adam.
- Mechanism: Adalayer maintains a per-layer second moment estimate, reducing memory overhead compared to full Adam. By treating the set of weights feeding into each logit as a separate block, Adalayer addresses the gradient scale heterogeneity in the last layer. This coarse-grained adaptivity is sufficient to recover most of Adam's benefits.
- Core assumption: Per-layer adaptivity is sufficient to address most sources of gradient scale heterogeneity in language models.
- Evidence anchors:
  - [abstract]: "we study a layerwise variant of Adam called Adalayer, that does per-layer preconditioning and recovers much of the stability and performance exhibited by Adam"
  - [section]: "To test this hypothesis, we run a corrected version4 of Adalayer where we treat the set of weights feeding into a logit as a separate block. We henceforth refer to Adalayer with this correction as Adalayer*. This is plotted in Figure 10 and we observe that Adalayer* almost recovers the performance as well as a large fraction of the stability of Adam."
  - [corpus]: Weak - the corpus does not directly address Adalayer, but mentions related work on memory-efficient optimizers and blockwise adaptive methods.
- Break condition: If the gradient scale heterogeneity within a layer is significant and cannot be addressed by treating logits separately, or if the memory savings of Adalayer come at the cost of insufficient adaptivity.

## Foundational Learning

- Concept: Adaptive optimization algorithms (e.g., Adam, Adafactor, Lion)
  - Why needed here: The paper compares multiple adaptive optimizers and investigates their performance and stability in language model training. Understanding how these algorithms work is crucial for interpreting the results and the proposed mechanisms.
  - Quick check question: What is the key difference between Adam and Adafactor in terms of how they maintain the second moment estimate?

- Concept: Layerwise and blockwise preconditioning
  - Why needed here: The paper introduces Adalayer, a layerwise variant of Adam, and investigates the importance of adaptivity at different levels of granularity. Understanding how per-layer and per-block preconditioning works is essential for interpreting the results and the proposed mechanisms.
  - Quick check question: How does Adalayer differ from Adam in terms of the granularity of the second moment estimate it maintains?

- Concept: Effective learning rate
  - Why needed here: The paper analyzes the effective learning rates used by different optimizers and layers. Understanding how the effective learning rate is calculated and its implications for optimization is crucial for interpreting the results and the proposed mechanisms.
  - Quick check question: How is the effective learning rate calculated for a layer in Adalayer, and what does it represent?

## Architecture Onboarding

- Component map: Language model (decoder-only transformer) -> Optimizers (Adam, Adafactor, Lion, Signum, SGD, Adalayer) -> Training setup (C4 dataset, T5 tokenizer, batch size 256, sequence length 512, 20x parameter tokens) -> Key architectural components (Last layer, LayerNorm, matrix layers)

- Critical path:
  1. Train language models with different optimizers and hyperparameters
  2. Analyze performance and stability across learning rates and other hyperparameter sweeps
  3. Investigate simplified versions of Adam (Signum, Adalayer) to understand key components
  4. Analyze the importance of adaptivity in different layers (last layer, LayerNorm, matrix layers)

- Design tradeoffs:
  - Memory vs. adaptivity: Adalayer reduces memory overhead compared to Adam but may sacrifice some adaptivity
  - Per-parameter vs. per-layer adaptivity: Per-parameter adaptivity (Adam) provides more fine-grained control but is more memory-intensive
  - Training stability vs. peak performance: Some optimizers (e.g., SGD) may achieve better peak performance but are less stable across hyperparameters

- Failure signatures:
  - Poor performance or instability when using SGD on all layers
  - Lack of stability when using Adalayer without the last layer correction
  - Insufficient performance when using fixed learning rates for matrix parameters

- First 3 experiments:
  1. Train a 150m parameter language model using Adam, Adafactor, Lion, Signum, and SGD, and compare their performance and stability across learning rates.
  2. Implement Adalayer and train a 150m parameter language model, comparing its performance and stability to Adam.
  3. Train a 150m parameter language model using Adalayer with the last layer correction, and analyze the effective learning rates used for different layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms allow LayerNorm parameters to require adaptivity while other matrix parameters do not?
- Basis in paper: [explicit] The authors show that training LayerNorm parameters with Adalayer* recovers performance and stability, but training them with SGD leads to instability.
- Why unresolved: The paper does not provide a mechanistic explanation for why LayerNorm parameters behave differently from other matrix parameters.
- What evidence would resolve it: Experiments isolating the specific contributions of LayerNorm parameters (e.g., removing them entirely, using different initialization schemes) would clarify their unique role.

### Open Question 2
- Question: Why does freezing Adalayer* learning rate ratios for all layers except the last layer and LayerNorm parameters work for the 150m model but not for the 300m model?
- Basis in paper: [explicit] The authors observe that frozen ratios work well for the 150m model but show a small gap to Adam performance for the 300m model.
- Why unresolved: The paper does not investigate the scale-dependent behavior of frozen ratios.
- What evidence would resolve it: Additional experiments at intermediate scales and analysis of the relationship between model size and the need for adaptivity would clarify this discrepancy.

### Open Question 3
- Question: Can the performance gap between Adam and Adalayer* be attributed to the need for adaptivity at a finer granularity than layer-wise?
- Basis in paper: [inferred] The authors suggest that adaptivity on the last layer and LayerNorm parameters is crucial, but a small gap remains to Adam performance.
- Why unresolved: The paper does not explore whether adaptivity at a sub-layer level (e.g., per-block) is necessary.
- What evidence would resolve it: Experiments comparing Adalayer* with and without sub-layer adaptivity would determine if finer granularity is required.

## Limitations
- The study primarily focuses on decoder-only transformers and autoregressive language modeling, so results may not generalize to other architectures (e.g., encoder-decoder models, convolutional networks).
- The analysis assumes a specific training setup (C4 dataset, T5 tokenizer, batch size 256, sequence length 512) and may not apply to different data regimes or optimization settings.
- The study does not explore the interaction between optimizers and specific architectural innovations (e.g., different normalization schemes, activation functions) that may affect gradient scales and heterogeneity.

## Confidence
- High confidence: Adam, Adafactor, Lion, and Signum exhibit comparable performance and stability, except for SGD
- Medium confidence: Proposed mechanisms (adaptive preconditioning in last layer and LayerNorm, signed momentum) due to theoretical grounding and empirical support, but further validation is needed
- Medium confidence: Practitioners can choose optimizers based on practical considerations, as the study does not explore the full space of possible use cases and constraints

## Next Checks
1. Replicate the study with a broader range of architectures (e.g., encoder-decoder models, convolutional networks) to assess the generalizability of the findings.
2. Investigate the interaction between optimizers and specific architectural innovations (e.g., different normalization schemes, activation functions) to identify potential sources of gradient scale heterogeneity not captured in the current study.
3. Conduct a comprehensive study of optimizer performance across different data regimes (e.g., varying dataset sizes, sequence lengths, batch sizes) to understand the robustness of the findings to changes in the training setup.