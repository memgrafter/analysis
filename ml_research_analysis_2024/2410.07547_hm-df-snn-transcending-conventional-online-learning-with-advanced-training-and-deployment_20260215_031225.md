---
ver: rpa2
title: 'HM-DF SNN: Transcending Conventional Online Learning with Advanced Training
  and Deployment'
arxiv_id: '2410.07547'
source_url: https://arxiv.org/abs/2410.07547
tags:
- training
- online
- learning
- em-pf
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the EM-PF model, a family of advanced spiking
  neural network models for efficient online training and deployment. The model combines
  floating-point spikes with binary synaptic weights to address the inseparability
  problem of temporal dependent gradients in online learning.
---

# HM-DF SNN: Transcending Conventional Online Learning with Advanced Training and Deployment

## Quick Facts
- arXiv ID: 2410.07547
- Source URL: https://arxiv.org/abs/2410.07547
- Reference count: 26
- Key outcome: The EM-PF model achieves state-of-the-art performance in online learning tasks while saving approximately 15× parameter memory compared to traditional approaches.

## Executive Summary
This paper proposes the EM-PF model, a family of advanced spiking neural network models that address the critical inseparability problem of temporal dependent gradients in online learning. By combining floating-point spikes with binary synaptic weights, the model achieves superior gradient separability while maintaining computational efficiency. The model introduces a hybrid mechanism-driven firing scheme that enables full-stage optimization of computation speed and memory footprint, achieving state-of-the-art performance across multiple datasets including CIFAR-10/100, ImageNet-200/1k, and DVS-CIFAR10.

## Method Summary
The EM-PF model addresses online learning challenges by using floating-point spikes during forward propagation with binary synaptic weights, creating gradient separability. The model incorporates membrane potential batch-normalization to regulate backward gradient distributions, employs random back-propagation to reduce training time by selecting only one time-step per mini-batch, and utilizes parallel computation during inference to eliminate temporal dependencies. The architecture can be flexibly combined with techniques like channel attention mechanisms to achieve high accuracy while saving approximately 15× parameter memory compared to traditional approaches.

## Key Results
- Top-1 accuracy of 79.91% on CIFAR-100, 60.68% on ImageNet-200, 68.07% on ImageNet-1k, and 83.00% on DVS-CIFAR10
- Saves approximately 15× parameter memory compared to traditional approaches
- Achieves superior performance within fewer time-steps
- Successfully demonstrates online learning capabilities across multiple challenging datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EM-PF model achieves gradient separability by using floating-point spikes with binary synaptic weights.
- Mechanism: The model emits floating-point spikes during forward propagation while maintaining binary synaptic weights. This separation allows the surrogate gradient to be independent of membrane potential values, enabling temporal gradients to be detached during online learning without performance degradation.
- Core assumption: The use of floating-point spikes can convey sufficient information while binary weights maintain computational efficiency.
- Evidence anchors:
  - [abstract] "The model combines floating-point spikes with binary synaptic weights to address the inseparability problem of temporal dependent gradients in online learning."
  - [section] "We choose ReLU as ActFunc(·) to make the surrogate gradient of EM-PF model independent of the corresponding membrane potential value."
- Break condition: If the floating-point spike representation becomes too sparse or the binary weights cannot adequately capture the learned features, gradient separability may fail.

### Mechanism 2
- Claim: Membrane potential batch-normalization regulates the distribution of membrane potentials to control gradient separability.
- Mechanism: By applying batch-normalization to membrane potentials at each time-step, the model can control the scale and shift of potential values, which directly affects the distribution of backward gradients. This allows for adaptive regulation of gradient separability during training.
- Core assumption: The distribution of membrane potentials significantly influences the separability of backward gradients.
- Evidence anchors:
  - [section] "ml[t] = λl[t] ⊙ vl[t−1] + I l[t], ˆml[t] = αl[t] ⊙ ml[t] + βl[t] ⊙ BNt(ml[t]),"
  - [section] "We assign corresponding BNt(·) for each time-step, achieving more precise control for membrane potential distribution and gradient separability."
- Break condition: If the normalization parameters (αl[t], βl[t]) become poorly tuned or the batch size is too small, the normalization may destabilize gradient flow.

### Mechanism 3
- Claim: Random back-propagation combined with parallel computation achieves full-stage optimization of training and inference.
- Mechanism: Random back-propagation reduces training time by propagating gradients through only one randomly selected time-step per mini-batch, while parallel computation eliminates temporal dependencies in the forward pass, allowing simultaneous computation across all time-steps during inference.
- Core assumption: The model can maintain learning performance with gradient updates at only one time-step per batch.
- Evidence anchors:
  - [section] "we randomly select only 1 time-step within T time-steps for each mini-batch data to propagate backward gradient."
  - [section] "This simplified EM-PF model does not take into account residual membrane potential information from previous time-steps, allowing for simultaneous forward calculation for all time-steps."
- Break condition: If the random selection consistently misses critical time-steps where gradients are most informative, learning may degrade significantly.

## Foundational Learning

- Concept: Temporal gradient separability in spiking neural networks
  - Why needed here: The core innovation of EM-PF is solving the inseparability problem of temporal dependent gradients in online learning.
  - Quick check question: What is the key difference between how STBP and EM-PF handle temporal gradients during back-propagation?

- Concept: Binary vs floating-point precision trade-offs
  - Why needed here: EM-PF uses floating-point spikes with binary synaptic weights, requiring understanding of when and why to use each precision type.
  - Quick check question: Why does the model use floating-point spikes during forward propagation but binary weights for synaptic connections?

- Concept: Surrogate gradient functions and their dependence on membrane potential
  - Why needed here: The choice of activation function (ReLU) in EM-PF is specifically designed to make the surrogate gradient independent of membrane potential.
  - Quick check question: How does using ReLU as the activation function in EM-PF differ from traditional surrogate gradient functions in STBP?

## Architecture Onboarding

- Component map: Input current -> Binary convolutional layers -> Membrane potential accumulation with leakage -> Membrane potential batch-normalization -> Floating-point spike generation (ReLU) -> Channel attention weighting -> Spike propagation to next layer

- Critical path:
  1. Input current flows through binary convolutional layers
  2. Membrane potentials accumulate with leakage
  3. Batch-normalization regulates membrane potential distribution
  4. Floating-point spikes are generated via activation function
  5. Channel attention weights the current
  6. Spikes propagate to next layer
  7. During training, gradients flow backward with random time-step selection
  8. During inference, parallel computation eliminates temporal dependencies

- Design tradeoffs:
  - Floating-point spikes provide better gradient separability but require more memory than binary spikes
  - Binary synaptic weights reduce memory footprint but may limit representational capacity
  - Random back-propagation accelerates training but may miss important gradient signals
  - Parallel computation speeds up inference but removes temporal information that might be useful

- Failure signatures:
  - Training instability: Check if membrane potential batch-normalization parameters are diverging
  - Poor accuracy: Verify that floating-point spikes are not becoming too sparse or too dense
  - Slow inference: Confirm parallel computation mode is properly activated
  - Memory issues: Ensure binary weight conversion is working correctly

- First 3 experiments:
  1. Implement a single EM-PF layer with floating-point spikes and binary weights, verify gradient flow is independent of membrane potential
  2. Add membrane potential batch-normalization and test gradient separability under different normalization strengths
  3. Integrate random back-propagation and measure training time reduction while monitoring accuracy stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the separability of backward gradients in the EM-PF model compare quantitatively to other online learning approaches for SNNs?
- Basis in paper: [explicit] The paper discusses the concept of Separable Backward Gradient and claims EM-PF has superior backward gradient separability compared to vanilla spiking models, but doesn't provide quantitative comparisons.
- Why unresolved: The paper establishes the theoretical concept but doesn't provide empirical measurements or comparisons of gradient separability across different models.
- What evidence would resolve it: Experimental results measuring and comparing the separability of backward gradients across EM-PF, vanilla online learning, and STBP approaches using metrics like gradient correlation or gradient norm differences.

### Open Question 2
- Question: What is the optimal balance between the three EM-PF model versions (learnable parameters, membrane potential batch-normalization, and parallel computation) for different types of tasks and datasets?
- Basis in paper: [inferred] The paper introduces three versions of EM-PF model and discusses their individual benefits, but doesn't systematically analyze when to use which combination.
- Why unresolved: The paper demonstrates the effectiveness of each version separately but doesn't provide guidance on optimal combinations for different scenarios.
- What evidence would resolve it: Systematic ablation studies testing different combinations of the three versions across various tasks, datasets, and computational constraints.

### Open Question 3
- Question: How does the performance of EM-PF models change when using different activation functions beyond ReLU for the spike generation?
- Basis in paper: [explicit] The paper mentions using ReLU as the activation function but doesn't explore other options.
- Why unresolved: The choice of activation function could significantly impact the model's performance and properties, but this aspect remains unexplored.
- What evidence would resolve it: Comparative experiments testing various activation functions (e.g., sigmoid, tanh, softplus) for spike generation in EM-PF models across different datasets and tasks.

## Limitations

- The paper lacks comprehensive ablation studies to quantify the individual contributions of each architectural component to overall performance
- Memory efficiency claims require verification across different hardware architectures and precision formats
- Scalability to very deep networks and more complex architectures remains unverified, particularly where temporal dependencies might be more critical

## Confidence

- High confidence: The core architectural components (floating-point spikes, binary weights, membrane potential batch-normalization) are technically sound and implementable
- Medium confidence: The claimed memory efficiency improvements are reasonable but depend on specific implementation choices not fully detailed in the paper
- Medium confidence: The accuracy improvements over baseline methods are demonstrated but lack comprehensive ablation studies to isolate the contribution of each innovation

## Next Checks

1. Implement a diagnostic tool to visualize the distribution of ∂sl[t]/∂ml[t] across time-steps during training to empirically verify that the surrogate gradient remains independent of membrane potential values as claimed
2. Conduct detailed memory profiling of the EM-PF model implementation to measure actual memory consumption versus theoretical savings, accounting for both training and inference scenarios
3. Perform controlled experiments removing individual components (floating-point spikes, binary weights, membrane potential batch-normalization) to quantify their specific contributions to both accuracy and efficiency gains