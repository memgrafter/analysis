---
ver: rpa2
title: 'Drama Engine: A Framework for Narrative Agents'
arxiv_id: '2408.11574'
source_url: https://arxiv.org/abs/2408.11574
tags:
- drama
- context
- companions
- prompt
- companion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Drama Engine framework adapts multi-agent systems to create
  narrative agents for creative writing applications. It enables dynamic, context-aware
  companions that can develop over time, interact with users and each other, and delegate
  tasks to specialized deputies.
---

# Drama Engine: A Framework for Narrative Agents

## Quick Facts
- arXiv ID: 2408.11574
- Source URL: https://arxiv.org/abs/2408.11574
- Authors: Martin Pichlmair; Riddhi Raj; Charlene Putney
- Reference count: 8
- Primary result: A TypeScript framework enabling dynamic, context-aware narrative agents with multi-agent delegation and automatic context summarization

## Executive Summary
The Drama Engine framework adapts multi-agent system principles to create narrative agents for creative writing applications. It enables dynamic companions that can develop over time, interact with users and each other, and delegate tasks to specialized deputies. The system supports automatic context summarization and integrates with external data through a world state database. Implemented in TypeScript, it is model-agnostic and supports various LLM backends.

## Method Summary
The Drama Engine is implemented in TypeScript as a model-agnostic framework that uses multi-agent delegation and dynamic prompt assembly to create narrative agents. The system orchestrates conversations through a moderator, allows agents to delegate complex tasks to specialized deputies, and manages context through automatic summarization. Companions can develop over time through mood systems and knowledge unlocking, while the framework maintains flexibility by supporting any backend that follows OpenAI's API standard.

## Key Results
- Enables multi-agent conversations with dynamic delegation to specialized deputies
- Supports companion development through mood systems and unlockable knowledge
- Successfully deployed in Writers Room product for multi-agent chats and virtual co-worker scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework enables dynamic narrative agents through multi-agent delegation and prompt assembly
- Mechanism: Uses a moderator to orchestrate conversations where agents delegate complex tasks to specialized deputies, allowing modular prompt chains and dynamic context-aware responses
- Core assumption: Multi-agent delegation and dynamic prompt assembly are effective for creating interactive narrative agents
- Evidence anchors: [abstract] mentions multi-agent system principles; [section] describes core features including delegation mechanisms
- Break condition: Delegation complexity or prompt assembly failure could break narrative flow

### Mechanism 2
- Claim: Companions develop over time through unlocked knowledge and mood systems
- Mechanism: Companions have configurable moods with probabilities and unlock new knowledge based on interactions or tracked values
- Core assumption: Character development through mood systems and knowledge unlocking enhances narrative experience
- Evidence anchors: [abstract] mentions companion development and mood systems; [section] describes unlocking functionality
- Break condition: Predictable mood systems or unbalanced knowledge unlocking could lead to repetitive behavior

### Mechanism 3
- Claim: The framework is model-agnostic and integrates with various LLM backends
- Mechanism: Designed to work with any backend supporting OpenAI's API standard and can use different prompt formats
- Core assumption: Model-agnostic design allows flexibility in choosing appropriate models for specific tasks
- Evidence anchors: [abstract] discusses architecture and prompt assembly; [section] mentions model- and vendor-agnostic capabilities
- Break condition: Poor integration with specific backends or unsuitable model selection could impact performance

## Foundational Learning

- Concept: Multi-agent systems and their principles
  - Why needed here: The framework adapts multi-agent system principles to create narrative agents
  - Quick check question: What are the key components of a multi-agent system, and how do they interact with each other?

- Concept: Prompt engineering and dynamic prompt assembly
  - Why needed here: The framework uses dynamic prompt assembly to create context-aware responses
  - Quick check question: How does dynamic prompt assembly differ from static prompt engineering, and what are the benefits of using dynamic prompts in a narrative agent system?

- Concept: Character development and role-playing in video games
  - Why needed here: The framework incorporates video game character development concepts
  - Quick check question: How do video games typically implement character development and mood systems, and how does the Drama Engine adapt these concepts for narrative agents?

## Architecture Onboarding

- Component map: Drama -> Context -> Moderator -> Companions (AutoCompanion, ChatCompanion) -> Deputies (InstructionDeputy) -> WorldState

- Critical path:
  1. User initiates conversation or action
  2. Drama object handles request and creates context
  3. Moderator selects next speaker based on prioritization rules
  4. Selected companion generates reply using context and prompt assembly
  5. Reply posted to chat and context updated
  6. Process repeats until conversation ends or action completed

- Design tradeoffs:
  - Flexibility vs. complexity: Extensive customization may introduce management complexity
  - Model-agnostic design vs. optimization: Works with various backends but may not be optimized for specific models
  - Character development vs. predictability: Mood systems add depth but may lead to less predictable behavior

- Failure signatures:
  - Unexpected or inappropriate companion responses
  - Difficulty managing complex multi-agent conversations
  - Poor integration with specific LLM backends
  - Overly predictable or repetitive character behavior
  - Performance issues from excessive context or complex prompt assembly

- First 3 experiments:
  1. Implement simple 1:1 conversation between user and NPC companion to test basic functionality
  2. Create multi-agent conversation with two NPCs and moderator to evaluate delegation mechanisms
  3. Integrate with specific LLM backend (e.g., OpenAI) and test model-agnostic design across different models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does companion development through unlockable knowledge and mood systems affect long-term user engagement compared to traditional single-agent systems?
- Basis in paper: [explicit] - Mentions companions can unlock knowledge and have mood systems that affect behavior
- Why unresolved: Paper lacks quantitative data on user engagement or long-term interaction patterns
- What evidence would resolve it: User studies comparing engagement metrics between Drama Engine and traditional single-agent systems over extended periods

### Open Question 2
- Question: What is the optimal balance between model size and quality for companion interactions, considering trade-offs in inference cost and speed?
- Basis in paper: [explicit] - Discusses trade-offs between model size and reply quality
- Why unresolved: Paper doesn't provide specific quantitative data on performance-quality trade-off curve
- What evidence would resolve it: Systematic testing of different model sizes across various metrics in real-world scenarios

### Open Question 3
- Question: How effective are current moderation and speaker selection algorithms in creating natural and engaging multi-agent conversations?
- Basis in paper: [explicit] - Describes moderation system but doesn't evaluate effectiveness
- Why unresolved: No user feedback or engagement metrics provided for multi-agent conversation features
- What evidence would resolve it: User studies measuring conversation quality, engagement, and perceived naturalness

## Limitations

- Model dependence: Framework behavior heavily influenced by underlying LLM's capabilities and fine-tuning
- Context management complexity: Dynamic prompt assembly may lead to context fragmentation in extended conversations
- Evaluation gaps: Limited empirical validation of framework's effectiveness in real creative writing scenarios

## Confidence

- High confidence: Core multi-agent architecture and TypeScript implementation
- Medium confidence: Model-agnostic design benefits and context summarization capabilities
- Low confidence: Long-term companion development mechanisms and their impact on narrative quality

## Next Checks

1. Implement controlled tests measuring how well the framework maintains narrative coherence across 10+ conversational turns, tracking context drift and topic consistency

2. Systematically test the framework across at least three different LLM backends (OpenAI, Anthropic, local models) measuring response quality, latency, and context handling capacity

3. Analyze companion conversations for repetitive patterns, measuring style convergence rates and testing whether mood/probability systems actually produce varied interactions rather than predictable behavior