---
ver: rpa2
title: 'From Babble to Words: Pre-Training Language Models on Continuous Streams of
  Phonemes'
arxiv_id: '2410.22906'
source_url: https://arxiv.org/abs/2410.22906
tags:
- language
- word
- phonemic
- blimp
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether language models can learn grammatical
  knowledge from continuous phoneme streams rather than standard orthographic text.
  To enable this, the authors develop a pipeline to convert text datasets into continuous
  phoneme streams and train a GPT-2 model using all combinations of three input transformations:
  character tokenization, word boundary removal, and phonemic transcription.'
---

# From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes

## Quick Facts
- arXiv ID: 2410.22906
- Source URL: https://arxiv.org/abs/2410.22906
- Reference count: 35
- Language models can learn grammatical knowledge from continuous phoneme streams, achieving competitive performance on language understanding tasks while excelling at phonological benchmarks

## Executive Summary
This paper investigates whether language models can learn grammatical knowledge from continuous phoneme streams rather than standard orthographic text. The authors develop a pipeline to convert text datasets into continuous phoneme streams and train a GPT-2 model using all combinations of three input transformations: character tokenization, word boundary removal, and phonemic transcription. Results show that phoneme-based training slightly reduces performance on traditional language understanding tasks (3.5% decrease on BLiMP, 1.5% on GLUE) but remains competitive with orthographic models. The models achieve state-of-the-art scores on BabySLM, demonstrating strong phonological and syntactic capabilities when trained on phonemes. This work validates phoneme streams as a viable training paradigm for language models, enabling future work on phonological interpretability and speech-based applications.

## Method Summary
The authors convert text datasets into continuous phoneme streams using the Corpus Phonemizer tool with espeak-ng backend. They train GPT-2 models using all combinations of three input transformations: character tokenization, word boundary removal, and phonemic transcription. The study uses the 100-million-word BabyLM pre-training dataset and evaluates models on BLiMP, GLUE, and BabySLM benchmarks. Training runs for 400k steps with 23 different configurations, and the best checkpoint is selected based on lowest perplexity. The evaluation measures grammatical capabilities, downstream language understanding, and phonological and syntactic performance across the different input representations.

## Key Results
- Phoneme-based training reduces BLiMP performance by 3.5% and GLUE by 1.5% compared to orthographic models
- Character tokenization affects GLUE due to truncation issues from increased sequence length
- Phonemic transcription significantly impacts BLiMP Supplement due to loss of punctuation information
- Models achieve state-of-the-art scores on BabySLM, demonstrating strong phonological and syntactic capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can learn grammatical knowledge from phoneme streams, not just orthographic text.
- Mechanism: The model's internal representations capture distributional patterns of phonemes, enabling syntactic generalization even without explicit word boundaries or orthography.
- Core assumption: The transformer architecture can adapt to character-level tokenization and continuous phoneme streams while preserving sufficient linguistic structure for downstream tasks.
- Evidence anchors:
  - [abstract] "Results show that phoneme-based training slightly reduces performance on traditional language understanding tasks... but remains competitive with orthographic models."
  - [section 5.1] "Comparing the GPT-2 model with no input adjustments (top row) to the same model with all three transformations applied (bottom row), we notice a decrease in performance... but the decrease is not substantial and scores remaining competitive with the baseline models."
  - [corpus] Weak corpus evidence - related work focuses on orthographic/phonemic robustness but not direct grammatical learning from phonemes.
- Break condition: If phoneme stream patterns lack sufficient distributional cues for syntax, or if the model's architecture cannot handle character-level continuous input without losing structural information.

### Mechanism 2
- Claim: Character tokenization enables fine-grained phoneme representation learning, beneficial for phonological tasks.
- Mechanism: Treating each phoneme as a separate token allows the model to learn individual phoneme distributions and minimal pair distinctions, crucial for phonological benchmarks.
- Core assumption: Character-level tokenization, despite increased sequence length, provides more granular input signals than subword tokenization for phonological learning.
- Evidence anchors:
  - [section 5.2] "For the lexical test... it seems more appropriate to use a character-based tokenization as the model needs to learn the distributional properties of individual phonemes, which may be lost in subword units."
  - [section 5.1] "Using a phonemic transcription instead of the original written text significantly decreases performance on BLiMP and GLUE... It also leads to the largest decrease of 11.3% for BLiMP Supplement."
  - [corpus] Weak evidence - corpus shows related work on phoneme representations but not character-level benefits specifically.
- Break condition: If character tokenization causes excessive sequence truncation (as seen with GLUE tasks), or if the model cannot effectively learn from such granular input without performance collapse.

### Mechanism 3
- Claim: Word boundary removal forces the model to learn implicit segmentation, mimicking speech processing.
- Mechanism: Training without explicit word boundaries encourages the model to develop internal representations that can segment continuous phoneme streams, similar to how humans process speech.
- Core assumption: The transformer can learn to segment continuous input effectively without explicit boundary markers, developing implicit segmentation strategies.
- Evidence anchors:
  - [section 5.1] "Word boundary removal... significantly decreases the BLiMP score, but the decreases for BLiMP Supplement and GLUE are not significant."
  - [section 2.3] "When using a phonemic input representation to model speech, word boundaries are not typically included, as word boundaries are not explicitly marked in the speech stream."
  - [corpus] Moderate evidence - corpus includes work on word segmentation from continuous speech streams.
- Break condition: If the model cannot effectively learn segmentation from continuous input, leading to degraded performance on tasks requiring word-level understanding.

## Foundational Learning

- Concept: Phoneme representation and International Phonetic Alphabet (IPA)
  - Why needed here: The entire study converts orthographic text to IPA phonemes, requiring understanding of phonetic transcription systems.
  - Quick check question: What is the primary difference between graphemes and phonemes, and why does this distinction matter for language modeling?

- Concept: Transformer architecture and tokenization strategies
  - Why needed here: The study uses GPT-2 and compares different tokenization approaches (character, subword, with/without word boundaries).
  - Quick check question: How does character-level tokenization differ from subword tokenization in terms of vocabulary size and sequence length, and what are the computational implications?

- Concept: Language acquisition and distributional learning
  - Why needed here: The work relates to how models learn language from raw input, similar to how children acquire language from continuous speech.
  - Quick check question: What is distributional learning, and how might it explain a model's ability to learn grammar from continuous phoneme streams?

## Architecture Onboarding

- Component map: Text -> Phoneme conversion -> Tokenizer preparation -> Model training -> Evaluation on BLiMP/GLUE/BabySLM
- Critical path: Text -> Phoneme conversion -> Tokenizer preparation -> Model training -> Evaluation on BLiMP/GLUE/BabySLM
- Design tradeoffs: Character tokenization increases vocabulary size and sequence length but provides finer-grained input; word boundary removal simplifies input but may hurt word-level tasks; phonemic transcription loses orthography-specific information like punctuation
- Failure signatures: Performance collapse on tasks requiring word-level understanding when word boundaries are removed; truncation issues when character tokenization increases sequence length beyond context window; punctuation-dependent task failures when punctuation is removed
- First 3 experiments:
  1. Train baseline GPT-2 with standard BPE tokenization and compare to character-tokenized version on GLUE tasks to measure truncation impact
  2. Train model with word boundaries removed and evaluate on BLiMP to quantify segmentation effects
  3. Train model with phonemic transcription only and evaluate on BabySLM lexical task to measure phonological learning capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do phonemic input representations affect language model performance across different languages and writing systems?
- Basis in paper: [explicit] The authors note their experiments are limited to English and suggest multilingual evaluation is needed.
- Why unresolved: The study only examines English with IPA transcriptions, leaving open whether results generalize to languages with different phonological systems or orthographic conventions.
- What evidence would resolve it: Conducting comparable experiments with phonemic streams for multiple languages (e.g., Mandarin, Arabic, Russian) and analyzing performance patterns across linguistic typologies.

### Open Question 2
- Question: What is the optimal model architecture for learning from phonemic input representations?
- Basis in paper: [inferred] The authors observe that standard GPT-2 architecture, optimized for written text, may not be ideal for phonemic streams, and that performance varies significantly with input transformations.
- Why unresolved: The study uses a fixed GPT-2 architecture without exploring architectural modifications specifically designed for phonemic input, leaving open whether performance gaps could be reduced through architectural changes.
- What evidence would resolve it: Systematic exploration of architectural modifications (attention mechanisms, positional encoding, tokenization strategies) specifically designed for phonemic streams and comparison of their performance against standard architectures.

### Open Question 3
- Question: How do phonemic representations compare to raw audio for language acquisition modeling?
- Basis in paper: [explicit] The authors discuss this in their conclusion, noting that while phonemic streams may seem more cognitively plausible than text, many studies go further and seek to train directly on raw audio.
- Why unresolved: The study establishes that language models can learn from phonemic streams but doesn't compare this to the substantial body of work training directly on raw audio, leaving open questions about the relative merits of different input representations for acquisition modeling.
- What evidence would resolve it: Direct comparison of language model performance on identical benchmarks when trained on (1) orthographic text, (2) phonemic streams, and (3) raw audio, controlling for model size and training data quantity.

## Limitations
- Focus on English-only data constrains generalizability to other languages with different phonological systems
- 128-token context window creates significant truncation issues when using character tokenization
- Loss of punctuation information during phonemic transcription substantially impacts tasks requiring syntactic markers
- Does not explore whether models can learn word segmentation implicitly from continuous streams

## Confidence

**High Confidence**: The finding that phoneme-based training remains competitive with orthographic models on traditional language understanding tasks (3.5% decrease on BLiMP, 1.5% on GLUE) is well-supported by the experimental results showing consistent performance across multiple configurations and benchmarks.

**Medium Confidence**: The claim that character tokenization provides benefits for phonological tasks is partially supported but limited by truncation effects. While the lexical task results suggest benefits for learning individual phoneme distributions, the computational constraints of increased sequence length create practical limitations that are not fully addressed.

**Low Confidence**: The assertion that models can learn implicit word segmentation from continuous phoneme streams without explicit boundaries is weakly supported. The significant performance decrease on BLiMP when word boundaries are removed suggests this capability may be limited or task-dependent.

## Next Checks
1. **Cross-linguistic validation**: Test the phoneme-based training paradigm on languages with non-phonemic orthographies (like French or Russian) and languages with rich morphology to assess generalizability beyond English.

2. **Boundary learning evaluation**: Design controlled experiments to explicitly test whether models can learn to segment continuous phoneme streams, using tasks that require word-level understanding and comparing models trained with vs without explicit boundaries.

3. **Extended context window analysis**: Train models with larger context windows (e.g., 512 or 1024 tokens) using character tokenization to determine whether truncation effects are the primary limitation, and measure the trade-off between granularity and computational efficiency.