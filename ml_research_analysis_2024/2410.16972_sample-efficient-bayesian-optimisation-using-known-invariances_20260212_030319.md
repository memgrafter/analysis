---
ver: rpa2
title: Sample-efficient Bayesian Optimisation Using Known Invariances
arxiv_id: '2410.16972'
source_url: https://arxiv.org/abs/2410.16972
tags:
- invariant
- kernel
- functions
- where
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a Bayesian optimization framework that incorporates
  known group invariances to improve sample efficiency. The authors construct totally
  invariant kernels by symmetrizing base kernels, enabling algorithms like UCB and
  MVR to exploit symmetries in the objective function.
---

# Sample-efficient Bayesian Optimisation Using Known Invariances

## Quick Facts
- arXiv ID: 2410.16972
- Source URL: https://arxiv.org/abs/2410.16972
- Authors: Theodore Brown; Alexandru Cioba; Ilija Bogunovic
- Reference count: 40
- Primary result: Develops Bayesian optimization framework incorporating group invariances that achieves 1/|G| improvement in sample complexity

## Executive Summary
This paper presents a method for incorporating known group invariances into Bayesian optimization (BO) to improve sample efficiency. The authors construct totally invariant kernels by symmetrizing base kernels, enabling algorithms like UCB and MVR to exploit symmetries in the objective function. Theoretical contributions include upper bounds on sample complexity showing a 1/|G| improvement and lower bounds that nearly match the upper bounds. Experiments demonstrate significant performance gains on synthetic tasks with permutation, cyclic, and dihedral symmetries, as well as on a nuclear fusion design problem where the invariant approach found high-performance solutions where standard methods failed.

## Method Summary
The method constructs totally invariant kernels by symmetrizing base kernels using group transformations. Specifically, given a base kernel k and a finite group G of isometries, the invariant kernel is defined as kG(x,y) = (1/|G|) Σ_{σ∈G} k(σ(x),y). This kernel is used in Gaussian process models for Bayesian optimization with standard acquisition functions like UCB and MVR. The theoretical analysis derives upper and lower bounds on sample complexity that show the 1/|G| improvement factor is tight up to logarithmic terms. For large groups where full invariance computation is expensive, the paper proposes using subgroup approximations as a computationally efficient alternative.

## Key Results
- Sample complexity upper bound shows 1/|G| improvement factor in information gain and regret bounds
- Upper and lower bounds on sample complexity match up to logarithmic factors and a gap in the exponent of 1/|G|
- Experimental results show invariant BO significantly outperforms standard BO on synthetic tasks with permutation, cyclic, and dihedral symmetries
- On a nuclear fusion design problem, invariant methods found high-performance solutions where standard methods failed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The symmetrization of base kernels creates totally invariant kernels that improve sample efficiency in Bayesian optimization by a factor of 1/|G|.
- Mechanism: The totally invariant kernel kG(x, y) = (1/|G|) Σ_{σ∈G} k(σ(x), y) aggregates information from all symmetric transformations, allowing each observation to inform multiple transformed points simultaneously.
- Core assumption: The objective function f is exactly invariant to the known group G of transformations.
- Evidence anchors:
  - [abstract]: "provide a method for incorporating group invariances into the kernel of the GP to produce invariance-aware algorithms that achieve significant improvements in sample efficiency"
  - [section 3.2]: "we have the following upper bound on sample complexity... γG_T = O(1/|G| T^{d-1}/β*)"
  - [corpus]: Weak - the corpus papers focus on general symmetry exploitation but don't specifically address the 1/|G| factor in Bayesian optimization context
- Break condition: If the true objective function deviates significantly from perfect invariance (quasi-invariance with large ε), the performance gains diminish as shown in Section 4.2.

### Mechanism 2
- Claim: The projection operator SG creates a reproducing kernel Hilbert space (RKHS) of invariant functions with a totally invariant kernel.
- Mechanism: SG(f) = (1/|G|) Σ_{σ∈G} f∘σ projects any function onto the subspace of G-invariant functions, and the resulting space Im[SG] is itself an RKHS with kernel kG that is totally invariant.
- Core assumption: The base kernel k is simultaneously invariant to G (k(σ(x), σ(y)) = k(x, y) for all σ∈G).
- Evidence anchors:
  - [section 2]: "The kernel defines a reproducing kernel Hilbert space, Hk... Define the symmetrization operator SG... Then, SG is a self-adjoint projection operator, whose image Im[SG] is the subspace of Hk that contains G-invariant functions"
  - [appendix A.1]: Full proof showing SG is self-adjoint projection and Im[SG] is an RKHS with kernel kG
  - [corpus]: Weak - no direct corpus evidence for this specific projection-based RKHS construction
- Break condition: If the base kernel k is not simultaneously invariant, the symmetrization process doesn't produce a valid reproducing kernel for the invariant subspace.

### Mechanism 3
- Claim: Upper and lower bounds on sample complexity match up to logarithmic factors, confirming the theoretical optimality of the invariance-aware approach.
- Mechanism: The information gain γG_T for the invariant kernel scales as O(1/|G| T^{d-1}/β*), and this translates to sample complexity bounds that show the 1/|G| improvement is tight.
- Core assumption: The eigenvalues of the integral operator decay polynomially at rate O(k^{-β*}) and the group G satisfies the spectral property from Assumption 1.
- Evidence anchors:
  - [section 3.2]: "γG_T = O(1/|G| T^{d-1}/β* log factors)" and "T = O(1/|G|^{2ν+d-1}/2ν ε^{-(2ν+d-1)/ν})"
  - [section 3.3]: "T = Ω(1/|G|^{(ν+d-1)/ν} ε^{-(2ν+d-1)/ν} σ^2 B^{d-1}/ν log 1/δ)"
  - [corpus]: Weak - the corpus contains related work on symmetry in RL and BO but doesn't provide matching upper/lower bounds for the specific 1/|G| factor

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The entire theoretical framework relies on understanding that functions can be represented in a Hilbert space with a reproducing kernel, and that invariance properties translate to kernel properties
  - Quick check question: What is the reproducing property of a kernel k, and how does it relate to function evaluation in the RKHS?

- Concept: Group theory and group actions
  - Why needed here: The paper uses finite groups G of isometries acting on the domain X, and the analysis depends on understanding how these group actions preserve structure
  - Quick check question: What is the difference between simultaneous invariance (k(σ(x), σ(y)) = k(x, y)) and total invariance (k(σ(x), τ(y)) = k(x, y)) of a kernel?

- Concept: Bayesian optimization with Gaussian processes
  - Why needed here: The practical algorithm uses GP models with invariant kernels and acquisition functions like UCB and MVR to optimize the objective
  - Quick check question: How does the posterior mean and variance update when using a totally invariant kernel kG instead of the standard kernel k?

## Architecture Onboarding

- Component map: Base kernel k (e.g., Matérn) -> Symmetrization operator SG -> Totally invariant kernel kG -> GP model with kG -> Acquisition function (UCB/MVR) -> Optimization loop
- Critical path: kernel symmetrization -> GP posterior computation -> acquisition function optimization -> observation collection -> regret minimization
- Design tradeoffs: Full invariance (|G| terms in kernel) vs. computational cost vs. partial invariance (subgroup approximation)
- Failure signatures:
  - Poor performance when objective is not truly invariant
  - Computational bottlenecks with large groups (|G|! terms)
  - Numerical instability in kernel matrix inversion with highly symmetric data
- First 3 experiments:
  1. Implement symmetrization for simple permutation group on 2D hypercube and verify kG(x,y) = (1/2)(k(x,y) + k(π(x),y))
  2. Test UCB with invariant kernel on synthetic permutation-invariant function vs standard kernel
  3. Measure computation time scaling with group size for kG evaluation on 6D example

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of improvement in sample complexity when incorporating group invariances into Bayesian optimization, and how does this depend on the group size and the specific structure of the invariance?
- Basis in paper: [explicit] The paper derives upper and lower bounds on sample complexity showing a 1/|G| improvement factor, but notes a gap between the bounds with the exponent of 1/|G| being 1+(d-1)/ν in the upper bound but 1+(d-1)/2ν in the lower bound.
- Why unresolved: The paper acknowledges this gap and suggests it may be due to the lower bound construction using packing arguments for invariant functions, but does not provide a method to close this gap.
- What evidence would resolve it: A tighter analysis of the lower bound construction that matches the upper bound's dependence on 1/|G|, or an improved upper bound that accounts for the looser dependence in the lower bound.

### Open Question 2
- Question: How does the performance of invariance-aware Bayesian optimization algorithms degrade when the target function only approximately satisfies the assumed invariance, and what is the theoretical framework for quantifying this degradation?
- Basis in paper: [explicit] The paper conducts experiments on "quasi-invariant" functions modeled as a sum of invariant and non-invariant components, showing that using the fully invariant kernel still provides significant improvements when the deviation from invariance is small.
- Why unresolved: While empirical results show robustness to approximate invariance, the paper does not provide a theoretical analysis of the trade-off between incorporating partial invariance and maintaining performance guarantees.
- What evidence would resolve it: A theoretical framework that quantifies the relationship between the degree of invariance violation and the resulting performance degradation, potentially through bounds on regret or sample complexity for quasi-invariant functions.

### Open Question 3
- Question: For large groups where computing the fully invariant kernel is computationally expensive, what is the optimal strategy for approximating the invariance to balance computational cost and sample efficiency gains?
- Basis in paper: [explicit] The paper notes that for very large groups (e.g., permutation groups in high dimensions), the cost of computing the invariant kernel becomes prohibitive, and proposes using subgroup approximations as a low-cost alternative.
- Why unresolved: The paper demonstrates empirically that subgroup approximations can still achieve significant performance improvements, but does not provide a theoretical framework for selecting the optimal subgroup or quantifying the trade-off between computational cost and sample efficiency.
- What evidence would resolve it: A theoretical analysis of the relationship between subgroup size, computational cost, and sample efficiency, potentially through bounds on regret or sample complexity for subgroup-invariant kernels, along with a principled method for selecting the optimal subgroup.

## Limitations

- Theoretical bounds assume perfect invariance, but the quasi-invariance analysis suggests performance degrades with ε > 0 without quantifying the exact threshold
- Computational cost scales linearly with |G| for kernel evaluation, becoming prohibitive for large groups like permutation groups in high dimensions
- Nuclear fusion application results are promising but lack comparison to domain-specific optimization methods that might already incorporate physical symmetries

## Confidence

- **High confidence**: The symmetrization framework for creating totally invariant kernels is mathematically sound and well-established in the literature. The 1/|G| improvement factor is theoretically justified through the information gain analysis.
- **Medium confidence**: The experimental results demonstrate clear performance improvements on synthetic tasks, but the nuclear fusion case study has limited validation and the quasi-invariance analysis is preliminary.
- **Low confidence**: The matching upper and lower bounds on sample complexity are asymptotic and the logarithmic factors could be significant in practice.

## Next Checks

1. **Scalability analysis**: Implement and measure the computational overhead of invariant kernels on synthetic problems with increasing group sizes (|G| = 4, 24, 120, 720) to quantify the trade-off between invariance and computation time.

2. **Robustness to quasi-invariance**: Systematically vary ε in the quasi-invariant function family and measure the crossover point where standard BO outperforms invariance-aware methods, testing the theoretical predictions from Section 4.2.

3. **Subgroup selection framework**: Develop and test a principled method for selecting subgroups when full invariance is too expensive, evaluating different selection strategies (random sampling, information-theoretic criteria, domain knowledge) on the 6D permutation-invariant task.