---
ver: rpa2
title: 'Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment
  of Large Vision-Language Model'
arxiv_id: '2406.15279'
source_url: https://arxiv.org/abs/2406.15279
tags:
- safety
- safe
- unsafe
- image
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new safety alignment challenge called Safe
  Inputs but Unsafe Output (SIUO) in large vision-language models (LVLMs), where safe
  images and texts combined can lead to unsafe outputs. To address this, the authors
  create the SIUO benchmark covering 9 safety domains with 167 human-crafted and 102
  AI-assisted test cases.
---

# Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Model

## Quick Facts
- **arXiv ID:** 2406.15279
- **Source URL:** https://arxiv.org/abs/2406.15279
- **Reference count:** 40
- **Key outcome:** Even advanced LVLMs achieve only 53.29% safe response rate on SIUO benchmark, with 13 out of 15 models performing below 50%.

## Executive Summary
This paper identifies a critical safety vulnerability in Large Vision-Language Models called Safe Inputs but Unsafe Output (SIUO), where benign images combined with benign text can produce unsafe outputs due to their combined semantic meaning. The authors create the first comprehensive SIUO benchmark with 269 test cases across 9 safety domains, revealing that current LVLMs lack robust cross-modal safety alignment. Through extensive evaluation of 15 LVLMs including GPT-4V, the study demonstrates that even state-of-the-art models struggle with this challenge, achieving only 53.29% safe response rate overall. The findings highlight the need for improved cross-modal reasoning and safety alignment mechanisms in vision-language models.

## Method Summary
The authors constructed the SIUO benchmark using both human-crafted (167 cases) and AI-assisted (102 cases) test cases across 9 safety domains. They evaluated 15 LVLMs in zero-shot settings using both human evaluation (with 90% pairwise agreement) and GPT-4V evaluation. The benchmark focuses on cases where individually safe images and texts combine to create unsafe semantic implications. Responses were assessed for safety (whether harmful content was generated) and effectiveness (whether the response addressed the user's intent appropriately).

## Key Results
- 15 LVLMs evaluated, including GPT-4V, achieving only 53.29% average safe response rate
- 13 out of 15 models scored below 50% on the SIUO benchmark
- Safe Rate ranged from 22.68% to 71.75% across different models
- Privacy, religion, and dangerous behavior domains showed the lowest safety scores
- Human evaluation achieved 90% pairwise consistency for safety assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SIUO vulnerability arises because LVLMs fail to integrate cross-modal semantic understanding, overlooking combined meanings in image-text pairs.
- **Mechanism:** When safe image and text combine to convey harmful intent (e.g., rooftop image + "I want to go to a new world"), models treat them separately and miss the dangerous implication.
- **Core assumption:** Current LVLMs prioritize unimodal content analysis over cross-modal semantic synthesis.
- **Evidence anchors:**
  - [abstract] "This paper identifies a new safety alignment challenge called Safe Inputs but Unsafe Output (SIUO) in large vision-language models (LVLMs), where safe images and texts combined can lead to unsafe outputs."
  - [section 3.1] "Integration Models must integrate insights from various modalities to form a unified understanding."
- **Break condition:** Enhanced cross-modal reasoning modules that fuse image and text semantics before safety evaluation.

### Mechanism 2
- **Claim:** SIUO exploits gaps in LVLMs' safety alignment training, which focuses on individual modalities rather than joint semantic space.
- **Mechanism:** Safety training filters harmful text and images separately, but SIUO cases involve benign content that becomes unsafe when combined. Training corpus lacks examples of such cross-modal threats.
- **Core assumption:** Safety alignment pipeline is modality-specific and doesn't account for semantic combinations across modalities.
- **Evidence anchors:**
  - [abstract] "Previous studies primarily focus on single-modality threats, which may not suffice given the integrated and complex nature of cross-modality interactions."
- **Break condition:** Safety training datasets expanded to include cross-modal harmful combinations.

### Mechanism 3
- **Claim:** SIUO vulnerability is amplified by models' reasoning limitations, failing to infer user intent from combined context.
- **Mechanism:** Models may integrate image and text but fail to reason about true user intent or potential consequences, treating requests at face value without deeper contextual analysis.
- **Core assumption:** Reasoning about user intent and potential harm requires sophisticated inference capabilities that current LVLMs lack.
- **Evidence anchors:**
  - [abstract] "This situation requires LVLMs to understand the semantics of the image and text, as well as the combined semantics of both, in order to provide safe responses."
- **Break condition:** Reasoning modules explicitly trained to detect implied user intent and potential harm in combined contexts.

## Foundational Learning

- **Concept:** Cross-modal semantic integration
  - **Why needed here:** SIUO vulnerabilities arise because LVLMs fail to integrate image and text semantics into unified understanding, missing dangerous implications that only emerge when modalities are combined.
  - **Quick check question:** If an image shows a rooftop edge and text says "I want to go to a new world," what is the combined semantic implication that a safety-aware model should detect?

- **Concept:** Safety alignment training scope
  - **Why needed here:** Current safety alignment focuses on individual modalities, leaving gaps when benign content from different modalities combines to create harmful meaning.
  - **Quick check question:** Why would a model trained only on harmful images and harmful texts separately fail to detect a case where safe image + safe text = unsafe output?

- **Concept:** User intent inference in multimodal contexts
  - **Why needed here:** SIUO requires the model to reason beyond surface-level content to understand user's true intent and potential consequences, which current LVLMs struggle with.
  - **Quick check question:** How should a model distinguish between a literal request and a metaphorical one that implies dangerous action?

## Architecture Onboarding

- **Component map:** Input → Vision Encoder → Text Encoder → Cross-Modal Fusion Module → Safety Alignment Layer → Output Generation
- **Critical path:** The Cross-Modal Fusion Module and Safety Alignment Layer are critical for detecting SIUO vulnerabilities, as they determine whether the model can integrate multimodal semantics and apply safety filters appropriately.
- **Design tradeoffs:**
  - Modality-specific safety filters are efficient but miss cross-modal threats
  - Cross-modal reasoning modules are computationally expensive but necessary for SIUO detection
  - Fine-tuning on SIUO-specific data improves detection but may reduce performance on other tasks
- **Failure signatures:**
  - Safe Rate below 50% on SIUO benchmark
  - Safe image + safe text combinations generating unsafe outputs
  - Lack of refusal or safety warnings for combined harmful contexts
- **First 3 experiments:**
  1. Evaluate SIUO benchmark performance on current model to establish baseline Safe Rate
  2. Add cross-modal fusion layer and test improvement in Safe Rate
  3. Fine-tune on expanded safety dataset including cross-modal harmful combinations and measure SIUO vulnerability reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop automated methods to generate challenging Safe Inputs but Unsafe Output (SIUO) test cases that match the diversity and difficulty of human-curated examples?
- **Basis in paper:** [explicit] The paper notes that while AI-assisted data generation was explored, "this approach faces challenges in data efficiency" and "lacks sufficient difficulty, leading to higher safety scores." The authors hope "future research will investigate more efficient automated data construction methods."
- **Why unresolved:** Current AI-assisted methods struggle to create sufficiently challenging cases that expose model weaknesses. Human curation remains necessary but doesn't scale well.
- **What evidence would resolve it:** Development and validation of automated methods that can generate test cases with comparable diversity and difficulty to human-curated SIUO examples, demonstrated through rigorous benchmarking against existing models.

### Open Question 2
- **Question:** What architectural modifications to vision-language models could improve their cross-modal safety alignment capabilities, particularly for detecting subtle unsafe intentions in seemingly safe inputs?
- **Basis in paper:** [explicit] The paper finds that "even advanced models, such as GPT-4V, struggle to consistently generate safe responses" and that models need better "Integration, Knowledge, and Reasoning" capabilities for cross-modal safety.
- **Why unresolved:** Current models fail to properly integrate visual and textual information to detect unsafe semantic combinations. The paper shows no existing model performs well on this task.
- **What evidence would resolve it:** Demonstration of model architectures that achieve significantly higher safety scores on the SIUO benchmark, particularly in domains where current models struggle most (privacy, religion, dangerous behavior).

### Open Question 3
- **Question:** How can we create scalable evaluation frameworks that maintain the reliability of human evaluation while addressing the cost and scalability issues identified in the paper?
- **Basis in paper:** [explicit] The paper states that "human evaluation suffers from scalability issues, and GPT-4V evaluation carries the risk of unreliability," highlighting the need for better evaluation methods.
- **Why unresolved:** The paper relies on expensive human evaluation and explores GPT-4V as an alternative, finding "consistency accuracy in terms of safety and effectiveness is above 80%" but not sufficient for full reliability.
- **What evidence would resolve it:** Development of evaluation frameworks that achieve human-level reliability at significantly lower cost, validated through comparison studies showing consistency rates above 95% with human judgments while being more scalable.

## Limitations

- Small test set size (269 cases across 9 safety domains) may limit generalizability of findings
- Reliance on both human and GPT-4V evaluation methods introduces potential subjectivity and systematic biases
- Zero-shot evaluation doesn't explore whether fine-tuning or prompt engineering could mitigate SIUO vulnerabilities

## Confidence

- **High confidence**: The identification of SIUO as distinct safety challenge and empirical finding that current LVLMs struggle with cross-modal safety alignment (13/15 models below 50% safe rate)
- **Medium confidence**: Generalizability of findings across different safety domains due to varying case counts per domain
- **Medium confidence**: Effectiveness of human vs. GPT-4V evaluation methods, as both approaches have inherent limitations in assessing safety

## Next Checks

1. **Dataset Expansion**: Replicate the study with a larger, more diverse test set (target 500+ cases) to verify whether the 53.29% average safe rate holds across broader safety domains and cultural contexts.

2. **Cross-Modal Fusion Implementation**: Implement and test explicit cross-modal reasoning modules that fuse image and text semantics before safety evaluation, then measure improvement in safe response rates on the SIUO benchmark.

3. **Safety Fine-tuning Impact**: Conduct controlled experiments fine-tuning models on cross-modal harmful combinations (expanding training corpus to include SIUO cases) and measure the reduction in SIUO vulnerabilities while monitoring performance degradation on other tasks.