---
ver: rpa2
title: Preference Poisoning Attacks on Reward Model Learning
arxiv_id: '2402.01920'
source_url: https://arxiv.org/abs/2402.01920
tags:
- attacks
- attack
- learning
- poisoning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper studies how an attacker can flip a small subset of pairwise
  preference labels to manipulate the learned reward model. Two attack classes are
  proposed: gradient-based (with relaxation, implicit gradient, and random initialization
  ensemble) and rank-by-distance (greedy, using Euclidean, reward, or embedding distances).'
---

# Preference Poisoning Attacks on Reward Model Learning

## Quick Facts
- **arXiv ID:** 2402.01920
- **Source URL:** https://arxiv.org/abs/2402.01920
- **Reference count:** 40
- **Primary result:** Attackers can achieve 100% success rate in manipulating reward models by flipping only 0.3% of preference labels

## Executive Summary
This paper introduces preference poisoning attacks that manipulate reward models learned from pairwise preferences. The authors demonstrate that flipping a small subset of preference labels can significantly bias the learned reward function toward target outcomes. Two attack classes are proposed: gradient-based methods that optimize label flips through implicit differentiation, and rank-by-distance heuristics that select flips based on proximity to target outcomes. Across MuJoCo, Atari, recommendation, and LLM safety alignment tasks, the attacks achieve high success rates with minimal data poisoning, while existing defenses show limited efficacy.

## Method Summary
The paper studies how an attacker can flip a small subset of pairwise preference labels to manipulate the learned reward model. Two attack classes are proposed: gradient-based (with relaxation, implicit gradient, and random initialization ensemble) and rank-by-distance (greedy, using Euclidean, reward, or embedding distances). The attacks target the Bradley-Terry model learned via maximum likelihood estimation. Gradient-based attacks use implicit differentiation to compute gradients through the MLE solution, while RBD methods select label flips based on distance heuristics to target outcomes.

## Key Results
- The best attacks achieve 100% success rate with only 0.3% of data poisoned in extreme cases
- Simpler rank-by-distance approaches often outperform gradient-based methods, especially in high-dimensional settings
- State-of-the-art defenses against other poisoning attacks show limited efficacy on preference poisoning
- Attack performance varies significantly by domain, with LLM safety alignment being particularly vulnerable

## Why This Works (Mechanism)

### Mechanism 1
Flipping a small subset of preference labels can manipulate the learned reward model to favor target outcomes with high success rate. The Bradley-Terry model learns reward parameters by maximizing the likelihood of pairwise preference labels. By flipping labels between high-reward and low-reward outcomes, the attacker changes the gradient direction during learning, causing the model to overestimate the value of target outcomes. If the dataset is sufficiently large and diverse, or if the reward model architecture has built-in robustness to label noise, the impact of small label flips may be negligible.

### Mechanism 2
Rank-by-distance (RBD) heuristics can outperform gradient-based methods in high-dimensional settings due to scalability and simplicity. RBD methods rank data points by distance (Euclidean, reward difference, or embedding distance) to target outcomes and flip the closest pairs. This avoids expensive gradient computations and Hessian inversions required in implicit gradient methods, making them faster and more practical for high-dimensional inputs like images. If the reward model uses complex non-linear embeddings where distance does not correlate with reward difference, RBD may fail to find effective label flips.

### Mechanism 3
Defenses designed for traditional classification/regression poisoning attacks have limited efficacy against preference poisoning due to structural differences in the learning problem. Preference poisoning involves flipping binary preference labels rather than continuous or multi-class labels, and the threat model focuses on promoting/demoting specific outcomes rather than maximizing general error. Common defenses like spectral outlier detection or loss-based filtering are not tailored to this structure and fail to identify poisoned preference pairs effectively. If a defense is specifically designed to handle pairwise preference data and promotion/demotion goals, it may be more effective.

## Foundational Learning

- **Concept: Bradley-Terry model for pairwise preference learning**
  - Why needed here: This is the foundational model the paper attacks; understanding its likelihood structure is essential to see how label flips influence learning
  - Quick check question: In the Bradley-Terry model, what is the probability that outcome x is preferred over y given reward functions R(x) and R(y)?

- **Concept: Maximum likelihood estimation (MLE) and implicit function theorem**
  - Why needed here: The gradient-based attack relies on computing gradients through the MLE solution using the implicit function theorem; without this, the attack cannot optimize label flips
  - Quick check question: Why does the paper need to compute the implicit derivative dθ/dδ when designing gradient-based poisoning attacks?

- **Concept: Vapnik-Chervonenkis (VC) dimension and sample complexity**
  - Why needed here: The paper proves polynomial sample complexity for the finite-sample approximation of the attack objective; this ensures the attack is theoretically grounded
  - Quick check question: What does the paper prove about the relationship between the number of samples N and the quality of the approximate attack objective?

## Architecture Onboarding

- **Component map:**
  Data ingestion -> Bradley-Terry reward model training -> Poisoning attack (gradient-based or RBD) -> Retraining on poisoned data -> Success rate and stealth evaluation

- **Critical path:**
  1. Load dataset and preprocess into pairwise comparisons
  2. Train clean reward model (baseline)
  3. Apply poisoning attack (gradient or RBD) with budget B
  4. Retrain reward model on poisoned data
  5. Evaluate success rate and stealth (accuracy drop)

- **Design tradeoffs:**
  - Gradient-based: More principled but computationally expensive (Hessian inversion), requires model architecture knowledge, may need dimensionality reduction for scalability
  - RBD: Simpler, faster, black-box friendly, but relies on heuristic distance measures that may not always correlate with reward
  - Defense integration: Spectral methods assume Gaussian-like data; may not detect preference flips effectively

- **Failure signatures:**
  - Attack fails to improve success rate: Check if budget is too small or distance metric in RBD is poorly chosen
  - Gradient-based attack is too slow: Likely due to high dimensionality; try PCA or embedding-based reduction
  - Defenses appear effective: Verify if defense is actually detecting poisoned pairs or just overfitting to clean data

- **First 3 experiments:**
  1. Run promotion attack on MuJoCo Reacher with 1% budget using RBD-Norm; expect ~50% success rate
  2. Run gradient-based attack on MuJoCo Hopper with 5% budget; expect higher success than RBD
  3. Apply ALIBI defense to LLM safety alignment with 0.3% poisoning; expect minimal impact on attack success

## Open Questions the Paper Calls Out
None

## Limitations
- Gradient-based attacks require access to model architecture and parameters, limiting real-world applicability
- RBD methods' effectiveness depends heavily on distance metric choice, which may not always correlate with reward differences
- The paper lacks extensive exploration of defenses specifically designed for preference poisoning attacks
- Computational cost of implicit gradient methods may limit scalability to very large datasets

## Confidence
- High confidence: The Bradley-Terry model framework and MLE-based learning are well-established
- Medium confidence: The comparative performance of gradient-based vs. RBD methods across domains
- Medium confidence: The limited efficacy of existing defenses against preference poisoning

## Next Checks
1. Test whether dimensionality reduction techniques (e.g., PCA, autoencoders) improve gradient-based attack scalability in high-dimensional domains like Atari
2. Evaluate whether custom defenses designed specifically for preference data (e.g., pairwise anomaly detection) outperform general poisoning defenses
3. Verify the sensitivity of RBD methods to different distance metrics by systematically comparing Norm, Reward, and Embedding variants across all domains