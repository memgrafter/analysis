---
ver: rpa2
title: 'SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition'
arxiv_id: '2401.18045'
source_url: https://arxiv.org/abs/2401.18045
tags:
- speech
- tasks
- language
- speechcomposer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpeechComposer, a unified decoder-only speech
  language model that can perform multiple speech tasks using a shared set of prompt
  tokens. By composing primary tasks such as speech synthesis, speech recognition,
  and language modeling, SpeechComposer achieves comparable or better performance
  than expert models designed for individual tasks.
---

# SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition

## Quick Facts
- arXiv ID: 2401.18045
- Source URL: https://arxiv.org/abs/2401.18045
- Reference count: 13
- Primary result: Unified decoder-only speech language model performing multiple speech tasks through prompt composition

## Executive Summary
SpeechComposer introduces a unified decoder-only speech language model that performs multiple speech tasks using a shared set of prompt tokens. By composing primary tasks such as speech synthesis, speech recognition, and language modeling, the model achieves comparable or better performance than expert models designed for individual tasks. The approach demonstrates effectiveness in primary tasks like speech synthesis and speech recognition, as well as composite tasks like voice conversion and speech enhancement, with the ability to generalize to unseen tasks in a zero-shot manner.

## Method Summary
SpeechComposer is a decoder-only transformer architecture that unifies multiple speech tasks through prompt composition. The model uses five primary prompt tokens (〈start-text〉, 〈start-speech〉, 〈generate-text〉, 〈generate-speech〉, 〈enroll-speech〉) to represent all tasks, enabling knowledge sharing during training. Primary tasks (SpeechLM, TextLM, TTS, ASR) are trained jointly, and composite tasks (VC, SE) are formulated as compositions of these primary tasks. The model employs a randomized sampling strategy for loss computation to stabilize training on composite tasks, and uses discrete speech tokens obtained from HuBERT+k-means clustering. Inference is performed using beam search decoding.

## Key Results
- SpeechComposer achieves comparable or better performance than expert models on primary tasks (ASR, TTS, SpeechLM, TextLM)
- The model successfully performs composite tasks like voice conversion and speech enhancement through prompt composition
- SpeechComposer demonstrates zero-shot generalization capability to unseen tasks

## Why This Works (Mechanism)

### Mechanism 1
SpeechComposer unifies multiple speech tasks by composing a fixed set of prompt tokens rather than using task-specific tokens. By representing all tasks with the same five primary prompt tokens, the model can perform both primary and composite tasks without additional architectural changes, enabling knowledge sharing between tasks during training. This design assumes that intrinsic connections between different speech tasks can be effectively captured through a shared token vocabulary and prompt structure.

### Mechanism 2
Composite tasks like voice conversion and speech enhancement are formulated as compositions of primary tasks through prompt sequences. Voice conversion is defined as ASR+TTS composition where enrollment speech guides target speaker conversion, while speech enhancement uses clean speech as enrollment to generate enhanced output through the same ASR+TTS pathway. This compositional approach enables zero-shot transfer to new tasks by assuming that complex speech tasks can be decomposed into sequences of simpler, well-defined primary tasks that share compatible data representations.

### Mechanism 3
Randomized sampling strategy for loss computation during training stabilizes learning when composing multiple tasks. Instead of computing losses for all predicted components in composite tasks, the model randomly selects which task losses to compute at each training step using hyper-parameters q1, q2, ..., qN, qglobal. This prevents training instability from overwhelming loss signals by assuming that balanced exposure to different task losses during training is more important than always computing all losses simultaneously.

## Foundational Learning

- Discrete speech tokenization using self-supervised learning models (HuBERT)
  - Why needed here: SpeechComposer requires discrete speech tokens to unify speech and text processing within the same autoregressive language model framework
  - Quick check question: Can you explain how k-means clustering on HuBERT embeddings creates a discrete vocabulary suitable for language modeling?

- Task composition and decomposition in multi-task learning
  - Why needed here: Understanding how complex tasks can be represented as compositions of simpler tasks is fundamental to SpeechComposer's design
  - Quick check question: How would you formulate voice conversion as a sequence of ASR and TTS operations using the prompt token framework?

- Autoregressive language modeling and beam search inference
  - Why needed here: SpeechComposer generates speech tokens sequentially, requiring understanding of conditional probability modeling and decoding strategies
  - Quick check question: What are the trade-offs between greedy decoding and beam search when generating speech tokens for composite tasks?

## Architecture Onboarding

- Component map: Input → Tokenizer → Embedding → Transformer layers → Prompt-conditioned generation → Decoder → Output
- Critical path: Speech/text data → Discrete tokenization → Embedding layer → 12-layer transformer decoder → Prompt-conditioned generation → HiFiGAN vocoder → Continuous speech output
- Design tradeoffs:
  - Fixed prompt tokens vs task-specific tokens: Shared prompts enable knowledge transfer but may limit task-specific optimization
  - Discrete vs continuous speech representation: Discrete tokens enable language modeling but may lose fine-grained acoustic information
  - Single model vs cascaded models: Unified model simplifies deployment but may underperform specialized models on individual tasks
- Failure signatures:
  - High CER/WER in ASR tasks: May indicate tokenization or prompt encoding issues
  - Poor speaker similarity: Could suggest enrollment speech processing problems
  - Training instability: Might result from improper loss sampling or task composition
- First 3 experiments:
  1. Test prompt token recognition: Feed known prompt sequences and verify correct task identification through output analysis
  2. Evaluate primary task isolation: Train on individual primary tasks (SpeechLM, TextLM, ASR, TTS) separately and measure baseline performance
  3. Test composite task formulation: Verify that composing ASR+TTS through prompts produces coherent intermediate text outputs before speech generation

## Open Questions the Paper Calls Out

### Open Question 1
How does the randomized sampling strategy impact training stability and performance across different model sizes and task compositions? The paper describes the strategy's effectiveness in improving intelligibility and stability, but doesn't explore its impact across a wider range of model sizes or more diverse task compositions. Systematic experiments varying model sizes and task compositions with and without the randomized sampling strategy would clarify its generalizability.

### Open Question 2
What is the upper limit of task complexity and diversity that SpeechComposer can handle effectively, and how does performance degrade beyond this limit? The paper demonstrates flexibility in handling primary and composite tasks but doesn't investigate how the model performs when tasked with more complex or diverse speech tasks. Experiments progressively adding more diverse and complex tasks while monitoring performance metrics would reveal the model's limits.

### Open Question 3
How does the choice of discrete speech tokenization method impact SpeechComposer's performance across different tasks, and is there an optimal tokenization strategy? The paper uses k-means clustering on HuBERT embeddings but doesn't explore alternative tokenization methods or their impact on performance. Comparative experiments using different tokenization methods while measuring task performance would identify optimal strategies.

## Limitations
- Discrete tokenization may lose critical prosodic information that specialized models preserve through continuous representations
- Performance gap between SpeechComposer and expert models still exists for certain tasks like speech enhancement
- Zero-shot generalization capability remains largely unproven on out-of-domain data or tasks requiring fine-grained acoustic control

## Confidence

**High Confidence**: The core mechanism of using shared prompt tokens for task identification is well-validated through ablation studies showing performance degradation when using task-specific tokens.

**Medium Confidence**: The zero-shot generalization claims are supported by results on seen but untrained tasks, but the extent of generalization to truly novel tasks remains uncertain.

**Low Confidence**: The randomized sampling strategy for loss computation lacks comprehensive ablation studies across different task compositions and data regimes.

## Next Checks

1. **Zero-shot generalization stress test**: Evaluate SpeechComposer on truly unseen speech tasks (e.g., singing voice synthesis, emotional speech conversion) without any task-specific fine-tuning to verify the claimed generalization capability.

2. **Discrete tokenization quality assessment**: Conduct detailed ablation studies comparing continuous vs discrete speech representations on tasks requiring fine-grained acoustic control to quantify information loss from tokenization.

3. **Randomized sampling strategy optimization**: Systematically vary the sampling parameters (q values) across different task compositions and data regimes to establish best practices for training stability and performance.