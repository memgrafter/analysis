---
ver: rpa2
title: 'Conv-CoA: Improving Open-domain Question Answering in Large Language Models
  via Conversational Chain-of-Action'
arxiv_id: '2405.17822'
source_url: https://arxiv.org/abs/2405.17822
tags:
- retrieval
- question
- conversational
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Conversational Chain-of-Action (Conv-CoA)
  framework to enhance open-domain conversational question answering (OCQA). The framework
  addresses challenges of unfaithful hallucinations, weak reasoning, and unsatisfying
  retrieval in conversational scenarios.
---

# Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action

## Quick Facts
- **arXiv ID**: 2405.17822
- **Source URL**: https://arxiv.org/abs/2405.17822
- **Reference count**: 40
- **Primary result**: Conv-CoA achieves 71.2 GPT-EM on QReCC and 83.7 on TopiOCQA, outperforming 23 state-of-the-art methods

## Executive Summary
This paper introduces Conv-CoA, a Conversational Chain-of-Action framework that addresses key challenges in open-domain conversational question answering: hallucinations, weak reasoning, and inefficient retrieval. The framework uses a dynamic reasoning-retrieval mechanism that decomposes questions into sub-questions, employs a novel Hopfield-based retriever, and verifies answers using a conversational-multi-reference faith score. Experiments show Conv-CoA significantly outperforms existing methods on two public benchmarks while demonstrating improved efficiency.

## Method Summary
Conv-CoA integrates retrieval-augmented generation with prompting methods to enhance open-domain conversational question answering. The framework decomposes each conversational question into sub-questions using Action Chain generation, retrieves relevant information via a resource-efficient Hopfield retriever, and verifies answers using Conv-MRFS. A Contextual Knowledge Set (CKS) maintains conversation history and retrieved information, enabling the system to avoid redundant information retrieval and detect hallucinations by comparing generated answers against stored knowledge segments.

## Key Results
- Achieves 71.2 GPT-EM score on QReCC benchmark, significantly surpassing previous methods
- Achieves 83.7 GPT-EM score on TopiOCQA benchmark
- Demonstrates faster retrieval times compared to baselines while maintaining high accuracy
- Outperforms 23 state-of-the-art methods across both evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conv-CoA reduces hallucinations by dynamically verifying answers against retrieved knowledge using the Conv-MRFS score
- Mechanism: The framework maintains a Contextual Knowledge Set (CKS) that stores conversation history and retrieved information. For each generated answer, Conv-MRFS calculates a faith score by comparing the answer against all segments in the CKS. If the score falls below a threshold, the answer is revised
- Core assumption: The CKS contains sufficiently relevant information to detect hallucinations, and the faith score calculation effectively captures semantic alignment between answers and retrieved knowledge
- Evidence anchors:
  - [abstract]: "we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations"
  - [section]: "To verify conflicts between generated answers and retrieved information in conversations, we introduce the Conversational-Multi-Reference Faith Score (Conv-MRFS)"
  - [corpus]: Weak - no direct corpus evidence about hallucination reduction effectiveness
- Break condition: The CKS becomes outdated or incomplete, the faith score threshold is poorly calibrated, or the LLM generates answers that are semantically similar to retrieved knowledge but factually incorrect

### Mechanism 2
- Claim: The Hopfield-based retriever improves retrieval efficiency and accuracy compared to traditional dense retrievers
- Mechanism: The framework uses a resource-efficiency Hopfield Retriever that leverages Modern Hopfield networks for fast convergence and exponential memory capacity. The retriever uses SparseHopfield layers that implement sparsemax attention instead of softmax, enabling faster computation while maintaining accuracy
- Core assumption: Modern Hopfield networks can effectively retrieve relevant information from large knowledge bases, and the sparsemax attention provides sufficient expressiveness for the retrieval task
- Evidence anchors:
  - [abstract]: "we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions"
  - [section]: "we propose a resource- efficiency Hopfield memory retrieval model designed to extract top- k relevant information and solution paths from existing conversation turns and the knowledge base"
  - [corpus]: Weak - while the paper claims improved efficiency, the corpus evidence shows no direct citations or comparisons with other Hopfield-based methods
- Break condition: The knowledge base is too large for the Hopfield model to handle effectively, the sparsemax attention cannot capture complex semantic relationships, or the retrieval dynamics fail to converge to relevant patterns

### Mechanism 3
- Claim: The dynamic reasoning-retrieval mechanism reduces information overlap and improves latency by decomposing questions into sub-questions
- Mechanism: Conv-CoA uses systematic prompting to decompose each question into sub-questions that target different aspects of the main query. The framework updates the CKS at each turn and uses it to generate new sub-questions, avoiding redundant information retrieval
- Core assumption: Question decomposition effectively reduces overlap in retrieved information, and the CKS provides sufficient context for generating relevant sub-questions in subsequent turns
- Evidence anchors:
  - [abstract]: "Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain"
  - [section]: "In the following turns, we iterative combine the CKS and the current turn's question to repeat the AC generation"
  - [corpus]: Weak - no direct corpus evidence about latency reduction or information overlap minimization
- Break condition: The question decomposition creates too many sub-questions, the CKS becomes too large to manage effectively, or the sub-questions generated in later turns fail to capture new information

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Conv-CoA builds on CoT by decomposing complex questions into simpler sub-questions that can be answered sequentially
  - Quick check question: How does breaking a complex question into sub-questions help improve reasoning accuracy in conversational QA?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Conv-CoA integrates RAG principles with prompting methods to combine the strengths of both approaches
  - Quick check question: What are the main limitations of traditional RAG methods that Conv-CoA aims to address?

- Concept: Modern Hopfield Networks
  - Why needed here: The framework uses Hopfield-based retrieval to achieve faster convergence and better memory capacity than traditional dense retrievers
  - Quick check question: How do Modern Hopfield Networks differ from classical Hopfield Networks in terms of memory capacity and retrieval speed?

## Architecture Onboarding

- Component map: Input Question → Action Chain generation → Sub-question retrieval → Answer verification → CKS update → Final answer
- Critical path: Question → Action Chain generation → Sub-question retrieval → Answer verification → CKS update → Final answer
- Design tradeoffs: The framework trades increased complexity (multiple components and interactions) for improved accuracy and reduced hallucinations
- Failure signatures: High latency due to excessive sub-question generation, inaccurate answers due to poor retrieval, or failure to detect hallucinations due to insufficient CKS information
- First 3 experiments:
  1. Test Action Chain generation with simple questions to verify sub-question decomposition works correctly
  2. Evaluate Hopfield Retriever performance on a small knowledge base to ensure retrieval accuracy
  3. Measure Conv-MRFS effectiveness by generating answers with known hallucinations and checking detection rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Conv-CoA framework handle cases where the initial answer has high confidence but the retrieved information contradicts it? What is the threshold for switching from the initial answer to the retrieved data?
- Basis in paper: [explicit] The paper mentions that specific actions verify if the initial answer needs to change based on retrieved data, and if the confidence of the initial answer is lower than that of retrieved data, the action requests to change.
- Why unresolved: The paper does not specify the exact mechanism or threshold for switching from the initial answer to the retrieved data when there is a contradiction.
- What evidence would resolve it: Experimental results comparing the framework's performance with different confidence thresholds or a detailed description of the decision-making process for switching answers.

### Open Question 2
- Question: How does the Hopfield-based retriever compare to other state-of-the-art retrievers in terms of retrieval accuracy and efficiency across different types of queries (e.g., factual, opinion-based, or complex reasoning)?
- Basis in paper: [explicit] The paper mentions that the Hopfield-based retriever is designed to enhance the efficiency and accuracy of conversational information retrieval and compares it to other retrievers in terms of retrieval performance.
- Why unresolved: The paper does not provide a detailed comparison of the Hopfield-based retriever's performance across different types of queries.
- What evidence would resolve it: A comprehensive evaluation of the Hopfield-based retriever's performance on various query types, including factual, opinion-based, and complex reasoning queries.

### Open Question 3
- Question: How does the Conv-CoA framework handle multi-turn conversations where the user's intent may shift or become more refined over time? What mechanisms are in place to adapt to these changes?
- Basis in paper: [inferred] The paper mentions that the framework updates the Contextual Knowledge Set (CKS) at each conversation turn and uses a different prompt template to generate sub-questions for missing content, which helps reduce information overlap.
- Why unresolved: The paper does not provide specific details on how the framework adapts to changes in user intent over multiple turns.
- What evidence would resolve it: Experimental results demonstrating the framework's ability to handle multi-turn conversations with shifting user intent, or a detailed description of the mechanisms in place to adapt to these changes.

### Open Question 4
- Question: How does the Conv-MRFS score handle cases where the generated answer is partially correct but contains some incorrect information? What is the impact of such cases on the overall faithfulness score?
- Basis in paper: [explicit] The paper mentions that the Conv-MRFS score evaluates the consistency of generated answers with the conversation history based on precision, recall, and average word length.
- Why unresolved: The paper does not provide specific details on how the Conv-MRFS score handles partially correct answers or the impact of such cases on the overall faithfulness score.
- What evidence would resolve it: A detailed explanation of the Conv-MRFS score's handling of partially correct answers, including examples and the impact on the overall faithfulness score.

## Limitations
- Evaluation relies heavily on GPT-4 as judge, introducing potential bias and reproducibility issues
- Framework complexity and maintenance overhead of CKS across conversation turns is not fully characterized
- Performance may vary significantly with different LLM implementations or prompt engineering approaches

## Confidence
- **High Confidence**: Question decomposition mechanism is well-supported by experimental results showing improved reasoning performance
- **Medium Confidence**: Hallucination reduction through Conv-MRFS is theoretically sound but lacks direct empirical validation
- **Low Confidence**: Efficiency claims regarding Hopfield-based retriever are weakly supported with limited timing comparisons

## Next Checks
1. Conduct ablation study removing Conv-MRFS component to quantify hallucination reduction contribution to accuracy improvements
2. Perform detailed timing analysis comparing Hopfield retriever against traditional dense retrievers across varying knowledge base sizes
3. Apply Conv-CoA to conversational QA datasets outside web domain (e.g., biomedical or legal) to assess generalization beyond tested benchmarks