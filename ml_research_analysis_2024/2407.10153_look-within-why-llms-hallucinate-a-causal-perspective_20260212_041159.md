---
ver: rpa2
title: 'Look Within, Why LLMs Hallucinate: A Causal Perspective'
arxiv_id: '2407.10153'
source_url: https://arxiv.org/abs/2407.10153
tags:
- llms
- arxiv
- layers
- hallucinations
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sources of hallucinations in large
  language models (LLMs) from a causal perspective, focusing on the role of self-attention
  layers. The authors propose a method to intervene in the self-attention layers of
  LLMs, specifically by disabling different self-attention layers, and evaluate the
  intervened models on hallucination assessment benchmarks.
---

# Look Within, Why LLMs Hallucinate: A Causal Perspective

## Quick Facts
- arXiv ID: 2407.10153
- Source URL: https://arxiv.org/abs/2407.10153
- Reference count: 37
- Primary result: Disabling specific self-attention layers in front or tail of LLMs can alleviate hallucination issues

## Executive Summary
This paper investigates the sources of hallucinations in large language models from a causal perspective, focusing on the role of self-attention layers. The authors propose a novel intervention method that disables specific self-attention layers and evaluate its effectiveness on hallucination assessment benchmarks. Their findings reveal that disabling certain front or tail self-attention layers can reduce hallucinations, while disabling middle layers may amplify them, suggesting that different layers encode different types of content.

## Method Summary
The authors propose a causal intervention method that disables self-attention layers in LLMs by setting their output tensors to zero during the forward pass. They test this approach on popular open-source LLMs (LLaMA 2-7B-Chat, Gemma-2B-instruct, Gemma-7B-instruct, Mistral-7B-v0.1) using hallucination detection benchmarks (TruthfulQA and HaluEval). The evaluation uses automated assessment with GPT-3.5-turbo as the evaluator LLM, comparing accuracy scores between original and intervened models.

## Key Results
- Disabling specific self-attention layers in the front or tail of LLMs can alleviate hallucination issues
- Disabling certain middle self-attention layers may amplify the degree of hallucination
- The intervention method maintains model architecture while reducing hallucinatory outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disabling specific self-attention layers reduces hallucination in LLMs
- Mechanism: Self-attention layers encode hallucinatory content, and disabling these layers removes or reduces hallucinatory representations
- Core assumption: Hallucination is partly encoded in specific self-attention layers, and removing these layers' outputs will reduce hallucination without severely degrading overall performance
- Evidence anchors: Abstract finding that disabling specific front/tail layers alleviates hallucination issues; results showing hallucinations are alleviated when disabling specific layers
- Break condition: If disabling layers causes catastrophic performance degradation or hallucinations shift to other layers without reduction

### Mechanism 2
- Claim: Different self-attention layers represent different types of content, with front/tail layers more prone to hallucinations and middle layers containing factual knowledge
- Mechanism: The positional distribution of self-attention layers in transformer architecture correlates with their content representation
- Core assumption: The architecture creates a natural distribution of content types across layers
- Evidence anchors: Finding that disabling middle layers amplifies hallucination; observation that middle layers may contain factual knowledge
- Break condition: If experiments show no significant difference between front, middle, and tail layer effects on hallucination

### Mechanism 3
- Claim: The front-door criterion from causal inference can be applied to LLM hallucination by treating hallucinated content as a mediating variable
- Mechanism: By modeling generation process causally and intervening on self-attention layers containing hallucinated content, hallucination can be reduced through causal mediation
- Core assumption: Hallucination can be modeled as a causal mediation problem
- Evidence anchors: Paper's motivation from causal formulation of LLM hallucination; modeling hallucinated content as mediating variable
- Break condition: If causal mediation doesn't reduce hallucination or causal model doesn't accurately represent generation process

## Foundational Learning

- Concept: Causal inference and structural causal models (SCM)
  - Why needed here: The paper uses causal inference (specifically front-door criterion) to understand and intervene in hallucination mechanisms
  - Quick check question: What are the three conditions required for the front-door criterion to identify a causal effect?

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: The intervention targets self-attention layers, requiring understanding of how they function in LLMs
  - Quick check question: How does the multi-head attention mechanism combine outputs from different attention heads?

- Concept: Hallucination detection benchmarks and evaluation methods
  - Why needed here: The paper evaluates interventions using TruthfulQA and HaluEval datasets, requiring understanding of how hallucination is measured
  - Quick check question: What's the difference between TruthfulQA and HaluEval in terms of their hallucination detection approach?

## Architecture Onboarding

- Component map: Input text → multiple transformer layers with self-attention → latent factors → hallucinated content (mediated) → output text

- Critical path: The self-attention layers in the middle of the transformer stack mediate between input and output, with specific layers more prone to encoding hallucinatory content

- Design tradeoffs: Disabling layers reduces hallucination but may also remove useful information; front/tail layers may be more replaceable than middle layers; method preserves model architecture but reduces capacity

- Failure signatures: Complete model failure when too many layers are disabled; increased hallucination when middle layers are disabled; no improvement when expected layers are disabled

- First 3 experiments:
  1. Disable individual layers starting from front, middle, and tail positions to identify which positions most affect hallucination
  2. Disable multiple layers simultaneously to test if effects are additive or interactive
  3. Compare hallucination reduction across different LLM architectures (LLaMA, Gemma, Mistral) to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different self-attention layers contribute to fact-conflicting hallucinations in large language models?
- Basis in paper: The paper investigates how self-attention layers affect LLM hallucinations through causal analysis, finding that disabling specific front or tail layers alleviates hallucinations while disabling middle layers may amplify them
- Why unresolved: The paper identifies differential effects of self-attention layers on hallucinations but doesn't provide a detailed mechanism explaining why front/tail layers are more prone to hallucinations compared to middle layers
- What evidence would resolve it: Detailed analysis of the attention patterns and information flow in different layers, showing how factual knowledge is processed differently in middle vs. front/tail layers

### Open Question 2
- Question: Can the proposed method of disabling self-attention layers be applied to smaller language models without compromising their performance?
- Basis in paper: The paper tests the method on LLMs with 2-7 billion parameters, but doesn't explore its effectiveness on smaller models or the threshold of model size where this approach becomes beneficial
- Why unresolved: The study focuses on large language models, leaving open the question of scalability and applicability to smaller models
- What evidence would resolve it: Experiments applying the method to various model sizes, including those below 1 billion parameters, to determine the minimum model size where disabling layers is beneficial

### Open Question 3
- Question: What are the long-term effects of disabling self-attention layers on LLM performance in diverse downstream tasks?
- Basis in paper: The paper evaluates the method on hallucination detection benchmarks but doesn't explore its impact on other tasks like reasoning, comprehension, or generation
- Why unresolved: The study focuses narrowly on hallucination reduction, not examining potential trade-offs or benefits in other capabilities
- What evidence would resolve it: Comprehensive evaluation of intervened models across multiple NLP benchmarks and real-world applications to assess overall impact on model performance

## Limitations
- Causal interpretation of self-attention layer interventions remains speculative without direct evidence of what specific patterns change when layers are disabled
- The method treats entire layers as black boxes without analyzing what specific representations or features change when hallucination is reduced
- Evaluation relies on automated judgment via GPT-3.5-turbo, introducing potential bias and circularity concerns when assessing LLM truthfulness

## Confidence
- **Mechanism 1 (Layer disabling reduces hallucination)**: Medium - supported by empirical results but mechanism is underspecified
- **Mechanism 2 (Layer position correlates with content type)**: Low - hypothesis not directly tested with content analysis
- **Mechanism 3 (Causal mediation framework)**: Medium-Low - causal interpretation extends beyond demonstrated effects

## Next Checks
1. **Layer content analysis**: Perform activation pattern analysis on disabled layers to identify what specific representations or features change when hallucination is reduced, testing the encoding hypothesis directly.

2. **Cross-architecture generalization**: Test the intervention method across diverse LLM architectures (RNNs, CNNs, different attention mechanisms) to determine if effects are specific to transformers or more general.

3. **Manual evaluation validation**: Conduct human evaluation on a subset of generated responses to validate the automated GPT-3.5-turbo judgments and quantify any systematic biases in the automated assessment method.