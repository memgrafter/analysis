---
ver: rpa2
title: On the Necessity of World Knowledge for Mitigating Missing Labels in Extreme
  Classification
arxiv_id: '2408.09585'
source_url: https://arxiv.org/abs/2408.09585
tags:
- knowledge
- missing
- skim
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of missing labels in extreme classification
  datasets, which are prone to systematic biases due to reliance on implicit user
  feedback like clicks. This leads to incomplete knowledge in the training data, hindering
  accurate relevance modeling.
---

# On the Necessity of World Knowledge for Mitigating Missing Labels in Extreme Classification

## Quick Facts
- arXiv ID: 2408.09585
- Source URL: https://arxiv.org/abs/2408.09585
- Authors: Jatin Prakash; Anirudh Buvanesh; Bishal Santra; Deepak Saini; Sachin Yadav; Jian Jiao; Yashoteja Prabhu; Amit Sharma; Manik Varma
- Reference count: 40
- Primary result: SKIM outperforms state-of-the-art methods by significant margins in offline and online evaluations, achieving over 10 absolute points improvement in Recall@100 on public datasets and showing a 1.23% increase in ad click-yield in live A/B tests.

## Executive Summary
The paper addresses the problem of missing labels in extreme classification datasets, which are prone to systematic biases due to reliance on implicit user feedback like clicks. This leads to incomplete knowledge in the training data, hindering accurate relevance modeling. Existing methods like propensity weighting and imputation are insufficient for recovering missing knowledge. The proposed solution, SKIM (Scalable Knowledge Infusion for Missing Labels), leverages a combination of a small language model and unstructured metadata to mitigate the missing label problem at scale. SKIM generates diverse synthetic queries using metadata and maps them to relevant training queries, augmenting the dataset with missing knowledge items.

## Method Summary
SKIM (Scalable Knowledge Infusion for Missing Labels) is a method that leverages world knowledge to mitigate missing labels in extreme classification datasets. It uses a small language model (SLM) finetuned on synthetic query generation and unstructured metadata associated with documents. The method consists of two main steps: (1) generating diverse synthetic queries using the SLM and metadata, and (2) mapping the synthetic queries to similar training queries using Approximate Nearest Neighbor Search (ANNS). The augmented dataset is then used to train extreme classification models like Ren√©e and DEXML. Extensive experiments on public and proprietary datasets demonstrate SKIM's effectiveness in improving Recall@K metrics and ad click-yield in live A/B tests.

## Key Results
- SKIM achieves over 10 absolute points improvement in Recall@100 on public datasets compared to state-of-the-art methods.
- SKIM shows a 1.23% increase in ad click-yield in live A/B tests, demonstrating its effectiveness in real-world settings.
- The method is practical in limited compute scenarios, as even a 1.3B SLM maintains close to LLM-level generation quality with minimal performance drop.

## Why This Works (Mechanism)
SKIM works by leveraging world knowledge from unstructured metadata to generate diverse synthetic queries that capture missing relevance signals in the training data. The SLM finetuned on synthetic query generation can produce high-quality queries even with unstructured metadata, as it has been trained to extract relevant information from various sources. The use of ANNS for mapping synthetic queries to training queries ensures scalability and efficiency, allowing SKIM to handle large-scale extreme classification datasets.

## Foundational Learning
- **Extreme Classification (XC)**: A task involving multi-label classification with a large number of labels (e.g., millions). [why needed] Understanding XC is crucial as SKIM is designed to mitigate missing labels in this specific setting. [quick check] Verify that the method can handle datasets with a large number of labels (e.g., 10M) and improve Recall@K metrics.
- **Synthetic Query Generation**: The process of generating diverse queries using a language model and metadata. [why needed] This is a key component of SKIM, as it helps capture missing relevance signals in the training data. [quick check] Evaluate the quality of synthetic queries generated by the SLM using metrics like GT coverage @ 100 or manual inspection.
- **Approximate Nearest Neighbor Search (ANNS)**: A scalable algorithm for finding nearest neighbors in high-dimensional spaces. [why needed] ANNS is used in SKIM to efficiently map synthetic queries to training queries. [quick check] Analyze the distribution of cosine similarities between synthetic and train queries and adjust the similarity threshold ùúè accordingly.

## Architecture Onboarding
- **Component Map**: SLM (finetuned on synthetic query generation) -> ANNS (mapping synthetic queries to train queries) -> XC model (trained on augmented dataset)
- **Critical Path**: SLM finetuning -> Synthetic query generation -> ANNS mapping -> XC model training and evaluation
- **Design Tradeoffs**: SKIM uses a small language model instead of a large language model to reduce computational costs while maintaining generation quality. The method also relies on unstructured metadata, which may vary in quality and relevance across different datasets.
- **Failure Signatures**: Poor quality synthetic queries due to inadequate SLM finetuning or lack of metadata, and ineffective mapping of synthetic queries to train queries due to suboptimal ANNS parameters.
- **3 First Experiments**:
  1. Finetune an SLM (e.g., Llama2-7B) on synthetic query generation using metadata, following the prompts and examples provided in the paper.
  2. Generate synthetic queries for each document in the dataset and evaluate their quality using metrics like GT coverage @ 100 or manual inspection.
  3. Map synthetic queries to train queries using ANNS and analyze the distribution of cosine similarities to adjust the similarity threshold ùúè.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does SKIM's performance scale with the size of the SLM used in Step 1 of the algorithm, and what is the minimum SLM size required to maintain LLM-level generation quality?
- **Open Question 2**: How does the quality of synthetic queries generated by SKIM vary with the quality and relevance of the unstructured metadata provided for each document?
- **Open Question 3**: How does SKIM's performance vary with the choice of retriever used in Step 2 of the algorithm, and what is the impact of using different retrieval models (e.g., ANNS vs. more sophisticated models)?

## Limitations
- The effectiveness of SKIM may be limited by the quality and diversity of the unstructured metadata available for each document.
- The paper does not provide a detailed analysis of the impact of different retrievers on SKIM's performance or the optimal choice of retriever for the method.
- The exact implementation details and hyperparameters used for SLM finetuning and ANNS are not fully specified, which may affect the reproducibility of the results.

## Confidence
- High: The problem of missing labels in extreme classification datasets and the potential of leveraging world knowledge to mitigate this issue.
- Medium: The effectiveness of SKIM in improving Recall@K and ad click-yield based on the reported results.
- Low: The exact implementation details and hyperparameters used for SLM finetuning and ANNS.

## Next Checks
1. Obtain the exact prompts and in-context examples used for finetuning the SLM on synthetic query generation to ensure faithful reproduction of the method.
2. Experiment with different hyperparameters for SLM finetuning and ANNS to optimize the quality of synthetic queries and their mapping to train queries.
3. Conduct ablation studies to assess the impact of metadata quality and diversity on the effectiveness of SKIM in mitigating missing labels.