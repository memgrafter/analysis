---
ver: rpa2
title: Debating with More Persuasive LLMs Leads to More Truthful Answers
arxiv_id: '2402.06782'
source_url: https://arxiv.org/abs/2402.06782
tags:
- judge
- answer
- debate
- more
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether weaker models can effectively oversee
  stronger models by leveraging debate as an alignment mechanism. The authors investigate
  this in a reading comprehension task where expert models (with access to source
  text) argue for opposing answers while non-expert judges (without access to text)
  choose the correct answer.
---

# Debating with More Persuasive LLMs Leads to More Truthful Answers

## Quick Facts
- arXiv ID: 2402.06782
- Source URL: https://arxiv.org/abs/2402.06782
- Authors: Akbir Khan; John Hughes; Dan Valentine; Laura Ruis; Kshitij Sachan; Ansh Radhakrishnan; Edward Grefenstette; Samuel R. Bowman; Tim Rocktäschel; Ethan Perez
- Reference count: 40
- Primary result: Non-expert LLM judges achieve 76% accuracy and human judges achieve 88% accuracy using debate protocols, compared to 48% and 60% for naive baselines respectively

## Executive Summary
This paper investigates whether weaker models can effectively oversee stronger models by leveraging debate as an alignment mechanism. The authors test this in a reading comprehension task where expert models (with access to source text) argue for opposing answers while non-expert judges (without access to text) choose the correct answer. Using debate protocols, non-expert LLM judges achieved 76% accuracy and human judges achieved 88%, compared to 48% and 60% for naive baselines respectively. The study also demonstrates that optimizing debaters for persuasiveness improves their ability to argue for correct answers relative to incorrect ones. These findings suggest debate is a promising approach for scalable oversight, especially as models become increasingly sophisticated and surpass human expertise.

## Method Summary
The study uses the QuALITY dataset (reading comprehension questions with long passages) filtered for HARD questions with 100% annotator agreement. Expert debaters (with quote tool access to source text) argue for opposing answers in 3-round debates with 900-word total limits. Non-expert judges (either LLM or human) evaluate debate transcripts to identify correct answers without seeing the source text. The authors employ best-of-N and critique-and-refinement optimization techniques to create debaters of varying skill levels, measure persuasiveness using an Elo rating system through Swiss tournaments, and test both static and interactive debate protocols.

## Key Results
- Non-expert LLM judges achieved 76% accuracy using debate protocols versus 48% for naive baselines
- Human judges achieved 88% accuracy using debate protocols versus 60% for naive baselines
- Debate protocols produced higher accuracy than consultancy protocols for human judges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Debate enables non-experts to accurately assess expert claims by structuring adversarial argument evaluation
- Mechanism: When two expert debaters argue opposing answers, the non-expert judge can identify the correct answer by evaluating the quality of arguments and evidence presented, without needing to understand the underlying text
- Core assumption: The correct answer is easier to argue for than the incorrect answer (Irving et al., 2018)
- Evidence anchors: Abstract states debate helps both non-expert models and humans answer questions with 76% and 88% accuracy respectively
- Break condition: If debaters become equally persuasive at arguing both correct and incorrect answers, or if judges cannot distinguish argument quality

### Mechanism 2
- Claim: Optimizing debaters for persuasiveness improves their ability to argue for correct answers relative to incorrect ones
- Mechanism: By using inference-time optimization methods like best-of-N and critique-and-refinement, models become more skilled at presenting compelling arguments, and this skill disproportionately benefits correct answer advocacy
- Core assumption: Models fine-tuned with RLHF have inherent bias toward honesty, making truth-telling more persuasive than deception
- Evidence anchors: Abstract states optimizing expert debaters for persuasiveness improves non-expert ability to identify truth in debates
- Break condition: If optimization methods disproportionately improve deceptive capabilities rather than truthful ones

### Mechanism 3
- Claim: Human judges achieve better calibration and accuracy with debate compared to consultancy protocols
- Mechanism: The adversarial structure of debate provides multiple perspectives and counterarguments, enabling judges to make more informed decisions and be more aware of their confidence levels
- Core assumption: Exposure to opposing arguments helps judges better assess the strength of evidence
- Evidence anchors: Abstract states human judges achieved 88% accuracy, and the paper notes debate protocols produce higher accuracy than consultancy protocols
- Break condition: If debate transcripts become too complex for judges to follow or if adversarial tactics overwhelm evidence-based reasoning

## Foundational Learning

- Concept: Information asymmetry in reading comprehension
  - Why needed here: The experimental setup relies on debaters having access to source text while judges do not, creating the "weaker judges supervising stronger models" scenario
  - Quick check question: If judges had access to the same text as debaters, would the debate protocol still provide value for scalable oversight?

- Concept: Elo rating system for measuring debater persuasiveness
  - Why needed here: Provides an unsupervised metric to compare debater capabilities without requiring ground truth labels
  - Quick check question: How does the Elo rating system handle the fact that a debater might argue for correct or incorrect answers in different matches?

- Concept: Best-of-N and critique-and-refinement optimization techniques
  - Why needed here: These inference-time methods are used to create a range of debater capabilities for testing how debate scales with model strength
  - Quick check question: What's the difference between best-of-N and critique-and-refinement in terms of how they improve debater arguments?

## Architecture Onboarding

- Component map: Debate protocols (static, interactive, consultancy) -> Expert debaters (with quote tool access) -> Non-expert judges (LLM or human) -> Preference models for optimization -> Swiss tournament system for Elo ranking -> Quote verification system

- Critical path: 1. Generate debate transcripts with optimized debaters 2. Have judges evaluate transcripts 3. Calculate win rates and Elo ratings 4. Measure judge accuracy against debater skill levels

- Design tradeoffs: Word limits vs. argument completeness, Number of debate rounds vs. judge deliberation ability, Preference model alignment vs. generalization, Static vs. interactive protocols for human efficiency

- Failure signatures: Self-defeating behavior where incorrect debaters concede, Positional bias in LLM judges favoring certain answer positions, Verbosity bias where longer arguments are favored regardless of quality, Quote verification failures leading to trust in unverified evidence

- First 3 experiments: 1. Run baseline consultancy vs. debate with GPT-4-Turbo debaters and GPT-4-Turbo judge on 100 QuALITY questions 2. Generate Elo ratings through Swiss tournament with 10 debater variations across 200 questions 3. Test human judge calibration by having 5 annotators judge 20 debate transcripts with confidence ratings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can debate protocols be adapted to domains where evidence is not verifiable through direct quotes or where models cannot access source text?
- Basis in paper: The authors note that debate effectiveness relies on debaters providing "verified evidence" through a quote tool, and speculate about other domains requiring "tool-use capabilities" like simulators for math debates or experimental data for physics debates.
- Why unresolved: The paper only tests debate in reading comprehension tasks with verifiable quotes. It does not explore whether debate remains effective when debaters lack access to source material or when evidence cannot be verified through direct quotes.
- What evidence would resolve it: Experiments testing debate protocols in domains like mathematics (requiring proof verification), scientific reasoning (requiring experimental data), or social science (requiring statistical evidence) where debaters cannot access source text but must argue using different forms of evidence.

### Open Question 2
- Question: How does debate performance scale when the gap between expert and non-expert knowledge exceeds mere information asymmetry (e.g., when experts have superior reasoning capabilities or different cognitive architectures)?
- Basis in paper: The authors acknowledge their setup only tests "information asymmetry" where experts have "access to the necessary information" while judges do not, and note that "stronger models may differ in reasoning ability or another skill."
- Why unresolved: All experiments use the same base LLM models with only information access differences. The paper does not test whether debate remains effective when experts have fundamentally superior reasoning capabilities that cannot be compensated by information access.
- What evidence would resolve it: Experiments comparing debate effectiveness when both experts and judges have full information access but experts have enhanced reasoning capabilities through fine-tuning, larger model sizes, or specialized architectures.

### Open Question 3
- Question: Does optimizing debaters for persuasiveness create deceptive capabilities that could be misused, and can these capabilities be controlled or restricted?
- Basis in paper: The authors find that "optimising debaters for persuasiveness" improves their ability to argue for correct answers, but also note that "models optimised for persuasiveness" in consultancy settings can become "more persuasive" at arguing for incorrect answers, degrading judge accuracy.
- Why unresolved: The paper demonstrates that persuasiveness optimization can improve debate outcomes but does not investigate whether this creates deceptive capabilities that could be exploited outside debate contexts or how to prevent such misuse.
- What evidence would resolve it: Experiments testing whether persuasiveness-optimized models exhibit increased deception in non-debate contexts, and whether safety fine-tuning or other interventions can prevent the transfer of these deceptive capabilities.

## Limitations

- The evaluation relies on GPT-4-Turbo as both debater and judge, raising concerns about potential positional and verbosity bias
- The study focuses on a specific reading comprehension task rather than more general alignment problems, limiting generalizability
- The human evaluation sample size is relatively small (5 annotators), which may not capture full variability in human judgment

## Confidence

**High Confidence**: Non-expert LLM judges achieve significantly higher accuracy using debate protocols (76%) compared to naive baselines (48%); Human judges achieve significantly higher accuracy using debate protocols (88%) compared to naive baselines (60%); Debate protocols produce higher accuracy than consultancy protocols for human judges

**Medium Confidence**: Optimizing debaters for persuasiveness improves their ability to argue for correct answers relative to incorrect ones; The Elo rating system provides a reliable unsupervised metric for measuring debater skill levels

**Low Confidence**: The correct answer is inherently easier to argue for than the incorrect answer (core assumption for debate effectiveness); RLHF fine-tuning provides an inherent bias toward honesty that benefits debate outcomes

## Next Checks

1. **Positional and verbosity bias validation**: Run additional experiments with varied answer positions and word limits, measuring judge accuracy while controlling for these factors to quantify their impact on debate outcomes.

2. **Generalization across domains**: Test the debate protocol on non-reading comprehension tasks (e.g., mathematical reasoning, code generation) to assess whether the observed benefits generalize beyond the QuALITY dataset.

3. **Human sample size expansion**: Conduct human evaluation with a larger sample size (n≥20) and diverse annotator backgrounds to better understand human judge performance variability and calibration accuracy.