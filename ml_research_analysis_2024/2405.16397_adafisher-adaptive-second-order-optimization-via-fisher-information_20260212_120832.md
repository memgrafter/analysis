---
ver: rpa2
title: 'AdaFisher: Adaptive Second Order Optimization via Fisher Information'
arxiv_id: '2405.16397'
source_url: https://arxiv.org/abs/2405.16397
tags:
- adafisher
- training
- learning
- adam
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaFisher, an adaptive second-order optimizer
  that improves upon first-order methods like Adam by leveraging a diagonal block-Kronecker
  approximation of the Fisher Information Matrix (FIM). By focusing on the diagonal
  elements of the KFs and incorporating EMA-based curvature estimation, AdaFisher
  achieves superior convergence and generalization while maintaining computational
  efficiency comparable to first-order methods.
---

# AdaFisher: Adaptive Second Order Optimization via Fisher Information

## Quick Facts
- arXiv ID: 2405.16397
- Source URL: https://arxiv.org/abs/2405.16397
- Reference count: 40
- This paper introduces AdaFisher, an adaptive second-order optimizer that improves upon first-order methods like Adam by leveraging a diagonal block-Kronecker approximation of the Fisher Information Matrix (FIM)

## Executive Summary
AdaFisher is a novel adaptive second-order optimization algorithm that achieves superior convergence and generalization compared to first-order methods like Adam. By approximating the Fisher Information Matrix using diagonal block-Kronecker factors and incorporating EMA-based curvature estimation, AdaFisher maintains computational efficiency comparable to first-order methods while leveraging the benefits of second-order information. Extensive experiments demonstrate that AdaFisher consistently outperforms state-of-the-art optimizers across image classification and language modeling tasks, reaching flatter minima with better generalization properties.

## Method Summary
AdaFisher computes the Fisher Information Matrix using Kronecker factors H (gradient of activation) and S (input of activation), which are approximated as diagonal matrices and combined via Kronecker product. These factors are estimated using Exponential Moving Average (EMA) across mini-batches, and the resulting diagonal block-Kronecker FIM is regularized with λI before inversion. The optimizer then uses this preconditioned information to update parameters without the square root operation found in Adam. This approach captures essential curvature information while maintaining O(d) computational complexity instead of O(d³) for full matrix inversion.

## Key Results
- AdaFisher achieves 95.2% top-1 accuracy on CIFAR-10 with ResNet-18, outperforming Adam (93.8%) and AdaHessian (94.1%)
- On ImageNet-1k with ResNet-50, AdaFisher reaches 77.8% accuracy compared to Adam's 76.5%
- Training speed is comparable to first-order methods while requiring less memory than full second-order approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagonal block-Kronecker approximation of the Fisher Information Matrix (FIM) captures essential curvature information while remaining computationally efficient.
- Mechanism: The Kronecker factors (KFs) H and S are approximated as diagonal matrices, and their Kronecker product is regularized with λI. This allows inversion in O(d) instead of O(d³) time while retaining the dominant eigenvalue information.
- Core assumption: The energy of the Kronecker factors is concentrated along the diagonal, making off-diagonal elements negligible for optimization.
- Evidence anchors:
  - [abstract] "by leveraging a diagonal block-Kronecker approximation of the Fisher Information Matrix (FIM)"
  - [section] "Our study from Section 3.1 suggests that the FIM's critical information predominantly resides along its diagonal"
  - [corpus] Weak - neighboring papers discuss Fisher Information but do not validate diagonal dominance empirically
- Break condition: If off-diagonal elements carry significant curvature information for specific architectures or tasks, the diagonal approximation will lose critical optimization signals.

### Mechanism 2
- Claim: AdaFisher's exclusion of the square root in the update rule improves both performance and computational efficiency.
- Mechanism: The update rule uses m(t)/(F(t)ᴰ) instead of m(t)/√(v(t)), aligning with the theoretical definition of second-order methods and avoiding unnecessary computational overhead.
- Core assumption: The FIM naturally incorporates EMA of its KFs, making the square root operation redundant for curvature estimation.
- Evidence anchors:
  - [abstract] "AdaFisher distinguishes itself from Adam by incorporating a higher fidelity approximation of the FIM"
  - [section] "AdaFisher omits the square root and the traditional EMA applied over the second moment"
  - [corpus] Weak - neighboring papers discuss Adam variants but do not specifically address square root removal
- Break condition: If empirical evidence shows that the square root operation provides regularization benefits that improve generalization in specific scenarios.

### Mechanism 3
- Claim: AdaFisher achieves superior generalization by converging to flatter local minima compared to first-order methods.
- Mechanism: The diagonal block-Kronecker FIM approximation provides better curvature information, guiding the optimization process toward regions of the loss landscape with lower curvature and better generalization properties.
- Core assumption: Flatter minima correspond to better generalization, and second-order methods can more effectively navigate to these regions.
- Evidence anchors:
  - [abstract] "AdaFisher not only converges more rapidly but also reaches a superior local minimum by effectively navigating through saddle points"
  - [section] "AdaFisher's FIM values exhibit narrow variations and lower mean values during training, suggesting a convergence towards flatter minima"
  - [corpus] Weak - neighboring papers discuss second-order optimization but do not provide empirical evidence of flatter minima convergence
- Break condition: If generalization performance does not improve despite flatter loss landscape convergence, suggesting other factors dominate generalization.

## Foundational Learning

- Concept: Fisher Information Matrix and its relationship to the Hessian
  - Why needed here: Understanding why FIM is used instead of Hessian and how it approximates curvature information
  - Quick check question: What is the key difference between Fisher Information Matrix and Hessian in the context of optimization?

- Concept: Kronecker product and block-diagonal matrix structures
  - Why needed here: Essential for understanding how the FIM is approximated and computed efficiently
  - Quick check question: How does the Kronecker product of diagonal matrices simplify to another diagonal matrix?

- Concept: Exponential Moving Average (EMA) and its role in optimization
  - Why needed here: Critical for understanding how curvature information is estimated across mini-batches
  - Quick check question: Why is EMA used for estimating Kronecker factors instead of computing them from scratch each batch?

## Architecture Onboarding

- Component map: Gradient computation → Kronecker factor computation (H and S) → EMA smoothing → Diagonal approximation and normalization → Regularization with λI → Diagonal block-Kronecker FIM formation → Inversion and preconditioning → Parameter update

- Critical path: Forward pass → Compute KFs H and S → Apply EMA → Diagonalize and normalize → Form diagonal block-Kronecker FIM → Invert and precondition gradients → Update parameters. Each step must complete before the next can begin in the backward pass.

- Design tradeoffs: Diagonal approximation reduces computational complexity from O(d³) to O(d) but may lose some curvature information. Regularization with λI ensures numerical stability but may over-smooth the curvature estimate. EMA provides smoother curvature estimates but introduces lag in adaptation.

- Failure signatures: Poor convergence may indicate inadequate damping (λ too small), overly aggressive diagonal approximation (off-diagonal information important), or incorrect EMA decay (γ not tuned). Memory issues suggest improper handling of large matrices or insufficient GPU resources.

- First 3 experiments:
  1. Implement AdaFisher on a simple CNN (e.g., ResNet-18) on CIFAR-10 with default HPs to verify basic functionality and compare against Adam
  2. Vary the damping parameter λ systematically to find optimal values for different architectures
  3. Compare training dynamics with and without the square root operation to validate the performance claims

## Open Questions the Paper Calls Out
None

## Limitations
- The diagonal block-Kronecker approximation may discard curvature information present in off-diagonal elements of the Kronecker factors
- EMA hyperparameters (γ) were tuned for specific tasks but their generalizability across different domains is unclear
- The interaction between regularization parameter λ and EMA decay γ is not thoroughly explored

## Confidence
- **High**: AdaFisher's computational efficiency claims and experimental setup are well-documented
- **Medium**: Generalization improvements and convergence speed benefits are demonstrated but could benefit from additional ablation studies
- **Low**: The theoretical foundation for diagonal approximation optimality and EMA-based curvature estimation requires further validation

## Next Checks
1. **Architecture Sensitivity Test**: Evaluate AdaFisher across a broader range of architectures (Transformers, LSTMs, Graph Neural Networks) to verify that diagonal approximation remains effective beyond standard CNNs and RNNs.

2. **Diagonal Dominance Verification**: Implement a diagnostic tool to measure the energy distribution across diagonal vs. off-diagonal elements of Kronecker factors during training, providing empirical evidence for the approximation assumption.

3. **EMA Sensitivity Analysis**: Systematically vary EMA decay parameters γ across multiple orders of magnitude to identify regimes where performance degrades, establishing robustness boundaries for the curvature estimation approach.