---
ver: rpa2
title: 'SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language
  Models'
arxiv_id: '2405.08317'
source_url: https://arxiv.org/abs/2405.08317
tags:
- attacks
- safety
- adversarial
- language
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive study on the safety
  robustness of speech-language models (SLMs) to adversarial jailbreaking attacks.
  The authors systematically evaluate SLMs trained for spoken question-answering tasks
  using white-box and black-box attack scenarios, showing that barely perceptible
  audio perturbations can successfully jailbreak safety-aligned models with 90% attack
  success rate.
---

# SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2405.08317
- Source URL: https://arxiv.org/abs/2405.08317
- Reference count: 26
- Primary result: Barely perceptible audio perturbations achieve 90% attack success rate against safety-aligned speech-language models

## Executive Summary
This paper presents the first comprehensive study on the safety robustness of speech-language models (SLMs) to adversarial jailbreaking attacks. The authors systematically evaluate SLMs trained for spoken question-answering tasks using white-box and black-box attack scenarios, demonstrating that minimally perturbed audio can successfully bypass safety guardrails. The study introduces a simple pre-processing countermeasure using time-domain noise flooding that significantly reduces attack success while maintaining model helpfulness. Results show that while white-box attacks achieve 90% success rates, transfer attacks across models succeed 10% of the time, highlighting vulnerabilities in current SLM safety alignments.

## Method Summary
The study trains SLMs using Conformer audio encoders combined with LLM backbones (Flan-T5-XL or Mistral-7B) through a two-stage process: ASR pre-adaptation followed by cross-modal instruction fine-tuning with safety alignment. Adversarial attacks are crafted using fast gradient sign method and projected gradient descent on input speech, measuring success by whether the model generates unsafe responses. The proposed defense, time-domain noise flooding, adds white Gaussian noise at controlled SNR levels to disrupt adversarial perturbations while preserving useful signal. Evaluation uses 360 spoken harmful questions across 12 toxic categories and 100 helpful questions, with safety and relevance assessed by human annotators using Claude 2.1.

## Key Results
- White-box adversarial attacks achieve 90% success rate in jailbreaking safety-aligned SLMs with barely perceptible perturbations
- Transfer attacks succeed across different SLM architectures with 10% success rate
- Time-domain noise flooding reduces attack success significantly while maintaining model helpfulness
- Safety-aligned models show low safety and relevance rates on harmful questions (8.5% and 16.1% respectively)

## Why This Works (Mechanism)

### Mechanism 1
Audio adversarial perturbations bypass speech-language model safety guardrails by manipulating the audio encoder's continuous input domain. The adversarial perturbation δ is crafted to minimally change the input speech signal while causing a large shift in the model's internal representation, ultimately leading to an unsafe response. This works because the audio encoder processes continuous speech signals, which can be subtly altered in ways that the LLM's safety training cannot detect.

### Mechanism 2
Transfer attacks work because adversarial perturbations learned on one model transfer to others with similar architectures or training data. A perturbation δ crafted on a surrogate model causes the same unsafe output when applied to a different (victim) model, exploiting shared vulnerabilities in the models' architectures or training data. This allows black-box attacks without direct gradient access.

### Mechanism 3
Time-domain noise flooding (TDNF) defends against attacks by adding random noise that drowns out adversarial perturbations while preserving useful signal for the audio encoder. Adding white Gaussian noise at a controlled SNR disrupts the adversarial perturbation's effect while the audio encoder's robustness to random noise maintains input understanding.

## Foundational Learning

- Concept: Adversarial attacks on continuous domains
  - Why needed here: SLMs operate on continuous audio signals, unlike text-only models, making them susceptible to classical adversarial attack methods
  - Quick check question: Why are audio signals more vulnerable to adversarial attacks than discrete text prompts?

- Concept: Transferability of adversarial examples
  - Why needed here: The paper demonstrates that attacks on one SLM can transfer to others, enabling black-box attacks without gradient access
  - Quick check question: What factors make adversarial perturbations transferable between different models?

- Concept: Randomized smoothing and its variants
  - Why needed here: The TDNF defense is based on the principle of randomized smoothing, adapted for audio preprocessing
  - Quick check question: How does adding random noise provide robustness against adversarial perturbations?

## Architecture Onboarding

- Component map: Audio encoder (Conformer) -> Convolutional layers -> LLM (Flan-T5 or Mistral) with LoRA adapters
- Critical path: Audio input -> Audio encoder -> Convolutional layers -> LLM -> Text response
- Design tradeoffs: Model size vs. computational efficiency (e.g., 300M Conformer vs. larger alternatives), Safety alignment vs. helpfulness (trade-off in training data and fine-tuning), Defense strength vs. utility degradation (TDNF SNR levels)
- Failure signatures: High attack success rate (>90%) indicates vulnerability to white-box attacks, Cross-model attack success (>10%) indicates transferable vulnerabilities, Low safety/relevance rates in original models indicate weak safety alignment
- First 3 experiments: 1) Replicate the white-box attack success rate on a simple SLM to verify vulnerability, 2) Test TDNF defense effectiveness by measuring attack success rate at different SNR levels, 3) Conduct cross-model attack to verify transferability between architectures

## Open Questions the Paper Calls Out

### Open Question 1
How effective are adaptive attacks against noise-flooding defenses compared to non-adaptive attacks? The paper mentions performing adaptive attacks where the attacker knows about the defense, and found that an adaptive attacker can evade the defense but with reduced success and lower SPR. This remains unresolved as the paper only briefly mentions adaptive attacks without detailed results or comparisons.

### Open Question 2
How do different architectures of speech-language models (SLMs) affect their vulnerability to adversarial attacks? The paper mentions that different model architectures show different levels of vulnerability to adversarial attacks, but does not provide a detailed analysis of how different architectures affect vulnerability, nor does it compare multiple architectures in depth.

### Open Question 3
What is the impact of incorporating general instruction tuning data during cross-modal instruction fine-tuning on the safety alignment of SLMs? The paper mentions that incorporating general instruction tuning data during cross-modal instruction fine-tuning improves the helpfulness of SLMs but also notes a healthy tension between helpfulness and harmlessness. This trade-off is not analyzed in detail.

## Limitations

- ASR training data is partially unspecified, affecting reproducibility of the exact SLM architecture
- Defense mechanism (time-domain noise flooding) may not scale to more sophisticated adaptive attacks
- Evaluation uses a limited set of 360 harmful questions and 100 helpful questions, potentially missing real-world attack diversity

## Confidence

**High Confidence:** The demonstration that barely perceptible audio perturbations can jailbreak SLMs (90% success rate) is well-supported by the experimental results and aligns with established adversarial attack literature on continuous domains.

**Medium Confidence:** The claim that adversarial attacks transfer across models with 10% success rate is supported by experiments, but the transferability rate may vary significantly with different model pairs and requires further validation.

**Medium Confidence:** The effectiveness of time-domain noise flooding as a defense is demonstrated empirically, but its robustness against adaptive attacks and its impact on real-world utility remains to be fully explored.

## Next Checks

1. **Transferability Stress Test:** Evaluate attack transferability across a broader range of SLM architectures (including different audio encoders and LLM backbones) to determine if the 10% transfer rate is consistent or architecture-dependent.

2. **Adaptive Attack Evaluation:** Test whether an attacker aware of the TDNF defense can adapt their attack strategy (e.g., using higher-magnitude perturbations or noise-aware attack methods) to maintain high success rates.

3. **Real-World Robustness Assessment:** Evaluate the SLMs and defenses using naturally occurring adversarial prompts from real-world interactions, rather than synthetically generated harmful questions, to assess practical security implications.