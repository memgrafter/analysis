---
ver: rpa2
title: Improving Legal Judgement Prediction in Romanian with Long Text Encoders
arxiv_id: '2402.19170'
source_url: https://arxiv.org/abs/2402.19170
tags:
- arxiv
- legal
- jurbert
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles Legal Judgment Prediction (LJP) in Romanian,
  where standard models struggle with long, specialized legal documents. The authors
  propose extending Transformer sequence length using SLED-style chunking and long-context
  adaptations (Longformer, Llama2, Okapi) on four LJP datasets from two sources: BankingCases
  (judges'' summaries, shorter) and BRDCases (plaintiff pleas, much longer).'
---

# Improving Legal Judgement Prediction in Romanian with Long Text Encoders

## Quick Facts
- arXiv ID: 2402.19170
- Source URL: https://arxiv.org/abs/2402.19170
- Authors: Mihai Masala; Traian Rebedea; Horia Velicu
- Reference count: 0
- Primary result: SLED encoding improves LJP performance on long Romanian legal documents

## Executive Summary
This work addresses Legal Judgment Prediction (LJP) in Romanian, where standard models struggle with long, specialized legal documents. The authors propose extending Transformer sequence length using SLED-style chunking and long-context adaptations (Longformer, Llama2, Okapi) on four LJP datasets. Their core method is feeding models longer documents via 32 chunks of 256 tokens each with SLED encoding, supplemented by handcrafted case metadata. Results show vanilla jurBERT outperforms general LLMs on short texts, but SLED encoding yields the best performance (up to ~73% AUC) on the longer BRDCases documents.

## Method Summary
The authors tackle Romanian LJP using four datasets from BankingCases (judge summaries) and BRDCases (plaintiff pleas). They fine-tune Romanian jurBERT with various sequence length extensions: 2×512/3×512 blocks, Longformer variants (1024/2048/4096 tokens), SLED encoding (32×256 tokens with overlap), and compare with Llama2/Okapi (4096 tokens). All models use 5-fold cross-validation, handcrafted features (year, county), and are evaluated on AUC. SLED splits long texts into overlapping chunks with full self-attention within each chunk, then aggregates representations for classification.

## Key Results
- jurBERT with 512 tokens outperforms Llama2/Okapi on short BankingCases documents
- SLED encoding with 32×256 chunks achieves highest AUC (~73%) on long BRDCases documents
- Longformer variants underperform due to limited training data (15k total samples)
- Specialized jurBERT vocabulary proves more efficient than general multilingual vocabularies for Romanian legal texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLED encoding allows longer documents to be processed by splitting them into overlapping chunks, improving performance on long legal texts.
- Mechanism: SLED divides text into 32 chunks of 256 tokens each with partial overlap, applies full self-attention within each chunk, then aggregates token representations for classification.
- Core assumption: Overlapping chunks preserve context continuity better than non-overlapping splits for legal judgment prediction.
- Evidence anchors:
  - [abstract] "We prove that SLED encoding applied on long documents for the legal judgement prediction tasks significantly improves performance"
  - [section 3.2] "SLED proposes an efficient way of splitting the text into smaller blocks with partial overlap to allow longer sequences to be encoded"
- Break condition: If overlap size is too small to capture cross-chunk legal arguments, or if the aggregation step loses critical context.

### Mechanism 2
- Claim: Specialized legal vocabulary models outperform general multilingual LLMs in low-resource settings.
- Mechanism: jurBERT uses domain-specific vocabulary trained on Romanian legal texts, encoding legal concepts more efficiently than general vocabulary models like Llama2/Okapi.
- Core assumption: Domain-specific tokenization captures legal terminology more effectively than general tokenization in low-resource languages.
- Evidence anchors:
  - [section 4] "jurBERT uses a specialized vocabulary (in Romanian juridical domain) and therefore is much more efficient in encoding legal texts compared to the general multi-language vocabulary used by Llama2/Okapi models"
  - [section 3.2] "jurBERT model [...] specialized vocabulary (in Romanian juridical domain)"
- Break condition: If domain-specific vocabulary becomes too narrow and misses general language patterns needed for legal reasoning.

### Mechanism 3
- Claim: Longformer variants underperform due to limited training data preventing proper learning of long-sequence patterns.
- Mechanism: Longformer extends sequence length but requires sufficient data to learn effective attention patterns across long contexts.
- Core assumption: Limited training data (15k total samples) prevents Longformer from learning meaningful long-range dependencies.
- Evidence anchors:
  - [section 4] "In the case of Longformer variants, we believe their lack of performance is due to the limited training data (15k total, 12k training samples) that does not allow the model to properly learn how to handle longer sequences"
  - [section 3.2] "Longformer makes use of dilated sliding windows enabling long-range coverage while keeping sparsity"
- Break condition: If additional training data doesn't improve Longformer performance, suggesting architectural limitations beyond data quantity.

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: Understanding why standard Transformers struggle with long legal documents and how alternatives like SLED and Longformer modify attention
  - Quick check question: Why does self-attention have quadratic complexity, and how do sparse attention methods reduce this?

- Concept: Legal judgment prediction task formulation
  - Why needed here: Understanding the binary classification task of predicting case outcomes from legal documents
  - Quick check question: How does the task differ when processing judge-written summaries versus plaintiff pleas?

- Concept: Domain adaptation for low-resource languages
  - Why needed here: Understanding why specialized models outperform general multilingual models in Romanian legal domain
  - Quick check question: What challenges arise when adapting models to specialized legal vocabulary in low-resource languages?

## Architecture Onboarding

- Component map: Input preprocessing → Tokenization (jurBERT/Llama2/Okapi) → Sequence length handling (512, 2×512, 3×512, Longformer, SLED) → Handcrafted features (year, county) → Classification layer
- Critical path: Text → Tokenizer → Sequence handler → Feature concatenation → Classifier
- Design tradeoffs: SLED offers better performance on long texts but adds preprocessing complexity; Longformer reduces complexity but needs more data; specialized vocabulary improves efficiency but limits generalization
- Failure signatures: High AUC variance across folds (data scarcity), underperformance on long texts (sequence handling), poor results on multilingual models (domain mismatch)
- First 3 experiments:
  1. Baseline jurBERT with 512 tokens on BankingCases ADM to verify strong performance
  2. SLED with 32×256 chunks on BRDCases ADM to test long document handling
  3. 2×512 variant on BRDCases ENF to compare simple multi-block approach vs SLED

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of specialized legal models like jurBERT versus general LLMs like Llama2/Okapi vary significantly when applied to other low-resource languages with similar characteristics to Romanian?
- Basis in paper: [explicit] The paper highlights that jurBERT outperforms Llama2/Okapi in Romanian legal judgment prediction, attributing this to jurBERT's specialized vocabulary and domain training, while noting that LLMs have been trained on very few Romanian texts.
- Why unresolved: The study is limited to Romanian; results may not generalize to other low-resource languages or different legal domains.
- What evidence would resolve it: Comparative experiments applying jurBERT and Llama2/Okapi to legal judgment prediction tasks in other low-resource languages (e.g., Bulgarian, Hungarian) would clarify whether the observed pattern holds.

### Open Question 2
- Question: How does the performance of SLED-style encoding scale with document length beyond the tested 32 chunks of 256 tokens, especially for extremely long legal documents?
- Basis in paper: [explicit] The paper shows SLED encoding improves performance for BRDCases with long documents but does not explore scaling beyond 32 chunks or test on documents longer than those in BRDCases.
- Why unresolved: The study does not investigate the limits of SLED's effectiveness or potential diminishing returns with even longer documents.
- What evidence would resolve it: Experiments testing SLED encoding on progressively longer documents (e.g., 64 or 128 chunks) and measuring performance degradation or stability would address this.

### Open Question 3
- Question: What is the impact of handcrafted metadata (year and county) on model performance across different legal judgment prediction tasks, and is it consistently beneficial?
- Basis in paper: [explicit] The paper injects handcrafted metadata (year and county) into the model and notes its use, but does not analyze its individual contribution to performance improvements.
- Why unresolved: The study does not isolate the effect of handcrafted features from other model improvements, leaving their value unclear.
- What evidence would resolve it: Ablation studies removing handcrafted features from models and comparing performance would determine their consistent benefit across tasks.

### Open Question 4
- Question: How do different long-context encoding strategies (e.g., SLED, Longformer, 2×512, 3×512) compare in terms of computational efficiency and scalability for very large legal corpora?
- Basis in paper: [explicit] The paper compares these strategies but focuses on accuracy; it does not provide detailed analysis of computational costs or scalability for large datasets.
- Why unresolved: The study prioritizes accuracy over efficiency, leaving questions about practical deployment for large-scale applications.
- What evidence would resolve it: Benchmarking experiments measuring training/inference time, memory usage, and scalability for each encoding strategy on large legal corpora would clarify trade-offs.

## Limitations
- Data scarcity limits generalizability, particularly for Longformer variants (15k total samples)
- SLED implementation details remain underspecified (overlap size, aggregation method)
- Comparison conflates domain-specific vocabulary vs. Romanian-specific training effects

## Confidence
- **High Confidence**: jurBERT's superior performance on short texts with specialized vocabulary
- **Medium Confidence**: SLED's effectiveness on long BRDCases documents
- **Low Confidence**: Claims about Longformer's poor performance due to limited training data

## Next Checks
1. **SLED Implementation Audit**: Test varying overlap sizes and aggregation methods to isolate performance drivers
2. **Longformer Data Scaling Experiment**: Train on progressively larger data subsets to test data quantity hypothesis
3. **Vocabulary Ablation Study**: Create hybrid models to isolate effects of vocabulary specialization vs. model architecture