---
ver: rpa2
title: Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective
  Reinforcement Learning
arxiv_id: '2405.03082'
source_url: https://arxiv.org/abs/2405.03082
tags:
- learning
- policy
- reward
- convergence
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of finite-time convergence and
  sample complexity analysis for multi-objective reinforcement learning (MORL) with
  potentially conflicting objectives. The authors propose a novel actor-critic algorithm
  called MOAC that iteratively makes trade-offs among conflicting reward signals to
  find a Pareto-stationary solution.
---

# Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.03082
- Source URL: https://arxiv.org/abs/2405.03082
- Reference count: 40
- Primary result: First O(1/ϵ²) sample complexity bound for MORL, independent of number of objectives

## Executive Summary
This paper addresses the fundamental challenge of finite-time convergence and sample complexity in multi-objective reinforcement learning (MORL) with potentially conflicting objectives. The authors propose MOAC (Multi-Objective Actor-Critic), a novel algorithm that combines actor-critic architecture with momentum-based multi-objective gradient descent. The key innovation is a momentum mechanism that mitigates cumulative estimation bias while enabling provable convergence rate and sample complexity guarantees independent of the number of objectives. The algorithm achieves O(1/ϵ²) sample complexity for finding an ϵ-Pareto stationary solution in both discounted and average reward settings, which represents a significant theoretical advance in the MORL literature.

## Method Summary
MOAC is an actor-critic framework that iteratively makes trade-offs among conflicting reward signals to find Pareto-stationary solutions. The critic uses mini-batch TD learning to estimate value functions for each objective, while the actor computes individual policy gradients and solves a quadratic programming problem to find a common descent direction. A momentum coefficient ηt updates gradient weights λt, smoothing the trajectory across iterations and mitigating cumulative estimation bias. The algorithm can initialize gradient weights using environment samples rather than manual initialization when ηt is properly scheduled (e.g., ηt = t^(-1) or t^(-2)). The method works with both average and discounted reward settings, using different TD error formulations for each case.

## Key Results
- Achieves O(1/ϵ²) sample complexity for finding an ϵ-Pareto stationary solution
- Convergence rate and sample complexity are independent of the number of objectives M
- Momentum mechanism mitigates cumulative estimation bias in stochastic MGDA-type updates
- Environment-driven initialization of gradient weights using samples instead of manual tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum coefficient ηt mitigates cumulative estimation bias in stochastic MGDA-type updates
- Mechanism: Updates λt = (1-ηt)λt-1 + ηtλt* to smooth gradient weight trajectory
- Core assumption: Momentum coefficient can balance exploration vs bias reduction
- Evidence anchors: Abstract mentions bias mitigation; Section 3.3 defines momentum update
- Break condition: If ηt too small, algorithm reduces to pre-specified weights

### Mechanism 2
- Claim: Finite-time convergence rate O(1/ϵ²) is independent of number of objectives M
- Mechanism: Momentum ensures convergence depends only on Lipschitz constant LJ and maximum reward rmax
- Core assumption: QP solver remains tractable regardless of M
- Evidence anchors: Abstract states objective-dimension-independence; Section 4.2 discusses convergence rate
- Break condition: If QP solver's complexity scales poorly with M

### Mechanism 3
- Claim: Proper momentum scheduling enables environment-driven initialization of gradient weights
- Mechanism: With ηt = t^(-1) or t^(-2), first iteration uses optimal λ1 from environment samples
- Core assumption: First batch of samples provides sufficient information for meaningful initialization
- Evidence anchors: Abstract mentions environment-driven initialization; Section 4.2 discusses advantage
- Break condition: If first batch is unrepresentative

## Foundational Learning

- Concept: Multi-objective optimization and Pareto-stationarity
  - Why needed here: Algorithm must find solutions where no objective can be improved without hurting others
  - Quick check question: What distinguishes a Pareto-stationary solution from a Pareto-optimal one in non-convex settings?

- Concept: Actor-critic architecture with function approximation
  - Why needed here: Algorithm uses separate actor and critic components with linear function approximation
  - Quick check question: How does critic's TD error computation differ between average and discounted reward settings?

- Concept: Markov chain mixing time and its impact on sample complexity
  - Why needed here: Convergence analysis must account for correlated samples from underlying MDP
  - Quick check question: What role does mixing time constant κ play in sample complexity bound?

## Architecture Onboarding

- Component map: Environment -> Critic (TD learning) -> Individual gradients -> QP solver -> Momentum module -> Actor (policy update) -> Next iteration
- Critical path: Critic → Individual gradients → QP solution → Momentum update → Policy update → Next iteration
- Design tradeoffs:
  - Momentum coefficient ηt: Larger values give faster convergence but higher bias; smaller values give slower convergence but lower bias
  - Batch size D: Larger batches reduce Markovian noise but increase computational cost per iteration
  - Step sizes: Actor step size α = 1/(3LJ) and critic step size β must satisfy specific bounds for convergence
- Failure signatures:
  - Policy gradient norm ∥gt∥ not converging → Check critic convergence or QP solver accuracy
  - Objectives diverging instead of improving → Check momentum schedule or gradient weight initialization
  - Sample complexity much worse than O(1/ϵ²) → Check mixing time assumptions or function approximation quality
- First 3 experiments:
  1. Run MOAC on Resource Gathering environment with ηt = t^(-1) and verify all three objectives improve while gradient norm converges
  2. Compare MOAC with fixed weights initialization to verify environment-driven initialization provides better performance
  3. Vary batch size D and verify convergence error bound decreases proportionally to 1/D as theory predicts

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but identifies several directions for future research including multi-agent MORL, continuous state-action spaces, and nonlinear function approximation.

## Limitations
- Analysis assumes linear function approximation with bounded features, which may not hold in practice for complex domains
- Convergence guarantees rely on Markov chain mixing time κ, which can be difficult to estimate or verify for real-world MDPs
- Theoretical sample complexity of O(1/ϵ²) has not been empirically validated across diverse MORL benchmarks

## Confidence

- High: The momentum mechanism effectively mitigates cumulative estimation bias in MGDA-type updates
- Medium: The convergence rate O(1/ϵ²) is independent of the number of objectives M
- Medium: Environment-driven initialization via proper momentum scheduling provides practical benefits

## Next Checks

1. Conduct empirical validation on additional MORL benchmarks to verify the O(1/ϵ²) sample complexity across different numbers of objectives
2. Compare MOAC's performance with and without momentum scheduling on domains where gradient weight initialization is critical
3. Measure the actual mixing time κ on representative MOMDPs and assess its impact on theoretical sample complexity bounds