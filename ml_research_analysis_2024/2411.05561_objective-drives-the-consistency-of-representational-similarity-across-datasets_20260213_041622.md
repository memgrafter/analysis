---
ver: rpa2
title: Objective drives the consistency of representational similarity across datasets
arxiv_id: '2411.05561'
source_url: https://arxiv.org/abs/2411.05561
tags:
- similarity
- datasets
- across
- in1k
- representational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a principled framework to measure how representational
  similarity between vision models varies across datasets, addressing the question
  of whether model convergence is confounded by commonly used datasets. Our approach
  computes pairwise representational similarities for multiple model sets across diverse
  datasets and quantifies their consistency using correlation coefficients.
---

# Objective drives the consistency of representational similarity across datasets

## Quick Facts
- arXiv ID: 2411.05561
- Source URL: https://arxiv.org/abs/2411.05561
- Reference count: 40
- Primary result: Self-supervised vision models show highly transferable representational similarities across datasets compared to supervised and image-text models

## Executive Summary
This paper investigates how representational similarity between vision models varies across datasets and whether this consistency is confounded by commonly used datasets. The authors propose a principled framework to measure pairwise representational similarities across multiple model sets and quantify their consistency using correlation coefficients. They find that training objectives are the primary factor influencing consistency, with self-supervised models showing highly transferable relative similarities across datasets while supervised and image-text models exhibit less consistent patterns. The study reveals that representational similarities correlate strongly with performance differences only on single-domain and structured datasets, not on multi-domain datasets like ImageNet-1k, challenging assumptions about universal representational convergence.

## Method Summary
The study computes pairwise representational similarities for 64 vision models across 26 diverse datasets using Centered Kernel Alignment (CKA) with both linear and RBF kernels. The authors analyze consistency by calculating Pearson correlation coefficients between similarity vectors across dataset pairs, and examine the relationship between similarity and performance differences using linear probes. The framework systematically compares model sets from different training objectives (self-supervised, image-text, supervised) and dataset structures (multi-domain, single-domain, structured) to understand what drives representational similarity consistency.

## Key Results
- Self-supervised models show the most consistent pairwise representational similarities across datasets, while supervised and image-text models exhibit less transferable patterns
- Multi-domain datasets support multiple successful representation strategies, resulting in weak correlations between similarity and performance, while single-domain and structured datasets show strong correlations
- Representational similarity consistency depends critically on both model training objectives and dataset structure, challenging assumptions about universal representational convergence

## Why This Works (Mechanism)

### Mechanism 1
Training objective is the primary driver of consistency in representational similarity across datasets. Different training objectives (self-supervised, image-text, supervised) shape the representational space in distinct ways. Self-supervised learning models learn dataset-independent features that generalize better across different domains, while supervised models learn features tightly coupled to specific class-label supervision, and image-text models align representations with language semantics. The nature of the training objective fundamentally shapes what features are prioritized in the learned representation.

### Mechanism 2
Dataset structure (single-domain vs multi-domain) determines whether representational similarity correlates with downstream performance. Multi-domain datasets contain rich contextual information and diverse semantic categories, enabling multiple high-performance strategies that don't converge to similar representations. Single-domain and structured datasets require capturing specific discriminative features, leading to stronger correlations between similarity and performance. The amount and type of contextual information in a dataset determines the diversity of viable representation strategies.

### Mechanism 3
Local similarity measures (RBF kernel) are more sensitive to specific stimuli than global measures (linear kernel). RBF kernel with small σ captures fine-grained, stimulus-specific details in representations, while linear kernel measures broader structural similarities. This leads to different stability properties across datasets. The choice of similarity metric fundamentally determines which aspects of representations are emphasized.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: RSA is a core tool for quantifying how similarly two models process the same stimuli by comparing their internal representations. Why needed here: RSA provides the mathematical framework for comparing model representations systematically across different datasets.

- **Centered Kernel Alignment (CKA)**: CKA is used to compute pairwise representational similarities between models, with different kernels (linear vs RBF) capturing different aspects of representational structure. Why needed here: CKA provides a principled way to measure representational similarity that is invariant to isotropic scaling and orthogonal transformations.

- **Pearson correlation coefficient for similarity consistency**: The Pearson correlation quantifies whether relative pairwise similarities between models are preserved across different datasets, which is the core metric for measuring transferability. Why needed here: Pearson correlation provides a standardized measure to assess whether models maintain their relative similarity relationships across different datasets.

## Architecture Onboarding

- **Component map**: Data extraction → Feature computation → CKA similarity calculation → Consistency correlation → Performance correlation → Analysis
- **Critical path**: Model representation extraction → CKA computation → Consistency correlation analysis → Performance correlation analysis
- **Design tradeoffs**: Using 10k samples per dataset balances computational feasibility with accuracy; using both linear and RBF kernels captures different similarity aspects but doubles computation
- **Failure signatures**: High variance in similarity consistency distributions indicates poor transferability; low correlations between similarity and performance suggest dataset-specific feature learning
- **First 3 experiments**:
  1. Compute CKA similarities for a small set of model pairs across two datasets to verify the basic pipeline works
  2. Calculate consistency correlation for a single model set across all dataset pairs to check the framework's functionality
  3. Compare linear vs RBF CKA results on the same model pairs to understand the impact of kernel choice

## Open Questions the Paper Calls Out

### Open Question 1
Does the domain-specific training data constrain the solution space more tightly than general-purpose datasets like ImageNet-1k, leading to higher consistency in representational similarities? The paper observes that Places365-trained models show more consistent representational similarities than ImageNet-1k-trained models, suggesting domain-specific training might constrain solutions more tightly. This remains unresolved as the paper only provides observational evidence without empirically testing whether domain-specific training leads to more constrained optimization landscapes.

### Open Question 2
Are self-supervised models' more consistent representational similarities across datasets due to their ability to capture dataset-independent features, or because their representations are further from task-specific behavior compared to image-text and supervised models? The paper identifies SSL models as having the most transferable similarity relationships but proposes two competing explanations without testing whether using intermediate layers from other model families would yield similar consistency patterns.

### Open Question 3
How does the amount of contextual information in datasets affect the relationship between representational similarity and downstream task performance? The paper notes that multi-domain datasets with rich contextual information show weak correlations between similarity and performance, while single-domain and structured datasets with limited context show strong correlations. The paper does not quantitatively measure contextual information or systematically manipulate it to establish causal relationships.

## Limitations

- The study establishes correlational relationships rather than causal mechanisms, leaving underlying reasons for observed patterns speculative
- Reliance on linear probes for performance evaluation may not capture the full capabilities of different model architectures
- The 10k-30k sample sizes per dataset may not fully represent the diversity of larger datasets
- Findings are based on 64 specific models and 26 datasets, limiting generalizability to other model architectures or data distributions

## Confidence

- **High confidence**: The methodological framework for computing representational similarity consistency is sound and well-established. The observation that self-supervised models show more transferable similarities across datasets is robustly supported by the data.
- **Medium confidence**: The interpretation that training objectives fundamentally drive similarity consistency is reasonable but could be confounded by other factors like architectural differences or training data characteristics.
- **Low confidence**: The specific mechanisms by which RBF vs linear kernels capture different aspects of representational structure, and the precise reasons why multi-domain datasets enable multiple successful strategies, require further investigation.

## Next Checks

1. **Architectural ablation study**: Repeat the consistency analysis using matched architectures (same backbone, different objectives) to isolate the effect of training objectives from architectural influences.

2. **Training data diversity analysis**: Examine whether the consistency patterns persist when models are trained on datasets with varying levels of diversity and complexity, controlling for domain overlap.

3. **Alternative performance metrics**: Validate the correlation findings using end-to-end fine-tuning rather than linear probes, and include additional downstream tasks beyond classification to assess whether the dataset structure effects generalize to other task types.