---
ver: rpa2
title: Complex Logical Query Answering by Calibrating Knowledge Graph Completion Models
arxiv_id: '2410.07165'
source_url: https://arxiv.org/abs/2410.07165
tags:
- queries
- query
- fuzzy
- complex
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses complex logical query answering (CLQA) over
  incomplete knowledge graphs by calibrating knowledge graph completion (KGC) models.
  The authors propose CKGC, a method that adapts KGC models to handle CLQA by calibrating
  their prediction values.
---

# Complex Logical Query Answering by Calibrating Knowledge Graph Completion Models

## Quick Facts
- arXiv ID: 2410.07165
- Source URL: https://arxiv.org/abs/2410.07165
- Authors: Changyi Xiao; Yixin Cao
- Reference count: 11
- Primary result: CKGC achieves 6.7% average relative improvement on existential positive first-order queries and 53.9% on negation queries compared to state-of-the-art method

## Executive Summary
This paper addresses complex logical query answering (CLQA) over incomplete knowledge graphs by calibrating knowledge graph completion (KGC) models. The authors propose CKGC, a method that adapts KGC models to handle CLQA by calibrating their prediction values. CKGC uses a simple adaptation function that maps prediction values to the range [0,1], ensuring true facts are close to 1 and false facts are close to 0. The method preserves ranking evaluation metrics of KGC models while improving CLQA performance. Experiments on three benchmark datasets show that CKGC achieves state-of-the-art performance, with an average relative improvement of 6.7% on existential positive first-order queries and 53.9% on negation queries compared to the prior state-of-the-art method.

## Method Summary
CKGC calibrates KGC models to handle complex logical queries by mapping their prediction values to the [0,1] range. The method involves training a KGC model (specifically ComplEx with N3 regularization), normalizing its scoring function using softmax with a query-specific normalization factor, and adapting it to CLQA datasets using a linear transformation. The calibrated KG tensor is created by replacing known true triplet values with 1. This calibrated tensor enables fuzzy set operations for complex logical query answering while preserving the KGC model's ranking performance. The approach uses sparse matrix operations with thresholding to manage memory efficiency during inference.

## Key Results
- CKGC achieves 6.7% average relative improvement on existential positive first-order queries compared to state-of-the-art
- CKGC achieves 53.9% improvement on negation queries, the most challenging query type
- Preserves KGC ranking metrics while improving CLQA performance across all 14 query types
- Reduces memory usage through sparse tensor representation while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CKGC calibrates KGC model predictions to the [0,1] range, improving CLQA accuracy while preserving ranking metrics.
- Mechanism: A simple adaptation function maps scoring function outputs through a linear transformation (W·f(h,r,t)) bounded by 1, preserving ranking but shifting absolute values toward calibration.
- Core assumption: KGC models trained on ranking metrics can still be adapted for calibrated probabilistic reasoning in CLQA without losing ranking performance.
- Evidence anchors:
  - [abstract]: "The core concept of CKGC is to map the values of predictions of KGC models to the range [0, 1], ensuring that values associated with true facts are close to 1, while values linked to false facts are close to 0."
  - [section 4.2]: "Since the adaptation function, linear function, is a monotonically increasing function, the new scoring function f̃(h, r, t) have the same results of KGC ranking metrics as f(h, r, t)."
- Break condition: If the KGC model's scoring function has very narrow or non-monotonic output distributions, the linear adaptation may not provide sufficient separation between true and false facts.

### Mechanism 2
- Claim: Replacing known true triplet scores with 1 in the calibrated KG tensor ensures accurate fuzzy set membership values.
- Mechanism: After normalizing and adapting scores, entries corresponding to training KG facts are clamped to 1, guaranteeing they are treated as fully true in fuzzy set computations.
- Core assumption: Training set facts are reliable ground truth and should be assigned maximal membership in fuzzy set computations.
- Evidence anchors:
  - [section 4.2]: "We replace the values of f̃(h, r, t) with 1 for triplets (h, r, t) ∈ Gtrain ∪ Gvalidation to get the calibrated tensor X."
- Break condition: If the training data contains noisy or incorrect facts, clamping them to 1 could introduce systematic errors in query answering.

### Mechanism 3
- Claim: Precomputing the sparse KG tensor with thresholding reduces runtime complexity while maintaining accuracy.
- Mechanism: Entries below a threshold ε are set to 0, allowing sparse matrix operations and memory efficiency during fuzzy set computations.
- Core assumption: Low-scoring entries in the KGC model's output correspond to highly improbable facts that can be safely ignored without affecting accuracy.
- Evidence anchors:
  - [section 4.1]: "Due to the sparsity of the KG, most entries of X have small values, which can be filtered to 0 by a threshold ε > 0 while maintaining precision."
- Break condition: If the threshold ε is set too high, potentially valid but low-confidence facts may be eliminated, degrading query answering performance.

## Foundational Learning

- Concept: Knowledge Graph Completion (KGC)
  - Why needed here: CKGC builds on pre-trained KGC models as the foundation for complex logical query answering.
  - Quick check question: What is the difference between KGC and CLQA tasks in terms of output format and evaluation metrics?

- Concept: Fuzzy Set Theory and Operations
  - Why needed here: CKGC uses fuzzy set operations (complement, intersection, union, projection) to compute answers for complex logical queries over calibrated KG tensors.
  - Quick check question: How do fuzzy set operations differ from classical set operations when handling membership values between 0 and 1?

- Concept: Calibration of Probabilistic Models
  - Why needed here: KGC models trained with ranking metrics need calibration to produce well-calibrated probabilities suitable for fuzzy reasoning in CLQA.
  - Quick check question: Why is calibration important for converting KGC model scores into meaningful probabilities for complex query answering?

## Architecture Onboarding

- Component map:
  KGC Model (pre-trained) → Calibration Layer (normalization + adaptation) → Calibrated KG Tensor → Fuzzy Set Operations → Query Answer Computation

- Critical path:
  1. Train KGC model on KG data
  2. Normalize scoring function with softmax and scaling
  3. Adapt with linear transformation W
  4. Replace known facts with 1 to create calibrated tensor
  5. Execute fuzzy set operations for query answering

- Design tradeoffs:
  - Memory vs. Accuracy: Lower ε reduces memory usage but may lose some information
  - Simplicity vs. Expressiveness: Linear adaptation is simple but may not capture complex calibration needs
  - Precomputation vs. Flexibility: Precomputing tensor speeds up inference but reduces adaptability to new data

- Failure signatures:
  - Poor performance on negation queries suggests calibration issues with false fact scoring
  - High memory usage indicates ε set too low or model dimension too large
  - Slow convergence during adaptation suggests learning rate or batch size issues

- First 3 experiments:
  1. Compare MRR on CLQA tasks before and after calibration on a small dataset to verify improvement
  2. Vary ε threshold to find optimal balance between memory usage and performance
  3. Test different KGC base models (e.g., DistMult, ComplEx) to assess CKGC's model-agnostic effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following areas remain unexplored:

### Open Question 1
- Question: How does the choice of the KGC model impact the final performance of CKGC on complex logical query answering?
- Basis in paper: [explicit] The paper mentions that CKGC can be incorporated with any KGC model and that it is preferable to select a KGC model with strong performance. Table 7 in the appendix shows results with different KGC models (DistMult, CP, SimplE, ComplEx).
- Why unresolved: While the paper demonstrates that CKGC improves performance with various KGC models, it does not provide a comprehensive comparison of how different KGC models affect the final CKGC performance.
- What evidence would resolve it: A detailed ablation study comparing CKGC performance across a wider range of KGC models, including their strengths and weaknesses, would help determine the optimal KGC model for CKGC.

### Open Question 2
- Question: Can CKGC be extended to handle more complex query types beyond the 14 types mentioned in the paper?
- Basis in paper: [inferred] The paper focuses on 14 types of queries and does not explore the method's scalability to more complex or diverse query structures.
- Why unresolved: The paper does not investigate the limitations of CKGC in handling queries with more intricate logical operations or larger variable scopes.
- What evidence would resolve it: Testing CKGC on a dataset with a broader range of query types, including those with nested quantifiers or more complex logical expressions, would reveal its scalability and potential limitations.

### Open Question 3
- Question: How does the calibration process in CKGC affect the interpretability of the KGC model's predictions?
- Basis in paper: [explicit] The paper emphasizes the importance of calibrating KGC models for CLQA but does not discuss the interpretability of the calibrated predictions.
- Why unresolved: While CKGC improves performance, it is unclear how the calibration process impacts the ability to understand and explain the model's reasoning.
- What evidence would resolve it: An analysis of the calibrated predictions, including visualizations or explanations of how the calibration affects the model's confidence and reasoning process, would provide insights into the interpretability of CKGC.

## Limitations
- Calibration assumption uncertainty: The linear adaptation may not capture complex calibration needs for all KGC model architectures
- Data quality dependency: Performance heavily depends on training data quality, with noisy facts potentially introducing systematic errors
- Threshold sensitivity: Performance is sensitive to the sparsity threshold ε, requiring careful tuning for each dataset

## Confidence
- Calibration Effectiveness Claims: High Confidence
- State-of-the-Art Performance Claims: High Confidence
- Method Generalization Claims: Medium Confidence

## Next Checks
1. Test CKGC with different KGC base models (e.g., DistMult, RotatE, TransE) to verify the method's model-agnostic effectiveness and identify any model-specific limitations in the calibration process.

2. Conduct experiments with intentionally corrupted training data to assess how CKGC's performance degrades when the assumption of clean training facts is violated, particularly focusing on negation query performance.

3. Perform systematic experiments varying ε across multiple orders of magnitude to establish precise guidelines for optimal threshold selection and quantify the trade-off between memory usage and query answering accuracy.