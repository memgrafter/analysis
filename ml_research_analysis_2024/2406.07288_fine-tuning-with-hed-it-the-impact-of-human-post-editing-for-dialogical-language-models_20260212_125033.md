---
ver: rpa2
title: 'Fine-tuning with HED-IT: The impact of human post-editing for dialogical language
  models'
arxiv_id: '2406.07288'
source_url: https://arxiv.org/abs/2406.07288
tags:
- dialogues
- data
- turns
- post-edited
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the impact of human post-editing on machine-generated
  dialogues used for fine-tuning dialogical language models. The authors created HED-IT,
  a dataset of over 16,000 Italian dialogues, pairing original machine-generated versions
  with human-edited counterparts.
---

# Fine-tuning with HED-IT: The impact of human post-editing for dialogical language models

## Quick Facts
- arXiv ID: 2406.07288
- Source URL: https://arxiv.org/abs/2406.07288
- Authors: Daniela Occhipinti; Michele Marchi; Irene Mondella; Huiyuan Lai; Felice Dell'Orletta; Malvina Nissim; Marco Guerini
- Reference count: 30
- Primary result: Human post-editing of machine-generated dialogues improves both data quality and fine-tuned model performance, with greater impact on smaller models

## Executive Summary
This study investigates whether human post-editing of machine-generated dialogues improves the quality of training data for fine-tuning dialogical language models. The authors created HED-IT, a dataset of over 16,000 Italian dialogues, pairing original machine-generated versions with human-edited counterparts. They fine-tuned three sizes of Pythia language models on both original and post-edited dialogues. Results showed that post-edited dialogues were consistently perceived as higher quality than originals by both humans and automatic metrics, and models fine-tuned on post-edited data exhibited improved performance. Importantly, the impact of post-editing was more pronounced for smaller models, with less effect on larger ones.

## Method Summary
The authors created HED-IT by having MSc students post-edit machine-generated Italian dialogues. They fine-tuned Pythia models (1.4B, 6.9B, and 12B parameters) on both original and post-edited dialogue subsets using QLoRA with batch size 4 and gradient accumulation 2. Models were trained for 9-19 epochs depending on size. Evaluation used automatic metrics (HTER, RR, CPPL, Acc@N) and human assessment of understandability and naturalness. The critical comparison was between models trained on original vs post-edited data, measuring both data quality improvement and downstream model performance.

## Key Results
- Post-edited dialogues showed significantly lower HTER scores and higher RR compared to originals
- Models fine-tuned on post-edited data outperformed those on original data across all evaluation metrics
- The performance gap between original and post-edited models decreased with model size (2.462 CPPL difference for 1.4B vs 0.748 for 12B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human post-editing of machine-generated dialogues consistently improves perceived quality compared to original machine-only outputs.
- Mechanism: The human reviewer identifies and corrects unnatural phrasing, redundancy, excessive politeness, and contextual inconsistencies in machine-generated dialogues, resulting in more natural and comprehensible text.
- Core assumption: Human annotators can reliably identify quality issues in machine-generated dialogues and make effective corrections.
- Evidence anchors:
  - [abstract] "Results from both human and automatic evaluation show that the different quality of training data is clearly perceived"
  - [section] "post-edited dialogues were perceived as higher quality than originals by both humans and automatic metrics"
  - [corpus] Weak evidence - corpus shows related work on post-editing but not direct comparison of quality improvement
- Break condition: If post-editing guidelines are too vague or if annotators lack sufficient training, the quality improvement may not materialize consistently.

### Mechanism 2
- Claim: Fine-tuning language models on post-edited dialogues yields improved model outputs compared to fine-tuning on original machine-generated dialogues.
- Mechanism: Models trained on higher-quality data (post-edited dialogues) learn better patterns and generate more natural, less repetitive outputs.
- Core assumption: The quality of training data directly impacts the quality of model outputs.
- Evidence anchors:
  - [abstract] "Models fine-tuned on post-edited data also exhibited improved performance"
  - [section] "Models trained on post-edited dialogues consistently outperform those trained on original dialogues across all Pythia dimensions"
  - [corpus] Weak evidence - corpus neighbors focus on post-editing but not specifically on impact on model fine-tuning
- Break condition: If the post-editing process introduces biases or if the original dialogues already contain sufficient quality, the improvement may be minimal.

### Mechanism 3
- Claim: The impact of post-editing on model performance is more pronounced for smaller language models compared to larger ones.
- Mechanism: Smaller models are more sensitive to training data quality because they have fewer parameters to compensate for poor data, while larger models can better generalize from noisy data.
- Core assumption: Model size affects sensitivity to training data quality.
- Evidence anchors:
  - [abstract] "Importantly, the impact of post-editing was more pronounced for smaller models, with less effect on larger ones"
  - [section] "CPPL difference between models trained on original and post-edited dialogues is lower for the biggest model (a difference of 2.462 for 1.4B vs a difference of 0.748 for 12B)"
  - [corpus] Weak evidence - corpus shows related work on model size but not specifically on size sensitivity to data quality
- Break condition: If the model architecture or training procedure changes, this size-sensitivity relationship may not hold.

## Foundational Learning

- Concept: Automatic metrics for dialogue quality (HTER, RR, CPPL, Acc@N)
  - Why needed here: These metrics provide quantitative measures of dialogue quality before and after post-editing, and model performance after fine-tuning
  - Quick check question: What does a lower HTER score indicate about post-edited dialogue quality?

- Concept: Human evaluation methodologies for dialogue systems
  - Why needed here: Human evaluation provides subjective quality assessments that complement automatic metrics
  - Quick check question: What are the two dimensions used for human evaluation of dialogues in this study?

- Concept: Fine-tuning procedures and their impact on model behavior
  - Why needed here: Understanding how different training data affects model outputs is crucial for interpreting the results
  - Quick check question: What is the key difference between models trained on original vs post-edited dialogues?

## Architecture Onboarding

- Component map:
  - Author module: Generates machine dialogues (either extracted from scripts, rewritten by LLM, or fully generated by LLM)
  - Reviewer module: Human post-editing of machine dialogues
  - Fine-tuning pipeline: Trains language models on either original or post-edited data
  - Evaluation pipeline: Assesses both data quality and model performance

- Critical path:
  1. Generate machine dialogues
  2. Human post-editing
  3. Split data into train/validation/test sets
  4. Fine-tune models on both original and post-edited data
  5. Evaluate data quality (automatic and human)
  6. Evaluate model outputs (automatic and human)

- Design tradeoffs:
  - Quality vs. efficiency: Post-editing improves quality but requires human effort
  - Data size vs. quality: Post-edited dataset is smaller but higher quality
  - Model size vs. data sensitivity: Smaller models benefit more from quality data

- Failure signatures:
  - If post-editing doesn't improve quality metrics, check annotator training and guidelines
  - If models don't show performance differences, verify that data distributions are properly controlled
  - If human evaluation shows bias, check for systematic differences in survey presentation

- First 3 experiments:
  1. Compare HTER and RR scores for original vs post-edited dialogues to verify quality improvement
  2. Fine-tune a small model (1.4B) on both original and post-edited data, compare CPPL scores
  3. Generate outputs from both fine-tuned models, conduct human evaluation on understandability and naturalness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The human evaluation involved only two annotators, which may not capture the full spectrum of quality perception
- The post-editing process required significant human effort (2,796 dialogues edited), raising questions about scalability for larger datasets or less resource-rich languages
- The impact on larger models (12B parameters) was minimal, suggesting diminishing returns for more capable architectures

## Confidence
- **High Confidence:** The quality improvement of post-edited dialogues over originals (supported by both automatic metrics and human evaluation)
- **Medium Confidence:** The differential impact across model sizes (mechanism plausible but sample size limited to three model scales)
- **Medium Confidence:** The automatic metrics' ability to capture human-perceived quality differences (metrics correlate with human judgment but aren't perfect substitutes)

## Next Checks
1. Test the post-editing impact on additional model sizes (e.g., 3B, 20B parameters) to better understand the scaling relationship between model capacity and data quality sensitivity
2. Expand human evaluation to include more annotators and additional quality dimensions (e.g., coherence, informativeness) to strengthen reliability
3. Implement a semi-automated post-editing pipeline using LLM assistance to reduce human effort while maintaining quality gains, then compare results against fully human-edited data