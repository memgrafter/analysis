---
ver: rpa2
title: 'Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis'
arxiv_id: '2411.06770'
source_url: https://arxiv.org/abs/2411.06770
tags:
- learning
- gradient
- adaptive
- noise
- sketching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of communication efficiency in
  federated learning by proposing a family of Sketched Adaptive Federated Learning
  (SAFL) algorithms that combine gradient compression with adaptive optimizers. The
  key insight is leveraging the anisotropic Hessian spectrum of deep learning losses
  to handle entry-wise sketching noise in adaptive methods.
---

# Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis

## Quick Facts
- arXiv ID: 2411.06770
- Source URL: https://arxiv.org/abs/2411.06770
- Authors: Zhijie Chen; Qiaobo Li; Arindam Banerjee
- Reference count: 40
- The paper proposes Sketched Adaptive Federated Learning (SAFL) algorithms that achieve O(1/√T) convergence with sketch size O(log d) by exploiting Hessian eigenspectrum decay.

## Executive Summary
This paper addresses communication efficiency in federated learning by proposing a family of Sketched Adaptive Federated Learning (SAFL) algorithms that combine gradient compression with adaptive optimizers. The key insight leverages the anisotropic Hessian spectrum of deep learning losses to handle entry-wise sketching noise in adaptive methods. The approach achieves dimension-independent convergence bounds by exploiting the sharp decay of Hessian eigenvalues, enabling communication-efficient training while maintaining convergence rates comparable to full-dimensional methods.

## Method Summary
The SAFL framework combines adaptive optimizers (Adam/AMSGRad) with gradient sketching operators (Gaussian, SRHT, CountSketch) in a federated learning setting. Clients perform local SGD updates, apply sketching operators to compress parameter updates, and send compressed updates and L2-norms to the server. The server averages these, applies desketching, and updates the model using an adaptive optimizer. The method eliminates error feedback requirements by using unbiased sketching operators and achieves communication efficiency through dimension-independent convergence guarantees based on Hessian eigenspectrum properties.

## Key Results
- SAFL achieves O(1/√T) asymptotic convergence with sketch size O(log d) in nearly i.i.d. FL settings
- SACFL maintains optimal convergence rates under non-i.i.d. settings with heavy-tailed noise through clipping mechanisms
- Extensive experiments on vision (ResNet, ViT) and language (BERT) tasks show SAFL achieves comparable performance to full-dimensional adaptive methods while being competitive with state-of-the-art communication-efficient algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sharp decay of Hessian eigenvalues enables dimension-independent convergence bounds by suppressing the impact of entry-wise sketching noise.
- Mechanism: In adaptive methods, the preconditioner matrix diag((√ˆvt + ϵ)²) transforms the sketching noise. When the Hessian eigenvalues decay rapidly (many close to zero), the product ⟨(ˆV⁻¹/²ₜmₜ), vⱼ⟩ becomes small for most eigen-directions, limiting the amplification of sketching errors.
- Core assumption: The loss Hessian has a sharply decaying eigenspectrum where the sum of absolute eigenvalues D is bounded and most eigenvalues are near zero.
- Evidence anchors:
  - [section] "Our sharper analysis leverages recent observations regarding the eigenspectrum structure of the loss Hessian in deep learning, which show the eigenvalues to be sharply decaying, with most eigenvalues being close to zero."
  - [section] "Assumption 4. (Hessian Matrix Eigenspectrum) The smoothness of the loss function L, i.e. the largest eigenvalue of the loss Hessian HL is bounded by L, maxi λᵢ ≤ L. The sum of absolute values of HL eigenspectrum is bounded by D, i.e. ∑ᵢ |λᵢ| ≤ D."
  - [corpus] Weak evidence - corpus neighbors don't discuss Hessian eigenspectrum decay.
- Break condition: If the Hessian eigenvalues do not decay sharply (e.g., uniform spectrum), the dimension-dependent bounds would reappear and communication efficiency would be lost.

### Mechanism 2
- Claim: Linearity of sketching operators preserves unbiased gradient estimation in federated settings without requiring error feedback.
- Mechanism: The sketching operators sk and desk satisfy sk(Pₙᵢ₌₁ vᵢ) = ∑ₙᵢ₌₁ sk(vᵢ) and desk(Pₙᵢ₌₁ ¯vᵢ) = ∑ₙᵢ₌₁ desk(¯vᵢ). This linearity ensures that the average of compressed updates across clients equals the compression of the average, maintaining unbiasedness.
- Core assumption: The sketching operators are linear and satisfy Property 2 (unbiased estimation: E[desk(sk(v))] = v).
- Evidence anchors:
  - [section] "Property 1. (Linearity). The compression operators are linear w.r.t the input vectors... Property 2. (Unbiased Estimation). For any vector v ∈ Rd, E[desk(sk(v))] = v."
  - [section] "Our SAFL algorithm adopts unbiased gradient estimators and hence eliminates the needs for error feedbacks."
  - [corpus] Weak evidence - corpus neighbors don't discuss linearity properties of sketching.
- Break condition: If the sketching operators are not linear (e.g., biased sparsification), error feedback would be required, increasing communication rounds.

### Mechanism 3
- Claim: Clipping methods can handle heavy-tailed gradient noise while maintaining convergence under sketched compression.
- Mechanism: The clipping operator min{1, τ/∥∆cₜ∥} scales down large gradients that exceed threshold τ. Under heavy-tailed noise (α-stable distributions with α ∈ (1,2]), this prevents unbounded second moments while the sketching noise remains controlled through bounded vector products.
- Core assumption: Gradient norms follow an α-stable distribution with bounded α-moment (Assumption 5: E[∥∇Lc(x,ξ)∥ᵅ] ≤ Gᵅ).
- Evidence anchors:
  - [section] "Assumption 5. (Bounded α-Moment). There exists a real number α ∈ (1, 2] and a constant G ≥ 0, such that E[∥∇Lc(x, ξ)∥ᵅ] ≤ Gᵅ, ∀c ∈ [C]..."
  - [section] "For the heavy-tailed noise common in data-heterogeneous FL, where non-adaptive methods are not guaranteed to converge, we propose the Sketched Adaptive Clipped Federated Learning (SACFL) which guarantees the boundedness of the second moments."
  - [corpus] Weak evidence - corpus neighbors don't discuss clipping methods or heavy-tailed noise handling.
- Break condition: If α ≤ 1 (infinite first moment) or the heavy-tailed noise is too severe, clipping may not sufficiently bound the moments for convergence.

## Foundational Learning

- Concept: Sub-Gaussian noise concentration
  - Why needed here: The analysis relies on sub-Gaussian concentration to bound stochastic gradient noise and martingale differences from sketching operations.
  - Quick check question: What is the tail probability bound for a σ-sub-Gaussian random variable?
    - Answer: P(|X| ≥ t) ≤ 2exp(-t²/σ²)

- Concept: Eigenspectrum decomposition and quadratic forms
  - Why needed here: The convergence analysis decomposes the Hessian quadratic term using eigenpairs {λᵢ, vᵢ} to exploit the sharp decay and bound the second-order terms.
  - Quick check question: How do you express a quadratic form xᵀHx using the Hessian's eigen-decomposition?
    - Answer: xᵀHx = ∑ᵢ λᵢ⟨x, vᵢ⟩²

- Concept: Martingale concentration inequalities
  - Why needed here: The sketching noise introduces dependencies across iterations, requiring martingale concentration (Azuma's inequality) to bound accumulated errors.
  - Quick check question: What is the form of Azuma's inequality for a sub-Gaussian martingale?
    - Answer: P(|Mₙ - M₀| ≥ t) ≤ 2exp(-t²/(2∑ᵢ cᵢ²)) where |Mᵢ - Mᵢ₋₁| ≤ cᵢ

## Architecture Onboarding

- Component map: Client computes K local SGD steps → sk() operator → Send compressed updates and L2-norm to server → Server averages compressed updates and norms → desk() operator → Adaptive optimizer update → Broadcast updated moments to clients

- Critical path:
  1. Client computes K local SGD steps
  2. Client applies sk() to parameter updates
  3. Client sends sk(Δx) and ∥Δx∥ to server
  4. Server averages received updates and norms
  5. Server applies desk() and updates model with adaptive optimizer
  6. Server broadcasts updated moments to all clients

- Design tradeoffs:
  - Sketch size b vs. convergence rate: Larger b improves convergence but reduces compression benefit
  - Choice of sketching method: Gaussian vs. Count-Sketch vs. SRHT affect constants in concentration bounds
  - Adaptive optimizer selection: AMSGrad provides convergence guarantees vs. Adam's practical performance

- Failure signatures:
  - Divergence: Sketch size too small (b too close to 1) or Hessian eigenvalues don't decay sharply
  - Slow convergence: Sketch dimension b larger than necessary or poor choice of learning rate schedule
  - Poor generalization: Excessive compression (very small b) leading to optimization of unfavorable local minima

- First 3 experiments:
  1. Verify eigenspectrum decay: Compute and plot Hessian eigenvalue distribution on CIFAR-10 with ResNet to confirm sharp decay assumption
  2. Test sketch size sensitivity: Run SAFL with b ∈ {100, 1000, 10000} on CIFAR-10 to find minimum b for convergence
  3. Compare with baselines: Run SAFL vs. FetchSGD vs. 1-bit Adam on CIFAR-10 with ResNet to measure communication-computation tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sketching dimension b scale with respect to the number of clients C in the federated setting, particularly when C is very large?
- Basis in paper: [explicit] The paper discusses the sketching dimension b but does not explicitly explore its scaling with the number of clients C, particularly in the context of non-i.i.d. data distributions.
- Why unresolved: The paper focuses on the dependency of b on the ambient dimension d and the total training rounds T, but does not address how b should be chosen when the number of clients varies significantly.
- What evidence would resolve it: Experiments or theoretical analysis showing the impact of varying C on the required sketching dimension b, particularly in non-i.i.d. settings, would provide insights into the optimal choice of b as a function of C.

### Open Question 2
- Question: Can the theoretical guarantees for SAFL be extended to more complex adaptive optimizers beyond AMSGrad, such as AdamW or AdaBelief?
- Basis in paper: [inferred] The paper uses AMSGrad as the base adaptive optimizer but does not explore whether the theoretical guarantees hold for other adaptive optimizers like AdamW or AdaBelief.
- Why unresolved: The paper provides a theoretical framework for AMSGrad but does not address whether the same convergence guarantees can be extended to other adaptive optimizers that are commonly used in practice.
- What evidence would resolve it: A theoretical extension of the convergence analysis to other adaptive optimizers, along with empirical validation showing their performance under the same sketching framework, would resolve this question.

### Open Question 3
- Question: How does the performance of SAFL and SACFL change when the data distribution across clients is highly non-i.i.d., such as when each client has data from only one class?
- Basis in paper: [explicit] The paper mentions the non-i.i.d. client setting and introduces SACFL for heavy-tailed noise, but does not specifically address scenarios where the data distribution is extremely non-i.i.d., such as each client having data from only one class.
- Why unresolved: The paper focuses on mild noise and heavy-tailed noise settings but does not explore the extreme case of highly non-i.i.d. data distributions, which could significantly impact the performance of SAFL and SACFL.
- What evidence would resolve it: Empirical studies and theoretical analysis showing the performance of SAFL and SACFL under highly non-i.i.d. data distributions, such as each client having data from only one class, would provide insights into their robustness in such scenarios.

## Limitations
- The convergence guarantees critically depend on Assumption 4 regarding sharply decaying Hessian eigenvalues, which may not hold universally across all architectures and tasks
- The heavy-tailed noise analysis (Assumption 5) assumes α ∈ (1,2], which may not capture all practical non-i.i.d. scenarios
- The analysis does not explore how sketching dimension scales with the number of clients in highly non-i.i.d. settings

## Confidence
- **High**: Empirical results showing SAFL achieves comparable performance to full-dimensional adaptive methods across different architectures (ResNet, ViT, BERT)
- **Medium-High**: Theoretical convergence guarantees for i.i.d. settings based on established adaptive method convergence theory
- **Medium**: Theoretical guarantees for non-i.i.d. heavy-tailed cases where clipping introduces additional hyperparameters and assumptions

## Next Checks
1. **Eigenspectrum Verification**: Compute and visualize the Hessian eigenvalue distributions for ResNet, ViT, and BERT architectures on their respective datasets to empirically validate the sharp decay assumption across different model types.

2. **Sketch Size Scaling**: Systematically vary sketch dimensions from O(1) to O(d) on CIFAR-10 and GLUE benchmarks to precisely characterize the minimum sketch size required for convergence and identify any phase transitions in optimization behavior.

3. **Adversarial Heterogeneity Test**: Design synthetic non-i.i.d. distributions with controlled tail behavior (varying α) to stress-test the clipping mechanism and quantify the maximum tolerable heterogeneity for SACFL to maintain convergence.