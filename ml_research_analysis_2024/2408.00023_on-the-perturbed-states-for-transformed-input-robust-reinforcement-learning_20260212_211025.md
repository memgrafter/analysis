---
ver: rpa2
title: On the Perturbed States for Transformed Input-robust Reinforcement Learning
arxiv_id: '2408.00023'
source_url: https://arxiv.org/abs/2408.00023
tags:
- state
- adversarial
- policy
- attacks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for learning robust reinforcement
  learning (RL) agents against adversarial attacks on state observations. The key
  idea is to apply input transformations to the state before feeding it into the policy
  network.
---

# On the Perturbed States for Transformed Input-robust Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.00023
- Source URL: https://arxiv.org/abs/2408.00023
- Reference count: 40
- Key outcome: Vector quantization achieves up to 55% improvement in robustness against state adversaries in MuJoCo continuous control tasks.

## Executive Summary
This paper introduces a novel method for enhancing the robustness of reinforcement learning (RL) agents against adversarial attacks on state observations. The key insight is to apply input transformations to the state before feeding it into the policy network, making the agent more resilient to perturbations. The authors propose two principles for applying transformation-based defenses: autoencoder-styled denoising to reconstruct the original state and bounded transformations (bit-depth reduction and vector quantization) to achieve close transformed inputs. Experiments on multiple MuJoCo environments demonstrate that the proposed input transformation-based defenses can effectively defend against state adversaries, with vector quantization achieving significant improvements in robustness compared to vanilla SAC.

## Method Summary
The method involves applying input transformations to the state before feeding it into the policy network during training and testing. Two principles are introduced: autoencoder-styled denoising to reconstruct the original state from a perturbed observation, and bounded transformations (bit-depth reduction and vector quantization) to achieve close transformed inputs. The authors demonstrate that vector quantization is particularly effective in continuous control tasks, significantly improving the agent's robustness against various adversaries. The proposed method is evaluated using SAC as the base RL algorithm on multiple MuJoCo environments, with experiments showing up to 55% improvement in robustness compared to vanilla SAC.

## Key Results
- Vector quantization (VQ) achieves up to 55% improvement in robustness against state adversaries in MuJoCo continuous control tasks.
- Bounded transformations, such as bit-depth reduction and vector quantization, can effectively defend against state adversaries by reducing the space of attacks.
- Autoencoder-styled denoising can reconstruct the original state from a perturbed observation, but may be vulnerable to white-box attacks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector quantization (VQ) reduces the space of adversarial attacks by discretizing the continuous state space into a finite set of cluster centroids, making it harder for small perturbations to cross decision boundaries.
- Mechanism: VQ maps each state to the nearest centroid in a learned codebook, so a bounded adversary can only perturb within a cluster. If the adversary's budget ϵ is smaller than the distance to the next nearest centroid, the perturbed state remains mapped to the same centroid.
- Core assumption: The codebook size K is large enough to capture essential state information but small enough that clusters are separated by more than 2ϵ.
- Evidence anchors:
  - [abstract] "vector quantization is particularly effective in continuous control tasks, significantly improving the agent's robustness"
  - [section] "VQ can produce a more 'sparse' state space, thus significantly reducing the space of attacks"
  - [corpus] Weak; no direct citations to VQ in RL from the neighbor list, but this is a novel application.
- Break condition: If K is too small (coarse quantization) or too large (dense quantization), clusters become too close and ϵ-radius attacks can cross boundaries.

### Mechanism 2
- Claim: Bounded transformation (bit-depth reduction and VQ) applied during training forces the policy network to learn a robust representation that is invariant to small input changes.
- Mechanism: By quantizing states during training, the policy learns to produce the same action for all states within a quantization bin, effectively smoothing the policy with respect to small perturbations.
- Core assumption: The policy network is Lipschitz continuous and the quantization preserves enough state information for the task.
- Evidence anchors:
  - [abstract] "two principles for applying transformation-based defenses... (2) bounded transformations... to achieve close transformed inputs"
  - [section] "design Tt and Td such that... max˜s∈Bϵ(s) ∥Tt(s) − Td(˜s)∥2 is minimized"
  - [corpus] Weak; neighbor papers focus on other defenses but do not directly test bounded transformations.
- Break condition: If quantization is too aggressive, the policy loses critical information and performance degrades even without attacks.

### Mechanism 3
- Claim: Autoencoder-styled denoising reconstructs the original state from a perturbed observation, effectively removing adversarial noise before the policy processes it.
- Mechanism: An encoder-decoder network is trained on pairs of clean states and their perturbed versions; during testing, the perturbed state is passed through the autoencoder to recover a clean approximation.
- Core assumption: The perturbation budget ϵ is small enough that the autoencoder can learn to invert the corruption; the adversary cannot adapt its attack to fool the denoiser.
- Evidence anchors:
  - [abstract] "(1) autoencoder-styled denoising to reconstruct the original state"
  - [section] "Td needs to reconstruct origin state s from the perturbed state ˜s. This naturally leads to the selection of denoisers for Td."
  - [corpus] Weak; denoising defenses are mentioned in the corpus but not in the RL context.
- Break condition: If the adversary has white-box access to the denoiser and crafts perturbations to maximize reconstruction error, the defense fails.

## Foundational Learning

- Concept: Markov Decision Process (MDP) and State-Adversarial MDP (SA-MDP)
  - Why needed here: The paper frames adversarial attacks in the context of MDP theory, showing how an adversary changes the effective policy by perturbing state observations.
  - Quick check question: In an SA-MDP, what remains unchanged when the adversary perturbs the state? (Answer: the environment's true state, rewards, and actions taken.)

- Concept: Kullback-Leibler (KL) divergence and Total Variation Distance (TVD)
  - Why needed here: The paper uses these to bound the difference in policy behavior between clean and adversarial states, showing that reducing ∥Tt(s) − Td(˜s)∥2 directly bounds the performance gap.
  - Quick check question: If two Gaussian policies have the same variance but different means, how does the KL divergence scale with the mean difference? (Answer: quadratically with the mean difference.)

- Concept: Lipschitz continuity of neural networks
  - Why needed here: The paper assumes the policy network is K-Lipschitz to bound how much the output can change for a bounded input perturbation, which is essential for the theoretical guarantees.
  - Quick check question: If a network is K-Lipschitz, by what factor can its output change when its input changes by δ? (Answer: at most K·δ.)

## Architecture Onboarding

- Component map:
  - Environment: MuJoCo simulators (Walker2d, Hopper, Ant, Inverted Pendulum, Reacher)
  - Agent backbone: SAC with policy πθ and critics Qϕ1, Qϕ2
  - Defense layer: Transformation module T applied to state before πθ
  - Transformations: Bit-depth reduction (BDR), vector quantization (VQ), autoencoder denoiser (AED)
  - Adversary model: State adversary Ψ with ℓ∞ budget ϵ

- Critical path:
  1. Collect state s from environment
  2. Adversary Ψ generates perturbed state ˜s = Ψ(s)
  3. Transformation T(˜s) → ŝ
  4. Policy πθ(ŝ) → action a
  5. Execute a, observe reward r and next state s'
  6. Store transition and train SAC (with T in policy input only)

- Design tradeoffs:
  - BDR vs VQ: BDR is simpler and faster but less robust; VQ is more robust but computationally heavier and may reduce natural performance.
  - Autoencoder denoising: Effective if perturbations are small and denoiser is not attacked; vulnerable to white-box attacks.
  - Training vs testing transforms: Using different transforms (Tt ≠ Td) can balance robustness and natural performance.

- Failure signatures:
  - High variance in returns under attack but not in natural conditions → transformation is not smoothing enough.
  - Large drop in natural performance → transformation is too aggressive (lossy).
  - Training instability → transformation introduces discontinuities or gradients vanish.

- First 3 experiments:
  1. Implement BDR with varying bin width bW on Hopper; measure robustness against Random and Action Diff attacks; plot returns vs ϵ.
  2. Replace BDR with VQ; tune codebook size K; compare robustness and natural performance to BDR baseline.
  3. Train autoencoder denoiser on Min Q-perturbed states from a pretrained SAC; evaluate robustness under white-box Action Diff attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TIRL-VQ scale with increasing state dimensionality?
- Basis in paper: [inferred] The paper mentions computational cost increases with larger codebook sizes, especially in higher-dimensional environments.
- Why unresolved: The paper only tested on low-dimensional MuJoCo environments and did not explore high-dimensional state spaces like images.
- What evidence would resolve it: Experiments applying TIRL-VQ to high-dimensional environments (e.g., Atari games or image-based RL tasks) and analyzing performance and computational efficiency trade-offs.

### Open Question 2
- Question: Can TIRL methods be effectively combined with robust training-based defenses to further enhance RL agent robustness?
- Basis in paper: [explicit] The authors suggest this as a potential direction, stating "Future research can address this challenge by developing more advanced and efficient VQ techniques. Additionally, our proposed TIRL method holds the potential to synergistically complement other robust training-based defenses."
- Why unresolved: The paper only evaluates TIRL as a standalone input transformation method and does not explore combinations with other defense strategies.
- What evidence would resolve it: Experiments combining TIRL with adversarial training or other robustness techniques and comparing performance against individual methods.

### Open Question 3
- Question: What is the theoretical limit of robustness improvement achievable through input transformation-based defenses alone?
- Basis in paper: [explicit] The authors provide theoretical analysis in Proposition 1, showing how input transformations can bound the performance gap between regular and adversarially attacked policies.
- Why unresolved: The theoretical bound depends on the Lipschitz constant of the policy network and the effectiveness of the transformation, which are not fully characterized in the paper.
- What evidence would resolve it: Deriving tighter bounds on the performance gap based on the specific transformation used and the policy network architecture, and validating these bounds empirically across various RL tasks and attack types.

## Limitations
- The theoretical guarantees rely on assumptions of Lipschitz continuity and bounded transformation differences, which may not hold in practice for complex neural network policies.
- The paper focuses on ℓ∞ bounded adversaries and specific attack methods, leaving the robustness against other adversary models and adaptive attacks unexplored.
- The experiments are conducted on a specific set of MuJoCo continuous control tasks using SAC, raising questions about the generalization to other RL domains and state representations.

## Confidence
- **High confidence**: The empirical results demonstrating the effectiveness of VQ in improving robustness against state adversaries in MuJoCo environments.
- **Medium confidence**: The theoretical analysis of the two principles for applying transformation-based defenses, as the assumptions may not hold in practice.
- **Low confidence**: The generalization of the proposed method to other RL domains, adversary models, and state representations.

## Next Checks
1. Evaluate robustness against ℓ2 and adaptive adversaries to assess the performance of the proposed transformations against a wider range of attack scenarios.
2. Test on diverse RL domains and state representations, such as Atari games or image-based observations, to understand the generalizability of the method.
3. Analyze the impact of transformation hyperparameters through a sensitivity analysis to identify the optimal settings for different tasks and adversary strengths.