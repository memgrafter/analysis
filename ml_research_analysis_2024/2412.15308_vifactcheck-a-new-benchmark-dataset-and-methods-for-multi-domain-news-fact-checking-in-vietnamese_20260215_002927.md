---
ver: rpa2
title: 'ViFactCheck: A New Benchmark Dataset and Methods for Multi-domain News Fact-Checking
  in Vietnamese'
arxiv_id: '2412.15308'
source_url: https://arxiv.org/abs/2412.15308
tags:
- dataset
- fact-checking
- evidence
- vietnamese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViFactCheck, the first multi-domain Vietnamese
  news fact-checking benchmark dataset, containing 7,232 human-annotated claim-evidence
  pairs from reputable Vietnamese online news sources across 12 diverse topics. The
  dataset was constructed through a rigorous annotation process involving pilot and
  main annotation phases, followed by self-checking and cross-checking validation,
  achieving a high Fleiss Kappa inter-annotator agreement score of 0.83.
---

# ViFactCheck: A New Benchmark Dataset and Methods for Multi-domain News Fact-Checking in Vietnamese

## Quick Facts
- arXiv ID: 2412.15308
- Source URL: https://arxiv.org/abs/2412.15308
- Reference count: 36
- ViFactCheck introduces the first multi-domain Vietnamese news fact-checking benchmark with 7,232 human-annotated claim-evidence pairs

## Executive Summary
This paper presents ViFactCheck, a groundbreaking multi-domain Vietnamese news fact-checking benchmark dataset containing 7,232 human-annotated claim-evidence pairs from reputable Vietnamese online news sources across 12 diverse topics. The dataset was constructed through a rigorous annotation process involving pilot and main annotation phases, followed by self-checking and cross-checking validation, achieving a high Fleiss Kappa inter-annotator agreement score of 0.83. The study evaluated state-of-the-art pre-trained and large language models using fine-tuning and prompting techniques, with Gemma model demonstrating superior performance with a macro F1 score of 89.90%. The dataset, model checkpoints, fact-checking pipelines, and source code are made freely available on GitHub to promote advances in fact-checking technology for low-resource languages.

## Method Summary
The study constructed ViFactCheck through a systematic data collection process from nine licensed Vietnamese news sources, followed by human annotation using Label Studio interface. Claims were generated to require synthesizing information from multiple evidence sources, creating complex reasoning scenarios. The annotation process involved pilot testing, main annotation, self-checking, and cross-checking phases to ensure quality. Models were evaluated using both fine-tuning and prompting approaches, with fine-tuning consistently outperforming prompting. The dataset was split into training (7,232 samples), development (1,024 samples), and test (2,048 samples) sets in a 7:1:2 ratio.

## Key Results
- ViFactCheck is the first multi-domain Vietnamese news fact-checking benchmark dataset with 7,232 human-annotated claim-evidence pairs
- The dataset achieved a Fleiss Kappa inter-annotator agreement score of 0.83, indicating very high annotation consistency
- Gemma model demonstrated superior performance with a macro F1 score of 89.90% on the test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-curated Vietnamese claims grounded in multiple pieces of evidence improve model reasoning complexity
- Mechanism: By generating claims that require synthesizing information from multiple evidence sources, the dataset forces models to perform multi-step reasoning rather than simple pattern matching
- Core assumption: Models trained on single-evidence claims will not develop the capability to integrate information from diverse sources
- Evidence anchors:
  - [abstract] "annotators are instructed not to limit their claims to single pieces of evidence... they are required to craft intricate claims that amalgamate multiple pieces of evidence"
  - [section 3.2] "The ability to synthesize complex evidence not only enriches the data but also crucially underpins more sophisticated analyses"
- Break condition: If models achieve similar performance on single-evidence vs multi-evidence claims, the complexity benefit is negated

### Mechanism 2
- Claim: Fine-tuning pre-trained language models on domain-specific Vietnamese news data significantly improves fact-checking accuracy
- Mechanism: Adapting general-purpose language models to the specific linguistic patterns and factual structures of Vietnamese news enables better contextual understanding and verification capabilities
- Core assumption: Domain adaptation provides more relevant features than general pre-training for fact-checking tasks
- Evidence anchors:
  - [abstract] "Gemma model demonstrated superior effectiveness, with an impressive macro F1 score of 89.90%"
  - [section 4.3] "Fine-tuning both PLMs and LLMs consistently produces better results than prompting methods"
- Break condition: If prompting methods achieve comparable results without domain-specific fine-tuning

### Mechanism 3
- Claim: Using gold evidence (directly relevant sentences) rather than full context improves model accuracy but reduces real-world applicability
- Mechanism: Focusing computational resources on highly relevant information reduces noise and improves verification precision, though at the cost of generalization
- Core assumption: Models can learn to extract relevant information from full context if properly trained
- Evidence anchors:
  - [section 4.3] "The use of Gold Evidence typically results in higher accuracy scores across models compared to when the Full Context is provided"
  - [section 4.3] "Gold evidence, being directly relevant to the claims, allows models to focus their computational power on a smaller, more pertinent dataset"
- Break condition: If models trained on gold evidence fail to generalize to full context scenarios

## Foundational Learning

- Concept: Inter-annotator agreement measurement (Fleiss Kappa)
  - Why needed here: Ensures dataset quality and reliability through quantitative assessment of annotation consistency
  - Quick check question: What Fleiss Kappa score indicates "very high level of agreement" among annotators?

- Concept: Evidence retrieval and ranking (BM25, SBERT)
  - Why needed here: Determines how effectively models can identify and prioritize relevant supporting information from large text corpora
  - Quick check question: What is the optimal K value (number of retrieved documents) for SBERT in this fact-checking context?

- Concept: Multi-class classification evaluation (macro F1 score)
  - Why needed here: Provides balanced assessment across support, refute, and NEI categories for evaluating model performance
  - Quick check question: Why is macro F1 score preferred over accuracy for this three-class fact-checking task?

## Architecture Onboarding

- Component map:
  Data collection pipeline → Vietnamese news sources (9 licensed sites) → Annotation interface → Label Studio with custom guidelines → Model training framework → PyTorch 2.2.1 + Transformers 4.41.2 → GPU infrastructure → RTX 4090 with 24GB memory → Evaluation metrics → Fleiss Kappa, macro F1, context length analysis

- Critical path: Data collection → Human annotation (pilot + main) → Quality validation → Model training → Performance evaluation

- Design tradeoffs:
  - Multi-evidence claims increase complexity but require more sophisticated reasoning
  - Fine-tuning provides better accuracy than prompting but requires computational resources
  - Gold evidence improves performance but reduces real-world applicability

- Failure signatures:
  - Low Fleiss Kappa scores indicate annotation quality issues
  - Performance plateau with increased training data suggests model capacity limitations
  - Large gap between full context and gold evidence performance indicates context understanding problems

- First 3 experiments:
  1. Compare performance of mBERT vs PhoBERT vs XLM-R on single-evidence claims to establish baseline model effectiveness
  2. Evaluate impact of evidence retrieval (top-K documents) on fact-checking accuracy using SBERT vs BM25
  3. Test multi-evidence vs single-evidence claim handling to measure reasoning complexity requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fact-checking performance change if ViFactCheck incorporated real-time news data rather than news from a single time period?
- Basis in paper: [explicit] The paper notes the dataset was collected from articles published between February and March 2023 to capture "current dynamics of news reporting" and provide a "contemporary snapshot"
- Why unresolved: The study only evaluated models on this static historical dataset. Real-time news would introduce different linguistic patterns, emerging misinformation types, and potentially different evidence availability that could affect model performance
- What evidence would resolve it: Retesting the models on a dynamically updated ViFactCheck dataset incorporating news from multiple time periods, comparing performance metrics across different timeframes

### Open Question 2
- Question: What specific architectural modifications to Gemma would further improve its performance on complex multi-evidence claims?
- Basis in paper: [explicit] The paper notes Gemma's superior performance (89.90% macro F1) but also identifies that "complex inferential chains" remain a challenge, and that "memory networks and knowledge graphs could markedly improve its capacity to process and link extended data sequences"
- Why unresolved: While the paper identifies potential improvements through memory networks and knowledge graphs, it doesn't specify which architectural changes to Gemma itself would be most effective
- What evidence would resolve it: Controlled experiments testing specific Gemma architectural modifications (e.g., enhanced attention mechanisms, modified layer structures) while measuring performance on multi-evidence claims

### Open Question 3
- Question: How does the performance of ViFactCheck-trained models compare when evaluated on fact-checking tasks in other low-resource languages?
- Basis in paper: [inferred] The paper focuses exclusively on Vietnamese fact-checking performance, but mentions that the dataset and methods could "enhance the accuracy of information in low-resource languages" and contribute to "fact-checking technologies can be adapted to different linguistic and cultural contexts"
- Why unresolved: The study doesn't test model transferability to other languages, though it suggests this could be valuable for the field
- What evidence would resolve it: Cross-lingual evaluation of ViFactCheck-trained models on fact-checking datasets in other low-resource languages (e.g., Indonesian, Thai) while measuring performance degradation and identifying transferable features

## Limitations

- The dataset focuses on gold evidence rather than full context retrieval, limiting real-world applicability
- Human annotation process introduces potential bias in what constitutes "relevant" information
- The study is limited to Vietnamese language, restricting generalizability to other low-resource languages

## Confidence

**High Confidence:**
- ViFactCheck represents the first multi-domain Vietnamese news fact-checking benchmark dataset
- Human annotation process achieved strong inter-annotator agreement (Fleiss Kappa = 0.83)
- Gemma model demonstrates superior performance with macro F1 score of 89.90%

**Medium Confidence:**
- Fine-tuning pre-trained models consistently outperforms prompting methods
- Multi-evidence claims require more sophisticated reasoning than single-evidence claims
- Gold evidence improves model accuracy compared to full context

**Low Confidence:**
- The specific combination of Vietnamese-specific and multilingual models represents the optimal architecture
- The 7:1:2 train-dev-test split is optimal for this dataset
- Results generalize to other low-resource languages

## Next Checks

1. **Real-world deployment testing**: Evaluate model performance on full context retrieval rather than gold evidence to assess practical applicability in real news verification scenarios.

2. **Cross-lingual generalization**: Test whether models trained on ViFactCheck can effectively transfer to fact-checking tasks in other low-resource languages with similar linguistic properties.

3. **Annotation consistency validation**: Conduct additional rounds of cross-checking with independent annotators to verify that the Fleiss Kappa score of 0.83 is maintained across different subsets of the dataset and different claim types.