---
ver: rpa2
title: 'The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint
  Incremental Learning of Homogeneous Tasks'
arxiv_id: '2406.05114'
source_url: https://arxiv.org/abs/2406.05114
tags:
- stability
- learning
- training
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability gap phenomenon in continual
  learning, demonstrating its presence even in the simplest setting of joint incremental
  learning of homogeneous tasks. The authors show that despite having access to all
  previous data, a significant performance drop occurs when transitioning to a new
  task.
---

# The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint Incremental Learning of Homogeneous Tasks

## Quick Facts
- arXiv ID: 2406.05114
- Source URL: https://arxiv.org/abs/2406.05114
- Authors: Sandesh Kamath; Albin Soutif-Cormerais; Joost van de Weijer; Bogdan Raducanu
- Reference count: 20
- Key outcome: Stability gap occurs even in joint incremental learning of homogeneous tasks with full data access, caused by SGD's failure to follow low-loss linear paths between task optima

## Executive Summary
This paper investigates the stability gap phenomenon in continual learning, demonstrating its presence even in the simplest setting of joint incremental learning of homogeneous tasks. The authors show that despite having access to all previous data, a significant performance drop occurs when transitioning to a new task. Through analysis, they reveal that a low-loss linear path exists between optima of consecutive tasks, but SGD optimization fails to follow this path, instead traversing high-loss regions. Additionally, the authors perform a detailed batch-wise analysis, discovering that while SGD updates decrease mini-batch loss, they paradoxically increase overall test loss immediately after task boundaries.

## Method Summary
The authors conduct experiments on CIFAR-10 and CIFAR-100 datasets using VGG-16 and ResNet-18 architectures. They implement joint incremental learning where a model is trained on one task, then continues training on a second task while having access to all previous data. The training uses standard SGD with lr=0.01, momentum=0.9, and batch size=64. Key analysis methods include linear interpolation between checkpoints to verify mode connectivity, and batch-wise monitoring of accuracy to track optimization dynamics at task boundaries.

## Key Results
- Stability gap occurs even in joint incremental learning of homogeneous tasks where both tasks share the same data distribution
- A low-loss linear path exists between optima of consecutive tasks, but SGD optimization fails to follow this path
- Mini-batch gradient updates after task boundaries decrease local loss but paradoxically increase overall test loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD optimization fails to follow the low-loss linear path between task optima, instead traversing high-loss regions.
- Mechanism: When transitioning to a new task, the optimal parameters for the previous task and the new task lie on a linear low-loss path, but SGD optimization does not navigate along this path, causing temporary performance drops.
- Core assumption: There exists a low-loss linear path connecting the optimal parameters of consecutive tasks in joint incremental learning.
- Evidence anchors:
  - [abstract]: "through analysis, they reveal that a low-loss linear path exists between optima of consecutive tasks, but SGD optimization fails to follow this path, instead traversing high-loss regions"
  - [section]: "we show that there exists a linear low-loss path to the optimal loss, but that SGD is not following this path"
  - [corpus]: Weak - no direct corpus evidence supporting the existence of a linear low-loss path
- Break condition: If the assumption about linear connectivity between task optima is invalid for the given architecture or dataset distribution.

### Mechanism 2
- Claim: Mini-batch gradient updates after task boundaries decrease local loss but increase overall test loss.
- Mechanism: After task transitions, SGD updates successfully reduce mini-batch loss but paradoxically increase overall test loss, indicating that the optimization moves parameters away from optimal task performance.
- Core assumption: The immediate mini-batch loss reduction does not correlate with improved generalization performance.
- Evidence anchors:
  - [abstract]: "they perform a detailed batch-wise analysis, discovering that while SGD updates decrease mini-batch loss, they paradoxically increase overall test loss immediately after task boundaries"
  - [section]: "we observe that the SGD update results in a loss decrease (or accuracy increase) for the particular mini-batch... However, when we look at the test accuracy... we see that even though initial steps lead to a lower loss on the mini-batch, they do not result in better test performance"
  - [corpus]: Weak - no corpus evidence about batch-wise loss behavior during task transitions
- Break condition: If the batch-wise analysis shows consistent improvement in both mini-batch and test loss after task boundaries.

### Mechanism 3
- Claim: Stability gap occurs even in the simplest continual learning setting with homogeneous tasks and full data access.
- Mechanism: The stability gap phenomenon manifests even when training on homogeneous tasks with access to all previous data, indicating it is a fundamental characteristic of SGD optimization in continual learning scenarios.
- Core assumption: The stability gap is not caused by catastrophic forgetting or heterogeneous task distributions.
- Evidence anchors:
  - [abstract]: "we show that the stability gap also occurs when applying joint incremental training of homogeneous tasks... the learner continues training on the same data distribution and has access to all data from previous tasks"
  - [section]: "we show that the stability gap also occurs during joint incremental learning from homogeneous tasks... even in the case that both tasks have the same distribution, SGD optimization does not succeed in going to the 'nearby' optimal position"
  - [corpus]: Moderate - related work on stability gap in different settings, but no direct evidence for homogeneous tasks
- Break condition: If experiments with other architectures or dataset splits do not show the stability gap in homogeneous settings.

## Foundational Learning

- Concept: Linear mode connectivity in neural networks
  - Why needed here: Understanding whether optimal parameters for different tasks lie on a low-loss linear path is crucial for explaining why SGD fails to navigate effectively between task optima
  - Quick check question: Can you explain what it means for two network parameter sets to be "linearly mode connected" and why this property matters for optimization?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Distinguishing between stability gap and catastrophic forgetting helps identify the specific optimization challenge being addressed
  - Quick check question: How does the stability gap differ from catastrophic forgetting in terms of when performance drops occur and whether data from previous tasks is accessible?

- Concept: Mini-batch gradient descent dynamics
  - Why needed here: Understanding how SGD updates affect both local mini-batch loss and global test performance is essential for interpreting the batch-wise analysis results
  - Quick check question: Why might an SGD update that decreases mini-batch loss still result in increased test loss for the overall task?

## Architecture Onboarding

- Component map: CIFAR-10/CIFAR-100 datasets -> ResNet-18/VGG-16 models -> SGD optimizer -> Joint incremental learning pipeline -> Linear interpolation analysis -> Batch-wise accuracy monitoring

- Critical path:
  1. Train on initial task A until convergence
  2. Switch to task B while monitoring test accuracy
  3. Track mini-batch accuracy before and after SGD updates
  4. Perform linear interpolation between initial and final checkpoints
  5. Analyze loss along SGD path vs linear path

- Design tradeoffs:
  - Homogeneous vs heterogeneous tasks: The paper focuses on homogeneous tasks to isolate the stability gap from task distribution effects
  - Joint incremental vs incremental learning: Joint incremental learning provides full data access but still exhibits stability gap, suggesting it's not a data availability issue
  - Architecture choice: ResNet-18 and VGG-16 are chosen for their widespread use, but the phenomenon may vary with architecture depth/complexity

- Failure signatures:
  - Temporary performance drop at task boundaries despite full data access
  - Discrepancy between mini-batch loss reduction and test loss increase
  - SGD path deviating from linear low-loss path between task optima

- First 3 experiments:
  1. Replicate the 50-50* setting on CIFAR-10 with ResNet-18 to verify stability gap occurrence
  2. Perform linear interpolation between checkpoints to verify existence of low-loss linear path
  3. Conduct batch-wise analysis to confirm mini-batch loss reduction vs test loss increase pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanism causes the stability gap in joint incremental learning of homogeneous tasks?
- Basis in paper: [explicit] The paper shows that the stability gap occurs even in joint incremental learning of homogeneous tasks, where the same data distribution is used, and all previous data is available.
- Why unresolved: The paper does not provide a new explanation for the stability gap but suggests that the focus should shift to how to optimize rather than what to optimize.
- What evidence would resolve it: Experimental results demonstrating a method that effectively eliminates the stability gap in joint incremental learning of homogeneous tasks, along with an analysis of why this method works.

### Open Question 2
- Question: How can the SGD optimization process be modified to follow the low-loss linear path between optima of consecutive tasks?
- Basis in paper: [explicit] The paper shows that a low-loss linear path exists between optima of consecutive tasks, but SGD optimization does not choose this path, instead traversing high-loss regions.
- Why unresolved: The paper does not provide a solution for modifying SGD to follow the low-loss path.
- What evidence would resolve it: Development and experimental validation of an optimization algorithm that consistently follows the low-loss linear path between task optima, along with an analysis of its effectiveness in eliminating the stability gap.

### Open Question 3
- Question: What is the relationship between the size of the initial task and the magnitude of the stability gap?
- Basis in paper: [explicit] The paper shows that the stability gap increases for a smaller-sized first task, with larger gaps observed when starting from a smaller initial task.
- Why unresolved: The paper does not provide a detailed analysis of why the size of the initial task affects the magnitude of the stability gap.
- What evidence would resolve it: A comprehensive study varying the size of the initial task and measuring the resulting stability gap, along with an analysis of the underlying factors contributing to this relationship.

## Limitations

- Limited architectural validation: The linear mode connectivity assumption is only tested on ResNet-18 with CIFAR-10
- Lack of statistical rigor: Batch-wise analysis relies on visual inspection rather than quantitative significance testing
- No solution proposed: The paper identifies the problem but does not offer concrete methods to mitigate the stability gap

## Confidence

- High Confidence: The observation that performance drops occur at task boundaries even with full data access
- Medium Confidence: The existence of a low-loss linear path between task optima
- Low Confidence: The claim that SGD specifically fails to follow this path due to optimization dynamics

## Next Checks

1. Test linear mode connectivity hypothesis across diverse architectures (CNNs, transformers) and datasets to establish generalizability
2. Conduct ablation studies varying batch size, learning rate, and momentum to determine which hyperparameters most affect SGD's ability to follow the linear path
3. Implement and compare alternative optimization strategies (e.g., SWA, Adam) to test whether the stability gap can be mitigated through different optimization approaches