---
ver: rpa2
title: The Impact of Inference Acceleration on Bias of LLMs
arxiv_id: '2410.22118'
source_url: https://arxiv.org/abs/2410.22118
tags:
- bias
- strategies
- inference
- acceleration
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Inference acceleration strategies like quantization and pruning
  reduce LLM inference cost but can unpredictably alter demographic bias. Across six
  bias metrics and five acceleration methods, bias changes were inconsistent: some
  strategies reduced bias in certain models but increased it in others.'
---

# The Impact of Inference Acceleration on Bias of LLMs

## Quick Facts
- arXiv ID: 2410.22118
- Source URL: https://arxiv.org/abs/2410.22118
- Reference count: 31
- Primary result: Inference acceleration strategies can unpredictably alter demographic bias in LLMs, requiring case-by-case evaluation.

## Executive Summary
This study systematically evaluates how inference acceleration strategies—specifically quantization and pruning—affect demographic bias in large language models. Across six bias metrics and five acceleration methods applied to three models, the research finds that bias changes are highly inconsistent: some strategies reduce bias in certain models while increasing it in others. The effects are model-dependent and unpredictable, with no universal patterns emerging. The findings highlight the need for careful bias evaluation when deploying accelerated models, as efficiency gains may come with unintended fairness implications.

## Method Summary
The study applies five inference acceleration strategies (INT4/8 quantization, AWQ quantization, KV cache quantization, unstructured and structured pruning) to three LLMs (LLaMA-2-7B, LLaMA-3.1-8B, Mistral-7B-v0.3). Model outputs are generated using standardized prompts from six bias evaluation datasets and evaluated with corresponding bias metrics. Bias scores are compared between base and accelerated models to assess changes in stereotype agreeability, representation, and other bias dimensions.

## Key Results
- Inference acceleration strategies cause unpredictable shifts in model bias, with no consistent patterns across models or bias types.
- AWQ quantization notably increased stereotyping bias across models.
- Structured pruning reduced bias scores but often degraded text coherence and fluency.
- KV cache quantization was the most stable strategy, showing minimal bias changes across models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inference acceleration strategies alter the probability distribution of generated tokens, leading to unpredictable shifts in model bias.
- Mechanism: Quantization and pruning modify the model's weights and activations, which in turn affect the softmax probabilities assigned to different demographic groups during text generation.
- Core assumption: Changes in model parameters due to acceleration strategies directly impact the generation of biased outputs.
- Evidence anchors:
  - [abstract] "Analysis of outputs before and after inference acceleration shows significant change in bias."
  - [section 5] "Inference acceleration strategies can have significant, though sometimes subtle, impacts on bias in LLMs."
  - [corpus] Weak evidence; corpus focuses on acceleration strategies but not their bias impacts.

### Mechanism 2
- Claim: Structured pruning can reduce bias scores but at the cost of text coherence and fluency.
- Mechanism: Pruning removes weights based on magnitude and activation norms, which can inadvertently eliminate pathways that contribute to biased but coherent text generation.
- Core assumption: There is a trade-off between reducing bias and maintaining text quality when applying structured pruning.
- Evidence anchors:
  - [section 5] "structured pruning consistently achieved the lowest bias score across models, followed closely by KV cache quantization. However, structured pruning exhibited certain drawbacks... it can cause the model to fail to perform the task, follow instructions, or produce nonsensical, repetitive outputs."
  - [corpus] No direct evidence; corpus does not address pruning's impact on text coherence.

### Mechanism 3
- Claim: The impact of inference acceleration strategies on bias is model-dependent and unpredictable.
- Mechanism: Different model architectures and training data compositions interact uniquely with acceleration strategies, leading to varied bias outcomes.
- Core assumption: Each model's baseline bias and architectural nuances determine how it responds to acceleration strategies.
- Evidence anchors:
  - [section 5] "The effects of inference acceleration strategies on stereotype agreeability vary markedly across models... The impact of a single acceleration strategy does not remain consistent across different models."
  - [corpus] Weak evidence; corpus does not provide specific insights into model-dependent bias changes.

## Foundational Learning

- Concept: Bias measurement in LLMs
  - Why needed here: Understanding how bias is quantified is crucial for interpreting the impact of acceleration strategies on model outputs.
  - Quick check question: What are the differences between embedding-based, probability-based, and generated text-based bias metrics?

- Concept: Model quantization and pruning
  - Why needed here: Knowledge of these acceleration techniques is essential to grasp how they modify model parameters and potentially affect bias.
  - Quick check question: How do 4-bit and 8-bit quantization differ in terms of their impact on model performance and bias?

- Concept: Token generation and probability distributions
  - Why needed here: Understanding how models generate text through probability distributions helps explain how changes in weights can lead to biased outputs.
  - Quick check question: How does altering model weights affect the softmax probabilities of different tokens during generation?

## Architecture Onboarding

- Component map:
  Input prompts → Model (base/accelerated) → Generated text → Bias evaluation metrics
  Acceleration strategies: Quantization (AWQ, INT4/8), Pruning (structured, unstructured), KV cache quantization
  Bias metrics: CrowSPairs, GlobalOpinionQA, WorldBench, DT-Stereotyping, DiscrimEval, DiscrimEvalGen

- Critical path:
  1. Load base model and apply chosen acceleration strategy.
  2. Generate outputs using standardized prompts across bias evaluation datasets.
  3. Compute bias scores using appropriate metrics for each dataset.
  4. Compare bias scores between base and accelerated models.

- Design tradeoffs:
  - Balancing inference efficiency with potential increases in model bias.
  - Choosing between different acceleration strategies based on their impact on specific bias types.
  - Deciding on the appropriate bit-width for quantization to optimize performance without significant bias increase.

- Failure signatures:
  - Significant increase in bias scores after applying certain acceleration strategies (e.g., AWQ quantization).
  - Model refusing to respond or generating nonsensical text, especially with structured pruning.
  - Inconsistent bias effects across different models and datasets.

- First 3 experiments:
  1. Apply AWQ quantization to LLaMA-2 and measure changes in CrowSPairs and DT-Stereotyping bias scores.
  2. Compare the effects of structured vs. unstructured pruning on Mistral-7B's performance in DiscrimEvalGen.
  3. Evaluate the impact of KV cache quantization (4-bit vs. 8-bit) on LLaMA-3.1's bias scores across all metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the biases in LLMs change when using different instruction templates or chat formats?
- Basis in paper: [explicit] The paper mentions that results were consistent with and without instruction templates, but some cases showed increased refusal rates or better task understanding with templates.
- Why unresolved: The study only tested one set of developer-provided instruction templates. It's unclear how other templates or formats would affect bias.
- What evidence would resolve it: Testing multiple instruction templates or chat formats across the same models and bias metrics to compare results.

### Open Question 2
- Question: How do different combinations of inference acceleration strategies (e.g., quantization + pruning) affect LLM bias compared to individual strategies?
- Basis in paper: [inferred] The paper tested individual strategies but did not explore hybrid approaches or combinations of multiple strategies.
- Why unresolved: The study focused on single-strategy effects, leaving the potential interactions between strategies unexplored.
- What evidence would resolve it: Experimenting with hybrid acceleration strategies (e.g., quantization + pruning) and measuring their impact on bias across multiple models and metrics.

### Open Question 3
- Question: How do inference acceleration strategies affect bias in LLMs across different domains or specialized tasks?
- Basis in paper: [inferred] The study used general bias metrics but did not explore domain-specific or task-specific biases.
- Why unresolved: The bias metrics used were broad and did not account for domain-specific nuances or specialized applications of LLMs.
- What evidence would resolve it: Evaluating bias in domain-specific datasets or tasks (e.g., healthcare, legal, or financial applications) after applying inference acceleration strategies.

## Limitations
- The bias changes caused by acceleration strategies are highly model-specific and metric-dependent, with no consistent patterns.
- Structured pruning reduces bias but significantly degrades text coherence and fluency, introducing a quality tradeoff.
- The study only examined three models and six bias metrics, which may not capture the full spectrum of possible bias outcomes.

## Confidence
- **High confidence**: AWQ quantization tends to increase stereotyping bias, as this was consistently observed across models.
- **Medium confidence**: Structured pruning reduces bias scores, but the tradeoff with text coherence is not fully quantified.
- **Medium confidence**: The model-dependent unpredictability of bias changes, given the limited model and metric diversity tested.

## Next Checks
1. **Validate pruning hyperparameters**: Conduct experiments with varying sparsity ratios and pruning iterations for structured pruning to determine if there exists an optimal configuration that minimizes bias without significantly degrading text coherence.

2. **Expand model and metric diversity**: Test the same acceleration strategies on additional LLM architectures (e.g., GPT-3.5, Mixtral) and incorporate more bias metrics to assess whether the observed unpredictability holds across a broader range of models and bias types.

3. **Longitudinal bias analysis**: Evaluate the stability of bias changes over time by applying inference acceleration to models that have been fine-tuned on diverse datasets, and measure whether the bias impacts persist or evolve with further training.