---
ver: rpa2
title: Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating
  Real and Synthetic Data
arxiv_id: '2404.01413'
source_url: https://arxiv.org/abs/2404.01413
tags:
- data
- test
- iteration
- collapse
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates model collapse in generative models trained
  on their own outputs. The authors compare two settings: replacing previous data
  with new synthetic data versus accumulating both real and synthetic data over time.'
---

# Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data

## Quick Facts
- **arXiv ID**: 2404.01413
- **Source URL**: https://arxiv.org/abs/2404.01413
- **Reference count**: 40
- **Primary result**: Accumulating real and synthetic data prevents model collapse across multiple model types, while replacing data causes collapse.

## Executive Summary
This paper investigates model collapse in generative models trained recursively on their own outputs. The authors compare two training strategies: replacing previous data with new synthetic data versus accumulating both real and synthetic data over time. Through extensive experiments with language models, diffusion models, and VAEs on multiple datasets, they demonstrate that accumulating data prevents model collapse while replacement leads to catastrophic performance degradation. Theoretical analysis of linear models shows that replacing data causes test error to grow linearly with iterations, while accumulating data bounds test error at a constant value. The results suggest a practical approach to prevent model collapse in recursive training scenarios.

## Method Summary
The study compares two data management strategies across multiple model types and datasets. Models are first pretrained on real data, then used to generate synthetic data which is either accumulated alongside the original data or replaces it entirely. This process is repeated over multiple iterations, with performance evaluated at each stage. The experiments cover GPT-2 and Llama2 language models on the TinyStories dataset, diffusion models on molecular conformations (GEOM-Drugs), and VAEs on face images (CelebA). Theoretical analysis using linear regression models provides mathematical justification for the empirical findings.

## Key Results
- Accumulating synthetic data alongside real data prevents model collapse across all tested model types and data modalities
- Replacing previous data with new synthetic generations causes test error to grow linearly with iterations
- Theoretical analysis shows that accumulating data bounds test error at a constant value, independent of iteration count
- Even with accumulation, diversity decreases over generations but remains above collapse thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulating real and synthetic data together prevents model collapse by keeping the model exposed to diverse original data distributions
- Mechanism: As new synthetic data is generated and added to the training set, the proportion of original real data decreases over time. However, because real data is never fully replaced, the model maintains a connection to the true data distribution. The accumulated data set becomes increasingly diverse as each generation adds new variations.
- Core assumption: Real data maintains statistical properties that synthetic data cannot fully replicate, even after many generations of recursion
- Evidence anchors:
  - [abstract] "we confirm that replacing the original real data by each generation's synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse"
  - [section 2] "accumulating synthetic data alongside real data avoids model collapse for all models and for all data modalities we test"
- Break condition: If synthetic data generation becomes so poor that it introduces systematic biases or mode collapse that overwhelm the signal from real data, the protective effect of accumulation could fail

### Mechanism 2
- Claim: The linear regression analysis shows that accumulating data creates a convergent test error bound due to the diminishing influence of later-generated synthetic data
- Mechanism: In the linear model framework, each new iteration's contribution to the training set is diluted by the accumulated data from previous iterations. The noise introduced by each synthetic generation is weighted by 1/i in the training set and by 1/i² in the squared error calculation, creating a convergent series.
- Core assumption: The noise added by synthetic data generation is independent across iterations and has bounded variance
- Evidence anchors:
  - [section 3.2] "if data instead accumulate across model-fitting iterations, then the test squared error is upper bounded by a relatively small constant, meaning model collapse is avoided"
  - [section 3.2] "the test error has a finite and (to us, surprisingly) well-controlled upper bound independent of the number of model-fitting iterations"
- Break condition: If the noise variance grows with each iteration or if there are systematic correlations between successive generations of synthetic data, the convergence properties could break down

### Mechanism 3
- Claim: The accumulation strategy preserves model diversity and prevents mode collapse by maintaining exposure to the full data manifold across generations
- Mechanism: When data is replaced, each generation trains only on the previous generation's output, which can lead to rapid mode collapse as the model loses track of the full data distribution. Accumulation keeps earlier data points in the training set, preserving the full diversity of the data manifold.
- Core assumption: The original real data contains sufficient diversity to anchor the model to the full data manifold, and this diversity is preserved in the accumulated set
- Evidence anchors:
  - [section 2.3] "Accumulating data at each iteration significantly slows model collapse: the test error increases significantly slower with each additional iteration. While the diversity of generations does go down... it still represents major axes of variation in the dataset"
  - [section 2.3] "the test error of accumulating data does increase with the number of iterations (albeit much more slowly than with replacing data)"
- Break condition: If the original data diversity is insufficient or if synthetic generations systematically remove certain modes from the accumulated data, mode collapse could still occur

## Foundational Learning

- Concept: Model collapse and its relationship to recursive training
  - Why needed here: Understanding what causes model collapse is essential to appreciate why accumulation prevents it
  - Quick check question: What is the key difference between the "replace" and "accumulate" strategies that prevents model collapse?

- Concept: Statistical learning theory and generalization bounds
  - Why needed here: The theoretical analysis relies on understanding how test error behaves with different training data strategies
  - Quick check question: Why does the test error grow linearly with iterations when data is replaced but remain bounded when data accumulates?

- Concept: Diffusion models and variational autoencoders
  - Why needed here: The paper tests the accumulation strategy across multiple model architectures
  - Quick check question: How might the accumulation strategy affect the latent space learning in VAEs compared to diffusion models?

## Architecture Onboarding

- Component map:
  - Data generation pipeline -> Data management system -> Model training pipeline -> Evaluation framework

- Critical path:
  1. Generate synthetic data from current model
  2. Add synthetic data to training set (accumulate) or replace old data
  3. Train new model on updated training set
  4. Evaluate performance and repeat

- Design tradeoffs:
  - Memory vs. performance: Accumulation requires more storage but prevents collapse
  - Training time: Larger accumulated datasets take longer to train on
  - Generation quality: Poor synthetic generation can still cause issues even with accumulation

- Failure signatures:
  - Increasing test error over iterations (model collapse)
  - Loss of diversity in generated samples
  - Mode collapse where generated data represents only a subset of the true distribution

- First 3 experiments:
  1. Implement linear regression framework with accumulation vs replacement to verify theoretical predictions
  2. Test accumulation strategy with a simple VAE on a small image dataset
  3. Scale up to larger models (e.g., GPT-2) on text data with controlled generation temperatures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions and why do discrepancies exist between studies showing fast performance deterioration with accumulating data versus those showing it avoids model collapse?
- Basis in paper: [inferred] The paper notes conflicting results from Martínez et al. (2023a) showing fast deterioration even with accumulating data on a smaller dataset, versus their own findings of avoiding collapse. They state "Understanding under what conditions and why these discrepancies exist is an interesting direction we leave for future research."
- Why unresolved: The paper acknowledges the discrepancy but does not investigate the underlying causes or conditions that lead to different outcomes between studies.
- What evidence would resolve it: Comparative experiments varying dataset size, model architecture, sampling temperature, and training regime to identify which factors cause deterioration vs. avoidance of collapse in accumulating data scenarios.

### Open Question 2
- Question: How does model collapse behavior change when synthetic data is generated deterministically (e.g., temperature 0) rather than stochastically as in most experiments?
- Basis in paper: [explicit] The paper states "it is worth noting that in all our experiments, the synthetic dataset is generated by sampling from the previous model, i.e., with some stochasticity; in future work, we would like to explore also what happens if data is generated deterministically, e.g. with temperature 0 in a typical language model."
- Why unresolved: All experiments used stochastic generation, so deterministic generation remains unexplored despite its relevance to real-world deployment where models may be used to generate data without randomness.
- What evidence would resolve it: Experiments comparing model collapse progression with deterministic vs. stochastic generation across multiple model types and datasets to determine if determinism accelerates, slows, or has no effect on collapse.

### Open Question 3
- Question: What is the optimal schedule for mixing real and synthetic data accumulation to maximize model performance while minimizing computational cost?
- Basis in paper: [inferred] The paper discusses various data accumulation regimes but does not explore optimal mixing schedules. They mention future work exploring "different schedules of how much synthetic data is generated at each iteration."
- Why unresolved: The paper only considers equal accumulation of real and synthetic data at each iteration, not exploring whether varying the ratio or schedule could yield better results.
- What evidence would resolve it: Experiments systematically varying the ratio of real to synthetic data at each iteration, the frequency of real data injection, and the total amount of synthetic data generated to find schedules that maintain performance while reducing computational requirements.

## Limitations
- The accumulation strategy requires increasing storage capacity over time, which could become impractical at scale
- Findings are based on relatively small-scale models and may not directly translate to large production systems
- The paper doesn't address computational costs of training on increasingly large accumulated datasets

## Confidence
- **High confidence**: The linear regression theoretical analysis showing bounded error with accumulation (Section 3.2)
- **Medium confidence**: Empirical results across different model types showing accumulation prevents collapse (Section 2)
- **Medium confidence**: The claim that accumulation preserves diversity while replacement causes mode collapse (Section 2.3)

## Next Checks
1. Test accumulation strategy with much larger models (e.g., 1B+ parameters) to verify scalability
2. Implement storage-efficient variants of accumulation (e.g., reservoir sampling) to address memory concerns
3. Evaluate performance in scenarios where synthetic data quality degrades over generations