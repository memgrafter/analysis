---
ver: rpa2
title: 'S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory
  GPUs'
arxiv_id: '2405.20314'
source_url: https://arxiv.org/abs/2405.20314
tags:
- decoding
- arxiv
- tokens
- preprint
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes S3D, a simple and cost-effective self-speculative
  decoding scheme for low-memory GPUs. The method combines simultaneous multi-token
  prediction with mid-layer skipping in the target model to create an efficient draft
  model.
---

# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs

## Quick Facts
- arXiv ID: 2405.20314
- Source URL: https://arxiv.org/abs/2405.20314
- Authors: Wei Zhong; Manasa Bharadwaj
- Reference count: 40
- One-line primary result: Achieves optimal performance-memory ratios among recent open-source speculative decoding systems while requiring minimal architecture changes and training data

## Executive Summary
This paper introduces S3D, a self-speculative decoding scheme designed specifically for low-memory GPUs. The method combines mid-layer skipping with simultaneous multi-token prediction to create an efficient draft model that requires no additional VRAM costs while maintaining high training efficiency. By using lower and upper layers from the target model while skipping middle layers, S3D reduces parameter count while preserving access to both early-layer features and late-layer representations. The authors demonstrate that their approach achieves 1.4 to 2 times faster decoding speeds than quantized EAGLE on constrained GPUs while using less VRAM.

## Method Summary
S3D is a self-speculative decoding scheme that creates a draft model by extracting lower and upper layers from a target model while skipping middle layers. The draft model predicts multiple tokens simultaneously by inserting mask tokens into the input sequence. During training, the model learns to decode both the next token and following masked tokens in a single forward pass. The scheme operates in half-precision to avoid quantization overhead and uses symmetric layer skipping to maintain optimal acceptance rates. The authors fine-tune on ShareGPT data and evaluate on multiple benchmarks including MT-Bench, HumanEval, and CNN-Daily datasets.

## Key Results
- Achieves optimal performance-memory ratios among recent open-source speculative decoding systems
- 1.4 to 2 times faster decoding than quantized EAGLE on A10G GPU while using less VRAM
- Demonstrates that simultaneous multi-token prediction combined with mid-layer skipping provides no added VRAM costs
- Shows optimal speedup occurs when approximately half the target model parameters are used for drafting (β → 0.5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mid-layer skipping combined with simultaneous multi-token prediction reduces both computation and memory overhead while maintaining high acceptance rates.
- Mechanism: The draft model uses lower and upper layers from the target model, skipping middle layers, and predicts multiple tokens simultaneously by inserting mask tokens. This reduces the number of parameters needed for drafting while still accessing high-level features.
- Core assumption: The acceptance rate of drafted tokens remains high enough when using partial layers, and the simultaneous prediction task can be effectively learned.
- Evidence anchors:
  - [abstract] "Our scheme features mid-layer skipping and simultaneous multi-token predictions, offering no added VRAM costs and high training efficiency."
  - [section 4] "Different from Xia et al. (2023), the simultaneously generated tokens at j = i + 1, i + 2, ..., i + γ require only propagating through lower and top layers, skipping middle m-th to n-th layers of the target model"
  - [corpus] Weak evidence - corpus lacks direct experimental results comparing acceptance rates with/without mid-layer skipping
- Break condition: If acceptance rates drop significantly due to insufficient context from skipped layers, or if simultaneous prediction becomes too noisy.

### Mechanism 2
- Claim: The optimal speedup occurs when approximately half the target model parameters are used for drafting (β → 0.5).
- Mechanism: By skipping symmetric layers from the middle, the draft model maintains access to both early-layer features and late-layer representations while reducing computational cost.
- Core assumption: The relationship between model size ratio β and acceptance rate follows the hypothesized function α(β; U) = 1 - Uβ/(1-U).
- Evidence anchors:
  - [section 5.3] "Our proposed formula for predicting self-speculative acceptance rates in Eq. 10 mostly matches with the empirical results except for the lowest β value"
  - [figure 5] Shows empirical acceptance rates aligning with predicted curves
  - [corpus] Missing - corpus doesn't contain studies of optimal β values
- Break condition: If the acceptance rate doesn't follow the predicted curve, particularly at extreme β values.

### Mechanism 3
- Claim: Using non-quantized Phi-3 target model avoids significant quantization overhead while achieving better speed-memory tradeoffs than quantized EAGLE.
- Mechanism: By switching to a smaller target model (Phi-3 Mini) and operating in half-precision without quantization, the system avoids the 7x slowdown from quantization under constrained memory.
- Core assumption: The smaller Phi-3 model can maintain effectiveness while providing better memory efficiency than larger quantized models.
- Evidence anchors:
  - [abstract] "By switching to a smaller target model, we have created a more effective SD model based on Phi-3, which decodes 1.4 to 2 times faster than EAGLE on A10G while using less VRAM"
  - [section 5.2] "By exploiting the memory efficiency, S3D can avoid quantization and surpass the speed of quantized EAGLE when a 16 GiB VRAM limit is imposed"
  - [corpus] Weak evidence - corpus lacks comparative studies of quantization overhead across different model sizes
- Break condition: If the Phi-3 model cannot maintain effectiveness at scale, or if memory savings are insufficient to offset the lack of quantization benefits.

## Foundational Learning

- Concept: Speculative decoding framework and its speedup mechanics
  - Why needed here: Understanding how S3D fits into the broader speculative decoding landscape and what makes it different
  - Quick check question: How does speculative decoding achieve speedup compared to direct decoding?

- Concept: Transformer architecture and layer skipping techniques
  - Why needed here: S3D relies on understanding which layers to skip and why symmetric middle-layer skipping works best
  - Quick check question: What information is primarily processed in early vs. middle vs. late transformer layers?

- Concept: Multi-token prediction strategies and their trade-offs
  - Why needed here: S3D uses simultaneous multi-token prediction, which has different challenges than sequential prediction
  - Quick check question: What are the main challenges in predicting multiple future tokens simultaneously versus one at a time?

## Architecture Onboarding

- Component map:
  Target model -> Draft model (lower layers + upper layers, skipping middle) -> Masking mechanism (special <M> token) -> Verification (standard speculative decoding verification) -> Training loop (fine-tuning on masked language modeling)

- Critical path:
  1. Input context passes through lower layers of target model
  2. Skip middle layers, use upper layers to predict multiple masked tokens
  3. Insert predicted tokens and verify using full target model
  4. Repeat until sequence completion

- Design tradeoffs:
  - Memory vs. Speed: More layers included in draft model → better acceptance rates but higher memory usage
  - Training complexity vs. Performance: More sophisticated training objectives could improve performance but increase training cost
  - Precision vs. Efficiency: Half-precision operations save memory but may impact numerical stability

- Failure signatures:
  - Low acceptance rates at draft stage → too many layers skipped or poor multi-token prediction
  - Memory overflow → draft model too large or too many simultaneous predictions
  - Degraded quality → draft model not well-trained or verification too aggressive

- First 3 experiments:
  1. Test different layer skipping configurations (symmetric vs asymmetric, different skip counts) to find optimal acceptance rates
  2. Vary the number of simultaneous tokens (γ) to find the sweet spot between speed and acceptance rate
  3. Compare performance with and without quantization on memory-constrained GPUs to verify the 7x slowdown claim

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on custom benchmarks rather than standardized academic benchmarks, with unclear metric specifications
- Memory efficiency claims are undermined by switching to a smaller target model rather than improving the original architecture
- Simultaneous multi-token prediction may face practical challenges with increased hallucination rates, not thoroughly investigated
- Focus on low-memory GPUs (16GB VRAM) limits generalizability to higher-end hardware

## Confidence
**High Confidence**: The core architectural innovations (mid-layer skipping and simultaneous multi-token prediction) are technically sound and well-motivated.

**Medium Confidence**: Empirical performance claims are based on experiments using a smaller target model (Phi-3), making direct comparisons less meaningful.

**Low Confidence**: Claims about quantization overhead being 7x slower are based on references rather than direct experimental validation.

## Next Checks
1. Conduct ablation studies varying layer skipping configuration across multiple model sizes to verify robustness
2. Systematically evaluate quality of outputs when predicting 2, 3, 4, and 5 tokens simultaneously
3. Test S3D's performance on larger models (13B, 34B parameters) and different architectures to determine scalability