---
ver: rpa2
title: 'Efficient and Private: Memorisation under differentially private parameter-efficient
  fine-tuning in language models'
arxiv_id: '2411.15831'
source_url: https://arxiv.org/abs/2411.15831
tags:
- peft
- privacy
- methods
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates privacy-preserving fine-tuning of large\
  \ language models using Parameter-Efficient Fine-Tuning (PEFT) methods under Differential\
  \ Privacy (DP) constraints. The authors evaluate three PEFT methods\u2014Adapters,\
  \ LoRA, and (IA)3\u2014against standard fine-tuning across IMDb and QNLI datasets."
---

# Efficient and Private: Memorisation under differentially private parameter-efficient fine-tuning in language models

## Quick Facts
- arXiv ID: 2411.15831
- Source URL: https://arxiv.org/abs/2411.15831
- Authors: Olivia Ma; Jonathan Passerat-Palmbach; Dmitrii Usynin
- Reference count: 40
- Primary result: PEFT methods achieve comparable accuracy to standard fine-tuning while reducing privacy leakage and memory usage under DP constraints

## Executive Summary
This study investigates privacy-preserving fine-tuning of large language models using Parameter-Efficient Fine-Tuning (PEFT) methods under Differential Privacy (DP) constraints. The authors evaluate three PEFT methods—Adapters, LoRA, and (IA)3—against standard fine-tuning across IMDb and QNLI datasets. Results show PEFT methods achieve comparable accuracy (e.g., LoRA at 85.3% vs. 84.3% for DistilBERT on IMDb under DP) while reducing trainable parameters by over 99% and memory usage by up to 50%. Privacy assessments using membership inference attacks and data poisoning experiments reveal PEFT methods exhibit lower memorisation risks (AUC scores of 0.52-0.53 vs. 0.59 for standard fine-tuning) even without DP, with DP further reducing privacy leakage. However, DP's effectiveness is weaker for PEFT methods, likely due to parameter-efficient updates concentrating noise. Increasing PEFT parameters slightly raises memorisation risks, but robustness remains higher than standard fine-tuning. PEFT methods offer a promising balance of efficiency, accuracy, and privacy preservation for resource-constrained, privacy-sensitive applications.

## Method Summary
The study fine-tunes DistilBERT and BERT-base using standard fine-tuning and PEFT methods (Adapters, LoRA, (IA)3) with and without DP (ε = 1.0, 4.0, 8.0) on IMDb and QNLI datasets. Models are evaluated using accuracy, memory usage, training time, and AUC scores from loss-based membership inference attacks. Data poisoning experiments with mislabelled samples assess memorisation risks. The implementation uses Hugging Face Transformers, PEFT, Opacus, and dp-transformers libraries.

## Key Results
- PEFT methods achieve comparable accuracy to standard fine-tuning while reducing trainable parameters by over 99% and memory usage by up to 50%
- PEFT methods exhibit lower memorisation risks (AUC scores of 0.52-0.53 vs. 0.59 for standard fine-tuning) even without DP
- DP further reduces privacy leakage for PEFT methods, but effectiveness is weaker compared to standard fine-tuning due to noise concentration on fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT methods reduce privacy leakage by limiting the number of parameters updated during fine-tuning, thereby reducing the model's capacity to memorize sensitive training data.
- Mechanism: By updating only a small subset of parameters (e.g., adapters, LoRA low-rank matrices), PEFT restricts the amount of information the model can encode about individual training examples.
- Core assumption: Fewer trainable parameters lead to less detailed memorization of training data, reducing membership inference attack success rates.
- Evidence anchors:
  - [abstract] "PEFT methods exhibit lower memorisation risks (AUC scores of 0.52-0.53 vs. 0.59 for standard fine-tuning)"
  - [section] "PEFT methods like LoRA and Adapter showed lower AUC scores of 0.52 and 0.53, respectively, underscoring their enhanced resilience against membership inference attacks."
- Break condition: If PEFT methods are configured with very large parameter counts (e.g., high-rank LoRA or large adapter bottlenecks), the privacy benefit may diminish.

### Mechanism 2
- Claim: DP's effectiveness is weaker for PEFT methods because noise is concentrated on fewer parameters, reducing the overall impact of differential privacy.
- Mechanism: In DP training, noise is added to gradients during optimization. For PEFT, this noise is applied to a smaller parameter set, potentially making it less effective at obscuring information about individual training examples.
- Core assumption: DP noise effectiveness scales with the number of parameters it's applied to, so concentrating it on fewer parameters reduces its privacy-preserving power.
- Evidence anchors:
  - [abstract] "DP's effectiveness is weaker for PEFT methods, likely due to parameter-efficient updates concentrating noise"
  - [section] "PEFT methods showed smaller AUC reductions under DP, implying a weaker interaction with DP mechanisms"
- Break condition: If DP noise is adaptively scaled or redistributed across the full model architecture, the differential in effectiveness might be reduced.

### Mechanism 3
- Claim: LoRA's robustness against memorisation is not solely due to fewer parameters but also due to its architectural integration within attention mechanisms.
- Mechanism: LoRA modifies attention weights through low-rank decomposition, which may inherently limit the model's ability to form precise associations with individual training examples.
- Core assumption: The specific architectural placement of PEFT modules (attention vs. feedforward layers) influences memorisation capacity beyond just parameter count.
- Evidence anchors:
  - [abstract] "Increasing PEFT parameters slightly raises memorisation risks, but robustness remains higher than standard fine-tuning"
  - [section] "LoRA and Adapter maintained low AUC scores across configurations with increased parameters"
- Break condition: If LoRA is modified to update more parameters or integrated into different architectural components, the memorisation benefits might decrease.

## Foundational Learning

- Concept: Differential Privacy (DP) and DP-SGD
  - Why needed here: Understanding how DP works and its limitations is crucial for evaluating the privacy benefits of PEFT methods under DP constraints.
  - Quick check question: What is the role of gradient clipping and noise addition in DP-SGD, and how do they affect model utility?

- Concept: Parameter-Efficient Fine-Tuning (PEFT) methods
  - Why needed here: Familiarity with different PEFT approaches (Adapters, LoRA, (IA)3) and their architectural differences is essential for understanding their privacy implications.
  - Quick check question: How do Adapters, LoRA, and (IA)3 differ in their approach to parameter-efficient fine-tuning, and what are the implications for privacy?

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: MIAs are used to quantify privacy leakage, so understanding how they work and what AUC scores indicate is critical for interpreting the results.
  - Quick check question: What does a high AUC score in a membership inference attack indicate about a model's privacy risks?

## Architecture Onboarding

- Component map:
  Pre-trained model (e.g., DistilBERT, BERT-base) -> PEFT modules (Adapters, LoRA, (IA)3) -> DP training pipeline (gradient clipping, noise addition) -> Evaluation components (MIAs, data poisoning experiments)

- Critical path:
  1. Initialize pre-trained model
  2. Apply PEFT method
  3. Integrate DP training (if applicable)
  4. Fine-tune on downstream task
  5. Evaluate privacy leakage (MIAs, data poisoning)
  6. Analyze results and iterate

- Design tradeoffs:
  - Parameter count vs. privacy: Higher PEFT parameter counts may improve utility but increase memorisation risks.
  - DP noise vs. utility: Stronger DP guarantees (lower ε) require more noise, potentially harming model performance.
  - Computational efficiency vs. privacy: PEFT methods offer efficiency but may interact differently with DP mechanisms.

- Failure signatures:
  - High AUC scores in MIAs indicate significant privacy leakage.
  - Inconsistent performance across datasets suggests sensitivity to task characteristics.
  - DP effectiveness diminishing for PEFT methods indicates a need for adaptive DP strategies.

- First 3 experiments:
  1. Compare AUC scores of standard fine-tuning vs. PEFT methods on IMDb dataset without DP.
  2. Evaluate the impact of increasing LoRA rank on privacy leakage using data poisoning experiments.
  3. Test DP effectiveness on PEFT methods by varying ε values and measuring AUC score changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interaction between PEFT methods and DP mechanisms vary across different types of data distributions, particularly for imbalanced or minority groups?
- Basis in paper: [explicit] The authors mention that future work should explore the robustness of PEFT methods when applied to biased datasets or minority groups, and that assessing PEFT's privacy and utility on imbalanced or skewed data distributions will be essential for understanding its effectiveness in protecting underrepresented groups from privacy risks.
- Why unresolved: The paper focuses on balanced datasets (IMDb and QNLI) and does not examine how PEFT methods perform under data skew or minority representation scenarios.
- What evidence would resolve it: Experiments comparing PEFT methods on balanced vs. imbalanced datasets, measuring privacy leakage (AUC scores) and utility across different demographic or class distributions.

### Open Question 2
- Question: What is the optimal DP noise distribution strategy for PEFT methods that would enhance privacy protection while maintaining computational efficiency?
- Basis in paper: [explicit] The authors note that DP mechanisms are less effective on PEFT methods because noise is concentrated on a smaller subset of parameters, and they suggest that developing DP strategies optimized for PEFT architectures could improve privacy without sacrificing utility.
- Why unresolved: The paper demonstrates that standard DP-SGD is suboptimal for PEFT but does not propose or test alternative noise distribution strategies.
- What evidence would resolve it: Comparative experiments testing different DP noise allocation schemes (e.g., adaptive noise levels per PEFT module, hierarchical noise application) measuring both privacy (AUC scores) and utility metrics.

### Open Question 3
- Question: Does the architectural placement of PEFT modules within the transformer model (e.g., attention vs. feedforward layers, specific attention subcomponents) significantly impact their privacy-preserving capabilities?
- Basis in paper: [explicit] The authors suggest that factors beyond parameter count, such as where PEFT modules are integrated within the model, could play a significant role in privacy preservation, and they mention that exploring these configurations could yield deeper insights into effective privacy-preserving strategies.
- Why unresolved: While the paper compares different PEFT methods (LoRA, Adapter, (IA)3), it does not systematically vary the placement of these modules within the transformer architecture.
- What evidence would resolve it: Controlled experiments varying PEFT module placement (e.g., LoRA applied to different attention subcomponents, Adapter in different transformer layers) while measuring privacy leakage and utility metrics.

## Limitations

- The study identifies that DP's effectiveness is weaker for PEFT methods due to noise concentration on fewer parameters, but the exact mechanisms and magnitude of this interaction require further investigation.
- The paper notes that increasing PEFT parameters slightly raises memorisation risks, but the threshold at which PEFT methods lose their privacy advantage compared to standard fine-tuning is unclear.
- The specific architectural integration of PEFT modules (attention vs. feedforward layers) and its impact on memorisation capacity beyond parameter count remains an open question.

## Confidence

- **High Confidence**: PEFT methods achieve comparable accuracy to standard fine-tuning while reducing trainable parameters by over 99% and memory usage by up to 50%. Privacy assessments show PEFT methods exhibit lower memorisation risks (AUC scores of 0.52-0.53 vs. 0.59 for standard fine-tuning) even without DP.
- **Medium Confidence**: DP further reduces privacy leakage for PEFT methods, but the effectiveness is weaker compared to standard fine-tuning. The study attributes this to parameter-efficient updates concentrating noise, but the exact mechanisms require more detailed analysis.
- **Low Confidence**: The claim that LoRA's robustness against memorisation is not solely due to fewer parameters but also due to its architectural integration within attention mechanisms needs further validation through targeted experiments.

## Next Checks

1. Conduct experiments varying the number of PEFT parameters (e.g., LoRA rank) to identify the threshold at which privacy benefits diminish compared to standard fine-tuning.
2. Implement adaptive DP strategies that redistribute noise across the full model architecture to test if DP effectiveness for PEFT methods can be improved.
3. Compare the memorisation risks of LoRA when integrated into attention layers versus feedforward layers to validate the architectural influence on privacy.