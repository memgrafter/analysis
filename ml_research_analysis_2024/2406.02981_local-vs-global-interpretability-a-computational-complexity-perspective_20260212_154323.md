---
ver: rpa2
title: 'Local vs. Global Interpretability: A Computational Complexity Perspective'
arxiv_id: '2406.02981'
source_url: https://arxiv.org/abs/2406.02981
tags:
- global
- sufficient
- local
- reason
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses computational complexity theory to rigorously analyze
  the interpretability of machine learning models, distinguishing between local interpretability
  (understanding specific predictions) and global interpretability (understanding
  overall model behavior). The authors prove novel results about the relationship
  between local and global explanations, including a duality between them and the
  uniqueness of certain global explanation forms.
---

# Local vs. Global Interpretability: A Computational Complexity Perspective

## Quick Facts
- arXiv ID: 2406.02981
- Source URL: https://arxiv.org/abs/2406.02981
- Authors: Shahaf Bassan; Guy Amir; Guy Katz
- Reference count: 40
- This paper uses computational complexity theory to rigorously analyze the interpretability of machine learning models, distinguishing between local interpretability (understanding specific predictions) and global interpretability (understanding overall model behavior).

## Executive Summary
This paper establishes a rigorous theoretical framework for comparing local and global interpretability using computational complexity theory. The authors prove novel duality and uniqueness theorems that create mathematical bridges between local and global explanation forms, then analyze the complexity of computing different explanation types across three model classes: linear models, decision trees, and neural networks. Key findings include that linear models are harder to interpret globally than locally for feature selection, while the opposite is true for decision trees and neural networks. The results provide theoretical justification for some intuitive claims about interpretability while challenging others.

## Method Summary
The paper formalizes local and global explanation queries (CSR, MSR, FR, FN, CC and their global variants) and analyzes their computational complexity across three model classes: free binary decision diagrams (FBDDs), perceptrons, and multilayer perceptrons (MLPs). Using reductions and complexity class membership proofs, the authors establish the complexity of each explanation form, then compare local vs. global variants. The analysis distinguishes between decision problems (finding explanations) and counting problems (enumerating explanations), revealing different complexity patterns across model types.

## Key Results
- Duality theorem: Global sufficient reasons correspond to hitting sets of local contrastive reasons
- Uniqueness theorem: Exactly one subset-minimal global sufficient reason exists for any non-trivial function
- Complexity reversals: Linear models show higher global complexity for feature selection, while decision trees and neural networks show higher local complexity
- Counting problem stability: Local and global variants of counting queries maintain the same complexity class across all model types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper proves a duality between local and global explanations where global sufficient reasons correspond to hitting sets of local contrastive reasons.
- Mechanism: By establishing that any global sufficient reason must intersect all local contrastive reasons (Theorem 2), the paper creates a mathematical bridge between local and global interpretability that enables rigorous complexity analysis.
- Core assumption: The definitions of local and global sufficient reasons and contrastive reasons are mathematically sound and consistent across all model types studied.
- Evidence anchors:
  - [abstract]: "a duality between local and global forms of explanations"
  - [section]: "Theorem 2 Any global sufficient reason of f intersects with all local contrastive reasons of ⟨f, x⟩"
  - [corpus]: Weak - the corpus papers focus on different explanation methods (concept-based, counterfactual, etc.) without establishing this specific duality relationship
- Break condition: If the duality relationship doesn't hold for certain model types or explanation forms, the complexity comparisons would become invalid for those cases.

### Mechanism 2
- Claim: Global explanations are uniquely minimal while local explanations are exponentially abundant, creating different computational complexity profiles.
- Mechanism: The paper proves that there exists exactly one subset-minimal global sufficient reason (Theorem 4), while local explanations can be Θ(2^n/√n) in number (Proposition 1). This uniqueness makes global explanations computationally simpler to find despite their stricter definition.
- Core assumption: The uniqueness of global explanations holds across all non-trivial functions and model types studied.
- Evidence anchors:
  - [abstract]: "the inherent uniqueness of certain global explanation forms"
  - [section]: "Theorem 4 There exists one unique subset-minimal global sufficient reason of f"
  - [corpus]: Weak - corpus papers don't discuss the uniqueness property of global explanations versus local abundance
- Break condition: If global explanations are not unique for certain model types, the complexity advantage disappears and both local and global forms become equally hard.

### Mechanism 3
- Claim: The computational complexity of explanation forms differs based on whether the task involves decision problems versus counting problems.
- Mechanism: The paper shows that while local and global variants of decision problems (like MSR and G-MSR) have different complexities, counting problems (like CC and G-CC) maintain the same complexity class across both variants (Proposition 11).
- Core assumption: The fundamental difference between decision and counting complexity classes persists across all model types studied.
- Evidence anchors:
  - [abstract]: "we explore a relaxed version of the previous forms, which is commonly analyzed in other formal frameworks"
  - [section]: "Proposition 11 For FBDDs, both CC and G-CC can be solved in polynomial time"
  - [corpus]: Weak - corpus papers focus on different aspects of explainability without comparing decision vs counting complexity
- Break condition: If counting problems somehow inherit the complexity gaps seen in decision problems, the theoretical framework would need revision.

## Foundational Learning

- Concept: Computational complexity classes (PTIME, NP, coNP, ΣP₂, ΠP₂, #P)
  - Why needed here: The entire paper's framework relies on placing explanation forms into precise complexity classes to rigorously compare interpretability
  - Quick check question: Can you explain why proving G-CSR is coNP-Complete for perceptrons requires showing both membership and hardness?
- Concept: Formal definitions of sufficient reasons, contrastive reasons, and necessity
- Concept: Model classes (FBDDs, perceptrons, MLPs) and their computational properties
  - Why needed here: The paper analyzes three specific model types representing different points on the interpretability spectrum
  - Quick check question: Why does the paper use free BDDs as a generalization of decision trees rather than actual decision trees?

## Architecture Onboarding

- Component map: Formal explanation definitions (Section 3) → Complexity proofs (Section 5) → Duality/uniqueness theorems (Section 4) → Model class analysis
- Critical path: Define explanation forms → Prove duality and uniqueness → Analyze complexity for each model class → Compare local vs global
- Design tradeoffs: Mathematical rigor vs practical applicability, theoretical completeness vs focused analysis
- Failure signatures: Incorrect complexity class assignments, flawed reduction proofs, missing edge cases in duality theorems
- First 3 experiments:
  1. Verify polynomial algorithms for CSR/MSR on perceptrons match the paper's claims
  2. Implement the hitting set duality check between local contrastive and global sufficient reasons
  3. Test the uniqueness of global sufficient reasons by attempting to find multiple subset-minimal solutions for various functions

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but leaves several theoretical extensions unexplored.

## Limitations
- The analysis only covers three specific model classes, limiting generalizability to other ML architectures
- Theoretical complexity gaps may not translate to practical interpretability differences in real-world applications
- The corpus comparison reveals limited validation from related work, suggesting potential gaps in practical applicability

## Confidence
- Theoretical framework and proofs: High confidence
- Complexity analysis for specific model classes: Medium confidence
- Practical interpretability implications: Low confidence

## Next Checks
1. Implement and verify the polynomial algorithms for CSR/MSR on perceptrons to confirm the claimed complexity bounds
2. Test the hitting set duality by constructing and verifying the correspondence between local contrastive and global sufficient reasons across multiple model instances
3. Systematically attempt to find multiple subset-minimal global sufficient reasons for various functions to validate the uniqueness theorem experimentally