---
ver: rpa2
title: 'All Against Some: Efficient Integration of Large Language Models for Message
  Passing in Graph Neural Networks'
arxiv_id: '2407.14996'
source_url: https://arxiv.org/abs/2407.14996
tags:
- graph
- arxiv
- llms
- node
- e-llagnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes E-LLaGNN, a framework that efficiently integrates
  Large Language Models (LLMs) into Graph Neural Networks (GNNs) by selectively enhancing
  node features during training. The core idea is to use LLMs as an on-demand service
  to augment a limited fraction of nodes with diverse prompts, rather than enhancing
  every node, which is computationally expensive.
---

# All Against Some: Efficient Integration of Large Language Models for Message Passing in Graph Neural Networks

## Quick Facts
- arXiv ID: 2407.14996
- Source URL: https://arxiv.org/abs/2407.14996
- Reference count: 28
- Primary result: Selective LLM augmentation improves GNN performance by up to 5.2% on Cora while reducing computational cost

## Executive Summary
E-LLaGNN proposes an efficient framework for integrating Large Language Models into Graph Neural Networks by selectively enhancing a fraction of node features during training. Instead of augmenting all nodes (computationally expensive), the framework uses LLMs on-demand to enrich a limited subset of nodes with diverse prompts, then performs message passing through standard GNN architectures. Experiments on four citation network datasets show significant performance improvements over state-of-the-art methods while enabling practical deployment through LLM-free inference.

## Method Summary
E-LLaGNN uses a selective augmentation strategy where only a fraction of nodes are enhanced with LLM-generated features during training. The process involves three key steps: (1) neighborhood sampling using LLM-enhanced query representations to identify high-quality neighbors, (2) on-demand feature enhancement of selected nodes using a diverse prompt catalog, and (3) aggregation using standard GNN architectures. The framework employs Sentence-BERT to encode LLM outputs and uses cosine similarity for neighborhood ranking. During inference, the model operates without LLM calls by relying on learned patterns from training.

## Key Results
- E-LLaGNN outperforms state-of-the-art methods by up to 5.2% on Cora and 2.1% on PubMed datasets
- Selective augmentation (5-25% of nodes) provides optimal performance, with 100% augmentation degrading results due to overfitting
- Deep GNN backbones (layers 2, 4, 8) show improved gradient flow when trained with LLM augmentation
- LLM-free inference achieves minimal performance loss compared to training with LLM augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective LLM augmentation improves gradient flow in deep GNNs without overfitting
- Mechanism: