---
ver: rpa2
title: A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models
arxiv_id: '2402.04787'
source_url: https://arxiv.org/abs/2402.04787
tags:
- gpt-3
- nles
- language
- hypothesis
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for analyzing self-rationalizing
  models by comparing their explanations to those generated by a hypothesis-driven
  surrogate model. The framework uses a Bayesian network to implement a hypothesis
  about how a task is solved, and generates natural language explanations based on
  the internal states of the network.
---

# A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models

## Quick Facts
- **arXiv ID:** 2402.04787
- **Source URL:** https://arxiv.org/abs/2402.04787
- **Reference count:** 26
- **Key outcome:** Framework compares self-rationalizing model explanations to hypothesis-driven surrogate model explanations using Bayesian networks, but showed minimal similarity to GPT-3.5 outputs

## Executive Summary
This paper introduces a novel framework for analyzing self-rationalizing models by comparing their explanations to those generated by hypothesis-driven surrogate models. The framework uses Bayesian networks to implement hypotheses about how tasks are solved, generating natural language explanations from internal network states. These explanations are then compared to LLM-generated explanations through both automatic and human evaluations. The authors demonstrate the approach using a natural language inference task but found that their surrogate models did not exhibit strong similarity to GPT-3.5's explanations, raising important questions about the framework's effectiveness and the challenges of modeling complex LLM reasoning patterns.

## Method Summary
The framework operates by first defining a hypothesis about how a specific task is solved, then implementing this hypothesis in a Bayesian network structure. The Bayesian network generates predictions and internal states that are converted into natural language explanations. These surrogate explanations are compared against self-generated explanations from large language models using both automatic metrics (similarity scores, explanation quality measures) and human evaluations. The authors applied this framework to the natural language inference task with two different Bayesian network realizations, systematically comparing the resulting explanations to those produced by GPT-3.5 to assess alignment and reasoning similarity.

## Key Results
- The hypothesis-driven framework successfully generates natural language explanations from Bayesian network internal states
- Surrogate model explanations showed minimal similarity to GPT-3.5's self-generated explanations across most evaluation metrics
- Human evaluations with 20 participants per condition revealed significant differences in explanation quality and alignment
- The framework demonstrates the technical feasibility of generating explanations from hypothesis-driven models but highlights challenges in matching LLM reasoning patterns

## Why This Works (Mechanism)
The framework leverages the structured reasoning capabilities of Bayesian networks to implement interpretable hypotheses about task-solving processes. By mapping internal network states to natural language explanations, the approach creates a transparent link between hypothesized reasoning mechanisms and verbalized explanations. This structure enables systematic comparison between human-designed reasoning models and the emergent reasoning patterns of large language models, potentially revealing gaps in our understanding of how complex models solve tasks.

## Foundational Learning

**Bayesian Networks** - Probabilistic graphical models representing dependencies between variables
*Why needed:* Provides interpretable structure for implementing hypotheses about task-solving processes
*Quick check:* Verify node relationships and conditional probability distributions accurately reflect the hypothesized reasoning steps

**Natural Language Generation from Model States** - Converting internal model representations into human-readable explanations
*Why needed:* Bridges the gap between computational reasoning and human-understandable justifications
*Quick check:* Ensure generated explanations maintain logical consistency with the underlying Bayesian network states

**Explanation Similarity Metrics** - Quantitative measures for comparing explanation quality and alignment
*Why needed:* Enables systematic evaluation of how well surrogate explanations match LLM-generated ones
*Quick check:* Validate that similarity metrics capture meaningful differences in reasoning patterns, not just surface-level text similarity

## Architecture Onboarding

**Component Map:** Hypothesis Definition -> Bayesian Network Implementation -> State-to-Text Generation -> Explanation Comparison

**Critical Path:** The most important sequence is Hypothesis Definition → Bayesian Network Implementation → Explanation Generation, as errors in early stages propagate through the entire analysis pipeline.

**Design Tradeoffs:** Bayesian networks offer interpretability but may oversimplify complex reasoning; alternative implementations (neural networks, symbolic systems) could provide different balances of accuracy versus interpretability.

**Failure Signatures:** Poor explanation alignment typically indicates either an inadequate hypothesis about task-solving mechanisms or limitations in the Bayesian network's ability to capture the complexity of LLM reasoning patterns.

**First Experiments:**
1. Test the framework on a simpler task with well-understood solution mechanisms to validate the basic methodology
2. Compare explanations generated from different hypothesis implementations on the same task to assess sensitivity to hypothesis design
3. Conduct ablation studies removing the Bayesian network layer to determine if explanation quality depends on this specific implementation choice

## Open Questions the Paper Calls Out
The authors explicitly raise questions about whether Bayesian networks are the appropriate mechanism for capturing LLM reasoning patterns, and whether the framework's limitations reflect fundamental challenges in modeling complex neural network behavior versus issues with the current implementation choices.

## Limitations
- The framework failed to achieve strong similarity between surrogate and LLM-generated explanations
- Bayesian networks may be too simplistic to capture the distributed representations used by modern LLMs
- Limited human evaluation sample size (20 participants per condition) may not provide robust conclusions
- Hand-crafted hypotheses require substantial domain expertise and may not generalize well across tasks

## Confidence
- **High:** Claims about the need for better hypothesis-driven approaches to model analysis
- **Medium:** Claims about the general methodology being technically sound
- **Low:** Claims about accurately capturing LLM reasoning patterns with the current implementation

## Next Checks
1. Conduct ablation studies comparing Bayesian network implementations against alternative hypothesis representation methods such as neural networks or hybrid symbolic-neural approaches
2. Expand the human evaluation study to include a larger, more diverse participant pool and incorporate think-aloud protocols
3. Test the framework on multiple LLM architectures (not just GPT-3.5) and across a broader range of tasks to assess whether limitations are implementation-specific or fundamental