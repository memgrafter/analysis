---
ver: rpa2
title: 'EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based
  Question Answering Systems'
arxiv_id: '2406.10393'
source_url: https://arxiv.org/abs/2406.10393
tags:
- answer
- knowledge
- quotes
- question
- webglm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of knowledge grounding and efficiency
  in citation-based QA systems, which often rely solely on web sources and use simple
  heuristics to extract quotes, leading to incomplete information. To mitigate these
  issues, the authors propose EWEK-QA, which combines adaptive web retrieval and efficient
  knowledge graph (KG) retrieval to enrich the extracted knowledge fed to the system.
---

# EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems

## Quick Facts
- arXiv ID: 2406.10393
- Source URL: https://arxiv.org/abs/2406.10393
- Reference count: 32
- Primary result: Combines adaptive web and efficient KG retrieval to achieve 3-6× speedup with >20% accuracy gains over state-of-the-art baselines

## Executive Summary
This paper addresses knowledge grounding and efficiency challenges in citation-based QA systems that rely primarily on web sources with simple heuristic quote extraction. The authors propose EWEK-QA, a system that combines adaptive web retrieval with efficient knowledge graph (KG) retrieval to provide more comprehensive and relevant information to the QA system. EWEK-QA demonstrates significant improvements over WebGLM baseline across multiple metrics including relevance (>20%), answer span coverage (>25%), self-containment (>35%), and human evaluation accuracy (>20%), while achieving 3-6× speedup using smaller LLMs compared to ToG baseline.

## Method Summary
EWEK-QA integrates two complementary retrieval components: an adaptive web retriever that extracts more informative and relevant quotes than baseline approaches, and an efficient KG retriever (ToG-E) that extracts relevant triples without requiring LLM calls. The web component adapts retrieval strategies based on question context to capture more comprehensive information, while the KG component provides structured knowledge efficiently. The system processes questions through both retrieval streams and combines the results to enrich the knowledge fed to the QA system. Evaluation covers KGQA, ODQA, and multi-hop reasoning tasks across various datasets, demonstrating consistent improvements in both retrieval quality and end-to-end QA performance.

## Key Results
- Web retrieval improvements: >20% gain in relevance, >25% increase in answer span coverage, >35% improvement in self-containment
- End-to-end QA performance: >20% accuracy improvement in human evaluation across KGQA, ODQA, and multi-hop reasoning tasks
- Efficiency gains: 3× to 6× speedup compared to ToG baseline while using smaller LLMs
- Consistent improvements across multiple QA tasks and datasets

## Why This Works (Mechanism)
EWEK-QA succeeds by addressing two fundamental limitations of citation-based QA systems: incomplete knowledge from web-only sources and inefficient retrieval processes. The adaptive web retriever captures more comprehensive information by adjusting its strategy based on question context rather than using fixed heuristics. The efficient KG retriever provides structured knowledge without the computational overhead of LLM-based extraction, enabling faster processing while maintaining information quality. By combining these complementary approaches, the system provides richer context for QA models while reducing computational costs.

## Foundational Learning

**Knowledge Graph Retrieval**: Extracting structured triples from KGs provides precise, unambiguous information compared to unstructured text. Needed because KGs contain verified facts that complement noisy web data. Quick check: Verify triple extraction accuracy against ground truth relations.

**Adaptive Retrieval Strategies**: Dynamically adjusting retrieval based on question context captures more relevant information than static approaches. Needed because different question types require different retrieval strategies. Quick check: Measure retrieval precision across different question categories.

**Multi-hop Reasoning**: Combining information from multiple sources requires tracking intermediate reasoning steps. Needed because complex questions often require synthesizing information from different knowledge sources. Quick check: Validate intermediate reasoning steps against expected logical progressions.

## Architecture Onboarding

**Component Map**: Question -> Adaptive Web Retriever -> Quote Extraction -> KG Retriever (ToG-E) -> Triple Extraction -> Knowledge Fusion -> QA Model

**Critical Path**: Question → Adaptive Web Retriever → Web Quotes → KG Retriever → KG Triples → Knowledge Fusion → QA Model → Answer

**Design Tradeoffs**: The system trades some precision in individual retrievals for broader coverage and efficiency gains. Using smaller LLMs with efficient retrieval offsets computational costs while maintaining accuracy through richer knowledge input.

**Failure Signatures**: Web retrieval may fail when sources are sparse or unreliable; KG retrieval depends on relevant triples existing in the knowledge graph; knowledge fusion may produce conflicts between web and KG sources requiring resolution strategies.

**First Experiments**:
1. Test adaptive web retriever on questions with known optimal retrieval strategies to validate adaptation logic
2. Evaluate KG retriever performance on questions where ground truth triples are available in the knowledge graph
3. Run ablation study comparing web-only, KG-only, and combined retrieval approaches on simple QA tasks

## Open Questions the Paper Calls Out

None identified in the provided materials.

## Limitations

- Evaluation focused on three specific QA tasks (KGQA, ODQA, multi-hop reasoning), limiting generalizability to other reasoning paradigms
- Web retrieval performance depends on initial result quality, with uncharacterized failure modes when sources are sparse
- KG retrieval assumes relevant information exists in the knowledge graph, without quantifying missing information rates
- Speedup claims may be confounded by comparing smaller LLMs against ToG baseline rather than architectural efficiency alone
- Human evaluation methodology lacks details on rater selection, inter-rater reliability, and potential biases

## Confidence

- Web retrieval improvements: **High** - Quantitative metrics show consistent gains across tasks
- KG retrieval efficiency claims: **Medium** - Speed improvements demonstrated but confounded by model size differences
- End-to-end accuracy improvements: **Medium** - Human evaluation shows gains but methodology details insufficient
- Cross-task generalization: **Low** - Limited to three QA paradigms with no exploration of other reasoning types

## Next Checks

1. Conduct ablation studies comparing EWEK-QA's web and KG components separately against baselines to isolate individual contributions
2. Test the system on additional QA tasks involving temporal reasoning, numerical inference, or long-form generation to assess generalization
3. Implement controlled experiments varying KG completeness to measure performance degradation when relevant triples are missing