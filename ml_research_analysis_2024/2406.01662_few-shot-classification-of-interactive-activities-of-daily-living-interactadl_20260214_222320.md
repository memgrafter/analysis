---
ver: rpa2
title: Few-Shot Classification of Interactive Activities of Daily Living (InteractADL)
arxiv_id: '2406.01662'
source_url: https://arxiv.org/abs/2406.01662
tags:
- tuning
- name
- few-shot
- dataset
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces InteractADL, a challenging new dataset for
  few-shot classification of complex, multi-person Activities of Daily Living (ADLs)
  in home environments. It also presents Name Tuning, a novel method that learns optimal
  class name vectors to improve semantic separability for fine-grained few-shot video
  classification.
---

# Few-Shot Classification of Interactive Activities of Daily Living (InteractADL)

## Quick Facts
- arXiv ID: 2406.01662
- Source URL: https://arxiv.org/abs/2406.01662
- Authors: Zane Durante; Robathan Harries; Edward Vendrow; Zelun Luo; Yuta Kyuragi; Kazuki Kozuka; Li Fei-Fei; Ehsan Adeli
- Reference count: 40
- This work introduces InteractADL, a challenging new dataset for few-shot classification of complex, multi-person Activities of Daily Living (ADLs) in home environments. It also presents Name Tuning, a novel method that learns optimal class name vectors to improve semantic separability for fine-grained few-shot video classification. Name Tuning outperforms existing prompt tuning strategies and achieves state-of-the-art results across 3 few-shot video classification benchmarks without requiring video pretraining. On the new InteractADL dataset, Name Tuning demonstrates strong performance in the challenging setting of classifying fine-grained atomic actions in long, multi-person videos.

## Executive Summary
This paper introduces InteractADL, a new dataset for few-shot classification of complex multi-person Activities of Daily Living in home environments, and proposes Name Tuning, a method that learns optimal class name vectors to improve semantic separability for fine-grained few-shot video classification. Name Tuning outperforms existing prompt tuning strategies and achieves state-of-the-art results across multiple benchmarks without requiring video pretraining. The method demonstrates strong performance on InteractADL, particularly in classifying fine-grained atomic actions in long, multi-person videos.

## Method Summary
Name Tuning is a novel method for fine-grained few-shot video classification that improves semantic separability by learning optimal class name vectors. The approach trains learnable offset vectors that are added to original class name token embeddings, allowing the model to adjust class names to be more discriminative for specific datasets. It is compatible with existing prompt tuning methods like CoOp and can be combined with them (CoNa) to boost performance on some datasets. The method leverages VLMs pre-trained on internet-scale data to generalize to few-shot activity recognition without requiring large meta-training sets.

## Key Results
- Name Tuning outperforms existing prompt tuning strategies and achieves state-of-the-art results across 3 few-shot video classification benchmarks
- On InteractADL, Name Tuning demonstrates strong performance in classifying fine-grained atomic actions in long, multi-person videos
- The method works without requiring video pretraining, leveraging VLMs pre-trained on internet-scale data instead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning optimal class name vectors improves semantic separability for fine-grained few-shot classification.
- Mechanism: The method trains learnable offset vectors that are added to the original class name token embeddings, allowing the model to adjust class names to be more discriminative for the specific dataset.
- Core assumption: The default class names provided with datasets are not optimized for the pre-trained VLM's embedding space and can be improved through fine-tuning.
- Evidence anchors:
  - [abstract] "We propose a novel method for fine-grained few-shot video classification called Name Tuning that enables greater semantic separability by learning optimal class name vectors."
  - [section 4.2] "Our method, Name Tuning, seeks to improve the expressiveness of provided class names directly, by training them to better match the content of example videos."
- Break condition: If the regularization term is too weak, the tuned class names may drift too far from the original semantic meaning and lose their prior information.

### Mechanism 2
- Claim: Combining name tuning with existing prompt tuning strategies (CoNa) can boost performance on some datasets.
- Mechanism: Name Tuning is compatible with CoOp (prompt tuning) because they tune separate parts of the input text - CoOp tunes the context vectors while Name Tuning tunes the class name vectors.
- Core assumption: The class name vectors and context vectors can be optimized independently without interfering with each other's optimization process.
- Evidence anchors:
  - [section 4.3] "Name Tuning is entirely compatible with the existing CoOp method since it tunes separate parts of the input text, and we observe that the combination of the two (abbreviated CoNa) can boost performance on some datasets."
- Break condition: If the learning rates for context and name tuning are not properly balanced, one component may dominate the optimization process.

### Mechanism 3
- Claim: VLMs pre-trained on internet-scale data can generalize to few-shot activity recognition without requiring a large meta-training set.
- Mechanism: The VLM leverages broad pre-training from internet scale data to perform well on downstream few-shot tasks, outperforming vision-only few-shot approaches.
- Core assumption: The pre-training data distribution captures enough semantic information to be useful for the downstream few-shot tasks, even without domain-specific adaptation.
- Evidence anchors:
  - [section 5.1] "In our experiments, we find that our VLM-based few-shot learning method outperforms the current state-of-the-art meta-learning approaches on all benchmarked datasets. This is notable since the VLM-based methods do not require a large meta-training set derived from visually similar classes and can instead leverage broad pre-training from internet scale data."
- Break condition: If the domain gap between pre-training data and target domain is too large (e.g., ego-view home environments vs. general internet images), the VLM may not generalize well without additional adaptation.

## Foundational Learning

- Concept: Few-shot learning paradigm
  - Why needed here: The paper evaluates Name Tuning in both traditional classification splits and meta-learning splits, requiring understanding of different few-shot learning formulations.
  - Quick check question: What's the key difference between traditional few-shot learning and meta-learning few-shot learning?

- Concept: Visual-language models (VLMs) and dual encoders
  - Why needed here: Name Tuning is designed specifically for VLMs with joint visual-language embedding spaces, and understanding how these models work is crucial for implementing the method.
  - Quick check question: How do dual encoder VLMs perform visual classification in the zero-shot setting?

- Concept: Prompt engineering and prompt tuning
  - Why needed here: Name Tuning builds upon existing prompt tuning methods like CoOp, so understanding these baseline approaches is important for implementing and extending the method.
  - Quick check question: What's the main difference between traditional prompt engineering and prompt tuning approaches like CoOp?

## Architecture Onboarding

- Component map: Pre-trained VLM -> Text encoder -> Visual encoder -> Similarity computation module -> Training loop

- Critical path:
  1. Tokenize input text (context + class name + offset)
  2. Compute text embeddings through frozen text encoder
  3. Compute video embeddings through visual encoder
  4. Calculate similarity scores between text and video embeddings
  5. Apply softmax to get class probabilities
  6. Backpropagate loss to update name offset parameters

- Design tradeoffs:
  - Context length vs. computational efficiency
  - Regularization strength vs. flexibility in learned class names
  - Number of training epochs vs. overfitting risk with few examples

- Failure signatures:
  - Overfitting when regularization is too weak
  - Underfitting when regularization is too strong
  - Poor performance on fine-grained datasets with subtle visual distinctions

- First 3 experiments:
  1. Implement Name Tuning with CLIP backbone on Kinetics dataset with 5-shot 5-way setup
  2. Compare performance with CoOp baseline using same hyperparameters
  3. Test different regularization strengths to find optimal balance between flexibility and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Name Tuning compare when applied to different video encoders beyond CLIP, MILES, and VideoCLIP?
- Basis in paper: [explicit] The paper tests Name Tuning with CLIP, MILES, and VideoCLIP VLMs, but does not explore other video encoders.
- Why unresolved: The study focuses on a limited set of pre-trained VLMs, leaving the generalizability of Name Tuning to other encoders untested.
- What evidence would resolve it: Experimental results comparing Name Tuning performance across a broader range of video encoders, including both transformer-based and non-transformer-based models.

### Open Question 2
- Question: What is the impact of varying the number of frames sampled from each video on Name Tuning's performance?
- Basis in paper: [explicit] The paper uses 10 frames for CLIP and 32 frames for VideoCLIP, but does not systematically explore the effect of frame sampling on performance.
- Why unresolved: The study uses fixed frame sampling strategies without analyzing how different numbers of frames might influence the learned class name vectors and overall classification accuracy.
- What evidence would resolve it: Experiments varying the number of frames sampled per video and analyzing the corresponding changes in Name Tuning's performance across different datasets.

### Open Question 3
- Question: How does Name Tuning's performance degrade when applied to datasets with classes that have significantly different semantic meanings but similar visual appearances?
- Basis in paper: [explicit] The paper focuses on fine-grained classification tasks but does not explicitly test Name Tuning on datasets with highly similar visual classes but different semantics.
- Why unresolved: The study does not address the challenge of distinguishing classes that are visually similar but semantically distinct, which is a common issue in real-world applications.
- What evidence would resolve it: Experiments on datasets known for this challenge (e.g., similar animal species or similar objects in different contexts) to measure Name Tuning's ability to learn discriminative class name vectors in such scenarios.

## Limitations
- Dataset generalization concerns: The evaluation is limited to a single new dataset (InteractADL) with only 800 videos across 20 classes, which may not be representative of the full complexity of ADLs in diverse home settings.
- Limited ablation studies: Insufficient exploration of critical design choices like regularization strengths and optimal context lengths for different datasets.
- Evaluation scope limitations: Performance claims primarily based on 5-shot and 20-shot settings, with limited examination of extreme few-shot (1-shot) or more conventional (100-shot) scenarios.

## Confidence
- High confidence: The technical implementation of Name Tuning as a method for learning class name vectors is sound and well-documented.
- Medium confidence: The claim that Name Tuning outperforms existing prompt tuning strategies across multiple benchmarks is supported by experimental results, but improvements are relatively modest (1-2% gains).
- Low confidence: The assertion that VLMs can effectively generalize to few-shot activity recognition without requiring large meta-training sets needs more rigorous validation.

## Next Checks
1. **Cross-dataset transfer evaluation**: Train Name Tuning on one fine-grained dataset (e.g., Food101) and evaluate the learned name vectors on a different dataset (e.g., Flowers102) without additional fine-tuning. This would validate whether the learned semantic improvements are dataset-specific or capture more general fine-grained discriminative features.

2. **Domain adaptation robustness test**: Evaluate Name Tuning performance when the pre-training data distribution significantly differs from the target domain. For example, test on InteractADL using a VLM pre-trained exclusively on general internet images versus one with some home environment exposure. This would quantify the method's robustness to domain gaps.

3. **Extreme few-shot scaling analysis**: Systematically evaluate Name Tuning across a wider range of shot numbers (1-shot, 2-shot, 5-shot, 10-shot, 20-shot, 50-shot, 100-shot) on multiple benchmarks. This would reveal the method's effectiveness across the full spectrum of few-shot learning scenarios and identify the point at which traditional fine-tuning becomes more effective than prompt-based approaches.