---
ver: rpa2
title: A Dynamical View of the Question of Why
arxiv_id: '2402.10240'
source_url: https://arxiv.org/abs/2402.10240
tags:
- event
- time
- causal
- state
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a process-based theory of causation for dynamical
  systems, where causal relationships are established through temporal analysis of
  underlying stochastic processes. The authors propose two fundamental lemmas that
  enable computation of causal contributions by framing them as reinforcement learning
  problems.
---

# A Dynamical View of the Question of Why

## Quick Facts
- arXiv ID: 2402.10240
- Source URL: https://arxiv.org/abs/2402.10240
- Authors: Mehdi Fatemi; Sindhu Gowda
- Reference count: 40
- Primary result: Process-based theory of causation for dynamical systems using temporal analysis of stochastic processes

## Executive Summary
This paper introduces a novel framework for causal reasoning in dynamical systems that operates directly on observational data without requiring interventions or causal graphs. The approach establishes causal relationships by analyzing how changes in system components affect the probability of future events through temporal decomposition. Two fundamental lemmas enable computation of causal contributions by framing them as reinforcement learning problems, allowing identification of causes in complex systems where traditional causal analysis methods fail.

## Method Summary
The framework establishes causation between events in dynamical systems through temporal analysis of underlying stochastic processes. It computes minimum and maximum probabilities of future events via optimal value functions from specially constructed MDPs, then decomposes expected changes in these probabilities into contributions from individual state and action components using Itô's lemma. Causation is determined by comparing contributions of ruling versus non-ruling variables to expected probability changes, with causes identified when ruling variables' contributions exceed those of non-ruling variables.

## Key Results
- Successfully identifies ball movement pixels as causes of losing points in Atari Pong game
- Correctly determines that changes in subcutaneous insulin levels cause hypoglycemia events in Type-1 Diabetes simulator
- Demonstrates framework applicability to complex real-world systems without requiring causal graphs or interventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimum and maximum probabilities of future events can be computed via optimal value functions
- Mechanism: The framework reframes causal reasoning as reinforcement learning by constructing two MDPs where optimal value functions yield the minimum (grit) and maximum (reachability) probabilities of future event occurrence
- Core assumption: The state evolution follows a diffusion process with stationary infinitesimal parameters and the future event occurs at a terminal state
- Evidence anchors:
  - [abstract]: "The first lemma shows that minimum and maximum probabilities of future events can be computed via optimal value functions"
  - [section]: "Lemma 1 (Value Lemma)... Define two MDPs MΓ and MΛ identical to M with their rewards being zero if B does not happen... The optimal value functions... yield core metrics to understand causation"
  - [corpus]: Weak evidence - corpus lacks RL-based causal inference papers
- Break condition: When state transitions are non-stationary or when the event B cannot be modeled as a terminal state with deterministic reward

### Mechanism 2
- Claim: Individual state and action components' contributions to causal metrics can be decomposed using Itô's lemma
- Mechanism: The second lemma applies Itô's lemma to the diffusion process and takes conditional expectations to decompose expected changes in grit/reachability into contributions from each state/action component
- Core assumption: Grit and reachability exist and are twice differentiable in state, and the process follows a diffusion with continuous infinitesimal parameters
- Evidence anchors:
  - [abstract]: "The second lemma decomposes expected changes in these probabilities into contributions from individual state and action components"
  - [section]: "Lemma 2 (Decomposition Lemma)... The expected change of grit... is expressed by the following formula: E [∆AΓB] = sum of E{gj|A} + E{˙gj|A} + sum of E{¨gj,i|A}"
  - [corpus]: Weak evidence - corpus lacks decomposition-based causal attribution methods
- Break condition: When higher moments of stochasticity matter or when the process cannot be approximated by diffusion equations

### Mechanism 3
- Claim: Causation between events can be established by comparing contributions of ruling vs non-ruling variables to grit changes
- Mechanism: The framework defines causation when ruling variables' contributions to expected grit growth exceed those of non-ruling variables, ensuring the cause increases the probability of the effect
- Core assumption: Events are defined as changes in state/action components over homogeneous time intervals, and the minimum probability chain of events determines causation
- Evidence anchors:
  - [abstract]: "Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes"
  - [section]: "Definition 1 (Causation)... A is a cause of B if and only if... Expected grit of B strictly increases from before to after A... The contribution of A's ruling variables in the growth of B's expected grit is strictly positive and is strictly larger in magnitude than that of non-ruling variables with negative impact"
  - [corpus]: Weak evidence - corpus lacks process-based causal definitions without interventions
- Break condition: When multiple causes have similar contributions or when non-ruling variables dominate the causal effect

## Foundational Learning

- Concept: Diffusion processes and Itô calculus
  - Why needed here: The framework relies on continuous-time stochastic processes and their differential properties to model state evolution and compute causal contributions
  - Quick check question: What are the infinitesimal parameters μ and σ in a diffusion process, and how do they relate to the drift and diffusion terms in Itô's lemma?

- Concept: Reinforcement learning value functions
  - Why needed here: Optimal value functions from specially constructed MDPs provide the minimum and maximum probabilities of future events, which are core causal metrics
  - Quick check question: How does setting γ=1 and using terminal rewards of ±1 in the constructed MDPs yield the minimum/maximum probabilities of event occurrence?

- Concept: Causal attribution and decomposition
  - Why needed here: The framework requires understanding how to attribute changes in system behavior to specific components, which is achieved through mathematical decomposition of expected changes
  - Quick check question: What is the difference between ruling variables (those involved in the cause event) and non-ruling variables in the causal decomposition framework?

## Architecture Onboarding

- Component map: Raw data -> state representation -> value function estimation -> gradient computation -> contribution decomposition -> causation determination
- Critical path: Raw data → state representation → value function estimation → gradient computation → contribution decomposition → causation determination
- Design tradeoffs:
  - Computational complexity vs accuracy in numerical integration (M parameter)
  - Model-free vs model-based approach (requires system equations vs data only)
  - Discrete approximation of continuous processes vs exact solutions
  - Memory requirements for storing gradients vs recomputation
- Failure signatures:
  - High approximation error in value functions → unreliable causation results
  - Non-smooth gradients → numerical instability in decomposition
  - Insufficient data coverage of state space → extrapolation errors
  - Violation of diffusion assumptions → incorrect causal attribution
- First 3 experiments:
  1. Synthetic diffusion process with known causal structure to validate decomposition accuracy
  2. Discrete-time Markov process to test framework generalization beyond continuous diffusion
  3. Simple physical system (mass-spring-damper) with controlled interventions to compare with ground truth causation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to handle higher-order moments of stochasticity beyond the second moment?
- Basis in paper: [explicit] The paper states "The main limitation of this work is two-fold: higher moments than two of the stochasticity are dismissed..."
- Why unresolved: The current framework only considers up to second-order moments in its decomposition lemma, which may limit its applicability to systems with significant higher-order stochastic effects.
- What evidence would resolve it: A theoretical extension of the decomposition lemma that incorporates higher-order moment terms, along with empirical validation showing improved accuracy in complex stochastic systems.

### Open Question 2
- Question: How can the framework be adapted to handle partially observable systems where the full state is not available?
- Basis in paper: [explicit] The paper states "the full information state is assumed" as a limitation.
- Why unresolved: Many real-world systems have partial observability, and extending the framework to handle hidden state variables would significantly broaden its applicability.
- What evidence would resolve it: A modified version of the framework that incorporates filtering or belief state estimation techniques, validated on partially observable MDP benchmarks.

### Open Question 3
- Question: How does the framework perform in continuous-time systems compared to discrete-time approximations?
- Basis in paper: [inferred] The framework is presented for continuous-time diffusion processes but experiments use discrete-time approximations with computational micro-steps.
- Why unresolved: While discrete-time approximations are practical, they may introduce errors that affect causal identification accuracy.
- What evidence would resolve it: A comparative study showing causal identification performance between continuous-time theoretical solutions and discrete-time approximations across various time discretization levels.

## Limitations
- Diffusion process assumption may not hold for systems with discrete dynamics or non-Gaussian noise
- Computational complexity scales poorly with state dimensionality, limiting applicability to high-dimensional systems
- Requires sufficient data coverage of state space to avoid extrapolation errors in complex applications

## Confidence
- Theoretical framework: Medium
- Empirical validation: Low (limited to two specific domains)
- Practical applicability: Medium (depends on system characteristics and data availability)

## Next Checks
1. Test the framework on a synthetic diffusion process with known ground truth causal structure to quantify decomposition accuracy and identify failure modes
2. Evaluate performance on discrete-time Markov processes to assess generalization beyond continuous diffusion assumptions
3. Apply the method to a simple physical system (e.g., mass-spring-damper) with controlled interventions to compare against established causal inference methods and ground truth causation