---
ver: rpa2
title: 'SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation Functions'
arxiv_id: '2407.04149'
source_url: https://arxiv.org/abs/2407.04149
tags:
- functions
- sinekan
- networks
- layer
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SineKAN, a Kolmogorov-Arnold Network (KAN)
  architecture that replaces learnable B-Spline activation functions with grids of
  re-weighted sine functions. The proposed method employs a phase shift initialization
  strategy to stabilize model scaling across different grid sizes and depths, ensuring
  consistent performance.
---

# SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation Functions

## Quick Facts
- arXiv ID: 2407.04149
- Source URL: https://arxiv.org/abs/2407.04149
- Reference count: 31
- Replaces learnable B-Spline activation functions with grids of re-weighted sine functions, achieving 4-9x faster inference with comparable or better accuracy

## Executive Summary
SineKAN introduces a Kolmogorov-Arnold Network architecture that uses sinusoidal activation functions instead of traditional B-Splines. The key innovation is a phase shift initialization strategy that stabilizes model scaling across different grid sizes and depths, ensuring consistent performance. The authors demonstrate that SineKAN achieves comparable or better numerical performance than B-SplineKAN on the MNIST benchmark, with improved accuracy at higher model depths. Additionally, SineKAN is 4-9 times faster than B-SplineKAN across all batch sizes, hidden layer sizes, and depths tested, making it a promising candidate for scalable, high-depth models.

## Method Summary
SineKAN replaces learnable B-Spline activation functions with grids of re-weighted sine functions. The architecture uses a phase shift initialization strategy where grid weights are initialized as sums of sine functions with phase shifts following Σg i=1 sin(x + i ∗ π/(g + 1)). This stabilizes the scaling ratio between consecutive grid sizes across arbitrary input values. The model maintains consistent performance through a recursive scaling function R(g) = A ∗ g^-K + C for subsequent layers. Forward propagation involves applying sin(x + phase) * weight for each grid point, summing all contributions, and adding bias terms.

## Key Results
- Achieves 4-9x faster inference than B-SplineKAN across all batch sizes (16-512) and depths
- Comparable or better accuracy on MNIST classification, with improved performance at depths ≥2
- Demonstrates stable scaling properties across different grid sizes and model depths

## Why This Works (Mechanism)

### Mechanism 1
Grid phase shift initialization stabilizes scaling ratios across grid sizes and depths by using Σg i=1 sin(x + i ∗ π/(g + 1)) instead of π/g, making the sum of sine functions independent of input value and stabilizing the ratio between consecutive grid sizes.

### Mechanism 2
Sinusoidal activation functions provide superior multi-layer scaling through the functional form y = a ∗ sin(ω ∗ sin(x)) in deeper layers, which introduces an additional degree of freedom allowing better approximation of complex mappings compared to fixed B-Spline basis.

### Mechanism 3
Sine functions provide 4-9x inference speedup due to their computational simplicity compared to B-Splines, which require De Boor's algorithm for evaluation.

## Foundational Learning

- **Kolmogorov-Arnold Representation Theorem**: States that any multivariate function can be represented as a sum of continuous univariate functions - needed as SineKAN is based on this theorem
  - Quick check: What is the mathematical form of the Kolmogorov-Arnold representation theorem?

- **Activation function scaling in deep networks**: Understanding why phase initialization and scaling matter for maintaining stable activations across layers
  - Quick check: What happens to activation distributions when layer outputs don't scale properly in deep networks?

- **Fourier series and periodic functions**: SineKAN uses sinusoidal functions which are periodic, and understanding their properties is crucial for proper initialization
  - Quick check: How does the sum of equally spaced sine functions behave as a function of grid size?

## Architecture Onboarding

- **Component map**: Input layer → Grid phase shift initialization → Learnable sine weights → Summation → Output layer
- **Critical path**: 1) Initialize first layer with mean 0, std 0.4 phase shifts 2) Apply recursive scaling function R(g) = A ∗ g^-K + C for subsequent layers 3) Forward pass: apply sin(x + phase) * weight for each grid point 4) Sum all contributions and add bias
- **Design tradeoffs**: Sine vs B-Spline: Speed vs potential expressiveness for certain function classes; Grid size: Larger grids increase parameter count but may improve approximation quality; Phase initialization: Proper initialization is critical for stability but adds complexity
- **Failure signatures**: Vanishing/exploding activations in deep models (indicates scaling issues); Poor convergence during training (may indicate incorrect learning rate or initialization); Numerical instability in grid phase calculations
- **First 3 experiments**: 1) Single layer MNIST classification with varying grid sizes (2, 4, 8, 16) to verify scaling properties 2) Multi-layer MNIST classification (1-4 layers) to test depth scaling compared to B-SplineKAN 3) Speed benchmark with different batch sizes (16, 32, 64, 128, 256, 512) to verify the 4-9x speedup claim

## Open Questions the Paper Calls Out

- **How does SineKAN performance compare to other KAN variants (wavelet, Chebyshev, rational, etc.) across a broader range of tasks and benchmarks?**
  - Basis: The paper states that other activation functions like wavelets, Chebyshev polynomials, and rational functions have shown superior speed and scaling at comparable numerical performance
  - Evidence needed: Systematic benchmarking of SineKAN against other KAN variants on multiple datasets and tasks

- **What is the optimal learning rate strategy for SineKAN across different depths and grid sizes, and how does it compare to other KAN implementations?**
  - Basis: The paper notes that SineKAN had a significantly different optimal learning rate compared to B-SplineKAN
  - Evidence needed: Comprehensive grid search experiments to determine optimal learning rates for SineKAN across different depths, grid sizes, and tasks

- **What are the theoretical limits of SineKAN's scalability in terms of depth, width, and batch size, and how do these compare to theoretical limits of other KAN implementations?**
  - Basis: The paper discusses practical advantages but doesn't explore theoretical limits of scalability
  - Evidence needed: Theoretical analysis of representational capacity and computational complexity as a function of depth, width, and batch size

## Limitations
- Only validated on MNIST classification benchmark, limiting generalizability to other domains
- Computational advantage may diminish on hardware with specialized B-Spline acceleration
- Phase shift initialization mechanism lacks empirical ablation studies showing its necessity

## Confidence
- **High Confidence**: 4-9x inference speedup claim is well-supported by Figure 8 with consistent measurements across batch sizes
- **Medium Confidence**: Comparable/better accuracy on MNIST is demonstrated but only against B-SplineKAN, not against other established architectures
- **Low Confidence**: Claims about superior multi-layer scaling beyond MNIST remain speculative without validation on more complex tasks

## Next Checks
1. **Ablation Study**: Remove phase shift initialization and compare performance degradation across grid sizes to quantify its contribution
2. **Cross-Domain Testing**: Evaluate SineKAN on CIFAR-10 and regression tasks to assess broader applicability beyond MNIST
3. **Scaling Analysis**: Systematically vary grid sizes beyond 8 to identify the point where computational advantages plateau or reverse