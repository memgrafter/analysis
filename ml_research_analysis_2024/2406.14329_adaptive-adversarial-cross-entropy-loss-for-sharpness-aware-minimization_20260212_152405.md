---
ver: rpa2
title: Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization
arxiv_id: '2406.14329'
source_url: https://arxiv.org/abs/2406.14329
tags:
- loss
- perturbation
- gradient
- aace
- cross-entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Sharpness-Aware Minimization
  (SAM) for deep learning model generalization. The authors identify that SAM's perturbation
  step, based on normalized gradient of cross-entropy loss, becomes unstable and risks
  gradient vanishing as models converge.
---

# Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2406.14329
- Source URL: https://arxiv.org/abs/2406.14329
- Reference count: 0
- Primary result: AACE-SAM achieves 84.33% accuracy on CIFAR-100 vs 83.52% for standard SAM

## Executive Summary
This paper addresses limitations in Sharpness-Aware Minimization (SAM) where the perturbation step becomes unstable as models converge, leading to gradient vanishing and poor exploration. The authors propose Adaptive Adversarial Cross-Entropy (AACE) loss to replace standard cross-entropy in SAM's perturbation step, ensuring consistent perturbation direction and avoiding gradient diminishing issues. Experiments show AACE-SAM outperforms standard SAM and SGD across multiple datasets and architectures, with significant improvements in generalization.

## Method Summary
AACE-SAM modifies the standard SAM optimizer by replacing the cross-entropy loss in the perturbation step with an adaptive adversarial cross-entropy loss. The AACE loss uses adversarial targets that increase the probability gap between predicted and target distributions, causing the gradient magnitude to grow as the model converges. Additionally, the perturbation generation function removes gradient normalization, allowing larger perturbations at convergence. The standard weight update uses the perturbed parameters, similar to SAM, but with these modifications to improve exploration and maintain consistent perturbation directions throughout training.

## Key Results
- AACE-SAM achieves 84.33% accuracy on CIFAR-100 with Wide ResNet vs 83.52% for standard SAM
- Consistent improvements across CIFAR-10, Fashion-MNIST, and Food101 datasets
- PyramidNet architecture shows similar improvements with AACE-SAM over standard SAM
- AACE-SAM maintains higher gradient magnitudes and consistent perturbation directions throughout training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AACE loss gradient magnitude increases as model converges, while cross-entropy gradient diminishes.
- Mechanism: Adversarial labeling increases probability gaps between predicted and target distributions, preventing gradient vanishing.
- Core assumption: Adversarial labeling strategy reliably increases loss magnitude as convergence approaches.
- Evidence anchors:
  - [abstract]: "AACE loss and its gradient uniquely increase as the model nears convergence, ensuring consistent perturbation direction and addressing the gradient diminishing issue."
  - [section]: "Our AACE loss with adversarial targets increases due to the growth of the gaps between the predicted probabilities and the newly defined adversarial labels. Hence, the gradient for AACE loss remains high even at a nearly optimum stage."

### Mechanism 2
- Claim: Removing gradient normalization in perturbation step increases perturbation magnitude at convergence.
- Mechanism: Without normalization, perturbation magnitude scales with growing AACE gradient, leading to larger perturbations at convergence.
- Core assumption: Larger perturbations at convergence improve exploration without destabilizing training.
- Evidence anchors:
  - [abstract]: "Additionally, a novel perturbation-generating function utilizing AACE loss without normalization is proposed, enhancing the model's exploratory capabilities in near-optimum stages."
  - [section]: "Since we prefer to enlarge the magnitude of the perturbation as the model converges, we also proposed to not normalize the gradient and define a new perturbation generating function as gAACE(w) = −▽LAACE(w)."

### Mechanism 3
- Claim: Stable, large gradients ensure consistent perturbation direction throughout training.
- Mechanism: AACE's growing gradients provide consistent direction signals even as cross-entropy gradients vanish near convergence.
- Core assumption: Consistent perturbation direction contributes to finding flatter minima and better generalization.
- Evidence anchors:
  - [abstract]: "ensuring consistent perturbation direction and addressing the gradient diminishing issue."
  - [section]: "The larger and growing gradient gives rise to a stronger and more stable direction of the perturbation in SAM's perturbation step."

## Foundational Learning

- Concept: Sharpness-Aware Minimization (SAM)
  - Why needed here: AACE is designed as a replacement for cross-entropy loss specifically within SAM's perturbation step, so understanding SAM's mechanics is essential.
  - Quick check question: What are the two main steps of SAM and how does it differ from standard SGD?

- Concept: Adversarial examples and loss functions
  - Why needed here: AACE creates adversarial targets to increase loss magnitude, requiring understanding of how adversarial objectives work.
  - Quick check question: How does the adversarial labeling in AACE differ from standard one-hot encoding?

- Concept: Gradient-based optimization and perturbation methods
  - Why needed here: AACE modifies how perturbations are generated and applied, requiring understanding of gradient mechanics and perturbation generation.
  - Quick check question: What is the role of gradient normalization in perturbation methods, and what changes when it's removed?

## Architecture Onboarding

- Component map: Base model -> AACE loss computation -> AACE gradient calculation -> Perturbation generation (no normalization) -> Perturbed parameters -> Cross-entropy loss computation -> Weight update

- Critical path:
  1. Compute AACE loss with adversarial labels
  2. Calculate gradient of AACE loss
  3. Generate perturbation using AACE gradient (no normalization)
  4. Apply perturbation to model parameters
  5. Compute standard cross-entropy loss at perturbed position
  6. Update model weights using this loss

- Design tradeoffs:
  - Stability vs. exploration: Removing normalization increases exploration but may reduce stability
  - Computational overhead: AACE adds minimal computation but requires modified loss calculation
  - Hyperparameter sensitivity: The ρ parameter becomes more important without normalization

- Failure signatures:
  - Training instability or divergence (perturbation too large)
  - No improvement over standard SAM (AACE not effective)
  - Gradient vanishing still occurs (adversarial labeling ineffective)

- First 3 experiments:
  1. Compare AACE-SAM vs standard SAM on CIFAR-10 with Wide ResNet (ρ=0.2, no normalization)
  2. Visualize AACE loss and gradient magnitude vs standard cross-entropy during training
  3. Test different ρ values for AACE-SAM to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but leaves several important areas unexplored, particularly regarding the theoretical justification for AACE's behavior and its performance relative to other SAM variants.

## Limitations
- Limited exploration of optimal ρ values across different architectures and datasets
- No theoretical analysis of why AACE gradients increase as models converge
- Absence of comparison to other SAM variants like ASAM and GA-SAM
- Results primarily demonstrated on image classification tasks, limiting generalizability

## Confidence

- **High confidence**: The empirical results showing improved accuracy on CIFAR-100 (84.33% vs 83.52% for SAM) and other datasets are well-supported by experimental data.
- **Medium confidence**: The theoretical mechanism explaining why AACE loss gradients increase while cross-entropy gradients diminish is plausible but relies on assumptions about the adversarial labeling's effectiveness that weren't exhaustively tested.
- **Medium confidence**: The claim that removing normalization improves exploration at convergence is supported by results but the optimal balance point between exploration and stability remains unclear.

## Next Checks

1. **Gradient magnitude tracking**: Monitor and visualize AACE gradient magnitudes versus cross-entropy gradients throughout entire training process across multiple runs to verify the claimed growth pattern.
2. **Hyperparameter sensitivity analysis**: Systematically test AACE-SAM performance across a range of ρ values (e.g., 0.1 to 0.5) and with/without normalization to identify optimal configurations and stability boundaries.
3. **Generalization to other tasks**: Evaluate AACE-SAM on non-image classification tasks (e.g., NLP or tabular data) to test whether the gradient amplification mechanism generalizes beyond the tested domains.