---
ver: rpa2
title: 'CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language
  Models'
arxiv_id: '2409.02119'
source_url: https://arxiv.org/abs/2409.02119
tags:
- matrix
- lora
- cora
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoRA, a method to optimize Low-Rank Adaptation
  (LoRA) for large language model fine-tuning by leveraging shared knowledge across
  models. The key idea is to replace LoRA's matrix B with a common subspace extracted
  from multiple fine-tuned models, reducing parameters by 50% while maintaining or
  improving performance.
---

# CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models

## Quick Facts
- arXiv ID: 2409.02119
- Source URL: https://arxiv.org/abs/2409.02119
- Reference count: 34
- Key result: Reduces LoRA parameters by 50% while maintaining or improving performance on text-to-code and instruction-following tasks

## Executive Summary
This paper introduces CoRA, a method to optimize Low-Rank Adaptation (LoRA) for fine-tuning large language models by leveraging shared knowledge across multiple fine-tuned models. The key innovation is replacing LoRA's matrix B with a common subspace extracted via SVD from concatenated Q, K, V matrices of multiple fine-tuned models. This approach reduces trainable parameters by 50% while maintaining or improving performance on downstream tasks. The paper validates this approach on text-to-code generation and instruction-following tasks, showing significant improvements in standard evaluation metrics while achieving substantial parameter reduction.

## Method Summary
CoRA extracts a common basis matrix from multiple fine-tuned models using SVD on concatenated Q, K, V matrices, then replaces LoRA's matrix B with this subspace. Two training strategies are explored: freezing the common basis matrix during training (FB) or using it as an enhanced initialization (TB). The method is evaluated on Llama2-13B across text-to-code and instruction-following tasks, comparing against standard LoRA while measuring performance and parameter efficiency.

## Key Results
- On the yahma dataset, CoRA outperformed LoRA in ROUGE and BERT scores while using 50% fewer parameters
- On the text-to-code dataset, CoRA showed significant improvements in ROUGE, METEOR, and SacreBLEU scores
- Manual code evaluation confirmed high readability and correctness of generated code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The common basis matrix extracted via SVD captures shared structural patterns across fine-tuned models, allowing CoRA to maintain performance while halving trainable parameters.
- Mechanism: SVD decomposes the concatenated Q, K, V matrices into singular vectors; retaining the top-r vectors creates a low-rank approximation that spans the shared subspace of multiple fine-tuned models. Replacing LoRA's B matrix with this subspace removes redundant parameters while preserving essential model behavior.
- Core assumption: Fine-tuned models for different tasks retain a common underlying knowledge space that can be represented by a low-rank structure.
- Evidence anchors:
  - [abstract] "leverage shared knowledge to optimize LoRA training by substituting its matrix B with a common subspace from large models"
  - [section] "informed by these comparative results, we have meticulously chosen SVD as the dimensionality reduction technique"
  - [corpus] Weak—no direct citations; assumption supported only by the paper's own empirical results.
- Break condition: If downstream tasks are too diverse or specialized, the common subspace may not generalize, leading to degraded performance.

### Mechanism 2
- Claim: Freezing the common basis matrix after replacement reduces trainable parameters without harming performance because the subspace is already well-adapted to the task distribution.
- Mechanism: The extracted common basis matrix is frozen to prevent parameter updates, effectively halving the number of trainable LoRA parameters (only matrix A trains). This works because the common basis already encodes useful adaptation patterns for the target tasks.
- Core assumption: The common basis matrix derived from multiple fine-tuned models provides a robust initialization that generalizes across similar downstream tasks.
- Evidence anchors:
  - [abstract] "Freezing the substitute matrix B to halve parameters while training matrix A for specific tasks"
  - [section] "the first approach achieves the same efficacy as the original LoRA fine-tuning while being more efficient than halving parameters"
  - [corpus] Weak—no external validation; relies on paper's ablation study results.
- Break condition: If task similarity drops or the common basis overfits to a specific subset, freezing may prevent necessary adaptation.

### Mechanism 3
- Claim: Using the common basis matrix as an enhanced initialization for B improves training outcomes by providing a better starting point than zero initialization.
- Mechanism: The common basis matrix replaces LoRA's zero-initialized B, giving the model a non-trivial starting point that accelerates convergence and improves final performance. Matrix A is still randomly initialized to adapt to task-specific details.
- Core assumption: A well-initialized B matrix accelerates learning and helps the model retain more of the pre-trained knowledge while adapting to new tasks.
- Evidence anchors:
  - [abstract] "Using the substitute matrix B as an enhanced initial state for the original matrix B, achieving improved results with the same parameters"
  - [section] "these metrics also well confirm our original hypothesis that when trained in conjunction with the A matrix, the common basis matrix can better retain the knowledge"
  - [corpus] Weak—no supporting citations; based on internal experiment comparisons.
- Break condition: If the common basis is poorly aligned with the target task, initialization may bias the model away from optimal solutions.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) and its role in parameter-efficient fine-tuning
  - Why needed here: CoRA directly modifies LoRA's architecture by replacing or initializing the B matrix; understanding LoRA is prerequisite to grasping CoRA's innovation.
  - Quick check question: In LoRA, what is the mathematical form of the low-rank update applied to the weight matrix W?
    Answer: W = W₀ + AB, where A ∈ ℝ³ᵈᵐᵒᵈᵉˡˣʳ and B ∈ ℝʳˣᵈᵏ.

- Concept: Singular Value Decomposition (SVD) and dimensionality reduction
  - Why needed here: CoRA uses SVD to extract the common subspace from concatenated Q, K, V matrices across multiple fine-tuned models.
  - Quick check question: What does SVD decompose a matrix into, and how is it used for dimensionality reduction?
    Answer: SVD decomposes a matrix into U, Σ, Vᵀ; dimensionality reduction keeps only the top-r singular values and corresponding vectors from Vᵀ.

- Concept: Evaluation metrics for text generation (ROUGE, METEOR, SacreBLEU, BERTScore)
  - Why needed here: CoRA's effectiveness is validated using these metrics; understanding them is essential to interpret experimental results.
  - Quick check question: Which metric evaluates semantic similarity using contextual embeddings?
    Answer: BERTScore.

## Architecture Onboarding

- Component map:
  Pre-trained frozen LLM (Llama2-13B) -> Multiple fine-tuned downstream models (source of Q, K, V matrices) -> SVD-based common subspace extractor -> LoRA module with replaceable B matrix -> Two training strategies (FB/TB) -> Evaluation pipeline

- Critical path:
  1. Extract Q, K, V matrices from multiple fine-tuned models
  2. Concatenate into W₀ = [WQ, WK, WV]ᵀ
  3. Apply SVD to W₀ and retain top-r right singular vectors as common basis
  4. Replace LoRA's B matrix with common basis
  5. Choose strategy: freeze B (FB) or train B (TB)
  6. Fine-tune on target task
  7. Evaluate with standard metrics

- Design tradeoffs:
  - FB vs TB: FB halves parameters but may underfit; TB uses more parameters but potentially better adapts
  - r (rank) selection: Higher r increases capacity but also parameters; CoRA aims to keep r small
  - Number of source models: More models may improve common basis quality but increase extraction cost

- Failure signatures:
  - Performance drops when using FB on highly diverse tasks
  - Overfitting when TB is used with too high r
  - Poor common basis extraction when source models are too dissimilar

- First 3 experiments:
  1. Replace B with zero matrix and freeze; verify performance drop to confirm B's importance
  2. Replace B with ones matrix and freeze; check if constant initialization helps or hurts
  3. Replace B with randomly initialized matrix and freeze; compare against CoRA's common basis to validate its advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the common subspace matrix B be further optimized or compressed beyond the SVD approach to achieve even greater parameter reduction without compromising performance?
- Basis in paper: [explicit] The paper discusses using SVD for dimensionality reduction of the common subspace matrix B but suggests future exploration of more optimal structures for employing this matrix.
- Why unresolved: The current approach using SVD achieves good results, but there may be more efficient methods for extracting and utilizing the common subspace that have not been explored.
- What evidence would resolve it: Experiments comparing CoRA with alternative dimensionality reduction techniques (e.g., PCA, autoencoders) or novel matrix compression methods, demonstrating improved parameter reduction or performance.

### Open Question 2
- Question: How does the effectiveness of the common subspace matrix B vary across different types of downstream tasks (e.g., classification, generation, reasoning)?
- Basis in paper: [inferred] The paper demonstrates CoRA's effectiveness on text-to-code and instruction-following tasks but does not explore a wide range of task types.
- Why unresolved: The experiments were limited to specific task types, and it's unclear if the common subspace concept generalizes across diverse NLP tasks.
- What evidence would resolve it: Extensive experiments applying CoRA to various task types (e.g., summarization, question answering, sentiment analysis) with comparative analysis of performance gains across tasks.

### Open Question 3
- Question: What is the optimal number of fine-tuned models needed to extract a representative common subspace matrix B, and how does this number scale with model size?
- Basis in paper: [explicit] The paper experiments with extracting common matrices from 1-5 models and observes stable performance, but does not determine the optimal number.
- Why unresolved: While performance is stable across different numbers of models, the trade-off between extraction accuracy and computational cost is not fully explored.
- What evidence would resolve it: Systematic experiments varying the number of source models (both fine-tuned and base models) across different LLM scales, measuring the quality of the extracted common subspace and its impact on downstream task performance.

## Limitations
- Core claims rely heavily on empirical results from a limited set of tasks and datasets
- Validation is primarily internal with no external benchmarks or comparisons to alternative subspace extraction methods
- The assumption of a common subspace across fine-tuned models is asserted but not rigorously proven
- Manual code evaluation introduces potential subjectivity bias that isn't quantified

## Confidence

### High confidence:
- CoRA can achieve comparable performance to standard LoRA while reducing parameters by 50% on the tested datasets

### Medium confidence:
- The generalization claim that a common subspace exists across diverse fine-tuned models and that CoRA's mechanism is broadly applicable

### Low confidence:
- The assertion that CoRA's improvements are primarily due to the common subspace mechanism rather than other factors like initialization effects

## Next Checks

1. **Cross-task generalization test**: Apply CoRA to a diverse set of downstream tasks (e.g., summarization, question answering, sentiment analysis) and compare against standard LoRA and other parameter-efficient methods to assess whether the common subspace assumption holds across domains.

2. **Source model sensitivity analysis**: Systematically vary the number and diversity of source models used for common basis extraction, measuring how this affects downstream performance and parameter efficiency to identify optimal configurations.

3. **Alternative subspace extraction comparison**: Replace SVD with other dimensionality reduction techniques (PCA, autoencoders, Nyström approximation) and evaluate whether SVD provides unique advantages or if the gains are due to general subspace learning rather than the specific SVD approach.