---
ver: rpa2
title: Optimal sampling for least-squares approximation
arxiv_id: '2409.02342'
source_url: https://arxiv.org/abs/2409.02342
tags:
- sampling
- approximation
- function
- where
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys recent advances in optimal sampling for least-squares
  approximation in arbitrary linear spaces. The core idea is to use the Christoffel
  function as a key quantity to analyze and construct sampling strategies that possess
  near-optimal sample complexity, scaling log-linearly with the dimension of the approximation
  space.
---

# Optimal sampling for least-squares approximation

## Quick Facts
- arXiv ID: 2409.02342
- Source URL: https://arxiv.org/abs/2409.02342
- Reference count: 40
- Key outcome: Introduces Christoffel sampling for near-optimal least-squares approximation with sample complexity scaling as O(n log n) for arbitrary linear spaces

## Executive Summary
This paper surveys recent advances in optimal sampling for least-squares approximation in arbitrary linear spaces. The core idea is to use the Christoffel function as a key quantity to analyze and construct sampling strategies that possess near-optimal sample complexity, scaling log-linearly with the dimension of the approximation space. The author introduces the concept of Christoffel sampling, which involves drawing samples from a probability measure proportional to the Christoffel function. This approach is shown to be near-optimal for any linear approximation space and significantly improves upon standard Monte Carlo sampling in many cases, especially when dealing with high-dimensional polynomial approximation on unbounded domains.

## Method Summary
The paper presents a framework for optimal sampling in least-squares approximation using the Christoffel function. The method involves constructing a sampling measure proportional to the Christoffel function divided by the dimension of the approximation space, then drawing independent samples from this measure. These samples are used to solve a weighted least-squares problem, which achieves near-optimal sample complexity scaling as O(n log n). The approach extends to general linear measurements and nonlinear approximation spaces through the concept of generalized Christoffel functions.

## Key Results
- Christoffel sampling achieves near-optimal sample complexity O(n log n) for arbitrary linear approximation spaces
- Standard Monte Carlo sampling can have arbitrarily bad sample complexity depending on the maximal behavior of the Christoffel function
- The framework extends to general linear measurements and nonlinear approximation spaces via generalized Christoffel functions
- Christoffel sampling significantly outperforms Monte Carlo in high-dimensional polynomial approximation on unbounded domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Christoffel function acts as a sampling density weighting that equalizes the contribution of each basis function in the approximation space, leading to near-optimal sample complexity scaling as n log n.
- Mechanism: By defining the sampling measure proportional to the Christoffel function K(P)(x), the least-squares problem achieves uniform stability constants (α_w and β_w close to 1) with high probability, ensuring both accuracy and stability.
- Core assumption: The Christoffel function is strictly positive almost everywhere on the support of the underlying measure ρ.
- Evidence anchors:
  - [abstract]: "We introduce the Christoffel function as a key quantity to analyze and construct sampling strategies that possess near-optimal sample complexity, scaling log-linearly with the dimension of the approximation space."
  - [section 6.1]: "The key tool in our analysis is the Christoffel function of P... Christoffel functions – or, more precisely, their reciprocals – are also fundamentally associated with random sampling for least-squares approximation."
- Break condition: If K(P)(x) = 0 on a set of positive measure, the weight function becomes undefined or infinite, breaking the sampling scheme.

### Mechanism 2
- Claim: Monte Carlo sampling (i.i.d. from ρ) can have arbitrarily bad sample complexity depending on the maximal behavior of the Christoffel function K(P).
- Mechanism: The sample complexity for Monte Carlo is governed by κ(P) = ||K(P)||_L^∞(ρ), which can be exponentially large in dimension d for polynomial spaces on tensor-product domains.
- Core assumption: The orthonormal basis is not uniformly bounded, leading to highly localized basis functions.
- Evidence anchors:
  - [section 7.2]: "Unfortunately, bounded orthonormal bases are quite rare in practice... It is also straightforward to construct subspaces P for which κ(P) can be arbitrarily large in comparison to n."
  - [section 7.3]: "Because of (5.6), (7.1) and (7.3), for any subspace PS = span {Ψ_ν : ν ∈ S} of Legendre polynomials, one has κ(PS) = ∑_ν∈S ∏_k=1^d (ν_k + 1/2), which can be arbitrarily large in comparison to n = |S|."
- Break condition: When κ(P) ≤ c·n for some constant c, Monte Carlo sampling is already near-optimal and no improvement is possible.

### Mechanism 3
- Claim: The generalized Christoffel function extends the sampling framework to arbitrary linear measurements and nonlinear approximation spaces while preserving the sample complexity benefits.
- Mechanism: By defining K(P,L)(θ) = sup_{f∈P,f≠0} ||L(θ)(f)||²_Y / ||f||²_X, the sampling measure optimizes the weighted average of this function, ensuring stability and accuracy for general operator-valued measurements.
- Core assumption: The sampling operators preserve norms up to constants (Assumption 9.2) and the approximation space satisfies a union-of-subspaces model (Assumption 9.3).
- Evidence anchors:
  - [section 9.3]: "The generalized Christoffel function of P with respect to L is the function K = K(P,L) : D → R defined by K(θ) = sup { ||L(θ)(f)||²_Y / ||f||²_X : f ∈ P, f ≠ 0 }, ∀θ ∈ D."
  - [section 9.4]: "This trivially holds with d = 1 and Q_1 = P' = P when P is an n-dimensional subspace. In general, Assumption 9.3 is a extension of the union-of-subspaces model, which is well-known in the context of compressed sensing [21, 51]."
- Break condition: If the sampling operators do not satisfy the nondegeneracy condition or the approximation space does not fit the union-of-subspaces model, the sample complexity bounds may not hold.

## Foundational Learning

- Concept: Lebesgue spaces L^p_ρ(D) and their norms
  - Why needed here: The approximation error is measured in L^2_ρ(D) norm, and the Christoffel function is defined using this space.
  - Quick check question: What is the relationship between the L^2_ρ-norm and the discrete semi-norm induced by sample points?

- Concept: Orthonormal bases and their properties
  - Why needed here: The Christoffel function has an explicit expression as the sum of squared absolute values of orthonormal basis functions.
  - Quick check question: How does orthonormality simplify the expression for the Christoffel function?

- Concept: Matrix concentration inequalities (e.g., Matrix Chernoff bound)
  - Why needed here: Used to bound the discrete stability constants α_w and β_w with high probability.
  - Quick check question: What is the key difference between scalar and matrix concentration inequalities?

- Concept: Reproducing kernel Hilbert spaces
  - Why needed here: The Christoffel function is the diagonal of the reproducing kernel of the approximation space.
  - Quick check question: How does the reproducing kernel property relate to pointwise evaluations of functions?

## Architecture Onboarding

- Component map:
  Measure space (D, D, ρ) -> Approximation space P -> Sampling measures {μ_i} -> Weight function w -> Sample points {x_i} -> Measurement vector b -> Least-squares matrix A -> Discrete stability constants α_w, β_w

- Critical path:
  1. Define the approximation space P and choose an orthonormal basis
  2. Compute or estimate the Christoffel function K(P)
  3. Construct the sampling measure μ proportional to K(P)/n
  4. Draw sample points independently from μ
  5. Solve the weighted least-squares problem
  6. Analyze the error bounds using the discrete stability constants

- Design tradeoffs:
  - Using the optimal weight function w(x) = n/K(P)(x) vs. the bounded version w(x) = (1/2 + K(P)(x)/(2n))^(-1)
  - Independent vs. non-independent sampling strategies (e.g., volume sampling)
  - Linear vs. nonlinear approximation spaces
  - Pointwise vs. general linear measurements

- Failure signatures:
  - If α_w is too small, the least-squares solution is unstable and sensitive to noise
  - If β_w is too large, the condition number of A is poor, leading to numerical instability
  - If the Christoffel function is not strictly positive, the sampling measure is undefined
  - If the approximation space is too complex relative to the number of samples, the error bounds degrade

- First 3 experiments:
  1. Implement the weighted least-squares approximation with Monte Carlo sampling for a simple polynomial space (e.g., Legendre polynomials on [-1,1]) and measure the error vs. number of samples
  2. Implement Christoffel sampling for the same polynomial space and compare the error vs. number of samples to the Monte Carlo case
  3. Extend the implementation to a nonlinear approximation space (e.g., sparse polynomial approximation) and test the generalized Christoffel sampling framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal choice of anisotropy parameter 'a' for the hyperbolic cross index set in high-dimensional polynomial approximation?
- Basis in paper: The paper discusses the use of hyperbolic cross index sets (SHC_p,a) with anisotropy parameters a_k > 0, but does not provide guidance on how to choose these parameters optimally.
- Why unresolved: The paper focuses on the Christoffel function and its applications, but does not delve into the specific details of choosing the best anisotropy parameters for different types of functions or domains.
- What evidence would resolve it: Empirical studies comparing the performance of different anisotropy parameter choices for various function classes and domains, or theoretical analysis deriving optimal anisotropy parameters based on function properties.

### Open Question 2
- Question: How does the Christoffel sampling strategy perform in the case of complex nonlinear approximation spaces, such as spaces of deep neural networks or low-rank tensor networks?
- Basis in paper: The paper mentions that Christoffel sampling is not well-suited for "complex" approximation spaces, but does not provide a detailed analysis of its performance in these cases.
- Why unresolved: The paper primarily focuses on linear approximation spaces and does not explore the challenges and potential solutions for applying Christoffel sampling to more complex nonlinear spaces.
- What evidence would resolve it: Numerical experiments comparing the performance of Christoffel sampling with other methods for various complex nonlinear approximation spaces, or theoretical analysis of the conditions under which Christoffel sampling is effective for these spaces.

### Open Question 3
- Question: What are the optimal sampling strategies for hierarchical or adaptive approximation schemes using Christoffel sampling?
- Basis in paper: The paper briefly mentions the possibility of using Christoffel sampling in adaptive schemes but does not provide a detailed analysis of the optimal strategies for this case.
- Why unresolved: The paper focuses on non-adaptive sampling strategies and does not explore the challenges and potential solutions for adapting the sampling strategy based on previous approximations.
- What evidence would resolve it: Theoretical analysis of the optimal sampling strategies for hierarchical or adaptive schemes, or numerical experiments comparing the performance of different adaptive strategies using Christoffel sampling.

## Limitations
- The theoretical guarantees rely on the assumption that the Christoffel function is strictly positive almost everywhere, which may fail for pathological examples
- The paper focuses primarily on linear approximation spaces, with limited discussion of nonlinear spaces
- The extension to nonlinear spaces relies on strong assumptions (9.2 and 9.3) that may not hold for all practical scenarios

## Confidence

**High Confidence**: The core mechanism of Christoffel sampling achieving near-optimal sample complexity (O(n log n)) for linear approximation spaces.

**Medium Confidence**: The comparison between Christoffel sampling and Monte Carlo sampling, particularly the claim that Monte Carlo can have arbitrarily bad sample complexity.

**Low Confidence**: The extension of the Christoffel sampling framework to nonlinear approximation spaces due to the strong assumptions required.

## Next Checks

1. **Numerical validation of Christoffel sampling for high-dimensional polynomial approximation**: Implement Christoffel sampling for polynomial approximation on a tensor-product domain in dimensions d ≥ 5 and compare the sample complexity and approximation error to Monte Carlo sampling. Verify that the sample complexity scales as O(n log n) as predicted by the theory.

2. **Testing the limits of the Christoffel function positivity assumption**: Construct an example of an approximation space where the Christoffel function vanishes on a set of positive measure and analyze how this affects the performance of Christoffel sampling. Compare the results to the theoretical predictions in Section 5.

3. **Empirical evaluation of Christoffel sampling for nonlinear approximation spaces**: Implement the generalized Christoffel sampling framework for a nonlinear approximation space, such as sparse polynomial approximation or a neural network approximation space. Verify that the sample complexity bounds hold empirically and compare the performance to other sampling strategies for nonlinear spaces.