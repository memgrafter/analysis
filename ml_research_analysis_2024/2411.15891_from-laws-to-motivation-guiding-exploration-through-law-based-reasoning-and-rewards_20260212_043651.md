---
ver: rpa2
title: 'From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and
  Rewards'
arxiv_id: '2411.15891'
source_url: https://arxiv.org/abs/2411.15891
tags:
- agent
- agents
- reward
- wood
- stone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve exploration in agents
  by extracting laws from interaction records and using them as internal motivation.
  The authors leverage LLMs to infer experience from human gameplay data in Crafter,
  which approximates the game's underlying laws.
---

# From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards

## Quick Facts
- arXiv ID: 2411.15891
- Source URL: https://arxiv.org/abs/2411.15891
- Authors: Ziyu Chen; Zhiqing Xiao; Xinbei Jiang; Junbo Zhao
- Reference count: 40
- One-line primary result: LLM agents guided by law-based experience outperform those using only benchmark paper information or action names alone, and RL agents improve when trained with these law-based rewards compared to basic health-based rewards.

## Executive Summary
This paper introduces a method to improve exploration in agents by extracting laws from interaction records and using them as internal motivation. The authors leverage LLMs to infer experience from human gameplay data in Crafter, which approximates the game's underlying laws. These experience are expressed as text describing preconditions, costs, and benefits for each action. For LLM agents, the experience is incorporated into prompts to improve reasoning. For RL agents, the experience is used to generate reward functions that provide conditional rewards based on action validity. Experiments show that LLM agents guided by this experience outperform those using only benchmark paper information or action names alone. RL agents also improve when trained with these law-based rewards compared to basic health-based rewards. The approach shifts agents from blind exploration to purposeful exploration by enabling self-motivation through learned laws.

## Method Summary
The method extracts experience from human gameplay records using LLMs to infer the underlying laws of the game environment. This experience, expressed in language, describes preconditions, costs, and benefits for each action. For LLM agents, the experience is incorporated into prompts to enhance reasoning capabilities. For RL agents, the experience is converted into code that generates reward functions, providing conditional rewards based on action validity. The approach leverages the flexibility of language to serve as both reasoning assistance and reward guidance, shifting agents from blind exploration to purposeful exploration through self-motivation based on learned laws.

## Key Results
- LLM agents guided by experience-enhanced prompts outperform those using only paper information or action names
- RL agents trained with law-based rewards show improved performance compared to basic health-based rewards
- The approach demonstrates the feasibility of using extracted laws as internal motivation for both LLM reasoning and RL reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer approximate environmental laws from interaction records
- Mechanism: LLMs compare states before and after actions across successful and failed records to identify preconditions, costs, and benefits for each action
- Core assumption: The patterns in successful records contain sufficient information to approximate the underlying game laws
- Evidence anchors:
  - [abstract] "We propose a method that extracts experience from interaction records to model the underlying laws of the game environment"
  - [section 3.2] "Dv=True g is a good starting point because for all d = ⟨s, a, s′, v⟩ ∈ D v=True g , a is indicative of achieving g and s well reflects all necessary and unnecessary preconditions for g"
- Break condition: If records are too sparse, too homogeneous, or if game mechanics are too complex for pattern recognition, the inferred experience will not approximate true laws

### Mechanism 2
- Claim: Experience can serve as internal motivation for both LLM reasoning and RL reward functions
- Mechanism: For LLMs, experience is incorporated into prompts to guide reasoning; for RL, experience is converted to code that validates when actions should receive rewards
- Core assumption: The same textual experience can be effectively used in both reasoning and reward generation contexts
- Evidence anchors:
  - [abstract] "These experience, expressed in language, are highly flexible and can either assist agents in reasoning directly or be transformed into rewards for guiding training"
  - [section 3.3] "By concatenating the original prompt p with E, p′ = p⊕E. This enhanced prompt p′ is then used for reasoning by LLMs"
  - [section 3.4] "We use the LLM in combination with E to generate code for each action corresponding to an achievement"
- Break condition: If the conversion from text to reward validation code is inaccurate, or if the LLM fails to incorporate experience effectively in reasoning

### Mechanism 3
- Claim: Law-based rewards shift agents from blind to purposeful exploration
- Mechanism: Rewards are self-assigned based on action validity determined by inferred laws, rather than waiting for environmental feedback
- Core assumption: Agents can effectively learn to explore purposefully when given immediate feedback about action validity
- Evidence anchors:
  - [abstract] "Our approach shifts agents from 'blind exploration' to 'purposeful exploration'"
  - [section 4] "The reward function actually only determines whether an action is valid (good enough) in a given state"
- Break condition: If the law-based rewards don't align well with actual game objectives, or if agents become too conservative and stop exploring

## Foundational Learning

- Concept: Constrained Markov Decision Process (CMDP)
  - Why needed here: The method explicitly extends standard MDPs with constraints to model the conditional nature of actions in Crafter
  - Quick check question: What additional components does a CMDP have compared to a standard MDP?

- Concept: In-context learning with LLMs
  - Why needed here: The method relies on incorporating extracted experience into LLM prompts to guide reasoning
  - Quick check question: How does concatenating experience E with the original prompt p create p′ for improved reasoning?

- Concept: Reward shaping in reinforcement learning
  - Why needed here: The method generates code-based rewards that validate action validity rather than providing dense reward signals
  - Quick check question: What is the key difference between law-based rewards and traditional reward shaping approaches?

## Architecture Onboarding

- Component map: Data collection -> LLM inference -> Experience extraction -> Prompt enhancement (LLM agents) / Reward code generation (RL agents) -> Agent training/execution
- Critical path: Interaction records -> LLM extraction of experience -> Integration into agent framework -> Performance improvement
- Design tradeoffs: More diverse records improve law inference but increase data collection cost; simpler reward functions are more stable but may provide less guidance
- Failure signatures: LLM agents make incorrect decisions despite having experience; RL agents don't improve with law-based rewards; experience extraction fails to capture essential constraints
- First 3 experiments:
  1. Verify LLM can correctly extract preconditions from a small set of well-labeled records
  2. Test LLM agent performance with and without experience-enhanced prompts on a simple task
  3. Generate and test reward functions for a single action to ensure validity checking works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity and quality of interaction records impact the accuracy of extracted experience?
- Basis in paper: [explicit] The paper mentions that "we found that the diversity of records had a significant impact on the LLM’s reasoning capabilities" and that LLMs can determine success/failure by comparing states without explicit labels.
- Why unresolved: The paper does not provide specific metrics or experiments showing how record diversity affects experience quality or agent performance.
- What evidence would resolve it: Controlled experiments varying record diversity and measuring the resulting experience quality and agent performance would clarify this relationship.

### Open Question 2
- Question: Can the experience extraction method generalize to other games beyond Crafter?
- Basis in paper: [inferred] The method relies on LLM reasoning over interaction records, which could theoretically apply to any game with similar record structures.
- Why unresolved: The paper only evaluates the approach in Crafter, so its generalizability to other games remains untested.
- What evidence would resolve it: Applying the method to multiple different games and comparing performance would demonstrate its generalizability.

### Open Question 3
- Question: How does the law-based reward system compare to other reward shaping techniques?
- Basis in paper: [explicit] The paper states that "reward function actually only determines whether an action is valid (good enough) in a given state" and can be combined with different reward shaping methods.
- Why unresolved: The paper only compares basic health rewards to law-based rewards, not to other sophisticated reward shaping techniques.
- What evidence would resolve it: Comparative experiments between law-based rewards and other reward shaping methods (e.g., potential-based shaping, HRL hierarchies) would clarify relative effectiveness.

## Limitations

- Data Dependency: The approach's performance critically depends on the quality and diversity of human gameplay records, with no quantitative analysis of this relationship
- Generalizability Concerns: The method is validated only on Crafter, raising questions about its effectiveness in more complex environments
- Evaluation Gaps: The paper doesn't measure computational overhead of LLM-based experience extraction or compare training efficiency with baseline approaches

## Confidence

**High Confidence** (supported by experimental results):
- LLM agents guided by experience outperform those using only paper information or action names
- RL agents trained with law-based rewards show improvement over basic health-based rewards

**Medium Confidence** (mechanisms plausible but under-specified):
- LLMs can accurately infer game laws from interaction records across diverse scenarios
- The same experience effectively serves both reasoning and reward function purposes

**Low Confidence** (theoretical claims with minimal empirical support):
- The approach fundamentally shifts agents from "blind" to "purposeful" exploration in a measurable way
- The method generalizes well beyond the Crafter environment

## Next Checks

1. **Cross-Environment Transferability Test**: Apply the method to at least two additional game environments with different mechanics and state complexities to validate generalizability beyond Crafter.

2. **Experience Quality Sensitivity Analysis**: Systematically vary the quality and quantity of human gameplay records to measure the relationship between record characteristics and extracted experience accuracy.

3. **Computational Overhead Measurement**: Benchmark the time and resource costs of LLM-based experience extraction compared to the performance gains achieved by agents using the extracted experience.