---
ver: rpa2
title: 'Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on
  Word Error Rate and Fusion Techniques'
arxiv_id: '2406.08353'
source_url: https://arxiv.org/abs/2406.08353
tags:
- fusion
- speech
- emotion
- transcripts
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between laboratory SER research and
  real-world applications by benchmarking SER performance using ASR-generated transcripts
  with varying WERs. The study evaluates eleven ASR models across three emotion corpora
  (IEMOCAP, CMU-MOSI, MSP-Podcast) and tests six fusion techniques to determine their
  robustness to ASR errors.
---

# Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques

## Quick Facts
- arXiv ID: 2406.08353
- Source URL: https://arxiv.org/abs/2406.08353
- Reference count: 0
- Primary result: SER performance generally decreases with increasing WER, but bimodal SER with fusion techniques mitigates this impact; certain ASR errors can enhance SER performance in some cases.

## Executive Summary
This paper addresses the gap between laboratory SER research and real-world applications by benchmarking SER performance using ASR-generated transcripts with varying WERs. The study evaluates eleven ASR models across three emotion corpora (IEMOCAP, CMU-MOSI, MSP-Podcast) and tests six fusion techniques to determine their robustness to ASR errors. Results show that SER performance generally decreases with increasing WER, but bimodal SER with fusion techniques mitigates this impact. Notably, certain ASR errors can enhance SER performance in some cases. The authors propose an ASR error-robust framework integrating two-stage ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to baseline models.

## Method Summary
The study generates ASR transcripts using 11 different models on three emotion corpora and computes WER for each. Text-only SER models are trained using RoBERTa-base encoder with two dense layers, five-fold cross-validation on IEMOCAP, and official splits on CMU-MOSI and MSP-Podcast. Six fusion techniques (early, late, cross-attention, tensor, NL-gate, MISA) are implemented with audio features from Wav2Vec2-base. The proposed ASR error-robust framework integrates two-stage error correction (LLM-based and S2S model) with modality-gated fusion. Performance is evaluated using accuracy metrics for IEMOCAP, Acc2/Acc7/MAE for CMU-MOSI, and CCC for MSP-Podcast.

## Key Results
- SER performance generally decreases with increasing WER, but bimodal SER with fusion techniques mitigates this impact
- Certain ASR errors can enhance SER performance in some cases by introducing words that align better with emotion labels
- The proposed ASR error-robust framework with modality-gated fusion achieves lower WER and higher SER results compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain ASR errors can enhance SER performance because misrecognized words may align better with the ground-truth emotion label than the original word.
- Mechanism: When ASR transcripts introduce errors, these errors sometimes replace emotionally neutral or ambiguous words with words that carry stronger emotional connotations, thus improving classification accuracy.
- Core assumption: The ASR model's error distribution is not purely random but influenced by phonetic similarity, which can inadvertently preserve or enhance emotional cues.
- Evidence anchors: "certain ASR errors can enhance SER performance in some cases" and "This might be due to certain words being misrecognized as words that have little effect on or even positively contribute to their ground-truth emotion labels."
- Break condition: If ASR errors become systematic distortions (e.g., consistently dropping emotionally salient words), the positive effect disappears and performance degrades.

### Mechanism 2
- Claim: Bimodal SER with fusion techniques can mitigate the negative impact of high WER because audio features compensate for text errors.
- Mechanism: Audio embeddings capture prosodic and tonal cues that remain intact regardless of ASR transcription quality; fusion models learn to weight audio more heavily when text quality drops.
- Core assumption: Audio features contain sufficient emotion-relevant information independent of the spoken content.
- Evidence anchors: "bimodal SER with fusion techniques mitigates this impact" and "The decrease in Acc4 based on WER reaches 10% without fusion on IEMOCAP, but only 4% with fusion."
- Break condition: If audio quality is poor or the fusion model fails to learn proper modality weighting, the mitigation effect weakens.

### Mechanism 3
- Claim: Dynamic modality gating with weighted attention improves SER robustness by focusing on the most salient parts of both inputs and adapting to input quality.
- Mechanism: The framework assigns trainable weights to each modality, updated per batch, and applies weighted cross-attention to emphasize reliable signals while suppressing noisy ones.
- Core assumption: The model can accurately estimate modality reliability in real time and adjust attention accordingly.
- Evidence anchors: "unified ASR error-robust framework integrating ASR error correction and modality-gated fusion" and "we adapt a tri-modal incongruity-aware fusion method...considering potential distortion of original emotion in the spoken content due to ASR errors."
- Break condition: If the gating mechanism overfits to training data or misestimates reliability, it may amplify errors instead of suppressing them.

## Foundational Learning

- Concept: Word Error Rate (WER) as a metric
  - Why needed here: WER quantifies transcription accuracy, which directly impacts SER performance; understanding its calculation and interpretation is essential for evaluating ASR models.
  - Quick check question: How is WER calculated from reference and hypothesis transcripts?

- Concept: Multimodal fusion techniques
  - Why needed here: Different fusion strategies (early, late, cross-attention, tensor, MISA, NL-gate) have distinct impacts on SER robustness to ASR errors; engineers must understand their trade-offs.
  - Quick check question: What is the main difference between early and late fusion in multimodal learning?

- Concept: Automatic Speech Recognition (ASR) error correction
  - Why needed here: The proposed framework uses a two-stage correction process (LLM-based and S2S model) to reduce WER before SER; understanding this pipeline is critical for implementation.
  - Quick check question: What is the role of the Alpaca prompt in the ASR error correction stage?

## Architecture Onboarding

- Component map: ASR models (11 variants) -> WER measurement -> Text encoder (RoBERTa-base) -> Audio encoder (HuBERT) -> Fusion module (six techniques + modality-gated) -> SER backbone (dense layers) -> Emotion prediction

- Critical path: 1. Generate ASR transcripts with varying WERs 2. Apply two-stage ASR error correction (Alpaca prompt -> S2S model) 3. Encode text and audio features 4. Apply modality-gated fusion with dynamic weighting 5. Feed fused representation into SER backbone 6. Output emotion prediction

- Design tradeoffs: Using N-best hypotheses from all ASR models increases diversity but also computational cost; removing punctuation from Whisper transcripts ensures fair comparison but loses syntactic cues; modality gating adds complexity and training time but improves robustness.

- Failure signatures: High WER not reduced by error correction -> check Alpaca prompt effectiveness and S2S model quality; SER performance worse than text-only baseline -> inspect modality weighting and fusion attention; Overfitting to low-WER data -> validate on held-out ASR-generated transcripts.

- First 3 experiments: 1. Benchmark SER performance on ground-truth vs. Whisper-tiny transcripts without fusion to establish baseline WER impact 2. Test cross-attention fusion on IEMOCAP with varying WERs to identify robustness threshold 3. Evaluate the complete ASR error-robust framework on MSP-Podcast and compare CCC scores to baseline fusion methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of ASR errors (e.g., substitutions, deletions, insertions) specifically impact SER performance across different emotion categories?
- Basis in paper: [inferred] The paper mentions that "certain words being misrecognized as words that have little effect on or even positively contribute to their ground-truth emotion labels" and notes that "certain types of errors (e.g., repetitions, disfluencies) may reveal indicators of dementia," suggesting that error types matter but are not explicitly analyzed.
- Why unresolved: The study focuses on overall WER and fusion techniques but does not break down error types or their specific impacts on different emotions.
- What evidence would resolve it: Detailed error analysis showing how substitutions, deletions, and insertions affect SER accuracy for each emotion category (angry, happy, neutral, sad) separately.

### Open Question 2
- Question: What is the optimal balance between ASR error correction and modality-gated fusion for maximizing SER performance across varying WER levels?
- Basis in paper: [explicit] The authors propose an ASR error-robust framework integrating two-stage ASR error correction and modality-gated fusion, but note that "it is difficult to identify a single best-performing technique that is universally effective in addressing ASR errors."
- Why unresolved: While the framework is proposed and tested, the study does not systematically explore different balances or configurations of error correction versus fusion techniques.
- What evidence would resolve it: Comparative experiments varying the strength of error correction (e.g., different correction models or thresholds) and fusion weight configurations across multiple WER ranges.

### Open Question 3
- Question: How does the mismatch between regression models during training and classification metrics during evaluation affect SER performance on continuous emotion dimensions like arousal and dominance?
- Basis in paper: [explicit] The authors note that "the mismatch between the regression model used during training (since MOSI is labeled with continuous values) and the classification metric applied to Acc7, where predicted and ground-truth values are grounded, could potentially blur accuracy and render SER insensitive to WER."
- Why unresolved: The study observes this mismatch but does not explore alternative evaluation strategies or training approaches to address it.
- What evidence would resolve it: Experiments comparing regression-based metrics (e.g., CCC) versus classification-based metrics on the same model outputs, or training models with classification objectives from the start.

## Limitations
- The positive effect of certain ASR errors on SER performance may not hold for all emotion corpora or ASR model error distributions
- The proposed modality-gated fusion framework's sensitivity to hyperparameter choices and potential overfitting is not fully explored
- The evaluation focuses on three emotion corpora, leaving questions about performance on other languages, acoustic environments, or spontaneous conversational speech

## Confidence

- **High confidence**: The core finding that SER performance generally decreases with increasing WER is well-supported by empirical results across all three corpora. The observation that bimodal SER with fusion techniques mitigates this impact is also strongly validated through comparative metrics showing reduced performance degradation (10% vs 4% on IEMOCAP).

- **Medium confidence**: The claim that certain ASR errors can enhance SER performance in some cases is supported by observed data patterns, but the underlying mechanism remains somewhat speculative without systematic analysis of which specific error types contribute positively. The effectiveness of the two-stage ASR error correction pipeline is demonstrated but not extensively compared against alternative correction strategies.

- **Low confidence**: The generalizability of the modality-gated fusion framework to other domains, languages, or ASR architectures is not established. The paper does not provide ablation studies showing how each component of the framework contributes to the observed improvements.

## Next Checks
1. **Error Type Analysis**: Conduct a systematic analysis of ASR error types (substitutions, deletions, insertions) to identify which specific error patterns correlate with improved SER performance, and test whether these patterns generalize across different ASR models and emotion corpora.

2. **Cross-Domain Robustness**: Evaluate the complete ASR error-robust framework on at least two additional emotion corpora from different domains (e.g., call center conversations, social media videos) and languages to assess generalization beyond the original three corpora.

3. **Fusion Method Comparison**: Perform an extensive ablation study comparing all six fusion techniques plus the proposed modality-gated approach across varying WER ranges, including statistical significance testing to determine which methods provide robust improvements versus those that perform similarly to text-only baselines.