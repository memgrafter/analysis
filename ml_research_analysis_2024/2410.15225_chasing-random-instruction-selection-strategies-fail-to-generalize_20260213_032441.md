---
ver: rpa2
title: 'Chasing Random: Instruction Selection Strategies Fail to Generalize'
arxiv_id: '2410.15225'
source_url: https://arxiv.org/abs/2410.15225
tags:
- selection
- random
- cost
- budget
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study finds that instruction selection strategies for language
  model fine-tuning fail to generalize across different datasets, selection budgets,
  and evaluation benchmarks. The performance of these strategies varies significantly
  depending on the experimental setup, with no single strategy consistently outperforming
  random sampling.
---

# Chasing Random: Instruction Selection Strategies Fail to Generalize

## Quick Facts
- arXiv ID: 2410.15225
- Source URL: https://arxiv.org/abs/2410.15225
- Reference count: 40
- Primary result: Instruction selection strategies fail to generalize across datasets, budgets, and benchmarks, with random sampling often matching or exceeding their performance

## Executive Summary
This study systematically evaluates instruction selection strategies for fine-tuning language models to follow user instructions. Through controlled experiments across four datasets, three selection budgets, and four evaluation benchmarks, the research finds that selection strategies fail to consistently outperform random sampling. The performance of these strategies varies significantly depending on the experimental setup, and the cost of data selection often exceeds the cost of training on the full dataset while yielding only marginal performance gains. The results suggest that random sampling is a competitive and cost-effective alternative to more complex selection strategies in this context.

## Method Summary
The study fine-tunes LLaMa-7B on instruction datasets using various selection strategies to create subsets of different sizes. Four source datasets (FLAN, DOLLY, EVOL, ALPACA) are processed with four selection strategies (Alpagasus, Longest, Cherry, DEITA) plus random baselines. The model is trained on selected subsets and evaluated across four benchmarks (IFEVAL, ALPACA EVAL, LLMBAR, OPENLLM). The experiments test three different hyperparameter configurations and analyze both performance and cost-effectiveness across varying selection budgets.

## Key Results
- No single selection strategy consistently outperforms random sampling across all experimental configurations
- The cost of data selection often exceeds the cost of training on the full dataset
- Selection strategies show high sensitivity to dataset choice, selection budget, and evaluation benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction selection strategies fail to generalize because their performance is highly sensitive to experimental setup
- Mechanism: The effectiveness of a selection strategy depends on the interaction between source dataset, selection budget, and evaluation benchmark. Small changes in any of these can cause large performance swings
- Core assumption: Instruction following is a subjective, multifaceted capability that cannot be captured by a single benchmark or setup
- Evidence anchors: [abstract] "Our results indicate that selection strategies generalize poorly, often failing to consistently outperform even random baselines." [section 4.1] "No strategy except DEITA, consistently dominates over the random across all experimental configurations."

### Mechanism 2
- Claim: The cost of instruction data selection often exceeds the cost of training on the full dataset, making it economically inefficient
- Mechanism: Selection strategies require significant compute resources for scoring, clustering, or embedding generation, which adds to the overall training cost. This overhead can surpass the cost of simply training on all available data
- Core assumption: The compute cost of selection is non-trivial and scales with dataset size, regardless of selection budget
- Evidence anchors: [abstract] "Our findings reveal that data selection can often exceed the cost of fine-tuning on the full dataset, yielding only marginal—and sometimes no gains compared to tuning on the full dataset or a random subset." [section 4.3] "The effective cost of selecting data can often overshoot the cost of finetuning Mfull-dataset in some cases and the gains achieved through selection are marginal in comparison to the additional cost expended at carrying out the selection."

### Mechanism 3
- Claim: Random sampling is a competitive baseline because it incurs negligible cost while achieving comparable performance to complex selection strategies
- Mechanism: The simplicity of random sampling avoids the computational overhead of selection strategies while still providing a representative sample of the data, which is sufficient for many instruction tuning tasks
- Core assumption: The distribution of high-quality instructions is not significantly skewed, so random sampling captures enough diversity and quality
- Evidence anchors: [abstract] "The performance of these strategies varies significantly depending on the experimental setup, with no single strategy consistently outperforming random sampling." [section 4.1] "Random baselines are reasonably competitive whilst incurring the least cost."

## Foundational Learning

- Concept: Instruction following as a multifaceted capability
  - Why needed here: Understanding that instruction following cannot be reduced to a single metric is crucial for interpreting the study's findings about strategy generalization
  - Quick check question: Why might a strategy that performs well on one benchmark fail on another?

- Concept: Cost-benefit analysis in machine learning
  - Why needed here: Evaluating whether the performance gains from selection strategies justify their computational costs is central to the study's conclusions
  - Quick check question: How would you calculate the cost-effectiveness of a data selection strategy?

- Concept: Generalization in machine learning
  - Why needed here: The study's core finding is about the lack of generalization of selection strategies, making this concept fundamental to understanding the results
  - Quick check question: What factors might cause a model trained with selected data to fail when tested on different benchmarks?

## Architecture Onboarding

- Component map: Source datasets (FLAN, DOLLY, EVOL, ALPACA) -> Selection strategies (Alpagasus, Cherry, DEITA, Longest, Random) -> LLaMa-7B base model -> Evaluation benchmarks (IFE VAL, ALPACA EVAL, LLMBAR, OPEN LLM) -> Compute infrastructure (8 A6000 GPUs, 128 CPUs)

- Critical path: 1. Load and preprocess source dataset 2. Apply selection strategy to create subset 3. Fine-tune base model on selected subset 4. Evaluate fine-tuned model on benchmarks 5. Compare performance and cost against baselines

- Design tradeoffs: Selection budget vs. performance (higher budgets may improve performance but increase cost); Strategy complexity vs. generalization (simpler strategies may generalize better but achieve lower peak performance); Benchmark choice vs. evaluation (different benchmarks may lead to conflicting conclusions about strategy effectiveness)

- Failure signatures: Strategy performance varies wildly across different setups; Selection cost exceeds full dataset training cost; Random baseline outperforms complex strategies

- First 3 experiments: 1. Compare random selection vs. no selection (full dataset) on a single benchmark 2. Test a single selection strategy across multiple budgets on the same dataset 3. Evaluate the same strategy on different datasets with fixed budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective and generalizable selection strategies for instruction tuning datasets across varying experimental setups?
- Basis in paper: [explicit] The paper explicitly states that instruction selection strategies fail to generalize across different datasets, selection budgets, and evaluation benchmarks
- Why unresolved: Despite testing various selection strategies, the paper finds that no single strategy consistently outperforms random sampling across all experimental configurations, suggesting a fundamental challenge in developing robust selection methods
- What evidence would resolve it: Evidence of a selection strategy that consistently outperforms random sampling across a wide range of datasets, selection budgets, and evaluation benchmarks would resolve this question

### Open Question 2
- Question: What is the optimal trade-off between the cost of data selection and the performance gains achieved through instruction tuning?
- Basis in paper: [explicit] The paper analyzes the cost-performance trade-offs of data selection, finding that the cost often exceeds the cost of training on the full dataset, with only marginal or no gains compared to tuning on the full dataset or a random subset
- Why unresolved: The paper highlights the need for a more cost-effective approach to data selection, but does not provide a definitive answer on the optimal balance between cost and performance
- What evidence would resolve it: Evidence of a data selection strategy that consistently provides significant performance gains while minimizing the cost of selection would resolve this question

### Open Question 3
- Question: How can we develop more reliable and aligned evaluation benchmarks for assessing instruction following capabilities in large language models?
- Basis in paper: [explicit] The paper finds that different evaluation benchmarks produce contradictory trends when assessing the performance of selection strategies, highlighting the subjectivity and limitations of current evaluation methods
- Why unresolved: The paper suggests that the lack of agreement between benchmarks makes it difficult to draw definitive conclusions about the effectiveness of selection strategies, indicating a need for more robust and aligned evaluation methods
- What evidence would resolve it: Evidence of evaluation benchmarks that consistently and reliably measure instruction following capabilities across different selection strategies and experimental setups would resolve this question

## Limitations

- Experimental scope limited to LLaMa-7B and English-only instruction datasets, limiting generalizability to other model scales or languages
- Focus on supervised fine-tuning (SFT) rather than more computationally intensive approaches like RLHF, which may have different sensitivity to data selection
- Cost analysis relies on public cloud pricing that may not reflect institutional or optimized deployment scenarios

## Confidence

**High Confidence**: The finding that no single selection strategy consistently outperforms random sampling across all experimental configurations. This is directly supported by the comparative analysis across 4 datasets × 3 budgets × 4 benchmarks.

**Medium Confidence**: The conclusion that selection costs often exceed full dataset training costs. While the cost calculations are explicit, they depend on specific infrastructure choices and pricing models that may vary.

**Low Confidence**: The implication that random sampling should be preferred in practice. This broader recommendation requires additional real-world validation beyond the controlled experimental setting.

## Next Checks

1. **Cross-lingual validation**: Test whether the observed pattern of selection strategy failure generalizes to multilingual instruction datasets and non-English benchmarks

2. **Cost optimization analysis**: Conduct a sensitivity analysis of selection costs across different infrastructure choices (spot instances, specialized hardware, optimized implementations) to determine if the cost-benefit tradeoff can be improved

3. **Long-tail performance evaluation**: Examine whether selection strategies show advantages on specific subsets of instructions (e.g., rare edge cases, domain-specific tasks) even if they fail to improve aggregate benchmark performance