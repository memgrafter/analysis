---
ver: rpa2
title: Grammaticality Representation in ChatGPT as Compared to Linguists and Laypeople
arxiv_id: '2406.11116'
source_url: https://arxiv.org/abs/2406.11116
tags:
- chatgpt
- grammatical
- sentences
- human
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This preregistered study investigates ChatGPT\u2019s grammatical\
  \ intuition by comparing its grammaticality judgments with those of linguists and\
  \ laypeople across three experimental tasks: Magnitude Estimation, Likert Scale,\
  \ and Forced-Choice. ChatGPT evaluated 2,355 English sentences representing 148\
  \ syntactic phenomena."
---

# Grammaticality Representation in ChatGPT as Compared to Linguists and Laypeople

## Quick Facts
- arXiv ID: 2406.11116
- Source URL: https://arxiv.org/abs/2406.11116
- Reference count: 0
- Primary result: ChatGPT's grammaticality judgments converge with linguists at 89% overall, showing significant correlation with laypeople

## Executive Summary
This preregistered study investigates ChatGPT's grammatical intuition by comparing its grammaticality judgments with those of linguists and laypeople across three experimental tasks: Magnitude Estimation, Likert Scale, and Forced-Choice. ChatGPT evaluated 2,355 English sentences representing 148 syntactic phenomena. Results showed convergence rates of 73%-95% between ChatGPT and linguists, with an overall point estimate of 89%. ChatGPT also exhibited significant correlations with laypeople's judgments, though correlation strength varied by task. Differences in judgment patterns were attributed to task-specific response styles and ChatGPT's tendency to normalize syntactic anomalies.

## Method Summary
The study used 2,355 English sentences from Sprouse et al. (2013) representing 148 syntactic phenomena. ChatGPT was prompted 50 times for each sentence using three task formats: Magnitude Estimation (relative to reference sentence rated 100), 7-point Likert Scale, and Forced-Choice between grammatical and ungrammatical alternatives. Responses were statistically analyzed using z-scores, Bayesian mixed-effects models, and t-tests to calculate convergence rates and correlations with human judgments from the original dataset.

## Key Results
- ChatGPT's grammaticality judgments converged with linguists at 73%-95% across tasks, with 89% overall point estimate
- Strong correlations observed between ChatGPT and laypeople in Magnitude Estimation and Likert Scale tasks, weak correlation in Forced-Choice task
- ChatGPT was more conservative than humans in Magnitude Estimation task, giving higher ratings to ungrammatical sentences
- For 141 out of 148 pairwise phenomena, ChatGPT rated grammatical sentences higher than ungrammatical ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's grammatical judgments converge with linguists at ~89% across tasks
- Mechanism: ChatGPT learns grammatical patterns from large-scale text and applies them consistently in judgment tasks
- Core assumption: Grammatical knowledge is represented in continuous embeddings and retrieved during prompting
- Evidence anchors: Results showed convergence rates of 73%-95% between ChatGPT and linguists, with an overall point estimate of 89%; For 141 out of 148 pairwise phenomena, ChatGPT's acceptability ratings for grammatical sentences surpassed those of ungrammatical sentences

### Mechanism 2
- Claim: ChatGPT exhibits conservative rating behavior in Magnitude Estimation tasks compared to humans
- Mechanism: Normalization of syntactic anomalies based on training data biases ChatGPT toward more moderate acceptability scores
- Core assumption: ChatGPT's training corpus overrepresents fluent language, reducing sensitivity to ungrammatical structures
- Evidence anchors: Humans tended to rate grammatical sentences higher than ChatGPT, while ChatGPT gave ungrammatical sentences higher ratings than humans did; Compared to humans, ChatGPT was more conservative in its ratings in the ME task

### Mechanism 3
- Claim: ChatGPT's grammatical judgments align more closely with laypeople in Likert Scale tasks than in Forced-Choice tasks
- Mechanism: Rating tasks allow for nuanced gradations that match human judgment patterns better than binary forced choices
- Core assumption: Humans and ChatGPT use similar internal scaling mechanisms when rating acceptability
- Evidence anchors: Strong correlations were observed in the ME and LS tasks, while a weak correlation was observed in the FC task; In the LS task, the effect of participant type on rating scores was not statistically significant

## Foundational Learning

- Concept: Grammaticality vs Acceptability distinction
  - Why needed here: The paper operationalizes grammaticality as acceptability to compare AI and human judgments meaningfully
  - Quick check question: Why do researchers often use "acceptability" instead of "grammaticality" in judgment tasks?

- Concept: Statistical significance in convergence analysis
  - Why needed here: Different statistical tests yield varying convergence estimates, affecting interpretation of results
  - Quick check question: What's the difference between directional analysis and mixed-effects model approaches in this context?

- Concept: Response style effects in rating tasks
  - Why needed here: Individual differences in scaling can affect between-participant reliability in grammaticality judgments
  - Quick check question: How does the "one trial per run" design mitigate response style effects?

## Architecture Onboarding

- Component map: Experimental stimuli -> ChatGPT judgment generation -> Statistical analysis pipeline -> Convergence rate calculation
- Critical path: Data collection (Python script + ChatGPT API) -> Statistical analysis (R packages: brms, lme4, BayesFactor) -> Comparison framework (convergence metrics)
- Design tradeoffs: "One trial per run" reduces bias but increases computational cost; Likert scale offers reliability but less categorical precision than forced-choice
- Failure signatures: Non-conforming responses (>1% in FC task); statistical method choice affecting convergence estimates; outlier trials where ChatGPT diverges from human judgments
- First 3 experiments:
  1. Magnitude Estimation task replication with "one trial per run" design
  2. 7-point Likert Scale task comparison with humans
  3. Forced-Choice task with binary coding of responses

## Open Questions the Paper Calls Out

- To what extent does ChatGPT's grammatical knowledge generalize to languages other than English?
- How does ChatGPT's normalization tendency affect its grammatical judgments for different syntactic phenomena?
- How do model size and fine-tuning procedures influence ChatGPT's grammatical judgments?

## Limitations

- Weak external validation: No directly comparable studies measuring convergence between LLMs and human grammaticality judgments
- Task-specific variability: Significant differences in ChatGPT's alignment with humans across tasks with unclear mechanisms
- Prompt sensitivity: Exact prompt formulations and potential ChatGPT idiosyncrasies not fully characterized

## Confidence

- High confidence: ChatGPT shows significant convergence with linguists (89% overall) supported by multiple statistical tests
- Medium confidence: ChatGPT being more conservative than humans in rating ungrammatical sentences, though convergence estimates vary by statistical method (73%-95%)
- Low confidence: Mechanism explanations for task-specific differences are speculative given current evidence

## Next Checks

1. Replicate with alternative statistical approaches using additional convergence metrics to verify robustness of 89% convergence estimate
2. Conduct prompt sensitivity analysis by systematically varying prompt formulations while keeping task structure constant
3. Test cross-linguistic validation by evaluating ChatGPT on non-English grammaticality judgments using parallel syntactic phenomena