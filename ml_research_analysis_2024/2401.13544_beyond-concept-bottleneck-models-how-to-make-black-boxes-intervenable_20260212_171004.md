---
ver: rpa2
title: 'Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?'
arxiv_id: '2401.13544'
source_url: https://arxiv.org/abs/2401.13544
tags:
- concept
- interventions
- intervention
- figure
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to perform concept-based interventions
  on pretrained black-box neural networks using only a small validation set with concept
  labels. The authors formalize intervenability as a measure of intervention effectiveness
  and propose a fine-tuning procedure to improve it.
---

# Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?

## Quick Facts
- arXiv ID: 2401.13544
- Source URL: https://arxiv.org/abs/2401.13544
- Authors: Sonia Laguna; Ričards Marcinkevičs; Moritz Vandenhirtz; Julia E. Vogt
- Reference count: 40
- Key outcome: Method for concept-based interventions on pretrained black-box neural networks using only a small validation set with concept labels, improving intervention effectiveness through fine-tuning.

## Executive Summary
This paper addresses the challenge of making pretrained black-box neural networks intervenable by introducing a method to perform concept-based interventions using only a small validation set with concept labels. The authors formalize intervenability as a measure of intervention effectiveness and propose a fine-tuning procedure to improve it. Their approach allows editing intermediate layer activations to align with given concept values, enabling instance-specific model corrections or steering. Experiments demonstrate significant improvements in intervention effectiveness across synthetic tabular data, natural images, and chest X-ray datasets, with the method working even with vision-language-model-based concept annotations.

## Method Summary
The proposed method involves training a probe to predict concept values from intermediate layer activations, then editing the representations to minimize distance to the original while enforcing consistency with provided concept values. The intervenability measure quantifies the gap between expected target prediction loss with and without interventions, which is differentiable and can be optimized using gradient descent to fine-tune the black-box model. The method works by formalizing concept-based interventions as editing intermediate layer activations to align with user-provided concept values, using a probe trained to predict concepts from these activations.

## Key Results
- Fine-tuning black-box models for intervenability significantly improves intervention effectiveness compared to baselines
- The method works effectively even with vision-language-model-based concept annotations, reducing the need for human annotation
- Demonstrated improvements across multiple datasets including synthetic tabular data, AwA2, CIFAR-10, ImageNet, and chest X-ray datasets
- The approach enables instance-specific model corrections and steering through concept-based interventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The intervention procedure allows instance-specific edits to black-box models by aligning intermediate layer activations with user-provided concept values.
- Mechanism: A probe is trained to predict concept values from intermediate layer activations. The activations are then edited to minimize the distance to the original while enforcing consistency with the provided concept values via the probe.
- Core assumption: The intermediate layer activations are correlated with the concept values, and the probe can learn to predict them accurately.
- Evidence anchors:
  - [abstract] "introduce a method to perform such concept-based interventions on pretrained neural networks, which are not interpretable by design, only given a small validation set with concept labels"
  - [section 3.1] "using the given validation data and concept values, our procedure edits the network's representations z = hϕ (x), where z ∈ Z, to align more closely with c′ and, thus, affects the downstream prediction"
- Break condition: If the intermediate layer activations are not correlated with the concept values, or the probe cannot be trained accurately, the interventions will be spurious and ineffective.

### Mechanism 2
- Claim: Fine-tuning for intervenability improves the effectiveness of concept-based interventions by reinforcing the model's reliance on high-level attributes.
- Mechanism: The black-box model is fine-tuned by maximizing the intervenability measure, which quantifies the gap between the expected target prediction loss with and without concept-based interventions.
- Core assumption: The intervenability measure is differentiable and can be optimized using gradient descent.
- Evidence anchors:
  - [abstract] "formalise the notion of intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black boxes"
  - [section 3.3] "Since the intervenability measure defined in Equation 3 is differentiable, a neural network can be fine-tuned by explicitly maximising it using, for example, mini-batch gradient descent"
- Break condition: If the intervenability measure is not differentiable or the fine-tuning process does not converge, the method will not improve intervention effectiveness.

### Mechanism 3
- Claim: The proposed method can alleviate the need for human-annotated concept labels by using vision-language models (VLMs) to generate concept labels.
- Mechanism: Concept labels are generated using VLMs by comparing the similarities between each instance and the concept text embedding with that of not the concept.
- Core assumption: VLMs can generate accurate concept labels that are predictive of the target variable.
- Evidence anchors:
  - [abstract] "Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set"
  - [section 4] "To demonstrate that our approaches are effective without human-annotated concepts... we present the results on CIFAR-10... Concept labels are produced based on CLIP similarities between each image and verbal descriptions"
- Break condition: If the VLM-generated concept labels are inaccurate or not predictive of the target variable, the interventions will be ineffective.

## Foundational Learning

- Concept: Probing
  - Why needed here: To align the network's activation vectors with concepts and interpret the activations in the intermediate layer.
  - Quick check question: What is the purpose of the probing function in the intervention procedure?

- Concept: Concept bottleneck models (CBMs)
  - Why needed here: To understand the notion of intervenability and the advantages of concept-based interventions.
  - Quick check question: How do CBMs differ from standard neural networks in terms of interpretability and intervenability?

- Concept: Regularization
  - Why needed here: To understand how fine-tuning for intervenability can have a regularizing effect and improve the model's calibration.
  - Quick check question: How does the intervenability measure act as a regularizer during fine-tuning?

## Architecture Onboarding

- Component map:
  Black-box model (fθ) -> Intermediate layer (⟨gψ, hϕ⟩) -> Probing function (qξ) -> Intervention procedure (Equation 1) -> Updated prediction

- Critical path: Black-box model → Intermediate layer → Probing function → Intervention procedure → Updated prediction

- Design tradeoffs:
  - Choice of distance function in Equation 1 (Euclidean vs. cosine)
  - Choice of intervention strategy (random-subset vs. uncertainty-based)
  - Choice of probing function (linear vs. nonlinear)

- Failure signatures:
  - Interventions have no effect on the model's predictions
  - Fine-tuning does not improve intervention effectiveness
  - VLM-generated concept labels are inaccurate

- First 3 experiments:
  1. Evaluate the intervention procedure on a simple black-box model with a known concept-label mapping.
  2. Compare the effectiveness of interventions using different distance functions and probing functions.
  3. Fine-tune a black-box model for intervenability and evaluate the improvement in intervention effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of interventions vary when using different distance functions beyond Euclidean and cosine distances?
- Basis in paper: [explicit] The paper explores Euclidean and cosine distances but suggests the distance function should be chosen to suit the neural network's latent space.
- Why unresolved: The paper only tests two distance functions and acknowledges that the choice may impact intervention effectiveness.
- What evidence would resolve it: Empirical results comparing intervention effectiveness across multiple distance functions (e.g., Manhattan, Mahalanobis) on various datasets.

### Open Question 2
- Question: How does the proposed fine-tuning approach scale to larger and more complex architectures like GPT-style transformers?
- Basis in paper: [inferred] The paper demonstrates scalability to Stable Diffusion but doesn't test on transformer architectures.
- Why unresolved: The paper focuses on vision models and simple FCNN architectures without exploring NLP or multimodal transformer models.
- What evidence would resolve it: Application of the fine-tuning method to transformer-based models like BERT or GPT, with analysis of computational requirements and intervention effectiveness.

### Open Question 3
- Question: Can the intervenability measure be used to guide the design of new interpretable architectures rather than just fine-tuning existing ones?
- Basis in paper: [explicit] The paper introduces intervenability as a measure but only uses it for fine-tuning black boxes.
- Why unresolved: The paper doesn't explore using intervenability as a design principle for creating inherently interpretable models.
- What evidence would resolve it: Development and evaluation of new model architectures optimized for intervenability from the ground up, compared to retrofitted models.

## Limitations

- The method's effectiveness depends on the correlation between intermediate layer activations and concept values, which may not hold for all model architectures or concept types
- The approach requires accurate concept probes, creating a potential failure point if the probe cannot learn meaningful concept predictions
- The validation experiments focus primarily on classification tasks, leaving open questions about performance on regression or other prediction types

## Confidence

*High Confidence:* The core mechanism of using probes to predict concepts from intermediate activations and then editing those activations is technically sound and well-supported by the presented experiments.

*Medium Confidence:* The fine-tuning procedure's effectiveness across diverse datasets is demonstrated, but the extent to which improvements generalize to real-world deployment scenarios requires further validation.

*Medium Confidence:* The claim that VLM-generated concept labels can replace human annotations shows promise in controlled experiments but needs testing across broader concept domains.

## Next Checks

1. **Probe Robustness Test**: Evaluate intervention effectiveness across different probing architectures (linear vs. nonlinear) and varying amounts of training data to establish the method's sensitivity to probe quality.

2. **Cross-Domain Validation**: Apply the fine-tuning procedure to a model trained on one domain (e.g., natural images) and test intervention effectiveness on a substantially different domain (e.g., medical imaging) to assess generalization.

3. **Concept Importance Analysis**: Systematically measure intervention effectiveness for concepts at different abstraction levels (e.g., low-level edges vs. high-level objects) to understand which concepts benefit most from the fine-tuning procedure.