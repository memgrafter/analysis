---
ver: rpa2
title: 'RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation'
arxiv_id: '2402.01253'
source_url: https://arxiv.org/abs/2402.01253
tags:
- user
- hierarchical
- refined
- multi-interest
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling user refined multi-interests
  in hierarchical structures for recommendation. It proposes a novel two-stage retrieval
  method, RimiRec, which first uses hierarchical clustering and a transformer-based
  model to generate user interests at different levels, then performs ANN searches
  in partitioned retrieval space to capture users' refined interests.
---

# RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation

## Quick Facts
- arXiv ID: 2402.01253
- Source URL: https://arxiv.org/abs/2402.01253
- Reference count: 11
- Primary result: Achieves state-of-the-art performance on offline datasets and shows 24.7% increase in Recall@500 in online A/B testing at Lofter

## Executive Summary
RimiRec addresses the challenge of modeling user refined multi-interests in hierarchical structures for recommendation systems. The method proposes a two-stage retrieval approach that first uses hierarchical clustering and a transformer-based model to generate user interests at different levels, then performs ANN searches in partitioned retrieval space to capture users' refined interests. The approach demonstrates significant improvements over existing methods, achieving 24.7% increase in Recall@500 in online A/B testing at Lofter and state-of-the-art performance on offline datasets.

## Method Summary
RimiRec employs a two-stage retrieval method that combines hierarchical clustering with transformer-based interest modeling. First, items are organized into a hierarchical tree structure using k-means clustering, where each item receives a semantic category ID path representing its position in the hierarchy. A transformer-based sequence-to-sequence model then processes user behavior sequences to generate semantic category IDs that reflect user interests at multiple hierarchical levels. The item library is partitioned into sub-libraries based on these semantic categories, and ANN searches are performed in the corresponding sub-libraries for each user, allowing the model to capture refined interests more accurately by focusing on relevant items within each interest category.

## Key Results
- Achieves 24.7% increase in Recall@500 in online A/B testing at Lofter
- Shows significant improvements in user engagement metrics including likes, reposts, and comments
- Demonstrates state-of-the-art performance on offline datasets compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical clustering creates a semantic tree that reflects user interest granularity, enabling the model to capture fine-grained interests. The paper first clusters all items into a hierarchical tree using k-means clustering, where each cluster at each layer is assigned a unique identifier. This creates a semantic category ID path (e.g., "Animation, Japanese Animation, Detective Conan") for each item. These IDs are then used to partition the item space into sub-libraries that align with user interests at different levels. The core assumption is that items that are semantically similar (e.g., within the same cluster) are more likely to reflect a user's refined interests when retrieved together.

### Mechanism 2
The transformer-based sequence-to-sequence model generates semantic category IDs that reflect user interests at multiple hierarchical levels. A bidirectional transformer encoder processes the user's behavior sequence to produce a hidden representation. The transformer decoder then generates semantic category IDs of varying lengths (early stopping mechanism) to represent user interests at different levels of granularity. This allows the model to flexibly capture both broad and fine-grained interests. The core assumption is that user behavior sequences contain sufficient information to predict their interests at multiple hierarchical levels, and the transformer can effectively learn this mapping.

### Mechanism 3
Partitioning the retrieval space into sub-libraries based on semantic category IDs allows the model to focus on relevant items and capture refined interests more accurately. The item library is partitioned into sub-libraries based on the semantic category ID set of leaf circles. For each user, the behavior sequence is split by semantic category ID, and separate user embeddings are learned for each category. ANN searches are then performed in the corresponding sub-libraries, allowing the model to retrieve items that match the user's interests at different levels of granularity. The core assumption is that partitioning the retrieval space reduces noise and allows the model to focus on relevant items, leading to better retrieval performance.

## Foundational Learning

- **Hierarchical clustering**: Why needed here - To create a semantic tree that reflects user interest granularity and enables the model to capture fine-grained interests. Quick check - How does hierarchical clustering differ from flat clustering, and why is it more suitable for modeling user interests in this context?

- **Transformer-based sequence-to-sequence models**: Why needed here - To generate semantic category IDs that reflect user interests at multiple hierarchical levels, allowing the model to capture both broad and fine-grained interests. Quick check - What are the key components of a transformer-based sequence-to-sequence model, and how do they contribute to generating accurate semantic category IDs?

- **Approximate Nearest Neighbor (ANN) search**: Why needed here - To efficiently retrieve relevant items from the partitioned sub-libraries in real-time, enabling the model to scale to large item spaces. Quick check - How does ANN search differ from exact nearest neighbor search, and what are the trade-offs in terms of accuracy and efficiency?

## Architecture Onboarding

- **Component map**: Hierarchical clustering module -> Transformer-based sequence-to-sequence model -> Partitioned retrieval space -> ANN search system

- **Critical path**: 1. Hierarchical clustering of items to create semantic tree. 2. Training transformer-based model to generate semantic category IDs for users. 3. Partitioning item library into sub-libraries based on semantic category IDs. 4. ANN search in corresponding sub-libraries for each user.

- **Design tradeoffs**: Hierarchical clustering vs. flat clustering allows for more granular modeling of user interests but may be more computationally expensive. Transformer-based model vs. simpler models can capture complex patterns in user behavior but may require more data and computational resources. Partitioning retrieval space vs. searching entire library reduces noise and allows for more focused retrieval but may miss relevant items if semantic category IDs are not well-aligned with user interests.

- **Failure signatures**: Poor clustering quality leads to meaningless semantic categories - check cluster coherence using silhouette scores. Inaccurate semantic category ID generation causes the retrieval performance to suffer. Inefficient ANN search may not retrieve relevant items efficiently, leading to poor user experience.

- **First 3 experiments**: 1. Evaluate the quality of the hierarchical clustering by examining the coherence of items within each cluster and the semantic meaning of the cluster IDs. 2. Test the accuracy of the transformer-based model in generating semantic category IDs by comparing the generated IDs with ground truth user interests. 3. Assess the effectiveness of the partitioned retrieval space by comparing the retrieval performance with and without partitioning, and analyzing the impact on user engagement metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture specificity lacks detailed specifications for transformer architecture and hierarchical clustering parameters
- Dataset dependency may limit effectiveness on datasets with different characteristics
- Evaluation scope focuses primarily on offline metrics and limited online A/B testing results

## Confidence
- **High Confidence**: The core hierarchical clustering approach and partitioned retrieval space concept are well-established in the literature and logically sound for multi-interest modeling.
- **Medium Confidence**: The transformer-based semantic category generation mechanism is theoretically sound but requires careful hyperparameter tuning for optimal performance.
- **Medium Confidence**: Online A/B testing results show positive improvements, but the magnitude of gains (24.7% Recall@500) should be validated across different time periods and user segments.

## Next Checks
1. **Clustering Quality Validation**: Perform silhouette analysis and manual inspection of cluster coherence to ensure that the hierarchical clustering produces semantically meaningful categories that align with user interests. Measure how well the cluster IDs capture known item relationships in the dataset.

2. **Transformer ID Generation Accuracy**: Conduct ablation studies on the transformer model by comparing generated semantic category IDs against ground truth user interests or manually labeled interest categories. Evaluate the early stopping mechanism's effectiveness in generating appropriate ID lengths for different users.

3. **Partitioning Impact Analysis**: Systematically test the retrieval performance with varying numbers of partitions and different partition granularity levels. Analyze whether the partitioned approach consistently outperforms a unified search across all user segments and interest types.