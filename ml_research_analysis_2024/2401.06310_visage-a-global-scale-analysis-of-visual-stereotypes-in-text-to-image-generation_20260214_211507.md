---
ver: rpa2
title: 'ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation'
arxiv_id: '2401.06310'
source_url: https://arxiv.org/abs/2401.06310
tags:
- identity
- stereotypes
- images
- visual
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViSAGe, a dataset for evaluating visual stereotypes
  in text-to-image generation models across 135 nationalities. The authors first identify
  'visual' stereotypes by distinguishing them from culturally subjective ones, using
  annotations from three annotators per attribute.
---

# ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation

## Quick Facts
- arXiv ID: 2401.06310
- Source URL: https://arxiv.org/abs/2401.06310
- Reference count: 7
- Text-to-image models show stronger stereotypical pull for identities from the Global South, even with non-stereotypical prompts.

## Executive Summary
This paper introduces ViSAGe, a dataset for evaluating visual stereotypes in text-to-image generation models across 135 nationalities. The authors distinguish between objectively "visual" and "culturally subjective" stereotypes, using annotations from three raters per attribute to identify 385 visual stereotypes. They generate 15 images per identity group using three different prompts and annotate the presence of stereotypes. The study finds that stereotypical attributes are three times more likely to be present in images of corresponding identities, with higher offensiveness for groups from Africa, South America, and South East Asia. The research reveals a "stereotypical pull" where default representations of all identity groups lean towards stereotypical depictions, especially for the Global South.

## Method Summary
The study uses Stable Diffusion-v1.4 to generate images for 135 identity groups, with three prompt types: default, stereotypical, and non-stereotypical. Images are annotated for stereotype presence using both manual and automated methods (CLIP captioning). The framework identifies visual stereotypes from SeeGULL, generates images, annotates stereotypes, and analyzes likelihood, offensiveness, and similarity metrics. The stereotypical pull is measured via cosine similarity between image embeddings, comparing default, stereotypical, and non-stereotypical representations.

## Key Results
- Stereotypical attributes are three times more likely to appear in images of corresponding identities compared to other attributes.
- Images of identities from Africa, South America, and South East Asia show higher offensiveness scores for stereotypical depictions.
- Default representations of all identity groups lean towards stereotypical depictions, with stronger pull observed for the Global South.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distinguishing visual stereotypes from culturally subjective ones improves annotation reliability and downstream evaluation validity.
- Mechanism: By requiring annotators to assess whether an attribute can be visually depicted, the approach filters out subjective stereotypes and retains only those with concrete visual markers.
- Core assumption: Annotators can reliably agree on the visual nature of attributes, and these cues are consistently interpreted across cultures.
- Evidence anchors: [abstract] identifies 385 "visual" attributes; [section] spans 135 nationality-based identity groups.
- Break condition: If annotators fail to reach consensus on visual vs. subjective attributes, or if cultural differences lead to inconsistent interpretations, the distinction breaks down.

### Mechanism 2
- Claim: Automated captioning models like CLIP can scale stereotype detection when grounded in pre-identified visual stereotypes.
- Mechanism: CLIP generates captions for images, which are matched against known visual stereotypes to detect their presence, reducing manual annotation burden.
- Core assumption: CLIP captions reliably describe visual content in a way that aligns with human perception of stereotypes.
- Evidence anchors: [abstract] shows disproportionate leaning towards stereotypes; [section] investigates automated techniques.
- Break condition: If CLIP captions are biased, inaccurate, or fail to capture subtle visual stereotypes, automated detection will miss or misclassify them.

### Mechanism 3
- Claim: The "stereotypical pull" reveals that T2I models default to stereotypical representations even with non-stereotypical prompts, especially for marginalized groups.
- Mechanism: By measuring cosine similarity between image embeddings, the approach quantifies how similar default, stereotypical, and non-stereotypical images are, revealing a pull toward stereotypes.
- Core assumption: Image embeddings capture meaningful semantic differences between stereotypical and non-stereotypical visual representations.
- Evidence anchors: [abstract] shows disproportionate leaning towards stereotypes; [section] terms this "stereotypical pull."
- Break condition: If embeddings fail to capture perceptual differences, or if similarity is confounded by other factors, the effect may be overstated.

## Foundational Learning

- Concept: Visual vs. cultural stereotypes
  - Why needed here: The entire framework hinges on distinguishing stereotypes that can be objectively depicted visually from those that are culturally subjective.
  - Quick check question: Can you give an example of a stereotype that is visual (e.g., "sombrero") and one that is cultural (e.g., "intelligent")? Why does this distinction matter for image evaluation?

- Concept: Annotation reliability and inter-annotator agreement
  - Why needed here: The study relies on annotations from multiple raters to identify visual stereotypes.
  - Quick check question: If three annotators rate an attribute as "strongly agree," "agree," and "unsure," what does this tell you about the reliability of that attribute as a visual stereotype?

- Concept: Embedding similarity and cosine distance
  - Why needed here: The "stereotypical pull" analysis uses cosine similarity between image embeddings to quantify how similar default, stereotypical, and non-stereotypical images are.
  - Quick check question: If two images have a cosine similarity of 0.95, what does that imply about their visual content? How would this compare to a similarity of 0.60?

## Architecture Onboarding

- Component map: SeeGULL textual stereotypes → visual attribute annotation → T2I image generation (Stable Diffusion) → stereotype annotation → CLIP captioning → similarity analysis
- Critical path: 1) Filter SeeGULL for visual attributes via annotation; 2) Generate images for 135 identity groups with three prompts; 3) Annotate stereotypes in images (manual + automated); 4) Compute likelihood, offensiveness, and similarity metrics; 5) Analyze stereotypical pull and global patterns
- Design tradeoffs: Manual vs. automated annotation (reliability vs. scalability); prompt specificity (reduces pull vs. limits generalizability); corpus coverage (improves evaluation vs. increases burden)
- Failure signatures: Low inter-annotator agreement on visual attributes; CLIP captions fail to capture stereotype-relevant details; high similarity between all prompt types
- First 3 experiments: 1) Annotate a small subset of SeeGULL attributes for visual nature; 2) Generate images for 10 identity groups with all three prompts; 3) Run CLIP captioning on a subset of images

## Open Questions the Paper Calls Out

- How do regional stereotypes in text-to-image models vary across different geographical regions within the Global South?
- What are the limitations of using automated captioning models like CLIP for detecting stereotypes in images?
- How do text-to-image models perpetuate stereotypes across different demographic axes such as gender, race, and ethnicity?
- What impact does the 'stereotypical pull' have on the perception and representation of historically marginalized groups in media and society?

## Limitations

- CLIP-based automated stereotype detection may miss subtle or context-dependent visual stereotypes.
- The distinction between "visual" and "culturally subjective" stereotypes depends heavily on annotator agreement and may not account for cross-cultural differences.
- Results are based on Stable Diffusion-v1.4 and may not generalize to other T2I models or newer versions.

## Confidence

- High confidence: Stereotypical attributes are three times more likely to appear in corresponding identity groups.
- Medium confidence: The "stereotypical pull" analysis is robust but relies on CLIP embeddings.
- Low confidence: Automated detection of stereotypes via CLIP captions requires further validation.

## Next Checks

1. Conduct a blind validation study where human annotators evaluate CLIP-generated captions for stereotype detection accuracy.
2. Test the "stereotypical pull" analysis using alternative embedding models (e.g., BLIP, LLaVA).
3. Generate images using a newer T2I model (e.g., Stable Diffusion XL) and repeat the analysis.