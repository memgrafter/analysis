---
ver: rpa2
title: Mixtures of In-Context Learners
arxiv_id: '2411.02830'
source_url: https://arxiv.org/abs/2411.02830
tags:
- demonstrations
- moicl
- weights
- number
- scalar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of In-Context Learners (MOICL), a
  method that dynamically partitions demonstrations into expert subsets and learns
  weighting functions to combine their predictions, addressing limitations of standard
  in-context learning such as quadratic complexity and sensitivity to demonstration
  quality. MOICL treats each demonstration subset as an expert and uses a weighting
  function (either scalar weights or a hyper-network) to merge their output distributions,
  with the ability to identify both helpful and anti-experts through gradient-based
  optimization.
---

# Mixtures of In-Context Learners

## Quick Facts
- arXiv ID: 2411.02830
- Source URL: https://arxiv.org/abs/2411.02830
- Authors: Giwon Hong; Emile van Krieken; Edoardo Ponti; Nikolay Malkin; Pasquale Minervini
- Reference count: 35
- Primary result: MOICL improves accuracy on 5/7 classification datasets vs strong baselines (up to +13%) while reducing inference time for equivalent performance with fewer demonstrations

## Executive Summary
This paper introduces Mixture of In-Context Learners (MOICL), a method that dynamically partitions demonstrations into expert subsets and learns weighting functions to combine their predictions, addressing limitations of standard in-context learning such as quadratic complexity and sensitivity to demonstration quality. MOICL treats each demonstration subset as an expert and uses a weighting function (either scalar weights or a hyper-network) to merge their output distributions, with the ability to identify both helpful and anti-experts through gradient-based optimization. The approach improves accuracy on 5 out of 7 classification datasets compared to strong baselines (up to +13%), reduces inference time for equivalent performance with fewer demonstrations, and demonstrates robustness to out-of-domain (+11%), imbalanced (+49%), and noisy demonstrations (+38%).

## Method Summary
MOICL partitions demonstrations into k subsets, each processed independently by the base LLM to generate next-token distributions. A weighting function (scalar vector or hyper-network) assigns weights to each expert's output, which are then combined to produce the final prediction. The weights are optimized via gradient-based training on a training set, allowing MOICL to identify helpful and anti-experts. For scalar weights, k-1 weights are learned; for hyper-networks, a smaller T5-based model maps concatenated demonstrations to weights, enabling generalization to unseen demonstration sets.

## Key Results
- MOICL improves accuracy on 5/7 classification datasets compared to baselines, with up to +13% improvement on SST2
- MOICL achieves equivalent accuracy to concat-based ICL with 50% fewer demonstrations, reducing inference time
- MOICL demonstrates robustness to out-of-domain (+11%), imbalanced (+49%), and noisy demonstrations (+38%), with learned weights identifying and downweighting harmful subsets

## Why This Works (Mechanism)

### Mechanism 1
Dynamic partitioning into expert subsets allows MOICL to mitigate the quadratic complexity of standard ICL by reducing the effective context length per expert. Instead of concatenating all demonstrations (n), MOICL splits them into k subsets, each with n/k demonstrations. The LLM processes each subset independently, and a weighting function merges the results. This reduces the quadratic self-attention cost from (n+1)² to k·(n/k+1)².

### Mechanism 2
Tunable weights enable MOICL to identify and downweight unhelpful or noisy demonstrations, improving robustness. Scalar or hyper-network weights are learned via gradient-based optimization on a training set. Negative weights act as "anti-experts," explicitly reducing the influence of harmful demonstrations.

### Mechanism 3
Hyper-network weighting generalizes to unseen demonstration subsets, overcoming the fixed-subset limitation of scalar weights. A smaller T5-based hyper-network maps concatenated demonstration subsets to weights, allowing dynamic weight generation for any new set of demonstrations.

## Foundational Learning

- **Concept: In-context learning (ICL)** - Why needed here: MOICL builds directly on ICL by treating each demonstration subset as an ICL expert; understanding ICL is prerequisite to understanding MOICL. Quick check question: In ICL, how are demonstrations typically provided to the LLM?
- **Concept: Self-attention complexity** - Why needed here: MOICL's main efficiency gain comes from reducing the quadratic self-attention cost; engineers must understand this to appreciate the speedup. Quick check question: What is the computational complexity of self-attention with respect to sequence length?
- **Concept: Gradient-based optimization for discrete structures** - Why needed here: MOICL uses gradient-based optimization to tune weights, including for sparse weight selection via IMLE; engineers need to understand how gradients flow through discrete operations. Quick check question: How does IMLE allow gradients to flow through a top-k selection operation?

## Architecture Onboarding

- **Component map**: Base LLM (e.g., Llama-3) -> Partitioner (static/random/BM25) -> Expert LLMs (each processes a subset) -> Weighting function (scalar vector or hyper-network) -> Weighted combination of expert outputs
- **Critical path**: Partition demonstrations -> Generate expert predictions -> Compute weights (if scalar, lookup; if hyper-network, forward pass) -> Combine predictions -> Final output
- **Design tradeoffs**: Scalar weights are simple but require fixed demonstration subsets; hyper-networks generalize but add a forward pass cost; partitioning strategy trades off diversity vs. computational load
- **Failure signatures**: Poor performance on imbalanced or noisy data if weights are not learned properly; high variance in results if partitioning is very uneven; degraded accuracy if context length per expert is too short
- **First 3 experiments**:
  1. Run MOICL with scalar weights on a small classification dataset (e.g., SST-2) with k=5 subsets; compare accuracy to baseline ICL
  2. Add noisy demonstrations and measure robustness; observe weight distribution
  3. Swap scalar for hyper-network; test generalization to a new set of demonstrations not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
How does MOICL perform when extended to multi-class classification tasks with more than two labels? The paper primarily evaluates MOICL on binary classification tasks and a generation task. The robustness analysis focuses on label imbalance but only demonstrates a binary case.

### Open Question 2
Can MOICL's hyper-network be further optimized to generalize better to unseen demonstrations without requiring fine-tuning on the available demonstrations? The current hyper-network approach still relies on demonstration-specific tuning, which may limit its true generalization capability.

### Open Question 3
How does MOICL compare to fine-tuned models in terms of computational efficiency when scaling to very large demonstration sets (e.g., 1000+ examples)? The paper demonstrates MOICL's efficiency advantages over concat-based ICL for smaller demonstration sets but does not explore scenarios with very large demonstration pools.

## Limitations

- Scalability of partitioning strategy is uncertain; no analysis of how performance degrades as k increases or dataset size grows
- Hyper-network generalization claims rest on experimental results rather than theoretical guarantees
- No discussion of sensitivity to choice of base LLM architecture

## Confidence

- **High confidence**: Improved accuracy on 5/7 datasets and robustness to noisy/imbalanced/out-of-domain demonstrations (directly supported by experimental results)
- **Medium confidence**: Computational efficiency gains and reduction of quadratic complexity (mechanistically sound but lack direct empirical validation)
- **Low confidence**: Hyper-network generalization to unseen demonstrations (demonstrated but without extensive testing on truly held-out demonstration sets)

## Next Checks

1. **Ablation study on partition size (k)**: Systematically vary k from 2 to 50 on SST-2 and measure both accuracy and inference time to identify the optimal tradeoff point and validate the claimed complexity reduction
2. **Robustness to demonstration quality**: Intentionally inject varying levels of noise (e.g., 10%, 30%, 50% incorrect labels) into demonstrations and measure how quickly MOICL's weighting function learns to downweight them compared to baseline ICL
3. **Hyper-network generalization test**: Hold out an entire class of demonstrations during training and test whether the hyper-network can still generate appropriate weights for this unseen class, quantifying the generalization gap