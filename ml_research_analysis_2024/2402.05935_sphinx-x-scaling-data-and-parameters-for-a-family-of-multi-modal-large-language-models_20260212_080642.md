---
ver: rpa2
title: 'SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language
  Models'
arxiv_id: '2402.05935'
source_url: https://arxiv.org/abs/2402.05935
tags:
- arxiv
- language
- visual
- sphinx-x
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents SPHINX-X, a family of multimodal large language
  models developed by extending the SPHINX framework with architectural improvements,
  expanded training data, and parameter scaling. The authors simplified the training
  pipeline to a single stage, removed redundant visual encoders, and introduced learnable
  skip tokens for handling fully-padded sub-images.
---

# SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models

## Quick Facts
- **arXiv ID:** 2402.05935
- **Source URL:** https://arxiv.org/abs/2402.05935
- **Reference count:** 40
- **Primary result:** Multi-modal large language models scaled through data and parameter scaling with performance gains correlated to both factors

## Executive Summary
SPHINX-X introduces a family of multi-modal large language models built by scaling both data and parameters from the SPHINX framework. The authors simplified the training pipeline to a single stage, removed redundant visual encoders, and introduced learnable skip tokens for handling fully-padded sub-images. A large-scale multi-domain dataset was curated including public resources and two specialized datasets for OCR-intensive tasks and Set-of-Mark prompting. The models were trained across different base LLMs (TinyLlama-1.1B, InternLM2-7B, LLaMA2-13B, Mixtral-8×7B) to produce varying parameter sizes and multilingual capabilities. Comprehensive benchmarking demonstrated strong performance gains correlated with both data and parameter scaling, with SPHINX-MoE achieving competitive results against closed-source models on multiple benchmarks including MathVerse, SciVerse, and MMVP.

## Method Summary
The authors extended the SPHINX framework through architectural improvements and parameter scaling, simplifying the training pipeline to a single stage and removing redundant visual encoders. They introduced learnable skip tokens to handle fully-padded sub-images and curated a large-scale multi-domain dataset including public resources plus two specialized datasets for OCR-intensive tasks and Set-of-Mark prompting. Training was conducted across different base LLMs (TinyLlama-1.1B, InternLM2-7B, LLaMA2-13B, Mixtral-8×7B) to produce models varying in parameter size and multilingual capabilities. Comprehensive benchmarking showed performance gains correlated with both data and parameter scaling, with SPHINX-MoE achieving competitive results against closed-source models on multiple benchmarks.

## Key Results
- SPHINX-X models demonstrated strong performance gains correlated with both data and parameter scaling
- SPHINX-MoE achieved competitive results against closed-source models on MathVerse, SciVerse, and MMVP benchmarks
- SPHINX-Plus showed strong video analysis capabilities despite being trained only on images
- The family includes models with varying parameter sizes from 1.1B to 8×7B parameters and multilingual capabilities

## Why This Works (Mechanism)
The paper presents SPHINX-X, a family of multimodal large language models developed by extending the SPHINX framework with architectural improvements, expanded training data, and parameter scaling. The authors simplified the training pipeline to a single stage, removed redundant visual encoders, and introduced learnable skip tokens for handling fully-padded sub-images. They curated a large-scale multi-domain dataset including public resources and two specialized datasets for OCR-intensive tasks and Set-of-Mark prompting. Training was conducted across different base LLMs (TinyLlama-1.1B, InternLM2-7B, LLaMA2-13B, Mixtral-8×7B) to produce models varying in parameter size and multilingual capabilities. Comprehensive benchmarking showed strong performance gains correlated with both data and parameter scaling, with SPHINX-MoE achieving competitive results against closed-source models on multiple benchmarks including MathVerse, SciVerse, and MMVP, while SPHINX-Plus demonstrated strong video analysis capabilities despite being trained only on images.

## Foundational Learning
- **Multi-modal training pipeline simplification**: Single-stage training replaces multi-stage approaches to reduce complexity and improve efficiency
- **Learnable skip tokens**: Mechanism to handle fully-padded sub-images by allowing the model to bypass empty regions
- **Parameter scaling correlation**: Performance improvements directly tied to increases in model parameters and training data volume
- **Specialized dataset curation**: Two custom datasets created for OCR-intensive tasks and Set-of-Mark prompting to improve domain-specific performance
- **Base model selection**: Different LLMs chosen to provide varying parameter sizes and multilingual capabilities across the family
- **Benchmark adaptation**: Models evaluated on multiple benchmarks including MathVerse, SciVerse, and MMVP for comprehensive performance assessment

## Architecture Onboarding

**Component map**: Input images → Visual encoder → Learnable skip tokens → Single-stage training pipeline → Base LLM (TinyLlama/InternLM2/LLaMA2/Mixtral) → Output

**Critical path**: Visual encoding → Skip token handling → Multi-modal fusion → LLM generation

**Design tradeoffs**: Simplified single-stage training vs. potential loss of multi-stage refinement benefits; learnable skip tokens vs. fixed padding strategies; parameter scaling vs. computational efficiency

**Failure signatures**: Overfitting to benchmark tasks due to dataset contamination; poor performance on unseen domains; computational bottlenecks with larger parameter counts; degraded multilingual capabilities when base model assumptions don't hold

**3 first experiments**: (1) Test model performance on held-out datasets not included in training corpus; (2) Evaluate multilingual capabilities across diverse language tasks; (3) Compare performance with other open-source multimodal models on key benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablation studies to isolate impact of individual architectural changes and training strategies
- Potential overfitting to benchmark tasks given use of public datasets in training
- Absence of comparisons with other open-source multimodal models on certain key benchmarks
- Video analysis claims for SPHINX-Plus lack comprehensive quantitative evaluation

## Confidence
- **High**: General scaling trends with data and parameters
- **Medium**: Specific architectural contributions without controlled ablations
- **Low**: Video analysis capabilities for SPHINX-Plus based only on demonstration examples
- **Medium**: Multilingual benefits inferred from base model choices rather than systematic testing

## Next Checks
1. Conduct ablation studies isolating the effects of learnable skip tokens, single-stage training, and simplified visual encoders
2. Test generalization on held-out datasets not included in the training corpus to verify against potential data contamination
3. Perform systematic evaluation of multilingual capabilities across diverse language tasks rather than relying on base model assumptions