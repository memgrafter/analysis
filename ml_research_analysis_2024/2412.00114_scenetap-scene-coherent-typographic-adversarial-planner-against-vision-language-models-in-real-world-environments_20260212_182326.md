---
ver: rpa2
title: 'SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language
  Models in Real-World Environments'
arxiv_id: '2412.00114'
source_url: https://arxiv.org/abs/2412.00114
tags:
- text
- adversarial
- image
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SceneTAP, the first approach to generate scene-coherent
  typographic adversarial attacks against vision-language models (LVLMs) that maintain
  visual naturalness while effectively misleading them. The method uses an LLM-based
  planner to automatically generate context-aware adversarial text, determine optimal
  placement locations within the scene, and provide instructions for natural integration.
---

# SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments

## Quick Facts
- arXiv ID: 2412.00114
- Source URL: https://arxiv.org/abs/2412.00114
- Reference count: 40
- Key outcome: First approach to generate scene-coherent typographic adversarial attacks against vision-language models (LVLMs) that maintain visual naturalness while effectively misleading them

## Executive Summary
This paper presents SceneTAP, a novel approach to generate typographic adversarial attacks against vision-language models (LVLMs) that are both effective and visually natural. The method uses an LLM-based planner to automatically generate context-aware adversarial text, determine optimal placement locations within scenes, and provide instructions for seamless integration. Extensive experiments demonstrate that SceneTAP significantly improves attack success rates while maintaining visual naturalness across multiple datasets and models, including ChatGPT-4o. The work also demonstrates practical physical-world deployment by printing and placing generated patches in real environments.

## Method Summary
SceneTAP employs an LLM-based planner operating through three stages: scene understanding, adversarial planning, and seamless integration. The planner analyzes both image content and question context to generate contextually appropriate adversarial text, determines optimal text placement locations based on semantic relationships, and generates detailed prompts for TextDiffuser to create visually natural text integration. This is followed by a local diffusion mechanism for image synthesis. The approach is extended to physical environments through printing and deployment in real-world scenes, with evaluation showing effectiveness across multiple LVLMs and datasets.

## Key Results
- Significantly improves attack success rates while maintaining visual naturalness across multiple datasets and models
- Demonstrates effectiveness against major LVLMs including ChatGPT-4o through physical-world deployment
- Shows strong correlation between contextual relevance of adversarial text and attack success rates
- Maintains contextual appropriateness while effectively misleading LVLMs about scene content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM planner generates context-aware adversarial text by analyzing both question and image content
- **Mechanism**: Uses chain-of-thought reasoning to identify key visual elements relevant to the question, then selects incorrect answers maintaining plausibility within image context
- **Core assumption**: LLM can accurately identify relevant image features and generate contextually appropriate incorrect answers
- **Evidence anchors**: Strong correlation between attack success rates and relevance of adversarial text to question and image content
- **Break condition**: LLM fails to accurately interpret image context or generates text lacking semantic relevance to question

### Mechanism 2
- **Claim**: Strategic placement of adversarial text near question-targeted regions increases attack success rates
- **Mechanism**: Analyzes semantic relationships between objects and question, identifies optimal placement locations through spatial analysis
- **Core assumption**: LVLMs process visual information with spatial attention patterns that can be manipulated by placing adversarial text near semantically relevant regions
- **Evidence anchors**: Regions with higher attack strengths are related to question and corresponding answers, highlighting importance of spatial context
- **Break condition**: LVLM's attention mechanisms don't follow expected patterns or semantic relationships are too complex for planner to model

### Mechanism 3
- **Claim**: Scene-coherent text insertion using diffusion models creates adversarial examples that maintain visual naturalness while being effective
- **Mechanism**: Generates detailed prompts for TextDiffuser specifying how to integrate adversarial text naturally, considering lighting, perspective, surface texture, and context
- **Core assumption**: Diffusion models can effectively synthesize text that appears naturally integrated into images when provided with appropriate guidance prompts
- **Evidence anchors**: Scene-coherent TextDiffuser executes attack using local diffusion mechanism, adversarial text must visually integrate within scene
- **Break condition**: Diffusion model cannot synthesize text that appears natural or physical printing process degrades visual quality

## Foundational Learning

- **Concept**: Chain-of-thought reasoning in LLMs
  - Why needed here: Planner uses CoT to systematically analyze images, questions, and generate appropriate adversarial text
  - Quick check question: How does chain-of-thought reasoning differ from direct prompting when generating complex outputs like adversarial text?

- **Concept**: Spatial attention mechanisms in vision-language models
  - Why needed here: Attack effectiveness depends on understanding how LVLMs allocate attention to different image regions
  - Quick check question: What evidence would indicate that an LVLM is focusing attention on a specific image region when processing a question?

- **Concept**: Diffusion models for image synthesis
  - Why needed here: SceneTAP uses TextDiffuser to naturally integrate adversarial text into images
  - Quick check question: How do guidance prompts influence the output of diffusion models when synthesizing text into existing images?

## Architecture Onboarding

- **Component map**: Input Layer (Image, question, correct answer) -> LLM Planner (Scene understanding -> Adversarial text generation -> Placement determination -> Integration instructions) -> TextDiffuser -> Output Layer (Adversarial image, evaluation metrics) -> Physical Deployment (Print and place in real-world environments)

- **Critical path**: Image → LLM Planner (Scene understanding → Adversarial text generation → Placement determination → Integration instructions) → TextDiffuser → Adversarial image

- **Design tradeoffs**:
  - Text naturalness vs. attack effectiveness: More natural text may be less effective as adversarial examples
  - Placement precision vs. visual coherence: Exact placement may be less visually natural than approximate placement
  - Computational cost vs. quality: More LLM reasoning steps improve quality but increase inference time

- **Failure signatures**:
  - Low ASR with high naturalness scores: Text is too subtle or contextually appropriate
  - High ASR with low naturalness scores: Text is obvious or poorly integrated
  - Inconsistent results across similar inputs: Planner lacks robustness in reasoning

- **First 3 experiments**:
  1. Test adversarial text generation on 10 image-question pairs with known correct answers, measuring ASR and naturalness scores
  2. Compare placement strategies (center vs. context-aware) on the same 10 pairs to quantify spatial optimization impact
  3. Evaluate physical deployment by printing and photographing adversarial examples in controlled environments, measuring detection rates by human observers

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and discussion, several important questions remain:

1. How does the effectiveness of SceneTAP's typographic attacks vary across different cultural contexts and language models trained on diverse datasets?

2. What are the long-term robustness implications of SceneTAP attacks against LVLMs in safety-critical applications like autonomous driving?

3. How does SceneTAP's performance scale with image complexity and scene density in real-world environments?

## Limitations

- Relies heavily on LLM planner's ability to accurately interpret image content and generate contextually appropriate adversarial text
- Assumes LVLMs process visual information with predictable spatial attention patterns that can be exploited
- Physical printing process may introduce artifacts that reduce effectiveness in real-world deployment
- Evaluation datasets may not fully capture diversity of real-world scenarios where LVLMs are deployed

## Confidence

- **High Confidence**: The mechanism for scene-coherent text insertion using diffusion models is well-established, and the three-stage planner architecture is clearly defined
- **Medium Confidence**: The LLM planner's ability to accurately analyze image content and generate effective adversarial text depends on quality of underlying LLM and prompt engineering
- **Medium Confidence**: The assumption about LVLM attention mechanisms and optimal text placement is supported by experimental evidence but may not generalize to all architectures

## Next Checks

1. Evaluate SceneTAP on a broader range of LVLM architectures and question types to assess generalization beyond tested models

2. Test approach's sensitivity to prompt variations in LLM planner and diffusion model parameters to quantify dependence on specific implementation details

3. Conduct extensive real-world testing by deploying printed adversarial examples in diverse environments and measuring detection rates by both LVLMs and human observers