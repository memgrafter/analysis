---
ver: rpa2
title: 'Embark on DenseQuest: A System for Selecting the Best Dense Retriever for
  a Custom Collection'
arxiv_id: '2407.06685'
source_url: https://arxiv.org/abs/2407.06685
tags:
- densequest
- dense
- collection
- system
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DenseQuest is a web-based system that addresses the challenge of
  selecting the best dense retriever for a private collection without requiring relevance
  judgments. The system implements multiple unsupervised performance evaluation methods,
  including query performance prediction and collection-based approaches, such as
  Binary Entropy, Query Alteration, and a novel LLM-powered method called LARMOR.
---

# Embark on DenseQuest: A System for Selecting the Best Dense Retriever for a Custom Collection

## Quick Facts
- arXiv ID: 2407.06685
- Source URL: https://arxiv.org/abs/2407.06685
- Authors: Ekaterina Khramtsova; Teerapong Leelanupab; Shengyao Zhuang; Mahsa Baktashmotlagh; Guido Zuccon
- Reference count: 19
- Primary result: Web-based system for dense retriever selection without relevance judgments

## Executive Summary
DenseQuest is a web-based system designed to help users select the best dense retriever for their custom collection without requiring relevance judgments. The system implements multiple unsupervised performance evaluation methods, including query performance prediction and collection-based approaches, to rank suitable dense retrievers. Users can upload their collections, choose from various model selection methods, and receive ranked recommendations along with downloadable model checkpoints and deployment instructions. The cloud-based architecture enables scalable and accessible deployment for search engine practitioners and researchers.

## Method Summary
DenseQuest consolidates state-of-the-art methods for dense retriever selection by implementing multiple unsupervised evaluation approaches. The system combines query performance prediction techniques with collection-based methods, including a novel LLM-powered approach called LARMOR. Users upload their private collections and specify parameters such as embedding models and the number of queries to generate. The system then evaluates various dense retrievers using the selected methods and provides ranked recommendations. The cloud-based infrastructure supports scalable deployment, allowing users to download the best-performing model checkpoint along with deployment instructions.

## Key Results
- Implements multiple unsupervised performance evaluation methods including Binary Entropy, Query Alteration, and LARMOR
- Provides ranked recommendations for suitable dense retrievers without requiring relevance judgments
- Enables cloud-based scalable deployment with downloadable model checkpoints and deployment instructions

## Why This Works (Mechanism)
DenseQuest leverages unsupervised evaluation methods to assess dense retriever performance without human relevance judgments. The system combines query performance prediction (measuring how well queries can be answered) with collection-based approaches that analyze the characteristics of the document collection itself. The novel LARMOR method uses large language models to enhance evaluation accuracy. By aggregating results from multiple evaluation methods, the system provides robust recommendations for the most suitable dense retriever for a given collection, addressing the challenge of selecting appropriate models when private data prevents using traditional supervised evaluation approaches.

## Foundational Learning

**Dense retrieval**: Neural network-based document retrieval using dense vector representations; needed to understand the core technology being evaluated; quick check: verify models use vector similarity for ranking.

**Unsupervised evaluation**: Performance assessment without human relevance judgments; needed because private collections lack labeled data; quick check: confirm evaluation relies on collection statistics and query properties.

**Query performance prediction**: Estimating query difficulty based on features like term frequency and ambiguity; needed to assess how well different retrievers handle varying query types; quick check: verify methods analyze query characteristics without external data.

**Collection-based analysis**: Evaluating retrievers based on document collection properties; needed to capture how retrievers perform on specific document characteristics; quick check: confirm methods examine collection statistics like term distributions.

**LARMOR (LLM-powered method)**: Novel approach using large language models for retriever evaluation; needed to improve accuracy of unsupervised assessment; quick check: verify LLM integration enhances traditional evaluation metrics.

## Architecture Onboarding

**Component map**: User uploads collection -> System generates queries -> Multiple evaluation methods run in parallel -> Results aggregated -> Ranked recommendations generated -> Model checkpoint prepared for download

**Critical path**: Collection upload -> Query generation -> Evaluation method execution -> Result aggregation -> Model recommendation and download

**Design tradeoffs**: Cloud-based deployment enables scalability but requires internet connectivity; multiple evaluation methods provide robustness but increase computational cost; unsupervised approaches avoid annotation burden but may be less accurate than supervised methods

**Failure signatures**: Poor recommendations may indicate inadequate query generation, unsuitable evaluation methods for collection type, or insufficient computational resources; collection characteristics outside evaluation method assumptions may lead to suboptimal results

**First experiments**: 1) Upload small test collection and verify query generation; 2) Run single evaluation method to confirm basic functionality; 3) Compare recommendations across different evaluation methods on the same collection

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on only 15 public datasets, potentially limiting generalizability to diverse private collections
- Unsupervised methods not validated against human relevance judgments for specific collections being evaluated
- Cloud-based architecture details remain vague regarding handling of different collection sizes and query loads

## Confidence

**High confidence**: The system's basic architecture and web interface functionality are well-defined and implementable

**Medium confidence**: The unsupervised evaluation methods are established techniques, but their effectiveness specifically for dense retriever selection across arbitrary collections is less certain

**Medium confidence**: The LLM-powered LARMOR method represents a novel contribution, but its performance relative to traditional methods across diverse collections requires further validation

## Next Checks
1. Conduct systematic evaluation of DenseQuest's recommendations against human relevance judgments across diverse private collections, particularly those with specialized terminology or domain-specific language

2. Test the system's scalability and performance with collections of varying sizes (from hundreds to millions of documents) to verify the cloud-based architecture's robustness

3. Compare accuracy and computational efficiency of LARMOR method against Binary Entropy and Query Alteration methods across multiple collection types to validate its contribution to the system