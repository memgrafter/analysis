---
ver: rpa2
title: Reassessing the Validity of Spurious Correlations Benchmarks
arxiv_id: '2409.04188'
source_url: https://arxiv.org/abs/2409.04188
tags:
- benchmarks
- benchmark
- spurious
- test
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper exposes a critical flaw in current spurious correlations
  benchmarks: they often disagree, with the best-performing method on one benchmark
  ranking poorly on another. To address this, the authors introduce three desiderata
  (ERM Failure, Discriminative Power, and Convergent Validity) that a valid benchmark
  should satisfy, and propose a model-dependent statistic K to measure task difficulty
  due to spurious correlation.'
---

# Reassessing the Validity of Spurious Correlations Benchmarks

## Quick Facts
- arXiv ID: 2409.04188
- Source URL: https://arxiv.org/abs/2409.04188
- Reference count: 40
- Primary result: Spurious correlation benchmarks disagree, with best methods varying across benchmarks; authors propose three desiderata and a model-dependent K statistic to evaluate benchmark validity and improve method selection.

## Executive Summary
This paper critically examines the validity of current spurious correlations benchmarks, revealing that they often disagree on which mitigation methods perform best. The authors propose three essential desiderata for valid benchmarks: ERM Failure (producing between-group performance disparities), Discriminative Power (assigning different scores to different methods), and Convergent Validity (agreeing with other similar benchmarks). They introduce a model-dependent statistic K to measure task difficulty due to spurious correlation, enabling better benchmark evaluation and method selection. The analysis shows that popular benchmarks like ImageNetBG fail to satisfy these desiderata, and many mitigation methods lack robustness across different tasks.

## Method Summary
The authors evaluate eight spurious correlations benchmarks (Waterbirds, CelebA, ImageNetBG, MetaShift, NICO++, CheXpert, CivilComments, MultiNLI) using three validity desiderata. They introduce a model-dependent statistic K based on Bayes Factor to quantify task difficulty due to spurious correlation. The evaluation uses ResNet-50 for vision benchmarks and BERT for language, with 16 random hyperparameter configurations and 3 random seeds per method. They analyze method robustness by measuring variance in worst-group test accuracy across benchmarks and perform Leave-One-Out analysis to develop practical recommendations for method selection based on K similarity between new datasets and existing benchmarks.

## Key Results
- ImageNetBG fails all three desiderata (ERM Failure, Discriminative Power, Convergent Validity), making it unsuitable for evaluating spurious correlation mitigation methods
- Most mitigation methods are not robust, with performance varying widely across benchmarks (e.g., ERM performs best on CheXpert but worst on MultiNLI)
- Method selection based on closest K value to a benchmark improves worst-group test accuracy in 5 out of 8 test datasets
- K values range from 10⁻⁴ to 10⁵ across benchmarks, with larger K indicating more task difficulty due to spurious correlation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spurious correlations benchmarks disagree because they measure different levels of task difficulty due to spurious correlation.
- Mechanism: The paper introduces a model-dependent statistic K that quantifies task difficulty due to spurious correlation. Benchmarks with similar K values should agree more strongly, while those with different K values should disagree.
- Core assumption: Task difficulty due to spurious correlation is the defining characteristic of a spurious correlations benchmark.
- Evidence anchors:
  - [abstract]: "We introduce a model-dependent statistic K to measure task difficulty due to spurious correlation."
  - [section 3.4.1]: "We argue that three factors should determine how a spurious correlations benchmark behaves... To account for all three factors, we propose using the Bayes Factor as a model-dependent statistic..."
- Break condition: If the model-dependent nature of K doesn't accurately capture task difficulty, or if other factors beyond spurious correlation significantly influence benchmark behavior.

### Mechanism 2
- Claim: Valid spurious correlations benchmarks must satisfy three desiderata: ERM Failure, Discriminative Power, and Convergent Validity.
- Mechanism: ERM Failure ensures benchmarks produce between-group performance disparities for ERM-trained models. Discriminative Power ensures benchmarks assign different scores to different methods. Convergent Validity ensures benchmarks agree with other similar benchmarks.
- Core assumption: These three properties are necessary and sufficient for a benchmark to meaningfully evaluate spurious correlation mitigation methods.
- Evidence anchors:
  - [section 3]: "For a benchmark to be a meaningful test of a method's ability to mitigate spurious correlations, we suggest it should satisfy three desiderata..."
  - [section 3.1]: "We evaluate the validity of eight benchmarks... Only certain benchmarks satisfy all three desiderata."
- Break condition: If additional properties are needed for validity, or if these properties are not jointly sufficient to ensure meaningful evaluation.

### Mechanism 3
- Claim: The practical recommendation to choose mitigation methods based on the closest benchmark according to K improves method selection.
- Mechanism: By calculating K for a new dataset and selecting the most similar benchmark, practitioners can identify which methods perform best on similar tasks.
- Core assumption: The similarity in K values between a new dataset and a benchmark indicates similar task difficulty due to spurious correlation, leading to similar optimal methods.
- Evidence anchors:
  - [section 5]: "We present a recipe for mapping between benchmarks and real-world datasets... Using the closest benchmark (col. 8) results in the best performance for 5 out of 8 test datasets."
  - [section 5]: "For five of eight test datasets, selecting a method according to the closest benchmark would produce superior worst-group test accuracies."
- Break condition: If other factors beyond K similarity influence method performance, or if the relationship between K similarity and method effectiveness doesn't hold for certain types of datasets.

## Foundational Learning

- Concept: Bayes Factor and likelihood ratios
  - Why needed here: The paper uses Bayes Factor to quantify task difficulty due to spurious correlation, comparing models that use vs. penalize spurious correlations.
  - Quick check question: How does the Bayes Factor differ from simple accuracy or mutual information in measuring the impact of spurious correlations?

- Concept: Convergent validity in measurement theory
  - Why needed here: The paper uses convergent validity to evaluate whether benchmarks that measure similar things (task difficulty due to spurious correlation) should agree with each other.
  - Quick check question: What other forms of validity (e.g., construct validity, criterion validity) might also be relevant for evaluating spurious correlations benchmarks?

- Concept: Empirical Risk Minimization (ERM) and its limitations
  - Why needed here: The paper evaluates whether benchmarks satisfy ERM Failure, which requires that ERM-trained models exhibit between-group performance disparities.
  - Quick check question: How does ERM's tendency to learn spurious correlations relate to the bias-variance tradeoff in machine learning?

## Architecture Onboarding

- Component map: Benchmark evaluation (ERM Failure, Discriminative Power, Convergent Validity) -> K calculation -> Method selection
- Critical path: 1) Calculate K for a new dataset, 2) Find the closest benchmark based on K, 3) Select the best-performing method from that benchmark
- Design tradeoffs: Using K as a measure of task difficulty provides a model-dependent perspective but requires group annotations and careful model training. The three desiderata provide a comprehensive evaluation framework but may exclude some potentially useful benchmarks.
- Failure signatures: Benchmarks that don't satisfy ERM Failure may not represent real-world spurious correlation problems. Benchmarks with low Discriminative Power may not effectively differentiate between methods. Benchmarks that disagree with others despite similar K values may have implementation issues.
- First 3 experiments:
  1. Implement K calculation for a simple benchmark (e.g., Waterbirds) and verify it changes as expected with synthetic modifications (e.g., increasing label-attribute correlation).
  2. Evaluate a new benchmark against the three desiderata and compare its K value to existing benchmarks.
  3. Test the method selection recommendation by calculating K for a new dataset and checking if the suggested method performs well on that dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What would be the effect of using different reference methods beyond Reweight and GroupDRO for computing K, such as using ERM with domain adversarial training?
- Basis in paper: [explicit] The authors mention that K could be implemented with alternative reference methods and briefly test GroupDRO, finding similar results to Reweight.
- Why unresolved: The paper only tests two reference methods and doesn't explore other potential reference methods that might capture different aspects of spurious correlation difficulty.
- What evidence would resolve it: Systematic comparison of K values computed using various reference methods (ERM, Reweight, GroupDRO, domain adversarial training, etc.) across multiple benchmarks, showing whether different reference methods lead to substantially different conclusions about benchmark validity.

### Open Question 2
- Question: How would K values change if computed using models trained with different architectures (e.g., vision transformers instead of ResNet-50, or different language models)?
- Basis in paper: [inferred] K is described as model-dependent, but the paper only reports K values using ResNet-50 for vision tasks and BERT for language tasks.
- Why unresolved: The paper doesn't explore how sensitive K is to architectural choices, which could affect its practical utility for comparing benchmarks across different model families.
- What evidence would resolve it: Computing K for the same benchmarks using multiple model architectures and analyzing the correlation between K values across architectures, particularly focusing on whether the relative ranking of benchmarks by K remains consistent.

### Open Question 3
- Question: Could benchmarks be designed to satisfy all three desiderata (ERM Failure, Discriminative Power, and Convergent Validity) while still maintaining practical relevance to real-world spurious correlation problems?
- Basis in paper: [explicit] The authors identify ImageNetBG as failing all three desiderata and discuss how certain benchmarks don't satisfy all requirements, raising questions about benchmark design.
- Why unresolved: The paper analyzes existing benchmarks but doesn't explore whether it's possible to construct new benchmarks that would satisfy all desiderata while remaining practically useful.
- What evidence would resolve it: Development and validation of new benchmark datasets that satisfy all three desiderata, along with analysis of whether these benchmarks maintain practical relevance to real-world spurious correlation scenarios.

## Limitations

- Model-dependent nature of K introduces uncertainty about generalizability across different architectures and training regimes
- Practical recommendation based on K similarity has limited validation, tested on only eight datasets
- Assumes ERM Failure, Discriminative Power, and Convergent Validity are sufficient criteria for benchmark validity without exploring additional necessary properties

## Confidence

**High Confidence**: The observation that spurious correlation benchmarks disagree and that no single method consistently performs best across all benchmarks is well-supported by the empirical results.

**Medium Confidence**: The three desiderata for benchmark validity are logically sound, but their sufficiency for meaningful evaluation of spurious correlation mitigation methods could benefit from further theoretical justification.

**Medium Confidence**: The K statistic effectively quantifies task difficulty due to spurious correlation, but its model-dependence and sensitivity to implementation choices warrant caution in interpretation.

## Next Checks

1. **Architecture Sensitivity Test**: Calculate K and evaluate method performance using different model architectures (e.g., Vision Transformers for vision tasks, RoBERTa for language tasks) to assess how sensitive the findings are to model choice.

2. **Extended Validation Set**: Apply the practical recommendation framework to a larger and more diverse set of real-world datasets beyond the eight tested, particularly focusing on domains with different types of spurious correlations.

3. **Additional Validity Criteria**: Investigate whether additional validity properties beyond the three proposed desiderata (e.g., construct validity, temporal stability) are needed to ensure benchmarks meaningfully evaluate spurious correlation mitigation methods.