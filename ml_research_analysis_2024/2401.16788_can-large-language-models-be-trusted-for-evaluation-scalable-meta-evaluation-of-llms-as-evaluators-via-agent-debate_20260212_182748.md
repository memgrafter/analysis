---
ver: rpa2
title: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
  of LLMs as Evaluators via Agent Debate
arxiv_id: '2401.16788'
source_url: https://arxiv.org/abs/2401.16788
tags:
- llms
- evaluation
- meta-evaluation
- human
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the performance
  of large language models (LLMs) as evaluators across diverse tasks and scenarios.
  To tackle this, the authors propose SCALE EVAL, a scalable meta-evaluation framework
  that leverages agent debate among multiple LLM agents to assist human annotators
  in discerning the most capable LLMs as evaluators.
---

# Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate

## Quick Facts
- arXiv ID: 2401.16788
- Source URL: https://arxiv.org/abs/2401.16788
- Reference count: 7
- Multi-agent debate framework SCALE EVAL achieves higher agreement with human experts compared to individual LLM evaluations

## Executive Summary
This paper introduces SCALE EVAL, a scalable meta-evaluation framework that leverages multi-agent debate among LLM evaluators to determine which models are most effective at evaluating other LLMs' outputs. The framework uses multiple communicative agents to conduct pairwise comparisons of LLM responses, reaching consensus through debate while minimizing human annotation workload. Experiments show SCALE EVAL generally achieves higher agreement rates with human experts compared to individual LLM evaluations across brainstorming, coding, math, and writing scenarios.

## Method Summary
SCALE EVAL employs a multi-agent debate system where three LLM agents (gpt-4-turbo, claude-2, gpt-3.5-turbo) evaluate pairwise comparisons of LLM-generated responses. Agents engage in multi-round discussions to reach consensus on which response is better, with human intervention only when consensus cannot be reached after D rounds. The framework supports user-defined scenarios and criteria, allowing flexible application across different evaluation contexts. Meta-evaluation results are compared against human expert annotations to assess the reliability of different LLMs as evaluators.

## Key Results
- SCALE EVAL achieves higher agreement rates with human experts compared to individual LLM evaluations
- Multi-agent debate reduces human annotation workload while maintaining evaluation quality
- LLM evaluator performance deteriorates with adversarially formatted criteria prompts (e.g., masked letters)
- gpt-4-turbo and gpt-3.5-turbo show resilience to criteria prompt format variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent debate improves meta-evaluation reliability compared to single LLM evaluation.
- Mechanism: Multiple communicative LLM agents engage in multi-round discussions, allowing each agent to review and reconsider others' assessments. This process leads to more comprehensive evaluations and higher agreement with human expert judgments.
- Core assumption: LLM agents can critically analyze and debate evaluation criteria and responses in a meaningful way.
- Evidence anchors:
  - [abstract] "SCALE EVAL, a scalable meta-evaluation framework that leverages agent debate among multiple LLM agents to assist human annotators in discerning the most capable LLMs as evaluators."
  - [section] "In cases where agents fail to reach a consensus after d = D âˆ’ 1 rounds of discussions, a human evaluator intervenes."
- Break condition: If agents cannot provide meaningful justifications or consistently disagree without reaching consensus, the framework's reliability would decrease.

### Mechanism 2
- Claim: SCALE EVAL reduces human annotation workload while maintaining evaluation quality.
- Mechanism: The framework uses multi-agent debate to handle most of the meta-evaluation, with human intervention only required when agents fail to reach consensus. This significantly reduces the need for extensive human annotation.
- Core assumption: Multi-agent debate can reliably replace human judgment in most cases.
- Evidence anchors:
  - [abstract] "This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation."
  - [section] "Since our framework allows users to use their own prompts and responses while applying the framework to any scenario or criterion that they define, it offers flexibility and adaptability in various evaluation contexts."
- Break condition: If human intervention is required too frequently (e.g., in more than 50% of cases), the workload reduction benefit would be diminished.

### Mechanism 3
- Claim: Criteria prompt format variations significantly impact LLM evaluator performance.
- Mechanism: Different formats of criteria prompts (e.g., shortened, gibberish, shuffled) affect how well LLMs can understand and apply evaluation criteria, revealing limitations in their robustness to input variations.
- Core assumption: LLMs are sensitive to input format and may struggle with non-standard or adversarial prompt formats.
- Evidence anchors:
  - [section] "Based on Table 5, we observe that the performance of LLMs as evaluators generally deteriorates when certain letters in the criteria prompts are masked."
  - [section] "Both gpt-4-turbo and gpt-3.5-turbo demonstrate some resilience to these adversarially formatted criteria prompts, maintaining a relatively consistent agreement rates across various criteria formats."
- Break condition: If LLM evaluators consistently perform well regardless of criteria prompt format variations, this mechanism would not hold.

## Foundational Learning

- Concept: Pairwise response comparison
  - Why needed here: The framework uses pairwise response comparison for both evaluation and meta-evaluation, comparing two LLM-generated responses to determine which is better.
  - Quick check question: In pairwise response comparison, if the evaluation metric outputs 1, 0, or -1, what does each value represent?

- Concept: Meta-evaluation
  - Why needed here: Meta-evaluation is used to assess the quality of automatic evaluation metrics (in this case, LLMs as evaluators) by comparing them to a gold-standard metric (human expert judgments).
  - Quick check question: What are the two types of agreement rates used in meta-evaluation, and how do they differ?

- Concept: Agent debate and consensus building
  - Why needed here: The framework relies on multiple LLM agents debating and reaching consensus on evaluations, with human intervention only when consensus cannot be reached.
  - Quick check question: How many rounds of discussion are allowed before human intervention is required in the SCALE EVAL framework?

## Architecture Onboarding

- Component map: User-defined scenarios and criteria -> Prompt and response generation -> Multi-agent debate system -> Human expert intervention (when needed) -> Agreement rate calculation

- Critical path: 1. User defines scenarios and criteria 2. Prompts are sent to LLMs to generate responses 3. Multi-agent debate system conducts meta-evaluation 4. Human experts intervene if agents don't reach consensus 5. Agreement rates are calculated and compared

- Design tradeoffs: More agents vs. computational cost and complexity; Number of debate rounds vs. time and potential for human intervention; Flexibility in criteria definition vs. consistency across evaluations

- Failure signatures: High frequency of human interventions; Low agreement rates between meta-evaluation and human expert judgments; Inconsistent performance across different criteria prompt formats

- First 3 experiments: 1. Run SCALE EVAL on a small set of prompts with known good responses to verify basic functionality 2. Compare agreement rates between multi-agent debate and single LLM evaluation on the same dataset 3. Test the impact of varying the number of debate rounds on consensus rates and agreement with human judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SCALE EVAL framework perform when evaluating LLMs as evaluators in user-defined scenarios that are not covered in the existing benchmarks?
- Basis in paper: [explicit] The authors mention that SCALE EVAL allows users to use their own prompts and responses while applying the framework to any scenario or criterion that they define.
- Why unresolved: The paper focuses on evaluating the framework's performance on a limited set of predefined scenarios. It does not explore how well the framework generalizes to entirely new, user-defined scenarios.
- What evidence would resolve it: Conducting experiments using SCALE EVAL to evaluate LLMs as evaluators on a diverse set of user-defined scenarios and comparing the results with human expert evaluations would provide insights into the framework's performance in such settings.

### Open Question 2
- Question: What is the impact of the number of debate rounds in the multi-agent debate on the quality and reliability of the meta-evaluation results?
- Basis in paper: [explicit] The authors mention that the framework supports multi-round discussions and allow agents to review and re-evaluate other agents' assessments.
- Why unresolved: The paper does not provide a detailed analysis of how the number of debate rounds affects the quality and reliability of the meta-evaluation results. It is unclear whether increasing the number of rounds leads to better outcomes or if there is a point of diminishing returns.
- What evidence would resolve it: Conducting experiments with varying numbers of debate rounds and comparing the resulting meta-evaluation outcomes would help determine the optimal number of rounds for achieving reliable results.

### Open Question 3
- Question: How do different LLM architectures and sizes impact their performance as evaluators in the SCALE EVAL framework?
- Basis in paper: [inferred] The authors evaluate the performance of different LLMs (e.g., gpt-4-turbo, claude-2, gpt-3.5-turbo) as evaluators in various scenarios and criteria.
- Why unresolved: The paper does not provide a comprehensive analysis of how the architectural differences and sizes of LLMs influence their effectiveness as evaluators. It is unclear whether larger or more advanced models consistently outperform smaller ones or if there are specific scenarios where smaller models excel.
- What evidence would resolve it: Conducting experiments comparing the performance of LLMs with different architectures and sizes as evaluators in the SCALE EVAL framework would provide insights into the impact of these factors on their evaluative capabilities.

## Limitations
- Limited human expert validation sample size (only 20 prompts across four scenarios)
- Framework performance unverified in domains beyond brainstorming, coding, math, and writing
- Statistical significance of improvements not fully established

## Confidence

**High confidence:** The multi-agent debate mechanism improves agreement with human judgments compared to single LLM evaluation (supported by quantitative results showing higher agreement rates)

**Medium confidence:** The framework significantly reduces human annotation workload (based on limited human intervention frequency data)

**Low confidence:** The framework's generalizability to scenarios outside the four tested domains

## Next Checks
1. Conduct human expert validation on a larger sample size (minimum 100 prompts) to verify the statistical significance of the claimed improvements
2. Test the framework's performance on domain-specific scenarios not included in the original study (e.g., medical, legal, or technical domains)
3. Evaluate the impact of varying the number of LLM agents in the debate system on consensus rates and agreement with human judgments