---
ver: rpa2
title: Gradient Descent on Logistic Regression with Non-Separable Data and Large Step
  Sizes
arxiv_id: '2406.05033'
source_url: https://arxiv.org/abs/2406.05033
tags:
- step
- size
- cycle
- dataset
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies gradient descent (GD) dynamics on logistic\
  \ regression problems with large, constant step sizes, particularly when data is\
  \ not linearly separable. The authors show that while \u03B7 < 2/\u03BB is necessary\
  \ for local convergence, it is not sufficient for global convergence."
---

# Gradient Descent on Logistic Regression with Non-Separable Data and Large Step Sizes

## Quick Facts
- arXiv ID: 2406.05033
- Source URL: https://arxiv.org/abs/2406.05033
- Reference count: 40
- Key outcome: GD on logistic regression can converge to stable cycles even with step sizes below 2/λ threshold when data is not linearly separable

## Executive Summary
This paper studies gradient descent (GD) dynamics on logistic regression problems with large, constant step sizes, particularly when data is not linearly separable. The authors show that while η < 2/λ is necessary for local convergence, it is not sufficient for global convergence. In one dimension, they prove that η ≤ 1/λ guarantees global convergence, while in higher dimensions, GD can converge to stable cycles even with step sizes below the critical 2/λ threshold. The authors construct specific datasets that induce such cyclic behavior and demonstrate that the implicit bias of GD at the edge of stability does not always guarantee convergence to the minimizer.

## Method Summary
The authors analyze GD on logistic regression by implementing the loss function L(w) = 1/n Σ log(1 + exp(-y_i w^T x_i)) and its gradient/Hessian. They run GD with constant step size η, iterating w_{t+1} = w_t - η∇L(w_t), and vary step sizes from small to large, including critical values like 2/λ and 1/λ. Experiments are conducted on heart dataset from LIBSVM and synthetic datasets constructed for specific purposes. The convergence behavior, loss function evolution, largest eigenvalue of Hessian (λmax), and power spectral density of loss over iterations are analyzed.

## Key Results
- η < 2/λ is necessary but not sufficient for global convergence of GD on non-separable logistic regression
- In 1D, η ≤ 1/λ guarantees global convergence, while in higher dimensions, GD can converge to stable cycles
- The authors construct datasets that induce cyclic behavior at specific step sizes below 2/λ threshold
- Implicit bias of GD at edge of stability doesn't always guarantee convergence to the minimizer

## Why This Works (Mechanism)
The paper's theoretical analysis suggests that in non-separable logistic regression, the loss landscape contains multiple local minima and saddle points that can trap GD with certain step sizes. The Hessian's eigenvalues near zero create conditions where GD oscillates rather than converging, particularly when the step size is large enough to overshoot the basin of attraction for the global minimum but still below the 2/λ threshold. The geometric construction of specific datasets amplifies these oscillatory dynamics by creating narrow valleys and plateaus in the loss landscape that interact poorly with fixed step sizes.

## Foundational Learning
1. Logistic regression loss landscape: Understanding the non-convex geometry of logistic loss with non-separable data, particularly how the Hessian's largest eigenvalue (λmax) governs stability.
   - Why needed: The 2/λ threshold is central to convergence analysis
   - Quick check: Verify that ∇²L(w) is positive semidefinite for logistic regression

2. Gradient descent stability analysis: The relationship between step size, Hessian eigenvalues, and convergence behavior, including how cycles can emerge from eigenvalues close to zero.
   - Why needed: Explains why η < 2/λ is necessary but not sufficient
   - Quick check: Compute eigenvalues of Hessian at different points in parameter space

3. Implicit bias in gradient descent: How GD's trajectory is influenced by initialization and step size, particularly near the edge of stability.
   - Why needed: The paper challenges assumptions about implicit bias guaranteeing convergence
   - Quick check: Track parameter updates to see if they exhibit progressive sharpening

## Architecture Onboarding
- Component map: Logistic regression loss -> Gradient computation -> GD update rule -> Loss monitoring -> Cycle detection
- Critical path: Initialization → GD iteration → Loss evaluation → Convergence/cycle check → Step size adjustment
- Design tradeoffs: Large step sizes enable faster convergence but risk cycles; smaller steps ensure stability but slow progress
- Failure signatures: Oscillating loss values, non-converging parameter updates, multiple attractors depending on initialization
- First experiments:
  1. Run GD on heart dataset with η = 1.9/λ and monitor loss for cyclic behavior
  2. Implement synthetic dataset construction and verify cycle induction at specific step sizes
  3. Compare GD trajectories with and without data normalization to test hypothesis about feature scaling

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does data normalization or feature scaling (e.g., requiring ∥xi∥ = 1) allow gradient descent to converge with larger step sizes, potentially up to 2/λ, in non-separable logistic regression problems?
- Basis in paper: Authors note that the datasets used in their experiments had features scaled within [-1, 1] and suggest this might explain why GD appeared to converge to the minimizer below 2/λ.
- Why unresolved: The authors did not conduct experiments with normalized data to verify this hypothesis.
- What evidence would resolve it: Experiments comparing GD convergence with and without data normalization on various non-separable datasets, measuring the maximum stable step size achieved in each case.

### Open Question 2
- Question: What is a sufficient condition for global convergence of gradient descent in higher dimensions (d > 1) that is less conservative than 2/L?
- Basis in paper: The authors prove that in 1D, η ≤ 1/λ guarantees global convergence, but show that for d ≥ 2, even smaller step sizes can lead to cycles.
- Why unresolved: The paper focuses on proving negative results rather than finding positive sufficient conditions.
- What evidence would resolve it: A theoretical proof establishing a step size threshold that guarantees global convergence in higher dimensions, or empirical evidence showing a practical bound through extensive experimentation.

### Open Question 3
- Question: Does the existence of cycles below 2/λ in non-separable logistic regression explain the Edge-of-Stability phenomenon observed in deep neural network training?
- Basis in paper: The authors discuss how their results seem to contradict predictions about self-stabilization in EoS, and suggest that the lack of progressive sharpening in their examples might explain the non-self-stabilizing behavior.
- Why unresolved: The connection between these simple logistic regression examples and complex neural network training is speculative.
- What evidence would resolve it: Analysis of real neural network training data to identify outlier patterns, and experiments showing whether removing such outliers affects EoS behavior.

## Limitations
- The theoretical analysis is rigorous for 1D cases but relies more on empirical observations for higher dimensions
- Cycle construction becomes increasingly complex in higher dimensions, limiting generalizability
- The paper doesn't provide a sufficient condition for global convergence in d > 1

## Confidence
- Main theoretical claims: Medium
- Experimental methodology: Medium
- Reproducibility of specific cycle patterns: Low-Medium

## Next Checks
1. Reconstruct the synthetic datasets using the described geometric properties and verify that they indeed induce the reported cyclic behavior with specific step sizes
2. Implement the power spectral density analysis of loss values over iterations to characterize the cyclic patterns observed in experiments
3. Conduct systematic experiments varying initialization scales to confirm the bifurcation behavior described in the paper, particularly for datasets that converge to different cycles depending on initialization