---
ver: rpa2
title: 'Show and Guide: Instructional-Plan Grounded Vision and Language Model'
arxiv_id: '2409.19074'
source_url: https://arxiv.org/abs/2409.19074
tags:
- step
- video
- multimodal
- plan
- moment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MM-PlanLLM, the first multimodal LLM designed
  to guide users through complex procedural tasks by leveraging both textual plans
  and visual information. The model is trained on three key tasks: Conversational
  Video Moment Retrieval (CVMR), where it retrieves relevant video segments based
  on user queries; Visually-Informed Step Generation (VSG), where it generates the
  next plan step conditioned on an image of the user''s current progress; and Plan-Grounded
  Answer Generation (PGAG).'
---

# Show and Guide: Instructional-Plan Grounded Vision and Language Model

## Quick Facts
- arXiv ID: 2409.19074
- Source URL: https://arxiv.org/abs/2409.19074
- Reference count: 32
- Primary result: First multimodal LLM for procedural task guidance with 5.50 R@1, 38.53 R@5, 53.82 R@10, and 21.52 mAP on CVMR

## Executive Summary
This paper introduces MM-PlanLLM, the first multimodal LLM designed to guide users through complex procedural tasks by leveraging both textual plans and visual information. The model is trained on three key tasks: Conversational Video Moment Retrieval (CVMR), where it retrieves relevant video segments based on user queries; Visually-Informed Step Generation (VSG), where it generates the next plan step conditioned on an image of the user's current progress; and Plan-Grounded Answer Generation (PGAG). The model uses a multitask-multistage training approach to gradually expose the model to multimodal instructional-plan semantic layers. MM-PlanLLM achieves strong performance on both multimodal and textual dialogue in a plan-grounded setting.

## Method Summary
MM-PlanLLM employs a multitask-multistage training approach with three stages: (1) general image captioning and retrieval on CC3M, (2) domain-specific video-action annotations on the Tasty dataset, and (3) multimodal plan-grounded dialogue on the synthetic TastyVidDial dataset. The architecture combines an LLM backbone with a frozen CLIP ViT-L/14 visual encoder and task-specific projection layers. The model uses a [RET] token for cross-modal retrieval and maps visual [CLS] tokens to the LLM embedding space for VSG. Training progresses through stages to prevent catastrophic forgetting while building hierarchical multimodal representations.

## Key Results
- MM-PlanLLM achieves 5.50 R@1, 38.53 R@5, 53.82 R@10, and 21.52 mAP on CVMR
- VSG performance reaches 38.16 Exact Match and 42.62 ROUGE-L
- PGAG performance demonstrates strong text-only capabilities at 38.16 Exact Match
- The model outperforms baselines (FROMAGe, random) across all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The multi-stage training scheme progressively aligns multimodal inputs with plan steps by starting with general image-text understanding and moving toward domain-specific plan-grounded dialogue.
- **Mechanism:** Each training stage introduces new semantic layers, from general image captioning to task-specific video annotations, culminating in multimodal dialogue with interleaved CVMR and VSG tasks.
- **Core assumption:** Gradual exposure prevents catastrophic forgetting and allows the model to learn hierarchical multimodal representations.
- **Evidence anchors:**
  - [abstract] "multitask-multistage approach, designed to gradually expose the model to multimodal instructional-plans semantic layers"
  - [section 3.4] "three-stage training approach tailored to our setting"
  - [corpus] No direct neighbor support; weak corpus evidence.
- **Break condition:** If stages are skipped or interleaved non-sequentially, performance on one task degrades without improvement on others.

### Mechanism 2
- **Claim:** The use of a [RET] token embedding and cross-modal retrieval space enables effective text-to-video moment retrieval.
- **Mechanism:** The [RET] token output is mapped to a retrieval embedding space via learned projection layers, allowing the model to retrieve the middle frame of the relevant video clip conditioned on the current plan step.
- **Core assumption:** Textual descriptions of actions map sufficiently to visual representations in the retrieval space for effective alignment.
- **Evidence anchors:**
  - [section 3.3] "a [RET] token is added to the language model's vocabulary... Its decoder-output embedding is then mapped onto a cross-modal retrieval embedding space"
  - [section 5.2] "MM-PlanLLM effectively learns to produce a representation that aligns closely with the video moment relevant to the target step"
  - [corpus] Weak support; no neighbor explicitly discusses retrieval token embeddings.
- **Break condition:** If the action descriptions are too abstract or the video moments too visually dissimilar, the retrieval space may not capture meaningful alignment.

### Mechanism 3
- **Claim:** VSG performance is enabled by mapping visual encoder [CLS] tokens to the LLM embedding space, allowing image-based step generation.
- **Mechanism:** A learned linear mapping transforms the visual [CLS] token into the LLM's embedding space, replacing the [IMG] token embedding during generation.
- **Core assumption:** Visual features from the middle frame of a step sufficiently represent the user's progress to condition appropriate next-step generation.
- **Evidence anchors:**
  - [section 3.3] "a single linear mapping Wc ∈ Rdve×d... that maps the [CLS], obtained from the visual encoder, token to the embedding space of the language model"
  - [section 5.1] "MM-PlanLLM achieves an Exact Match score of 38%"
  - [corpus] No direct neighbor support for visual-to-text mapping via [CLS] tokens.
- **Break condition:** If the middle frame does not capture the key visual state of the step, the mapping may not condition generation appropriately.

## Foundational Learning

- **Concept: Multitask-multistage training**
  - Why needed here: Gradual introduction of multimodal capabilities prevents catastrophic forgetting and allows hierarchical learning of cross-modal alignments.
  - Quick check question: What would happen if all tasks were trained jointly from the start without staged progression?

- **Concept: Cross-modal retrieval embeddings**
  - Why needed here: Enables the model to find video moments relevant to textual plan steps by aligning them in a shared embedding space.
  - Quick check question: How does the model know which frames are relevant if the textual step doesn't describe visual content directly?

- **Concept: Visual-to-text conditioning**
  - Why needed here: Allows the model to generate the next plan step based on the user's visual progress, grounding the plan in the current state.
  - Quick check question: What visual information is most critical for determining the next step in a recipe?

## Architecture Onboarding

- **Component map:** LLM backbone (decoder-only Transformer) -> frozen CLIP ViT-L/14 visual encoder -> task-specific projection layers (Wc for VSG, Wt and Wi for CVMR) -> [RET] token for retrieval tasks
- **Critical path:** User request → LLM context processing → task-specific layer activation → multimodal alignment → response generation
- **Design tradeoffs:**
  - Frozen visual encoder vs. fine-tuning: Faster training, but may limit adaptation to domain-specific visual features
  - Single linear projection vs. deeper modules: Simpler, more efficient, but may not capture complex visual-language alignments
- **Failure signatures:**
  - CVMR fails with high similarity between consecutive frames: Retrieval space may not discriminate subtle visual differences
  - VSG fails with abstract steps: Visual encoder may not capture the relevant state if the step is conceptual rather than visually distinct
- **First 3 experiments:**
  1. Train only Stage 1 (general image captioning) and evaluate CVMR to see if general visual understanding is sufficient
  2. Add Stage 2 (domain-specific video annotations) and re-evaluate to measure impact of task specialization
  3. Train with and without positional embeddings in Stage 3 to quantify the value of temporal information in CVMR

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the performance of MM-PlanLLM scale with increased context window sizes beyond 4 turns?
- **Open Question 2:** What is the impact of using different visual encoders on MM-PlanLLM's performance, particularly in distinguishing subtle visual changes between frames?
- **Open Question 3:** How does the inclusion of visual question answering (VQA) capabilities affect MM-PlanLLM's performance on multimodal tasks?

## Limitations
- The evaluation relies on a synthetic dataset (TastyVidDial) constructed using templates and generative models rather than real user interactions
- The frozen CLIP visual encoder may limit the model's ability to adapt to domain-specific visual features
- The three-stage training approach requires substantial computational resources and careful hyperparameter tuning

## Confidence
- **High confidence**: The core architectural innovations (task-specific projection layers, cross-modal retrieval embeddings, and visual-to-text conditioning) are well-specified and reproducible
- **Medium confidence**: The claimed performance improvements over baselines are supported by quantitative metrics, but the synthetic nature of the evaluation dataset limits confidence in real-world applicability
- **Low confidence**: The long-term generalization of the model to diverse procedural domains beyond cooking recipes remains untested

## Next Checks
1. Deploy MM-PlanLLM with actual users attempting procedural tasks and compare performance against the synthetic evaluation to quantify the domain gap
2. Evaluate the model's ability to generalize to non-cooking procedural tasks (e.g., assembly instructions, software tutorials) without domain-specific fine-tuning
3. Replace the frozen CLIP encoder with a fine-tuned visual backbone trained on the target procedural domain to assess the impact of visual feature adaptation on task performance