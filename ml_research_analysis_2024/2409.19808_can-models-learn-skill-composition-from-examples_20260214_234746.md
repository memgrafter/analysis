---
ver: rpa2
title: Can Models Learn Skill Composition from Examples?
arxiv_id: '2409.19808'
source_url: https://arxiv.org/abs/2409.19808
tags:
- skills
- skill
- training
- dskill
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether smaller language models can learn
  compositional generalization of language skills from examples. The authors fine-tune
  LLaMA-2-13B-Chat and Mistral-7B-Instruct on synthetic data generated by GPT-4, where
  each text exhibits a random combination of 1-3 training skills.
---

# Can Models Learn Skill Composition from Examples?

## Quick Facts
- **arXiv ID**: 2409.19808
- **Source URL**: https://arxiv.org/abs/2409.19808
- **Reference count**: 40
- **Primary result**: Smaller language models can learn compositional generalization of language skills from examples, achieving 37% success rate for composing 3 held-out skills (vs 4% baseline)

## Executive Summary
This paper demonstrates that smaller language models can learn compositional generalization of language skills from examples, challenging the assumption that such capabilities require large model sizes. The authors fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct on synthetic data containing random combinations of 1-3 training skills, then test their ability to compose held-out skills they never saw during training. The key finding is that models trained on skill combinations show significantly improved ability to compose both training and held-out skills, with LLaMA-2-13B-Chat achieving 37% success rate for composing 3 held-out skills compared to a 4% baseline.

## Method Summary
The authors generate synthetic training data using GPT-4, creating text pieces that exhibit random combinations of 1-3 training skills from a set of 53 rhetorical/literary skills and 48 held-out reasoning/logic skills. They fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct on this data for 4000 steps with batch size 64, Adam optimizer, linear warmup for 64 steps, then constant learning rate of 2e-5. Maximum token length is 1024, with pre-training data mixed in during fine-tuning. Models are evaluated using SKILL-MIX, a scoring system that measures success rate of composing k held-out skills in short paragraphs, with GPT-4 and Claude-3 serving as graders.

## Key Results
- Fine-tuning on skill combinations (k=2,3) improves models' ability to compose both training and held-out skills, with LLaMA-2-13B-Chat achieving 37% success rate for composing 3 held-out skills (vs 4% baseline)
- Models trained on 1-3 skill combinations can compose 4-5 skills without explicit training on such combinations
- Skill-richer training data is more data-efficient for inducing compositional ability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Models acquire meta-skills for composition through exposure to examples of composing multiple skills
- **Mechanism**: When fine-tuned on data containing k=2,3 skill combinations, the model learns higher-order representations that capture how to combine skills, enabling generalization to unseen k=4,5 combinations
- **Core assumption**: The model's architecture can learn abstract compositional rules from concrete examples without explicit training on all possible combinations
- **Evidence anchors**:
  - [abstract] "Training on combinations of k = 2 and 3 skills results in noticeable improvements in the ability to compose texts with k = 4 and 5 skills, despite models never having seen such examples during training"
  - [section 4.2] "the model is only trained on SKILL -MIX k = 2, 3 data, but it improves the ability to compose k = 4, 5 skills"
- **Break condition**: If the model's capacity is too small to represent abstract compositional rules, or if the skill combinations are too dissimilar to share compositional structure

### Mechanism 2
- **Claim**: Fine-tuning on skill combinations improves ability to compose held-out skills not seen during training
- **Mechanism**: The model learns compositional principles that transfer across skill categories, enabling it to combine previously unseen skills by applying learned composition strategies
- **Core assumption**: Skills share underlying compositional patterns that can be learned and transferred
- **Evidence anchors**:
  - [abstract] "When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning"
  - [section 4.2] "the ability to combine language skills from held-out categories improved at the same rate as the skills used in the training examples"
- **Break condition**: If skills in different categories have fundamentally different compositional structures that don't transfer

### Mechanism 3
- **Claim**: Skill-richer training data is more data-efficient for inducing compositional ability
- **Mechanism**: Training on combinations of more skills (k=3) provides more compositional signal per example, allowing the model to learn composition faster than training on single skills (k=1)
- **Core assumption**: Each example with multiple skills provides more information about how to compose than an example with a single skill
- **Evidence anchors**:
  - [abstract] "incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models"
  - [section 4.3] "fine-tuning on texts that compose more skills (i.e., with a larger k) is more data-efficient for learning skill compositions"
  - [section 4.3] Control experiment showing model fine-tuned on DSKILL -MIX (1,2,3) performs better than on DSKILL -MIX (1,2) despite smaller dataset
- **Break condition**: If the increased complexity of multi-skill examples overwhelms the model's learning capacity

## Foundational Learning

- **Concept**: Compositional generalization
  - **Why needed here**: The paper's core contribution is showing models can learn to compose skills they haven't seen explicitly combined
  - **Quick check question**: Can you explain the difference between systematic composition and mere memorization in the context of language models?

- **Concept**: Skill decomposition and combination
  - **Why needed here**: The evaluation requires understanding how individual skills can be combined and what constitutes successful composition
  - **Quick check question**: What distinguishes a text that merely lists skills from one that genuinely composes them?

- **Concept**: Evaluation metrics for composition
  - **Why needed here**: Understanding Ratio of Full Marks vs Skills Fraction is crucial for interpreting results
  - **Quick check question**: How would a text that demonstrates all skills but violates length requirements score under each metric?

## Architecture Onboarding

- **Component map**: Data generation (GPT-4) -> Fine-tuning pipeline (LLaMA-2-13B-Chat/Mistral-7B-Instruct) -> Evaluation (SKILL-MIX scoring with GPT-4/Claude-3)
- **Critical path**: Data generation → Fine-tuning (4000 steps, batch size 64) → Evaluation (SKILL-MIX scoring)
- **Design tradeoffs**: Using synthetic data allows controlled experiments but may not capture natural skill combinations; smaller models enable experimentation but may have limited composition capacity
- **Failure signatures**: Poor performance on held-out skills indicates lack of compositional generalization; good performance only on training skills suggests overfitting
- **First 3 experiments**:
  1. Fine-tune LLaMA-2-13B-Chat on DSKILL -MIX (1) and evaluate on SKILL -MIX held-out(k) to establish baseline for individual skill knowledge
  2. Fine-tune on DSKILL -MIX (1,2) and compare SKILL -MIX train(2) vs held-out(2) performance to isolate composition ability
  3. Fine-tune on DSKILL -MIX (1,2,3) and test SKILL -MIX train(4) and held-out(4) to verify out-of-distribution generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the maximum number of skills that smaller language models can successfully compose after fine-tuning, and how does this scale with model size and training data quality?
- **Basis in paper**: [explicit] The paper shows LLaMA-2-13B-Chat improves from 4% to 37% success rate for composing 3 held-out skills, and can compose 4-5 skills without explicit training on such combinations
- **Why unresolved**: The paper only tests up to k=5 skills, and doesn't systematically explore the upper limits of compositional ability across different model sizes
- **What evidence would resolve it**: Testing models on composing 6+ skills with varying training data compositions and model sizes to establish scaling laws for compositional generalization

### Open Question 2
- **Question**: Does compositional generalization transfer to entirely new skill categories beyond the rhetorical/literary and reasoning/logic categories tested in this paper?
- **Basis in paper**: [inferred] The paper only tests held-out skills from reasoning, logic, theory of mind, pragmatics, common sense, and physical knowledge categories, but doesn't test skills from completely different domains like mathematical reasoning or code generation
- **Why unresolved**: The skill categories used are still relatively close to language processing, leaving uncertainty about generalization to fundamentally different cognitive domains
- **What evidence would resolve it**: Fine-tuning models on skill combinations from one domain (e.g., language skills) and testing their ability to compose skills from completely different domains (e.g., mathematical problem-solving skills)

### Open Question 3
- **Question**: How does the compositional generalization ability learned from synthetic data transfer to real-world text generation tasks that require skill composition?
- **Basis in paper**: [inferred] The paper uses synthetic data generated by GPT-4 for training, but doesn't test whether these improvements transfer to naturalistic writing tasks like creative writing, persuasive essays, or technical documentation
- **Why unresolved**: The SKILL-MIX evaluation is a controlled benchmark that may not capture the complexity and variability of real-world composition tasks
- **What evidence would resolve it**: Testing fine-tuned models on standardized creative writing prompts, academic writing tasks, or professional communication scenarios that require combining multiple skills simultaneously

### Open Question 4
- **Question**: What is the minimum amount and quality of skill-composition training data needed to achieve strong compositional generalization, and how does this vary with model size?
- **Basis in paper**: [explicit] The paper shows that skill-richer training data is more data-efficient, but doesn't systematically explore the data requirements across different model sizes or the relationship between data diversity and compositional ability
- **Why unresolved**: The study uses a fixed dataset size and doesn't investigate the trade-off between dataset size, diversity, and model performance across different model scales
- **What evidence would resolve it**: Conducting controlled experiments varying the number of training examples, diversity of skill combinations, and model sizes to establish data efficiency curves for compositional generalization

## Limitations

- The evaluation relies heavily on GPT-4 and Claude-3 as graders, which may introduce grader bias in what constitutes "successful composition"
- The synthetic data generation process using GPT-4 may embed implicit compositional patterns that don't generalize to natural text
- The study focuses on short paragraphs, limiting conclusions about compositional generalization in longer, more complex texts

## Confidence

- **High confidence**: The finding that fine-tuning on skill combinations improves composition of training skills is well-supported by direct comparisons across multiple k values and both model architectures
- **Medium confidence**: The claim about improved composition of held-out skills is supported but requires more extensive testing across different skill partitions to rule out skill similarity effects
- **Medium confidence**: The data-efficiency finding is based on controlled comparisons but needs validation with larger datasets and different model sizes to confirm the relationship holds broadly

## Next Checks

1. **Cross-partition validation**: Repeat the experiment with different random partitions of skills into training and held-out groups to verify the compositional generalization effect isn't specific to the particular skill split used.

2. **Human evaluation**: Conduct blinded human evaluation of model outputs on held-out skills to validate the automated SKILL-MIX scoring and check for grader bias in the automated evaluation.

3. **Scaling experiment**: Test the same methodology with larger models (e.g., LLaMA-2-70B) to determine whether the compositional benefits observed here are complementary to model scale or if they diminish as model size increases.