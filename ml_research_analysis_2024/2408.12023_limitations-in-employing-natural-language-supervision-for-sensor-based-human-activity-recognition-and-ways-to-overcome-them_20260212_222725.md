---
ver: rpa2
title: Limitations in Employing Natural Language Supervision for Sensor-Based Human
  Activity Recognition -- And Ways to Overcome Them
arxiv_id: '2408.12023'
source_url: https://arxiv.org/abs/2408.12023
tags:
- data
- activity
- activities
- text
- sensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines natural language supervision (NLS) for wearable
  sensor-based Human Activity Recognition (HAR). The authors found that despite its
  success in other domains, NLS performs significantly worse than standard supervised
  and self-supervised methods for HAR, primarily due to sensor heterogeneity and a
  lack of rich text descriptions of activities.
---

# Limitations in Employing Natural Language Supervision for Sensor-Based Human Activity Recognition -- And Ways to Overcome Them

## Quick Facts
- arXiv ID: 2408.12023
- Source URL: https://arxiv.org/abs/2408.12023
- Authors: Harish Haresamudram; Apoorva Beedu; Mashfiqui Rabbi; Sankalita Saha; Irfan Essa; Thomas Ploetz
- Reference count: 17
- Key outcome: Natural language supervision (NLS) performs significantly worse than standard supervised and self-supervised methods for sensor-based HAR due to sensor heterogeneity and lack of rich text descriptions, but can be substantially improved through adaptation strategies and enhanced text diversity.

## Executive Summary
This paper investigates the application of natural language supervision (NLS) for wearable sensor-based Human Activity Recognition (HAR), revealing that despite its success in other domains, NLS performs substantially worse than standard supervised and self-supervised methods for HAR. The authors identify two primary causes: sensor heterogeneity across different wearable devices and the lack of rich, diverse text descriptions of activities. Through systematic experimentation, they demonstrate that these limitations can be effectively addressed by adapting projection layers with minimal target data and enhancing text diversity through additional templates and external knowledge, bringing NLS performance closer to supervised methods while enabling zero-shot recognition of unseen activities.

## Method Summary
The authors employ cross-modal contrastive pre-training between IMU sensor data and natural language activity descriptions using the Capture-24 dataset. They evaluate performance across six target datasets (HHAR, Myogym, Mobiact, Motionsense, PAMAP2, MHEALTH) using macro F1-score. To address the identified limitations, they propose two key strategies: (1) adapting only the text and sensor projection layers with small amounts of target data (as little as 100 labeled windows), and (2) increasing text diversity through additional activity description templates and incorporation of external knowledge about body parts and movements using ChatGPT. The evaluation includes comparisons with supervised methods (Conv. classifier, DeepConvLSTM) and self-supervised approaches (Autoencoder, SimCLR, Enhanced CPC).

## Key Results
- Standard NLS performs significantly worse than supervised and self-supervised methods for HAR, primarily due to sensor heterogeneity and lack of rich text descriptions
- Adapting projection layers with minimal target data (100 windows per class) improves performance by 20-40% across datasets
- Increasing text diversity through additional templates and external knowledge substantially boosts recognition performance
- Adapted NLS models achieve performance close to supervised methods while enabling zero-shot recognition of unseen activities and cross-modal video retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal contrastive pre-training fails for HAR primarily due to sensor heterogeneity and lack of rich text descriptions.
- Mechanism: Sensor heterogeneity creates distribution shifts between pre-training and target datasets that the model cannot adapt to without further training. The lack of rich, diverse text descriptions limits the model's ability to learn general concepts needed for zero-shot recognition.
- Core assumption: The effectiveness of cross-modal contrastive learning relies on consistent data distributions between training and test phases, and sufficient textual diversity to learn generalizable concepts.
- Evidence anchors:
  - [abstract] "we identify that-surprisingly-it performs substantially worse than standard end-to-end training and self-supervision. We identify the primary causes for this as: sensor heterogeneity and the lack of rich, diverse text descriptions of activities."
  - [section 5.2] "Differences in data distributions between pre-training and zero shot prediction have a substantial impact on performance... Such differences in distributions are unique to wearables, where there is high diversity in sensors deployed, along with associated hardware constraints and settings"
  - [corpus] Weak evidence - no direct citation available
- Break condition: When sensor distributions between pre-training and target datasets are similar, or when rich, diverse text descriptions are available.

### Mechanism 2
- Claim: Adaptation of projection layers with small amounts of target data significantly improves HAR performance.
- Mechanism: By updating only the text and sensor projection heads with target data, the model can align modalities in the target domain without requiring full retraining, which is computationally efficient and requires minimal labeled data.
- Core assumption: The encoder layers learn general features that can be transferred across domains, while projection layers need domain-specific alignment.
- Evidence anchors:
  - [section 6.1] "We propose to update only the text and sensor projection heads with the target labeled data... adaptation with minimal amounts of target data can be sufficient for improved HAR"
  - [section 6.1] "Adapting projecting layers is highly advantageous, with performance increases of 20-40% (across datasets) with just 100 labeled windows, i.e., less than 4 minutes per activity"
  - [corpus] Weak evidence - no direct citation available
- Break condition: When the amount of target data is too small to learn meaningful alignment, or when the encoder layers are not transferable.

### Mechanism 3
- Claim: Increasing text diversity through additional templates and external knowledge improves recognition performance.
- Mechanism: By providing more diverse ways to describe activities and incorporating external knowledge about body parts and movements, the model can learn richer representations that generalize better to unseen activities.
- Core assumption: Text diversity and external knowledge provide additional context that helps the model distinguish between similar activities and recognize new activities based on their descriptions.
- Evidence anchors:
  - [section 6.2] "We propose two measures to increase text diversity... utilizing ChatGPT to diversify activity sentences results in substantial boosts in performance across target datasets"
  - [section 6.2] "Information about body parts and movements used for activities generally results in increased performance"
  - [corpus] Weak evidence - no direct citation available
- Break condition: When the additional text diversity or external knowledge does not provide meaningful new information about the activities.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: This is the fundamental training approach being evaluated and adapted for HAR
  - Quick check question: How does contrastive learning differ from traditional classification, and why is it particularly suited for zero-shot learning scenarios?

- Concept: Sensor heterogeneity
  - Why needed here: This is the primary challenge unique to HAR that explains why standard cross-modal approaches fail
  - Quick check question: What are the main sources of sensor heterogeneity in wearable devices, and how do they affect data distributions?

- Concept: Few-shot learning adaptation
  - Why needed here: This is the key strategy for overcoming the distribution shift problem without requiring large amounts of target data
  - Quick check question: Why is adapting only projection layers more efficient than fine-tuning the entire model, and what are the trade-offs?

## Architecture Onboarding

- Component map:
  - IMU Encoder: Extracts features from raw sensor data
  - Text Encoder: Encodes activity descriptions into embeddings
  - Projection Layers: Project both modalities into a common space
  - Contrastive Loss: Trains the model to align corresponding pairs
  - Adaptation Module: Updates projection layers with target data

- Critical path: IMU Encoder → Projection Layer → Contrastive Loss → Text Encoder → Projection Layer → Similarity Computation

- Design tradeoffs:
  - Encoder complexity vs. computational efficiency
  - Number of text templates vs. model complexity
  - Amount of target data for adaptation vs. performance gains
  - External knowledge integration vs. model generalization

- Failure signatures:
  - Poor performance on target datasets indicates distribution shift
  - Inability to recognize unseen activities suggests insufficient text diversity
  - Overfitting to target data when too much adaptation is performed

- First 3 experiments:
  1. Evaluate baseline performance of standard NLS vs supervised methods on target datasets
  2. Test adaptation of projection layers with varying amounts of target data
  3. Measure impact of text diversity through additional templates and external knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can natural language supervision (NLS) ever match or surpass the performance of supervised and self-supervised methods for sensor-based HAR in the long term, especially with improved pre-training strategies and richer text descriptions?
- Basis in paper: [explicit] The authors demonstrate that NLS performs significantly worse than standard methods, but also show improvements through adaptation and enhanced text diversity.
- Why unresolved: The paper shows that NLS can approach the performance of baselines with adaptation and improved text descriptions, but it does not definitively prove that NLS can surpass supervised methods in the long run.
- What evidence would resolve it: Long-term studies comparing NLS to supervised and self-supervised methods across a wider range of datasets and activity types, with further advancements in pre-training strategies and text description techniques.

### Open Question 2
- Question: How can the sensor heterogeneity challenge in wearable sensor data be addressed more effectively, potentially leading to better cross-dataset generalization of NLS models?
- Basis in paper: [explicit] The authors identify sensor heterogeneity as a major challenge for NLS, causing performance degradation due to differences in data distributions across sensors and recording conditions.
- Why unresolved: While the paper proposes adaptation on target data as a solution, it does not explore other potential strategies for mitigating sensor heterogeneity, such as sensor calibration or domain adaptation techniques.
- What evidence would resolve it: Comparative studies evaluating different approaches to handling sensor heterogeneity, including sensor calibration, domain adaptation, and other techniques, to determine their effectiveness in improving cross-dataset generalization of NLS models.

### Open Question 3
- Question: What are the potential applications and limitations of NLS for HAR beyond activity recognition, such as activity segmentation, anomaly detection, or personalized activity modeling?
- Basis in paper: [explicit] The authors briefly mention the potential of NLS for recognizing unseen activities and cross-modal video retrieval, but do not explore other applications in detail.
- Why unresolved: The paper focuses primarily on activity recognition using NLS, leaving open questions about its applicability and limitations in other HAR tasks and domains.
- What evidence would resolve it: Empirical studies investigating the effectiveness of NLS for various HAR tasks beyond activity recognition, such as activity segmentation, anomaly detection, and personalized activity modeling, across different sensor modalities and application scenarios.

## Limitations

- The evaluation relies heavily on benchmark datasets which may not capture real-world deployment complexity and sensor diversity
- The effectiveness of external knowledge integration through ChatGPT is not rigorously validated for consistency and potential biases
- The paper focuses on activity recognition without exploring NLS applications in other HAR tasks like segmentation or anomaly detection

## Confidence

**High Confidence (Likelihood > 70%):**
- NLS performs worse than supervised/self-supervised methods for HAR due to sensor heterogeneity and limited text diversity
- Adapting projection layers with small target data improves performance significantly
- Text diversity enhancement through templates and external knowledge boosts recognition accuracy

**Medium Confidence (Likelihood 40-70%):**
- The proposed adaptation strategies will generalize to real-world deployment scenarios
- Cross-modal video retrieval and unseen activity recognition are practically feasible with NLS
- Sensor heterogeneity is the dominant factor limiting NLS effectiveness in HAR

**Low Confidence (Likelihood < 40%):**
- The specific external knowledge sources and integration methods will remain effective as language models evolve
- The computational efficiency gains from partial adaptation will scale to large-scale deployments
- The observed improvements will persist when moving from benchmark datasets to long-term real-world data collection

## Next Checks

1. **Real-world Deployment Validation**: Deploy the adapted NLS model on a cohort of participants wearing different commercial wearable devices over extended periods (minimum 2 weeks) to assess performance stability across varying sensor configurations and real-world activity patterns.

2. **Ablation Study on External Knowledge**: Systematically remove different components of the external knowledge integration (body parts, movement types, intensity levels) to quantify their individual contributions and identify which aspects provide the most robust improvements across diverse activity types.

3. **Temporal Generalization Test**: Evaluate model performance on datasets collected at different time periods (e.g., pre-pandemic vs. post-pandemic activity patterns) to assess the model's ability to handle temporal shifts in activity distributions and user behavior changes.