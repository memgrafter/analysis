---
ver: rpa2
title: Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual
  Learning
arxiv_id: '2403.18886'
source_url: https://arxiv.org/abs/2403.18886
tags:
- task
- learning
- expansion
- adapter
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SEMA, a continual learning method that enables
  pre-trained models to self-expand with modular adapters. SEMA automatically decides
  when and where to add new adapters based on representation-level distribution shift
  detection, allowing sub-linear model growth while reusing existing knowledge.
---

# Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning

## Quick Facts
- arXiv ID: 2403.18886
- Source URL: https://arxiv.org/abs/2403.18886
- Authors: Huiyi Wang; Haodong Lu; Lina Yao; Dong Gong
- Reference count: 40
- Achieves state-of-the-art performance on continual learning benchmarks using pre-trained models with adaptive adapter expansion

## Executive Summary
This paper introduces SEMA (Self-Expansion with Mixture of Adapters), a continual learning method that enables pre-trained models to dynamically expand their capacity through modular adapters. The key innovation lies in automatically detecting representation-level distribution shifts and deciding when and where to add new adapters, allowing sub-linear model growth while reusing existing knowledge. SEMA eliminates the need for memory rehearsal by introducing functional adapters and representation descriptors, combined with an expandable weighting router for mixing adapter outputs.

## Method Summary
SEMA operates by monitoring distribution shifts in feature representations to determine when new tasks require additional model capacity. The method employs modular adapters consisting of functional adapters (task-specific transformations) and representation descriptors (feature embeddings that capture task characteristics). An expandable weighting router dynamically mixes adapter outputs based on their relevance to incoming data. When significant distribution shift is detected, SEMA automatically adds new adapters at appropriate layers of the pre-trained model, enabling knowledge transfer while preventing catastrophic forgetting. The system grows adaptively, maintaining efficiency through sub-linear expansion rather than fixed capacity allocation.

## Key Results
- Demonstrates state-of-the-art performance compared to other pre-trained model-based continual learning methods
- Achieves higher accuracy on CIFAR-100, ImageNet-R, ImageNet-A, and VTAB without memory rehearsal
- Shows sub-linear model growth while maintaining strong performance across sequential tasks

## Why This Works (Mechanism)
SEMA's effectiveness stems from its intelligent detection of when additional capacity is truly needed rather than allocating resources upfront. By monitoring representation-level distribution shifts, the method ensures adapters are added only when incoming tasks significantly differ from previously seen ones. The modular design allows selective activation of relevant adapters through the weighting router, preventing interference between tasks. The combination of functional adapters for task-specific transformations and representation descriptors for capturing task characteristics enables efficient knowledge reuse while maintaining task discriminability.

## Foundational Learning
- **Continual Learning**: Learning from sequential tasks without forgetting previous knowledge - needed to understand the fundamental challenge SEMA addresses
- **Catastrophic Forgetting**: The phenomenon where neural networks lose previously learned information when trained on new tasks - critical for appreciating SEMA's anti-forgetting mechanisms
- **Distribution Shift Detection**: Methods for identifying when input data distribution changes significantly - essential for understanding SEMA's adaptive expansion trigger
- **Adapter-based Fine-tuning**: Using small trainable modules to adapt large pre-trained models - foundational to SEMA's architecture
- **Mixture-of-Experts**: Routing mechanisms that combine multiple specialized components - relevant to SEMA's weighting router
- **Representation Learning**: Learning meaningful feature embeddings - important for understanding representation descriptors

## Architecture Onboarding

Component Map: Input Data -> Distribution Shift Detector -> Expandable Weighting Router -> Modular Adapters -> Pre-trained Model -> Output

Critical Path: Distribution shift detection triggers adapter expansion → New adapters are placed at optimal layers → Weighting router learns to mix adapter outputs based on representation descriptors → Model adapts to new task while preserving old knowledge

Design Tradeoffs: SEMA trades some inference latency for adaptive capacity expansion and eliminates memory storage requirements. The modular design allows selective activation but requires careful routing decisions. The distribution shift detection mechanism adds computational overhead but enables more efficient resource allocation compared to fixed expansion strategies.

Failure Signatures: Performance degradation when distribution shift detection fails to trigger (under-expansion) or triggers too frequently (over-expansion). Routing errors can cause interference between tasks if the weighting router assigns incorrect weights to adapters.

First Experiments:
1. Verify distribution shift detection sensitivity across different magnitudes of domain shift
2. Test adapter placement effectiveness at different model layers for varying task complexities
3. Evaluate the weighting router's ability to correctly route inputs to appropriate adapters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited validation on datasets with severe domain shifts beyond standard image classification benchmarks
- Insufficient characterization of computational overhead during inference, particularly for the expandable weighting router
- Lack of thorough stress-testing on very long task sequences to evaluate long-term scalability

## Confidence
High confidence in SEMA's performance on tested benchmarks and the core modular adapter framework. Medium confidence in the generalizability to real-world scenarios and the distribution shift detection mechanism's reliability across diverse data distributions. Claims about practical efficiency require additional validation through concrete timing and memory measurements.

## Next Checks
1. Test SEMA on datasets with severe domain shifts (e.g., Office-31 or DomainNet) to assess robustness beyond standard image classification benchmarks
2. Conduct ablation studies on the distribution shift detection mechanism to quantify its reliability and false positive/negative rates across different data distributions
3. Measure the wall-clock inference time and memory footprint of SEMA compared to baseline methods to provide concrete evidence of its practical efficiency claims, particularly for large-scale deployments