---
ver: rpa2
title: Reinforcement Learning-Guided Semi-Supervised Learning
arxiv_id: '2405.01760'
source_url: https://arxiv.org/abs/2405.01760
tags:
- learning
- data
- labeled
- rlgssl
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RLGSSL, a novel reinforcement learning-guided
  approach to semi-supervised learning. It formulates SSL as a one-armed bandit problem
  where the prediction model serves as the policy function and pseudo-labeling as
  actions.
---

# Reinforcement Learning-Guided Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2405.01760
- Source URL: https://arxiv.org/abs/2405.01760
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on CIFAR-10 (9.15% test error with 1000 labels), CIFAR-100, and SVHN benchmarks

## Executive Summary
This paper introduces RLGSSL, a novel reinforcement learning-guided approach to semi-supervised learning that reformulates SSL as a one-armed bandit problem. The method uses a prediction model as the policy function with pseudo-labeling as actions, guided by a carefully designed reward function that balances labeled and unlabeled data usage. Enhanced by a teacher-student framework for stability, RLGSSL demonstrates significant improvements over existing methods on standard benchmarks, reducing test error on CIFAR-10 from 12.44% to 9.15% with only 1000 labeled samples.

## Method Summary
RLGSSL combines three loss components: a supervised loss on labeled data, a consistency loss for regularization, and an RL loss guided by a reward function. The RL loss is computed using mixup interpolation between labeled and pseudo-labeled data, with rewards based on negative mean squared error between predictions and mixup labels. A teacher-student framework with exponential moving average updates provides stable pseudo-label generation. The method is pre-trained with Mean Teacher for 50 epochs, then trained with RLGSSL for 400 epochs using SGD with Nesterov momentum or L2 regularization, with cosine learning rate annealing.

## Key Results
- Achieves 9.15% test error on CIFAR-10 with 1000 labeled samples (vs 12.44% baseline)
- Outperforms existing SSL methods on CIFAR-100 and SVHN benchmarks
- Ablation studies confirm importance of each component (RL loss, teacher-student framework, mixup reward)
- Sensitivity analysis validates robustness across different hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLGSSL leverages a reward function based on mixup interpolation to balance learning from labeled and unlabeled data.
- Mechanism: The reward is defined as the negative mean squared error between model predictions on interpolated mixup samples and their corresponding mixup labels.
- Core assumption: Mixup interpolation between labeled and pseudo-labeled data generates informative supervision signals that improve generalization.
- Evidence anchors:
  - [abstract] "A carefully designed reward function balances the use of labeled and unlabeled data, enhanced by a teacher-student framework for stability."
  - [section] "We define a simple reward function that balances the use of labeled and unlabeled data and improves generalization capacity by leveraging linear data interpolation"
- Break condition: If mixup interpolation creates noisy supervision or if the interpolation ratio is poorly chosen, the reward signal may mislead the RL agent.

### Mechanism 2
- Claim: Using a teacher-student framework with exponential moving average stabilizes the pseudo-label generation process.
- Mechanism: The teacher model generates pseudo-labels for unlabeled data using EMA-updated parameters, while the student model is updated through the RL loss.
- Core assumption: EMA-updated teacher parameters produce more stable pseudo-labels than directly using student parameters.
- Evidence anchors:
  - [abstract] "A semi-supervised teacher-student framework is further deployed to increase the learning stability."
  - [section] "We extend this mechanism to provide a teacher-student framework for RL-guided SSL by maintaining a dual set of model parameters"
- Break condition: If the EMA decay rate is too high, the teacher model becomes too slow to adapt; if too low, the stability benefit diminishes.

### Mechanism 3
- Claim: KL divergence weighting of the reward function encourages more informative pseudo-labels.
- Mechanism: The RL loss includes KL divergence between predicted probability vectors and a uniform distribution as a weighting factor.
- Core assumption: More discriminative pseudo-labels lead to better learning from unlabeled data.
- Evidence anchors:
  - [section] "Given that a uniform probability distribution signifies the least informative prediction outcome, the expected KL-divergence captures the level of informativeness in the policy output and hence serves as a meaningful weight for the reward"
- Break condition: If the KL divergence term dominates the reward signal, the model may prioritize confidence over accuracy.

## Foundational Learning

- Concept: Reinforcement Learning - One-armed bandit problem formulation
  - Why needed here: SSL is reformulated as a decision-making problem where the model learns to generate pseudo-labels that maximize a reward signal
  - Quick check question: How does the one-armed bandit formulation differ from standard RL in terms of state transitions and action space?

- Concept: Semi-supervised learning - Pseudo-labeling and consistency regularization
  - Why needed here: RLGSSL builds on existing SSL techniques but uses RL to guide pseudo-label generation rather than relying on fixed heuristics
  - Quick check question: What are the key differences between consistency-based SSL methods and the RL-guided approach in terms of loss function design?

- Concept: Data augmentation - Mixup interpolation
  - Why needed here: Mixup is used both in the reward function computation and potentially in data preprocessing to create more robust supervision signals
  - Quick check question: How does mixup interpolation between labeled and pseudo-labeled samples create supervision signals that pure labeled data cannot provide?

## Architecture Onboarding

- Component map: Student model (θS) -> RL loss -> Student update; Teacher model (θT) -> Pseudo-labels -> Mixup -> Reward -> RL loss; Labeled data -> Supervised loss -> Student update; Both losses -> Joint optimization
- Critical path: Labeled data → Supervised loss → Student update; Unlabeled data → Teacher pseudo-labels → Mixup → Reward → RL loss → Student update; Both losses → Joint optimization
- Design tradeoffs: RL-guided approach vs. consistency-based SSL - more adaptive but potentially less stable; Teacher-student framework vs. single model - more stable but computationally heavier; Mixup-based reward vs. direct pseudo-label supervision - more robust but requires additional computation
- Failure signatures: Degraded performance on labeled data suggests overemphasizing unlabeled data; High variance in training indicates instability in pseudo-label generation; Poor generalization suggests the reward function is not properly balancing the data sources
- First 3 experiments:
  1. Implement the teacher-student framework with standard supervised loss and consistency loss to verify the baseline SSL performance
  2. Add the mixup-based reward function and verify it produces reasonable values across different data points
  3. Integrate the full RLGSSL loss function and verify the training process converges with improved performance on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RLGSSL performance scale with different mixing parameters µ in the reward function across diverse datasets?
- Basis in paper: [explicit] The paper discusses the use of linear data interpolation (mixup) and mentions the mixing parameter µ is sampled from a Beta distribution, but does not extensively explore the sensitivity of performance to different µ values.
- Why unresolved: The ablation study in the paper does not specifically address the impact of varying the mixing parameter µ on the performance of RLGSSL.
- What evidence would resolve it: Conducting experiments with different values of µ (e.g., µ = 0.2, 0.5, 0.8) and analyzing the resulting performance metrics across multiple datasets would provide insights into the optimal range for µ.

### Open Question 2
- Question: Can RLGSSL be effectively extended to semi-supervised learning tasks beyond image classification, such as natural language processing or speech recognition?
- Basis in paper: [inferred] The paper focuses on image classification benchmarks (CIFAR-10, CIFAR-100, SVHN) and demonstrates superior performance. However, it does not explore applications in other domains like NLP or speech recognition.
- Why unresolved: The methodology and reward function are designed for image data, and adapting them to other data modalities may require significant modifications.
- What evidence would resolve it: Applying RLGSSL to semi-supervised NLP tasks (e.g., text classification) or speech recognition tasks and comparing its performance to state-of-the-art methods would validate its generalizability.

### Open Question 3
- Question: What is the impact of the teacher-student framework on the convergence speed and final performance of RLGSSL compared to a single-model approach?
- Basis in paper: [explicit] The paper introduces a teacher-student framework with exponential moving average (EMA) to enhance learning stability and augment the RL loss with supervised and consistency losses.
- Why unresolved: While the ablation study shows the importance of the EMA component, it does not directly compare the convergence speed or final performance of RLGSSL with and without the teacher-student framework.
- What evidence would resolve it: Conducting experiments to measure the training time and test performance of RLGSSL with and without the teacher-student framework would provide insights into its impact on efficiency and effectiveness.

### Open Question 4
- Question: How sensitive is RLGSSL to the choice of hyperparameters, and can an automated hyperparameter optimization method further improve its performance?
- Basis in paper: [explicit] The paper includes a sensitivity analysis for two hyperparameters (λ1 and λ2) and reports the optimal values used in experiments.
- Why unresolved: The analysis is limited to two hyperparameters, and the paper does not explore the sensitivity to other hyperparameters or the potential benefits of automated optimization methods.
- What evidence would resolve it: Performing a more comprehensive hyperparameter sensitivity analysis and applying automated hyperparameter optimization techniques (e.g., Bayesian optimization) could identify better hyperparameter settings and improve performance.

## Limitations
- Performance heavily dependent on specific architectural choices (CNN-13, WRN-28) that may not generalize
- Mixup-based reward function lacks theoretical grounding for why this specific interpolation approach optimally balances labeled and unlabeled data
- Teacher-student framework with EMA updates may not be optimal for RL-guided learning

## Confidence
- **High Confidence**: The empirical results showing state-of-the-art performance on CIFAR-10, CIFAR-100, and SVHN benchmarks are well-documented and reproducible
- **Medium Confidence**: The theoretical framework of reformulating SSL as a one-armed bandit problem is sound, though the specific reward function design could be better justified
- **Low Confidence**: The claim that the KL-divergence weighting specifically improves pseudo-label informativeness is based on intuitive reasoning rather than rigorous validation

## Next Checks
1. **Reward Function Sensitivity**: Systematically vary the mixup Beta distribution parameters (α) and the reward function scaling factors to determine if the current configuration is optimal or simply one of many viable options

2. **Alternative RL Algorithms**: Replace the one-armed bandit formulation with other RL approaches (e.g., actor-critic methods) to verify whether the proposed formulation is necessary or if simpler RL algorithms would achieve similar results

3. **Cross-Architecture Generalization**: Test RLGSSL on different model architectures (e.g., ResNet, Vision Transformers) and datasets beyond CIFAR and SVHN to assess the method's broader applicability and identify potential limitations