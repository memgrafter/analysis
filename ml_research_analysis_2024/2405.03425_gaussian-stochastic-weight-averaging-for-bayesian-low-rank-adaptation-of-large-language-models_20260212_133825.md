---
ver: rpa2
title: Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large
  Language Models
arxiv_id: '2405.03425'
source_url: https://arxiv.org/abs/2405.03425
tags:
- bayesian
- lora
- methods
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple yet effective method for Bayesian
  inference in large language models by combining LoRA (Low-Rank Adaptation) with
  SWAG (Stochastic Weight Averaging with Gaussians). The approach addresses overconfidence
  and poor calibration in fine-tuned LLMs, particularly when trained on small datasets.
---

# Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2405.03425
- Source URL: https://arxiv.org/abs/2405.03425
- Authors: Emre Onal; Klemens Flöge; Emma Caldwell; Arsen Sheverdin; Vincent Fortuin
- Reference count: 29
- Key outcome: SWAG applied to LoRA parameters provides efficient Bayesian inference for LLMs, achieving competitive calibration and OOD detection performance compared to more complex methods

## Executive Summary
This paper introduces a simple yet effective method for Bayesian inference in large language models by combining LoRA (Low-Rank Adaptation) with SWAG (Stochastic Weight Averaging with Gaussians). The approach addresses overconfidence and poor calibration in fine-tuned LLMs, particularly when trained on small datasets. By using SWAG to estimate a Gaussian posterior over the LoRA parameters, the method achieves competitive performance with more complex Bayesian inference techniques while being computationally efficient. Experiments on multiple NLP benchmarks show that MultiSWAG (an ensemble of SWAG models) consistently outperforms baseline methods in both accuracy and calibration, achieving results comparable to more sophisticated approaches like Laplace-LoRA. The method also demonstrates improved robustness to distribution shifts and out-of-distribution data.

## Method Summary
The method combines Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG) to enable approximate Bayesian inference in large language models. During fine-tuning, SWAG collects stochastic gradient descent (SGD) iterates across epochs, computes a running mean (SWA estimate), and estimates a low-rank covariance matrix from the deviation of the last K epochs. This creates a tractable Gaussian posterior over the LoRA parameters. For inference, the method samples from this posterior to generate predictions, which are then averaged. MultiSWAG extends this by running multiple SWA ensembles with different seeds or learning rates and averaging their predictions, improving calibration by exploring different basins of attraction in the loss landscape.

## Key Results
- MultiSWAG consistently outperforms baseline methods in both accuracy and calibration metrics (NLL, ECE, Brier score)
- The method achieves results comparable to more complex approaches like Laplace-LoRA while being computationally more efficient
- SWAG-based methods demonstrate improved robustness to distribution shifts and out-of-distribution data detection
- Gaussian sampling significantly improves calibration but may slightly reduce accuracy compared to point estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SWAG applied to LoRA parameters produces a tractable Gaussian posterior over the adaptation subspace, avoiding the computational cost of full-weight Bayesian inference.
- **Mechanism:** SWAG collects SGD iterates across epochs, computes a running mean (SWA estimate) and a low-rank covariance approximation from the deviation matrix of the last K epochs. This posterior is then used to sample LoRA weights for Bayesian model averaging.
- **Core assumption:** The SGD iterates in the LoRA subspace are approximately Gaussian-distributed around the MAP, and the last K epochs provide sufficient samples to estimate the posterior.
- **Evidence anchors:** [abstract] "propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs" and [section] "SWAG implementation uses these SGD iterates to calculate a Gaussian distribution around the SWA estimate"
- **Break condition:** If the LoRA subspace trajectory is multimodal or non-Gaussian, the posterior estimate becomes inaccurate.

### Mechanism 2
- **Claim:** MultiSWAG improves calibration by averaging predictions over multiple SWA models, implicitly exploring different basins of attraction in the loss landscape.
- **Mechanism:** Multiple SWA runs with different seeds or learning rates generate diverse solutions; the ensemble averages their predictions, reducing overconfidence and improving uncertainty estimates.
- **Core assumption:** Each SWA run converges to a different basin, and these basins collectively capture the uncertainty in the model's predictions.
- **Evidence anchors:** [abstract] "MultiSWAG (an ensemble of SWAG models) consistently outperforms baseline methods in both accuracy and calibration" and [section] "MultiSWAG is a natural extension of SWAG that leverages an ensemble of SWAG models, enabling an effective mechanism for Bayesian marginalization across multiple modes of the posterior"
- **Break condition:** If all SWA runs converge to the same basin, the ensemble offers no additional benefit.

### Mechanism 3
- **Claim:** Gaussian sampling in SWAG improves calibration by smoothing the posterior, reducing overconfidence without sacrificing much accuracy.
- **Mechanism:** Instead of using the SWA point estimate, SWAG samples from the Gaussian posterior, which introduces variance that reflects uncertainty in the LoRA parameters.
- **Core assumption:** The variance in the LoRA subspace captures meaningful uncertainty about the adaptation, not just noise.
- **Evidence anchors:** [abstract] "MultiSWAG... performs on par with more complicated and expensive methods like Laplace-LoRA" and [section] "Gaussian sampling significantly improves calibration but not accuracy"
- **Break condition:** If the LoRA parameters are underdetermined, the Gaussian posterior may be overconfident.

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA reduces the number of trainable parameters from billions to thousands, making Bayesian inference tractable.
  - Quick check question: How does LoRA achieve parameter efficiency, and what is the role of the low-rank matrices A and B?

- **Concept:** Stochastic Weight Averaging (SWA)
  - Why needed here: SWA finds flatter minima in the loss landscape, which improves generalization and robustness.
  - Quick check question: Why does averaging weights across SGD iterates lead to flatter and more generalizable solutions?

- **Concept:** Gaussian Posterior Estimation
  - Why needed here: Estimating a Gaussian posterior over the LoRA parameters allows for efficient sampling and uncertainty quantification.
  - Quick check question: How does SWAG estimate the covariance matrix from SGD iterates, and why is a low-rank approximation used?

## Architecture Onboarding

- **Component map:** Pre-trained LLM (frozen) -> LoRA adapters (trainable) -> SWAG/MultiSWAG (posterior estimation) -> Ensemble sampling (inference)

- **Critical path:**
  1. Fine-tune LoRA adapters with SWAG enabled
  2. Collect K SGD iterates for covariance estimation
  3. Compute SWA mean and low-rank covariance
  4. Sample from posterior for inference
  5. Average predictions across samples/ensemble

- **Design tradeoffs:**
  - Accuracy vs. calibration: Gaussian sampling improves calibration but may slightly reduce accuracy
  - Computational cost vs. robustness: MultiSWAG is more robust but requires multiple SWA runs
  - Memory vs. precision: Low-rank covariance approximation is memory-efficient but may miss some variance structure

- **Failure signatures:**
  - Poor calibration: NLL or Brier score much higher than expected
  - Instability in sampling: Posterior samples produce wildly different predictions
  - No improvement over baseline: SWAG/MultiSWAG performs similarly to MAP

- **First 3 experiments:**
  1. Run SWAG with K=1 (no covariance) to confirm SWA improves accuracy
  2. Increase K and observe changes in calibration metrics (NLL, ECE)
  3. Compare MultiSWAG (ensemble of 3) vs. SWAG (single run) on OOD detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LoRA rank (r) and alpha (α) parameters affect the performance of SWAG-LoRA in terms of accuracy and calibration?
- Basis in paper: [explicit] The paper uses LoRA rank r = 8 and α = 16 but does not explore the sensitivity of these hyperparameters to SWAG-LoRA performance.
- Why unresolved: The paper only tests a single configuration of LoRA hyperparameters, leaving the impact of different rank and alpha values unexplored.
- What evidence would resolve it: Systematic experiments varying r and α across a range of values while measuring accuracy, calibration (NLL, Brier score), and computational efficiency would reveal optimal configurations.

### Open Question 2
- Question: Does SWAG-LoRA maintain its performance advantages when applied to larger LLMs (e.g., LLaMA-2-13B or LLaMA-2-70B) compared to smaller models?
- Basis in paper: [inferred] The experiments are limited to LLaMA-2-7B, and the paper does not discuss scalability to larger models.
- Why unresolved: The paper's evaluation is constrained to a single model size, making it unclear whether the computational and performance benefits scale with model size.
- What evidence would resolve it: Applying SWAG-LoRA to multiple model sizes while comparing accuracy, calibration, and computational overhead would demonstrate scalability.

### Open Question 3
- Question: How does SWAG-LoRA compare to other Bayesian LoRA methods (e.g., Laplace-LoRA) in terms of robustness to domain shifts beyond the datasets tested?
- Basis in paper: [explicit] The paper evaluates OOD performance on specific datasets but does not explore broader domain shifts or compare robustness systematically.
- Why unresolved: The OOD evaluation is limited to a few datasets, and the paper does not provide a comprehensive comparison of robustness across diverse domain shifts.
- What evidence would resolve it: Testing SWAG-LoRA on a wider range of domain-shifted datasets (e.g., cross-lingual, cross-domain) and comparing it to other Bayesian LoRA methods would clarify its robustness.

## Limitations

- The Gaussian posterior assumption may not hold for all fine-tuning scenarios, potentially invalidating the SWAG approximation
- Running multiple SWA ensembles (3-5 runs) requires significant computational resources, limiting practical adoption
- Results are primarily demonstrated on multiple-choice QA datasets, with uncertain generalization to other NLP tasks
- Implementation details for SWAG with LoRA modules are sparse, making faithful reproduction challenging

## Confidence

**High Confidence (Empirical Evidence Strong):**
- MultiSWAG consistently improves calibration metrics (NLL, ECE) across all tested datasets
- SWAG-based methods achieve competitive accuracy compared to more complex Bayesian approaches
- OOD detection capabilities show measurable improvement over baseline methods

**Medium Confidence (Evidence Present but Limited):**
- The Gaussian posterior approximation is sufficient for practical inference
- The low-rank covariance estimation captures meaningful uncertainty structure
- MultiSWAG's ensemble effect stems from exploring different basins of attraction

**Low Confidence (Theoretical or Empirical Gaps):**
- The specific rank (8) and α (16) hyperparameters are optimal across all scenarios
- The calibration improvements generalize beyond multiple-choice QA tasks
- The computational savings over alternatives are substantial in all practical settings

## Next Checks

1. **Gaussian Distribution Validation:** Analyze the distribution of LoRA parameters during SWAG collection using statistical tests (e.g., Kolmogorov-Smirnov test) to verify the Gaussian assumption holds across different fine-tuning scenarios and datasets.

2. **Ablation Study on K Parameter:** Systematically vary the number of epochs (K) used for covariance estimation in SWAG and measure the impact on calibration metrics, accuracy, and computational cost to identify the optimal tradeoff point.

3. **Cross-Dataset Robustness Testing:** Evaluate SWAG/MultiSWAG performance on non-QA NLP tasks (e.g., text classification, summarization) to test the generalizability of calibration and OOD detection improvements beyond the current experimental scope.