---
ver: rpa2
title: 'CAP: Evaluation of Persuasive and Creative Image Generation'
arxiv_id: '2412.10426'
source_url: https://arxiv.org/abs/2412.10426
tags:
- image
- images
- score
- alignment
- persuasiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces three novel metrics\u2014Creativity, Alignment,\
  \ and Persuasiveness (CAP)\u2014for evaluating advertisement image generation from\
  \ visually implicit prompts. The metrics measure how well generated images convey\
  \ abstract messages, demonstrate originality, and employ persuasion techniques."
---

# CAP: Evaluation of Persuasive and Creative Image Generation

## Quick Facts
- arXiv ID: 2412.10426
- Source URL: https://arxiv.org/abs/2412.10426
- Reference count: 40
- Primary result: Novel LLM-based metrics for evaluating advertisement image generation from implicit prompts achieve substantial human agreement (0.68-0.80 Krippendorff's Alpha)

## Executive Summary
This work introduces three novel metrics—Creativity, Alignment, and Persuasiveness (CAP)—for evaluating advertisement image generation from visually implicit prompts. The metrics measure how well generated images convey abstract messages, demonstrate originality, and employ persuasion techniques. An LLM-based approach expands implicit messages into detailed prompts, improving alignment (6%), creativity (7%), and persuasiveness (10%) for commercial ads, and even more for public service ads (39%, 38%, 43%). Human evaluations show the proposed metrics achieve substantial agreement (0.68 alignment, 0.54 creativity, 0.80 persuasiveness) versus poor performance of existing baselines. The study reveals that current T2I models struggle with implicit text inputs, and that LLM-based prompt expansion is a simple yet effective solution for generating more aligned, creative, and persuasive advertisements.

## Method Summary
The method introduces CAP metrics for evaluating advertisement image generation using visually implicit prompts. It employs an LLM to expand abstract messages into detailed prompts, which are then used by T2I models to generate images. A fine-tuned LLM with Contrastive Preference Optimization evaluates alignment by comparing generated action-reason statements with the original message. Creativity is measured using CLIP-based similarity between images, while persuasiveness combines multiple components including audience targeting, benefits, and appeal category. The approach is evaluated on the PittAd dataset with human agreement assessment using Krippendorff's Alpha.

## Key Results
- LLM-based prompt expansion improves alignment (6%), creativity (7%), and persuasiveness (10%) for commercial ads
- Public service ads show even larger improvements: alignment (39%), creativity (38%), persuasiveness (43%)
- Human evaluation agreement: alignment (0.68), creativity (0.54), persuasiveness (0.80)
- Current T2I models struggle with implicit text inputs, benefiting significantly from LLM expansion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based prompt expansion improves alignment, creativity, and persuasiveness scores for ad generation.
- Mechanism: Abstract, implicit messages lack explicit visual cues. LLMs expand these into detailed, explicit descriptions that T2I models can interpret accurately.
- Core assumption: LLMs can accurately interpret abstract ad messages and generate detailed visual descriptions that maintain the original intent.
- Evidence anchors:
  - [abstract] "An LLM-based approach expands implicit messages into detailed prompts, improving alignment (6%), creativity (7%), and persuasiveness (10%) for commercial ads, and even more for public service ads (39%, 38%, 43%)."
  - [section 3.5] "We propose to first generate an expanded message for an advertisement image(E LLM )utilizing an LLM... Next, we use the resulting expansion as the prompt for the T2I method."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.433, average citations=0.0.
- Break condition: LLM fails to accurately interpret the abstract message, generating descriptions that misrepresent the intended ad content.

### Mechanism 2
- Claim: Fine-tuning an LLM with Contrastive Preference Optimization (CPO) improves alignment evaluation accuracy.
- Mechanism: Standard LLMs may hallucinate or misinterpret ad images. CPO training with preferred and dis-preferred action-reason statements helps the LLM generate more accurate interpretations.
- Core assumption: CPO effectively teaches the LLM to distinguish between semantically similar but different action-reason statements based on image content.
- Evidence anchors:
  - [section 3.2] "We utilize a Multimodal Large Language Model... to generate the image descriptionD I. Next, we input this description into a fine-tuned LLM... to generate an action-reason statement for the input image (AR gen)."
  - [section 3.2] "We opt to fine-tune the LLM because it sometimes fails to generate statements with the correctmessagegiven an image."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.433, average citations=0.0.
- Break condition: CPO training fails to provide sufficient distinction between preferred and dis-preferred statements, or the LLM cannot learn the fine-grained semantic differences.

### Mechanism 3
- Claim: Combining multiple persuasiveness components yields better agreement with human evaluations than any single component alone.
- Mechanism: Persuasiveness involves multiple dimensions (audience targeting, benefits, appeal category, elaboration, originality, imagination, synthesis). Combining these creates an ensemble effect where individual component errors cancel out.
- Core assumption: Individual persuasiveness components have different strengths and weaknesses, and their combination provides a more comprehensive evaluation than any single metric.
- Evidence anchors:
  - [section 3.4] "The evaluation of the components is shown in Fig. 3. We first generate a description of the image using an MLLM... Then, for each 'How/How well' question... we ask the LLM to score each image in the range (0, 5)."
  - [section 5.3] "Table 5 represents the agreement among human annotators and scores fromP comp across each component... the difference betweenP comp+AIM andP comp shows the value of adding alignment to the components."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.433, average citations=0.0.
- Break condition: Individual components become highly correlated, reducing the ensemble benefit, or the combination weighting becomes suboptimal.

## Foundational Learning

- Concept: Text-to-Image (T2I) model limitations with implicit prompts
  - Why needed here: Understanding why existing T2I models struggle with implicit messages is crucial for appreciating the need for LLM expansion
  - Quick check question: Why do T2I models perform poorly when given abstract messages compared to explicit descriptions?

- Concept: Persuasiveness evaluation in multimodal content
  - Why needed here: The paper introduces novel methods for evaluating persuasiveness in images, which requires understanding the theoretical foundations
  - Quick check question: What are the key components that make an advertisement image persuasive according to established theories?

- Concept: Contrastive Preference Optimization (CPO) for fine-tuning
  - Why needed here: CPO is used to improve the LLM's ability to generate accurate action-reason statements from images
  - Quick check question: How does CPO differ from standard supervised fine-tuning in terms of training data structure?

## Architecture Onboarding

- Component map: PittAd dataset -> InternVL-v2-26B (MLLM) -> LLAMA3-Instruct-8B/Qwen-2.5-7B-Instruct (LLMs) -> AuraFlow-v0.2/FLUX-v1-dev (T2I models) -> CAP metrics

- Critical path: Input abstract action-reason message -> LLM expands message into detailed prompt -> T2I model generates image -> MLLM generates image description -> LLM evaluates alignment, creativity, persuasiveness -> Output CAP scores

- Design tradeoffs:
  - LLM expansion vs. direct T2I input: Better results but requires additional LLM inference
  - Multiple LLM evaluations vs. single metric: More comprehensive but computationally expensive
  - Fine-tuning vs. zero-shot: Higher accuracy but requires labeled training data

- Failure signatures:
  - Low alignment scores despite visually relevant images: LLM may have failed to interpret the abstract message correctly
  - Inconsistent creativity scores: CLIP-based similarity may not capture all aspects of creativity
  - Poor persuasiveness scores: Individual components may be imbalanced or the combination weighting may be suboptimal

- First 3 experiments:
  1. Generate ILLM images using different LLM expansion models and compare alignment scores
  2. Test AIM metric with and without CPO fine-tuning to measure improvement
  3. Evaluate persuasiveness using individual components vs. combined Pcomp+AIM to verify ensemble benefit

## Open Questions the Paper Calls Out

- Open Question 1: How would the CAP metrics perform when evaluated on non-advertisement creative images such as artwork, memes, or social media content?
- Open Question 2: What is the impact of using different LLM architectures (beyond LLaMA and Qwen) for the ELLM expansion and AIM evaluation steps?
- Open Question 3: How do CAP metrics compare to human evaluation consistency when applied to real-world user-generated advertisement prompts rather than curated PittAd dataset examples?
- Open Question 4: What is the computational cost trade-off between the proposed LLM-based CAP metrics and traditional image quality metrics like FID or CLIPScore for large-scale advertisement generation systems?
- Open Question 5: How would the CAP metrics perform when applied to non-photorealistic or stylized advertisement images such as illustrations, 3D renders, or mixed media formats?

## Limitations

- The evaluation framework relies heavily on LLM-based metrics, which may not generalize beyond advertisement images
- CPO fine-tuning process lacks detailed ablation studies showing individual component contributions
- Human evaluation agreement scores (0.68-0.80) represent moderate rather than substantial agreement according to standard interpretation guidelines

## Confidence

- **High Confidence**: LLM-based prompt expansion improves T2I generation performance for implicit prompts with consistent quantitative results
- **Medium Confidence**: Specific numerical improvements (6%, 7%, 10% for commercial ads; 39%, 38%, 43% for public service ads) based on methodology but lack baseline comparisons
- **Low Confidence**: "Substantial agreement" claim requires careful interpretation as Krippendorff's Alpha values of 0.68 and 0.54 represent moderate agreement

## Next Checks

1. **Ablation study of LLM expansion**: Compare T2I performance using original implicit messages versus LLM-expanded prompts across different types of implicit messages (e.g., emotional vs. logical appeals) to identify which message types benefit most from expansion.

2. **Cross-model robustness**: Test the CAP evaluation framework with different MLLM and LLM combinations to verify that the reported human agreement levels are consistent across model choices rather than specific to the InternVL and LLAMA3 combination used in the paper.

3. **Baseline comparison**: Implement and evaluate traditional ad evaluation metrics (e.g., standard creativity indices, simple alignment measures) alongside CAP to establish whether the novel metrics provide meaningful improvements over established approaches for ad evaluation tasks.