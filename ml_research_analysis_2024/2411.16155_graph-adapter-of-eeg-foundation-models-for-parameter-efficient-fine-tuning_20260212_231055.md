---
ver: rpa2
title: Graph Adapter of EEG Foundation Models for Parameter Efficient Fine Tuning
arxiv_id: '2411.16155'
source_url: https://arxiv.org/abs/2411.16155
tags:
- bendr
- downstream
- data
- tasks
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EEG-GraphAdapter (EGA) is proposed as a parameter-efficient fine-tuning
  (PEFT) method for EEG foundation models that integrates a GNN-based module to capture
  spatial relationships between EEG sensors while freezing a pre-trained temporal
  backbone (BENDR). By learning only the adapter parameters, EGA significantly reduces
  computational cost and data requirements compared to full fine-tuning.
---

# Graph Adapter of EEG Foundation Models for Parameter Efficient Fine Tuning

## Quick Facts
- arXiv ID: 2411.16155
- Source URL: https://arxiv.org/abs/2411.16155
- Authors: Toyotaro Suzumura; Hiroki Kanezashi; Shotaro Akahori
- Reference count: 25
- Primary result: EGA achieves up to 16.1% F1-score improvement over BENDR baseline

## Executive Summary
EEG-GraphAdapter (EGA) is a parameter-efficient fine-tuning method for EEG foundation models that integrates a GNN-based module to capture spatial relationships between EEG sensors while freezing a pre-trained temporal backbone (BENDR). By learning only the adapter parameters, EGA significantly reduces computational cost and data requirements compared to full fine-tuning. Experiments on two healthcare-related downstream tasks—Major Depressive Disorder (MDD) and Abnormality Detection (TUAB)—demonstrate performance improvements of up to 16.1% in F1-score over the baseline BENDR model, with faster training times. EGA effectively combines temporal and spatial EEG features for scalable and accurate neurological disorder prediction.

## Method Summary
EGA combines a frozen BENDR backbone (6-layer 1D CNN + Transformer) for temporal feature extraction with a trainable GNN module (GCN/GraphSAGE/GAT) for spatial feature extraction. The method constructs a fully-connected EEG graph with geodesic distance weights, aggregates spatial information through the GNN, then combines temporal and spatial features using a linear aggregator before classification. The approach freezes BENDR parameters to prevent catastrophic forgetting while fine-tuning only the EGA adapter, achieving parameter efficiency and faster training compared to full fine-tuning.

## Key Results
- EGA-GCN achieved 12.8% higher F1-score on MDD dataset compared to BENDR baseline
- EGA-GraphSAGE achieved 16.1% improvement on TUAB dataset over BENDR baseline
- EGA demonstrated faster training times while maintaining or improving model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing the BENDR backbone while fine-tuning only the EGA adapter prevents catastrophic forgetting of temporal representations.
- Mechanism: BENDR is pre-trained on large-scale EEG data to capture temporal dynamics. By freezing its parameters, EGA ensures that the temporal features learned during pre-training remain intact while the GNN adapter learns spatial relationships.
- Core assumption: The temporal features learned by BENDR are sufficiently general to transfer across downstream tasks.
- Evidence anchors:
  - [abstract]: "freezing the backbone and allowing only the adapter to be fine-tuned"
  - [section]: "freezing the BENDR model to preserve the representation learned during pre-training"
  - [corpus]: Weak evidence - no direct citations on catastrophic forgetting in EEG models.
- Break condition: If temporal patterns in the downstream task significantly differ from pre-training data, fine-tuning BENDR would be necessary.

### Mechanism 2
- Claim: The fully-connected EEG graph with geodesic distance weights captures meaningful spatial relationships between sensors.
- Mechanism: EGA constructs a graph where nodes represent EEG sensors and edges are weighted by geodesic distance. This allows the GNN to aggregate information across spatially related sensors, encoding connectivity patterns.
- Core assumption: Geodesic distance between EEG sensors correlates with functional connectivity in brain regions.
- Evidence anchors:
  - [section]: "we adopt the geodesic distance as the edge weight, using the same approach as EEG-GCNN[6]"
  - [section]: "EEG-GCNN [6] represents the relationships between EEG sensors as a fully-connected graph with weights based on sensor distances"
  - [corpus]: Weak evidence - no direct citations on geodesic distance effectiveness in EEG.
- Break condition: If spatial relationships in the downstream task follow different patterns than geodesic distance, alternative graph constructions would be needed.

### Mechanism 3
- Claim: Different GNN architectures (GCN, GraphSAGE, GAT) capture spatial relationships differently, leading to task-specific performance variations.
- Mechanism: GCN aggregates features by averaging neighbor representations, GraphSAGE uses random sampling of neighbors, and GAT learns attention weights for each node pair. These different mechanisms result in varying effectiveness across tasks.
- Core assumption: The optimal GNN architecture depends on the specific spatial patterns in each downstream task.
- Evidence anchors:
  - [section]: "We adopted GCN, GraphSAGE, and GAT models as the EEG-GraphAdapter"
  - [section]: "EGA-GCN achieved a 12.8% higher F1-score... EGA-GraphSAGE achieved a 16.1% improvement... These results suggest that the most suitable GNN architecture can vary depending on the downstream task"
  - [corpus]: No direct evidence - corpus papers focus on different adapter architectures.
- Break condition: If all GNN variants perform similarly, a simpler architecture could be chosen to reduce computational overhead.

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: Full fine-tuning of large EEG foundation models is computationally expensive and risks overfitting on limited healthcare datasets
  - Quick check question: How does PEFT reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Graph Neural Networks for spatial representation
  - Why needed here: EEG sensors have spatial relationships that standard temporal models like Transformers cannot capture
  - Quick check question: What graph construction method is used to represent relationships between EEG sensors?

- Concept: Catastrophic forgetting in transfer learning
  - Why needed here: Fine-tuning large models can overwrite useful pre-trained representations, especially with limited downstream data
  - Quick check question: Why does freezing the BENDR backbone help preserve temporal representations?

## Architecture Onboarding

- Component map:
  BENDR backbone (frozen) -> EEG-GraphAdapter (trainable) -> Linear Aggregator -> Classifier

- Critical path: Raw EEG → BENDR (frozen) → EGA (trainable) → Linear Aggregator → Classifier → Prediction

- Design tradeoffs:
  - Freezing BENDR reduces computational cost but limits task-specific temporal adaptation
  - Fully-connected graph captures all sensor relationships but may introduce noise from distant sensors
  - Three GNN options provide flexibility but require task-specific selection

- Failure signatures:
  - Performance worse than baseline BENDR: GNN architecture not capturing useful spatial patterns
  - High variance across folds: Insufficient regularization or unstable graph construction
  - Slow training: GNN parameters too large or inefficient implementation

- First 3 experiments:
  1. Baseline: BENDR with Linear Aggregator (full fine-tuning) vs BENDR frozen + EGA-GCN
  2. GNN architecture comparison: BENDR frozen + EGA-GCN vs EGA-GraphSAGE vs EGA-GAT
  3. Graph construction validation: BENDR frozen + EGA with fully-connected graph vs limited connectivity graph

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pre-trained backbone model (e.g., BENDR vs. Transformer-based models like BrainWave) affect the performance of EGA in different downstream tasks?
- Basis in paper: [explicit] The authors mention that future work will explore other pre-trained models like BrainWave and hypothesize that EGA may be more effective for Transformer-based models due to their higher computational costs for fine-tuning.
- Why unresolved: The paper only evaluates EGA with BENDR as the backbone model, leaving the comparative effectiveness of other backbones unexplored.
- What evidence would resolve it: Experimental results comparing EGA's performance using different pre-trained backbone models (e.g., BENDR vs. BrainWave) on the same downstream tasks.

### Open Question 2
- Question: Can EGA maintain or improve its performance in downstream tasks with highly imbalanced datasets, such as those commonly found in neurological disorder detection?
- Basis in paper: [inferred] The authors evaluate EGA on TUAB, a dataset with nearly balanced classes, but do not test its robustness on imbalanced datasets, which are common in healthcare applications.
- Why unresolved: The paper does not address how EGA handles class imbalance, which is critical for real-world healthcare applications.
- What evidence would resolve it: Experiments testing EGA on imbalanced datasets (e.g., with severe class imbalance) and comparing its performance to baseline models using metrics like precision-recall curves or F1-score.

### Open Question 3
- Question: How does the spatial resolution of EEG sensors (e.g., 10-20 vs. higher-density electrode configurations) impact the effectiveness of EGA in capturing spatial relationships?
- Basis in paper: [inferred] The authors preprocess data to use the standard 19-channel 10-20 configuration, but do not explore how EGA performs with higher-density electrode setups, which could provide richer spatial information.
- Why unresolved: The paper does not investigate the scalability of EGA to different sensor configurations, which could affect its spatial feature extraction capabilities.
- What evidence would resolve it: Experiments comparing EGA's performance on datasets with varying electrode densities (e.g., 10-20 vs. 64 or 128 channels) to assess its adaptability to different spatial resolutions.

## Limitations

- The effectiveness of geodesic distance as edge weights in the fully-connected EEG graph lacks strong empirical validation compared to alternative spatial encoding methods
- The GNN architecture selection appears somewhat arbitrary, with different architectures performing differently across tasks without clear explanation of why
- The reported performance improvements may be partially attributed to the specific characteristics of the small downstream datasets used

## Confidence

- **High confidence**: The parameter-efficient fine-tuning approach (freezing BENDR while training only EGA) is well-established and the computational benefits are clearly demonstrated
- **Medium confidence**: The overall performance improvements (12.8-16.1% F1-score gains) are significant but may be partially attributed to the specific characteristics of the small downstream datasets used
- **Low confidence**: The choice of geodesic distance for graph edge weights and the lack of comparison with alternative spatial encoding methods

## Next Checks

1. **Spatial encoding comparison**: Implement and compare EGA with alternative graph constructions (functional connectivity-based weights, learnable edge weights) to validate whether geodesic distance is optimal for capturing EEG spatial relationships

2. **Architecture selection guidance**: Conduct ablation studies to identify which graph properties (e.g., local vs. global connectivity, attention mechanisms) drive performance differences between GCN, GraphSAGE, and GAT variants, providing clearer guidance for architecture selection

3. **Generalization testing**: Evaluate EGA on larger, more diverse EEG datasets to determine whether the reported performance improvements generalize beyond the specific MDD and TUAB datasets used in this study