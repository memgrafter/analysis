---
ver: rpa2
title: Neural Network Symmetrisation in Concrete Settings
arxiv_id: '2412.09469'
source_url: https://arxiv.org/abs/2412.09469
tags:
- symmetrisation
- markov
- cornish
- category
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a high-level overview of a general theory of
  neural network symmetrisation in Markov categories, extending deterministic approaches
  to stochastic settings. It explains how symmetrisation procedures can be characterised
  as functions that upgrade subgroup-equivariant Markov kernels to group-equivariant
  ones, and how these procedures can be composed to build complex equivariance properties.
---

# Neural Network Symmetrisation in Concrete Settings

## Quick Facts
- arXiv ID: 2412.09469
- Source URL: https://arxiv.org/abs/2412.09469
- Reference count: 10
- Primary result: General theory of neural network symmetrisation extending from deterministic to stochastic settings via Markov categories

## Executive Summary
This paper presents a comprehensive theoretical framework for neural network symmetrisation that unifies and extends existing approaches to equivariance. The work establishes a bijection between subgroup-equivariant and group-equivariant functions via coset space constructions, and extends this to stochastic settings using Markov kernels. The framework encompasses both deterministic architectures like canonicalisation and probabilistic symmetrisation, allowing for stochastic equivariance without requiring convexity assumptions or costly averaging operations.

## Method Summary
The framework characterises symmetrisation procedures as functions that upgrade subgroup-equivariant Markov kernels to group-equivariant ones through a bijective mapping involving coset spaces. The core mechanism involves composing three components: an input H-equivariant kernel, a mapping via the coset space construction f♯, and precomposition with a G-equivariant Markov kernel Γ. The theory extends from deterministic functions to stochastic kernels by replacing functions with G-equivariant Markov kernels in the symmetrisation pipeline, avoiding costly averaging operations. The framework also allows compositional construction of complex equivariance properties by chaining symmetrisation procedures along group homomorphisms.

## Key Results
- Establishes a bijection between SetH(RX, RY) and SetG(G/H ⊗ X, Y) via the mapping f ↦ f♯ where f♯([g], x) := g · f(g⁻¹ · x)
- Extends deterministic symmetrisation to stochastic settings using Markov kernels while avoiding convexity assumptions and costly averaging operations
- Provides a compositional framework for building complex equivariance properties by chaining homomorphisms between groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetrisation procedures can be characterised as functions that upgrade subgroup-equivariant Markov kernels to group-equivariant ones via a bijective mapping involving coset spaces.
- Mechanism: The paper establishes a bijection between SetH(RX, RY) and SetG(G/H ⊗ X, Y) via the mapping f ↦ f♯ where f♯([g], x) := g · f(g⁻¹ · x). This allows any function that is only H-equivariant to be "upgraded" to full G-equivariance through a composition involving a coset space G/H.
- Core assumption: The group actions are well-defined and the coset space G/H forms a valid G-set with the diagonal action.
- Evidence anchors: Theorem 1 states that for all choices of the various components involved, there is a bijection SetH(RX, RY) ∼ SetG(G/H ⊗ X, Y) that sends f ↦ f♯ defined as f♯([g], x) := g · f(g⁻¹ · x).

### Mechanism 2
- Claim: Stochastic symmetrisation can be performed without costly averaging operations by working directly with the stochastic condition k(dy|g·x) = g·k(dy|x).
- Mechanism: The framework extends from deterministic functions to Markov kernels by replacing functions with G-equivariant Markov kernels in the symmetrisation pipeline. The resulting stochastic kernel can be used directly without computing expectations, avoiding convexity assumptions.
- Core assumption: The stochastic condition (7) is a valid generalization of deterministic equivariance and can be checked/implemented for neural network outputs.
- Evidence anchors: Section 3 explains that when k(dy|x) is given by the distribution of some f(x, U), the condition becomes f(g·x, U) d= g·f(x, U), which avoids averaging.

### Mechanism 3
- Claim: Complex equivariance properties can be built compositionally by composing multiple symmetrisation procedures along group homomorphisms.
- Mechanism: The framework allows composing symmetrisation procedures by chaining homomorphisms (e.g., Sn → O(3) × Sn → E(3) × Sn) to progressively upgrade equivariance from one group to another, building complex equivariance like E(3) × Sn from simpler components.
- Core assumption: The homomorphisms between groups preserve the necessary structure for symmetrisation to compose correctly.
- Evidence anchors: Appendix B explains that given groups H and K, there is an obvious homomorphism H → K × H that sends h ↦→ (eK, h), allowing compositional symmetrisation.

## Foundational Learning

- Concept: Group actions and G-sets
  - Why needed here: The entire framework is built on group actions acting on sets, with equivariance defined as f(g·x) = g·f(x). Understanding this is fundamental to grasping what symmetrisation achieves.
  - Quick check question: Given a group G acting on sets X and Y, what does it mean for a function f: X → Y to be G-equivariant?

- Concept: Markov categories and Markov kernels
  - Why needed here: The paper extends from deterministic functions to stochastic kernels using the framework of Markov categories, which provides the algebraic structure for reasoning about probability in a rigorous way.
  - Quick check question: How does a Markov kernel k: X → Y differ from a standard function, and how is composition defined for Markov kernels?

- Concept: Coset spaces and quotient groups
  - Why needed here: The bijective mapping in Theorem 1 involves the coset space G/H, which is crucial for the characterisation of symmetrisation procedures and appears in the concrete implementation.
  - Quick check question: Given a subgroup H ⊆ G, what is the set G/H and how does G act on it?

## Architecture Onboarding

- Component map: Input H-equivariant kernel -> Coset space mapping f♯ -> Precomposition with G-equivariant Markov kernel Γ -> Final G-equivariant kernel
- Critical path: The critical path is the composition of the three components in sequence: k (H-equivariant) → f♯ (via Theorem 1 bijection) → f♯ ◦ Γ (G-equivariant). The stability condition requires Γ to have the specific "shape" Γ(x) = (γ(x), x).
- Design tradeoffs: Using stochastic kernels avoids costly averaging and convexity assumptions but requires sampling capabilities. The choice of homomorphism for compositional symmetrisation affects the complexity of the resulting equivariance. The coset space G/H can become large for complex groups, affecting computational efficiency.
- Failure signatures: If the resulting function is not G-equivariant, check the well-definedness of f♯ and the G-equivariance of Γ. If sampling is inefficient, the Markov kernel representation may be too complex. If composition fails, the homomorphisms may not preserve the required structure.
- First 3 experiments:
  1. Implement the deterministic symmetrisation pipeline for a simple group (e.g., cyclic group C3) acting on R², verifying equivariance by testing f(g·x) = g·f(x) for all group elements.
  2. Extend to stochastic symmetrisation using Gaussian noise in the Markov kernel, comparing results with and without averaging to demonstrate the avoidance of costly operations.
  3. Compose two symmetrisation procedures along a simple homomorphism (e.g., Z2 → Z2 × Z2) to build a more complex equivariance property, verifying the composition works as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the averaging operation (ave) and the stochastic symmetrization approach when the group G is compact? Can we characterize conditions under which these approaches yield equivalent deterministic results?
- Basis in paper: The paper discusses both averaging approaches (Kim et al., 2023) and stochastic symmetrization, noting that averaging can be costly and may not be defined for non-convex spaces, while stochastic symmetrization avoids these issues.
- Why unresolved: The paper mentions that averaging can be used to obtain deterministic results from stochastic symmetrization but doesn't explore the theoretical connections or equivalence conditions between these approaches in detail.
- What evidence would resolve it: A formal proof characterizing when stochastic symmetrization followed by averaging is equivalent to direct deterministic symmetrization, along with experimental validation on various group actions and dimensions.

### Open Question 2
- Question: How can the Markov category framework be extended to handle continuous-time stochastic processes or diffusion models while maintaining equivariance properties?
- Basis in paper: The paper focuses on Markov kernels and discrete group actions but mentions that the framework extends beyond Set to any Markov category, suggesting potential for more complex stochastic settings.
- Why unresolved: The current framework is developed for Markov kernels and discrete group actions, but many modern machine learning applications involve continuous-time processes where the group actions might be time-dependent or infinite-dimensional.
- What evidence would resolve it: A generalized framework for continuous-time equivariant processes, potentially using categorical approaches to stochastic differential equations or diffusion processes, with applications to equivariant generative modeling.

### Open Question 3
- Question: What are the computational trade-offs between different symmetrization approaches (canonicalization, frame averaging, probabilistic averaging) in terms of training stability, inference speed, and model capacity?
- Basis in paper: The paper mentions that averaging can be costly, especially in high dimensions, and contrasts this with stochastic symmetrization, but doesn't provide detailed empirical comparisons.
- Why unresolved: While the paper provides theoretical characterizations of various symmetrization approaches, it lacks empirical evaluation comparing their practical performance across different architectures and tasks.
- What evidence would resolve it: Comprehensive benchmark studies comparing different symmetrization methods across multiple tasks (e.g., molecular modeling, point cloud processing), measuring both theoretical properties (equivariance preservation) and practical metrics (training time, inference latency, model accuracy).

## Limitations
- The framework assumes well-defined group actions and measurable structures, which may not hold for all practical scenarios
- Computational complexity concerns arise when working with large coset spaces G/H, potentially limiting scalability
- The theory does not address learnability or optimization of these symmetrisation procedures within standard neural network training paradigms

## Confidence
- High confidence in the bijective characterization of symmetrisation procedures (Theorem 1) and the compositional framework
- Medium confidence in practical implementation details, particularly around concrete examples of G-equivariant Markov kernels and coset representative sampling for specific groups
- Low empirical validation of the stochastic extension in the provided context

## Next Checks

1. **Empirical Verification of Stochastic Condition**: Implement and test the stochastic equivariance condition k(dy|g·x) = g·k(dy|x) on a simple group action (e.g., cyclic group C3 on R²) using Gaussian noise perturbations, comparing results with and without explicit averaging to verify the claimed efficiency benefits.

2. **Compositional Homomorphism Construction**: Build a concrete example of compositional symmetrisation by chaining two simple homomorphisms (e.g., Z2 → Z2 × Z2) and verify through exhaustive testing that the resulting function achieves the expected combined equivariance properties.

3. **Coset Space Sampling Implementation**: Implement the coset representative sampling s: G/H → G for a non-trivial group (e.g., dihedral group D4) and use it to construct the symmetrisation map f♯([g], x) := g · f(s([g])⁻¹ · x), verifying that the resulting function is indeed G-equivariant for various test functions f.