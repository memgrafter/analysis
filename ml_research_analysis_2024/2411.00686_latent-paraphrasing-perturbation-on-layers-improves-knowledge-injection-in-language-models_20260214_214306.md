---
ver: rpa2
title: 'Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in
  Language Models'
arxiv_id: '2411.00686'
source_url: https://arxiv.org/abs/2411.00686
tags:
- latent
- knowledge
- llms
- fine-tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of injecting new or evolving
  knowledge into large language models (LLMs) by improving fine-tuning performance.
  Traditional approaches using paraphrased data face high computational costs and
  limited diversity.
---

# Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models

## Quick Facts
- arXiv ID: 2411.00686
- Source URL: https://arxiv.org/abs/2411.00686
- Reference count: 40
- Primary result: Latent-level paraphrasing (LaPael) improves knowledge injection in LLMs through input-dependent noise applied to early layers

## Executive Summary
LaPael introduces a novel approach to knowledge injection in LLMs by applying input-dependent noise to early transformer layers, creating diverse and semantically consistent augmentations. Unlike traditional methods that rely on data-level paraphrasing, LaPael trains latent paraphrasers to align latent distributions between original and paraphrased text using KL divergence minimization. This eliminates the need for repeated paraphrase generation during fine-tuning, offering computational efficiency while maintaining or improving knowledge injection performance across multiple question-answering benchmarks.

## Method Summary
LaPael trains latent paraphrasers on paraphrased documents to generate input-dependent noise for early transformer layers. The method involves generating paraphrases using GPT models, training latent paraphrasers to minimize KL divergence between original and paraphrased latent distributions, then fine-tuning LLMs with these paraphrasers on knowledge documents. The learnable mask controls perturbation scale for individual tokens, and the approach is compatible with parameter-efficient fine-tuning methods like LoRA.

## Key Results
- LaPael significantly improves knowledge injection performance over standard fine-tuning and noise-based baselines
- The method complements data-level paraphrasing, further enhancing performance when combined
- Ablation studies confirm the importance of learnable mask and input-dependent noise components
- LaPael proves effective across diverse LLMs (Vicuna-7B, LLaMA-2-7B) and parameter-efficient fine-tuning settings

## Why This Works (Mechanism)

### Mechanism 1
LaPael improves knowledge injection by applying input-dependent noise to early transformer layers, creating semantically consistent augmentations. The latent paraphraser learns to match the latent distribution of paraphrased sentences to the original by minimizing KL divergence, generating stochastic perturbations that preserve semantic meaning while introducing diversity. Core assumption: Matching latent distributions is equivalent to preserving semantic meaning while enabling diverse augmentations.

### Mechanism 2
The learnable mask in the latent paraphraser controls perturbation scale for individual tokens, preventing semantic degradation on key tokens. The mask uses a concrete distribution to sample binary values, modulating noise application based on token importance (e.g., less noise on named entities). Core assumption: Different tokens require different perturbation scales to maintain semantic integrity while enabling effective augmentation.

### Mechanism 3
Training on diverse paraphrased data enables the latent paraphraser to generalize across domains without retraining. By training on Dtrain with paraphrases from multiple sources, the paraphraser learns a noise distribution that can be applied to any domain-specific knowledge documents. Core assumption: Noise learned from diverse paraphrased sentences captures generalizable patterns of semantic variation.

## Foundational Learning

- Concept: KL divergence and probability distribution matching
  - Why needed here: The method optimizes latent paraphrasers by minimizing KL divergence between original and paraphrased latent distributions
  - Quick check question: What does minimizing KL divergence between two distributions accomplish in the context of latent paraphrasers?

- Concept: Concrete distribution for differentiable sampling
  - Why needed here: The learnable mask uses concrete distribution to sample binary values while maintaining differentiability for gradient-based training
  - Quick check question: How does the concrete distribution enable training of binary masks in neural networks?

- Concept: Transformer architecture and layer positioning
  - Why needed here: The method inserts latent paraphrasers before MLP layers in early transformer layers, requiring understanding of transformer component interactions
  - Quick check question: Where in the transformer layer architecture are the latent paraphrasers inserted, and why is this positioning important?

## Architecture Onboarding

- Component map: Base LLM -> Latent paraphrasers (5 paraphrasers with linear layer → noise sampling → mask → multiplicative noise) -> Fine-tuning pipeline -> QA evaluation pipeline
- Critical path: Paraphrase generation → Latent paraphraser training → Fine-tuning with latent paraphrasers → Evaluation
- Design tradeoffs: Computational cost of paraphrase generation vs. effectiveness of latent-level augmentation; number of paraphrasers vs. performance; position of paraphrasers vs. effectiveness
- Failure signatures: Performance worse than baseline fine-tuning; training instability during latent paraphraser optimization; poor generalization to new domains
- First 3 experiments:
  1. Train latent paraphraser on small Dtrain, fine-tune on DK, evaluate on DQA to verify basic functionality
  2. Compare performance with and without learnable mask to validate mask importance
  3. Test cross-domain transfer by training on one dataset and evaluating on another to verify generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does LaPael perform on knowledge injection tasks when the knowledge in the documents is not present in the LLM's pre-training corpus? The paper uses synthetic documents and datasets like Films 2024 and Events 2024 to evaluate LaPael on new knowledge, but the performance on truly unseen knowledge is not explicitly compared to other methods.

### Open Question 2
What is the impact of LaPael on knowledge retention in LLMs when fine-tuned on multiple knowledge injection tasks sequentially? The paper acknowledges that knowledge injection can lead to knowledge forgetting but does not explore the long-term effects of LaPael on knowledge retention when LLMs undergo sequential fine-tuning on multiple knowledge injection tasks.

### Open Question 3
Can LaPael be effectively applied to parameter-efficient fine-tuning methods like prefix tuning or prompt tuning for knowledge injection in LLMs? While the paper demonstrates effectiveness with LoRA, it does not explore applicability to other parameter-efficient fine-tuning methods that operate on the input space rather than model parameters.

## Limitations
- Heavy reliance on synthetic paraphrase data generated by GPT models, potentially introducing training distribution bias
- Lack of end-to-end computational efficiency comparisons with traditional paraphrase-based approaches
- Limited evaluation scope to 7B parameter models without testing scalability to larger models

## Confidence
- Core mechanism confidence: Medium - theoretical framework is sound but validation relies heavily on synthetic data
- Computational efficiency claims: Low confidence - no wall-clock comparisons provided
- Generalization across LLMs: Medium confidence - limited to 7B parameter models tested

## Next Checks
1. **Cross-paraphraser robustness test**: Train latent paraphrasers using paraphrases from multiple sources (GPT-3.5, GPT-4, human-written) and evaluate whether performance degrades systematically with paraphrase quality.

2. **End-to-end computational analysis**: Measure total training time for traditional paraphrase fine-tuning vs. LaPael fine-tuning, including paraphrase generation time.

3. **Multi-model scalability study**: Evaluate LaPael on models ranging from 1B to 70B parameters, testing whether the method's effectiveness and computational benefits scale with model size.