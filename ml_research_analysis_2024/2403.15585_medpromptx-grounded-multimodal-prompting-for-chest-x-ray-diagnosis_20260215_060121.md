---
ver: rpa2
title: 'MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis'
arxiv_id: '2403.15585'
source_url: https://arxiv.org/abs/2403.15585
tags:
- data
- image
- medpromptx
- multimodal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedPromptX, a multimodal framework that combines
  few-shot prompting, visual grounding, and dynamic proximity selection to improve
  chest X-ray diagnosis by integrating imaging data with incomplete EHR records. It
  leverages a pre-trained MLLM to fill EHR gaps and uses visual grounding to focus
  on relevant image regions.
---

# MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis

## Quick Facts
- arXiv ID: 2403.15585
- Source URL: https://arxiv.org/abs/2403.15585
- Reference count: 33
- Primary result: Achieves 11% improvement in F1-score over baselines for chest X-ray diagnosis

## Executive Summary
MedPromptX introduces a multimodal framework that integrates few-shot prompting, visual grounding, and dynamic proximity selection to enhance chest X-ray diagnosis using incomplete electronic health records (EHR). The system leverages a pre-trained multimodal LLM to fill EHR gaps and employs visual grounding to focus on relevant image regions. By dynamically refining few-shot examples in real-time, MedPromptX demonstrates state-of-the-art performance with an 11% improvement in F1-score compared to baseline methods.

## Method Summary
MedPromptX is a multimodal framework that combines few-shot prompting with visual grounding and dynamic proximity selection for chest X-ray diagnosis. It processes X-ray images using Grounding DINO to detect regions of interest, integrates these with engineered EHR text features, and uses a multimodal LLM (Med-Flamingo) for diagnosis. The dynamic proximity selection filters and reorders few-shot examples based on multimodal similarity, while feature engineering transforms 357 laboratory test features into textual representations. The system was trained and evaluated on the MedPromptX-VQA dataset, constructed from MIMIC-IV and MIMIC-CXR-JPG databases with 968 records across 12 pathological conditions.

## Key Results
- Achieves 11% improvement in F1-score over baseline methods for chest X-ray diagnosis
- Demonstrates effective integration of multimodal data (images and EHR) for medical diagnosis
- Shows that dynamic proximity selection improves few-shot learning performance by filtering candidates based on multimodal similarity

## Why This Works (Mechanism)

### Mechanism 1
Dynamic Proximity Selection (DPS) improves few-shot learning performance by filtering candidates based on multimodal similarity. DPS computes cosine similarity between grounded image embeddings and text embeddings of candidates and query, then dynamically filters and reorders examples. The core assumption is that higher similarity between candidate and query embeddings correlates with better model performance. Weak evidence exists in related papers, with no direct references to DPS. Break condition: If similarity threshold is set too high, DPS may exclude all candidates; if too low, it may include irrelevant examples.

### Mechanism 2
Visual Grounding (VG) improves model focus on relevant image regions for abnormality detection. GDINO-based VG detects regions of interest in X-ray images based on textual pathological condition inputs, generating grounded image embeddings. The core assumption is that focusing on specific image regions improves model's ability to identify abnormalities compared to processing entire images. Weak evidence exists in related papers, which focus on multimodal fusion but not specifically on visual grounding for X-ray interpretation. Break condition: If VG model is not fine-tuned on chest X-ray images, it may struggle with complex cases where abnormalities are in small regions.

### Mechanism 3
Combining multimodal data (images and EHR) provides richer context for medical diagnosis compared to single-modality approaches. MedPromptX integrates grounded X-ray images with engineered EHR text features through a multimodal LLM (Med-Flamingo) for diagnosis. The core assumption is that multimodal information provides complementary data that improves diagnostic accuracy over single-modality approaches. Weak evidence exists in related papers, which discuss multimodal fusion but don't report specific performance improvements from combining imagery with EHR. Break condition: If EHR data is too incomplete or noisy, multimodal integration may introduce more noise than signal.

## Foundational Learning

- **Few-shot learning and prompting**: Why needed - MedPromptX uses few-shot prompting to adapt to new diagnostic tasks without extensive retraining, addressing the challenge of limited labeled medical data. Quick check - How does few-shot prompting differ from traditional supervised learning in terms of data requirements and model adaptation?

- **Multimodal embeddings and similarity metrics**: Why needed - DPS relies on computing similarity between image and text embeddings to filter and order examples, requiring understanding of embedding spaces and distance functions. Quick check - What are the advantages and limitations of using cosine similarity for comparing multimodal embeddings?

- **Visual grounding and object detection**: Why needed - VG uses grounding techniques to focus on relevant regions in X-ray images, improving abnormality detection compared to processing entire images. Quick check - How does zero-shot detection in visual grounding differ from traditional object detection approaches?

## Architecture Onboarding

- **Component map**: Image → VG → Image Embeddings; EHR Text → Text Embeddings; Embeddings → DPS → Filtered Examples → MLLM → Diagnosis
- **Critical path**: Image → VG → Image Embeddings; EHR Text → Text Embeddings; Embeddings → DPS → Filtered Examples → MLLM → Diagnosis
- **Design tradeoffs**: VG vs. raw images - VG improves focus but may miss subtle abnormalities if not fine-tuned on X-ray data; DPS threshold - Higher threshold improves quality but may reduce candidate pool; Feature selection - Limiting to 10 features per label balances context length with diagnostic information
- **Failure signatures**: Low similarity scores across all candidates - VG or feature engineering may not be capturing relevant information; Inconsistent predictions with similar inputs - DPS threshold may be too high or examples may be too diverse; Poor performance on specific conditions - May indicate insufficient positive examples for that label in the dataset
- **First 3 experiments**: Test DPS with varying similarity thresholds (0.5, 0.7, 0.9) to find optimal balance between candidate quality and quantity; Compare model performance with and without VG to quantify impact of visual grounding; Test different feature selection methods (Pearson correlation vs. mutual information) to optimize EHR feature engineering

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal threshold for dynamic proximity selection that balances performance and the number of retained candidates? The paper states that an extremely high threshold restricts meaningful examples while an extremely low threshold retains nearly the same examples, and shows results for a 0.7 threshold. This remains unresolved as the paper only tests one threshold value and doesn't systematically explore the threshold space. A systematic ablation study testing multiple threshold values (e.g., 0.6, 0.7, 0.8, 0.9) with corresponding performance metrics would identify the optimal threshold.

### Open Question 2
How does MedPromptX performance vary across different pathological conditions? The paper reports overall performance metrics (F1-score, precision, recall) but doesn't break down results by individual condition, despite mentioning 12 pathological conditions in the dataset. This remains unresolved as aggregated metrics don't reveal whether the model performs equally well across all conditions or if there are specific conditions where it struggles. Detailed per-class performance metrics (precision, recall, F1-score) for each of the 12 pathological conditions would show condition-specific strengths and weaknesses.

### Open Question 3
How does the performance of MedPromptX compare to traditional machine learning approaches that use numerical attributes without semantic meaning? The paper mentions that "classical machine learning algorithms, which treat input as numerical attributes without considering the semantic meaning" but doesn't compare against such approaches. This remains unresolved as the paper establishes superiority over other LLM-based approaches but doesn't benchmark against traditional numerical ML methods. Direct comparison of MedPromptX performance against traditional ML models (like random forests or gradient boosting) using the same numerical EHR features would quantify the benefit of semantic processing.

## Limitations
- Dataset Specificity: The MedPromptX-VQA dataset was constructed from MIMIC-IV and MIMIC-CXR-JPG databases, limiting generalizability to other healthcare systems
- Visual Grounding Performance: The Grounding DINO model was not fine-tuned on chest X-ray images, which may affect performance on complex cases
- Dynamic Proximity Selection Sensitivity: The DPS technique relies on a similarity threshold of 70%, but the paper does not extensively explore how sensitive the performance is to this threshold

## Confidence
- High Confidence: The 11% F1-score improvement over baselines is well-supported by experimental results
- Medium Confidence: The mechanisms of visual grounding and dynamic proximity selection are plausible but implementation details lack thorough exploration
- Medium Confidence: The multimodal integration approach is promising but lacks ablation studies to isolate component contributions

## Next Checks
1. Systematically test DPS performance with similarity thresholds ranging from 0.5 to 0.95 in 0.05 increments to identify the optimal threshold and understand sensitivity to this hyperparameter
2. Evaluate MedPromptX on a completely independent chest X-ray dataset (e.g., CheXpert or PadChest) to assess generalizability beyond the MIMIC-derived MedPromptX-VQA dataset
3. Perform systematic ablation by removing each component (VG, DPS, multimodal LLM) individually to quantify their individual contributions to the overall 11% improvement, and test whether VG performance improves when the model is fine-tuned on chest X-ray images