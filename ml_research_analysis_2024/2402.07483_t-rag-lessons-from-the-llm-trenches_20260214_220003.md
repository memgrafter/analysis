---
ver: rpa2
title: 'T-RAG: Lessons from the LLM Trenches'
arxiv_id: '2402.07483'
source_url: https://arxiv.org/abs/2402.07483
tags:
- context
- text
- questions
- question
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Tree-RAG (T-RAG), a question-answering system
  for private organizational documents that combines RAG with a finetuned open-source
  LLM and an entity hierarchy tree. The system retrieves relevant context from both
  a vector database and an entity tree, improving response accuracy.
---

# T-RAG: Lessons from the LLM Trenches

## Quick Facts
- arXiv ID: 2402.07483
- Source URL: https://arxiv.org/abs/2402.07483
- Authors: Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla
- Reference count: 40
- Key outcome: T-RAG combines RAG with a finetuned LLM and entity hierarchy tree, achieving 27/37 correct answers vs 21-20 for baselines

## Executive Summary
This paper presents T-RAG, a question-answering system designed for private organizational documents that combines Retrieval-Augmented Generation (RAG) with a finetuned open-source LLM and an entity hierarchy tree. The system demonstrates that integrating parametric knowledge from finetuning with contextual retrieval from both a vector database and a structured entity tree significantly improves answer accuracy. Evaluations show T-RAG outperforms simple RAG and finetuned models alone, particularly for entity-related questions where the tree context doubles correct responses.

## Method Summary
T-RAG finetunes Llama-2 7B using QLoRA on an instruction dataset of ~1,600 question-answer pairs generated from an organizational governance document. The system uses ChromaDB for vector storage of document chunks and builds an entity hierarchy tree through spaCy NER parsing. When processing queries, it retrieves relevant document chunks via RAG, generates tree context for detected entities, and combines these with Q&A pairs to provide comprehensive context to the finetuned LLM for response generation.

## Key Results
- T-RAG correctly answered 27 out of 37 questions, outperforming RAG (21 correct) and finetuned models (20 correct)
- Including entity tree context roughly doubled the number of correct responses for entity-related questions
- MMLU evaluation showed finetuned model maintained general capabilities with only 2.3% accuracy drop (43% vs 45.3%)
- T-RAG's combination of approaches performed better than either finetuning or RAG alone in needle-in-haystack tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining finetuned LLM with RAG retrieval improves accuracy over either approach alone
- Mechanism: Finetuned model provides domain-specific parametric knowledge while RAG grounds responses in actual document content; the two sources complement each other
- Core assumption: Finetuning doesn't cause catastrophic forgetting of general language capabilities
- Evidence anchors: T-RAG answered 27/37 questions correctly vs 21-20 for individual approaches; MMLU evaluation shows minimal performance drop

### Mechanism 2
- Claim: Entity-tree context doubles accuracy for entity-related questions
- Mechanism: Hierarchical entity relationships encoded in the tree provide structured, disambiguated information that reduces hallucinations
- Core assumption: spaCy NER reliably identifies organization-specific entities in user queries
- Evidence anchors: Tree context roughly doubled correct responses on entity questions; manual evaluation showed improved accuracy

### Mechanism 3
- Claim: QLoRA fine-tuning with small dataset doesn't cause overfitting
- Mechanism: Parameter-efficient fine-tuning updates only ~33.5M parameters out of 7B, minimizing disruption to pre-trained knowledge
- Core assumption: Instruction dataset is diverse enough to cover domain concepts without memorizing phrasings
- Evidence anchors: MMLU evaluation shows 43% vs 45.3% accuracy; similar performance across humanities and STEM domains

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: T-RAG extends standard RAG by adding entity-tree context retrieval; understanding RAG fundamentals is prerequisite to grasping the augmentation
  - Quick check question: In a RAG pipeline, what two processes occur—one at startup and one per query?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: T-RAG uses LoRA/QLoRA to finetune Llama-2 7B efficiently; knowing PEFT mechanics explains why finetuning is feasible on limited hardware
  - Quick check question: In LoRA, which parts of the model weights are updated during fine-tuning?

- Concept: Knowledge Graphs and Tree Structures
  - Why needed here: The entity tree is a hierarchical knowledge graph; understanding graph traversal and node-path extraction explains how context is generated
  - Quick check question: How would you encode "Global Service Center in Budapest" under "Deputy High Commissioner" in a parent-child tree?

## Architecture Onboarding

- Component map: Document ingestion → chunker → ChromaDB vector store → Entity tree builder → spaCy NER parser → Finetuned LLM (LoRA/QLoRA) + base embedding model (Instructor) → Context assembler → Prompt template + inference engine

- Critical path: 1) User query arrives 2) spaCy NER detects org entities 3) Tree search for entity paths (if any) 4) ChromaDB retrieval for document chunks 5) Assemble context (tree + RAG + Q&A) 6) Generate response via finetuned LLM

- Design tradeoffs: Using finetuned model reduces context window pressure but risks overfitting; tree context improves entity QA accuracy but adds parsing overhead and requires maintaining entity hierarchy; parameter-efficient finetuning enables deployment on limited GPUs but may underfit if rank is too low

- Failure signatures: No tree context returned despite entity mention → spaCy rule mismatch or tree missing node; high hallucination rate → finetuned model overfitting or tree context too sparse; slow response → excessive context length or inefficient tree traversal

- First 3 experiments: 1) Run RAG-only baseline on small held-out question set; record accuracy and context length 2) Add finetuned LLM (no context) to measure if finetuning alone improves accuracy 3) Enable tree context for entity queries only; compare entity QA accuracy vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does T-RAG's performance scale with increasing document size and entity complexity?
- Basis in paper: [inferred] The paper demonstrates T-RAG's effectiveness on a specific organizational document but doesn't explore scalability to larger, more complex documents or entity hierarchies
- Why unresolved: The paper focuses on a single case study without exploring performance across varying document sizes and entity complexities
- What evidence would resolve it: Systematic experiments comparing T-RAG's accuracy, retrieval speed, and memory usage across documents of different sizes and with varying numbers of entities and hierarchy levels

### Open Question 2
- Question: What is the optimal balance between RAG, finetuning, and entity tree context for different types of questions?
- Basis in paper: [explicit] The paper shows that T-RAG outperforms both RAG and finetuning alone but doesn't provide guidance on when to prioritize each component
- Why unresolved: The paper demonstrates the benefits of combining all three components but doesn't explore scenarios where one approach might be preferable over others
- What evidence would resolve it: Detailed analysis of question types and their performance across different system configurations (RAG only, finetuning only, T-RAG with varying emphasis on each component)

### Open Question 3
- Question: How does T-RAG's entity detection and tree traversal perform with noisy or ambiguous entity mentions in user queries?
- Basis in paper: [inferred] The paper describes a rule-based entity detection approach but doesn't evaluate its robustness to variations in entity naming or ambiguous mentions
- Why unresolved: The paper doesn't explore edge cases in entity detection, such as partial matches, synonyms, or context-dependent entity references
- What evidence would resolve it: Systematic evaluation of T-RAG's entity detection accuracy across queries with various entity mention patterns, including partial names, abbreviations, and ambiguous references requiring contextual understanding

## Limitations
- Evaluation relies on a single private organizational document and internally generated question-answer pairs, limiting generalizability to other domains
- Does not compare T-RAG against modern commercial LLMs like GPT-4 or Claude, nor does it evaluate performance on out-of-domain questions
- Entity tree construction and maintenance process is not detailed, making scalability unclear for organizations with frequently changing hierarchies

## Confidence

**High confidence**: The core claim that combining finetuned models with RAG improves accuracy over either approach alone. This is supported by direct experimental comparisons showing T-RAG outperforming both baselines on 27/37 questions versus 20-21 for individual approaches.

**Medium confidence**: The claim that entity-tree context doubles accuracy for entity-related questions. While the paper reports this improvement, it's based on a small evaluation set (9 entity questions) and lacks external validation.

**Low confidence**: The assertion that QLoRA fine-tuning with ~1,600 pairs doesn't cause catastrophic forgetting. The MMLU evaluation shows only a 2.3% drop in accuracy, but this is based on a single benchmark without domain-specific tests.

## Next Checks

1. **External corpus replication**: Apply T-RAG to a publicly available organizational document (e.g., UN charter or corporate annual report) and evaluate performance on standardized question sets to test generalizability

2. **Cross-model comparison**: Benchmark T-RAG against GPT-4 with RAG and Claude with RAG on the same question sets to establish relative performance and cost-effectiveness

3. **Dynamic hierarchy test**: Simulate organizational changes by modifying the entity tree structure and measure the system's ability to maintain accuracy without retraining the finetuned model