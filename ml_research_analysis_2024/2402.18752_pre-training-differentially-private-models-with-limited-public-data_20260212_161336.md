---
ver: rpa2
title: Pre-training Differentially Private Models with Limited Public Data
arxiv_id: '2402.18752'
source_url: https://arxiv.org/abs/2402.18752
tags:
- training
- pre-training
- data
- learning
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical understanding of DP optimization
  by analyzing per-iteration loss improvement through the lens of Hessian matrices.
  It shows that DP noise, rather than gradient clipping, is the main cause of performance
  degradation in pre-training, but this can be mitigated by using a small amount of
  public data.
---

# Pre-training Differentially Private Models with Limited Public Data

## Quick Facts
- arXiv ID: 2402.18752
- Source URL: https://arxiv.org/abs/2402.18752
- Authors: Zhiqi Bu; Xinwei Zhang; Mingyi Hong; Sheng Zha; George Karypis
- Reference count: 40
- Primary result: Achieves 41.5% DP accuracy on ImageNet-21k (ε=8) and 55.7%/60.0% non-DP accuracy on downstream tasks

## Executive Summary
This paper provides a theoretical understanding of DP optimization degradation by analyzing per-iteration loss improvement through Hessian matrices. The key insight is that DP noise, rather than gradient clipping, is the main cause of performance degradation during pre-training. The authors propose a DP continual pre-training strategy that uses limited public data to mitigate this degradation, achieving competitive accuracy on both upstream and downstream tasks while maintaining strong privacy protection.

## Method Summary
The method employs DP continual pre-training with an automatic switching mechanism. It starts with non-DP pre-training on public ImageNet-1k using self-supervised DINO learning, then switches to DP pre-training on private ImageNet-11k when the decelerator term becomes small enough. The approach uses AdamW optimizer with batch size 4096, automatic per-sample gradient clipping, and layer-wise clipping. The switching point is determined by early stopping criteria on validation loss. The pre-trained model is then evaluated on downstream tasks including CIFAR-10/100, Food101, SVHN, Places365, and iNaturalist-2021.

## Key Results
- Achieves 41.5% accuracy on ImageNet-21k with ε=8 privacy budget
- Matches non-DP accuracy of 55.7% on Places365 and 60.0% on iNaturalist-2021
- Demonstrates effective membership inference attack resistance for privacy protection
- Shows DP continual pre-training outperforms pure DP pre-training across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
DP noise is the main cause of pre-training performance degradation, not gradient clipping. The per-iteration loss improvement analysis shows the decelerator term σ²tr(H)/(Bc²) dominates in pre-training, while Hessian trace tr(H) is much smaller during fine-tuning, making DP noise relatively less harmful there. The core assumption is that tr(H) is significantly larger during early pre-training epochs than during fine-tuning.

### Mechanism 2
Limited public data during pre-training significantly improves DP optimization convergence. Mixed data training with optimal ratio α* balances non-private public gradients and private gradients, reducing the effective decelerator term and improving per-iteration loss improvement. The core assumption is that the optimal mixing ratio α* is non-zero and can be approximated with a binary switch from public to private training.

### Mechanism 3
DP continual pre-training with automatic switching achieves accuracy comparable to non-DP pre-training. Starting with non-DP pre-training on public data, then switching to DP training when the decelerator becomes small enough, allows the model to learn general features first before applying privacy constraints. The core assumption is that the Hessian trace tr(H) decreases sufficiently during early training epochs to make DP training effective afterward.

## Foundational Learning

- Concept: Hessian matrix and its trace in neural network optimization
  - Why needed here: The analysis shows that the decelerator term σ²tr(H)/(Bc²) is the key factor determining DP training effectiveness, requiring understanding of how Hessian relates to optimization dynamics
  - Quick check question: What does the trace of the Hessian matrix represent in terms of loss landscape curvature, and why does a larger trace make DP training more difficult?

- Concept: Differential privacy and privacy accounting
  - Why needed here: The paper relies on understanding how Gaussian noise scales with batch size and privacy parameters, and how different privacy accountants affect the noise magnitude
  - Quick check question: How does the relationship between batch size B and noise magnitude σ(B) differ between Rényi DP, Gaussian DP, and PRV accounting methods?

- Concept: Per-sample gradient clipping and its interaction with DP noise
  - Why needed here: The analysis shows that clipping factor c and noise interact through the term σ²tr(H)/(Bc²), requiring understanding of how clipping affects gradient sensitivity and DP guarantees
  - Quick check question: Why does per-sample gradient clipping not significantly affect convergence when σ=0, but becomes problematic when DP noise is added?

## Architecture Onboarding

- Component map: Public data loader → Non-DP optimizer (AdamW) → Model parameters → Private data loader → DP optimizer (DP-AdamW with automatic clipping) → Model parameters → Switch controller → Monitors loss/accuracy → Triggers transition from public to private training → Privacy accountant → Computes noise scale σ(B) based on batch size and privacy budget

- Critical path: 1. Load and preprocess public data 2. Perform non-DP pre-training until switching condition met 3. Switch to DP training mode 4. Load and preprocess private data 5. Perform DP continual pre-training with automatic clipping 6. Evaluate on downstream tasks

- Design tradeoffs: Public data ratio - More public data improves DP training but may introduce distribution shift; Switching timing - Early switching preserves privacy budget but may hurt performance; late switching improves performance but uses more privacy budget; Batch size - Larger batches reduce noise but may hurt generalization and increase memory usage

- Failure signatures: Accuracy plateaus early during DP training - Decelerator term still too large, consider more public pre-training; Privacy budget exhausted quickly - Batch size too large or switching too late; Distribution shift between public and private data - Model struggles during DP phase despite good public pre-training

- First 3 experiments: 1. Train with only public data (α=1) and only private data (α=0) to establish baseline performance gap 2. Test different switching points (early, medium, late) to find optimal balance between performance and privacy 3. Compare different privacy accountants (RDP, GDP, PRV) to see impact on noise magnitude and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the decelerator term (σ²tr(H)/(Bc²)) evolve during different phases of pre-training, and what are the underlying mechanisms driving its behavior? While the paper provides a theoretical framework and empirical observations, it doesn't fully explain the mechanisms driving the decelerator's evolution during different pre-training phases. Detailed analysis of tr(H) evolution during pre-training, including its dependence on model architecture, optimization dynamics, and data characteristics, could provide insights.

### Open Question 2
How can the DP continual pre-training strategy be extended to handle data distribution shifts between public and private datasets, and what are the implications for model performance? The paper mentions the possibility of distribution shift but doesn't provide a comprehensive solution or analysis of its impact. Empirical studies evaluating DP continual pre-training performance under various data distribution shifts could reveal the limitations and potential solutions.

### Open Question 3
How does the choice of optimizer and its hyperparameters (e.g., learning rate, momentum) affect the decelerator term and overall DP training performance? While the paper provides a theoretical framework for analyzing general optimizers, it doesn't explore the practical implications of different choices. Empirical studies comparing different optimizers and hyperparameter settings on DP training tasks could reveal insights into optimal choices for specific scenarios.

## Limitations
- Theoretical analysis assumes smooth loss landscapes and relies on Hessian trace comparisons between pre-training and fine-tuning phases
- Public-private data mixing strategy requires careful public data selection to avoid distribution shift
- Automatic switching mechanism depends on early stopping criteria that may not generalize across datasets with different convergence characteristics

## Confidence

- High confidence: The observation that DP noise, not gradient clipping, is the primary cause of pre-training degradation is well-supported by the per-iteration loss improvement analysis and aligns with existing DP optimization literature
- Medium confidence: The effectiveness of limited public data in mitigating DP performance degradation is demonstrated empirically but lacks extensive ablation studies across diverse public-private data distributions
- Low confidence: The automatic switching strategy's generalizability beyond the ImageNet-1k/ImageNet-11k setup requires further validation, particularly for scenarios with significant domain mismatch between public and private data

## Next Checks

1. Empirically measure and compare Hessian trace tr(H) magnitudes during pre-training versus fine-tuning across multiple vision tasks to validate the core theoretical assumption
2. Test the DP continual pre-training strategy with public data from different distributions (e.g., synthetic data, out-of-domain datasets) to assess robustness to distribution shifts
3. Evaluate the automatic switching mechanism with alternative early stopping criteria and privacy accounting methods to determine sensitivity to hyperparameter choices