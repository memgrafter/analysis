---
ver: rpa2
title: 'HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction'
arxiv_id: '2412.13187'
source_url: https://arxiv.org/abs/2412.13187
tags:
- hand
- handsonvlm
- trajectory
- prediction
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HandsOnVLM, a vision-language model designed
  to predict future hand trajectories in egocentric videos based on natural language
  instructions. The core idea is to extend a pre-trained VLM with hand trajectory
  prediction capabilities by adding a hand token to the vocabulary and using auto-regressive
  decoding to generate future hand positions.
---

# HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction

## Quick Facts
- arXiv ID: 2412.13187
- Source URL: https://arxiv.org/abs/2412.13187
- Authors: Chen Bao; Jiarui Xu; Xiaolong Wang; Abhinav Gupta; Homanga Bharadhwaj
- Reference count: 18
- Primary result: HandsOnVLM extends pre-trained VLMs to predict future hand trajectories from language instructions, outperforming task-specific methods on both Vanilla Hand Prediction and Reasoning-based Hand Prediction tasks.

## Executive Summary
HandsOnVLM introduces a novel approach to predict future hand trajectories in egocentric videos using natural language instructions. The model extends a pre-trained VLM with a new `<HAND>` token to represent hand positions in the language space, enabling auto-regressive decoding of future hand positions. Trained on diverse datasets including Epic-Kitchen, H2O, FPHA, and Ego4D, HandsOnVLM demonstrates superior performance on both explicit action description tasks (VHP) and reasoning-based tasks (RBHP) that require understanding implicit instructions. The model successfully translates high-level language reasoning into low-level hand movement predictions.

## Method Summary
HandsOnVLM fine-tunes a pre-trained VLM (LLaVA) by adding a `<HAND>` token to the vocabulary and training it to predict future hand trajectories from video and language inputs. The model uses slow-fast pooling to compress video information, then embeds ground truth hand positions into `<HAND>` tokens during tokenization. A combined loss function trains the model to simultaneously generate coherent text responses and accurate hand trajectories. During inference, iterative decoding predicts hand positions one step at a time, with each prediction conditioning the next.

## Key Results
- HandsOnVLM outperforms existing task-specific methods and VLM baselines on both Vanilla Hand Prediction (VHP) and Reasoning-based Hand Prediction (RBHP) tasks.
- The model demonstrates strong generalization capabilities, maintaining performance on unseen datasets in zero-shot evaluations.
- Qualitative results show HandsOnVLM can generate plausible hand trajectories that align with given instructions, even in complex scenarios requiring reasoning from implicit, colloquial instructions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending the vocabulary with a `<HAND>` token allows the model to represent hand positions in the language space while preserving distinguishability between different hand tokens.
- Mechanism: The model embeds ground truth hand positions into the `<HAND>` tokens during tokenization, so each token carries unique positional information. This enables auto-regressive decoding to predict future hand positions conditioned on both visual and language context.
- Core assumption: The `<HAND>` token embeddings can effectively encode continuous positional information in a discrete language model framework.
- Evidence anchors:
  - [abstract]: "We extend the original vocabulary with hand tokens and having iterative position encodings for auto-regressive predictions during inference."
  - [section]: "To represent hand in the language space, we extend the existing vocabulary with a new <HAND> token. However, a typical embedding layer would encode each <HAND> token identically, resulting in individual <HAND> token being indistinguishable from one another. To overcome this limitation, we embed ground truth hand positions into the <HAND> tokens during the tokenization process."
- Break condition: If the positional encoding is too coarse or the embedding space cannot distinguish nearby positions, predictions will collapse to similar trajectories.

### Mechanism 2
- Claim: SlowFast pooling enables the model to capture both fine temporal resolution and spatial information from the input video frames.
- Mechanism: The fast path averages tokens within each frame to get T tokens, while the slow path selects s frames and applies spatial pooling to get M slow tokens. This combination preserves spatial detail while maintaining temporal context.
- Core assumption: The slow-fast pooling architecture can effectively compress video information into a format suitable for VLM fusion without losing critical temporal dynamics.
- Evidence anchors:
  - [section]: "We embed them into T × M visual tokens using a visual backbone, where M is the number of tokens in each frame. Then we apply slow-fast pooling to get T + M visual tokens."
  - [corpus]: The cited works on LLaVA and Video-ChatGPT use similar temporal pooling strategies, suggesting this is a validated approach for video VLMs.
- Break condition: If the temporal resolution is too low or spatial information is overly compressed, the model will fail to capture fine-grained hand motion patterns.

### Mechanism 3
- Claim: Training with both text generation and trajectory prediction losses enables the model to leverage world knowledge for reasoning about low-level hand trajectories.
- Mechanism: The combined loss function λ_txt L_txt + λ_hand L_hand forces the model to simultaneously generate coherent text responses and accurate hand trajectories, allowing it to reason about future actions based on high-level context.
- Core assumption: The pre-trained VLM contains sufficient world knowledge about human-object interactions that can be transferred to trajectory prediction when properly fine-tuned.
- Evidence anchors:
  - [abstract]: "Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context."
  - [section]: "We fine-tune a pre-trained VLM with auto-regressive trajectory predictions of human hand positions, given a few seconds of video and a language description of the task."
- Break condition: If the world knowledge is not relevant to hand-object interactions or the fine-tuning process corrupts the pre-trained knowledge, the model will fail to reason effectively.

## Foundational Learning

- Concept: Vision-Language Model (VLM) architecture and tokenization
  - Why needed here: HandsOnVLM builds upon pre-trained VLMs like LLaVA, requiring understanding of how visual tokens are processed and fused with language tokens.
  - Quick check question: How does the vision-to-language projector work in VLMs, and what role does it play in combining visual and textual information?

- Concept: Auto-regressive decoding and positional encoding
  - Why needed here: The model uses iterative decoding where each predicted hand position conditions the next prediction, requiring understanding of sequence modeling.
  - Quick check question: What is the difference between causal masking in auto-regressive models versus bidirectional attention, and why is auto-regressive necessary for trajectory prediction?

- Concept: Hand trajectory representation and interpolation
  - Why needed here: The model predicts 2D hand positions over time, requiring understanding of trajectory representation and smoothing techniques.
  - Quick check question: Why use cubic Hermite spline interpolation for smoothing hand trajectories, and what are the advantages over linear interpolation?

## Architecture Onboarding

- Component map:
  - Visual Backbone (CLIP-L-14) -> Vision-to-Language Projector -> LLM (Vicuna) -> Trajectory Decoder (CVAE) -> SlowFast Pooling

- Critical path:
  1. Input video frames → Visual Backbone → T × M visual tokens
  2. SlowFast pooling → T + M visual tokens
  3. Vision-to-language projector → Language-aligned visual tokens
  4. LLM with <HAND> tokens → Embedding sequences
  5. Iterative decoding → Trajectory decoder → 2D hand positions

- Design tradeoffs:
  - Using pre-trained VLM vs training from scratch: Faster convergence but limited by VLM capabilities
  - 2D vs 3D hand positions: Simpler but less expressive for real-world applications
  - Single frame vs video context: More flexible but may lose temporal information

- Failure signatures:
  - Poor trajectory quality when hands are occluded or moving rapidly
  - Mode collapse when the model predicts similar trajectories for diverse instructions
  - Inconsistent predictions when test-time computation is insufficient

- First 3 experiments:
  1. Verify that <HAND> tokens can encode and decode positional information correctly by testing with synthetic data
  2. Test the slow-fast pooling mechanism with different frame rates and spatial resolutions to find optimal compression
  3. Evaluate the combined loss function by training with only text loss vs only trajectory loss to understand their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HandsOnVLM scale with the size of the LLM backbone?
- Basis in paper: [explicit] The paper mentions using LLaVA-V1.3-7B and LLaVA-V1.3-13B as LLM backbones and refers to them as HandsOnVLM-7B and HandsOnVLM-13B, with performance results shown in Figure 7.
- Why unresolved: While the paper shows performance differences between the 7B and 13B models, it doesn't explore scaling beyond these sizes or provide a clear analysis of how performance changes with different model sizes.
- What evidence would resolve it: Experiments testing HandsOnVLM with various LLM sizes (e.g., 3B, 30B, 65B) and plotting performance metrics (ADE, FDE, WDE) against model size would show the scaling relationship.

### Open Question 2
- Question: What is the impact of different observation frame configurations on HandsOnVLM's performance?
- Basis in paper: [explicit] The paper mentions investigating performance when conditioned on just one observation frame instead of an observation video, showing results in Table 5.
- Why unresolved: The paper only compares performance between using the last frame versus the full video context. It doesn't explore intermediate configurations (e.g., using 2-3 frames) or analyze how performance changes with different temporal contexts.
- What evidence would resolve it: Experiments testing HandsOnVLM with varying numbers of observation frames (1, 2, 4, 8 frames) and analyzing performance trends would reveal the optimal temporal context for different tasks.

### Open Question 3
- Question: How does HandsOnVLM's performance compare to other VLM architectures for hand trajectory prediction?
- Basis in paper: [inferred] The paper compares HandsOnVLM to LLaVA-based baselines (LLaVA-Traj, LLaVA-Pixel2Seq) but doesn't explore other VLM architectures like Flamingo, IDEFICS, or Qwen-VL.
- Why unresolved: The comparison is limited to a single VLM architecture family, leaving open the question of whether HandsOnVLM's approach is specifically beneficial or if similar results could be achieved with other VLMs.
- What evidence would resolve it: Implementing HandsOnVLM's trajectory prediction mechanism on top of different VLM architectures and comparing their performance on the same tasks would show whether the architecture choice matters significantly.

## Limitations
- The model relies on accurate hand detection and projection, which can fail with occlusions or rapid hand movements.
- 2D hand position predictions may not be rich enough for direct robotics applications without additional depth information.
- The evaluation focuses on distance-based metrics without assessing semantic correctness or practical usability of predicted trajectories.

## Confidence
- **High Confidence**: The model architecture and training procedure are well-specified, and the reported performance improvements over baselines on the proposed VHP and RBHP tasks are supported by quantitative metrics.
- **Medium Confidence**: The claim that HandsOnVLM effectively leverages world knowledge for reasoning about low-level hand trajectories is supported by results on the RBHP task, but the evaluation scope may not be comprehensive enough to fully validate this capability.
- **Low Confidence**: The assertion that the model can handle complex, real-world hand-object interaction scenarios is primarily supported by qualitative results, which are inherently subjective.

## Next Checks
1. **Ablation Study on Loss Components**: Conduct experiments to isolate the contribution of the text generation loss versus the trajectory prediction loss. Train separate models using only one loss at a time and compare their performance on both VHP and RBHP tasks to understand the synergy between language understanding and trajectory prediction.

2. **Robustness to Occlusion and Fast Motion**: Create a synthetic test set where hands are partially occluded or moving at high speeds. Evaluate HandsOnVLM's performance on this set and compare it to baselines to assess the model's robustness to challenging visual conditions that are common in real-world scenarios.

3. **Downstream Task Evaluation**: Integrate HandsOnVLM's predictions into a simple robotics simulation or a virtual reality environment. Measure how well the predicted trajectories can be used to control a robotic hand or an avatar to perform the intended actions, providing a more practical assessment of the model's output quality beyond distance metrics.