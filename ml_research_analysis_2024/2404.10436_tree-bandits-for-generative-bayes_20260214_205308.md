---
ver: rpa2
title: Tree Bandits for Generative Bayes
arxiv_id: '2404.10436'
source_url: https://arxiv.org/abs/2404.10436
tags:
- posterior
- algorithm
- each
- parameter
- acceptance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops Tree Bandits, a framework for accelerating Approximate
  Bayesian Computation (ABC) by combining recursive partitioning and multi-armed bandit
  algorithms. The core idea is to partition the parameter space into boxes and treat
  each box as a bandit arm, where ABC acceptance serves as the binary reward.
---

# Tree Bandits for Generative Bayes

## Quick Facts
- arXiv ID: 2404.10436
- Source URL: https://arxiv.org/abs/2404.10436
- Reference count: 40
- Primary result: Tree Bandits framework accelerates ABC by partitioning parameter space into bandit arms, improving acceptance rates and convergence speed

## Executive Summary
This paper introduces Tree Bandits, a novel framework that accelerates Approximate Bayesian Computation (ABC) by combining recursive partitioning with multi-armed bandit algorithms. The approach partitions the parameter space into boxes, treating each as a bandit arm where ABC acceptance serves as binary reward. Two versions are developed: ABC-Tree for posterior sampling and ABC-MAP for MAP estimation. The method learns from past trials by updating Beta posteriors on box acceptance rates, then proposes parameters from high-reward boxes more frequently.

Theoretical analysis shows nearly optimal regret bounds, while experiments demonstrate improved acceptance rates and faster convergence compared to rejection ABC and SMC-ABC, particularly in image generation tasks using deep generative models. The framework addresses the fundamental inefficiency of standard ABC by adaptively focusing computational resources on promising regions of the parameter space.

## Method Summary
Tree Bandits accelerates ABC by recursively partitioning the parameter space into boxes and applying multi-armed bandit algorithms to each partition. Each box represents a bandit arm where ABC acceptance serves as the binary reward signal. The method maintains Beta posteriors over acceptance rates for each box, updating them based on trial outcomes. Parameter proposals are generated by sampling from boxes with high estimated acceptance probabilities, determined through either UCB or Thompson sampling strategies. The framework includes two variants: ABC-Tree for posterior sampling through sequential parameter generation, and ABC-MAP for maximum a posteriori estimation through local optimization within promising regions.

## Key Results
- Demonstrates improved acceptance rates compared to rejection ABC and SMC-ABC across multiple benchmark problems
- Achieves faster convergence to the true posterior in image generation tasks using deep generative models
- Provides theoretical regret bounds showing near-optimal performance in the multi-armed bandit setting

## Why This Works (Mechanism)
The framework works by leveraging the exploration-exploitation tradeoff inherent in multi-armed bandit problems. By treating each parameter space partition as a bandit arm, Tree Bandits can adaptively focus computational resources on regions with higher posterior density. The recursive partitioning ensures that promising areas are subdivided for finer-grained exploration, while Beta posterior updates provide uncertainty quantification that prevents premature convergence to suboptimal regions. This combination of spatial adaptation and uncertainty-aware decision making allows for more efficient sampling than uniform or naive sequential approaches.

## Foundational Learning

Approximate Bayesian Computation (ABC): Likelihood-free inference method using simulation and summary statistics. Why needed: Core problem Tree Bandits aims to accelerate. Quick check: Understand rejection sampling and its inefficiency.

Multi-armed Bandits: Sequential decision framework balancing exploration and exploitation. Why needed: Provides the adaptive allocation mechanism for parameter proposals. Quick check: Review UCB and Thompson sampling strategies.

Recursive Partitioning: Space-dividing technique creating hierarchical subdivisions. Why needed: Enables local adaptation of sampling intensity based on observed acceptance rates. Quick check: Understand how partitions affect bandit arm granularity.

Beta Distribution: Conjugate prior for binomial likelihood used in bandit updates. Why needed: Provides closed-form posterior updates for acceptance rate estimation. Quick check: Verify how posterior parameters are updated after each trial.

## Architecture Onboarding

Component Map: Parameter space -> Recursive partitioning -> Box creation -> Beta posterior updates -> Arm selection (UCB/Thompson) -> Parameter proposal -> ABC simulation -> Acceptance check -> Reward update

Critical Path: Parameter generation requires: (1) Select box using bandit strategy, (2) Sample parameter from box, (3) Run ABC simulation, (4) Update Beta posterior, (5) Return parameter if accepted

Design Tradeoffs: Partitioning granularity vs. bandit complexity; UCB's optimism vs. Thompson's sampling efficiency; local vs. global optimization strategies

Failure Signatures: Poor performance when: (1) Acceptance rates are uniformly low across space, (2) Posterior is highly multimodal requiring many partitions, (3) Parameter space dimension exceeds practical partitioning limits

First Experiments:
1. Run on simple Gaussian toy problem with known posterior to verify basic functionality
2. Compare acceptance rates against rejection ABC on same problem
3. Test scalability by increasing parameter dimension incrementally

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on demonstrating the framework's effectiveness and providing theoretical guarantees for the proposed approach.

## Limitations

- Scalability concerns for high-dimensional parameter spaces where recursive partitioning becomes computationally prohibitive
- Limited comparison against state-of-the-art likelihood-free inference methods that may employ more sophisticated proposal mechanisms
- Experimental validation primarily focused on image generation and synthetic problems, with unclear generalizability to other domains

## Confidence

Theoretical analysis: High - regret bounds are rigorously derived using established bandit theory
Practical superiority claims: Medium - comparisons primarily against simpler ABC methods rather than modern alternatives
Experimental validation: Low - limited benchmark scope and absence of comparisons with adaptive ABC methods

## Next Checks

1. Test Tree Bandits on problems with 10+ parameters to assess scalability limitations
2. Compare against recent adaptive ABC methods like sequential neural likelihood or neural ratio estimation
3. Evaluate performance on non-image, non-synthetic datasets to establish broader applicability