---
ver: rpa2
title: Fine-grained large-scale content recommendations for MSX sellers
arxiv_id: '2407.06910'
source_url: https://arxiv.org/abs/2407.06910
tags:
- sellers
- opportunities
- opportunity
- documents
- seismic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a large-scale content recommendation system
  for Microsoft sellers that operates at the opportunity level, providing the top-5
  relevant documents from a catalog of approximately 40,000 Seismic documents for
  each seller-managed opportunity. The system uses semantic matching between opportunity
  attributes and document metadata, implemented through a two-stage approach combining
  fast bi-encoder retrieval with cross-encoder re-ranking.
---

# Fine-grained large-scale content recommendations for MSX sellers

## Quick Facts
- arXiv ID: 2407.06910
- Source URL: https://arxiv.org/abs/2407.06910
- Reference count: 10
- Key outcome: Recommends top-5 relevant documents from ~40K catalog for each seller-managed opportunity, achieving 90ms processing time and Pearson correlation of 0.78 with human expert ratings

## Executive Summary
This paper presents a large-scale content recommendation system for Microsoft sellers that provides personalized document recommendations for each opportunity managed within the Microsoft Seller Experience (MSX) platform. The system processes approximately 700,000 opportunities and recommends the top-5 relevant documents from a catalog of approximately 40,000 Seismic documents. By leveraging semantic matching between opportunity attributes and document metadata through a two-stage approach combining fast bi-encoder retrieval with cross-encoder re-ranking, the system achieves both high recommendation quality and computational efficiency. The solution is currently in pilot phase and integrated into the MSX interface, enabling sellers to access and share recommended content directly within their workflow.

## Method Summary
The system uses a two-stage semantic matching approach where opportunity and document metadata are first summarized into text-based descriptions, then embedded using DistilBERT for fast candidate retrieval. The top-50 candidates are re-ranked using a cross-encoder with MiniLM to produce the final top-5 recommendations per opportunity. Processing is parallelized using Spark with Pandas UDFs, reducing per-opportunity processing time from 2 seconds to 90 milliseconds. The system handles daily updates of approximately 10,000 new opportunities while maintaining recommendation freshness through weekly content embedding updates.

## Key Results
- Achieves 90ms processing time per opportunity (vs 2s previously) through Spark parallelization
- Strong correlation (Pearson coefficient of 0.78) between model scores and human domain expert ratings
- Ablation study confirms "sales play" feature as critical for recommendation quality
- System handles 700,000 opportunities with weekly content updates of 40,000 documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage approach (bi-encoder + cross-encoder) achieves both scalability and high recommendation quality
- Mechanism: The bi-encoder provides fast candidate retrieval (top-50) using efficient embedding comparisons, while the cross-encoder performs expensive but accurate re-ranking on this smaller set, balancing computational efficiency with recommendation quality
- Core assumption: The top-50 candidates from bi-encoder retrieval contain most relevant documents for each opportunity
- Evidence anchors: [abstract] "The system uses semantic matching between opportunity attributes and document metadata, implemented through a two-stage approach combining fast bi-encoder retrieval with cross-encoder re-ranking"

### Mechanism 2
- Claim: Semantic matching based on carefully engineered metadata prompts effectively captures relevant document-opportunity relationships
- Mechanism: By summarizing both documents and opportunities into textual descriptions containing common features ("sales play", "solution area", "product"), the semantic matching learns to identify meaningful relationships between opportunity contexts and relevant content
- Core assumption: The selected metadata features adequately represent the semantic relationship between opportunities and documents
- Evidence anchors: [section 2.1] "Essentially, the idea boils down to summarizing the Seismic documents and the opportunities into a short text-based description that contains the most important attributes"

### Mechanism 3
- Claim: Parallel Spark computation with Pandas UDFs enables processing 700,000 opportunities within acceptable time constraints
- Mechanism: By treating each opportunity as independent and distributing the cross-encoder re-ranking computation across Spark clusters using Pandas UDFs, the system achieves a 90ms processing time per opportunity instead of 2 seconds
- Core assumption: Opportunities can be treated as independent without significant loss of recommendation quality
- Evidence anchors: [section 2.4] "Using a Spark cluster with 96 vcores as required for production deployment of the model, we have reduced the processing time from ≈ 2s per opportunity to ≈ 90ms"

## Foundational Learning

- Concept: Semantic matching using transformer embeddings
  - Why needed here: Enables meaningful comparison between text descriptions of opportunities and documents without requiring exact keyword matches
  - Quick check question: What is the difference between semantic matching and traditional keyword-based matching?

- Concept: Bi-encoder vs cross-encoder architectures
  - Why needed here: Understanding the tradeoff between retrieval speed (bi-encoder) and matching accuracy (cross-encoder) is crucial for the two-stage design
  - Quick check question: Why would a system use both bi-encoder and cross-encoder instead of just one architecture?

- Concept: Spark distributed computing with Pandas UDFs
  - Why needed here: Essential for understanding how the system scales to handle hundreds of thousands of opportunities efficiently
  - Quick check question: What problem do Pandas UDFs solve in Spark that regular UDFs don't address?

## Architecture Onboarding

- Component map: MSX opportunities (700K total) -> Metadata prompt engineering -> DistilBERT embedding -> Top-50 candidate retrieval -> Cross-encoder re-ranking (MiniLM) -> Top-5 recommendations -> Cosmos DB storage -> MSX UI display

- Critical path: Opportunity prompt → DistillBERT embedding → Top-50 candidate retrieval → Cross-encoder re-ranking → Top-5 recommendations stored in Cosmos DB → Displayed in MSX UI

- Design tradeoffs:
  - Scalability vs accuracy: Two-stage approach trades some accuracy for massive scalability
  - Freshness vs computation: Daily batch processing balances up-to-date recommendations with computational cost
  - Feature selection: Using only 3 features as filters reduces combinations from 28B to 5B, but may miss some relevant documents

- Failure signatures:
  - Low correlation between cross-encoder scores and human ratings (target: Pearson > 0.78)
  - High processing time per opportunity (>100ms indicates scaling issues)
  - Poor recall rate (expected documents not appearing in top-5)
  - LLM-as-judge scores significantly diverging from human expert scores

- First 3 experiments:
  1. Test bi-encoder retrieval quality: Run bi-encoder only on a sample of opportunities and manually verify if relevant documents appear in top-50
  2. Validate independence assumption: Compare recommendations for opportunities from the same seller to check for patterns that parallel processing might miss
  3. Benchmark processing performance: Measure end-to-end processing time for 1,000 opportunities with different Spark cluster sizes to validate scalability claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system handle cold-start opportunities with minimal metadata or attributes that could be used for semantic matching?
- Basis in paper: [explicit] The paper mentions that opportunities are classified as δ-opportunities if they have critical properties that have changed in the last 24 hours, but does not address how the system handles opportunities with sparse metadata
- Why unresolved: The paper focuses on opportunities with complete metadata and does not discuss strategies for handling opportunities lacking key attributes needed for semantic matching
- What evidence would resolve it: Performance metrics showing how recommendation quality degrades for opportunities with minimal metadata, or description of fallback strategies (e.g., using opportunity name only, leveraging seller history, or using collaborative filtering approaches)

### Open Question 2
- Question: What is the impact on recommendation quality when the Seismic catalog contains documents with varying levels of relevance or quality?
- Basis in paper: [inferred] The paper assumes all 40,000 documents are relevant candidates but does not discuss document quality control or how irrelevant/low-quality documents affect the top-5 recommendations
- Why unresolved: The evaluation focuses on opportunity-document matching quality but does not address whether the document corpus itself contains noise or irrelevant content that could degrade recommendations
- What evidence would resolve it: A study showing how recommendation quality changes when filtering out low-quality documents, or metrics on the actual relevance distribution of the 40,000 document catalog

### Open Question 3
- Question: How does the system's performance scale when the number of opportunities grows beyond 700,000 or the Seismic catalog expands significantly?
- Basis in paper: [explicit] The paper discusses current performance optimization achieving 90ms per opportunity but does not address future scalability beyond current volumes
- Why unresolved: While the paper demonstrates current scalability through Spark parallelization, it does not model or test performance at larger scales or discuss architectural limitations that might emerge
- What evidence would resolve it: Performance benchmarks showing linear/non-linear scaling patterns, or architectural analysis identifying potential bottlenecks at larger scales (e.g., memory constraints for embeddings, cross-encoder re-ranking limits)

## Limitations

- Domain specificity: The system was developed and validated within Microsoft's specific MSX ecosystem with proprietary Seismic content, limiting generalizability
- Evaluation methodology: Limited details on evaluation methodology and LLM-as-judge framework reliability
- Scalability under load: Performance claims based on specific cluster configuration (96 vcores) may not generalize to different deployment scenarios

## Confidence

**High Confidence**: The two-stage architecture approach (bi-encoder + cross-encoder) is well-established in information retrieval literature. The 0.78 Pearson correlation with human experts provides strong empirical support for the system's effectiveness within the tested domain.

**Medium Confidence**: The parallel Spark computation claims are credible given the established nature of Pandas UDFs, but real-world performance may vary with different cluster configurations and data characteristics.

**Low Confidence**: The LLM-as-judge evaluation framework's reliability and correlation with actual user satisfaction remains unclear due to limited methodological details.

## Next Checks

1. **Cross-domain validation**: Test the recommendation system on a different content domain (e.g., technical documentation, marketing materials) to assess whether the semantic matching approach generalizes beyond the Microsoft/MSX ecosystem.

2. **Real-world usage metrics**: Deploy the system to a broader user base and measure actual usage patterns, content sharing rates, and seller productivity metrics to validate that high model scores translate to practical value.

3. **Feature interaction analysis**: Conduct a more comprehensive ablation study exploring all possible combinations of the three filtering features to identify optimal feature subsets and potential interactions that weren't captured in the initial analysis.