---
ver: rpa2
title: 'Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey'
arxiv_id: '2412.13840'
source_url: https://arxiv.org/abs/2412.13840
tags:
- learning
- data
- federated
- knowledge
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines Non-Centralized Continual
  Learning (NCCL), a paradigm that enables distributed devices to collaboratively
  handle streaming data from non-stationary environments while addressing challenges
  of catastrophic forgetting, distribution shifts, heterogeneity, and privacy. The
  paper reviews existing solutions across three levels: data-level (experience replay,
  generative replay, proxy datasets), model-level (weight regularization, dynamic
  networks, LLM foundations), and device-level (device selection, clustering).'
---

# Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey

## Quick Facts
- arXiv ID: 2412.13840
- Source URL: https://arxiv.org/abs/2412.13840
- Authors: Yichen Li; Haozhao Wang; Wenchao Xu; Tianzhe Xiao; Hong Liu; Minzhu Tu; Yuying Wang; Xin Yang; Rui Zhang; Shui Yu; Song Guo; Ruixuan Li
- Reference count: 40
- Key outcome: Comprehensive survey of Non-Centralized Continual Learning (NCCL) examining data-level, model-level, and device-level solutions for distributed devices handling streaming data while addressing catastrophic forgetting, distribution shifts, and heterogeneity.

## Executive Summary
This survey presents a comprehensive examination of Non-Centralized Continual Learning (NCCL), a paradigm enabling distributed devices to collaboratively handle streaming data from non-stationary environments. The paper systematically categorizes existing solutions across three levels - data-level methods like experience replay and generative replay, model-level approaches including weight regularization and dynamic networks, and device-level strategies for device selection and clustering. It addresses four key challenges: catastrophic forgetting, distribution shifts, heterogeneity (data, model, computation, communication), and privacy/security threats. The survey includes real-world applications across IoT, intelligent transportation, medical diagnosis, and network optimization, while identifying critical resource constraints and future research directions.

## Method Summary
The survey evaluates state-of-the-art NCCL approaches through a large-scale benchmark testing six datasets with three Class-IL and three Domain-IL tasks. Methods are evaluated using ResNet18 backbone with Dirichlet distribution (α=0.1) for high heterogeneity, 20-40 clients per dataset, 15-20 local epochs, and 300 memory buffer per client. The evaluation measures final accuracy A(f) and average accuracy across all tasks, plus communication efficiency via rounds to reach best accuracy. The benchmark includes methods like FedAvg, Re-Fed, FedCIL, GLFC, FOT, FedWeIT, CFeD, Target, AF-FCL, MFCL, pFedDIL, and SR-FDIL across CIFAR-10, CIFAR-100, Tiny-ImageNet, Digit-10, Office-31, and Office-Caltech-10 datasets.

## Key Results
- FOT achieves state-of-the-art performance on Tiny-ImageNet by eliminating data storage requirements through orthogonal projection techniques
- NCCL reduces resource consumption by enabling devices to continually train on new data without retraining from scratch
- The paradigm enables real-time decisions at edge devices without centralized retraining, improving latency
- Security threats including data poisoning and backdoor attacks are identified alongside defense methods like differential privacy and anomaly detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NCCL reduces resource consumption by enabling devices to continually train on new data without retraining from scratch.
- Mechanism: Each device maintains a model that incrementally learns from streaming data, using cached samples or synthetic data for rehearsal to retain knowledge from previous tasks.
- Core assumption: Local storage and computational resources are sufficient to cache samples or generate synthetic data for replay.
- Evidence anchors:
  - [abstract]: "Less computational resources and storage are required to learn the new streaming data. For example, instead of training the new data from scratch, devices only continually train the new data with the model converged on previous tasks."
  - [section III.A.1]: "Its main idea is to use cached data with limited memory buffers to help models retain the knowledge of previous tasks as they learn new ones..."
  - [corpus]: Weak evidence - corpus focuses on related papers but doesn't directly address resource consumption claims.
- Break condition: If local storage becomes insufficient for caching samples or generating synthetic data, the mechanism fails to prevent catastrophic forgetting.

### Mechanism 2
- Claim: NCCL improves model performance by leveraging knowledge from previous tasks to enhance learning on new tasks.
- Mechanism: Continual learning algorithms use techniques like experience replay, generative replay, and regularization to retain and transfer knowledge across tasks.
- Core assumption: There exists meaningful overlap or transferable knowledge between consecutive tasks.
- Evidence anchors:
  - [abstract]: "Continual learning algorithms are designed for streaming tasks with effective skills like data rehearsal and regularization methods."
  - [section III.A]: "The data-level methods focus on the strategic use of data to mitigate forgetting... employs strategies such as experience replay... or the generation of synthetic data for replay purposes."
  - [corpus]: Weak evidence - corpus mentions related approaches but doesn't directly validate performance improvement claims.
- Break condition: If new tasks have minimal overlap with previous tasks, the knowledge transfer becomes ineffective.

### Mechanism 3
- Claim: NCCL reduces latency by enabling real-time decisions at edge devices without centralized retraining.
- Mechanism: Devices continuously update their local models with streaming data and can make immediate decisions without waiting for global model synchronization.
- Core assumption: Local computation and communication infrastructure can support continuous model updates and real-time inference.
- Evidence anchors:
  - [abstract]: "With CL, distributed devices can be consistently trained and updated... in the NCL paradigm, real-time decisions... can be made locally at the edge devices."
  - [section II.A]: "Each device trains the local model with a direct exchange of model information between different devices... brings multiple advantages... data privacy is also ensured..."
  - [corpus]: Weak evidence - corpus discusses related concepts but doesn't specifically address latency reduction.
- Break condition: If network latency or device computational capacity becomes a bottleneck, real-time decision-making is compromised.

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: NCCL must address the fundamental problem of neural networks losing previously learned knowledge when adapting to new tasks.
  - Quick check question: What happens to a neural network's performance on previous tasks when it is trained on new data without any prevention mechanisms?

- Concept: Data Heterogeneity
  - Why needed here: NCCL operates in distributed environments where each device may have different data distributions, requiring algorithms that can handle non-IID data.
  - Quick check question: How does non-IID data distribution across devices affect the convergence and performance of federated learning algorithms?

- Concept: Federated Learning vs Decentralized Learning
  - Why needed here: Understanding the architectural differences between these NCL paradigms is crucial for implementing NCCL in different deployment scenarios.
  - Quick check question: What are the key architectural differences between federated learning and decentralized learning, and how do these differences impact privacy and scalability?

## Architecture Onboarding

- Component map: Distributed devices (clients/nodes) → Local model updates → Parameter exchange → Global model aggregation → Model distribution → Inference
- Critical path: Data collection → Local model update → Parameter exchange → Global model aggregation → Model distribution → Inference. This cycle repeats as new tasks arrive.
- Design tradeoffs: Centralized aggregation provides better global model quality but introduces privacy concerns and communication overhead. Decentralized approaches preserve privacy but may suffer from slower convergence and model drift.
- Failure signatures: Catastrophic forgetting (degraded performance on previous tasks), distribution shift (model performs poorly on new data), communication bottlenecks (slow updates), and privacy breaches (sensitive data leakage).
- First 3 experiments:
  1. Implement FedAvg baseline on CIFAR-10 with 5 class-incremental tasks to establish performance baseline.
  2. Add experience replay with limited memory buffer to measure catastrophic forgetting mitigation.
  3. Introduce Dirichlet distribution for non-IID data allocation to test robustness to data heterogeneity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific convergence bounds can be theoretically established for gradient descent-based NCCL methods on both convex and non-convex loss functions?
- Basis in paper: [explicit] The paper identifies "Convergence Analysis" as a key future research direction, noting that theoretical research on NCCL convergence is lacking despite empirical demonstrations of effectiveness.
- Why unresolved: Most existing NCCL research focuses on empirical performance without rigorous mathematical proofs of convergence rates or bounds. The federated learning convergence literature provides some foundation, but NCCL introduces additional complexity through streaming tasks and continual learning dynamics.
- What evidence would resolve it: Formal proofs establishing convergence rates for NCCL algorithms under different task arrival patterns, loss function properties, and device participation scenarios.

### Open Question 2
- Question: How can Generative AI models be effectively integrated into NCCL frameworks while addressing catastrophic forgetting and computational constraints on edge devices?
- Basis in paper: [explicit] The paper identifies "Generative AI" as a future research direction, noting that even centralized continual learning for Generative AI faces challenges, and NCCL deployment on resource-constrained edge devices complicates matters further.
- Why unresolved: Current Generative AI models are typically large and computationally intensive, making them difficult to deploy on resource-constrained edge devices. The continual learning aspect adds complexity as models need to adapt to new data without forgetting previous knowledge.
- What evidence would resolve it: Empirical studies demonstrating effective Generative AI integration in NCCL systems, showing acceptable performance on both model quality and computational efficiency metrics.

### Open Question 3
- Question: What novel techniques can enable effective asynchronous learning in NCCL while preventing global knowledge forgetting and ensuring efficient cross-task knowledge integration?
- Basis in paper: [explicit] The paper identifies "Asynchronous Learning" as a future research direction, noting that most current NCCL methods focus on synchronous tasks, while asynchronous learning more accurately reflects practical scenarios but poses challenges.
- Why unresolved: Asynchronous learning allows devices to collect and train on new data at different times, which better reflects real-world scenarios. However, this introduces challenges in maintaining global model consistency and preventing catastrophic forgetting when different devices learn at different paces.
- What evidence would resolve it: Novel algorithms demonstrating effective knowledge synchronization and retention in asynchronous NCCL settings, validated through experiments showing improved scalability and performance compared to synchronous approaches.

## Limitations

- Benchmark evaluation only tests methods on six datasets with fixed memory buffer sizes (300 samples per client), which may not reflect real-world resource constraints
- Assumes homogeneous computational capabilities across devices, though in practice device heterogeneity significantly impacts algorithm performance and convergence
- Security threats are discussed theoretically, but effectiveness of defense mechanisms against sophisticated attacks remains largely unproven in NCCL context

## Confidence

**High Confidence**: The classification of NCCL into data-level, model-level, and device-level methods is well-supported by the literature review. The identification of four types of heterogeneity (data, model, computation, communication) aligns with established research in distributed learning systems.

**Medium Confidence**: The claim that NCCL reduces resource consumption by avoiding full retraining from scratch is theoretically sound but lacks empirical validation across diverse deployment scenarios. The performance improvement claims through knowledge transfer depend heavily on task similarity, which varies significantly in real-world applications.

**Low Confidence**: The assertion that NCCL reduces latency for real-time decisions at edge devices requires further validation, as communication overhead and computational constraints can create bottlenecks that negate latency benefits. The effectiveness of security defenses against sophisticated poisoning attacks in NCCL environments remains largely theoretical.

## Next Checks

1. **Resource Constraint Validation**: Implement a systematic evaluation of NCCL methods with varying memory buffer sizes (10, 50, 100, 300, 500 samples per client) on Tiny-ImageNet to determine the minimum buffer size required for acceptable performance and identify the point where catastrophic forgetting becomes prohibitive.

2. **Heterogeneity Stress Test**: Design an experiment with heterogeneous device capabilities (varying computational power, memory, and network bandwidth) to evaluate how different NCCL methods perform under realistic edge device constraints, measuring both accuracy degradation and convergence speed.

3. **Security Defense Validation**: Conduct an empirical study where NCCL systems are subjected to data poisoning attacks with varying intensities and types, then measure the effectiveness of differential privacy and anomaly detection defenses in maintaining model accuracy while preventing attack success.