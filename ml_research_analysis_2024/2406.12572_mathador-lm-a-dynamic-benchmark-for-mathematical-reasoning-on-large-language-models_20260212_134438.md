---
ver: rpa2
title: 'Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language
  Models'
arxiv_id: '2406.12572'
source_url: https://arxiv.org/abs/2406.12572
tags:
- mathador-lm
- benchmark
- arxiv
- number
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mathador-LM is a new benchmark for evaluating mathematical reasoning
  in large language models, inspired by the Mathador game where players reach a target
  number using basic arithmetic operations on a given set of base numbers. The benchmark
  combines ruleset interpretation, planning, and problem-solving, and is designed
  to dynamically generate instances following a target difficulty level to prevent
  test-set leakage and overfitting.
---

# Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models

## Quick Facts
- arXiv ID: 2406.12572
- Source URL: https://arxiv.org/abs/2406.12572
- Reference count: 6
- State-of-the-art LLMs score below 15% on Mathador-LM benchmark

## Executive Summary
Mathador-LM is a new benchmark for evaluating mathematical reasoning in large language models, inspired by the Mathador game where players reach a target number using basic arithmetic operations on a given set of base numbers. The benchmark combines ruleset interpretation, planning, and problem-solving, and is designed to dynamically generate instances following a target difficulty level to prevent test-set leakage and overfitting. Evaluations on leading open and closed-source LLMs, including Llama, Qwen2, Claude, and GPT models, show that state-of-the-art models score below 15% on average, significantly lower than average 3rd graders. The study finds clear correlations between model size and performance, with larger models achieving higher scores, but even the best models struggle with this task. The benchmark's dynamic generation ensures stable performance across problem instances of the same difficulty, making it a reliable tool for assessing LLM reasoning abilities.

## Method Summary
The Mathador-LM benchmark dynamically generates mathematical reasoning problems based on the Mathador game, where players must reach a target number using basic arithmetic operations on five base numbers. Base numbers are sampled uniformly from ranges [1,4], [1,6], [1,8], [1,12], and [1,20], while target numbers are generated based on desired difficulty levels. The benchmark uses programmatic generation of problem instances with controlled difficulty levels, creating unique instances for each evaluation run rather than using a fixed test set. Models are evaluated using few-shot prompting with varying numbers of shots (2, 5, 10, 20) and different text-generation strategies. The scoring system rewards reaching the target (5 points), using each operator type (1-3 points each), and achieving a Mathador Bonus for using all numbers and operators exactly once (6 points).

## Key Results
- Leading LLMs score below 15% accuracy on average, significantly worse than average 3rd graders
- Clear correlation between model size and performance: models below 3B parameters obtain negligible accuracy, while 70-72B models reach top scores of 10-15%
- Dynamic generation ensures stable performance across randomly-generated problem instances of the same difficulty
- State-of-the-art models struggle with formatting, calculation, and illegal operand errors when attempting to solve problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic generation of benchmark instances prevents overfitting to specific problem instances
- Mechanism: The benchmark uses programmatic generation of problem instances with controlled difficulty levels, creating unique instances for each evaluation run rather than using a fixed test set
- Core assumption: Random sampling from difficulty-constrained spaces produces sufficient variation to prevent memorization while maintaining consistent difficulty distribution
- Evidence anchors: [abstract] "We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data"

### Mechanism 2
- Claim: The benchmark's scoring system rewards both reaching the target and using optimal strategies
- Mechanism: Points are awarded for reaching the target (5 points), using each operator type (1-3 points each), and achieving a Mathador Bonus for using all numbers and operators exactly once (6 points)
- Core assumption: The scoring system creates a meaningful gradient that distinguishes between different levels of solution quality and encourages optimal problem-solving approaches
- Evidence anchors: [section 2] "Table 1: Scoring system for Mathador-LM benchmark... Category Points: Target number reached 5 points, Addition 1 point, Multiplication 1 point, Subtraction 2 points, Division 3 points, Mathador Bonus 6 points"

### Mechanism 3
- Claim: The difficulty measure accurately predicts model performance across different problem instances
- Mechanism: Difficulty is defined as the sum of scores of all solutions divided by the square of the number of solutions, capturing the intuition that problems with few but high-scoring solutions are harder
- Core assumption: This mathematical formulation of difficulty correlates well with actual human and model performance, allowing for stable comparisons across randomly generated instances
- Evidence anchors: [section 2] "For a specific set of operands, Et = {E ∈ E| val(E) = t, s(E) > 0} is the set of all solutions for target t. We define the difficulty measure of target t as P E∈Et s(E)/|Et|2"

## Foundational Learning

- Concept: Basic arithmetic operations and their properties
  - Why needed here: The benchmark fundamentally relies on addition, subtraction, multiplication, and division with proper handling of integer constraints
  - Quick check question: What happens when you divide 5 by 2 in the context of this benchmark?

- Concept: Parentheses and order of operations
  - Why needed here: Solutions require proper expression of operations using parentheses to indicate evaluation order
  - Quick check question: How would you express "add 3 and 4, then multiply by 2" in the required format?

- Concept: Permutation and combination of operands
  - Why needed here: Understanding how to systematically explore different arrangements of the five base numbers and operations
  - Quick check question: How many ways can you arrange 3 distinct numbers?

## Architecture Onboarding

- Component map: Problem generator -> Prompt formatter -> Model response -> Solution parser -> Scorer
- Critical path: Generate problem instance → Format prompt → Get model response → Parse solution → Calculate score
- Design tradeoffs: Dynamic generation provides contamination resistance but requires careful difficulty calibration; comprehensive scoring captures solution quality but increases evaluation complexity
- Failure signatures: Inconsistent scores across difficulty levels indicate calibration issues; parsing errors suggest format specification problems; low accuracy across all models might indicate overly difficult problems
- First 3 experiments:
  1. Generate 100 instances across all difficulty levels and verify score distributions match expectations
  2. Test prompt variations with a small model to identify format issues
  3. Run end-to-end evaluation with a simple baseline model to validate the complete pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between model size and performance on Mathador-LM, and what is the minimum model size required to achieve reasonable performance?
- Basis in paper: Explicit. The paper states "We observe clear correlations between model size and game performance, where models below 3B parameters obtain negligible accuracy, state-of-the-art models in the 7-8B range obtain scores of 5-7%, and 70-72B models reach the top scores of 10-15%."
- Why unresolved: The paper provides general trends but doesn't offer a precise quantitative relationship or specific minimum model size threshold.
- What evidence would resolve it: A detailed analysis plotting model size against performance for a wide range of model sizes, including statistical significance testing and confidence intervals.

### Open Question 2
- Question: How does allowing multiple attempts per question affect model performance, and what is the optimal number of attempts?
- Basis in paper: Explicit. The paper states "we analyze the impact of allowing multiple attempts per question" and shows results for K=5 attempts.
- Why unresolved: The paper only tests up to 5 attempts and doesn't explore higher numbers or determine an optimal number.
- What evidence would resolve it: Experiments testing a range of attempt numbers (e.g., 1-10) and analyzing performance gains at each level.

### Open Question 3
- Question: What specific error patterns do different model sizes exhibit, and how do these patterns change as model size increases?
- Basis in paper: Explicit. The paper presents an error analysis table showing four error types (Formatting, Calculation, Missed Target, Illegal Operand) for different models.
- Why unresolved: The paper provides a general error breakdown but doesn't analyze how error patterns vary across model sizes.
- What evidence would resolve it: A detailed analysis breaking down error types by model size and investigating correlations between model size and specific error types.

## Limitations
- Missing implementation details including exact prompt templates and few-shot examples used for evaluation
- Limited empirical validation of the difficulty calibration mechanism despite theoretical motivation
- No direct comparison with established benchmarks to demonstrate unique assessment capabilities

## Confidence

**High Confidence**: The core mechanism of dynamic generation preventing overfitting is well-established conceptually, though the specific difficulty calibration method would benefit from additional validation.

**Medium Confidence**: The scoring system and difficulty measure are logically sound, but the paper lacks empirical evidence demonstrating their effectiveness in practice, particularly the correlation between the proposed difficulty metric and actual model performance.

**Low Confidence**: Missing implementation specifics, particularly around prompt design and few-shot examples, create significant uncertainty about the reproducibility and validity of the reported results.

## Next Checks

1. **Difficulty Calibration Validation**: Generate 100 instances across all difficulty levels and verify that the proposed difficulty measure correlates with actual model performance. Compare results using alternative difficulty metrics to assess robustness.

2. **Prompt Format Testing**: Conduct controlled experiments varying prompt format, few-shot examples, and decoding parameters on a small model to identify which factors most significantly impact performance. Test at least three different prompt variations.

3. **Benchmark Comparison**: Evaluate the same set of models on Mathador-LM and established benchmarks (GSM8K, MATH) to determine whether Mathador-LM captures unique aspects of mathematical reasoning ability not measured by existing benchmarks.