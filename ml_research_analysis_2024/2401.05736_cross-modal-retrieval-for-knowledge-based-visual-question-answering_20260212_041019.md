---
ver: rpa2
title: Cross-modal Retrieval for Knowledge-based Visual Question Answering
arxiv_id: '2401.05736'
source_url: https://arxiv.org/abs/2401.05736
tags:
- retrieval
- https
- visual
- cross-modal
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge-based Visual Question Answering about Named Entities
  is challenging because entities have diverse visual representations. This work argues
  that cross-modal retrieval can bridge the semantic gap between an entity and its
  depictions, and is complementary with mono-modal retrieval.
---

# Cross-modal Retrieval for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2401.05736
- Source URL: https://arxiv.org/abs/2401.05736
- Authors: Paul Lerner; Olivier Ferret; Camille Guinaudeau
- Reference count: 40
- Key outcome: Cross-modal retrieval with CLIP outperforms mono-modal retrieval for knowledge-based VQA about named entities, and combining both yields best results.

## Executive Summary
This paper addresses the challenge of knowledge-based visual question answering (KVQAE) about named entities by leveraging cross-modal retrieval. Named entities have diverse visual representations, making retrieval difficult when only textual entity names are provided. The authors propose using CLIP's pre-trained cross-modal embeddings to bridge the semantic gap between entity names and their visual depictions. By combining mono-modal (image-image) and cross-modal (image-text) retrieval at the score level, they achieve superior entity retrieval performance on three datasets: ViQuAE, InfoSeek-Automatic, and Encyclopedic-VQA. The approach is conceptually simpler and computationally cheaper than billion-parameter models while remaining competitive.

## Method Summary
The method uses CLIP's dual encoder architecture for both mono-modal and cross-modal retrieval. Entity names are mapped to images via text-to-image similarity, while visual questions are encoded with the image encoder for image-to-image similarity. Scores from both modalities are linearly combined to rank candidate entities. The system then retrieves relevant knowledge base passages using a hybrid of DPR (text-text) and CLIP (image-text, image-image) similarities, and finally extracts answers using a multi-passage BERT reader. The authors compare three fine-tuning strategies: mono-modal, cross-modal, and joint training, finding that disjointly trained models combined at inference time perform best.

## Key Results
- Cross-modal retrieval (image-text) significantly outperforms mono-modal retrieval (image-image) on all three KVQAE datasets.
- Combining mono- and cross-modal retrieval scores at the score level yields the best overall performance.
- The proposed method is competitive with billion-parameter models while being conceptually simpler and computationally cheaper.

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal retrieval (image-text) can effectively bridge semantic gaps between named entities and their diverse visual representations, outperforming mono-modal retrieval. CLIP's pre-training aligns image and text representations, so retrieving an entity by its name (text) can find relevant images even when the visual depictions are very different (e.g., statue vs. photograph of Winston Churchill). Entity names carry meaningful visual attributes (e.g., gender, nationality, entity type) that map well into CLIP's joint embedding space. Break condition: If entity names lack visual discriminativeness (e.g., abstract concepts), cross-modal retrieval fails; also if CLIP's pre-training corpus does not contain enough examples of the target entities.

### Mechanism 2
Combining mono-modal and cross-modal retrieval at the score level yields complementary benefits and improves overall retrieval performance. Different retrieval modes capture different aspects of similarity (e.g., mono-modal sensitive to visual details, cross-modal to semantic identity); fusing scores leverages both. The similarity scores from the two retrieval modes are sufficiently independent to justify simple linear combination without interference. Break condition: If scores are highly correlated or one retrieval mode dominates, fusion adds little value; if retrieval tasks change drastically, combination may mislead.

### Mechanism 3
Fine-tuning CLIP jointly for mono- and cross-modal objectives outperforms single-strategy fine-tuning. Joint training adapts both image-image and image-text similarity functions simultaneously, allowing them to specialize while still benefiting from shared representations. CLIP's pre-training was only cross-modal, so joint fine-tuning can bootstrap effective mono-modal retrieval without losing cross-modal performance. Break condition: If training data is too limited, joint fine-tuning may cause interference; if objectives conflict, performance on one modality may degrade.

## Foundational Learning

- **Concept**: Contrastive learning and embedding spaces
  - Why needed here: Retrieval relies on cosine similarity in CLIP's joint embedding space; understanding how contrastive loss shapes this space is key to interpreting retrieval behavior.
  - Quick check question: Why does a cross-modal similarity score work for finding images of an entity when only the entity's name is given?

- **Concept**: Multimodal dual encoders and pre-training
  - Why needed here: CLIP's architecture and pre-training strategy determine its generalization to unseen named entities; engineers must know what biases exist.
  - Quick check question: What is the difference between CLIP's pre-training objective and the fine-tuning objective used here?

- **Concept**: Knowledge-based VQA vs. standard VQA
  - Why needed here: This work targets retrieval of entity-related passages, not just image content; the retrieval step is critical before answer extraction.
  - Quick check question: How does the retrieval step in this pipeline differ from the answer generation step in standard VQA?

## Architecture Onboarding

- **Component map**: Visual question (image + text) -> CLIP image encoder + text encoder -> Mono-modal similarity + Cross-modal similarity -> Score fusion -> Ranked entities -> KB passage retrieval -> Multi-passage BERT reader -> Final answer

- **Critical path**:
  1. Encode visual question with CLIP's image encoder
  2. Compute both mono- and cross-modal similarities to all entities
  3. Fuse scores and rank entities
  4. Map top entities to KB passages
  5. Fuse with DPR text retrieval
  6. Feed top passages to reader for answer extraction

- **Design tradeoffs**:
  - Using only cross-modal retrieval is simpler and cheaper but misses visual detail cues.
  - Full fusion increases computation but yields best accuracy; hyperparameter tuning of weights is important.
  - Joint fine-tuning vs. disjoint fine-tuning trades off specialization vs. generalization.

- **Failure signatures**:
  - Poor cross-modal retrieval if entity names are ambiguous or lack visual cues.
  - Over-reliance on mono-modal retrieval if entity depictions are too varied.
  - Joint fine-tuning may underfit if training data is small; disjoint training may overfit.

- **First 3 experiments**:
  1. Compare mono-modal vs cross-modal retrieval on a small validation set to confirm complementarity.
  2. Test score fusion with different weight combinations to find optimal balance.
  3. Train disjoint mono-modal and cross-modal models, then fuse them, to validate best approach.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical results rely on a relatively small set of retrieval tasks and datasets (ViQuAE, ISA, EVQA), limiting generalizability to other entity-centric scenarios.
- Missing KB entities in ISA (11.5% of total) may bias performance results and affect fair comparison across datasets.
- Limited ablation studies on fine-tuning strategies; the claim that disjoint fine-tuning outperforms joint fine-tuning is based on a single observation without systematic comparison.

## Confidence

- **High Confidence**: The conceptual mechanism that cross-modal retrieval can leverage CLIP's joint embedding space to bridge semantic gaps between named entities and their visual depictions.
- **Medium Confidence**: The empirical claim that combining mono- and cross-modal retrieval scores yields better performance than either alone.
- **Low Confidence**: The claim that joint fine-tuning underperforms disjoint fine-tuning.

## Next Checks
1. Evaluate the retrieval methods on additional entity-centric datasets beyond the three used in the paper to assess the robustness of the conclusions about cross-modal vs mono-modal retrieval performance.
2. Conduct a systematic ablation study of the score fusion weights and fine-tuning hyperparameters to determine the stability of the reported improvements and identify potential overfitting to the specific experimental setup.
3. Re-run the ISA experiments after completing the KB (if possible) to quantify the impact of the missing entities on the reported performance and validate the fairness of the comparison across datasets.