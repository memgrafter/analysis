---
ver: rpa2
title: Auxiliary task demands mask the capabilities of smaller language models
arxiv_id: '2404.02418'
source_url: https://arxiv.org/abs/2404.02418
tags:
- task
- demands
- evaluation
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how auxiliary task demands affect the performance
  of language models (LMs) across various cognitive tasks. It finds that evaluation
  methods with higher task demands (e.g., production vs.
---

# Auxiliary task demands mask the capabilities of smaller language models

## Quick Facts
- arXiv ID: 2404.02418
- Source URL: https://arxiv.org/abs/2404.02418
- Authors: Jennifer Hu; Michael C. Frank
- Reference count: 25
- Key outcome: Evaluation methods with higher task demands yield lower performance, especially for smaller models or those with less training data

## Executive Summary
This study investigates how auxiliary task demands affect the performance of language models across various cognitive tasks. The research finds that evaluation methods with greater task demands (e.g., production vs. forced choice, metalinguistic judgment vs. direct probability measurement) consistently yield lower performance, particularly for smaller models or those with less training data. This "demand gap" suggests that LM performance should be interpreted as a reflection of their capabilities under specific evaluation designs, rather than as a direct measure of intelligence.

## Method Summary
The study used the Huggingface Transformers library to run inference on various language models, collecting probability outputs with the minicons package. The researchers evaluated models across four cognitive domains using multiple evaluation contrasts: analogical reasoning (production vs. forced choice), reflective reasoning (metalinguistic judgment vs. direct probability), word prediction (metalinguistic judgment vs. direct probability), and grammaticality judgment (metalinguistic judgment vs. direct probability). Models tested included Llama-2 variants (7B to 70B parameters) and OLMo 7B at different training checkpoints, using datasets from analogical reasoning, critical thinking, word prediction, and grammaticality judgment tasks.

## Key Results
- Higher-demand evaluation methods consistently yielded lower performance than reduced-demand alternatives
- The "demand gap" between high and low-demand evaluations was most pronounced for models with fewer parameters and less training data
- The gap between high-demand and low-demand evaluations decreased as model size and training time increased

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluation methods with greater task demands yield lower performance, especially for models with fewer parameters or less training data.
- Mechanism: Higher task demands introduce auxiliary challenges that require additional cognitive resources beyond the core capacity being measured. Less capable models have fewer resources to handle these auxiliary demands, creating a performance gap.
- Core assumption: Task demands impose challenges unrelated to the underlying capacity being measured, and these challenges affect models differently based on their resource availability.
- Evidence anchors:
  - [abstract] "evaluation methods with greater task demands yield lower performance than evaluations with reduced demands"
  - [section] "task demands are one of many different factors that can undermine validity" and "a task demand can be thought of as a feature of the task that imposes challenges unrelated to the capacity that the task is designed to measure"
  - [corpus] Weak evidence - corpus neighbors focus on multi-task learning and reasoning enhancement but don't directly address task demand gaps
- Break condition: If auxiliary demands are equally distributed across model sizes or if models develop mechanisms to handle auxiliary demands proportionally to their growth in parameters/training time.

### Mechanism 2
- Claim: The performance gap between high-demand and low-demand evaluations (demand gap) shrinks as models increase in size or training time.
- Mechanism: As models grow larger or receive more training, they develop greater representational capacity and better ability to handle auxiliary task demands. This reduces the differential impact of task demands between high-demand and low-demand evaluations.
- Core assumption: Model growth leads to proportional improvement in handling auxiliary demands, not just core capacities.
- Evidence anchors:
  - [abstract] "This 'demand gap' is most pronounced for models with fewer parameters and less training data"
  - [section] "less capable agents should suffer more from task demands than more capable agents" and "the difference in log odds tends to decrease as the number of parameters increases"
  - [corpus] Weak evidence - corpus neighbors don't address how model growth affects task demand handling
- Break condition: If model growth only improves core capacities without proportional improvement in handling auxiliary demands, or if auxiliary demands become irrelevant as models scale.

### Mechanism 3
- Claim: Low-demand evaluation methods can reveal abilities in models earlier during training that might not be revealed by higher-demand methods.
- Mechanism: Low-demand evaluations align more closely with training objectives or constrain possibilities, requiring fewer auxiliary resources. This allows models to demonstrate core capabilities even before developing full capacity to handle auxiliary demands.
- Core assumption: The ability to handle auxiliary demands develops later in training than core capacities, creating a window where low-demand evaluations reveal knowledge that high-demand evaluations mask.
- Evidence anchors:
  - [section] "evaluation methods with lower task demands can reveal abilities in a model earlier during training that might not otherwise be revealed by higher-demand methods"
  - [section] "For word prediction... the log probabilities achieved by the direct method quickly plateau at a high value, whereas they increase more consistently under the metalinguistic method"
  - [corpus] Weak evidence - corpus neighbors focus on multi-task learning and reasoning enhancement but don't directly address early ability revelation
- Break condition: If core capacities and auxiliary demand handling develop at similar rates, eliminating the window where low-demand evaluations reveal hidden abilities.

## Foundational Learning

- Concept: Task demands as auxiliary challenges unrelated to target capacity
  - Why needed here: Understanding that task demands are separate from the construct being measured is fundamental to interpreting why evaluation methods affect performance differently
  - Quick check question: What distinguishes a task demand from the core capacity being measured in an evaluation?

- Concept: Validity in measurement and how task demands undermine it
  - Why needed here: Recognizing that auxiliary demands reduce the validity of inferences from performance to underlying constructs explains why controlling for task demands matters
  - Quick check question: How do task demands affect the validity of inferences from observable performance to latent psychological constructs?

- Concept: Interaction between model capability and evaluation method
  - Why needed here: Understanding that evaluation performance is a function of both model capability and the specific demands of the evaluation method is crucial for proper interpretation
  - Quick check question: Why might a less capable model perform better on a low-demand evaluation than a high-demand evaluation, even when both target the same construct?

## Architecture Onboarding

- Component map: Model evaluation pipeline (Huggingface Transformers) -> Probability measurement utilities (minicons package) -> Dataset loading mechanisms -> Performance metrics computation -> Demand gap analysis

- Critical path: For a new evaluation, the critical path is: (1) Select appropriate dataset and evaluation contrast, (2) Configure model and evaluation method, (3) Run inference and collect probabilities, (4) Compute performance metrics, (5) Analyze demand gap patterns across model sizes/training checkpoints.

- Design tradeoffs: The choice between production vs. forced choice affects the precision needed in probability estimates; metalinguistic vs. direct probability measurement trades interpretability for alignment with training objectives. Higher-demand methods may be more intuitive but risk masking capabilities in less capable models.

- Failure signatures: If demand gaps don't decrease with model size as predicted, this could indicate either that auxiliary demands aren't the primary factor or that model growth doesn't proportionally improve auxiliary demand handling. If demand gaps are constant across all models, this suggests auxiliary demands affect all models equally regardless of capability.

- First 3 experiments:
  1. Replicate the production vs. forced choice contrast on analogical reasoning using Llama-2 models of different sizes to establish baseline demand gap patterns
  2. Test the metalinguistic vs. direct probability measurement contrast on word prediction using intermediate training checkpoints of OLMo 7B to observe demand gap development over training time
  3. Analyze the interaction effect between model size and evaluation method using mixed effects models on grammaticality judgment data to formally test demand gap predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific architectural mechanisms (e.g., attention patterns, residual connections) contribute to task demand sensitivity in language models?
- Basis in paper: [inferred] The paper suggests that less capable models are more sensitive to task demands but does not investigate the underlying computational mechanisms that cause this sensitivity.
- Why unresolved: The study focuses on behavioral differences across evaluation methods rather than analyzing internal model mechanisms. Understanding the specific architectural factors would require detailed mechanistic analysis beyond the scope of this work.
- What evidence would resolve it: Experiments comparing different architectural variants across the same evaluation contrasts, or mechanistic interpretability studies showing how different components handle auxiliary task demands.

### Open Question 2
- Question: Do task demand effects vary systematically across different domains of cognitive ability (e.g., logical reasoning vs. linguistic knowledge)?
- Basis in paper: [explicit] The authors note that their investigation covered "only samples from the much broader literature on cognitive evaluation" and suggest more work is needed to investigate impacts across different cognitive domains.
- Why unresolved: The study tested only analogical reasoning, reflective reasoning, word prediction, and grammaticality judgment. The pattern of task demand effects across these domains was mixed, with some showing clear size-dependent effects and others showing more consistent patterns.
- What evidence would resolve it: Systematic testing of task demand effects across a broader range of cognitive domains using the same evaluation contrasts.

### Open Question 3
- Question: How do task demand effects interact with model fine-tuning and instruction-following capabilities?
- Basis in paper: [inferred] The study used only base models without fine-tuning, but notes that "a growing trend" uses metalinguistic prompts for evaluation. This suggests potential interactions between task demands and instruction-following abilities.
- Why unresolved: The experiments did not test whether instruction-tuned models show different patterns of task demand sensitivity compared to base models. The authors mention this as an "interesting (and speculative) direction" but did not investigate it.
- What evidence would resolve it: Direct comparison of base vs. instruction-tuned models across the same evaluation contrasts, testing whether fine-tuning reduces or alters task demand sensitivity.

## Limitations
- Focus on English language tasks may limit generalizability to other languages or domains
- Reliance on autoregressive transformer models leaves open questions about other architectures
- Assumption that auxiliary demands are truly orthogonal to core capacities needs more rigorous validation

## Confidence
- High confidence in the empirical observation that higher-demand evaluations yield lower performance across models
- Medium confidence in the theoretical interpretation that task demands are truly auxiliary to core capacities
- Medium confidence in the claim that demand gaps shrink with model size/training time

## Next Checks
1. Test the demand gap hypothesis on non-English languages and cross-linguistic datasets to determine if the observed patterns generalize beyond English-language cognitive tasks.

2. Design experiments that manipulate specific auxiliary demands (e.g., working memory load, attention requirements) independently to isolate which auxiliary challenges most contribute to performance gaps.

3. Apply the demand gap framework to non-language domains (e.g., visual reasoning, mathematical problem-solving) to assess whether the concept extends beyond linguistic evaluations and what this implies for general AI evaluation practices.