---
ver: rpa2
title: Can Large Language Models Logically Predict Myocardial Infarction? Evaluation
  based on UK Biobank Cohort
arxiv_id: '2409.14478'
source_url: https://arxiv.org/abs/2409.14478
tags:
- chatgpt
- medical
- language
- llms
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the ability of large language models (LLMs)\
  \ like ChatGPT and GPT-4 to predict myocardial infarction risk using UK Biobank\
  \ cohort data. Patient risk factor data were transformed into standardized textual\
  \ descriptions, and models were asked to assign a 0\u201310 risk score."
---

# Can Large Language Models Logically Predict Myocardial Infarction? Evaluation based on UK Biobank Cohort

## Quick Facts
- arXiv ID: 2409.14478
- Source URL: https://arxiv.org/abs/2409.14478
- Reference count: 40
- Key outcome: GPT-4 (AUC 0.69) outperformed ChatGPT (AUC 0.62) but both underperformed traditional ML models (AUC 0.76–0.79) in MI risk prediction

## Executive Summary
This study evaluated large language models' ability to predict myocardial infarction risk using UK Biobank cohort data. Patient risk factor data were transformed into standardized textual descriptions, and models were asked to assign a 0–10 risk score. While GPT-4 showed slightly better performance than ChatGPT, both underperformed traditional machine learning models. Chain-of-thought prompting did not improve predictions, suggesting limited logical reasoning capability. The findings indicate that current LLMs lack the precision needed for clinical decision support and require domain-specific expertise to reliably interpret quantified medical data.

## Method Summary
The study used UK Biobank cohort data (690 participants: 316 MI, 374 non-MI) with risk factors transformed into standardized textual descriptions. LLMs (ChatGPT, GPT-4, Llama, MedLlama, Mixtral, Meditron) were prompted to assign MI risk scores on a 0–10 scale. Chain-of-thought questioning was employed to test logical reasoning. Performance was evaluated using AUC, accuracy, sensitivity, specificity, and Youden Index, then compared against traditional machine learning models and medical indices.

## Key Results
- GPT-4 achieved AUC 0.69, outperforming ChatGPT (AUC 0.62) but underperforming traditional ML models (AUC 0.76–0.79)
- Chain-of-thought prompting failed to improve performance across all models
- Fine-tuned medical LLMs (MedLlama, Meditron) showed minimal improvement (AUC ~0.62) over general LLMs
- Model performance varied by age groups, with better discrimination in younger patients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with Chain-of-Thought prompting fail to improve clinical prediction accuracy because they do not perform logical inference.
- Mechanism: When information is given incrementally, LLMs do not effectively integrate and reason over new data; they simply concatenate outputs based on learned patterns without causal understanding.
- Core assumption: LLMs lack built-in reasoning mechanisms and treat step-by-step prompts as separate inputs rather than a coherent reasoning process.
- Evidence anchors:
  - [abstract] "Chain of Thought (CoT) questioning was used to evaluate whether LLMs make prediction logically" and results showed no improvement.
  - [section] "The CoT approach, designed to simulate human-like stepwise reasoning, failed to leverage the incremental data in a way that improved prediction, suggesting a lack of logical reasoning ability in current LLMs."
  - [corpus] No direct corpus evidence; the study’s own experiments serve as primary evidence.
- Break condition: If a model with explicit logical reasoning modules or symbolic reasoning integration is used, or if the prompt engineering is changed to guide explicit causal reasoning steps.

### Mechanism 2
- Claim: LLMs underperform compared to traditional machine learning models in predicting myocardial infarction because they lack domain-specific training and quantified medical data understanding.
- Mechanism: General LLMs are trained on broad language tasks, not specialized medical risk factor modeling; thus they cannot weigh and integrate clinical variables as precisely as models explicitly trained on these tasks.
- Core assumption: Specialized ML models (e.g., SVM, decision trees) outperform LLMs on structured, quantified medical data because they are trained specifically for that purpose.
- Evidence anchors:
  - [abstract] "both underperformed traditional machine learning models (AUC 0.76–0.79)"
  - [section] "GPTs is predicting less accurate than machine learning models... with significant discrimination"
  - [corpus] Weak corpus evidence; this is derived from the study’s comparison results.
- Break condition: If LLMs are fine-tuned on large, high-quality medical datasets and trained specifically for quantified clinical risk prediction.

### Mechanism 3
- Claim: Fine-tuning LLMs on medical corpora yields modest but insufficient gains in clinical prediction performance.
- Mechanism: Medical fine-tuning improves domain knowledge but does not fundamentally change the model’s architecture or reasoning ability; improvements are incremental and still below traditional ML.
- Core assumption: Fine-tuning changes knowledge but not core reasoning capabilities; thus performance gains are limited.
- Evidence anchors:
  - [section] "MedLlama, a fine-tuned version of LLAMA on medical data, shows slight improvement with an AUC of 0.62" and still underperforms GPT-4.
  - [abstract] "showed similar performance to other LLMs (Llama, 0.61; MedLlama, 0.62; Mixtral and Meditron, 0.60)"
  - [corpus] No direct corpus evidence; these results are from the study’s own experiments.
- Break condition: If future models combine fine-tuning with reasoning modules or hybrid symbolic-numeric approaches.

## Foundational Learning

- Concept: Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC)
  - Why needed here: These metrics are used to evaluate and compare model discrimination performance in the study.
  - Quick check question: If a model has AUC 0.62, what does that say about its ability to distinguish between MI and non-MI cases compared to random guessing?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT was used to test whether LLMs can reason logically over stepwise provided information.
  - Quick check question: In the study, did CoT prompting improve model performance, and what does that imply about LLM reasoning?

- Concept: Fine-tuning vs. general pre-training
  - Why needed here: The study compared general LLMs with fine-tuned medical LLMs to assess domain-specific adaptation.
  - Quick check question: What was the performance difference between Llama and MedLlama, and what does that suggest about the impact of fine-tuning?

## Architecture Onboarding

- Component map: Data transformation (tabular → textual) → Prompt engineering (CoT vs. full info) → LLM inference → Output score (0-10) → Evaluation (AUC, accuracy, sensitivity, specificity)
- Critical path: Data transformation → Prompt design → Model inference → Performance evaluation
- Design tradeoffs: Generalist vs. specialist models; incremental vs. full information prompts; computational cost vs. prediction accuracy
- Failure signatures: Low AUC (~0.6) compared to traditional ML (~0.78); no improvement with CoT; similar performance across general and fine-tuned LLMs
- First 3 experiments:
  1. Replicate the study’s comparison by evaluating a base LLM and a fine-tuned medical LLM on the same dataset.
  2. Test different prompt structures (e.g., explicit causal reasoning vs. CoT) to see if logical reasoning can be elicited.
  3. Combine LLM output with a traditional ML model (ensemble) to assess if hybrid approaches improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications would enable LLMs to better handle quantified medical data for clinical decision support?
- Basis in paper: [explicit] The paper concludes that current LLMs lack robust and quantified logical reasoning capabilities, which are crucial for making complex clinical decisions.
- Why unresolved: The study identifies the limitation but does not explore potential architectural solutions or modifications that could address this issue.
- What evidence would resolve it: Empirical comparisons of modified LLM architectures (e.g., hybrid models incorporating rule-based systems) against current models in predicting medical outcomes using quantified data.

### Open Question 2
- Question: How would the performance of LLMs change if they were fine-tuned on medical-specific corpora rather than general language data?
- Basis in paper: [inferred] The paper suggests that future medical LLMs should be expert in medical domain knowledge and compares the performance of general LLMs with those fine-tuned on medical data (e.g., MedLlama, Meditron).
- Why unresolved: While the paper includes some fine-tuned models in the comparison, it does not explore the extent to which fine-tuning on larger or more specialized medical corpora could improve performance.
- What evidence would resolve it: Systematic evaluation of LLMs fine-tuned on progressively larger and more specialized medical datasets, comparing their performance in clinical prediction tasks.

### Open Question 3
- Question: Can incorporating evidence-based medical rules into LLMs improve their logical reasoning and predictive accuracy in clinical settings?
- Basis in paper: [explicit] The paper suggests that clinical scientific rules proved by evidence-based medicine might become an indispensable element in future LLM-based CDS systems.
- Why unresolved: The study identifies the potential importance of evidence-based rules but does not investigate how integrating such rules into LLMs would affect their performance.
- What evidence would resolve it: Comparative studies of LLMs with and without integrated evidence-based medical rules in their reasoning processes, measuring improvements in predictive accuracy and logical consistency.

## Limitations
- Data transformation bottleneck: Converting tabular medical data to textual prompts may have introduced information loss
- Small test cohort: With only 690 participants, results may not generalize to larger populations
- Limited prompt engineering exploration: Only tested CoT prompting; other prompt structures were not evaluated

## Confidence
- **High confidence**: GPT-4 (AUC 0.69) outperforms ChatGPT (AUC 0.62) in MI prediction; both underperformed traditional ML models (AUC 0.76-0.79)
- **Medium confidence**: Chain-of-thought prompting failed to improve performance across all models tested
- **Medium confidence**: Fine-tuning on medical corpora provided minimal performance improvement (MedLlama AUC 0.62)

## Next Checks
1. Validate data transformation fidelity: Compare LLM predictions using raw tabular inputs (via embeddings) versus textual descriptions to quantify information loss during transformation
2. Test hybrid reasoning approaches: Implement explicit causal reasoning prompts that ask models to justify risk factor contributions rather than using standard CoT, and measure performance changes
3. Scale validation: Replicate the study with a larger, independent cardiovascular cohort (e.g., Framingham Heart Study) to assess whether performance patterns hold at scale