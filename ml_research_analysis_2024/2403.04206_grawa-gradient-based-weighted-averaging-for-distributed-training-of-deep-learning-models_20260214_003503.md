---
ver: rpa2
title: 'GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep
  Learning Models'
arxiv_id: '2403.04206'
source_url: https://arxiv.org/abs/2403.04206
tags:
- workers
- distributed
- learning
- training
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of distributed training of deep
  learning models in time-constrained environments, focusing on achieving faster convergence
  and better generalization. The core method idea is GRAWA (Gradient-based Weighted
  Averaging), which periodically pulls workers towards a center variable computed
  as a weighted average of workers, with weights inversely proportional to the gradient
  norms of the workers to prioritize flat regions in the optimization landscape.
---

# GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models

## Quick Facts
- arXiv ID: 2403.04206
- Source URL: https://arxiv.org/abs/2403.04206
- Authors: Tolga Dimlioglu; Anna Choromanska
- Reference count: 40
- One-line primary result: GRAWA algorithms achieve faster convergence and better generalization than baselines by prioritizing flat minima through gradient-norm-based weighting

## Executive Summary
This paper introduces GRAWA (Gradient-based Weighted Averaging), a novel approach for distributed training of deep learning models that achieves faster convergence and better generalization by prioritizing flat regions in the optimization landscape. The method assigns weights inversely proportional to the gradient norms of workers, ensuring that workers in flatter regions have more influence on the center model. Two asynchronous variants, MGRAWA (Model-level) and LGRAWA (Layer-level), are developed with theoretical convergence guarantees for both convex and non-convex settings. Extensive experiments demonstrate that GRAWA outperforms state-of-the-art baselines like EASGD, LSGD, DP+SGD, and DP+SAM in terms of convergence speed, generalization ability, and communication overhead.

## Method Summary
GRAWA operates in a distributed data parallel setting where multiple workers train on disjoint data shards and periodically synchronize via weighted averaging. The core innovation is computing weights inversely proportional to the gradient norms of workers, where smaller norms indicate flatter regions of the loss landscape. The center model is constructed as a weighted average of worker models using these weights. Two variants are proposed: MGRAWA applies weighting at the model level, while LGRAWA applies it at the layer level to adapt to different learning rates across layers. A proximity search mechanism softly pulls workers toward the center model between communication rounds to mitigate staleness. The method includes theoretical convergence guarantees for both convex and non-convex settings, with bounds on the average squared gradient norm across workers.

## Key Results
- LGRAWA achieved 8.93% test error on CIFAR-10 with ResNet-20 and 4 workers, compared to 9.67% for DP+SGD
- GRAWA algorithms require less frequent communication and fewer distributed updates than state-of-the-art baselines
- Theoretical convergence guarantees proven for both convex and non-convex settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning higher weights to workers with smaller gradient norms pushes the center model toward flatter regions of the loss landscape.
- Mechanism: The algorithm computes weights inversely proportional to the Euclidean norm of accumulated gradients per worker. Smaller norms indicate the worker is in a flatter valley, so it receives higher weight in the weighted average that defines the center model.
- Core assumption: Gradient norm is a reliable proxy for flatness of the loss valley, and flatter valleys generalize better.
- Evidence anchors:
  - [abstract]: "weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized."
  - [section 4.2]: "If the Euclidean norm of the gradient vector is high, it implies that the point at which the gradient is calculated is still at the steep slope of the loss valley. Otherwise, if the Euclidean norm of the gradient vector is low, the point at which the gradient is calculated is in the flat region of the loss landscape."
  - [corpus]: Weak. The corpus does not discuss gradient-norm-based weighting or flatness-seeking mechanisms.
- Break condition: If gradient norms do not correlate with flatness (e.g., due to pathological loss surfaces), the weighting scheme may misdirect the center model.

### Mechanism 2
- Claim: Layer-wise weighting (LGRAWA) adapts to different learning rates across layers, prioritizing mature layers.
- Mechanism: For each layer, the algorithm assigns weights inversely proportional to that layer's accumulated gradient norm across workers. Layers with smaller norms are considered "mature" and are granted larger weights, influencing the center model more strongly.
- Core assumption: Neural network layers learn at different rates; mature layers should guide the center model more than still-learning layers.
- Evidence anchors:
  - [abstract]: "LGRAWA (Layer-level Gradient-based Weighted Averaging)...prioritizes model layers appropriately, when computing worker averages and model updates, to take advantage of the layers that learn faster than the others."
  - [section 5.2]: "a smaller gradient norm means that the layer requires less correction...mature layers are granted larger weights."
  - [corpus]: Weak. No corpus papers describe layer-specific gradient norm weighting in distributed training.
- Break condition: If all layers learn at similar rates or if gradient norms are noisy, layer-wise weighting may provide little benefit over model-level weighting.

### Mechanism 3
- Claim: The proximity search mechanism mitigates staleness caused by infrequent center model updates.
- Mechanism: Between communication rounds, workers are softly pulled toward the most recent center model using a proximity coefficient scaled by the inverse of the communication period. This keeps workers from drifting too far and encourages them to stay near the flatter region identified by the center.
- Core assumption: Even with infrequent updates, a small continuous pull toward the last center helps maintain convergence toward flatter valleys.
- Evidence anchors:
  - [section 5.3]: "inspired by a similar mechanism in (Teng et al., 2019), we apply an additional force in the local optimization phase that pulls the workers toward the previously calculated center model...we refer to this additional force as the proximity search mechanism."
  - [corpus]: Weak. The corpus does not discuss proximity search mechanisms in distributed training.
- Break condition: If the communication period is too long, the outdated center may mislead workers; if too short, the benefit diminishes.

## Foundational Learning

- Concept: Distributed data parallel training and parameter-sharing schemes.
  - Why needed here: GRAWA operates in a distributed data parallel setting where multiple workers train on disjoint data shards and periodically synchronize via weighted averaging.
  - Quick check question: In data parallel training, do workers share gradients or parameters during communication?

- Concept: Gradient norm as a proxy for flatness of loss landscape.
  - Why needed here: The weighting scheme in GRAWA depends on gradient norms; understanding this link is crucial to grasping why the algorithm seeks flat minima.
  - Quick check question: What does a small gradient norm indicate about the local geometry of the loss surface?

- Concept: Convergence guarantees for non-convex optimization in distributed settings.
  - Why needed here: GRAWA's theoretical analysis relies on bounding the average squared gradient norm across workers; this ensures convergence even when the loss is non-convex.
  - Quick check question: In non-convex optimization, what quantity is typically bounded to prove convergence?

## Architecture Onboarding

- Component map:
  Local optimizer -> Gradient accumulation module -> Weight calculation module -> Center model construction -> Distributed communication scheduler -> Proximity search controller

- Critical path:
  1. Local SGD step
  2. Gradient accumulation (full or layer-wise)
  3. Weight calculation when communication triggers
  4. Center model construction
  5. Parameter update toward center
  6. Proximity search step (between updates)

- Design tradeoffs:
  - Frequent communication → lower staleness but higher overhead
  - Infrequent communication → lower overhead but requires stronger proximity search
  - Model-level vs layer-level weighting → simplicity vs adaptability

- Failure signatures:
  - Divergence: weights become zero or unstable
  - Slow convergence: proximity coefficient too small
  - Suboptimal flatness: gradient norm not correlated with flatness

- First 3 experiments:
  1. Single-worker sanity check: run GRAWA-style weighting on one worker and verify it moves toward flatter regions.
  2. Toy 2D Vincent function: confirm MGRAWA/LGRAWA recover flatter valleys than EASGD/LSGD.
  3. CIFAR-10 ResNet-20 with 4 workers: compare test error and flatness (Hessian Frobenius norm) against EASGD baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GRAWA algorithms scale with an increasing number of workers beyond 12?
- Basis in paper: [explicit] The paper mentions an ablation study on scalability with 4, 8, and 12 workers, but notes that further investigation is needed to determine if benefits continue with more workers.
- Why unresolved: The experiments only tested up to 12 workers, leaving uncertainty about performance in larger distributed environments.
- What evidence would resolve it: Experimental results testing GRAWA algorithms with 16, 32, or more workers to observe performance trends and potential limitations.

### Open Question 2
- Question: How would the GRAWA algorithms perform when combined with other optimization techniques like learning rate scheduling or adaptive optimizers?
- Basis in paper: [inferred] The paper uses standard SGD with momentum and Nesterov acceleration, but doesn't explore combinations with other optimization strategies that are common in modern deep learning.
- Why unresolved: The experiments only tested GRAWA with basic SGD variants, leaving open questions about compatibility with more advanced optimization techniques.
- What evidence would resolve it: Comparative experiments testing GRAWA algorithms with various learning rate schedules (cosine annealing, step decay) and adaptive optimizers (Adam, AdaBelief) against standard configurations.

### Open Question 3
- Question: What is the theoretical relationship between the gradient norm-based weighting scheme and other flatness measures like Hessian-based sharpness?
- Basis in paper: [explicit] The paper acknowledges that gradient norm is used as a computationally efficient proxy for flatness, but doesn't provide theoretical justification for why this correlates with other established flatness metrics.
- Why unresolved: While empirical evidence shows correlation, there's no theoretical analysis explaining the relationship between gradient norm and other sharpness measures.
- What evidence would resolve it: Mathematical analysis proving bounds or relationships between gradient norm-based weighting and Hessian-based sharpness measures under various assumptions about the loss landscape.

## Limitations
- The evidence for the core flatness-seeking mechanism is primarily derived from the authors' own experiments rather than independent validation.
- The mechanistic claim that inverse gradient norm weighting specifically recovers flatter minima remains partially speculative without direct measurement of flatness metrics across all experiments.
- The paper provides strong empirical evidence for convergence speed and generalization improvements, but the theoretical relationship between gradient norm and flatness is not fully established.

## Confidence

**Confidence labels:**
- Convergence speed improvements: High
- Generalization benefits: High
- Flatness-seeking mechanism: Medium
- Layer-wise weighting advantages: Medium

## Next Checks

1. Measure and compare Hessian Frobenius norms (or another flatness proxy) at final solutions across all methods in the CIFAR-10 experiments to directly verify the flatness-seeking claim.

2. Conduct ablation studies on the proximity search mechanism by training with and without it across different communication periods to quantify its contribution to stability.

3. Test GRAWA on additional architectures (e.g., Transformers) and tasks (e.g., language modeling) to assess generalizability beyond vision tasks.