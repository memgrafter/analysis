---
ver: rpa2
title: Training Agents with Weakly Supervised Feedback from Large Language Models
arxiv_id: '2411.19547'
source_url: https://arxiv.org/abs/2411.19547
tags:
- agents
- arxiv
- trajectories
- critic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework for training LLM-based agents
  without relying on expert trajectories or definitive environmental feedback. The
  core idea is to use a critic LLM to provide weak supervision by selecting high-quality
  trajectories from agent-environment interactions, which are then used to iteratively
  update the agent.
---

# Training Agents with Weakly Supervised Feedback from Large Language Models

## Quick Facts
- arXiv ID: 2411.19547
- Source URL: https://arxiv.org/abs/2411.19547
- Reference count: 5
- 13B model achieved 49.5% accuracy on API-bank dataset, comparable to GPT-4's 51.6%

## Executive Summary
This paper introduces a novel framework for training LLM-based agents using weak supervision from critic models rather than relying on expert trajectories or definitive environmental feedback. The approach uses a critic LLM to select high-quality trajectories from agent-environment interactions, which are then used to iteratively update the agent. The method demonstrates that much smaller models (6B-13B) can achieve competitive performance with GPT-4 on the API-bank dataset while avoiding the need for expensive expert demonstrations.

## Method Summary
The framework trains LLM-based agents by iteratively refining their behavior through weak supervision from a critic model. Instead of requiring expert trajectories or direct environmental rewards, the approach uses a critic LLM to evaluate and select high-quality trajectories generated by the agent during environment interactions. These selected trajectories are then used as training data to update the agent model. This creates a self-improving loop where the agent generates data, the critic evaluates it, and the agent learns from the critic's selections. The method was evaluated on the API-bank dataset, showing consistent improvement across training iterations and achieving competitive results with much smaller models compared to GPT-4.

## Key Results
- 6B model achieved 47.5% accuracy on API-bank dataset
- 13B model achieved 49.5% accuracy, comparable to GPT-4's 51.6%
- Consistent improvement across training iterations
- Significant gains over existing open-source baselines

## Why This Works (Mechanism)
The method leverages the reasoning capabilities of LLMs to provide nuanced feedback on agent behavior without requiring explicit reward functions or expert demonstrations. By using a critic LLM to evaluate trajectories, the framework can capture complex task-relevant feedback that traditional reinforcement learning approaches might miss. The iterative refinement process allows the agent to gradually improve by learning from trajectories that the critic deems high-quality, effectively bootstrapping the learning process from weak signals.

## Foundational Learning
- **Critic-based supervision**: Using a separate model to evaluate and select training data - needed to provide nuanced feedback without explicit rewards; quick check: validate critic's selection consistency
- **Iterative refinement**: Repeated cycles of generation, evaluation, and training - needed for gradual improvement from weak signals; quick check: monitor performance gains per iteration
- **Trajectory-based learning**: Learning from complete interaction sequences rather than individual actions - needed to capture context and decision-making patterns; quick check: ensure trajectories maintain task coherence

## Architecture Onboarding
- **Component map**: Agent LLM -> Environment -> Critic LLM -> Selected trajectories -> Agent update
- **Critical path**: The agent generates trajectories, the critic evaluates them, and the selected trajectories are used to update the agent - this feedback loop is the core mechanism
- **Design tradeoffs**: The approach trades computational efficiency (using smaller models) for the need to maintain a separate critic model and perform multiple training iterations
- **Failure signatures**: Poor critic selection quality can lead to agent learning from suboptimal trajectories; insufficient iterations may result in incomplete training
- **First experiments**: 1) Validate critic selection quality on known good/bad trajectories, 2) Test single iteration improvement on a simple task, 3) Compare training dynamics with different critic model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on critic LLM may introduce bias and may not generalize to domains where high-quality trajectories are harder to distinguish
- Evaluation limited to single banking dataset raises questions about domain generalization
- Scalability with respect to critic model size and number of iterations needed for convergence remains unclear

## Confidence
- High confidence: The core methodology and iterative training framework are well-defined and implemented
- Medium confidence: The comparative results against GPT-4 and open-source baselines are valid within the tested domain
- Low confidence: Generalization claims to other domains and tasks beyond the API-bank dataset

## Next Checks
1. Test the framework on multiple diverse datasets beyond banking to assess domain generalization and identify potential limitations in different task types
2. Conduct ablation studies varying critic model size and number of training iterations to determine optimal configurations and scalability boundaries
3. Implement a human evaluation component to assess whether the weak supervision from the critic LLM captures meaningful task-relevant feedback or introduces domain-specific biases