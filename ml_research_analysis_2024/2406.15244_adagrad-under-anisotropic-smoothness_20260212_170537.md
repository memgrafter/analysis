---
ver: rpa2
title: AdaGrad under Anisotropic Smoothness
arxiv_id: '2406.15244'
source_url: https://arxiv.org/abs/2406.15244
tags:
- adagrad
- arxiv
- gradient
- convergence
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the theoretical gap between the practical
  success of adaptive gradient methods like Adagrad and their limited theoretical
  understanding in the large-batch smooth optimization setting. The authors propose
  novel anisotropic smoothness and noise assumptions to better capture the problem
  structure that benefits adaptive methods.
---

# AdaGrad under Anisotropic Smoothness

## Quick Facts
- arXiv ID: 2406.15244
- Source URL: https://arxiv.org/abs/2406.15244
- Reference count: 40
- Key outcome: AdaGrad achieves improved dimensional dependence under anisotropic smoothness and noise assumptions compared to uniform step-size methods

## Executive Summary
This paper addresses the theoretical gap between AdaGrad's practical success and its limited theoretical understanding in large-batch smooth optimization settings. The authors propose novel anisotropic smoothness and noise assumptions to capture problem structures that benefit adaptive methods. Under these assumptions, they provide new convergence analysis showing AdaGrad achieves better dimensional dependence than uniform-step methods like SGD and AdaGrad-Norm. The analysis extends to generalized anisotropic smoothness assumptions and is supported by experiments on logistic regression and instruction following tasks.

## Method Summary
The paper analyzes AdaGrad's convergence under anisotropic smoothness and noise assumptions. It introduces novel generalized forms of anisotropic smoothness extending results to more practical settings. The method involves AdaGrad with diagonal preconditioning, where the preconditioner adapts coordinate-wise to historical gradient magnitudes. Convergence guarantees are established for both convex and non-convex optimization, with experiments validating the theoretical claims on logistic regression tasks and instruction-following fine-tuning with GPT-2.

## Key Results
- AdaGrad achieves faster convergence under anisotropic smoothness conditions with better dimensional dependence
- Convergence rate improves from O(1/√T) to O(∥σ∥₁/√M T) with large batch sizes
- Generalized anisotropic (L₀, L₁)-smoothness extends theoretical guarantees to more practical settings
- Experiments on logistic regression and instruction following tasks support theoretical claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaGrad adapts step sizes coordinate-wise to match anisotropic smoothness, leading to faster convergence when gradients have varying magnitudes across coordinates.
- Mechanism: The diagonal preconditioner Λ_t = diag(√v_t) scales each coordinate inversely with historical gradient magnitude, implementing coordinate-specific smoothness adaptation.
- Core assumption: Loss function exhibits anisotropic smoothness with different curvature characteristics across coordinates, and gradient noise is anisotropic with coordinate-specific variance.
- Evidence anchors: [abstract] "Under anisotropic smoothness and noise conditions, AdaGrad can achieve faster convergence guarantees in terms of better dimensional dependence than algorithms with uniform step sizes"; [section] "When the loss function f(·) is twice-differentiable, Assumption 3.3 is equivalent to ∇²f ⪯ diag(L)"
- Break condition: When problem becomes isotropic or gradient noise is uniform across coordinates.

### Mechanism 2
- Claim: AdaGrad's convergence rate improves from O(1/√T) to O(∥σ∥₁/√M T) with large batch sizes due to variance scaling with ∥σ∥₁ instead of ∥σ∥₂.
- Mechanism: Per-coordinate gradient accumulation v_t reduces impact of gradient noise in directions with small historical gradients.
- Core assumption: Gradient noise is anisotropic such that ∥σ∥₁ ≪ √d · ∥σ∥₂, occurring when noise is concentrated in few coordinates.
- Evidence anchors: [abstract] "under anisotropic smoothness and noise conditions, AdaGrad can achieve faster convergence guarantees in terms of better dimensional dependence"; [section] "when M is small such that the variance term is dominant"
- Break condition: When gradient noise becomes isotropic or batch size is too small.

### Mechanism 3
- Claim: AdaGrad's convergence under generalized anisotropic (L₀, L₁)-smoothness provides superior performance compared to SGD with clipping.
- Mechanism: Adaptive preconditioning naturally handles gradient-dependent smoothness without explicit clipping.
- Core assumption: Problem satisfies anisotropic (L₀, L₁)-smoothness where local smoothness varies coordinate-wise and depends on gradient magnitude.
- Evidence anchors: [abstract] "We additionally introduce a novel generalized form of anisotropic smoothness, extending these results to more practical settings"; [section] "Assumption 5.1 offers several evident advantages"
- Break condition: When generalized smoothness assumption fails to capture actual problem structure.

## Foundational Learning

- Concept: Anisotropic smoothness and noise assumptions
  - Why needed here: Foundation for understanding when and why AdaGrad outperforms uniform-step methods by capturing coordinate-wise variation in function curvature and gradient noise.
  - Quick check question: Can you explain the difference between isotropic smoothness (L-smoothness) and anisotropic smoothness, and why this matters for adaptive methods?

- Concept: Coordinate-wise preconditioning and adaptive learning rates
  - Why needed here: Understanding how diagonal preconditioner Λ_t = diag(√v_t) works and its effectiveness requires grasping connection between historical gradient information and local problem geometry.
  - Quick check question: How does update v_t = v_{t-1} + (g_t ⊙ g_t) create coordinate-specific learning rate, and what's intuition behind this accumulation?

- Concept: Convergence analysis techniques for non-convex optimization
  - Why needed here: Proof techniques used (auxiliary sequences, noise variance bounds, smoothness adaptation) are sophisticated and require understanding of modern optimization theory.
  - Quick check question: Can you explain why paper introduces auxiliary sequence ˜Λ_t and how it helps avoid dealing with coupled Λ_t and g_t terms directly?

## Architecture Onboarding

- Component map: AdaGrad with diagonal preconditioning (Λ_t) -> Gradient accumulation (v_t) -> Adaptive step size computation -> Update rule w_{t+1} = w_t - η_t Λ_t^{-1} g_t

- Critical path:
  1. Initialize v_{-1} = ϵ·1_d and w_0
  2. At each iteration t: compute g_t from batch, update v_t = v_{t-1} + (g_t ⊙ g_t)
  3. Compute Λ_t = diag(√v_t) and set step size η_t
  4. Update w_{t+1} = w_t - η_t Λ_t^{-1} g_t (with optional projection)
  5. Convergence analysis relies on bounding gradient norm ∥∇f(w_t)∥₁

- Design tradeoffs:
  - Memory vs. adaptivity: Storing v_t requires O(d) memory but enables per-coordinate adaptation
  - Stability vs. responsiveness: ϵ term prevents division by zero but may slow adaptation to changing curvature
  - Batch size vs. convergence: Larger batches reduce variance but may increase bias; AdaGrad handles this tradeoff better than SGD

- Failure signatures:
  - Poor performance when problem is nearly isotropic (all coordinates have similar curvature)
  - Slow convergence when gradient noise is uniform across coordinates
  - Numerical instability when gradients become very small (mitigated by ϵ but still possible)

- First 3 experiments:
  1. Compare AdaGrad vs. SGD on logistic regression with sparse features (e.g., a4a dataset) to verify variance reduction advantage
  2. Test convergence under varying batch sizes on convex problem to observe ∥σ∥₁ vs ∥σ∥₂ scaling
  3. Validate generalized smoothness assumption on neural network by checking L₀, L₁ relationship empirically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the anisotropic smoothness assumption perform when applied to more complex neural network architectures beyond GPT-2, such as transformers with attention mechanisms?
- Basis in paper: Paper verifies assumption on GPT-2 with Alpaca dataset but does not extend to other architectures.
- Why unresolved: Different architectures may exhibit different Hessian spectrum distributions and local smoothness properties.
- What evidence would resolve it: Experiments showing validity of anisotropic smoothness assumption across various architectures (BERT, ViT, ResNets) with different datasets.

### Open Question 2
- Question: What is the precise relationship between anisotropic (L₀, L₁)-smoothness assumption and original (L₀, L₁)-smoothness assumption in terms of practical training scenarios?
- Basis in paper: Paper notes relationship between these assumptions is "generally consistent" but acknowledges "things become more complicated."
- Why unresolved: Paper does not provide detailed analysis of when one assumption might be more appropriate than other.
- What evidence would resolve it: Empirical studies comparing convergence behavior of Adagrad under both assumptions across different tasks and datasets.

### Open Question 3
- Question: How does convergence of Adagrad under anisotropic smoothness assumptions compare to other adaptive methods like Adam or RMSProp in practice?
- Basis in paper: Paper focuses on Adagrad and does not compare convergence guarantees to other adaptive methods under same assumptions.
- Why unresolved: Theoretical advantages of Adagrad under anisotropic assumptions may not translate to practical superiority over other adaptive methods.
- What evidence would resolve it: Comparative experiments of Adagrad, Adam, and RMSProp under various anisotropic smoothness conditions, measuring both convergence speed and generalization performance.

## Limitations
- Anisotropic smoothness and noise assumptions lack comprehensive validation across diverse problem domains
- Empirical validation is limited to specific datasets and models (logistic regression datasets, single GPT-2 instruction-following task)
- Generalized anisotropic smoothness assumption (L₀, L₁)-smoothness lacks comprehensive empirical validation across different model architectures

## Confidence

**High Confidence**: Mathematical proofs showing AdaGrad's convergence under stated anisotropic assumptions are sound and follow standard optimization analysis techniques. Dimensional dependence improvements (from O(∥σ∥₂/√T) to O(∥σ∥₁/√T)) are clearly established.

**Medium Confidence**: Empirical validation, while supporting theoretical claims, is limited to specific datasets and models. Logistic regression experiments use relatively standard datasets, and non-convex results are from single instruction-following task with GPT-2.

**Low Confidence**: Generalized anisotropic smoothness assumption (L₀, L₁)-smoothness is introduced but lacks comprehensive empirical validation across different model architectures and problem types.

## Next Checks

1. **Cross-domain validation**: Test AdaGrad under anisotropic smoothness assumptions on additional non-convex problems including image classification (CNNs on CIFAR/ImageNet) and language modeling (transformers on diverse NLP tasks) to verify generalizability of assumptions.

2. **Assumption verification**: Systematically measure and visualize anisotropic smoothness and noise properties across different coordinates for multiple deep learning models and datasets to quantify how well assumptions hold in practice.

3. **Alternative method comparison**: Compare AdaGrad against other adaptive methods (Adam, RMSProp) and newly proposed methods like AdaGrad with momentum under same anisotropic smoothness framework to determine if theoretical advantages translate to practical performance gains.