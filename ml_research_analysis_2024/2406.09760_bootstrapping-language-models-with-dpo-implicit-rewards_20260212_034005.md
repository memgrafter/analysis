---
ver: rpa2
title: Bootstrapping Language Models with DPO Implicit Rewards
arxiv_id: '2406.09760'
source_url: https://arxiv.org/abs/2406.09760
tags:
- reward
- preference
- dataset
- implicit
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DICE, a method that uses the implicit reward
  model from DPO to iteratively align LLMs with human preferences without external
  feedback. The key idea is to generate preference datasets from a DPO-tuned model's
  own responses using length-regularized implicit rewards, then fine-tune further
  with DPO.
---

# Bootstrapping Language Models with DPO Implicit Rewards

## Quick Facts
- arXiv ID: 2406.09760
- Source URL: https://arxiv.org/abs/2406.09760
- Authors: Changyu Chen; Zichen Liu; Chao Du; Tianyu Pang; Qian Liu; Arunesh Sinha; Pradeep Varakantham; Min Lin
- Reference count: 29
- Primary result: >8% improvement in AlpacaEval 2 length-controlled win rate across different base models, including outperforming Gemini Pro with an 8B model without extra data or reward models

## Executive Summary
This paper introduces DICE, a method that uses the implicit reward model from DPO to iteratively align LLMs with human preferences without external feedback. The key innovation is generating preference datasets from a DPO-tuned model's own responses using length-regularized implicit rewards, then fine-tuning further with DPO. DICE combines this with experience replay mixing generated and offline data to prevent catastrophic forgetting. Experiments show DICE achieves consistent improvements across different base models and benchmarks, demonstrating the viability of self-bootstrapping for language model alignment.

## Method Summary
DICE leverages the implicit reward model inherent to DPO to create a self-bootstrapping alignment loop. Starting from a base DPO-tuned model, it generates multiple responses per prompt, ranks them using the implicit reward function, and constructs preference pairs. To address length bias, it applies length regularization to the implicit rewards before selecting preference pairs. The method then mixes these generated preference pairs with the original offline dataset using experience replay, and fine-tunes the model with DPO on this mixed dataset. This process can be iterated, with each iteration producing a new implicit reward model for the next round.

## Key Results
- DICE achieves >8% improvement in AlpacaEval 2 length-controlled win rate across different base models
- Outperforms Gemini Pro with an 8B model without extra data or reward models
- Shows consistent improvements across both AlpacaEval 2 and Arena-Hard benchmarks
- Maintains performance gains when iterated for multiple rounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO implicitly generates a reward model that can be used to score its own outputs and bootstrap further alignment
- Mechanism: After DPO training, the optimal policy satisfies a closed-form relation where rewards are expressed as the log-ratio between the policy and a reference model (r(x, y) = β log πθ(y|x) / πref(y|x)). This implicit reward function can be applied to any generated output to rank them without requiring a separate reward model
- Core assumption: The implicit reward accurately reflects human preferences and can be trusted to rank model-generated responses
- Evidence anchors: [abstract] "DPO, once trained, implicitly specifies a reward model." [section] "Theorem 1 in Rafailov et al. (2024b) demonstrates that this parameterization of a reward model is indeed valid without loss of generality."

### Mechanism 2
- Claim: Length-regularized reward shaping mitigates the length exploitation bias that occurs when DPO is used to bootstrap itself
- Mechanism: The vanilla implicit reward favors longer responses because human preference data tends to prefer verbose answers. By adding a penalty term α|y| to the implicit reward (rLR = β log πθ(y|x) / πref(y|x) - α|y|), the method constructs a length-unbiased preference dataset
- Core assumption: The length bias in human preference data is the primary source of length exploitation in iterative DPO
- Evidence anchors: [section] "We resort to reward shaping (Sutton & Bathe, 2018) for this purpose. In particular, we introduce a length-regularized (LR) reward shaping term in the implicit reward that penalizes the length of the response." [section] "With LR reward shaping defined by Eq. (5), the length bias is mitigated and the length difference becomes more evenly distributed."

### Mechanism 3
- Claim: Experience replay with a mixture of generated and offline data prevents catastrophic forgetting during iterative self-alignment
- Mechanism: By mixing the generated dataset D(α⋆) with the original offline preference dataset Doffline (pDt = (1-γ)pD(α⋆) + γpDoffline), the method maintains knowledge from the initial DPO training while incorporating improvements from the implicit reward bootstrapping
- Core assumption: The implicit reward model is imperfect and can introduce noise when used exclusively for preference labeling
- Evidence anchors: [section] "Solely relying on the implicit reward model may result in forgetting the knowledge inbuilt in the initial policy at the first DPO stage." [section] "Combining the two can make for a good balance."

## Foundational Learning

- Concept: Direct Preference Optimization (DPO) and its implicit reward model
  - Why needed here: Understanding how DPO creates an implicit reward model is essential for grasping the core bootstrapping mechanism
  - Quick check question: What is the mathematical form of the implicit reward in DPO, and how does it differ from an explicit reward model?

- Concept: Reinforcement learning from human feedback (RLHF) vs DPO
  - Why needed here: The paper positions DPO as a simplification of RLHF, and understanding this relationship helps explain why the implicit reward approach works
  - Quick check question: What are the key differences between RLHF and DPO in terms of implementation complexity and reward model requirements?

- Concept: Length bias in preference data and reward shaping
  - Why needed here: The length regularization technique is critical for preventing degradation during iterative bootstrapping
  - Quick check question: Why do human preference datasets typically exhibit length bias, and how does reward shaping address this?

## Architecture Onboarding

- Component map: Base DPO-tuned model → Generate responses → Apply implicit rewards → Select preferences → Optimize α → Mix with offline data → DPO fine-tune → New policy

- Critical path: Base model → Generate responses → Apply implicit rewards → Select preferences → Optimize α → Mix with offline data → DPO fine-tune → New policy

- Design tradeoffs:
  - Pure on-policy vs offline data: Pure on-policy risks catastrophic forgetting; pure offline risks overfitting
  - Implicit reward quality: Higher quality rewards lead to better bootstrapping but may still diverge from true human preferences
  - Length regularization strength: Too weak allows exploitation; too strong degrades response quality

- Failure signatures:
  - Performance degradation over iterations (catastrophic forgetting or reward model collapse)
  - Increasing response lengths without quality improvement (insufficient length regularization)
  - Model collapse to trivial solutions (implicit rewards becoming uninformative)

- First 3 experiments:
  1. Test implicit reward quality by comparing rankings with human judgments on a small validation set
  2. Tune length regularization α on a small dataset to verify it reduces length bias without quality loss
  3. Run one iteration of DICE with different γ values to identify the optimal experience replay ratio

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on the implicit reward model maintaining alignment with human preferences over multiple iterations, which is not empirically validated beyond two rounds
- Method's effectiveness depends critically on the quality of the initial DPO model and the assumption that the implicit reward function remains stable when used to evaluate its own outputs
- Length regularization parameter α is tuned on validation data, but robustness across different domains is unclear
- Experience replay mixing ratio γ is fixed at 0.5, though sensitivity to this hyperparameter is not thoroughly explored

## Confidence

**High Confidence**: The mathematical foundation of DPO's implicit reward model is well-established (Theorem 1 in Rafailov et al., 2024b). The length regularization mechanism is theoretically sound and directly addresses a known bias in preference data.

**Medium Confidence**: The iterative bootstrapping approach with experience replay is conceptually reasonable, but the long-term stability of this approach across multiple iterations has limited empirical validation. The claim that DICE outperforms Gemini Pro with an 8B model is based on limited benchmark comparisons.

**Low Confidence**: The robustness of the length regularization optimization procedure across different datasets and domains has not been demonstrated. The method's behavior when the implicit reward model begins to diverge from true human preferences is not well-characterized.

## Next Checks

1. **Multi-iteration stability test**: Run DICE for 4-5 iterations instead of 2 and monitor performance curves to identify potential reward model collapse or catastrophic forgetting patterns.

2. **Human preference validation**: Collect human judgments on a subset of model-generated responses before and after DICE training to verify that the implicit reward rankings align with actual human preferences, not just benchmark scores.

3. **Cross-dataset generalization**: Apply DICE to preference datasets from different domains (e.g., technical QA, creative writing) to assess whether the method's performance gains transfer beyond the AlpacaEval-style tasks.