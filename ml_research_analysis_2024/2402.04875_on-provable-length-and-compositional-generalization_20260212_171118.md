---
ver: rpa2
title: On Provable Length and Compositional Generalization
arxiv_id: '2402.04875'
source_url: https://arxiv.org/abs/2402.04875
tags:
- generalization
- length
- compositional
- assumption
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first provable guarantees on length and
  compositional generalization for common sequence-to-sequence models including deep
  sets, transformers, state space models, and recurrent neural networks. The authors
  show that limited capacity versions of these architectures achieve both forms of
  generalization provided the training distribution is sufficiently diverse.
---

# On Provable Length and Compositional Generalization

## Quick Facts
- arXiv ID: 2402.04875
- Source URL: https://arxiv.org/abs/2402.04875
- Authors: Kartik Ahuja; Amin Mansouri
- Reference count: 40
- Primary result: First provable guarantees on length and compositional generalization for common sequence-to-sequence models including deep sets, transformers, state space models, and RNNs.

## Executive Summary
This paper establishes the first provable guarantees on length and compositional generalization for sequence-to-sequence models including deep sets, transformers, state space models, and RNNs. The authors show that limited capacity versions of these architectures achieve both forms of generalization when trained on sufficiently diverse data, while higher capacity models require chain-of-thought supervision. A key insight is that learned representations are linearly related to true labeling function representations (linear identification), which enables generalization to sequences longer than those seen during training. Experimental results validate the theoretical predictions, showing small test losses (≈10⁻⁴-10⁻⁷) at all sequence lengths and strong linear relationships (R²≈0.96-0.99) between learned and true hidden representations.

## Method Summary
The paper studies sequence-to-sequence models trained via expected risk minimization on diverse training distributions. Four architectures are analyzed: deep sets with sum/mean pooling, transformers with sigmoid attention, state space models, and RNNs. Training data consists of sequences sampled from continuous distributions with labels generated by labeling functions from the same hypothesis class. Models are trained using AdamW with learning rate 10⁻³, weight decay 0.01, for 100 epochs with batch size 256. The key theoretical insight is that when models minimize expected risk on diverse data, learned representations become linearly related to true labeling function representations, enabling generalization. Chain-of-thought supervision provides intermediate computations as privileged information for higher capacity models.

## Key Results
- Limited capacity models achieve both length and compositional generalization when trained on sufficiently diverse data
- Learned representations are linearly related to true labeling function representations (R²≈0.96-0.99)
- Chain-of-thought supervision enables length generalization in higher capacity models
- Small test losses (≈10⁻⁴-10⁻⁷) achieved at all sequence lengths up to 100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Limited capacity models achieve both length and compositional generalization when trained on sufficiently diverse data.
- Mechanism: The learned representations are linearly related to the true labeling function representations (linear identification). This occurs because the model minimizes expected risk on diverse training data, forcing the learned model to match the true labeling function everywhere in the support.
- Core assumption: Realizability holds (true labeling function is in the hypothesis class) and the training distribution is sufficiently diverse.
- Evidence anchors:
  - [abstract]: "limited capacity versions of these architectures achieve both length and compositional generalization provided the training distribution is sufficiently diverse."
  - [section]: "From equation(11) it follows that the hidden representation learned by the model is a linear transform of the true hidden representation, i.e., it achieves linear identificationψ = A−1Bϕ"
  - [corpus]: Weak evidence - corpus papers discuss compositional generalization but don't specifically address the linear identification mechanism.
- Break condition: If the hypothesis class has too much capacity, there exist solutions that match training data but fail on out-of-distribution sequences.

### Mechanism 2
- Claim: Chain-of-thought supervision enables length generalization in higher capacity models.
- Mechanism: CoT provides intermediate computations as privileged information, reducing underspecification. The model learns exact intermediate representations, forcing it to match the true labeling function everywhere.
- Core assumption: CoT data provides accurate intermediate steps that the model can learn from.
- Evidence anchors:
  - [abstract]: "chain-of-thought supervision enables length generalization in higher capacity counterparts"
  - [section]: "Supervision from chain-of-thought data is meant to provide data on the intermediate thoughts/computations that lead to the output"
  - [corpus]: Weak evidence - corpus papers discuss CoT but don't specifically address its role in length generalization for high-capacity models.
- Break condition: If CoT data is inaccurate or incomplete, the model may still fail to generalize.

### Mechanism 3
- Claim: Uniform Lipschitz continuity enables length generalization for general hypothesis classes with bounded complexity.
- Mechanism: If the hypothesis class is Lipschitz continuous with uniform constant and has finite covering number, then a constrained learner that achieves ε-optimality at all training sequence lengths can generalize to all longer sequences.
- Core assumption: The hypothesis class satisfies uniform Lipschitz continuity and has bounded covering number.
- Evidence anchors:
  - [abstract]: "approximate length generalization is achievable, provided training sequences are sufficiently long"
  - [section]: "Owing to Lipschitzness, if η ≤ ϵ/L, then we obtain anϵ-cover for all{Ht}∞t=1"
  - [corpus]: Weak evidence - corpus papers discuss generalization but don't specifically address uniform Lipschitz continuity for sequence models.
- Break condition: If the hypothesis class violates uniform Lipschitz continuity or has infinite covering number, the result doesn't hold.

## Foundational Learning

- Concept: Expected Risk Minimization
  - Why needed here: The paper studies when models trained via standard risk minimization achieve OOD generalization.
  - Quick check question: What is the difference between empirical risk and expected risk in this context?

- Concept: Linear Identification
  - Why needed here: The paper shows that learned representations are linearly related to true data-generating representations, which is key to proving generalization.
  - Quick check question: How does linear identification differ from standard representation learning?

- Concept: Lipschitz Continuity
  - Why needed here: The paper uses uniform Lipschitz continuity to bound the complexity of hypothesis classes for general results.
  - Quick check question: What does it mean for a hypothesis class to be uniformly Lipschitz continuous?

## Architecture Onboarding

- Component map:
  - Deep Sets: Sum/mean pooling of embeddings
  - Transformers: Alternating layers of attention and position-wise non-linearities
  - SSMs: Linear recurrence followed by non-linear projection
  - RNNs: Non-linear recurrence with hidden state dynamics

- Critical path: Training data generation → Model training via ERM → Linear identification proof → Generalization guarantee

- Design tradeoffs:
  - Limited capacity vs high capacity: Limited capacity ensures linear identification but restricts expressiveness; high capacity requires CoT supervision
  - Structural assumptions vs general assumptions: Structural assumptions yield stronger guarantees with less data diversity requirements
  - Training sequence length: Longer training sequences enable generalization to longer sequences

- Failure signatures:
  - High test loss on sequences longer than maximum training length
  - Poor linear relationship (low R²) between learned and true hidden representations
  - Model outputs diverge from true labels as sequence length increases

- First 3 experiments:
  1. Train deep set on sum task with T=10, test on sequences up to length 100, verify small test loss
  2. Train transformer with softmax attention, verify linear relationship between learned and true attention representations
  3. Train SSM with CoT supervision, verify length generalization without structural constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact theoretical bounds on the threshold T₀ for length generalization across different architectures, and how do these bounds scale with sequence length and model capacity?
- Basis in paper: Explicit - The paper discusses T₀ thresholds in Theorems 4-9 and Theorem 10 but notes that T₀ can be very large for finite hypothesis classes and seeks tighter bounds.
- Why unresolved: The authors only provide existence proofs for T₀ thresholds without explicit formulas, leaving the scaling behavior and practical implications unclear.
- What evidence would resolve it: Empirical studies showing T₀ thresholds across various architectures and capacities, combined with theoretical derivations of scaling laws for T₀ as functions of sequence length and model parameters.

### Open Question 2
- Question: How does the choice of learning procedure (e.g., gradient descent variants) affect which solution is selected among multiple optimal solutions in the hypothesis class, and does this impact length generalization guarantees?
- Basis in paper: Explicit - The paper acknowledges that their results focus on all possible solutions to risk minimization, but in practice the optimization procedure may be biased towards a subset of those.
- Why unresolved: The authors do not analyze how different optimization dynamics (SGD, Adam, etc.) interact with the hypothesis class structure to select specific solutions, which could affect generalization.
- What evidence would resolve it: Controlled experiments comparing different optimizers on the same tasks, showing how solution selection impacts generalization, combined with theoretical analysis of optimization bias in these architectures.

### Open Question 3
- Question: Can the theoretical framework be extended to handle non-realizable cases where the labeling function f is not in the hypothesis class H, and what are the implications for length and compositional generalization?
- Basis in paper: Explicit - The authors state that most results rely on the realizability assumption and suggest that a more general framework capturing non-realizable cases would be promising future work.
- Why unresolved: The current proofs leverage realizability to establish linear identification and zero error, but these properties may not hold when f ∉ H, requiring new proof techniques.
- What evidence would resolve it: Theoretical extensions of the current framework to non-realizable settings, possibly using approximation bounds, and empirical validation showing generalization behavior when f ∉ H.

## Limitations

- Guarantees rely heavily on structural assumptions about hypothesis class and realizability of true labeling function
- Chain-of-thought supervision mechanism requires accurate intermediate steps that may not be available for many real-world tasks
- Theoretical framework focuses on worst-case guarantees that may not reflect practical performance

## Confidence

- High Confidence: The structural generalization guarantees for limited capacity models when the true labeling function is realizable
- Medium Confidence: The length generalization results for general hypothesis classes using uniform Lipschitz continuity
- Low Confidence: The chain-of-thought supervision mechanism for enabling length generalization in high-capacity models

## Next Checks

1. **Empirical verification of linear identification**: Train a deep set model on a sum task with varying training set sizes and compute the R² relationship between learned and true hidden representations across different diversity levels.

2. **Testing realizability assumptions**: Evaluate whether models can generalize when trained on architectures that cannot perfectly represent the true labeling function by training a deep set on sum tasks but testing on product tasks.

3. **CoT supervision in practical settings**: Implement the CoT mechanism on a multi-step reasoning task like arithmetic word problems, comparing length generalization with and without CoT supervision on sequences longer than training length.