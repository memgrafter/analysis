---
ver: rpa2
title: 'Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous
  Question Answering'
arxiv_id: '2409.02361'
source_url: https://arxiv.org/abs/2409.02361
tags:
- passages
- retrieval
- question
- diva
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of low-quality retrieval and
  inefficiency in retrieval-augmented generation (RAG) frameworks when handling ambiguous
  questions in QA systems. The authors propose a new framework called DIVA that improves
  RAG performance through two key components: retrieval diversification (RD) and adaptive
  generation (AG).'
---

# Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering

## Quick Facts
- arXiv ID: 2409.02361
- Source URL: https://arxiv.org/abs/2409.02361
- Reference count: 40
- Key outcome: DIVA achieves 42.2 DR score on ASQA vs 40.1 for CRAG and iterative RAG, while being 1.5-3x faster and 1.8x cheaper

## Executive Summary
This paper addresses the challenge of handling ambiguous questions in retrieval-augmented generation (RAG) systems, where traditional approaches suffer from low-quality retrieval and inefficiency. The authors propose DIVA, a framework that improves RAG performance through two key components: retrieval diversification and adaptive generation. DIVA achieves state-of-the-art performance on ASQA and SituatedQA datasets while being significantly faster and cheaper than iterative RAG approaches, demonstrating that comprehensive coverage of ambiguous interpretations can be achieved without the computational overhead of iterative exploration.

## Method Summary
DIVA introduces a two-component framework for ambiguous question answering. First, Retrieval Diversification (RD) infers pseudo-interpretations of ambiguous questions and uses them to guide diverse passage retrieval without iterative exploration. Second, Adaptive Generation (AG) verifies retrieval quality using the pseudo-interpretations and adapts between using retrieved passages or the LLM's internal knowledge based on this assessment. The framework relies on few-shot prompting with GPT-4 or GPT-3.5 and requires no fine-tuning, making it efficient to deploy while maintaining high performance on ambiguous QA tasks.

## Key Results
- DIVA achieves 42.2 DR score on ASQA dataset compared to 40.1 for CRAG and iterative RAG baselines
- DIVA is 1.5-3x faster and 1.8x cheaper than iterative RAG approaches while maintaining superior accuracy
- The framework successfully handles five types of ambiguity (subject, object, predicate, time, location) in ambiguous question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval diversification (RD) improves RAG performance by inferring pseudo-interpretations of ambiguous questions to guide retrieval of diverse passages.
- Mechanism: DIVA uses an LLM to infer multiple interpretations of an ambiguous question, then retrieves passages covering each interpretation without iterative exploration.
- Core assumption: Pseudo-interpretations accurately represent true interpretations.
- Evidence anchors:
  - [abstract]: "RD infers pseudo-interpretations of questions and uses them to retrieve diverse passages without iterative exploration"
  - [section 3.2]: "The key idea of RD is to infer pseudo-interpretations of a question, using them to retrieve a set of passages that broadly cover these interpretations"
- Break condition: If LLM cannot accurately infer true interpretations, RD will fail to retrieve comprehensive passages.

### Mechanism 2
- Claim: Adaptive generation (AG) improves robustness by verifying retrieval quality and switching between RAG and closed-book generation based on quality assessment.
- Mechanism: DIVA verifies if retrieved passages cover all interpretations using pseudo-interpretations, then adapts to use either RAG or closed-book LLM.
- Core assumption: Verification using pseudo-interpretations accurately assesses retrieval quality.
- Evidence anchors:
  - [abstract]: "AG verifies retrieval quality and adapts between using retrieved passages or LLM internal knowledge based on quality assessment"
  - [section 3.3]: "We introduce an adaptive generation (AG) method that dynamically adjust the response generation strategy"
- Break condition: If verification misclassifies passage quality, DIVA will either use irrelevant passages or miss useful information.

### Mechanism 3
- Claim: DIVA achieves superior efficiency by eliminating iterative exploration while maintaining comprehensive coverage.
- Mechanism: Unlike iterative RAG requiring multiple retrieval steps, DIVA performs single-pass retrieval guided by pseudo-interpretations.
- Core assumption: Single-pass retrieval with guided pseudo-interpretations can achieve coverage comparable to iterative methods.
- Evidence anchors:
  - [abstract]: "DIVA achieves state-of-the-art performance...while being 1.5-3x faster and 1.8x cheaper"
  - [section 3.4]: "DIVA...requires substantially less inference time and API costs"
- Break condition: If single-pass retrieval cannot match iterative exploration's coverage, performance will degrade.

## Foundational Learning

- Concept: Question ambiguity types (subject, object, predicate, time, location ambiguity)
  - Why needed here: Understanding different ambiguity types helps in designing proper pseudo-interpretation inference and verification strategies
  - Quick check question: What are the five types of ambiguity that DIVA explicitly addresses?

- Concept: Retrieval quality assessment for ambiguous questions
  - Why needed here: Traditional retrieval verification methods don't account for multiple interpretations required in ambiguous QA
  - Quick check question: How does DIVA's quality assessment differ from standard retrieval verification approaches?

- Concept: In-context learning and few-shot prompting
  - Why needed here: DIVA relies on LLM prompting to infer pseudo-interpretations and verify retrieval without fine-tuning
  - Quick check question: What are the key components of DIVA's prompt design for pseudo-interpretation inference?

## Architecture Onboarding

- Component map: Ambiguous Question → Pseudo-interpretation Inference → Retrieval Diversification → Retrieval Verification → Adaptive Generation → Final Answer
- Critical path: Question → Pseudo-interpretations → Diverse passages → Quality verification → Adaptive response generation
- Design tradeoffs:
  - Efficiency vs coverage: Single-pass RD trades some potential coverage for significant speed gains
  - LLM dependency: Heavy reliance on LLM reasoning capabilities for both pseudo-interpretation and verification
  - Prompt complexity: Requires carefully designed prompts for each module to function correctly
- Failure signatures:
  - Poor pseudo-interpretations: Generated interpretations don't align with true interpretations
  - Inadequate coverage: Retrieved passages miss some interpretations despite RD
  - Misclassification: RV incorrectly labels useful passages as useless or vice versa
  - Over-reliance: System fails when LLM reasoning is insufficient
- First 3 experiments:
  1. Implement pseudo-interpretation inference and test on sample ambiguous questions to verify it generates meaningful interpretations
  2. Add retrieval diversification and measure MRecall@k improvement over vanilla RAG
  3. Integrate verification module and test switching behavior between RAG and closed-book generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DIVA perform when handling ambiguous queries that involve multiple types of ambiguity simultaneously (e.g., both temporal and geographical ambiguity)?
- Basis in paper: The paper discusses different types of ambiguity but does not specifically address how the framework handles questions containing multiple ambiguity types simultaneously.
- Why unresolved: The paper's experimental setup and case studies focus on single-ambiguity examples, leaving the performance on multi-ambiguity queries unexplored.
- What evidence would resolve it: Experiments testing DIVA on queries containing combinations of different ambiguity types, comparing performance metrics against single-ambiguity queries.

### Open Question 2
- Question: What is the impact of using fine-tuned retrievers (e.g., DPR) instead of frozen SentenceBERT encoders on DIVA's retrieval diversification performance?
- Basis in paper: The paper mentions that errors in generated pseudo-interpretations and limitations of the base retriever contribute to failure cases.
- Why unresolved: While the paper acknowledges this limitation, it does not conduct experiments to quantify the potential performance improvement from using fine-tuned retrievers.
- What evidence would resolve it: Comparative experiments between DIVA using frozen SentenceBERT versus DIVA using fine-tuned retrievers like DPR, measuring retrieval accuracy and QA performance.

### Open Question 3
- Question: How does DIVA's performance scale with increasing ambiguity complexity, such as questions with many plausible interpretations (e.g., 10+ interpretations)?
- Basis in paper: The paper's experiments and case studies focus on questions with relatively few interpretations, and the iterative RAG baseline mentions a maximum of 10 nodes.
- Why unresolved: The paper does not explore the performance limits of DIVA when dealing with highly complex ambiguous questions containing numerous interpretations.
- What evidence would resolve it: Experiments testing DIVA on questions with varying numbers of interpretations (e.g., 5, 10, 20+), measuring performance degradation and computational overhead as complexity increases.

## Limitations

- The framework's dependence on LLM reasoning for both pseudo-interpretation and verification creates a potential single point of failure that isn't thoroughly explored
- Generalizability to other types of ambiguous questions or different knowledge domains beyond ASQA and SituatedQA datasets is not tested
- The analysis of failure modes is limited, with no comprehensive error analysis or ablation studies to quantify the relative contribution of different failure modes

## Confidence

- **High Confidence**: DIVA's efficiency improvements (1.5-3x faster, 1.8x cheaper) are well-supported by reported inference times and API costs
- **Medium Confidence**: Performance superiority on DR score and D-F1 metrics is demonstrated empirically but relies on assumptions about metric quality
- **Low Confidence**: Generalizability to other ambiguous QA tasks beyond the tested datasets remains untested

## Next Checks

1. Conduct detailed error analysis categorizing failures by type (pseudo-interpretation errors, verification misclassifications, coverage gaps) to quantify their relative contributions

2. Perform ablation experiments removing RD, RV, or AG components individually to measure their independent contributions to both performance and efficiency

3. Test DIVA on additional ambiguous QA datasets beyond ASQA and SituatedQA to assess whether performance gains generalize to different types of ambiguity and question distributions