---
ver: rpa2
title: "D\u03B5pS: Delayed \u03B5-Shrinking for Faster Once-For-All Training"
arxiv_id: '2407.06167'
source_url: https://arxiv.org/abs/2407.06167
tags:
- training
- full
- accuracy
- shrinking
- subnets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of efficiently training CNNs\
  \ for multiple deployment scenarios by introducing a scalable Once-for-all training\
  \ approach called D\u03B5pS. The key idea is to delay the shrinking of the full\
  \ model until it is partially trained, which reduces training cost and improves\
  \ weight-shared knowledge distillation to smaller subnets."
---

# DεpS: Delayed ε-Shrinking for Faster Once-For-All Training

## Quick Facts
- arXiv ID: 2407.06167
- Source URL: https://arxiv.org/abs/2407.06167
- Authors: Aditya Annavajjala; Alind Khare; Animesh Agrawal; Igor Fedorov; Hugo Latapie; Myungjin Lee; Alexey Tumanov
- Reference count: 40
- Primary result: DεpS achieves 1.83% higher ImageNet-1k top1 accuracy or same accuracy with 1.3x reduction in FLOPs and 2.5x drop in training cost

## Executive Summary
This paper addresses the challenge of efficiently training CNNs for multiple deployment scenarios by introducing DεpS, a scalable Once-for-all (OFA) training approach. The key insight is that delaying the shrinking of the full model until it is partially trained (~50%) significantly improves training efficiency while maintaining or improving accuracy across diverse subnets. The method combines three novel techniques: FM-Warmup (delayed shrinking), E-Shrinking (dynamic learning rate adjustment), and IKD-Warmup (knowledge distillation from partially trained models). Across multiple datasets including ImageNet-1k, CIFAR-10/100, and ImageNet-100, DεpS consistently outperforms state-of-the-art OFA methods.

## Method Summary
DεpS improves OFA training through three key innovations. First, FM-Warmup delays the shrinking process until the full model is partially trained (~50% of total epochs), providing better weight initialization for subnets. Second, E-Shrinking dynamically adjusts subnet learning rates while maintaining higher full model learning rates, preventing accuracy degradation during the shrinking phase. Third, IKD-Warmup implements knowledge distillation from multiple partially trained full models rather than a single fully trained model, providing richer information transfer to subnets. The approach is evaluated on MobileNet-based supernets across CIFAR-10/100, ImageNet-100, and ImageNet-1k datasets.

## Key Results
- Achieves 1.83% higher ImageNet-1k top1 accuracy compared to OFA baselines
- Maintains same accuracy with 1.3x reduction in FLOPs across the spectrum
- Reduces training cost by 2.5x (GPU hours) compared to existing OFA methods
- Outperforms state-of-the-art OFA techniques across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1: FM-Warmup
Delayed shrinking improves supernet initialization for smaller subnets by training the full model to partial completion before introducing shrinking. This prevents over-specialization and keeps supernet weights more adaptable to subnet architectures. Break condition: Full model warmup period set too low (<30%) or too high (>70%) degrades accuracy.

### Mechanism 2: E-Shrinking
Gradually increasing subnet learning rates while keeping full model learning rate higher prevents rapid weight changes that cause accuracy degradation. This addresses gradient magnitude and direction differences between full model and shrinking updates. Break condition: E parameter increments too aggressive or too slow cause full model accuracy drop or slow convergence.

### Mechanism 3: IKD-Warmup
Distilling knowledge from multiple progressively better partially trained full models provides more information about non-target classes than single fully trained models. This enhances subnet learning through richer knowledge transfer. Break condition: Incorrect implementation results in subnets not receiving meaningful knowledge transfer.

## Foundational Learning

- **Weight sharing in neural architecture search**: Understanding how subnets share weights in a supernet is fundamental to grasping why shrinking strategies matter. Quick check: What happens to subnet performance when supernet weights become too specialized to the full model architecture?

- **Knowledge distillation in neural networks**: The IKD-Warmup mechanism relies on understanding how knowledge can be transferred from larger to smaller models. Quick check: Why might a partially trained model provide better knowledge distillation than a fully trained one?

- **Learning rate schedules and warmup**: E-Shrinking depends on understanding how gradual learning rate increases can stabilize training. Quick check: How does a learning rate warmup schedule prevent training instability when new components are introduced?

## Architecture Onboarding

- **Component map**: Supernet -> FM-Warmup (50% training) -> E-Shrinking (dynamic LR adjustment) -> IKD-Warmup (knowledge distillation) -> Convergence

- **Critical path**: 1) Initialize supernet, 2) Train full model for P_fm_warmup epochs, 3) Begin shrinking with E-Shrinking, 4) Apply IKD-Warmup during shrinking phase, 5) Continue until convergence

- **Design tradeoffs**: Early shrinking saves total epochs but increases per-epoch cost and requires more hyperparameter tuning; late shrinking increases total epochs but has simpler per-epoch training; the chosen P_fm_warmup of ~50% balances these tradeoffs

- **Failure signatures**: 1) Full model accuracy drops significantly when shrinking starts (indicates E-Shrinking not working), 2) Small subnet accuracy remains low (indicates IKD-Warmup or FM-Warmup issues), 3) Training takes longer than baselines (indicates suboptimal P_fm_warmup setting)

- **First 3 experiments**:
  1. Run DεpS with P_fm_warmup=50% and compare full model accuracy drop when shrinking starts vs baseline OFA
  2. Test E-Shrinking by comparing gradient magnitudes/direction differences with and without E parameter
  3. Validate IKD-Warmup by comparing small subnet accuracy with IKD-Warmup vs standard inplace KD

## Open Questions the Paper Calls Out
None

## Limitations
- Core mechanisms lack theoretical grounding and rely primarily on empirical validation
- Study is limited to MobileNet-based architectures and may not generalize to other network families
- Computational cost comparisons based on GPU hours which may vary across hardware configurations

## Confidence
**High Confidence**: DεpS achieves better accuracy than OFA baselines across multiple datasets with extensive empirical results.
**Medium Confidence**: Mechanism explanations for delayed shrinking and learning rate adjustment are plausible but not rigorously proven.
**Low Confidence**: Assertion that IKD-Warmup provides meaningful knowledge transfer beyond standard knowledge distillation lacks strong theoretical justification.

## Next Checks
1. **Cross-architecture generalization test**: Implement DεpS on a ResNet-based supernet and evaluate whether the same improvements are observed.
2. **Hyperparameter sensitivity analysis**: Systematically vary P_fm_warmup (20%, 40%, 60%, 80%) and E-Shrinking increment rates to determine approach robustness.
3. **Theoretical analysis of gradient dynamics**: Conduct formal analysis of gradient magnitude and direction differences between full model and subnet updates during shrinking phase.