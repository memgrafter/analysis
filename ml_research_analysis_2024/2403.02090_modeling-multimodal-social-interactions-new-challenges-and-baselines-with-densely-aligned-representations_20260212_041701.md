---
ver: rpa2
title: 'Modeling Multimodal Social Interactions: New Challenges and Baselines with
  Densely Aligned Representations'
arxiv_id: '2403.02090'
source_url: https://arxiv.org/abs/2403.02090
tags:
- player
- social
- language
- visual
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces three new multimodal social tasks\u2014\
  speaking target identification, pronoun coreference resolution, and mentioned player\
  \ prediction\u2014in social deduction games. The authors propose a novel baseline\
  \ that leverages densely aligned language-visual representations by tracking individual\
  \ players and synchronizing their visual features with corresponding utterances."
---

# Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations

## Quick Facts
- arXiv ID: 2403.02090
- Source URL: https://arxiv.org/abs/2403.02090
- Authors: Sangmin Lee; Bolin Lai; Fiona Ryan; Bikram Boote; James M. Rehg
- Reference count: 40
- Introduces three new multimodal social tasks in social deduction games

## Executive Summary
This paper addresses the challenge of modeling complex social interactions in social deduction games by introducing three new multimodal tasks: speaking target identification, pronoun coreference resolution, and mentioned player prediction. The authors propose a novel baseline that leverages densely aligned language-visual representations by tracking individual players and synchronizing their visual features with corresponding utterances. Through extensive experiments on YouTube and Ego4D datasets, the proposed method demonstrates significant improvements over baselines that use holistic visual features or language alone, achieving accuracy gains of up to 6.6% across all tasks.

## Method Summary
The authors propose a multimodal baseline that densely aligns language and visual representations by tracking individual players in both video frames and corresponding utterances. The approach uses AlphaPose for player detection and tracking, then synchronizes visual features (pose keypoints and positions) with utterances through continuous identification. The model encodes speaker kinesics and listener positions, combines this with conversation context from pre-trained language models, and fuses the multimodal representations using a transformer architecture. A key innovation is the use of player permutation learning during training to prevent the model from relying on specific player identities and instead learn generalizable interaction patterns.

## Key Results
- Densely aligned multimodal representations achieve 3.3-6.6% accuracy improvements across all three social tasks
- Visual interaction features provide complementary information to linguistic context, particularly for pronoun coreference resolution
- Player permutation learning consistently improves performance by forcing the model to learn identity-invariant interaction patterns
- The approach effectively captures fine-grained verbal and non-verbal dynamics between multiple people in social deduction games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Densely aligned language-visual representations improve multi-party interaction modeling by enabling fine-grained correspondence between spoken utterances and individual player behaviors.
- Mechanism: The model tracks individual players in both visual and language domains, aligning visual features (pose keypoints, positions) with corresponding utterances through continuous synchronization. This alignment allows the model to identify the speaker and listeners, and capture spatial relationships alongside linguistic context.
- Core assumption: Visual player tracking can be reliably synchronized with language references, and this synchronization preserves the causal relationships needed for social reasoning tasks.
- Evidence anchors:
  - [abstract]: "propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances"
  - [section 4.1]: "We detect and track players visually in video frames over time using AlphaPose framework [17]. Once we initially match player visuals with the player references in the utterances (i.e., assigning each tracking ID to Player#), we can continuously identify players in both visual and language domains."
  - [corpus]: Weak evidence - no directly comparable mechanisms found in the 8 nearest neighbors
- Break condition: If player tracking becomes unreliable (occlusions, rapid movements) or if language references are ambiguous (multiple people mentioned), the alignment quality degrades and model performance drops.

### Mechanism 2
- Claim: Visual interaction features capturing speaker kinesics and listener positions provide complementary information to linguistic context for predicting referents in social interactions.
- Mechanism: The model encodes speaker gestures (eye movements, shoulder/limb positions) and relative listener positions, then uses self-attention to model dependencies between these visual cues across time. This captures non-verbal dynamics that disambiguate referents when language alone is insufficient.
- Core assumption: Non-verbal cues like gestures and spatial positioning contain information that directly relates to who someone is addressing or referring to, and this information can be effectively captured through keypoint-based visual encoding.
- Evidence anchors:
  - [abstract]: "leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances"
  - [section 4.2]: "Based on the speaker kinesics features fS and player position feature fP, we encode the visual interaction by capturing speaker kinesics motion with the context of player visual positions."
  - [corpus]: Weak evidence - while some papers mention gesture recognition, none specifically address dense alignment with language for social reasoning tasks
- Break condition: If non-verbal cues are culturally ambiguous or if the game setting doesn't involve clear gestural communication, the visual features may add noise rather than signal.

### Mechanism 3
- Claim: Player permutation learning during training forces the model to learn generalizable representations of player interactions that don't depend on specific identifiers, improving robustness.
- Mechanism: During training, the mapping from player names to anonymized identities is randomly shuffled for each iteration, requiring the model to learn interaction patterns that are invariant to specific player identities.
- Core assumption: The social dynamics and interaction patterns are more important than the specific identities of players for the tasks, and the model can learn these patterns without relying on identity-specific cues.
- Evidence anchors:
  - [section 4.4]: "At training time, we apply permutations to anonymized identities to prevent the model from relying on consistent identities. Specifically, we randomly shuffle the mapping from player names to the anonymized player identities in utterances for every iteration."
  - [section 5.5]: "The permutation learning approach consistently improves the performances for all tasks, implying it helps the model learn more generalizable representations of player interactions."
  - [corpus]: Weak evidence - no directly comparable permutation learning approaches found in nearest neighbors
- Break condition: If the social dynamics are highly identity-specific (e.g., certain players have characteristic behaviors that are crucial for the tasks), permutation learning could hurt performance by forcing the model to ignore useful identity-specific patterns.

## Foundational Learning

- Concept: Multimodal representation learning and alignment
  - Why needed here: The paper's core contribution relies on effectively combining and aligning language and visual features to capture social interactions
  - Quick check question: Can you explain the difference between early, late, and cross-modal fusion, and which approach this paper uses?

- Concept: Transformer architectures and self-attention mechanisms
  - Why needed here: The visual interaction encoder and multimodal transformer use transformer architectures to model dependencies between different modalities and across time
  - Quick check question: How does self-attention in transformers differ from traditional recurrent networks when modeling sequential data?

- Concept: Masked language modeling and pre-trained language models
  - Why needed here: The paper leverages BERT, RoBERTa, and ELECTRA as pre-trained language models for encoding conversational context
  - Quick check question: What is the key difference between BERT's masked language modeling objective and ELECTRA's approach?

## Architecture Onboarding

- Component map: Video frames → Player detection and tracking → Visual feature extraction → Alignment with utterances → Visual interaction encoding → Conversation context encoding → Multimodal fusion → Prediction

- Critical path: The critical path is: video frames → player detection and tracking → visual feature extraction → alignment with utterances → visual interaction encoding → conversation context encoding → multimodal fusion → prediction. Any bottleneck in the tracking or alignment stages directly impacts the final prediction quality.

- Design tradeoffs: The architecture trades off computational complexity for alignment quality by using continuous tracking rather than frame-by-frame analysis. It also trades off using holistic visual features (like DINOv2 or MViT) for dense alignment with individual player features, which requires more sophisticated tracking but provides better task-specific performance.

- Failure signatures: Common failure modes include: (1) Tracking failures leading to misaligned visual features, (2) Ambiguous language references that cannot be resolved even with visual cues, (3) Cultural or contextual differences in non-verbal communication that the model doesn't capture, and (4) Over-reliance on visual cues when language is clear, leading to degraded performance on tasks where language alone is sufficient.

- First 3 experiments:
  1. Ablation study removing visual features entirely to establish the baseline performance of language-only models and quantify the contribution of visual information
  2. Comparison of different visual feature types (gesture vs gaze features) to understand which non-verbal cues are most informative for the tasks
  3. Evaluation of conversation context length (varying n in the surrounding utterances) to determine the optimal amount of linguistic context needed for each task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed densely aligned multimodal baseline perform on datasets with more than 6 players?
- Basis in paper: [explicit] The model uses a fixed maximum player number of N=6 and applies zero padding for fewer players.
- Why unresolved: The paper only evaluates on datasets with 6 or fewer players and does not test scalability.
- What evidence would resolve it: Testing the model on datasets with 7+ players and measuring accuracy degradation.

### Open Question 2
- Question: What is the impact of different player tracking failures (e.g., occlusion, lighting) on the model's performance?
- Basis in paper: [explicit] The paper mentions using AlphaPose for tracking and a buffer for missing positions, but doesn't analyze failure modes.
- Why unresolved: The paper doesn't systematically study how tracking errors affect downstream task performance.
- What evidence would resolve it: Ablation studies showing performance with different levels of tracking accuracy.

### Open Question 3
- Question: How would the model perform on social deduction games with different interaction patterns than Werewolf or Avalon?
- Basis in paper: [explicit] The paper only evaluates on Werewolf and Avalon datasets.
- Why unresolved: The paper doesn't test generalizability to other social deduction games or social interaction scenarios.
- What evidence would resolve it: Testing the model on other social deduction games or social interaction datasets.

## Limitations
- The approach relies heavily on accurate player tracking and alignment, which may degrade with occlusions or rapid movements
- Performance gains depend on the assumption that non-verbal cues in social deduction games are consistent and interpretable across different contexts
- The model's effectiveness is limited to the specific social deduction game setting and may not generalize to other social interaction scenarios

## Confidence
- **High confidence**: The ablation study showing that visual features improve accuracy by 3.3-6.6% across all tasks provides strong empirical support for the core claim that densely aligned multimodal representations enhance social interaction modeling.
- **Medium confidence**: While the permutation learning approach shows consistent improvements, the paper doesn't explore whether this benefit persists when identity-specific patterns are actually informative for the tasks.
- **Medium confidence**: The claim that visual interaction features capture complementary information to linguistic context is supported by performance gains, but the paper doesn't conduct controlled experiments isolating the contribution of specific non-verbal cues (e.g., gestures vs. spatial positioning).

## Next Checks
1. **Tracking robustness evaluation**: Test model performance with varying tracking quality by artificially degrading player detection accuracy to quantify the sensitivity of dense alignment to tracking errors.
2. **Cross-cultural validation**: Evaluate the model on datasets from different cultural contexts to verify whether the non-verbal cues captured by visual interaction features generalize beyond the specific social deduction game setting.
3. **Identity-specific pattern analysis**: Conduct experiments comparing permutation learning with identity-aware training on tasks where player-specific behaviors might be crucial, to determine when identity preservation is beneficial.