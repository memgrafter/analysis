---
ver: rpa2
title: 'Goldfish: Monolingual Language Models for 350 Languages'
arxiv_id: '2408.10441'
source_url: https://arxiv.org/abs/2408.10441
tags:
- latn
- language
- languages
- dataset
- goldfish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Goldfish, a suite of over 1000 monolingual
  language models for 350 languages, addressing the lack of dedicated models for low-resource
  languages. The models are trained on up to 1GB of text data per language, with the
  largest being 125M parameters.
---

# Goldfish: Monolingual Language Models for 350 Languages

## Quick Facts
- arXiv ID: 2408.10441
- Source URL: https://arxiv.org/abs/2408.10441
- Authors: Tyler A. Chang; Catherine Arnett; Zhuowen Tu; Benjamin K. Bergen
- Reference count: 40
- Goldfish models outperform XGLM, BLOOM, and MaLA-500 on 98 of 204 FLORES languages in log-perplexity

## Executive Summary
This paper introduces Goldfish, a suite of over 1000 monolingual language models for 350 languages, addressing the lack of dedicated models for low-resource languages. The models are trained on up to 1GB of text data per language, with the largest being 125M parameters. Goldfish models outperform XGLM, BLOOM, and MaLA-500 on 98 of 204 FLORES languages in terms of log-perplexity, despite being 10x smaller. However, they underperform larger multilingual models on reasoning benchmarks, suggesting that multilingual pre-training primarily benefits abstract reasoning over basic text generation.

## Method Summary
The method involves training monolingual autoregressive GPT-2 Transformer models for 350 languages, with dataset sizes ranging from 5MB to 1GB after byte premium scaling. Models are 125M parameters for larger datasets and 39M for smaller ones, trained for 10 epochs. Monolingual SentencePiece tokenizers (50K vocab) are trained for each language. The approach uses byte premium scaling to ensure fair dataset comparisons across languages with different encoding efficiencies.

## Key Results
- Goldfish models achieve lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 languages
- Despite being over 10× smaller than multilingual models, Goldfish outperforms on perplexity for many low-resource languages
- Goldfish significantly underperforms larger multilingual models on reasoning benchmarks (Belebele, XCOPA, XStoryCloze)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual models can outperform multilingual models on perplexity for many low-resource languages due to higher relative training data allocation per language.
- Mechanism: By training exclusively on one language, each model receives up to 1GB of text after byte premium scaling, whereas multilingual models often allocate only a few MB to the same language.
- Core assumption: Per-language model capacity and data volume are the primary drivers of perplexity performance for low-resource languages.
- Evidence anchors:
  - [abstract] "The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10× smaller."
  - [section] "These extremely small quantities of low-resource language data often do not leverage recent efforts to compile text data in low-resource languages."
  - [corpus] Weak; the corpus evidence focuses on dataset compilation, not on per-language data allocation differences.
- Break Condition: If multilingual models receive proportionally similar or larger datasets for low-resource languages, the monolingual advantage in perplexity would diminish.

### Mechanism 2
- Claim: Multilingual pre-training primarily benefits abstract reasoning over basic text generation in low-resource languages.
- Mechanism: Larger multilingual models, despite worse perplexities, achieve higher reasoning benchmark scores, indicating that multilinguality fosters reasoning patterns that are more language-agnostic.
- Core assumption: Abstract reasoning abilities are less dependent on grammatical text generation proficiency and more on exposure to diverse linguistic patterns.
- Evidence anchors:
  - [abstract] "However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation."
  - [section] "This indicates that the combination of larger datasets and model sizes in multilingual pre-training can allow language models to develop reasoning capabilities in specific languages, even when perplexities in those languages remain high."
  - [corpus] Weak; the corpus evidence does not directly address reasoning capabilities.
- Break Condition: If reasoning tasks are found to rely heavily on grammatical text generation proficiency, the advantage of multilingual pre-training for reasoning would be questioned.

### Mechanism 3
- Claim: Byte premium scaling ensures fairer dataset size comparisons across languages with different encoding efficiencies.
- Mechanism: By dividing the desired dataset size by the estimated byte premium, the actual number of tokens used for training is more comparable across languages, accounting for differences in UTF-8 encoding.
- Core assumption: Encoding efficiency significantly impacts the number of tokens available for training, and adjusting for this leads to more equitable model performance comparisons.
- Evidence anchors:
  - [abstract] "To sample pre-training datasets of the desired sizes in a language L, we first use the Byte Premium Tool (Arnett et al., 2024) to estimate the byte premium for L..."
  - [section] "We divide each dataset size by the estimated byte premium for the corresponding language, thus measuring all datasets in units of 'equivalent' English text bytes."
  - [corpus] Weak; the corpus evidence focuses on dataset compilation, not on the specific impact of byte premium scaling.
- Break Condition: If byte premium scaling does not significantly impact model performance or if the estimates are inaccurate, the benefit of this approach would be reduced.

## Foundational Learning

- Concept: Perplexity as a measure of language model quality.
  - Why needed here: Perplexity is used to evaluate and compare the performance of Goldfish models against multilingual models.
  - Quick check question: What does a lower perplexity score indicate about a language model's performance?

- Concept: Byte premium scaling for equitable dataset comparisons.
  - Why needed here: Ensures that the dataset sizes are comparable across languages with different encoding efficiencies.
  - Quick check question: How does byte premium scaling affect the actual number of tokens used for training in languages with different encoding efficiencies?

- Concept: Monolingual vs. multilingual pre-training trade-offs.
  - Why needed here: Understanding the trade-offs between training a model on a single language versus multiple languages is crucial for interpreting the results.
  - Quick check question: What are the potential advantages and disadvantages of monolingual pre-training compared to multilingual pre-training?

## Architecture Onboarding

- Component map: Monolingual autoregressive Transformer models (up to 125M parameters) -> Byte premium scaling for dataset size adjustment -> SentencePiece tokenization (50K vocab) -> FLORES log-perplexity evaluation and reasoning benchmarks (Belebele, XCOPA, XStoryCloze)
- Critical path:
  1. Dataset compilation and preprocessing (including byte premium scaling)
  2. Tokenizer training for each language
  3. Model pre-training for each language and dataset size
  4. Evaluation on FLORES log-perplexity and reasoning benchmarks
- Design tradeoffs:
  - Model size vs. dataset size: Smaller models are used for smaller datasets to avoid overfitting
  - Monolingual vs. multilingual training: Monolingual training allows for more data per language but lacks the reasoning benefits of multilingual training
  - Byte premium scaling: Ensures fair dataset comparisons but relies on accurate byte premium estimates
- Failure signatures:
  - Poor perplexity performance despite large datasets may indicate issues with tokenization or model architecture
  - Low reasoning benchmark scores despite good perplexity may suggest that the model lacks abstract reasoning capabilities
  - Overfitting on small datasets may indicate the need for smaller models or fewer training epochs
- First 3 experiments:
  1. Evaluate perplexity performance of Goldfish models against a simple bigram model to confirm the advantage of neural language models
  2. Compare reasoning benchmark scores of Goldfish models against multilingual models to assess the trade-off between perplexity and reasoning
  3. Test the impact of different byte premium scaling methods on model performance to validate the effectiveness of the chosen approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset size for monolingual language models across different language families, considering the trade-off between model performance and computational efficiency?
- Basis in paper: [explicit] The paper trains models on 5MB, 10MB, 100MB, and 1GB datasets, but doesn't explore sizes in between or larger than 1GB for low-resource languages.
- Why unresolved: The paper focuses on low-resource languages and uses 1GB as the maximum dataset size, but it's unclear if this is optimal for all language families or if larger datasets could provide significant benefits for certain languages.
- What evidence would resolve it: Experiments training models on a wider range of dataset sizes (e.g., 2GB, 5GB, 10GB) for various language families, comparing performance metrics and computational costs.

### Open Question 2
- How do monolingual language models compare to multilingual models when fine-tuned on downstream tasks for low-resource languages?
- Basis in paper: [explicit] The paper shows monolingual models outperform multilingual models on FLORES perplexity but underperform on reasoning benchmarks.
- Why unresolved: The paper doesn't explore fine-tuning scenarios, which are common in practical NLP applications. It's unclear if monolingual models' advantage in perplexity translates to better performance after fine-tuning on specific tasks.
- What evidence would resolve it: Comparative studies of monolingual vs. multilingual models fine-tuned on various downstream tasks (e.g., text classification, named entity recognition) for low-resource languages, measuring performance and computational efficiency.

### Open Question 3
- What is the impact of byte premium scaling on model performance across different language families and scripts?
- Basis in paper: [explicit] The paper uses byte premium scaling to ensure comparable dataset sizes across languages, but doesn't analyze its effect on model performance.
- Why unresolved: While byte premium scaling aims to create fair comparisons, its impact on model learning and performance across diverse languages and scripts is not explored. Some languages might benefit more from this approach than others.
- What evidence would resolve it: Ablation studies training models with and without byte premium scaling across various language families and scripts, analyzing performance differences and potential biases introduced by the scaling method.

## Limitations

- The evaluation methodology relies on FLORES log-perplexity, which may not fully capture model quality across diverse linguistic phenomena
- The dataset compilation approach lacks transparency about exact data sources and quality control measures for each language
- The reasoning benchmark results involve only three tasks that may not comprehensively represent abstract reasoning capabilities across languages

## Confidence

- **High confidence**: The technical implementation details (model architecture, training procedure, evaluation methodology) are well-specified and reproducible.
- **Medium confidence**: The comparative results between Goldfish and multilingual models, as these depend on external factors like dataset quality and evaluation consistency.
- **Medium confidence**: The interpretation that multilingual pre-training primarily benefits reasoning over basic text generation, as this conclusion is based on limited benchmark tasks.

## Next Checks

1. **Cross-validation with alternative perplexity metrics**: Re-evaluate the Goldfish models using standardized perplexity calculations (rather than FLORES-specific metrics) on held-out test sets to confirm the perplexity advantage over multilingual models is robust across evaluation methods.

2. **Benchmark expansion**: Test the models on additional reasoning tasks beyond Belebele, XCOPA, and XStoryCloze, including numerical reasoning, logical inference, and commonsense reasoning benchmarks to determine if the reasoning performance gap is consistent across task types.

3. **Byte premium scaling validation**: Conduct ablation studies by training models with different byte premium scaling approaches (or no scaling) on representative languages to quantify the impact of this mechanism on final model performance and verify that scaling estimates are accurate across language families.