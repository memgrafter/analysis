---
ver: rpa2
title: 'DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate
  Time Series'
arxiv_id: '2404.11269'
source_url: https://arxiv.org/abs/2404.11269
tags:
- time
- anomaly
- domain
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DACAD, a novel contrastive learning framework
  for multivariate time series anomaly detection using unsupervised domain adaptation.
  DACAD addresses the challenge of detecting anomalies in unlabeled target domains
  by leveraging labeled data from related source domains, even when anomaly classes
  differ between domains.
---

# DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series

## Quick Facts
- arXiv ID: 2404.11269
- Source URL: https://arxiv.org/abs/2404.11269
- Reference count: 40
- Primary result: DACAD achieves F1 scores up to 0.81, AUPR of 0.86, and AUROC of 0.86 on real-world multivariate time series anomaly detection

## Executive Summary
DACAD introduces a novel contrastive learning framework for multivariate time series anomaly detection using unsupervised domain adaptation. The method addresses the challenge of detecting anomalies in unlabeled target domains by leveraging labeled data from related source domains, even when anomaly classes differ between domains. DACAD combines supervised contrastive loss in the source domain, self-supervised contrastive triplet loss in the target domain, domain adversarial training, and a Center-based Entropy Classifier (CEC) to learn discriminative feature representations and accurate anomaly boundaries. Extensive evaluations on real-world datasets demonstrate DACAD's superior performance compared to recent TSAD and UDA models.

## Method Summary
DACAD employs a temporal convolutional network (TCN) for feature extraction, supervised contrastive loss in the source domain using label information, self-supervised contrastive triplet loss in the target domain, and domain adversarial training with a discriminator. The method incorporates anomaly injection to create synthetic anomalies for both domains, enhancing generalization across unseen anomaly types. A Center-based Entropy Classifier (CEC) refines anomaly detection boundaries by learning a spatial separation between normal and anomalous samples. The overall loss combines supervised contrastive loss, self-supervised contrastive loss, discriminator loss, and classifier loss with specific weighting parameters.

## Key Results
- DACAD achieves F1 scores of up to 0.81 on real-world multivariate time series datasets
- The model demonstrates AUPR of 0.86 and AUROC of 0.86, significantly outperforming recent TSAD and UDA models
- DACAD successfully transfers knowledge from labeled source domains to detect anomalies in unlabeled target domains across diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
Anomaly injection enhances generalization across unseen anomalous classes by creating synthetic negative pairs during contrastive learning. The model applies five types of synthetic anomalies to both source normal samples and target windows, forcing the feature extractor to learn representations that distinguish normal from abnormal patterns even when the exact anomaly types differ between domains.

### Mechanism 2
Supervised contrastive loss in source domain improves class separation by using label information to form meaningful positive and negative pairs. The model computes a mean-margin contrastive loss where anchors are normal samples, positives are other normal samples, and negatives include both real anomalies and injected anomalies.

### Mechanism 3
Center-based Entropy Classifier (CEC) refines anomaly detection boundaries by explicitly pulling normal samples toward a learnable center and pushing anomalies away. CEC uses an MLP with a learnable center parameter that minimizes distance for normal samples and maximizes it for anomalies, creating spatial separation that defines anomaly scores as squared Euclidean distances.

## Foundational Learning

- **Domain Adaptation (DA) and Unsupervised Domain Adaptation (UDA)**: Why needed here - DACAD explicitly transfers knowledge from a labeled source domain to detect anomalies in an unlabeled target domain, addressing the domain shift problem where data distributions differ between domains. Quick check: What is the fundamental challenge that UDA addresses in machine learning, and how does it differ from supervised learning?

- **Contrastive Learning and Triplet Loss**: Why needed here - The model uses both supervised contrastive loss (with labels) and self-supervised contrastive triplet loss (without labels) to learn discriminative feature representations that separate normal from anomalous patterns. Quick check: In contrastive learning, how do positive and negative pairs influence the learned feature space, and why is this particularly useful for anomaly detection?

- **Anomaly Injection and Synthetic Data Augmentation**: Why needed here - Since anomalies are rare and may differ between domains, the model injects synthetic anomalies to create negative samples for training, improving generalization to unseen anomaly types. Quick check: What are the potential risks and benefits of using synthetic anomalies for training anomaly detection models, particularly when real anomalies are scarce?

## Architecture Onboarding

- **Component map**: TCN (ϕ R) -> Pair Selection -> Contrastive Loss Computation -> Discriminator -> CEC -> Anomaly Scoring
- **Critical path**: TCN → Pair Selection → Contrastive Loss Computation → Discriminator → CEC → Anomaly Scoring
- **Design tradeoffs**: TCN chosen for temporal dependencies but may limit long-range pattern capture; synthetic vs real anomalies balance training data improvement against potential noise; supervised vs self-supervised balance between label utilization and target domain adaptability
- **Failure signatures**: Poor F1 scores despite high AUROC indicates classifier threshold may need adjustment; high variance across datasets suggests model overfitting to specific domain characteristics; low discriminator accuracy indicates feature extractor not producing domain-invariant representations
- **First 3 experiments**: 1) Test anomaly injection effectiveness by comparing DACAD with and without injection on a controlled dataset with known anomaly types; 2) Evaluate contrastive learning contribution by removing L SelfCont and measuring impact on target domain performance; 3) Assess CEC classifier performance by replacing it with a standard binary classifier and comparing anomaly detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DACAD compare to other state-of-the-art models when applied to univariate time series anomaly detection? The paper focuses on multivariate time series anomaly detection and mentions the intention to evolve the model to encompass univariate time series analysis in future work, but does not provide results for univariate time series.

### Open Question 2
What is the impact of using alternative feature extraction architectures, such as transformers or LSTMs, on the performance of DACAD? The paper mentions the intention to explore alternative feature extraction architectures beyond TCN in future work but only uses a TCN for the representation layer.

### Open Question 3
How does the anomaly injection mechanism in DACAD perform when applied to domains with significantly different anomaly characteristics compared to the source domain? The paper states that DACAD uses anomaly injection to enhance generalization across unseen anomalous classes but does not provide specific analysis on performance when target domain has anomalies very different from source domain.

## Limitations

- The paper lacks detailed implementation specifications for the anomaly injection mechanism, with five types of synthetic anomalies mentioned but their exact generation procedures unclear
- The method doesn't thoroughly explore performance when source and target domains have vastly different anomaly distributions
- The paper doesn't address how DACAD performs when source domain labels are noisy or inaccurate

## Confidence

- **High**: The core mechanism of combining contrastive learning with domain adaptation for time series anomaly detection
- **Medium**: The effectiveness of anomaly injection for improving generalization across unseen anomaly types
- **Medium**: The Center-based Entropy Classifier's ability to accurately learn normal boundaries in the source domain

## Next Checks

1. **Ablation Study on Anomaly Injection**: Conduct controlled experiments comparing DACAD's performance with and without anomaly injection across multiple datasets where real anomaly patterns are known, measuring the specific contribution of synthetic anomalies to detection accuracy.

2. **Cross-Domain Anomaly Transfer Analysis**: Systematically evaluate DACAD's performance when source and target domains have progressively increasing differences in anomaly types, documenting the threshold where domain adaptation becomes ineffective.

3. **Center-based Classifier Robustness Test**: Replace CEC with a standard binary classifier and compare performance across all datasets, specifically analyzing cases where the single-center assumption may break down due to diverse normal patterns.