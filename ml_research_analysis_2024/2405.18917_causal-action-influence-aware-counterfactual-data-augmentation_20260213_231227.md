---
ver: rpa2
title: Causal Action Influence Aware Counterfactual Data Augmentation
arxiv_id: '2405.18917'
source_url: https://arxiv.org/abs/2405.18917
tags:
- data
- learning
- causal
- agent
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses causal confusion in offline reinforcement
  learning, where agents learn spurious correlations from limited data. The proposed
  method, CAIAC, performs counterfactual data augmentation by identifying state-space
  factors not causally influenced by actions and swapping these between trajectories.
---

# Causal Action Influence Aware Counterfactual Data Augmentation

## Quick Facts
- arXiv ID: 2405.18917
- Source URL: https://arxiv.org/abs/2405.18917
- Authors: Núria Armengol Urpí; Marco Bagatella; Marin Vlastelica; Georg Martius
- Reference count: 40
- Key outcome: CAIAC substantially increases robustness to distributional shifts in offline RL, achieving 0.75-0.81 success rates for OOD tasks vs 0.0-0.18 for baselines in Franka-Kitchen

## Executive Summary
This paper addresses causal confusion in offline reinforcement learning, where agents learn spurious correlations from limited data. The proposed method, CAIAC, performs counterfactual data augmentation by identifying state-space factors not causally influenced by actions and swapping these between trajectories. This is achieved using state-conditioned mutual information to detect causal action influence. Empirical results show that CAIAC substantially increases robustness to distributional shifts in both offline goal-conditioned RL and self-supervised skill learning, with significant performance improvements across multiple benchmark environments.

## Method Summary
CAIAC addresses causal confusion by augmenting offline datasets with counterfactual samples. The method first trains a transition model to predict next-state entities given current state and action. It then computes causal action influence (CAI) scores using state-conditioned mutual information to identify which state-space factors are not causally influenced by actions. Based on these scores, CAIAC determines uncontrollable entity sets for each state and generates counterfactual transitions by swapping these uncontrollable entities between trajectories that share common uncontrollable sets. The augmented dataset containing both original and counterfactual samples is then used to train any downstream data-driven control algorithm, improving robustness to distributional shifts at test time.

## Key Results
- In Franka-Kitchen environment, CAIAC achieved success rates of 0.75-0.81 for out-of-distribution tasks compared to 0.0-0.18 for baselines
- On Fetch-Pick&Lift, CAIAC achieved 0.8 success rate versus 0.0-0.6 for baselines
- CAIAC increased the support of the joint state space distribution by a factor of 1.7 in low data regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAIAC creates counterfactual data that follow the environment's dynamics and increase the support of the joint state space distribution.
- Mechanism: By using state-conditioned mutual information (causal action influence, CAI) to identify state-space factors not causally influenced by actions, CAIAC swaps these factors between trajectories to create counterfactual samples without requiring model rollouts or online environment interactions.
- Core assumption: Interactions between entities are sparse enough to be neglected, meaning entities are mostly controlled by agent actions rather than interacting with each other.
- Evidence anchors:
  - [abstract] "By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping action-unaffected parts of the state-space between independent trajectories in the dataset."
  - [section 4.2] "With our assumptions and the sets Us and CRs we find that the local causal graph Gs is divided into the disconnected subgraphs GCR s, that contains the entities in CR and A, and into |Us| disconnected subgraphs GU si, i ∈ [1, |Us|], each of which contains an entity in Us with only self-links."
  - [corpus] Weak corpus evidence. The most relevant related paper (Out-of-Distribution Adaptation in Offline RL) discusses counterfactual reasoning via causal normalizing flows, but doesn't provide direct evidence for this specific mechanism.
- Break condition: If the assumption of sparse entity interactions fails (e.g., in highly interactive environments where objects frequently affect each other), the counterfactual samples created may violate the environment's dynamics.

### Mechanism 2
- Claim: CAIAC prevents the agent from suffering from causal confusion and improves robustness to distributional shifts at test time.
- Mechanism: By augmenting real data with counterfactual modifications to causally action-unaffected entities, CAIAC creates samples outside the support of the data distribution, forcing the learning algorithm to learn causal relationships rather than spurious correlations.
- Core assumption: Causal action influence can be accurately detected using state-conditioned mutual information, and this measure is sufficient to identify which state factors should be swapped.
- Evidence anchors:
  - [abstract] "We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift."
  - [section 4.1] "To predict the existence of the edge A → S′ j in the local causal graph Gs, Seitzer et al. (2021) use conditional mutual information (CMI) (Cover, 1999) as a measure of dependence, which is zero if S′ j ⊥ ⊥A|S = s."
  - [corpus] Weak corpus evidence. The related paper "Counterfactual Influence in Markov Decision Processes" discusses counterfactual inference in MDPs but doesn't provide evidence for this specific mechanism.
- Break condition: If the conditional mutual information measure fails to accurately detect causal action influence (e.g., due to model inaccuracies or complex causal structures), the wrong state factors may be swapped, leading to invalid counterfactuals.

### Mechanism 3
- Claim: CAIAC works as an independent module that can be combined with any data-driven control algorithm.
- Mechanism: CAIAC generates counterfactual data as an offline preprocessing step, creating an augmented dataset that can be used by any downstream learning algorithm without modification to the algorithm itself.
- Core assumption: The counterfactual data generated by CAIAC is compatible with various learning algorithms and doesn't require algorithm-specific adaptations.
- Evidence anchors:
  - [abstract] "Our framework works as an independent module and can be used with any data-driven control algorithm."
  - [section 6] "We demonstrate this through empirical results in high-dimensional offline goal-conditioned tasks, applying our method to fundamentally different data distributions and learning methods."
  - [corpus] Weak corpus evidence. The related paper "Causal Generative Explainers using Counterfactual Inference" discusses using counterfactual inference for explainable AI, but doesn't provide evidence for this specific mechanism.
- Break condition: If the counterfactual data format or distribution is incompatible with certain learning algorithms, integration may require modifications or may not provide benefits.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper builds upon MDP formalism as the basic framework for sequential decision-making problems, defining states, actions, transitions, and rewards.
  - Quick check question: What are the components of a Markov Decision Process tuple (S, A, P, R, ρ0, γ)?

- Concept: Causal Graphical Models and Structural Causal Models (SCMs)
  - Why needed here: The paper uses SCMs to represent causal relationships between variables, allowing for interventions and counterfactual reasoning to address causal confusion.
  - Quick check question: How does an intervention do(V = v) on variable V in an SCM affect the structural equations?

- Concept: Conditional Mutual Information (CMI)
  - Why needed here: CMI is used as the core measure (causal action influence, CAI) to detect which state factors are not causally influenced by actions, enabling the identification of factors to swap.
  - Quick check question: What does it mean when the conditional mutual information I(S′ j; A | S = s) equals zero?

## Architecture Onboarding

- Component map:
  Data collection module -> Transition model -> CAI computation module -> Counterfactual generation module -> Integration module

- Critical path:
  1. Train transition model on dataset D
  2. Compute CAI scores for each state in D
  3. Determine uncontrollable sets Us for each state
  4. Generate counterfactual transitions by swapping uncontrollable entities
  5. Train downstream learning algorithm on combined original and counterfactual data

- Design tradeoffs:
  - Computational cost vs. accuracy: Using K = 64 actions for CAI estimation balances accuracy with computational efficiency
  - Sparsity assumption vs. generality: Assuming sparse entity interactions reduces the problem to action influence detection but may not hold in all environments
  - Model complexity vs. data efficiency: Transition model complexity affects counterfactual quality but requires more data for training

- Failure signatures:
  - Low downstream performance despite CAIAC augmentation: May indicate inaccurate CAI detection or violation of sparsity assumption
  - High variance in CAI scores across different world model trainings: Suggests instability in the transition model or insufficient data
  - Counterfactuals with low likelihood under environment dynamics: Indicates the sparsity assumption may not hold or the transition model is inaccurate

- First 3 experiments:
  1. Run CAIAC on a simple environment with known sparse interactions (e.g., two blocks on a table) and verify that the identified uncontrollable sets match ground truth
  2. Compare CAIAC performance with and without data augmentation on an offline RL benchmark (e.g., Fetch-Push) to verify the independent module claim
  3. Test different thresholds θ for classifying controllable vs. uncontrollable entities and observe the impact on downstream performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAIAC change when applied to environments with significant object-object interactions that violate the sparsity assumption?
- Basis in paper: [explicit] The paper explicitly states "we assume that interactions between entities are sparse (i.e. only occur rarely) and are thus negligible" and acknowledges this as a limitation
- Why unresolved: The paper only tests CAIAC on environments where the sparsity assumption holds (kitchen, fetch environments). No experiments or analysis are provided for environments with strong object-object interactions.
- What evidence would resolve it: Experiments on benchmark environments where objects frequently interact (e.g., block stacking, pushing multiple objects simultaneously) showing success/failure rates compared to baselines would directly address this.

### Open Question 2
- Question: What is the optimal ratio of counterfactual to real data for different downstream learning algorithms and data regimes?
- Basis in paper: [explicit] The paper performs an ablation study on the ratio in Franka-Kitchen (Fig. 6) but only tests one environment and one learning algorithm (LMP)
- Why unresolved: The paper shows the ratio matters but doesn't systematically study how it varies across algorithms (TD3, TD3+BC vs LMP) or data regimes (high vs low data)
- What evidence would resolve it: A comprehensive study varying the ratio across multiple environments, data regimes, and learning algorithms showing optimal ratios for each combination would resolve this.

### Open Question 3
- Question: How sensitive is CAIAC to the choice of influence threshold θ, and can this threshold be learned rather than hand-tuned?
- Basis in paper: [explicit] The paper performs extensive analysis on threshold selection (Section A.5) and shows high variance across models for baseline methods, but CAIAC still requires manual threshold tuning
- Why unresolved: While the paper shows CAIAC has lower variance than baselines, it still requires manual threshold selection. The paper doesn't explore automated threshold selection methods.
- What evidence would resolve it: Experiments comparing fixed vs learned thresholds, or showing performance degradation when thresholds are poorly chosen, would clarify the importance of this hyperparameter.

## Limitations
- The sparsity assumption about entity interactions may not hold in all environments, potentially limiting the method's applicability
- The transition model accuracy directly affects CAI detection quality, creating a potential failure point
- The method requires state-space factorization, which may not be available in all settings

## Confidence
- **High confidence**: The mechanism of using CAI for identifying action-unaffected state factors is well-established in the causal inference literature
- **Medium confidence**: The empirical results show consistent improvements across multiple environments and algorithms
- **Medium confidence**: The claim that CAIAC works as an independent module with any data-driven control algorithm is supported but not exhaustively validated

## Next Checks
1. Test CAIAC on environments with known dense entity interactions to validate the sparsity assumption's impact on performance
2. Compare CAIAC with varying transition model complexities to quantify the impact of model accuracy on counterfactual quality
3. Evaluate CAIAC's performance when applied to non-factored state representations through learned entity decompositions