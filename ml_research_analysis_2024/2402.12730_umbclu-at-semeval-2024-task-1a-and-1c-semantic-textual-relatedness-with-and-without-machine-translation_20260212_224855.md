---
ver: rpa2
title: 'UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and
  without machine translation'
arxiv_id: '2402.12730'
source_url: https://arxiv.org/abs/2402.12730
tags:
- languages
- sentence
- translation
- semantic
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors participated in the SemEval 2024 Task 1, which focuses
  on developing models for identifying semantic textual relatedness (STR) between
  sentences in multiple African and Asian languages. They developed two models: TranSem,
  which uses sentence embeddings from translated text, and FineSem, which directly
  fine-tunes a T5 model on STR data.'
---

# UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation

## Quick Facts
- arXiv ID: 2402.12730
- Source URL: https://arxiv.org/abs/2402.12730
- Reference count: 6
- Key outcome: Developed TranSem (translation-based) and FineSem (direct fine-tuning) models for STR task across 14 languages, achieving top-2 rankings in cross-lingual settings

## Executive Summary
This paper presents two approaches for Semantic Textual Relatedness (STR) in the SemEval-2024 Task 1 across 14 African and Asian languages. The authors developed TranSem, which translates text to English and uses sentence embeddings, and FineSem, which directly fine-tunes T5 models on original language data. Their methods were evaluated in both supervised and cross-lingual settings, showing competitive performance with particular success in the cross-lingual track where they achieved 1st place for Afrikaans and 2nd place for Indonesian.

## Method Summary
The authors developed two complementary approaches: TranSem uses machine translation (via NLLB models) to convert sentences to English, then computes cosine similarity between sentence embeddings from DistilRoBERTa; FineSem directly fine-tunes T5 models (base, large, XL) on STR datasets using three data options - individual languages, unified untranslated, and translated+augmented data. They explored the effectiveness of machine translation and data augmentation, training on 4-fold augmented data created by translating each example with 4 different NLLB models.

## Key Results
- In supervised setting: outperformed baseline for 3 languages, performed on par for 4 languages
- In cross-lingual setting: achieved 1st place for Afrikaans, 2nd place for Indonesian
- Comparable performance for 2 languages in cross-lingual setting
- Poor performance on 7 of 9 languages in cross-lingual setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating sentences to English and using pre-trained sentence embeddings allows the model to leverage rich English resources for low-resource languages.
- Mechanism: The model encodes translated sentence pairs using a sentence encoder (e.g., DistilRoBERTa) and computes cosine similarity to predict relatedness scores.
- Core assumption: The translated sentences preserve the semantic meaning of the original language sufficiently for the model to learn correct relatedness.
- Evidence anchors:
  - [abstract] "translating to English leads to better performance for some languages"
  - [section 2.1] "Using Meta's 'No Language Left Behind (NLLB) open-source models' that provide direct high-quality translations for 200 languages"
  - [corpus] Weak evidence - no corpus citations directly supporting translation quality for the specific 14 languages.

### Mechanism 2
- Claim: Direct fine-tuning a T5 model on the STR task with untranslated data performs comparably to using translated data.
- Mechanism: The model treats STR as a text-to-text task, fine-tuning on original language pairs to learn relatedness patterns directly.
- Core assumption: T5's pre-training on semantic similarity (STS) provides sufficient knowledge transfer for STR without translation.
- Evidence anchors:
  - [abstract] "direct fine-tuning on the task is comparable to using sentence embeddings"
  - [section 2.3] "T5 (Raffel et al., 2020) is a transformer model that uses transfer learning; the model trained on 'Colossal Clean Crawled Corpus' is fine-tuned on a mixture of 8 downstream unsupervised and supervised tasks"
  - [corpus] Weak evidence - no corpus citations directly supporting T5's effectiveness on these specific STR datasets.

### Mechanism 3
- Claim: Using multiple translation models with data augmentation improves model robustness and performance.
- Mechanism: The system translates each training example with 4 different NLLB models, creating 4-fold augmented data, and trains on this combined dataset.
- Core assumption: Different translation models capture complementary semantic nuances, and their combination provides more robust training signals.
- Evidence anchors:
  - [section 2.1] "Using each of the 4 models, we translated the training data for all languages in track A, except Amharic and Algerian Arabic, and obtained 4 translated datasets for each language"
  - [section 4.1.4] "We find that direct fine-tuning with the translated and augmented data is comparable with the TranSem model"
  - [corpus] Weak evidence - no corpus citations directly supporting the effectiveness of multi-model translation augmentation for STR.

## Foundational Learning

- Concept: Cosine similarity for measuring semantic relatedness
  - Why needed here: The model needs to compare sentence embeddings to predict relatedness scores
  - Quick check question: If two sentence embeddings have cosine similarity of 0.9, are they more or less related than embeddings with similarity 0.3?

- Concept: Siamese network architecture
  - Why needed here: The model uses identical encoders for both sentences in a pair to learn symmetric relatedness
  - Quick check question: In a Siamese network, do both sentence encoders share weights or have separate parameters?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The model leverages pre-trained language models (T5, sentence transformers) and adapts them to the STR task
  - Quick check question: When fine-tuning, should you train all layers or freeze some layers of the pre-trained model?

## Architecture Onboarding

- Component map: Translation pipeline (NLLB models) → Sentence encoder (DistilRoBERTa/T5) → Similarity computation → Loss function (MSE) → Model output → Data augmentation layer for multiple translations → Separate fine-tuning paths for untranslated vs translated data

- Critical path:
  1. Data preparation (translation + augmentation)
  2. Model selection (TranSem vs FineSem)
  3. Training with appropriate loss function
  4. Validation and checkpoint selection
  5. Test inference

- Design tradeoffs:
  - Translation quality vs computational cost (4 models vs 1)
  - Direct fine-tuning vs sentence embedding approach (model complexity vs performance)
  - Mean pooling vs CLS pooling (simplicity vs potential semantic capture)

- Failure signatures:
  - Poor performance on specific languages → Check translation quality for those languages
  - Inconsistent results across runs → Check random seeds and batch size effects
  - High training loss but low validation loss → Check for overfitting on training data

- First 3 experiments:
  1. Compare mean pooling vs CLS pooling on a single language
  2. Test direct fine-tuning vs translated data approach on English
  3. Evaluate impact of batch size (2, 8, 16, 32) on training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of machine translation models vary across different African and Asian languages in the STR task?
- Basis in paper: [explicit] The authors explored the effectiveness of machine translation by using different translation models and observed that performance varied across languages, with some languages benefiting more from translation than others.
- Why unresolved: The paper does not provide a detailed analysis of the performance variation across individual languages, which could be influenced by factors such as language similarity to English, availability of training data, and linguistic complexity.
- What evidence would resolve it: A comprehensive analysis of translation quality and STR performance for each language, including metrics such as BLEU scores and error analysis, would provide insights into the factors affecting translation quality and its impact on STR.

### Open Question 2
- Question: What is the impact of using different sentence embedding models on the performance of the TranSem model in the STR task?
- Basis in paper: [explicit] The authors experimented with several sentence embedding models and found that the choice of model affected the performance of the TranSem model, but did not provide a detailed comparison of the models' strengths and weaknesses.
- Why unresolved: The paper does not delve into the specific characteristics of each sentence embedding model that contribute to its performance in the STR task, such as its ability to capture semantic relatedness, handle linguistic nuances, or adapt to low-resource languages.
- What evidence would resolve it: A thorough evaluation of different sentence embedding models using standardized benchmarks and qualitative analysis of their outputs would help identify the most suitable models for the STR task and provide insights into their strengths and limitations.

### Open Question 3
- Question: How does the performance of the FineSem model compare to other fine-tuning approaches in the STR task, such as few-shot learning or meta-learning?
- Basis in paper: [explicit] The authors explored the effectiveness of direct fine-tuning on the STR task using the T5 model and observed that it performed comparably to the TranSem model, but did not compare it to other fine-tuning approaches.
- Why unresolved: The paper does not investigate alternative fine-tuning strategies that could potentially improve the performance of the FineSem model, especially in low-resource settings or for languages with limited training data.
- What evidence would resolve it: Comparative experiments using different fine-tuning approaches, such as few-shot learning, meta-learning, or prompt-based learning, would provide insights into the effectiveness of these methods in the STR task and help identify the most suitable approach for different languages and data scenarios.

## Limitations
- Evaluation shows significant performance variability across languages, with poor results on 7 of 9 languages in cross-lingual setting
- Weak corpus evidence for translation quality across all 14 target languages
- Ablation studies lack statistical significance testing for observed differences

## Confidence
**High Confidence**: Core methodology of using machine translation for cross-lingual tasks and general effectiveness of direct fine-tuning on STR tasks are well-established
**Medium Confidence**: Claim that multi-model translation augmentation improves robustness has some empirical support but lacks controlled ablation studies
**Low Confidence**: Effectiveness of specific translation pipeline for the 14 target languages, particularly low-resource languages, cannot be fully verified without access to specific models and quality metrics

## Next Checks
1. **Translation Quality Validation**: Conduct human evaluation or automatic metrics (BLEU, chrF) to verify translation quality for the 14 target languages, focusing on languages where cross-lingual performance was poor
2. **Ablation on Translation Approaches**: Compare performance using single vs multiple translation models on a subset of languages to quantify the actual benefit of data augmentation through translation
3. **Pooling Strategy Statistical Test**: Perform statistical significance testing (paired t-tests) on CLS vs mean pooling performance across multiple runs to determine if observed differences are meaningful rather than random variation