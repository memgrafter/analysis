---
ver: rpa2
title: 'Explainable Multi-hop Question Generation: An End-to-End Approach without
  Intermediate Question Labeling'
arxiv_id: '2404.00571'
source_url: https://arxiv.org/abs/2404.00571
tags:
- question
- questions
- multi-hop
- e2eqr
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an end-to-end multi-hop question generation
  model that incrementally increases question complexity through sequential rewriting
  without requiring intermediate question labels. The model uses accumulated decoder
  hidden states and attention matrices from prior steps to implicitly rewrite questions,
  trained only on the final multi-hop questions.
---

# Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling

## Quick Facts
- arXiv ID: 2404.00571
- Source URL: https://arxiv.org/abs/2404.00571
- Reference count: 0
- Introduces end-to-end multi-hop question generation without intermediate question labeling

## Executive Summary
This paper presents a novel end-to-end approach for generating multi-hop questions through incremental rewriting. The method eliminates the need for labeled intermediate questions by using accumulated decoder hidden states and attention matrices from previous rewriting steps to implicitly guide the generation of increasingly complex questions. The model employs an adaptive curriculum learning strategy to prevent catastrophic forgetting during training. Experiments on the MuSiQue dataset demonstrate that the approach achieves performance comparable to strong baselines while generating interpretable intermediate questions, with particular improvements in answer alignment for higher-hop questions.

## Method Summary
The proposed approach generates multi-hop questions through sequential rewriting without requiring intermediate question labels. The model takes an initial single-hop question and relevant document paragraphs as input, then performs iterative rewriting steps where each step increases the hop count by one. At each rewriting step, the model uses accumulated decoder hidden states and attention matrices from previous steps to implicitly encode information about intermediate reasoning. An adaptive curriculum learning strategy is employed during training, where the model first learns to generate the final multi-hop question before being trained on earlier steps. This prevents catastrophic forgetting and allows the model to focus on generating the final output while still producing interpretable intermediate questions.

## Key Results
- Achieves comparable ROUGE-L scores to BART (44.01/40.04/36.98 for 2/3/4-hop questions on MuSiQue)
- Human evaluation shows better answer alignment than baselines, especially for 3- and 4-hop generation
- Generated synthetic QA pairs improve downstream QA model performance by up to 4.2 percentage points
- Successfully generates interpretable intermediate questions without requiring their labels during training

## Why This Works (Mechanism)
The approach works by leveraging the accumulated decoder states and attention matrices from previous rewriting steps to implicitly encode the reasoning chain needed for multi-hop questions. Each rewriting step uses information from all prior steps, creating a progressive refinement process where the model builds upon previously generated content. The adaptive curriculum learning prevents the model from forgetting how to generate the final output while learning to produce intermediate steps. This sequential rewriting mechanism allows the model to generate increasingly complex questions while maintaining coherence and relevance to the source documents.

## Foundational Learning
**Multi-hop Question Answering**: Understanding that multi-hop questions require reasoning across multiple pieces of evidence or documents, where each hop represents a reasoning step. Why needed: To appreciate the complexity of generating questions that require multi-step reasoning. Quick check: Can the model generate questions that require combining information from at least two distinct sources?

**Curriculum Learning**: Training strategy that starts with easier tasks and progressively increases difficulty. Why needed: To prevent catastrophic forgetting when training on multiple related but increasingly complex tasks. Quick check: Does the model maintain performance on the final task while learning to generate intermediate steps?

**Attention Mechanisms**: Neural network components that allow models to focus on relevant parts of input when generating output. Why needed: To understand how the model can selectively use information from previous rewriting steps. Quick check: Can the attention weights be visualized to show how the model uses previous information?

## Architecture Onboarding

**Component Map**: Input (Question + Documents) -> Encoder -> Decoder (Step 1) -> Decoder (Step 2) -> ... -> Final Multi-hop Question

**Critical Path**: The encoder processes the initial question and relevant documents, then passes information to the first decoder step. Each subsequent decoder step takes the output from the previous step along with accumulated states, progressively refining the question until the final multi-hop question is generated.

**Design Tradeoffs**: The approach trades computational complexity (multiple rewriting steps) for the benefit of not requiring labeled intermediate questions. While sequential rewriting is more computationally intensive than direct generation, it enables the model to produce interpretable intermediate questions and generalize better to higher-hop questions.

**Failure Signatures**: Common failure modes include error propagation (mistakes in early steps affecting later ones), generation of generic or repetitive questions, and difficulty in maintaining coherence across multiple rewriting steps. The model may also struggle with questions requiring very long reasoning chains.

**First 3 Experiments**:
1. Compare ROUGE-L scores against BART baseline for 2-hop question generation
2. Evaluate human assessment of answer alignment for 3-hop questions
3. Test downstream QA performance using synthetic QA pairs generated by the model

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Requires 5-hop training data to generate questions up to 4 hops, creating dependency on complex labeled datasets
- Performance degrades when generating higher-hop questions from lower-hop training data
- Inherits BART's limitations including tendency to generate generic or repetitive outputs
- Sequential rewriting complexity may lead to error propagation across steps

## Confidence

**High Confidence**: The model achieves comparable ROUGE-L scores to BART for multi-hop question generation, as evidenced by quantitative results in Tables 2 and 3. The ablation studies and manual evaluation provide strong support for the effectiveness of accumulated attention and adaptive curriculum learning.

**Medium Confidence**: The claim that synthetic QA pairs improve downstream QA performance by up to 4.2 percentage points is supported by experiments, though improvements may vary with different QA model architectures.

**Low Confidence**: The assertion that the model can generate 4-hop questions from 2-hop training data should be treated cautiously due to observed performance degradation and dependency on higher-hop training data.

## Next Checks
1. Evaluate the model's ability to generate multi-hop questions across different domains and datasets beyond MuSiQue
2. Conduct detailed error analysis to identify types and frequencies of errors during sequential rewriting
3. Expand human evaluation to include larger, more diverse annotator pools and assess additional quality metrics like question naturalness and difficulty calibration