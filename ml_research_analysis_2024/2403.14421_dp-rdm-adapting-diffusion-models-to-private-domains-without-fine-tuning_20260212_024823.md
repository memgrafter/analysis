---
ver: rpa2
title: 'DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning'
arxiv_id: '2403.14421'
source_url: https://arxiv.org/abs/2403.14421
tags:
- retrieval
- dataset
- private
- privacy
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the privacy risks of retrieval-augmented text-to-image
  diffusion models, which can leak sample-level information from their private retrieval
  datasets. The authors propose a differentially private retrieval-augmented diffusion
  model (DP-RDM) that adds calibrated noise to the retrieved samples and adapts the
  existing RDM architecture to this mechanism.
---

# DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning

## Quick Facts
- arXiv ID: 2403.14421
- Source URL: https://arxiv.org/abs/2403.14421
- Authors: Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo
- Reference count: 40
- Primary result: Achieves FID of 10.9 with ε=10 on MS-COCO, 3.5 point improvement over public-only retrieval

## Executive Summary
Retrieval-augmented diffusion models (RDMs) enable text-to-image generation by retrieving relevant samples from private datasets and conditioning the diffusion process on these retrievals. However, this creates significant privacy risks as the retrieval mechanism can leak information about the private dataset. DP-RDM addresses this by applying differential privacy to the retrieval process, adding calibrated noise to retrieved samples while maintaining the core RDM architecture. The method achieves strong privacy guarantees while preserving generation quality, demonstrating that private domain adaptation is possible without costly fine-tuning.

## Method Summary
DP-RDM modifies retrieval-augmented diffusion models to satisfy differential privacy by adding calibrated noise to retrieved samples before they are used for conditioning. The key innovation is adapting the RDM architecture to work with noisy retrievals while maintaining generation quality. The approach uses the Rènyi differential privacy framework to provide rigorous privacy guarantees. During inference, the model retrieves relevant images, adds carefully calibrated noise based on the privacy budget ε, and uses these noisy retrievals to condition the diffusion process. The method is evaluated on the MS-COCO dataset, showing that it can generate high-quality images while protecting the privacy of the retrieval dataset.

## Key Results
- Achieves FID of 10.9 with privacy budget ε=10 on MS-COCO dataset
- Provides 3.5 point improvement in FID compared to public-only retrieval baseline for up to 10,000 queries
- Demonstrates that rigorous differential privacy guarantees can be maintained while generating high-quality image samples
- Shows the approach scales effectively for large numbers of queries while preserving privacy

## Why This Works (Mechanism)
The method works by injecting calibrated noise into the retrieval samples before they are used for conditioning the diffusion process. This noise addition follows differential privacy principles, where the amount of noise is carefully calibrated based on the desired privacy budget ε. The RDM architecture is adapted to handle these noisy retrievals by modifying how the retrieved information is incorporated into the denoising process. The noise mechanism ensures that the presence or absence of any single sample in the retrieval dataset has minimal impact on the generated outputs, providing formal privacy guarantees while maintaining sufficient information for quality generation.

## Foundational Learning

**Differential Privacy (DP)**: A mathematical framework for quantifying and limiting the privacy leakage from individual data points in a dataset. *Why needed*: Provides the theoretical foundation for measuring and guaranteeing privacy in the retrieval process. *Quick check*: Verify that the privacy accountant correctly accumulates privacy loss across multiple queries.

**Rènyi Differential Privacy (RDP)**: A relaxation of traditional DP that provides tighter composition properties for multiple queries. *Why needed*: Enables more accurate privacy accounting when the model processes many retrieval queries. *Quick check*: Confirm that RDP parameters are properly converted to standard (ε, δ)-DP guarantees.

**Retrieval-Augmented Generation**: Using retrieved relevant samples to condition and improve generation quality. *Why needed*: Forms the basis of how RDMs incorporate external knowledge into the generation process. *Quick check*: Validate that retrieval quality remains sufficient after noise addition.

**Diffusion Models**: Generative models that denoise random noise through a learned reverse process. *Why needed*: The core architecture that is being augmented with private retrievals. *Quick check*: Ensure the diffusion process can handle noisy conditioning information.

**Privacy-Utility Tradeoff**: The fundamental tension between protecting privacy and maintaining generation quality. *Why needed*: Guides the calibration of noise levels and evaluation of the approach. *Quick check*: Plot FID vs. privacy budget to understand the full tradeoff curve.

## Architecture Onboarding

**Component Map**: Text Prompt -> Retrieval Module -> Noise Addition -> Diffusion Model -> Generated Image

**Critical Path**: The retrieval module is the critical privacy-sensitive component where noise must be added before conditioning the diffusion process. The noise calibration depends on the privacy budget and the sensitivity of the retrieval operation.

**Design Tradeoffs**: Higher noise levels provide stronger privacy but degrade generation quality. The architecture must balance between preserving enough information in retrievals for quality generation while adding sufficient noise for privacy. The choice of noise distribution (typically Gaussian) affects both privacy guarantees and generation performance.

**Failure Signatures**: If privacy is compromised, individual samples from the private dataset may be reconstructable from generated outputs. If generation quality suffers, the noise level may be too high or the architecture adaptation may be insufficient. Poor retrieval quality before noise addition indicates issues with the retrieval module itself.

**First Experiments**:
1. Verify privacy guarantees by testing membership inference attacks on generated samples
2. Evaluate FID scores across different privacy budgets (ε values) to establish the tradeoff curve
3. Measure retrieval quality degradation by comparing pre- and post-noise retrieval similarity metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Privacy guarantees depend on assumptions about retrieval dataset privacy and proper noise calibration implementation
- Limited comparison to state-of-the-art private fine-tuning approaches makes relative performance unclear
- Privacy-utility tradeoff for ε values beyond 10 is not thoroughly explored
- Retrieval quality degradation under DP constraints is not quantified
- Generalizability to domains beyond MS-COCO and scalability to larger datasets remain uncertain

## Confidence

**High Confidence**: Theoretical framework for DP in retrieval-augmented diffusion models is sound; architecture modifications appear technically correct.

**Medium Confidence**: Reported FID improvement of 3.5 points is plausible but requires validation against stronger baselines; quality preservation claims need broader empirical validation.

**Low Confidence**: Generalizability to other domains, scalability to larger datasets, and robustness to adversarial attacks are not thoroughly addressed; computational overhead is unquantified.

## Next Checks
1. Compare DP-RDM against the most recent private diffusion model fine-tuning approaches on identical evaluation metrics and datasets to establish true performance improvements.

2. Systematically evaluate FID scores across a broader range of privacy budgets (ε from 1 to 100) to understand the full tradeoff curve and identify optimal operating points for different use cases.

3. Quantify the impact of noise addition on retrieval effectiveness by measuring retrieval accuracy, recall, and embedding similarity before and after DP noise injection, establishing the information loss inherent to the privacy mechanism.