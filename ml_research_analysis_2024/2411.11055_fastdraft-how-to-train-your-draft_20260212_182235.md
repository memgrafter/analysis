---
ver: rpa2
title: 'FastDraft: How to Train Your Draft'
arxiv_id: '2411.11055'
source_url: https://arxiv.org/abs/2411.11055
tags:
- draft
- code
- pre-training
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastDraft introduces a novel approach for training high-quality,
  hardware-efficient draft models for speculative decoding. By combining efficient
  pre-training, continued pre-training with mixed code and natural language data,
  and sequence-level knowledge distillation, FastDraft produces draft models that
  achieve up to 3x memory-bound speedup and 2x wall-clock speedup on real hardware.
---

# FastDraft: How to Train Your Draft

## Quick Facts
- arXiv ID: 2411.11055
- Source URL: https://arxiv.org/abs/2411.11055
- Reference count: 40
- Primary result: Achieves up to 3x memory-bound and 2x wall-clock speedup for speculative decoding

## Executive Summary
FastDraft introduces a novel approach for training high-quality, hardware-efficient draft models for speculative decoding. By combining efficient pre-training, continued pre-training with mixed code and natural language data, and sequence-level knowledge distillation, FastDraft produces draft models that achieve significant speedups while maintaining strong acceptance rates. The method enables rapid end-to-end training of draft models in under 24 hours on modest hardware, unlocking efficient LLM inference on edge devices.

## Method Summary
FastDraft employs a three-stage training pipeline: efficient pre-training on large text corpora, continued pre-training on mixed code and natural language data for code completion tasks, and fine-tuning on synthetic datasets generated by the target model using sequence-level knowledge distillation. The approach is optimized for specific hardware through architectural choices that balance depth and width while maintaining a fixed parameter budget. Training is accelerated using Intel Gaudi 2 accelerators and can be completed in under 24 hours.

## Key Results
- Achieves up to 3x memory-bound speedup and 2x wall-clock speedup on real hardware
- Draft models trained in under 24 hours on single server with 8 Intel Gaudi 2 accelerators
- Strong acceptance rates across summarization, code completion, and instruction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FastDraft achieves high acceptance rates by aligning draft models to target model behavior through synthetic data generation
- Mechanism: The draft model is fine-tuned on tokens generated by the target model, exposing it to the distribution of tokens the target is likely to accept
- Core assumption: The target model's generated tokens reflect the distribution of acceptable tokens during inference
- Evidence anchors: [abstract] "achieving up to 3x memory-bound speedup"; [section] "fine-tuning over synthetic datasets generated by the target model"
- Break condition: If synthetic data distribution differs significantly from real-world usage patterns

### Mechanism 2
- Claim: FastDraft improves hardware efficiency by optimizing draft model architecture for the target hardware
- Mechanism: Hardware-aware experiments vary depth and width while maintaining fixed parameter budget to minimize latency
- Core assumption: Latency reduction while maintaining acceptance rate directly improves overall speedup
- Evidence anchors: [abstract] "average 1.5x speedup for natural language tasks"; [section] "altering the depth of the models by changing the number of layers"
- Break condition: If optimization leads to significant drop in acceptance rate or other bottlenecks dominate

### Mechanism 3
- Claim: FastDraft achieves rapid training through efficient pre-training and knowledge distillation
- Mechanism: Combination of efficient pre-training, continued pre-training on code, and sequence-level knowledge distillation
- Core assumption: Pre-training on diverse data followed by targeted fine-tuning produces efficient draft model quickly
- Evidence anchors: [abstract] "under 24 hours on a single server with 8 Intel® Gaudi® 2 accelerators"; [section] "sequence-level KD by fine-tuning the draft model on a synthetic dataset"
- Break condition: If rapid training compromises draft model quality

## Foundational Learning

- Concept: Speculative Decoding
  - Why needed here: FastDraft is specifically designed to improve speculative decoding by creating better draft models
  - Quick check question: In speculative decoding, what is the relationship between the draft model latency, target model latency, block efficiency, and overall speedup?

- Concept: Knowledge Distillation (KD)
  - Why needed here: FastDraft uses knowledge distillation to align the draft model with the target model
  - Quick check question: What are the differences between sequence-level KD and token-level KD, and when would you use each?

- Concept: Hardware-Aware Model Design
  - Why needed here: FastDraft optimizes draft model architecture for specific hardware to maximize speedup
  - Quick check question: How do depth and width of a neural network model affect its latency on different hardware platforms, and what are the trade-offs?

## Architecture Onboarding

- Component map: Draft Model -> Pre-training Data -> Continued Pre-training Data -> Synthetic Dataset -> Target Model -> Hardware Platform
- Critical path: 1. Pre-train draft model on large text corpus; 2. Continued pre-train on code dataset (if needed); 3. Generate synthetic dataset using target model; 4. Fine-tune draft model on synthetic dataset; 5. Optimize draft model architecture for target hardware; 6. Deploy and benchmark
- Design tradeoffs: Model size vs. latency; Pre-training data size vs. training time; Token-level vs. sequence-level KD; Hardware constraints
- Failure signatures: Low acceptance rate; High latency; Slow training; Poor generalization
- First 3 experiments: 1. Pre-train small draft model on FineWeb subset and evaluate perplexity; 2. Generate small synthetic dataset and fine-tune, evaluating acceptance rate; 3. Optimize architecture by varying depth/width while measuring latency and acceptance rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do FastDraft's acceptance rates and speedup performance translate to non-English languages?
- Basis in paper: [explicit] "Our training recipe for draft models has been validated exclusively in English"
- Why unresolved: Paper acknowledges limitation but provides no experimental data for non-English languages
- What evidence would resolve it: Experimental results showing performance across multiple languages with varying syntactic complexity

### Open Question 2
- Question: What is the optimal architectural configuration for draft models across different target model sizes?
- Basis in paper: [inferred] Explores architectural configurations but only tests Phi-3-mini and Llama-3.1-8B drafts
- Why unresolved: Doesn't establish clear guidelines for different target model sizes or systematic framework
- What evidence would resolve it: Comprehensive study testing architectures across multiple target model sizes with performance trade-off analysis

### Open Question 3
- Question: How would incorporating multiple-sequence speculation techniques affect FastDraft's performance?
- Basis in paper: [explicit] "restricted our implementation to single-sequence speculation due to computational constraints"
- Why unresolved: Acknowledges potential benefits but provides no experimental comparison of single vs multiple sequence speculation
- What evidence would resolve it: Direct performance comparison using both single-sequence and multi-sequence speculation on same hardware

## Limitations

- Generalizability of synthetic data approach across different target model families is uncertain
- Hardware-specific optimization may not translate to other platforms like GPUs
- Mixed pre-training approach's optimal balance between code and natural language domains is not thoroughly explored

## Confidence

**High Confidence:** Claims about significant speedup improvements (up to 3x memory-bound, 2x wall-clock) are well-supported by experimental results across multiple tasks and draft model sizes.

**Medium Confidence:** Claims about rapid training (under 24 hours) are supported by reported token counts and hardware specifications, but relationship between training duration and draft model quality is not fully characterized.

**Medium Confidence:** Hardware-aware architectural optimization is methodologically sound, but generalizability of specific configurations to other hardware platforms is uncertain.

## Next Checks

1. **Cross-Model Generalization Test:** Generate synthetic data using different target model family (e.g., Mistral or Gemma) and evaluate whether FastDraft-trained draft models maintain comparable acceptance rates and speedups.

2. **Hardware Portability Assessment:** Replicate architectural optimization experiments on GPU hardware (e.g., NVIDIA A100 or H100) to determine if depth-width trade-offs translate to GPU architectures.

3. **Training Duration Sensitivity Analysis:** Systematically vary pre-training and fine-tuning durations (e.g., 6 hours, 12 hours, 24 hours, 48 hours) while measuring acceptance rates and speedups to establish relationship between training time and draft model quality.