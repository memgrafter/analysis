---
ver: rpa2
title: Towards training digitally-tied analog blocks via hybrid gradient computation
arxiv_id: '2409.03306'
source_url: https://arxiv.org/abs/2409.03306
tags:
- block
- learning
- algorithm
- feedforward
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Feedforward-tied Energy-based Models (ff-EBMs),
  a hybrid model that combines feedforward and energy-based blocks to enable gradient-based
  optimization in mixed digital-analog hardware settings. The authors propose a novel
  algorithm to compute gradients end-to-end by chaining backpropagation through feedforward
  blocks and equilibrium propagation through energy-based blocks.
---

# Towards training digitally-tied analog blocks via hybrid gradient computation

## Quick Facts
- arXiv ID: 2409.03306
- Source URL: https://arxiv.org/abs/2409.03306
- Reference count: 40
- State-of-the-art equilibrium propagation performance on ImageNet32 (46% top-1 accuracy)

## Executive Summary
This work introduces Feedforward-tied Energy-based Models (ff-EBMs), a hybrid model that combines feedforward and energy-based blocks to enable gradient-based optimization in mixed digital-analog hardware settings. The authors propose a novel algorithm to compute gradients end-to-end by chaining backpropagation through feedforward blocks and equilibrium propagation through energy-based blocks. They demonstrate the effectiveness of their approach on Deep Hopfield Networks, showing that performance is maintained across various block splits and that training ff-EBMs on ImageNet32 achieves state-of-the-art results in the equilibrium propagation literature with 46% top-1 accuracy. The method offers a principled and scalable roadmap for integrating self-trainable analog computational primitives into existing digital accelerators.

## Method Summary
The authors propose ff-EBMs that combine feedforward blocks (using standard backpropagation) with energy-based blocks (using equilibrium propagation). The key innovation is a novel algorithm that chains EP error signals backward through EB blocks and BP error signals backward through feedforward blocks, enabling end-to-end gradient computation. The method is demonstrated using Deep Hopfield Networks as the energy-based component, with experiments showing maintained performance across different block splits and achieving state-of-the-art results on ImageNet32 with 46% top-1 accuracy.

## Key Results
- Maintains performance across different block splits (89-90% accuracy on CIFAR-10 with 6-layer EBMs)
- Achieves 46% top-1 accuracy on ImageNet32, establishing new SOTA in equilibrium propagation literature
- Successfully trains 16-layer deep ff-EBMs with performance exceeding previous EP approaches by ~10% top-1 accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chaining EP error signals backward through EB blocks and BP error signals backward through feedforward blocks enables end-to-end gradient computation in mixed digital-analog systems.
- Mechanism: The algorithm passes error signals through two parallel backward passes: EP gradients through EB blocks via implicit differentiation and BP gradients through feedforward blocks via explicit differentiation. These are combined at each junction according to Theorem 3.1.
- Core assumption: EB blocks maintain invertible second-order derivatives and feedforward blocks have differentiable mappings throughout training.
- Evidence anchors: [abstract] "We derive a novel algorithm to compute gradients end-to-end in ff-EBMs by backpropagating and 'eq-propagating' through feedforward and energy-based parts respectively" [section 3.3] "it prescribes an explicit chaining of EP error signals passing backward through Ek (δsk → ∆xk) and BP error signals passing backward through ∂F k⊤ (∆xk → δsk−1)"

### Mechanism 2
- Claim: Splitting a standard DHN into multiple EB blocks while maintaining the same total number of layers preserves performance across different block sizes.
- Mechanism: The total number of parameters and model depth remain constant when splitting, so the model's representational capacity is unchanged. The EP-BP chaining algorithm computes gradients correctly regardless of block boundaries.
- Core assumption: The gradient computation remains accurate across block boundaries due to the implicit BP-EP chaining.
- Evidence anchors: [abstract] "We first show that a standard DHN can be arbitrarily split into any uniform size while maintaining performance" [section 4.3] "We observe that the performance achieved by EP on the 6-layers deep EBM is maintained across 4 different block splits between 89% and 90%"

### Mechanism 3
- Claim: EP-BP chaining enables training deeper ff-EBMs (16 layers) to achieve state-of-the-art results on ImageNet32 in the EP literature.
- Mechanism: By combining the computational efficiency of EP through EB blocks with the architectural flexibility of BP through feedforward blocks, the algorithm scales to deeper models while maintaining training stability and gradient accuracy.
- Core assumption: The EP-BP chaining algorithm maintains gradient accuracy and training stability as model depth increases.
- Evidence anchors: [abstract] "we train ff-EBMs on ImageNet32 where we establish new SOTA performance in the EP literature (46 top-1 %)" [section 4.4] "the performance obtained by training the 16-layers deep ff-EBM by EP exceeds state-of-the-art performance on ImageNet32 by around 10% top-1 validation accuracy"

## Foundational Learning

- Concept: Energy-based models and implicit differentiation
  - Why needed here: ff-EBMs use EB blocks that define predictions through energy minimization rather than explicit forward computation, requiring implicit differentiation for gradient calculation
  - Quick check question: How does implicit differentiation through fixed-point iterations differ from standard backpropagation through explicit layers?

- Concept: Equilibrium propagation algorithm
  - Why needed here: EP is the specific EBL algorithm used to compute gradients through EB blocks, requiring two relaxation phases and contrast-based updates
  - Quick check question: What are the two phases of equilibrium propagation and how do they differ from standard backpropagation phases?

- Concept: Bilevel optimization
  - Why needed here: Training ff-EBMs involves optimizing outer parameters while solving inner equilibrium problems for each EB block, naturally forming a multilevel optimization structure
  - Quick check question: How does the multilevel structure of ff-EBMs extend the bilevel optimization framework used for standard EBMs?

## Architecture Onboarding

- Component map: Input layer → Feedforward block 1 → EB block 1 → Feedforward block 2 → EB block 2 → ... → Feedforward block N → Output layer

- Critical path:
  - Forward pass: Input flows through alternating feedforward and EB blocks with equilibrium finding at each EB block
  - Backward pass: EP gradients flow backward through EB blocks while BP gradients flow backward through feedforward blocks, meeting at block boundaries

- Design tradeoffs:
  - Block size vs. convergence speed: Smaller EB blocks may converge faster but require more boundary transitions
  - Fixed-point iteration steps vs. accuracy: More steps improve equilibrium approximation but increase computation time
  - β nudging strength vs. gradient accuracy: Larger β may speed convergence but reduce gradient fidelity

- Failure signatures:
  - Training instability: Oscillations or divergence in loss curves suggest issues with gradient chaining or equilibrium finding
  - Poor performance: Suboptimal accuracy may indicate insufficient equilibrium convergence or inappropriate block sizing
  - Gradient mismatch: Large discrepancies between EP and ID gradients suggest implementation errors in the chaining algorithm

- First 3 experiments:
  1. Verify gradient chaining on a simple 2-block ff-EBM using the GDU property (cosine similarity between EP and ID gradients should be near 1)
  2. Test block splitting on CIFAR-10 with fixed total depth, comparing performance across different block sizes
  3. Scale to deeper models (8-12 layers) on CIFAR-10 to verify that performance improves with depth as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm scale to deeper architectures with more complex block splits beyond the demonstrated 16-layer model?
- Basis in paper: [explicit] The authors discuss the need for considerable work to prove ff-EBM further at scale on more difficult tasks, considerably deeper architectures, and beyond vision tasks.
- Why unresolved: The paper only demonstrates scaling up to 16 layers on ImageNet32. Theoretical guarantees for deeper architectures and their stability during training are not established.
- What evidence would resolve it: Experimental results on models with 32+ layers on challenging datasets like standard ImageNet, showing maintained performance and convergence stability.

### Open Question 2
- Question: What is the impact of analog device non-idealities (e.g., noise, drift, stuck-at faults) on the performance of ff-EBMs when implemented on physical hardware?
- Basis in paper: [explicit] The authors mention that considerable work is needed to incorporate more hardware realism into ff-EBMs, particularly regarding device non-idealities which may affect the inference pathway.
- Why unresolved: The current implementation is purely simulated and does not account for physical imperfections that would occur in real analog circuits.
- What evidence would resolve it: Simulation results incorporating realistic noise models and device imperfections, or experimental results from physical prototypes showing robustness to these issues.

### Open Question 3
- Question: Can the ff-EBM framework be extended to non-vision tasks, such as natural language processing or reinforcement learning, and what modifications would be necessary?
- Basis in paper: [inferred] The authors suggest that one exciting research direction would be the design of ff-EBM based transformers, with attention layers being chained with energy-based fully connected layers inside attention blocks.
- Why unresolved: The current work only demonstrates ff-EBMs on vision tasks (CIFAR and ImageNet). The applicability to other domains and the required architectural changes are unexplored.
- What evidence would resolve it: Implementation and evaluation of ff-EBM models on NLP benchmarks (e.g., GLUE, SuperGLUE) or RL environments (e.g., Atari, MuJoCo), showing competitive performance.

## Limitations
- Empirical validation is primarily focused on CIFAR datasets with limited ablation studies on ImageNet32 architecture choices
- Convergence guarantees for the EP-BP chaining algorithm are not rigorously proven
- Computational overhead of equilibrium finding across multiple EB blocks remains unquantified
- Performance on deeper networks beyond 16 layers is unexplored

## Confidence
- **High Confidence**: The core algorithmic framework for chaining EP and BP gradients is well-defined and theoretically grounded in implicit differentiation principles. The mathematical derivations in Appendix A are sound and the GDU property provides a useful diagnostic tool.
- **Medium Confidence**: The empirical results on CIFAR datasets showing performance maintenance across different block splits are convincing, though the ImageNet32 results would benefit from more extensive hyperparameter sensitivity analysis. The state-of-the-art claim is relative to the limited EP literature, which hasn't extensively explored deep architectures.
- **Low Confidence**: The computational efficiency claims are not fully supported by wall-clock timing measurements or comparisons to pure digital implementations. The robustness of the method to hardware imperfections in analog blocks is only discussed conceptually without experimental validation.

## Next Checks
1. Implement a minimal 2-block ff-EBM on MNIST and verify that the EP-BP chained gradients match those computed via direct implicit differentiation (ID) using the GDU property (cosine similarity should exceed 0.95).
2. Systematically vary block sizes on CIFAR-10 (keeping total depth constant) to quantify the performance drop as block boundaries increase, measuring both accuracy and equilibrium convergence time.
3. Add realistic analog noise models to the EB blocks and measure degradation in training stability and final accuracy, comparing against clean digital baselines to quantify the method's robustness to analog imperfections.