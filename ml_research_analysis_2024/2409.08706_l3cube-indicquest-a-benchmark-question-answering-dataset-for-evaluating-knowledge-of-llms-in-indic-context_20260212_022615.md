---
ver: rpa2
title: 'L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating Knowledge
  of LLMs in Indic Context'
arxiv_id: '2409.08706'
source_url: https://arxiv.org/abs/2409.08706
tags:
- languages
- indic
- language
- dataset
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L3Cube-IndicQuest, a gold-standard question-answering
  benchmark dataset designed to evaluate knowledge representation of large language
  models (LLMs) in 20 languages (English + 19 Indic languages). The dataset contains
  4,000 factual question-answer pairs across five culturally relevant domains (Literature,
  History, Geography, Politics, Economics).
---

# L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating Knowledge of LLMs in Indic Context

## Quick Facts
- arXiv ID: 2409.08706
- Source URL: https://arxiv.org/abs/2409.08706
- Reference count: 5
- Dataset: 4,000 factual QA pairs across 20 languages (English + 19 Indic languages) in 5 domains

## Executive Summary
L3Cube-IndicQuest is a gold-standard benchmark dataset designed to evaluate knowledge representation of large language models in Indic languages and cultural contexts. The dataset comprises 4,000 factual question-answer pairs across five domains (Literature, History, Geography, Politics, Economics) in 20 languages, with the primary goal of assessing multilingual model performance beyond English-centric benchmarks. Evaluation of five multilingual models reveals a clear performance hierarchy with GPT-4o outperforming others, while all models consistently perform better in English than in Indic languages, highlighting significant representation gaps that need to be addressed for truly multilingual AI systems.

## Method Summary
The dataset construction involved creating factual question-answer pairs across five culturally relevant domains in 20 languages, including English and 19 Indic languages. The evaluation methodology employed both reference-based metrics and LLM-as-a-judge approaches to assess model performance across languages and domains. Five multilingual models were benchmarked: GPT-4o, Llama-3.1-405B-Instruct, Llama-3.1-8B-it, Gemma-2-9B-it, and Gemma-2-2B-it. Performance was measured by comparing model-generated answers against reference answers using automated metrics, with additional validation through LLM-based judging to capture nuanced differences in answer quality.

## Key Results
- GPT-4o achieved the highest performance across all languages and domains, followed by Llama-3.1-405B-Instruct, Gemma-2-9B-it, Llama-3.1-8B-it, and Gemma-2-2B-it
- All models consistently performed better in English than in Indic languages, revealing significant knowledge representation gaps
- Domain performance varied, with Economics showing the strongest results and Geography the weakest across all models

## Why This Works (Mechanism)
The dataset works by providing a culturally grounded evaluation framework that captures domain-specific knowledge relevant to Indic contexts while maintaining multilingual comparability. By using factual question-answer pairs with reference-based and LLM-as-a-judge metrics, the benchmark can systematically assess both knowledge accuracy and cultural appropriateness across languages. The inclusion of both high-resource (English) and low-resource (Indic) languages creates a natural comparison framework that reveals representation gaps in current multilingual models.

## Foundational Learning
- **Multilingual evaluation metrics**: Needed to fairly assess model performance across languages with different linguistic structures; quick check by comparing metric consistency across language pairs
- **Cultural domain relevance**: Essential for creating meaningful benchmarks that reflect real-world knowledge needs; quick check by domain expert validation of question relevance
- **Reference-based vs LLM-as-judge evaluation**: Critical distinction for understanding different types of answer quality assessment; quick check by correlation analysis between metric types
- **Language representation gaps**: Fundamental concept for understanding multilingual model limitations; quick check by performance delta analysis between English and Indic languages
- **Domain-specific knowledge modeling**: Important for assessing specialized knowledge areas; quick check by domain-wise performance breakdown

## Architecture Onboarding
- **Component map**: Dataset creation -> Model evaluation -> Metric computation -> Performance analysis
- **Critical path**: Question-answer pair generation → Model inference → Answer validation → Performance ranking
- **Design tradeoffs**: Comprehensive cultural coverage vs. annotation consistency; multiple languages vs. depth in each language
- **Failure signatures**: Poor performance in specific domains indicates knowledge gaps; English advantage suggests data quality or representation issues
- **First experiments**: 1) Test with additional multilingual models, 2) Analyze domain-specific performance patterns, 3) Compare translation-based vs. native language performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction methodology lacks explicit annotation process details, raising potential bias concerns
- Performance gap between English and Indic languages may reflect data quality differences rather than pure model capability gaps
- Domain distribution across the five areas is not provided, making it difficult to assess potential skew

## Confidence
- High confidence: Performance hierarchy across models (GPT-4o > Llama-3.1-405B-Instruct > Gemma-2-9B-it > Llama-3.1-8B-it > Gemma-2-2B-it) and consistent English advantage over Indic languages
- Medium confidence: Domain-specific performance variations (Economics strongest, Geography weakest) require validation across different datasets
- Medium confidence: Claims about "culturally relevant domains" lack explicit definition of cultural relevance criteria

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of the dataset to establish annotation reliability and consistency across languages
2. Test the benchmark with additional multilingual models not included in the original evaluation to verify if observed performance patterns hold more broadly
3. Perform a comparative analysis using a subset of questions translated between English and Indic languages to isolate whether performance differences stem from language-specific knowledge gaps or translation/representation issues