---
ver: rpa2
title: 'PerSRV: Personalized Sticker Retrieval with Vision-Language Model'
arxiv_id: '2410.21801'
source_url: https://arxiv.org/abs/2410.21801
tags:
- sticker
- retrieval
- stickers
- user
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PerSRV addresses personalized sticker retrieval by introducing
  a Vision-Language Model (VLM)-based framework that combines semantic understanding,
  utility evaluation, and user preference modeling. The method fine-tunes LLaVA-1.5-7B
  to generate human-like sticker descriptions, integrates OCR-extracted text and historical
  query context, and evaluates sticker utility using three metrics: popularity, cross-user
  adaptability, and query adaptability.'
---

# PerSRV: Personalized Sticker Retrieval with Vision-Language Model

## Quick Facts
- arXiv ID: 2410.21801
- Source URL: https://arxiv.org/abs/2410.21801
- Reference count: 40
- Key outcome: 19% improvement in M-MRR@1 for personalized sticker retrieval

## Executive Summary
PerSRV addresses personalized sticker retrieval by combining semantic understanding, utility evaluation, and user preference modeling. The framework fine-tunes LLaVA-1.5-7B to generate human-like sticker descriptions, integrates OCR-extracted text and historical query context, and evaluates sticker utility using popularity, cross-user adaptability, and query adaptability metrics. User preferences are modeled through style clustering based on interaction history. Evaluated on a WeChat dataset, PerSRV achieves significant improvements in multi-modal sticker retrieval with an M-MRR@20 score of 0.3020.

## Method Summary
PerSRV implements a Vision-Language Model (VLM)-based framework with three offline modules and online processing. The offline components include VLM fine-tuning using LoRA on LLaVA-1.5-7B to generate sticker semantics, utility score calculation using three metrics (popularity, cross-user adaptability, query adaptability), and user preference modeling through style clustering using CLIP-cn embeddings. The online process uses BM25 for semantic recall enhanced with utility scores, followed by personalized ranking based on user style preferences. The framework is evaluated on a WeChat dataset of 543,098 stickers and 12,568 interactions.

## Key Results
- Achieves M-MRR@20 score of 0.3020, significantly outperforming existing methods
- 19% improvement in M-MRR@1 compared to baseline approaches
- Demonstrates effective personalization through style-based ranking component

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLaVA-1.5-7B with user click queries generates domain-specific sticker semantics more accurate than base VLM. The fine-tuning uses historical user queries as ground truth, allowing the model to learn sticker-specific language interpretation rather than generic image captions. Break condition: ambiguous or incomplete user queries may lead to biased semantic descriptions.

### Mechanism 2
Combining BM25 semantic recall with utility scores improves retrieval by balancing relevance and quality. BM25 handles semantic matching while utility metrics filter for high-quality stickers, reducing noise. Break condition: utility metrics may not align with actual user preferences or could filter out semantically relevant stickers.

### Mechanism 3
Clustering user interaction history to model sticker style preferences enables effective personalization. CLIP-cn embeddings capture visual style features, k-means clustering identifies preference patterns, and personalized ranking aligns results with user tastes. Break condition: static clustering may fail if user preferences are diverse or frequently changing.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and domain adaptation
  - Why needed here: General VLMs lack sticker-specific semantic understanding
  - Quick check question: What makes VLMs suitable for combining visual and textual understanding?

- Concept: Information retrieval fundamentals (BM25, recall, ranking)
  - Why needed here: Core IR techniques form the retrieval architecture
  - Quick check question: How does BM25 balance term frequency and document frequency?

- Concept: User preference modeling through clustering and embeddings
  - Why needed here: Personalization requires understanding user style patterns
  - Quick check question: What distance metrics work best for comparing image embeddings?

## Architecture Onboarding

- Component map: Offline VLM fine-tuning → OCR text extraction → Query integration → Utility calculation → Style clustering → Online BM25 recall → Utility boosting → Style-based ranking → Final results
- Critical path: User query → BM25 recall → Utility score filtering → Style preference matching → Result delivery
- Design tradeoffs: Fine-tuning vs. prompt engineering (accuracy vs. data requirements), utility metrics vs. pure relevance (quality vs. completeness), static clustering vs. dynamic personalization (efficiency vs. adaptability)
- Failure signatures: Low M-MRR despite high utility scores indicates metric-relevance misalignment; poor personalization suggests ineffective clustering; high inference times indicate computational bottlenecks
- First 3 experiments: 1) Baseline comparison: BM25 alone vs. BM25+Utility vs. Full PerSRV; 2) VLM ablation: Base LLaVA vs. fine-tuned LLaVA vs. with OCR; 3) Personalization impact: With vs. without style-based ranking

## Open Questions the Paper Calls Out

### Open Question 1
How can sticker utility evaluation metrics be further refined or expanded? The paper acknowledges the three proposed metrics are "sticker-scenario specific" but doesn't validate their comprehensiveness. Comparative studies with additional metrics or user validation would resolve this.

### Open Question 2
How does PerSRV perform on datasets from different cultural contexts or messaging platforms? The evaluation is limited to WeChat data without cross-platform or cross-cultural testing. Experiments on other platforms (WhatsApp, Telegram) would address generalizability concerns.

### Open Question 3
What is the impact of different clustering algorithms or hyperparameters on personalization? The paper uses k-means with CLIP-cn but doesn't explore alternatives or perform sensitivity analysis. Comparative studies of clustering methods and k-value sensitivity would resolve this.

## Limitations

- Evaluation relies on a single proprietary WeChat dataset, limiting generalizability to other platforms or cultural contexts
- Utility metrics lack empirical validation showing correlation with actual user satisfaction beyond ranking performance
- Dataset composition and user demographics are not fully disclosed, affecting reproducibility

## Confidence

- High confidence: BM25-based retrieval architecture and clustering methodology are well-established
- Medium confidence: VLM fine-tuning approach for sticker semantics, given success in similar domain adaptation tasks
- Medium confidence: Utility evaluation framework, though specific metrics need further validation
- Medium confidence: Personalization through style clustering, dependent on dataset characteristics

## Next Checks

1. Conduct ablation studies to isolate contribution of each component (VLM fine-tuning, utility scores, style clustering) to performance improvements
2. Test framework on alternative sticker datasets or cross-platform data to evaluate generalization beyond WeChat
3. Implement A/B testing with real users to validate that utility metrics and style-based personalization improve user satisfaction, not just ranking metrics