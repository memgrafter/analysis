---
ver: rpa2
title: 'M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation'
arxiv_id: '2410.21157'
source_url: https://arxiv.org/abs/2410.21157
tags:
- code
- retrieval
- completion
- language
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2RC-EVAL, the first massively multilingual
  repository-level code completion benchmark covering 18 programming languages. The
  benchmark provides fine-grained annotations (bucket-level and semantic-level) based
  on parsed abstract syntax trees to evaluate code large language models' performance
  across different completion scenarios.
---

# M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation

## Quick Facts
- arXiv ID: 2410.21157
- Source URL: https://arxiv.org/abs/2410.21157
- Reference count: 40
- This paper introduces M2RC-EVAL, the first massively multilingual repository-level code completion benchmark covering 18 programming languages

## Executive Summary
This paper introduces M2RC-EVAL, the first massively multilingual repository-level code completion benchmark covering 18 programming languages. The benchmark provides fine-grained annotations (bucket-level and semantic-level) based on parsed abstract syntax trees to evaluate code large language models' performance across different completion scenarios. The authors also curate M2RC-INSTRUCT, a multilingual instruction corpus to enhance repository-level code completion abilities. Experimental results show that cross-file context significantly improves performance, with retrieval and fine-tuning on M2RC-INSTRUCT yielding up to 70% exact match and 75% edit similarity scores across multiple languages.

## Method Summary
The method involves collecting code from The Stack v2, parsing it with Tree-sitter to generate ASTs, and creating fine-grained annotations at bucket (AST depth) and semantic levels. Cross-file context retrieval is implemented using Jaccard similarity to extract relevant code snippets from the same repository. The M2RC-INSTRUCT corpus of 50k instruction samples per language is generated to fine-tune models for instruction following. Evaluation uses Exact Match and Edit Similarity metrics across 18 programming languages with 100 validation and 500 test samples each.

## Key Results
- Cross-file context retrieval improves code completion accuracy significantly
- Fine-grained AST-based annotations enable targeted model improvements
- Multilingual instruction corpus enables strong cross-lingual transfer capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-file context retrieval improves code completion accuracy significantly.
- Mechanism: The model uses inter-file context to resolve dependencies and understand broader program structure, reducing ambiguity in completion tasks.
- Core assumption: Relevant code snippets from other files in the same repository provide contextual cues that improve prediction accuracy.
- Evidence anchors:
  - [abstract] "Experimental results show that cross-file context significantly improves performance"
  - [section] "Cross file context is highly effective, resulting in a significant improvement compared to using only in-file context"
  - [corpus] Weak: No direct citations or usage examples found in corpus.
- Break condition: If retrieval returns irrelevant or noisy context, performance could degrade below in-file only baseline.

### Mechanism 2
- Claim: Fine-grained annotations (bucket and semantic levels) enable targeted model improvements.
- Mechanism: By categorizing completion positions based on AST depth (buckets) and semantic roles, models can learn specialized strategies for different code structures.
- Core assumption: Different syntactic/semantic categories require different completion strategies, and the model can learn these distinctions.
- Evidence anchors:
  - [abstract] "two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided"
  - [section] "performance disparities across different semantic levels" and "as the bucket level decreases, the performance... correspondingly declines"
  - [corpus] Weak: No explicit evidence in corpus that models use these annotations during training.
- Break condition: If annotation categories are too fine-grained or poorly defined, the model may not learn meaningful distinctions.

### Mechanism 3
- Claim: Multilingual instruction corpus improves cross-lingual transfer capabilities.
- Mechanism: Fine-tuning on diverse multilingual instruction data teaches the model to follow instructions across languages, even when fine-tuned on single language.
- Core assumption: Instruction-following skills are transferable across languages once learned.
- Evidence anchors:
  - [section] "fine-tuning the model exclusively with Python data resulted in a significant improvement in its M2RC-E VAL score, coming close to the ES performance achieved through fine-tuning with data from 18 languages"
  - [abstract] "curate a massively multilingual instruction corpora M2RC-INSTRUCT of 18 languages"
  - [corpus] No direct evidence in corpus, but related works mention cross-lingual transfer in code models.
- Break condition: If languages are too structurally different, transfer may not occur effectively.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs) and their role in code representation
  - Why needed here: The benchmark uses ASTs to categorize completion positions and create fine-grained annotations
  - Quick check question: Can you explain how an AST represents program structure and why it's useful for code completion?

- Concept: Retrieval-augmented generation (RAG) for code
  - Why needed here: The evaluation framework includes retrieval of cross-file context to improve completion accuracy
  - Quick check question: How does RAG differ from standard code completion, and what challenges arise when applying it to code?

- Concept: Multilingual code representation and transfer learning
  - Why needed here: The benchmark covers 18 languages and evaluates cross-lingual transfer capabilities
  - Quick check question: What challenges arise when training models on multiple programming languages, and how can transfer learning help?

## Architecture Onboarding

- Component map: Data collection pipeline (Stack v2 → quality filters → M2RC-EVAL/M2RC-INSTRUCT) -> AST parser (Tree-sitter) for annotation generation -> Retrieval module for cross-file context -> Evaluation metrics (EM, ES, CodeBLEU) -> Fine-tuning pipeline for instruction following

- Critical path: 1. Parse source code → generate AST 2. Select completion positions from AST nodes 3. Generate fine-grained annotations (bucket/semantic) 4. Apply quality filters 5. Run evaluation with/without retrieval 6. Fine-tune on instruction corpus

- Design tradeoffs:
  - Depth vs. breadth in AST bucketing (10 levels chosen)
  - Retrieval context size vs. computational efficiency (4096 token limit)
  - Annotation granularity vs. annotation cost and model complexity

- Failure signatures:
  - Poor performance on low bucket levels suggests difficulty with shallow syntax
  - Language-specific performance gaps indicate insufficient cross-lingual transfer
  - Retrieval degradation suggests poor context selection or noise in retrieved snippets

- First 3 experiments:
  1. Compare performance with/without retrieval on a single language to establish baseline improvement
  2. Test different AST bucketing schemes (e.g., 5 vs. 10 levels) to optimize annotation granularity
  3. Evaluate cross-lingual transfer by fine-tuning on one language and testing on others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the bucket-level and semantic-level annotations correlate with completion difficulty across different programming languages?
- Basis in paper: [explicit] The paper introduces two types of fine-grained annotations (bucket-level and semantic-level) based on parsed abstract syntax trees, with experimental results showing performance differences across buckets and semantic levels
- Why unresolved: The paper provides some analysis of performance differences across buckets and semantic levels, but doesn't fully explain the underlying reasons for these differences or provide a comprehensive mapping between annotation types and completion difficulty
- What evidence would resolve it: A detailed statistical analysis showing correlation coefficients between annotation types and completion accuracy, along with qualitative analysis of why certain semantic categories are more challenging than others

### Open Question 2
- Question: What is the optimal training data size for multilingual repository-level code completion across different programming languages?
- Basis in paper: [explicit] The paper mentions that increasing dataset size from 0.1k to 50k samples per language improves results, but doesn't determine if 50k is optimal or if there's a point of diminishing returns
- Why unresolved: The paper only tests up to 50k samples per language and doesn't explore whether larger datasets would yield further improvements or examine language-specific optimal sizes
- What evidence would resolve it: Results showing performance curves for various dataset sizes (e.g., 100k, 200k, 500k) per language, along with analysis of the point where additional data stops providing meaningful improvements

### Open Question 3
- Question: How transferable are code completion skills learned from one programming language to another?
- Basis in paper: [explicit] The paper shows that fine-tuning on Python-only data nearly matches the performance of training on all 18 languages, suggesting strong cross-lingual transfer
- Why unresolved: While the paper demonstrates cross-lingual transfer exists, it doesn't explore the extent or limitations of this transfer, nor does it explain why certain languages might transfer better than others
- What evidence would resolve it: Detailed analysis of cross-lingual transfer performance between different language pairs, identifying which languages transfer most effectively and exploring the characteristics that enable successful transfer

### Open Question 4
- Question: How do execution-based evaluation metrics compare to the current textual similarity metrics for repository-level code completion?
- Basis in paper: [inferred] The paper acknowledges that textual similarity metrics (EM and ES) may not fully capture code completion effectiveness and mentions that execution-based evaluation is challenging but important
- Why unresolved: The paper doesn't implement execution-based evaluation despite acknowledging its potential importance, citing technical challenges in creating comprehensive unit tests and environments for repository-level completion
- What evidence would resolve it: Implementation and results of execution-based evaluation using unit tests or other runtime metrics, comparing these results with the current textual similarity metrics to validate their effectiveness

### Open Question 5
- Question: What is the relationship between code completion difficulty and the structural position of code within the abstract syntax tree?
- Basis in paper: [explicit] The paper observes that performance declines as bucket levels decrease (i.e., moving from deeper to shallower nodes in the AST), suggesting structural position affects difficulty
- Why unresolved: The paper identifies this correlation but doesn't explain the underlying reasons why shallower nodes are more challenging or explore whether this pattern holds across all programming languages
- What evidence would resolve it: In-depth analysis of completion patterns at different AST depths, examining whether certain language constructs at specific tree positions consistently present challenges and exploring potential reasons for these difficulties

## Limitations
- Focus on 18 popular programming languages may miss domain-specific or emerging languages
- Reliance on static AST-based annotations may not capture dynamic runtime behaviors
- Absence of human evaluation for instruction quality in the multilingual corpus

## Confidence
- High confidence in benchmark construction methodology and cross-file context effectiveness
- Medium confidence in generalizability across different model architectures
- Low confidence in practical deployment implications and computational overhead

## Next Checks
1. **Retrieval Context Quality Analysis**: Systematically evaluate the relevance and utility of retrieved cross-file snippets by human annotation, measuring precision@k for different context window sizes and similarity thresholds.

2. **Cross-Lingual Transfer Robustness**: Test the stability of cross-lingual transfer by conducting leave-one-language-out experiments, measuring performance degradation when fine-tuning on N-1 languages and evaluating on the held-out language.

3. **Computational Overhead Benchmarking**: Measure and report the latency and memory overhead introduced by the retrieval-augmented generation system compared to in-file only completion, including GPU memory usage and inference time per completion.