---
ver: rpa2
title: 'MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification'
arxiv_id: '2406.13066'
source_url: https://arxiv.org/abs/2406.13066
tags:
- adversarial
- maskpure
- text
- robustness
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses improving robustness of NLP models against
  adversarial attacks. It introduces MaskPure, a novel diffusion-inspired defense
  mechanism that enhances text classification robustness by randomly masking and refilling
  input tokens using a masked language model, followed by classification and voting
  to determine final output.
---

# MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification

## Quick Facts
- arXiv ID: 2406.13066
- Source URL: https://arxiv.org/abs/2406.13066
- Reference count: 40
- Primary result: First stochastic-purification method successfully defending against both character-level and word-level attacks

## Executive Summary
This paper introduces MaskPure, a novel diffusion-inspired defense mechanism that enhances text classification robustness against adversarial attacks. The method employs stochastic purification by randomly masking and refilling input tokens using a masked language model, followed by classification and voting to determine final output. MaskPure demonstrates superior empirical performance compared to previous defenses, achieving accuracy scores up to 25% higher under attack on AG News and IMDB datasets. The paper also provides theoretical guarantees of certified robustness, bridging the gap between theoretical and practical robustness guarantees in NLP adversarial defense.

## Method Summary
MaskPure operates by randomly masking a proportion of tokens in the input text, refilling the masked positions using a fine-tuned masked language model, and then classifying the refilled text. This process is repeated multiple times with different random maskings, and the final prediction is determined through voting (averaged logit, majority vote, or naive max). The method leverages the intuition from diffusion models in computer vision, where noising and de-noising processes help remove adversarial perturbations. Unlike previous methods that combine training for both classification and mask-filling, MaskPure fine-tunes a separate MLM on the dataset being tested, which the authors hypothesize improves performance by better aligning with the original data distribution.

## Key Results
- Achieves accuracy scores up to 25% higher under attack compared to previous defenses
- First stochastic-purification method successfully defending against both character-level and word-level attacks
- Provides provable certified robustness with meaningful certificates (e.g., 3-4 character changes for 95% confidence on AG News)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random masking and refilling introduces stochastic noise that disrupts the adversarial perturbation patterns.
- Mechanism: The method creates multiple perturbed copies of the input, each with different masked positions, then refills them using a masked language model. This stochastic process effectively "purifies" the text by averaging out adversarial modifications.
- Core assumption: The MLM's predictions are based on general language patterns and not the adversarial perturbations.
- Evidence anchors:
  - [abstract] "MaskPure demonstrates superior empirical performance compared to previous defenses, achieving accuracy scores up to 25% higher under attack"
  - [section] "The intuition behind the generative portion of a diffusion model is that of 'denoising' or purifying data"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism, but diffusion model denoising is well-established in computer vision
- Break condition: If the adversarial perturbations align with common language patterns that the MLM learns during fine-tuning, the purification may reinforce rather than remove them.

### Mechanism 2
- Claim: Voting aggregation over multiple refilled copies improves robustness by majority decision.
- Mechanism: By creating multiple copies with different random maskings and classifications, then using majority voting or logit averaging, the method can overcome single-point failures that occur with adversarial inputs.
- Core assumption: Adversarial examples are less likely to fool the classifier consistently across multiple stochastic variations of the same input.
- Evidence anchors:
  - [section] "Our results in Table 2 show that naive max logit scores perform better than basic majority voting"
  - [section] "The idea behind this approach is that the masking and de-masking of tokens mimics the noising and de-noising that occurs in diffusion-based adversarial defenses"
  - [corpus] Moderate - voting-based defenses are common in adversarial defense literature
- Break condition: If the attack strategy specifically targets the voting mechanism or creates adversarial examples that consistently fool all variations.

### Mechanism 3
- Claim: Fine-tuning the MLM on the specific dataset improves the quality of mask-filling compared to using a generic model.
- Mechanism: By adapting the masked language model to the domain of the classification task, the refill predictions are more likely to restore the original, correct text rather than introducing new errors.
- Core assumption: The adversarial perturbations are less likely to align with the learned domain patterns than the original text.
- Evidence anchors:
  - [section] "We hypothesize that the reason for MaskPure's better performance may be that this combined training actually hinders performance when compared with fine-tuning two separate models on each task"
  - [section] "The intuition behind this is based on diffusion purification in computer vision; since the goal of an adversarial purification process is to remove noise and faults in a text, it makes sense for the purification process to bring the perturbed sample closer to the original distribution of data"
  - [corpus] Moderate - fine-tuning for domain adaptation is well-established
- Break condition: If the fine-tuned MLM learns to reproduce the adversarial patterns present in the training data.

## Foundational Learning

- Concept: Diffusion models and their denoising process
  - Why needed here: Understanding how diffusion models work in image domains provides the theoretical foundation for applying similar principles to text
  - Quick check question: How does the noising and de-noising process in diffusion models help with adversarial purification?

- Concept: Masked Language Models (MLMs) and their capabilities
  - Why needed here: The mask-filling process relies on MLMs to predict appropriate tokens, so understanding their strengths and limitations is crucial
  - Quick check question: What are the key differences between using a pre-trained MLM versus a fine-tuned MLM for mask-filling?

- Concept: Adversarial attack strategies and their characteristics
  - Why needed here: To understand what types of perturbations the method needs to defend against and how different attacks behave
  - Quick check question: What distinguishes character-level attacks from word-level attacks in terms of their impact on model robustness?

## Architecture Onboarding

- Component map: Input text -> Masking layer (random mask positions) -> MLM for mask-filling -> Classification model -> Voting aggregator -> Final prediction
- Critical path: 1. Random masking of input text, 2. Mask-filling using fine-tuned MLM, 3. Classification of refilled texts, 4. Aggregation of classification results
- Design tradeoffs:
  - Masking rate vs. computational cost (higher rates = more purification but slower)
  - Number of voting copies vs. accuracy (more copies = better voting but slower)
  - Fine-tuning MLM on dataset vs. using generic model (better domain fit but requires more training)
- Failure signatures:
  - Clean accuracy drops significantly (masking rate too high)
  - No improvement over baseline defense (voting mechanism not working)
  - Increased vulnerability to certain attack types (MLM learning adversarial patterns)
- First 3 experiments:
  1. Test different masking rates (0.1 to 0.9) on a small sample to find the optimal balance
  2. Compare voting mechanisms (averaged logit, majority vote, naive max) on the same dataset
  3. Test against both character-level and word-level attacks to verify dual capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of masking rate (m) affect the trade-off between computational cost and certified robustness in MaskPure?
- Basis in paper: [explicit] The paper discusses testing MaskPure with different masking rates (m = 0.1 to 0.7) and observes that lower masking rates can achieve comparable performance to higher rates used in other methods, indicating a potential trade-off between computational efficiency and robustness.
- Why unresolved: While the paper shows that MaskPure achieves high robustness with lower masking rates, it does not provide a detailed analysis of the computational cost associated with each masking rate or a systematic study of the optimal masking rate for different datasets and attack types.
- What evidence would resolve it: A comprehensive evaluation of the computational cost (e.g., training time, inference time) for different masking rates across various datasets and attack scenarios, along with an analysis of the relationship between masking rate, computational cost, and robustness.

### Open Question 2
- Question: How does the fine-tuning of the masked language model (MLM) on the specific dataset impact the performance of MaskPure compared to using a pre-trained MLM?
- Basis in paper: [explicit] The paper highlights that MaskPure fine-tunes the MLM on the dataset being tested, hypothesizing that this improves performance by aligning the mask-filling process with the original data distribution. This is contrasted with previous methods that use a combined-training approach or a pre-trained MLM.
- Why unresolved: While the paper suggests that fine-tuning the MLM improves performance, it does not provide a direct comparison between fine-tuned and pre-trained MLMs or a detailed analysis of the impact of MLM fine-tuning on the overall robustness of MaskPure.
- What evidence would resolve it: A controlled experiment comparing the performance of MaskPure with a fine-tuned MLM against a version using a pre-trained MLM, across various datasets and attack types, to quantify the impact of MLM fine-tuning on robustness.

### Open Question 3
- Question: Can the voting mechanisms used in MaskPure be further optimized to improve robustness against more sophisticated adversarial attacks?
- Basis in paper: [explicit] The paper experiments with different voting methods (averaged logit, majority vote-based logit, naive max logit) and observes that naive max logit voting performs best. However, it also notes that better majority-vote-resistant attacks need to be discovered to keep up with defenses that trick greedy-search algorithms.
- Why unresolved: While the paper demonstrates the effectiveness of different voting methods, it does not explore advanced voting mechanisms or ensemble methods that could potentially improve robustness against more complex adversarial attacks.
- What evidence would resolve it: An investigation into advanced voting mechanisms (e.g., weighted voting, ensemble methods) and their impact on robustness against a wider range of adversarial attacks, including those that are specifically designed to bypass majority voting defenses.

## Limitations
- Dataset and Attack Bias: Limited evaluation to two datasets (AG News and IMDB) and two specific attack methods (DeepWordBug and TextFooler)
- Computational Overhead: Introduces significant computational overhead due to multiple mask-filling and classification passes
- Generalization to Other Architectures: All experiments use BERT-based models; effectiveness on other transformer architectures remains untested

## Confidence
- High Confidence: The empirical performance improvements (25% accuracy gain) are well-supported by the experimental results presented
- Medium Confidence: The dual capability against character-level and word-level attacks is demonstrated but limited to specific attack implementations
- Low Confidence: Claims about generalization to other datasets, attack types, and model architectures are not empirically validated in the paper

## Next Checks
1. **Attack Diversity Test**: Evaluate MaskPure against a broader range of attack methods including more recent approaches like BAE, BERT-Attack, and hybrid methods that combine character and word-level perturbations to verify the claimed dual capability.
2. **Architectural Transferability**: Test MaskPure's effectiveness when applied to different transformer architectures (RoBERTa, DeBERTa, DistilBERT) and smaller models to assess generalization beyond BERT.
3. **Computational Efficiency Analysis**: Conduct a detailed study of the trade-off between robustness gains and computational overhead, including analysis of optimal masking rates and voting quantities for different deployment scenarios.