---
ver: rpa2
title: 'PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report
  Generation'
arxiv_id: '2411.05085'
source_url: https://arxiv.org/abs/2411.05085
tags:
- boxes
- images
- finding
- findings
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PadChest-GR is the first manually annotated dataset for grounded
  radiology report generation (GRRG) from chest X-rays. It contains 4,555 studies
  with bilingual (Spanish/English) reports describing all positive and negative findings
  at the sentence level, with bounding box annotations for 6,237 positive findings
  across 3,099 abnormal studies.
---

# PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation

## Quick Facts
- arXiv ID: 2411.05085
- Source URL: https://arxiv.org/abs/2411.05085
- Reference count: 40
- First manually annotated dataset for grounded radiology report generation from chest X-rays with bilingual (Spanish/English) reports and bounding box annotations

## Executive Summary
PadChest-GR is a novel dataset containing 4,555 chest X-ray studies with bilingual radiology reports and detailed annotations. The dataset provides sentence-level grounding by associating each finding description with spatial bounding boxes, categorical labels for finding type and location, and progression status where applicable. Built by curating studies from the larger PadChest dataset and using GPT-4 for translation, PadChest-GR enables development of models that generate interpretable, spatially-grounded radiology reports. The dataset addresses a critical gap in medical imaging by linking complete descriptive sentences to their anatomical locations, facilitating both training and evaluation of grounded report generation models.

## Method Summary
PadChest-GR was constructed by curating studies from the existing PadChest dataset, extracting individual finding sentences, and translating them between Spanish and English using GPT-4. Radiologists then performed quality control and annotated bounding boxes for positive findings across 3,099 abnormal studies. The dataset contains 6,237 positive findings with comprehensive annotations linking each finding sentence to its spatial location in the corresponding chest X-ray image. The annotation process ensured that all positive and negative findings were described at the sentence level, creating a rich resource for grounded report generation tasks.

## Key Results
- Contains 4,555 chest X-ray studies with bilingual (Spanish/English) radiology reports
- Provides 6,237 positive finding annotations with bounding boxes across 3,099 abnormal studies
- Links each finding sentence to spatial location, categorical labels for finding type and location, and progression status
- Enables development and evaluation of models that generate interpretable, grounded radiology reports

## Why This Works (Mechanism)
The dataset works by providing explicit spatial grounding through bounding box annotations that link radiological findings to specific anatomical regions. By including both positive and negative findings at the sentence level, the dataset captures the complete clinical narrative while maintaining spatial context. The bilingual nature enables cross-linguistic validation and broader accessibility for diverse research communities.

## Foundational Learning
The dataset builds upon the foundational PadChest dataset while extending it with crucial spatial annotations and bilingual translations. This creates a unique resource that bridges the gap between image-based medical datasets and natural language processing tasks in radiology.

## Architecture Onboarding
Component map: Chest X-ray image -> Finding sentences (bilingual) -> Bounding box annotations -> Categorical labels (finding type, location) -> Progression status

Critical path: Image acquisition → Sentence extraction and translation → Radiologist annotation → Quality control → Dataset compilation

Design tradeoffs: Manual annotation provides high quality but limits dataset size; GPT-4 translation enables scalability but may introduce subtle semantic shifts

Failure signatures: Incomplete bounding box coverage, translation inconsistencies, missing negative findings

First experiments:
1. Train a simple image-to-text model on the dataset and evaluate grounded report generation quality
2. Assess translation consistency by comparing radiologist-annotated vs. GPT-4 translated reports
3. Evaluate inter-observer variability in bounding box annotations across multiple radiologists

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions, but implicit questions include the impact of translation quality on model performance and the generalizability of findings across different patient populations and imaging protocols.

## Limitations
- Dataset representativeness may be limited due to single-source origin from PadChest
- Translation quality concerns may introduce semantic shifts between Spanish and English versions
- Validation scope focuses on annotation quality rather than comprehensive model performance benchmarking
- Assumption: The manual annotation process, while thorough, may still contain subjective interpretations that affect annotation consistency

## Confidence
- High confidence: Dataset construction methodology and basic quality control procedures
- Medium confidence: Completeness and accuracy of bilingual sentence-level annotations
- Medium confidence: Representativeness of dataset for real-world clinical practice

## Next Checks
1. Conduct independent annotation by multiple radiologists on a subset of studies to quantify inter-observer agreement for spatial annotations
2. Evaluate model-generated reports in a clinical validation study to assess impact on diagnostic accuracy and clinical decision-making
3. Assess model performance when applied to chest X-ray datasets from different institutions and patient populations to determine robustness to domain shifts