---
ver: rpa2
title: Label-free Monitoring of Self-Supervised Learning Progress
arxiv_id: '2409.06612'
source_url: https://arxiv.org/abs/2409.06612
tags:
- learning
- entropy
- training
- clustering
- simclr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces label-free metrics to monitor SSL training
  progress without requiring annotated data. The authors propose using k-means clustering
  and measuring cluster quality via silhouette score and clustering agreement, as
  well as measuring embedding entropy.
---

# Label-free Monitoring of Self-Supervised Learning Progress

## Quick Facts
- arXiv ID: 2409.06612
- Source URL: https://arxiv.org/abs/2409.06612
- Authors: Isaac Xu; Scott Lowe; Thomas Trappenberg
- Reference count: 28
- This paper introduces label-free metrics to monitor SSL training progress without requiring annotated data.

## Executive Summary
This paper addresses the challenge of monitoring self-supervised learning (SSL) training progress without requiring labeled data. The authors propose using k-means clustering with silhouette score and clustering agreement, along with embedding entropy, as label-free metrics to track SSL training progress. These metrics are evaluated against linear probe accuracy across three SSL methods (SimCLR, MoCo-v2, SimSiam) on CIFAR datasets. While clustering metrics show weak correlations with LP accuracy except for SimCLR/MoCo-v2, entropy provides a potentially architecture-independent monitoring approach despite early training instability.

## Method Summary
The authors evaluate three label-free metrics for monitoring SSL training: k-means clustering with silhouette score, clustering agreement (AMI), and embedding entropy. They train SSL models (SimCLR, MoCo-v2, SimSiam) on CIFAR-10 and CIFAR-100 using ResNet-18 backbones, extracting embeddings every 20 epochs. Clustering metrics measure how well embeddings form distinct groups corresponding to classes, while entropy measures the dispersion of the embedding distribution. These metrics are compared against linear probe accuracy to assess their effectiveness as progress indicators. The study also investigates whether these metrics work across different SSL methods and architectures.

## Key Results
- Clustering metrics (silhouette score, AMI) correlate with linear probe accuracy for SimCLR and MoCo-v2 but not for SimSiam
- Embedding entropy decreases during training for SimCLR and MoCo-v2 but increases for SimSiam
- Early training instability causes outlier embeddings that affect entropy measurements
- Clustering metrics appear architecture-dependent while entropy may be architecture-independent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering-based metrics can track SSL progress because embedding space organization improves as training proceeds.
- Mechanism: During SSL, augmentations of the same image should produce similar embeddings. As training progresses, embeddings of similar images cluster together, which can be measured via silhouette scores and clustering agreement.
- Core assumption: The embedding space becomes more semantically organized as SSL training progresses.
- Evidence anchors: [abstract] "we apply k-means clustering and measure the clustering quality with the silhouette score and clustering agreement"
- Break condition: If SSL method doesn't create semantic embedding organization (e.g., SimSiam in this study).

### Mechanism 2
- Claim: Entropy of embedding distribution decreases during SSL training, indicating progress.
- Mechanism: As SSL training progresses, embeddings become more clustered around learned modes, reducing the overall entropy of the distribution.
- Core assumption: The embedding distribution transitions from high dispersion to more concentrated modes during effective SSL training.
- Evidence anchors: [abstract] "entropy did not always have strong correlations with LP accuracy, this appears to be due to instability arising from early training, with the metric stabilizing and becoming more reliable at later stages of learning"
- Break condition: If SSL method causes entropy to increase (observed for SimSiam) or if early training instability masks the trend.

### Mechanism 3
- Claim: Clustering metrics work better with some SSL methods than others due to architectural dependencies.
- Mechanism: The effectiveness of clustering-based metrics depends on how the specific SSL method organizes the embedding space, with some methods (SimCLR, MoCo-v2) creating more clusterable representations than others (SimSiam).
- Core assumption: Different SSL methods create embedding spaces with different organizational properties that affect metric effectiveness.
- Evidence anchors: [abstract] "clustering metrics correlated with the linear probe accuracy only when training with SSL methods SimCLR and MoCo-v2, but not with SimSiam"
- Break condition: If SSL method doesn't create semantically meaningful clusters or if architectural differences prevent metric generalization.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL)
  - Why needed here: Understanding SSL is crucial because the paper evaluates label-free monitoring methods specifically for SSL training progress
  - Quick check question: What distinguishes SSL from supervised learning in terms of data requirements and training objectives?

- Concept: k-means clustering and silhouette score
  - Why needed here: These are the primary metrics used to evaluate embedding quality without labels
  - Quick check question: How does the silhouette score quantify the quality of clustering, and what values indicate good versus poor clustering?

- Concept: Entropy as a measure of distribution
  - Why needed here: Entropy is proposed as an architecture-independent metric for monitoring SSL progress
  - Quick check question: Why would decreasing entropy indicate progress in SSL training, and what does high entropy in early training suggest?

## Architecture Onboarding

- Component map: Input augmentation -> Encoder backbone -> Projector MLP -> (Predictor MLP for SimSiam) -> Loss computation -> Parameter updates
- Critical path: Input augmentation -> Encoder -> Projector -> Similarity computation -> Loss -> Gradient update
- Design tradeoffs: SimCLR uses InfoNCE with negative samples requiring memory queue; SimSiam uses stop-gradient to prevent collapse; MoCo-v2 uses momentum encoder for stability
- Failure signatures: Poor correlation with LP accuracy indicates metrics not capturing meaningful progress; entropy increasing for SimSiam suggests dimensional collapse; early training instability shows in outlier embeddings
- First 3 experiments:
  1. Reproduce SimCLR on CIFAR-10 and verify entropy decreases while silhouette score increases over training
  2. Test clustering agreement metric on MoCo-v2 and check if it correlates with LP probe accuracy
  3. Apply entropy metric to pre-trained torchvision models and verify cross-architecture consistency on CIFAR-100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the entropy of SimSiam embeddings to increase during training, contrary to the decreasing trend observed in SimCLR and MoCo-v2?
- Basis in paper: [explicit] The authors observe that "entropy generally decreases as learning progresses, this trend reverses for SimSiam" and note "More research is required to establish the cause for this unexpected behaviour."
- Why unresolved: The paper identifies this as an unexpected behavior but does not provide a definitive explanation, suggesting it may be related to SimSiam's susceptibility to dimensional collapse.
- What evidence would resolve it: Analysis of singular value spectra of the projector and predictor embedding spaces, or systematic experiments varying SimSiam's architectural components to identify which elements trigger this entropy increase.

### Open Question 2
- Question: Why do label-free clustering metrics correlate with linear probe accuracy for SimCLR and MoCo-v2 but not for SimSiam?
- Basis in paper: [explicit] "We find that while the clusters did correspond better to the ground truth annotations as training of the network progressed, label-free clustering metrics correlated with the linear probe accuracy only when training with SSL methods SimCLR and MoCo-v2, but not with SimSiam."
- Why unresolved: The authors observe this discrepancy but do not provide a theoretical explanation for why clustering metrics work differently across SSL methods.
- What evidence would resolve it: Comparative analysis of the embedding space structures produced by each SSL method, examining how well clusters form and whether negative samples or other methodological differences affect cluster quality.

### Open Question 3
- Question: What causes the outlier embeddings that appear sporadically during early training stages in SimCLR and MoCo-v2, and why do they affect entropy measurements?
- Basis in paper: [explicit] "Investigating the distribution of CIFAR-100 Z3 embeddings in early milestones, we discovered that SimCLR had outlier embeddings (at a large distance away from the rest of the samples) which caused a lower entropy measurement" and note these outliers "occur sporadically during the early stages of training."
- Why unresolved: The paper identifies these outliers and their effect on metrics but does not determine their origin or whether they represent problematic samples or a normal part of training dynamics.
- What evidence would resolve it: Examination of the raw images corresponding to outlier embeddings to determine if they have unusual properties, analysis of whether outliers appear consistently for the same samples across training runs, or investigation of whether these samples behave like out-of-domain data.

## Limitations
- Clustering metrics show inconsistent correlations with linear probe accuracy across different SSL methods
- Entropy metric exhibits early training instability that affects reliability
- Architectural dependencies suggest clustering metrics may not generalize across different model architectures

## Confidence
- Medium: Clustering metrics correlation with LP accuracy (method-dependent results)
- Medium: Entropy as architecture-independent metric (limited cross-architecture testing)
- Low: Generalization to other datasets and SSL methods (narrow experimental scope)

## Next Checks
1. Test entropy metric stability by running multiple training seeds and measuring variance across early epochs
2. Apply metrics to additional SSL methods (BYOL, Barlow Twins) to verify method-dependent patterns
3. Evaluate cross-architecture generalization by testing metrics on ResNet-50 vs ResNet-18 trained with same SSL method