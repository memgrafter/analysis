---
ver: rpa2
title: 'DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent
  Framework'
arxiv_id: '2408.11788'
source_url: https://arxiv.org/abs/2408.11788
tags:
- video
- videos
- dreamfactory
- framework
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DreamFactory, a multi-agent framework for
  generating long, multi-scene videos with consistent characters and styles. The framework
  simulates an animation company using LLM agents in roles like CEO, director, and
  artist to collaboratively create scripts, storyboards, and keyframes.
---

# DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework

## Quick Facts
- **arXiv ID**: 2408.11788
- **Source URL**: https://arxiv.org/abs/2408.11788
- **Reference count**: 8
- **Primary result**: Introduces a multi-agent LLM framework that generates long, multi-scene videos with consistent characters and styles, outperforming existing models in quality metrics and human preference ratings.

## Executive Summary
DreamFactory presents a novel multi-agent framework for generating long, multi-scene videos with consistent characters and styles. The system simulates an animation company using specialized LLM agents in roles like CEO, director, and artist to collaboratively create scripts, storyboards, and keyframes. A key innovation is the Keyframe Iteration Design method, which maintains long-term consistency by iteratively refining frames while preserving base features. The framework introduces new evaluation metrics like Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score, demonstrating superior performance compared to existing models across multiple quality metrics and human preference ratings.

## Method Summary
DreamFactory uses a multi-agent LLM framework where specialized agents collaborate to generate long, multi-scene videos. The framework defines distinct roles (CEO, Director, Screenwriter, Filmmaker, Reviewer) with specific responsibilities that work through structured conversations. A Keyframe Iteration Design method transforms long-term consistency challenges into manageable short-term memory problems by generating a base frame that establishes style and characters, then iteratively refining subsequent frames while preserving base features. The system integrates with image/video generation tools like DALL-E 3 and Stable Video Diffusion, and introduces novel evaluation metrics including Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score to assess video quality.

## Key Results
- DreamFactory-generated videos outperform existing models in quality metrics (FID, IS, CLIP, FVD, KVD)
- Achieved higher human preference ratings compared to AI videos from the internet
- Successfully addresses the challenge of creating coherent, long-duration videos with multiple scenes and characters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent collaboration enables specialization of roles that mimics real-world film production, allowing complex tasks like scriptwriting, storyboard creation, and keyframe generation to be decomposed and handled by specialized LLM agents.
- Mechanism: The framework defines distinct roles (CEO, Director, Screenwriter, Filmmaker, Reviewer) each with specific responsibilities. These agents collaborate through structured conversations, with each phase producing intermediate outputs that guide subsequent phases. This division of labor allows each agent to focus on its domain expertise while maintaining overall coherence through shared context and memory.
- Core assumption: That LLM agents can effectively simulate human creative roles and maintain consistent collaboration through structured communication protocols.
- Evidence anchors:
  - [abstract] "DreamFactory leverages multi-agent collaboration principles and a Key Frames Iteration Design Method to ensure consistency and style across long videos"
  - [section 3.1] "The definition prompts for their roles primarily consist of three main parts: Job, Task and Requirements"
  - [corpus] Weak evidence - only 5 related papers, average neighbor FMR=0.479 suggests moderate relevance but limited corpus support
- Break condition: If agents fail to maintain coherent conversation flow or if role specialization leads to conflicting creative decisions that cannot be resolved through the collaboration framework.

### Mechanism 2
- Claim: Keyframe Iteration Design transforms long-term consistency challenges into manageable short-term memory problems by iteratively refining frames while preserving base features.
- Mechanism: The method generates a "Base" frame that establishes style, characters, and background, then creates a "Base Description" (BD) capturing essential features. Subsequent frames are generated using both contextual environment (Ct-1) from adjacent scenes and BD to maintain consistency across the video.

## Foundational Learning

### LLM Multi-Agent Systems
- **Why needed**: Enables decomposition of complex video generation tasks into specialized roles that can be handled by different agents, mimicking real-world production workflows.
- **Quick check**: Verify that agents can maintain coherent conversation threads and produce outputs that align with their defined roles and responsibilities.

### Keyframe Iteration Design
- **Why needed**: Addresses the challenge of maintaining long-term consistency across extended video sequences by breaking it down into iterative refinement steps.
- **Quick check**: Confirm that base features are preserved across iterations by comparing feature embeddings of generated frames to the original base frame.

### Cross-Scene Consistency Metrics
- **Why needed**: Provides quantitative measures for evaluating character and style consistency across different scenes in long videos.
- **Quick check**: Validate that these metrics correlate with human perception of video quality by conducting user studies.

## Architecture Onboarding

### Component Map
CEO -> Director -> Screenwriter -> Filmmaker -> Reviewer -> (Video Generation Tools)

### Critical Path
Story input → CEO analysis → Director planning → Screenwriter script → Filmmaker keyframe generation → Reviewer validation → Final video output

### Design Tradeoffs
- **Pros**: Specialized agents allow focused expertise in different production phases, structured collaboration ensures coherence, iterative refinement maintains consistency
- **Cons**: Increased computational overhead from multiple agent interactions, potential communication overhead between agents, complexity in coordinating multiple specialized components

### Failure Signatures
- Inconsistent character appearance across scenes (detected by high Cross-Scene Face Distance Score)
- Narrative incoherence between keyframes and script (detected by low Average Key-Frames CLIP Score)
- Agent communication breakdowns leading to conflicting creative decisions

### First 3 Experiments to Run
1. Test multi-agent collaboration with simplified role definitions to verify basic communication framework
2. Validate Keyframe Iteration Design on short video segments before scaling to longer sequences
3. Evaluate Cross-Scene Face Distance Score on videos with known consistency issues to confirm metric sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the keyframe iteration method handle character consistency across drastically different scene transitions (e.g., from a forest to an urban environment)?
- Basis in paper: [explicit] The paper mentions the keyframe iteration design method and its use of base description (BD) to maintain long-term memory across scenes, but doesn't provide specific examples of how it handles drastic scene changes.
- Why unresolved: The paper discusses maintaining consistency within scenes and across scene transitions, but doesn't address how the model handles significant environmental changes that might affect character appearance or style.
- What evidence would resolve it: Comparative analysis of character consistency metrics (like CSFD Score) between videos with gradual scene transitions versus videos with drastic scene changes would clarify this.

### Open Question 2
- Question: What is the computational overhead of the DreamFactory framework compared to traditional video generation models?
- Basis in paper: [inferred] The paper mentions that video generation still involves substantial resource consumption as a limitation, but doesn't provide specific metrics comparing the computational cost of DreamFactory to other models.
- Why unresolved: While the paper discusses the framework's capabilities, it doesn't quantify the additional computational resources required for the multi-agent system and keyframe iteration method.
- What evidence would resolve it: Detailed benchmarks comparing processing time, memory usage, and GPU requirements between DreamFactory and single-model video generation approaches would resolve this.

### Open Question 3
- Question: How does the performance of DreamFactory scale with video length beyond the tested examples?
- Basis in paper: [explicit] The paper states that DreamFactory can generate "long-duration videos" and "unrestricted length" videos, but the experiments focus on specific examples without testing the limits of video length.
- Why unresolved: The paper demonstrates capability for generating multi-scene videos but doesn't explore how quality metrics (FID, IS, FVD, etc.) or consistency metrics (CSFD Score, CSSC Score) degrade as video length increases.
- What evidence would resolve it: Systematic testing of DreamFactory across various video lengths (e.g., 1 minute, 5 minutes, 10 minutes) with corresponding quality and consistency metrics would clarify the scaling behavior.

## Limitations
- Experimental validation relies heavily on automated metrics that may not fully capture long-term consistency in multi-scene videos
- Comparison with "AI videos from the internet" lacks specificity about which models were used for benchmarking
- Implementation complexity of the Keyframe Iteration Design method may make independent reproduction challenging

## Confidence

**High confidence**: The claim that DreamFactory can generate multi-scene videos with improved quality metrics compared to baseline models is supported by reported numerical improvements across multiple evaluation metrics.

**Medium confidence**: The assertion that the multi-agent framework effectively simulates real-world animation production workflows is plausible given the described role definitions, but lacks empirical evidence showing that this specific agent structure is optimal or necessary.

**Low confidence**: The claim about human preference ratings being higher than "AI videos from the internet" is difficult to verify without knowing the specific comparison models and the methodology for human evaluation.

## Next Checks

1. **Reproduce the evaluation metrics independently**: Implement the Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score metrics from scratch and verify they produce consistent results with the reported values when applied to the same videos.

2. **Conduct ablation studies on the multi-agent framework**: Test the system's performance when removing or modifying specific agent roles to determine which components are essential for achieving the reported improvements.

3. **Validate long-term consistency with extended videos**: Generate videos significantly longer than those reported in the paper (e.g., 5+ minutes) and evaluate character and style consistency across all scenes to test the limits of the Keyframe Iteration Design method.