---
ver: rpa2
title: 'TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive
  Knowledge Assistants'
arxiv_id: '2405.02637'
source_url: https://arxiv.org/abs/2405.02637
tags:
- response
- trec
- ptkb
- ikat
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TREC iKAT 2023 introduces a test collection for evaluating conversational
  and interactive knowledge assistants. The collection includes 36 personalized dialogues
  over 20 topics, each coupled with a Personal Text Knowledge Base (PTKB) defining
  user personas.
---

# TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants

## Quick Facts
- arXiv ID: 2405.02637
- Source URL: https://arxiv.org/abs/2405.02637
- Reference count: 34
- Primary result: Introduces a test collection for conversational and interactive knowledge assistants with 36 personalized dialogues, showing G→R→G pipelines generally outperform R→G methods.

## Executive Summary
TREC iKAT 2023 introduces a test collection for evaluating conversational and interactive knowledge assistants (CSAs). The collection includes 36 personalized dialogues over 20 topics, each coupled with a Personal Text Knowledge Base (PTKB) defining user personas. A total of 344 turns with approximately 26,000 passages are assessed for relevance, completeness, groundedness, and naturalness. The evaluation involves both human annotators and GPT-4. Results show that G→R→G approaches generally outperform R→G methods in passage ranking, with run-4-GPT-4 achieving the best performance (nDCG@5 of 0.4382). For PTKB statement ranking, run-1-llama-zero-shot performs best (nDCG@3 of 0.7254). Human evaluation of generated responses shows run-4-GPT-4 achieving the highest relevance (2.42) and completeness (2.54) scores.

## Method Summary
The TREC iKAT 2023 test collection consists of 36 personalized dialogues over 20 topics, with each dialogue paired with a PTKB defining user personas. Systems are evaluated using two main pipelines: Retrieve-Then-Generate (R→G) and Generate-Retrieve-Generate (G→R→G). Evaluation metrics include passage ranking (nDCG@3, nDCG@5, P@20, Recall@20, mAP), PTKB statement ranking (nDCG@3, P@3, Recall@3, MRR), and response generation quality (relevance, completeness, groundedness, naturalness). Both human annotators and GPT-4 are used for evaluation, with GPT-4 showing high correlation with human labels in preliminary experiments.

## Key Results
- G→R→G pipelines outperform R→G methods, with run-4-GPT-4 achieving nDCG@5 of 0.4382
- PTKB-dependent turns show lower performance, highlighting the importance of accurate persona statement selection
- Run-1-llama-zero-shot achieves best PTKB statement ranking (nDCG@3 of 0.7254)
- GPT-4 evaluation reveals run-4-GPT-4 excels in naturalness (4.00) while other methods show better groundedness performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G→R→G pipelines outperform R→G pipelines because initial LLM generation leverages learned knowledge to guide subsequent retrieval and response synthesis.
- Mechanism: The LLM first generates an answer using its internal knowledge, then formulates search queries from that answer, retrieves relevant passages, and finally generates a refined response grounded in retrieved evidence.
- Core assumption: The LLM's internal knowledge is complementary to the retrieval corpus and can improve query formulation and response quality.
- Evidence anchors:
  - [abstract] states "Results show that G→R→G approaches generally outperform R→G methods"
  - [section] 5.1.1 notes "G→R→G runs tend to perform better than R→G runs, suggesting that leveraging the learned knowledge of LLMs (GPT-4 and Llama in this case) leads to a better starting point for subsequent retrieval of relevant results and then the generation of a relevant response."
- Break condition: If the LLM's internal knowledge is misaligned with the retrieval corpus or if the generated queries are too generic, the advantage disappears.

### Mechanism 2
- Claim: PTKB-dependent turns are more challenging because they require accurate selection of relevant persona statements before effective retrieval and response generation.
- Mechanism: The system must first rank PTKB statements relevant to the current turn, then use those statements to inform query rewriting and retrieval. Errors in PTKB selection cascade into poorer retrieval and response quality.
- Core assumption: PTKB statements are necessary for understanding user intent in personalized turns and directly influence the relevance of retrieved passages.
- Evidence anchors:
  - [abstract] notes the test collection "challenges CSAs to efficiently navigate diverse personal contexts, elicit pertinent persona information, and employ context for relevant conversations."
  - [section] 5.1.3 observes that "PTKB dependence leads to lower performance" and "the significance of predicting the right PTKB statements in the early turns is essential."
- Break condition: If PTKB statements are irrelevant or redundant, or if the system ignores them when they are needed, performance degrades.

### Mechanism 3
- Claim: LLMs as evaluators correlate well with human judgments for response quality, enabling scalable assessment of groundedness and naturalness.
- Mechanism: GPT-4 is prompted with response and provenance passages to assess groundedness, and with response alone to assess naturalness. These assessments correlate with human labels and allow rapid evaluation of many responses.
- Core assumption: LLM evaluators can reliably distinguish grounded from ungrounded responses and natural from unnatural language in conversational contexts.
- Evidence anchors:
  - [section] 3.6 states "we use GPT-4 to evaluate the relevance, completeness, groundedness, and naturalness of the responses, as it demonstrated a high correlation with human labels in our preliminary experiments."
  - [section] 5.3.2 notes "LLM evaluators can reliably distinguish grounded from ungrounded responses."
- Break condition: If the LLM evaluator is biased toward its own generation style or lacks understanding of the conversational context, assessments may not reflect true quality.

## Foundational Learning

- Concept: Conversational information seeking and mixed initiative
  - Why needed here: The test collection involves multi-turn dialogues where both user and agent can initiate queries, requiring understanding of context-dependent relevance and personalization.
  - Quick check question: In a conversational search scenario, how does the agent determine which user persona information is relevant for the current turn?

- Concept: Passage ranking and re-ranking metrics (nDCG, P@K, Recall)
  - Why needed here: Evaluation of passage retrieval performance at different cutoffs (e.g., nDCG@3, nDCG@5) is central to assessing system effectiveness.
  - Quick check question: What does nDCG@5 measure, and why is it particularly important for conversational search where only top results are typically used?

- Concept: PTKB (Personal Text Knowledge Base) and personalization
  - Why needed here: The collection couples each dialogue with a PTKB defining user personas, and systems must use this to tailor responses; understanding how to extract and apply persona information is critical.
  - Quick check question: How would you determine which PTKB statements are relevant to a given conversational turn?

## Architecture Onboarding

- Component map: PTKB ranking -> conversational query rewriting/expansions -> retrieval engine (e.g., BM25, ColBERT, dense retrievers) -> passage re-ranking (neural cross-encoders) -> response generation (LLM summarization) -> evaluation (human and GPT-4 labeling)
- Critical path: PTKB ranking → query rewriting → retrieval → re-ranking → response generation → evaluation
- Design tradeoffs: G→R→G vs. R→G pipelines; sparse vs. dense retrieval; manual vs. automatic PTKB selection; human vs. LLM evaluation
- Failure signatures: Low PTKB ranking scores indicate poor persona extraction; low retrieval nDCG suggests ineffective query rewriting or retrieval; poor GPT-4 evaluation scores point to hallucination or lack of grounding
- First 3 experiments:
  1. Compare nDCG@5 for G→R→G vs. R→G pipelines on a subset of turns to quantify the impact of LLM-initialized retrieval.
  2. Evaluate PTKB statement ranking accuracy (P@3, Recall@3) to ensure persona statements are being selected correctly.
  3. Run a small set of human evaluations on generated responses to validate GPT-4 evaluation scores and check for evaluator bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of conversational search agents vary when handling dialogues with high dependency on PTKB statements versus those with low dependency?
- Basis in paper: [explicit] The paper discusses performance differences in turns with and without PTKB dependency in Section 5.1.3, noting that PTKB-dependent turns often show lower performance.
- Why unresolved: While initial observations are provided, a comprehensive comparative analysis across all dialogues is not presented.
- What evidence would resolve it: Detailed statistical analysis and visualizations comparing performance metrics (e.g., nDCG@5) for PTKB-dependent and non-dependent turns across all dialogues.

### Open Question 2
- Question: What is the impact of using different query rewriting techniques on the effectiveness of passage retrieval in conversational search?
- Basis in paper: [inferred] The paper mentions various approaches to query rewriting and their effects on retrieval performance in Sections 3.7 and 5.1, but does not directly compare different techniques.
- Why unresolved: The paper does not provide a systematic comparison of the effectiveness of different query rewriting methods.
- What evidence would resolve it: A controlled experiment comparing multiple query rewriting techniques and their impact on retrieval metrics like nDCG and Recall.

### Open Question 3
- Question: How do human and GPT-4 evaluations of generated responses correlate with actual user satisfaction in conversational search tasks?
- Basis in paper: [explicit] The paper compares human and GPT-4 evaluations in Section 5.3, noting differences in scores and potential biases.
- Why unresolved: The paper does not link these evaluations to actual user satisfaction or engagement metrics.
- What evidence would resolve it: User studies measuring satisfaction and engagement with responses evaluated by both humans and GPT-4, correlating these with the evaluation scores.

## Limitations
- The reliance on GPT-4 as both a participant in evaluation and a method component introduces potential circularity.
- The relatively small number of topics (20) and dialogues (36) limits generalizability to broader conversational search scenarios.
- The study does not explore edge cases where PTKB statements are irrelevant or contradictory, which could reveal fundamental weaknesses in personalization handling.

## Confidence

- **High Confidence**: The superiority of G→R→G pipelines over R→G methods (supported by multiple runs and clear nDCG@5 differences)
- **Medium Confidence**: The effectiveness of LLM evaluators (based on preliminary correlation with human labels, but limited cross-validation)
- **Medium Confidence**: The importance of PTKB statement selection (supported by performance degradation in PTKB-dependent turns, but not extensively analyzed)

## Next Checks

1. **Cross-Validation of LLM Evaluation**: Implement human evaluation on a larger sample of responses across all runs to verify the correlation between GPT-4 and human judgments for groundedness and naturalness metrics
2. **PTKB Robustness Testing**: Create controlled test cases where PTKB statements are irrelevant or contradictory to measure how different systems handle such edge cases and whether the G→R→G advantage persists
3. **Generalization Assessment**: Test the best-performing runs on a separate subset of dialogues or an external conversational search dataset to evaluate whether performance gains transfer beyond the original 20 topics