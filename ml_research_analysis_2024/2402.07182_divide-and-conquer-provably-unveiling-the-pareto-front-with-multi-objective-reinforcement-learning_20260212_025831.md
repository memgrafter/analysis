---
ver: rpa2
title: 'Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective
  Reinforcement Learning'
arxiv_id: '2402.07182'
source_url: https://arxiv.org/abs/2402.07182
tags:
- pareto
- ipro
- front
- optimal
- oracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Iterated Pareto Referent Optimisation (IPRO) provably learns the
  Pareto front for multi-objective reinforcement learning by decomposing the problem
  into a sequence of constrained single-objective problems. Each iteration queries
  a Pareto oracle with a lower bound referent to identify a new weakly Pareto optimal
  policy, systematically trimming the search space using dominated and infeasible
  sets.
---

# Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2402.07182
- **Source URL**: https://arxiv.org/abs/2402.07182
- **Reference count**: 40
- **Primary result**: IPRO learns Pareto fronts with provable convergence and error bounds without requiring domain-specific assumptions

## Executive Summary
This paper introduces Iterated Pareto Referent Optimisation (IPRO), a novel algorithm for multi-objective reinforcement learning that provably learns the Pareto front by decomposing the problem into a sequence of constrained single-objective problems. The approach systematically explores the Pareto front by iteratively querying a Pareto oracle with lower bounds to identify new weakly Pareto optimal policies while maintaining upper and lower bounds that provide guarantees on the distance to undiscovered solutions. IPRO demonstrates strong empirical performance across multiple environments including Deep Sea Treasure, Minecart, and MO-Reacher, matching or outperforming baselines while providing theoretical convergence guarantees in polynomial time for a constant number of objectives.

## Method Summary
IPRO addresses multi-objective RL by iteratively querying a Pareto oracle with lower bounds from a dominated set, using the oracle's responses to update upper and lower bounds that systematically trim the search space. The algorithm maintains a bounding box defined by ideal and nadir points, along with sets of lower bounds (inner corners of the dominated set's reachable boundary) and upper bounds (inner corners of the infeasible set's reachable boundary). Each iteration selects a lower bound, queries the oracle to find a new policy in the target region, and updates the boundaries. The process continues until the maximum distance between any upper bound and its nearest lower bound falls below the specified tolerance τ, guaranteeing a τ-Pareto front with provable error bounds.

## Key Results
- IPRO provably learns the Pareto front by decomposing the problem into constrained single-objective queries
- The algorithm provides an upper bound on the distance to undiscovered solutions at each iteration
- IPRO matches or outperforms baselines across multiple environments including Deep Sea Treasure, Minecart, and MO-Reacher
- Guarantees convergence to a τ-Pareto front in polynomial time for a constant number of objectives

## Why This Works (Mechanism)

### Mechanism 1: Problem Decomposition
IPRO decomposes the multi-objective RL problem into a sequence of constrained single-objective problems, each targeting a specific region of the Pareto front. The algorithm uses a Pareto oracle to query a lower bound referent and find a new weakly Pareto optimal policy. The oracle's response is used to update the dominated set D and infeasible set I, systematically trimming the search space. Core assumption: A valid Pareto oracle exists that can solve the constrained single-objective problems to find policies in the target region. Break condition: If the Pareto oracle cannot find feasible solutions within the target region, IPRO will mark the lower bound as complete and continue, potentially leading to gaps in the final Pareto front.

### Mechanism 2: Bound Maintenance
IPRO maintains upper and lower bounds on the Pareto front, providing guarantees on the distance to undiscovered solutions at each iteration. The algorithm tracks the ideal and nadir points to form a bounding box, then maintains sets of lower bounds L and upper bounds U that cover the inner corners of the dominated and infeasible sets. The maximum distance between any upper bound and its nearest lower bound provides an upper bound on the approximation error. Core assumption: The Pareto front lies within the bounding box defined by the ideal and nadir, and the upper and lower bounds can be maintained efficiently. Break condition: If the upper bounds are not maintained correctly, the error bound becomes invalid and IPRO may terminate prematurely or continue unnecessarily.

### Mechanism 3: Systematic Convergence
IPRO converges to a τ-Pareto front in polynomial time for a constant number of objectives by systematically exploring and closing regions of the search space. The algorithm iteratively selects lower bounds, queries the oracle, and updates the boundaries. When the distance between every upper bound and its nearest lower bound falls below the tolerance τ, the algorithm terminates with a τ-Pareto front. The complexity analysis shows polynomial dependence on τ but exponential dependence on the number of objectives. Core assumption: The Pareto oracle can find solutions within the target region with the specified tolerance, and the search space can be systematically explored. Break condition: If the number of objectives is not constant or the tolerance τ is very small, the exponential dependence on objectives may make the algorithm intractable.

## Foundational Learning

- **Concept**: Multi-objective Markov Decision Processes (MOMDPs)
  - Why needed here: IPRO operates on MOMDPs, which model sequential decision-making problems with multiple conflicting objectives
  - Quick check question: What is the key difference between a regular MDP and a MOMDP?

- **Concept**: Pareto optimality and dominance
  - Why needed here: IPRO's goal is to find the Pareto front, which consists of policies that are not dominated by any other policy
  - Quick check question: What is the difference between Pareto dominance and strict Pareto dominance?

- **Concept**: Achievement scalarizing functions (ASFs)
  - Why needed here: IPRO uses ASFs to construct Pareto oracles that can find policies in specific target regions
  - Quick check question: What is the difference between order representing and order approximating ASFs?

## Architecture Onboarding

- **Component map**: Bounding box (ideal, nadir) -> Dominated set D -> Infeasible set I -> Lower bounds L (D's corners) -> Upper bounds U (I's corners) -> Pareto oracle

- **Critical path**:
  1. Initialize bounding box and Pareto front with d weakly optimal solutions
  2. Iteratively select lower bounds, query oracle, update sets
  3. Terminate when error bound falls below tolerance τ

- **Design tradeoffs**:
  - Oracle quality vs. computational cost: Better oracles find solutions faster but may be more expensive
  - Tolerance τ vs. convergence time: Smaller τ gives better approximation but requires more iterations
  - Memory-based vs. stationary policies: Memory-based policies can achieve better Pareto fronts but require more complex oracles

- **Failure signatures**:
  - Algorithm gets stuck: May indicate poor oracle performance or need for different lower bound selection strategy
  - Slow convergence: May indicate need for smaller tolerance or better oracle
  - Incomplete Pareto front: May indicate oracle cannot find solutions in certain regions

- **First 3 experiments**:
  1. Run IPRO on Deep Sea Treasure with τ=0 to verify complete Pareto front recovery
  2. Compare IPRO with GPI-LS on Minecart to verify performance on convex Pareto fronts
  3. Test IPRO with different Pareto oracles (convex MDP vs. constrained MDP) to compare convergence properties

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of scalarisation function affect the quality and diversity of the Pareto front discovered by IPRO? The paper discusses using the augmented Chebyshev scalarisation function but acknowledges that other ASFs could be used, and the choice of ASF affects whether weakly or strongly Pareto optimal solutions are returned.

- **Open Question 2**: Can IPRO be extended to learn multiple policies concurrently rather than sequentially, and what would be the theoretical guarantees for such an extension? The paper explicitly states "For future work, we aim to extend IPRO to learn multiple policies concurrently" in the conclusion, acknowledging that current IPRO learns only one Pareto optimal solution per iteration.

- **Open Question 3**: What is the optimal strategy for selecting lower bounds from L during IPRO execution to minimize the number of oracle queries? The paper mentions that IPRO "prioritises lower bounds using a heuristic selection function based on the hypervolume improvement metric" but doesn't provide theoretical analysis of optimal selection strategies.

## Limitations
- Relies on an idealized Pareto oracle that may be difficult to implement efficiently in practice
- Exponential dependence on the number of objectives limits scalability to high-dimensional problems
- Sequential learning approach is less sample-efficient than concurrent methods like PCN

## Confidence

**High confidence**:
- Polynomial-time complexity claim for constant objectives
- Mechanism of decomposing the problem into constrained single-objective problems

**Medium confidence**:
- Upper bound guarantee on approximation error
- Experimental validation across three environments

**Low confidence**:
- Oracle performance in high-dimensional state spaces
- Sample complexity and scalability to problems with more than 4 objectives

## Next Checks

1. **Oracle Performance Analysis**: Systematically evaluate how different oracle implementations (DQN, A2C, PPO) affect IPRO's convergence rate and final solution quality across environments with varying Pareto front geometries.

2. **Scalability Study**: Test IPRO on problems with more than 4 objectives to empirically verify the claimed exponential complexity scaling and identify practical limits of the approach.

3. **Robustness Testing**: Evaluate IPRO's performance when the Pareto oracle occasionally fails to find feasible solutions in target regions, measuring the impact on final Pareto front completeness and convergence guarantees.