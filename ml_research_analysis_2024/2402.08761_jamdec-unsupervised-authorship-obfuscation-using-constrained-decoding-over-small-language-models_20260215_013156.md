---
ver: rpa2
title: 'JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over
  Small Language Models'
arxiv_id: '2402.08761'
source_url: https://arxiv.org/abs/2402.08761
tags:
- jamdec
- text
- cola
- authorship
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JAMDEC, an unsupervised, inference-time
  authorship obfuscation method using constrained decoding with small language models.
  The approach addresses challenges of authorship obfuscation: lack of supervision
  data for diverse authorship and domains, and the need for substantial revision while
  preserving content and fluency.'
---

# JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models

## Quick Facts
- arXiv ID: 2402.08761
- Source URL: https://arxiv.org/abs/2402.08761
- Reference count: 40
- Key outcome: JAMDEC achieves high style concealment, content preservation, and language quality across scholarly and diary-style datasets with 3-10 authors using small language models

## Executive Summary
JAMDEC introduces an unsupervised authorship obfuscation method that uses constrained decoding with small language models to rewrite text while preserving content and improving fluency. The approach addresses the challenge of lacking supervision data for diverse authorship by employing inference-time techniques rather than model training. By combining keyword constraints with diversity penalties through Constrained Diverse Beam Search (CoDi-BS), JAMDEC generates varied candidates that satisfy content requirements while avoiding style detection. Experimental results demonstrate that JAMDEC outperforms state-of-the-art methods using comparably small models and performs competitively against GPT3.5 175B, which is two orders of magnitude larger.

## Method Summary
JAMDEC is an unsupervised inference-time algorithm that operates in three stages: keyword extraction, constrained generation, and filtering. First, it extracts content-bearing tokens using likelihood scores from language models, representing words the model struggles to generate accurately. These keywords serve as constraints for the generation stage, where CoDi-BS combines keyword constraints with diversity penalties to produce varied candidates. The method uses GPT2-XL with beam width 50 and sampling/greedy decoding to generate multiple candidates. Finally, a multi-stage filtering process applies NLI for semantic similarity, CoLA for grammatical correctness, and optionally stylometric filtering to ensure the final output meets all three objectives of style concealment, content preservation, and language quality.

## Key Results
- JAMDEC outperforms previous state-of-the-art methods on style concealment, content preservation, and language quality metrics
- The method performs competitively against GPT3.5 175B despite using a model that is two orders of magnitude smaller
- Achieves high performance across both scholarly articles and diary-style datasets with 3-10 authors each

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained Diverse Beam Search (CoDi-BS) improves creative power of small models by balancing content preservation and style diversity.
- Mechanism: Combines keyword constraints from Constrained Beam Search with diversity penalties from Diverse Beam Search to generate varied candidates that satisfy content requirements.
- Core assumption: Adding diversity penalty while maintaining keyword constraints produces more diverse yet content-preserving generations.
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If diversity penalty overwhelms constraint satisfaction, leading to content loss or irrelevant generations.

### Mechanism 2
- Claim: Keyword extraction using likelihood scores identifies content-bearing tokens that guide generation toward preserving original meaning.
- Mechanism: Tokens with lowest conditional probabilities from language models are selected as keywords, representing content the model struggles to generate accurately.
- Core assumption: Low probability tokens correspond to semantically important content that should be preserved during rewriting.
- Evidence anchors: [section 3.1], [section 4.3]
- Break condition: If keyword selection becomes too broad or too narrow, either failing to constrain content or overly restricting generation diversity.

### Mechanism 3
- Claim: Multi-stage filtering (NLI + CoLA + optional stylometric) ensures content preservation and language quality while achieving obfuscation.
- Mechanism: First filters generations for semantic similarity to original (NLI), then for grammatical correctness (CoLA), optionally further filters for style concealment (stylometric).
- Core assumption: Sequential filtering progressively refines candidate pool to meet all three objectives simultaneously.
- Evidence anchors: [abstract], [section 4.1]
- Break condition: If filtering thresholds are too strict, original sentence must be used; if too lenient, quality and preservation suffer.

## Foundational Learning

- Concept: Beam Search and its variants (Diverse Beam Search, Constrained Beam Search)
  - Why needed here: Forms the core generation mechanism for producing diverse, constraint-satisfying candidates
  - Quick check question: What's the difference between standard beam search and diverse beam search in terms of objective function?

- Concept: Keyword extraction methods (embedding-based vs likelihood-based)
  - Why needed here: Determines which tokens guide content preservation during generation
  - Quick check question: Why might likelihood-based keyword extraction be more effective than embedding-based for this task?

- Concept: Natural Language Inference (NLI) and grammatical acceptability evaluation (CoLA)
  - Why needed here: Provides automated metrics for content preservation and language quality filtering
  - Quick check question: How does NLI differ from simple token overlap metrics like METEOR in measuring content preservation?

## Architecture Onboarding

- Component map: Pre-processing -> Keyword Extraction -> Generation -> NLI Filtering -> CoLA Filtering -> Output
- Critical path: Pre-processing → Keyword Extraction → Generation → NLI Filtering → CoLA Filtering → Output
- Design tradeoffs:
  - Beam width vs. runtime: Higher beam width increases diversity but slows processing
  - Keyword extraction method vs. constraint quality: Different methods capture different aspects of content
  - Filtering thresholds vs. coverage: Stricter thresholds improve quality but may force use of original text
- Failure signatures:
  - All generations fail NLI/CoLA: Original sentence used, indicates filtering too strict or generation poor
  - Very few generations pass: Consider loosening thresholds or improving keyword extraction
  - Content not preserved: Check keyword extraction quality and constraint satisfaction
  - Poor language quality: Verify CoLA model performance and generation decoding parameters
- First 3 experiments:
  1. Run JAMDEC with minimum configuration (beam width 10, one keyword extraction method) on AMT-3 to verify basic functionality
  2. Compare performance with and without diversity penalty in CoDi-BS to measure impact on generation diversity
  3. Test different NLI/CoLA threshold combinations to find optimal balance between quality and coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different keyword extraction methods (KeyBERT, Likelihood-T5, Likelihood-GPT2) on the quality and diversity of the generated obfuscated text?
- Basis in paper: [explicit] The paper mentions using three keyword extraction methods and experimenting with using them individually or combined.
- Why unresolved: The paper shows some results comparing these methods, but doesn't provide a comprehensive analysis of their individual impact on the final obfuscated text quality and diversity.
- What evidence would resolve it: A detailed ablation study showing the performance of JAMDEC using each keyword extraction method individually across all evaluation metrics would clarify their relative contributions.

### Open Question 2
- Question: How does the choice of base language model (e.g., GPT2-XL vs. other models) affect the performance of JAMDEC in terms of authorship obfuscation quality and computational efficiency?
- Basis in paper: [inferred] The paper focuses on using GPT2-XL as the base model, but doesn't explore how other models might perform in the JAMDEC framework.
- Why unresolved: Different base models may have varying capabilities in generating diverse and fluent text, which could impact the effectiveness of constrained decoding.
- What evidence would resolve it: Comparative experiments using JAMDEC with different base models while measuring both performance metrics and inference time would provide insights into optimal model choices.

### Open Question 3
- Question: What is the optimal balance between the diversity penalty and constraint penalty in the CoDi-BS algorithm for maximizing authorship obfuscation while maintaining content preservation and fluency?
- Basis in paper: [explicit] The paper introduces the CoDi-BS algorithm which combines diversity and constraint penalties, but the specific weights are not extensively explored or optimized.
- Why unresolved: The trade-off between diversity (to avoid style detection) and constraint satisfaction (to preserve content) is crucial for effective obfuscation.
- What evidence would resolve it: A parameter sensitivity analysis showing how different combinations of diversity and constraint weights affect all evaluation metrics would help identify optimal settings.

## Limitations

- The study relies heavily on stylometric classifiers (ENS and BertAA) which may not capture all aspects of writing style used by humans
- Computational resources required for CoDi-BS with beam width 50 and 3 generation rounds are substantial, potentially limiting real-world deployment
- The filtering approach occasionally fails, requiring fallback to original sentences which undermines the obfuscation goal in those cases

## Confidence

**High Confidence**: The claim that JAMDEC outperforms previous methods on style concealment, content preservation, and language quality. This is supported by comprehensive quantitative comparisons across multiple datasets and metrics, with statistically significant improvements demonstrated.

**Medium Confidence**: The claim that JAMDEC achieves competitive performance against GPT3.5 despite using much smaller models. While numerical results support this, the comparison is limited to specific datasets and may not generalize to all authorship scenarios.

**Low Confidence**: The claim that constrained diverse beam search fundamentally enables small models to match larger models' creative power. The paper lacks direct ablation studies isolating the impact of the CoDi-BS mechanism from other factors.

## Next Checks

1. **Ablation study of decoding mechanisms**: Systematically compare JAMDEC performance with standard beam search, Diverse Beam Search alone, Constrained Beam Search alone, and CoDi-BS to isolate the contribution of each component to overall performance. Measure style concealment, content preservation, and language quality for each variant across multiple datasets.

2. **Adversarial robustness testing**: Evaluate JAMDEC against adaptive authorship verification methods that incorporate knowledge of the obfuscation technique. Test whether an attacker who knows the obfuscation pipeline can still identify authors with high accuracy, and measure the degradation in performance.

3. **Resource efficiency analysis**: Conduct detailed runtime and memory usage measurements for JAMDEC across different beam widths, generation rounds, and dataset sizes. Compare the actual computational overhead against claimed efficiency gains from using smaller models directly, and identify the break-even point where JAMDEC becomes less efficient than using larger models directly.