---
ver: rpa2
title: Combining Automated Optimisation of Hyperparameters and Reward Shape
arxiv_id: '2406.18293'
source_url: https://arxiv.org/abs/2406.18293
tags:
- reward
- optimisation
- dehb
- performance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the joint optimisation of hyperparameters
  and reward shaping in reinforcement learning. Prior research treated these problems
  separately, despite their mutual dependency.
---

# Combining Automated Optimisation of Hyperparameters and Reward Shape

## Quick Facts
- arXiv ID: 2406.18293
- Source URL: https://arxiv.org/abs/2406.18293
- Authors: Julian Dierkes; Emma Cramer; Holger H. Hoos; Sebastian Trimpe
- Reference count: 40
- This work investigates the joint optimisation of hyperparameters and reward shaping in reinforcement learning, showing that combined optimisation consistently matches or exceeds individual optimisation in performance.

## Executive Summary
This paper addresses a fundamental gap in reinforcement learning by proposing a methodology for jointly optimising hyperparameters and reward shaping parameters. While prior research treated these problems separately, the authors demonstrate that hyperparameters and reward parameters are mutually dependent, meaning neither can be fully optimised without appropriate values for the other. Using the state-of-the-art black-box optimisation algorithm DEHB, they conduct experiments across four diverse environments with both PPO and SAC algorithms. The results show that joint optimisation consistently matches or exceeds individual optimisation in performance, with only minor computational overhead, making it a best practice for RL applications.

## Method Summary
The methodology employs DEHB (Dynamic, Evolutionary, Hyperband) for joint optimisation of hyperparameters and reward parameters in reinforcement learning. The approach treats reward parameters as additional hyperparameters to be optimised alongside traditional RL hyperparameters. The search space includes both algorithm hyperparameters (learning rates, network sizes, etc.) and reward shaping parameters specific to each environment. The optimisation uses multi-fidelity evaluation, starting with limited training budgets and progressing to full training as configurations show promise. A variance penalty can be included as a multi-objective metric to improve policy stability. The method is evaluated on four environments (LunarLander, Ant, Humanoid, Wipe) using both PPO and SAC algorithms.

## Key Results
- Joint optimisation consistently matches or exceeds individual optimisation in performance across all tested environments
- Multi-objective optimisation with variance penalty improves policy stability with only marginal performance loss
- Computational overhead of joint optimisation is minor compared to individual optimisation despite larger search spaces
- The approach is particularly effective in complex environments like Humanoid and Wipe
- Including reward scaling parameters (λ) is crucial, especially for SAC, to prevent performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimisation works because hyperparameters and reward parameters are mutually dependent
- Mechanism: Hyperparameters influence how the agent learns from rewards, while reward parameters shape what the agent learns. Poor settings in one component limit the potential of the other.
- Core assumption: The performance landscape contains complex interactions between hyperparameters and reward parameters
- Evidence anchors: Empirical results show interdependency of varying strength between hyperparameters and reward parameters across all tested environments
- Break condition: If the interaction between hyperparameters and reward parameters is weak or if one component dominates the performance

### Mechanism 2
- Claim: Including a variance penalty improves policy stability without significant performance loss
- Mechanism: The optimisation algorithm is encouraged to find configurations that produce consistent results across different random seeds
- Core assumption: Policies with lower variance in performance are more robust and reliable in real-world applications
- Evidence anchors: Multi-objective optimisation improves policy stability with only marginal performance loss, sometimes achieving slight gains
- Break condition: If the variance penalty is too strong, it could overly constrain the search space

### Mechanism 3
- Claim: Multi-fidelity optimisation allows efficient exploration of the combined search space
- Mechanism: DEHB evaluates many parameter configurations with limited training budgets first, then focuses computational resources on promising configurations
- Core assumption: The performance ranking of parameter configurations is relatively stable across different training budgets
- Evidence anchors: DEHB has been demonstrated to outperform random search for hyperparameter optimisation; combined optimisation is comparable in speed to individual approaches
- Break condition: If the performance ranking changes significantly with training budget

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (Markov Decision Processes, policies, returns)
  - Why needed here: The paper builds on RL concepts to formulate the joint optimisation problem
  - Quick check question: What is the difference between the shaped reward function and the task objective in RL?

- Concept: Hyperparameter optimisation methods (grid search, random search, Bayesian optimisation)
  - Why needed here: The paper uses DEHB, a specific multi-fidelity optimisation method
  - Quick check question: How does multi-fidelity optimisation differ from standard hyperparameter optimisation?

- Concept: Reward shaping principles
  - Why needed here: The paper optimises reward parameters as part of the joint optimisation
  - Quick check question: Why might adding intermediate rewards (reward shaping) be necessary even when the final task objective is clear?

## Architecture Onboarding

- Component map: DEHB optimiser -> RL training environment -> Performance evaluator -> Variance calculator -> Configuration manager -> Back to DEHB
- Critical path: DEHB selects parameters → RL training runs with those parameters → Performance evaluated → Results fed back to DEHB → Incumbent updated → Repeat
- Design tradeoffs:
  - Larger search space (joint optimisation) vs. computational cost
  - Single-objective (performance only) vs. multi-objective (performance + stability)
  - Explicit reward scaling (normalised weights) vs. implicit scaling (weight magnitudes)
  - Fixed vs. optimised reward scale parameter
- Failure signatures:
  - No improvement over baselines despite larger search space
  - High variance in incumbent performance across optimisation runs
  - Significant performance drop when switching from shaped rewards to task objectives
  - Optimisation getting stuck in local optima
- First 3 experiments:
  1. Replicate single-objective optimisation on LunarLander to establish baseline performance
  2. Run multi-objective optimisation on the same environment to test variance penalty effects
  3. Test joint optimisation on a simpler environment (like Ant) before moving to complex ones (Humanoid, Wipe)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do more sophisticated risk-averse metrics compare to the simple variance penalty in multi-objective optimisation for RL?
- Basis in paper: The authors suggest investigating more sophisticated risk-averse metrics in future work
- Why unresolved: The paper only tests a simple variance penalty and acknowledges the potential for more advanced risk-averse metrics without exploring them
- What evidence would resolve it: Comparative experiments showing performance and stability trade-offs between variance penalty and other risk-averse metrics

### Open Question 2
- Question: What is the optimal balance between computational cost and performance improvement when jointly optimising hyperparameters and reward shape?
- Basis in paper: The authors note that combined optimisation achieves competitive performance with only minor increases in computational costs
- Why unresolved: The paper focuses on demonstrating feasibility and effectiveness but doesn't systematically analyze the computational trade-offs
- What evidence would resolve it: Empirical studies varying optimisation budgets and measuring both performance gains and computational costs

### Open Question 3
- Question: How sensitive are the joint optimisation results to the choice of search space for hyperparameters and reward parameters?
- Basis in paper: The authors use specific search spaces but acknowledge that their choices may impact results
- Why unresolved: The paper uses fixed search spaces without exploring how different search space designs affect optimisation performance
- What evidence would resolve it: Experiments systematically varying search space bounds and granularities

## Limitations
- The study is limited to four specific environments, which may not represent the full diversity of RL problems
- The computational overhead analysis doesn't provide detailed quantification of the trade-off between search space size and optimisation time
- The paper doesn't explore alternative multi-objective metrics beyond simple variance penalty

## Confidence

**Confidence Labels:**
- Mechanism 1 (mutual dependency): Medium - Strong empirical evidence but limited theoretical explanation
- Mechanism 2 (variance penalty): Medium - Consistent improvements but small sample size
- Mechanism 3 (DEHB efficiency): High - Clear demonstration of comparable computational cost

## Next Checks

1. Test joint optimisation across 10+ diverse RL environments to verify the generalisability of mutual dependency findings
2. Conduct ablation studies systematically removing variance penalty to quantify its contribution to policy stability
3. Compare DEHB with alternative multi-fidelity methods (like Hyperband or ASHA) on identical joint optimisation tasks to validate efficiency claims