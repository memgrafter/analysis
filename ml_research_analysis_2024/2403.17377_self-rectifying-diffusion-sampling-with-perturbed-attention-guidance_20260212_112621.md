---
ver: rpa2
title: Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
arxiv_id: '2403.17377'
source_url: https://arxiv.org/abs/2403.17377
tags:
- guidance
- diffusion
- sampling
- samples
- unconditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of existing diffusion sampling
  guidance techniques, such as classifier guidance and classifier-free guidance, which
  often require additional training or external modules and are unavailable in unconditional
  generation. The authors propose Perturbed-Attention Guidance (PAG), a novel sampling
  guidance method that improves diffusion sample quality across both unconditional
  and conditional settings without requiring additional training or external modules.
---

# Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance

## Quick Facts
- **arXiv ID**: 2403.17377
- **Source URL**: https://arxiv.org/abs/2403.17377
- **Reference count**: 40
- **Primary result**: Perturbed-Attention Guidance (PAG) improves sample quality across both unconditional and conditional settings without requiring additional training or external modules

## Executive Summary
This paper introduces Perturbed-Attention Guidance (PAG), a novel sampling guidance method for diffusion models that improves sample quality by leveraging perturbed self-attention maps as an implicit discriminator. Unlike classifier guidance or classifier-free guidance which require additional training or external modules, PAG works directly with existing pretrained diffusion models. The method replaces selected self-attention maps in the U-Net with an identity matrix to generate degraded samples, then uses the difference between original and perturbed predictions to guide the denoising process away from structural collapse. Applied to ADM and Stable Diffusion, PAG significantly improves FID scores in both conditional and unconditional generation, and shows particular strength in downstream tasks where traditional guidance methods are unavailable.

## Method Summary
PAG works by creating a dual forward pass system where the original diffusion U-Net processes noisy latents alongside a perturbed version where selected self-attention maps are replaced with identity matrices. This perturbation removes structural information while preserving appearance, generating degraded intermediate samples. The guidance signal is computed as the difference between the original and perturbed noise predictions, scaled by a guidance factor, and added to the original prediction. This steers the sampling process away from the undesirable (degraded) distribution toward the desirable distribution learned by the pretrained model. The method requires no additional training, works with existing diffusion architectures, and can be applied to both unconditional and conditional generation scenarios.

## Key Results
- PAG achieves FID 3.35 on ImageNet 256×256 unconditional generation with ADM, improving over baseline by 20%
- For text-to-image generation on MS-COCO, PAG achieves FID 13.4 compared to 15.5 for baseline, outperforming CFG in quality metrics
- In ControlNet with empty prompts, PAG achieves FID 22.6 versus 34.8 for baseline, where CFG cannot be applied due to lack of text conditioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PAG improves sample quality by steering the denoising trajectory away from structural collapse using perturbed self-attention maps.
- Mechanism: PAG replaces selected self-attention maps in the diffusion U-Net with an identity matrix, generating intermediate samples with degraded structure. These degraded samples act as an implicit discriminator that guides the denoising process away from collapsed structures.
- Core assumption: Self-attention maps capture structural information between image patches, and substituting them with identity matrix removes this structural information while preserving appearance.
- Evidence anchors: [abstract] "by considering the self-attention mechanisms' ability to capture structural information, and guiding the denoising process away from these degraded samples"; [section 4.2] "Motivated by this insight, we focus on perturbing only the self-attention map to minimize excessive deviation from the original sample"; [corpus] Weak evidence - no direct citations about self-attention capturing structure, but referenced in related works [2,15,58,59]
- Break condition: If self-attention maps do not capture structural information as assumed, or if replacing with identity matrix causes out-of-distribution issues for subsequent layers.

### Mechanism 2
- Claim: PAG works as a perturbation guidance framework where the perturbed forward pass acts as an implicit discriminator distinguishing desirable from undesirable samples.
- Mechanism: PAG creates an energy function using the implicit discriminator D, where undesirable samples are generated by perturbing the self-attention maps. The sampling process adds guidance that pushes samples away from this undesirable distribution.
- Core assumption: The pretrained diffusion model has already learned the desired distribution, so we can approximate the score for desirable samples using the existing model.
- Evidence anchors: [section 4.1] "we define a new diffusion sampling such that ˜ϵθ(xt) = ϵθ(xt) + sσt∇xt LG"; [section 4.1] "the implicit discriminator D guides samples towards the desirable distribution and away from the undesirable distribution"; [corpus] Moderate evidence - connection to WGAN and energy-based models is established
- Break condition: If the perturbed samples deviate too far from the original distribution, causing the guidance signal to be incorrect.

### Mechanism 3
- Claim: PAG complements classifier-free guidance (CFG) by addressing different aspects of the sampling process - CFG handles early-stage prompt alignment while PAG handles mid-to-late stage structural refinement.
- Mechanism: CFG provides strong guidance early in the sampling process when the model relies heavily on the prompt, while PAG provides continuous structural guidance throughout the sampling process, especially in mid-to-late stages when self-attention maps become more active.
- Core assumption: The sampling process transitions from prompt reliance to visual feature reliance, with self-attention maps becoming more important in later stages.
- Evidence anchors: [section E.3] "This shift is logical; in the early stage, the model relies on the prompt for cues on what to denoise in the image. As the denoising process progresses and the images take shape, the model shifts focus to refine these emerging visual details."; [section E.3] "Compared with CFG, PAG continues to influence throughout the mid to late stages of the timestep, offering highly detailed guidance"; [corpus] Strong evidence - supported by recent research [2,41] on temporal dynamics of diffusion models
- Break condition: If the model's temporal dynamics differ significantly from what's assumed, or if CFG and PAG interfere rather than complement each other.

## Foundational Learning

- **Concept: Diffusion models and the denoising process**
  - Why needed here: PAG is applied during the reverse sampling process of diffusion models, so understanding how noise is iteratively removed is fundamental
  - Quick check question: Can you explain the forward and reverse processes in diffusion models, and how the noise schedule works?

- **Concept: Self-attention mechanisms in transformers**
  - Why needed here: PAG specifically targets self-attention maps in the U-Net architecture, requiring understanding of query-key-value operations and attention map computation
  - Quick check question: How are query, key, and value matrices computed in self-attention, and what does the attention map represent?

- **Concept: Classifier-free guidance (CFG) and its mathematical formulation**
  - Why needed here: PAG builds on the guidance framework established by CFG, extending it to unconditional settings through perturbed self-attention
  - Quick check question: What is the mathematical difference between conditional and unconditional noise predictions in CFG, and how does the guidance scale affect the output?

## Architecture Onboarding

- **Component map**: U-Net backbone (ADM/Stable Diffusion) -> Self-attention modules within U-Net layers -> Perturbed self-attention (PSA) implementation -> Dual forward pass system (original + perturbed) -> Guidance combination layer -> Sampling scheduler (DDIM or DDPM)

- **Critical path**: 1) Input noisy latent xt enters both original and perturbed U-Net branches; 2) Self-attention maps are replaced with identity matrix in perturbed branch; 3) Noise predictions ϵθ(xt) and ˆϵθ(xt) are computed; 4) Guidance signal is computed: s(ϵθ(xt) - ˆϵθ(xt)); 5) Final noise prediction: ˜ϵθ(xt) = ϵθ(xt) + s(ϵθ(xt) - ˆϵθ(xt)); 6) Denoising step produces xt-1 using the final prediction; 7) Repeat until clean image x0 is generated

- **Design tradeoffs**: Layer selection for perturbation: Deeper layers (near bottleneck) generally perform better but may require more computation; Guidance scale s: Higher values improve quality but can cause over-saturation; finding optimal scale is dataset-dependent; Identity matrix vs other perturbations: Identity matrix preserves appearance while removing structure; other perturbations may cause excessive deviation

- **Failure signatures**: Artifacts appearing at high guidance scales; Loss of fine details when perturbation is too aggressive; Inconsistent improvements across different layers; Degradation in diversity metrics when quality improves

- **First 3 experiments**: 1) Apply PAG to ADM unconditional model on ImageNet 256×256 and measure FID/IS improvements compared to baseline; 2) Test different guidance scales (0.0 to 5.0) on Stable Diffusion unconditional generation to find optimal scale; 3) Compare identity matrix perturbation vs random masking vs additive noise on self-attention maps using ADM unconditional model

## Open Questions the Paper Calls Out

- **Open Question 1**: Does training the perturbed self-attention model (ˆϵθ) improve the stability and robustness of PAG compared to the current implementation without additional training? [explicit] The authors mention that although their carefully designed perturbed self-attention method effectively mitigates the out-of-distribution issue without additional training, incorporating training can further improve its ability to address the OOD problem and enhance its robustness to hyperparameter settings. Why unresolved: The paper does not provide experimental results comparing the current PAG implementation with a trained version of ˆϵθ. What evidence would resolve it: Conducting experiments with a trained ˆϵθ and comparing its performance (e.g., FID, IS, LPIPS) with the current PAG implementation would provide insights into the potential benefits of additional training.

- **Open Question 2**: What are the optimal perturbation strategies for different tasks and models beyond the identity matrix replacement used in PAG? [inferred] The authors suggest that while the identity matrix replacement is an effective method, there might be better perturbations for different tasks and models, and they leave this exploration as future work. Why unresolved: The paper only explores the identity matrix replacement as the perturbation strategy and does not investigate other potential methods. What evidence would resolve it: Experimenting with various perturbation strategies (e.g., random masking, additive noise, Gaussian blur) and evaluating their performance on different tasks and models would help identify optimal perturbation methods.

- **Open Question 3**: How does the choice of layers for applying perturbed self-attention affect the performance of PAG across different diffusion models? [explicit] The authors conduct an ablation study on layer selection and find that perturbations applied to deeper layers generally yield relatively better outcomes compared to those applied to shallower layers of U-Net. Why unresolved: The paper does not provide a comprehensive analysis of the impact of layer selection on PAG's performance across various diffusion models. What evidence would resolve it: Performing extensive experiments with different layer selections on various diffusion models (e.g., ADM, Stable Diffusion, Latent Diffusion Models) and comparing their performance would help determine the optimal layer choices for PAG.

## Limitations

- **Structural assumption weakness**: The core mechanism relies on the assumption that self-attention maps capture structural information between image patches, but lacks direct empirical validation of this specific claim within the paper.
- **Layer selection sensitivity**: The paper mentions that deeper layers near the bottleneck generally perform better for PAG, but provides limited systematic analysis of layer selection across different models.
- **Guidance scale optimization**: While various guidance scales are tested, the optimal scale appears dataset-dependent and model-specific, with break conditions where excessive perturbation causes out-of-distribution samples not thoroughly characterized.

## Confidence

- **High confidence**: The empirical improvements in FID/IS scores across multiple datasets (ImageNet, FFHQ, MS-COCO) and the successful application to downstream tasks (inpainting, super-resolution, ControlNet) are well-documented and reproducible.
- **Medium confidence**: The mechanism of how perturbed self-attention maps act as an implicit discriminator is theoretically sound and builds on established connections to energy-based models, but lacks direct empirical validation of the structural information capture assumption.
- **Low confidence**: The paper's claims about PAG being particularly effective in scenarios where CFG cannot be fully utilized (such as ControlNet with empty prompts) are demonstrated but not deeply analyzed.

## Next Checks

1. **Layer selection ablation study**: Systematically test PAG with perturbed self-attention at different U-Net layers (shallow, middle, deep) across both ADM and Stable Diffusion models, measuring FID/IS quality and computational overhead to identify optimal layer selection criteria.

2. **Structural information validation**: Design experiments to empirically validate whether self-attention maps capture structural information as claimed. This could involve visualizing attention patterns, comparing perturbed samples with and without identity matrix substitution, and measuring structural similarity metrics.

3. **Break condition characterization**: Identify the threshold where PAG guidance becomes detrimental by systematically varying guidance scales beyond the optimal range (e.g., s=0 to 10) and analyzing the point where FID degradation, artifact emergence, or diversity loss becomes significant.