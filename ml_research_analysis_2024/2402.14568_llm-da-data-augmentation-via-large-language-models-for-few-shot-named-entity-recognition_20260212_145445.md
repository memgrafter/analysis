---
ver: rpa2
title: 'LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity
  Recognition'
arxiv_id: '2402.14568'
source_url: https://arxiv.org/abs/2402.14568
tags:
- data
- augmentation
- entity
- sentence
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLM-DA, a data augmentation method using large
  language models for few-shot named entity recognition (NER). The method leverages
  LLMs' rewriting capabilities and world knowledge to generate semantically coherent
  sentences while preserving entities.
---

# LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition

## Quick Facts
- arXiv ID: 2402.14568
- Source URL: https://arxiv.org/abs/2402.14568
- Authors: Junjie Ye; Nuo Xu; Yikun Wang; Jie Zhou; Qi Zhang; Tao Gui; Xuanjing Huang
- Reference count: 24
- Primary result: LLM-DA significantly improves NER model performance compared to existing methods in low-resource scenarios

## Executive Summary
This paper proposes LLM-DA, a data augmentation method using large language models for few-shot named entity recognition (NER). The method leverages LLMs' rewriting capabilities and world knowledge to generate semantically coherent sentences while preserving entities. LLM-DA augments data at both context and entity levels using 14 contextual rewriting strategies and entity replacements of the same type. Noise injection is also used to enhance robustness. Experiments show that LLM-DA significantly improves NER model performance compared to existing methods, especially in low-resource scenarios. The quality of LLM-DA-generated data is also found to be higher than other methods.

## Method Summary
LLM-DA is a data augmentation technique for few-shot NER that operates at both context and entity levels. It uses 14 contextual rewriting strategies across four dimensions (sentence length, vocabulary usage, subordinate clauses, presentation styles) and entity replacements guided by LLM world knowledge. The method employs a two-stage approach, first performing context-level augmentation followed by entity-level augmentation to maintain semantic coherence. Noise injection enhances robustness. The approach uses a greedy algorithm for k-shot sampling and specialized prompts for GPT-3.5-turbo to generate augmented data, which is then filtered and annotated before training NER models.

## Key Results
- LLM-DA significantly outperforms existing data augmentation methods in few-shot NER scenarios
- The method achieves superior performance especially in low-resource settings
- LLM-DA-generated data demonstrates higher quality compared to other augmentation approaches
- Context-level augmentation is particularly effective for larger k-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-DA improves NER performance by augmenting data at both contextual and entity levels, ensuring semantic coherence while increasing diversity.
- Mechanism: The method uses 14 contextual rewriting strategies and entity replacements of the same type, guided by LLM world knowledge, to generate semantically coherent sentences while preserving entity types. Noise injection enhances robustness.
- Core assumption: LLM rewriting capabilities can preserve entity semantics while generating diverse contexts.
- Evidence anchors:
  - [abstract]: "We propose LLM-DA, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels."
  - [section 2.2]: "To impose necessary constraints on the generated content while preserving the diversity of the augmented data, we apply data augmentation at both the context and entity levels."
  - [corpus]: Corpus provides related work on NER data augmentation but lacks direct evidence of LLM-DA's specific mechanisms.
- Break condition: If LLM-generated text frequently violates entity type constraints or produces incoherent contexts despite prompt engineering.

### Mechanism 2
- Claim: LLM-DA achieves superior data quality compared to existing methods by leveraging LLM world knowledge for entity replacement and context rewriting.
- Mechanism: Entity-level augmentation uses LLM knowledge to replace entities with others of the same type, going beyond training data limitations. Context-level augmentation employs 14 strategies across four dimensions (sentence length, vocabulary usage, subordinate clauses, presentation styles).
- Core assumption: LLM world knowledge enables generation of diverse, contextually appropriate entities beyond training data.
- Evidence anchors:
  - [abstract]: "To enrich context-level augmentation, we present a diverse set of 14 contextual rewriting strategies... For entity-level augmentation, we leverage the extensive world knowledge of LLMs to substitute entities in a sentence with others of the same type."
  - [section 2.2]: "Unlike existing entity replacement methods... our approach leverages the world knowledge embedded within LLMs, allowing the generation of entities beyond those present in the training set."
  - [corpus]: Corpus shows related work on NER data augmentation but doesn't provide evidence of LLM-DA's specific quality advantages.
- Break condition: If LLM world knowledge fails to generate appropriate entities or contexts for specific domains or entity types.

### Mechanism 3
- Claim: LLM-DA's two-stage augmentation (context then entity) maintains meaningful connections between generated sentences and original data while avoiding excessive deviation.
- Mechanism: First performs context-level augmentation, then applies entity-level augmentation to already contextually augmented sentences. This prevents the inherent uncertainty of LLM generation from producing sentences too far from original meaning.
- Core assumption: Sequential application of context and entity augmentation preserves semantic relationships better than simultaneous application.
- Evidence anchors:
  - [abstract]: "To address this, we adopt a two-stage strategy that first performs context-level augmentation and then follows up with entity-level augmentation based on the already generated contextually augmented sentences."
  - [section 2.2]: "However, when both levels are simultaneously rewritten, the inherent uncertainty of the generated content often leads to new sentences that deviate significantly from the original, posing challenges for label handling."
  - [corpus]: Corpus lacks direct evidence for this specific two-stage mechanism.
- Break condition: If context-level augmentation introduces too much variation, making entity-level augmentation ineffective or if entity replacement disrupts contextually coherent sentences.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Understanding NER task requirements is crucial for designing appropriate data augmentation strategies that preserve entity semantics while generating diverse contexts.
  - Quick check question: What are the key challenges in few-shot NER that LLM-DA aims to address?

- Concept: Large Language Model (LLM) capabilities
  - Why needed here: LLM rewriting capabilities and world knowledge form the foundation of LLM-DA's augmentation approach. Understanding these capabilities is essential for effective prompt engineering.
  - Quick check question: How do LLM rewriting capabilities differ from traditional data augmentation methods in terms of semantic preservation?

- Concept: Data augmentation techniques
  - Why needed here: Familiarity with existing data augmentation methods helps understand LLM-DA's innovations and advantages over traditional approaches.
  - Quick check question: What are the main limitations of existing NER data augmentation methods that LLM-DA aims to overcome?

## Architecture Onboarding

- Component map: Data Sampling -> Data Augmentation (Context/Entity/Both) -> Data Annotation -> LLM Interface -> Model Training

- Critical path:
  1. Sample k-shot data using greedy algorithm
  2. Generate augmented data through context and entity-level strategies
  3. Filter and annotate augmented data
  4. Train NER model on combined original and augmented data

- Design tradeoffs:
  - Augmentation diversity vs. semantic coherence
  - Context-level vs. entity-level augmentation effectiveness across dataset sizes
  - Noise injection amount vs. model robustness
  - Augmentation ratio vs. performance improvement

- Failure signatures:
  - Performance degradation with increased augmentation ratio
  - Entity type violations in augmented data
  - Loss of contextual coherence in generated sentences
  - Overfitting to augmented data distribution

- First 3 experiments:
  1. Compare LLM-DA (Context) vs. Gold baseline on CoNLL'03 5-shot scenario
  2. Evaluate LLM-DA (Entity) vs. MELM on OntoNotes 5.0 10-shot scenario
  3. Test LLM-DA (All) vs. DAGA on MIT-Movie 20-shot scenario

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas for future research are implied:
- How does LLM-DA perform when applied to domains or languages beyond those tested in the paper?
- What is the impact of using larger or smaller language models than GPT-3.5 for LLM-DA?
- How does the quality of LLM-DA-generated data change over time as the LLM's training data becomes outdated?

## Limitations
- The specific prompt templates for LLM-based augmentation are not fully disclosed, making exact reproduction challenging
- The paper doesn't provide extensive ablation studies on the 14 contextual strategies to determine which contribute most to performance gains
- The filtering criteria for augmented data quality assessment are not clearly specified

## Confidence
- **High**: LLM-DA significantly improves NER performance compared to baseline methods in few-shot scenarios
- **Medium**: The two-stage augmentation approach (context then entity) is more effective than simultaneous augmentation
- **Medium**: LLM world knowledge enables generation of more diverse and contextually appropriate entities than training data alone

## Next Checks
1. Conduct ablation studies to determine the relative contribution of each of the 14 contextual strategies and the noise injection component
2. Test LLM-DA's performance across additional entity types and domains not represented in the evaluation datasets
3. Implement and compare alternative filtering mechanisms for augmented data to assess robustness of the approach to annotation quality