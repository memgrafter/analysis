---
ver: rpa2
title: Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation
  neural network ensemble
arxiv_id: '2402.07347'
source_url: https://arxiv.org/abs/2402.07347
tags:
- adversarial
- activation
- loss
- sign
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the robustness of 01 loss sign activation
  neural networks against TextFooler black-box adversarial attacks on four text classification
  datasets (IMDB, Yelp, MR, AG). The authors compare their sign activation networks
  to sigmoid activation cross entropy and binary neural networks.
---

# Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble

## Quick Facts
- arXiv ID: 2402.07347
- Source URL: https://arxiv.org/abs/2402.07347
- Reference count: 25
- Key outcome: 01 loss sign activation neural networks show significantly higher resistance to TextFooler attacks compared to baseline models, with up to 74.0% adversarial accuracy on AG news dataset versus 22.5% for sigmoid baseline.

## Executive Summary
This paper evaluates the robustness of 01 loss sign activation neural networks against TextFooler black-box adversarial attacks on four text classification datasets. The authors compare their sign activation networks to traditional sigmoid activation and binary neural networks. They find that 01 loss sign activation networks are significantly more resistant to TextFooler attacks, requiring more queries and achieving much higher adversarial accuracy. The authors also propose a novel variation with global pooling that further improves adversarial accuracy, making TextFooler practically useless against it.

## Method Summary
The authors train sign activation neural networks using stochastic coordinate descent to minimize 01 loss, contrasting with traditional gradient-based training for sigmoid activation networks. They employ four text classification datasets (IMDB, Yelp, MR, AG news) with pre-trained 200-dimensional GloVe embeddings. TextFooler is used as the black-box attack method, and adversarial accuracy is measured by evaluating model performance on generated adversarial examples. The models are ensembled across 8 instances for majority voting.

## Key Results
- Sign activation networks achieve significantly higher adversarial accuracy than baseline models across all four datasets
- CNN01-FS with global pooling variation shows up to 12% improvement in adversarial accuracy over standard CNN01
- TextFooler requires substantially more queries to attack sign activation networks, with some attacks failing completely
- Binary weight variant (SCD01) shows robustness comparable to continuous weight variant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 01 loss sign activation networks are harder to attack because they optimize a non-differentiable 0/1 loss instead of cross-entropy, making gradient-based attack methods less effective.
- Mechanism: TextFooler and similar black-box attacks rely on probability outputs to identify important words and craft adversarial examples. 01 loss networks produce binary outputs (0 or 1) that are thresholded and not informative for probabilistic reasoning.
- Core assumption: The attacker's effectiveness depends on the quality of probability estimates from the model.
- Evidence anchors:
  - [abstract] "Our models focus on the sign of the outputs in their loss. We output probabilities TextFooler needs by counting the number of 0 and 1 outputs in the ensemble. However, these probabilities are not useful enough for TextFooler to volley an effective attack."
  - [section] "One reason why our models are hard to deceive is that TextFooler relies upon output probabilities to craft its attack."
  - [corpus] No direct evidence in corpus; this is a novel contribution not found in related papers.
- Break condition: If an attacker can bypass probability estimation and directly optimize for sign outputs or use gradient-free optimization that works well with binary outputs.

### Mechanism 2
- Claim: The novel global pooling variation (CNN01-FS) improves adversarial accuracy by changing how word importance is aggregated.
- Mechanism: Traditional global average pooling averages +1 and -1 values, which can dilute the impact of important keywords. CNN01-FS sums only the +1 values, maintaining higher weights for positive contributions regardless of sentence length.
- Core assumption: The pooling method significantly affects how the network weights important words versus background text.
- Evidence anchors:
  - [section] "With sign activation and 01 loss the CNN becomes CNN01. We study a new variation of it where instead of global average pooling over +1 and -1 we only sum the 1's... We find this to significantly improve the adversarial accuracy of CNN01."
  - [section] "For example if we have two sentences of length 10 and 100 each with 4 keywords, by averaging the 4 words we have weights 40% and 4% respectively (lower in the larger sentence), but by summing it's the same in both."
  - [corpus] No direct evidence in corpus; this pooling variation appears to be a novel contribution.
- Break condition: If an attacker discovers that the summed +1 values create predictable patterns that can be exploited.

### Mechanism 3
- Claim: The stochastic coordinate descent training procedure creates models that are inherently more robust to adversarial attacks.
- Mechanism: The gradient-free training approach avoids learning smooth decision boundaries that are easy for adversarial attacks to exploit, instead creating more discontinuous decision surfaces.
- Core assumption: Non-gradient-based training leads to decision boundaries that are harder for gradient-based or probability-based attacks to navigate.
- Evidence anchors:
  - [abstract] "Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks."
  - [section] "We employ a stochastic coordinate descent algorithm... to minimize this objective" and "We can train sign activation networks with and without binary weights using our SCD training procedure"
  - [corpus] Weak evidence - related papers discuss various attack methods but don't specifically address gradient-free training as a defense mechanism.
- Break condition: If an attacker develops optimization methods specifically designed for discontinuous decision surfaces.

## Foundational Learning

- Concept: Stochastic Coordinate Descent for Non-Differentiable Losses
  - Why needed here: The 01 loss is non-differentiable, requiring gradient-free optimization methods like SCD to train the networks effectively.
  - Quick check question: Why can't we use standard backpropagation to train 01 loss networks?

- Concept: Sign Activation Functions and Their Properties
  - Why needed here: Understanding how sign activation creates binary outputs and affects network behavior is crucial for both implementation and explaining robustness.
  - Quick check question: How does a sign activation function differ from sigmoid or ReLU in terms of output range and gradient properties?

- Concept: TextFooler Attack Methodology
  - Why needed here: Understanding how TextFooler works (word replacement based on importance and semantic similarity) is essential for interpreting the results and designing defenses.
  - Quick check question: What are the key steps TextFooler uses to generate adversarial examples?

## Architecture Onboarding

- Component map: Word embeddings (200-dim GloVe) -> Model forward pass (MLP/BNN/CNN/CNN01/CNN01-FS) -> Ensemble voting (8 instances) -> Probability estimation (counting outputs) -> TextFooler attack attempt

- Critical path: Embedding → Model forward pass → Ensemble voting → Probability estimation → TextFooler attack attempt

- Design tradeoffs:
  - Binary vs continuous weights: Binary weights offer potential hardware efficiency but may reduce clean accuracy
  - Pooling method: Global average pooling vs summing +1 values affects both clean accuracy and adversarial robustness
  - Ensemble size: Larger ensembles improve robustness but increase computational cost

- Failure signatures:
  - Sharp drop in adversarial accuracy indicates attack success
  - High query count with low success rate suggests model is resisting attack
  - Discrepancy between clean and adversarial accuracy indicates vulnerability

- First 3 experiments:
  1. Implement SCD01 training and verify it produces sign activation models with reasonable clean accuracy on a small dataset
  2. Compare adversarial accuracy of SCD01 models vs MLP/BNN on a single dataset using TextFooler
  3. Implement CNN01-FS and test whether the global pooling variation improves adversarial accuracy compared to standard CNN01

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do 01 loss sign activation neural networks perform against other black-box adversarial attacks beyond TextFooler?
- Basis in paper: [explicit] The authors state their work suggests 01 loss sign activation networks could be further developed to create fool proof models against text adversarial attacks, but only tested against TextFooler.
- Why unresolved: The paper only evaluated robustness against one specific black-box attack method. Other attacks like PWWS, BAE, or BERT-based attacks may have different effectiveness.
- What evidence would resolve it: Testing 01 loss sign activation networks against a comprehensive suite of black-box text adversarial attacks and comparing their adversarial accuracy to baseline models.

### Open Question 2
- Question: What is the theoretical basis for why 01 loss sign activation networks are more resistant to adversarial attacks?
- Basis in paper: [inferred] The authors note that TextFooler relies on output probabilities to craft attacks, while 01 loss networks focus on the sign of outputs. However, they don't provide a theoretical explanation for the robustness.
- Why unresolved: The paper demonstrates empirical robustness but doesn't explain the underlying mechanism or theory behind why 01 loss networks resist attacks better.
- What evidence would resolve it: Mathematical analysis showing how the 01 loss function and sign activation create a more difficult optimization landscape for adversarial attacks, or formal proofs of robustness bounds.

### Open Question 3
- Question: How does the novel global pooling variation (CNN01-FS) in sign activation networks improve adversarial robustness?
- Basis in paper: [explicit] The authors state that CNN01-FS (which sums positive activations instead of averaging) significantly improves adversarial accuracy compared to the original CNN01, but don't explain why.
- Why unresolved: The paper shows empirical improvements but doesn't investigate the mechanism by which the pooling variation enhances robustness.
- What evidence would resolve it: Ablation studies showing how different pooling strategies affect gradient magnitudes, sensitivity to input perturbations, or the decision boundary smoothness.

## Limitations
- Binary weight variant shows lower clean accuracy, suggesting potential tradeoffs between robustness and performance
- Limited generalizability - only tested on four text classification datasets with relatively short documents
- No ablation studies to isolate which specific component (sign activation, 01 loss, global pooling, ensemble) contributes most to robustness

## Confidence
- **High confidence**: Sign activation networks show consistently higher adversarial accuracy across all four datasets compared to baseline models (MLP, BNN, CNN)
- **Medium confidence**: The novel CNN01-FS global pooling variation significantly improves adversarial accuracy over standard CNN01
- **Medium confidence**: TextFooler's reliance on probability outputs explains why it struggles against sign activation networks

## Next Checks
1. Test alternative black-box attack methods (e.g., genetic algorithms, reinforcement learning approaches) against the sign activation networks to verify robustness isn't specific to TextFooler
2. Perform ablation studies by training models with: (a) sign activation + cross-entropy loss, (b) sigmoid activation + 01 loss, (c) ensemble of standard CNNs without sign activation
3. Evaluate model performance on longer documents (>500 words) and non-review text classification tasks to assess generalizability beyond the four studied datasets