---
ver: rpa2
title: Analyzing and Bridging the Gap between Maximizing Total Reward and Discounted
  Reward in Deep Reinforcement Learning
arxiv_id: '2407.13279'
source_url: https://arxiv.org/abs/2407.13279
tags:
- discounted
- return
- state
- total
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the misalignment between maximizing total
  return and discounted return in deep reinforcement learning (RL). The authors analyze
  the performance gap between policies optimizing these two objectives, revealing
  that increasing the discount factor cannot effectively eliminate this gap in environments
  with cyclic states.
---

# Analyzing and Bridging the Gap between Maximizing Total Reward and Discounted Reward in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.13279
- Source URL: https://arxiv.org/abs/2407.13279
- Authors: Shuyu Yin; Fei Wen; Peilin Liu; Tao Luo
- Reference count: 30
- Primary result: Proposes methods to align policies optimizing total return vs discounted return in deep RL, showing improved performance in Hopper and Walker2D tasks

## Executive Summary
This paper addresses a fundamental misalignment in deep reinforcement learning where policies optimized for discounted returns may not maximize total returns, particularly in environments with cyclic states. The authors analyze the performance gap between these two objectives and propose two approaches to bridge this gap: modifying terminal state values and calibrating reward data in trajectories. These methods ensure monotonic relationships between discounted and total returns at the trajectory level, enhancing the robustness of off-policy RL algorithms to discount factor choices and improving performance in tasks with large trajectory lengths.

## Method Summary
The paper proposes two approaches to align total return and discounted return objectives in deep RL. First, it modifies terminal state values as tunable hyper-parameters to adjust the comparative relationship of discounted returns between trajectories without affecting their total returns. Second, it calibrates reward data in trajectories to ensure modified discounted returns increase monotonically with original total returns. The trajectory reward data calibration method involves estimating suitable discounted returns for new trajectories based on historical data, then adjusting reward data to match these estimates. This approach is integrated with off-policy RL algorithms like DQN, SAC, and TD3, showing improved performance across benchmark environments.

## Key Results
- Demonstrated that increasing discount factor alone cannot eliminate the performance gap in cyclic state environments
- Proposed terminal state value modification aligns optimal policies for total and discounted return maximization
- Trajectory reward data calibration method improves robustness to discount factor and enhances performance in tasks with large trajectory lengths
- Experiments on Hopper and Walker2D show substantial performance improvements compared to standard discount factors (0.99 or 0.999)

## Why This Works (Mechanism)

### Mechanism 1
Modifying terminal state value can align optimal policies by adjusting discounted return relationships between trajectories without affecting total returns. The terminal state value serves as a tunable parameter that can reverse the relative ranking of discounted returns while preserving total return rankings. This method fails when optimal policies inherently require cyclic states, as in CartPole where increasing discount factor close to one destabilizes training.

### Mechanism 2
Calibrating reward data ensures monotonic increase of modified discounted returns with respect to original total returns by distributing alignment adjustments across trajectory reward data. The method estimates suitable discounted returns for new trajectories using historical data through interpolation or extrapolation. This approach becomes ineffective when trajectory lengths are extremely large (e.g., 8000 steps) with discount factors close to one, causing discounted returns to exceed neural network learning capacity.

### Mechanism 3
Ensuring discounted returns increase monotonically with total returns at trajectory level improves policy optimization by helping agents better distinguish high-return from low-return trajectories. This monotonic relationship enhances both exploitation (prioritizing high-return trajectories) and exploration (discovering higher-return trajectories). The method fails when total returns are extremely large (e.g., around 3500 in Walker2D), causing modified discounted returns to exceed 1000 and become difficult for neural networks to learn accurately.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper's analysis and methods are built on MDP theory where states, actions, rewards, and transition probabilities define the learning environment
  - Quick check question: In an MDP with states S = {s1, s2, s3} where s3 is terminal, what is the key difference between total return and discounted return for a trajectory starting at s1?

- Concept: Bellman Optimality Equation
  - Why needed here: The paper analyzes performance gaps between policies derived from minimizing discounted Bellman loss versus maximizing total return
  - Quick check question: How does the Bellman optimality equation differ when using discount factor γ versus when γ = 1 (total return)?

- Concept: Suboptimality Bounds
  - Why needed here: The paper uses suboptimality analysis to quantify performance gaps between policies optimized for different objectives
  - Quick check question: What does it mean when the third term in Theorem 1's suboptimality bound is "unaffected by minimizing the loss"?

## Architecture Onboarding

- Component map: Trajectory collection -> Historical data processing (Algorithm 1) -> New trajectory estimation (Algorithm 2) -> Reward data adjustment (Algorithm 3) -> Off-policy training using modified trajectories
- Critical path: 1) Collect trajectories → 2) Process historical trajectories with Algorithm 1 → 3) Estimate discounted returns for new trajectories with Algorithm 2 → 4) Adjust reward data with Algorithm 3 → 5) Train off-policy on modified trajectories → 6) Evaluate and repeat
- Design tradeoffs: Off-policy training allows leveraging historical data but requires careful replay buffer management; calibration method trades computational overhead for improved alignment but may lose original reward semantics
- Failure signatures: Performance plateaus despite increased training, trajectories showing inconsistent total/discounted return relationships, neural network struggling to fit extremely large discounted returns (>1000)
- First 3 experiments:
  1. Test Algorithm 1 on synthetic trajectories with known total/discounted return relationships to verify monotonic alignment is achieved
  2. Apply Algorithm 2 and 3 to small set of new trajectories and visualize modified vs original discounted returns to confirm adjustment effectiveness
  3. Run complete training loop with DQN on LunarLander using γ = 0.97 and kmin = 0.3, comparing performance with and without calibration method

## Open Questions the Paper Calls Out

### Open Question 1
How can we design new proxy objectives that inherently increase monotonically with the total return while maintaining controllable maximum values? The paper concludes that developing such proxies could resolve objective misalignment more fundamentally than current trajectory-level adjustments. This remains unresolved because current calibration methods involve rescaling and sign changes that strip rewards of physical meaning, suggesting room for more principled alternatives.

### Open Question 2
What is the optimal strategy for tuning the minimum slope parameter (kmin) dynamically during training rather than using a fixed value? The paper shows kmin performance varies with environment and that starting small and gradually increasing may be beneficial, but doesn't explore adaptive strategies. This remains unresolved because experiments only test fixed kmin values across training, and the paper notes large kmin values can cause problems when total returns are large.

### Open Question 3
Can the terminal state value modification approach be extended to continuous state spaces where terminal states are not clearly defined? The theorems and experiments focus on discrete state spaces with explicit terminal states, while practical DRL often uses continuous spaces. This remains unresolved because the theoretical analysis relies on discrete state space assumptions and explicit terminal state accessibility conditions that don't directly translate to continuous environments.

## Limitations
- The proposed methods rely on theoretical alignment conditions that may not generalize well across diverse MDP structures, particularly environments with complex cyclic dependencies or stochastic transitions
- The calibration method's effectiveness depends heavily on the quality of historical trajectory data and the assumption that total returns correlate well with discounted returns, which may break down in sparse-reward or highly stochastic environments
- The paper does not address computational overhead or potential overfitting risks when modifying reward data across long trajectories

## Confidence
- Medium: The theoretical analysis of objective misalignment in cyclic MDPs is well-founded, but practical generalization remains uncertain
- Medium: The terminal state value modification approach shows promise in controlled examples but lacks extensive empirical validation across diverse environments
- Medium: The trajectory reward calibration method demonstrates improvements in benchmark tasks, but the underlying assumptions about monotonic relationships may not hold universally

## Next Checks
1. Test the calibration method on a diverse set of environments including sparse-reward tasks (e.g., Montezuma's Revenge) to evaluate robustness beyond dense-reward continuous control tasks
2. Implement ablation studies to quantify the contribution of each component (terminal state adjustment vs. reward calibration) and determine their relative importance in different environment types
3. Measure the computational overhead and memory requirements of the calibration process across varying trajectory lengths and compare against baseline performance gains to assess practical feasibility