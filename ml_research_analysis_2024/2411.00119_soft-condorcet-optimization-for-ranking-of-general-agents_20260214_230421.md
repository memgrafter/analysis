---
ver: rpa2
title: Soft Condorcet Optimization for Ranking of General Agents
arxiv_id: '2411.00119'
source_url: https://arxiv.org/abs/2411.00119
tags:
- ranking
- agents
- data
- condorcet
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Soft Condorcet Optimization (SCO) is a ranking scheme for general
  agents that computes the optimal ranking by minimizing prediction errors in evaluation
  data. It treats agent comparisons as votes from a ground truth ranking and uses
  differentiable loss functions to find ratings that best explain the data.
---

# Soft Condorcet Optimization for Ranking of General Agents

## Quick Facts
- arXiv ID: 2411.00119
- Source URL: https://arxiv.org/abs/2411.00119
- Reference count: 40
- Key outcome: SCO approximates Kemeny-Young rankings with normalized Kendall-tau distances of 0-0.043 on 865 preference profiles and outperforms Elo in sparse data regimes

## Executive Summary
Soft Condorcet Optimization (SCO) is a ranking scheme that computes optimal agent ratings by minimizing prediction errors in evaluation data. It treats agent comparisons as votes from a ground truth ranking and uses differentiable loss functions to find ratings that best explain the data. Unlike classical Elo rating systems, SCO guarantees top-ranking Condorcet winners when they exist and approximates Kemeny-Young optimal rankings with high accuracy across various preference profiles.

## Method Summary
SCO optimizes agent ratings by minimizing a soft Kendall-tau distance between observed votes and the induced ranking. The method treats evaluation data as preference profiles where agent comparisons are votes from a ground truth ranking. Three optimization approaches are proposed: gradient descent on the soft Kendall-tau distance, Fenchel-Young loss optimization, and solving a sigmoidal program. The differentiable loss function allows for efficient gradient-based optimization while maintaining theoretical guarantees about Condorcet winner detection.

## Key Results
- SCO achieves normalized Kendall-tau distances of 0-0.043 from Kemeny-Young optimal rankings across 865 preference profiles
- Outperforms Elo and voting-as-evaluation methods on 31,049 human Diplomacy games with 52,958 players
- Demonstrates superior performance in sparse data regimes with 59%+ missing data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCO finds ratings that minimize prediction errors by modeling agent comparisons as noisy samples from a ground truth ranking
- Mechanism: SCO treats evaluation data as "votes" from a preference profile, defines a differentiable loss function (soft Kendall-tau distance), and optimizes ratings to minimize the sum of distances between observed votes and the induced ranking
- Core assumption: The ground truth ranking exists and evaluation data can be modeled as noisy samples from it
- Evidence anchors:
  - [abstract] "treats agent comparisons as votes from a ground truth ranking and uses differentiable loss functions to find ratings that best explain the data"
  - [section 3.1] "Define a discrete loss: L([≽], A, V, θ) = sum over votes and pairs of the discrepancy function D_v(θ_a, θ_b)"

### Mechanism 2
- Claim: SCO guarantees top-ranking Condorcet winners when they exist, unlike Elo
- Mechanism: SCO's sigmoid loss is monotonically decreasing in the rating of a Condorcet winner, ensuring that its rating reaches the maximum value at the global optimum
- Core assumption: A Condorcet winner exists in the preference profile
- Evidence anchors:
  - [abstract] "SCO ratings are maximal for Condorcet winners when they exist, which we show is not necessarily true for the classical rating system Elo"
  - [section 3.4, Theorem 1] "If for preference profile [≽], voters V, there exists a candidate c ∈ A that is a Condorcet winner, the loss is monotonically decreasing with θ_c"

### Mechanism 3
- Claim: SCO approximates Kemeny-Young rankings with low normalized Kendall-tau distance
- Mechanism: SCO's optimization methods (gradient descent, sigmoidal programming, Fenchel-Young loss) find ratings that induce rankings close to the Kemeny-Young optimal ranking on various preference profiles
- Core assumption: The preference profiles are representative of real-world agent evaluation scenarios
- Evidence anchors:
  - [abstract] "When serving as an approximation to the Kemeny-Young voting method, SCO rankings are on average 0 to 0.043 away from the optimal ranking in normalized Kendall-tau distance across 865 preference profiles from the PrefLib open ranking archive"
  - [section 4.2] "SCO ratings (computed using both sigmoid and Fenchel-Young losses) achieve the lowest KTD and MRTD when 59% or more of the data is missing"

## Foundational Learning

- Concept: Kendall-tau distance
  - Why needed here: SCO uses the Kendall-tau distance as a measure of disagreement between rankings, which is then smoothed to create a differentiable loss function
  - Quick check question: What is the maximum possible Kendall-tau distance between two rankings of n elements?

- Concept: Condorcet winner
  - Why needed here: SCO guarantees top-ranking Condorcet winners when they exist, which is a key property that distinguishes it from other ranking methods like Elo
  - Quick check question: How is a Condorcet winner defined in a preference profile?

- Concept: Differentiable optimization
  - Why needed here: SCO uses differentiable loss functions to optimize ratings, which allows for efficient gradient-based optimization methods
  - Quick check question: Why is it important for the loss function in SCO to be differentiable?

## Architecture Onboarding

- Component map: Preference profile -> Soft Kendall-tau loss -> Optimization (gradient descent/sigmoidal programming/Fenchel-Young) -> SCO ratings -> Induced ranking

- Critical path:
  1. Parse preference profile from evaluation data
  2. Compute soft Kendall-tau loss based on current ratings
  3. Optimize ratings using chosen method (gradient descent, sigmoidal programming, or Fenchel-Young loss)
  4. Sort optimized ratings to obtain final ranking

- Design tradeoffs:
  - Gradient descent is efficient but may get stuck in local optima
  - Sigmoidal programming guarantees global optimum but is computationally expensive
  - Fenchel-Young loss is convex and has convergence guarantees but may not top-rank Condorcet winners

- Failure signatures:
  - Rankings that do not align with intuitive agent skill
  - High normalized Kendall-tau distance to Kemeny-Young rankings
  - Ratings that do not reflect Condorcet winners when they exist

- First 3 experiments:
  1. Verify SCO rankings on a simple preference profile with a known Condorcet winner
  2. Compare SCO rankings to Elo and Kemeny-Young on a larger preference profile from PrefLib
  3. Evaluate SCO's performance in a sparse data regime with synthetic evaluation data

## Open Questions the Paper Calls Out

- Question: How does SCO perform compared to other learning-to-rank methods like LambdaMART or RankNet when ranking agents?
  - Basis in paper: The paper mentions comparing SCO to other learning-to-rank methods as future work in the discussion section
  - Why unresolved: The paper only compares SCO to Elo and voting-as-evaluation methods, not other learning-to-rank methods
  - What evidence would resolve it: Running experiments comparing SCO to other learning-to-rank methods on the same datasets used in the paper

- Question: Can SCO be used as a reward model for post-training and alignment of language models?
  - Basis in paper: The paper suggests investigating using SCO as a reward model for post-training and alignment of language models as future work
  - Why unresolved: The paper does not experiment with using SCO as a reward model
  - What evidence would resolve it: Running experiments where SCO is used as a reward model for language model post-training and alignment tasks

- Question: How does the performance of SCO scale with the number of agents and tasks?
  - Basis in paper: The paper shows SCO works well on problems with up to 52,958 agents, but does not explore how performance scales with even larger numbers
  - Why unresolved: The paper does not run experiments with more than 52,958 agents
  - What evidence would resolve it: Running experiments with SCO on datasets with increasing numbers of agents and tasks to measure how performance changes

- Question: How sensitive is SCO to the choice of hyperparameters like learning rate and temperature?
  - Basis in paper: The paper mentions using different hyperparameters in experiments but does not systematically explore their impact
  - Why unresolved: The paper does not run sensitivity analyses on hyperparameters
  - What evidence would resolve it: Running experiments where SCO is trained with different combinations of hyperparameters to measure their impact on performance

## Limitations

- Computational complexity of sigmoidal programming prevents scaling to very large agent pools
- Performance in real-world noisy evaluation data may degrade if the Condorcet winner assumption is violated
- The three optimization methods may produce different rankings for the same input data, requiring careful method selection

## Confidence

- Condorcet winner guarantee mechanism: High
- Kemeny-Young approximation results: Medium
- Sparse data regime claims: Medium

## Next Checks

1. Implement the full sigmoidal programming approach and benchmark against the gradient descent method on medium-sized preference profiles (50-100 agents) to verify the claimed global optimality

2. Generate synthetic preference profiles with varying degrees of Condorcet winner strength and evaluate SCO's ranking accuracy against ground truth to test the robustness of the Condorcet guarantee

3. Apply SCO to a real-world multi-agent reinforcement learning benchmark with continuous action spaces and compare rankings to established baselines like TrueSkill and ELO in terms of both Kendall-tau distance and actual win rates