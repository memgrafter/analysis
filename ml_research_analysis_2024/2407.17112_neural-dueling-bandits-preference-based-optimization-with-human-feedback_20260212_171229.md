---
ver: rpa2
title: 'Neural Dueling Bandits: Preference-Based Optimization with Human Feedback'
arxiv_id: '2407.17112'
source_url: https://arxiv.org/abs/2407.17112
tags:
- which
- reward
- bandits
- function
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of contextual dueling bandits with
  non-linear reward functions, a common challenge in online recommendation, web search
  ranking, and LLM alignment. Existing methods assume linear reward functions, which
  can be limiting in practice.
---

# Neural Dueling Bandits: Preference-Based Optimization with Human Feedback

## Quick Facts
- arXiv ID: 2407.17112
- Source URL: https://arxiv.org/abs/2407.17112
- Authors: Arun Verma; Zhongxiang Dai; Xiaoqiang Lin; Patrick Jaillet; Bryan Kian Hsiang Low
- Reference count: 40
- One-line primary result: Neural dueling bandits with sub-linear regret using preference feedback

## Executive Summary
This paper addresses the contextual dueling bandit problem with non-linear reward functions, a common challenge in online recommendation, web search ranking, and LLM alignment. Existing methods assume linear reward functions, which can be limiting in practice. The authors propose using neural networks to estimate non-linear reward functions using preference feedback (0/1). Two algorithms are developed: NDB-UCB (based on upper confidence bounds) and NDB-TS (based on Thompson sampling), both with sub-linear regret guarantees. The key insight is using neural tangent kernel (NTK) linearization to apply confidence ellipsoid results from linear bandit theory to the non-linear setting.

## Method Summary
The method uses a neural network (2 hidden layers, width 50) trained with cross-entropy loss on preference feedback to estimate non-linear reward functions. Two algorithms are proposed: NDB-UCB selects the first arm greedily and the second arm optimistically using confidence bounds, while NDB-TS uses Thompson sampling for both arm selections. The neural network's output is linearized via NTK approximation, allowing the application of confidence ellipsoids to bound estimation error. The algorithms balance exploration and exploitation to achieve sub-linear cumulative regret in selecting the best arm across T rounds.

## Key Results
- NDB-UCB and NDB-TS outperform linear baselines (LinDB-UCB and LinDB-TS) on synthetic datasets
- Sub-linear regret bounds are proven for both algorithms under reasonable assumptions
- The neural network width must be sufficiently large (m≥poly(T,L,K,1/κµ,Lµ,1/λ0,1/λ,log(1/δ))) for theoretical guarantees
- Cross-entropy loss enables accurate reward estimation from pairwise comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network's linearization via neural tangent kernel (NTK) allows applying confidence ellipsoid results from linear bandit theory to the non-linear reward estimation problem.
- Mechanism: When the neural network is sufficiently wide, its output can be approximated as a linear function of its random features. This linearization allows the use of confidence ellipsoids (derived for linear functions) to bound the estimation error of the reward difference between any pair of arms.
- Core assumption: The neural network must be sufficiently wide for the NTK approximation to hold.
- Evidence anchors:
  - [abstract]: "we derive an upper bound on the estimation error (i.e., represented as a confidence ellipsoid) of the difference between the reward values of any pair of arms (Theorem 1) predicted by the trained neural network"
  - [section 3.4.1]: "we show that as long as the NN is wide enough, its output can be approximated by a linear function"
  - [corpus]: Weak evidence - the corpus mentions "neural" and "dueling bandits" but doesn't specifically discuss NTK linearization or confidence ellipsoids.
- Break condition: The linearization breaks down if the neural network is not wide enough, violating the NTK approximation.

### Mechanism 2
- Claim: The cross-entropy loss function is specifically designed to handle preference feedback (0/1) and enables accurate reward function estimation from pairwise comparisons.
- Mechanism: The loss function minimizes the negative log-likelihood of the observed preferences, which corresponds to maximum likelihood estimation (MLE) of the neural network parameters. This allows the neural network to learn the reward function using only pairwise comparisons, unlike standard neural bandits that use RMSE for continuous rewards.
- Core assumption: The preference feedback follows the Bradley-Terry-Luce (BTL) model.
- Evidence anchors:
  - [abstract]: "we can estimate the non-linear function by using either a Gaussian processes... or a neural network... However, due to the limited expressive power of the Gaussian processes, it fails when optimizing highly complex functions"
  - [section 3.1]: "we use cross-entropy loss as an objective function for training the neural network to estimate the unknown non-linear reward function due to the preference feedback (i.e., 0/1)"
  - [corpus]: Weak evidence - the corpus mentions "dueling bandits" and "preference" but doesn't specifically discuss cross-entropy loss or MLE.
- Break condition: The mechanism breaks if the preference feedback doesn't follow the BTL model or if the observations are not binary.

### Mechanism 3
- Claim: The UCB and TS algorithms use the estimated reward function and confidence bounds to efficiently balance exploration and exploitation in the dueling bandit setting.
- Mechanism: The algorithms select the first arm greedily based on the estimated reward, then select the second arm optimistically by adding a confidence bonus that encourages exploration of arms that are different from the first arm. This ensures that the algorithm eventually finds the best arm while maintaining sub-linear regret.
- Core assumption: The confidence bounds derived from the neural network's linearization are valid and can be used to guide exploration.
- Evidence anchors:
  - [abstract]: "We propose upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees that efficiently select arms in each round"
  - [section 3.2]: "Using upper confidence bound for dealing with the exploration-exploitation trade-off is common in many sequential decision-making problems"
  - [corpus]: Weak evidence - the corpus mentions "bandits" and "preference" but doesn't specifically discuss UCB/TS algorithms or exploration-exploitation trade-offs.
- Break condition: The mechanism breaks if the confidence bounds are invalid or if the exploration-exploitation balance is not properly maintained.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) approximation
  - Why needed here: To linearize the neural network output so that confidence ellipsoid results from linear bandit theory can be applied to the non-linear reward estimation problem.
  - Quick check question: What happens to the neural network's output as its width approaches infinity in the NTK regime?

- Concept: Bradley-Terry-Luce (BTL) model
  - Why needed here: To model the probability of preference between two arms based on their reward values, which is necessary for the cross-entropy loss function and the regret analysis.
  - Quick check question: How does the BTL model relate the probability of preferring one arm over another to their reward values?

- Concept: Confidence ellipsoids in bandit algorithms
  - Why needed here: To provide theoretical guarantees on the accuracy of the estimated reward function and to guide the exploration-exploitation trade-off in the UCB and TS algorithms.
  - Quick check question: What is the role of the confidence ellipsoid in deriving regret bounds for bandit algorithms?

## Architecture Onboarding

- Component map:
  - Neural Network (NN) -> Cross-entropy Loss Function -> Trained NN
  - Trained NN -> Estimated Reward Function
  - Estimated Reward Function + Confidence Bounds -> UCB/TS Algorithm
  - UCB/TS Algorithm -> Arm Selection
  - Arm Selection + Preference Feedback -> Updated History

- Critical path:
  1. Receive context and context-arm feature vectors
  2. Train neural network using available preference feedback and cross-entropy loss
  3. Select first arm greedily based on estimated rewards
  4. Select second arm optimistically using confidence bounds
  5. Observe preference feedback and update history
  6. Repeat for T rounds

- Design tradeoffs:
  - Neural network width vs. computational efficiency: Wider networks provide better approximation but are more computationally expensive
  - Exploration vs. exploitation: UCB and TS algorithms balance these competing objectives
  - Linear vs. non-linear reward functions: Non-linear functions are more expressive but harder to learn

- Failure signatures:
  - Linear regret: Indicates the algorithm is not finding the best arm
  - High variance in estimated rewards: Suggests the neural network is not converging
  - Poor exploration: Results in suboptimal arm selection

- First 3 experiments:
  1. Synthetic reward functions: Test the algorithms on known non-linear reward functions to verify theoretical guarantees
  2. Varying dimensions and arms: Evaluate the impact of problem complexity on algorithm performance
  3. Comparison with linear baselines: Demonstrate the advantage of non-linear reward estimation in dueling bandits

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations and scope, potential open questions include:
1. How do NDB-UCB and NDB-TS perform in settings with multi-way preferences or continuous feedback?
2. What is the impact of the neural network architecture on regret bounds and practical performance?
3. How do these algorithms scale to real-world applications with higher dimensions and more complex reward functions?

## Limitations
- Requires sufficiently wide neural networks (m≥poly(T,L,K,1/κµ,Lµ,1/λ0,1/λ,log(1/δ))) for theoretical guarantees
- Assumes access to offline dataset with feature vectors satisfying bounded norm and minimum eigenvalue assumptions
- Empirical evaluation limited to synthetic reward functions and small-scale problems (d=5, K=5)
- Computational complexity of training neural networks may be prohibitive for large-scale applications

## Confidence

- **High Confidence**: The core mechanism of using neural networks for non-linear reward estimation in dueling bandits (Mechanism 1) is well-supported by established NTK theory and the derivation of confidence ellipsoids (Theorem 1).
- **Medium Confidence**: The cross-entropy loss function for handling preference feedback (Mechanism 2) is theoretically sound under the BTL model assumption, but real-world preference data may not perfectly follow this model.
- **Medium Confidence**: The UCB and TS algorithms with confidence bounds (Mechanism 3) provide a principled exploration-exploitation framework, but their practical effectiveness depends on the validity of the confidence bounds in the non-linear setting.

## Next Checks
1. **Scalability Test**: Evaluate the algorithms on larger-scale problems with higher dimensions (d>10) and more arms (K>10) to assess computational feasibility and performance degradation.
2. **Real-World Data Validation**: Test the algorithms on real-world preference datasets (e.g., recommendation systems, web search ranking) to verify the practical applicability beyond synthetic functions.
3. **Robustness to Model Misspecification**: Assess the algorithms' performance when the preference feedback deviates from the BTL model or when the neural network is not sufficiently wide, to understand the break conditions identified in the mechanisms.