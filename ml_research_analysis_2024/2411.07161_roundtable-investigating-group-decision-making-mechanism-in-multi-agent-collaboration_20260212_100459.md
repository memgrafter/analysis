---
ver: rpa2
title: 'RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration'
arxiv_id: '2411.07161'
source_url: https://arxiv.org/abs/2411.07161
tags:
- voting
- agents
- agent
- collaboration
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies group decision-making in decentralized multi-agent
  systems, focusing on how different voting rules impact collaboration quality and
  efficiency. The authors introduce RoundTable, a controlled platform for multi-round,
  multi-agent decentralized collaboration, and systematically compare six voting rules
  (unanimous, majority, plurality, rated, ranked, and cumulative voting).
---

# RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2411.07161
- Source URL: https://arxiv.org/abs/2411.07161
- Reference count: 40
- Primary result: Strict voting thresholds (e.g., unanimous) reduce initial collaboration performance by 87% and final round performance by 39% compared to rated/plurality voting rules.

## Executive Summary
This paper introduces RoundTable, a platform for studying decentralized multi-agent collaboration, and systematically evaluates six voting rules across two controlled environments. The authors find that strict voting mechanisms like unanimous voting severely degrade collaboration performance, while preference-based methods (rated, plurality) achieve superior results. Analysis of cross-agent communication reveals significant inefficiencies, with message length increasing by 84% and similarity to previous rounds reaching 90%. Based on these insights, the paper proposes language-based early stopping methods that improve performance by 13% while reducing collaboration rounds by 50%, demonstrating that linguistic patterns can effectively optimize MAS decision-making.

## Method Summary
RoundTable is a controlled platform enabling multi-round, decentralized collaboration between LLM agents. The system implements six voting rules (unanimous, majority, plurality, rated, ranked, cumulative) across two environments: an exchange economy and a recommendation system using MovieLens-100k data. Each collaboration runs for 10 rounds with 100 independent runs per configuration. Agents use gpt-4o-mini by default, with ablation studies across multiple LLM families. The platform tracks group utility, recommendation accuracy (MAE/RMSE), fairness metrics, and communication patterns including message length, complexity, and information distance to previous rounds. Language-based early stopping leverages these communication features to identify optimal stopping points.

## Key Results
- Unanimous voting achieves 87% lower initial round performance and 39% lower final round performance compared to rated/plurality voting
- Message length increases by 84% over collaboration rounds with 90% similarity to previous rounds
- Language-based early stopping improves performance by 13% while reducing rounds by 50%
- Preference voting rules (rated, plurality) consistently outperform discrete voting rules across both environments

## Why This Works (Mechanism)
The effectiveness of different voting rules stems from their ability to balance consensus requirements with decision efficiency. Strict rules like unanimous voting create bottlenecks where any single agent's disagreement halts progress, while preference-based methods allow partial satisfaction to move collaboration forward. The communication analysis reveals that agents engage in increasingly redundant exchanges as collaboration progresses, with high message similarity indicating diminishing returns from additional rounds. Language-based early stopping exploits this pattern by detecting when communication complexity and novelty plateau, signaling that further collaboration yields minimal benefit. The 13% performance gain with 50% round reduction demonstrates that agents often reach optimal solutions before exhausting the full collaboration budget.

## Foundational Learning
- **Voting rule mechanics**: Understanding how different voting thresholds affect consensus formation is critical for designing effective MAS collaboration protocols. Quick check: Verify that unanimous voting indeed requires 100% agreement while plurality only needs plurality.
- **Language feature extraction**: Measuring message length, complexity, and information distance enables quantitative analysis of communication efficiency. Quick check: Confirm that information distance metrics decrease as collaboration progresses.
- **Early stopping criteria**: Identifying optimal termination points requires balancing performance gains against collaboration costs. Quick check: Validate that the 50% round reduction claim holds across different task types.

## Architecture Onboarding

**Component Map**: LLM Agents -> RoundTable Platform -> Voting Rules -> Environment Tasks -> Metrics Collection

**Critical Path**: Agent Communication → Voting Decision → Task Update → Performance Measurement → Early Stopping Evaluation

**Design Tradeoffs**: 
- Strict voting rules provide fairness guarantees but sacrifice efficiency
- Preference voting enables faster decisions but may ignore minority preferences
- Longer collaborations increase solution quality but incur communication overhead
- Early stopping reduces costs but risks premature termination

**Failure Signatures**:
- Low agreement rates indicate voting rule mismatch with task structure
- Message length inflation without quality improvement suggests communication inefficiency
- High round similarity values reveal stagnation in collaboration progress
- Declining individual utility ratios indicate fairness degradation

**First Experiments**:
1. Run 10-round collaborations with unanimous voting in the exchange economy to observe bottleneck behavior
2. Compare message length trajectories between rated and majority voting rules
3. Test early stopping at round 5 vs round 10 to quantify performance trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal voting rule and threshold vary across different task complexities and domain types in decentralized MAS? The paper only tests two environments and doesn't explore how voting mechanisms perform across broader ranges of task complexities, information asymmetries, or domain types.

### Open Question 2
Can dialogue act patterns predict not just when to stop, but also how to adjust agent behaviors or voting mechanisms mid-collaboration to improve outcomes? The paper uses dialogue acts only for identifying stopping points, not for dynamically adjusting collaboration strategies.

### Open Question 3
What are the theoretical limits of language-based early stopping in MAS, and how do these limits depend on agent capabilities and task characteristics? The paper demonstrates practical effectiveness but doesn't analyze when language signals become unreliable indicators of collaboration quality.

## Limitations
- Results depend heavily on specific LLM configurations and may not generalize to other agent architectures
- The 50% round reduction claim for early stopping may not hold across all task types or agent populations
- Message redundancy analysis uses aggregate metrics that don't account for task-specific variations
- Fairness and rationality metrics are computed within a single experimental setup

## Confidence
- **High**: Relative performance ranking of voting rules is well-supported by consistent results across two environments
- **Medium**: Language-based early stopping efficiency gains are plausible but depend on specific threshold settings
- **Low**: Generalizability of message redundancy findings to other MAS domains remains unclear

## Next Checks
1. Replicate voting rule experiments with Llama-3.1-70b to assess performance consistency across model families
2. Test early stopping thresholds on a third, unseen MAS task to evaluate robustness of the 50% round reduction claim
3. Conduct ablation studies on RoundTable's messaging and voting phases to quantify impact of prompt design on collaboration outcomes