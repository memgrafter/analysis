---
ver: rpa2
title: 'The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective'
arxiv_id: '2412.09460'
source_url: https://arxiv.org/abs/2412.09460
tags: []
core_contribution: This study empirically assesses the impact of copyrighted materials
  on the performance of generative large language models (LLMs) for Norwegian. The
  authors trained multiple 7-billion-parameter models using different combinations
  of copyrighted and non-copyrighted datasets, including books, newspapers, and fiction
  works.
---

# The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective

## Quick Facts
- arXiv ID: 2412.09460
- Source URL: https://arxiv.org/abs/2412.09460
- Reference count: 10
- Copyrighted materials improve Norwegian LLM performance when added to training data

## Executive Summary
This study empirically evaluates how copyrighted materials affect the performance of 7-billion-parameter large language models trained for Norwegian. The researchers trained multiple models using different combinations of copyrighted (books, newspapers, fiction) and non-copyrighted datasets, then evaluated them across 28 NLP tasks grouped into nine skill categories. Models trained on copyrighted materials generally outperformed those trained on non-copyrighted data, with books and newspapers improving performance while fiction books decreased it. The study also found that instruction tuning consistently improved performance across all configurations, suggesting post-training fine-tuning can mitigate dataset composition differences.

## Method Summary
The researchers trained 7-billion-parameter models using Mistral architecture on base and extended datasets, where the extended dataset included copyrighted Norwegian materials. They conducted experiments comparing models trained from scratch versus warm-started from Mistral 7B v0.1, and evaluated performance across 28 tasks in nine skill categories. The methodology included domain-specific fine-tuning on books, newspapers, and fiction subsets, followed by instruction tuning on ~5,000 instructions. All models were evaluated using normalized scores (0-100) across different shot configurations for each task.

## Key Results
- Models trained on copyrighted materials outperformed those trained on non-copyrighted data
- Adding books and newspapers to training data improved model performance
- Fiction books decreased performance by 1.40% compared to non-fiction additions
- Instruction-tuned models showed consistent gains across all configurations

## Why This Works (Mechanism)

### Mechanism 1
Copyrighted materials improve Norwegian LLM performance because they provide domain-specific, high-quality language exposure absent from generic web data. Training on curated Norwegian books and newspapers exposes models to formal syntax, idiomatic expressions, and domain vocabulary that web corpora often lack. Core assumption: The improvement is driven by content quality and linguistic diversity rather than sheer volume. Evidence: Adding books and newspapers improved performance while fiction decreased it. Break condition: If quality differences are negligible or if model pre-training already captures similar linguistic patterns.

### Mechanism 2
Warm-starting with Mistral 7B v0.1 reduces sensitivity to copyrighted data because the base model already encodes some Norwegian linguistic features. Pre-existing weights from multilingual training partially cover Norwegian language structures, so additional copyrighted Norwegian text yields diminishing returns. Core assumption: The original Mistral weights were trained on Norwegian data or data with similar properties. Evidence: Warm-started models outperformed from-scratch models but showed reduced sensitivity to copyrighted materials. Break condition: If Mistral weights were not trained on Norwegian at all, warm-starting would show no advantage.

### Mechanism 3
Instruction tuning homogenizes performance differences between base and extended models, suggesting supervised fine-tuning can compensate for data quality disparities. Post-training fine-tuning on instruction data aligns models toward task completion, reducing the gap created by different pre-training datasets. Core assumption: Instruction tuning datasets are sufficiently large and diverse to normalize performance across different pre-training regimes. Evidence: Instruction-tuned models showed consistent gains across all configurations. Break condition: If instruction tuning data is too small or biased, the homogenization effect would not occur.

## Foundational Learning

- Language modeling fundamentals (tokenization, subword vocabularies, perplexity): Understanding why a 32,768 token vocabulary was chosen and how perplexity sampling balances language representation. Quick check: Why does perplexity-based sampling help maintain language balance in a multilingual corpus?

- Evaluation metrics for generative models (BLEU, ROUGE, accuracy, F1): The study aggregates across 28 tasks using different metrics; engineers must know how to interpret and normalize them. Quick check: What does it mean to normalize metrics to a 0-100 range before aggregation?

- Warm-starting vs. training from scratch in transformer models: The study compares both approaches to isolate the effect of copyrighted materials. Quick check: How does warm-starting affect the learning trajectory compared to training from scratch?

## Architecture Onboarding

- Component map: Mistral 7B architecture -> Tokenizer (base dataset) -> Pre-training pipeline (base/extended datasets) -> Fine-tuning stages (domain-specific, instruction) -> Evaluation suite (28 tasks, 9 skill categories)
- Critical path: Dataset preparation -> Tokenizer training -> Base model pre-training -> Domain-specific fine-tuning -> Instruction tuning -> Evaluation
- Design tradeoffs: High-quality copyrighted data improves performance but limits redistribution; web data is redistributable but noisier; warm-starting speeds training but masks data effects
- Failure signatures: Overfitting on copyrighted data (low validation loss but poor generalization); imbalance in language representation (high perplexity for non-target languages); instruction tuning fails to close performance gaps (suggests insufficient fine-tuning data)
- First 3 experiments:
  1. Compare validation loss curves for base vs. extended models to confirm training stability
  2. Run a small-scale ablation removing only fiction books to verify the negative impact claim
  3. Test instruction tuning on a subset of tasks to measure homogenization effect before full fine-tuning

## Open Questions the Paper Calls Out

- Does the warm-start approach diminish the impact of adding high-quality Norwegian copyrighted texts compared to training from scratch? The paper suggests further testing is required to assess whether performance differences between base and extended warm-started models are statistically significant.

- How do different types of fiction (e.g., fantasy, historical fiction) impact tasks like Sentiment Analysis and Commonsense Reasoning compared to other genres? The paper notes fiction books decrease performance but doesn't examine genre-specific effects.

- At what model scale does the impact of copyrighted material on performance vary significantly? The study only used 7-billion-parameter models, suggesting future work should test various scales.

## Limitations

- Findings are specific to Norwegian language tasks and may not generalize to other languages
- Study uses only Mistral 7B architecture without testing different model sizes or architectures
- Exact composition and licensing terms of the extended dataset remain unspecified
- Does not test whether performance improvements translate to commercial deployment scenarios

## Confidence

- High Confidence: Instruction-tuned models show consistent performance gains across all configurations
- Medium Confidence: Copyrighted materials generally improve model performance for Norwegian tasks
- Low Confidence: Fiction books specifically decrease performance; mechanism not rigorously tested

## Next Checks

1. Ablation Study on Fiction Books: Remove only the fiction book subset from the extended dataset and retrain models to verify whether the negative impact on performance is specifically attributable to fiction content.

2. Cross-Lingual Generalization Test: Apply the same experimental protocol to another Scandinavian language (Swedish or Danish) using equivalent copyrighted and non-copyrighted datasets.

3. Commercial Deployment Benchmark: Evaluate the trained models on real-world Norwegian NLP tasks (customer service conversations, legal document analysis, or news summarization) to determine whether benchmark performance translates to practical utility.