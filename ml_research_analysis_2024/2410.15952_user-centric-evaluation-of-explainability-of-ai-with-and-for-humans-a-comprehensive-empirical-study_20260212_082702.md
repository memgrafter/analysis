---
ver: rpa2
title: 'User-centric evaluation of explainability of AI with and for humans: a comprehensive
  empirical study'
arxiv_id: '2410.15952'
source_url: https://arxiv.org/abs/2410.15952
tags:
- data
- explanations
- were
- students
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper conducts a user-centered evaluation of explainable AI
  (XAI) algorithms to investigate how humans understand and interact with AI-generated
  explanations. The study employs a multidisciplinary approach, combining computer
  science perspectives on explanation development with social science methods to assess
  user understanding.
---

# User-centric evaluation of explainability of AI with and for humans: a comprehensive empirical study

## Quick Facts
- arXiv ID: 2410.15952
- Source URL: https://arxiv.org/abs/2410.15952
- Authors: Szymon Bobek; Paloma Korycińska; Monika Krakowska; Maciej Mozolewski; Dorota Rak; Magdalena Zych; Magdalena Wójcik; Grzegorz J. Nalepa
- Reference count: 38
- Primary result: User-centered evaluation reveals limitations in existing XAI methods and demonstrates need for new design principles that address specific information needs of different AI stakeholders

## Executive Summary
This paper presents a comprehensive user-centered evaluation of explainable AI (XAI) algorithms, combining computer science perspectives on explanation development with social science methods to assess how humans actually understand and interact with AI-generated explanations. The study employed a multidisciplinary approach involving 39 participants from three expertise groups - data science students, data visualization students, and mushroom domain experts - to evaluate common XAI algorithms (SHAP, LIME, Anchor, DICE) using a mushroom dataset and Gradient Boosting Classifier model. The research reveals that current XAI methods are insufficient for non-technical users and demonstrates the need for new evaluation techniques and design principles that bridge the gap between technical explanation accuracy and human comprehension.

## Method Summary
The study combined state-of-the-art XAI algorithms with social science research methods to evaluate explanation comprehensibility. A publicly available mushroom dataset was used to train a Gradient Boosting Classifier (XGBoost) model with 99.97% accuracy. Four XAI algorithms (SHAP, LIME, Anchor, and DICE) generated explanations that were presented to participants through infographics. The evaluation employed think-aloud protocols and thematic analysis to capture real-time cognitive processing, supplemented by UX questionnaires and Kano-inspired comparison techniques to assess user satisfaction and information needs across different expertise levels.

## Key Results
- Current XAI algorithms are insufficient for non-technical users who struggle to connect explanations to real-world referents and underlying data
- Users with different expertise backgrounds process XAI explanations through fundamentally different cognitive lenses - experts use them as mediational tools while non-experts treat them as isolated information
- The semiotic triangle framework reveals incompleteness in XAI explanation comprehension, with most users failing to complete the full circle between symbols, objects, and interpretants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multidisciplinary evaluation bridges the gap between technical explanation accuracy and human comprehension
- Mechanism: The study combines computer science XAI algorithm development with social science methods (thematic analysis, think-aloud protocol) to assess not just what explanations say but how users actually interpret them
- Core assumption: Technical correctness of explanations does not guarantee user understanding; evaluation must include user cognitive perspectives
- Evidence anchors:
  - [abstract] "employed a multi-disciplinary approach that included state-of-the-art research methods from social sciences to measure the comprehensibility of explanations"
  - [section] "we devised an original approach for a thorough evaluation of state-of-the-art XAI algorithms, with a primary emphasis on gauging user understanding"
  - [corpus] Weak evidence - corpus neighbors discuss explainability frameworks but don't explicitly address multidisciplinary evaluation methods
- Break condition: If user evaluation methods don't capture actual comprehension differences between technical and non-technical audiences

### Mechanism 2
- Claim: Different user groups process XAI explanations through fundamentally different cognitive lenses
- Mechanism: Experts use explanations as tools to assess dataset quality and real-world validity, while non-experts treat them as isolated information sources without connecting to underlying data reality
- Core assumption: User background knowledge fundamentally shapes how explanations are interpreted and valued
- Evidence anchors:
  - [section] "explanations generated by XAI algorithms were insufficient, because strictly limited to superficial visual level of explanations. The presented infographics did not allow them to reach beyond to the primary dataset"
  - [section] "For mycologists and mycophiles, explanations have a proper mediation function, in the sense that they are perceived as a gateway giving insight to the dataset"
  - [corpus] Weak evidence - corpus neighbors discuss user-centric explanations but don't provide empirical evidence of differential processing across expertise levels
- Break condition: If user background doesn't significantly influence explanation interpretation patterns

### Mechanism 3
- Claim: The semiotic triangle framework reveals incompleteness in XAI explanation comprehension
- Mechanism: Users must complete the full semiotic circle (symbol → object → interpretant) to achieve comprehension; current XAI methods often fail at the object connection stage
- Core assumption: Full comprehension requires connecting model outputs to real-world referents, not just interpreting the explanation itself
- Evidence anchors:
  - [section] "we define comprehensibility as the property that enables users to complete a full semiotic circle, moving back and forth between the following three stages: (1) perceiving and deciphering a symbol (explicit XAI content), (2) identifying the chain of objects underlying this symbol (ML algorithm operation mode <= input data <= specimens of macrofungi), and (3) embracing and integrating both of the later into a mental representation"
  - [section] "The semiotic triangle is incomplete for all of the participants groups" and "Students, regardless of the visual reading skills, were stuck in the relation between representamen and interpretant, failing to obtain both full understanding of the AI model operating mode and the congruency between the dataset and biological evidence"
  - [corpus] No direct corpus evidence for semiotic triangle application in XAI evaluation
- Break condition: If users can achieve satisfactory comprehension without completing the full semiotic circle

## Foundational Learning

- Concept: Think-Aloud Protocol (TAP)
  - Why needed here: TAP captures real-time user cognitive processing of XAI explanations, revealing how users actually interpret rather than how they should interpret them
  - Quick check question: What is the key advantage of concurrent TAP over retrospective TAP in evaluating XAI explanations?

- Concept: Thematic Analysis
  - Why needed here: Thematic analysis identifies patterns in user responses about explanation comprehensibility, moving beyond surface-level feedback to underlying cognitive themes
  - Quick check question: How does thematic analysis differ from simple frequency counting of user comments in XAI evaluation?

- Concept: Semiotic Triangle Framework
  - Why needed here: Provides theoretical foundation for understanding why users may comprehend XAI explanations differently based on their ability to connect symbols to real-world referents
  - Quick check question: In the context of XAI, what represents the "object" in the semiotic triangle relationship?

## Architecture Onboarding

- Component map: Model training -> Explanation generation -> User presentation -> Think-aloud protocol -> Thematic analysis -> UX questionnaires -> Kano-inspired comparison
- Critical path: Model training → Explanation generation → User presentation → Cognitive evaluation → Pattern analysis → Design recommendations
- Design tradeoffs: Technical explanation accuracy vs. user comprehension accessibility; comprehensive explanation detail vs. cognitive load; domain-specific vs. general audience applicability
- Failure signatures: Users unable to connect explanations to real-world referents; significant comprehension gaps between technical and non-technical users; explanations functioning as autotelic rather than mediational tools
- First 3 experiments:
  1. Test comprehension differences between technical and non-technical users with identical XAI explanations
  2. Evaluate whether adding dataset context to explanations improves comprehension for non-experts
  3. Compare semiotic triangle completeness across different XAI explanation types (attribution vs. rule-based vs. counterfactual)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI explanations be designed to bridge the gap between model outputs and domain knowledge for non-expert users?
- Basis in paper: [explicit] The paper identifies that students with low macrofungi literacy were unable to connect XAI explanations to real-world mushroom taxonomy, remaining stuck in a "blind loop" between representamen and interpretant.
- Why unresolved: The study shows that non-experts struggle to move beyond surface-level explanations to understand the underlying data and model logic, but does not provide design solutions.
- What evidence would resolve it: User studies testing prototype XAI interfaces that explicitly link explanations to domain-specific knowledge and real-world examples, measuring comprehension improvements.

### Open Question 2
- Question: What role does user trust in AI play in the effectiveness of XAI explanations, and how can this trust be appropriately calibrated?
- Basis in paper: [explicit] The study found that some students expressed high trust in AI, even declaring abandonment of their own mushroom knowledge, while experts maintained a pragmatic, dataset-focused approach.
- Why unresolved: The paper observes varying trust levels but does not investigate how trust affects explanation comprehension or how to manage trust appropriately.
- What evidence would resolve it: Controlled experiments measuring explanation effectiveness across different trust levels, with interventions to calibrate trust based on explanation quality and uncertainty.

### Open Question 3
- Question: How can XAI evaluation methodologies be standardized to account for diverse user backgrounds and expertise levels?
- Basis in paper: [inferred] The study employed a multidisciplinary approach combining computer science and social science methods, revealing that different user groups (students vs. experts) interpreted explanations very differently, suggesting current evaluation methods may be inadequate.
- Why unresolved: While the paper demonstrates the need for diverse evaluation approaches, it does not establish standardized methodologies for assessing XAI across different user profiles.
- What evidence would resolve it: Development and validation of evaluation frameworks that incorporate user background variables, tested across multiple domains and user groups.

## Limitations
- Sample size of 39 participants may not fully capture diversity of user backgrounds in real-world AI applications
- Focus on visual explanations limits insights about textual or interactive explanation modalities
- Results may not generalize to other XAI algorithms not tested (counterfactual, example-based explanations)

## Confidence
- Medium confidence in core claims about differential explanation processing across user groups

## Next Checks
1. **Cross-domain validation**: Replicate the study with a different dataset (e.g., medical diagnosis or financial risk assessment) to test whether user expertise patterns and semiotic triangle incompleteness persist across domains with different stakes and familiarity levels.

2. **Algorithm diversity testing**: Evaluate the same user groups with counterfactual and example-based explanations to determine if the observed comprehension patterns are specific to attribution-based methods or represent broader XAI challenges.

3. **Longitudinal comprehension assessment**: Conduct follow-up sessions 1-2 weeks after initial exposure to measure retention and whether repeated interaction with explanations improves semiotic triangle completion rates, particularly for non-expert users.