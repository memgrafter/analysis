---
ver: rpa2
title: 'Efficient Data Subset Selection to Generalize Training Across Models: Transductive
  and Inductive Networks'
arxiv_id: '2409.12255'
source_url: https://arxiv.org/abs/2409.12255
tags:
- subset
- training
- architecture
- architectures
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUBSELNET, a novel framework for generalizable
  data subset selection across neural network architectures. The key innovation lies
  in using a GNN-based architecture encoder and a model approximator to predict outputs
  of unseen architectures without explicit training.
---

# Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks

## Quick Facts
- arXiv ID: 2409.12255
- Source URL: https://arxiv.org/abs/2409.12255
- Reference count: 40
- Introduces SUBSELNET framework for generalizable data subset selection across neural architectures

## Executive Summary
This paper presents SUBSELNET, a novel framework that addresses the challenge of selecting optimal data subsets that generalize well across different neural network architectures. The key innovation is the use of a GNN-based architecture encoder combined with a model approximator to predict outputs of unseen architectures without explicit training. The framework offers two variants: Transductive-SUBSELNET, which solves a small optimization problem per architecture, and Inductive-SUBSELNET, which uses a trained subset selector. Experimental results across five real datasets demonstrate superior accuracy-efficiency trade-offs compared to state-of-the-art methods, with up to 77% memory reduction while maintaining high accuracy.

## Method Summary
SUBSELNET introduces a GNN-based architecture encoder that learns to represent neural network architectures in a way that enables generalization across models. The framework uses a model approximator to predict outputs of unseen architectures, eliminating the need for explicit training of each architecture. This enables two operational modes: Transductive-SUBSELNET, which performs a small optimization problem for each new architecture, and Inductive-SUBSELNET, which relies on a trained subset selector. The approach addresses a fundamental limitation in existing subset selection methods that require model-specific training, making it particularly valuable for AutoML tasks like hyperparameter tuning and neural architecture search.

## Key Results
- Achieves superior accuracy-efficiency trade-offs compared to six state-of-the-art subset selection methods
- Reduces memory usage by up to 77% while maintaining high accuracy across datasets
- Demonstrates strong performance on five real-world datasets spanning different domains
- Provides two variants (transductive and inductive) to balance between computational efficiency and optimality

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to encode architectural information using GNNs and predict model outputs without explicit training. By learning a shared representation space for architectures and their expected performance, SUBSELNET can transfer knowledge from seen architectures to unseen ones. The model approximator learns the relationship between architecture embeddings and their performance characteristics, enabling accurate predictions. This approach circumvents the computational bottleneck of training multiple architectures while still providing architecture-specific subset selection.

## Foundational Learning
- GNN-based architecture encoding: Needed to create meaningful representations of neural network architectures that capture their structural and functional properties. Quick check: Verify that architecture embeddings cluster similar architectures together in embedding space.
- Model approximation techniques: Essential for predicting the performance of unseen architectures without actual training. Quick check: Compare predicted vs. actual performance on held-out architectures.
- Subset selection optimization: Required to select the most informative data points given architecture-specific constraints. Quick check: Validate that selected subsets maintain diversity and coverage of the full dataset.

## Architecture Onboarding

Component Map:
Architecture Encoder (GNN) -> Model Approximator -> Subset Selector -> Data Subset

Critical Path:
GNN encoding of architecture → performance prediction via model approximator → subset selection optimization

Design Tradeoffs:
- Transductive vs. Inductive: Transductive provides better accuracy but requires optimization per architecture; Inductive is faster but depends on training quality
- GNN depth vs. computational efficiency: Deeper GNNs capture more complex relationships but increase training time
- Model approximator complexity vs. generalization: More complex approximators may overfit to training architectures

Failure Signatures:
- Poor architecture embeddings lead to inaccurate predictions and suboptimal subset selection
- Model approximator overfitting to training architectures results in poor generalization to unseen architectures
- Optimization instability in transductive variant causes convergence issues

First Experiments:
1. Verify architecture embeddings cluster similar architectures together
2. Test model approximator predictions on held-out architectures
3. Compare subset diversity and coverage between SUBSELNET and baseline methods

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Scalability to very large-scale datasets remains untested and potentially problematic
- Performance on non-vision domains like NLP or tabular data needs validation
- Memory reduction claims require verification across diverse architectures beyond tested ones
- Inductive variant's performance depends heavily on the representativeness of training architectures

## Confidence
High: The core methodology of using GNN-based architecture encoding combined with model approximation is technically sound and addresses a genuine gap in existing subset selection approaches.

Medium: The experimental results showing superior accuracy-efficiency trade-offs are compelling but may be influenced by the specific architectures and datasets chosen. The 77% memory reduction claim requires independent verification.

Low: The framework's generalizability to extremely large-scale problems or different data modalities remains largely untested.

## Next Checks
1. Test SUBSELNET on significantly larger datasets (10x the size) to evaluate scalability limits
2. Validate performance on non-vision tasks, particularly NLP and tabular data
3. Conduct ablation studies to quantify the individual contributions of the GNN encoder and model approximator components