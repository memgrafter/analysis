---
ver: rpa2
title: 'Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large
  Language Models'
arxiv_id: '2410.01532'
source_url: https://arxiv.org/abs/2410.01532
tags:
- features
- arxiv
- human
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GazeReward, a novel framework that integrates
  eye-tracking (ET) data as implicit feedback into reward models (RMs) for aligning
  large language models (LLMs) with human preferences. By leveraging synthetic ET
  features generated from text inputs, the approach enhances RM performance without
  requiring expensive real-world gaze data collection.
---

# Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models

## Quick Facts
- arXiv ID: 2410.01532
- Source URL: https://arxiv.org/abs/2410.01532
- Authors: Angela Lopez-Cardona; Carlos Segura; Alexandros Karatzoglou; Sergi Abadal; Ioannis Arapakis
- Reference count: 40
- Primary result: GazeReward improves reward modeling accuracy by over 10% by integrating synthetic eye-tracking features into large language model alignment

## Executive Summary
This paper introduces GazeReward, a framework that enhances reward models for aligning large language models with human preferences by incorporating eye-tracking data as implicit feedback. The approach leverages synthetic ET features generated from text inputs, avoiding the need for expensive real-world gaze data collection. Experiments demonstrate that adding these multimodal signals to reward models significantly improves their ability to predict human preferences, with accuracy gains of over 10% across multiple model architectures and datasets. The work advances AI alignment research by showing that non-traditional feedback modalities can effectively improve model-human alignment.

## Method Summary
The GazeReward framework integrates synthetic eye-tracking features into reward models by generating ET data from text inputs using pre-trained prediction models. These features are then mapped to the reward model's tokenizer and incorporated via either concatenation or element-wise addition with text embeddings. The approach is evaluated by fine-tuning various LLMs (Llama 3 8B, Llama 3 8B-instruct, Mistral 7B) as reward models on human preference datasets using QLoRA. The method demonstrates improved accuracy in predicting preferred responses without requiring real eye-tracking hardware or data collection.

## Key Results
- GazeReward improves reward modeling accuracy by over 10% on human preference datasets
- Synthetic ET features consistently enhance performance across different model initializations
- Feature concatenation outperforms element-wise addition for integrating ET signals
- The approach works with both Llama 3 and Mistral model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ET features act as implicit cognitive feedback that captures attention patterns correlated with preference.
- Mechanism: Eye-tracking data measures oculomotor behavior during text processing, which reflects underlying cognitive processes like attention and comprehension. When these features are incorporated into reward models, they provide additional signal beyond explicit text-based feedback.
- Core assumption: The ET features generated synthetically correlate with human preferences in a way that improves reward modeling accuracy.
- Evidence anchors:
  - [abstract]: "ET – unlike other (explicit) forms of feedback (e.g., questionnaire data, data annotation) – does not suffer from human biases, and offers a better temporal and spatial resolution"
  - [section]: "ET measures oculomotor behavior i.e. the movements and fixations of the eyes, which offers insight into visual attention and information processing"
  - [corpus]: Weak evidence - corpus contains gaze-related papers but none directly address reward modeling or preference alignment with ET features

### Mechanism 2
- Claim: Concatenating ET embeddings with text embeddings provides more robust multimodal input representation.
- Mechanism: By using special separator tokens (〈eye〉 and 〈/eye〉), the model can distinguish between text and ET modalities, allowing it to process both types of information through its attention mechanisms.
- Core assumption: The model can effectively integrate multimodal information when provided with clear modality boundaries.
- Evidence anchors:
  - [section]: "In the first approach, GazeConcat, we concatenate the ET embeddings with the text embeddings"
  - [section]: "These special tokens are randomly initialized as one-dimensional vectors and added to the embedding layer or the RM model for training"
  - [corpus]: Weak evidence - corpus contains gaze-based interaction systems but not reward modeling architectures

### Mechanism 3
- Claim: Synthetic ET features generated by prediction models provide scalable implicit feedback without expensive data collection.
- Mechanism: Two state-of-the-art ET prediction models generate features from text inputs, creating artificial eye-tracking data that can be used to augment reward models at scale.
- Core assumption: Pre-trained ET prediction models can generate meaningful features that correlate with human attention patterns for arbitrary text.
- Evidence anchors:
  - [section]: "we explore the use of ET prediction models that can generate – automatically and with little effort – ET features in response to text input"
  - [section]: "Unlike other (explicit) forms of feedback (e.g., questionnaire data, data annotation) – does not suffer from human biases"
  - [corpus]: Weak evidence - corpus contains gaze prediction papers but none specifically address synthetic ET for reward modeling

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The reward model is a core component of RLHF, and understanding how it works is essential to grasp why adding ET features improves performance
  - Quick check question: What are the three main steps of RLHF as described in the paper?

- Concept: Eye-tracking features and their interpretation
  - Why needed here: The paper uses specific eye-tracking features (FFD, GPT, TRT, nFix, fixProp) that need to be understood to evaluate the approach
  - Quick check question: Which eye-tracking feature measures the total time spent fixating on a word?

- Concept: Reward modeling architecture
  - Why needed here: The paper modifies reward model architecture by adding ET features, so understanding the baseline architecture is crucial
  - Quick check question: What is the typical output of a reward model in RLHF?

## Architecture Onboarding

- Component map: Input (Prompt-Response Pairs) -> ET Prediction Module -> Feature Projector -> Reward Model -> Output (Scalar Reward)

- Critical path: Prompt + Response → ET Prediction → Feature Projection → Reward Model → Reward Score

- Design tradeoffs:
  - Using frozen ET models vs. training end-to-end
  - Concatenation vs. addition for feature integration
  - Synthetic vs. real eye-tracking data

- Failure signatures:
  - Poor performance on baseline without ET features
  - No improvement when adding ET features
  - Increased variance in reward scores
  - Difficulty in mapping between tokenizers

- First 3 experiments:
  1. Baseline: Train reward model without any ET features on OASST1 dataset
  2. Simple ET integration: Use GazeConcat with only TRT feature from first ET model
  3. Full ET integration: Use GazeConcat with all five features from second ET model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different eye-tracking features (beyond FFD and TRT) contribute to reward model performance, and are there optimal feature combinations for specific tasks or model architectures?
- Basis in paper: [explicit] The paper mentions that f_comb2.5 (which includes nFix, FFD, GPT, TRT, and fixProp) sometimes performs worse than the baseline, while f_comb2.2 (TRT and FFD) generally performs better.
- Why unresolved: The paper does not provide a detailed analysis of why certain feature combinations work better than others, nor does it explore whether optimal combinations vary by task or model architecture.
- What evidence would resolve it: Systematic experiments testing all possible feature combinations across diverse tasks and model architectures, along with interpretability analysis of which features drive performance improvements.

### Open Question 2
- Question: Can the GazeReward framework be effectively scaled to larger reward models (e.g., 70B+ parameters) without parameter-efficient fine-tuning methods, and what computational trade-offs would this entail?
- Basis in paper: [explicit] The paper acknowledges that scaling trends show larger models trained on massive datasets perform better, but computational costs make this difficult to explore.
- Why unresolved: The paper uses QLoRA for computational efficiency but does not test whether the same improvements hold with full fine-tuning of larger models.
- What evidence would resolve it: Training and evaluating GazeReward on large-scale reward models (70B+ parameters) with and without parameter-efficient fine-tuning, comparing performance and computational costs.

### Open Question 3
- Question: How does the integration of eye-tracking features into reward models affect their performance on multilingual tasks, especially for languages not represented in the training data of the eye-tracking prediction models?
- Basis in paper: [explicit] The paper notes that the ET prediction models were trained exclusively on English data, raising questions about generalizability to other languages.
- Why unresolved: The paper does not test the framework on non-English datasets or explore whether the improvements transfer across languages.
- What evidence would resolve it: Evaluating GazeReward on multilingual datasets and training eye-tracking prediction models on diverse language corpora to assess cross-linguistic performance.

### Open Question 4
- Question: What are the long-term effects of using synthetic eye-tracking data on the alignment quality of large language models, and how does this compare to models trained with real human feedback?
- Basis in paper: [explicit] The paper uses synthetic ET data generated by pre-trained models, but does not compare its alignment quality to models trained with real human feedback.
- Why unresolved: The paper does not conduct longitudinal studies or direct comparisons between synthetic ET-based alignment and traditional human feedback methods.
- What evidence would resolve it: Long-term evaluations comparing the alignment quality of models trained with synthetic ET data versus those trained with real human feedback, including robustness and generalization tests.

## Limitations

- The approach relies on synthetic ET features generated by pre-trained models, which may not perfectly capture actual human cognitive patterns
- The framework assumes consistent mapping between text tokenizers and ET features, which may not generalize across all domains
- Evaluation is limited to preference ranking tasks on specific datasets, leaving questions about generalization to other alignment scenarios

## Confidence

**High Confidence**: The methodology for incorporating ET features (concatenation vs. addition) is clearly described and experimentally validated with statistically significant improvements (10%+ accuracy gains). The ablation studies showing consistent gains across model initializations support the robustness of findings.

**Medium Confidence**: Claims about ET features being "bias-free" and offering superior temporal/spatial resolution are asserted but not empirically validated against other implicit feedback methods. The synthetic nature of ET features means actual human cognitive patterns may differ from model predictions.

**Low Confidence**: The paper's assertion that this approach scales effectively to real-world deployment lacks supporting evidence. No experiments test the approach with actual eye-tracking hardware or validate that synthetic features generalize beyond the test datasets.

## Next Checks

1. **Cross-Domain Validation**: Test the ET-enhanced reward models on a different alignment task (e.g., code generation preference ranking) to assess generalization beyond chat responses.

2. **Real vs. Synthetic Comparison**: Conduct a small-scale experiment comparing performance when using real eye-tracking data versus synthetic features to quantify the approximation gap.

3. **Bias Analysis**: Systematically evaluate whether the ET features introduce any systematic biases by analyzing model performance across different response types (e.g., technical vs. creative content).