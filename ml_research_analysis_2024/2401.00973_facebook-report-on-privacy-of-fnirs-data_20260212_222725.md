---
ver: rpa2
title: Facebook Report on Privacy of fNIRS data
arxiv_id: '2401.00973'
source_url: https://arxiv.org/abs/2401.00973
tags:
- learning
- privacy
- training
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This report presents privacy-preserving machine learning approaches\
  \ for fNIRS data using differential privacy (DP) and federated learning (FL). The\
  \ authors developed DP-Adam and DP-SGD optimizers for centralized training, achieving\
  \ up to 86.33% accuracy with (\u03F5=12.0, \u03B4=10\u22125)-DP guarantee."
---

# Facebook Report on Privacy of fNIRS data

## Quick Facts
- arXiv ID: 2401.00973
- Source URL: https://arxiv.org/abs/2401.00973
- Reference count: 40
- Primary result: DP-Adam/DP-SGD achieved up to 86.33% accuracy with (ϵ=12.0, δ=10^-5)-DP guarantee; LDP-FL achieved 78.90% accuracy with same privacy budget

## Executive Summary
This report presents privacy-preserving machine learning approaches for fNIRS data using differential privacy (DP) and federated learning (FL). The authors developed DP-Adam and DP-SGD optimizers for centralized training, achieving up to 86.33% accuracy with (ϵ=12.0, δ=10^-5)-DP guarantee. They also implemented LDP in FL, achieving 78.90% accuracy with the same privacy budget. The study found that hyperparameters optimal for non-private training may not be optimal for private training, emphasizing the need for careful tuning.

## Method Summary
The study uses the Tufts fNIRS Mental Workload dataset with 68 participants for binary classification of mental workload intensity. DP-Adam and DP-SGD optimizers were implemented using the Opacus library with hyperparameters including batch size 2048, learning rate 0.003, clipping bound S=4, and noise multiplier σ=0.8. For federated learning, LDP was implemented with 5-20 clients using IID data distribution. The authors conducted systematic hyperparameter searches and used Moments Accountant for privacy accounting.

## Key Results
- DP-Adam/DP-SGD optimizers achieved up to 86.33% accuracy with (ϵ=12.0, δ=10^-5)-DP guarantee
- LDP in federated learning achieved 78.90% accuracy with (ϵ=12.0, δ=10^-5)-DP guarantee
- Hyperparameters optimal for non-private training may not be optimal for private training, requiring careful tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-Adam and DP-SGD optimizers preserve privacy while maintaining high classification accuracy for fNIRS data.
- Mechanism: The optimizers add calibrated Gaussian noise to clipped gradients, bounding the influence of any single data sample and ensuring (ϵ, δ)-differential privacy.
- Core assumption: Clipping bounds and noise multipliers can be tuned to balance privacy and utility.
- Evidence anchors:
  - [abstract] The authors developed DP-Adam and DP-SGD optimizers for centralized training, achieving up to 86.33% accuracy with (ϵ=12.0, δ=10^-5)-DP guarantee.
  - [section] "Differentially private deep learning [34, 33, 35] typically relies on differentially private stochastic gradient descent (DP-SGD) to control the influence of training data on the model."
  - [corpus] Weak evidence; related papers focus on federated learning variants but do not directly validate fNIRS DP-Adam/DP-SGD performance.
- Break condition: If the gradient clipping norm S is set too low, significant signal loss occurs, degrading accuracy; if σ is too high, learning becomes ineffective.

### Mechanism 2
- Claim: LDP in federated learning prevents gradient leakage attacks while allowing collaborative model training.
- Mechanism: Each client adds local noise to gradients before upload, ensuring that intercepted updates reveal minimal information about the client's private fNIRS data.
- Core assumption: Local noise addition is sufficient to mask individual contributions without overly degrading the aggregated model.
- Evidence anchors:
  - [abstract] "They also implemented LDP in FL, achieving 78.90% accuracy with (ϵ=12.0, δ=10^-5)-DP."
  - [section] "Each participant runs a random perturbation algorithm M and communicates the results to the server."
  - [corpus] Weak evidence; corpus neighbors discuss LDP in FL but lack direct fNIRS validation.
- Break condition: If privacy budget ϵ is too low, noise dominates updates and accuracy drops sharply; if clients collude, LDP guarantees may be compromised.

### Mechanism 3
- Claim: Hyperparameter tuning is critical when training with differential privacy because optimal non-private settings may not transfer.
- Mechanism: Adjusting learning rate, batch size, noise multiplier, and clipping norm specifically for the DP setting improves accuracy while preserving privacy.
- Core assumption: The interaction between DP noise and optimization dynamics requires separate tuning from standard training.
- Evidence anchors:
  - [abstract] "The study found that hyperparameters optimal for non-private training may not be optimal for private training, emphasizing the need for careful tuning."
  - [section] "We conduct hyperparameter searches for each model separately... We use a batch size of 1024 for the MLP models and 256 for the remaining models."
  - [corpus] Weak evidence; corpus does not directly address DP hyperparameter transfer effects.
- Break condition: If hyperparameters are copied from non-private settings, privacy budget is exceeded quickly or accuracy degrades.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: Provides formal privacy guarantees that prevent membership inference and model inversion attacks on sensitive fNIRS data.
  - Quick check question: What is the formal definition of (ϵ, δ)-differential privacy?

- Concept: Federated Learning (FL)
  - Why needed here: Enables collaborative training without sharing raw fNIRS datasets, reducing privacy risk at the data source.
  - Quick check question: How does FedAvg aggregate client updates in standard FL?

- Concept: Local Differential Privacy (LDP)
  - Why needed here: Adds noise at the client side in FL, protecting against gradient leakage attacks when updates are intercepted.
  - Quick check question: What is the key difference between LDP and Central DP in FL?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training (centralized or federated) -> DP noise addition -> Evaluation -> Privacy accounting
  Centralized path: Local dataset -> DP optimizer -> privacy budget tracking
  Federated path: Multiple clients -> Local DP updates -> Secure aggregation -> Global model

- Critical path:
  1. Load and preprocess fNIRS data into binary workload classes.
  2. Train baseline non-private model (MLP) to establish performance ceiling.
  3. Apply DP-Adam/DP-SGD with tuned hyperparameters to achieve target (ϵ, δ).
  4. For FL, distribute data to clients, run LDP updates, aggregate, and evaluate.

- Design tradeoffs:
  - Higher noise multiplier -> stronger privacy but lower accuracy.
  - Smaller batch size -> more epochs possible but slower per-epoch progress.
  - Model complexity vs. privacy: larger models require larger clipping bounds, increasing noise scale.

- Failure signatures:
  - Accuracy stalls or drops sharply -> likely noise multiplier too high or clipping norm too low.
  - Privacy budget exceeded early -> batch size too small or learning rate too high.
  - FedAvg divergence -> non-IID data distribution or insufficient local epochs.

- First 3 experiments:
  1. Train baseline MLP without privacy, record test accuracy (~99%).
  2. Train same MLP with DP-Adam, vary σ from 0.6 to 5.0, fix batch size 2048, observe accuracy drop.
  3. Set up 5-client FL with IID data, run FedAvg baseline, then repeat with LDP, compare accuracies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is central DP (CDP) in federated learning compared to local DP (LDP) for fNIRS data?
- Basis in paper: [explicit] The authors plan to explore central DP in federated learning as an extension of their current work using LDP, noting that CDP might help address the accuracy issues seen with LDP.
- Why unresolved: The paper only implemented LDP in FL and did not compare its effectiveness to CDP. The authors acknowledge that CDP might yield better performance but did not conduct experiments to verify this claim.
- What evidence would resolve it: Experimental results comparing the privacy-accuracy trade-offs of CDP and LDP in FL using the same fNIRS dataset and varying privacy budgets.

### Open Question 2
- Question: How does dynamically adjusting noise multiplier and gradient norm bound during training affect model accuracy in differentially private learning?
- Basis in paper: [explicit] The authors mention that dynamically adjusting these two values during training often yields better performance according to previous research, and they plan to study the effects of doing this in the future.
- Why unresolved: The paper used constant values for these parameters and did not investigate the impact of dynamic adjustment on model accuracy.
- What evidence would resolve it: Experimental results comparing the accuracy of models trained with constant vs. dynamically adjusted noise multiplier and gradient norm bound, while maintaining the same overall privacy budget.

### Open Question 3
- Question: How does the performance of differentially private learning change with different model architectures and parameter sizes?
- Basis in paper: [explicit] The authors plan to examine other models with various architectures and a different number of parameters to better understand the impact of model architecture on differentially private deep learning, noting that simpler models with fewer parameters are usually preferred in DP learning.
- Why unresolved: The paper only used a single MLP model with three hidden layers for all experiments and did not explore the effect of model architecture on DP learning performance.
- What evidence would resolve it: Experimental results comparing the accuracy of models with different architectures (e.g., CNN, LSTM, ResNet) and parameter sizes when trained with differential privacy, using the same fNIRS dataset and privacy budget.

## Limitations
- Limited empirical validation on real-world fNIRS data distributions, particularly non-IID scenarios
- Unknown robustness of LDP guarantees against gradient leakage attacks in realistic federated settings
- Potential performance degradation when scaling to larger client pools (>20 clients)

## Confidence
- **High**: DP-Adam/DP-SGD optimizers can achieve target privacy budgets while maintaining reasonable accuracy for fNIRS data
- **Medium**: LDP in FL provides meaningful privacy protection without catastrophic accuracy loss
- **Low**: Hyperparameter tuning requirements for DP training are fully characterized and reproducible

## Next Checks
1. **Gradient Leakage Test**: Implement gradient inversion attacks on LDP updates to empirically verify privacy guarantees under realistic threat models
2. **Non-IID Distribution Test**: Train LDP-FL models with controlled non-IID data splits (e.g., label skew, feature distribution drift) and measure accuracy degradation
3. **Scaling Experiment**: Extend client count from 5 to 100+ participants, monitor accuracy, convergence speed, and privacy budget consumption