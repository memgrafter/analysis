---
ver: rpa2
title: 'USP: A Unified Sequence Parallelism Approach for Long Context Generative AI'
arxiv_id: '2405.07719'
source_url: https://arxiv.org/abs/2405.07719
tags:
- sequence
- parallelism
- communication
- degree
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified sequence parallelism (USP) method
  that combines DeepSpeed-Ulysses and Ring-Attention to overcome limitations of both
  approaches. The method enables efficient long-context training by partitioning the
  sequence dimension across multiple devices.
---

# USP: A Unified Sequence Parallelism Approach for Long Context Generative AI

## Quick Facts
- arXiv ID: 2405.07719
- Source URL: https://arxiv.org/abs/2405.07719
- Authors: Jiarui Fang; Shangchun Zhao
- Reference count: 18
- Achieved 47% MFU on two 8xA800 nodes using sequence length 208K

## Executive Summary
This paper introduces a unified sequence parallelism (USP) method that combines DeepSpeed-Ulysses and Ring-Attention to overcome limitations of both approaches. The method enables efficient long-context training by partitioning the sequence dimension across multiple devices. Experiments on LLAMA3-8B show that USP achieves 47% MFU on two 8xA800 nodes using sequence length 208K. The paper provides comprehensive analysis of SP integration with other parallelism methods and establishes best practices for 4D hybrid parallelism. The unified approach demonstrates superior performance in heterogeneous network environments and removes head number limitations present in prior SP methods.

## Method Summary
The unified sequence parallelism approach integrates DeepSpeed-Ulysses and Ring-Attention by partitioning the SP process group into orthogonal Ulysses and Ring process groups. This allows sequence dimension partitioning across multiple devices without being constrained by attention head count. The method uses a combination of All2All operations across Ulysses process groups and P2P communications across Ring process groups, making it particularly well-suited for heterogeneous communication networks. The approach also incorporates load balancing reordering of input sequence tokens to resolve load-unbalancing issues in causal attention while maintaining computational efficiency.

## Key Results
- Achieved 47% MFU on two 8xA800 nodes using sequence length 208K with LLAMA3-8B
- Unified approach removes head number limitations present in prior SP methods
- Demonstrates superior performance in heterogeneous network environments compared to using either DeepSpeed-Ulysses or Ring-Attention alone

## Why This Works (Mechanism)

### Mechanism 1
The unified approach partitions the SP process group into orthogonal Ulysses and Ring process groups, allowing sequence dimension partitioning across multiple devices without being constrained by attention head count. The product of Ulysses degree and Ring degree equals the total SP degree, enabling flexible combinations that maintain computational correctness.

### Mechanism 2
By setting the Ulysses degree to a value between 1 and N, the attention communication pattern becomes a mix of P2P and All2All operations, which is particularly well-suited for heterogeneous communication networks. This allows All2All operations to operate in high-bandwidth interconnections while asynchronous P2P communications operate in lower-bandwidth sections.

### Mechanism 3
The unified approach incorporates load balancing reordering of input sequence tokens along the sequence dimension, which was not addressed in the original Ring-Attention approach. This reordering resolves load-unbalancing issues in causal attention while maintaining computational efficiency.

## Foundational Learning

- Concept: Sequence Parallelism (SP) fundamentals
  - Why needed here: Understanding how SP partitions the sequence dimension across devices is critical for implementing and optimizing the unified approach
  - Quick check question: How does SP differ from Data Parallelism in terms of memory and communication costs for transformer attention operations?

- Concept: Attention mechanism and tensor partitioning
  - Why needed here: The paper's innovations rely on understanding how Q, K, V, and O tensors can be partitioned along different dimensions while maintaining computational correctness
  - Quick check question: What are the shape transformations that occur when applying All2All operations to attention tensors in SP-Ulysses versus Ring-Attention?

- Concept: Collective communication operations and their bandwidth costs
  - Why needed here: The performance tradeoffs between different SP approaches depend heavily on understanding AllReduce, AllGather, ReduceScatter, and AllToAll operations
  - Quick check question: How does the communication volume scale with sequence length and parallelism degree for All2All operations compared to AllReduce?

## Architecture Onboarding

- Component map: SP Process Groups -> Orthogonal Ulysses and Ring process groups -> Communication Patterns (All2All + P2P) -> Load Balancer -> Integration Layer with TP, DP, ZeRO, PP

- Critical path: 1) Input sequence partitioning and reordering for load balance 2) All2All operations across Ulysses process group 3) Ring-Attention computation across Ring process group 4) Output tensor gathering and reordering 5) Integration with other parallelism dimensions

- Design tradeoffs: Higher Ulysses degree reduces Ring communication but may increase All2All overhead; more sophisticated reordering can improve balance but adds overhead; memory vs communication tradeoff

- Failure signatures: Load imbalance (GPU utilization varies significantly), communication bottleneck (performance degrades in insufficient bandwidth), memory pressure (OOM errors with long sequences)

- First 3 experiments: 1) Benchmark throughput comparison between SP-Ulysses, SP-Ring, and Unified SP on homogeneous NVLink-connected nodes 2) Test Unified SP performance on heterogeneous network topologies 3) Validate convergence of Unified SP vs DP baseline on LLAMA3-8B with varying sequence lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal degree of sequence parallelism (SP) versus tensor parallelism (TP) for different model architectures and sequence lengths?
- Basis in paper: The paper states "The optimal performance for sequence lengths of 64K and 80K is achieved with SP-only without TP-sp" but also notes "At this sequence length, SP-only meets an OOM issue" for longer sequences.
- Why unresolved: The paper shows different optimal configurations for different sequence lengths, suggesting the relationship between SP and TP depends on specific model and sequence characteristics.
- What evidence would resolve it: Systematic experiments varying model architectures, sequence lengths, and hybrid SP/TP configurations across a wider range of parameters to identify general patterns.

### Open Question 2
- Question: How does Unified Sequence Parallelism (USP) scale to large-scale clusters with more than 10,000 GPUs?
- Basis in paper: The paper states "Currently, for publicly disclosed large-scale model training tasks over 10K GPUs, SP has not been utilized" and suggests USP could be beneficial for such scenarios.
- Why unresolved: The paper only demonstrates results on two 8xA800 nodes, not testing the scalability of USP to massive distributed systems.
- What evidence would resolve it: Large-scale experiments demonstrating USP performance and efficiency on clusters with 10K+ GPUs, showing improvements over existing approaches.

### Open Question 3
- Question: What is the impact of Unified SP on convergence compared to other parallelism methods?
- Basis in paper: The paper states "We compared the convergence differences between USP and DP... the curves for USP and DP completely overlapped" but does not compare to other parallelism methods.
- Why unresolved: The paper only compares convergence with data parallelism, not exploring how Unified SP affects convergence compared to tensor parallelism or other hybrid approaches.
- What evidence would resolve it: Comprehensive convergence studies comparing Unified SP with various parallelism combinations across different model architectures and training tasks.

## Limitations
- Evaluation primarily focused on LLAMA3-8B with specific sequence lengths (208K) on A800 hardware, limiting broader applicability claims
- Performance metrics lack comparison against alternative architectures or models
- Communication pattern analysis assumes specific network topologies that may not generalize to all deployment scenarios

## Confidence

- **High Confidence:** Theoretical foundation of combining Ulysses and Ring-Attention approaches with clear mathematical relationships between tensor partitioning and communication patterns
- **Medium Confidence:** Performance benefits in heterogeneous network environments are plausible but need broader empirical validation across diverse network topologies
- **Low Confidence:** Generalizability of the 47% MFU result to other model architectures, sequence lengths, or hardware configurations remains uncertain without broader empirical validation

## Next Checks

1. Validate the unified SP approach across at least three different model architectures and varying sequence lengths (e.g., 32K, 64K, 128K, 208K) to establish performance trends and identify any architecture-specific limitations

2. Systematically evaluate performance across different network configurations (NVLink-only, PCIe-only, and mixed topologies) to quantify the claimed advantages in heterogeneous environments and identify threshold conditions

3. Conduct controlled experiments comparing model convergence rates and final accuracy between unified SP and traditional DP baselines across multiple random seeds, sequence lengths, and batch sizes to ensure partitioning and reordering operations don't introduce subtle training instabilities