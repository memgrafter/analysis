---
ver: rpa2
title: 'Language Models Resist Alignment: Evidence From Data Compression'
arxiv_id: '2406.06144'
source_url: https://arxiv.org/abs/2406.06144
tags:
- alignment
- data
- arxiv
- language
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of elasticity in large language
  models (LLMs), demonstrating that models resist alignment due to their tendency
  to revert to pre-training distributions when fine-tuned. Using compression theory,
  the authors show that fine-tuning disproportionately undermines alignment relative
  to pre-training, with the rate of change in normalized compression rates inversely
  proportional to dataset size.
---

# Language Models Resist Alignment: Evidence From Data Compression

## Quick Facts
- arXiv ID: 2406.06144
- Source URL: https://arxiv.org/abs/2406.06144
- Authors: Jiaming Ji; Kaile Wang; Tianyi Qiu; Boyuan Chen; Jiayi Zhou; Changye Li; Hantao Lou; Juntao Dai; Yunhuai Liu; Yaodong Yang
- Reference count: 40
- One-line primary result: LLMs exhibit elasticity, resisting alignment by reverting to pre-training distributions when fine-tuned

## Executive Summary
This paper introduces the concept of elasticity in large language models, demonstrating that models resist alignment due to their tendency to revert to pre-training distributions when fine-tuned. Using compression theory, the authors show that fine-tuning disproportionately undermines alignment relative to pre-training, with the rate of change in normalized compression rates inversely proportional to dataset size. Experiments across various model types and scales validate this elasticity phenomenon, revealing that resistance increases with model size and pre-training data volume. The findings highlight the need for more robust alignment methods that address the inherent elasticity of LLMs to achieve deep and lasting alignment.

## Method Summary
The study employs compression theory to analyze language model behavior during fine-tuning. Researchers implemented a compression protocol to calculate normalized compression rates for different datasets and model sizes, then conducted fine-tuning experiments with varying data volumes and model architectures. The methodology involved comparing forward alignment (pre-trained to aligned) with inverse alignment (aligned to pre-trained) difficulty, measuring distributional shifts using KL divergence, and evaluating rebound phenomena across different model scales. The experiments tested resistance and rebound across multiple model families including Llama2, Llama3, Qwen, and TinyLlama with various alignment algorithms.

## Key Results
- Elasticity manifests as inverse proportionality between normalized compression rate changes and dataset sizes
- Resistance to alignment exists because inverse alignment is easier than forward alignment for pre-trained models
- Rebound phenomenon increases with model size and pre-training data volume, creating stronger elastic forces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit elasticity, reverting to pre-training behavior when fine-tuned, because normalized compression rate changes are inversely proportional to dataset size.
- Mechanism: Compression theory shows that the model's optimal resource allocation across datasets mimics a spring system where larger datasets dominate. When perturbed, the model preferentially compresses larger datasets, causing smaller datasets to lose representation.
- Core assumption: Token tree modeling accurately captures the model's internal compression behavior, and mass distributions follow Pareto distributions.
- Evidence anchors:
  - [abstract] "compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude"
  - [section] "Theorem 4.2 shows that as the perturbation increases, the normalized compression rates of the model for Dp decrease and Da increase and the rate of changes is strongly correlated with the size of the datasets"
  - [corpus] Weak evidence - no direct corpus papers discuss this specific inverse proportionality mechanism
- Break condition: If token tree pruning depth or mass distribution assumptions fail, the inverse proportionality relationship breaks down.

### Mechanism 2
- Claim: Resistance to alignment exists because forward alignment is harder than inverse alignment for pre-trained models.
- Mechanism: Pre-trained models have established internal representations that resist modification. Inverse alignment (reverting aligned models back to pre-trained behavior) requires less optimization effort because the pre-trained distribution is already encoded in the model's structure.
- Core assumption: The optimization landscape for inverse alignment has shallower gradients than forward alignment.
- Evidence anchors:
  - [abstract] "Even a well-conducted alignment process can be easily circumvented"
  - [section] "We verify the existence of resistance by arguing that forward alignment is harder than inverse alignment for pre-trained models"
  - [corpus] Weak evidence - related papers discuss fragility but not this specific resistance mechanism
- Break condition: If alignment modifies model representations more deeply than assumed, forward alignment becomes comparably difficult.

### Mechanism 3
- Claim: Rebound phenomenon increases with model size and pre-training data volume because larger models have stronger internal consistency with pre-training distributions.
- Mechanism: As models scale, their parameter space becomes more optimized for the pre-training distribution. This creates stronger elastic forces that resist deviation from the original distribution when perturbed.
- Core assumption: Model size and pre-training data volume create increasingly rigid representations that resist change.
- Evidence anchors:
  - [abstract] "elasticity positively correlates with the increased model size and the expansion of pre-training data"
  - [section] "As the model parameter size increases, there is an increase in rebound in response to both positive and negative data"
  - [corpus] Weak evidence - related papers discuss scaling but not this specific elasticity relationship
- Break condition: If larger models develop more flexible representations or if regularization techniques weaken the elastic forces.

## Foundational Learning

- Concept: Compression theory and lossless compression
  - Why needed here: The entire elasticity mechanism relies on treating language model training as a compression problem where minimizing loss equals minimizing compression rate
  - Quick check question: Can you explain why minimizing log-likelihood loss is equivalent to minimizing compression rate under arithmetic coding?

- Concept: Token tree modeling and Pareto distributions
  - Why needed here: The theoretical framework assumes token trees can be pruned to finite depth and that mass distributions follow Pareto patterns, which enables the inverse proportionality result
  - Quick check question: How does the Pareto distribution assumption about leaf node probabilities enable the elasticity theorem?

- Concept: KL divergence and distributional similarity metrics
  - Why needed here: Evaluating elasticity requires measuring how close a model's behavior is to its pre-training or aligned distributions, which is quantified using KL divergence and similar metrics
  - Quick check question: What does it mean when KL divergence between two model distributions approaches zero?

## Architecture Onboarding

- Component map: Pre-train -> Calculate compression rates -> Fine-tune with perturbation data -> Measure compression rate changes -> Analyze inverse proportionality -> Validate resistance/rebound phenomena

- Critical path: Pre-train → Calculate compression rates → Fine-tune with perturbation data → Measure compression rate changes → Analyze inverse proportionality → Validate resistance/rebound phenomena

- Design tradeoffs: Using token tree modeling provides theoretical elegance but computational complexity; compression rate calculations are accurate but expensive; experimental validation requires extensive compute for different model scales

- Failure signatures: If normalized compression rates don't show inverse proportionality to dataset sizes, if forward alignment becomes easier than inverse alignment, or if rebound doesn't increase with model size - these indicate broken assumptions in the elasticity framework

- First 3 experiments:
  1. Implement compression protocol on a small language model and verify that log-likelihood minimization equals compression rate minimization
  2. Create synthetic datasets with controlled size ratios and measure how normalized compression rates change under fine-tuning perturbations
  3. Test resistance phenomenon by comparing forward vs inverse alignment difficulty on a pre-trained model with different slice pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical form of the mass distribution in the token tree that follows the Pareto distribution assumption?
- Basis in paper: [explicit] Assumption A.7 states that the mass distribution of pruned token trees follows a Pareto distribution, but the paper does not specify the exact parameters or validate this assumption empirically.
- Why unresolved: The paper relies on this assumption for theoretical derivation but does not provide experimental validation or exploration of the actual mass distribution in real token trees.
- What evidence would resolve it: Empirical measurement of the mass distribution across various datasets and model sizes, showing whether it follows Pareto distribution and determining the specific parameters.

### Open Question 2
- Question: How does elasticity manifest across the entire lifecycle of pre-training and alignment phases, and does it intensify or diminish as the model undergoes multiple rounds of fine-tuning?
- Basis in paper: [inferred] The paper mentions that the resistance phenomenon becomes more pronounced with increased pre-training data volume and model size, but does not systematically validate elasticity throughout the entire training lifecycle.
- Why unresolved: Due to cost constraints, the authors could not validate elasticity across all phases of pre-training and alignment, leaving the question of how elasticity evolves with multiple fine-tuning rounds unanswered.
- What evidence would resolve it: Experiments tracking elasticity changes across multiple fine-tuning cycles with varying data volumes and model sizes, showing how resistance develops or diminishes over time.

### Open Question 3
- Question: What is the precise relationship between model elasticity and scaling laws, specifically how much training data is required for elasticity to manifest and whether elasticity intensifies as model parameters and pre-training data volume increase?
- Basis in paper: [explicit] The authors state they plan to explore the relationship between model elasticity and scaling laws in future work, including determining the amount of training data required for elasticity to manifest.
- Why unresolved: The paper provides preliminary observations but lacks a quantitative analysis of how elasticity scales with model size and pre-training data volume.
- What evidence would resolve it: Systematic experiments varying model sizes and pre-training data volumes to establish a scaling relationship, identifying the threshold where elasticity becomes significant and quantifying its growth rate.

## Limitations

- The theoretical framework relies heavily on compression theory assumptions that may not fully capture the complexity of modern transformer architectures
- The study's scope is limited to specific model architectures (primarily Llama variants) and alignment methods, raising questions about generalizability
- The paper does not address temporal aspects of elasticity - whether resistance effects accumulate over multiple fine-tuning rounds or if they remain bounded

## Confidence

**High Confidence Claims:**
- The inverse proportionality relationship between normalized compression rates and dataset sizes (Theorem 4.2)
- The resistance phenomenon where inverse alignment is easier than forward alignment (Section 5.1)
- The rebound phenomenon showing increased elasticity with model scale (Section 5.2)

**Medium Confidence Claims:**
- The theoretical framework's applicability to transformer architectures beyond the analyzed cases
- The practical implications of elasticity for alignment strategy design
- The generalizability of results across different alignment methods

**Low Confidence Claims:**
- The claim that elasticity poses fundamental barriers to alignment (rather than practical challenges)
- The universality of Pareto distribution assumptions for token tree modeling
- The long-term stability of elasticity effects across multiple fine-tuning iterations

## Next Checks

1. **Cross-Architecture Validation**: Test the elasticity framework on non-Llama transformer architectures (e.g., GPT-style, BERT variants) to verify whether the inverse proportionality relationship holds across different model families.

2. **Multi-Round Fine-Tuning Study**: Design experiments to assess whether resistance and rebound effects compound over successive fine-tuning rounds, examining the temporal dynamics of elasticity.

3. **Alternative Compression Metrics**: Implement and compare results using different compression-based metrics (beyond normalized compression rates) to validate the robustness of the theoretical framework's predictions.