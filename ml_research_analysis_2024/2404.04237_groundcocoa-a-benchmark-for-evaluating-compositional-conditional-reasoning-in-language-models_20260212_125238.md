---
ver: rpa2
title: 'GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning
  in Language Models'
arxiv_id: '2404.04237'
source_url: https://arxiv.org/abs/2404.04237
tags:
- reasoning
- user
- language
- flight
- groundcocoa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GroundCocoa is a benchmark dataset designed to evaluate large language
  models' compositional and conditional reasoning abilities through a flight-booking
  scenario. The dataset uses a controllable generation pipeline that creates complex
  user requirements with varying numbers of slots and minterms, imposing logical dependencies
  between flight attributes.
---

# GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models

## Quick Facts
- arXiv ID: 2404.04237
- Source URL: https://arxiv.org/abs/2404.04237
- Authors: Harsh Kohli; Sachin Kumar; Huan Sun
- Reference count: 18
- Performance range: Near-random guessing to 67% accuracy (GPT-4 Turbo) on compositional reasoning tasks

## Executive Summary
GroundCocoa is a benchmark dataset designed to evaluate large language models' compositional and conditional reasoning abilities through a flight-booking scenario. The dataset uses a controllable generation pipeline that creates complex user requirements with varying numbers of slots and minterms, imposing logical dependencies between flight attributes. These requirements are matched against five flight options in a multiple-choice format. Experiments with state-of-the-art models show performance ranging from near-random guessing to 67% accuracy (GPT-4 Turbo), with significant performance drops when unconventional user requirements are included. Chain-of-thought and least-to-most prompting techniques provide only modest improvements. The results demonstrate that even the best current models struggle with conditional reasoning tasks, particularly as complexity increases.

## Method Summary
The GroundCocoa benchmark employs a 5-stage pipeline for generating flight booking queries that test compositional and conditional reasoning. First, flight data is scraped from Google Flights for the top 50 busiest airports. Second, user requirements are generated by randomly selecting flight attributes (slots) and creating logical minterms that must be satisfied. Third, these minterms are converted to Product-of-Sums (POS) expressions using symbolic logic. Fourth, the logical expressions are paraphrased into natural language using GPT-4 Turbo. Finally, the generated requirements are matched against the scraped flight data to create multiple-choice questions with exactly one correct answer. The benchmark evaluates models across different complexity levels (2-6 slots, 2-3 minterms) and includes both typical and atypical queries to test robustness.

## Key Results
- State-of-the-art models show wide performance variation from near-random guessing to 67% accuracy (GPT-4 Turbo)
- Including unconventional user requirements leads to a drop in accuracy of as much as 6% in GPT-4 Turbo
- Chain-of-thought and least-to-most prompting provide only modest improvements on compositional reasoning tasks
- Model performance degrades significantly as the number of slots and minterms increases
- Open-source models (Llama, Mistral) show substantially lower performance compared to closed-source models (GPT-4 Turbo, Gemini Pro)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The controllable generation pipeline enables precise control over compositional and conditional complexity through slot/minterm configurations
- Mechanism: By varying the number of slots (flight attributes) and minterms (logical combinations that evaluate to true), the pipeline can systematically increase the reasoning width and conditional complexity of generated queries
- Core assumption: The logical relationships between slots can be effectively represented through Product-of-Sums (POS) expressions that maintain satisfiability
- Evidence anchors:
  - [abstract]: "Our data generation process (§2) consists of a 5-stage pipeline including online scraping, constraint generation, symbolic logic to impose conditionality, paraphrasing user requirements, and matching generated requirements to available flight options"
  - [section]: "To generate a POS expression, we first randomly select a small number of flight attributes or slots... We then randomly generate 2-3 'minterms'... The slot symbols and generated minterms are input to SymPy which uses a redundant-group eliminating algorithm to output the smallest POS expression consistent with the minterm table"
  - [corpus]: Weak - The corpus contains related work on compositional reasoning but doesn't directly address the specific generation mechanism used in GroundCocoa
- Break condition: If the generated POS expressions become unsatisfiable due to conflicting constraints, or if the symbolic logic engine cannot find minimal representations for complex minterm tables

### Mechanism 2
- Claim: The pipeline's validation steps ensure that generated queries have exactly one positive and four negative flight options, maintaining multiple-choice integrity
- Mechanism: After generating user requirements through the POS expression and primitive rules, the system checks that at least one flight option satisfies the criteria and at least four do not, creating well-formed multiple-choice questions
- Core assumption: The flight data collected contains sufficient diversity to always find both satisfying and non-satisfying options for any generated requirement
- Evidence anchors:
  - [section]: "Corresponding to each slot, we have developed a rule-based system that randomly imposes constraints on its values... However, these might cause the final user criteria to become impossible to satisfy even if the corresponding POS expression is satisfiable. Thus, for a generated user requirement we perform checks to ensure that there exists at least 1 route that satisfies the criteria and at least 4 that do not"
  - [abstract]: "The task involves aligning detailed user preferences with available flight options presented in a multiple-choice format"
  - [corpus]: Weak - No direct evidence in corpus about validation of multiple-choice integrity
- Break condition: If the flight data collection yields insufficient variety, or if the constraint generation creates overly restrictive requirements that cannot be satisfied by any flight option

### Mechanism 3
- Claim: Separating "atypical" queries with unconventional requirements creates a robustness test that reveals model biases and limitations
- Mechanism: By isolating primitives that represent unusual user needs (carbon emissions above average, price above minimum threshold, multiple layovers), the pipeline can create a subset of queries that are likely underrepresented in pretraining data
- Core assumption: Models will perform worse on atypical queries because they deviate from common patterns in training data
- Evidence anchors:
  - [section]: "At this stage, we also isolate samples that include any one of the following three primitives... Such samples (henceforth referred to as 'atypical' queries) are able to successfully encapsulate contrarian needs that are unlikely to manifest often during pretraining"
  - [abstract]: "Including unconventional user requirements leads to a drop in accuracy of as much as 6% in GPT-4 Turbo, indicating a training bias towards more typical needs"
  - [corpus]: Weak - The corpus contains related work on evaluating model robustness but doesn't specifically address atypical query generation
- Break condition: If atypical queries become too common in future training data, or if models develop better generalization capabilities that reduce the performance gap

## Foundational Learning

- Concept: Propositional logic and Product-of-Sums (POS) expressions
  - Why needed here: The core of query generation relies on converting minterm tables into logical expressions that can represent complex conditional requirements
  - Quick check question: Given a minterm table with 3 slots and 2 minterms, can you manually construct the corresponding POS expression?

- Concept: Symbolic computation with SymPy
  - Why needed here: SymPy is used to automatically generate minimal POS expressions from minterm tables, ensuring satisfiability and optimal complexity
  - Quick check question: How would you use SymPy to check if a logical expression is satisfiable and find its minimal POS form?

- Concept: Information theory and entropy calculation
  - Why needed here: Entropy is used to quantify the conditional complexity of answer choices, helping explain why models succeed or fail on seemingly similar queries
  - Quick check question: Given the probabilities of primitives being satisfied or unsatisfied for a flight option, can you calculate the entropy and interpret what it means for model difficulty?

## Architecture Onboarding

- Component map: Data Collection -> Constraint Generation -> Logical Processing -> Validation -> Natural Language Generation -> Evaluation
- Critical path: Data Collection → Constraint Generation → Logical Processing → Validation → Natural Language Generation → Evaluation
- Design tradeoffs: The pipeline trades computational complexity for controlled generation of increasingly difficult examples. The use of LLMs for paraphrasing adds quality but requires careful prompt engineering and validation
- Failure signatures: Unsatisfiable requirements, insufficient flight data diversity, LLM paraphrasing that loses logical meaning, entropy calculations that don't correlate with difficulty
- First 3 experiments:
  1. Generate 100 samples with 2 slots and 2 minterms, manually verify that all have exactly 1 positive and 4 negative options
  2. Test GPT-4 Turbo on the generated samples using direct prompting, compare performance to random guessing baseline
  3. Generate samples with varying numbers of slots (2-6) while keeping minterms constant, measure how model performance changes with complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance on GroundCocoa correlate with performance on other compositional reasoning benchmarks like ProofWriter or LogicNLI?
- Basis in paper: Explicit - The paper compares GroundCocoa to other reasoning benchmarks and notes that performance varies more across models than on standard benchmarks
- Why unresolved: The paper only mentions similarity to other benchmarks but doesn't provide direct comparative performance data between GroundCocoa and other compositional reasoning tasks
- What evidence would resolve it: Empirical results showing correlation between GroundCocoa performance and performance on other compositional reasoning benchmarks across the same set of models

### Open Question 2
- Question: What is the impact of grounding tasks versus pure logical reasoning tasks on model performance, and can models perform better on GroundCocoa when given additional world knowledge?
- Basis in paper: Explicit - The paper notes that GroundCocoa requires grounding user requirements to flight options, which is different from pure logical reasoning tasks, and mentions that additional world knowledge could be incorporated
- Why unresolved: The paper doesn't test models with additional world knowledge or compare grounding tasks to pure logical reasoning tasks
- What evidence would resolve it: Experimental results comparing model performance on GroundCocoa with and without additional world knowledge, and comparison with performance on pure logical reasoning tasks

### Open Question 3
- Question: How does the performance gap between open-source and closed-source models on GroundCocoa evolve as model size and capabilities increase?
- Basis in paper: Explicit - The paper shows a significant performance gap between open-source models (Llama, Mistral) and closed-source models (GPT-4 Turbo, Gemini Pro) on GroundCocoa
- Why unresolved: The paper only provides a snapshot of performance differences at current model sizes, but doesn't explore how this gap changes with larger models
- What evidence would resolve it: Longitudinal study tracking performance gaps as new, larger models are released across both open and closed source ecosystems

## Limitations
- The flight data was scraped from Google Flights for the top 50 busiest airports, which may not capture full diversity and could introduce geographic or carrier biases
- The 5-stage generation pipeline involves multiple points of potential failure, including rule-based constraint systems, SymPy-based POS expression generation, and LLM paraphrasing
- The 6% performance drop on atypical queries may reflect training data gaps rather than fundamental reasoning limitations, but this interpretation remains uncertain

## Confidence
- High Confidence: The benchmark dataset generation methodology is well-specified and reproducible
- Medium Confidence: The claim that even state-of-the-art models struggle with conditional reasoning is supported by experimental results
- Low Confidence: The assertion that the 6% performance drop on atypical queries definitively indicates "training bias towards more typical needs" is not fully substantiated

## Next Checks
1. Manually verify that randomly sampled 50 generated user requirements have satisfiable POS expressions, exactly one satisfying flight option, and paraphrased requirements that maintain logical consistency
2. Repeat the entire benchmark generation process using flight data from alternative sources to verify results are not artifacts of Google Flights scraping or the specific 50 airports chosen
3. Create an ablation study replacing atypical primitives with typical alternatives while keeping all other factors constant to determine whether the 6% drop is due to training bias or other factors