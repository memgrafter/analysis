---
ver: rpa2
title: All Language Models Large and Small
arxiv_id: '2402.12061'
source_url: https://arxiv.org/abs/2402.12061
tags:
- deepthink
- londi
- switcher
- quick
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LONDI, a framework that selectively activates
  large language models (LLMs) only when needed, reducing computational costs. It
  uses a switching control mechanism to determine when to activate a resource-intensive
  LLM (DEEPTHINK) versus a smaller, faster one (QUICK).
---

# All Language Models Large and Small

## Quick Facts
- arXiv ID: 2402.12061
- Source URL: https://arxiv.org/abs/2402.12061
- Reference count: 26
- Primary result: Achieves up to 30% reduction in GPU usage while maintaining high performance through selective LLM activation

## Executive Summary
This paper introduces LONDI, a framework that reduces computational costs by selectively activating large language models (LLMs) only when needed. The framework uses a switching control mechanism to determine when to activate a resource-intensive LLM (DEEPTHINK) versus a smaller, faster one (QUICK). LONDI employs reinforcement learning to learn optimal switching policies, achieving significant GPU usage reduction while maintaining task performance. A variant, LONDI-B, introduces budget constraints on LLM calls to ensure optimal usage within specified limits.

## Method Summary
LONDI implements a switching control framework where an RL agent (Switcher) decides when to activate a large LLM versus a small one. The system uses two language models: QUICK (small, fast) and DEEPTHINK (large, resource-intensive). The Switcher, trained with SAC, learns to activate DEEPTHINK only at states where it improves system performance, while using QUICK elsewhere. LONDI-B adds budget constraints by tracking remaining DEEPTHINK activations. The framework converges to optimal solutions through dynamic programming principles, with theoretical guarantees on convergence.

## Key Results
- Achieves up to 30% reduction in GPU usage while maintaining high performance
- LONDI-B variant successfully implements budget constraints on LLM calls
- Theoretical analysis proves convergence to optimal solutions
- Experiments on ScienceWorld and BabyAI-Text tasks demonstrate effectiveness compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LONDI reduces GPU usage by selectively activating the resource-intensive LLM only when needed.
- Mechanism: The framework uses a switching control policy (Switcher) that learns to activate the DEEPTHINK model only at states where it improves system performance, while using the QUICK model elsewhere.
- Core assumption: The Switcher can learn which states require DEEPTHINK through reinforcement learning with a cost parameter that penalizes unnecessary activations.
- Evidence anchors:
  - [abstract]: "LONDI achieves up to 30% reduction in GPU usage while maintaining high performance"
  - [section 3.2]: "the switch agent, based on switching control policy Mguni et al. [2023a,b,c], determines the states in which to activate DEEPTHINK"
  - [corpus]: Weak - no direct corpus evidence on switching control mechanisms
- Break condition: If the cost parameter is set too low, Switcher may activate DEEPTHINK too frequently, negating computational savings.

### Mechanism 2
- Claim: LONDI-B maintains budget constraints on LLM calls while optimizing performance.
- Mechanism: Introduces a budget parameter that tracks remaining DEEPTHINK activations, with a penalty for exceeding the budget, allowing exploration within constraints.
- Core assumption: The budget tracking variable xt can be incorporated into the state space without breaking convergence guarantees.
- Evidence anchors:
  - [section 5]: "LONDI-B that imposes a budgetary constraint of the number of DEEPTHINK activation that can be executed by Switcher"
  - [section 5]: "apply a penalty rather than an outright restriction, the agent retains the flexibility to reevaluate"
  - [corpus]: Weak - no direct corpus evidence on budget-constrained switching mechanisms
- Break condition: If the budget penalty is too severe, the agent may never explore DEEPTHINK states, leading to suboptimal performance.

### Mechanism 3
- Claim: The framework converges to optimal solutions through dynamic programming principles.
- Mechanism: The Bellman operator for the switching control problem is a contraction, ensuring convergence to the optimal policy that minimizes DEEPTHINK activations while maintaining performance.
- Core assumption: Standard RL assumptions (ergodicity, bounded rewards) hold for the switching control problem.
- Evidence anchors:
  - [section 4]: "Theorem 1 proves the solution to Switcher's problem... can be obtained by computing the limit of a (RL) dynamic programming procedure"
  - [section 4]: "LONDI converges under standard assumptions to the solution"
  - [corpus]: Weak - no direct corpus evidence on contraction proofs for switching control
- Break condition: If the transition dynamics are not ergodic, convergence guarantees may not hold.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The framework formalizes the switching control problem as an MDP where the Switcher learns optimal activation policies
  - Quick check question: In an MDP, what does the transition probability P(s'|s,a) represent?

- Concept: Reinforcement Learning (RL) and Policy Gradient Methods
  - Why needed here: The Switcher uses RL algorithms (SAC) to learn the optimal switching policy based on environmental rewards and costs
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Dual Process Theory in Cognitive Science
  - Why needed here: The framework is inspired by the psychological theory of fast (automatic) and slow (deliberate) thinking systems
  - Quick check question: According to Dual Process Theory, what characterizes System 1 vs System 2 thinking?

## Architecture Onboarding

- Component map:
  - QUICK model -> Small, fast language model (Flan-T5-small)
  - DEEPTHINK model -> Large, resource-intensive language model (Flan-T5-large)
  - Switcher -> RL agent using SAC that decides when to activate DEEPTHINK
  - Encoder -> Transformer+MLP that processes text states for the RL agent
  - Budget tracker -> Additional state variable in LONDI-B that counts remaining DEEPTHINK calls

- Critical path:
  1. Environment state → Encoder → Processed state vector
  2. Switcher agent → Binary decision (activate DEEPTHINK or not)
  3. If g=0 → QUICK model generates action
  4. If g=1 → DEEPTHINK model generates action (with cost penalty)
  5. Environment returns reward and next state
  6. Experience stored in replay buffer
  7. Switcher policy updated via SAC

- Design tradeoffs:
  - Computational cost vs performance: Higher cost parameter c leads to fewer DEEPTHINK activations but potentially lower performance
  - Exploration vs exploitation: Budget penalties must balance between preventing overspending and allowing necessary exploration
  - Model size vs responsiveness: QUICK model must be small enough for fast inference but capable enough to handle common cases

- Failure signatures:
  - Switcher always activates DEEPTHINK (cost parameter too low)
  - Switcher never activates DEEPTHINK (cost parameter too high or QUICK model too capable)
  - Training instability (learning rate too high, insufficient exploration)
  - Budget exhaustion too quickly (budget penalty too low or budget too restrictive)

- First 3 experiments:
  1. Run LONDI with varying cost parameters (0.1, 0.3, 0.5) on a simple ScienceWorld task and measure GPU usage vs performance
  2. Implement LONDI-B with different budget constraints (2, 4, 6) and compare to unconstrained LONDI
  3. Replace QUICK model with a random agent to verify LONDI learns to compensate by activating DEEPTHINK more frequently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal cost parameter c for different types of tasks and environments?
- Basis in paper: [explicit] The paper mentions that the cost c plays a critical role in calibrating the resource-consumption/performance trade-off and discusses how different values affect performance, but does not provide a method to determine the optimal value for specific scenarios.
- Why unresolved: The paper only demonstrates that varying c affects performance and provides intuition about its role, but doesn't offer a systematic approach to find the optimal value for a given task or environment.
- What evidence would resolve it: Empirical studies across diverse tasks and environments to identify patterns in optimal c values, or theoretical analysis providing guidelines for selecting c based on task characteristics.

### Open Question 2
- Question: How does LONDI's performance scale with increasingly complex environments or longer planning horizons?
- Basis in paper: [inferred] The paper tests LONDI on ScienceWorld and BabyAI-Text tasks, but doesn't explore its performance limits or how it handles environments with greater complexity or longer-term planning requirements.
- Why unresolved: The experimental setup focuses on specific tasks without pushing the boundaries of environment complexity or planning horizon length.
- What evidence would resolve it: Testing LONDI on environments with increasing complexity (e.g., larger state spaces, more intricate reward structures) and longer planning horizons to identify performance degradation points or scalability limits.

### Open Question 3
- Question: Can LONDI be effectively combined with other model compression techniques for LLMs?
- Basis in paper: [inferred] The paper introduces LONDI as a framework for selective activation of LLMs but doesn't explore potential synergies with other techniques aimed at reducing LLM computational costs.
- Why unresolved: The focus is on the switching mechanism itself rather than exploring combinations with other efficiency methods.
- What evidence would resolve it: Empirical studies combining LONDI with techniques like quantization, pruning, or knowledge distillation to measure potential cumulative benefits or identify optimal combinations.

## Limitations
- The framework's effectiveness depends on the assumption that the QUICK model can handle most cases independently
- Theoretical convergence proof assumes standard RL conditions without verification in practice
- Performance metrics focus on GPU usage rather than wall-clock time improvements

## Confidence
**High Confidence:**
- The mathematical framework for switching control MDPs is sound
- The convergence proof follows established RL theory
- The general approach of selective LLM activation is technically feasible

**Medium Confidence:**
- The 30% GPU usage reduction claim is supported by experimental results
- The budget-constrained variant LONDI-B functions as described
- The switching mechanism effectively learns when to activate DEEPTHINK

**Low Confidence:**
- Generalizability to tasks beyond ScienceWorld and BabyAI-Text
- Real-world performance gains (wall-clock time vs GPU usage)
- Impact of budget constraints on long-horizon tasks

## Next Checks
1. **Cross-domain validation**: Test LONDI on a diverse set of NLP tasks (e.g., code generation, summarization, dialogue) to assess generalizability beyond the experimental domains. Measure both computational savings and performance degradation across task types.

2. **Ablation study on QUICK model capability**: Systematically vary the size and capability of the QUICK model to determine the minimum requirements for effective switching. Measure how different QUICK model capacities affect the frequency of DEEPTHINK activations and overall system performance.

3. **Budget sensitivity analysis**: Conduct experiments with LONDI-B across a wider range of budget constraints and task complexities to characterize the relationship between budget tightness, performance, and computational savings. Identify breaking points where budget constraints severely limit task completion.