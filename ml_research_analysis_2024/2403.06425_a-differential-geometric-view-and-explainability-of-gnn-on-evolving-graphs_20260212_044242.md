---
ver: rpa2
title: A Differential Geometric View and Explainability of GNN on Evolving Graphs
arxiv_id: '2403.06425'
source_url: https://arxiv.org/abs/2403.06425
tags:
- uni00000013
- uni00000048
- uni00000003
- uni00000044
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of explaining how graph neural
  network (GNN) predictions evolve as graphs change over time. The key innovation
  is a differential geometric approach that models the evolution of predicted class
  distributions as smooth curves on a manifold.
---

# A Differential Geometric View and Explainability of GNN on Evolving Graphs

## Quick Facts
- **arXiv ID**: 2403.06425
- **Source URL**: https://arxiv.org/abs/2403.06425
- **Reference count**: 40
- **Primary result**: A differential geometric approach for explaining how GNN predictions evolve on changing graphs, outperforming baselines in sparsity, faithfulness, and intuitiveness across multiple tasks.

## Executive Summary
This paper addresses the problem of explaining how graph neural network (GNN) predictions change as graphs evolve over time. The authors propose a novel approach that models the evolution of predicted class distributions as smooth curves on a manifold, using differential geometry and information geometry. By embedding distributions in a Euclidean space and defining a curved metric based on Fisher information, the method enables faithful and sparse explanations of distributional evolution. The approach outperforms existing methods in sparsity, faithfulness, and intuitiveness across node classification, link prediction, and graph classification tasks on eight datasets.

## Method Summary
The method works by first training a GNN model on static graphs, then computing path contributions to class probability changes using DeepLIFT for GNN. For evolving graph pairs, it identifies altered paths in the computation graph and formulates a convex optimization problem to select a minimal set of salient paths that best approximate the distributional evolution. The selected paths provide human-interpretable explanations of how predictions change as graphs evolve, leveraging differential geometry to respect the intrinsic geometry of the distribution manifold.

## Key Results
- The proposed method (AxiomPath-Convex) achieves the smallest KL divergence (KL+ and KL-) across all explanation complexity levels and datasets
- Extensive experiments on eight datasets demonstrate superior sparsity, faithfulness, and intuitiveness compared to state-of-the-art methods
- The approach handles node classification, link prediction, and graph classification tasks effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The differential geometric view enables faithful modeling of distributional evolution on a manifold
- Mechanism: By embedding predicted class distributions in a low-dimensional manifold within a high-dimensional embedding space and defining a curved metric based on Fisher information, the method captures the intrinsic geometry of how distributions evolve
- Core assumption: The manifold of predicted distributions has a well-defined Riemannian structure with positive definite Fisher information matrix
- Evidence anchors:
  - [abstract]: "We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold"
  - [section]: "We adopt information geometry Amari (2016) to defined a curved metric on the manifold M(G, J)"
  - [corpus]: Weak. The corpus neighbors discuss Riemannian manifolds and geometric graph neural networks, but do not specifically address explaining evolving distributions on manifolds

### Mechanism 2
- Claim: The convex optimization problem selects salient paths that explain distributional evolution concisely
- Mechanism: By reparameterizing families of curves on the manifold and formulating a convex optimization problem, the method finds a unique curve that concisely approximates the distributional evolution
- Core assumption: The optimization problem is convex and has a unique solution, and the selected paths capture the most significant contributors to distributional change
- Evidence anchors:
  - [abstract]: "We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation"
  - [section]: "We optimize En to minimize the KL-divergence... The optimization problem is convex and has a unique optimal solution"
  - [corpus]: Weak. The corpus does not provide direct evidence for the effectiveness of convex optimization in selecting salient paths

### Mechanism 3
- Claim: The method outperforms existing approaches in sparsity, faithfulness, and intuitiveness
- Mechanism: By combining the differential geometric view with convex optimization, the method provides explanations that are more sparse, faithful, and intuitive compared to gradient-based approaches, GNNExplainer, and DeepLIFT
- Core assumption: The proposed method indeed outperforms baseline methods on evaluation metrics
- Evidence anchors:
  - [abstract]: "Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods"
  - [section]: "AxiomPath-Convex has the smallest KL+ over all levels of explanation complexities and over all datasets... AxiomPath-Convex significantly uniformly outperform the runner-ups"
  - [corpus]: Weak. The corpus does not provide direct evidence for the superiority of the proposed method

## Foundational Learning

- **Concept: Differential geometry and information geometry**
  - Why needed here: To understand the mathematical foundation of modeling distributional evolution on a manifold and defining a curved metric based on Fisher information
  - Quick check question: Can you explain the concept of a Riemannian manifold and how it differs from a Euclidean space?

- **Concept: Graph neural networks and their computations**
  - Why needed here: To understand how GNN predictions change as graphs evolve and how the method attributes these changes to specific paths on the computation graph
  - Quick check question: Can you describe the message-passing mechanism in a GNN and how it computes node representations?

- **Concept: Convex optimization and its properties**
  - Why needed here: To understand how the convex optimization problem is formulated and why it has a unique solution that selects salient paths for explaining distributional evolution
  - Quick check question: Can you explain the properties of a convex optimization problem and why it is desirable for this application?

## Architecture Onboarding

- **Component map**: Input graphs (G0, G1) -> Path computation (identify altered paths, compute contributions) -> Manifold embedding (embed distributions) -> Curved metric (define Fisher information metric) -> Convex optimization (select salient paths) -> Output (selected paths explaining distributional evolution)

- **Critical path**: The computation of path contributions using DeepLIFT and the convex optimization problem are the critical components, as they directly determine the selected paths that explain distributional evolution.

- **Design tradeoffs**:
  - Computational complexity vs. explanation quality: Computing path contributions and solving the convex optimization problem can be computationally expensive
  - Sparsity vs. faithfulness: There is a tradeoff between selecting a sparse set of paths (for interpretability) and accurately capturing distributional evolution (for faithfulness)
  - Choice of metric: The Fisher information-based metric may not always be the most appropriate for all types of distributional changes

- **Failure signatures**:
  - Poor sparsity: If selected paths do not form a sparse set, the explanation may be difficult to interpret
  - Low faithfulness: If selected paths do not accurately capture distributional evolution, the explanation may be misleading
  - High computational cost: If computation of path contributions or convex optimization is too expensive, the method may not be practical for large-scale applications

- **First 3 experiments**:
  1. Verify the computation of path contributions using DeepLIFT for GNN on a simple graph with known changes
  2. Test the convex optimization problem on a small manifold with known optimal solutions to ensure it selects the correct paths
  3. Compare the sparsity and faithfulness of the selected paths with baseline methods on a synthetic dataset with ground truth explanations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations and discussion sections.

## Limitations
- The method's performance depends on the validity of manifold assumptions and the positive definiteness of the Fisher information matrix
- Computational complexity of path contribution computation and convex optimization may limit scalability to large graphs
- The generalizability of the method to different types of graph changes and GNN architectures has not been thoroughly validated

## Confidence
- **High Confidence**: The mathematical formulation of the differential geometric approach and convex optimization problem appears sound and well-defined
- **Medium Confidence**: The superiority claims over baseline methods are supported by experimental results, but the extent of improvement varies across datasets and tasks
- **Low Confidence**: The generalizability of the method to different types of graph changes and GNN architectures has not been thoroughly validated

## Next Checks
1. **Manifold Assumption Validation**: Test the method on synthetic datasets where the manifold structure is known to be well-behaved versus poorly-behaved to quantify the impact of manifold assumptions on explanation quality
2. **Computational Scalability Analysis**: Measure wall-clock time and memory usage for path contribution computation and convex optimization on graphs of increasing size to establish practical scalability limits
3. **Cross-Architecture Generalization**: Apply the method to GNNs with different aggregation mechanisms (e.g., attention-based, graph attention networks) to assess robustness beyond the two-layer GNN with element-wise sum aggregation used in experiments