---
ver: rpa2
title: '"In-Context Learning" or: How I learned to stop worrying and love "Applied
  Information Retrieval"'
arxiv_id: '2405.01116'
source_url: https://arxiv.org/abs/2405.01116
tags:
- examples
- retrieval
- number
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper draws an analogy between in-context learning (ICL)
  and ad-hoc information retrieval (IR), proposing that IR methodologies can enhance
  ICL effectiveness. It suggests three main research directions: adaptive ICL using
  query performance prediction (QPP) to dynamically select the number of examples,
  supervised ranking models to prioritize useful examples based on downstream task
  utility, and faceted IR techniques to diversify examples and prevent decoder bias.'
---

# "In-Context Learning" or: How I learned to stop worrying and love "Applied Information Retrieval"

## Quick Facts
- arXiv ID: 2405.01116
- Source URL: https://arxiv.org/abs/2405.01116
- Reference count: 40
- One-line primary result: Supervised adaptive ICL significantly outperforms static ICL in both effectiveness and efficiency for text classification tasks.

## Executive Summary
This paper proposes that in-context learning (ICL) can be viewed through the lens of information retrieval, where examples serve as "documents" retrieved to assist LLM inference. By adapting IR techniques like query performance prediction and neural ranking models, the authors demonstrate that dynamic selection of ICL examples improves downstream task performance compared to static approaches. The work establishes a framework for applying IR methodologies to enhance ICL effectiveness and efficiency.

## Method Summary
The paper investigates adaptive ICL strategies that dynamically select the number of examples for text classification using large language models. Three main approaches are explored: supervised adaptive ICL (SAICL) using a neural ranker trained to predict example usefulness, unsupervised QPP-based adaptive ICL (QPP-AICL) using normalized NQC scores to estimate example utility, and faceted IR techniques for example diversification. The methods are evaluated against static ICL and zero-shot baselines on AGNews, Jigsaw Toxic Comment, and SST2 datasets using macro-averaged precision, recall, and F1-scores.

## Key Results
- Supervised adaptive ICL significantly outperforms static ICL and zero-shot baselines on all three text classification datasets
- QPP-based unsupervised methods show improved efficiency but lag behind supervised approaches in effectiveness
- Dynamic example selection through adaptive ICL provides better trade-offs between computational cost and prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL examples can be ranked by task-specific utility rather than similarity alone.
- Mechanism: A supervised ranking model (bi-encoder or cross-encoder) is trained on triplets (test instance, useful example, non-useful example) to predict downstream prediction correctness.
- Core assumption: Downstream task performance correlates with example selection quality.
- Evidence anchors:
  - [abstract]: "considering an example to be relevant if including it in the prompt instruction leads to a correct prediction."
  - [section 4]: "For training the ranking model, pairs of instances (relevant and non-relevant examples) were collected... each pair (x, z_i) was then tested to check whether a 1-shot prediction with z_i was correct."
  - [corpus]: Weak - no direct corpus evidence for supervised example ranking effectiveness.
- Break condition: If example usefulness is context-dependent and non-transferable across different LLM decoders or prompts.

### Mechanism 2
- Claim: Query performance prediction (QPP) techniques can dynamically adjust the number of ICL examples.
- Mechanism: QPP scores estimate the usefulness of top-ranked examples; low scores trigger inclusion of more examples to improve downstream prediction quality.
- Core assumption: QPP metrics (e.g., NQC) that estimate retrieval quality for queries can be adapted to estimate example usefulness in ICL.
- Evidence anchors:
  - [section 3.2]: "Different from the rank cut-off strategies, query performance prediction (QPP) models seek to estimate the retrieval quality of a query... can be applied to the top-similar examples retrieved in ICL."
  - [section 6]: "We employ a QPP-based unsupervised strategy... compute the rank cutoff in a relatively simple way, stated as follows."
  - [corpus]: Weak - no direct corpus evidence that QPP adaptations improve ICL effectiveness.
- Break condition: If QPP metrics do not correlate with example usefulness across different datasets or tasks.

### Mechanism 3
- Claim: Faceted IR techniques can diversify ICL examples to prevent decoder bias.
- Mechanism: Topical diversity in examples ensures broader coverage and reduces bias toward a single topic or class descriptor.
- Core assumption: Diversity in examples improves LLM decoder performance by providing varied contexts.
- Evidence anchors:
  - [section 5]: "topical diversity of the examples should play an important role in preventing a decoder bias towards a single topic."
  - [section 5]: "Levy et al. [38] show that diversifying the few-shot examples... improves the downstream task of compositional generalisation."
  - [corpus]: Weak - no direct corpus evidence that diversity improves ICL effectiveness.
- Break condition: If example diversity does not translate to improved downstream prediction accuracy.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the foundation for understanding how examples are used to control LLM output without fine-tuning.
  - Quick check question: In ICL, are model parameters updated during inference?

- Concept: Query Performance Prediction (QPP)
  - Why needed here: QPP techniques estimate retrieval quality and can be adapted to predict example usefulness in ICL.
  - Quick check question: What is the main difference between QPP and rank cut-off strategies?

- Concept: Neural Ranking Models
  - Why needed here: Neural rankers can learn task-specific relevance (usefulness) for ICL examples.
  - Quick check question: What is the key difference between bi-encoder and cross-encoder architectures?

## Architecture Onboarding

- Component map: Test instance -> Retrieval (BM25/neural ranker) -> Example selection (adaptive/QPP/supervised) -> LLM inference -> Prediction
- Critical path: Test instance ‚Üí Retrieval ‚Üí Example selection ‚Üí LLM inference ‚Üí Prediction
- Design tradeoffs:
  - Efficiency vs. effectiveness: More examples improve performance but increase latency
  - Supervised vs. unsupervised: Supervised methods are more effective but require labeled data
  - Bi-encoder vs. cross-encoder: Bi-encoders are faster, cross-encoders are more precise
- Failure signatures:
  - Low prediction accuracy: Examples are not useful or diverse enough
  - High latency: Too many examples are being selected
  - Poor generalization: Model overfits to training example selection
- First 3 experiments:
  1. Compare static ICL (fixed number of examples) vs. adaptive ICL (QPP-based) on text classification datasets
  2. Train a supervised ranker to predict example usefulness and evaluate on held-out test sets
  3. Implement faceted example selection to improve diversity and measure impact on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the score distributions of useful and not useful ICL examples be modeled to improve unsupervised rank cutoff methods?
- Basis in paper: [explicit] The paper discusses the challenge of adapting unsupervised approaches like score distribution-based models (e.g., mixture of Normal-Exponential distributions) to ICL, noting that the score distributions of useful and not useful examples may not follow the same pattern as in traditional IR.
- Why unresolved: The paper highlights that the notion of relevance in ICL (downstream utility) is fundamentally different from traditional IR, making it unclear how existing score distribution models can be adapted.
- What evidence would resolve it: Empirical studies comparing the score distributions of useful and not useful ICL examples and testing whether adapted models (e.g., different mixture distributions) improve rank cutoff accuracy.

### Open Question 2
- Question: How sensitive is ICL to the choice of neural retrieval model (e.g., bi-encoder vs. cross-encoder) for selecting few-shot examples?
- Basis in paper: [explicit] The paper suggests exploring the potential benefits of different neural retrieval models, such as bi-encoders and cross-encoders, for selecting useful ICL examples, noting that cross-encoders may be more precise but less efficient.
- Why unresolved: The paper does not provide empirical comparisons of different retrieval models in the context of ICL, leaving the optimal choice unclear.
- What evidence would resolve it: Controlled experiments comparing the effectiveness and efficiency of different retrieval models (e.g., bi-encoder, cross-encoder, Siamese) in selecting ICL examples across various downstream tasks.

### Open Question 3
- Question: How can the standard notion of diversity in IR be extended to consider the latent dependence between the input and output of an LLM decoder in ICL?
- Basis in paper: [explicit] The paper discusses the importance of topical diversity in ICL examples to prevent decoder bias and suggests adapting IR metrics like ùõº-nDCG to measure example effectiveness for downstream tasks.
- Why unresolved: The paper does not propose a concrete method for defining or measuring diversity in ICL that accounts for the decoder‚Äôs generation path and its alignment with the downstream task.
- What evidence would resolve it: Development and validation of a diversity metric or method that incorporates both the input examples and their impact on the LLM‚Äôs output, tested across multiple ICL tasks.

## Limitations

- The paper relies heavily on theoretical mapping between IR and ICL without extensive empirical validation across diverse scenarios
- Experimental results are limited to text classification tasks using a single LLM (GPT-J-6B), limiting generalizability
- The effectiveness of unsupervised QPP-based methods and example diversity benefits require further empirical substantiation

## Confidence

- High confidence: The empirical superiority of supervised adaptive ICL over static ICL and zero-shot baselines is well-established through experimental results
- Medium confidence: The theoretical mapping between IR concepts and ICL is logically sound but lacks extensive validation across diverse scenarios
- Low confidence: The effectiveness of unsupervised QPP-based methods and the benefits of example diversity require further empirical substantiation

## Next Checks

1. Test the proposed methods across diverse task types (e.g., question answering, summarization) to evaluate generalizability beyond text classification
2. Conduct ablation studies to isolate the individual contributions of supervised ranking, QPP adaptation, and example diversity to overall performance
3. Compare against alternative example selection strategies, including random sampling and similarity-based methods, to establish relative effectiveness