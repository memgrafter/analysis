---
ver: rpa2
title: 'AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive
  Scenarios'
arxiv_id: '2410.19346'
source_url: https://arxiv.org/abs/2410.19346
tags:
- social
- goals
- information
- goal
- scenario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentSense, a benchmark for evaluating the
  social intelligence of language agents in interactive scenarios. The authors address
  limitations in previous work by constructing 1,225 diverse social scenarios from
  scripts using a bottom-up approach, ensuring scenario diversity, complexity, and
  multi-perspective evaluation.
---

# AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios

## Quick Facts
- **arXiv ID**: 2410.19346
- **Source URL**: https://arxiv.org/abs/2410.19346
- **Reference count**: 38
- **Key outcome**: The authors introduce AgentSense, a benchmark for evaluating social intelligence of language agents in interactive scenarios, finding that while larger models like GPT-4o show strong performance, all models struggle with complex scenarios and high-level growth goals.

## Executive Summary
This paper introduces AgentSense, a benchmark for evaluating the social intelligence of language agents in interactive scenarios. The authors address limitations in previous work by constructing 1,225 diverse social scenarios from scripts using a bottom-up approach, ensuring scenario diversity, complexity, and multi-perspective evaluation. They evaluate agents through multi-turn interactions, focusing on goal completion and implicit reasoning about private information, using ERG theory to analyze social goals. Experiments reveal that while larger models like GPT-4o and Qwen2.5-14b show strong social intelligence, all models struggle with complex scenarios and high-level growth goals. Even the best models need improvement in private information reasoning, and agents are more sensitive to profile changes when pursuing certain social goals.

## Method Summary
The authors construct 1,225 social scenarios using a bottom-up approach, extracting templates from 74,452 script episodes and synthesizing diverse characters to instantiate scenarios. They evaluate language model-driven agents through multi-turn interactions, emphasizing goal completion and implicit reasoning about private information. The evaluation employs multi-perspective assessment (self, other, and external judges) and introduces a profile sensitivity index (PSI) to measure stability across different character instantiations. Agents role-play characters in scenarios, interact through conversations, and are evaluated on their ability to achieve social goals and reason about private information.

## Key Results
- Larger models like GPT-4o and Qwen2.5-14b outperform smaller models on social intelligence tasks.
- All models struggle with complex scenarios and high-level growth goals, with significant performance gaps in implicit reasoning about private information.
- Agents show profile sensitivity, being more affected by character changes when pursuing relatedness and growth goals compared to existence goals.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bottom-up scenario construction from real scripts ensures diverse and realistic social scenarios.
- Mechanism: The authors extract scenarios from actual scripts rather than manually designing them, mitigating data leakage and expanding into 1,225 scenarios covering existence, relatedness, and growth goals via ERG theory.
- Core assumption: Real scripts provide naturally occurring social interactions with varied goals and complexities that can be reliably extracted and generalized.
- Evidence anchors:
  - [abstract] "Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts."
  - [section] "We propose pipelines for the two parts respectively as follows: Template Construction... Scenario Instantiating."
- Break condition: If script extraction fails to capture sufficient goal diversity or introduces biases from script genres, the benchmark may not generalize to broader social contexts.

### Mechanism 2
- Claim: Multi-perspective evaluation (self, other, external judges) reduces bias in assessing goal completion.
- Mechanism: Agents are interviewed about their own goal completion, other agents evaluate them, and third-party models judge based on conversation history, with average scores used for final assessment.
- Core assumption: Different perspectives provide complementary information, and averaging reduces individual bias from any single evaluation method.
- Evidence anchors:
  - [abstract] "We evaluate LLM-driven agents through multi-turn interactions, emphasizing both goal completion and implicit reasoning."
  - [section] "Since social goals can be subjective, we judge its completion from three different aspects, as shown in Figure 2: (1) Self... (2) Other... (3) External..."
- Break condition: If all evaluation methods share common biases (e.g., all favor certain personality types), averaging won't eliminate systematic errors.

### Mechanism 3
- Claim: Profile sensitivity index (PSI) measures stability of social intelligence across diverse character instantiations.
- Mechanism: For scenarios sharing the same template but with different synthesized characters, the standard deviation of goal completion and information reasoning metrics across templates is calculated as PSI.
- Core assumption: Social intelligence should be relatively stable across different character profiles if the underlying model has robust social understanding.
- Evidence anchors:
  - [abstract] "We find that LLMs struggle with complex social scenarios, particularly with high-level growth goals, and their social intelligence is affected by profiles..."
  - [section] "By incorporating diverse characters, we not only enrich the scenarios but also gain insights into the stability of social intelligence when simulating different roles. Thus, we propose profile sensitivity index (PSI)."
- Break condition: If PSI is low due to all models performing poorly rather than stable performance, it may not indicate robustness but rather consistent underperformance.

## Foundational Learning

- **Concept**: Dramaturgical Theory (Goffman, 1959)
  - Why needed here: Provides theoretical framework for viewing social interactions as theatrical performances with roles, settings, and scripts, justifying the scenario construction approach.
  - Quick check question: How does conceptualizing social interaction as performance help in designing evaluation scenarios?

- **Concept**: ERG Theory (Alderfer, 1969)
  - Why needed here: Categorizes social goals into existence, relatedness, and growth needs, enabling systematic classification and analysis of agent objectives across scenarios.
  - Quick check question: What distinguishes existence needs from growth needs in the context of social interactions?

- **Concept**: Multi-turn interactive evaluation
  - Why needed here: Captures the dynamic nature of social interactions where goals evolve and information is revealed gradually, unlike static benchmarks.
  - Quick check question: Why might single-turn interactions fail to capture important aspects of social intelligence?

## Architecture Onboarding

- **Component map**: Scenario construction pipeline (extraction → template generation → leakage mitigation) → Agent simulation (multi-turn conversation) → Multi-perspective evaluation (self/other/external judges + PSI calculation)
- **Critical path**: Scenario template generation → Character synthesis → Multi-turn interaction → Evaluation collection → PSI computation
- **Design tradeoffs**: Automated pipeline increases scalability but requires manual validation; multiple judges reduce bias but increase computational cost; diverse scenarios improve coverage but complicate analysis
- **Failure signatures**: High PSI values indicate poor stability; significant performance gaps between sender/receiver roles suggest communication asymmetry issues; overestimation by self-evaluation indicates need for stronger external validation
- **First 3 experiments**:
  1. Test a single scenario template with multiple character profiles to verify PSI calculation and identify profile sensitivity patterns
  2. Compare single-model vs mixed-model interactions to establish baseline performance and interaction dynamics
  3. Validate leakage mitigation by testing script prediction and blind test performance before and after mitigation steps

## Open Questions the Paper Calls Out
None

## Limitations
- The bottom-up scenario construction approach may introduce systematic biases from source material, potentially limiting generalizability to broader social contexts.
- The multi-perspective evaluation assumes averaging reduces bias, but this may not hold if all evaluation methods share common limitations in assessing social intelligence.
- The profile sensitivity index (PSI) measures variability across character profiles, but it's unclear whether high PSI indicates poor robustness or reflects inherent complexity of certain social scenarios.

## Confidence
- **High Confidence**: The experimental finding that larger models (GPT-4o, Qwen2.5-14b) outperform smaller models on social intelligence tasks is well-supported by the data and aligns with general trends in LLM performance.
- **Medium Confidence**: The claim that all models struggle with complex scenarios and high-level growth goals is supported by the results, but the categorization of scenario complexity and goal levels could benefit from more rigorous validation.
- **Low Confidence**: The assertion that models are more sensitive to profile changes when pursuing certain social goals requires further investigation, as the relationship between goal types and profile sensitivity is not fully established.

## Next Checks
1. **Cross-genre validation**: Test the benchmark's scenarios and evaluation methods on scripts from different genres (e.g., drama, comedy, thriller) to assess generalizability and identify potential genre-specific biases.

2. **Human evaluation comparison**: Conduct a human evaluation study where human participants interact with the agents and judge their social intelligence, comparing these judgments with the automated multi-perspective evaluation to validate the assessment methods.

3. **Ablation study on PSI**: Perform an ablation study by systematically removing certain character attributes (e.g., personality traits, social roles) to determine which factors most significantly contribute to profile sensitivity and whether PSI accurately captures robustness.