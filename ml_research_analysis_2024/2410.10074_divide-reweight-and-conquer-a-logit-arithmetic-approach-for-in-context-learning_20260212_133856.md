---
ver: rpa2
title: 'Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context
  Learning'
arxiv_id: '2410.10074'
source_url: https://arxiv.org/abs/2410.10074
tags:
- lara
- examples
- arxiv
- b-lara
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LARA, a logit arithmetic reweighting approach
  that improves in-context learning by dividing demonstrations into subgroups and
  reweighting their logits using a non-gradient optimization method. Experiments on
  BBH and MMLU benchmarks with Llama3.1-8B, Mistral-7B, and Gemma-7B models show that
  LARA and its binary variant B-LARA outperform direct in-context learning and baseline
  methods across all tested models, achieving up to 2.12 point accuracy gains.
---

# Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning

## Quick Facts
- arXiv ID: 2410.10074
- Source URL: https://arxiv.org/abs/2410.10074
- Reference count: 28
- Primary result: LARA and B-LARA improve ICL accuracy by up to 2.12 points on BBH and MMLU benchmarks while reducing memory usage

## Executive Summary
This paper introduces LARA, a logit arithmetic approach that improves in-context learning by dividing demonstrations into subgroups and reweighting their logits using non-gradient optimization. The method addresses two key challenges in ICL: memory constraints with long demonstrations and the inability to adapt to task-specific information distribution. Experiments show LARA and its binary variant B-LARA outperform direct ICL and baseline methods across Llama3.1-8B, Mistral-7B, and Gemma-7B models, achieving up to 2.12 point accuracy gains while being more memory efficient.

## Method Summary
LARA divides long input demonstrations into smaller subgroups, generates logits for each subgroup using the LLM, then optimizes continuous weights via CMA-ES to reweight these logits before aggregation. The binary variant B-LARA constrains weights to {0,1}, simplifying the search space and enabling direct demonstration selection. Both methods optimize weights on a held-out validation subset to avoid extra labeling while adapting to task-specific patterns.

## Key Results
- LARA and B-LARA outperform direct ICL and baseline methods on BBH and MMLU benchmarks
- Accuracy gains up to 2.12 points across all tested models (Llama3.1-8B, Mistral-7B, Gemma-7B)
- Memory efficiency allows handling longer input sequences without exceeding GPU limits
- Binary constraint in B-LARA further enhances performance by simplifying search space

## Why This Works (Mechanism)

### Mechanism 1
Dividing demonstrations into subgroups reduces attention matrix size from O(n²) to O(n²/k), allowing the model to process longer demonstrations within fixed GPU memory limits while maintaining or improving performance.

### Mechanism 2
Reweighting logits via non-gradient optimization (CMA-ES) allows adaptive aggregation of demonstration contributions, outperforming uniform averaging by optimizing weights to minimize cross-entropy loss on validation data.

### Mechanism 3
Binary constraints in B-LARA simplify the search space and act as demonstration selection, improving both efficiency and accuracy by eliminating groups with zero weight during inference.

## Foundational Learning

- **In-Context Learning (ICL)**: Understanding ICL is essential since LARA augments it by improving how demonstrations are used. Quick check: What distinguishes ICL from fine-tuning in terms of model parameter usage?
- **Logit arithmetic and ensembling**: Necessary to understand LARA's core innovation of combining multiple logit outputs via weighted averaging. Quick check: How does logit averaging differ from probability averaging in model ensembling?
- **Non-gradient optimization (CMA-ES)**: Key to explaining why LARA avoids backpropagation. Quick check: What is the main advantage of CMA-ES over gradient-based methods in high-dimensional, non-differentiable optimization?

## Architecture Onboarding

- **Component map**: Partition → LLM inference per subgroup → Cross-validation weight optimization → Apply weights at inference
- **Critical path**: Partition demonstrations → Generate logits for each subgroup → Optimize weights using CMA-ES or (1+1)-ES → Combine reweighted logits for final prediction
- **Design tradeoffs**: Memory vs. performance (more subgroups reduce memory but may hurt performance), optimization complexity (continuous weights allow finer control but require more iterations), validation strategy (using Dtrain for validation avoids extra labeling but reduces available demonstrations)
- **Failure signatures**: Out-of-memory errors with small k, degraded accuracy with poorly balanced subgroups, slow optimization convergence with large search spaces
- **First 3 experiments**: 1) Run ICL with N=32 examples and record accuracy/memory usage, 2) Apply LARA with k=4 subgroups and L=8, optimize weights, compare accuracy/memory, 3) Apply B-LARA with same parameters, compare accuracy, memory, and inference speed

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Performance sensitivity to partitioning strategy and optimal subgroup size remains unclear
- Using demonstration set for validation introduces potential bias and reduces available training examples
- Limited discussion of optimization stability, convergence properties, and runtime efficiency

## Confidence
- **High Confidence**: Memory efficiency improvements and benchmark accuracy gains are well-supported
- **Medium Confidence**: Mechanism explaining why logit reweighting improves performance is plausible but not conclusively proven
- **Low Confidence**: Generalization claims to low-resource and many-shot scenarios lack comprehensive validation

## Next Checks
1. **Partitioning Sensitivity Analysis**: Systematically vary k and L to determine performance sensitivity to partitioning strategy, measuring accuracy, memory usage, and optimization convergence time.
2. **Cross-Validation Stability Test**: Evaluate weight optimization stability by performing multiple runs with different validation set selections from Dtrain, measuring variance in weights and accuracy.
3. **Memory-Performance Tradeoff Analysis**: Conduct controlled experiments varying GPU memory constraints to quantify the relationship between memory savings and accuracy degradation.