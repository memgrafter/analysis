---
ver: rpa2
title: A Competition Winning Deep Reinforcement Learning Agent in microRTS
arxiv_id: '2402.08112'
source_url: https://arxiv.org/abs/2402.08112
tags:
- competition
- training
- raisocketai
- maps
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes RAISocketAI, the first deep reinforcement
  learning (DRL) agent to win the IEEE microRTS competition. The agent uses 7 policy
  networks selected based on map size and compute capabilities, trained using Proximal
  Policy Optimization with shaped and sparse rewards.
---

# A Competition Winning Deep Reinforcement Learning Agent in microRTS

## Quick Facts
- arXiv ID: 2402.08112
- Source URL: https://arxiv.org/abs/2402.08112
- Authors: Scott Goodfriend
- Reference count: 40
- Primary result: First DRL agent to win IEEE microRTS competition with 72% win rate

## Executive Summary
RAISocketAI is a deep reinforcement learning agent that won the IEEE microRTS competition by combining map-specific transfer learning with a hybrid reward schedule. The agent uses 7 policy networks selected based on map size and compute capabilities, trained using Proximal Policy Optimization with shaped and sparse rewards. Transfer learning to specific maps was critical to winning performance, achieving a 72% win rate in the competition and defeating all prior competition winners on 7 of 8 maps.

## Method Summary
RAISocketAI employs Proximal Policy Optimization with three value heads (shaped reward, win-loss, unit cost difference) and a dynamic reward weighting schedule transitioning from shaped to sparse rewards. The approach uses transfer learning by fine-tuning base policies on specific maps, and optionally incorporates imitation learning via behavior cloning from prior competition winners followed by PPO fine-tuning. The agent architecture includes observation encoders (DoubleCone or squnet), GridNet-style action logits, and invalid action masking to ensure legal moves.

## Key Results
- First DRL agent to win IEEE microRTS competition with 72% overall win rate
- Defeated all prior competition winners on 7 of 8 maps
- Achieved >90% win rate on specific maps through transfer learning
- Imitation learning approach (RAI-BC-PPO) achieved 88% win rate without map-specific models

## Why This Works (Mechanism)

### Mechanism 1
Map-specific transfer learning with hybrid reward schedule enables rapid adaptation to local terrain and timing constraints. Training on multiple small maps creates a general policy, then fine-tuning on target maps rapidly adapts behavior to local map features. Core assumption: local map structure dominates optimal strategy. Break condition: transfer may fail on highly varied maps without curriculum learning.

### Mechanism 2
DoubleCone architecture with multiple value heads improves learning stability and policy expressiveness. Three separate value heads estimate shaped, sparse, and cost-based returns, dynamically weighted to balance exploration and exploitation. Core assumption: joint training of separate value heads is stable and accelerates convergence. Break condition: poor reward weight tuning may cause overfitting to dense rewards.

### Mechanism 3
Behavior cloning from prior winners bootstraps learning, reducing training time and eliminating manual reward shaping. Supervised pretraining on high-quality action trajectories provides a reasonable prior policy, with PPO fine-tuning refining performance. Core assumption: prior agents' actions encode near-optimal generalizable policies. Break condition: cloned trajectories may be too specialized or adversarial, causing divergence during fine-tuning.

## Foundational Learning

- **Reinforcement Learning Policy Gradient (PPO)**: Needed for stable training in high-dimensional action spaces without destructive policy collapse. Quick check: What is the role of the clipping term in PPO loss and why is it critical?
- **Transfer Learning in RL**: Enables rapid adaptation to specific maps without retraining from scratch. Quick check: How does freezing network layers during transfer help preserve general strategies?
- **Imitation Learning (Behavior Cloning)**: Bootstraps learning from expert trajectories to reduce sample complexity. Quick check: Why is behavior cloning alone insufficient, requiring subsequent RL fine-tuning?

## Architecture Onboarding

- **Component map**: Observation encoder -> Policy head (GridNet-style logits) -> Invalid action masking -> Environment step -> Reward aggregation -> Backward pass -> Policy/value update
- **Critical path**: Forward pass (observation → action logits) → invalid action masking → sampling → environment step → reward aggregation → backward pass → policy/value update
- **Design tradeoffs**: DoubleCone vs squnet (expressiveness vs inference speed); three value heads vs one (stability vs memory); shaped vs sparse rewards (early learning vs generalization)
- **Failure signatures**: Invalid action masking errors → NaNs or stuck policy; reward schedule misconfiguration → collapse to trivial strategies; aggressive transfer learning → catastrophic forgetting
- **First 3 experiments**: 1) Validate invalid action masking on small map; 2) Train baseline on 16x16 map with shaped rewards, confirm >50% win rate; 3) Transfer baseline to difficult map, verify win rate improvement

## Open Questions the Paper Calls Out

### Open Question 1
Does imitation learning with behavior cloning followed by PPO fine-tuning consistently produce agents that generalize better across map sizes compared to training from scratch with PPO? The paper shows RAI-BC-PPO achieved 71% win rate without map-specific models, but only compared for a single training run. Multiple training runs on various map sets would provide a definitive answer.

### Open Question 2
How does performance scale with increasing map size and game duration? Both agents struggle on the largest map (4BloodBath.scmB) with longer game lengths and larger observation-action spaces, but the exact scaling relationship is unexplored. Testing on progressively larger maps would reveal scaling behavior.

### Open Question 3
What is the impact of different neural network architectures on performance and inference time? The paper compares DoubleCone, squnet, and deep squnet, but doesn't explore other architectures like transformers. Training with different architectures on the same maps would provide comprehensive comparison.

## Limitations
- Neural network architecture details and complete training hyperparameters are underspecified
- Scalability of DoubleCone architecture to larger maps remains uncertain
- Long-term stability of three-value-head approach in dynamic competitive environments needs validation

## Confidence

**High confidence**: Map-specific transfer learning combined with hybrid reward schedules improves competition performance (supported by competition results and ablation studies).

**Medium confidence**: Imitation learning can train competitive agents without handcrafted rewards (supported by results, but sample efficiency and generalization need further investigation).

**Low confidence**: Scalability of DoubleCone architecture to larger maps and long-term stability of three-value-head approach remain uncertain without additional empirical validation.

## Next Checks

1. **Architecture ablation study**: Compare DoubleCone vs squnet architectures across varying map sizes, measuring both performance and inference time to validate scalability claims.

2. **Reward schedule sensitivity analysis**: Test different shaped-to-sparse reward transition schedules on learning stability and final performance, focusing on optimal transition points.

3. **Transfer learning robustness**: Evaluate transfer learning when fine-tuning on multiple maps simultaneously versus sequential transfer, measuring catastrophic forgetting of base policy behaviors.