---
ver: rpa2
title: A Claim Decomposition Benchmark for Long-form Answer Verification
arxiv_id: '2410.12558'
source_url: https://arxiv.org/abs/2410.12558
tags:
- claim
- decomposition
- atomic
- claims
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Chinese Atomic Claim Decomposition Dataset
  (CACDD) for long-form answer verification. The dataset addresses the challenge of
  identifying atomic, checkworthy claims in LLM-generated responses by decomposing
  complex sentences into indivisible, verifiable facts.
---

# A Claim Decomposition Benchmark for Long-form Answer Verification

## Quick Facts
- arXiv ID: 2410.12558
- Source URL: https://arxiv.org/abs/2410.12558
- Reference count: 27
- This paper introduces the Chinese Atomic Claim Decomposition Dataset (CACDD) for long-form answer verification

## Executive Summary
This paper introduces the Chinese Atomic Claim Decomposition Dataset (CACDD) for long-form answer verification. The dataset addresses the challenge of identifying atomic, checkworthy claims in LLM-generated responses by decomposing complex sentences into indivisible, verifiable facts. The authors developed a three-step annotation pipeline: sentence decomposition, classification, and atomic claim decomposition, resulting in 500 question-answer pairs with 4,956 atomic claims. Experimental results show that existing LLMs struggle with this task, with zero-shot and few-shot performances remaining low across various metrics (EM, Rouge-l, BertScore). Even fine-tuned models show only modest improvements, indicating significant room for advancement in claim decomposition capabilities.

## Method Summary
The authors created the CACDD dataset through a three-step annotation pipeline: first decomposing responses into sentences, then classifying them into fact/opinion/instruction/other categories, and finally decomposing factual sentences into atomic claims with co-reference resolution. The dataset contains 500 question-answer pairs with 4,956 atomic claims. They evaluated multiple LLMs (Baichuan2-7B, Glm4-9B, Llama3-8B, Mistral-7B, Qwen2-7B, Solar-10.7B, GPT-3.5) using zero-shot, few-shot, and fine-tuned approaches, with fine-tuning performed on Llama3-8B using 350 training examples and 150 test examples.

## Key Results
- Existing LLMs show poor performance on atomic claim decomposition, with zero-shot and few-shot results remaining low across EM, Rouge-l, and BertScore metrics
- Even the fine-tuned Llama3-8B model shows only modest improvements, indicating fundamental challenges in the task
- The dataset demonstrates significant variance in atomic claim counts across similar responses, highlighting the complexity of the decomposition task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Atomic claim decomposition enables reliable long-form answer verification by isolating checkworthy facts from subjective content.
- Mechanism: By decomposing responses into indivisible, verifiable atomic claims, the system ensures each fact can be independently assessed without interference from context-dependent references or opinions.
- Core assumption: The definition of "atomic" (indivisible, semantically complete, verifiable, context-independent) is both theoretically sound and practically implementable.
- Evidence anchors:
  - [abstract] "identifying atomic, checkworthy claims in LLM-generated responses by decomposing complex sentences into indivisible, verifiable facts"
  - [section] "An atomic claim should possess the following properties: Indivisibility, Semantic integrity, Verifiability, Context independence"
- Break condition: If claims cannot be truly isolated from context or if verification resources cannot handle the volume of decomposed claims.

### Mechanism 2
- Claim: The three-step annotation pipeline (decomposition → classification → atomic decomposition) ensures high-quality claim extraction.
- Mechanism: First sentences are structurally separated, then filtered for factual content, and finally decomposed into atomic units with co-reference resolution.
- Core assumption: Human annotators can consistently apply the four properties of atomic claims across diverse Chinese long-form responses.
- Evidence anchors:
  - [section] "We design an annotation pipeline for atomic claim decomposition. Based on this pipeline, we have created the CACDD dataset through manual annotation, which includes three steps: sentence decomposition, sentence classification and atomic claim decomposition"
  - [section] "Each claim in this dataset has been resolved for co-references to satisfy the principle of context independence"
- Break condition: Inconsistent annotation quality or inability to handle complex co-reference chains.

### Mechanism 3
- Claim: Zero-shot and few-shot prompting limitations reveal fundamental challenges in atomic claim decomposition that require new architectural approaches.
- Mechanism: LLMs fail to decompose complex claims accurately even with instruction tuning, indicating the task requires specialized decomposition capabilities beyond general language understanding.
- Core assumption: Performance gaps between human annotations and LLM outputs reflect inherent task difficulty rather than prompt engineering limitations.
- Evidence anchors:
  - [abstract] "Experimental results show that existing LLMs struggle with this task, with zero-shot and few-shot performances remaining low across various metrics"
  - [section] "Results indicate that the open-source LLMs do not perform satisfactorily on this task, and even the outstanding GPT-3.5 still has a significant gap compared to humans"
- Break condition: If fine-tuned models cannot bridge the performance gap with additional high-quality training data.

## Foundational Learning

- Concept: Atomic claim properties (indivisibility, semantic integrity, verifiability, context independence)
  - Why needed here: These properties define the quality criteria that distinguish good atomic claims from incomplete or context-dependent statements
  - Quick check question: Can you identify which property is violated if a claim references "it" without resolving what "it" refers to?

- Concept: Co-reference resolution in Chinese text
  - Why needed here: The dataset requires co-reference resolution to satisfy context independence, which is particularly challenging in Chinese due to topic-prominent structure
  - Quick check question: How would you handle co-references in a sentence like "The company announced its new product. It will launch next month"?

- Concept: Fact vs. opinion classification criteria
  - Why needed here: The pipeline requires accurate classification to filter non-checkworthy content before decomposition
  - Quick check question: What distinguishes "The GDP grew by 3%" from "The economy is performing well" in terms of checkworthiness?

## Architecture Onboarding

- Component map: Data collection → Sentence decomposition → Classification (fact/opinion/instruction/other) → Atomic claim decomposition → Verification pipeline → Evaluation metrics
- Critical path: Question + Response → Sentence decomposition → Fact classification → Atomic decomposition → Claim verification
- Design tradeoffs: Manual annotation quality vs. scalability, atomic granularity vs. verification efficiency, Chinese language specificity vs. model generalization
- Failure signatures: High variance in atomic claim counts across similar responses, poor co-reference resolution in decomposed claims, classification errors leading to non-factual claims being decomposed
- First 3 experiments:
  1. Test zero-shot performance on the entire CACDD dataset to establish baseline LLM capabilities
  2. Evaluate the impact of co-reference resolution by comparing decomposed claims with and without reference resolution
  3. Measure classification accuracy by comparing human-annotated categories against LLM predictions on the sentence classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the granularity of claim decomposition impact the overall accuracy of fact-checking systems in long-form question answering?
- Basis in paper: [explicit] The paper discusses the importance of identifying and decomposing claims for effective answer verification, highlighting that existing approaches overlook the importance of identifying and decomposing claims worthy of verification.
- Why unresolved: While the paper introduces a benchmark for claim decomposition, it does not explore how varying levels of granularity in claim decomposition affect the accuracy of fact-checking systems. The relationship between granularity and verification accuracy remains unexplored.
- What evidence would resolve it: Experiments comparing fact-checking accuracy using different levels of claim granularity, analyzing how detailed decomposition impacts the identification of false claims.

### Open Question 2
- Question: What are the limitations of current LLMs in handling context-dependent claims, and how can these limitations be addressed to improve claim decomposition?
- Basis in paper: [inferred] The paper mentions that the task of atomic claim decomposition is challenging, with existing LLMs showing low performance in zero-shot and few-shot settings. It also notes the importance of context independence for atomic claims.
- Why unresolved: The paper highlights the challenges but does not delve into specific limitations of LLMs regarding context-dependent claims or propose solutions to address these limitations.
- What evidence would resolve it: Analysis of LLM performance on context-dependent claims, identifying specific weaknesses, and experiments testing methods to enhance context handling in claim decomposition.

### Open Question 3
- Question: How can the principles of logical atomism be further applied to improve the definition and identification of atomic claims in complex sentence structures?
- Basis in paper: [explicit] The paper defines atomic claims based on Bertrand Russell’s philosophy of logical atomism, emphasizing properties like indivisibility, semantic integrity, verifiability, and context independence.
- Why unresolved: While the paper provides a definition of atomic claims, it does not explore how these principles can be further refined or applied to handle more complex sentence structures effectively.
- What evidence would resolve it: Development and testing of enhanced models or methods that incorporate logical atomism principles to improve the identification and decomposition of atomic claims in complex sentences.

## Limitations

- Limited generalizability: The dataset focuses exclusively on Chinese language long-form answers, raising questions about applicability to other languages and domains
- Annotation quality variance: The paper does not report inter-annotator agreement statistics, making it difficult to assess annotation consistency across the dataset
- Model architecture constraints: Experiments focus on decoder-only LLMs without exploring encoder-decoder architectures or specialized fine-tuning approaches

## Confidence

**High confidence**: The fundamental problem definition (need for atomic claim decomposition in long-form verification) and the dataset construction methodology are well-established and clearly described.

**Medium confidence**: The experimental results showing LLM limitations are credible, but the specific performance numbers depend on implementation details not fully specified in the paper.

**Low confidence**: Claims about the theoretical soundness of atomic claim properties across all domains require further empirical validation beyond the Chinese long-form QA context studied here.

## Next Checks

**Validation Check 1**: Conduct inter-annotator agreement analysis on a subset of the dataset to establish the reliability of the three-step annotation pipeline and atomic claim definitions.

**Validation Check 2**: Test the zero-shot performance of multiple LLMs on the same dataset using the exact prompt templates from the GitHub repository to verify reproducibility of the reported performance gaps.

**Validation Check 3**: Evaluate whether the fine-tuned Llama3-8B model maintains performance when tested on out-of-domain long-form responses (e.g., medical, technical, or legal domains) to assess generalizability beyond the WebCPM-based training data.