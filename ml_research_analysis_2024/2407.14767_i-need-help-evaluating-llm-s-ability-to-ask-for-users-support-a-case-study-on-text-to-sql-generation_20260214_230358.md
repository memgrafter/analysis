---
ver: rpa2
title: 'I Need Help! Evaluating LLM''s Ability to Ask for Users'' Support: A Case
  Study on Text-to-SQL Generation'
arxiv_id: '2407.14767'
source_url: https://arxiv.org/abs/2407.14767
tags:
- support
- then
- user
- llms
- curve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes metrics to evaluate LLMs'' ability to proactively
  seek user support, focusing on the trade-off between performance improvement and
  user burden. The authors use text-to-SQL generation as a case study, introducing
  three methods for LLMs to request support: Direct Ask, Write then Ask, and Execute
  then Ask.'
---

# I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation

## Quick Facts
- arXiv ID: 2407.14767
- Source URL: https://arxiv.org/abs/2407.14767
- Reference count: 40
- Key outcome: LLMs significantly benefit from external signals like SQL execution results when deciding to seek user support, with the ability to identify need and utilize support being distinct capabilities.

## Executive Summary
This paper proposes metrics to evaluate LLMs' ability to proactively seek user support, focusing on the trade-off between performance improvement and user burden. The authors use text-to-SQL generation as a case study, introducing three methods for LLMs to request support: Direct Ask, Write then Ask, and Execute then Ask. The primary finding is that without external feedback like SQL execution results, many LLMs struggle to recognize their need for user support, performing similarly to random baselines. However, when provided with execution results, LLMs can significantly improve their performance-burden trade-off. The study highlights the importance of external signals in helping LLMs determine when to seek help and suggests that the ability to identify the need for support and the ability to utilize that support are distinct capabilities that require separate enhancement.

## Method Summary
The study evaluates LLMs' support-seeking behavior using text-to-SQL generation tasks. Three methods are implemented: Direct Ask (using only database schema and query), Write then Ask (generating SQL first then deciding to ask), and Execute then Ask (using execution results to decide). The evaluation uses the BIRD dataset with human-annotated external knowledge as support, and measures performance using Delta-Burden Curve (DBC), Area Under Delta-Burden Curve (AUDBC), Precision and Recall of Asking for Support (Pask, Rask), and Flip Rate metrics. The experiments compare these methods against random baselines to assess how effectively LLMs can determine when to seek user assistance.

## Key Results
- Without external feedback, LLMs perform near-random baselines on support-seeking tasks
- Execution results significantly improve LLMs' ability to balance performance gains with user burden
- The ability to identify need for support and the ability to utilize support are distinct capabilities requiring separate enhancement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External execution results provide critical signals that help LLMs determine when they need support
- Mechanism: When LLMs receive execution results from the database, they can compare these results with their expectations to identify discrepancies that indicate uncertainty in their SQL generation
- Core assumption: LLMs can effectively process and interpret execution results to calibrate their confidence in generated SQL
- Evidence anchors:
  - [abstract]: "However, when provided with execution results, LLMs can significantly improve their performance-burden trade-off."
  - [section]: "Our findings indicate that LLMs significantly benefit from external signals, such as SQL execution results."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.423, average citations=0.0.

### Mechanism 2
- Claim: The ability to identify the need for support and the ability to utilize support are distinct capabilities
- Mechanism: LLMs must first recognize when their output may be incorrect, then separately determine how to effectively use provided support to correct errors
- Core assumption: Recognition of need for support is not inherently tied to the ability to use that support effectively
- Evidence anchors:
  - [abstract]: "The study highlights the importance of external signals in helping LLMs determine when to seek help and suggests that the ability to identify the need for support and the ability to utilize the support are distinct capabilities"
  - [section]: "We further decompose DBC into the ability to identify the need for support and the ability to utilize the support"
  - [corpus]: Learning to Ask: When LLM Agents Meet Unclear Instruction

### Mechanism 3
- Claim: Different information sources (instruction, output, execution results) have varying effectiveness in helping LLMs determine support needs
- Mechanism: The more comprehensive the information available to the LLM, the better it can assess its confidence and need for support
- Core assumption: Information about execution results provides more reliable signals than just the instruction or generated output alone
- Evidence anchors:
  - [abstract]: "Our experiments reveal that without external feedback, many LLMs struggle to recognize their need for additional support"
  - [section]: "Direct Ask (DA) : w = ( db, x), composed of database schema db and user data requirement x"
  - [corpus]: User Perceptions vs. Proxy LLM Judges: Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios

## Foundational Learning

- Concept: Text-to-SQL generation
  - Why needed here: The entire study is based on text-to-SQL as a case study for evaluating LLM support-seeking behavior
  - Quick check question: What are the key challenges in mapping natural language to SQL queries?

- Concept: ROC curves and AUC metrics
  - Why needed here: The paper uses Delta-Burden Curve (DBC) which is inspired by ROC curve concepts to evaluate performance
  - Quick check question: How does the Area Under Curve (AUC) metric help in comparing different methods?

- Concept: Confidence calibration in LLMs
  - Why needed here: The study relies on LLMs' ability to provide well-calibrated confidence scores when deciding whether to ask for support
  - Quick check question: What factors affect the calibration of confidence scores in language models?

## Architecture Onboarding

- Component map: User query -> LLM processing -> Support decision -> (if needed) User support -> SQL generation -> Database execution -> Performance evaluation
- Critical path: User query → LLM processing → Support decision → (if needed) User support → SQL generation → Database execution → Performance evaluation
- Design tradeoffs: Balancing user burden (frequency of support requests) against performance improvement, choosing between different information sources for support decisions
- Failure signatures: Random baseline performance, inability to improve without execution results, mismatch between recognition and utilization abilities
- First 3 experiments:
  1. Compare Direct Ask method performance against random baseline
  2. Test Write then Ask method to see if self-generated output helps in support decision
  3. Implement Execute then Ask method to measure impact of execution results on performance-burden tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better identify when they need support based on instruction and output alone, without requiring external feedback like SQL execution results?
- Basis in paper: [explicit] The paper notes that without external feedback, many LLMs struggle to recognize their need for user support, performing similarly to random baselines. The authors state that "the ability to identify the need for support and the ability to utilize that support are distinct capabilities that require separate enhancement."
- Why unresolved: While the paper identifies this as a key limitation, it doesn't provide concrete methods for improving LLMs' ability to self-assess their need for support without external signals. The challenge of developing internal mechanisms for uncertainty quantification remains an open problem in the field.
- What evidence would resolve it: Empirical results showing significant improvements in LLMs' ability to accurately assess their need for support without external feedback, demonstrated through rigorous evaluation across multiple tasks beyond text-to-SQL.

### Open Question 2
- Question: What other types of user support beyond human-annotated external knowledge could be effectively utilized by LLMs, and how can we evaluate their effectiveness?
- Basis in paper: [inferred] The paper mentions that it "primarily focuses on a single type of support: human-annotated external knowledge" and notes that "there are many other types of support that LLMs might require." However, it doesn't explore these alternatives.
- Why unresolved: The paper's scope is limited to one type of support, leaving a significant gap in understanding how LLMs could benefit from other forms of assistance such as tool use, dynamic knowledge retrieval, or interactive clarification sessions with users.
- What evidence would resolve it: Comparative studies evaluating multiple types of support across various tasks, showing which forms of support are most effective in different contexts and how they impact the performance-burden trade-off.

### Open Question 3
- Question: How can we develop methods that enable LLMs to better manage without external feedback in practical applications where such feedback is limited or unavailable?
- Basis in paper: [explicit] The paper's limitations section states that "this reliance on external feedback may not always be feasible in practical applications, where immediate execution or access to external data might be limited."
- Why unresolved: While the paper identifies this practical limitation, it doesn't propose solutions for scenarios where external feedback is constrained. This is particularly relevant for real-world deployment where LLMs need to function with limited resources.
- What evidence would resolve it: Development and validation of novel techniques that allow LLMs to maintain high performance in support-seeking tasks even when external feedback is scarce or delayed, demonstrated through real-world application testing.

## Limitations
- The study is limited to text-to-SQL generation as a single application domain
- The paper relies on execution results being available, which may not be feasible in many practical scenarios
- Weak corpus connections (average citations=0.0) suggest this is a relatively novel research direction with limited prior work

## Confidence
- High confidence: LLMs significantly benefit from external signals like SQL execution results when deciding to seek user support
- Medium confidence: Recognition and utilization of support are distinct capabilities requiring separate enhancement
- Low confidence: Generalizability of findings beyond text-to-SQL tasks

## Next Checks
1. **Cross-task validation**: Test the three support-seeking methods on a different task domain (such as text summarization or code generation) to verify whether the observed patterns generalize beyond text-to-SQL.

2. **Human evaluation study**: Conduct user studies to measure actual user burden and satisfaction with different support-seeking frequencies, comparing model predictions with human preferences for when LLMs should ask for help.

3. **Confidence calibration analysis**: Systematically analyze the relationship between LLM confidence scores and actual SQL correctness across different model sizes and architectures to determine whether poor calibration is a fundamental limitation or model-specific issue.