---
ver: rpa2
title: Improve Vision Language Model Chain-of-thought Reasoning
arxiv_id: '2410.16198'
source_url: https://arxiv.org/abs/2410.16198
tags:
- answer
- direct
- reasoning
- data
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving chain-of-thought
  (CoT) reasoning in vision language models (VLMs), which is crucial for interpretability
  and trustworthiness but hindered by limited training data with detailed rationales.
  The authors propose a two-step approach: first, they distill CoT reasoning paths
  from GPT-4o using short answer annotations to create a dataset of 193k examples
  across various VQA tasks, then fine-tune VLMs with supervised fine-tuning (SFT).'
---

# Improve Vision Language Model Chain-of-thought Reasoning

## Quick Facts
- arXiv ID: 2410.16198
- Source URL: https://arxiv.org/abs/2410.16198
- Reference count: 40
- Key outcome: VLMs achieve 60.9% accuracy on ChartQA using CoT training and DPO, outperforming baseline models

## Executive Summary
This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in vision language models (VLMs), which is crucial for interpretability and trustworthiness but hindered by limited training data with detailed rationales. The authors propose a two-step approach: first, they distill CoT reasoning paths from GPT-4o using short answer annotations to create a dataset of 193k examples across various VQA tasks, then fine-tune VLMs with supervised fine-tuning (SFT). Second, they apply Direct Preference Optimization (DPO) to further refine reasoning by constructing positive and negative pairs from model-generated responses and optimizing for better alignment. Experiments show that SFT significantly improves CoT performance, and DPO further enhances reasoning quality and generalization, outperforming baseline models on benchmarks like ChartQA, A-OKVQA, and MathVista. The study highlights the effectiveness of incorporating detailed rationales and reinforcement learning to strengthen VLM reasoning capabilities.

## Method Summary
The authors employ a two-stage training approach to enhance CoT reasoning in VLMs. First, they use GPT-4o to distill detailed reasoning chains from short-answer VQA datasets, creating a large-scale CoT dataset (193k examples). They then fine-tune LLaVA-Next with supervised fine-tuning (SFT) on this distilled data combined with direct-answer data. Second, they apply Direct Preference Optimization (DPO) to refine reasoning quality by comparing model-generated reasoning chains against ground truth answers to create positive and negative preference pairs. The approach uses LLaVA-Next-8B as the base model, trains on 8 H100 GPUs, and evaluates using both direct and CoT inference templates with answer extraction. The method significantly improves CoT performance across multiple benchmarks including ChartQA, A-OKVQA, and MathVista.

## Key Results
- CoT training significantly improves reasoning accuracy (e.g., 14.4% gain on A-OKVQA)
- DPO further refines CoT performance beyond SFT alone
- Combined CoT+Direct training outperforms task-specific optimization
- Achieves state-of-the-art results on ChartQA (60.9%) and competitive performance on A-OKVQA and MathVista

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought (CoT) reasoning in VLMs requires explicit training with detailed rationales rather than being implicitly learned from short answers.
- Mechanism: When models are trained only on short answers, they optimize for direct prediction accuracy but do not develop the internal reasoning steps needed for CoT. Training on CoT data with explicit reasoning steps forces the model to generate and evaluate intermediate steps, improving its ability to solve complex reasoning tasks.
- Core assumption: CoT reasoning is a distinct skill that cannot be fully acquired through indirect exposure during direct prediction training.
- Evidence anchors:
  - [abstract]: "training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses"
  - [section 4.3]: "training on direct only prediction may not effectively help with CoT prediction"
  - [corpus]: Weak evidence - related papers discuss CoT prompting but don't directly address implicit vs explicit learning distinction
- Break condition: If CoT performance improves significantly from direct-only training, the assumption that CoT requires explicit training would be invalidated.

### Mechanism 2
- Claim: GPT-4o distillation can generate high-quality CoT rationales from short-answer datasets, creating a scalable training signal for VLMs.
- Mechanism: GPT-4o acts as a reasoning teacher, taking short ground-truth answers and producing detailed rationales that connect visual elements to conclusions. This distilled data provides the explicit reasoning steps VLMs need to learn CoT capabilities without requiring manual annotation.
- Core assumption: GPT-4o can reliably generate correct and useful reasoning paths when given accurate short answers as reference.
- Evidence anchors:
  - [abstract]: "we distill rationales from GPT-4o model to enrich the training data"
  - [section 3.1]: "leverage datasets with short ground truth annotations and employing the GPT-4o model to generate reasoning paths"
  - [corpus]: Moderate evidence - papers like "Vision-Language Models Can Self-Improve Reasoning via Reflection" show GPT-4o can generate reasoning, but effectiveness varies by task complexity
- Break condition: If GPT-4o-generated rationales contain systematic errors or fail to capture task-specific reasoning patterns, the distillation approach would fail.

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) with model-generated positive/negative pairs can further refine CoT reasoning by aligning the model to produce more accurate reasoning chains.
- Mechanism: The SFT model generates multiple reasoning attempts for each question, which are compared against ground truth answers to create preference pairs. DPO then optimizes the model to favor reasoning patterns that lead to correct answers over those that lead to incorrect answers.
- Core assumption: The correctness of final answers can serve as a reliable proxy for the quality of intermediate reasoning steps.
- Evidence anchors:
  - [abstract]: "construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers"
  - [section 3.3]: "optimizing positive (correct) and negative (incorrect) pairs of rationales with Direct Preference Optimization (DPO)"
  - [corpus]: Moderate evidence - "Iterative Reasoning Preference Optimization" shows DPO can improve reasoning, but effectiveness on VLMs is less established
- Break condition: If incorrect reasoning chains sometimes produce correct answers (or vice versa), the preference signal would become unreliable.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Understanding CoT is fundamental to grasping why explicit training data is necessary and how the distillation process works
  - Quick check question: Can you explain the difference between a CoT response and a direct answer in the context of visual question answering?

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: SFT is the primary training method used to incorporate the distilled CoT data into VLMs
  - Quick check question: What is the difference between SFT and pre-training, and why is SFT appropriate for incorporating reasoning data?

- Concept: Preference learning and DPO
  - Why needed here: DPO is the core mechanism for the second-stage refinement of reasoning quality
  - Quick check question: How does DPO differ from standard supervised learning, and what type of training signal does it optimize?

## Architecture Onboarding

- Component map: Short-answer datasets → GPT-4o distillation → SFT training → DPO fine-tuning
- Critical path: Data distillation → SFT → DPO → Evaluation
  - Each stage depends on the previous one's output quality
- Design tradeoffs:
  - GPT-4o distillation vs. manual annotation: Scalability vs. potential quality issues
  - Direct vs. CoT training: Balanced skill development vs. task-specific optimization
  - Response truncation in DPO: Computational efficiency vs. complete reasoning capture
- Failure signatures:
  - SFT stage: Model generates answers without reasoning or produces irrelevant CoT
  - DPO stage: No improvement in reasoning quality despite preference optimization
  - Evaluation: Inconsistent answer extraction from CoT responses
- First 3 experiments:
  1. Verify GPT-4o distillation produces reasonable rationales by manually checking a sample
  2. Train SFT model on distilled data and evaluate CoT vs direct performance on a single dataset
  3. Apply DPO on the SFT model and measure reasoning improvement on both trained and unseen datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of DPO for CoT reasoning depend on the base model's initial reasoning capability?
- Basis in paper: [explicit] The paper states "We hypothesize that for models with relatively weak CoT reasoning capabilities, RFT may be more effective in enhancing model performance, whereas DPO with preference modeling may be less impactful."
- Why unresolved: While the paper shows that DPO improves CoT reasoning, it does not directly compare DPO's effectiveness against RFT on models with varying initial reasoning capabilities. The ablation study comparing DPO and RFT focuses on different datasets rather than model baselines.
- What evidence would resolve it: A controlled experiment comparing DPO and RFT performance on models with varying initial CoT reasoning abilities (e.g., LLaVA-Next vs. LLaVA-Reasoner-SFT) would clarify this relationship.

### Open Question 2
- Question: What is the optimal balance of CoT vs. direct prediction data for maximizing both reasoning and generalization performance?
- Basis in paper: [explicit] The paper shows that combining CoT and direct data yields the best overall performance, but the analysis focuses on specific task combinations rather than exploring the optimal ratio.
- Why unresolved: The experiments use fixed ratios of CoT to direct data (e.g., ChartQA-C+D vs. ChartQA-C vs. ChartQA-D) without systematically varying the proportions to find an optimal balance.
- What evidence would resolve it: A systematic study varying the ratio of CoT to direct prediction data across multiple tasks would identify optimal proportions for different types of reasoning tasks.

### Open Question 3
- Question: How does token-level reward assignment in DPO translate to long-range reasoning dependencies?
- Basis in paper: [explicit] The paper demonstrates that DPO learns token-level rewards and shows sensitivity to errors, but does not analyze how these local rewards affect the overall reasoning chain.
- Why unresolved: While the paper shows that DPO assigns rewards at the token level, it does not investigate whether these local corrections propagate to improve longer reasoning chains or if they remain isolated fixes.
- What evidence would resolve it: Analysis of how token-level corrections in early steps of CoT chains affect the accuracy of final answers across different reasoning depths would clarify this relationship.

## Limitations
- Success heavily depends on GPT-4o's ability to generate accurate and useful reasoning chains
- Assumption that final answer correctness reliably indicates reasoning quality may not always hold
- Computational cost of generating 32 reasoning candidates per example for DPO training is substantial

## Confidence

High confidence:
- The general framework of using distilled CoT data with SFT is well-established
- CoT performance improves with explicit training (14.4% accuracy gain on A-OKVQA)

Medium confidence:
- GPT-4o's effectiveness as a distillation teacher depends on task complexity
- DPO approach for refining reasoning shows promise but has less established precedent for VLMs

Low confidence:
- Scalability to more complex reasoning tasks and robustness to variations in short-answer quality remain untested
- Long-term effectiveness of combining CoT and direct training versus task-specific optimization is unclear

## Next Checks

1. **Manual validation of GPT-4o rationales**: Randomly sample 100 distilled rationales and manually verify their correctness and usefulness for the corresponding visual questions.

2. **Ablation study on preference quality**: Create a synthetic dataset where incorrect reasoning chains are intentionally generated, then test whether DPO successfully discriminates against them.

3. **Cross-dataset generalization test**: Train the SFT model on only one dataset's CoT data, then evaluate on completely unseen datasets.