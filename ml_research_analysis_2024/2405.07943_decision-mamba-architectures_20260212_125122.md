---
ver: rpa2
title: Decision Mamba Architectures
arxiv_id: '2405.07943'
source_url: https://arxiv.org/abs/2405.07943
tags:
- sequence
- mamba
- data
- learning
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Decision Mamba (DM) and Hierarchical Decision
  Mamba (HDM), which replace the Transformer architecture in Decision Transformer
  (DT) and Hierarchical Decision Transformer (HDT) with the Mamba architecture. The
  key innovation is leveraging Mamba's structured state space sequence models, which
  scale linearly with sequence length and use an evolutionary parameter to guide task
  performance without requiring a reward sequence.
---

# Decision Mamba Architectures

## Quick Facts
- **arXiv ID**: 2405.07943
- **Source URL**: https://arxiv.org/abs/2405.07943
- **Reference count**: 24
- **Key outcome**: Decision Mamba and Hierarchical Decision Mamba replace Transformer with Mamba architecture in Decision Transformer and Hierarchical Decision Transformer, eliminating the need for reward sequences while achieving competitive or better performance on D4RL benchmark tasks.

## Executive Summary
This paper introduces Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), which replace the Transformer architecture in Decision Transformer (DT) and Hierarchical Decision Transformer (HDT) with the Mamba architecture. The key innovation is leveraging Mamba's structured state space sequence models, which scale linearly with sequence length and use an evolutionary parameter to guide task performance without requiring a reward sequence. This eliminates the need for user-specified desired rewards and task knowledge. Evaluated on seven tasks from the D4RL benchmark, DM and HDM outperform their Transformer counterparts in most settings. DM achieves higher performance without relying on the reward sequence, while HDM shows competitive results despite requiring two models. Additionally, Mamba-based methods are faster during inference, providing computational benefits over Transformers.

## Method Summary
The authors replace Transformer layers in DT and HDT with Mamba layers, which use selective state space modeling. DM conditions on states and actions without requiring a reward sequence, while HDM uses a two-level architecture where a high-level model predicts sub-goals and a low-level model predicts actions. Both architectures are trained using behavioral cloning on D4RL datasets with 1M epochs, batch size 16, learning rate 1e-4, and validation every 1000 epochs on 100 episodes across 4 random seeds.

## Key Results
- DM outperforms DT on 6 out of 7 D4RL tasks while eliminating the need for reward sequences
- HDM achieves competitive results with HDT despite requiring two separate models
- Mamba-based methods demonstrate faster inference times compared to Transformer-based approaches
- DM's evolutionary parameter successfully guides task performance without explicit reward signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Mamba architecture's evolutionary parameter replaces the need for a reward sequence in Decision Transformer.
- **Mechanism**: The selective state space modeling in Mamba uses an evolutionary parameter to guide the agent through tasks, providing the necessary guidance signal that the reward sequence previously offered in Transformer-based models.
- **Core assumption**: The evolutionary parameter of SSMs can effectively substitute the sequence of rewards in guiding task performance without requiring task-specific desired reward values.
- **Evidence anchors**:
  - [abstract] "identify that the evolutionary parameter of SSMs provides the guidance signal that the sequence of rewards offers DT"
  - [section] "We identify that the evolutionary parameter of SSMs provides the guidance signal that the sequence of rewards offers DT"
  - [corpus] Weak evidence - no direct citations about evolutionary parameters in Mamba for RL tasks
- **Break condition**: If the evolutionary parameter cannot capture the necessary task guidance information, or if tasks require specific reward shaping that the parameter cannot approximate.

### Mechanism 2
- **Claim**: Mamba models achieve linear scalability with sequence length compared to Transformer's quadratic scaling.
- **Mechanism**: The structured state space sequence models use global convolution operations that scale linearly with input length, avoiding the quadratic complexity of self-attention mechanisms.
- **Core assumption**: Linear scalability translates to faster inference and training times without sacrificing model performance.
- **Evidence anchors**:
  - [abstract] "Mamba's structured state space sequence models, which scale linearly with sequence length"
  - [section] "structured state space sequence models (SSMs) [6], have garnered attention for their linear scalability with sequence length"
  - [corpus] Weak evidence - no direct performance comparisons of linear vs quadratic scaling in this paper's results
- **Break condition**: If the linear scalability advantage is offset by other computational bottlenecks, or if sequence length requirements exceed the practical benefits of linear scaling.

### Mechanism 3
- **Claim**: The hierarchical structure in HDM provides better task decomposition than single-level approaches.
- **Mechanism**: HDM splits decision-making into high-level sub-goal selection and low-level action execution, allowing the agent to break complex tasks into manageable sub-tasks.
- **Core assumption**: The high-level model can effectively identify valuable sub-goals that guide the low-level controller toward successful task completion.
- **Evidence anchors**:
  - [abstract] "HDM shows competitive results despite requiring two models"
  - [section] "The HDM splits the decision-making process of the agent into two models: a high-level mechanism which defines sub-goal states for a low-level controller to try and reach"
  - [corpus] Weak evidence - no detailed analysis of sub-goal effectiveness or comparison with other hierarchical approaches
- **Break condition**: If the sub-goal identification process fails to capture meaningful task structure, or if the coordination between high and low levels becomes a bottleneck.

## Foundational Learning

- **Concept: Reinforcement Learning fundamentals (MDP formulation)**
  - Why needed here: The paper builds on RL concepts like states, actions, rewards, and policies to frame imitation learning as sequence modeling
  - Quick check question: What are the three core components of an MDP tuple (S, A, P, R) and their roles in RL?

- **Concept: Sequence modeling architectures (Transformers vs SSMs)**
  - Why needed here: The paper compares Mamba's SSM approach against Transformer architectures, requiring understanding of both models' strengths and limitations
  - Quick check question: What is the computational complexity difference between Transformer self-attention and Mamba's global convolution?

- **Concept: Imitation learning and behavioral cloning**
  - Why needed here: The methods are evaluated in offline RL settings where agents learn from demonstration data rather than environment interaction
  - Quick check question: How does behavioral cloning differ from standard RL in terms of learning signal and data requirements?

## Architecture Onboarding

- **Component map**: Input sequence → Linear embeddings → Mamba layers → Output projection → Action prediction (DM). High-level path (states, sub-goals → Mamba → sub-goal projection) runs in parallel with low-level path (states, actions, sub-goals → Mamba → action projection) (HDM).

- **Critical path**: For DM: Input sequence → Linear embeddings → Mamba layers → Output projection → Action prediction. For HDM: High-level path (states, sub-goals → Mamba → sub-goal projection) runs in parallel with low-level path (states, actions, sub-goals → Mamba → action projection).

- **Design tradeoffs**: Mamba vs Transformer - linear vs quadratic scaling, simpler architecture (no causal masks or positional encoding) vs potentially richer context modeling. HDM vs DM - better task decomposition vs increased model complexity and data preprocessing requirements.

- **Failure signatures**: Poor performance on stochastic environments (if evolutionary parameter doesn't capture uncertainty), degraded results with short context lengths (if Mamba can't maintain sufficient state), failure to learn without reward sequence (if evolutionary parameter is insufficient).

- **First 3 experiments**:
  1. Compare DM performance with and without reward sequence on a simple D4RL task to verify the evolutionary parameter's effectiveness
  2. Vary Mamba layer count and embedding size on HalfCheetah to identify optimal architecture configuration
  3. Test HDM vs DM on a hierarchical task like Antmaze to evaluate sub-goal effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Limited theoretical analysis of when the evolutionary parameter can effectively substitute for reward sequences
- No empirical validation of computational benefits through direct timing comparisons with Transformers
- Lack of analysis on whether HDM's high-level model actually learns meaningful sub-goals versus random state selections

## Confidence
- **High confidence**: DM and HDM architectures are correctly implemented and benchmarked against DT/HDT on D4RL tasks with appropriate metrics
- **Medium confidence**: The claim that Mamba's evolutionary parameter replaces reward sequences is supported by empirical results but lacks theoretical justification
- **Low confidence**: The assertion that linear scalability translates to practical computational benefits is not demonstrated with direct timing comparisons

## Next Checks
1. **Ablation study**: Train DM with and without the evolutionary parameter on multiple tasks to quantify its contribution beyond standard conditioning mechanisms
2. **Computational benchmarking**: Measure wall-clock training and inference times for DM vs DT on identical hardware to validate the linear scaling advantage
3. **Sub-goal analysis**: Visualize and analyze the sub-goals learned by HDM's high-level model to verify they correspond to meaningful task progressions rather than random state selections