---
ver: rpa2
title: Investigating on RLHF methodology
arxiv_id: '2410.01789'
source_url: https://arxiv.org/abs/2410.01789
tags:
- preference
- responses
- dataset
- training
- basic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates RLHF methodology for aligning Large Language
  Models with human preferences. The authors explore Preference Model training, Reinforcement
  Learning with PPO, and Direct Preference Optimization (DPO).
---

# Investigating on RLHF methodology

## Quick Facts
- arXiv ID: 2410.01789
- Source URL: https://arxiv.org/abs/2410.01789
- Reference count: 27
- Primary result: DPO training on perplexity-filtered dataset improved LLM performance (64 DPO wins vs 36 SFT wins) across 8/10 task types

## Executive Summary
This paper investigates Reinforcement Learning from Human Feedback (RLHF) methodology for aligning Large Language Models with human preferences. The authors explore three key components: Preference Model training, Reinforcement Learning with Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). They introduce a novel approach for creating preference datasets using perplexity filtering, which makes the process more cost-effective and easier compared to traditional human annotation methods. The study evaluates these methods on a relatively small dataset of 16,000 samples and demonstrates significant improvements in model performance.

## Method Summary
The paper proposes a comprehensive RLHF pipeline consisting of three main stages. First, Preference Model training involves creating a reward model that learns to predict human preferences between pairs of responses. Second, Reinforcement Learning with PPO is applied to fine-tune the language model using the learned reward model and KL-divergence regularization to prevent excessive deviation from the base model. Third, the authors introduce Direct Preference Optimization (DPO) as an alternative to PPO that directly optimizes the policy without requiring a separate reward model. A key innovation is the use of perplexity filtering to create preference datasets automatically - responses with lower perplexity (more likely under the model) are considered preferred over higher-perplexity responses, eliminating the need for expensive human annotation.

## Key Results
- DPO training on perplexity-filtered dataset achieved 64 wins vs 36 losses against SFT baseline across 10 task types
- PPO training showed steady score improvement but mixed results across different task types
- Perplexity filtering successfully created on-distribution preference datasets without human annotation
- DPO demonstrated superior performance compared to PPO on automated preference metrics

## Why This Works (Mechanism)
The methodology works by creating a feedback loop where the model learns to generate responses that are both likely under its own distribution (low perplexity) and preferred by humans. The perplexity filtering acts as a proxy for human preference, selecting responses that are coherent and well-formed. DPO then directly optimizes the policy to increase the likelihood of preferred responses while maintaining proximity to the original model through KL regularization. This approach bypasses the complexity of training separate reward models and reinforcement learning loops, leading to more stable and efficient fine-tuning.

## Foundational Learning

**Reinforcement Learning from Human Feedback (RLHF)**
Why needed: Aligns language models with human values and preferences
Quick check: Can the model generate responses that humans consistently prefer?

**Preference Modeling**
Why needed: Captures human preference patterns between response pairs
Quick check: Does the model correctly rank human-preferred responses higher?

**Perplexity-based Filtering**
Why needed: Provides automated way to create preference datasets
Quick check: Are lower-perplexity responses actually preferred by humans?

**Direct Preference Optimization (DPO)**
Why needed: Alternative to PPO that avoids reward modeling complexity
Quick check: Does DPO achieve comparable or better performance than PPO?

**Proximal Policy Optimization (PPO)**
Why needed: Standard RL algorithm for safe policy updates
Quick check: Does KL regularization prevent excessive policy deviation?

## Architecture Onboarding

**Component Map**
Base LLM -> Perplexity Filter -> Preference Dataset -> DPO/PPO -> Fine-tuned LLM

**Critical Path**
Base LLM generation -> Perplexity scoring -> Dataset creation -> Model fine-tuning -> Preference evaluation

**Design Tradeoffs**
- Perplexity filtering trades annotation cost for potential bias in preference capture
- DPO trades implementation simplicity for potential suboptimal policy updates
- Small dataset size trades computational efficiency for potential overfitting

**Failure Signatures**
- High perplexity responses dominate preference dataset → model learns to generate unlikely text
- KL penalty too high → model fails to improve beyond base capabilities
- Dataset too small → overfitting and poor generalization

**First Experiments**
1. Generate responses on diverse prompts and compute perplexity scores
2. Create preference pairs using perplexity ranking and verify dataset quality
3. Run DPO fine-tuning with small learning rate and monitor KL divergence

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of human evaluation to validate automated preference dataset creation method
- Limited dataset size (16,000 samples) may not generalize to larger-scale applications
- Comparison between DPO and PPO uses different datasets, making algorithmic effects unclear

## Confidence
- DPO performance improvement: Medium
- Perplexity filtering effectiveness: Medium
- Algorithmic comparison validity: Low

## Next Checks
1. Conduct human preference evaluation studies comparing responses from SFT, PPO, and DPO models
2. Run DPO and PPO using identical datasets to isolate algorithmic effects
3. Test perplexity filtering approach on larger datasets (100K+ samples) and different LLM base models