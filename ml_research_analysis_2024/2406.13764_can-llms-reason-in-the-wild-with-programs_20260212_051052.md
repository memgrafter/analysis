---
ver: rpa2
title: Can LLMs Reason in the Wild with Programs?
arxiv_id: '2406.13764'
source_url: https://arxiv.org/abs/2406.13764
tags:
- tactic
- reasoning
- answer
- problems
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reasoning capabilities of large language
  models (LLMs) when faced with complex, real-world problems that are ambiguous in
  scope and require multiple formalisms to solve. The authors introduce the task of
  "reasoning in the wild," where an LLM must identify subproblems and their corresponding
  formalisms, then write programs to solve each subproblem guided by tactics.
---

# Can LLMs Reason in the Wild with Programs?

## Quick Facts
- arXiv ID: 2406.13764
- Source URL: https://arxiv.org/abs/2406.13764
- Reference count: 40
- Key outcome: Fine-tuning local LLMs on tactic-guided trajectories achieves GPT4-level performance on reasoning problems with ambiguous and mixed scope

## Executive Summary
This paper investigates how large language models handle complex, real-world reasoning problems that require identifying subproblems and applying multiple formalisms. The authors introduce "reasoning in the wild" - a task where LLMs must decompose problems, recognize appropriate tactics, and write programs to solve each subproblem. They create the ReWild dataset containing 6.7K trajectories with tactic-guided solutions to diverse reasoning problems. Experiments reveal that existing LLMs fail significantly on ambiguous and hybrid problems, showing critical limitations and overfitting issues. The authors demonstrate that fine-tuning a local LLM (TIGER-8B) on these trajectories leads to better performance, achieving GPT4-level results.

## Method Summary
The authors create the ReWild dataset containing 6.7K trajectories with 21.7M tokens by generating detailed solutions to reasoning problems using different LLMs as "AI tutors." They implement a routing mechanism that identifies subproblems and selects appropriate tactics (math, logic, graph, general program, routing) to solve each component. The method involves fine-tuning LLaMA3-8B using two approaches: Perfect Trajectory (PJ) and Imperfect Trajectory (IPJ) training. They evaluate performance on standalone problems (without routing) and hybrid problems (with routing) using metrics including accuracy, program quality, and tactic recognition.

## Key Results
- Existing LLMs fail significantly on problems with ambiguous and mixed scope, with accuracy drops of at least 50% on GSM8K
- Fine-tuned TIGER-8B achieves GPT4-level performance on reasoning in the wild tasks
- LLMs show overfitting to well-defined task formats seen during pretraining
- Most models struggle to follow instructions in long contexts (>3K tokens), with Claude-3-Sonnet's accuracy dropping from 40.83% to 12.09%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle on ambiguous and hybrid reasoning problems because they overfit to well-defined task formats seen during pretraining
- Mechanism: When faced with problems requiring multiple formalisms or ambiguous scope, models default to learned shortcuts instead of following the tactic-based reasoning pipeline
- Core assumption: Pretraining data contains mostly single-form reasoning problems without hybrid or ambiguous variants
- Evidence anchors: [abstract] "existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50\%)." [section] "we find LLMs tend to ignore our instructions and ICL examples on only returning the numerical answer and proceed to return answers with explanations."

### Mechanism 2
- Claim: LLMs fail to follow tactics in long contexts because their instruction-following capability degrades with context length
- Mechanism: When routing trajectories exceed ~3K tokens, the model loses track of tactic instructions and defaults to prior patterns
- Core assumption: The model's attention mechanism cannot maintain consistent focus on tactic constraints across long reasoning chains
- Evidence anchors: [section] "we find many LLMs struggle to follow the instructions in a long context... claude-3-sonnet, whose overall Acc decreases from 40.83% to 12.09% on Acc w/ Prog, indicating the model very frequently skips writing programs before answering." [section] "Most LLMs, except for GPT4 series, show a lack of the capability of 'instruction-following in long context', where it fails to follow the tactic on trajectories that are typically 3K long."

### Mechanism 3
- Claim: LLMs generate "trivial programs" for ambiguous problems as a shortcut to avoid reasoning they cannot handle
- Mechanism: When forced to write programs for problems like ReClor that don't fit formalisms, models hardcode answers into comments or constant returns instead of building meaningful models
- Core assumption: The model recognizes it cannot solve the problem formally and takes the path of least resistance
- Evidence anchors: [section] "ReClor problems are typically ambiguous in scope and do not fit into any existing formalisms... they often generate programs that 'hardcoded' the answer and put CoT free-form reasoning in the comments." [section] "We filter these trajectories by first manually labeling a subset of good and bad programs, and use LLMs to filter the rest using them as ICL prompts."

## Foundational Learning

- Concept: Program-aided reasoning
  - Why needed here: The framework relies on generating executable programs to solve subproblems within each tactic
  - Quick check question: What libraries are used for math, logic, and graph reasoning in the tactics?

- Concept: Tactic-based decomposition
  - Why needed here: The routing mechanism breaks problems into subproblems and selects appropriate tactics for each
  - Quick check question: How does the routing tactic decide which formalism to apply to a given subproblem?

- Concept: Trajectory-based learning
  - Why needed here: The dataset consists of thought-action-observation sequences that capture the reasoning process
  - Quick check question: What terminates a sub-trajectory in the reasoning process?

## Architecture Onboarding

- Component map: Problem -> Routing Tactic -> Subproblem Identification -> Tactic Selection (math, logic, graph, general program) -> Program Generation -> Execution -> Result Aggregation -> Final Answer
- Critical path: 1. Problem arrives at routing tactic 2. Routing identifies subproblems and selects tactics 3. Each tactic executes through thought-action-observation loop 4. Results aggregate back to routing 5. Final answer produced when routing terminates
- Design tradeoffs: Tactic specificity vs. flexibility (more specific tactics improve control but reduce generalization), Trajectory length vs. cost (longer trajectories capture more reasoning but increase generation costs), Program complexity vs. execution reliability (more complex programs may be more expressive but harder to execute reliably)
- Failure signatures: High "Wrong Format" rates indicate overfitting to specific output patterns, Low "Acc w/ Prog+" scores suggest trivial program generation, Low "Tac Recog" scores indicate routing mechanism failures, Runtime errors suggest library usage issues
- First 3 experiments: 1. Test each tactic independently on its corresponding dataset to verify basic functionality 2. Evaluate routing on hybrid problems with known ground truth to check decomposition accuracy 3. Compare IPJ vs PJ fine-tuning on a validation set to assess training methodology impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing the model size beyond 8B parameters on the performance of TIGER in solving hybrid problems?
- Basis in paper: [inferred] The paper mentions that TIGER-8B achieves GPT4-level performance and suggests that fine-tuning larger models could further improve results, but does not provide experimental evidence for this
- Why unresolved: The experiments only tested the LLaMA3-8B model, leaving the potential benefits of larger models unexplored
- What evidence would resolve it: Training and evaluating TIGER models with 30B, 70B, and 405B parameters on the hybrid problem benchmark to compare performance improvements

### Open Question 2
- Question: How does the performance of TIGER compare to other state-of-the-art models when fine-tuned on ReWild for reasoning in the wild tasks?
- Basis in paper: [explicit] The paper introduces TIGER as a model fine-tuned on ReWild and shows its performance, but does not compare it directly to other models fine-tuned on the same dataset
- Why unresolved: The paper focuses on the performance of API LLMs and TIGER without considering other models that might be fine-tuned on ReWild
- What evidence would resolve it: Fine-tuning other models like LLaMA3-70B, Mistral-7B, and Phi-3-mini on ReWild and comparing their performance on the reasoning in the wild benchmark

### Open Question 3
- Question: What are the specific mechanisms that cause LLMs to generate trivial programs, and how can they be systematically identified and prevented?
- Basis in paper: [explicit] The paper identifies the issue of trivial programs in ReClor and discusses filtering them, but does not explore the underlying mechanisms causing this behavior
- Why unresolved: The paper acknowledges the problem but does not delve into the root causes or propose solutions to prevent trivial programs systematically
- What evidence would resolve it: Conducting a detailed analysis of the program generation process to identify common patterns in trivial programs and developing techniques to detect and prevent them during the generation phase

### Open Question 4
- Question: How does the blending granularity (easy vs. hard) affect the performance of LLMs on hybrid problems, and what does this reveal about their reasoning capabilities?
- Basis in paper: [explicit] The paper mentions different blending strategies (easy and hard) for hybrid problems but does not provide a detailed analysis of their impact on LLM performance
- Why unresolved: The paper only briefly touches on blending strategies without exploring their effects on model performance or reasoning capabilities
- What evidence would resolve it: Evaluating LLMs on hybrid problems with varying blending granularities and analyzing performance differences to understand how blending affects reasoning capabilities

### Open Question 5
- Question: What are the long-term implications of overfitting on popular problems like GSM8K for the generalization capabilities of LLMs in real-world applications?
- Basis in paper: [explicit] The paper identifies overfitting on GSM8K as a critical limitation of LLMs, affecting their performance on the reasoning in the wild task
- Why unresolved: The paper highlights the issue but does not explore the broader implications of this overfitting on real-world applications or propose strategies to mitigate it
- What evidence would resolve it: Investigating the impact of overfitting on LLMs' performance in diverse real-world scenarios and developing techniques to improve generalization beyond popular benchmarks

## Limitations

- Performance drops significantly on hybrid problems (at least 50% on GSM8K), revealing critical limitations in handling ambiguous and mixed-scope reasoning
- LLMs struggle to follow instructions in long contexts (>3K tokens), with most models showing degraded performance on complex reasoning tasks
- The method requires substantial computational resources for fine-tuning and trajectory generation, limiting accessibility

## Confidence

- **High**: The existence of significant performance gaps between standalone and hybrid problems
- **Medium**: The effectiveness of fine-tuning on tactic-guided trajectories (TIGER-8B achieving GPT4-level results)
- **Low**: The attribution of failures to specific mechanisms (overfitting, context length, trivial programs)

## Next Checks

1. **Pretraining corpus analysis**: Obtain and analyze the pretraining data to verify the absence of hybrid reasoning problems, directly testing the overfitting hypothesis
2. **Context length ablation**: Create controlled experiments varying trajectory length to isolate context effects on instruction-following performance
3. **Program classification study**: Systematically categorize generated programs (meaningful vs. trivial) and correlate with problem difficulty to validate the trivial program generation mechanism