---
ver: rpa2
title: 'Representing the Under-Represented: Cultural and Core Capability Benchmarks
  for Developing Thai Large Language Models'
arxiv_id: '2410.04795'
source_url: https://arxiv.org/abs/2410.04795
tags:
- thai
- language
- llms
- thailand
- thaicli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thai-H6 and ThaiCLI, two benchmarks designed
  to evaluate Thai large language models on core capabilities and cultural understanding.
  Thai-H6 adapts six English benchmarks to Thai, covering reasoning, knowledge, and
  commonsense, while ThaiCLI assesses cultural alignment with Thai societal norms
  through factoid and instruction-based questions.
---

# Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models

## Quick Facts
- **arXiv ID:** 2410.04795
- **Source URL:** https://arxiv.org/abs/2410.04795
- **Reference count:** 40
- **Key outcome:** Thai-H6 and ThaiCLI benchmarks reveal that while Thai LLMs perform well on core capabilities, they struggle with cultural understanding, with open-source models lagging behind closed APIs.

## Executive Summary
This paper introduces Thai-H6 and ThaiCLI, two benchmarks designed to evaluate Thai large language models on core capabilities and cultural understanding. Thai-H6 adapts six English benchmarks to Thai, covering reasoning, knowledge, and commonsense, while ThaiCLI assesses cultural alignment with Thai societal norms through factoid and instruction-based questions. The evaluation involved 12 native Thai speakers and tested open-source models like Llama-3 and Qwen2, alongside closed APIs such as GPT-4o. Results show that while models perform well on Thai-H6, cultural understanding remains a challenge, with open-source models lagging behind closed APIs on ThaiCLI. The authors provide datasets and evaluation code to encourage further research in Thai LLM development.

## Method Summary
The paper introduces two benchmarks: Thai-H6, which adapts six English benchmarks to Thai through machine translation followed by human expert validation, and ThaiCLI, which uses a triplet structure (Question, Chosen, Rejected) to evaluate cultural alignment. Thai-H6 uses log-probability evaluation for most datasets and exact match for th-GSM8K, while ThaiCLI employs an LLM-as-a-Judge approach with GPT-4o. Both benchmarks underwent multiple rounds of human review to ensure linguistic and cultural accuracy. The evaluation tested open-source models like Meta-Llama-3.1-8B-Instruct and Qwen2-72B-Instruct, as well as closed APIs including GPT-4o and Gemini.

## Key Results
- Open-source models (Llama-3, Qwen2) perform well on Thai-H6 core capabilities but significantly underperform on ThaiCLI cultural understanding
- GPT-4o outperforms all other models on both benchmarks, particularly excelling in cultural alignment tasks
- Factoid questions in ThaiCLI are easier for models than instruction-based questions requiring cultural reasoning
- Larger models generally perform better, but model size alone doesn't guarantee cultural competence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation + human expert review improves cultural and linguistic accuracy of benchmark questions.
- Mechanism: Automated translation provides initial coverage; native Thai speakers validate and correct translations to ensure contextual and cultural fidelity.
- Core assumption: Machine translations can capture basic semantics but miss cultural nuances, requiring expert review.
- Evidence anchors:
  - [section] "Afterward, we hired 43 native Thai translators as annotators to review the translated samples and confirm that the translations retained the necessary depth for evaluating LLMs reasoning, knowledge comprehension, and commonsense capabilities in Thai."
  - [section] "Third, these re-translated or adjusted samples undergo additional rounds of review to guarantee text quality."
- Break condition: If cultural validation is insufficient, models may score well on Thai-H6 but fail to understand Thai cultural contexts, as seen in ThaiCLI results.

### Mechanism 2
- Claim: ThaiCLI's {Question, Chosen, Rejected} triplet structure enables nuanced evaluation of cultural alignment.
- Mechanism: Each question is paired with culturally appropriate (Chosen) and inappropriate (Rejected) responses, allowing models to be judged on their ability to distinguish culturally sensitive answers.
- Core assumption: Cultural appropriateness can be reliably encoded as distinct response types that serve as positive and negative examples.
- Evidence anchors:
  - [section] "Each question in the dataset is paired with two distinct types of responses: Chosen and Rejected, forming {Question, Chosen, Rejected} triplets."
  - [section] "When evaluating a model's response to a given question, the chosen and rejected answers serve as positive and negative examples to assess the appropriateness of the model's output."
- Break condition: If cultural norms are ambiguous or context-dependent, the chosen/rejected labels may not capture the full range of acceptable responses.

### Mechanism 3
- Claim: LLM-as-a-Judge approach effectively evaluates cultural alignment when direct scoring is impractical.
- Mechanism: A powerful external LLM (GPT-4o) is prompted to score model responses against chosen/rejected examples, providing scalable and consistent cultural evaluation.
- Core assumption: A strong LLM can reliably judge cultural appropriateness based on few-shot examples provided in the prompt.
- Evidence anchors:
  - [section] "Due to the respective shortcomings of the aforementioned approaches, we propose to utilize an LLM-as-a-Judge approach (Zheng et al., 2023; Dubois et al.), where a powerful LLM is queried to evaluate the quality of a generated model answer."
  - [section] "Specifically, we use the latest stable GPT-4o model, gpt-4o-2024-05-13, from OpenAI as our external LLM judge."
- Break condition: If the external judge's cultural understanding differs from Thai speakers, evaluation scores may not reflect true cultural alignment.

## Foundational Learning

- Concept: Cultural competence in language models
  - Why needed here: ThaiCLI benchmarks cultural understanding, which requires models to recognize and respect cultural norms beyond literal language understanding.
  - Quick check question: Can a model distinguish between culturally appropriate and inappropriate responses to the same question?

- Concept: Benchmark localization methodology
  - Why needed here: Thai-H6 adapts English benchmarks to Thai, requiring careful translation and cultural adaptation to maintain evaluation validity.
  - Quick check question: What steps ensure that translated benchmark questions preserve their original evaluation intent?

- Concept: Multi-round human review processes
  - Why needed here: Both Thai-H6 and ThaiCLI rely on iterative human review to ensure quality and cultural accuracy in benchmark construction.
  - Quick check question: How many review rounds are typically needed to achieve acceptable quality in culturally-sensitive NLP datasets?

## Architecture Onboarding

- Component map: English benchmarks → Machine translation → Human validation → Final Thai datasets → Model inference → LLM-as-a-Judge scoring → Score aggregation → Analysis
- Critical path: Translation → Validation → Evaluation → Analysis
  - Translation must be accurate before validation can proceed
  - Validation must be complete before evaluation can begin
  - Evaluation results drive analysis and insights
- Design tradeoffs:
  - Machine translation provides speed but requires extensive human validation
  - LLM-as-a-Judge provides scalability but may not perfectly align with human cultural judgments
  - Expert annotators ensure quality but increase development time and cost
- Failure signatures:
  - Low ThaiCLI scores despite high Thai-H6 scores indicate cultural understanding gaps
  - Inconsistent LLM-as-a-Judge scores suggest prompt engineering issues
  - High translation error rates indicate need for better translation processes
- First 3 experiments:
  1. Test machine translation accuracy on sample Thai-H6 questions before human validation
  2. Run pilot evaluation with smaller LLM-as-a-Judge to tune prompts and scoring consistency
  3. Compare open-source LLM performance on factoid vs. instruction ThaiCLI categories to identify capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Thai-H6 benchmark compare to existing Thai language benchmarks in terms of evaluating core capabilities like reasoning, knowledge, and commonsense?
- Basis in paper: Explicit
- Why unresolved: The paper introduces Thai-H6 as a new benchmark but does not provide a direct comparison with existing Thai language benchmarks, leaving a gap in understanding its relative effectiveness.
- What evidence would resolve it: A comparative analysis of Thai-H6 against existing Thai benchmarks, highlighting differences in scope, methodology, and results.

### Open Question 2
- Question: What specific challenges arise in translating and adapting English benchmarks like H6 to Thai, and how are these addressed in the Thai-H6 benchmark?
- Basis in paper: Inferred
- Why unresolved: The paper mentions the adaptation process but does not delve into the specific challenges encountered or the solutions implemented to ensure linguistic and cultural accuracy.
- What evidence would resolve it: Detailed case studies or examples of translation challenges and the methods used to overcome them in the Thai-H6 benchmark.

### Open Question 3
- Question: How do the cultural nuances captured in the ThaiCLI benchmark differ from those in other cultural alignment benchmarks, and what makes Thai-specific cultural norms unique?
- Basis in paper: Explicit
- Why unresolved: While the paper outlines the cultural domains covered in ThaiCLI, it does not explore how these differ from or align with cultural norms in other languages or regions.
- What evidence would resolve it: A comparative study of ThaiCLI with cultural benchmarks in other languages, highlighting unique Thai cultural elements and their implications for LLM evaluation.

### Open Question 4
- Question: What is the impact of model size on the performance of Thai LLMs in both the Thai-H6 and ThaiCLI benchmarks, and how does this vary between open-source and closed LLM APIs?
- Basis in paper: Explicit
- Why unresolved: The paper notes differences in performance based on model size but does not provide a detailed analysis of how this impacts both benchmarks across different types of models.
- What evidence would resolve it: A comprehensive analysis of model size effects on Thai-H6 and ThaiCLI scores, comparing open-source and closed APIs to identify patterns and insights.

### Open Question 5
- Question: How can the ThaiCLI benchmark be updated to reflect evolving Thai cultural norms and societal values, and what mechanisms are in place to ensure its continued relevance?
- Basis in paper: Inferred
- Why unresolved: The paper acknowledges the fluid nature of cultural norms but does not specify how the benchmark will be maintained or updated to stay relevant over time.
- What evidence would resolve it: A framework or strategy for periodic updates to the ThaiCLI benchmark, including stakeholder involvement and validation processes.

## Limitations
- Reliance on machine translation followed by human validation may not fully capture all cultural nuances
- LLM-as-a-Judge approach may not perfectly align with Thai cultural experts' judgments
- Limited testing of larger commercial models compared to extensive open-source model evaluation

## Confidence
- **High confidence** in the methodology for Thai-H6 benchmark construction and evaluation
- **Medium confidence** in ThaiCLI's cultural assessment validity, given the triplet structure approach
- **Medium confidence** in the comparative analysis between open-source and closed models, due to limited model diversity

## Next Checks
1. Conduct blind evaluation of ThaiCLI responses by independent Thai cultural experts to validate LLM-as-a-Judge scoring
2. Test additional large commercial models (e.g., GPT-4 Turbo, Claude 3) on both benchmarks to establish comprehensive performance baselines
3. Perform error analysis on low-scoring responses to identify specific cultural understanding gaps and refine the benchmark accordingly