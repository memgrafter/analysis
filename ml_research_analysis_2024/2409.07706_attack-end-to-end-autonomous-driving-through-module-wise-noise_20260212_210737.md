---
ver: rpa2
title: Attack End-to-End Autonomous Driving through Module-Wise Noise
arxiv_id: '2409.07706'
source_url: https://arxiv.org/abs/2409.07706
tags:
- driving
- attack
- autonomous
- noise
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive adversarial attack
  study on modular end-to-end autonomous driving models by introducing module-wise
  noise injection. The authors target vulnerabilities at the interfaces between sub-modules
  in the UniAD model, injecting adversarial noise at five key stages: input images,
  tracking features, map features, motion predictions, and ego-vehicle intentions.'
---

# Attack End-to-End Autonomous Driving through Module-Wise Noise

## Quick Facts
- arXiv ID: 2409.07706
- Source URL: https://arxiv.org/abs/2409.07706
- Authors: Lu Wang; Tianyuan Zhang; Yikai Han; Muyang Fang; Ting Jin; Jiaqi Kang
- Reference count: 21
- Key outcome: This paper presents the first comprehensive adversarial attack study on modular end-to-end autonomous driving models by introducing module-wise noise injection. The authors target vulnerabilities at the interfaces between sub-modules in the UniAD model, injecting adversarial noise at five key stages: input images, tracking features, map features, motion predictions, and ego-vehicle intentions. The attack method optimizes all noise types simultaneously using a combined adversarial loss that maximizes planning deviation while minimizing noise magnitude. Extensive experiments on the nuScenes dataset demonstrate that this approach significantly degrades model performance across all tasks, achieving a 5.37m L2 trajectory error and 4.33% collision rate in planning, outperforming existing image-level attack methods. The results reveal critical security vulnerabilities in current modular autonomous driving architectures.

## Executive Summary
This paper introduces the first comprehensive study of adversarial attacks on modular end-to-end autonomous driving models by injecting noise at module interfaces. The authors target the UniAD architecture, injecting adversarial noise at five key stages: input images, tracking features, map features, motion predictions, and ego-vehicle intentions. By optimizing all noise types simultaneously using a combined adversarial loss, the attack causes significant performance degradation across all tasks, particularly in planning where it achieves a 5.37m L2 trajectory error and 4.33% collision rate. The study reveals critical security vulnerabilities in current modular autonomous driving architectures and demonstrates that attacking intermediate representations is more effective than attacking raw images alone.

## Method Summary
The attack method injects adversarial noise at five key interfaces in the UniAD model's pipeline: input images, tracking features, map features, motion predictions, and ego-vehicle intentions. Each noise vector is optimized simultaneously using a combined adversarial loss that maximizes planning deviation while minimizing noise magnitude. The attack uses a PGD-style optimization approach with 10 iterations, updating all noise vectors based on their respective gradient directions. The combined loss function incorporates tracking, mapping, motion prediction, occupancy, and planning task losses, allowing the attack to target multiple objectives while maintaining low noise levels. This module-wise approach exploits the tight coupling between UniAD's sub-modules, causing error propagation that degrades performance across all tasks.

## Key Results
- Planning performance degrades significantly with 5.37m L2 trajectory error and 4.33% collision rate
- Module-wise attack outperforms image-level attacks by targeting intermediate representations
- All five tasks (tracking, mapping, motion prediction, occupancy, planning) show degraded performance
- Attack effectiveness confirmed on nuScenes validation set with UniAD architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting adversarial noise at module interfaces causes error accumulation across perception, prediction, and planning tasks
- Mechanism: The UniAD model processes information through a pipeline where each module's output serves as input to the next. By injecting noise at these interfaces (tracking features, map features, motion predictions, ego intentions), errors propagate and compound, ultimately degrading planning performance
- Core assumption: The end-to-end autonomous driving model's modules are tightly coupled such that upstream errors significantly impact downstream decisions
- Evidence anchors:
  - [abstract] "injecting adversarial noise at the interfaces between sub-modules in the UniAD model"
  - [section] "attacking complex models should not only focus on the image level but also consider the vulnerability of the interaction process between modules"
  - [corpus] Weak evidence - no direct corpus papers discussing error accumulation in modular end-to-end autonomous driving models
- Break condition: If the model implements robust error correction or isolation between modules, preventing error propagation

### Mechanism 2
- Claim: Joint optimization of all noise types simultaneously creates more effective attacks than single-module approaches
- Mechanism: The attack method uses a combined adversarial loss that maximizes planning deviation while minimizing noise magnitude across all five attack points. This coordinated optimization allows the attack to exploit interdependencies between modules rather than treating each module independently
- Core assumption: Optimizing noise at multiple points in the pipeline creates synergistic effects that exceed the sum of individual attacks
- Evidence anchors:
  - [abstract] "optimizes all noise types simultaneously using a combined adversarial loss"
  - [section] "We synchronize the update of all noise by the adversarial loss"
  - [corpus] Moderate evidence - related work on "Module-wise Adaptive Adversarial Training" suggests module-level optimization is important
- Break condition: If the model's modules operate independently or if noise optimization at one point disrupts optimization at others

### Mechanism 3
- Claim: Attacking intermediate representations (tracking features, map features) is more effective than attacking raw images alone
- Mechanism: By injecting noise into the intermediate feature representations rather than just the input images, the attack bypasses potential image-level defenses and directly corrupts the information that downstream modules rely on for decision-making
- Core assumption: Intermediate representations contain critical information that, when corrupted, cannot be recovered by downstream modules
- Evidence anchors:
  - [abstract] "injecting adversarial noise at five key stages: input images, tracking features, map features, motion predictions, and ego-vehicle intentions"
  - [section] "we introduce adversarial noise at the interfaces between modules"
  - [corpus] Strong evidence - multiple corpus papers discuss attacks on perception and intermediate representations in autonomous driving
- Break condition: If the model implements robust feature extraction or redundancy that can recover corrupted intermediate representations

## Foundational Learning

- Concept: Adversarial attacks in machine learning
  - Why needed here: Understanding how adversarial examples work is fundamental to grasping the attack methodology
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: End-to-end autonomous driving architectures
  - Why needed here: The paper targets a specific modular architecture (UniAD) that combines perception, prediction, and planning
  - Quick check question: What are the main sub-modules in the UniAD architecture and how do they interact?

- Concept: Gradient-based optimization and PGD attacks
  - Why needed here: The attack uses gradient ascent to optimize adversarial noise, similar to Projected Gradient Descent methods
  - Quick check question: How does the noise update rule in equation (6) relate to standard PGD attacks?

## Architecture Onboarding

- Component map: Input images -> Perception -> Tracking -> Map processing -> Motion prediction -> Planning
- Critical path: The attack targets five key interfaces where errors can propagate: input images, tracking features (QA), map features (QM), motion predictions (QT), ego intentions (QE)
- Design tradeoffs: Attacking multiple modules simultaneously increases effectiveness but requires careful balancing of noise magnitudes across different feature spaces
- Failure signatures: Degraded performance metrics in all five tasks, with planning errors showing the most dramatic impact (5.37m L2 trajectory error, 4.33% collision rate)
- First 3 experiments:
  1. Replicate the baseline image-specific and image-agnostic attacks on UniAD to establish comparison points
  2. Implement single-module attacks (only image noise, only tracking noise, etc.) to understand individual module vulnerabilities
  3. Gradually increase the number of simultaneously attacked modules to identify the point of maximum effectiveness

The architecture relies on the UniAD model's modular structure, where each module's output serves as input to the next. The attack exploits this pipeline structure by injecting noise at the interfaces between modules, causing error propagation. The effectiveness depends on the tight coupling between modules - if the model had more robust error isolation or redundancy, the attack would be less effective. The attack is specifically designed for the nuScenes dataset and UniAD implementation, so results may vary with different datasets or model architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do module-wise adversarial attacks transfer between different end-to-end autonomous driving architectures beyond UniAD?
- Basis in paper: [explicit] The authors state their noise injection scheme is "applicable to any modular end-to-end autonomous driving model" but only test on UniAD
- Why unresolved: The paper only evaluates their attack method on UniAD, leaving uncertainty about effectiveness across different architectures with varying module designs and interactions
- What evidence would resolve it: Systematic testing of the module-wise attack approach across multiple end-to-end autonomous driving architectures (MP3, ST-P3, etc.) with comparison of attack transferability and effectiveness

### Open Question 2
- Question: What are the minimal perturbation thresholds that cause catastrophic failures in autonomous driving planning while remaining visually imperceptible?
- Basis in paper: [inferred] The paper uses fixed perturbation parameters (σI = 8 × 10^-6, σA = σM = σT = σE = 2 × 10^-4) without exploring sensitivity or minimal effective perturbations
- Why unresolved: The study uses predetermined noise magnitudes without systematically investigating the relationship between perturbation size and system failure, leaving unclear how small perturbations can be while still causing dangerous planning errors
- What evidence would resolve it: Gradient-based analysis of perturbation sensitivity, experiments with gradually decreasing noise levels to find minimal thresholds that trigger planning failures

### Open Question 3
- Question: How do temporal dynamics affect the effectiveness of module-wise adversarial attacks across multiple frames?
- Basis in paper: [explicit] The attack is described as "iterative" but evaluated only on single frames without considering temporal consistency or attack effectiveness across video sequences
- Why unresolved: The experimental setup evaluates attacks on static frames rather than video sequences, ignoring potential temporal defenses or attack amplification across frames in real-world autonomous driving scenarios
- What evidence would resolve it: Longitudinal experiments testing attack effectiveness across multi-frame sequences, analysis of temporal noise propagation through recurrent modules, evaluation of temporal consistency defenses

## Limitations

- **Architectural Specificity**: The attack methodology is highly tailored to the UniAD architecture, limiting generalizability to other end-to-end autonomous driving models
- **Dataset Dependency**: Results are validated exclusively on the nuScenes dataset, raising questions about performance on different urban environments and driving scenarios
- **Computational Cost**: The attack requires joint optimization of five noise vectors across 10 iterations, making it computationally expensive compared to single-point attacks

## Confidence

- **High Confidence**: The core claim that module-wise noise injection can degrade end-to-end autonomous driving performance is well-supported by experimental results
- **Medium Confidence**: The claim about error accumulation through module interfaces is theoretically sound but lacks extensive empirical validation across different model architectures
- **Low Confidence**: The assertion that this represents the "first comprehensive" study of module-wise attacks is difficult to verify given the rapid evolution of adversarial research

## Next Checks

1. **Architecture Transferability Test**: Apply the module-wise attack methodology to a different end-to-end autonomous driving architecture (e.g., NÜWA or OPV2V) to assess generalizability beyond UniAD

2. **Cross-Dataset Validation**: Evaluate the attack's effectiveness on alternative autonomous driving datasets (e.g., Waymo Open Dataset, Argoverse) to determine dataset dependency and robustness to different urban environments

3. **Real-Time Feasibility Analysis**: Measure the computational overhead of the multi-module attack and assess whether optimization can be performed within real-time constraints for practical adversarial scenarios