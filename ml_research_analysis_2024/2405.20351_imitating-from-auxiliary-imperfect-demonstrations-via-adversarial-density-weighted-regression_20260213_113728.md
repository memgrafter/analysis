---
ver: rpa2
title: Imitating from auxiliary imperfect demonstrations via Adversarial Density Weighted
  Regression
arxiv_id: '2405.20351'
source_url: https://arxiv.org/abs/2405.20351
tags:
- policy
- learning
- arxiv
- preprint
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial Density Regression (ADR), a one-step
  supervised imitation learning framework that addresses limitations in existing IL
  algorithms that rely on Bellman operators and suffer from cumulative offsets and
  out-of-distribution (OOD) issues. ADR uses density-weighted Behavioral Cloning with
  auxiliary imperfect demonstrations to correct the policy distribution learned on
  unknown-quality datasets toward expert distribution, without requiring conservative
  terms or extensive hyperparameter tuning.
---

# Imitating from auxiliary imperfect demonstrations via Adversarial Density Weighted Regression

## Quick Facts
- arXiv ID: 2405.20351
- Source URL: https://arxiv.org/abs/2405.20351
- Reference count: 40
- Primary result: ADR achieves 1008.7 total normalized score on Gym-Mujoco and 89.5% improvement over IQL on Adroit/Kitchen tasks

## Executive Summary
This paper introduces Adversarial Density Regression (ADR), a one-step supervised imitation learning framework that addresses limitations in existing IL algorithms that rely on Bellman operators and suffer from cumulative offsets and out-of-distribution (OOD) issues. ADR uses density-weighted Behavioral Cloning with auxiliary imperfect demonstrations to correct the policy distribution learned on unknown-quality datasets toward expert distribution, without requiring conservative terms or extensive hyperparameter tuning. Theoretically, minimizing ADR's objective is shown to approach optimal policy value, and experimentally, ADR achieves state-of-the-art performance on Gym-Mujoco tasks and demonstrates an 89.5% improvement over IQL on Adroit and Kitchen domains when using ground truth rewards.

## Method Summary
ADR is a two-stage imitation learning framework that first pre-trains VAEs to estimate the density of state-action pairs in expert and sub-optimal datasets, then trains a policy using a density-weighted Behavioral Cloning objective. The density weight log(P^(a|s)/P*(a|s)) acts as a corrective factor that prioritizes state-action pairs where the learned policy distribution deviates from the expert distribution. ADR avoids Bellman operators and multi-step updates, eliminating cumulative bias and OOD problems. The method uses adversarial learning to improve density estimation accuracy and demonstrates robustness to noisy demonstrations while maintaining decision consistency with expert behavior.

## Key Results
- Achieves 1008.7 total normalized score on Gym-Mujoco benchmark tasks
- Demonstrates 89.5% improvement over IQL on Adroit and Kitchen domains with ground truth rewards
- Shows robustness to noisy demonstrations with only slight performance decline as noise ratio increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADR achieves near-optimal policy convergence by directly minimizing the difference between learned and expert policy distributions in a one-step supervised learning framework, avoiding the cumulative bias introduced by multi-step Bellman updates.
- Mechanism: ADR uses a density-weighted Behavioral Cloning objective that aligns the learned policy with expert behavior while diverging from sub-optimal behavior. The density weight log(P^(a|s)/P*(a|s)) acts as a corrective factor that prioritizes state-action pairs where the learned policy distribution deviates from the expert distribution.
- Core assumption: The expert state-action distribution has non-zero coverage over the states present in the unknown-quality dataset (Remark 4.1), ensuring the policy can learn from the available data.
- Evidence anchors:
  - [abstract] "ADR addresses several limitations in previous IL algorithms: First, most IL algorithms are based on the Bellman operator, which inevitably suffer from cumulative offsets from sub-optimal rewards during multi-step update processes."
  - [section 5, Proposition 5.3] "the value bound is proportional to the lower bound of ADR's objective, indicating that minimizing ADR's objective is akin to approaching the optimal value."
  - [corpus] Weak evidence - no directly related papers found on density-weighted BC for IL.
- Break condition: If the expert demonstrations are insufficient to cover the state space of the unknown-quality dataset, or if the density estimation is poor, the corrective mechanism fails.

### Mechanism 2
- Claim: The Adversarial Density Estimation (ADE) component improves the accuracy of expert and sub-optimal behavior density estimation by using adversarial learning to maximize expert density and minimize sub-optimal density.
- Mechanism: ADE trains a VAE to estimate the log density of state-action pairs, then uses adversarial learning (Equation 14) to push the estimated expert density higher and sub-optimal density lower, creating a more discriminative density weight.
- Core assumption: The VAE can effectively model the complex distribution of state-action pairs in the expert and sub-optimal datasets, and the adversarial loss improves this estimation.
- Evidence anchors:
  - [section 6] "During the VAE pre-training stage, we utilize VQ-VAE to separately estimate the target density P*(a|s) and the suboptimal density P^(a|s) by minimizing Equation 11 (or Equation 15) and the VQ loss."
  - [section 6.1] "To alleviate the limitation of demonstrations' scarcity, we utilize adversarial learning (AL) in density estimation."
  - [corpus] Weak evidence - no directly related papers found on adversarial density estimation for IL.
- Break condition: If the VAE fails to capture the underlying structure of the state-action distributions, or if the adversarial learning destabilizes the density estimates.

### Mechanism 3
- Claim: ADR is robust to noisy demonstrations and avoids the out-of-distribution (OOD) problem by operating entirely in-sample and using density weighting to focus learning on relevant state-action pairs.
- Mechanism: ADR uses the density weight to downweight state-action pairs from the unknown-quality dataset that are far from the expert distribution, preventing the policy from being misled by OOD samples. The one-step nature of ADR also avoids the compounding errors from multi-step updates.
- Core assumption: The density weight effectively identifies and downweights OOD state-action pairs, and the one-step update prevents error accumulation.
- Evidence anchors:
  - [section 6.1] "ADR is a one-step supervised IL framework, where all training samples are in-sample, effectively eliminating the challenges of OOD issues."
  - [section 7.2] "ADR can be well adapt to the demonstrations' noisy. As the noise ratio increases, our method shows only a slight decline in performance on ant-m."
  - [section 7.2] "ADR maintains its decision mode as a demonstration while being less susceptible to OOD scenarios."
  - [corpus] Weak evidence - no directly related papers found on OOD robustness in density-weighted IL.
- Break condition: If the density weight fails to accurately identify OOD samples, or if the noise in demonstrations is too high for the policy to learn effectively.

## Foundational Learning

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: ADR's objective is formulated in terms of KL divergence between policy distributions, which measures how one probability distribution diverges from a second, expected probability distribution.
  - Quick check question: If π1 and π2 are two policies, what does DKL[π1||π2] measure, and what does it mean if this value is small?

- Concept: Variance Auto-Encoder (VAE)
  - Why needed here: ADR uses VAEs to estimate the density of state-action pairs in the expert and sub-optimal datasets, which is crucial for computing the density weight.
  - Quick check question: What is the Evidence Lower Bound (ELBO) in a VAE, and how is it used to estimate the log density of data?

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem is formulated within the MDP framework, where the goal is to learn a policy that maximizes the expected return.
  - Quick check question: What are the key components of an MDP, and how does the policy interact with the environment to generate trajectories?

## Architecture Onboarding

- Component map: VAE Pre-training -> Density Weight Computation -> Policy Training
- Critical path: VAE Pre-training → Density Weight Computation → Policy Training
- Design tradeoffs: The choice between minimizing the upper bound (Equation 11) or the objective (Equation 8) involves a tradeoff between computational efficiency and potential overestimation.
- Failure signatures: Poor performance on tasks with high noise in demonstrations, instability in VAE training, or failure to cover the expert state-action space with the unknown-quality dataset.
- First 3 experiments:
  1. Verify VAE pre-training: Train VAEs on expert and sub-optimal data, visualize the learned latent space, and check the estimated densities.
  2. Test density weight computation: Compute the density weight on a held-out set, visualize the weight distribution, and check if it aligns with intuition.
  3. Evaluate policy training: Train the policy using the density-weighted BC objective, monitor the training curve, and compare the learned policy's behavior to the expert demonstrations.

## Open Questions the Paper Calls Out

- Question: How does ADR perform in sequential decision-making tasks compared to RL-based IL methods?
  - Basis in paper: [inferred] The paper notes that extending ADR to sequential models like Decision Transformers showed less impressive results than in MDP settings, suggesting potential limitations in handling sequential dependencies.
  - Why unresolved: The paper does not provide detailed comparisons or theoretical analysis for sequential decision-making tasks, leaving the effectiveness of ADR in such settings unclear.
  - What evidence would resolve it: Empirical results comparing ADR's performance on sequential decision-making tasks (e.g., Atari games) against RL-based IL methods, along with theoretical guarantees for handling sequential dependencies.

- Question: What is the impact of varying the number of expert demonstrations on ADR's performance?
  - Basis in paper: [explicit] The paper mentions that ADR is evaluated with 5 expert trajectories for Gym-Mujoco tasks and 1 trajectory for Kitchen and Adroit tasks, but does not explore the effect of varying the number of demonstrations.
  - Why unresolved: The paper does not provide ablation studies or theoretical analysis on how the number of demonstrations affects ADR's performance, leaving the scalability and robustness of ADR to demonstration scarcity unclear.
  - What evidence would resolve it: Empirical results showing ADR's performance across different numbers of expert demonstrations, along with theoretical bounds on performance degradation as the number of demonstrations decreases.

- Question: How does ADR handle environments with sparse rewards or long-horizon tasks?
  - Basis in paper: [explicit] The paper notes that ADR outperforms IQL by 89.5% on Adroit and Kitchen domains with ground truth rewards, but does not explicitly address its performance in sparse reward or long-horizon scenarios.
  - Why unresolved: The paper does not provide detailed analysis or experiments on ADR's effectiveness in sparse reward or long-horizon tasks, which are common challenges in reinforcement learning.
  - What evidence would resolve it: Empirical results demonstrating ADR's performance on tasks with sparse rewards or long horizons, along with theoretical guarantees on its ability to handle such challenges.

## Limitations

- The method's effectiveness depends critically on accurate density estimation using VAEs, with limited ablation studies on alternative density estimation methods.
- Computational complexity is significantly higher than standard IL methods due to the two-stage training process with separate VAEs and adversarial training.
- The claims about robustness to OOD states and noisy demonstrations are primarily based on a single experiment with limited systematic testing across different types of distribution shift.

## Confidence

**High confidence**: The mathematical formulation of the density-weighted objective (Equation 11) is sound, and the theoretical connection to optimal policy value through Proposition 5.3 is well-established.

**Medium confidence**: The empirical results showing state-of-the-art performance on benchmark tasks are convincing, but the lack of ablations and comparisons to simpler density-based methods limits confidence in the specific contributions of the adversarial components.

**Low confidence**: The claims about ADR's robustness to OOD states and noisy demonstrations are primarily based on a single experiment (ant-m with varying noise ratios). More systematic testing across different types of distribution shift is needed.

## Next Checks

1. **Ablation study on density estimation**: Compare ADR's performance using VAE density estimation versus simpler methods (e.g., kernel density estimation or normalizing flows) to isolate the contribution of the VAE-based approach.

2. **Stress test on OOD scenarios**: Systematically evaluate ADR on environments with known distribution shift (e.g., different friction coefficients, altered object positions) to verify the claimed OOD robustness.

3. **Scalability analysis**: Measure training time and memory usage for ADR compared to baseline IL methods across varying dataset sizes and state space dimensions to quantify the computational overhead.