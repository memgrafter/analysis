---
ver: rpa2
title: 'Abductive explanations of classifiers under constraints: Complexity and properties'
arxiv_id: '2409.12154'
source_url: https://arxiv.org/abs/2409.12154
tags:
- weak
- explanations
- constraints
- axpc
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three new types of abductive explanations
  (AXp) for classifier decisions under constraints, addressing limitations of existing
  definitions that ignore dependencies between features. The authors propose coverage-based
  prime implicant explanations (CPI-Xp), minimal CPI-Xp, and preferred CPI-Xp, which
  leverage coverage and subsumption relations to eliminate redundant or superfluous
  explanations.
---

# Abductive explanations of classifiers under constraints: Complexity and properties

## Quick Facts
- arXiv ID: 2409.12154
- Source URL: https://arxiv.org/abs/2409.12154
- Authors: Martin Cooper; Leila Amgoud
- Reference count: 33
- Key outcome: Introduces three new types of abductive explanations (CPI-Xp, minimal CPI-Xp, preferred CPI-Xp) that reduce redundancy by leveraging coverage and subsumption relations, with preferred CPI-Xp being the only type satisfying all desirable properties

## Executive Summary
This paper addresses a critical limitation in abductive explanations (AXp) for classifiers operating under constraints: existing methods ignore dependencies between features, leading to redundant and superfluous explanations. The authors introduce three novel explanation types based on coverage-based prime implicant (CPI) theory. These methods use coverage sets and subsumption relations to eliminate redundant explanations, even when constraints are present. The work provides a comprehensive analysis of computational complexity, showing that while full-space CPI-Xp generation is ΠP2-complete, sample-based versions remain polynomial-time tractable. Preferred CPI-Xp emerges as the most desirable explanation type, satisfying all key properties while dramatically reducing explanation redundancy.

## Method Summary
The paper proposes three new explanation types for classifier decisions under constraints: coverage-based prime implicant explanations (CPI-Xp), minimal CPI-Xp, and preferred CPI-Xp. The core mechanism uses coverage sets (cov(E)) that represent which instances a partial assignment explains, combined with subsumption relations to filter redundant explanations. For computational tractability, the authors also introduce sample-based versions (d-CPI-Xp, d-mCPI-Xp, d-pCPI-Xp) that operate on representative samples rather than the full feature space. The methods are analyzed for computational complexity, with full-space versions requiring ΠP2-complete complexity while sample-based versions remain in P. The work also establishes which explanation types satisfy desirable properties like success, non-triviality, and independence.

## Key Results
- Coverage-based subsumption reduces explanation redundancy by eliminating explanations that are strictly subsumed by others
- Preferred CPI-Xp is the only explanation type that satisfies all desirable properties (success, non-triviality, independence, etc.)
- Full-space CPI-Xp generation is ΠP2-complete, while sample-based versions are polynomial-time
- Sample-based explanations sacrifice coherence for tractability, as they may violate the property that explanations valid on samples generalize to the full feature space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coverage-based subsumption eliminates redundant explanations by comparing the set of instances each partial assignment explains
- Mechanism: The system generates explanations by examining the coverage set cov(E) for each partial assignment E, then discards any explanation E' that strictly subsumes another explanation E (meaning cov(E) ⊂ cov(E')). This ensures that only explanations that apply to the maximal set of instances are retained
- Core assumption: Dependencies between features can be captured through the coverage relationship between partial assignments
- Evidence anchors:
  - [abstract]: "They are based on a key notion of coverage of an explanation, the set of instances it explains. We show that coverage is powerful enough to discard redundant and superfluous AXp's"
  - [section]: "Deﬁnition 6. Let X ⊆ F and E, E ′ ∈ E. W e say that E′ subsumes E in X if ∀x ∈ X. (E(x) → E′(x)). E′ strictly subsumes E in X if E′ subsumes E in X but E does not subsume E′ in X"
  - [corpus]: Weak or missing - no direct mention of coverage-based subsumption in related papers
- Break condition: If constraints don't create meaningful dependencies between partial assignments, the coverage-based filtering becomes ineffective and may discard valid explanations

### Mechanism 2
- Claim: Sample-based explanations reduce computational complexity from NP-hard to polynomial time
- Mechanism: Instead of testing explanations over the entire feature space F[C], the system tests explanations only on a sample T ⊆ F[C]. This changes the complexity from testing ∀y ∈ F[C] to testing ∀y ∈ T, which is computationally feasible
- Core assumption: The sample T is representative enough that explanations valid on T are also valid on F[C]
- Evidence anchors:
  - [abstract]: "The proposed methods satisfy desirable properties like success, non-triviality, and independence, with preferred CPI-Xp being the only type meeting all criteria"
  - [section]: "The idea is to avoid exhaustive search by examining only a sample of the feature space. T he obtained explanations are approximations that can be obtained with lower complexity"
  - [corpus]: Weak - while related papers discuss sample-based explanations, they don't specifically address the complexity reduction mechanism
- Break condition: If the sample T is too small or unrepresentative, the explanations may not generalize to the full feature space, leading to coherence violations

### Mechanism 3
- Claim: Preferred CPI-Xp provides the only explanation type that satisfies all desirable properties
- Mechanism: By selecting representatives from equivalence classes of explanations (where explanations are equivalent if they cover the same set of instances), the system reduces explanation sets while maintaining key properties
- Core assumption: Equivalence classes can be defined based on coverage sets, and selecting one representative per class preserves the essential explanatory information
- Evidence anchors:
  - [abstract]: "preferred CPI-Xp being the only type meeting all criteria"
  - [section]: "Deﬁnition 10 (Preferred CPI-Xp) . Let x ∈ F[C]. A preferred coverage-based PI-explanation (pCPI-Xp) of κ(x) is a representative of the set of miminal CPI-Xp's of κ(x)"
  - [corpus]: Weak - related papers discuss explanation properties but don't specifically address the preferred explanation selection mechanism
- Break condition: If equivalence classes are too large or contain explanations with different intuitive meanings, the representative selection may lose important explanatory nuances

## Foundational Learning

- Concept: Coverage and subsumption relations
  - Why needed here: These concepts form the mathematical foundation for the new explanation types that handle constraints
  - Quick check question: Given two partial assignments E and E' where cov(E) = {x1, x2} and cov(E') = {x2, x3}, does E strictly subsume E'? (Answer: No, because cov(E') is not a subset of cov(E))

- Concept: Complexity classes (P, NP, co-NP, ΠP2)
  - Why needed here: Understanding these complexity classes is crucial for interpreting the computational cost analysis of different explanation types
  - Quick check question: If finding a CPI-Xp requires n calls to a ΠP2 oracle, what is its overall complexity class? (Answer: FPΣP2)

- Concept: Constraint satisfaction and dependency constraints
  - Why needed here: The paper distinguishes between integrity constraints (preventing impossible assignments) and dependency constraints (creating relationships between features)
  - Quick check question: If we have the constraint "pregnant → female", what type of constraint is this? (Answer: Dependency constraint)

## Architecture Onboarding

- Component map:
  - Constraint parser: Processes input constraints C and validates them against assumptions (C1, C2)
  - Coverage calculator: Computes cov(E) for partial assignments E
  - Subsumption checker: Determines if E' subsumes E in a given set
  - Explanation generator: Main component that produces explanations using different algorithms (full space vs sample-based)
  - Property verifier: Checks if generated explanations satisfy desired properties
  - Complexity estimator: Analyzes computational cost of different explanation types

- Critical path:
  1. Parse constraints and validate assumptions
  2. For each decision κ(x) to explain:
     - Generate candidate explanations (either from full space or sample)
     - Apply coverage-based filtering to eliminate redundancies
     - Select preferred representatives if needed
     - Verify properties of final explanations
  3. Return explanations with complexity guarantees

- Design tradeoffs:
  - Full space vs sample-based: Full space gives coherent explanations but is computationally expensive; sample-based is tractable but may violate coherence
  - Coverage-based vs traditional: Coverage-based eliminates redundancies but increases complexity; traditional is simpler but produces more redundant explanations
  - Preferred vs all explanations: Preferred reduces output size and satisfies more properties but may lose some explanatory nuances

- Failure signatures:
  - Exponential explosion in explanation count: Indicates constraints aren't being properly leveraged for redundancy elimination
  - Coherence violations in sample-based explanations: Indicates sample is too small or unrepresentative
  - Missing valid explanations: Indicates coverage-based filtering is too aggressive or constraints are incorrectly specified

- First 3 experiments:
  1. Implement and test the coverage calculator and subsumption checker on a small feature space with known constraints
  2. Compare explanation counts and properties between traditional AXpc and CPI-Xp on the same decision
  3. Benchmark computational time for full space vs sample-based explanations on a medium-sized problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop sample-based explanation methods that maintain the coherence property while still being computationally tractable?
- Basis in paper: [explicit] The paper identifies that sample-based explanations violate coherence due to incomplete information about the feature space, and suggests this is a key trade-off.
- Why unresolved: The authors acknowledge this as a limitation but do not provide a solution or framework for addressing it.
- What evidence would resolve it: A proposed algorithm or method that demonstrates sample-based explanations can maintain coherence while preserving polynomial-time complexity.

### Open Question 2
- Question: What is the computational complexity of enumerating all preferred CPI-Xp explanations, and can this be done efficiently in practice?
- Basis in paper: [explicit] The authors identify this as a "challenging open problem" but do not analyze its complexity or propose any algorithms.
- Why unresolved: The paper only analyzes the complexity of finding one preferred CPI-Xp, not enumerating all of them.
- What evidence would resolve it: A complexity analysis showing whether this enumeration problem is in a tractable complexity class (P, FP, etc.) and/or experimental results demonstrating practical performance.

### Open Question 3
- Question: How can constraints be automatically learned from datasets to improve explanation quality?
- Basis in paper: [explicit] The authors mention this as a future research direction, noting that learning small-arity constraints could make explanations more feasible.
- Why unresolved: The paper does not explore constraint learning algorithms or evaluate their impact on explanation quality.
- What evidence would resolve it: A constraint learning method that can extract meaningful dependencies from data, along with experiments showing improved explanation properties (reduced redundancy, better coverage, etc.) compared to using no constraints.

## Limitations
- Coverage-based subsumption effectiveness depends on constraint quality - if constraints don't create meaningful dependencies, filtering becomes ineffective
- Sample-based explanations sacrifice coherence for tractability, potentially producing explanations that don't generalize to the full feature space
- The preferred CPI-Xp selection process may lose explanatory nuances when equivalence classes contain explanations with different intuitive meanings

## Confidence
- **High**: The computational complexity analysis (ΠP2-complete for CPI-Xp generation) is well-grounded with clear reductions from established problems
- **Medium**: The property satisfaction proofs for different explanation types are correct but rely on specific assumptions about the feature space and constraints
- **Medium**: The experimental validation showing redundancy reduction is convincing but limited to specific datasets and classifier types

## Next Checks
1. Test the coverage-based filtering mechanism on datasets with intentionally weak constraints to measure the degradation in redundancy elimination performance
2. Systematically vary sample sizes in the sample-based explanations to empirically determine the threshold where coherence violations begin occurring
3. Evaluate the preferred CPI-Xp selection process on cases with large equivalence classes to verify that representative selection preserves essential explanatory content