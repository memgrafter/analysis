---
ver: rpa2
title: Can LLMs Generate Visualizations with Dataless Prompts?
arxiv_id: '2406.17805'
source_url: https://arxiv.org/abs/2406.17805
tags:
- data
- visualization
- queries
- chart
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the ability of GPT-3 and GPT-4 to generate
  data visualizations from "dataless prompts" - queries without accompanying data
  files. The authors test GPT-4 with 15 prompts based on popular data visuals from
  the web and crowd-sourced queries.
---

# Can LLMs Generate Visualizations with Dataless Prompts?

## Quick Facts
- arXiv ID: 2406.17805
- Source URL: https://arxiv.org/abs/2406.17805
- Reference count: 16
- Primary result: GPT-4 successfully generated appropriate chart types from dataless prompts but showed variable accuracy in reproducing specific data values

## Executive Summary
This paper investigates whether large language models, specifically GPT-3 and GPT-4, can generate accurate data visualizations from "dataless prompts" - queries without accompanying data files. The authors tested GPT-4 with 15 prompts based on popular data visuals from the web and crowd-sourced queries, evaluating the results by comparing to expert-created visualization cheat sheets. While GPT-4 consistently selected appropriate chart types matching expert recommendations, the generated visualizations showed discrepancies in specific data values, with line charts capturing general trends but differing in exact values, and bar charts displaying inconsistent bar heights. The study demonstrates GPT-4's strong visualization knowledge and design principles but highlights the need for improvement in precise data reproduction.

## Method Summary
The researchers created 15 dataless prompts based on popular web visualizations and crowd-sourced queries, then used GPT-4 to generate visualizations for each prompt. They evaluated the outputs by comparing the generated chart types against established visualization cheat sheets (like Lundblad's) for appropriateness, and compared data values to original web visualizations. The study examined both chart type selection accuracy and data value precision across different visualization types including line charts and bar charts.

## Key Results
- GPT-4 consistently selected appropriate chart types matching expert recommendations for all 15 prompts
- Generated line charts captured similar trends to original data but differed in specific values
- Bar charts showed inconsistent bar heights that didn't always match original data values
- GPT-4 demonstrated strong knowledge of visualization design principles and grammar

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can generate appropriate visualizations from dataless prompts because it has learned patterns from web-based data and visualization design principles.
- Mechanism: The LLM is trained on vast amounts of web data, including both raw data and visualization examples, allowing it to infer both the data structure and appropriate chart types for queries without explicit data input.
- Core assumption: The web data available during training includes sufficient examples of public datasets and their visualizations for the model to learn these patterns.
- Evidence anchors:
  - [abstract] "we investigate the ability of large language models to provide accurate data and relevant visualizations in response to such queries"
  - [section] "We define dataless prompts as text-based queries provided to LLMs that can be answered with publicly available data but do not contain the data themselves"
  - [corpus] Weak - neighboring papers focus on code generation rather than direct visualization from prompts
- Break condition: If the prompt refers to very recent data (post-training cutoff) or highly specialized datasets not well-represented on the web.

### Mechanism 2
- Claim: GPT-4 consistently selects appropriate chart types because it has learned visualization design rules from examples.
- Mechanism: Through exposure to many visualization examples and accompanying descriptions, the model learns the "visualization grammar" - when to use bar charts, line charts, etc. based on the nature of the data and the query's intent.
- Core assumption: The training corpus included enough labeled or structured visualization examples for the model to learn these design patterns.
- Evidence anchors:
  - [abstract] "all returned charts matched what is commonly recommended for the query's task and scenario"
  - [section] "We found that every chart generated was appropriate! This can be gleaned from figure 1 which shows the placement of each GPT-4 generated chart into the appropriate slot in Lundblad's cheat sheet"
  - [corpus] Moderate - neighboring papers discuss evaluation of visualization generation but focus more on code output
- Break condition: When the prompt contains ambiguous requirements or when multiple visualization types could be equally appropriate.

### Mechanism 3
- Claim: GPT-4 can produce similar trends in line charts even when exact values differ because it captures the underlying patterns in data.
- Mechanism: The model learns statistical relationships and temporal patterns from web data, allowing it to reproduce general trends even if specific values aren't exact matches to source data.
- Core assumption: The model's training included sufficient time-series data examples for it to learn pattern recognition beyond memorization.
- Evidence anchors:
  - [section] "When generating line charts GPT-4 could procudce charts with the same general trend, however the values were not identical and the trends were only correct at a larger scale"
  - [section] "GPT-4 introduced a dip in the U.S. debt between the years 2013 to 2016 however the original data showed that the debt was monotonically increasing"
  - [corpus] Missing - no direct evidence in neighboring papers about trend vs. value accuracy
- Break condition: When precise numerical accuracy is required for the visualization to be useful.

## Foundational Learning

- Concept: Natural Language Processing and parsing
  - Why needed here: Understanding how the model interprets natural language prompts and extracts semantic meaning about what data is being requested
  - Quick check question: How does the model distinguish between "U.S. debt over the last 20 years" and "U.S. GDP over the last 20 years"?

- Concept: Data visualization principles and chart selection
  - Why needed here: Understanding why certain chart types are more appropriate than others for different data scenarios
  - Quick check question: What characteristics of data make a line chart more appropriate than a bar chart?

- Concept: Pattern recognition vs. memorization in LLMs
  - Why needed here: Distinguishing between the model's ability to generate plausible trends versus exact data values
  - Quick check question: How can you tell if a visualization shows learned patterns versus exact memorized data?

## Architecture Onboarding

- Component map:
  - Prompt parser → Data inference engine → Chart type selector → Value generator → Visualization renderer
  - Key dependencies: Web knowledge base, visualization design rules, data pattern recognition

- Critical path:
  1. Prompt interpretation and intent extraction
  2. Data type and structure inference
  3. Appropriate chart type selection
  4. Data value generation (trend vs. exact)
  5. Visualization formatting and output

- Design tradeoffs:
  - Precision vs. plausibility: Exact data values vs. realistic-looking trends
  - Generalizability vs. specificity: Broad pattern recognition vs. exact replication
  - Interpretability vs. automation: Understanding model decisions vs. seamless generation

- Failure signatures:
  - Incorrect chart type selection for data characteristics
  - Generated values that contradict known data trends
  - Missing data points or incorrect time scales
  - Charts that follow design rules but misrepresent the underlying data

- First 3 experiments:
  1. Test with prompts about well-known public datasets (e.g., U.S. population, stock market indices) to verify pattern recognition
  2. Compare generated line chart trends against ground truth data to measure trend accuracy
  3. Test prompts with ambiguous chart requirements to evaluate chart type selection consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4 generate accurate visualizations for queries involving complex data relationships beyond simple trends and rankings?
- Basis in paper: [explicit] The paper mentions that GPT-4's line charts showed similar trends but different values, and bar charts had inconsistent bar heights, indicating limitations in accurately reproducing specific data values.
- Why unresolved: The paper does not provide examples or analysis of GPT-4's performance with more complex data relationships, such as correlations, distributions, or multi-dimensional data.
- What evidence would resolve it: Testing GPT-4 with prompts involving complex data relationships and comparing the generated visualizations to the actual data would provide evidence of its capabilities in this area.

### Open Question 2
- Question: How does GPT-4's performance in generating visualizations compare to other state-of-the-art visualization tools when given the same dataless prompts?
- Basis in paper: [explicit] The paper compares GPT-4's performance to visualization cheat sheets created by experts but does not compare it to other visualization tools or systems.
- Why unresolved: The paper focuses solely on GPT-4's performance without benchmarking it against other visualization tools, making it difficult to assess its relative strengths and weaknesses.
- What evidence would resolve it: Conducting a comparative study where GPT-4 and other visualization tools are given the same set of dataless prompts and their outputs are evaluated and compared would provide evidence of GPT-4's performance relative to other tools.

### Open Question 3
- Question: Can GPT-4 learn and incorporate user feedback to improve the accuracy and relevance of its generated visualizations over time?
- Basis in paper: [inferred] The paper mentions that GPT-4 has learned visualization knowledge from its training data, suggesting that it has the potential to learn and adapt. However, it does not discuss its ability to incorporate user feedback for continuous improvement.
- Why unresolved: The paper does not explore the possibility of GPT-4 learning from user feedback or its ability to improve its visualization generation capabilities over time.
- What evidence would resolve it: Conducting a study where users provide feedback on GPT-4's generated visualizations, and then assessing whether GPT-4's future outputs show improvements based on that feedback, would provide evidence of its ability to learn and adapt.

## Limitations

- GPT-4 shows variable accuracy in reproducing specific data values, with line charts differing in exact values and bar charts displaying inconsistent bar heights
- The model relies more on pattern recognition and general knowledge than precise data retrieval, leading to discrepancies between generated and original data
- GPT-4's performance with complex data relationships beyond simple trends and rankings remains unexplored

## Confidence

- High confidence: GPT-4's ability to select appropriate chart types based on query intent and data characteristics
- Medium confidence: GPT-4's capability to generate charts showing similar trends to original data, particularly for line charts
- Low confidence: GPT-4's accuracy in reproducing exact data values, especially for bar charts and specific numerical comparisons

## Next Checks

1. **Trend accuracy measurement**: Systematically compare generated line chart trends against ground truth data across multiple prompts to quantify the degree of value deviation and determine if trends remain statistically similar.

2. **Bar chart precision analysis**: Test GPT-4 with prompts requiring precise bar height comparisons to evaluate whether the model can consistently maintain proportional relationships between data values.

3. **Temporal data consistency**: Evaluate GPT-4's handling of time-series data by testing prompts about well-documented historical datasets to determine if the model can accurately reproduce both trends and specific time points.