---
ver: rpa2
title: Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware
  Street Crossing
arxiv_id: '2402.06794'
source_url: https://arxiv.org/abs/2402.06794
tags:
- safety
- score
- visual
- crossing
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling blind and low-vision
  individuals to safely cross streets by developing an interpretable risk assessment
  system using GPT-4V. The authors collected multiview egocentric images from a quadruped
  robot in crosswalk settings and annotated them with safety scores based on predefined
  criteria.
---

# Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing

## Quick Facts
- arXiv ID: 2402.06794
- Source URL: https://arxiv.org/abs/2402.06794
- Authors: Hochul Hwang; Sunjae Kwon; Yekyung Kim; Donghyun Kim
- Reference count: 40
- Primary result: Optical flow visual feature achieved highest accuracy (45.45%) and correlation (0.4348) for safety score prediction

## Executive Summary
This paper addresses the challenge of enabling blind and low-vision individuals to safely cross streets by developing an interpretable risk assessment system using GPT-4V. The authors collected multiview egocentric images from a quadruped robot in crosswalk settings and annotated them with safety scores based on predefined criteria. They then extracted visual knowledge (object detection bounding boxes, segmentation masks, optical flow) and used these along with text prompts to evaluate GPT-4V's performance in predicting safety scores and providing scene descriptions. Results show that optical flow was the most effective visual feature, achieving the highest accuracy (45.45%) and correlation (0.4348) for safety score prediction. Segmentation masks also improved accuracy but reduced correlation. The authors conclude that large multimodal models show promise for safety-aware scene understanding, but prompt engineering and data diversity need further refinement.

## Method Summary
The authors developed a system that uses GPT-4V to assess street crossing safety from multiview egocentric images. They collected crosswalk intersection images using a quadruped robot, annotated them with safety scores ranging from -2 to 2, and extracted visual knowledge including object detection bounding boxes from YOLOv8, segmentation masks from YOLOv8-Seg, and optical flow from Lucas-Kanade. These visual features were combined with text prompts in a zero-shot evaluation scheme using GPT-4V to predict safety scores and generate scene descriptions. The system was evaluated using accuracy and Spearman's Ï correlation metrics across different visual feature combinations and prompt engineering approaches.

## Key Results
- Optical flow achieved the highest accuracy (45.45%) and correlation (0.4348) for safety score prediction
- Segmentation masks improved accuracy but reduced correlation compared to optical flow
- Chain-of-Thought prompting degraded performance rather than enhancing it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can generate reliable safety scores for street crossing when provided with multiview egocentric images and structured visual knowledge.
- Mechanism: The model leverages its cross-modal reasoning capability to integrate textual descriptions of traffic conditions (e.g., vehicle presence, traffic light status, pedestrian signals) with visual information (bounding boxes, segmentation masks, optical flow) to assess risk levels.
- Core assumption: The predefined safety score categories are clear, comprehensive, and align with the model's understanding of real-world traffic scenarios.
- Evidence anchors:
  - [abstract] "By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals."
  - [section] "Our approach marks a pioneering effort in autonomously calculating safety scores by integrating vision and language in a zero-shot scheme."
  - [corpus] Weak evidence - corpus focuses on safety in multimodal models generally but lacks direct connection to street crossing scenarios.
- Break condition: Safety score categories become ambiguous or incomplete, or the model fails to correctly interpret visual cues (e.g., optical flow for vehicle motion).

### Mechanism 2
- Claim: Optical flow is the most effective visual feature for improving safety score prediction accuracy.
- Mechanism: Optical flow provides temporal information about moving objects, allowing the model to infer vehicle direction and speed, which are critical factors in assessing crossing safety.
- Core assumption: The average optical flow vector accurately represents the overall movement direction of vehicles in the scene.
- Evidence anchors:
  - [section] "The optical flow information stands out in the experimental results. It not only achieves the highest accuracy among all tested features but also maintains a relatively high correlation degree."
  - [section] "This result supports our assumption that temporal information in the scene, especially vehicle direction information, is an important factor in assessing risk."
  - [corpus] Weak evidence - corpus discusses safety in multimodal models but does not specifically address optical flow for traffic scenarios.
- Break condition: Optical flow becomes unreliable in low-light conditions or when vehicle movement is erratic.

### Mechanism 3
- Claim: Chain-of-Thought prompting degrades performance in this task.
- Mechanism: CoT prompts introduce verbosity and excessive sensitivity to keywords, leading the model to focus on irrelevant details or misinterpret the scene.
- Core assumption: The CoT instructions are not optimally configured for safety assessment tasks and may not translate well from NLP to multimodal reasoning.
- Evidence anchors:
  - [section] "In contrast to the findings of Liu et al. [29], the incorporation of CoT into the prompt did not enhance performance in our task; rather, it resulted in degradation."
  - [section] "Our analysis determined that the model becomes more responsive to risk factors upon receiving a segmentation mask as input, leading to a higher classification of scenes into unsafe categories."
  - [corpus] Weak evidence - corpus does not provide direct insights into CoT effectiveness for safety assessment.
- Break condition: CoT instructions are refined and better aligned with the task requirements.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: MLLMs like GPT-4V are essential for integrating textual and visual information to assess street crossing safety.
  - Quick check question: What are the key components of an MLLM, and how do they contribute to cross-modal reasoning?

- Concept: Object Detection and Segmentation
  - Why needed here: These techniques provide structured visual information (bounding boxes, segmentation masks) that enhance the model's understanding of the scene.
  - Quick check question: How do bounding boxes and segmentation masks differ, and what type of information does each provide?

- Concept: Optical Flow Estimation
  - Why needed here: Optical flow captures temporal information about moving objects, which is crucial for assessing vehicle motion and crossing safety.
  - Quick check question: What is the principle behind the Lucas-Kanade method for optical flow estimation, and how does it differ from other methods?

## Architecture Onboarding

- Component map: Data Collection -> Visual Knowledge Extraction -> Prompt Engineering -> GPT-4V Model -> Evaluation
- Critical path:
  1. Collect multiview egocentric images in crosswalk settings
  2. Annotate images with safety scores based on predefined criteria
  3. Extract visual knowledge (bounding boxes, segmentation masks, optical flow)
  4. Generate prompts combining visual knowledge and textual instructions
  5. Input prompts to GPT-4V and obtain safety scores and scene descriptions
  6. Evaluate model performance using accuracy and correlation metrics
- Design tradeoffs:
  - Using CoT prompts vs. simple instructions: CoT may introduce verbosity and keyword sensitivity, potentially degrading performance.
  - Different visual features (bounding boxes, segmentation masks, optical flow): Each has varying impacts on accuracy and correlation, with optical flow showing the most promise.
- Failure signatures:
  - Low accuracy or correlation: Indicates the model struggles to interpret visual cues or align them with safety score categories.
  - Overemphasis on certain risk factors: Suggests the model is overly sensitive to specific features (e.g., segmentation masks leading to higher unsafe classifications).
  - Inconsistent scene descriptions: Implies the model's reasoning is unreliable or misaligned with the actual scene.
- First 3 experiments:
  1. Baseline evaluation: Assess GPT-4V's performance using only text prompts without any visual knowledge.
  2. Visual feature comparison: Evaluate the impact of different visual features (bounding boxes, segmentation masks, optical flow) on model performance.
  3. Prompt engineering: Test the effectiveness of various prompt structures (e.g., with and without CoT) in eliciting accurate safety assessments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of different visual knowledge types (bounding boxes, segmentation masks, optical flow) impact the model's performance in safety score prediction for street crossing scenarios?
- Basis in paper: [explicit] The paper evaluates GPT-4V's performance with various visual features, finding that optical flow achieves the highest accuracy (45.45%) and correlation (0.4348) for safety score prediction, while segmentation masks improve accuracy but reduce correlation.
- Why unresolved: The study shows that different visual features have varying impacts on accuracy and correlation, but does not fully explore the underlying reasons for these differences or how they might be optimized for better performance.
- What evidence would resolve it: Detailed analysis of how each visual feature type influences the model's reasoning process, and experiments testing combinations or modifications of these features to optimize safety score prediction.

### Open Question 2
- Question: Why did the Chain-of-Thought (CoT) approach not enhance performance in this task, contrary to findings in other domains?
- Basis in paper: [explicit] The paper notes that incorporating CoT into the prompt did not enhance performance and may have even degraded it, unlike in other studies where CoT improved reasoning capabilities.
- Why unresolved: The paper suggests potential reasons such as verbose reasoning and suboptimal configuration, but does not provide a definitive explanation or explore alternative CoT methodologies.
- What evidence would resolve it: Comparative studies testing different CoT configurations, analysis of CoT effectiveness in multimodal tasks, and exploration of task-specific adaptations to improve reasoning performance.

### Open Question 3
- Question: How can data diversity and user-specific preferences be integrated into the model to improve its reliability as a mobility aid for blind and low-vision individuals?
- Basis in paper: [inferred] The paper acknowledges the need for greater diversity in data collection and mentions future efforts to focus on user-specific preferences, indicating an unresolved challenge in developing a reliable mobility aid.
- Why unresolved: The current dataset is limited in diversity and does not account for individual user preferences, which are crucial for developing a personalized and reliable mobility aid.
- What evidence would resolve it: Collection of a more diverse dataset representing various environments and user scenarios, user studies to understand individual preferences, and model adaptations to incorporate these preferences effectively.

## Limitations
- Accuracy remains relatively low at 45.45% even with the best visual feature (optical flow), suggesting the system needs significant refinement before real-world deployment
- Dataset collection limited to one geographic location using a quadruped robot, potentially limiting generalizability across different urban environments and traffic patterns
- Evaluation metrics focus on correlation and accuracy without assessing practical reliability for actual blind and low-vision users in dynamic, real-world conditions

## Confidence

- **Medium confidence** in the overall approach: The methodology is sound, but the relatively low accuracy scores indicate the system is not yet robust enough for safety-critical applications.
- **High confidence** in the effectiveness of optical flow: The experimental results consistently show optical flow outperforms other visual features, with strong correlation (0.4348) between predicted and true scores.
- **Medium confidence** in the conclusion about Chain-of-Thought prompting: While the study found CoT degraded performance, this finding is based on a single task and may not generalize to other multimodal reasoning applications.

## Next Checks

1. **Real-world deployment testing**: Conduct field trials with blind and low-vision participants in diverse urban environments to assess practical usability and safety beyond controlled laboratory conditions.

2. **Cross-city dataset collection**: Expand the dataset to include multiple cities with different traffic patterns, signage systems, and crosswalk designs to evaluate generalizability.

3. **Error analysis on failure cases**: Systematically analyze instances where the model predicted incorrect safety scores to identify specific visual patterns or scenarios that consistently lead to errors, informing targeted model improvements.