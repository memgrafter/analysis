---
ver: rpa2
title: Enhanced Facet Generation with LLM Editing
arxiv_id: '2403.16345'
source_url: https://arxiv.org/abs/2403.16345
tags:
- facets
- query
- search
- facet
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating query facets
  without relying on search engine results pages (SERP) due to their volatility and
  limited access to private documents. The proposed method employs two strategies:
  multi-task learning to predict SERP as a target during training, and editing predicted
  facets from a small model using a large language model (LLM).'
---

# Enhanced Facet Generation with LLM Editing

## Quick Facts
- arXiv ID: 2403.16345
- Source URL: https://arxiv.org/abs/2403.16345
- Authors: Joosung Lee; Jinhong Kim
- Reference count: 0
- Primary result: LLM editing combined with multi-task learning achieves performance comparable to state-of-the-art models that leverage SERP, without requiring SERP at test time

## Executive Summary
This paper addresses the challenge of generating query facets without relying on search engine results pages (SERP) due to their volatility and limited access to private documents. The proposed method employs two strategies: multi-task learning to predict SERP as a target during training, and editing predicted facets from a small model using a large language model (LLM). Experiments on the MIMICS dataset show that the proposed model, especially when combined with LLM editing, achieves performance comparable to state-of-the-art models that leverage SERP. LLM-based evaluation confirms the superiority of the proposed approach.

## Method Summary
The proposed method consists of two main strategies. First, multi-task learning is used to predict SERP elements (documents and related queries) as targets during training, helping the small model (BART-base) better understand queries. Second, the predicted facets from the small model are enhanced using LLM editing, where a large language model refines the initial predictions based on in-context examples. This approach enables facet generation without requiring SERP at test time, making it suitable for private/internal search contexts.

## Key Results
- The proposed model (FD+M+E) achieves performance comparable to state-of-the-art models that leverage SERP
- Multi-task learning alone (FD+M) improves performance over baseline in all metrics
- LLM editing significantly enhances facet quality when combined with multi-task learning
- LLM-based evaluation confirms the superiority of the proposed approach over baselines

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning improves query understanding by training the model to generate documents and related queries as targets, not just facets. By prepending special tokens ([document], [related]) to the input query, the model learns to reconstruct SERP elements during training. This forces deeper semantic understanding of the query beyond facet generation. The distribution of documents and related queries in SERP captures complementary semantic information about the query that improves facet generation.

### Mechanism 2
LLM editing improves facet quality by combining the distribution knowledge from a fine-tuned small model with the generative capabilities of a large language model. The small model generates initial facets that encode the dataset's facet distribution. The LLM then edits these facets using in-context examples showing the mapping from predicted to ground-truth facets. This provides the LLM with distributional priors it would otherwise lack.

### Mechanism 3
Removing dependency on SERP during testing enables deployment in private/internal search contexts where SERP access is unavailable or unreliable. By training the model to predict facets from queries alone (using multi-task learning and LLM editing), the system can generate facets without requiring SERP documents or related queries at inference time.

## Foundational Learning

- Concept: Multi-task learning in NLP
  - Why needed here: The paper uses multi-task learning to leverage SERP information during training while removing SERP dependency at test time.
  - Quick check question: What is the key difference between multi-task learning and transfer learning in the context of this paper?

- Concept: In-context learning and few-shot prompting
  - Why needed here: LLM editing relies on providing the LLM with examples of predicted => ground-truth facet mappings to guide the editing process.
  - Quick check question: How does the number of examples provided to the LLM affect the quality of facet editing?

- Concept: Semantic similarity metrics (BERTScore, BLEU)
  - Why needed here: The paper evaluates facet quality using both semantic similarity (BERTScore) and n-gram overlap (BLEU) metrics.
  - Quick check question: Why might BERTScore be more appropriate than BLEU for evaluating facet generation quality?

## Architecture Onboarding

- Component map: Query → Small model (BART-base) → Facet predictions → LLM editing → Final facets
- Critical path: Query encoding → Multi-task prediction → LLM editing → Facet output
- Design tradeoffs: Larger LLMs improve editing quality but increase cost and latency; multi-task learning improves query understanding but adds training complexity
- Failure signatures: Low term overlap indicates poor semantic coverage; high duplicate facets suggest the model is over-repeating patterns
- First 3 experiments:
  1. Test multi-task learning alone (FD+M) vs baseline (F) to measure query understanding improvement
  2. Test LLM editing alone (E(few)) vs baseline to measure editing capability
  3. Test the full pipeline (FD+M+E) vs previous methods on MIMICS-Manual to validate end-to-end performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed method change when using different multi-task learning targets (e.g., document snippets, related queries, or a combination of both) during training? The paper shows FD+M outperforms FR+M but doesn't analyze why or whether combining targets would help.

### Open Question 2
How does the proposed LLM editing technique compare to other prompt engineering techniques (e.g., chain-of-thought prompting, contrastive chain-of-thought prompting) in improving facet generation performance? The paper doesn't compare against these alternative prompting strategies.

### Open Question 3
How does the performance of the proposed method vary across different query types (e.g., navigational, informational, transactional) and query lengths? The paper doesn't analyze performance variations across query categories.

## Limitations

- The approach relies heavily on the quality of SERP data during training and the ability of the LLM to meaningfully edit generated facets
- Limited to the MIMICS dataset without validation on other domains or languages
- Computational cost of LLM editing is not thoroughly analyzed
- Lacks detailed failure analysis for the LLM editing component

## Confidence

- Multi-task learning improves query understanding: High confidence
- LLM editing provides meaningful quality improvements: Medium confidence
- The approach works without SERP at test time: High confidence

## Next Checks

1. Conduct ablation study on multi-task components to isolate contribution of document prediction vs. related query prediction
2. Systematically test LLM editing failure thresholds by providing progressively worse initial facet predictions
3. Evaluate trained model on a different facet generation dataset to assess cross-dataset generalization