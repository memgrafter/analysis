---
ver: rpa2
title: A Survey of Explainable Knowledge Tracing
arxiv_id: '2403.07279'
source_url: https://arxiv.org/abs/2403.07279
tags:
- uni00000013
- knowledge
- uni00000011
- tracing
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of explainable knowledge
  tracing (KT), addressing the critical need for interpretability in AI-driven educational
  models. The authors classify KT models into transparent and black-box models, reviewing
  interpretable methods across three stages: ante-hoc, post-hoc, and other dimensions.'
---

# A Survey of Explainable Knowledge Tracing

## Quick Facts
- **arXiv ID**: 2403.07279
- **Source URL**: https://arxiv.org/abs/2403.07279
- **Authors**: Yanhong Bai; Jiabao Zhao; Tingjiang Wei; Qing Cai; Liang He
- **Reference count**: 40
- **Key outcome**: Comprehensive survey of explainable knowledge tracing (xKT) models, classifying KT approaches into transparent and black-box categories, reviewing interpretable methods across ante-hoc, post-hoc, and other dimensions, conducting experiments with LRP, LIME, and SHAP on ASSISTment2009 dataset, and proposing evaluation frameworks for both professional and non-professional users.

## Executive Summary
This paper addresses the critical need for interpretability in AI-driven educational models by providing a comprehensive survey of explainable knowledge tracing (xKT). The authors systematically classify KT models into transparent and black-box approaches and review interpretable methods across three key stages: ante-hoc (inherently interpretable), post-hoc (applied after training), and other dimensions. Through experimental analysis using three XAI methods (LRP, LIME, SHAP) on the ASSISTment2009 dataset, the study demonstrates how different interpretability techniques can be applied to KT models and evaluates their effectiveness.

The research also proposes novel evaluation frameworks for xKT from both professional (educators, researchers) and non-professional (students, parents) user perspectives. Looking forward, the paper identifies key research directions including balancing model performance with interpretability, developing user-friendly explainable methods, integrating causal inference, and addressing ethical and privacy considerations in educational AI systems.

## Method Summary
The authors conducted a systematic literature review of explainable knowledge tracing models, classifying them into transparent and black-box categories. They analyzed interpretable methods across three stages: ante-hoc (inherently interpretable models), post-hoc (explanation methods applied after training), and other dimensions. Experimental validation was performed using three XAI methods - LRP (Layer-wise Relevance Propagation), LIME (Local Interpretable Model-agnostic Explanations), and SHAP (SHapley Additive exPlanations) - applied to the ASSISTment2009 dataset. The evaluation frameworks for xKT were proposed from both professional and non-professional user perspectives, though these frameworks were not empirically validated with actual end-users in this study.

## Key Results
- Comprehensive classification of KT models into transparent and black-box categories with detailed review of interpretable methods
- Experimental comparison of three XAI methods (LRP, LIME, SHAP) on ASSISTment2009 dataset demonstrating different approaches to model interpretability
- Proposal of evaluation frameworks for xKT from professional (educators, researchers) and non-professional (students, parents) user perspectives
- Identification of future research directions including balancing performance with interpretability, user-friendly explainable methods, causal inference integration, and ethical/privacy considerations

## Why This Works (Mechanism)
The paper's approach works by systematically organizing the complex landscape of explainable knowledge tracing into a coherent taxonomy that bridges the gap between model interpretability research and educational applications. By classifying models into transparent and black-box categories and reviewing methods across ante-hoc and post-hoc stages, the authors create a structured framework for understanding how different xKT approaches can be applied. The experimental validation using multiple XAI methods demonstrates practical implementation pathways, while the proposed evaluation frameworks address the critical need for context-specific interpretability measures that serve different stakeholder groups in educational settings.

## Foundational Learning
1. **Knowledge Tracing Fundamentals** (why needed: Understanding the base concept of modeling student knowledge acquisition over time; quick check: Can explain how KT models predict student performance on future questions)
2. **Explainable AI Methods** (why needed: Familiarity with LRP, LIME, SHAP and other interpretability techniques; quick check: Can describe how each XAI method generates explanations for model predictions)
3. **Educational Data Mining** (why needed: Understanding how student interaction data is collected and processed in learning systems; quick check: Can identify key features used in KT models from student interaction logs)
4. **Model Interpretability Taxonomy** (why needed: Grasping the difference between ante-hoc and post-hoc interpretability approaches; quick check: Can classify a given KT model as transparent or black-box)
5. **User-Centered Evaluation** (why needed: Understanding how different stakeholders (teachers, students) require different types of explanations; quick check: Can articulate why professional and non-professional users need different evaluation metrics)
6. **Causal Inference in Education** (why needed: Recognizing the importance of understanding causal relationships in learning rather than just correlations; quick check: Can explain why correlation-based explanations may be insufficient for educational decision-making)

## Architecture Onboarding

**Component Map:**
Data Collection -> Feature Engineering -> KT Model Training -> XAI Method Application -> Explanation Generation -> User Interface

**Critical Path:**
The critical path for explainable KT systems flows from raw student interaction data through preprocessing and feature extraction, to KT model training (whether transparent or black-box), application of chosen XAI method, generation of explanations, and finally presentation through a user interface tailored to the target audience (professional or non-professional users).

**Design Tradeoffs:**
The primary tradeoff involves balancing model performance with interpretability - more complex black-box models often achieve better prediction accuracy but sacrifice transparency, while simpler transparent models offer better interpretability but may underperform on complex prediction tasks. Another key tradeoff is between the granularity of explanations (detailed vs. high-level) and the cognitive load placed on different user types.

**Failure Signatures:**
Common failure modes include explanations that are technically correct but pedagogically meaningless, over-reliance on spurious correlations identified by XAI methods, explanations that are too complex for non-professional users to understand, and privacy violations when explanations inadvertently reveal sensitive student information.

**3 First Experiments:**
1. Apply LRP, LIME, and SHAP to a simple Bayesian Knowledge Tracing model on ASSISTment2009 to establish baseline interpretability results
2. Test explanation comprehensibility with both educators and students using think-aloud protocols
3. Evaluate explanation usefulness by measuring whether users can correctly identify which skills need intervention based on model explanations

## Open Questions the Paper Calls Out
The paper identifies several open questions in the field of explainable knowledge tracing: How to effectively balance model performance with interpretability requirements? What are the most user-friendly explainable methods for different stakeholder groups? How can causal inference be better integrated into KT explanations to provide more actionable insights? What are the ethical implications and privacy considerations when providing explanations in educational settings? How can we empirically validate the proposed evaluation frameworks with actual end-users?

## Limitations
- Experimental scope limited to ASSISTment2009 dataset without validation across multiple educational contexts or student populations
- Proposed evaluation frameworks for professional and non-professional users lack empirical validation with actual end-users
- Classification of KT models into transparent and black-box categories oversimplifies the nuanced spectrum of interpretability

## Confidence

**High confidence**: The comprehensive survey of existing KT literature and the clear taxonomy of explainable methods
**Medium confidence**: The experimental results with XAI methods on ASSISTment2009
**Medium confidence**: The proposed evaluation frameworks for different user types

## Next Checks
1. Conduct user studies with both educators and students to empirically validate the proposed evaluation frameworks for explainability in KT systems
2. Test the interpretability methods across multiple diverse datasets representing different educational contexts, age groups, and subject domains
3. Implement a longitudinal study examining how interpretable KT models impact student learning outcomes and teacher decision-making over extended periods