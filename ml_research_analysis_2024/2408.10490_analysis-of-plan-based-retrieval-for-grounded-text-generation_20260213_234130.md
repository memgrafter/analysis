---
ver: rpa2
title: Analysis of Plan-based Retrieval for Grounded Text Generation
arxiv_id: '2408.10490'
source_url: https://arxiv.org/abs/2408.10490
tags:
- retrieval
- plan-based
- generation
- language
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how planning can improve retrieval-augmented
  generation to reduce hallucinations in long-form text generation. The authors propose
  using language models to generate detailed outlines and search queries to retrieve
  comprehensive facts, then generate responses conditioned on this plan and retrieved
  evidence.
---

# Analysis of Plan-based Retrieval for Grounded Text Generation

## Quick Facts
- arXiv ID: 2408.10490
- Source URL: https://arxiv.org/abs/2408.10490
- Reference count: 33
- This paper analyzes how planning can improve retrieval-augmented generation to reduce hallucinations in long-form text generation.

## Executive Summary
This paper proposes plan-guided retrieval as a method to improve factuality and attribution in long-form text generation. The approach uses language models to first generate detailed outlines of what should be included in each paragraph, then creates targeted search queries based on these outlines to retrieve comprehensive facts. The method shows significant improvements in attribution rates compared to direct generation or simple retrieval-augmentation, producing more informative and grounded outputs while maintaining fluency.

## Method Summary
The approach uses a multi-stage process where an initial prompt triggers outline generation describing what should be included in each paragraph. These outlines guide the creation of specific search queries, which are used to retrieve documents. The system optionally uses a QA model to answer questions and mark unanswerable ones, then generates final responses conditioned on the outline, questions, and retrieved evidence. Experiments compare this plan-based retrieval against direct generation and one-round retrieval across four datasets.

## Key Results
- Plan-based retrieval significantly improves attribution rates compared to direct generation or one-round retrieval
- Explicitly marking unanswerable questions reduces hallucinations by preventing the model from fabricating answers
- Generating outlines before creating questions improves attribution by ensuring comprehensive information gathering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Plan-guided retrieval improves fact coverage by generating targeted search queries that capture specific aspects of the entity.
- **Mechanism**: The language model first generates a detailed outline describing what should be included in each paragraph. This outline then guides the generation of specific search queries for each paragraph, allowing the retrieval system to gather fine-grained facts that the LLM needs to write its final response.
- **Core assumption**: Language models hallucinate when they lack specific facts needed for generation, and providing comprehensive contextual information can mitigate this.
- **Evidence anchors**:
  - [abstract]: "We hypothesize that language models hallucinate when they are required to generate certain facts based on the prompt but do not have the information either memorized or in their context."
  - [section 4.4]: "Observation 4: Gathering Information to Answer Questions in a Second Round of Search Improves Attribution. Recall that web search happens two times in the Plan-based Retrieval model."
- **Break condition**: If the generated search queries are too generic or miss critical aspects of the entity, the additional retrieval won't improve fact coverage.

### Mechanism 2
- **Claim**: The format of retrieved information presentation affects grounding effectiveness.
- **Mechanism**: Two variants are tested - one that simply places retrieved documents in context (Var.A) and another that uses a QA model to answer the generated questions and marks unanswerable ones (Var.B). Explicitly marking unanswerable questions reduces hallucinations by preventing the model from fabricating answers.
- **Core assumption**: Language models will attempt to answer all questions presented in context, potentially hallucinating when information is unavailable.
- **Evidence anchors**:
  - [abstract]: "We also show that including unanswerable search queries (and explicitly marking them as unanswerable) is influential in reducing the generation of ungrounded outputs."
  - [section 4.4]: "Observation 5. Indicating Unanswerable Questions in the Plan Improves Attribution."
- **Break condition**: If the QA model confidence threshold is set too high, valuable answerable questions might be incorrectly marked as unanswerable, reducing information coverage.

### Mechanism 3
- **Claim**: Planning improves retrieval quality by creating a structured approach to information gathering.
- **Mechanism**: The outline generation step forces the model to think about what information is needed before retrieving it, leading to more comprehensive and targeted searches compared to simple keyword-based retrieval.
- **Core assumption**: Structured planning leads to more systematic information gathering than ad-hoc approaches.
- **Evidence anchors**:
  - [abstract]: "By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents."
  - [section 4.4]: "Observation 7. Generating an Outline Before Generating Questions Improves Attribution."
- **Break condition**: If the outline generation fails to capture important aspects of the entity, subsequent queries will miss critical information regardless of planning structure.

## Foundational Learning

- **Concept: Retrieval-augmented generation (RAG)**
  - Why needed here: The paper builds on RAG by adding planning to improve retrieval effectiveness
  - Quick check question: What is the primary benefit of adding retrieval to language model generation?

- **Concept: Language model hallucination**
  - Why needed here: The paper addresses hallucinations by providing comprehensive factual context
  - Quick check question: How does the paper define hallucinations in the context of language model generation?

- **Concept: In-context learning**
  - Why needed here: The approach relies on language models using information provided in their context
  - Quick check question: What mechanism allows language models to use information from retrieved documents?

## Architecture Onboarding

- **Component map**:
  Initial prompt → Outline generator → Query generator → First retrieval → Second retrieval (per-query) → QA model (optional) → Final generation

- **Critical path**:
  1. Generate outline from initial prompt and search results
  2. Generate specific questions for each outline paragraph
  3. Retrieve documents for each question
  4. Optionally run QA model on retrieved documents
  5. Generate final response conditioned on outline, questions, and retrieved evidence

- **Design tradeoffs**:
  - Multiple LLM calls increase latency but improve grounding
  - More comprehensive retrieval increases attribution but also computational cost
  - QA model adds complexity but can improve fact presentation

- **Failure signatures**:
  - Low attribution scores indicate insufficient or irrelevant retrieved information
  - Repetitive outputs suggest poor query diversity or over-reliance on initial search
  - Factual errors despite high attribution may indicate QA model errors or misinterpretation

- **First 3 experiments**:
  1. Compare attribution rates between direct generation and one-round retrieval
  2. Test the impact of marking unanswerable questions explicitly
  3. Evaluate the effect of QA-based vs. document-based evidence presentation

## Open Questions the Paper Calls Out

1. How to effectively discover the comprehensive collection of facts needed to generate text about a particular subject? (§3.2,§3.3, Table 1, 2, 7)
2. How can we effectively retrieve and represent these needed facts? (§3.3, §4.2, Table 3, 4 5)

## Limitations

- The evaluation relies heavily on automated AIS metrics, which may not capture all nuances of attribution quality despite validation against human judgment
- The study focuses on English-language entities and may not generalize well to other languages or cultural contexts
- Performance on entities with limited web presence may not reflect real-world scenarios with extensive online documentation

## Confidence

- **High Confidence**: The claim that plan-guided retrieval improves attribution rates compared to direct generation is well-supported by consistent improvements across multiple datasets and evaluation metrics
- **Medium Confidence**: The finding that explicitly marking unanswerable questions reduces hallucinations is supported by the data but requires further investigation into the interaction between question complexity and model behavior
- **Low Confidence**: The specific impact of different plan generation strategies on attribution rates has limited support, with only one dataset showing statistically significant differences

## Next Checks

1. Conduct a comprehensive human evaluation of attribution quality on a subset of outputs to validate the correlation findings and identify potential systematic biases in the evaluation framework
2. Replicate the core experiments using open-source models and alternative retrieval systems to assess generalizability beyond the Google Cloud ecosystem
3. Evaluate the approach's performance over extended periods with dynamic web content to assess robustness to evolving information landscapes