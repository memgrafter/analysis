---
ver: rpa2
title: Large-Scale Evaluation of Open-Set Image Classification Techniques
arxiv_id: '2406.09112'
source_url: https://arxiv.org/abs/2406.09112
tags:
- samples
- unknown
- negative
- softmax
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first large-scale evaluation of open-set
  image classification methods, comparing training-based (SoftMax, Garbage, EOS) and
  post-processing techniques (MSS, MLS, OpenMax, EVM, PROSER) on three challenging
  ImageNet protocols with varying semantic similarity between known and unknown classes.
  The evaluation used realistic protocols with real negative samples and Open-Set
  Classification Rate (OSCR) curves for performance measurement.
---

# Large-Scale Evaluation of Open-Set Image Classification Techniques

## Quick Facts
- arXiv ID: 2406.09112
- Source URL: https://arxiv.org/abs/2406.09112
- Reference count: 40
- Primary result: First large-scale evaluation of open-set image classification methods on ImageNet with varying semantic similarity protocols

## Executive Summary
This study presents the first comprehensive large-scale evaluation of open-set image classification techniques, comparing both training-based and post-processing approaches across three challenging ImageNet protocols. The research introduces realistic evaluation protocols with real negative samples and uses Open-Set Classification Rate (OSCR) curves for performance measurement. The study demonstrates that EOS training consistently improves post-processing technique performance, particularly when combined with OpenMax or PROSER, while the Garbage class approach shows inferior results. The evaluation provides complete open-source implementations for reproducible research.

## Method Summary
The study evaluates multiple open-set classification techniques across three distinct protocols on ImageNet, using both training-based approaches (SoftMax, Garbage, EOS) and post-processing methods (MSS, MLS, OpenMax, EVM, PROSER). The protocols are designed with varying semantic similarity between known and unknown classes, using real negative samples rather than synthetic ones. OSCR curves are employed to measure performance, providing a comprehensive assessment of each method's effectiveness across different operating points.

## Key Results
- EOS training consistently improves post-processing performance, especially with OpenMax and PROSER
- OpenMax excels when unknown classes are semantically distant from known classes
- MLS and MSS show greater robustness in challenging scenarios with similar classes
- Garbage class approach generally underperforms compared to other training methods

## Why This Works (Mechanism)
The effectiveness of open-set classification methods stems from their ability to handle the fundamental challenge of recognizing unknown classes during inference. Training-based methods like EOS learn to distinguish between known and unknown classes during training, while post-processing techniques analyze the output distribution to detect outliers. The combination of EOS training with post-processing methods like OpenMax or PROSER provides complementary benefits: EOS improves the quality of probability distributions, while post-processing methods enhance the detection of open-space samples.

## Foundational Learning
- Open-set recognition: Understanding the problem of handling unknown classes during inference is crucial for developing effective solutions. Quick check: Can the model identify samples from classes not seen during training?
- Semantic similarity: The relationship between known and unknown classes affects method performance. Quick check: How similar are the unknown classes to the known training classes?
- OSCR curves: These provide a more comprehensive evaluation than single-point metrics for open-set scenarios. Quick check: Does the method maintain good performance across different false positive rates?
- Probability calibration: Proper calibration of output probabilities is essential for effective open-set detection. Quick check: Are the predicted probabilities well-calibrated and reliable?

## Architecture Onboarding

Component Map:
ImageNet dataset -> Training methods (SoftMax, Garbage, EOS) -> Base classifier -> Post-processing methods (OpenMax, PROSER, MLS, MSS, EVM) -> OSCR evaluation

Critical Path:
The critical path for open-set classification involves: training a base classifier on known classes, applying either specialized training (EOS/Garbage) or post-processing techniques, and evaluating performance using OSCR curves with real unknown samples.

Design Tradeoffs:
Training-based methods (EOS, Garbage) require modification of the training process but can provide more robust representations, while post-processing methods (OpenMax, PROSER) are more flexible and can be applied to existing models. The tradeoff involves computational cost during training versus inference flexibility.

Failure Signatures:
Poor performance typically occurs when unknown classes are semantically similar to known classes, or when the base classifier's feature representations are not discriminative enough. Methods that rely heavily on distance metrics in feature space may struggle with high-dimensional, complex image data.

First Experiments:
1. Compare base SoftMax classifier performance against EOS-trained model on protocol with distant unknown classes
2. Evaluate OpenMax performance with and without EOS training on protocol with similar unknown classes
3. Test MLS and MSS robustness across all three protocols to identify scenarios where each excels

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to ImageNet dataset, may not generalize to other domains or datasets
- Does not address computational efficiency or training time comparisons between methods
- Does not explore class imbalance effects or impact of different network architectures

## Confidence
- High: EOS training consistently improves post-processing performance, particularly with OpenMax and PROSER
- High: OpenMax performs well with semantically distant unknown classes
- Medium: MLS and MSS are more robust in challenging scenarios with similar classes
- High: Garbage class approach is generally inferior across protocols

## Next Checks
1. Evaluate the same techniques on non-ImageNet datasets, particularly those with different domain characteristics like medical imaging or satellite imagery
2. Test the scalability of these methods on much larger class vocabularies (10,000+ classes) to assess performance at industrial scales
3. Conduct ablation studies to determine which components of the EOS training approach contribute most to performance improvements