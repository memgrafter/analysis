---
ver: rpa2
title: 'Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models'
arxiv_id: '2412.13702'
source_url: https://arxiv.org/abs/2412.13702
tags:
- thai
- data
- speech
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Typhoon 2 introduces a family of large language models optimized
  for Thai language and multimodal tasks. The models are built on state-of-the-art
  open models like Llama 3 and Qwen2, with continual pre-training on a mixture of
  English and Thai data.
---

# Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models

## Quick Facts
- arXiv ID: 2412.13702
- Source URL: https://arxiv.org/abs/2412.13702
- Reference count: 27
- Key outcome: Typhoon 2 introduces a family of large language models optimized for Thai language and multimodal tasks

## Executive Summary
Typhoon 2 presents a comprehensive family of large language models specifically optimized for Thai language and multimodal tasks. The model series includes text models ranging from 1 to 70 billion parameters, vision models for Thai document understanding, audio models for speech-to-speech processing, and a safety classifier for Thai-sensitive content. Built on state-of-the-art open models like Llama 3 and Qwen2, the models undergo continual pre-training on mixed English-Thai data followed by sophisticated post-training techniques. All models are publicly available on Hugging Face Hub.

## Method Summary
Typhoon 2 employs a systematic approach of continual pre-training on a 50% English and 50% Thai data mixture, followed by sequential post-training techniques including supervised fine-tuning, domain-specific adaptation, long-context training, function calling, and distillation. The vision models enhance Thai OCR capabilities while maintaining general visual understanding, and the audio models introduce an end-to-end speech-to-speech architecture using Whisper, BEATs, and Q-Former components. A dedicated safety classifier based on mDeBERTa-v3 detects Thai-sensitive content.

## Key Results
- Text models show improved Thai language performance while preserving English capabilities
- Vision models excel in Thai OCR and document understanding tasks
- Audio models achieve state-of-the-art performance in Thai speech processing and generation
- Safety classifier demonstrates strong detection of Thai-sensitive content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continual pre-training on a mixture of English and Thai data improves Thai language performance while preserving base model capabilities.
- **Mechanism**: The model undergoes fine-tuning on high-quality Thai data while maintaining a 50% English subset to mitigate catastrophic forgetting.
- **Core assumption**: The base model already has some Thai language exposure, and additional Thai data can enhance performance without degrading English capabilities.
- **Evidence anchors**:
  - [abstract]: "continual pre-training on a mixture of English and Thai data"
  - [section 2.3]: "Our English dataset ratio, which is 50%, is inspired by previous studies... to mitigate catastrophic forgetting"
  - [corpus]: Limited evidence for English preservation claims; relies on Typhoon and SambaLingo studies
- **Break condition**: If the base model lacks Thai exposure, continual pre-training may not improve Thai performance significantly.

### Mechanism 2
- **Claim**: Post-training techniques enhance Thai language performance while preserving base model capabilities.
- **Mechanism**: A combination of general supervised fine-tuning (SFT), domain-specific SFT, long context adaptation, function calling, and distillation techniques are applied sequentially.
- **Core assumption**: Each post-training stage improves specific capabilities without interfering with previously learned skills.
- **Evidence anchors**:
  - [abstract]: "employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities"
  - [section 3.1.3]: "the inclusion of a high-quality English dataset contributes to improving the performance of Thai language models"
  - [corpus]: Limited evidence for preservation claims; relies on experimental results showing maintained performance
- **Break condition**: If post-training data distribution is imbalanced, it may lead to catastrophic forgetting of base capabilities.

### Mechanism 3
- **Claim**: Multimodal training with Thai-specific data improves document understanding and speech processing.
- **Mechanism**: Vision and audio models are fine-tuned on Thai-specific datasets (OCR, financial documents, speech data) while maintaining general capabilities.
- **Core assumption**: Thai-specific data can enhance multimodal understanding without degrading performance on other languages or modalities.
- **Evidence anchors**:
  - [abstract]: "Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities" and "Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture"
  - [section 4.5]: "Typhoon2-Qwen2-VL excels in key areas such as ChartQA, OCR (TH), MTVQ (TH), and M3Exam Images (TH)"
  - [corpus]: Limited evidence for Thai-specific performance claims; relies on benchmark results
- **Break condition**: If Thai-specific data is insufficient or of poor quality, multimodal performance may not improve significantly.

## Foundational Learning

- **Concept**: Language modeling and tokenization
  - Why needed here: Understanding how models process and generate text is fundamental to grasping the Typhoon 2 architecture
  - Quick check question: What is the difference between subword tokenization and character-level tokenization, and why is it important for Thai language processing?

- **Concept**: Transfer learning and fine-tuning
  - Why needed here: Typhoon 2 builds on existing models through various fine-tuning techniques
  - Quick check question: What is the difference between full fine-tuning and parameter-efficient methods like LoRA, and when would you use each?

- **Concept**: Multimodal learning
  - Why needed here: Typhoon 2 includes vision and audio models that process multiple input types
  - Quick check question: What are the key challenges in aligning visual, audio, and textual representations in a single model?

## Architecture Onboarding

- **Component map**: Base models (Llama 3, Qwen2) → Continual pre-training → Post-training (SFT, domain-specific, long context, function calling, distillation) → Multimodal fine-tuning → Safety classifier development
- **Critical path**: Base model selection → Continual pre-training → Post-training sequence → Multimodal fine-tuning → Safety classifier development
- **Design tradeoffs**: Model size vs. performance, English vs. Thai data balance, general vs. Thai-specific capabilities, computational resources vs. model complexity
- **Failure signatures**: Catastrophic forgetting of base capabilities, poor Thai language performance, multimodal misalignment, safety classifier false positives/negatives
- **First 3 experiments**:
  1. Evaluate base model Thai performance on benchmark tasks
  2. Test continual pre-training on English-Thai data mixture
  3. Assess post-training impact on Thai language capabilities while monitoring English preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the long-context capabilities of Llama-based models beyond 90K tokens while maintaining performance on other tasks?
- Basis in paper: [explicit] The paper notes that Typhoon2-Llama3.1 models support approximately 90K tokens, less than the original Llama 3.1's 128K, due to training constraints.
- Why unresolved: The authors hypothesize the limitation stems from training with shorter context lengths (8K) and incremental training stages in the original model, but this requires further exploration.
- What evidence would resolve it: Comparative experiments testing incremental long-context training stages versus full fine-tuning on extended contexts, measuring performance across both long-context and general tasks.

### Open Question 2
- Question: What is the optimal balance between Thai and English data in the training mixture to maximize performance in both languages without overfitting?
- Basis in paper: [explicit] The paper found that increasing Thai data from a 1:9 to a 3:7 Thai-English ratio improves Thai performance, but further optimization may be possible.
- Why unresolved: The optimal ratio may vary depending on model size, task, and domain, and the authors only empirically determined 3:7 as optimal for their specific setup.
- What evidence would resolve it: Systematic ablation studies varying Thai-English ratios across different model sizes and tasks, measuring performance on both languages.

### Open Question 3
- Question: How can we develop a more robust safety classifier that better handles the nuances of Thai cultural sensitivities while maintaining performance on universal safety concerns?
- Basis in paper: [explicit] The Typhoon2-Safety model achieves state-of-the-art performance but the authors note that direct numerical comparisons may not tell the complete story due to policy differences in safety approaches.
- Why unresolved: The complexity of Thai cultural sensitivities and the need to balance them with universal safety concerns requires ongoing refinement of detection methods and threshold settings.
- What evidence would resolve it: Comparative studies testing different classification approaches (e.g., scoring vs. direct classification) and threshold settings across various Thai and universal safety scenarios, measuring both precision and recall.

## Limitations

- Limited details on data filtering and quality control methods for Thai text data
- Performance claims primarily based on benchmark datasets rather than diverse real-world scenarios
- Key implementation details for distillation and model merging processes are not fully specified

## Confidence

**High Confidence**:
- Technical feasibility of building Thai-optimized language models using established fine-tuning techniques
- Basic architecture and component structure of the Typhoon 2 family
- Availability and accessibility of models through Hugging Face

**Medium Confidence**:
- Claims about preserving base model capabilities during Thai-specific optimization
- Performance improvements on benchmark tasks for Thai language processing
- Effectiveness of safety classifier on Thai-sensitive content

**Low Confidence**:
- Real-world generalization beyond benchmark datasets
- Robustness of multimodal integration across diverse scenarios
- Long-term preservation of capabilities without catastrophic forgetting

## Next Checks

1. **Cross-Domain Performance Evaluation**: Test the Typhoon 2 models on diverse, real-world Thai language datasets across multiple domains (e.g., social media, academic papers, news articles) to assess generalization beyond benchmark datasets.

2. **Longitudinal Capability Preservation**: Conduct a study tracking model performance over extended fine-tuning periods to empirically validate claims about preserving base capabilities and preventing catastrophic forgetting.

3. **Multimodal Alignment Quality Assessment**: Design experiments to quantitatively measure the alignment quality between different modalities (text, vision, audio) in the multimodal models, particularly focusing on Thai-specific content.