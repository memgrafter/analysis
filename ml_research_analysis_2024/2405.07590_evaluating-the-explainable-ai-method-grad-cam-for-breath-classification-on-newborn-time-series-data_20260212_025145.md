---
ver: rpa2
title: Evaluating the Explainable AI Method Grad-CAM for Breath Classification on
  Newborn Time Series Data
arxiv_id: '2405.07590'
source_url: https://arxiv.org/abs/2405.07590
tags:
- data
- breath
- classification
- breaths
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates Grad-CAM explainability for a CNN model classifying
  neonatal breaths in mechanical ventilation data. The CNN model achieved mixed accuracy
  (34-90%) on test data with particular difficulty in detecting mechanical breaths
  and frequent misclassification of artefacts as spontaneous breaths.
---

# Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data

## Quick Facts
- arXiv ID: 2405.07590
- Source URL: https://arxiv.org/abs/2405.07590
- Reference count: 8
- Primary result: Grad-CAM explanations for CNN-based breath classification in neonatal ventilation data received mixed evaluations from medical professionals and AI developers, with domain experts rating them lower than developers.

## Executive Summary
This paper evaluates Grad-CAM as an explainability method for a CNN model classifying neonatal breaths in mechanical ventilation data. The CNN achieved mixed accuracy (34-90%) on test data, particularly struggling with mechanical breath detection and frequently misclassifying artefacts as spontaneous breaths. A user study with 12 participants (7 medical professionals, 5 AI developers) assessed Grad-CAM explanations through questionnaires. While both groups found visualizations useful, domain experts rated performance lower than developers, especially for predictability. The study concluded Grad-CAM is unsuitable for clinical use but promising for research, with key limitations including zero-padding artifacts and insufficient participant numbers for robust trends.

## Method Summary
The study used an XCM CNN architecture to classify neonatal breaths into four categories (spontaneous, mechanical, triggered, unclassifiable) from multivariate flow and pressure time series. The model processed flow and pressure separately in early convolutional layers before concatenating features. Grad-CAM was applied to generate heatmaps highlighting important temporal regions for classification decisions, producing separate visualizations for flow and pressure inputs. A user study with medical professionals and AI developers evaluated these explanations using questionnaires covering trustworthiness, causality, informativeness, confidence, and fairness metrics.

## Key Results
- CNN achieved mixed accuracy (34-90%) with particular difficulty detecting mechanical breaths and frequent misclassification of artefacts as spontaneous breaths
- Domain experts rated explanation performance lower than AI developers, especially for predictability metrics
- Both groups found visualizations useful but criticized unclear connections between separate flow/pressure explanations and combined classification
- Method deemed unsuitable for clinical use but promising for research due to zero-padding artifacts and explanation clarity issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grad-CAM applied to CNN feature maps reveals which input time points contribute most to classification decisions.
- Mechanism: Grad-CAM computes gradients of the output class score with respect to the last convolutional layer activations. These gradients weight the feature maps, producing a heatmap highlighting influential regions. For breath classification, this highlights temporal segments where flow or pressure changes drive the prediction.
- Core assumption: Gradients from the output back to feature maps reliably indicate input importance.
- Evidence anchors:
  - [abstract] "The classification of breaths can be of interest for clinical and medical engineering applications, for instance during the weaning process of patients"
  - [section] "Grad-CAM allows generating heatmaps for the current inputs based on the trained weights and the resulting feature maps of a convolutional layer"
- Break condition: If gradients vanish (e.g., due to ReLU saturation) or if the network relies heavily on non-visualizable layers, Grad-CAM may misrepresent true decision drivers.

### Mechanism 2
- Claim: Separate heatmaps for flow and pressure improve interpretability compared to combined inputs.
- Mechanism: XCM architecture processes flow and pressure separately in early conv layers, then concatenates features. Grad-CAM applied to the 2D conv layer produces separate maps for each input variable, while the 1D conv layer yields a combined map. This two-tier approach lets users see whether decisions are based on one modality or their interaction.
- Core assumption: Separate feature extraction captures modality-specific patterns useful for understanding model decisions.
- Evidence anchors:
  - [section] "This enables the network to process the input data both separately and as the combination of all observed variables"
  - [section] "The obtained heatmap of the separately explained inputs is made optional to the user"
- Break condition: If one modality dominates in the concatenated features, the separate maps may be misleading or redundant.

### Mechanism 3
- Claim: User study evaluation exposes the gap between algorithmic explanations and clinical interpretability.
- Mechanism: Domain experts and AI developers rate Grad-CAM explanations on metrics like causality, informativeness, and confidence. Differences in ratings reveal how background knowledge affects perception of explanations. Poor performance on mechanical breath detection lowers trust in explanations.
- Core assumption: Subjective human ratings are valid proxies for explanation usefulness in practice.
- Evidence anchors:
  - [abstract] "A user study with 12 participants... found domain experts rated performance lower than developers, especially for predictability"
  - [section] "The participants were asked to assess their impression for each question based on the hour of data presented for evaluation"
- Break condition: If user sample is too small or not representative, conclusions about explanation quality may not generalize.

## Foundational Learning

- Concept: Convolutional neural networks for time series
  - Why needed here: The paper uses a CNN variant (XCM) to classify neonatal breath types from multivariate flow/pressure signals.
  - Quick check question: What property of CNNs makes them suitable for time series classification compared to RNNs?
    - Answer: Convolution is translation-invariant and shares weights across time, capturing local temporal patterns efficiently.

- Concept: Zero-padding in fixed-length CNN inputs
  - Why needed here: Breaths vary in duration; zero-padding standardizes input length for the CNN but can cause artifacts in explanations.
  - Quick check question: Why does zero-padding complicate Grad-CAM explanations?
    - Answer: Grad-CAM may highlight padded regions as important, misleading users about actual signal relevance.

- Concept: Explainable AI evaluation metrics
  - Why needed here: The paper uses subjective ratings for trustworthiness, causality, informativeness, confidence, and fairness of explanations.
  - Quick check question: Which metric directly assesses whether explanations reflect the model's reasoning?
    - Answer: Causality, as it evaluates whether correlations shown have a causal justification.

## Architecture Onboarding

- Component map: Input preprocessing (flow/pressure → fixed-length segments) → XCM CNN (2D conv → 1D conv → pooling → softmax) → Grad-CAM (2D conv for separate, 1D conv for combined) → Data viewer (heatmaps overlaid on signals)

- Critical path:
  1. Segment raw ventilation data into breaths (zero-crossing detection)
  2. Pad/resample to fixed length and feed to XCM
  3. Compute Grad-CAM heatmaps from conv layer activations
  4. Display heatmaps overlaid on input signals in viewer
  5. Collect user ratings on explanation quality

- Design tradeoffs:
  - Fixed input length simplifies CNN design but introduces padding artifacts
  - Separate modality heatmaps add interpretability but may confuse if relation to final decision is unclear
  - Subjective user ratings capture practical usefulness but lack objectivity

- Failure signatures:
  - Heatmaps highlight padded zeros or noise instead of signal features
  - High confidence predictions with contradictory or unexplained heatmaps
  - Consistent misclassification of artifacts as spontaneous breaths

- First 3 experiments:
  1. Run Grad-CAM on a synthetic signal where the true important region is known; verify heatmap alignment.
  2. Test with variable-length inputs without padding to observe impact on accuracy and explanations.
  3. Compare domain expert vs. AI developer ratings on a small set of explanations to confirm divergent interpretations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the combination of Grad-CAM visualizations with different stakeholders' expertise levels significantly impact the perceived usefulness of explanations in medical AI applications?
- Basis in paper: [explicit] The paper compares domain experts (medical professionals) and developers, finding that domain experts rated performance lower and found explanations less satisfactory than developers, especially regarding predictability and connection between separate visualizations.
- Why unresolved: The study had a small sample size (12 participants split into two groups) which prevented statistically significant conclusions about trends within each group. The paper notes that self-assessed AI expertise did not clearly correlate with practicability assessment, but the relationship between expertise levels and explanation usefulness remains unclear.
- What evidence would resolve it: A larger user study with stratified sampling across multiple expertise levels (novice to expert in both AI and domain knowledge) measuring perceived explanation usefulness through standardized questionnaires with sufficient statistical power to detect differences.

### Open Question 2
- Question: Can alternative explanation methods provide more clinically actionable insights than Grad-CAM for time series classification in neonatal ventilation data?
- Basis in paper: [explicit] Participants expressed desire for more explicit explanations, criticized Grad-CAM for highlighting seemingly arbitrary regions (including zero-padding), and questioned the connection between separate flow/pressure explanations and combined classification. The method was deemed unsuitable for clinical use but promising for research.
- Why unresolved: The paper only evaluated Grad-CAM and did not compare it with alternative explanation methods. The zero-padding artifact and unclear connections between separate and combined explanations suggest Grad-CAM may have fundamental limitations for this application.
- What evidence would resolve it: Comparative evaluation of multiple explanation methods (e.g., SHAP, LIME, attention mechanisms) on the same dataset with domain expert feedback on clinical actionability and trustworthiness of explanations.

### Open Question 3
- Question: How does model architecture influence the quality and interpretability of explanations in multivariate time series classification for medical applications?
- Basis in paper: [explicit] The CNN architecture required zero-padding for fixed-length inputs, which led to explanations focusing on padding regions that confused participants. The paper suggests this "brought more error room for the classifier and the explanations."
- Why unresolved: The paper used a specific CNN architecture and did not explore how different architectures (e.g., recurrent networks, attention-based models, variable-length CNNs) might affect explanation quality and reduce artifacts like padding focus.
- What evidence would resolve it: Systematic comparison of explanation quality across different model architectures for the same medical classification task, measuring explanation faithfulness, stability, and user comprehension while controlling for model performance differences.

## Limitations

- Small sample size (12 participants) in user study limits generalizability of explainability assessment results
- CNN architecture's poor performance on mechanical breath detection (34-90% accuracy) undermines usefulness of explanations
- Zero-padding artifacts create spurious explanation regions that could mislead users about important features
- Questionnaire methodology for evaluating explanations lacks detailed validation and standardization

## Confidence

- **High confidence**: CNN's mixed classification performance (34-90% accuracy) and systematic misclassification patterns (artifacts as spontaneous breaths) are well-supported by confusion matrix data
- **Medium confidence**: User study findings showing domain experts rating explanations lower than AI developers are plausible but limited by small sample size and lack of questionnaire details
- **Low confidence**: Conclusion that Grad-CAM is "unsuitable for clinical use but promising for research" is somewhat speculative given limited validation and lack of direct clinical workflow testing

## Next Checks

1. Validate Grad-CAM against ground truth by testing explanations on synthetic signals where true important regions are known to verify heatmap alignment rather than padding or noise highlighting.

2. Expand user study with more diverse participants including bedside clinicians to assess whether pattern of domain experts being more critical than AI developers holds with sufficient statistical power.

3. Test alternative architectures by comparing XCM performance and explanations against RNN-based approaches or models handling variable-length inputs natively to determine if padding artifacts and mechanical breath detection issues are architecture-specific.