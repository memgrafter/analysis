---
ver: rpa2
title: 'RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction'
arxiv_id: '2412.18390'
source_url: https://arxiv.org/abs/2412.18390
tags:
- image
- diffusion
- discrete
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RDPM proposes a novel discrete diffusion paradigm that combines
  diffusion probabilistic models with recurrent token prediction to enable high-fidelity
  image generation with improved efficiency. Unlike traditional diffusion models that
  operate on continuous latent space, RDPM performs denoising in discrete token space,
  using a recurrent VQ-VAE to progressively quantize noisy latents into discrete tokens
  over multiple timesteps.
---

# RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction

## Quick Facts
- arXiv ID: 2412.18390
- Source URL: https://arxiv.org/abs/2412.18390
- Authors: Xiaoping Wu; Jie Hu; Xiaoming Wei
- Reference count: 40
- Primary result: Achieves FID 2.56-3.68 on ImageNet 256×256 using only 10 inference steps

## Executive Summary
RDPM introduces a novel discrete diffusion paradigm that combines diffusion probabilistic models with recurrent token prediction to enable high-fidelity image generation with improved efficiency. The method performs denoising in discrete token space using a recurrent VQ-VAE, progressively quantizing noisy latents into discrete tokens over multiple timesteps. This approach aligns optimization with GPT-style cross-entropy loss while maintaining the benefits of iterative refinement from diffusion processes. RDPM achieves state-of-the-art results among discrete-tokenizer approaches on ImageNet 256×256 with FID of 2.56-3.68 across different model sizes.

## Method Summary
RDPM operates by encoding images into latent space using a VAE, then progressively adding Gaussian noise over T timesteps. The noisy latents are quantized into discrete tokens using a recurrent VQ-VAE codebook. A transformer-based model predicts the discrete token codes for each timestep using cross-entropy loss, similar to autoregressive models. During inference, the model generates from random noise by predicting token sequences in reverse order. The approach uses classifier-free guidance with Gumbel noise to improve generation quality and diversity, requiring only 10 inference steps compared to hundreds in conventional diffusion models.

## Key Results
- Achieves FID of 2.56-3.68 on ImageNet 256×256 across different model sizes
- Demonstrates superior computational efficiency using only 10 inference steps
- Shows better performance than autoregressive models while maintaining comparable quality to conventional diffusion models
- Achieves state-of-the-art results among discrete-tokenizer approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDPM enables discrete diffusion by progressively quantizing noisy latents into discrete tokens over multiple timesteps, allowing diffusion to operate in discrete space while preserving iterative refinement benefits.
- Mechanism: The model introduces Gaussian noise into latent representations in T steps, then uses a recurrent VQ-VAE to encode these noisy latents into discrete tokens. At each timestep, the model predicts the next discrete code using cross-entropy loss, aligning with GPT-style models while maintaining the denoising process of diffusion.
- Core assumption: The discrete token space can capture sufficient information for high-fidelity image generation, and the recurrent prediction can effectively model the diffusion process in discrete space.
- Evidence anchors:
  - [abstract]: "By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains."
  - [section 3.1]: "This subsection aims to divide the original single-step vector quantization operation into a T-step diffusion process."
  - [corpus]: Weak - the corpus contains related papers on discrete diffusion but lacks direct comparison to RDPM's specific approach.
- Break condition: If the discrete token space cannot capture sufficient information for image reconstruction, or if the recurrent prediction fails to model the diffusion process effectively in discrete space.

### Mechanism 2
- Claim: RDPM achieves computational efficiency by compressing the diffusion process into only 10 timesteps while maintaining image quality comparable to conventional diffusion models.
- Mechanism: By using a recurrent token prediction framework, RDPM can predict multiple tokens in parallel at each timestep, rather than requiring one prediction per token as in autoregressive models. This reduces the number of inference steps from hundreds to just 10.
- Core assumption: The parallel token prediction at each timestep can maintain image quality while significantly reducing computational cost.
- Evidence anchors:
  - [abstract]: "RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps."
  - [section 4.4]: "RDPM delivers high-quality image generation with only 10 timesteps and fewer parameters than autoregressive models."
  - [corpus]: Weak - while the corpus mentions discrete diffusion models, it doesn't provide specific evidence about RDPM's efficiency claims.
- Break condition: If parallel token prediction at each timestep cannot maintain image quality, or if the reduction to 10 steps is insufficient for proper denoising.

### Mechanism 3
- Claim: RDPM aligns with GPT-style models by using cross-entropy loss for discrete token prediction, enabling unified optimization with other discrete tokens like text.
- Mechanism: Instead of using diffusion-based loss functions on continuous values, RDPM predicts discrete codes at each timestep using cross-entropy loss, similar to autoregressive models. This allows for consistent optimization strategies across different modalities.
- Core assumption: Cross-entropy loss on discrete tokens can effectively train the diffusion process while maintaining compatibility with other discrete token-based models.
- Evidence anchors:
  - [abstract]: "This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function."
  - [section 3.2]: "The objective is to minimize the discrepancy between the predicted bv′t and the ground truth v′t. In practice, noise and quantized vectors are concatenated..."
  - [corpus]: Weak - the corpus contains related work on discrete diffusion but lacks direct evidence about RDPM's specific loss function alignment.
- Break condition: If cross-entropy loss on discrete tokens cannot effectively train the diffusion process, or if the alignment with GPT-style models proves insufficient for unified multimodal generation.

## Foundational Learning

- Concept: Vector Quantization (VQ) and VQ-VAE
  - Why needed here: RDPM relies on vector quantization to convert continuous latents into discrete tokens, which is fundamental to its discrete diffusion approach.
  - Quick check question: How does the VQ-VAE codebook size (4096 vectors) affect the trade-off between reconstruction quality and computational efficiency in RDPM?

- Concept: Diffusion Probabilistic Models (DPMs)
  - Why needed here: RDPM builds upon the diffusion framework but adapts it for discrete token spaces, requiring understanding of both continuous and discrete diffusion processes.
  - Quick check question: What are the key differences between the forward and reverse diffusion processes in RDPM compared to traditional continuous DPMs?

- Concept: Autoregressive Models and Cross-Entropy Loss
  - Why needed here: RDPM uses cross-entropy loss similar to autoregressive models, aligning its optimization strategy with GPT-style approaches for potential multimodal unification.
  - Quick check question: How does the cross-entropy loss in RDPM differ from traditional autoregressive models in terms of token prediction strategy and timestep handling?

## Architecture Onboarding

- Component map:
  - Encoder (E) -> Diffusion-based Quantization -> Recurrent Token Prediction Transformer -> Decoder (D) -> Classifier-Free Guidance (CFG) -> Gumbel Noise

- Critical path:
  1. Image encoding through VAE
  2. T-step diffusion and quantization process
  3. Recurrent token prediction with transformer
  4. Image reconstruction from predicted discrete tokens

- Design tradeoffs:
  - T-step diffusion vs. reconstruction quality: Higher T improves quality but increases computational cost
  - Codebook size vs. information loss: Larger codebooks reduce quantization loss but increase memory usage
  - CFG strength vs. diversity: Higher guidance improves quality but may reduce diversity
  - Gumbel temperature vs. prediction sharpness: Higher temperature increases diversity but may reduce quality

- Failure signatures:
  - High reconstruction FID indicates issues with the quantization or prediction process
  - Mode collapse or repetitive patterns suggest problems with the diffusion process or token prediction
  - Training instability or slow convergence may indicate issues with the loss function alignment or transformer architecture

- First 3 experiments:
  1. Vary the number of diffusion steps T (6, 8, 10, 12) and measure reconstruction FID to find the optimal balance between quality and efficiency
  2. Test different variance schedules (sin, linear, pow) for the diffusion process and evaluate their impact on generation quality and diversity
  3. Compare the effects of different CFG guidance strengths during inference to optimize the trade-off between quality and diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Limited direct experimental validation for core mechanisms compared to established continuous diffusion models
- Claims about computational efficiency and unified optimization with other discrete tokens lack independent verification
- The superiority of the pow variance schedule over other schedules is not well-supported by provided evidence

## Confidence
- **High confidence**: The basic framework combining VQ-VAE with recurrent token prediction is technically sound and the reported ImageNet results (FID 2.56-3.68) are plausible given the methodology.
- **Medium confidence**: The computational efficiency claims (10 steps, parallel token prediction) are supported by the ablation studies but lack independent benchmarks against established models.
- **Low confidence**: The claims about unified optimization with other discrete tokens and the superiority of the pow variance schedule are not well-supported by the provided evidence.

## Next Checks
1. **Independent Efficiency Verification**: Replicate the 10-step inference process and measure actual computational costs (FLOPs, memory usage) compared to standard diffusion models running 100+ steps, using identical hardware and batch sizes.

2. **Discrete vs. Continuous Information Capture**: Conduct controlled experiments comparing image reconstruction quality from the same latent representations using RDPM's discrete quantization versus direct continuous decoding from the VAE, measuring both perceptual quality and information-theoretic metrics.

3. **Loss Function Alignment Validation**: Test the cross-entropy loss approach with alternative variance schedules (sin, linear) while keeping all other factors constant, to verify whether the pow schedule's superiority is genuinely due to the loss alignment or other factors.