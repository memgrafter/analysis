---
ver: rpa2
title: Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL
arxiv_id: '2410.01930'
source_url: https://arxiv.org/abs/2410.01930
tags:
- baseline
- experts
- expert
- okenizedavg
- okenizedsum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why soft mixtures of experts (SoftMoEs)
  improve performance in deep reinforcement learning. The authors conduct extensive
  experiments to isolate the contributions of various SoftMoE components, including
  tokenization, expert specialization, architectural dimensions, and the use of multiple
  experts.
---

# Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL

## Quick Facts
- arXiv ID: 2410.01930
- Source URL: https://arxiv.org/abs/2410.01930
- Authors: Ghada Sokar; Johan Obando-Ceron; Aaron Courville; Hugo Larochelle; Pablo Samuel Castro
- Reference count: 40
- One-line primary result: Tokenization—not expert specialization—is the key driver of SoftMoE performance gains in deep RL

## Executive Summary
This paper investigates why soft mixtures of experts (SoftMoEs) improve performance in deep reinforcement learning. Through extensive experiments, the authors isolate the contributions of various SoftMoE components and surprisingly find that tokenization—rather than the use of multiple experts—is the primary factor driving performance gains. Specifically, tokenizing the encoder output (rather than flattening it) significantly improves performance, and a single scaled expert with tokenization performs comparably to multiple experts. The results suggest that the common practice of flattening encoder outputs in RL is suboptimal and that maintaining spatial structure through tokenization is crucial.

## Method Summary
The authors conduct experiments using Rainbow and DER agents on the Arcade Learning Environment (ALE) benchmark, with 5 independent seeds for each experiment. They implement SoftMoE architectures with various tokenization schemes (PerConv, PerFeat, PerPatch, Shuffled) and routing strategies (expert choice, token choice). The experiments include varying numbers of experts, architectural dimensions, and expert utilization techniques, with performance evaluated using human-normalized aggregated interquartile mean (IQM), regular mean, median, and optimality gap metrics with 95% stratified bootstrap confidence intervals.

## Key Results
- Tokenizing the encoder output (rather than flattening it) significantly improves performance in deep RL
- A single scaled expert with tokenization performs comparably to multiple experts
- The common practice of flattening multi-dimensional outputs of convolutional encoders is ineffective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization preserves spatial structure learned by convolutional encoders
- Mechanism: Instead of flattening the 3D encoder output into a 1D vector, tokenization maintains the (height, width, depth) tensor structure as separate tokens, allowing the model to retain spatial relationships learned by convolutional layers
- Core assumption: The spatial structure captured by convolutional encoders contains important information for downstream value prediction that is lost when flattening
- Evidence anchors:
  - [abstract]: "tokenizing the encoder output (rather than flattening it) significantly improves performance"
  - [section]: "this finding has broader implications for deep RL agents trained on pixel-based environments, suggesting that the common practice of flattening the multi-dimensional outputs of the convolutional encoders is ineffective"
  - [corpus]: Weak - no direct corpus evidence supporting spatial structure preservation
- Break condition: When spatial relationships in input frames are not important for the task, or when using architectures without convolutional encoders

### Mechanism 2
- Claim: Combined tokenization (weighted averaging of all tokens) is more effective than sparse routing
- Mechanism: SoftMoE uses learned weights to create weighted combinations of all tokens for each expert slot, rather than assigning individual tokens to specific experts, leading to better utilization of information across the entire feature map
- Core assumption: The optimal processing of tokens requires considering all tokens simultaneously rather than selecting subsets
- Evidence anchors:
  - [abstract]: "we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization"
  - [section]: "using combined tokens with a single expert brings the performance even closer to the multiple expert setting"
  - [corpus]: Weak - no direct corpus evidence comparing combined vs sparse tokenization
- Break condition: When token independence is high or when sparse routing mechanisms are specifically optimized for the task

### Mechanism 3
- Claim: Maintaining encoder output dimensions through projection layers preserves learned representations
- Mechanism: SoftMoE includes projection layers within each expert to maintain the original token shape, preventing information loss that occurs when dimensionality is reduced
- Core assumption: The dimensionality reduction from encoder output to value prediction is a bottleneck that loses important information
- Evidence anchors:
  - [abstract]: "Obando Ceron* et al. (2024) add a projection layer after the 'combine' step (ΦC in Figure 2)"
  - [section]: "SoftMoE models have an additional projection layer within each expert to maintain input token size"
  - [corpus]: Weak - no direct corpus evidence about projection layer effectiveness
- Break condition: When the reduction in dimensionality is not information-losing or when alternative architectures handle this better

## Foundational Learning

- Concept: Reinforcement Learning basics (Q-learning, value functions)
  - Why needed here: The paper builds on value-based RL algorithms like Rainbow and DQN, requiring understanding of how value functions work
  - Quick check question: What is the difference between Q(s,a) and V(s) in reinforcement learning?

- Concept: Neural network architectures for RL (CNN encoders, flattening operations)
  - Why needed here: The paper contrasts traditional flattening approaches with tokenization, requiring understanding of how encoder outputs are typically processed
  - Quick check question: Why do RL agents for Atari games typically use convolutional layers as their first processing stage?

- Concept: Mixture of Experts (MoE) architecture and routing mechanisms
  - Why needed here: The paper investigates SoftMoEs and compares different routing strategies (expert choice vs token choice)
  - Quick check question: How does the expert choice routing mechanism differ from token choice routing in MoE architectures?

## Architecture Onboarding

- Component map: Rainbow agent -> Impala/CNN encoder -> tokenization layer -> SoftMoE module -> projection layers -> Q-value output
- Critical path: Input frames -> Convolutional encoder -> Tokenization -> SoftMoE routing/combination -> Expert processing -> Projection -> Q-values
- Design tradeoffs: Single scaled expert vs multiple experts (parameter efficiency vs potential specialization), PerConv vs PerFeat tokenization (spatial structure preservation vs feature aggregation)
- Failure signatures: Performance degradation when flattening encoder outputs, lack of improvement with multiple experts, inconsistent results across different RL algorithms
- First 3 experiments:
  1. Replace flattening with PerConv tokenization in baseline Rainbow agent
  2. Implement single expert SoftMoE with combined tokenization
  3. Compare PerConv, PerFeat, and shuffled tokenization variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes beyond tokenization are necessary to improve expert utilization in SoftMoE models for reinforcement learning?
- Basis in paper: [inferred] The paper demonstrates that experts in SoftMoE are largely redundant and that techniques like resets and shrink-and-perturb show inconsistent results across different agents (Rainbow vs. DER).
- Why unresolved: While the paper explores some techniques for improving expert utilization, it does not identify which architectural modifications are most effective for ensuring diverse expert contributions.
- What evidence would resolve it: Comparative experiments testing various architectural modifications (e.g., different routing strategies, expert architectures, or regularization methods) with systematic analysis of expert activation patterns and performance impacts.

### Open Question 2
- Question: Why does the performance benefit of tokenization not extend to DQN, while it works for Rainbow-lite and DER?
- Basis in paper: [explicit] The paper notes that SoftMoE-1 with DQN does not show significant improvements, while it does for Rainbow-lite and DER, suggesting categorical losses may play a complementary role.
- Why unresolved: The paper does not investigate the interaction between tokenization and the specific loss functions or network architectures used in DQN versus Rainbow/DER.
- What evidence would resolve it: Experiments comparing tokenization effects across different loss functions (Huber vs. MSE), with and without distributional components, and varying network architectures to isolate the contributing factors.

### Open Question 3
- Question: How does the spatial structure preservation in tokenization contribute to the performance gains, and what are the minimal requirements for this structure?
- Basis in paper: [explicit] The paper shows that maintaining spatial structure (PerConv) outperforms shuffling (Shuffled) and that PerPatch (with reduced dimensionality) is close to PerConv, suggesting structure matters but with some flexibility.
- Why unresolved: While the paper demonstrates that spatial structure is important, it does not quantify how much structure is necessary or identify the specific aspects of spatial relationships that drive performance.
- What evidence would resolve it: Systematic ablation studies varying the degree of spatial information preservation, analyzing performance with different tokenization schemes that preserve varying levels of spatial relationships.

## Limitations
- The experiments focus primarily on Atari games and a subset of Procgen environments, limiting conclusions about other RL domains
- The study uses a specific MoE implementation (SoftMoE) and may not generalize to other MoE variants or routing strategies
- The analysis doesn't fully explore the interaction between tokenization and other architectural components like attention mechanisms or different encoder architectures

## Confidence
- **High Confidence**: The empirical demonstration that tokenization improves performance over flattening, and that single expert SoftMoE with tokenization performs comparably to multi-expert variants
- **Medium Confidence**: The specific mechanism explanation (spatial structure preservation) and the claim that tokenization is the "key" driver rather than expert specialization
- **Low Confidence**: The broader implications for RL architecture design beyond the specific settings tested

## Next Checks
1. **Cross-domain validation**: Test the tokenization hypothesis on non-visual RL tasks (e.g., continuous control, language-based environments) to assess generalizability
2. **Architecture ablation**: Isolate the contribution of tokenization from other architectural choices by testing on different backbone networks (non-convolutional encoders)
3. **Routing mechanism comparison**: Systematically compare combined tokenization against sparse routing approaches under identical conditions to validate the mechanism claims