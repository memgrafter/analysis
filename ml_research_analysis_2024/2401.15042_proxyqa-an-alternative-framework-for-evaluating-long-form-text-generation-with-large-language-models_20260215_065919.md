---
ver: rpa2
title: 'PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation
  with Large Language Models'
arxiv_id: '2401.15042'
source_url: https://arxiv.org/abs/2401.15042
tags:
- parallelism
- language
- llms
- training
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROXYQA introduces a novel framework for evaluating long-form text
  generation by using meta-questions paired with proxy-questions and answers. Instead
  of comparing outputs to a reference, it asks an evaluator to answer proxy-questions
  based solely on the generated text.
---

# PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models

## Quick Facts
- arXiv ID: 2401.15042
- Source URL: https://arxiv.org/abs/2401.15042
- Reference count: 27
- Primary result: Evaluates long-form text generation via proxy-questions answered by evaluators using only generated text as context, showing high self-consistency and human alignment.

## Executive Summary
PROXYQA introduces a novel framework for evaluating long-form text generation by LLMs, using meta-questions paired with proxy-questions and answers. Instead of comparing outputs to a reference, it asks an evaluator to answer proxy-questions based solely on the generated text. Performance is measured by how accurately the evaluator answers these questions, indicating the text's informativeness. Experiments with various LLMs show that web-augmented models like New Bing outperform others, especially on harder, non-Wikipedia-based questions. Human evaluation confirms PROXYQA is highly self-consistent (88% agreement) and closely aligned with human judgment (66% agreement with majority human preference). The framework eliminates the need for gold references and reduces data contamination risks.

## Method Summary
PROXYQA comprises in-depth human-curated meta-questions, each accompanied by specific proxy-questions with pre-annotated answers. The framework assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions. Unlike previous datasets compiled from online sources that risk embedding answers used in training LLMs, all the proxy-questions and answers are hidden to the public, thereby preventing data leakage. The method uses GPT-4 and GPT-4-turbo as evaluators, with results aggregated to improve robustness. The proxy-questions are simple booleans to ensure reliability, and the evaluation is performed by having the evaluator answer these questions based only on the generated long-form text as context.

## Key Results
- New Bing (web-augmented) outperforms other LLMs on PROXYQA, especially on harder, non-Wikipedia-based questions.
- PROXYQA shows high self-consistency (88% agreement) and strong alignment with human judgment (66% agreement with majority human preference).
- The framework effectively avoids reference-based metrics and data contamination risks by using hidden, curated proxy-questions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating long-form text generation through proxy-questions reduces reliance on reference-based metrics.
- Mechanism: Instead of comparing generated text to a gold reference, the framework uses a series of simple proxy-questions whose answers are known. The evaluator (human or LLM) answers these questions using only the generated text as context. The accuracy of these answers reflects the informativeness and coverage of the generated content.
- Core assumption: If a generated text is informative, it will contain enough information to answer basic proxy-questions about its content.
- Evidence anchors: [abstract] "PROXYQA comprises in-depth human-curated meta-questions... each accompanied by specific proxy-questions with pre-annotated answers... assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions." [section 3.2] "Each annotated response is provided in a boolean format, ensuring that evaluators can effortlessly answer these proxy-questions, given a sufficiently high-quality generated context."
- Break condition: If proxy-questions are too complex or multi-hop, evaluators may struggle even with good content, reducing reliability.

### Mechanism 2
- Claim: Using hidden proxy-questions prevents data contamination from model training.
- Mechanism: The proxy-questions and answers are curated by experts and kept hidden from public access. This ensures that models cannot retrieve the answers from training data, unlike benchmarks built from public datasets like Wikipedia or Reddit.
- Core assumption: If the evaluation questions are not exposed during training, models must generate informative content rather than copy or retrieve answers.
- Evidence anchors: [abstract] "Unlike previous datasets compiled from online sources that risk embedding answers used in training LLMs... all the proxy-questions and answers are hidden to the public, thereby preventing data leakage." [section 3.3] "all the proxy-questions and answers are hidden to the public, thereby preventing data leakage."
- Break condition: If proxy-questions are leaked or become publicly known, contamination risk returns.

### Mechanism 3
- Claim: Ensemble evaluation of multiple LLMs improves robustness of assessment.
- Mechanism: The evaluation uses multiple GPT-based evaluators (GPT-4 and GPT-4-turbo) to answer proxy-questions independently. Their results are aggregated to reduce evaluator-specific bias and improve consistency.
- Core assumption: Different evaluators may interpret the same content differently; combining them smooths out individual biases.
- Evidence anchors: [section 4.2] "We ensemble the evaluation results from GPT-4 and GPT-4-Turbo to reinforce the reliability and robustness of the assessment." [section 5.2] "Self-agreement: Given a pair of meta-question and its corresponding generated report, each evaluation method is required to score and vote the preferred reports n times... the self-agreement rate is calculated."
- Break condition: If evaluators are highly correlated or biased in the same way, ensemble gains diminish.

## Foundational Learning

- Concept: Understanding of long-context language models and their evaluation challenges.
  - Why needed here: The framework evaluates long-form text generation, which requires handling long context windows and assessing informativeness over extensive outputs.
  - Quick check question: What is the difference between long-context understanding and long-form text generation?

- Concept: Basics of information retrieval and factuality checking.
  - Why needed here: Proxy-questions act like retrieval queries; evaluators must verify if generated content contains specific facts.
  - Quick check question: How does a boolean proxy-question differ from a multi-hop reasoning question?

- Concept: Principles of dataset construction and contamination avoidance.
  - Why needed here: The framework avoids public data contamination by curating hidden questions; understanding this is key to its design.
  - Quick check question: Why might using public datasets for evaluation lead to inflated model performance?

## Architecture Onboarding

- Component map:
  - Meta-questions -> Proxy-questions -> Evaluator -> Accuracy metric -> Dataset

- Critical path:
  1. Submit meta-question to LLM to generate long-form text.
  2. Feed generated text as context to evaluator.
  3. Evaluator answers all proxy-questions based on context.
  4. Calculate accuracy score.
  5. Aggregate scores across multiple runs or evaluators.

- Design tradeoffs:
  - Simplicity vs. depth: Proxy-questions are simple booleans to ensure reliability but may miss nuanced understanding.
  - Hidden vs. public: Hidden questions prevent contamination but limit transparency and reproducibility.
  - Human vs. automated: Human evaluation is more reliable but costly; automated evaluation is scalable but may have consistency issues.

- Failure signatures:
  - Low accuracy even with high-quality text: Proxy-questions may be misaligned or too specific.
  - High variance across evaluators: Proxy-questions may be ambiguous or context-dependent.
  - Low self-agreement: Evaluator bias or inconsistency in answering proxy-questions.

- First 3 experiments:
  1. Run baseline evaluation with GPT-4 only on a small subset of meta-questions; measure accuracy and self-agreement.
  2. Compare GPT-4 vs. human evaluator agreement on same generated texts.
  3. Test with GPT-4-turbo added to ensemble; measure improvement in self-agreement and correlation with human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of web-augmented LLMs like New Bing compare to traditional non-augmented models on PROXY QA, and what factors contribute to any observed differences?
- Basis in paper: [explicit] The paper discusses the performance of web-augmented LLMs like New Bing and compares them to non-augmented models on PROXY QA.
- Why unresolved: The paper provides performance metrics but does not deeply analyze the factors contributing to the differences in performance.
- What evidence would resolve it: A detailed analysis of the web-augmented features and their impact on model performance, including a comparison of specific tasks where web-augmented models excel.

### Open Question 2
- Question: What are the limitations of using proxy questions for evaluating the informativeness of long-form text generation, and how might these limitations affect the assessment of LLMs?
- Basis in paper: [inferred] The paper introduces proxy questions as a method for evaluating text generation but does not extensively discuss potential limitations.
- Why unresolved: The effectiveness of proxy questions is demonstrated, but the paper does not explore potential biases or gaps in this evaluation method.
- What evidence would resolve it: Empirical studies comparing proxy question-based evaluations with other evaluation methods, identifying specific scenarios where proxy questions might fall short.

### Open Question 3
- Question: How does the iterative annotation process in PROXY QA influence the quality and consistency of the meta and proxy questions, and what impact does this have on the overall evaluation?
- Basis in paper: [explicit] The paper mentions the iterative annotation process for meta and proxy questions but does not provide a detailed analysis of its impact.
- Why unresolved: The paper describes the process but lacks a thorough examination of how it affects the evaluation's reliability and validity.
- What evidence would resolve it: Comparative studies of evaluations conducted with and without iterative annotation, measuring differences in consistency and quality of results.

### Open Question 4
- Question: In what ways do domain-specific challenges affect the performance of LLMs on PROXY QA, and how can these challenges be addressed to improve model evaluation?
- Basis in paper: [explicit] The paper discusses performance differences across domains but does not explore strategies for addressing domain-specific challenges.
- Why unresolved: While domain performance is analyzed, the paper does not propose solutions for improving model evaluation in challenging domains.
- What evidence would resolve it: Development and testing of domain-specific evaluation strategies or enhancements to the PROXY QA framework that address identified challenges.

### Open Question 5
- Question: How does the length of generated content influence the accuracy of proxy question evaluations, and what implications does this have for assessing the informativeness of LLM outputs?
- Basis in paper: [inferred] The paper mentions generation length but does not explore its relationship with evaluation accuracy.
- Why unresolved: The paper provides data on generation length but lacks an analysis of its impact on evaluation outcomes.
- What evidence would resolve it: Studies correlating generation length with evaluation accuracy, identifying optimal content lengths for informative and accurate LLM outputs.

## Limitations
- The framework's reliance on hidden proxy-questions limits reproducibility and external validation.
- Boolean proxy-questions may oversimplify complex long-form content and miss nuanced understanding.
- Reported self-consistency (88%) and human alignment (66%) are based on internal evaluations with limited sample sizes.

## Confidence
- **High confidence**: The core mechanism of using proxy-questions to assess informativeness is well-founded and supported by evidence.
- **Medium confidence**: The reported self-consistency (88%) and human alignment (66%) are promising but based on internal evaluations with limited sample sizes.
- **Low confidence**: The exact impact of hidden questions on reproducibility and external validation is unclear due to lack of transparency.

## Next Checks
1. Reproduce with Public Proxy-Questions: Create a small public dataset of meta-questions and proxy-questions to test if similar results are achievable with transparent, reproducible evaluation.
2. Cross-Evaluator Robustness: Test PROXYQA with non-GPT evaluators (e.g., Claude, Llama) to assess whether ensemble gains hold across diverse models.
3. Generalization Study: Apply PROXYQA to a different domain (e.g., scientific or technical writing) to evaluate its adaptability beyond the original curated dataset.