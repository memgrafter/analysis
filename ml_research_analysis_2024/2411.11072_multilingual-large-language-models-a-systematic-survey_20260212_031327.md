---
ver: rpa2
title: 'Multilingual Large Language Models: A Systematic Survey'
arxiv_id: '2411.11072'
source_url: https://arxiv.org/abs/2411.11072
tags:
- arxiv
- language
- multilingual
- mllms
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of multilingual large
  language models (MLLMs), covering their architectures, training methodologies, and
  evaluation benchmarks. It discusses the challenges of handling linguistic diversity
  and proposes strategies for data construction, model tuning, and alignment.
---

# Multilingual Large Language Models: A Systematic Survey

## Quick Facts
- arXiv ID: 2411.11072
- Source URL: https://arxiv.org/abs/2411.11072
- Authors: Shaolin Zhu; Supryadi; Shaoyang Xu; Haoran Sun; Leiyu Pan; Menglong Cui; Jiangcun Du; Renren Jin; AntÃ³nio Branco; Deyi Xiong
- Reference count: 40
- Primary result: Comprehensive survey of MLLMs covering architectures, training methodologies, evaluation benchmarks, and applications across domains

## Executive Summary
This survey provides a structured taxonomy of multilingual large language models across six interconnected domains: foundational multilingual data, neural architectures, pre-training and fine-tuning methodologies, evaluation benchmarks, interpretability techniques, and real-world applications. The paper addresses core challenges in handling linguistic diversity while emphasizing responsible AI practices including fairness, bias mitigation, and interpretability. Applications in biology, medicine, computer science, mathematics, and law demonstrate the potential of MLLMs to drive innovation across diverse fields.

## Method Summary
The survey synthesizes findings from 40 references to create a comprehensive overview of MLLM research. While not a model training procedure, it follows a systematic approach of gathering and categorizing research papers across six domains, analyzing key trends and challenges, and synthesizing findings into an organized framework. The methodology emphasizes connecting foundational elements with technical methodologies, evaluation approaches, interpretability techniques, and practical applications to provide a holistic view of the MLLM landscape.

## Key Results
- Systematic organization of MLLM research into six interconnected domains covering the complete development lifecycle
- Comprehensive coverage of multilingual data construction strategies, neural architectures, and training methodologies
- Discussion of responsible AI practices including fairness, bias mitigation, and interpretability throughout the MLLM development process
- Showcase of diverse real-world applications across biology, medicine, computer science, mathematics, and law

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey successfully provides a comprehensive overview of MLLMs by organizing the research landscape into six interconnected domains.
- Mechanism: The structured taxonomy systematically covers foundational multilingual data, neural architectures, pre-training and fine-tuning methodologies, evaluation benchmarks, interpretability techniques, and real-world applications.
- Core assumption: A comprehensive survey must address foundational elements, technical methodologies, evaluation, interpretability, and practical applications in a connected manner.
- Evidence anchors:
  - [abstract] "We will discuss data construction strategies, model training and fine-tuning approaches, and core Responsible AI issues such as fairness, bias and toxicity in the context of MLLMs."
  - [section] "This paper adopts a structured taxonomy, organizing the research landscape into six fundamental and interconnected domains."
- Break Condition: If any of the six domains is missing or poorly connected, the survey fails to provide a comprehensive overview.

### Mechanism 2
- Claim: The survey effectively highlights the importance of responsible AI practices in MLLMs.
- Mechanism: It addresses core Responsible AI issues like fairness, bias mitigation, and interpretability throughout the discussion of data construction, model training, evaluation, and applications.
- Core assumption: Responsible AI practices are integral to the development and deployment of MLLMs.
- Evidence anchors:
  - [abstract] "The survey highlights the importance of responsible AI practices, including fairness, bias mitigation, and interpretability."
  - [section] "Building upon these foundations, this survey addresses the methodologies for pre-training and fine-tuning MLLMs, discussing various objectives like masked language modeling and translation language modeling, as well as fine-tuning techniques like instruction tuning and preference tuning."
- Break Condition: If responsible AI practices are not adequately addressed in any of the discussed areas, the survey fails to highlight their importance.

### Mechanism 3
- Claim: The survey showcases the potential of MLLMs across diverse domains.
- Mechanism: It provides a comprehensive review of real-world applications of MLLMs in domains such as biology, medicine, computer science, mathematics, and law.
- Core assumption: Demonstrating real-world applications is crucial for showcasing the potential of MLLMs.
- Evidence anchors:
  - [abstract] "Applications of MLLMs across various domains such as biology, medicine, and law are also explored, showcasing their potential to drive innovation and address global challenges."
  - [section] "Finally, the survey showcases diverse real-world applications of MLLMs. Our overarching objective is to address these six fundamental domains, as illustrated in Figure 1."
- Break Condition: If the survey fails to cover a significant number of relevant domains or doesn't provide sufficient detail on the applications, it fails to showcase the potential of MLLMs.

## Foundational Learning

- Concept: Multilingual Corpora
  - Why needed here: Understanding the sources and types of multilingual data is crucial for building effective MLLMs.
  - Quick check question: What are the main sources of multilingual pre-training data for MLLMs?

- Concept: Pre-training Objectives
  - Why needed here: Pre-training objectives determine how MLLMs learn from multilingual data.
  - Quick check question: What are the key differences between pre-training objectives for monolingual and multilingual LLMs?

- Concept: Multilingual Tuning Strategies
  - Why needed here: Tuning strategies adapt the general capabilities of MLLMs to specific objectives.
  - Quick check question: What are the main approaches for multilingual tuning, and how do they differ?

## Architecture Onboarding

- Component map: MLLMs typically adopt either the decoder-only or encoder-decoder architecture. The main architectural difference between multilingual and monolingual LLMs lies in the vocabulary, which needs to be larger in MLLMs to address the out-of-vocabulary issue.

- Critical path: Data curation (language identification, quality filtering, safety filtering, deduplication, up-sampling) -> Pre-training (data curation, pre-training objectives, pre-training strategies) -> Multilingual tuning (basic tuning strategies, direct multilingual tuning, multilingual tuning augmented by cross-lingual alignment, enhancement of specific multilingual abilities) -> Evaluation (multilingual tokenizer evaluation, multilingual evaluation benchmarks and datasets, multilingualism evaluation, MLLMs as multilingual evaluator) -> Interpretability (interpretability of multilingual capabilities, interpretability of cross-lingual transfer, interpretability of linguistic bias) -> Application (MLLMs for biology and medicine, MLLMs for computer science, MLLMs for mathematics, MLLMs for law)

- Design tradeoffs: Larger vocabulary size for MLLMs can improve multilingual capabilities but also increase computational costs. Balancing data quality and quantity is crucial for effective pre-training.

- Failure signatures: Poor performance on low-resource languages, bias towards high-resource languages, lack of cultural sensitivity, safety concerns, and lack of interpretability.

- First 3 experiments:
  1. Evaluate the performance of different MLLM architectures on a multilingual benchmark.
  2. Investigate the impact of data curation techniques on the quality and safety of multilingual pre-training data.
  3. Explore the effectiveness of different multilingual tuning strategies for enhancing specific multilingual abilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for allocating multilingual instruction data to maximize cross-lingual transfer across diverse language families?
- Basis in paper: [explicit] The paper discusses multilingual instruction tuning and cross-lingual transfer elicitation, noting that factors like source language selection, linguistic relationships, number of languages, and data volume influence transfer. It suggests that simply adding a small number of multilingual samples can improve performance.
- Why unresolved: While the paper identifies factors influencing transfer, it does not provide a definitive framework for optimal data allocation across languages with varying resource levels and linguistic distances.
- What evidence would resolve it: Empirical studies comparing different allocation strategies (e.g., uniform distribution vs. focusing on linguistically related languages vs. prioritizing low-resource languages) across diverse language families and downstream tasks.

### Open Question 2
- Question: How can MLLMs be effectively adapted to new languages without requiring extensive vocabulary expansion and continued pre-training?
- Basis in paper: [explicit] The paper discusses adaptation to new languages, mentioning strategies like vocabulary expansion and continual pre-training. It also mentions alternative approaches like romanization and adapter-based methods, but notes the computational costs and trade-offs involved.
- Why unresolved: The paper highlights the challenges and trade-offs but does not provide a clear recommendation for the most efficient and effective approach, especially for low-resource languages.
- What evidence would resolve it: Comparative studies evaluating the performance and efficiency of different adaptation strategies (e.g., vocabulary expansion vs. romanization vs. adapter-based methods) across a range of new languages with varying resource levels.

### Open Question 3
- Question: What is the most effective way to integrate cultural knowledge into MLLMs to ensure culturally-aware and adaptive behavior across diverse language communities?
- Basis in paper: [explicit] The paper discusses the importance of cultural adaptation and mentions efforts to assess and enrich MLLMs' cultural and value diversity. It also mentions training on culturally diverse datasets and developing evaluation metrics that capture cultural nuances.
- Why unresolved: While the paper acknowledges the importance of cultural awareness, it does not provide a concrete methodology for integrating cultural knowledge into MLLMs or evaluating their cultural competence.
- What evidence would resolve it: Studies developing and evaluating methods for incorporating cultural knowledge into MLLMs, such as culturally-specific instruction tuning, cultural knowledge graphs, or culturally-aware reward models, and assessing their impact on downstream tasks and user experience across diverse cultural contexts.

## Limitations

- Limited reference corpus: The analysis relies on 40 references, which may not capture the full breadth of rapidly evolving MLLM research.
- Surface-level treatment of critical issues: The discussion of responsible AI practices lacks quantitative depth, particularly regarding bias mitigation and fairness mechanisms.
- Qualitative rather than quantitative applications: The applications section lacks quantitative evaluation of MLLM performance across different domains.

## Confidence

- High: The survey's organizational framework and coverage of foundational topics (data construction, architectures, pre-training) are well-established and clearly presented.
- Medium: The discussion of multilingual tuning strategies and evaluation benchmarks is comprehensive but may not fully capture emerging methodologies.
- Low: The treatment of responsible AI practices and real-world applications lacks quantitative depth and may not fully address the complexity of these issues.

## Next Checks

1. Conduct a citation network analysis to identify additional key papers that may have been missed in the initial 40-reference corpus, particularly focusing on recent preprints and conference proceedings.

2. Perform a systematic comparison of MLLM performance across the domains mentioned (biology, medicine, law) using standardized benchmarks to validate the claimed capabilities.

3. Analyze the survey's treatment of bias and fairness issues by comparing it against recent research on multilingual toxicity detection and cultural bias mitigation strategies.