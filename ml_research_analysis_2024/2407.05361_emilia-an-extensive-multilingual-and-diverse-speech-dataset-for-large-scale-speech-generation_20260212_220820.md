---
ver: rpa2
title: 'Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale
  Speech Generation'
arxiv_id: '2407.05361'
source_url: https://arxiv.org/abs/2407.05361
tags:
- speech
- data
- emilia
- dataset
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Emilia, the first large-scale, multilingual,
  and diverse speech dataset for speech generation, containing over 101k hours of
  speech data across six languages. To efficiently preprocess raw in-the-wild speech
  data into high-quality training data, the authors present Emilia-Pipe, an open-source
  preprocessing pipeline that integrates source separation, speaker diarization, fine-grained
  segmentation, ASR, and filtering.
---

# Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation

## Quick Facts
- **arXiv ID**: 2407.05361
- **Source URL**: https://arxiv.org/abs/2407.05361
- **Reference count**: 24
- **Primary result**: Emilia dataset contains over 101k hours of multilingual speech across six languages with quality comparable to audiobook datasets

## Executive Summary
This paper introduces Emilia, the first large-scale, multilingual speech dataset containing over 101k hours of spontaneous speech across six languages (English, Chinese, German, French, Japanese, Korean). To efficiently preprocess raw in-the-wild speech data into high-quality training data, the authors present Emilia-Pipe, an open-source preprocessing pipeline that integrates source separation, speaker diarization, fine-grained segmentation, ASR, and filtering. The dataset achieves a DNSMOS P.835 OVRL score of 3.26, comparable to high-quality audiobook datasets. Evaluations on TTS tasks show that models trained on Emilia achieve similar intelligibility and speaker similarity to models trained on MLS, with improved naturalness on spontaneous speech.

## Method Summary
The Emilia dataset was created using Emilia-Pipe, a preprocessing pipeline that transforms raw in-the-wild speech data into high-quality training data. The pipeline consists of six stages: standardization (format conversion and sampling rate adjustment), source separation (removing background noise/music using Ultimate Vocal Remover), speaker diarization (isolating single-speaker segments using pyannote.audio), fine-grained segmentation (creating 3-30 second utterances with Silero-VAD), ASR (generating text annotations with WhisperX), and filtering (removing low-quality samples using DNSMOS and other criteria). The pipeline processes approximately 2.50 hours of data per minute and outputs data in ESPnet2 format.

## Key Results
- Emilia dataset contains over 101k hours of speech data across six languages with DNSMOS P.835 OVRL score of 3.26
- TTS models trained on Emilia achieve similar intelligibility and speaker similarity to MLS models, with improved naturalness on spontaneous speech
- Multilingual TTS models trained on full Emilia dataset demonstrate strong performance across all six languages
- Emilia-Pipe processes 2.50 hours of data per minute, enabling efficient creation of large-scale speech datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emilia-Pipe's multi-stage preprocessing transforms noisy in-the-wild data into high-quality training data suitable for large-scale speech generation models
- Mechanism: Each preprocessing step addresses specific degradation modes: source separation removes background noise/music, speaker diarization isolates single-speaker segments, fine-grained segmentation ensures manageable utterance lengths, ASR provides necessary text annotations, and filtering removes low-quality samples
- Core assumption: Each preprocessing component operates with sufficient accuracy to preserve speech quality while removing degradation sources
- Evidence anchors: Processing results show average DNSMOS P.835 OVRL score improves to 3.26; however, direct comparison of quality before/after specific preprocessing stages is weak

### Mechanism 2
- Claim: Emilia dataset's diversity in speaking styles enables training of speech generation models that produce more natural, human-like output compared to models trained on formal audiobook datasets
- Mechanism: Training data diversity exposes the model to the full distribution of human speech patterns, allowing it to learn to generate these patterns rather than defaulting to formal, robotic speech
- Core assumption: Model capacity is sufficient to learn from the diverse patterns present in the training data
- Evidence anchors: Dataset exhibits broader dispersion compared to MLS clustering; corpus shows related work on spontaneous speech datasets supports importance of this claim

### Mechanism 3
- Claim: Emilia-Pipe's efficiency enables scaling to create datasets an order of magnitude larger than previous spontaneous speech datasets
- Mechanism: Engineering optimizations (batched inference, parallel processing, bypassing redundant components) reduce per-hour processing time, making it feasible to process hundreds of thousands of hours of raw data
- Core assumption: Efficiency gains scale linearly with data volume without hitting computational bottlenecks
- Evidence anchors: Pipeline processes 2.50 hours per minute; however, direct comparison of processing speeds with competing pipelines is weak

## Foundational Learning

- **Speech preprocessing pipeline design**: Understanding how each preprocessing component addresses specific data quality issues is critical for troubleshooting and extending the pipeline
  - *Why needed*: Each component targets specific degradation modes in raw speech data
  - *Quick check*: What is the primary purpose of source separation in Emilia-Pipe, and what happens if it fails to isolate vocals properly?

- **Multilingual speech processing**: Emilia covers six languages, requiring understanding of language-specific challenges in preprocessing, ASR, and TTS
  - *Why needed*: Different languages present unique challenges in speech processing
  - *Quick check*: How does Emilia-Pipe handle language identification, and what filtering criteria are applied based on language confidence scores?

- **Speech quality evaluation metrics**: Evaluating dataset quality requires understanding of DNSMOS P.835 OVRL and other metrics used in the paper
  - *Why needed*: Quality assessment is crucial for dataset validation and comparison
  - *Quick check*: What does a DNSMOS P.835 OVRL score of 3.26 indicate about speech quality, and how does this compare to studio-recorded datasets?

## Architecture Onboarding

- **Component map**: Raw data ingestion → Standardization → Source separation → Speaker diarization → Fine-grained segmentation → ASR → Filtering → Final dataset
- **Critical path**: Source separation → Speaker diarization → Filtering (quality assessment)
  - These components most directly impact final dataset quality
  - ASR can be considered separately as it affects annotation availability rather than audio quality
- **Design tradeoffs**:
  - Speed vs. accuracy: Whisper-Medium chosen over Whisper-Large for 4x faster processing with minimal accuracy loss
  - Language coverage vs. quality: Language filtering with 80% confidence threshold balances dataset size with relevance
  - Segmentation granularity vs. utterance utility: 3-30 second segments balance manageability with context preservation
- **Failure signatures**:
  - Source separation failure: Remaining music/noise in audio, detected by high DNSMOS scores after filtering
  - Speaker diarization failure: Multiple speakers in single segment, detected during manual quality checks
  - ASR hallucination: Transcript-text mismatch, detected by comparing ASR output to reference transcripts on validation set
  - Filtering too aggressive: Dataset size significantly smaller than expected, detected by comparing raw vs. processed durations
- **First 3 experiments**:
  1. Process a small subset (1 hour) of raw data through the full pipeline and manually verify output quality at each stage
  2. Compare DNSMOS scores before and after source separation on a held-out validation set
  3. Test language identification accuracy on mixed-language samples to validate the 80% confidence threshold

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Emilia dataset's diversity in spontaneous speech styles specifically impact the naturalness of generated speech compared to controlled datasets like MLS in real-world applications?
- **Basis in paper**: [explicit] The paper highlights that Emilia includes diverse spontaneous speech styles and shows improved naturalness on spontaneous speech in evaluations
- **Why unresolved**: While the paper demonstrates improved naturalness on spontaneous speech, it doesn't provide a direct comparison of how well models trained on Emilia perform in real-world spontaneous speech scenarios versus models trained on controlled datasets like MLS
- **What evidence would resolve it**: A study comparing the performance of TTS models trained on Emilia versus MLS in real-world spontaneous speech scenarios, such as generating speech for conversational AI or voice assistants in everyday use

### Open Question 2
- **Question**: What is the long-term scalability and adaptability of the Emilia-Pipe preprocessing pipeline when incorporating new languages or significantly larger datasets?
- **Basis in paper**: [inferred] The paper mentions that Emilia-Pipe is designed for efficiency and supports multiple languages, but it doesn't explore its performance with new languages or much larger datasets
- **Why unresolved**: The paper demonstrates the pipeline's efficiency on the current dataset but doesn't address potential challenges or performance changes when scaling up to include more languages or significantly larger amounts of data
- **What evidence would resolve it**: An analysis of Emilia-Pipe's performance when processing new languages not included in the initial dataset or when handling datasets that are an order of magnitude larger than the current one

### Open Question 3
- **Question**: How do the subjective evaluations of speaker similarity and naturalness in the Emilia dataset compare to other large-scale spontaneous speech datasets beyond MLS?
- **Basis in paper**: [explicit] The paper compares Emilia with MLS in subjective evaluations, showing similar or improved speaker similarity and naturalness
- **Why unresolved**: The evaluation focuses on comparing Emilia with MLS, but there might be other large-scale spontaneous speech datasets that could provide a more comprehensive benchmark for speaker similarity and naturalness
- **What evidence would resolve it**: Subjective evaluations comparing TTS models trained on Emilia with those trained on other large-scale spontaneous speech datasets, assessing speaker similarity and naturalness across a broader range of datasets

## Limitations

- DNSMOS P.835 OVRL score of 3.26 was measured on a subset rather than the full dataset
- Claims about improved naturalness on spontaneous speech compared to MLS lack direct quantitative comparison
- Paper doesn't address potential quality degradation from pipeline efficiency optimizations
- Generalizability of results across all six languages is uncertain as evaluation primarily focused on English

## Confidence

**High Confidence**: Technical feasibility of Emilia-Pipe's multi-stage preprocessing approach, dataset's multilingual coverage (six languages), and core claim of over 101k hours of speech data

**Medium Confidence**: Claims about improved naturalness and diversity compared to existing datasets, specific quality metrics (DNSMOS P.835 OVRL score of 3.26), and assertion that models trained on Emilia achieve similar intelligibility and speaker similarity to MLS-trained models

**Low Confidence**: Claims about relative naturalness improvement over MLS (lacks direct quantitative comparison), scalability benefits of pipeline efficiency (no comparison with alternative approaches), and generalizability of results across all six languages (evaluation primarily focused on English)

## Next Checks

1. **Quality Validation Across Languages**: Process a representative sample from each of the six languages through Emilia-Pipe and conduct human evaluations to verify that the DNSMOS P.835 OVRL score of 3.26 is consistent across all languages, not just English

2. **Direct Comparison with MLS**: Conduct a controlled experiment training identical TTS models on Emilia vs. MLS datasets, then evaluate both models on the same spontaneous speech test set using standardized naturalness metrics (FSD, MOS) to quantify the claimed improvements

3. **Pipeline Robustness Testing**: Create a synthetic dataset with known degradation patterns (varying noise levels, overlapping speech, low-quality recordings) and process it through Emilia-Pipe to measure the accuracy of each component and identify failure modes not captured in the paper's evaluation