---
ver: rpa2
title: 'Supervised Algorithmic Fairness in Distribution Shifts: A Survey'
arxiv_id: '2402.01327'
source_url: https://arxiv.org/abs/2402.01327
tags:
- fairness
- shifts
- shift
- distribution
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of supervised algorithmic
  fairness under distribution shifts, focusing on the challenge of maintaining equitable
  and unbiased predictions when data distributions change from source to target domains.
  The authors categorize existing methods into six main approaches: feature disentanglement,
  data augmentation, causal inference, reweighting, robust optimization, and regularization-based
  methods.'
---

# Supervised Algorithmic Fairness in Distribution Shifts: A Survey

## Quick Facts
- arXiv ID: 2402.01327
- Source URL: https://arxiv.org/abs/2402.01327
- Reference count: 5
- One-line primary result: Comprehensive survey categorizing six main approaches for maintaining algorithmic fairness under distribution shifts

## Executive Summary
This survey addresses the challenge of maintaining equitable and unbiased predictions when data distributions change from source to target domains in supervised learning. The authors systematically categorize existing methods into six main approaches: feature disentanglement, data augmentation, causal inference, reweighting, robust optimization, and regularization-based methods. They provide a taxonomy of distribution shift types and their impact on fairness, compile publicly available datasets for empirical studies, and discuss significant challenges such as fairness under conditional shifts and out-of-distribution detection.

## Method Summary
The survey synthesizes existing research on supervised algorithmic fairness under distribution shifts by organizing methods into six categories. Feature disentanglement and data augmentation methods capture invariance across domains by extracting high-dimensional representations that preserve semantic information unrelated to sensitive attributes. Reweighting techniques adjust sample weights to match target domain distributions, while robust optimization minimizes worst-case loss over perturbations. Causal inference methods analyze causal graphs to identify unfair factors, and regularization-based approaches explicitly penalize dependence between predictions and sensitive attributes. The methods aim to minimize prediction error while mitigating dependence between outcomes and sensitive attributes.

## Key Results
- Six main methodological categories identified for addressing fairness under distribution shifts
- Comprehensive compilation of publicly available datasets including UCI Adult, COMPAS, and Waterbirds
- Taxonomy of distribution shift types and their specific impacts on fairness metrics
- Identification of major challenges including conditional shifts and out-of-distribution detection fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised algorithmic fairness under distribution shifts works by learning invariant representations that remain predictive across domains while being uncorrelated with sensitive attributes.
- Mechanism: Methods like feature disentanglement, data augmentation, and regularization-based approaches extract high-dimensional representations that preserve semantic information unrelated to sensitive attributes, allowing fair decision-making even when distributions shift.
- Core assumption: Semantic information relevant for prediction remains stable across domains while sensitive attribute correlation changes.
- Evidence anchors:
  - [abstract]: "supervised algorithmic fairness under distribution shifts...addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains"
  - [section 2.2]: "methods falling under feature disentanglement and data augmentation focus on capturing invariance across various domains"
- Break condition: When semantic relationships between features and labels change across domains (concept shift).

### Mechanism 2
- Claim: Reweighting and robust optimization methods maintain fairness by adjusting the relative importance of samples or features to match target domain distributions.
- Mechanism: Reweighting techniques assign weights to instances from source domain to represent target domain distribution, while robust optimization minimizes worst-case loss over training subsets or perturbations around data.
- Core assumption: Relationship between source and target distributions can be modeled through weighting or perturbation sets.
- Evidence anchors:
  - [section 3]: "Reweighting approach...is employed to adjust the weights of samples or features to enhance model performance or correct biases in the dataset"
  - [section 3]: "robust optimization...minimize the worst-case loss over various subsets of the training set or other well-defined perturbation sets around the data"
- Break condition: When shift is too complex to model through simple weighting or perturbation sets don't capture true distribution shift.

### Mechanism 3
- Claim: Causal inference methods identify and address unfair factors by analyzing causal graphs and paths that explain how model predictions are generated for different sensitive groups.
- Mechanism: By examining causal graphs and paths, these methods help understand how model predictions for different groups are formed, thereby identifying and addressing potential unfair factors.
- Core assumption: Causal relationships between features, sensitive attributes, and outcomes can be identified and remain partially stable across domains.
- Evidence anchors:
  - [section 3]: "Causal models have been widely applied in machine learning to address issues related to model fairness. Analyzing causal graphs and paths helps understand how the model's predictions for different groups are formed"
  - [section 2.2]: "Examining causal graphs and paths aids in comprehending how the model's predictions for different sensitive groups are generated"
- Break condition: When causal structure itself changes significantly between domains.

## Foundational Learning

- Concept: Distribution shifts and their types
  - Why needed here: Understanding different types of distribution shifts (covariate, label, concept, demographic, dependence) is essential for selecting appropriate fairness preservation methods
  - Quick check question: What distinguishes covariate shift from concept shift in terms of their impact on fairness?

- Concept: Fairness notions (group, individual, counterfactual)
  - Why needed here: Different fairness metrics require different approaches to maintain fairness under distribution shifts
  - Quick check question: How does counterfactual fairness differ from group fairness in terms of what it aims to achieve?

- Concept: Domain adaptation and generalization techniques
  - Why needed here: Many methods for fairness under distribution shifts borrow from domain adaptation literature
  - Quick check question: What is the key difference between domain adaptation and domain generalization in the context of fairness?

## Architecture Onboarding

- Component map: Data preprocessing -> Invariant representation learning -> Fairness regularization -> Model training -> Evaluation
- Critical path: Data preprocessing → Invariant representation learning → Fairness regularization → Model training → Evaluation
- Design tradeoffs:
  - Interpretability vs performance: Feature disentanglement offers better interpretability but may sacrifice some predictive performance
  - Computational cost vs robustness: Robust optimization methods are computationally expensive but provide strong guarantees
  - Domain knowledge vs automation: Causal inference methods require domain expertise but can provide more targeted fairness interventions
- Failure signatures:
  - Degradation in fairness metrics without corresponding drop in accuracy
  - Performance degradation on minority groups
  - High variance in fairness metrics across different target domains
  - Model sensitivity to hyperparameter choices in fairness regularization
- First 3 experiments:
  1. Implement a simple reweighting approach on a tabular dataset with known demographic shift to verify basic functionality
  2. Add feature disentanglement to the pipeline and compare fairness metrics across source and target domains
  3. Test a causal inference-based method on a dataset with known causal structure to validate the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fairness be ensured under conditional shifts where the label distribution changes based on input features, beyond just aligning input feature distributions?
- Basis in paper: [explicit] The paper identifies "Fairness under conditional shift" as a significant challenge, noting that aligning PX only when X is the causative factor of Y may not suffice when Y causes X or when PY|X changes with PX shifts.
- Why unresolved: Existing methods focus on covariate shift and label shift but do not adequately address scenarios where the conditional distribution PY|X itself changes in unknown domains.
- What evidence would resolve it: Empirical studies demonstrating improved fairness metrics (e.g., equalized odds, counterfactual fairness) when applying methods that explicitly account for conditional shifts in PY|X, validated across diverse real-world datasets.

### Open Question 2
- Question: What are effective strategies for maintaining both in-distribution multi-class fairness and out-of-distribution binary-class fairness simultaneously in anomaly detection?
- Basis in paper: [explicit] The paper highlights "Fairness in out-of-distribution detection" as a challenge, noting that existing work ensures fairness during testing but does not address balancing fairness for both in-distribution multi-class classification and out-of-distribution detection.
- Why unresolved: Current approaches focus on either in-distribution fairness or out-of-distribution detection fairness separately, without integrated solutions for scenarios requiring both.
- What evidence would resolve it: Development and validation of a unified framework that achieves comparable fairness metrics (e.g., consistency, equalized opportunity) for both in-distribution and out-of-distribution samples, tested on benchmark datasets with known distribution shifts.

### Open Question 3
- Question: How can the quality of feature disentanglement be effectively evaluated in the context of fairness-aware machine learning under distribution shifts?
- Basis in paper: [explicit] The paper mentions that "directly evaluating the quality of the disentanglement poses challenges," despite its advantages in interpretability and preserving semantic information unrelated to sensitive attributes.
- Why unresolved: While feature disentanglement is used to isolate sensitive attributes from predictive features, there is no standardized or reliable method to assess whether the disentanglement process successfully removes bias without losing relevant information.
- What evidence would resolve it: Introduction of quantitative metrics or evaluation protocols that measure the degree of disentanglement (e.g., mutual information between representations and sensitive attributes) and their impact on fairness metrics, validated across multiple datasets and distribution shift scenarios.

## Limitations

- Limited empirical validation across all proposed approaches without comprehensive benchmarking
- Lack of computational complexity analysis and scalability considerations for real-world deployment
- No standardized evaluation protocol comparing relative effectiveness of different methodological approaches

## Confidence

- High confidence: The categorization of distribution shift types and their relationship to fairness violations
- Medium confidence: The effectiveness of feature disentanglement and data augmentation approaches based on existing literature
- Low confidence: The practical superiority of any specific approach without empirical benchmarking across diverse datasets and shift types

## Next Checks

1. Implement and benchmark all six methodological approaches on a standardized dataset (e.g., Waterbirds) with controlled distribution shifts to compare fairness preservation effectiveness
2. Conduct ablation studies to determine the relative importance of different components within each approach (e.g., disentanglement vs regularization strength)
3. Test the generalization of these methods across multiple domain shift scenarios (e.g., synthetic to real data, different demographic distributions) to validate robustness claims