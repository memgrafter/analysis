---
ver: rpa2
title: Datasets for Multilingual Answer Sentence Selection
arxiv_id: '2406.10172'
source_url: https://arxiv.org/abs/2406.10172
tags:
- datasets
- masnq
- trained
- languages
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of high-quality multilingual datasets
  for Answer Sentence Selection (AS2), a critical component of question-answering
  systems. The authors create new AS2 datasets (mASNQ, mWikiQA, mTREC-QA) by translating
  existing English AS2 datasets into five European languages (French, German, Italian,
  Portuguese, Spanish) using a large translation model and applying heuristics to
  improve translation quality.
---

# Datasets for Multilingual Answer Sentence Selection

## Quick Facts
- arXiv ID: 2406.10172
- Source URL: https://arxiv.org/abs/2406.10172
- Reference count: 10
- New AS2 datasets created by translating English datasets into five European languages

## Executive Summary
This work addresses the lack of high-quality multilingual datasets for Answer Sentence Selection (AS2), a critical component of question-answering systems. The authors create new AS2 datasets (mASNQ, mWikiQA, mTREC-QA) by translating existing English AS2 datasets into five European languages (French, German, Italian, Portuguese, Spanish) using a large translation model and applying heuristics to improve translation quality. They evaluate their datasets by training multilingual transformer models, achieving strong performance across all target languages. Results show that models trained on the translated datasets perform comparably to English baselines, with MAP scores ranging from 0.836-0.904 and P@1 scores from 0.752-0.903, demonstrating the effectiveness of their approach in closing the performance gap between English and other languages.

## Method Summary
The authors create multilingual AS2 datasets by translating existing English datasets into five European languages using a large translation model. They apply heuristics to improve translation quality, though the specific heuristics are not fully detailed in the summary. The translated datasets are then used to train multilingual transformer models, which are evaluated on their performance in selecting correct answer sentences for given questions. The evaluation focuses on standard AS2 metrics such as Mean Average Precision (MAP) and Precision at 1 (P@1) to compare the performance of models trained on translated data against English baselines.

## Key Results
- Models trained on translated datasets achieve MAP scores ranging from 0.836-0.904 across five European languages
- P@1 scores for models trained on translated data range from 0.752-0.903, showing strong performance
- The approach effectively closes the performance gap between English and other languages in AS2 tasks

## Why This Works (Mechanism)
The success of this approach stems from the use of large translation models to create high-quality multilingual datasets, combined with targeted heuristics to improve translation accuracy. By leveraging existing English AS2 datasets and translating them into multiple languages, the authors effectively expand the available training data for multilingual AS2 tasks without the need for manual annotation in each target language. The strong performance of models trained on these translated datasets suggests that the translation quality is sufficient to capture the semantic relationships between questions and answer sentences across languages, enabling effective cross-lingual transfer learning.

## Foundational Learning
1. Answer Sentence Selection (AS2) - The task of identifying correct answer sentences for given questions from a pool of candidate sentences. Why needed: AS2 is a critical component of question-answering systems, enabling efficient retrieval of relevant information from large text corpora.
2. Transformer-based models - Deep learning architectures that have achieved state-of-the-art results in various natural language processing tasks. Why needed: These models are used to evaluate the effectiveness of the translated datasets by training them for AS2 tasks in multiple languages.
3. Machine translation - The process of automatically translating text from one language to another using computational methods. Why needed: Machine translation is the core technique used to create the multilingual AS2 datasets from existing English datasets.

## Architecture Onboarding
Component map: Question -> Answer candidates -> Transformer model -> AS2 prediction
Critical path: Question and answer sentences are encoded by the transformer model, which then computes a similarity score to determine if the answer sentence is relevant to the question.
Design tradeoffs: The use of machine translation for dataset creation offers scalability and cost-effectiveness but may introduce translation artifacts or errors that could impact model performance. The choice of transformer-based models for evaluation provides strong baseline performance but may not capture the full potential of the translated datasets.
Failure signatures: Poor translation quality may lead to semantic mismatches between questions and answer sentences, resulting in lower AS2 performance. Over-reliance on English patterns in the translated data may also limit the model's ability to generalize to truly multilingual scenarios.
First experiments:
1. Fine-tune a multilingual transformer model on the translated datasets and evaluate its AS2 performance.
2. Compare the performance of models trained on translated data against those trained on monolingual data in each target language.
3. Conduct human evaluation of translation quality to identify potential artifacts or errors that may impact AS2 performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on machine translation quality may introduce artifacts or errors affecting downstream model performance
- The heuristics used to improve translation quality are not fully described, making it difficult to assess their effectiveness across all language pairs
- Evaluation focuses primarily on transformer-based models, leaving open questions about performance with other architectures

## Confidence
- Claim: Models trained on translated datasets achieve comparable performance to English baselines
  - Confidence: Medium
- Claim: The approach effectively closes the performance gap between English and other languages
  - Confidence: Long-term validation across more diverse languages and domains would strengthen this claim

## Next Checks
1. Conduct human evaluation studies to assess translation quality and identify potential artifacts that may impact model performance across different language pairs.
2. Test the datasets with a broader range of model architectures beyond transformer-based models to evaluate generalizability.
3. Perform cross-domain validation by applying the datasets to specialized question-answering tasks outside the general domains covered in the source datasets.