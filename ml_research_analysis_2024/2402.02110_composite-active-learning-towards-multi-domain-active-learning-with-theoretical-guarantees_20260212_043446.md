---
ver: rpa2
title: 'Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical
  Guarantees'
arxiv_id: '2402.02110'
source_url: https://arxiv.org/abs/2402.02110
tags:
- domain
- domains
- round
- learning
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes Composite Active Learning (CAL) as the first
  general deep active learning method for multi-domain active learning. CAL addresses
  two key challenges: domain similarity and distribution shift across domains.'
---

# Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees

## Quick Facts
- **arXiv ID**: 2402.02110
- **Source URL**: https://arxiv.org/abs/2402.02110
- **Reference count**: 40
- **Primary result**: CAL achieves up to 13.1% accuracy improvement over state-of-the-art active learning methods in multi-domain settings

## Executive Summary
This paper introduces Composite Active Learning (CAL), the first general deep active learning method designed specifically for multi-domain active learning (MUDAL). CAL addresses two fundamental challenges in MUDAL: domain similarity and distribution shift across domains. The method constructs surrogate domains as weighted combinations of labeled data, estimates domain similarity through optimization of an upper error bound, and aligns feature distributions across domains. Theoretical analysis demonstrates that CAL achieves a better error bound compared to existing methods, while experiments on both synthetic and real-world datasets show significant performance improvements.

## Method Summary
CAL operates through a two-level selection process: domain-level selection (budget allocation based on domain similarity) and instance-level selection (query strategies for sample selection). The method constructs surrogate domains as weighted combinations of labeled domains, estimates domain similarity weights by minimizing the distance between original and surrogate domains, and performs feature alignment to reduce distribution shift. The framework optimizes an upper error bound through a minimax game involving classification loss, domain discrimination loss, and similarity weights. Experiments run in R+1 rounds, with initial random sampling followed by budget allocation based on similarity weights and query strategy-based sample selection.

## Key Results
- CAL significantly outperforms state-of-the-art active learning methods, achieving up to 13.1% accuracy improvement on average across all rounds
- Performance improvements increase with the number of domains, demonstrating better scalability compared to single-domain baselines
- CAL shows consistent improvements across multiple datasets including RotatingMNIST variants, Office-Home, ImageCLEF, and Office-Caltech

## Why This Works (Mechanism)
CAL's effectiveness stems from its dual approach to multi-domain challenges. By constructing surrogate domains and estimating domain similarity weights, it enables intelligent budget allocation that focuses resources on domains most beneficial to overall performance. The feature alignment component reduces distribution shift across domains, ensuring the model learns domain-invariant representations. This combination allows CAL to leverage similarities between domains while maintaining domain-specific knowledge, leading to better generalization and more efficient use of limited labeling budgets.

## Foundational Learning
- **Domain similarity estimation**: Why needed - to allocate labeling budget efficiently across domains; Quick check - verify similarity matrix reflects known domain relationships
- **Feature alignment**: Why needed - to reduce distribution shift and improve generalization; Quick check - monitor domain discrimination loss during training
- **Surrogate domain construction**: Why needed - to create intermediate representations for similarity estimation; Quick check - ensure surrogate domains are convex combinations of original domains
- **Minimax optimization**: Why needed - to balance classification accuracy with domain alignment; Quick check - verify convergence of both classification and domain discrimination objectives
- **Two-level selection**: Why needed - to optimize both domain allocation and instance selection; Quick check - track budget utilization across domains
- **Multi-domain error bounds**: Why needed - to provide theoretical justification for the approach; Quick check - verify error bound improves compared to single-domain baselines

## Architecture Onboarding
**Component Map**: Encoder -> Classifier + Discriminator -> Similarity Weight Optimization -> Feature Alignment -> Instance Selection
**Critical Path**: Input data → Domain similarity estimation → Budget allocation → Instance selection → Model training → Performance evaluation
**Design Tradeoffs**: CAL balances between domain-level budget allocation (which may oversimplify domain relationships) and instance-level selection (which may miss global domain patterns). The feature alignment component adds computational overhead but improves generalization.
**Failure Signatures**: Poor domain similarity estimates lead to inefficient budget allocation; ineffective feature alignment results in high domain discrimination loss; incorrect temperature parameter in instance selection causes suboptimal sample diversity.
**First Experiments**: 1) Test domain similarity estimation on synthetic data with known similarities; 2) Evaluate feature alignment effectiveness on rotated MNIST variants; 3) Compare budget allocation strategies across domains with varying similarity structures.

## Open Questions the Paper Calls Out
- How does CAL's performance scale with an increasing number of domains beyond the tested range (e.g., 30+ domains)?
- Can CAL be effectively adapted for active learning in natural language processing (NLP) tasks with multi-domain data?
- How sensitive is CAL's performance to the choice of the temperature parameter T in the GraDS instance-level strategy?
- How does CAL perform in imbalanced multi-domain settings where some domains have significantly fewer data points than others?

## Limitations
- CAL's performance depends heavily on accurate domain similarity estimation, which may be challenging in real-world scenarios with complex domain relationships
- The method introduces additional computational overhead through feature alignment and similarity weight optimization
- CAL's effectiveness on very large numbers of domains (>30) remains untested, raising questions about scalability

## Confidence
**High confidence** in theoretical framework and experimental methodology; **Medium confidence** in practical implementation details and real-world applicability.

## Next Checks
1. Implement and test the feature alignment component separately to verify it effectively reduces domain discrimination loss across all dataset splits
2. Compare similarity weight estimates from the proposed optimization approach against ground-truth domain similarities on synthetic datasets where true similarities are known
3. Conduct ablation studies to quantify the individual contributions of domain-level selection, instance-level selection, and feature alignment to overall performance improvements