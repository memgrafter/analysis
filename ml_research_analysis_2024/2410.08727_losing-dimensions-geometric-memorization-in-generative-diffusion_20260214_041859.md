---
ver: rpa2
title: 'Losing dimensions: Geometric memorization in generative diffusion'
arxiv_id: '2410.08727'
source_url: https://arxiv.org/abs/2410.08727
tags:
- data
- singular
- size
- values
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how diffusion models memorize data when training
  sets are small, extending previous theory to manifold-supported data. Using statistical
  physics tools, the authors show that different tangent subspaces are lost at different
  critical times and dataset sizes depending on local data variance along their directions.
---

# Losing dimensions: Geometric memorization in generative diffusion

## Quick Facts
- arXiv ID: 2410.08727
- Source URL: https://arxiv.org/abs/2410.08727
- Authors: Beatrice Achilli; Enrico Ventura; Gianluigi Silvestri; Bao Pham; Gabriel Raya; Dmitry Krotov; Carlo Lucibello; Luca Ambrogioni
- Reference count: 40
- Key outcome: Extends theory of memorization in diffusion models to manifold-supported data, showing that different tangent subspaces are lost at different critical times depending on local data variance, with higher variance subspaces often lost first due to memorization effects.

## Executive Summary
This paper analyzes how diffusion models memorize data when training sets are small, extending previous theory to manifold-supported data. Using statistical physics tools, the authors show that different tangent subspaces are lost at different critical times and dataset sizes depending on local data variance along their directions. Counterintuitively, subspaces of higher variance are often lost first due to memorization effects, leading to selective dimensionality loss where prominent features are memorized without full collapse on individual training points. The theory is validated through comprehensive experiments on both synthetic linear manifolds and natural image datasets, showing remarkable qualitative agreement with theoretical predictions.

## Method Summary
The authors combine statistical physics (Random Energy Models and glassy phase transitions) with geometric analysis of diffusion models to study memorization. They analyze the empirical score function's behavior as a positional REM, deriving critical condensation times that depend on both global and directional variance terms. The theory predicts that subspaces with higher variance lose dimensions first due to earlier condensation in the random energy landscape. Experiments measure singular values of the Jacobian of score functions at different diffusion times to detect spectral gaps indicating dimensionality loss, comparing results with theoretical predictions across synthetic and natural datasets.

## Key Results
- Shows that diffusion models exhibit selective dimensionality loss where different tangent subspaces are lost at different critical times depending on local data variance
- Demonstrates counterintuitively that higher variance subspaces are often lost first due to memorization effects
- Validates theoretical predictions through comprehensive experiments on synthetic linear manifolds and natural image datasets (MNIST, CIFAR-10, CelebA)
- Shows remarkable qualitative agreement between theory and experiments, with dimensionality increasing with dataset size as predicted

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subspaces of higher variance in the data manifold are lost first during diffusion due to geometric memorization.
- Mechanism: In the condensation phase of the positional Random Energy Model (REM), the effective number of data points contributing to the score function at a given state is determined by the directional variance term $\omega^2(x) = \frac{1}{d}\sum_i x_i^2\sigma_i^2$. Higher directional variance leads to earlier condensation, causing the empirical score to rely on fewer training points and thus memorize that subspace first.
- Core assumption: The data distribution is supported on a linear manifold with distinct variances along different subspaces, and the empirical score behaves like a Boltzmann average over energy levels defined by Euclidean distance to training points.
- Evidence anchors:
  - [abstract] "Perhaps counterintuitively, we find that, under some conditions, subspaces of higher variance are lost first due to memorization effects."
  - [section 5.3] "The 'spatial' dependency of these fluctuations ultimately depend on the data distribution $p_0(x)$"
  - [corpus] "Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes" - related work on geometric analysis of memorization.
- Break condition: If the data distribution is isotropic or the manifold is non-linear with complex curvature, the directional variance approximation breaks down and the mechanism no longer predicts which subspaces are lost first.

### Mechanism 2
- Claim: The transition from generalization to memorization in diffusion models is a glassy phase transition in the underlying energy function.
- Mechanism: The empirical score function's behavior can be analyzed as a positional REM, where the partition function is proportional to the sum of energies (distances) from all training points. At high noise levels (low effective temperature), the system is self-averaging and generalizes. Below a critical condensation time, the system enters a glassy phase where the score depends on a sub-exponential fraction of energy levels, leading to memorization.
- Core assumption: The number of training points scales exponentially with ambient dimension ($N = \exp(\alpha d)$), allowing thermodynamic analysis using statistical physics techniques.
- Evidence anchors:
  - [abstract] "the behavior is known to transition from an associative memory regime to a generalization phase in a phenomenon that has been described as a glassy phase transition"
  - [section 5.3] "the statistical system specified by Eq.(10) undergoes a random phase transition that separates a self-averaging high-temperature regime to a condensation regimes"
  - [corpus] "Dynamical Regimes of Diffusion Models" - directly studies phase transitions in diffusion models using statistical physics.
- Break condition: For finite-dimensional systems or when the dataset size doesn't scale exponentially with dimension, the REM approximation fails and the glassy transition picture no longer applies.

### Mechanism 3
- Claim: The effective number of data points used to estimate the score at a given state depends on the condensation time, which is directionally dependent.
- Mechanism: The effective number of data points $\tilde{N}_t(x)$ is given by $\min(N, t^{1-t_c^{-1}(x)})$, where $t_c(x)$ is the condensation time that depends on both the global variance term $r_{4,\sigma}$ and the directional variance $\omega^2(x)$. This creates a spatially varying effective dataset size that influences which subspaces are generalized versus memorized.
- Core assumption: The score function's variance at a given state can be approximated by the posterior variance divided by the effective number of data points, and this approximation holds in the large-N limit.
- Evidence anchors:
  - [section 5.3] "we can express the effective number of data points used to estimate the score at $x$ at time $t$ as $\tilde{N}_t(x) = \min(N, t^{1-t_c^{-1}(x)})$"
  - [section 5.2] "var$[\nabla\log p_{N,t}(x)]\approx var(x_0|x)/E[\tilde{N}_t(x)]$"
  - [corpus] "A Geometric Framework for Understanding Memorization in Generative Models" - related geometric approach to memorization analysis.
- Break condition: If the data distribution has complex dependencies or the noise schedule doesn't follow the variance-exploding framework, the simple form of the condensation time and effective data points breaks down.

## Foundational Learning

- Concept: Random Energy Models (REM) and glassy phase transitions
  - Why needed here: The paper uses REM theory to analyze the memorization phenomenon in diffusion models as a thermodynamic phase transition
  - Quick check question: In a REM with $N$ energy levels, what is the critical temperature below which condensation occurs?

- Concept: Local Intrinsic Dimensionality (LID) estimation
  - Why needed here: The paper uses LID methods to estimate the dimensionality of the latent manifold from the singular values of the Jacobian of the score function
  - Quick check question: How do you estimate the intrinsic dimension of a manifold from the singular values of a matrix sampled around a point?

- Concept: Score matching and denoising score matching
  - Why needed here: Diffusion models are trained using denoising score matching to learn the gradient of the log probability density
  - Quick check question: What is the relationship between the denoising score matching objective and the true score function?

## Architecture Onboarding

- Component map: Data manifold structure -> Diffusion process -> Score network -> Empirical score estimator -> Jacobian computation -> Singular value analysis -> Dimensionality detection
- Critical path: Data manifold → Diffusion process → Score network training → Empirical score estimation → Jacobian computation → Singular value analysis → Dimensionality detection
- Design tradeoffs:
  - Linear vs. non-linear manifold assumption: Linear simplifies analysis but may miss complex geometry
  - Number of training samples: Affects generalization vs. memorization balance
  - Noise schedule: Variance-exploding vs. variance-preserving affects the theoretical analysis
- Failure signatures:
  - If singular values don't show clear gaps, the manifold structure may be too complex
  - If dimensionality estimates don't increase with dataset size, geometric memorization may be occurring
  - If experimental results don't match theoretical predictions, the linear manifold assumption may be violated
- First 3 experiments:
  1. Generate synthetic linear manifold data with two subspaces of different variances, train diffusion models on different dataset sizes, and measure singular values at various diffusion times to observe gap opening/closing
  2. Vary the relative magnitudes of subspace variances and repeat experiment 1 to test the counterintuitive prediction about higher variance subspaces being lost first
  3. Apply the dimensionality estimation pipeline to real image datasets (MNIST, CIFAR-10) with different training set sizes to observe the generalization-to-memorization transition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the theory of geometric memorization extend to non-linear manifolds beyond the linear Gaussian case studied in this paper?
- Basis in paper: [explicit] The authors state "we extend the theory of memorization in generative diffusion to manifold-supported data" and discuss local geometry analysis, but their theoretical analysis focuses on linear manifolds
- Why unresolved: The paper explicitly limits its theoretical analysis to linear manifolds, noting that "at timet the curvature ofMt is suppressed by the smoothing induced by the forward process" but doesn't provide a complete theory for curved manifolds
- What evidence would resolve it: Experimental validation on natural datasets with known manifold structure (e.g., face images on a pose manifold) showing consistent gap patterns, or theoretical extension of the random matrix analysis to non-linear cases

### Open Question 2
- Question: What is the relationship between the effective number of data points ˜Nt(x) and the actual generalization performance of trained networks across different architectures?
- Basis in paper: [explicit] The paper derives ˜Nt(x) = min(N, t/(1-t/tc(x))) as a theoretical measure of effective data points, but notes "we can finally estimate the distribution of the eigenvalues estimated from the empirical Jacobian matrix"
- Why unresolved: The paper shows theoretical and experimental agreement for the empirical score function, but doesn't systematically compare these predictions with actual trained network performance across different model architectures
- What evidence would resolve it: Controlled experiments varying network capacity, training set size, and architecture while measuring both ˜Nt(x) and actual generalization metrics (e.g., FID scores on held-out data)

### Open Question 3
- Question: How does the dimensionality loss phenomenon manifest in the latent space of pretrained diffusion models when fine-tuned on smaller datasets?
- Basis in paper: [inferred] The paper shows dimensionality increases with dataset size for untrained networks, and discusses "the behavior of the network seems to reflect the global fit of a parameterized linear model" for small datasets
- Why unresolved: The paper focuses on training from scratch rather than fine-tuning, and the experiments show that "the behavior of the network disaligns from the theoretical prediction as the network returns to exhibit the high variance gap even for t = 1e−5" in the small data regime
- What evidence would resolve it: Fine-tuning experiments on large pretrained models showing whether latent dimensionality decreases predictably with fine-tuning dataset size, and whether this follows the theoretical predictions about variance-dependent memorization

## Limitations
- The linear manifold assumption may not capture the complex geometry of real-world data distributions
- The exponential scaling of dataset size with ambient dimension is a critical assumption for REM analysis that may not hold practically
- The directional variance approximation for predicting which subspaces are lost first may break down for highly non-linear manifolds

## Confidence
- High: The experimental validation on synthetic linear manifolds showing spectral gap opening/closing behavior matches theoretical predictions
- Medium: The counterintuitive finding that higher variance subspaces are lost first due to memorization effects, as this relies on the specific directional variance approximation
- Medium: The glassy phase transition interpretation of the generalization-to-memorization transition, as this depends on the REM framework holding

## Next Checks
1. **Non-linear manifold validation**: Generate synthetic data on non-linear manifolds (e.g., spheres, tori) and test whether the directional variance mechanism still predicts which subspaces are lost first during diffusion.

2. **Finite-size scaling analysis**: Systematically vary the dataset size scaling with dimension (not just exponential) to test the robustness of the glassy phase transition picture and determine critical exponents.

3. **Cross-architecture generalization**: Test the dimensionality loss predictions on different diffusion model architectures (e.g., U-Net, score-based models with different network designs) to verify the mechanism is architecture-agnostic.