---
ver: rpa2
title: 'Regurgitative Training: The Value of Real Data in Training Large Language
  Models'
arxiv_id: '2407.12835'
source_url: https://arxiv.org/abs/2407.12835
tags:
- data
- training
- performance
- regurgitative
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Training LLMs with LLM-generated data (regurgitative training)
  leads to lower performance compared to training with real human-generated data,
  even when large amounts of synthetic data are available. This occurs across multiple
  tasks (translation, question answering) and model settings (fine-tuning commercial
  LLMs, training from scratch).
---

# Regurgitative Training: The Value of Real Data in Training Large Language Models

## Quick Facts
- **arXiv ID**: 2407.12835
- **Source URL**: https://arxiv.org/abs/2407.12835
- **Reference count**: 32
- **Primary result**: Training with synthetic data from LLMs produces worse performance than real human data, even with mitigation strategies.

## Executive Summary
This paper investigates whether synthetic data generated by LLMs can effectively replace human-generated data in LLM training (regurgitative training). Across multiple tasks including translation and question answering, the authors find that models trained with synthetic data consistently underperform those trained with real human data. This performance gap persists even when implementing various mitigation strategies such as prioritizing high-quality synthetic data, mixing data from multiple LLMs, or using AI detection to select synthetic data resembling human content. The authors identify two key factors contributing to this gap: synthetic data contains more errors and exhibits lower lexical diversity than real data.

## Method Summary
The authors conduct experiments across two primary tasks: translation (using the Europarl parallel corpus) and question answering (using SQuAD). They generate synthetic data using GPT-3.5, GPT-4, and LLAMA2, then compare model performance when trained on synthetic versus real human data. The experiments include fine-tuning existing LLMs and training transformer models from scratch, with progressive data addition. Three mitigation strategies are tested: quality-based ranking using BLEU prediction or entropy scoring, data mixture from multiple LLMs, and AI detection-based prioritization of synthetic data that resembles human-generated content.

## Key Results
- Models trained on synthetic data consistently underperform those trained on real human data across all tested tasks and settings
- Even when synthetic data is prioritized by quality scores or mixed from multiple LLMs, the performance gap remains significant
- AI detection-based mitigation can partially improve results but cannot fully close the performance gap
- Synthetic data exhibits both higher error rates and lower lexical diversity compared to real human-generated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regurgitative training with lower-quality LLM-generated data leads to higher error rates in the trained model.
- Mechanism: LLM-generated data contains more errors than real human-generated data, and training on these error-prone instances propagates mistakes to the model.
- Core assumption: Error rates in synthetic data are higher and directly degrade model performance when used in training.
- Evidence anchors:
  - [abstract] "synthetic data contains more errors"
  - [section 4] "errors in LLM-generated data is one of the culprits"
  - [corpus] No direct evidence; assumption supported indirectly by cited work.
- Break condition: If synthetic data error rates drop to near human levels, the mechanism would lose predictive power.

### Mechanism 2
- Claim: Regurgitative training with lower lexical diversity LLM-generated data leads to reduced model generalization.
- Mechanism: Synthetic data has lower lexical diversity than real data, reducing the model's ability to generalize across varied language use.
- Core assumption: Lexical diversity is positively correlated with model generalization ability.
- Evidence anchors:
  - [abstract] "synthetic data ...exhibits lower lexical diversity"
  - [section 4] "LLM-generated data exhibit lower degrees of lexical diversity"
  - [corpus] Weak evidence; diversity effects inferred from related work rather than directly tested.
- Break condition: If synthetic data achieves equal or greater lexical diversity than real data, this mechanism would be neutralized.

### Mechanism 3
- Claim: AI detection-based mitigation can partially recover regurgitative training performance by prioritizing synthetic data that resembles real data.
- Mechanism: AI detectors identify synthetic data with human-like characteristics (fewer errors, higher diversity), which when prioritized in training partially close the performance gap.
- Core assumption: AI detectors can reliably distinguish synthetic from real data based on multiple features, and higher resemblance correlates with better training outcomes.
- Evidence anchors:
  - [abstract] "using AI detection to prioritize data resembling human-generated content"
  - [section 5.3] "AI detection-based mitigation strategy"
  - [corpus] Weak evidence; AI detector performance varies by model type and is not universally strong.
- Break condition: If AI detectors become ineffective at distinguishing synthetic from real data, this mitigation would fail.

## Foundational Learning

- Concept: Translation evaluation metrics (BLEU, self-BLEU)
  - Why needed here: The paper evaluates model performance using BLEU scores and lexical diversity via self-BLEU; understanding these metrics is essential for interpreting results.
  - Quick check question: What does a higher self-BLEU score indicate about a text's lexical diversity?
- Concept: Semi-supervised learning and pseudo-label confidence
  - Why needed here: The paper draws parallels between regurgitative training and semi-supervised learning, particularly around confidence-based data selection.
  - Quick check question: In semi-supervised learning, why are high-confidence predictions preferred for pseudo-labeling?
- Concept: Transformer architecture fundamentals
  - Why needed here: The paper trains models from scratch using transformer architecture; basic knowledge of its encoder-decoder structure and attention mechanism is required.
  - Quick check question: What are the two main components of a transformer architecture used in translation tasks?

## Architecture Onboarding

- Component map: Data generation → Quality filtering (BLEU prediction or entropy scoring) → Mixture or AI detection prioritization → Model training (fine-tuning or from-scratch) → Performance evaluation
- Critical path: Quality assessment → Data prioritization → Training → Evaluation
- Design tradeoffs: Balancing data quantity vs. quality; computational cost of quality filtering vs. potential performance gains; diversity vs. consistency in synthetic data
- Failure signatures: Stagnant or declining BLEU scores during regurgitative training; high self-BLEU scores indicating low diversity; poor AI detector performance leading to ineffective prioritization
- First 3 experiments:
  1. Fine-tune GPT-3.5 on a small synthetic dataset (1,000 instances) and measure BLEU score improvement vs. real data
  2. Train a transformer from scratch on synthetic data ranked by entropy scores and compare performance to random ordering
  3. Mix data from two different LLMs (e.g., GPT-3.5 + GPT-4) and evaluate whether lexical diversity improves over single-model synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of regurgitative training effectiveness as LLM capabilities improve?
- Basis in paper: [inferred] The paper suggests performance loss may be a "fundamental issue of current paradigm of generative AI" but acknowledges this remains an open question requiring "advanced theoretical frameworks"
- Why unresolved: Current understanding relies on empirical observations and simplified models rather than comprehensive theoretical analysis of the fundamental limits
- What evidence would resolve it: A mathematical framework proving whether there exists a threshold of LLM capability beyond which regurgitative training becomes viable, or demonstrating that the diversity gap is fundamentally irreducible regardless of model quality

### Open Question 2
- Question: How does regurgitative training affect LLM performance on open-ended tasks where quality is difficult to measure?
- Basis in paper: [explicit] "LLMs are used not only in tasks with established performance metrics... but also in open-ended tasks whose quality are very hard to gauge... How to conceptually think about and empirically evaluate the impact of regurgitative training in these open-ended tasks remains an interesting question"
- Why unresolved: The paper only tests on translation and question-answering tasks with clear evaluation metrics, leaving open-ended creative tasks unexplored
- What evidence would resolve it: Controlled experiments measuring how creative diversity, originality, and novelty change under regurgitative training in tasks like story generation, idea brainstorming, or artistic composition

### Open Question 3
- Question: What is the optimal strategy for mixing synthetic and real data in LLM training?
- Basis in paper: [explicit] The paper tests different proportions but finds even small amounts of synthetic data can hurt performance, and asks "What is the optimal strategy for mixing synthetic and real data in LLM training?"
- Why unresolved: While the paper tests fixed proportions, it doesn't explore dynamic mixing strategies, curriculum learning approaches, or optimal scheduling of data types during training
- What evidence would resolve it: Empirical studies comparing fixed vs. dynamic mixing schedules, curriculum-based approaches that gradually introduce synthetic data, and adaptive strategies that adjust mixing ratios based on training progress and model performance

## Limitations

- The study examines fixed training phases rather than the iterative data collection and model refinement common in real-world LLM development
- Results are primarily based on translation and question answering tasks, with limited generalizability to other domains like code generation or creative writing
- The paper doesn't establish whether performance gaps could eventually close with sufficient data volume or significantly improved generation techniques

## Confidence

- **High confidence**: The core finding that regurgitative training produces inferior results compared to real human data, supported by consistent performance gaps across multiple tasks and model settings
- **Medium confidence**: The proposed mechanisms (error rates and lexical diversity) as contributing factors, though causation isn't definitively proven through controlled isolation
- **Low confidence**: The long-term implications, as the study doesn't explore whether performance gaps might close with future LLM generations, larger synthetic datasets, or more sophisticated mitigation strategies

## Next Checks

1. **Cross-domain validation**: Test regurgitative training on code generation and mathematical reasoning tasks to assess whether performance gaps persist across different cognitive domains
2. **Temporal scaling study**: Examine how performance gaps evolve as synthetic data volume increases by orders of magnitude (10x, 100x, 1000x the current dataset sizes)
3. **Adaptive mitigation evaluation**: Implement an iterative training approach where synthetic data generation and selection adapt based on model performance feedback, testing whether this dynamic strategy can close the performance gap more effectively than static mitigation methods