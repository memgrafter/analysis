---
ver: rpa2
title: 'Rank-DistiLLM: Closing the Effectiveness Gap Between Cross-Encoders and LLMs
  for Passage Re-Ranking'
arxiv_id: '2405.07920'
source_url: https://arxiv.org/abs/2405.07920
tags:
- trec
- pages
- proceedings
- effectiveness
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to effectively distill cross-encoders
  from large language models (LLMs) for passage re-ranking. The authors hypothesize
  that the effectiveness gap between distilled models and LLMs stems from suboptimal
  training methods, such as not using hard-negative sampling, deep rankings, or listwise
  loss functions.
---

# Rank-DistiLLM: Closing the Effectiveness Gap Between Cross-Encoders and LLMs for Passage Re-Ranking

## Quick Facts
- arXiv ID: 2405.07920
- Source URL: https://arxiv.org/abs/2405.07920
- Reference count: 40
- Primary result: Cross-encoders distilled from LLMs can match LLM effectiveness while being up to 173x faster and 24x more memory efficient

## Executive Summary
This paper addresses the effectiveness gap between cross-encoders and large language models (LLMs) for passage re-ranking. The authors hypothesize that previous work has not applied optimal methods for fine-tuning cross-encoders on labeled data, including hard-negative sampling, deep rankings, and listwise loss functions. To investigate this, they create a new dataset called Rank-DistiLLM, which contains rankings from multiple first-stage retrieval models re-ranked by an LLM. Through systematic experiments, they demonstrate that cross-encoders fine-tuned on this dataset can match LLM effectiveness while offering significant efficiency gains.

## Method Summary
The authors create Rank-DistiLLM by first retrieving top 100 passages per query using BM25 and ColBERTv2, then re-ranking these passages using RankZephyr. They fine-tune ELECTRA BASE and LARGE models on this dataset using either RankNet (pairwise) or their proposed ADR-MSE (listwise) loss functions. The training follows standard procedures with batch size 32, AdamW optimizer with 10^-5 learning rate, and evaluation on TREC Deep Learning 2019/2020 tracks and TIREx framework. The key innovation is the systematic investigation of how first-stage retrieval models, ranking depth, and loss functions affect distillation effectiveness.

## Key Results
- Cross-encoders distilled from RankZephyr achieve similar effectiveness to RankZephyr itself
- ColBERTv2-then-RankZephyr is more effective than BM25-then-RankZephyr
- Increasing ranking depth up to 50 passages improves effectiveness
- Pairwise loss functions (RankNet) are sufficient for LLM distillation, matching or exceeding listwise losses

## Why This Works (Mechanism)

### Mechanism 1: Hard-negative Sampling from Effective Retrieval Models
- Claim: Hard-negatives from strong first-stage retrieval models improve distillation effectiveness
- Mechanism: Passages retrieved by effective models but not labeled as relevant provide challenging examples
- Core assumption: Strong retrieval models retrieve passages that are difficult to distinguish from relevant ones
- Evidence anchors: Abstract mentions suboptimal methods; section notes lack of hard-negative sampling in previous datasets
- Break condition: If first-stage model is ineffective, hard-negatives may not provide meaningful training signals

### Mechanism 2: Deeper Rankings for Better Relative Judgments
- Claim: More passages per query (deeper rankings) improve distillation effectiveness
- Mechanism: Wider range of passages allows better learning of relative relevance
- Core assumption: More passages provide more informative training signals than limited sets
- Evidence anchors: Abstract notes previous datasets only provide up to 30 passages; section cites Zhuang et al. 2022
- Break condition: If additional passages are too far down to be relevant, they may not provide meaningful signals

### Mechanism 3: Sufficiency of Pairwise Loss Functions for LLM Distillation
- Claim: Pairwise loss functions are sufficient for LLM distillation, unlike MS MARCO fine-tuning
- Mechanism: LLM ranking information provides sufficient signal for pairwise comparisons
- Core assumption: LLM ranking captures relative relevance judgments needed for effective training
- Evidence anchors: Abstract notes listwise loss functions yield no benefit; section compares RankNet to proposed ADR-MSE
- Break condition: If LLM ranking information is incomplete, listwise losses may be necessary

## Foundational Learning

- **Transformer-based cross-encoders**: Understanding how cross-encoders process query-passage pairs is crucial for distillation. Quick check: How does a cross-encoder process query and passage pairs differently from a dual-encoder?

- **Hard-negative sampling**: Key concept for effective distillation emphasized in this paper. Quick check: What makes a negative sample "hard" in cross-encoder training?

- **Loss functions for ranking (pairwise vs listwise)**: Paper compares effectiveness of different loss types. Quick check: How do pairwise and listwise loss functions differ in their approach to training ranking models?

## Architecture Onboarding

- **Component map**: First-stage retrieval model (BM25/ColBERTv2) -> LLM re-ranker (RankZephyr) -> Cross-encoder (ELECTRA) -> Distillation dataset (Rank-DistiLLM) -> Loss functions (RankNet/ADR-MSE)

- **Critical path**: 1) Retrieve top 100 passages using first-stage model, 2) Re-rank with LLM, 3) Sample hard-negatives from re-ranked passages, 4) Train cross-encoder using loss function, 5) Evaluate on downstream tasks

- **Design tradeoffs**: Stronger first-stage retrieval vs. computational cost, deeper rankings vs. memory constraints, pairwise vs. listwise losses vs. effectiveness

- **Failure signatures**: Low effectiveness vs. LLM teacher, high variance across queries, long training times or memory issues

- **First 3 experiments**: 1) Compare BM25 vs. ColBERTv2 rankings effectiveness, 2) Evaluate ranking depth impact (10, 25, 50, 100 passages), 3) Test ADR-MSE vs. RankNet loss effectiveness

## Open Questions the Paper Calls Out

The paper identifies several open questions that arise from their systematic investigation:

1. **Effectiveness of ADR-MSE at deeper rankings**: The authors propose ADR-MSE as a novel listwise loss but only test up to 100 passages per query, leaving open whether it would outperform pairwise losses at even deeper rankings.

2. **Comparison to other distillation datasets**: While Rank-DistiLLM uses ColBERTv2 and BM25, the paper doesn't compare its effectiveness when using the same first-stage retrieval models as other datasets like RankGPT or TWOLAR.

3. **Impact of dataset size**: The paper only evaluates on 10,000 queries, raising questions about how effectiveness scales with larger training datasets across different domains.

## Limitations

- **Limited loss function comparison**: Only one pairwise (RankNet) and one listwise (ADR-MSE) loss function were compared, leaving open whether other loss functions might yield different results.

- **Dataset generalization**: The Rank-DistiLLM dataset is based on 10,000 MS MARCO queries, which may not generalize to other domains or query distributions.

- **Computational requirements**: While more efficient than LLMs, the distilled models still require significant computational resources, particularly for larger ELECTRA models.

## Confidence

- **Uncertainty in Loss Function Comparison**: Medium confidence - limited to one pairwise and one listwise function
- **Dataset Size and Generalization**: Medium confidence - reasonable sample size but may not generalize across domains
- **Computational Requirements**: High confidence - clearly stated and measured

## Next Checks

1. **Ablation Study on Ranking Depth**: Evaluate cross-encoders trained on varying ranking depths (10, 25, 50, 100 passages) to confirm optimal depth for distillation

2. **Cross-Domain Evaluation**: Test distilled models on datasets from different domains (biomedical, legal) to assess generalization capabilities

3. **Alternative Loss Function Comparison**: Compare effectiveness of other pairwise and listwise loss functions (ApproxN, LambdaLoss) to validate sufficiency of pairwise losses for LLM distillation