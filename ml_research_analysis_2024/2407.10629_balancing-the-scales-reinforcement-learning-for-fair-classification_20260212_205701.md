---
ver: rpa2
title: 'Balancing the Scales: Reinforcement Learning for Fair Classification'
arxiv_id: '2407.10629'
source_url: https://arxiv.org/abs/2407.10629
tags:
- reward
- fairness
- scaling
- classification
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using reinforcement learning (RL) to address
  bias in imbalanced multi-class classification by scaling the reward function. The
  authors formalize fair classification as a contextual multi-armed bandit problem
  and adapt three RL algorithms (LinUCB, DQN, PPO) with different reward scaling strategies
  to mitigate bias among protected groups.
---

# Balancing the Scales: Reinforcement Learning for Fair Classification

## Quick Facts
- arXiv ID: 2407.10629
- Source URL: https://arxiv.org/abs/2407.10629
- Reference count: 26
- One-line primary result: RL with reward scaling significantly reduces bias among protected groups in imbalanced classification tasks

## Executive Summary
This paper proposes using reinforcement learning to address bias in imbalanced multi-class classification by scaling the reward function to counteract imbalances among protected groups. The authors formalize fair classification as a contextual multi-armed bandit problem and adapt three RL algorithms (LinUCB, DQN, PPO) with different reward scaling strategies. Experiments on two datasets (BiasBios with 28 professions and Emoji sentiment analysis) demonstrate that deep RL methods (DQN, PPO) perform best on multi-class data, while LinUCB excels on binary classification. The proposed reward scaling significantly reduces the true positive ratio gap for fairness, though with some trade-off in accuracy.

## Method Summary
The paper frames fair classification as a Contextual Multi-Armed Bandit (CMAB) problem where each input is a context vector and the RL agent selects a class action. Three RL algorithms are adapted: LinUCB (linear CMAB), DQN, and PPO (deep RL). The key innovation is reward scaling, where rewards are scaled inversely proportional to class frequency to counteract data imbalance. Four scaling strategies are explored: Wρ+, Wρ−, WEO (Equal Opportunity), and WIPW (Inverse Propensity Weighting). The context vectors are generated using pretrained encoders (BERT-base for BiasBios, DeepMoji for Emoji). The framework optimizes both accuracy and fairness simultaneously through the scaled reward signal.

## Key Results
- Deep RL algorithms (DQN, PPO) outperform classical CMAB (LinUCB) on the 28-class BiasBios dataset
- Reward scaling significantly reduces the True Positive Ratio gap (GAP) for fairness across all algorithms
- Supervised learning with loss reweighting achieves state-of-the-art performance on BiasBios, outperforming existing debiasing methods
- LinUCB shows oversensitivity to reward scaling, causing performance degradation for majority groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling rewards inversely to class frequency reduces bias by increasing learning signals for minority groups.
- Mechanism: When a minority group's reward is scaled up, its gradient contribution becomes stronger, leading the RL agent to update its policy more aggressively for those cases, thereby improving recall for the minority group.
- Core assumption: Reward scaling preserves policy convergence while redistributing gradient emphasis toward under-represented groups.
- Evidence anchors: [abstract] "by scaling the reward function to counteract imbalances among protected groups"; [section 3.3] "scaling the rewards inversely proportional to the class frequency"
- Break condition: If scaling is too aggressive, majority group performance may degrade or the policy may overfit to minority patterns, causing unstable convergence.

### Mechanism 2
- Claim: Treating classification as a Contextual Multi-Armed Bandit enables RL agents to learn class decisions directly from continuous reward signals instead of fixed labels.
- Mechanism: Each input becomes a context vector, and the agent selects a class action. Rewards are computed from accuracy and scaled by fairness correction, allowing the agent to optimize both performance and fairness simultaneously.
- Core assumption: The classification problem can be decomposed into independent, sequential decisions where the CMAB assumption of independent states holds.
- Evidence anchors: [abstract] "We employ the contextual multi-armed bandit framework"; [section 3.1] "we propose to frame the fair classification task as a Contextual Multi-Armed Bandit (CMAB) problem"
- Break condition: If the independence assumption is violated (e.g., strong temporal or batch effects), CMAB may fail to capture necessary dependencies, leading to suboptimal or unfair decisions.

### Mechanism 3
- Claim: Deep RL algorithms (DQN, PPO) outperform classical CMAB (LinUCB) on multi-class problems due to their ability to model non-linear relationships in the context-reward space.
- Mechanism: The neural network approximators in DQN and PPO learn complex decision boundaries, while LinUCB assumes a linear relationship, which is insufficient for multi-class, high-dimensional tasks.
- Core assumption: Multi-class datasets contain non-linear separability that benefits from non-linear function approximation.
- Evidence anchors: [section 5.2] "deep RL algorithms perform best on the multi-class dataset, while the classical CMAB algorithm excels on the binary dataset"; [section 3.2] "we select two popular deep RL algorithms... which allow us to leverage non-linear approximations"
- Break condition: If the dataset is linearly separable or very small, the added complexity of deep RL may cause overfitting or unnecessary computational overhead without accuracy gains.

## Foundational Learning

- Concept: Contextual Multi-Armed Bandit (CMAB)
  - Why needed here: CMAB formalizes the classification problem as sequential decisions with rewards, allowing RL algorithms to optimize for both accuracy and fairness in a principled way.
  - Quick check question: How does a CMAB differ from a standard multi-armed bandit in terms of state dependence?

- Concept: Reward Scaling
  - Why needed here: Reward scaling adjusts the magnitude of feedback signals to counteract data imbalance, ensuring minority groups receive sufficient learning updates.
  - Quick check question: What is the effect of scaling rewards by the inverse of group frequency on the gradient update direction?

- Concept: Function Approximation in RL
  - Why needed here: Deep RL uses neural networks to approximate Q-values or policies, enabling handling of high-dimensional context vectors and non-linear decision boundaries.
  - Quick check question: Why might a linear CMAB method fail on a multi-class problem with non-linear class boundaries?

## Architecture Onboarding

- Component map:
  Context encoder (e.g., BERT, DeepMoji) -> Context vector -> RL agent (LinUCB/DQN/PPO) -> Action (class label) -> Reward calculator (accuracy + scaling matrix) -> Reward signal -> Environment (data iterator) -> Context-reward loop

- Critical path:
  Input -> Encoder -> Agent selects action -> Compute reward -> Update agent -> Repeat

- Design tradeoffs:
  - Linear (LinUCB) vs. non-linear (DQN/PPO): simplicity and speed vs. expressive power
  - Reward scaling type (Wρ+, Wρ−, WEO, WIPW): fairness impact vs. stability
  - Exploration strategy: ϵ-greedy vs. entropy regularization

- Failure signatures:
  - Low F1 for sparse classes -> RL ignoring minority classes
  - Negative TPR gap for majority -> Overcompensation from scaling
  - Poor convergence on binary tasks -> Model complexity mismatch

- First 3 experiments:
  1. Train baseline supervised model without scaling -> Establish performance baseline
  2. Train RL agents without scaling -> Check if RL alone improves fairness
  3. Apply EO scaling to RL agents -> Test effectiveness of fairness-aware reward scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying reasons for LinUCB's performance degradation when reward scaling is applied, and how can this be mitigated without compromising fairness?
- Basis in paper: [explicit] The paper observes that LinUCB's performance significantly drops with reward scaling, suggesting oversensitivity to scaling that causes overcompensation against majority groups.
- Why unresolved: The paper identifies the problem but doesn't provide a detailed analysis of why LinUCB is particularly sensitive to reward scaling compared to other algorithms.
- What evidence would resolve it: Comparative analysis of LinUCB's weight updates and confidence bounds under different scaling strategies, and testing modified LinUCB variants with adaptive scaling factors.

### Open Question 2
- Question: How do deep RL algorithms perform on extremely imbalanced datasets where some classes have very few samples, and what modifications could improve their performance on these rare classes?
- Basis in paper: [explicit] The paper notes that DQN and PPO fail to recall two very sparse classes in BiasBios, resulting in lower F1 scores despite competitive accuracy.
- Why unresolved: The paper identifies this limitation but doesn't explore techniques like data augmentation, hierarchical classification, or curriculum learning that might help.
- What evidence would resolve it: Experiments testing various techniques to improve performance on rare classes, such as hierarchical classification structures or transfer learning approaches.

### Open Question 3
- Question: How does the strength of protected attribute signal in embeddings affect the effectiveness of reward scaling across different RL algorithms?
- Basis in paper: [explicit] The paper investigates how adding explicit gender information or using MP-debiased embeddings affects performance and fairness, finding varying impacts across algorithms.
- Why unresolved: While the paper tests two scenarios (adding and removing gender signal), it doesn't systematically explore the full spectrum of signal strengths or their interaction with reward scaling.
- What evidence would resolve it: Experiments varying the strength of protected attribute signal systematically and measuring how different RL algorithms respond to reward scaling under these conditions.

## Limitations

- The study focuses on only two specific datasets (BiasBios and Emoji sentiment analysis), limiting generalizability to other domains and imbalance patterns
- Evaluation primarily measures TPR gap as the fairness metric, potentially missing other fairness dimensions like equality of opportunity or demographic parity
- Deep RL methods struggle with very sparse classes, resulting in lower F1 scores despite competitive accuracy

## Confidence

- RL with reward scaling reduces bias: Medium confidence (supported by empirical results but limited dataset scope)
- Deep RL outperforms LinUCB on multi-class: High confidence (clear experimental evidence on BiasBios)
- LinUCB excels on binary classification: Medium confidence (supported by Emoji data but limited validation)
- Supervised learning with loss reweighting is state-of-the-art: High confidence (explicit comparison with existing methods)

## Next Checks

1. Test RL algorithms on additional multi-class datasets with different imbalance patterns and protected attribute types to verify generalizability of deep RL superiority
2. Compare RL-based approaches against other non-RL debiasing methods (e.g., adversarial debiasing, reweighting) across multiple fairness metrics beyond TPR gap
3. Conduct ablation studies to isolate the contribution of reward scaling versus the RL framework itself in achieving fairness improvements