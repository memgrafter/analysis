---
ver: rpa2
title: Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering
arxiv_id: '2411.12395'
source_url: https://arxiv.org/abs/2411.12395
tags:
- question
- questions
- llms
- ambiguous
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses how large language models (LLMs) handle ambiguity
  in open-domain question answering. The authors investigate whether training-free,
  prompt-based disambiguation methods can improve LLM performance on ambiguous questions.
---

# Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering

## Quick Facts
- arXiv ID: 2411.12395
- Source URL: https://arxiv.org/abs/2411.12395
- Reference count: 29
- Primary result: Simple, training-free prompt-based disambiguation methods significantly improve LLM performance on ambiguous questions

## Executive Summary
This study investigates how large language models handle ambiguity in open-domain question answering tasks. Using two state-of-the-art models (GPT-4o and GPT-4o-mini) and a dataset of real-world ambiguous questions, the authors evaluate whether training-free, prompt-based disambiguation methods can improve performance. The research demonstrates that both rephrasing questions and adding contextual information significantly outperform naive prompting, with context addition yielding the best results. Notably, small-scale fine-tuning failed to improve performance, suggesting that simple, training-free approaches are more effective for this task.

## Method Summary
The study sampled 1,000 ambiguous questions from the AmbigQA dataset and applied three disambiguation strategies: naive prompting, rephrasing questions to start with "what", and adding contextual information. These methods were tested on GPT-4o and GPT-4o-mini using both default (temperature=1) and low (temperature=0.2) settings. Performance was evaluated using cosine similarity between LLM responses and ground truth answers through text embeddings. The authors also conducted a small-scale fine-tuning experiment on GPT-4o-mini for comparison, finding that the prompt-based approaches significantly outperformed the fine-tuned model.

## Key Results
- Both rephrasing and context addition disambiguation methods significantly improved LLM performance over naive prompting
- Context addition performed better than rephrasing for both GPT-4o and GPT-4o-mini models
- Small-scale fine-tuning did not improve performance and may have suffered from catastrophic forgetting
- Lower temperature settings provided only minor improvements compared to the substantial gains from disambiguation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training-free prompt-based disambiguation significantly improves LLM performance on ambiguous questions
- Mechanism: By explicitly rephrasing questions to reduce ambiguity or adding contextual information, the LLM can better interpret the intended meaning, leading to more accurate answers
- Core assumption: LLMs can leverage their pre-existing knowledge and understanding of context when provided with disambiguated or enriched prompts, even without task-specific fine-tuning
- Evidence anchors:
  - [abstract] "We demonstrate how simple, training-free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks."
  - [section IV-C] "We employed three distinct prompting strategies... aimed to comprehensively measure the models' accuracy to comprehend ambiguity through different types of perturbations."
- Break condition: If the LLM's pre-training data does not contain sufficient context or examples to disambiguate the question, or if the ambiguity is too complex for simple rephrasing or context addition to resolve

### Mechanism 2
- Claim: Disambiguation via adding context performs better than rephrasing questions to start with "what"
- Mechanism: Adding contextual information leverages the LLM's vast world knowledge to clarify the intended meaning of ambiguous questions, while rephrasing with "what" only provides linguistic perturbation
- Core assumption: LLMs have sufficient world knowledge to disambiguate questions when provided with relevant context, and this knowledge is effectively utilized in the response generation process
- Evidence anchors:
  - [section IV-C] "Adding Context to the Ambiguous Question: Since LLMs have vast amounts of world knowledge due to the extensive pre-training and instruction tuning done on them, we use that world knowledge from LLMs to find and return relevant information about the ambiguous question."
  - [section V] "Out of the two simple disambiguating methods explored, we see that disambiguation via adding context performs better for both LLMs."
- Break condition: If the added context is irrelevant or introduces new ambiguities, or if the LLM's world knowledge is insufficient to disambiguate the question based on the provided context

### Mechanism 3
- Claim: Small-scale fine-tuning does not improve LLM performance on ambiguous questions
- Mechanism: The complexity and variability of ambiguous questions may require more extensive fine-tuning or a different approach to effectively improve LLM performance
- Core assumption: The sample size and diversity of the fine-tuning data used in the study are sufficient to capture the nuances of ambiguous question answering, and the fine-tuning process does not lead to catastrophic forgetting
- Evidence anchors:
  - [section V] "RQ2: To evaluate whether small scale fine-tuning helps in improving LLM performance on ambiguous questions, we perform few-shot fine-tuning on GPT 4o-mini... The GT Answer Overlap for the 4o-mini model is 0.643 while that for the fine-tuned 4o-mini model is 0.626."
  - [section VI] "We suspect that our fine-tuning approach may have underperformed due to catastrophic forgetting [24]."
- Break condition: If the fine-tuning data is not representative of the full range of ambiguous questions, or if the fine-tuning process is not optimized to preserve the LLM's general knowledge and reasoning capabilities

## Foundational Learning

- Concept: Natural Language Ambiguity
  - Why needed here: Understanding the different types of ambiguity (e.g., lexical, syntactic, semantic) and their impact on LLM performance is crucial for designing effective disambiguation strategies
  - Quick check question: What are the main types of ambiguity in natural language, and how do they affect the interpretation of questions by LLMs?

- Concept: Prompt Engineering and Disambiguation
  - Why needed here: Knowledge of prompt engineering techniques, such as rephrasing, adding context, and using specific keywords, is essential for designing effective disambiguation strategies that leverage the LLM's capabilities
  - Quick check question: What are some common prompt engineering techniques used to disambiguate questions for LLMs, and how do they work?

- Concept: Evaluation Metrics for Question Answering
  - Why needed here: Understanding the appropriate metrics for evaluating LLM performance on ambiguous question answering, such as semantic similarity and distance measures, is crucial for interpreting the results and comparing different approaches
  - Quick check question: What are some commonly used evaluation metrics for question answering tasks, and how do they account for the nuances of ambiguous questions?

## Architecture Onboarding

- Component map: AmbigQA dataset -> GPT-4o and GPT-4o-mini models -> Three disambiguation methods (naive, rephrase, context) -> Cosine similarity evaluation
- Critical path: 1. Sample 1,000 ambiguous questions from AmbigQA dataset -> 2. Apply three disambiguation methods to each question -> 3. Evaluate performance using semantic similarity metrics -> 4. Compare results to identify most effective strategy
- Design tradeoffs: Using pre-trained LLMs vs. fine-tuning for the specific task of ambiguous question answering; simplicity and generalizability of disambiguation methods vs. their effectiveness in resolving ambiguity; computational cost and latency of adding context vs. potential improvement in performance
- Failure signatures: Low semantic similarity between LLM responses and ground truth answers across all disambiguation methods; inconsistency in LLM performance across different types of ambiguity or question domains; over-reliance on context addition leading to irrelevant or incorrect answers
- First 3 experiments: 1. Evaluate naive prompting performance on small sample of ambiguous questions -> 2. Apply "rephrasing with what" method to same sample and compare results -> 3. Implement context addition method and assess effectiveness vs. other two methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of ambiguity (multiple answers, time-dependent interpretations, multiple answer types) affect LLM performance differently, and which disambiguation strategies work best for each type?
- Basis in paper: [explicit] The authors acknowledge in the limitations section that their study adopted a general form of "ambiguity" and that a more thorough investigation of various types of ambiguity could provide deeper insights
- Why unresolved: The current study only examined ambiguous questions as a general category without distinguishing between different types of ambiguity, making it unclear how specific disambiguation strategies perform across different ambiguity types
- What evidence would resolve it: A systematic analysis comparing LLM performance across different ambiguity categories (temporal, referential, structural) using the same disambiguation methods, with performance metrics broken down by ambiguity type

### Open Question 2
- Question: Can targeted fine-tuning strategies effectively overcome catastrophic forgetting while improving LLM performance on ambiguous questions?
- Basis in paper: [explicit] The authors note in the limitations section that their fine-tuning approach may have underperformed due to catastrophic forgetting and suggest more targeted fine-tuning strategies could be beneficial
- Why unresolved: The study only tested a simple few-shot fine-tuning approach that did not improve performance, but did not explore more sophisticated fine-tuning methods or their effectiveness
- What evidence would resolve it: Experiments comparing various fine-tuning approaches (adapter-based, prompt tuning, LoRA) on ambiguous question answering tasks, measuring both performance gains and retention of general capabilities

### Open Question 3
- Question: How does the performance of training-free disambiguation methods on ambiguous questions compare to task-specific models or traditional information retrieval approaches?
- Basis in paper: [inferred] While the paper demonstrates that simple training-free methods improve LLM performance, it does not compare these approaches to specialized question disambiguation systems or retrieval-based methods
- Why unresolved: The study focuses on comparing different prompting strategies within the LLM framework but does not benchmark against alternative approaches to handling ambiguous questions
- What evidence would resolve it: Head-to-head comparisons between LLM-based disambiguation, specialized disambiguation models, and retrieval-based approaches on the same ambiguous question datasets with standardized evaluation metrics

## Limitations
- Study generalizability is limited to English-language questions from AmbigQA dataset without evaluation across different languages or cultural contexts
- Temperature tuning showed only minor improvements, suggesting potential overfitting to specific dataset or missing optimal temperature ranges
- Fine-tuning results were negative, but study did not explore alternative fine-tuning strategies or larger-scale training approaches

## Confidence
- High confidence: The effectiveness of prompt-based disambiguation methods (rephrasing and context addition) for improving LLM performance on ambiguous questions
- Medium confidence: The superiority of context addition over rephrasing, as results may be sensitive to specific context generation method used
- Medium confidence: The conclusion that small-scale fine-tuning is ineffective, given limited exploration of alternative fine-tuning approaches

## Next Checks
1. Test the disambiguation methods across multiple languages and cultural contexts to assess generalizability
2. Conduct ablation studies with different temperature ranges and context generation strategies to optimize performance
3. Evaluate the computational cost and latency implications of the context addition approach in production settings