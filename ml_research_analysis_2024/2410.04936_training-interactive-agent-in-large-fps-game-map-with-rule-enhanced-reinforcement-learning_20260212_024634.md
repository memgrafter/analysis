---
ver: rpa2
title: Training Interactive Agent in Large FPS Game Map with Rule-enhanced Reinforcement
  Learning
arxiv_id: '2410.04936'
source_url: https://arxiv.org/abs/2410.04936
tags:
- agent
- game
- navigation
- nsrl
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PMCA, a rule-enhanced DRL system for large-scale
  3D FPS games, deployed in Arena Breakout. PMCA addresses global navigation and realistic
  combat challenges by combining navigation mesh (Navmesh) with shooting rules via
  the NSRL method.
---

# Training Interactive Agent in Large FPS Game Map with Rule-enhanced Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.04936
- Source URL: https://arxiv.org/abs/2410.04936
- Reference count: 14
- Key outcome: PMCA achieves 30% higher competitive win rate than pure RL by combining Navmesh navigation with rule-based shooting control in Arena Breakout

## Executive Summary
This paper introduces PMCA, a rule-enhanced DRL system for large-scale 3D FPS games, deployed in Arena Breakout. PMCA addresses global navigation and realistic combat challenges by combining navigation mesh (Navmesh) with shooting rules via the NSRL method. The DRL model decides when to use Navmesh, preserving exploration diversity while enabling long-range movement. Shooting rules control recoil and aiming, promoting human-like behavior. The framework uses PPO and IMPALA for training, with customized rewards. Experiments show NSRL significantly improves navigation range, competitive win rate (30% higher), and human-like shooting patterns over pure RL, enabling practical deployment in high-level matches.

## Method Summary
PMCA employs a hierarchical action architecture with 9 action heads generated sequentially using auto-regressive embedding and masking. The core innovation is the NSRL method that combines DRL decisions with Navmesh navigation and shooting rules. The DRL model predicts when to enable Navmesh for optimal global paths while maintaining local exploration capabilities. Shooting rules define confidence regions around targets to simulate human recoil control. Training uses PPO with IMPALA architecture, featuring 4 actors, 4 learners, and 6000 clients. The reward function includes navigation rewards, combat/movement auxiliary rewards, and final win/loss/draw rewards.

## Key Results
- NSRL improves global navigation range and efficiency compared to pure RL navigation
- Competitive win rate increases by 30% over pure RL baseline
- Rule-enhanced shooting produces more human-like bullet dispersion patterns
- Agent deployed in high-level competitive matches since early 2024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NSRL's hybrid decision system improves long-range navigation in large maps by combining DRL model predictions with precomputed Navmesh paths.
- Mechanism: The DRL model decides when to switch to Navmesh navigation, which provides optimal global paths. When Navmesh is enabled, the agent follows precomputed paths rather than attempting to navigate long distances with atomic steps. This reduces pathfinding errors and improves traversal efficiency.
- Core assumption: The DRL model can reliably predict when global navigation is needed versus when local atomic movement suffices.
- Evidence anchors:
  - [abstract] "The integration of Navmesh enhances the agent's global navigation capabilities while shooting behavior is controlled using rule-based methods to ensure controllability."
  - [section III.B] "The decision-making power to use the Navmesh is delegated to the DRL model, allowing the model to predict whether to enable the Navmesh."
  - [corpus] Weak - no direct neighbor evidence about hybrid navigation systems.
- Break condition: If the DRL model frequently mispredicts when to use Navmesh, the agent will either waste resources on unnecessary global navigation or fail to navigate long distances efficiently.

### Mechanism 2
- Claim: Rule-enhanced shooting control creates more human-like behavior than pure RL by constraining bullet dispersion patterns.
- Mechanism: Shooting rules define a confidence region around the target and limit bullet placement to this area, simulating human recoil control. The DRL model predicts gun barrel direction and firing decisions, but the actual shot placement follows the rule-based dispersion pattern.
- Core assumption: Human players control recoil by limiting shot dispersion to a predictable region around the target.
- Evidence anchors:
  - [abstract] "shooting behavior is controlled using rule-based methods to ensure controllability" and "shooting rules control recoil and aiming, promoting human-like behavior."
  - [section III.B] "We have set arctan(40/1500) as the baseline angle, and the confidence region radius r for different distances is calculated based on the baseline radius."
  - [corpus] Weak - no direct neighbor evidence about shooting rule constraints.
- Break condition: If the shooting rules are too restrictive, the agent may miss targets that fall outside the confidence region. If too permissive, the agent will exhibit non-human-like spray patterns.

### Mechanism 3
- Claim: Hierarchical action masking through auto-regressive embedding reduces action dependency conflicts while preserving exploration diversity.
- Mechanism: Actions are generated sequentially with masking based on previous actions. The policy learns which actions are incompatible (e.g., firing while moving) and masks them during prediction. Auto-regressive embedding propagates these constraints through the action generation chain.
- Core assumption: Action dependencies can be effectively learned and enforced through sequential generation with masking.
- Evidence anchors:
  - [section III.A] "We sequentially output the action heads using a hierarchical action mask with auto-regressive embedding" and "Through auto-regressive embedding, the policy implicitly learns the dependencies between preceding and succeeding actions."
  - [corpus] Weak - no direct neighbor evidence about hierarchical action masking.
- Break condition: If the action dependency learning fails, the agent may generate invalid action combinations or the masking may be too restrictive, limiting exploration.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: Arena Breakout must be modeled as an MDP to apply reinforcement learning algorithms. The state space, action space, transition dynamics, and reward function need clear definition.
  - Quick check question: What are the five components of the MDP tuple ⟨S, A, P, r, γ⟩ used in this paper?

- Concept: Proximal Policy Optimization (PPO) algorithm
  - Why needed here: PPO provides stable policy updates with clipped probability ratios, preventing destructive policy updates that could destabilize training in complex environments.
  - Quick check question: How does the PPO clip function prevent large policy updates, and what hyperparameter controls the clip range?

- Concept: Advantage estimation with Generalized Advantage Estimation (GAE)
  - Why needed here: GAE reduces variance in policy gradient estimates while maintaining low bias, which is crucial for stable training in the high-dimensional action space of FPS games.
  - Quick check question: What is the mathematical form of the GAE advantage estimator, and how do the discount factor γ and GAE parameter λ affect it?

## Architecture Onboarding

- Component map: Perception module -> LSTM -> Policy network -> Action generation -> Rule application -> Environment step -> Reward calculation -> Policy update
- Critical path: State observation → Feature extraction → LSTM encoding → Action generation → Rule application → Environment step → Reward calculation → Policy update
- Design tradeoffs:
  - Navmesh vs pure RL navigation: Navmesh provides optimal paths but reduces diversity; pure RL preserves diversity but struggles with global navigation
  - Rule-based vs learned shooting: Rules ensure human-like behavior but limit adaptability; pure RL could learn optimal patterns but may produce non-human-like behavior
  - Sequential vs parallel action generation: Sequential generation enables dependency masking but may be slower; parallel generation is faster but harder to manage dependencies
- Failure signatures:
  - Poor global navigation: Agent gets stuck or takes inefficient paths, visible in trajectory visualization
  - Non-human-like shooting: Bullet patterns show either excessive clustering or random dispersion
  - Invalid action combinations: Agent attempts incompatible actions (e.g., firing while jumping)
  - Training instability: Win rate fluctuates wildly or fails to improve over training steps
- First 3 experiments:
  1. Compare Navmesh vs pure RL navigation on a simple corridor environment to verify Navmesh provides optimal paths
  2. Test shooting rules by having agent shoot at fixed targets and measuring bullet dispersion patterns
  3. Validate action masking by attempting to generate known incompatible action combinations and confirming they're blocked

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the traditional sense, but it does highlight several areas for future work and exploration:

- The long-term implications of deploying the NSRL agent in high-level competitive matches on player experience and game balance
- How the agent handles dynamic changes in the game environment, such as moving obstacles or changing terrain
- How the NSRL agent's global navigation performance scales when applied to maps significantly larger than 1000x400 meters

## Limitations

- The paper lacks detailed evaluation of the DRL model's accuracy in predicting when to use Navmesh navigation
- No quantitative comparison of bullet dispersion patterns between rule-enhanced and pure RL agents is provided
- The hierarchical action masking mechanism lacks empirical validation showing its necessity compared to simpler action architectures

## Confidence

- Navigation improvement claims: High (supported by Navmesh integration logic and clear performance metrics)
- Win rate improvement: Medium (based on reported 30% increase but lacks statistical significance testing)
- Human-like shooting patterns: Low (qualitative claim without quantitative behavioral analysis)

## Next Checks

1. Analyze Navmesh decision accuracy by logging when the DRL model enables/disables global navigation and measuring the optimality of resulting paths
2. Compare bullet dispersion statistics (mean radius, clustering metrics) between rule-enhanced and pure RL agents across multiple shooting scenarios
3. Test action masking effectiveness by attempting to generate known invalid action combinations and measuring the rejection rate versus a baseline without masking