---
ver: rpa2
title: SNNAX -- Spiking Neural Networks in JAX
arxiv_id: '2409.02842'
source_url: https://arxiv.org/abs/2409.02842
tags:
- snnax
- learning
- neural
- spiking
- snns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SNNAX is a JAX-based framework for simulating and training spiking
  neural networks (SNNs) that combines PyTorch-like intuitiveness with JAX-like execution
  speed. Built on Equinox, SNNAX offers automatic differentiation, just-in-time compilation,
  and flexible automatic differentiation, enabling efficient training of SNNs while
  supporting custom architectures and neuromorphic hardware targets.
---

# SNNAX -- Spiking Neural Networks in JAX

## Quick Facts
- arXiv ID: 2409.02842
- Source URL: https://arxiv.org/abs/2409.02842
- Authors: Jamie Lohoff; Jan Finkbeiner; Emre Neftci
- Reference count: 31
- Primary result: JAX-based framework for simulating and training spiking neural networks that combines PyTorch-like intuitiveness with JAX-like execution speed

## Executive Summary
SNNAX is a JAX-based framework for simulating and training spiking neural networks (SNNs) that combines PyTorch-like intuitiveness with JAX-like execution speed. Built on Equinox, SNNAX offers automatic differentiation, just-in-time compilation, and flexible automatic differentiation, enabling efficient training of SNNs while supporting custom architectures and neuromorphic hardware targets. Performance comparisons show SNNAX achieves competitive execution times to other SNN libraries for feed-forward architectures without requiring custom CUDA kernels, and supports recurrent connections crucial for biologically plausible SNN models. The framework provides a user-friendly interface that maintains JAX's functional programming paradigm while simplifying state management and gradient computation for complex spiking dynamics.

## Method Summary
SNNAX leverages JAX's XLA compiler and Equinox's PyTree-based state management to implement spiking neural networks with automatic differentiation capabilities. The framework uses a step-by-step execution approach that evaluates time-loops over entire networks, enabling recurrent connections across layers. State management is handled through a thin layer that abstracts JAX's functional programming requirements, while surrogate gradient approaches circumvent the non-differentiable threshold function. The implementation supports various neuron models, learning rules, and hardware deployment through the Neuromorphic Intermediate Representation (NIR) initiative.

## Key Results
- Achieves competitive execution times for feed-forward architectures without custom CUDA kernels
- Supports recurrent connections across layers, enabling biologically plausible SNN models
- Provides automatic differentiation infrastructure for efficient surrogate gradient computation
- Integrates with hardware deployment frameworks through Neuromorphic Intermediate Representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNNAX achieves competitive execution times without custom CUDA kernels by leveraging JAX's XLA compiler and Equinox's PyTree-based state management.
- Mechanism: The XLA backend compiles JAX computational graphs into optimized machine code, enabling efficient GPU/TPU execution. Equinox represents network parameters and states as immutable PyTrees, which allows JAX's functional transformations (like jax.jit and jax.grad) to operate directly on model instances without additional wrappers.
- Core assumption: JAX's XLA compiler can optimize SNN-specific operations (threshold functions, state updates) as efficiently as hand-written CUDA kernels.
- Evidence anchors:
  - [abstract] "Performance comparisons show SNNAX achieves competitive execution times to other SNN libraries for feed-forward architectures without requiring custom CUDA kernels"
  - [section III-A] "SNNAX exploits JAX' XLA backend to run these on dedicated accelerators such as GPUs and TPUs"
  - [corpus] Weak evidence - no direct citations comparing XLA optimization to CUDA kernels
- Break condition: If XLA fails to optimize discontinuous spiking dynamics or recurrent state updates efficiently compared to specialized kernels.

### Mechanism 2
- Claim: The step-by-step execution approach enables recurrent connections crucial for biologically plausible SNN models while maintaining differentiability.
- Mechanism: By evaluating the entire network in a time-loop over all layers (rather than layer-by-layer unrolling), SNNAX preserves the temporal dependencies between layers that enable recurrent architectures. This approach treats SNNs as RNNs with multiple internal states, allowing backpropagation through time to compute gradients across recurrent connections.
- Core assumption: Step-by-step execution is necessary for implementing e-prop and other biologically plausible learning algorithms that require recurrent connectivity.
- Evidence anchors:
  - [section III-B] "SNNs are temporally evolving dynamical systems... Dealing with the internal states of the neurons is often tedious within JAX' functional programming paradigm. SNNAX therefore handles the state management behind a thin layer"
  - [section III-B] "SNNAX evaluates the time-loop over the entire network in a step-by-step manner... we believe that this design choice is necessary since it enables recurrent connections across layers, a key feature of bio-plausibe SNNs"
  - [corpus] Weak evidence - no direct citations demonstrating e-prop implementation in SNNAX
- Break condition: If step-by-step execution creates prohibitive performance bottlenecks that outweigh the benefits of recurrent connectivity.

### Mechanism 3
- Claim: JAX's automatic differentiation infrastructure enables efficient surrogate gradient computation for spiking neuron non-differentiabilities.
- Mechanism: SNNAX leverages JAX's advanced AD features to replace the non-differentiable threshold function with surrogate functions (e.g., sigmoid approximations). Since JAX supports custom gradient definitions and higher-order derivatives, users can implement and experiment with various surrogate gradient approaches without leaving the framework.
- Core assumption: Surrogate gradient methods can approximate the true gradient of spiking neuron dynamics sufficiently well for effective training.
- Evidence anchors:
  - [section III-C2] "Several approaches exist to circumvent the non-differentiable threshold with surrogate gradient approaches currently being the most popular solution... Since SNNAX is built on JAX' advanced AD features, it is straightforward to define these surrogates and customize them according to the users requirements"
  - [section III-C3] "JAX allows, with only very few limitations, to compute arbitrarily high orders of derivatives, thereby facilitating Meta-learning and learning-to-learn approaches"
  - [corpus] Weak evidence - no direct citations showing SNNAX's implementation of specific surrogate gradient methods
- Break condition: If surrogate gradients fail to propagate meaningful error signals through the spiking threshold, leading to poor learning performance.

## Foundational Learning

- Concept: JAX functional programming paradigm
  - Why needed here: Understanding JAX's pure function requirements and state management is crucial for correctly implementing and extending SNNAX models
  - Quick check question: How does JAX's immutability requirement affect the implementation of stateful spiking neuron dynamics?

- Concept: Automatic differentiation and backpropagation through time
  - Why needed here: SNN training relies on BPTT to compute gradients across temporal sequences, requiring understanding of how gradients flow through time-dependent states
  - Quick check question: What modifications are needed to standard BPTT when dealing with discontinuous spiking neuron activations?

- Concept: Neuromorphic hardware mapping and Neuromorphic Intermediate Representation (NIR)
  - Why needed here: SNNAX supports deployment to neuromorphic hardware, requiring understanding of how trained parameters map to hardware-specific constraints and representations
  - Quick check question: How does the NIR initiative enable cross-compatibility between different SNN frameworks and hardware targets?

## Architecture Onboarding

- Component map: Model definition -> State initialization -> Step-by-step execution loop -> Gradient computation -> Parameter updates -> Hardware mapping
- Critical path: Model definition → State initialization → Step-by-step execution loop → Gradient computation (via JAX AD) → Parameter updates (via Optax) → Hardware mapping (via NIR)
- Design tradeoffs: Step-by-step execution enables recurrent connections but may be slower than layer-by-layer approaches for feed-forward networks. The framework prioritizes flexibility and biological plausibility over maximum feed-forward performance.
- Failure signatures: Poor training performance may indicate inadequate surrogate gradient choice or learning rate; memory issues suggest need for gradient checkpointing; slow execution may require XLA optimization analysis.
- First 3 experiments:
  1. Implement a simple LIF neuron layer and verify forward pass with toy input data
  2. Train a two-layer MLP on MNIST using standard surrogate gradients and compare performance to baseline implementations
  3. Add recurrent connections to the network and implement e-prop to evaluate biologically plausible learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SNNAX's performance scale when simulating very large-scale SNNs with hundreds of thousands of neurons on distributed computing systems?
- Basis in paper: [explicit] The paper mentions that SNNAX achieves competitive execution times to other SNN libraries for feed-forward architectures without requiring custom CUDA kernels, but does not address scalability to extremely large models or distributed systems.
- Why unresolved: The paper focuses on comparing SNNAX with other frameworks using standard benchmarks and RTX 4090 GPU, but does not explore performance at scales beyond what fits on a single GPU or distributed multi-node systems.
- What evidence would resolve it: Comprehensive benchmark studies testing SNNAX on large-scale SNNs across multiple GPUs and distributed computing environments, comparing memory usage, execution time, and scalability metrics against established frameworks.

### Open Question 2
- Question: What are the limitations of SNNAX's surrogate gradient approaches for training SNNs with non-standard neuron models or complex synaptic dynamics?
- Basis in paper: [explicit] The paper mentions that SNNAX supports several commonly used surrogate functions for gradient computation across spikes, but does not discuss the limitations or performance of these approaches for custom neuron models or complex synaptic dynamics.
- Why unresolved: While the paper demonstrates SNNAX's support for standard neuron models and learning approaches, it does not investigate the effectiveness or limitations of surrogate gradients when applied to more complex or biologically detailed neuron models.
- What evidence would resolve it: Systematic evaluation of SNNAX's training performance using various surrogate gradient methods across a range of custom neuron models and synaptic plasticity rules, including comparison of convergence rates and final accuracy.

### Open Question 3
- Question: How does SNNAX's step-by-step execution approach impact performance compared to layer-by-layer execution for specific classes of recurrent SNN architectures?
- Basis in paper: [explicit] The paper states that SNNAX evaluates the time-loop over the entire network in a step-by-step manner as opposed to the layer-by-layer approach in Rockpool or Spyx, but does not provide quantitative performance comparisons for recurrent architectures.
- Why unresolved: While the paper justifies the step-by-step approach as necessary for enabling recurrent connections crucial for biologically plausible SNNs, it does not empirically demonstrate the performance trade-offs between step-by-step and layer-by-layer execution for different recurrent architectures.
- What evidence would resolve it: Detailed benchmark comparisons measuring execution time and memory usage of SNNAX's step-by-step approach versus layer-by-layer execution for various recurrent SNN architectures, including tasks requiring long-term memory or complex temporal dependencies.

## Limitations
- Performance claims lack direct comparative benchmarks against established SNN libraries
- Framework's ability to handle large-scale recurrent architectures remains untested
- Effectiveness of surrogate gradient methods for complex learning tasks is assumed rather than demonstrated
- Hardware deployment capabilities through NIR are described conceptually but lack implementation details

## Confidence
- **High**: JAX-based implementation with Equinox/PyTree state management, automatic differentiation capabilities
- **Medium**: Competitive performance claims, recurrent connection support, surrogate gradient framework
- **Low**: Hardware deployment effectiveness, large-scale model performance, specific learning algorithm implementations

## Next Checks
1. Run benchmark comparisons on standard Neurobench tasks using SNNAX, BindsNET, and SpikingJelly on identical hardware to verify performance claims
2. Implement a recurrent SNN with e-prop learning and evaluate convergence on temporal pattern recognition tasks
3. Test hardware deployment pipeline by exporting a trained model through NIR to a supported neuromorphic platform and measuring execution efficiency