---
ver: rpa2
title: 'CarLLaVA: Vision language models for camera-only closed-loop driving'
arxiv_id: '2406.10165'
source_url: https://arxiv.org/abs/2406.10165
tags:
- driving
- language
- autonomous
- waypoints
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CarLLaVA addresses autonomous driving by using a vision-language
  model (VLM) without requiring expensive sensors or labeled data. It leverages the
  vision encoder of LLaVA-NeXT and the LLaMA architecture to achieve state-of-the-art
  closed-loop driving performance using only camera input.
---

# CarLLaVA: Vision language models for camera-only closed-loop driving

## Quick Facts
- arXiv ID: 2406.10165
- Source URL: https://arxiv.org/abs/2406.10165
- Authors: Katrin Renz; Long Chen; Ana-Maria Marcu; Jan Hünermann; Benoit Hanotte; Alice Karnsund; Jamie Shotton; Elahe Arani; Oleg Sinavski
- Reference count: 40
- Primary result: Ranked 1st in CARLA Autonomous Driving Challenge 2.0, outperforming previous state-of-the-art by 458% and best concurrent submission by 32.6%

## Executive Summary
CarLLaVA demonstrates that vision-language models pretrained on internet-scale data can effectively transfer to autonomous driving tasks using only camera input. By leveraging the vision encoder of LLaVA-NeXT and the LLaMA architecture, the system achieves state-of-the-art closed-loop driving performance without expensive sensors or labeled data. The approach employs a semi-disentangled output representation combining path waypoints for lateral control and standard waypoints for longitudinal control, improving both steering and collision avoidance. Training efficiency is enhanced through data bucketing that focuses on challenging scenarios rather than trivial data.

## Method Summary
CarLLaVA uses a vision encoder pre-trained on internet-scale vision-language data (CLIPViT-L-336px with LLaVA's "anyres" technique) combined with LLaMA architecture for autonomous driving. The model employs a semi-disentangled output representation using path waypoints for lateral control and standard waypoints for longitudinal control. Training uses data bucketing to focus on challenging scenarios across six categories (acceleration/deceleration, steering, vehicle hazards, stop signs/red lights/walkers, swerving around obstacles, and uneventful data). The system was evaluated on CARLA Town 12 and Town 13 datasets, achieving first place in the CARLA Autonomous Driving Challenge 2.0.

## Key Results
- Ranked 1st in CARLA Autonomous Driving Challenge 2.0
- Outperformed previous state-of-the-art by 458%
- Outperformed best concurrent submission by 32.6%
- Achieved efficient training in 27 hours using data bucketing approach

## Why This Works (Mechanism)

### Mechanism 1
Vision-Language Pretraining from CLIP/LLaVA provides superior feature representations for driving tasks compared to training from scratch. Pretrained vision encoder learns rich visual features from internet-scale image-text pairs, capturing semantic understanding that transfers well to driving scenarios. This allows the model to recognize complex objects and situations with minimal task-specific training. Core assumption: Visual features learned from general internet data are transferable to driving-specific tasks without extensive domain adaptation.

### Mechanism 2
Semi-disentangled output representation (path waypoints for lateral control + standard waypoints for longitudinal control) improves driving performance. Path waypoints provide denser supervision for steering behavior since they're predicted even when the vehicle is stationary, while standard waypoints maintain better collision avoidance for speed control. This separation addresses the different control requirements for lateral and longitudinal dynamics. Core assumption: Lateral and longitudinal control can be effectively decoupled and optimized separately while maintaining overall driving performance.

### Mechanism 3
Efficient data bucketing focuses training on challenging scenarios, reducing compute waste on trivial data. By categorizing data into buckets based on difficulty and sampling proportionally, the model spends more time learning from interesting scenarios while still maintaining exposure to easy driving segments. Core assumption: Easy driving scenarios are learned quickly and don't require extensive training, while challenging scenarios benefit from additional training time.

## Foundational Learning

- Concept: Vision Transformers and Multi-Head Attention
  - Why needed here: CarLLaVA uses CLIP's vision transformer encoder, which relies on self-attention mechanisms to process visual information. Understanding how attention works is crucial for interpreting the model's visual processing.
  - Quick check question: How does multi-head attention in vision transformers differ from traditional convolutional feature extraction?

- Concept: Vision-Language Pre-training and Transfer Learning
  - Why needed here: The model leverages pre-trained vision-language models (CLIP/LLaVA) and transfers their knowledge to driving tasks. Understanding pre-training objectives and transfer mechanisms is essential.
  - Quick check question: What are the key differences between CLIP's contrastive learning objective and traditional supervised image classification?

- Concept: End-to-End Driving Output Representations
  - Why needed here: CarLLaVA uses a semi-disentangled representation with path waypoints and standard waypoints. Understanding different output representations and their control implications is crucial for model design.
  - Quick check question: How do path waypoints differ from standard waypoints in terms of control behavior and supervision density?

## Architecture Onboarding

- Component map: Camera image → Vision encoder → Adapter → LLaMA → Output head → Waypoints → PID control
- Critical path: Camera image → Vision encoder → Adapter → LLaMA → Output head → Waypoints → PID control
- Design tradeoffs:
  - High resolution vs. computational cost: Using "anyres" technique to balance detail and efficiency
  - Semi-disentangled vs. fully entangled outputs: Better control separation but potentially more complex training
  - Camera-only vs. multi-sensor: Simpler deployment but may miss some environmental cues
- Failure signatures:
  - Poor steering behavior: Check path waypoint predictions and lateral PID controller
  - Collision issues: Examine standard waypoint predictions and longitudinal control
  - General driving errors: Verify vision encoder features and overall waypoint generation
- First 3 experiments:
  1. Test vision encoder feature quality by visualizing attention maps and checking if important driving elements (traffic lights, pedestrians) are captured
  2. Validate output representation by comparing semi-disentangled vs. fully entangled performance on simple driving scenarios
  3. Test data bucketing effectiveness by training on different bucket combinations and measuring learning efficiency on challenging scenarios

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of LoRA finetuning affect the scalability of VLMs to larger parameter counts (e.g., 1B+ models) for autonomous driving tasks? The paper reports that scaling up to 1B parameters with LoRA finetuning resulted in worse performance compared to smaller models, suggesting unresolved issues with LoRA's effectiveness at larger scales. Comparative experiments testing different finetuning strategies and hyperparameter optimizations for 1B+ parameter models would clarify LoRA's limitations.

### Open Question 2
Can the semi-disentangled output representation be further optimized to reduce steering errors in complex scenarios like unprotected turns or obstacle avoidance? The paper notes that the semi-disentangled representation improves lateral control but still observes failure cases like rear-end collisions and merging difficulties. Ablation studies comparing the semi-disentangled approach to other representations in complex driving scenarios would identify the optimal architecture.

### Open Question 3
How does the efficiency of the data bucketing strategy generalize to other driving datasets or real-world data collection scenarios? The paper introduces data bucketing to reduce training on trivial data but only validates it on the CARLA dataset. Testing the bucketing strategy on diverse real-world driving datasets would validate its generalizability.

### Open Question 4
What is the impact of temporal and multi-view inputs on driving performance in scenarios where rear-end collisions or lane changes are critical? The paper reports preliminary results showing improved rear-end collision avoidance with temporal input and better lane-change behavior with a back camera, but these improvements do not translate to higher overall scores. Comprehensive evaluations of temporal and multi-view inputs in specific critical scenarios with detailed failure mode analysis would clarify their true impact.

## Limitations
- Limited ablation studies on output representation entanglement levels
- Data bucketing strategy effectiveness needs validation across different datasets
- Comparison with concurrent submissions limited to single entry

## Confidence
- High confidence: Vision-language pretraining transfers effectively to driving tasks
- Medium confidence: Semi-disentangled output representation design
- Medium confidence: Data bucketing efficiency claim

## Next Checks
1. Conduct output representation ablation study to identify optimal entanglement levels between lateral and longitudinal control
2. Evaluate CarLLaVA's performance on driving scenarios outside CARLA Challenge benchmarks for real-world robustness assessment
3. Perform controlled computational efficiency benchmark comparing training time and resource usage with and without data bucketing across different hardware configurations