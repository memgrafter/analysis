---
ver: rpa2
title: Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference
  Gaps in Entailment Verification
arxiv_id: '2402.03686'
source_url: https://arxiv.org/abs/2402.03686
tags:
- premise
- reasoning
- datasets
- hypothesis
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies entailment verification (EV) of multi-sentence
  premises, which is important for tasks requiring complex multi-hop reasoning like
  detecting inconsistent model-generated rationales. Current NLI datasets mostly contain
  short premises, so we compile a benchmark from three domains (NLI, QA, rationales)
  containing multi-sentence premises.
---

# Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification

## Quick Facts
- arXiv ID: 2402.03686
- Source URL: https://arxiv.org/abs/2402.03686
- Reference count: 36
- Primary result: Ranking-based finetuning of Flan-T5 outperforms GPT-3.5 and rivals GPT-4 on entailment verification, and improves CoT filtering accuracy by 6% on average

## Executive Summary
This work studies entailment verification (EV) of multi-sentence premises, a critical task for complex multi-hop reasoning in applications like detecting inconsistent model-generated rationales. The authors compile a benchmark from three domains (NLI, QA, rationales) containing multi-sentence premises, as current NLI datasets mostly contain short premises. They evaluate humans and LLMs on this benchmark, finding that LLMs outperform humans at multi-hop reasoning across long contexts, while humans excel at simple deductive reasoning tasks. The paper also finetunes a Flan-T5 model using classification and ranking objectives, with ranking performing better, especially on contextual QA datasets. Finally, they demonstrate that their model can filter inconsistent model-generated rationales in self-consistency decoding, achieving a 6% accuracy improvement on average across three MCQ datasets.

## Method Summary
The authors compile a benchmark from three domains containing multi-sentence premises for entailment verification. They evaluate humans and LLMs on this benchmark, finding LLMs are better at multi-hop reasoning while humans excel at simple deductive tasks. To improve performance, they finetune Flan-T5-xxl using classification and ranking objectives. For ranking, they generate negative hypotheses via GPT-3.5 prompting. The ranking objective outperforms classification, particularly on contextual QA datasets. Finally, they use the best model to filter inconsistent model-generated rationales in self-consistency decoding, achieving a 6% average accuracy improvement across three MCQ datasets.

## Key Results
- LLMs outperform humans at multi-hop reasoning across long contexts, while humans perform better at simple deductive reasoning tasks
- Ranking-based finetuning is better than classification, especially for contextual QA datasets, as it learns a softer decision boundary
- Finetuned Flan-T5-xxl outperforms GPT-3.5 and rivals GPT-4 on the entailment verification benchmark
- Filtering inconsistent model-generated rationales in self-consistency decoding improves accuracy by 6% on average across three MCQ datasets

## Why This Works (Mechanism)

### Mechanism 1: Ranking Objective Advantage
The ranking-based objective outperforms classification because it learns a softer decision boundary, particularly useful when incorrect options are "less wrong" rather than clearly wrong. In contextual QA datasets, options are designed to be plausible distractors. A classification objective forces the model to hard-classify each hypothesis, potentially penalizing the model for not perfectly discriminating between similar options. Ranking allows the model to focus on relative plausibility between options, naturally creating a softer boundary that better reflects the inherent ambiguity in distractor quality. The ranking loss margin captures the notion that "good" hypotheses should be preferred over "weaker" hypotheses, not just distinguished from "wrong" ones.

### Mechanism 2: LLM Context Processing Advantage
LLMs are better than humans at complex multi-step reasoning because they can process longer contexts without the same short-term memory limitations. Cognitive psychology studies show humans can only retain about four chunks in short-term memory, limiting their ability to track information across multiple sentences. LLMs trained on long-context data can maintain coherence over extended premises, enabling them to combine multiple pieces of information for complex inferences. The LLM's parametric knowledge and attention mechanisms allow it to maintain relevant information across the entire premise length.

### Mechanism 3: Human Deductive Reasoning Advantage
Humans are more consistent at simple deductive reasoning because they excel at basic logical operations like substitutions, negations, and word meaning understanding. Simple deductive tasks require minimal information combination and rely on well-practiced logical operations that humans perform reliably. Humans have strong intuitions for basic logical relationships and can quickly apply substitutions, understand negations, and recognize word meanings without extensive context processing. Humans have developed robust heuristics for simple logical operations through extensive practice and education.

## Foundational Learning

- **Macro-F1 vs. Accuracy/Micro-F1**: The datasets have label imbalance (more "not support" labels), making macro-F1 the appropriate metric as it treats all classes equally regardless of frequency. Quick check: If a dataset has 90% "not support" labels and 10% "support" labels, which metric would better reflect performance on the minority class?

- **Entailment Verification vs. Natural Language Inference**: EV focuses on multi-sentence premises requiring complex reasoning, while traditional NLI datasets often contain short premises that only partially capture these challenges. Quick check: What key distinction between premise length and reasoning complexity makes entailment verification a distinct task from standard NLI?

- **Ranking Objective vs. Classification Objective**: The ranking objective is beneficial for contextual QA because it learns a softer decision boundary that better handles plausible distractors. Quick check: How does the ranking loss formulation differ from cross-entropy loss in terms of what it optimizes?

## Architecture Onboarding

- **Component map**: Premise-Hypothesis Pair Input → Prompt Template → LLM → Token Probability Distribution → Score Calculation (Equation 1) → Label Assignment (threshold-based); For finetuning: Training Dataset → Prompt Template → Model → Loss Function (Cross-entropy or Ranking Loss) → Parameter Updates; For filtering CoTs: Multiple CoT Outputs → QA-to-Statement Converter → Entailment Verification → Score Ranking → Top-k Selection → Majority Voting

- **Critical path**: 1. Prompt template construction (Box 1 format) 2. Score calculation using token probabilities 3. Label assignment based on threshold 4. (Optional) Finetuning using either classification or ranking objective

- **Design tradeoffs**: Prompt format: Different prompts yield similar results (robustness), but consistency is important for fair evaluation; Threshold selection: Default 0.5 works well, but dataset-specific tuning could improve performance; Ranking margin (m): Controls how strongly the model should prefer one hypothesis over another

- **Failure signatures**: Poor performance on datasets with high label imbalance indicates need for class weighting or different metrics; Inconsistent results across different prompt formats suggest model sensitivity issues; Failure to improve on contextual QA datasets suggests ranking objective may not be learning the right comparisons

- **First 3 experiments**: 1. Test different prompt formats with Flan-T5-xxl to verify robustness (Table 6) 2. Compare classification vs. ranking objectives on contextual QA datasets (Table 4) 3. Evaluate few-shot performance vs. finetuned models (Table 7)

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the entailment verification models vary across different reasoning types (simple deductive, complex deductive, missing entity-grounded/commonsense knowledge, missing localized knowledge)? The paper mentions that humans are more consistent in simple deductive reasoning, while LLMs excel at complex inferences, but doesn't provide a detailed breakdown of performance across different reasoning types for the models.

### Open Question 2
How does the ranking-based finetuning approach compare to the classification-based approach in terms of generalization to unseen datasets? The paper mentions that ranking-based finetuning is better than classification for contextual QA datasets, but doesn't provide a detailed comparison of the generalization abilities of the two approaches to unseen datasets.

### Open Question 3
How does the filtering of inconsistent model-generated rationales in self-consistency decoding affect the performance of different base models (e.g., UL2, Codex-001, LaMDA-137B, ChatGPT)? The paper mentions that filtering improves performance over the self-consistency baseline, with more gains for weaker base models such as UL2, but doesn't provide a detailed analysis of the effect of filtering on the performance of different base models.

## Limitations
- Dataset construction validity concerns due to potential artifacts from QA option conversion and NLI label merging
- Human evaluation constraints with 60-minute time limits may have affected reasoning quality, especially for complex multi-hop tasks
- Limited testing of ranking objective improvements across different parameter settings and negative sampling strategies
- Downstream application scope limited to three MCQ datasets, with broader applicability unverified

## Confidence

**High Confidence**:
- The core finding that LLMs outperform humans on multi-hop reasoning over extended contexts
- The ranking objective's superiority over classification for contextual QA datasets
- The general trend that humans perform better on simple deductive reasoning tasks

**Medium Confidence**:
- The specific magnitude of performance differences between humans and LLMs
- The robustness of ranking objective improvements across different parameter settings
- The exact contribution of entailment verification filtering to downstream accuracy improvements

**Low Confidence**:
- The generalizability of findings to other reasoning domains beyond the three tested
- The long-term stability of LLM advantages as context windows and reasoning capabilities evolve

## Next Checks
1. Conduct systematic experiments varying the ranking margin (m) and negative sampling strategies to determine the sensitivity of the ranking objective's performance improvements and identify optimal parameter ranges

2. Apply the best-performing entailment verification model to at least two additional domains not included in the original benchmark (e.g., scientific reasoning or commonsense inference) to assess the broader applicability of the human-LLM performance gap findings

3. Repeat the human evaluation with extended time limits (e.g., 2+ hours per participant) and expert annotators to establish upper bounds on human performance for multi-hop reasoning tasks, helping to better understand the true capability gap between humans and LLMs