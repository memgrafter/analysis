---
ver: rpa2
title: 'David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs
  at the Deep Edge'
arxiv_id: '2404.05688'
source_url: https://arxiv.org/abs/2404.05688
tags:
- adversarial
- int-8
- float-32
- attacks
- qnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive empirical evaluation of adversarial
  attacks and defenses for quantized neural networks (QNNs) at the deep edge. The
  study focuses on three QNNs targeting TinyML applications, ten attacks, and six
  defenses.
---

# David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge

## Quick Facts
- arXiv ID: 2404.05688
- Source URL: https://arxiv.org/abs/2404.05688
- Reference count: 40
- Primary result: Comprehensive evaluation of adversarial attacks and defenses on quantized neural networks shows quantization increases decision boundary distance but introduces gradient masking challenges.

## Executive Summary
This paper presents a comprehensive empirical study evaluating adversarial attacks and defenses on quantized neural networks (QNNs) targeting TinyML applications at the deep edge. The authors systematically evaluate ten different attacks and six defenses across three QNN models on different datasets, comparing 8-bit and 16-bit quantization. Their findings reveal that quantization significantly impacts adversarial robustness through multiple mechanisms including increased decision boundary distance, gradient explosion/vanishing phenomena, and reduced adversarial transferability due to quantization-shift effects. The study also highlights the limitations of existing defenses when applied to QNNs and identifies key challenges for future research in this domain.

## Method Summary
The authors evaluate adversarial robustness of QNNs using TensorFlow Lite Micro quantization policy (8-bit or 16-bit) on three models across three datasets. They systematically test ten adversarial attacks (white-box, gray-box, and black-box variants) and six defenses (three train-based and three input pre-processing). The evaluation measures adversarial accuracy, distortion metrics (L0, L1, L2, Lâˆž norms), and point distance to decision boundaries. All artifacts are open-sourced to enable independent validation. The study focuses on understanding how quantization affects attack effectiveness and defense performance compared to full-precision models.

## Key Results
- Quantization increases the average point distance to the decision boundary, providing inherent robustness
- Gradient explosion or vanishing occurs in quantized models depending on perturbation magnitude
- Input pre-processing defenses perform well against small perturbations but degrade with larger attacks
- Train-based defenses increase decision boundary distance but struggle with quantization-shift and gradient misalignment
- Adversarial example transferability from float-32 ANNs to QNNs is reduced by quantization effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization increases the point distance to the decision boundary.
- Mechanism: Reducing bit-width increases quantization error, pushing decision boundaries farther from data points.
- Core assumption: The average point distance to the decision boundary increases as the precision of a neural network decreases.
- Evidence anchors:
  - [abstract] "quantization increases the point distance to the decision boundary"
  - [section] "The fact that the distortion significantly increases for similar or higher adversarial accuracy as the precision lowers suggests that the average point distance to the decision boundary increases as precision lowers."
  - [corpus] Weak - related papers do not discuss decision boundary distance explicitly.
- Break condition: If quantization policy changes to reduce quantization error (e.g., higher bit-width or different scaling), this mechanism weakens.

### Mechanism 2
- Claim: Quantization can cause gradient explosion or vanishing.
- Mechanism: Small perturbations may be erased by quantization, making gradients vanish; large perturbations can cause gradients to explode due to significant output variation.
- Core assumption: Quantization makes it more difficult to optimize over the loss surface.
- Evidence anchors:
  - [abstract] "quantization increases the point distance to the decision boundary and leads the gradient estimated by some attacks to explode or vanish"
  - [section] "Small perturbations may be erased by quantization and make the gradient vanish while bigger perturbations can make the gradient explode."
  - [corpus] Weak - related papers do not discuss gradient explosion/vanishing in quantized models.
- Break condition: If gradient estimation techniques improve (e.g., better approximation methods), this mechanism weakens.

### Mechanism 3
- Claim: Adversarial example transferability from float-32 ANNs to QNNs is reduced by quantization-shift and gradient misalignment.
- Mechanism: Quantization maps multiple float-32 values into the same int-16 or int-8 value, altering decision boundaries and gradient directions.
- Core assumption: Quantization causes gradient misalignment and quantization-shift phenomena.
- Evidence anchors:
  - [abstract] "quantization can act as a noise attenuator or amplifier, depending on the noise magnitude, and causes gradient misalignment"
  - [section] "By mapping different ANN weights and activations into the same integer value, quantization can cause gradient misalignment: the gradient estimated for float-32 ANNs may not be the same for the equivalent int-16 or int-8 models and even have different directions."
  - [corpus] Weak - related papers do not discuss transferability reduction in quantized models.
- Break condition: If defenses are developed that account for quantization-shift and gradient misalignment, this mechanism weakens.

## Foundational Learning

- Concept: Quantization policies (e.g., TF Lite Micro)
  - Why needed here: Understanding the quantization policy is crucial as it affects the robustness of QNNs against adversarial examples.
  - Quick check question: What are the key differences between TF Lite Micro quantization policy and other quantization policies?

- Concept: Adversarial attacks (white-box, gray-box, black-box)
  - Why needed here: Knowing the types of attacks and their mechanisms is essential for evaluating the robustness of QNNs.
  - Quick check question: How do white-box, gray-box, and black-box attacks differ in their approach to crafting adversarial examples?

- Concept: Adversarial defenses (train-based, input pre-processing)
  - Why needed here: Understanding the types of defenses and their mechanisms is crucial for evaluating their effectiveness on QNNs.
  - Quick check question: What are the key differences between train-based and input pre-processing defenses?

## Architecture Onboarding

- Component map: CIFAR-10 -> ResNet-8 -> Attacks/Defenses -> QNN Evaluation; Visual Wake Words -> DS-CNN -> Attacks/Defenses -> QNN Evaluation; Coffee Disease -> DS-CNN -> Attacks/Defenses -> QNN Evaluation
- Critical path: 1. Load dataset and model; 2. Apply quantization (8-bit or 16-bit); 3. Craft adversarial examples using attacks; 4. Apply defenses (if any); 5. Evaluate adversarial accuracy and distortion
- Design tradeoffs: Bit-width vs. accuracy: Lower bit-width increases robustness but may decrease accuracy; Defense type vs. deployment: Train-based defenses are portable but less robust for small perturbations; input pre-processing defenses are more robust but require additional computation
- Failure signatures: Gradient masking: Attack fails to produce adversarial examples due to quantization; Quantization-shift: Small perturbations are erased, while large perturbations are amplified; Gradient misalignment: Gradients estimated for float-32 ANNs differ from those of QNNs
- First 3 experiments: 1. Evaluate the robustness of QNNs against direct attacks (no defense) for 8-bit and 16-bit quantization; 2. Evaluate the transferability of adversarial examples from float-32 ANNs to QNNs for 8-bit and 16-bit quantization; 3. Evaluate the effectiveness of train-based defenses on QNNs for 8-bit and 16-bit quantization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quantization policy of TF Lite Micro specifically increase adversarial robustness compared to other quantization policies (e.g., binary connect, Dorefa Net)?
- Basis in paper: [explicit] The paper states that most previous works use policies that enable gradient approximation through STE or Mirror Descent, which are not applicable in TF Lite Micro, suggesting their policy is distinct.
- Why unresolved: The paper only compares QNNs using the TF Lite Micro policy to full-precision ANNs, not to QNNs using other quantization policies.
- What evidence would resolve it: Empirical evaluation of adversarial robustness comparing QNNs using TF Lite Micro policy to QNNs using other quantization policies (e.g., binary connect, Dorefa Net) under the same attack scenarios.

### Open Question 2
- Question: Can train-based defenses specifically designed to address quantization-shift and gradient misalignment effectively mitigate adversarial example transferability to QNNs?
- Basis in paper: [inferred] The paper concludes that existing train-based defenses do not significantly affect adversarial example transferability and suggests that smoothing quantization-shift and gradient misalignment is required.
- Why unresolved: The paper does not evaluate train-based defenses specifically designed to address these phenomena.
- What evidence would resolve it: Empirical evaluation of novel train-based defenses explicitly designed to smooth quantization-shift and gradient misalignment, comparing their effectiveness in mitigating adversarial transferability to QNNs against existing defenses.

### Open Question 3
- Question: What is the impact of quantization on the adversarial robustness of QNNs for tasks other than image classification (e.g., natural language processing, speech recognition)?
- Basis in paper: [inferred] The paper focuses on image processing tasks and concludes that quantization increases the average point distance to the decision boundary, but this may not generalize to other task domains.
- Why unresolved: The paper only evaluates image classification tasks and does not explore the generalizability of its findings to other domains.
- What evidence would resolve it: Empirical evaluation of adversarial robustness of QNNs for various task domains (e.g., NLP, speech recognition) under the same attack and defense scenarios, comparing the impact of quantization across domains.

## Limitations

- The study only evaluates image classification tasks, limiting generalizability to other domains like NLP or speech recognition
- Only one quantization policy (TF Lite Micro) is evaluated without comparison to alternative quantization schemes
- Exact training hyperparameters for baseline models are not specified, potentially affecting reproducibility
- The observed robustness mechanisms need further experimental isolation to confirm causality

## Confidence

- Quantization increasing decision boundary distance: Medium
- Quantization causing gradient explosion/vanishing: Low
- Transferability reduction due to quantization-shift: Medium

## Next Checks

1. Verify gradient zero density calculations across different quantization bit-widths to confirm gradient masking effects
2. Test transferability of adversarial examples between float-32 and QNN models using identical attack configurations
3. Compare robustness results using alternative quantization policies (e.g., symmetric vs asymmetric scaling) to isolate policy-specific effects