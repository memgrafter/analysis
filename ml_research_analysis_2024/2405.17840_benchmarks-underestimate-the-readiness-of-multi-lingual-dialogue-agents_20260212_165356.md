---
ver: rpa2
title: Benchmarks Underestimate the Readiness of Multi-lingual Dialogue Agents
arxiv_id: '2405.17840'
source_url: https://arxiv.org/abs/2405.17840
tags:
- dialogue
- state
- language
- association
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to multilingual task-oriented
  dialogue (TOD) using in-context learning with large language models (LLMs). The
  key challenge addressed is the high cost of training data acquisition for multilingual
  TOD systems.
---

# Benchmarks Underestimate the Readiness of Multi-lingual Dialogue Agents

## Quick Facts
- **arXiv ID**: 2405.17840
- **Source URL**: https://arxiv.org/abs/2405.17840
- **Reference count**: 12
- **Primary result**: Multi-stage in-context learning pipeline achieves 89.6%-96.8% DST accuracy and >99% response generation accuracy across 6 languages

## Executive Summary
This paper demonstrates that hierarchical in-context learning can achieve state-of-the-art performance on multilingual task-oriented dialogue without fine-tuning. The authors develop a multi-stage pipeline that breaks down dialogue state tracking into schema selection, state generation, and entity normalization steps, each optimized with focused few-shot examples. Their approach achieves remarkable accuracy across six languages including challenging code-mixed Hindi-English, while requiring no expensive training data acquisition.

## Method Summary
The authors propose a multi-stage pipeline for multilingual TOD using in-context learning with GPT-4/GPT-3.5. The pipeline consists of schema selection (identifying relevant domains), dialogue state generation using the selected schema, and entity normalization for enumerated slots. Other subtasks (ACD, DAG, RG) use simple few-shot prompting. The approach is evaluated on the X-RiSAWOZ dataset covering 12 domains in Chinese, English, French, Korean, Hindi, and code-mixed Hindi-English.

## Key Results
- 89.6%-96.8% DST accuracy across 6 languages using in-context learning
- >99% correct response generation accuracy across languages
- LLM-based entity normalization outperforms dictionary-based approaches by 9.6% for Hindi and 22.2% for code-mixed Hindi-English
- Manual evaluation reveals significant gold label errors in multilingual datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical prompting breaks down DST into schema selection, dialogue state generation, and entity normalization steps that are more compatible with in-context learning
- Mechanism: Each stage uses few-shot examples specific to that subtask, reducing cognitive load on the LLM and improving accuracy in handling complex dialogue state schemas
- Core assumption: LLMs can effectively learn specialized subtasks through focused in-context examples rather than general few-shot examples
- Evidence anchors: The paper shows hierarchical prompting improves DST accuracy across languages compared to naive prompting

### Mechanism 2
- Claim: LLM-based entity normalization outperforms dictionary-based approaches for enumerated slot values
- Mechanism: LLMs can dynamically map generated slot values to ontology values without requiring pre-built translation dictionaries, handling linguistic diversity better
- Core assumption: LLMs can generalize better than static dictionaries for entity normalization across languages
- Evidence anchors: LLM-approach is significantly better for Hindi and English-Hindi by 9.6% and 22.2%, respectively, compared to dictionary-based approaches

### Mechanism 3
- Claim: Manual evaluation reveals significant gold label errors in multilingual datasets that automated metrics fail to account for
- Mechanism: Human evaluation can distinguish between true LLM errors and annotation issues, providing more accurate assessment of system performance
- Core assumption: Current automatic metrics are inadequate for evaluating multilingual in-context learning systems due to dataset annotation issues
- Evidence anchors: After correcting gold label errors and improving dataset annotation schema, GPT-4 achieves 89.6%-96.8% DST accuracy and >99% response generation accuracy

## Foundational Learning

- **Concept**: In-context learning with few-shot examples
  - Why needed here: Traditional fine-tuning requires large amounts of labeled data which is expensive to acquire for multilingual systems
  - Quick check question: What is the key difference between in-context learning and traditional fine-tuning?

- **Concept**: Dialogue state tracking (DST) as a core TOD subtask
  - Why needed here: DST is identified as the most challenging subtask requiring proper domain and slot handling
  - Quick check question: What are the three main challenges in DST mentioned in the paper?

- **Concept**: Entity normalization for enumerated slots
  - Why needed here: Generated slot values must match ontology values, requiring normalization for enumerated types
  - Quick check question: Why can't dictionary-based approaches handle all entity normalization cases?

## Architecture Onboarding

- **Component map**: User utterance → Schema Selection → Dialogue State Generation → Entity Normalization → ACD → DAG → RG
- **Critical path**: User utterance → DST pipeline (3 stages) → ACD → DAG → RG
- **Design tradeoffs**: Hierarchical prompting increases complexity but improves accuracy; manual evaluation provides accuracy but is resource-intensive
- **Failure signatures**: Domain misclassification, incorrect slot updates, failed entity normalization, verbose responses
- **First 3 experiments**:
  1. Compare naive prompting vs hierarchical prompting for DST accuracy
  2. Test dictionary-based vs LLM-based entity normalization on Hindi and English-Hindi
  3. Conduct manual evaluation of 1000 responses per language for RG accuracy

## Open Questions the Paper Calls Out

- **Question**: How well would this approach generalize to truly low-resource languages?
  - Basis in paper: The paper acknowledges this limitation, stating "it is likely that there will be a significant drop in quality when this work is evaluated on truly low-resource languages."
  - Why unresolved: The paper only tests on six languages, including Hindi and a code-mixed variant, which are not considered low-resource. Testing on languages with minimal digital presence would be valuable.
  - What evidence would resolve it: Evaluating the in-context learning approach on a multilingual TOD dataset that includes low-resource languages like Swahili, Bengali, or indigenous languages with limited digital data.

- **Question**: How would the performance change with different dialogue representation formats beyond slot-value pairs?
  - Basis in paper: The paper mentions this limitation, noting that the dataset uses slot-values "as opposed to more complex representations like SQL, or less common representations like TreeDST, dataflow or ThingTalk which LLMs might not be familiar with."
  - Why unresolved: The paper only tests on slot-value representations. Exploring more complex representations could reveal new challenges or advantages for in-context learning.
  - What evidence would resolve it: Adapting the in-context learning approach to work with alternative dialogue representations like SQL or dataflow, and comparing the performance to the slot-value approach on the same dataset.

## Limitations

- The approach may not scale well to hundreds of domains or truly low-resource languages
- Manual evaluation methodology is not fully specified, making reproducibility challenging
- Entity normalization relies on LLM capabilities that may vary across model versions and language pairs

## Confidence

**High confidence**: The hierarchical prompting approach effectively improves DST accuracy compared to naive few-shot prompting, as demonstrated by the consistent performance improvements across multiple languages.

**Medium confidence**: The claim that current benchmarks underestimate multilingual TOD system readiness due to annotation errors is supported by the manual evaluation, but the extent of this issue across other datasets remains uncertain.

**Low confidence**: The assertion that in-context learning approaches will replace fine-tuning for multilingual TOD systems requires more evidence, as the paper doesn't address scalability, cost, or performance on more diverse language pairs.

## Next Checks

1. **Replication study**: Re-run the DST pipeline on an independent multilingual TOD dataset (e.g., multilingual MultiWOZ) to verify the 89.6%-96.8% accuracy claims and assess generalizability.

2. **Error analysis extension**: Perform the same manual evaluation methodology on automatically generated dialogue datasets to determine if the annotation error rate is dataset-specific or systematic across multilingual TOD evaluation.

3. **Cost-benefit analysis**: Compare the operational costs and development time between the proposed in-context learning approach and traditional fine-tuning across different languages, accounting for API usage costs and few-shot example preparation.