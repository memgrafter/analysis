---
ver: rpa2
title: Challenges in explaining deep learning models for data with biological variation
arxiv_id: '2406.09981'
source_url: https://arxiv.org/abs/2406.09981
tags:
- methods
- explanations
- gradients
- data
- explainability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in explaining deep learning models
  for biological data, specifically grain images with diseases like pink fusarium
  and skinned grains. The core method involves evaluating various post-hoc explainability
  methods (e.g., Gradients, LRP, LIME, SHAP) on grain classification models.
---

# Challenges in explaining deep learning models for data with biological variation

## Quick Facts
- arXiv ID: 2406.09981
- Source URL: https://arxiv.org/abs/2406.09981
- Authors: Lenka Tětková; Erik Schou Dreier; Robin Malm; Lars Kai Hansen
- Reference count: 40
- One-line primary result: LRP (EpsilonPlusFlat) consistently performs best across different metrics and data types for grain disease classification explainability

## Executive Summary
This paper addresses the challenge of explaining deep learning models for biological data, specifically grain images with diseases like pink fusarium and skinned grains. The study evaluates 14 post-hoc explainability methods (including Gradients, LRP, LIME, SHAP) on grain classification models using metrics like robustness to data augmentation, faithfulness, complexity, and similarity to ground truth annotations. The primary finding is that LRP (EpsilonPlusFlat) consistently outperforms other methods, though rankings vary depending on the specific evaluation criteria. The study also demonstrates that aggregating explanations from multiple methods often outperforms individual methods, highlighting the importance of careful hyperparameter selection and problem-specific evaluation frameworks.

## Method Summary
The study employs a comprehensive evaluation framework for explainability methods on grain disease classification. The core approach involves applying 14 different post-hoc explainability methods to a SimpleNet architecture trained on grain images. The evaluation uses multiple metrics including robustness to data augmentation (brightness, rotation, translation), faithfulness (pixel-flipping, IROF), complexity (sparsity, entropy), and similarity to ground truth annotations. A novel ranking aggregation method using mean reciprocal rank with Monte Carlo simulation is proposed to combine these metrics into a robust overall ranking specific to the biological data at hand.

## Key Results
- LRP (EpsilonPlusFlat) consistently outperforms other explainability methods across different metrics and data types
- Aggregating explanations from multiple methods shows promise, often outperforming individual methods
- The ranking aggregation method using mean reciprocal rank with Monte Carlo simulation provides reliable method selection
- Faithfulness scores correlate with robustness scores, suggesting these metrics capture related aspects of explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRP (EpsilonPlusFlat) consistently outperforms other explainability methods because its propagation rules align well with the hierarchical feature extraction of grain damage classification.
- Mechanism: Layer-wise Relevance Propagation redistributes the prediction score backward through the network, assigning relevance to input pixels based on learned weights. EpsilonPlusFlat uses the epsilon rule for dense layers and ZPlus rule for convolutional layers, preserving non-negative relevance values that match human perception of disease markers.
- Core assumption: Grain damage detection relies on spatially localized features that can be meaningfully propagated backward without distortion.
- Evidence anchors:
  - [abstract] "LRP (EpsilonPlusFlat) consistently performs best across different metrics and data types"
  - [section 3.3] "We use an architecture inspired by a similar architecture originally proposed for hyperspectral images in [30]"
- Break condition: If grain damage features are distributed non-locally or involve complex interactions that cannot be decomposed into additive pixel relevances.

### Mechanism 2
- Claim: Aggregating explanations from multiple methods improves robustness by averaging out method-specific biases and artifacts.
- Mechanism: Individual explainability methods have different sensitivities to data augmentation, hyperparameters, and channel pooling. Averaging their outputs creates a consensus explanation that captures the most stable and reliable features.
- Core assumption: Different explainability methods capture complementary aspects of model behavior, and their combination reduces variance without introducing systematic bias.
- Evidence anchors:
  - [abstract] "Aggregating explanations from multiple methods also shows promise, often outperforming individual methods"
  - [section 4.2] "A notable observation was the superior performance of aggregated explanations... which, in some instances, surpassed even the best-performing single method"
- Break condition: If explainability methods are highly correlated in their errors or if aggregation dilutes critical but method-specific signals.

### Mechanism 3
- Claim: The proposed ranking aggregation using mean reciprocal rank with Monte Carlo simulation provides a more reliable method selection than simple averaging.
- Mechanism: Individual metrics have overlapping uncertainty intervals and different scales. Monte Carlo sampling from the distributions of metric scores captures the uncertainty in rankings, while MRR ensures that consistently high performance across metrics is rewarded.
- Core assumption: The distributions of metric scores across images are approximately normal and independent, allowing valid statistical aggregation.
- Evidence anchors:
  - [abstract] "We propose a method for aggregating these results into one robust ranking specific for the data in hand"
  - [section 4.4] "We could rely on the means only if the distributions were sufficiently separated. In our case, sampling from the distributions is necessary for reliable aggregation of the metrics"
- Break condition: If metric distributions are highly non-normal or strongly correlated, invalidating the Monte Carlo assumptions.

## Foundational Learning

- Concept: Post-hoc explainability methods and their mathematical foundations
  - Why needed here: Understanding how different methods (Gradients, LRP, LIME, SHAP) compute attributions is essential for interpreting results and choosing appropriate methods for grain data
  - Quick check question: What is the fundamental difference between gradient-based methods and perturbation-based methods like LIME and SHAP?

- Concept: Evaluation metrics for explainability (faithfulness, robustness, complexity, similarity to ground truth)
  - Why needed here: The paper uses multiple metrics to assess explanations, and understanding what each metric measures is crucial for interpreting the aggregated rankings
  - Quick check question: How does pixel-flipping measure faithfulness differently from IROF?

- Concept: Data augmentation and its effects on model robustness
  - Why needed here: The study evaluates robustness to data augmentation, which requires understanding how different augmentation types (brightness, rotation, translation) affect both model predictions and explanations
  - Quick check question: Why are brightness, hue, and saturation considered invariant augmentations while rotation and translation are equivariant?

## Architecture Onboarding

- Component map: Grain images → preprocessing (masking, channel selection) → train/validation/test split → SimpleNet model → explainability methods → multiple evaluation metrics → ranking aggregation
- Critical path: Model training → explanation generation → metric computation → ranking aggregation → result interpretation
- Design tradeoffs: SimpleNet architecture chosen for interpretability vs. more complex architectures that might capture more nuanced features
- Failure signatures: Poor performance of explainability methods may indicate model reliance on spurious correlations or dataset bias
- First experiments: 1) Evaluate individual explainability methods on held-out test data, 2) Test robustness to data augmentation, 3) Compare aggregated explanations to individual method performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should ground truth explanations be defined for biological data with inherent variability?
- Basis in paper: [explicit] The paper discusses challenges in defining ground truth, noting that human annotators may create explanations based on what they think is important, while machine learning models may solve tasks differently.
- Why unresolved: The paper highlights that biological samples often have multiple semantic components leading to complex and entangled signals, making it difficult to pinpoint what part of the grain is most important for prediction.
- What evidence would resolve it: Empirical studies comparing different ground truth annotation methods (e.g., highlighting whole grains vs. specific damaged areas) and their impact on evaluation metrics for explanation methods.

### Open Question 2
- Question: What is the optimal way to aggregate multiple explanation methods into a single, robust explanation?
- Basis in paper: [explicit] The paper proposes a method for aggregating explanations from different methods into a mean heatmap, finding that this often outperforms individual methods.
- Why unresolved: While the paper shows promise for aggregation, it doesn't explore other potential aggregation strategies (e.g., voting, segment-level aggregation) or provide a theoretical framework for when and how to aggregate explanations.
- What evidence would resolve it: Systematic comparison of different aggregation strategies across multiple datasets and evaluation metrics, along with theoretical analysis of the properties of aggregated explanations.

### Open Question 3
- Question: How does the choice of model architecture influence the performance of different explainability methods?
- Basis in paper: [inferred] The paper notes that the applicability of some explainability methods is closely linked to model architectures, particularly for LRP which relies on model canonization.
- Why unresolved: The paper focuses on a single model architecture (SimpleNet) and doesn't explore how explainability method rankings might change with different architectures.
- What evidence would resolve it: Comparative studies of explainability method performance across multiple model architectures (e.g., ResNet, VGG, Transformer-based models) for the same dataset and task.

## Limitations
- The evaluation framework relies heavily on proxy metrics rather than direct validation against true model behavior
- The grain image dataset, while domain-specific, has limited size (14,166 images) compared to larger computer vision benchmarks
- The assumption that aggregated explanations provide more reliable results needs further validation, as the underlying correlation structure between different explainability methods remains unexplored

## Confidence
- High confidence: LRP (EpsilonPlusFlat) performance across metrics - supported by multiple independent evaluations
- Medium confidence: Aggregated explanations superiority - promising but needs replication with different datasets
- Medium confidence: Ranking aggregation methodology - statistically sound but domain-specific validation required

## Next Checks
1. Test the aggregated explanation approach on a completely different domain (e.g., medical imaging) to verify cross-domain robustness
2. Conduct ablation studies on the ranking aggregation method using synthetic data with known ground truth to validate the Monte Carlo approach
3. Measure the correlation structure between different explainability methods to determine if aggregation truly provides complementary information or redundant signals