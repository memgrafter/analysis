---
ver: rpa2
title: 'Technical report: Improving the properties of molecules generated by LIMO'
arxiv_id: '2407.14968'
source_url: https://arxiv.org/abs/2407.14968
tags:
- molecules
- limo
- space
- latent
- property
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper conducts ablative studies to improve the Latent Inceptionism
  on Molecules (LIMO) framework for generating molecules with desired properties.
  It investigates variants of molecular representation (SELFIES, GroupSELFIES), decoder
  model (non-autoregressive vs autoregressive Transformer), and surrogate model training
  scheme (sequential vs joint training).
---

# Technical report: Improving the properties of molecules generated by LIMO

## Quick Facts
- arXiv ID: 2407.14968
- Source URL: https://arxiv.org/abs/2407.14968
- Reference count: 11
- Key outcome: Ablative studies improve LIMO framework for generating molecules with desired properties

## Executive Summary
This technical report presents ablative studies to improve the Latent Inceptionism on Molecules (LIMO) framework for generating molecules with desired properties. The authors investigate three key components: molecular representation (SELFIES vs GroupSELFIES), decoder model architecture (non-autoregressive vs autoregressive Transformer), and surrogate model training scheme (sequential vs joint training). Experiments on the ZINC-250K dataset demonstrate that an autoregressive Transformer decoder with GroupSELFIES tokenization achieves the best average properties for random generation, outperforming the original LIMO model. Joint training of the VAE and surrogate model leads to smoother latent space and better property predictions, while the generated molecules exhibit superior properties compared to the training dataset.

## Method Summary
The method involves training a beta-VAE with GroupSELFIES tokenization on the ZINC-250K dataset, using either autoregressive or non-autoregressive decoder architectures. The VAE maps molecular strings to a continuous latent space, which is then optimized for target properties using a jointly trained surrogate model. The optimized latent points are decoded back to molecular strings. The authors compare different combinations of molecular representations, decoder types, and training schemes, evaluating generated molecules using oracle property predictors for synthetic accessibility (SA), drug-likeness (QED), and binding affinity (BA) to the human estrogen receptor.

## Key Results
- Autoregressive Transformer decoder with GroupSELFIES achieves best average properties for random generation
- Joint training of VAE and surrogate model leads to smoother latent space (Dirichlet energy 33.31 vs 94.04) and better property predictions
- Generated molecules show superior properties compared to the ZINC-250K training dataset
- GroupSELFIES tokenization captures larger chemical fragments, improving molecular representation

## Why This Works (Mechanism)

### Mechanism 1
Joint training of VAE and surrogate improves latent space smoothness, leading to better property optimization. When VAE and surrogate are trained sequentially, the latent space is unaware of property gradients, causing discontinuous property values across nearby latent points. Joint training allows the VAE to be informed by property feedback, creating a smoother latent-to-property mapping.

### Mechanism 2
Autoregressive decoder with GroupSELFIES achieves better property optimization than non-autoregressive decoder. AR models can capture full joint token dependencies without the multi-modality problem inherent to NAR models. GroupSELFIES further improves by enabling larger, chemically meaningful fragments, reducing invalid token sequences.

### Mechanism 3
GroupSELFIES tokenization improves molecular representation over standard SELFIES by capturing larger chemical fragments. GroupSELFIES allows tokens to represent larger molecular fragments, improving the generative model's ability to capture meaningful chemical substructures and dependencies, leading to higher-quality molecule generation.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture and training objectives.
  - Why needed here: LIMO uses a VAE to map molecules to a continuous latent space for optimization; understanding ELBO loss, KL divergence, and reconstruction is critical.
  - Quick check question: What is the purpose of the KL divergence term in the VAE loss, and how does weighting it (beta-VAE) affect latent space learning?

- Concept: Autoregressive vs non-autoregressive sequence generation.
  - Why needed here: The paper compares AR and NAR decoders; understanding their tradeoffs (speed vs dependency modeling) is essential for modifying LIMO.
  - Quick check question: Why do NAR models suffer from the multi-modality problem, and how does an AR model avoid it?

- Concept: Molecular string representations (SELFIES vs GroupSELFIES).
  - Why needed here: Different tokenization schemes directly affect the quality of generated molecules; understanding their guarantees and limitations is key.
  - Quick check question: How does GroupSELFIES ensure chemical validity while allowing larger token fragments?

## Architecture Onboarding

- Component map: ZINC-250K dataset -> VAE Encoder -> VAE Decoder -> Surrogate Model -> Reverse Optimization
- Critical path: 1. Train VAE on molecular strings. 2. Train surrogate property predictor. 3. Optimize latent points for target properties. 4. Decode optimized latent points to molecules.
- Design tradeoffs: AR vs NAR decoder (AR gives better property optimization but slower generation); Sequential vs joint training (joint training smooths latent space but increases complexity); Tokenization granularity (GroupSELFIES can improve chemical coherence but may require careful fragmentation).
- Failure signatures: High KL divergence vanishing during VAE training; Surrogate predictions mismatch oracle values; Generated molecules invalid.
- First 3 experiments: 1. Reproduce baseline LIMO with SELFIES + NAR decoder + sequential surrogate training. 2. Swap NAR decoder for AR Transformer decoder; compare property scores. 3. Train VAE and surrogate jointly; measure latent space smoothness and property gains.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of molecular string representation (SELFIES vs GroupSELFIES variants) affect the quality of the generated molecules beyond property optimization? The paper shows property optimization results but doesn't deeply investigate why certain GroupSELFIES variants perform better or how the token extraction method impacts molecular quality in other aspects beyond the tested properties.

### Open Question 2
Can the latent space organization be improved through topological constraints while maintaining or enhancing property optimization performance? The paper identifies this as future work but doesn't explore how topological constraints might balance the tradeoff between smoothness and local gradient accuracy in the latent space.

### Open Question 3
How can semi-autoregressive models bridge the tradeoff between controllable generation and sequence modeling ability in molecular generation? While the paper discusses this as a future direction, it doesn't investigate how semi-autoregressive approaches would perform specifically for molecular generation or how they would affect scaffold-constrained generation capabilities.

## Limitations
- Missing ablation studies isolating the effect of each modification (joint training, GroupSELFIES, AR decoder)
- No direct comparison with alternative state-of-the-art molecule generation approaches
- Evaluation relies on oracle property predictors rather than experimental validation

## Confidence
- Joint training effectiveness: Medium confidence (strong internal ablation evidence but no corpus comparison)
- Autoregressive vs non-autoregressive: Medium confidence (theoretical explanation supported by empirical ablation)
- Superiority over original LIMO: Low confidence (missing isolated ablation data for each improvement)

## Next Checks
1. Conduct an ablation study isolating the effect of joint training by comparing sequential vs joint training with identical VAE and decoder architectures (SELFIES + NAR).
2. Evaluate the impact of GroupSELFIES tokenization independently by training with GroupSELFIES + NAR decoder and comparing to SELFIES + NAR decoder under the same training scheme.
3. Compare LIMO variants against alternative molecular generation approaches (e.g., diffusion models, graph-based methods) on standardized benchmarks to contextualize the claimed improvements.