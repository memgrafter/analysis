---
ver: rpa2
title: 'Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for
  Robust Large Vision-Language Models'
arxiv_id: '2402.12336'
source_url: https://arxiv.org/abs/2402.12336
tags:
- clip
- adversarial
- robust
- vision
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the vulnerability of large vision-language models
  (LVLMs) to adversarial attacks on image inputs. It proposes an unsupervised adversarial
  fine-tuning method, FARE, to robustify the CLIP vision encoder while preserving
  its original embeddings.
---

# Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models

## Quick Facts
- arXiv ID: 2402.12336
- Source URL: https://arxiv.org/abs/2402.12336
- Reference count: 40
- Key outcome: Unsupervised adversarial fine-tuning of CLIP vision encoder significantly improves robustness against adversarial attacks while maintaining or improving clean performance on downstream tasks.

## Executive Summary
This paper addresses the vulnerability of large vision-language models (LVLMs) to adversarial attacks on image inputs by proposing an unsupervised adversarial fine-tuning method called FARE. The approach robustifies the CLIP vision encoder while preserving its original embeddings, enabling plug-and-play substitution without retraining downstream LVLMs. Experiments demonstrate that FARE-CLIP significantly improves robustness against various attacks including stealthy targeted attacks, while maintaining or even improving clean performance on downstream tasks like image captioning, visual question answering, and zero-shot classification compared to supervised alternatives. The method also reduces hallucinations and maintains reasoning capabilities in LVLMs.

## Method Summary
The FARE method performs unsupervised adversarial fine-tuning of the CLIP vision encoder using a loss function that maximizes the ℓ2 distance between robust and original embeddings under adversarial perturbations, then minimizes this difference during training. The approach preserves CLIP's original embedding space for clean inputs through an ℓ2 distance constraint, allowing plug-and-play substitution of the vision encoder without retraining downstream LVLMs. Training uses ImageNet data, 10 steps of PGD, cosine decaying learning rate schedule, weight decay of 1e-4, batch size of 128, and two epochs of training.

## Key Results
- FARE-CLIP significantly improves robustness against multiple attack types (PGD, Square, MIM, T-FGSM, AutoAttack, TTR, ZOO, TIM) compared to vanilla CLIP.
- Maintains or improves clean performance on downstream tasks including image captioning, visual question answering, and zero-shot classification.
- Reduces hallucination rates and maintains chain-of-thought reasoning capabilities in LVLMs.
- Outperforms supervised robust alternatives like TeCoA on both robustness and clean performance metrics.

## Why This Works (Mechanism)

### Mechanism 1
The unsupervised adversarial fine-tuning preserves CLIP's original embedding space for clean inputs by minimizing the ℓ2 distance between fine-tuned and original embeddings during training. This ensures clean inputs map to nearly identical feature vectors, allowing plug-and-play substitution of the vision encoder without retraining downstream LVLMs. The core assumption is that preserving the ℓ2 distance between embeddings also preserves cosine similarity for zero-shot classification tasks.

### Mechanism 2
The adversarial fine-tuning creates robustness by forcing the model to produce similar embeddings under adversarial perturbations. The loss function maximizes the embedding difference under adversarial attacks, then the model learns to minimize this difference, effectively creating a robust decision boundary around clean embeddings. The core assumption is that making embeddings similar under perturbations is sufficient to maintain task performance under attack.

### Mechanism 3
The unsupervised nature of the fine-tuning allows it to generalize better across different downstream tasks. By not being tied to any specific dataset labels or classification scheme, the fine-tuning preserves the general semantic alignment of CLIP embeddings, making them useful for diverse tasks like captioning and VQA. The core assumption is that CLIP's pre-trained embedding space is general enough that preserving it leads to good performance across diverse tasks.

## Foundational Learning

- **Concept**: Adversarial training and its variants (PGD, AT)
  - Why needed here: The method uses projected gradient descent to find adversarial examples during fine-tuning, which is fundamental to creating robustness
  - Quick check question: What is the difference between adversarial training and adversarial fine-tuning in the context of pre-trained models?

- **Concept**: Contrastive learning and embedding spaces
  - Why needed here: CLIP's core functionality relies on mapping images and text to a shared embedding space where semantically similar pairs are close; understanding this is crucial for grasping why preserving embeddings matters
  - Quick check question: How does CLIP's contrastive loss differ from standard classification loss?

- **Concept**: Vision-language model architecture
  - Why needed here: The method works by replacing the frozen vision encoder in LVLMs; understanding this architecture is essential for seeing how the approach provides robustness without retraining
  - Quick check question: In LLaVA's architecture, what component is frozen and why?

## Architecture Onboarding

- **Component map**: CLIP vision encoder (ViT-L/14) → Embedding space → Downstream tasks (zero-shot classification, LLaVA captioning, visual question answering)
- **Critical path**: Clean input → Original CLIP embedding → Downstream task output; Adversarial input → Robust CLIP embedding → Protected downstream task output
- **Design tradeoffs**: Unsupervised fine-tuning trades some ImageNet-specific robustness for better generalization across diverse tasks
- **Failure signatures**: If clean performance drops significantly, it indicates the embedding preservation isn't working; if adversarial robustness is weak, the adversarial training isn't effective
- **First 3 experiments**:
  1. Verify that FARE-CLIP produces nearly identical embeddings to original CLIP on clean ImageNet validation set
  2. Test that LLaVA with FARE-CLIP produces similar outputs to original LLaVA on clean COCO images
  3. Verify that LLaVA with FARE-CLIP resists targeted attacks that break original LLaVA on the same images

## Open Questions the Paper Calls Out

### Open Question 1
How does the FARE method perform when applied to newer LVLMs that fine-tune their vision encoders instead of using frozen ones? The paper mentions that the method can be easily extended to newer LVLMs which fine-tune the vision encoder, but this is not experimentally tested. Experimental results showing the performance of FARE on LVLMs with fine-tuned vision encoders would resolve this question.

### Open Question 2
What is the impact of using robust CLIP-enabled LVLMs for instruction following, explainability, and perception-related tasks? The paper does not examine the influence of using robust CLIP-enabled LVLMs for these specific tasks. Experimental results demonstrating the performance of robust CLIP-enabled LVLMs on instruction following, explainability, and perception tasks would resolve this question.

### Open Question 3
How does the FARE method compare to other unsupervised adversarial fine-tuning approaches for CLIP-like models? The paper does not compare FARE to other unsupervised adversarial fine-tuning methods for CLIP-like models. A comparative study of FARE against other unsupervised adversarial fine-tuning methods would resolve this question.

## Limitations

- Limited direct evidence that ℓ2 distance preservation translates to cosine similarity preservation across all downstream tasks
- Evaluation focuses primarily on attack robustness rather than demonstrating clear generalization advantages on clean performance
- Insufficient experimental validation showing embedding preservation translates to task performance across diverse datasets

## Confidence

- **High confidence**: Core approach's ability to improve adversarial robustness, supported by extensive attack evaluation results
- **Medium confidence**: Claim that clean performance is maintained or improved, demonstrated but with limited comparison to supervised alternatives
- **Low confidence**: Mechanism-level explanations, particularly regarding how embedding preservation translates to task performance

## Next Checks

1. **Embedding Space Analysis**: Measure and compare cosine similarities between clean and adversarial embeddings for both original CLIP and FARE-CLIP across multiple downstream tasks to verify that embedding preservation translates to task performance.

2. **Generalization Benchmark**: Conduct head-to-head comparison of FARE-CLIP against supervised robust alternatives (TeCoA, BALMS) on clean performance across all tested datasets to validate the claimed generalization advantages.

3. **Robustness Under Stress**: Test FARE-CLIP's performance under extreme adversarial conditions (higher epsilon values, stronger attack configurations) to identify the limits of the robust region and potential failure modes.