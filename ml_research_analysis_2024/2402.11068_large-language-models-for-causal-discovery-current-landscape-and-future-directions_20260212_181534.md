---
ver: rpa2
title: 'Large Language Models for Causal Discovery: Current Landscape and Future Directions'
arxiv_id: '2402.11068'
source_url: https://arxiv.org/abs/2402.11068
tags:
- causal
- discovery
- llms
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically examines the integration of Large Language
  Models (LLMs) with causal discovery (CD), presenting a comprehensive analysis of
  how LLMs can enhance traditional causal discovery methods. The paper identifies
  three primary approaches: direct causal inference from text, posterior refinement
  of statistical results, and integration of domain knowledge as priors.'
---

# Large Language Models for Causal Discovery: Current Landscape and Future Directions

## Quick Facts
- arXiv ID: 2402.11068
- Source URL: https://arxiv.org/abs/2402.11068
- Authors: Guangya Wan; Yunsheng Lu; Yuqi Wu; Mengxuan Hu; Sheng Li
- Reference count: 8
- Primary result: Survey examines three approaches for integrating LLMs with causal discovery: direct inference, posterior refinement, and domain knowledge integration

## Executive Summary
This survey systematically examines the integration of Large Language Models (LLMs) with causal discovery (CD), presenting a comprehensive analysis of how LLMs can enhance traditional causal discovery methods. The paper identifies three primary approaches: direct causal inference from text, posterior refinement of statistical results, and integration of domain knowledge as priors. It provides practical examples of prompts for various causal discovery frameworks and reviews benchmark datasets and applications across diverse domains. The authors highlight key challenges including the need for standardized evaluation protocols and the development of domain-specific LLMs.

## Method Summary
The paper analyzes how LLMs can be integrated with causal discovery through three distinct approaches: (1) direct inference from natural language descriptions of variables to infer causal relationships without statistical data analysis, (2) posterior refinement where LLMs evaluate and correct statistical causal discoveries against their knowledge base, and (3) integration of domain knowledge as priors into traditional causal discovery algorithms. The methods involve using LLMs to generate probabilistic priors about edge existence and direction that are incorporated into statistical causal discovery frameworks, or using LLMs as expert judges to refine learned causal structures.

## Key Results
- LLMs can serve as automated domain experts for causal discovery by synthesizing knowledge from their pre-training corpus
- Three distinct approaches identified: direct inference from text, posterior refinement of statistical results, and integration of domain knowledge as priors
- Current limitations include lack of standardized evaluation protocols and need for domain-specific LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can function as automated domain experts for causal discovery by synthesizing knowledge from their pre-training corpus.
- Mechanism: LLMs process natural language descriptions of variables to infer causal relationships without requiring statistical data analysis.
- Core assumption: The LLM's pre-training corpus contains sufficient domain knowledge to reason about causal relationships between variables.
- Evidence anchors:
  - [abstract] "LLMs excel at natural language processing and generation" and can "serve as comprehensive prior information sources"
  - [section 3.1] "LLMs can utilize their broad understanding of domain concepts, scientific principles, and real-world relationships to infer causality at scale"
  - [corpus] Weak - corpus lacks direct causal discovery benchmarks, but contains related work on LLMs for causal reasoning
- Break condition: If the LLM's pre-training corpus lacks relevant domain knowledge or contains predominantly correlational information rather than causal understanding.

### Mechanism 2
- Claim: LLMs can refine and validate causal structures derived from traditional statistical methods through posterior correction.
- Mechanism: LLMs evaluate statistical causal discoveries against their knowledge base to identify and correct potential errors in edge orientations or relationships.
- Core assumption: LLMs possess sufficient causal reasoning capability to distinguish valid causal relationships from spurious correlations.
- Evidence anchors:
  - [abstract] LLMs can provide "refinement of causal structures" and function as "posterior correction mechanisms"
  - [section 3.2] "LLMs can serve as a expert judge by correcting and refining the learned causal structures from the traditional statistical causal discovery (SCD) methods"
  - [corpus] Limited - corpus contains related work but lacks direct evidence of LLM posterior correction effectiveness
- Break condition: When LLM responses are overly conservative or when the statistical method produces results too complex for LLM reasoning.

### Mechanism 3
- Claim: LLMs can integrate domain knowledge as priors into traditional causal discovery algorithms, enhancing their performance.
- Mechanism: LLMs generate probabilistic priors about edge existence and direction that are incorporated into statistical causal discovery frameworks.
- Core assumption: LLM-derived priors can improve causal discovery accuracy when combined with statistical methods.
- Evidence anchors:
  - [abstract] LLMs can provide "integration of domain knowledge into statistical methods"
  - [section 3.3] "LLMs can also be used in conjunction with traditional methods to provide source of prior knowledge by leveraging meta-data extracted from textual descriptions"
  - [corpus] Moderate - contains related work on LLM priors but limited direct evaluation evidence
- Break condition: When LLM priors are inconsistent with observed data or when the integration method fails to properly weight the prior information.

## Foundational Learning

- Concept: Markov equivalence classes
  - Why needed here: Understanding why traditional causal discovery methods can only identify DAGs up to their Markov equivalence class is crucial for appreciating LLM's role in breaking this limitation
  - Quick check question: Why can multiple DAGs represent the same set of conditional independence relationships?

- Concept: Conditional independence testing
  - Why needed here: LLMs can serve as oracle for conditional independence tests in constraint-based causal discovery methods
  - Quick check question: How does conditional independence testing relate to edge removal in the PC algorithm?

- Concept: Structural Causal Models (SCM)
  - Why needed here: LLMs can help verify SCM properties like functional forms and noise distributions, not just graph structures
  - Quick check question: What components make up a Structural Causal Model and how do they relate to causal discovery?

## Architecture Onboarding

- Component map: Input (variable descriptions, metadata, or statistical outputs) -> LLM module (natural language processing and causal reasoning) -> Integration layer (converts LLM outputs to causal discovery inputs) -> Evaluation framework (benchmarks and metrics)

- Critical path:
  1. Prepare variable descriptions or statistical outputs
  2. Generate LLM prompts for causal reasoning
  3. Process LLM responses into usable format
  4. Integrate with causal discovery algorithm
  5. Evaluate results against benchmarks

- Design tradeoffs:
  - Direct inference vs. posterior refinement: Speed vs. accuracy
  - Hard constraints vs. soft priors: Rigidity vs. flexibility
  - General-purpose vs. domain-specific LLMs: Breadth vs. depth

- Failure signatures:
  - Hallucination in LLM responses leading to incorrect causal relationships
  - Over-conservatism in independence judgments
  - Inconsistency between LLM priors and observed data

- First 3 experiments:
  1. Pairwise causal discovery on small synthetic datasets using direct LLM inference
  2. Posterior correction of PC algorithm output using LLM as expert judge
  3. Integration of LLM-derived priors into LiNGAM or NOTEARS framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized evaluation protocols for LLM-based causal discovery that account for both synthetic datasets (to avoid data leakage) and real-world benchmarks while ensuring fair comparison across different approaches?
- Basis in paper: [explicit] The paper explicitly identifies "lack of standardized evaluation protocols and comprehensive comparative analyses" as a significant challenge and proposes evaluation using both synthetic datasets and established benchmarks
- Why unresolved: Current research lacks systematic comparison between different LLM-based approaches, making it difficult to establish true state-of-the-art performance
- What evidence would resolve it: A comprehensive evaluation framework incorporating multiple performance metrics beyond accuracy, including subgraph and domain-specific metrics, computational efficiency, and standardized sampling procedures for large graphs

### Open Question 2
- Question: What are the key architectural and training modifications needed to develop domain-specific LLMs that can reliably serve as expert agents for causal discovery in specialized fields like medicine, finance, or molecular biology?
- Basis in paper: [explicit] The paper envisions "significant potential in developing domain-specialized models and expert agents that can better capture field-specific causal relationships" and suggests integrating LLMs with field-specific knowledge bases
- Why unresolved: Current approaches primarily utilize general-purpose LLMs, which may lack the specialized knowledge required for accurate causal reasoning in specific domains
- What evidence would resolve it: Comparative studies showing improved performance of domain-specialized models versus general LLMs on field-specific causal discovery tasks, along with analysis of optimal integration strategies with domain knowledge bases

### Open Question 3
- Question: How can we distinguish between genuine causal reasoning and pattern recognition or "causal parroting" in LLMs when performing causal discovery tasks?
- Basis in paper: [explicit] The paper references studies suggesting "LLMs are like parrots in that they simply recite the causal knowledge embedded in the data" and provides evidence that current LLMs function as "weak 'causal parrots'"
- Why unresolved: Despite advancements in statistical causal discovery, the capability of LLMs to perform genuine causal discovery remains questionable, with unclear understanding of their internal causal reasoning processes
- What evidence would resolve it: Development of evaluation frameworks that can reliably differentiate genuine causal reasoning from spurious correlations, including systematic probing studies of attention patterns and intervention-based task analysis

## Limitations

- Limited Empirical Validation: The paper identifies a significant gap in the current literature - while multiple approaches for using LLMs in causal discovery are proposed, there is a lack of standardized evaluation protocols and comprehensive empirical validation.

- Generalization Concerns: The performance of LLMs for causal discovery may be highly dependent on their pre-training corpus and the specific domain knowledge required, with current LLMs potentially lacking sufficient causal reasoning capability.

- Technical Integration Challenges: Several open challenges exist in effectively integrating LLM outputs with traditional causal discovery algorithms, including how to properly weight LLM-derived priors against observed data.

## Confidence

**High Confidence**: The identification of three distinct approaches for LLM integration with causal discovery (direct inference, posterior refinement, and prior integration) is well-supported by the literature review and logical analysis. The mechanisms described are coherent and theoretically sound.

**Medium Confidence**: The assessment of current limitations and challenges is well-founded based on the literature review, but the proposed solutions and future directions are more speculative given the nascent state of this field.

**Low Confidence**: Specific performance claims about LLM effectiveness in causal discovery tasks are difficult to assess due to the lack of standardized benchmarks and comprehensive empirical studies in the literature.

## Next Checks

1. **Benchmark Development**: Create standardized datasets and evaluation protocols specifically for LLM-assisted causal discovery, including both synthetic and real-world causal graphs across multiple domains.

2. **Cross-Domain Validation**: Test LLM performance on causal discovery tasks across diverse domains (healthcare, economics, social sciences) to assess generalization capabilities and identify domain-specific challenges.

3. **Integration Framework Testing**: Implement and evaluate specific methods for integrating LLM-derived priors with traditional causal discovery algorithms, measuring the impact on accuracy and computational efficiency compared to standalone approaches.