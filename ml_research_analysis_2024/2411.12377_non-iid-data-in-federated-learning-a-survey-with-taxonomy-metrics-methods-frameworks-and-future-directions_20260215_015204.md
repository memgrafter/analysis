---
ver: rpa2
title: 'Non-IID data in Federated Learning: A Survey with Taxonomy, Metrics, Methods,
  Frameworks and Future Directions'
arxiv_id: '2411.12377'
source_url: https://arxiv.org/abs/2411.12377
tags:
- data
- learning
- federated
- non-iid
- skew
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive systematic review of non-IID
  data challenges in federated learning, addressing the lack of consensus in classifying
  and quantifying data heterogeneity. The authors developed a detailed taxonomy covering
  six main types of data heterogeneity (label skew, attribute skew, quantity skew,
  spatiotemporal heterogeneity, participation skew, and modality skew) with multiple
  subtypes.
---

# Non-IID data in Federated Learning: A Survey with Taxonomy, Metrics, Methods, Frameworks and Future Directions

## Quick Facts
- arXiv ID: 2411.12377
- Source URL: https://arxiv.org/abs/2411.12377
- Authors: Daniel M. Jimenez G.; David Solans; Mikko Heikkila; Andrea Vitaletti; Nicolas Kourtellas; Aris Anagnostopoulos; Ioannis Chatzigiannakis
- Reference count: 40
- Key outcome: Comprehensive systematic review of non-IID data challenges in federated learning, addressing classification and quantification gaps through a detailed taxonomy covering six main types of data heterogeneity with standardized metrics and protocols.

## Executive Summary
This paper presents a comprehensive systematic review of non-IID data challenges in federated learning, addressing the lack of consensus in classifying and quantifying data heterogeneity. The authors developed a detailed taxonomy covering six main types of data heterogeneity (label skew, attribute skew, quantity skew, spatiotemporal heterogeneity, participation skew, and modality skew) with multiple subtypes. They also introduced standardized partition protocols and metrics for quantifying non-IIDness across different skew dimensions.

The review identified FedProx as the most prevalent solution, appearing in 25% of analyzed papers. Key findings include the dominance of label skew studies, the interdependence of different skew types during data partitioning, and the limited use (14.2%) of standardized frameworks despite their availability. The authors suggest future research directions including multimodal learning approaches, improved evaluation frameworks, and enhanced communication efficiency methods to address these challenges.

## Method Summary
The study conducted a systematic literature review of 85 papers from 2017 to 2023, focusing on non-IID data in federated learning. The authors developed a comprehensive taxonomy through keyword-based search strategies and content analysis, covering six main types of data heterogeneity with multiple subtypes. They analyzed papers' methodologies, metrics, and solutions, identifying patterns in research focus and implementation approaches. The review also examined standardized frameworks and their adoption rates, while proposing partition protocols and quantification metrics to enable consistent evaluation of non-IID scenarios across different studies.

## Key Results
- Developed detailed taxonomy covering six main types of data heterogeneity (label skew, attribute skew, quantity skew, spatiotemporal heterogeneity, participation skew, and modality skew) with multiple subtypes
- Identified FedProx as the most prevalent solution, appearing in 25% of analyzed papers
- Found only 14.2% of papers utilize standardized frameworks despite their availability
- Revealed that current research often focuses on single types of skewness, with limited work addressing multiple heterogeneity types simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The systematic review's comprehensive taxonomy addresses the research gap in non-IID data classification and quantification in FL.
- **Mechanism:** By developing a detailed classification system covering six main types of data heterogeneity with multiple subtypes, the review provides researchers with a standardized framework for understanding and comparing non-IID scenarios.
- **Core assumption:** The existing literature lacked a unified taxonomy for non-IID data classification and quantification.
- **Evidence anchors:** [abstract] "there is a lack of consensus among researchers about its classification and quantification"; [section] "Despite the significance of non-IID data in FL, there is a lack of consensus among researchers about its classification and quantification"
- **Break condition:** If researchers continue to use inconsistent classification systems despite the proposed taxonomy, or if the proposed taxonomy fails to capture important aspects of non-IID data heterogeneity.

### Mechanism 2
- **Claim:** The introduction of standardized partition protocols and metrics enables consistent evaluation and comparison of FL algorithms across different non-IID scenarios.
- **Mechanism:** By providing both partition protocols and metrics to quantify non-IIDness across different skew dimensions, the review creates a common framework that allows researchers to create reproducible experiments and compare results fairly.
- **Core assumption:** Without standardized protocols and metrics, FL research suffers from inconsistent experimental setups and incomparable results.
- **Evidence anchors:** [abstract] "The latter remains an unsolved challenge that can result in poorer model performance and slower training times"; [section] "Establishing agreed-upon methods for creating, quantifying, and evaluating non-IID partitions is essential for enabling fair comparisons, enhancing reproducibility, and providing a common framework for describing various non-IID scenarios"
- **Break condition:** If researchers continue to use ad-hoc experimental setups despite the availability of standardized protocols and metrics, or if the proposed metrics fail to capture the complexity of non-IID scenarios.

### Mechanism 3
- **Claim:** The comprehensive coverage of solutions and frameworks provides practitioners with actionable guidance for implementing FL systems that handle heterogeneous data.
- **Mechanism:** By surveying popular solutions and standardized frameworks, the review equips practitioners with knowledge about available tools and techniques to address non-IID challenges in real-world deployments.
- **Core assumption:** Practitioners need guidance on which solutions and frameworks to use for specific non-IID scenarios.
- **Evidence anchors:** [abstract] "Additionally, we describe popular solutions to address non-IID data and standardized frameworks employed in FL with heterogeneous data"; [section] "This review not only builds upon previous work, providing a more robust overview of the prevalent methods and metrics for simulating non-IID schemes, but also provides a collection of good practices to promote consistency in experimental setups"
- **Break condition:** If practitioners continue to implement custom solutions without leveraging the surveyed frameworks and techniques, or if the surveyed solutions prove insufficient for real-world non-IID challenges.

## Foundational Learning

- **Concept:** Federated Learning (FL) fundamentals and types
  - **Why needed here:** Understanding FL basics is essential for grasping why non-IID data poses unique challenges in this distributed learning paradigm.
  - **Quick check question:** What are the key differences between cross-silo and cross-device FL, and how do these differences affect data heterogeneity challenges?

- **Concept:** Data heterogeneity and skew types
  - **Why needed here:** Recognizing different types of data heterogeneity (label, attribute, quantity, spatiotemporal, participation, and modality skew) is crucial for understanding the complexity of non-IID data in FL.
  - **Quick check question:** How does label skew differ from attribute skew in FL, and what are the implications of each for model training and aggregation?

- **Concept:** Evaluation metrics and benchmarking in ML
  - **Why needed here:** Understanding how to evaluate and compare ML models is essential for appreciating the importance of standardized metrics for quantifying non-IIDness in FL.
  - **Quick check question:** Why is it important to have standardized metrics for quantifying non-IIDness, and how do these metrics differ from traditional ML evaluation metrics?

## Architecture Onboarding

- **Component map:** Taxonomy Engine -> Partition Protocol Generator -> Metrics Calculator -> Solution Repository -> Framework Integrator

- **Critical path:**
  1. Define the non-IID scenario using the taxonomy
  2. Select appropriate partition protocol(s) to create the desired data distribution
  3. Apply relevant metrics to quantify the degree of non-IIDness
  4. Choose suitable solutions and frameworks based on the specific heterogeneity characteristics
  5. Implement and evaluate the FL system using the standardized approach

- **Design tradeoffs:**
  - Comprehensive vs. focused: The taxonomy covers many skew types but may be overwhelming for specific use cases
  - Standardization vs. flexibility: Providing standardized protocols and metrics enables comparison but may limit exploration of novel approaches
  - Breadth vs. depth: Covering many solutions and frameworks provides options but may sacrifice detailed implementation guidance

- **Failure signatures:**
  - Inconsistent results across different studies due to varying definitions of non-IIDness
  - Difficulty in reproducing experiments or comparing results between different FL approaches
  - Overlooking important types of data heterogeneity when designing FL systems

- **First 3 experiments:**
  1. Implement a simple label skew scenario using the Dirichlet partition method and measure the resulting non-IIDness using the HI (Heterogeneity Index) metric
  2. Compare the performance of FedProx and Scaffold on a dataset with both label and quantity skew using the Dataskew metric
  3. Use the FDQM (Four-Dimensional Quantitative Measure) to quantify the combined effects of label, attribute, and quantity skew in a multi-modal FL scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective approach to simultaneously handle multiple types of data skewness (label, attribute, quantity, spatiotemporal, participation, and modality) in federated learning systems?
- Basis in paper: [explicit] The paper identifies that current research often focuses on single types of skewness and notes that "quantifying only one skew type at a time is insufficient for fully understanding and addressing non-IID-ness in FL" while emphasizing that "in real-life scenarios, the data is generally affected by more than one type of skewness simultaneously."
- Why unresolved: Most existing solutions target specific types of non-IIDness, and the paper highlights the need for more comprehensive approaches that can handle multiple skewness types simultaneously.
- What evidence would resolve it: Empirical studies comparing multi-skewness handling approaches against single-skewness methods across diverse federated learning scenarios, measuring performance metrics like accuracy, convergence speed, and robustness.

### Open Question 2
- Question: How can we develop more accurate metrics for quantifying multimodality skew in federated learning that capture both distribution differences within each modality and relationships between different modalities across clients?
- Basis in paper: [explicit] The paper introduces modality skew as a new category in the taxonomy and notes it's "an understudied topic" with "only 2% of the papers including it," while highlighting the need for metrics that can measure "cross-modal correlation discrepancies, modality-specific feature disparities, and variations in the relative importance of different modalities across clients."
- Why unresolved: Current metrics primarily focus on single modalities or simple combinations, lacking comprehensive measures that capture the complex interactions between multiple data modalities in federated settings.
- What evidence would resolve it: Development and validation of comprehensive multimodality skew metrics that can accurately quantify heterogeneity across multiple data types simultaneously in real-world federated learning scenarios.

### Open Question 3
- Question: What standardized evaluation frameworks can be developed to test federated learning algorithms under realistic conditions that combine multiple types of data heterogeneity, communication constraints, and system heterogeneity?
- Basis in paper: [explicit] The paper identifies the need for "more sophisticated evaluation and benchmarking methodologies" that can "consider various forms of data heterogeneity concurrently" and notes that "only 14.2% of them use FL frameworks to implement their proposals," suggesting a gap in standardized testing approaches.
- Why unresolved: Current evaluation frameworks often focus on isolated aspects of federated learning challenges, and there's a lack of comprehensive benchmarks that reflect real-world complexity.
- What evidence would resolve it: Creation and validation of comprehensive benchmarking suites that incorporate multiple heterogeneity types, realistic system constraints, and standardized protocols for evaluating federated learning algorithms across diverse scenarios.

## Limitations
- The review's confidence in FedProx dominance (25% of papers) and standardized framework underutilization (14.2%) depends on potentially incomplete coverage of rapidly evolving FL literature
- Scope is limited to classification and quantification aspects, excluding detailed coverage of privacy, security, and resource-constrained scenarios that significantly impact real-world FL deployments
- The comprehensive taxonomy may be overwhelming for specific use cases, potentially limiting its practical applicability in focused research scenarios

## Confidence
- **High** confidence in taxonomic framework and classification of non-IID data types
- **Medium** confidence in claimed dominance of FedProx (25% of papers)
- **Low** confidence in assertion that standardized frameworks remain underutilized (14.2%)

## Next Checks
1. **Empirical validation of taxonomy utility**: Apply the proposed taxonomy to classify 20 recently published non-IID FL papers not included in the original survey to assess its comprehensiveness and practical applicability.

2. **Framework adoption tracking**: Conduct a longitudinal study tracking framework usage (FEDML, Flower, TFF) across FL publications from 2020-2024 to verify the claimed 14.2% adoption rate and identify trends in framework utilization.

3. **Interdependence validation**: Design controlled experiments varying multiple skew types simultaneously (e.g., label + quantity + attribute skew) to empirically validate the paper's claim that these heterogeneity dimensions are interdependent during data partitioning.