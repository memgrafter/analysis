---
ver: rpa2
title: Fusing Audio and Metadata Embeddings Improves Language-based Audio Retrieval
arxiv_id: '2406.15897'
source_url: https://arxiv.org/abs/2406.15897
tags:
- audio
- metadata
- retrieval
- hybrid
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid retrieval system that combines audio
  signals and metadata for language-based audio retrieval. The core method idea is
  to use two independent modality encoders to embed audio signals and metadata into
  separate embedding spaces, then fuse these embeddings to obtain a single representation
  for an item.
---

# Fusing Audio and Metadata Embeddings Improves Language-based Audio Retrieval

## Quick Facts
- arXiv ID: 2406.15897
- Source URL: https://arxiv.org/abs/2406.15897
- Reference count: 26
- Primary result: Hybrid audio retrieval system combining audio signals and metadata improves performance by 2.36-3.69 percentage points mAP@10 over content-based baseline

## Executive Summary
This paper presents a hybrid retrieval system that combines audio signals and metadata for language-based audio retrieval. The core method idea is to use two independent modality encoders to embed audio signals and metadata into separate embedding spaces, then fuse these embeddings to obtain a single representation for an item. Two fusion strategies are explored: late fusion (summing the output vectors) and mid-level fusion (allowing crossmodal interactions via a multimodal transformer architecture). The primary results show that using metadata significantly improves retrieval performance compared to a pure content-based approach.

## Method Summary
The hybrid retrieval system uses dual-encoder architecture with separate encoders for audio and metadata. Audio signals are processed through a pretrained MobileNetV3-based CNN to obtain sequential embeddings, which are then pooled into a single vector. Metadata and queries are embedded using a shared BERT model. The system explores two fusion strategies: late fusion (summing audio and metadata embeddings) and mid-level fusion using a multimodal transformer architecture. The model is trained using contrastive learning with NT-Xent loss and evaluated on ClothoV2 and AudioCaps benchmarks.

## Key Results
- Hybrid approach with keyword metadata and late fusion improved mAP@10 by 2.36 percentage points on ClothoV2 benchmark
- Hybrid approach with keyword metadata and late fusion improved mAP@10 by 3.69 percentage points on AudioCaps benchmark
- Late fusion approach generally outperformed mid-level fusion across both benchmarks
- Combining open- and closed-set tags for training led to further improvements on ClothoV2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using metadata alongside audio signals improves retrieval performance by providing additional semantic context.
- Mechanism: Metadata (keywords and descriptions) is embedded using the same text encoder as queries, then fused with audio embeddings. This creates a richer representation that captures both acoustic content and textual descriptors.
- Core assumption: Metadata is semantically aligned with the audio content and can be meaningfully embedded in the same space as queries.
- Evidence anchors: [abstract] Improvement of 2.36 and 3.69 pp. mAP@10; [section] including any metadata type leads to improvement over baseline; Weak evidence from corpus.
- Break condition: Metadata is noisy, irrelevant, or poorly aligned with audio content.

### Mechanism 2
- Claim: Late fusion (summing audio and metadata embeddings) is simpler and more effective than mid-level fusion for this task.
- Mechanism: Audio and metadata embeddings are projected independently into a shared space and then summed element-wise. This avoids complex crossmodal interactions that may introduce noise.
- Core assumption: Simple combination of modality representations is sufficient for effective retrieval without needing crossmodal attention mechanisms.
- Evidence anchors: [abstract] Late fusion generally outperformed mid-level fusion; [section] late fusion tends to be better at ranking items; Weak evidence from corpus.
- Break condition: Complex crossmodal interactions are actually needed for certain types of metadata or audio content.

### Mechanism 3
- Claim: Sharing the text encoder for both query and metadata embedding improves performance by reducing parameter count and encouraging consistent text representations.
- Mechanism: The same BERT model is used to embed both metadata and queries, ensuring that their representations are in the same space and share learned linguistic features.
- Core assumption: Metadata and queries have similar linguistic characteristics and can benefit from shared encoding parameters.
- Evidence anchors: [section] Shared text encoder achieved 29.16 mAP@10 vs 28.05 with separate encoders; No direct evidence in corpus.
- Break condition: Metadata and queries have fundamentally different linguistic properties that require separate encoders.

## Foundational Learning

- Concept: Contrastive learning with NT-Xent loss
  - Why needed here: The model is trained to bring matching audio-metadata-query triples close together in the shared embedding space while pushing non-matching items apart.
  - Quick check question: What is the main difference between NT-Xent loss and traditional cross-entropy classification loss?

- Concept: Dual-encoder architecture
  - Why needed here: The system uses separate encoders for audio and text modalities, which are then fused. Understanding how dual-encoders work is crucial for implementing and debugging the system.
  - Quick check question: How does a dual-encoder architecture differ from a single-encoder approach for multimodal retrieval?

- Concept: Audio embeddings from CNN models
  - Why needed here: The audio signal is processed through a pretrained CNN (MobileNetV3) to obtain sequential embeddings, which are then pooled into a single vector. Understanding this pipeline is essential for working with the audio encoder.
  - Quick check question: What are the advantages of using a CNN-based audio encoder over a transformer-based one for this task?

## Architecture Onboarding

- Component map: Audio signal -> MobileNetV3 CNN -> Pooling -> Audio embedding; Metadata/Query text -> BERT -> Text embedding; Fusion module (late or mid-level) -> Shared retrieval space

- Critical path: 1) Audio signal → audio encoder → pooled embedding; 2) Metadata text → shared text encoder → metadata embedding; 3) Fusion of audio and metadata embeddings; 4) Query text → shared text encoder → query embedding; 5) Cosine similarity between fused item embedding and query embedding

- Design tradeoffs: Late fusion vs mid-level fusion (simplicity vs potential for richer crossmodal interactions); Shared vs separate text encoders (parameter efficiency vs potential for specialized representations); Pretrained vs trained-from-scratch encoders (leveraging existing knowledge vs task-specific optimization)

- Failure signatures: Poor performance despite training (check metadata-audio alignment); Degraded performance with metadata (metadata may be noisy or irrelevant); Overfitting to training data (model relying too heavily on metadata-text similarity)

- First 3 experiments: 1) Compare late fusion vs mid-level fusion performance on a small validation set; 2) Test the impact of using separate vs shared text encoders for metadata and queries; 3) Evaluate the effect of different metadata types (keywords vs descriptions) on retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gain from metadata-based audio retrieval systems diminish when the metadata and query are not perfectly aligned?
- Basis in paper: [explicit] The authors note that the improvement observed with full-sentence descriptions as metadata must be interpreted with caution due to the high similarity between the metadata and queries, which simulates ideal conditions.
- Why unresolved: The study uses datasets where the metadata and queries are either closely aligned (ClothoV2 with full-sentence descriptions) or partially overlapping (AudioCaps with acoustic event tags), which does not reflect real-world scenarios where metadata might be noisy or irrelevant.
- What evidence would resolve it: Conducting experiments with datasets containing noisier or less relevant metadata, or simulating real-world conditions with mismatched metadata-query pairs, would help determine the actual performance gain under practical scenarios.

### Open Question 2
- Question: How do different types of metadata, such as multi-sentence descriptions or unstructured keywords, impact the performance of hybrid audio retrieval systems?
- Basis in paper: [inferred] The study focuses on three categories of metadata: closed-set tags, open-set tags, and full-sentence descriptions. However, it does not explore the impact of other potential metadata types, such as multi-sentence descriptions or unstructured keywords, which might provide richer contextual information.
- Why unresolved: The current investigation is limited to specific types of metadata that are readily available in the datasets used. There is a lack of exploration into how other forms of metadata, which might be more common in real-world applications, could affect retrieval performance.
- What evidence would resolve it: Experimenting with a broader range of metadata types, including multi-sentence descriptions and unstructured keywords, and comparing their impact on retrieval performance would provide insights into the effectiveness of different metadata formats.

### Open Question 3
- Question: Is the superior performance of late fusion over mid-level fusion in hybrid audio retrieval systems a general trend, or is it specific to the multimodal transformer architecture used in this study?
- Basis in paper: [explicit] The authors observe that late fusion tends to outperform mid-level fusion in their experiments, which is surprising. They suggest that this could be due to the focus on nouns and verbs in retrieval models, making direct matching of keywords and descriptions less error-prone.
- Why unresolved: The study uses a specific multimodal transformer architecture for mid-level fusion, which might not be optimal. It is unclear whether the observed trend is due to the architecture or if it would hold true for other fusion methods.
- What evidence would resolve it: Conducting experiments with different fusion architectures, such as attention-based mechanisms or other neural network designs, would help determine if the late fusion advantage is consistent across various methods or specific to the current architecture.

## Limitations
- Reliance on metadata quality and alignment with audio content - effectiveness depends on semantic relevance and proper matching
- Relatively small improvement margins (2.36-3.69 percentage points mAP@10) may not justify added complexity for all use cases
- Limited exploration of metadata noise and irrelevance effects on retrieval performance

## Confidence
- High confidence: Experimental results showing metadata improves retrieval performance over pure content-based approaches
- Medium confidence: Superiority of late fusion over mid-level fusion (acknowledged potential benefits of crossmodal attention in other scenarios)
- Low confidence: Generalizability to datasets with significantly different metadata characteristics or quality

## Next Checks
1. **Metadata Quality Impact**: Conduct experiments varying the quality and relevance of metadata to quantify how metadata noise affects retrieval performance
2. **Crossmodal Interaction Exploration**: Implement and evaluate alternative mid-level fusion strategies that might better capture complex relationships between audio and metadata
3. **Generalization Testing**: Apply the hybrid retrieval system to a third dataset with different characteristics to validate robustness beyond the two benchmarks studied