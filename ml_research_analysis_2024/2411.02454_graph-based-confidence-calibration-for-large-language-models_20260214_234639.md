---
ver: rpa2
title: Graph-based Confidence Calibration for Large Language Models
arxiv_id: '2411.02454'
source_url: https://arxiv.org/abs/2411.02454
tags:
- confidence
- responses
- calibration
- question
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a graph-based confidence calibration method
  for large language models (LLMs) that leverages self-consistency among multiple
  model responses. The approach constructs a similarity graph over LLM responses to
  a question and trains a graph neural network (GNN) to estimate the probability of
  each response being correct.
---

# Graph-based Confidence Calibration for Large Language Models

## Quick Facts
- arXiv ID: 2411.02454
- Source URL: https://arxiv.org/abs/2411.02454
- Authors: Yukun Li; Sijia Wang; Lifu Huang; Li-Ping Liu
- Reference count: 38
- Key outcome: Graph-based method achieves up to 0.6 reduction in ECE compared to state-of-the-art baselines

## Executive Summary
This paper introduces a novel graph-based confidence calibration method for large language models (LLMs) that leverages self-consistency among multiple model responses. The approach constructs a similarity graph over LLM responses to a question and trains a graph neural network (GNN) to estimate the probability of each response being correct. The method achieves significant improvements in calibration performance across multiple benchmark datasets, with up to 0.6 reduction in ECE compared to state-of-the-art baselines. The approach also demonstrates strong generalization to out-of-domain settings, with consistently better performance than competing methods.

## Method Summary
The method generates multiple LLM responses to a question, constructs a similarity graph using Sentence-BERT embeddings and cosine similarity, applies K-means clustering to create node features, and trains a 3-layer GCN to predict response correctness. The model learns purely from response consistency patterns without processing textual information, enabling better generalization across different domains. The approach uses 30 responses per question with temperature=0.6, trains on binary cross-entropy loss with Adam optimizer, and evaluates using ECE, Brier Score, and AUROC metrics.

## Key Results
- Achieves up to 0.6 reduction in ECE compared to state-of-the-art baselines
- Demonstrates strong generalization to out-of-domain settings with consistently better performance
- GNN model learns purely from response consistency patterns without textual information
- Temperature=0.6 and 30 responses per question provide optimal calibration performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph neural network leverages response consistency patterns to estimate correctness probabilities without processing textual information.
- Mechanism: The GNN takes a similarity graph constructed from multiple LLM responses as input, where nodes represent responses and edge weights represent semantic similarity. By analyzing the graph structure, the GNN can identify clusters of consistent responses and predict the probability of each response being correct based on its position within the graph.
- Core assumption: The consistency among LLM responses (reflected in the similarity graph) contains sufficient information to predict the probability of correctness.
- Evidence anchors:
  - [abstract]: "The approach constructs a similarity graph over LLM responses to a question and trains a graph neural network (GNN) to estimate the probability of each response being correct."
  - [section]: "Our model achieves premium performance in the empirical study. Compared with previous calibration methods, our model has much better calibration performance because of the usage of consistency graphs."
  - [corpus]: Weak evidence. The corpus contains related papers on graph-based confidence estimation, but specific evidence for this mechanism is not directly mentioned.
- Break condition: If the similarity graph fails to capture meaningful patterns of consistency, or if the GNN cannot effectively learn from the graph structure alone without textual information.

### Mechanism 2
- Claim: Using multiple rephrased prompts reduces variance and improves semantic consistency estimation.
- Mechanism: By generating multiple rephrased versions of the same question, the LLM is prompted to provide diverse responses that all aim to answer the same underlying semantic question. This approach helps to reveal the LLM's uncertainty and improves the accuracy of confidence calibration by making the consistency (or lack thereof) more apparent.
- Core assumption: Rephrasing questions while maintaining semantic meaning leads to more diverse and informative responses, which better reveal the LLM's uncertainty.
- Evidence anchors:
  - [section]: "It is well known that the syntactic form of a question influences responses and introduces additional variance. To reduce this variance and evaluate the LLM's semantic consistency, we analyze the LLM's responses to multiple prompts derived from the same question."
  - [section]: "The model is more likely to produce diverse responses for questions about which the LLM is less certain. Confidence calibration is more accurate in this scenario because the model's uncertainty becomes more apparent."
  - [corpus]: Weak evidence. The corpus mentions related work on LLM calibration and uncertainty estimation, but specific evidence for this mechanism is not directly mentioned.
- Break condition: If the rephrased prompts do not effectively capture the same semantic meaning, or if the LLM fails to produce diverse responses despite rephrasing.

### Mechanism 3
- Claim: The graph-based approach generalizes better to out-of-domain settings compared to text-based methods.
- Mechanism: By relying solely on the consistency graph and clustering features rather than textual information, the GNN model is less sensitive to domain-specific features and more robust to distribution shifts. This allows the model to maintain stable performance across different datasets and domains.
- Core assumption: Similarity graph patterns are highly invariant to data distribution, making the model more robust to domain shifts.
- Evidence anchors:
  - [section]: "Furthermore, the proposed approach significantly improves the generalizability of confidence calibration on out-of-domain (OOD) data."
  - [section]: "This is primarily because similarity graph patterns are highly invariant to the data distribution. Specifically, our model employs the consistency graph and the clustering feature that does not alter with data distribution shifts, enabling it to maintain stable performance across different datasets."
  - [corpus]: Weak evidence. The corpus mentions related work on uncertainty estimation and calibration, but specific evidence for this mechanism is not directly mentioned.
- Break condition: If the similarity graph patterns are not sufficiently invariant across domains, or if other factors related to domain shift affect the model's performance.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to process the similarity graph constructed from LLM responses and estimate the probability of each response being correct based on the graph structure.
  - Quick check question: How does a GNN propagate information through a graph, and how does this help in predicting node properties?

- Concept: Self-Consistency
  - Why needed here: Self-consistency among multiple LLM responses is the key indicator used to estimate the model's confidence in its answers. The more consistent the responses, the higher the confidence.
  - Quick check question: What are the different ways to measure consistency among LLM responses, and how do they relate to confidence estimation?

- Concept: Calibration
  - Why needed here: Calibration is the process of aligning the model's predicted confidence with the actual probability of correctness. The goal is to ensure that when the model predicts a 90% confidence, it is correct 90% of the time.
  - Quick check question: What are the common metrics used to evaluate calibration performance, and how do they differ?

## Architecture Onboarding

- Component map:
  - LLM (e.g., Llama3, Vicuna) -> generates multiple responses to a question
  - Sentence-BERT -> computes embeddings for responses
  - Similarity graph construction -> creates a fully connected graph with cosine similarity as edge weights
  - K-means clustering -> assigns cluster IDs to responses based on embeddings
  - GNN (3-layer GCN) -> predicts the probability of each response being correct
  - Training pipeline -> optimizes GNN using binary cross-entropy loss

- Critical path:
  1. Generate multiple responses from LLM
  2. Compute embeddings and construct similarity graph
  3. Apply K-means clustering and create node features
  4. Feed graph and features to GNN
  5. Predict correctness probabilities and compute loss
  6. Update GNN parameters

- Design tradeoffs:
  - Using a similarity graph vs. directly processing text: The graph approach generalizes better but may miss some semantic nuances captured by text processing.
  - Number of responses sampled: More responses provide better consistency estimates but increase computational cost.
  - GNN architecture: Deeper GNNs may capture more complex patterns but risk overfitting or increased training time.

- Failure signatures:
  - High ECE or Brier score: Indicates poor calibration
  - Low AUROC: Suggests the model cannot effectively discriminate between correct and incorrect responses
  - Large performance gap between in-domain and out-of-domain data: Implies poor generalization

- First 3 experiments:
  1. Ablation study: Remove the similarity graph and use only text embeddings as input to the GNN. Compare calibration performance to the full model.
  2. Sensitivity analysis: Vary the number of responses sampled and the number of GNN layers. Analyze the impact on calibration performance.
  3. Out-of-domain evaluation: Train the model on one dataset and evaluate on a different dataset. Compare performance to baseline methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the graph-based confidence calibration method perform when the number of sampled responses varies significantly?
- Basis in paper: [inferred]
- Why unresolved: The paper only tests with a fixed number of 30 responses per question and mentions that performance slightly improves with more responses but does not systematically explore the impact of varying response numbers.
- What evidence would resolve it: A comprehensive study varying the number of responses (e.g., 10, 20, 30, 50) and analyzing the calibration performance across different configurations would provide clarity on the optimal number of responses needed for reliable calibration.

### Open Question 2
- Question: Can the graph-based approach handle ambiguous questions where multiple answers could be considered correct?
- Basis in paper: [inferred]
- Why unresolved: The paper assumes a single correct answer for each question and uses ROUGE similarity for labeling. It does not address how the method would perform when questions have multiple valid answers or when the correct answer is subjective.
- What evidence would resolve it: Testing the method on datasets with inherently ambiguous questions or implementing a multi-label approach to handle cases with multiple correct answers would demonstrate the method's robustness in such scenarios.

### Open Question 3
- Question: How does the proposed method scale with the size of the language model and the complexity of the tasks?
- Basis in paper: [inferred]
- Why unresolved: The experiments are conducted on two specific LLMs (Llama3-8B and Vicuna-7b) and two benchmark datasets. There is no exploration of how the method performs with larger models or more complex tasks like long-form question answering or multi-step reasoning.
- What evidence would resolve it: Evaluating the method on a range of LLMs with different parameter sizes and on diverse tasks of varying complexity would reveal its scalability and generalization capabilities.

## Limitations

- Limited evaluation scope to two conversational QA benchmarks without testing on generation, classification, or code tasks
- Underspecified implementation details including exact Sentence-BERT model variant and prompt templates
- Significant computational overhead from generating 30 responses per question without cost-benefit analysis

## Confidence

**High Confidence**: The graph-based approach and use of self-consistency for confidence calibration are well-founded. The reported calibration improvements (up to 0.6 ECE reduction) are substantial and the methodology is technically sound.

**Medium Confidence**: The out-of-domain generalization claims are supported by experimental results but the specific domain shifts tested are not clearly described. The superiority over text-based methods is demonstrated but could benefit from more comprehensive comparisons.

**Low Confidence**: The claim about avoiding "catastrophic failures" is based on a single anecdotal example and lacks systematic evaluation. The computational efficiency claims are not substantiated with actual runtime measurements.

## Next Checks

1. **Ablation on Response Count**: Systematically vary the number of responses (5, 10, 15, 20, 30) to determine the optimal trade-off between calibration performance and computational cost. This would validate whether 30 responses are truly necessary.

2. **Cross-Domain Robustness**: Evaluate the model on additional diverse datasets beyond CoQA and TriviaQA, including non-QA tasks like text classification, summarization, and code generation. This would better establish the claimed OOD generalization.

3. **Comparison with Simpler Baselines**: Implement and compare against simpler confidence calibration methods (e.g., temperature scaling, ensemble methods) on the same datasets to quantify the added value of the graph-based approach.