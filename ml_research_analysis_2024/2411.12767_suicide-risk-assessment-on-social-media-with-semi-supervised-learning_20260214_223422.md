---
ver: rpa2
title: Suicide Risk Assessment on Social Media with Semi-Supervised Learning
arxiv_id: '2411.12767'
source_url: https://arxiv.org/abs/2411.12767
tags:
- data
- suicide
- labeled
- roberta
- posts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles automated suicide risk assessment on social
  media, a task complicated by limited labeled data and severe class imbalance. The
  authors propose a semi-supervised framework that uses self-training with a novel
  Stratified Confidence Sampling (SCS) method to generate high-quality pseudo-labels
  from 1,500 unlabeled posts, supplementing the 500 labeled posts.
---

# Suicide Risk Assessment on Social Media with Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2411.12767
- Source URL: https://arxiv.org/abs/2411.12767
- Reference count: 32
- Key outcome: Semi-supervised learning with Stratified Confidence Sampling boosts RoBERTa's suicide risk classification F1 from 0.648 to 0.760, especially improving the rare "Attempt" class by 25.3%.

## Executive Summary
This paper tackles automated suicide risk assessment on social media, a task complicated by limited labeled data and severe class imbalance. The authors propose a semi-supervised framework that uses self-training with a novel Stratified Confidence Sampling (SCS) method to generate high-quality pseudo-labels from 1,500 unlabeled posts, supplementing the 500 labeled posts. A subset of pseudo-labels with low unanimity across multiple model trials is manually verified to further improve quality. Experiments compare RoBERTa, BERT, and BioBERT, with RoBERTa achieving the best performance. Using pseudo-labeled data boosts RoBERTa's micro-F1 from 0.648 to 0.760 and macro-F1 from 0.599 to 0.745. The improvement is especially notable for the underrepresented "Attempt" class, whose F1 score increases by 25.3%. The method also shows promise for addressing class imbalance in other tasks. While Llama3-8B was evaluated, RoBERTa outperformed it on this task. Overall, the study demonstrates that semi-supervised learning with SCS and partial human verification substantially enhances suicide risk assessment performance.

## Method Summary
The authors use a semi-supervised framework combining self-training with Stratified Confidence Sampling (SCS) to expand a small labeled dataset (500 posts) using unlabeled data (1,500 posts). They train RoBERTa as the backbone model, generate pseudo-labels via SCS, and manually verify a subset of low-unanimity pseudo-labels. The approach improves both micro and macro F1 scores, with notable gains for the rare "Attempt" class. The method involves five-fold cross-validation, and SCS ensures minority classes are proportionally sampled during pseudo-label acquisition.

## Key Results
- RoBERTa achieves micro-F1 of 0.760 and macro-F1 of 0.745 using semi-supervised learning, up from 0.648 and 0.599 respectively on labeled data only.
- The "Attempt" class F1 improves by 25.3%, addressing severe class imbalance.
- SCS and manual verification of low-unanimity pseudo-labels are critical to performance gains.
- RoBERTa outperforms BERT and BioBERT, and also beats Llama3-8B on this task.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stratified Confidence Sampling (SCS) improves pseudo-label quality by ensuring underrepresented classes are sampled proportionally during self-training.
- Mechanism: SCS groups unlabeled data by predicted class, sorts each group by confidence, and samples the top p% from each class. This avoids dominance of majority classes and improves recall for minority classes like "Attempt."
- Core assumption: The unlabeled data distribution mirrors the labeled data distribution, so predicted pseudo-labels reflect true class frequencies.
- Evidence anchors:
  - [abstract] "...we propose a semi-supervised framework that leverages labeled (n=500) and unlabeled (n=1,500) data and expands upon the self-training algorithm with a novel pseudo-label acquisition process designed to handle imbalanced datasets."
  - [section II-B2] "This is done by separating the initial predictions by predicted class, ordering each subset based on predicted confidence, and then selecting the p percent most-confident samples in each subset to be removed from the unlabeled set and added to the training set."
  - [corpus] Found 25 related papers; average neighbor FMR=0.449. Weak evidence for SCS usage in corpus; method appears novel here.
- Break condition: If the unlabeled data distribution differs significantly from the labeled set, SCS will bias pseudo-labels toward overrepresented classes, degrading performance.

### Mechanism 2
- Claim: Manual verification of low-unanimity pseudo-labels improves classification of underrepresented classes by correcting systematic errors.
- Mechanism: After generating five sets of pseudo-labels, instances with low unanimity are flagged. Human annotators verify these labels, increasing the proportion of correctly labeled minority-class samples.
- Core assumption: Low-unanimity instances are more likely to be misclassified, and manual correction of these improves downstream model performance.
- Evidence anchors:
  - [abstract] "...we manually verify a subset of the pseudo-labeled data that was not predicted unanimously across multiple trials of pseudo-label generation."
  - [section III-B] "We propose that labeling these samples is an efficient way to improve the quality of pseudo-labeled data while only manually validating a small portion of the pseudo-labeled dataset."
  - [corpus] Weak evidence in corpus; manual verification strategy is task-specific, not widely reported.
- Break condition: If annotator agreement is low or if most non-unanimous labels are already correct, verification adds cost without performance gain.

### Mechanism 3
- Claim: Using RoBERTa as the backbone model outperforms BERT and BioBERT in this suicide risk classification task.
- Mechanism: RoBERTa's training objective (dynamic masking, full-sentences) and architecture yield better text representations for social media text, improving both micro and macro F1 scores.
- Core assumption: The characteristics of Reddit posts (informal, short, context-dependent) align better with RoBERTa's strengths.
- Evidence anchors:
  - [section III-A] "RoBERTa also exceeded the other two models in precision, recall, and binary-F1 scores, with the most noticeable improvement occurring in the F1 score for the ‘Attempt’ class."
  - [section II-B1] "Specifically, we use RoBERTa to generate feature representations of input sentences given that previous literature has established that RoBERTa achieves strong performance on text classification tasks."
  - [corpus] Weak evidence for RoBERTa vs. BERT/BioBERT in suicide risk detection specifically; comparison appears novel.
- Break condition: If the dataset contains long-form or domain-specific medical text, BioBERT might outperform RoBERTa.

## Foundational Learning

- Concept: Self-training in semi-supervised learning
  - Why needed here: The dataset has only 500 labeled examples but 1,500 unlabeled ones; self-training leverages the unlabeled data to improve model generalization.
  - Quick check question: What happens to the unlabeled pool after each iteration of self-training? (Answer: Confident pseudo-labels are removed and added to the training set.)

- Concept: Class imbalance handling in NLP
  - Why needed here: The "Attempt" class is only 8.2% of labeled data, so naive pseudo-label selection would ignore it. SCS ensures minority classes are represented.
  - Quick check question: How does SCS differ from threshold-based pseudo-label selection? (Answer: SCS samples proportionally per class instead of globally by confidence.)

- Concept: Pseudo-label quality estimation via model unanimity
  - Why needed here: Low unanimity suggests model uncertainty, making those samples good candidates for human verification to improve overall label quality.
  - Quick check question: What threshold of unanimity would you use to trigger manual review? (Answer: Less than 5/5 votes in this study.)

## Architecture Onboarding

- Component map: Input (Reddit posts) -> RoBERTa (feature extraction) -> Self-training loop (train/predict/SCS/update) -> Manual verification (low-unanimity labels) -> Evaluation (5-fold CV, micro/macro F1)

- Critical path:
  1. Train RoBERTa on 500 labeled posts.
  2. Predict on 1,500 unlabeled posts.
  3. Apply SCS to select 25% most confident per class.
  4. Add pseudo-labels to training set, retrain.
  5. Repeat until unlabeled pool < 200 posts.
  6. Manually verify non-unanimous pseudo-labels.
  7. Final evaluation on held-out test set.

- Design tradeoffs:
  - SCS vs. threshold: SCS ensures minority class inclusion but requires class prediction; threshold is simpler but risks majority class dominance.
  - Manual verification vs. no verification: Verification improves minority class recall but adds annotation cost; the study shows gains mainly for "Attempt" class.
  - RoBERTa vs. BERT/BioBERT: RoBERTa gives better performance here but may be slower; BioBERT might be better for biomedical text.

- Failure signatures:
  - Macro F1 stagnates while micro F1 improves → class imbalance persists.
  - "Attempt" F1 does not improve after SCS → unlabeled distribution differs from labeled.
  - Model accuracy drops after manual verification → annotator disagreement or mislabeled corrections.

- First 3 experiments:
  1. Train RoBERTa on 500 labeled posts only; report baseline micro/macro F1.
  2. Apply SCS to unlabeled pool; verify improvement in class balance without manual correction.
  3. Add manual verification for low-unanimity labels; compare class-specific F1 gains, especially for "Attempt."

## Open Questions the Paper Calls Out

- How does the performance of the semi-supervised framework change when using different acquisition rates (p) in the Stratified Confidence Sampling algorithm?
- How does the model's performance change when the size of the unlabeled dataset is increased beyond 1,500 posts?
- How does the Stratified Confidence Sampling strategy compare to other semi-supervised learning strategies, especially for training on class-imbalanced datasets?

## Limitations

- SCS assumes unlabeled data distribution matches labeled data; distribution shift may degrade performance.
- Manual verification is costly and may not scale; gains are mainly for the rare "Attempt" class.
- Limited comparison with other semi-supervised methods; only Llama3-8B evaluated as alternative backbone.

## Confidence

- **High confidence**: SCS improves pseudo-label quality for minority classes; RoBERTa outperforms BERT/BioBERT on this dataset; manual verification of non-unanimous labels yields performance gains.
- **Medium confidence**: Gains are task-specific to suicide risk detection; SCS will transfer to other class-imbalanced NLP tasks.
- **Low confidence**: SCS is robust to distribution shift in unlabeled data; manual verification is cost-effective at scale.

## Next Checks

1. Test SCS on a labeled dataset with known distribution shift to measure robustness.
2. Compare manual verification gains when unanimity threshold is varied (e.g., 3/5 vs. 4/5).
3. Replicate the experiment on a non-Reddit social media dataset (e.g., Twitter) to assess generalizability.