---
ver: rpa2
title: Using Large Language Models for Automated Grading of Student Writing about
  Science
arxiv_id: '2412.18719'
source_url: https://arxiv.org/abs/2412.18719
tags:
- instructor
- answer
- question
- points
- writer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can reliably
  grade student writing in science courses. Using GPT-4, the researchers compared
  LLM grades to instructor and peer grades across 120 students in three MOOCs.
---

# Using Large Language Models for Automated Grading of Student Writing about Science

## Quick Facts
- arXiv ID: 2412.18719
- Source URL: https://arxiv.org/abs/2412.18719
- Authors: Chris Impey; Matthew Wenger; Nikhil Garuda; Shahriar Golchin; Sarah Stamer
- Reference count: 40
- Primary result: LLM grades closely match instructor grades when provided with model answers and rubrics, outperforming peer grading in reliability.

## Executive Summary
This study demonstrates that large language models (LLMs) can reliably grade student writing in science courses. Using GPT-4, the researchers compared LLM grades to instructor and peer grades across 120 students in three MOOCs. The LLM achieved statistically similar grades to the instructor when provided with model answers and rubrics, and outperformed peer grading in reliability. LLMs also generated effective rubrics, suggesting they can be scaled for automated, high-quality grading of science writing in online education.

## Method Summary
The researchers used GPT-4 to grade student writing assignments from three MOOCs (astronomy, astrobiology, history and philosophy of astronomy) with 120 students. They compared LLM grades to instructor and peer grades using three different prompt configurations: (1) instructor-provided answer only, (2) instructor-provided answer plus instructor rubric, and (3) LLM-generated rubric plus instructor answer. Statistical analysis included Friedman Test, Conover's post-hoc tests, RMS differences, and ICC calculation to assess grading reliability and performance.

## Key Results
- LLM grades closely matched instructor grades when provided with both model answers and rubrics
- LLM-generated rubrics were as effective as instructor-provided rubrics
- LLM grading was more reliable than peer grading in MOOCs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM grades closely match instructor grades when provided with both a model answer and a rubric.
- Mechanism: The rubric constrains the LLM's evaluation criteria, ensuring alignment with instructor standards, while the model answer provides a concrete target for expected quality.
- Core assumption: The instructor's rubric accurately captures the grading criteria and is applicable to the student responses.
- Evidence anchors:
  - [abstract]: "Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses."
  - [section]: "GPT-4 had a similar problem with reliability when provided with only an example answer. Table 1 shows that GPT-4 produced grades that differ significantly from the instructor when only prompted with an example answer, yet these results were not statistically different from peer graders. This difference disappears when GPT-4 was prompted with both a rubric and an example answer."
  - [corpus]: "Average neighbor FMR=0.374, average citations=0.0" - Limited corpus evidence available to support this specific mechanism.
- Break condition: If the rubric is poorly constructed or does not accurately reflect the instructor's grading criteria, the LLM's grades will diverge from the instructor's grades.

### Mechanism 2
- Claim: LLM-generated rubrics are as effective as instructor-provided rubrics for grading student writing.
- Mechanism: The LLM leverages its training data to create rubrics that capture the essential elements of a well-structured response, mirroring the instructor's implicit grading criteria.
- Core assumption: The LLM's training data includes a sufficient representation of educational content and grading practices to enable effective rubric generation.
- Evidence anchors:
  - [abstract]: "LLMs also generated effective rubrics, suggesting they can be scaled for automated, high-quality grading of science writing in online education."
  - [section]: "These rubrics were evaluated by having the LLM assign grades using the AI-generated rubric in combination with the instructor-provided answer, and then comparing that with the results of the instructor rubric and instructor-provided answer. The differences between the resulting scores were not statistically significant, indicating that LLM-generated rubrics are of similar utility to the instructor-provided rubrics."
  - [corpus]: "Average neighbor FMR=0.374, average citations=0.0" - Limited corpus evidence available to support this specific mechanism.
- Break condition: If the LLM's training data does not adequately represent the subject matter or grading practices, the generated rubrics may be ineffective or misaligned with the instructor's expectations.

### Mechanism 3
- Claim: LLM grading is more reliable than peer grading in MOOCs.
- Mechanism: The LLM applies consistent evaluation criteria across all responses, eliminating the variability inherent in peer grading due to individual differences in understanding and interpretation.
- Core assumption: The LLM's evaluation criteria are consistent and accurately reflect the instructor's grading standards.
- Evidence anchors:
  - [abstract]: "The LLM achieved statistically similar grades to the instructor when provided with model answers and rubrics, and outperformed peer grading in reliability."
  - [section]: "In line with our previous research (Formanek et al., 2017), this study confirmed that peer grades differed significantly from the instructor's grades, indicating variability in peer assessments. GPT-4 had a similar problem with reliability when provided with only an example answer."
  - [corpus]: "Average neighbor FMR=0.374, average citations=0.0" - Limited corpus evidence available to support this specific mechanism.
- Break condition: If the LLM's evaluation criteria are not consistently applied or if the training data introduces biases, the reliability advantage over peer grading may diminish.

## Foundational Learning

- Concept: Statistical significance and hypothesis testing
  - Why needed here: To determine whether the differences between LLM grades, instructor grades, and peer grades are meaningful or due to chance.
  - Quick check question: What is the p-value threshold commonly used to determine statistical significance?
- Concept: Intraclass Correlation Coefficient (ICC)
  - Why needed here: To assess the inter-rater reliability of the LLM's grading compared to instructor and peer grading.
  - Quick check question: What does an ICC value of 0.92 indicate about the reliability of the LLM's grading?
- Concept: Bootstrap resampling
  - Why needed here: To estimate the uncertainty in the average grades and calculate p-values when the sample size is small.
  - Quick check question: How does bootstrap resampling differ from traditional parametric statistical methods?

## Architecture Onboarding

- Component map: Student writing assignments -> LLM prompt (with model answer and rubric) -> LLM evaluation -> Grade comparison and analysis
- Critical path: Student assignment -> LLM prompt (with model answer and rubric) -> LLM evaluation -> Grade comparison and analysis
- Design tradeoffs:
  - Accuracy vs. scalability: Providing detailed rubrics and model answers improves accuracy but may limit scalability for large numbers of assignments.
  - LLM-generated rubrics vs. instructor-provided rubrics: LLM-generated rubrics offer potential for scalability but may lack the nuanced understanding of an instructor.
- Failure signatures:
  - LLM grades consistently diverge from instructor grades despite using model answers and rubrics
  - LLM-generated rubrics produce inconsistent or nonsensical grading criteria
  - LLM grading performance degrades significantly for more open-ended or creative writing assignments
- First 3 experiments:
  1. Compare LLM grades using only model answers vs. LLM grades using model answers and instructor rubrics for a subset of assignments.
  2. Generate LLM rubrics for a set of assignments and compare their effectiveness to instructor-provided rubrics in producing LLM grades that match instructor grades.
  3. Evaluate the impact of providing additional context or instructions to the LLM on its grading performance for assignments with more open-ended or creative prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key factors that determine when LLMs can reliably grade student writing assignments, and how can these factors be optimized?
- Basis in paper: Inferred from the finding that LLM performance depended on prompt design and the information provided (e.g., model answers, rubrics).
- Why unresolved: The paper only explored a few prompt variations and did not systematically investigate all factors influencing LLM grading reliability.
- What evidence would resolve it: Controlled experiments testing different combinations of input information (e.g., model answers, rubrics, assignment instructions) and prompt structures to identify optimal configurations for reliable LLM grading.

### Open Question 2
- Question: How does the performance of LLMs compare to human graders for more complex writing assignments that require higher-order thinking skills, such as analysis, evaluation, and creation?
- Basis in paper: Explicit statement that current LLMs struggle with assignments requiring "Create" level thinking according to Bloom's taxonomy.
- Why unresolved: The study focused on assignments at the "Remember," "Understand," "Apply," and "Analyze" levels. It did not test LLM performance on more complex assignments.
- What evidence would resolve it: Comparative studies of LLM and human grader performance on writing assignments that explicitly require higher-order thinking skills, with careful analysis of the types of errors made by each.

### Open Question 3
- Question: What are the ethical implications of using LLMs for grading student writing, particularly in high-stakes educational contexts?
- Basis in paper: Inferred from the acknowledgment that the study was conducted in a low-stakes MOOC environment and the potential for inaccurate grades in high-stakes settings.
- Why unresolved: The paper did not delve into the ethical considerations of using LLMs for grading, such as bias, transparency, and the potential impact on student learning and motivation.
- What evidence would resolve it: Empirical studies examining the impact of LLM grading on student outcomes, as well as ethical analyses of the use of AI in educational assessment.

## Limitations

- Limited sample size: The study evaluates only 120 students across three MOOCs, which may not generalize to all science writing contexts or larger student populations.
- Prompt engineering dependency: The LLM's performance heavily depends on the quality of prompts and rubrics provided, which the study did not fully explore.
- Subject matter constraints: The research focuses on science writing, and it is unclear whether the LLM's grading capabilities extend to other disciplines or more creative, open-ended writing tasks.

## Confidence

- High Confidence:
  - LLM grades are statistically similar to instructor grades when provided with both model answers and rubrics.
  - LLM-generated rubrics are as effective as instructor-provided rubrics for grading student writing.
  - LLM grading is more reliable than peer grading in MOOCs.

- Medium Confidence:
  - LLM grading can be scaled for automated, high-quality grading of science writing in online education.
  - The effectiveness of LLM grading extends beyond the specific science courses and questions evaluated in this study.

## Next Checks

1. Generalizability Assessment: Evaluate LLM grading performance on a larger and more diverse set of student writing assignments across multiple disciplines, including non-science subjects and more creative writing tasks.

2. Prompt Robustness Testing: Systematically test the impact of different prompt formulations, rubric qualities, and LLM parameters (e.g., temperature settings) on grading performance to identify optimal configurations and assess robustness.

3. Longitudinal Reliability Study: Conduct a longitudinal study to assess the consistency of LLM grading over time, across multiple iterations of the same course, and with different instructor graders to ensure sustained reliability and fairness.