---
ver: rpa2
title: Benchmarking Large Language Models on Answering and Explaining Challenging
  Medical Questions
arxiv_id: '2402.18060'
source_url: https://arxiv.org/abs/2402.18060
tags:
- answer
- medical
- question
- patient
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new datasets, JAMA Clinical Challenge
  and Medbullets, containing challenging medical questions with expert-written explanations
  to evaluate large language models (LLMs) on realistic clinical decision-making tasks.
  The datasets are harder than previous benchmarks, with GPT-4 achieving 67.32% accuracy
  on JAMA Clinical Challenge compared to higher scores on existing medical QA benchmarks.
---

# Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions

## Quick Facts
- **arXiv ID**: 2402.18060
- **Source URL**: https://arxiv.org/abs/2402.18060
- **Reference count**: 40
- **Primary result**: GPT-4 achieves 67.32% accuracy on challenging JAMA Clinical Challenge dataset

## Executive Summary
This paper introduces two new medical question-answering datasets, JAMA Clinical Challenge and Medbullets, designed to evaluate large language models (LLMs) on realistic clinical decision-making tasks. The datasets are harder than existing benchmarks, with GPT-4 achieving 67.32% accuracy on JAMA Clinical Challenge compared to higher scores on simpler medical QA benchmarks. The authors evaluate seven LLMs using various prompting strategies and find that few-shot prompting and chain-of-thought reasoning provide marginal improvements. Automatic and human evaluations of model-generated explanations reveal deficiencies in LLM reasoning capabilities, with weak correlations between automatic metrics and human judgments.

## Method Summary
The authors constructed two new datasets of challenging medical questions with expert-written explanations: JAMA Clinical Challenge (1,524 examples from clinical cases) and Medbullets (308 examples from simulated clinical scenarios). They evaluated seven LLMs (GPT-3.5, GPT-4, PaLM 2, Llama 2, Llama 3, MedAlpaca, Meerkat) using three prompting strategies (zero-shot, few-shot, and chain-of-thought) on both prediction accuracy and explanation quality. Evaluation employed multiple automatic metrics (ROUGE-L, BERTScore, BLEURT, BARTScore variants, CTC metrics, G-Eval) and human assessment of explanation quality dimensions.

## Key Results
- GPT-4 achieves 67.32% accuracy on JAMA Clinical Challenge, significantly lower than performance on existing medical QA benchmarks
- Few-shot and chain-of-thought prompting provide marginal improvements, with chain-of-thought only improving GPT-3.5 and Meerkat
- Automatic evaluation metrics show weak correlation with human judgments on explanation quality
- Model-generated explanations exhibit deficiencies including irrelevance and errors despite promising surface-level performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The datasets are harder because they require reasoning beyond memorized medical facts.
- **Mechanism**: JAMA Clinical Challenge uses actual clinical cases with complex presentations, while Medbullets contains simulated clinical scenarios that demand application of knowledge rather than recall. Both datasets include explanations that reveal the reasoning process.
- **Core assumption**: Medical board exams primarily test textbook knowledge, whereas realistic clinical cases require synthesis of information and clinical reasoning.
- **Evidence anchors**:
  - [abstract]: "medical board exams or general clinical questions do not capture the complexity of realistic clinical cases"
  - [section]: "Board exam questions...rely on textbook knowledge...In contrast, doctors need support with clinical presentations that differ from textbook definitions and require blending established knowledge with clinical experience"
  - [corpus]: Weak - corpus lacks direct evidence about textbook vs. clinical reasoning distinction
- **Break condition**: If models can memorize patterns from training data that map clinical presentations to answers without genuine reasoning, the datasets may not be as challenging as intended.

### Mechanism 2
- **Claim**: CoT prompting improves performance on MedQA and Medbullets but not JAMA because JAMA requires deeper reasoning.
- **Mechanism**: Chain-of-thought prompting helps models break down simpler reasoning chains but fails when cases require integrating multiple complex clinical concepts simultaneously.
- **Core assumption**: The difficulty difference between datasets correlates with the depth and complexity of reasoning required.
- **Evidence anchors**:
  - [section]: "CoT only improves GPT-3.5 and Meerkat but not other models. This suggests that the challenging clinical cases in JAMA are intrinsically more difficult for models to reason about compared to board exam questions"
  - [section]: "The challenging clinical cases in JAMA are intrinsically more difficult for models to reason about"
  - [corpus]: Weak - corpus lacks evidence about specific reasoning depths required
- **Break condition**: If CoT prompting fails to improve performance even on simpler datasets, the mechanism may be flawed.

### Mechanism 3
- **Claim**: Automatic evaluation metrics poorly correlate with human judgments because they cannot assess explanation quality dimensions like completeness and correctness.
- **Mechanism**: Current metrics (ROUGE-L, BERTScore, etc.) measure surface-level similarity or semantic overlap but miss nuanced aspects of explanation quality that humans prioritize.
- **Core assumption**: Human evaluators assess explanations based on criteria that automatic metrics cannot capture.
- **Evidence anchors**:
  - [section]: "The weak correlation between human and automatic scores underscores the necessity of developing metrics that can support future research on explainable medical QA"
  - [section]: "While LLMs produce promising explanations, they also exhibit deficiencies such as irrelevance and errors"
  - [corpus]: Weak - corpus lacks specific evidence about which human evaluation criteria are missing from automatic metrics
- **Break condition**: If new automatic metrics are developed that achieve strong correlation with human judgments, this mechanism would need revision.

## Foundational Learning

- **Concept**: Multiple-choice question answering evaluation
  - **Why needed here**: The paper evaluates models on medical multiple-choice questions, requiring understanding of accuracy metrics and prompting strategies
  - **Quick check question**: What is the difference between zero-shot and few-shot prompting in the context of multiple-choice medical QA?

- **Concept**: Chain-of-thought reasoning
  - **Why needed here**: The paper tests whether CoT prompting helps models reason through medical questions step-by-step
  - **Quick check question**: How does chain-of-thought prompting differ from standard prompting in terms of model output structure?

- **Concept**: Natural language generation evaluation
  - **Why needed here**: The paper evaluates model-generated explanations using multiple automatic and human evaluation metrics
  - **Quick check question**: What are the key differences between ROUGE-L and BERTScore when evaluating medical explanations?

## Architecture Onboarding

- **Component map**: Data collection → Model selection → Prompting strategies → Prediction accuracy measurement → Explanation generation → Evaluation (automatic + human)
- **Critical path**: Dataset construction → Model prompting → Prediction → Explanation generation → Evaluation
- **Design tradeoffs**: Closed-source vs. open-source models affect reproducibility; actual clinical cases vs. simulated scenarios affect dataset difficulty; automatic vs. human evaluation affects efficiency vs. quality
- **Failure signatures**: Low correlation between automatic and human evaluation scores; inconsistent performance across prompting strategies; poor generalization across medical domains
- **First 3 experiments**:
  1. Test model performance on JAMA Clinical Challenge vs. Medbullets to confirm dataset difficulty difference
  2. Compare zero-shot vs. few-shot prompting effectiveness across all models
  3. Evaluate correlation between automatic metrics (ROUGE-L, BERTScore) and human judgments on explanation quality

## Open Questions the Paper Calls Out
- Does in-context learning with CoT exemplars improve LLM performance on challenging medical QA tasks?
- How do multimodal LLMs perform on the complete JAMA Clinical Challenge dataset including images?
- What automatic evaluation metrics best correlate with human judgments for medical QA explanations?

## Limitations
- Weak correlation between automatic evaluation metrics and human judgments suggests current metrics may not capture explanation quality effectively
- Models show deficiencies in explanation generation, including irrelevance and errors, indicating limitations in reasoning capabilities
- The paper only evaluates text-based LLMs on datasets excluding images, leaving multimodal model performance unexplored

## Confidence
- **High**: Dataset construction methodology and prediction accuracy results
- **Medium**: Claims about dataset difficulty relative to existing benchmarks
- **Medium**: Conclusions about automatic evaluation metric limitations

## Next Checks
1. Conduct ablation studies to identify which specific aspects of clinical reasoning (e.g., temporal reasoning, differential diagnosis) make JAMA Clinical Challenge particularly difficult
2. Develop and test new automatic evaluation metrics that explicitly measure explanation completeness and correctness dimensions identified as important by human evaluators
3. Test additional prompting strategies beyond zero-shot, few-shot, and CoT to determine if other approaches might better elicit model reasoning capabilities