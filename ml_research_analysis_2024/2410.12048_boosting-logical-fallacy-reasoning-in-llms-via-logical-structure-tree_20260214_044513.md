---
ver: rpa2
title: Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree
arxiv_id: '2410.12048'
source_url: https://arxiv.org/abs/2410.12048
tags:
- fallacy
- logical
- tree
- text
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve logical fallacy detection
  and classification by constructing a logical structure tree that explicitly represents
  the hierarchical logical flow among relation connectives and their arguments. The
  tree is built in an unsupervised manner using a taxonomy of ten common logical relations.
---

# Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree

## Quick Facts
- arXiv ID: 2410.12048
- Source URL: https://arxiv.org/abs/2410.12048
- Authors: Yuanyuan Lei; Ruihong Huang
- Reference count: 23
- Primary result: Logical structure trees improve logical fallacy detection F1 by up to 3.45% and classification F1 by up to 6.75%

## Executive Summary
This paper addresses the challenge of improving logical fallacy reasoning in large language models (LLMs) by explicitly representing the hierarchical logical structure of arguments. The authors propose constructing a logical structure tree that captures the relationship between connective words (like "because" or "therefore") and their argument spans. This tree is then integrated into LLMs through two strategies: a textualized tree as a hard prompt and tree-based embeddings as a soft prompt. Experiments across four benchmark datasets show significant improvements in both fallacy detection and classification compared to baseline approaches.

## Method Summary
The method involves constructing a logical structure tree in an unsupervised manner using a taxonomy of ten common logical relations. The tree captures the juxtaposition between connective-suggested logical relations and actual semantic relations, which exposes the inconsistency that defines a logical fallacy. This tree is then integrated into LLMs through two strategies: (1) a textualized tree converted into natural language descriptions and used as a hard prompt, and (2) tree-based soft prompts generated through relation-specific encoders and projection layers. The approach is evaluated on four benchmark datasets using both Llama-2 and Flan-T5 models, showing consistent improvements in fallacy detection and classification tasks.

## Key Results
- Logical structure tree improves fallacy detection F1 by up to 3.45% across four datasets
- Fallacy classification F1 increases by up to 6.75% when using the proposed approach
- Both textualized tree (hard prompt) and tree-based soft prompt strategies show consistent improvements
- The approach generalizes well across different domains including social media, climate discussions, and formal logic datasets

## Why This Works (Mechanism)

### Mechanism 1: Connective-semantics mismatch detection
Logical fallacies often use connective words to indicate an intended logical relation between two arguments, while the argument semantics do not actually support the logical relation. The logical structure tree captures this juxtaposition by explicitly representing the hierarchical logical flow among relation connectives and their arguments. When a connective suggests a logical relation that the actual argument semantics don't support, the tree structure makes this inconsistency explicit for the LLM to detect.

### Mechanism 2: Structured information as natural language
Representing the logical structure as natural language descriptions (textualized tree) provides LLMs with interpretable context that improves fallacy reasoning. The tree is converted into a table of (left argument, relation connective, right argument) triplets in bottom-up order, then embedded and concatenated with the instruction prompt. This format aligns with how LLMs process structured information during training, making the logical relationships more accessible for reasoning.

### Mechanism 3: Relation-aware tree embeddings
Tree-based soft prompts allow direct learning of relation-aware tree embeddings that can be tuned alongside the LLM. Relation-specific encoders process each logical relation type to derive tree embeddings incrementally from bottom-up, then a projection layer transforms these into the LLM's embedding space as soft prompts. This approach captures the distinct structural patterns of different logical relations before integration with the LLM.

## Foundational Learning

- Concept: Constituency parsing and elementary discourse units (EDUs)
  - Why needed here: The logical structure tree construction relies on identifying text spans that correspond to arguments, which are mostly EDUs identified through constituency parsing
  - Quick check question: How does a constituency tree differ from a dependency tree, and why is constituency structure more suitable for identifying argument spans in this context?

- Concept: Logical relation taxonomies and discourse connectives
  - Why needed here: The system requires a predefined set of logical relations and their corresponding connectives to identify potential fallacies based on the mismatch between suggested and actual relations
  - Quick check question: What are the ten logical relations used in this work, and how might expanding this taxonomy affect fallacy detection performance?

- Concept: Prompt engineering for LLMs (hard vs soft prompts)
  - Why needed here: The approach uses both textualized trees as hard prompts and tree-based embeddings as soft prompts, requiring understanding of how different prompt types influence LLM behavior
  - Quick check question: What is the key difference between hard prompts and soft prompts in LLM fine-tuning, and how might each affect the model's ability to incorporate external information?

## Architecture Onboarding

- Component map: Text → Constituency tree parser → Logical structure tree constructor → (Textualized tree + Tree embedding) → LLM with enhanced prompt → Output

- Critical path: Text → Constituency tree → Logical structure tree → (Textualized tree + Tree embedding) → LLM with enhanced prompt → Output

- Design tradeoffs:
  - Taxonomy completeness vs. computational efficiency: Larger connective sets improve coverage but increase matching complexity
  - Hard prompt verbosity vs. information richness: More detailed textualized trees provide better context but may exceed context window limits
  - Soft prompt dimensionality vs. tuning stability: Higher-dimensional embeddings capture more nuance but require more training data to avoid overfitting

- Failure signatures:
  - Poor fallacy detection: Tree construction fails to identify critical connectives or extracts incorrect argument spans
  - Overfitting: Tree embeddings show high performance on training data but poor generalization to new domains
  - Context window overflow: Textualized tree exceeds LLM's maximum input length, causing truncation of critical information

- First 3 experiments:
  1. Baseline comparison: Run fallacy detection on Argotario dataset using Llama-2 without any tree information to establish performance floor
  2. Individual component ablation: Test fallacy detection with only textualized tree vs. only tree-based soft prompt to measure individual contribution
  3. Cross-dataset generalization: Evaluate the full model on Climate dataset after training on Reddit dataset to assess domain transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the logical structure tree approach vary across different types of logical fallacies, and are there specific fallacy types that benefit more from this method? The paper mentions that the logical structure tree brings bigger improvements for fallacy types such as Red Herring, Hasty Generalization, Irrelevant Authority, Ad Populum, Extension Fallacy, Equivocation, and Circular Reasoning, but does not offer a comprehensive analysis across all fallacy types.

### Open Question 2
What are the potential limitations of using a predefined set of logical relations and connectives, and how might expanding this set impact the effectiveness of the logical structure tree? The paper acknowledges that the set of connective words and phrases for the ten logical relations may not contain all possible connectives, but does not explore the impact of expanding this set.

### Open Question 3
How does the logical structure tree approach perform on datasets with logical fallacies that are not explicitly defined by the ten common logical relations used in the study? The paper focuses on ten common logical relations but does not address the performance on datasets with fallacies outside these relations, leaving a gap in understanding the approach's generalizability.

## Limitations

- The taxonomy of ten logical relations may not capture all fallacy types, potentially limiting the approach's effectiveness on domain-specific or novel fallacies
- The unsupervised nature of tree construction means errors in constituency parsing or argument extraction propagate through the entire system without correction mechanisms
- Performance gains are demonstrated on benchmark datasets but may not generalize to real-world deployment scenarios where fallacy patterns are more subtle or contextually complex

## Confidence

**High Confidence**: The empirical results showing consistent F1 score improvements across four benchmark datasets are reliable, with appropriate baselines and statistical significance testing.

**Medium Confidence**: The mechanism claims about why the approach works are plausible but rely on indirect evidence. The connection between tree structure and improved reasoning could also be explained by better overall context representation.

**Low Confidence**: The generalizability of the taxonomy-based approach is uncertain. The predefined set of ten logical relations may not capture all fallacy types, and the constituency parsing approach may struggle with complex sentence structures or domain-specific language patterns.

## Next Checks

1. **Ablation study on connective taxonomy**: Systematically remove individual logical relations from the taxonomy and measure impact on performance across different fallacy types to validate whether all ten relations are necessary.

2. **Cross-linguistic evaluation**: Test the logical structure tree approach on fallacy detection in languages other than English using the same taxonomy to reveal whether the connective-semantics mismatch hypothesis holds across linguistic structures.

3. **Real-world deployment simulation**: Apply the system to detect fallacies in argumentative text from social media or news sources not represented in the benchmark datasets to test whether performance gains generalize beyond curated academic datasets.