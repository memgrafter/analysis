---
ver: rpa2
title: 'TSO: Self-Training with Scaled Preference Optimization'
arxiv_id: '2409.02118'
source_url: https://arxiv.org/abs/2409.02118
tags:
- preference
- responses
- optimization
- negative
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes TSO, a self-training framework for preference
  optimization that addresses key challenges in aligning large language models with
  human preferences. TSO enhances diversity of responses by constructing a model matrix
  across different model versions and sizes, incorporates human and AI feedback to
  correct validity bias in evaluations, and employs iterative mini-batch training
  with a dual clip reward loss to balance optimization between positive and negative
  responses.
---

# TSO: Self-Training with Scaled Preference Optimization

## Quick Facts
- arXiv ID: 2409.02118
- Source URL: https://arxiv.org/abs/2409.02118
- Reference count: 16
- Primary result: TSO outperforms DPO, RSO, IPO, and cDPO on multiple alignment benchmarks

## Executive Summary
This paper introduces TSO, a self-training framework for preference optimization that addresses key challenges in aligning large language models with human preferences. TSO enhances diversity of responses by constructing a model matrix across different model versions and sizes, incorporates human and AI feedback to correct validity bias in evaluations, and employs iterative mini-batch training with a dual clip reward loss to balance optimization between positive and negative responses. Experiments show that TSO outperforms existing methods like DPO, RSO, IPO, and cDPO on multiple alignment benchmarks, with the TSO-3 model achieving improvements of 0.88 on AlignBench, 0.56 on MT-Bench, 11.96% on AlpacaEval-v2, and 15.4% on Arena-Hard compared to the base model.

## Method Summary
TSO constructs a model matrix with different versions and sizes (13B, 66B, 175B) to generate diverse preference data, using human responses and latest model outputs as positives while smaller/older models provide negatives. An evaluator fine-tuned on human/AI feedback corrects validity bias in response scoring. The framework employs iterative mini-batch training with a dual clip reward loss that applies asymmetric clipping to prevent domination by negative samples. Three stages of training are performed using 30,000 preference pairs each, with the reference model updated after each mini-batch to mitigate distributional shift.

## Key Results
- TSO-3 achieves 0.88 improvement on AlignBench compared to base model
- TSO-3 shows 0.56 improvement on MT-Bench
- TSO-3 improves AlpacaEval-v2 by 11.96% and Arena-Hard by 15.4% over base model
- TSO outperforms DPO, RSO, IPO, and cDPO across multiple alignment benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constructing a model matrix across different model versions and sizes improves diversity of preference data.
- Mechanism: By sampling responses from models of different scales and versions, the method generates both high-quality positive responses (from larger/latest models) and diverse negative responses (from smaller/older models), enriching the training dataset.
- Core assumption: Model performance correlates positively with scale and version; thus, different versions and sizes capture complementary aspects of the response distribution.
- Evidence anchors: [abstract] "TSO enhances the diversity of responses by constructing a model matrix and incorporating human preference responses." [section 3.1] "By leveraging models of different versions and sizes throughout the iterative process, we construct a model matrix that generates a variety of response outcomes." [corpus] Weak; neighboring papers discuss model diversity but do not validate the specific matrix construction.
- Break condition: If the correlation between model size/version and quality is weak or inconsistent, the diversity gain will diminish.

### Mechanism 2
- Claim: Human and AI feedback corrects validity bias in evaluation and improves handling of out-of-distribution samples.
- Mechanism: A supervised fine-tuned evaluator trained on human/AI feedback scores model-generated responses, allowing the method to detect and correct evaluation errors caused by out-of-distribution (OOD) instruction data.
- Core assumption: Human/AI feedback can effectively label OOD responses, and a fine-tuned evaluator can generalize this correction to unseen data.
- Evidence anchors: [abstract] "incorporates human and AI feedback to correct validity bias in evaluations" [section 3.2] "human and AI feedback are applied to continuously correct validity bias in the evaluation process" [corpus] Weak; neighboring works mention feedback loops but not specifically for validity bias correction.
- Break condition: If feedback labels are noisy or the evaluator fails to generalize, the correction will introduce bias instead of reducing it.

### Mechanism 3
- Claim: Mini-batches iterative DPO with dual clip reward loss balances optimization between positive and negative responses.
- Mechanism: The dataset is split into mini-batches; after each batch, the reference model is updated to the latest target model, resetting the reward scale. The dual clip reward loss applies asymmetric clipping to prevent domination by negative samples.
- Core assumption: Positive and negative samples should be optimized separately to avoid the positive reward collapsing, and frequent reference model updates mitigate distributional shift.
- Evidence anchors: [abstract] "employs iterative mini-batch training with a dual clip reward loss to balance optimization between positive and negative responses." [section 3.3] "dual clip reward loss, which effectively mitigates the imbalance between positive and negative samples during the optimization process" [section 4.3.2] "Using the LDP O, due to the coupling of the positive and negative response losses, the negative responses always had a dominant advantage."
- Break condition: If the mini-batch size is too small or too large, the benefit of frequent reference updates may be lost.

## Foundational Learning

- Concept: Preference optimization via pairwise ranking
  - Why needed here: The core task is aligning language models to human preferences using pairwise preference data (preferred vs. non-preferred responses).
  - Quick check question: In DPO, what does the logistic function σ compute given the difference of log probabilities of yw and yl?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: TSO builds on the RLHF framework but replaces online reward modeling with offline preference optimization and self-training.
  - Quick check question: What is the main advantage of offline RLHF methods like DPO compared to online RLHF with PPO?

- Concept: Supervised fine-tuning (SFT) for evaluator training
  - Why needed here: The evaluator that scores model-generated responses is trained via SFT on human/AI feedback to correct validity bias.
  - Quick check question: In SFT, what loss is typically minimized when training a model to imitate labeled examples?

## Architecture Onboarding

- Component map:
  Model matrix -> Preference dataset generator -> Evaluator (SFT on feedback) -> DPO trainer (mini-batch with dual clip loss) -> Reference model updater

- Critical path:
  1. Generate multi-model responses -> 2. Apply evaluator feedback -> 3. Build preference dataset -> 4. Train DPO in mini-batches -> 5. Update reference model

- Design tradeoffs:
  - Model matrix size vs. computational cost: larger matrices improve diversity but increase inference load
  - Mini-batch size vs. gradient stability: smaller batches allow frequent reference updates but may cause noisy gradients
  - Clip margin γw vs. γl: must balance to prevent positive reward collapse while not letting negatives dominate

- Failure signatures:
  - Degraded alignment performance: likely from insufficient diversity in negative responses or overly strong clipping
  - Evaluator overfits to feedback: check validation loss on held-out feedback pairs
  - Reference model lag: if target model drifts faster than reference updates, training instability may occur

- First 3 experiments:
  1. Compare alignment scores using single-model vs. multi-model preference datasets
  2. Ablate the dual clip loss by running DPO with standard loss on the same data
  3. Measure evaluator bias by comparing scores before/after SFT on OOD samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and version of the reference model to use for negative sampling in the model matrix?
- Basis in paper: [inferred]
- Why unresolved: The paper suggests using smaller and older version models for negative sampling but does not specify the optimal size or version. It also mentions that using the base model itself as a negative source leads to reverse alignment, indicating a need to explore the relationship between negative response distribution and model capability.
- What evidence would resolve it: Systematic experiments varying the size and version of the reference model used for negative sampling, measuring the impact on alignment performance across different evaluation benchmarks.

### Open Question 2
- Question: How does the dual clip reward loss function's hyperparameters (γw and γl) affect the balance between positive and negative response optimization?
- Basis in paper: [explicit]
- Why unresolved: The paper sets γw=20 and γl=10 in experiments but does not explore the sensitivity of these hyperparameters or provide guidance on how to choose them for different scenarios. The discussion mentions that increasing γl reduces final rewards but does not provide a systematic analysis.
- What evidence would resolve it: Comprehensive ablation studies varying γw and γl values, analyzing their impact on reward curves, alignment performance, and the balance between positive and negative response optimization across different training stages.

### Open Question 3
- Question: What is the optimal number of mini-batches (T) for the iterative DPO training strategy?
- Basis in paper: [explicit]
- Why unresolved: The paper uses T=3 mini-batches in experiments but does not explore whether this is optimal. The discussion mentions that mini-batches help with data utilization efficiency but does not provide theoretical justification for the chosen number or explore the trade-offs involved.
- What evidence would resolve it: Systematic experiments varying the number of mini-batches (T) while keeping other factors constant, measuring the impact on convergence speed, final alignment performance, and computational efficiency. Analysis of gradient magnitude changes across different T values would provide insights into the optimal choice.

## Limitations
- The specific contribution of each component (model diversity, feedback correction, dual clip loss) to performance gains remains unclear due to lack of ablation studies
- The method assumes model size and version correlate with response quality, but this relationship may not hold across all domains or task types
- The validity bias correction mechanism depends heavily on the quality and representativeness of human/AI feedback

## Confidence
**High confidence**: The experimental results demonstrating TSO's performance improvements over baseline methods on established alignment benchmarks. The methodology for constructing preference datasets and the dual clip reward loss formulation are clearly specified.

**Medium confidence**: The claim that model matrix construction specifically improves diversity of preference data. While the approach is sound, the paper lacks direct evidence showing that the diversity gain translates to better alignment, as opposed to simply having more data.

**Low confidence**: The assertion that human/AI feedback effectively corrects validity bias in evaluation. The paper describes the mechanism but provides limited evidence of its effectiveness beyond the final benchmark results.

## Next Checks
1. Conduct ablation studies to isolate the contribution of each TSO component (model matrix diversity, feedback correction, dual clip loss) to overall performance gains.
2. Test the evaluator's bias correction capability by measuring performance degradation when evaluated on out-of-distribution samples before and after feedback-based fine-tuning.
3. Verify the robustness of the training process by varying mini-batch sizes and measuring the impact on gradient stability and final alignment quality.