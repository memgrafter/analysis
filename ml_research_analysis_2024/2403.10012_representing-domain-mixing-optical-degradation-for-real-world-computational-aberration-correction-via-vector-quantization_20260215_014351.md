---
ver: rpa2
title: Representing Domain-Mixing Optical Degradation for Real-World Computational
  Aberration Correction via Vector Quantization
arxiv_id: '2403.10012'
source_url: https://arxiv.org/abs/2403.10012
tags:
- domain
- real-world
- images
- image
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of synthetic-to-real domain
  gap in computational aberration correction (CAC), where learning-based methods trained
  on synthetic data perform suboptimally on real-world images due to various factors
  causing the domain gap. To mitigate this issue, the authors formulate the task as
  unsupervised domain adaptation (UDA) and propose a novel Quantized Domain-Mixing
  Representation (QDMR) framework.
---

# Representing Domain-Mixing Optical Degradation for Real-World Computational Aberration Correction via Vector Quantization

## Quick Facts
- arXiv ID: 2403.10012
- Source URL: https://arxiv.org/abs/2403.10012
- Reference count: 40
- Key outcome: Proposed QDMR methods consistently outperform competitive CAC models and UDA frameworks, improving NIQE by up to 36% on Real-Snap benchmark

## Executive Summary
This paper addresses the challenge of domain gap in computational aberration correction (CAC), where learning-based methods trained on synthetic data perform poorly on real-world images. The authors propose a Quantized Domain-Mixing Representation (QDMR) framework that learns domain-mixing degradation-aware priors via a VQGAN, enabling effective unsupervised domain adaptation. The approach captures shared degradation patterns across synthetic and real domains through a trainable Domain-Mixing Codebook (DMC), which guides the CAC model to generalize to real-world data.

## Method Summary
The QDMR framework formulates real-world CAC as an unsupervised domain adaptation problem. It uses a VQGAN pretrained on both synthetic and real aberrated images to learn a Domain-Mixing Codebook (DMC) that characterizes shared degradation patterns. The QDMR-Base model leverages this DMC to modulate deep features of the CAC network, while QDMR-UDA extends this with source-to-target transformation (s2tT) and adversarial feature alignment to adapt the model to the target domain.

## Key Results
- QDMR methods consistently outperform competitive CAC models and UDA frameworks on both synthetic and real-world benchmarks
- On Real-Snap benchmark, QDMR-UDA improves NIQE metric by up to 36% compared to source-only models
- Visual results show fewer artifacts and more natural-looking real-world CAC outputs
- The framework achieves strong performance with as few as 100 target domain images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning a Domain-Mixing Codebook (DMC) via VQGAN enables effective domain adaptation by capturing shared degradation priors across synthetic and real domains.
- Mechanism: The VQGAN reconstructs aberrated images from both domains using a shared encoder and decoder, forcing the codebook to represent degradation patterns common to both. This learned DMC then guides the CAC model by modulating features to transfer target domain knowledge.
- Core assumption: The degradation patterns in aberrated images are consistent enough across domains that a shared representation can capture them effectively.

### Mechanism 2
- Claim: The s2tT (source to target transformation) constraint enables effective domain adaptation by generating pseudo-target image pairs for supervised training.
- Mechanism: The trained VQGAN transforms source domain images into target domain-like images, creating paired data {Ys2t, X}. This allows supervised training on target-like data while the adversarial discriminator ensures realistic transformation.
- Core assumption: The VQGAN can effectively transform source images to resemble target domain images when trained with adversarial loss on both domains.

### Mechanism 3
- Claim: Adversarial feature alignment between domains creates domain-invariant features that enable the CAC model to generalize across domains.
- Mechanism: A feature-level discriminator forces the restoration features from both domains to have similar distributions, allowing the decoder trained on source features to work effectively on target features.
- Core assumption: The CAC task can be solved with domain-invariant features, and aligning feature distributions will transfer this invariance.

## Foundational Learning

- Concept: Vector Quantization and Codebook Learning
  - Why needed here: VQGAN uses vector quantization to learn discrete representations of image features, which is crucial for capturing degradation patterns in the DMC.
  - Quick check question: How does the nearest-neighbor quantization in Eq. 1 create a discrete representation that can be used for image reconstruction?

- Concept: Unsupervised Domain Adaptation (UDA)
  - Why needed here: The paper frames real-world CAC as a UDA problem where paired synthetic data is available but real-world data is unpaired, requiring adaptation strategies.
  - Quick check question: What is the key difference between supervised domain adaptation and unsupervised domain adaptation in terms of available data?

- Concept: Adversarial Training and Feature Alignment
  - Why needed here: Adversarial losses are used both in VQGAN training and feature alignment to ensure realistic image generation and domain-invariant features.
  - Quick check question: How does the discriminator in feature alignment (Lf a) encourage domain-invariant feature distributions?

## Architecture Onboarding

- Component map:
  - VQGAN: Shared encoder (Evq) and decoder (Gvq) with Domain-Mixing Codebook (DMC)
  - QDMR-Base: CAC encoder (Ecac), affine transformation module (M), bottleneck (Bcac), decoder (Gcac)
  - QDMR-UDA: Adds feature-level discriminator (Df a) and modified training objectives
  - Training data flows: Source domain (DS), pseudo-target domain (DS2T), target domain (DT)

- Critical path:
  1. VQGAN pretraining with both domains to learn DMC
  2. QDMR-Base training with DMC modulation on source domain
  3. QDMR-UDA training with s2tT and feature alignment on all data flows

- Design tradeoffs:
  - Using a shared VQGAN for both domains enables learning common degradation patterns but may miss domain-specific details
  - The s2tT approach generates pseudo-target data but relies on the VQGAN's transformation quality
  - Feature alignment promotes domain invariance but may lose task-specific information

- Failure signatures:
  - Poor adaptation: High LPIPS values between source-only and QDMR results
  - Artifacts in s2tT: Generated target images contain obvious source domain characteristics
  - Degraded quality: Feature alignment causes loss of important restoration details

- First 3 experiments:
  1. Train QDMR-Base without feature alignment (λf a=0) to isolate DMC's contribution
  2. Train with only source domain data (λs2t=0, λt=0) to measure s2tT's impact
  3. Test with different codebook sizes (K) to find optimal representation capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum number of target domain images required for effective UDA training in the DACAC task?
- Basis in paper: The paper explores this question in Section 7.3, reporting that only a few target images (as few as 100) can yield effective QDMR learning, with negligible impact on QDMR-Base results. QDMR-UDA shows some benefit from larger target data scales, but 500 samples provide comparable performance to the entire training set.
- Why unresolved: While the paper provides some insights, the optimal target data scale likely depends on factors such as the complexity of the domain gap, the diversity of the target domain, and the specific UDA strategies employed. A more comprehensive study with varying domain gap scenarios and UDA methods could further elucidate the relationship between target data scale and UDA performance.

### Open Question 2
- Question: How can the QDMR framework be extended to handle multiple optical systems with diverse aberration behaviors?
- Basis in paper: The paper focuses on two specific optical systems (MOS-S1 and MOS-S2) with distinct aberration behaviors. However, real-world applications may involve a wider range of optical systems, each with unique aberration characteristics. The effectiveness of QDMR in handling such diverse scenarios remains unexplored.
- Why unresolved: The paper does not investigate the generalization capability of QDMR to multiple optical systems with varying aberration behaviors. The learned DMC and UDA strategies may need to be adapted or extended to effectively capture and address the complexities of diverse optical systems.

### Open Question 3
- Question: Can the QDMR framework be integrated with more advanced UDA strategies to further improve performance?
- Basis in paper: The paper proposes the QDMR-UDA framework, which incorporates s2tT and feature alignment strategies. However, there are other advanced UDA techniques, such as self-supervised learning, meta-learning, and domain-invariant feature learning, that could potentially enhance the performance of QDMR in the DACAC task.
- Why unresolved: The paper primarily focuses on the proposed QDMR framework and its integration with s2tT and feature alignment. The potential benefits of incorporating other advanced UDA strategies remain unexplored.

## Limitations
- Framework performance heavily depends on the quality of VQGAN's domain-mixing codebook, which may struggle with significantly different degradation patterns between domains
- Adversarial feature alignment assumes domain-invariant features are sufficient for CAC task, which may not hold for all optical aberrations
- s2tT effectiveness is limited by VQGAN's ability to generate realistic target-like images from source data

## Confidence
- **High Confidence**: The mechanism of using vector quantization to learn domain-mixing degradation-aware priors (Mechanism 1) is well-supported by the VQGAN's established capabilities in image reconstruction and codebook learning.
- **Medium Confidence**: The s2tT approach for generating pseudo-target image pairs (Mechanism 2) is reasonable but depends critically on the VQGAN's transformation quality, which may vary across different domain gaps.
- **Medium Confidence**: The adversarial feature alignment mechanism (Mechanism 3) is theoretically sound but may have limitations when the CAC task requires domain-specific features that cannot be effectively aligned.

## Next Checks
1. **Codebook Robustness Analysis**: Test QDMR-Base performance across varying levels of synthetic-to-real domain gap by systematically degrading synthetic images to simulate different real-world conditions. Measure how codebook size (K) affects adaptation robustness.

2. **Transformation Quality Assessment**: Evaluate the s2tT mechanism by conducting a human perceptual study comparing VQGAN-generated pseudo-target images against real target images, measuring both visual similarity and aberration characteristics.

3. **Feature Alignment Ablation**: Conduct controlled experiments removing the adversarial feature alignment component while keeping s2tT active, to quantify the marginal benefit of feature alignment versus pseudo-target supervision alone.