---
ver: rpa2
title: 'WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting'
arxiv_id: '2410.03159'
source_url: https://arxiv.org/abs/2410.03159
tags:
- attention
- attn
- matrix
- arma
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WAVE attention, a mechanism that integrates
  moving-average (MA) components into autoregressive (AR) attention mechanisms for
  time series forecasting (TSF). The key innovation is an indirect MA weight generation
  method that enables efficient computation of the MA term without increasing time
  complexity or parameter size.
---

# WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.03159
- Source URL: https://arxiv.org/abs/2410.03159
- Reference count: 40
- Authors: Jiecheng Lu; Xu Han; Yan Sun; Shihao Yang
- Primary result: WAVE attention consistently improves AR attention mechanisms across 12 datasets, achieving state-of-the-art results with 3.7 position average ranking improvement

## Executive Summary
This paper introduces WAVE attention, a mechanism that integrates moving-average (MA) components into autoregressive (AR) attention mechanisms for time series forecasting. The key innovation is an indirect MA weight generation method that enables efficient computation of the MA term without increasing time complexity or parameter size. Experimental results demonstrate that WAVE attention consistently improves performance across various AR attention mechanisms and achieves state-of-the-art results on 12 widely-used time series datasets.

## Method Summary
WAVE attention combines autoregressive and moving-average components through an indirect weight generation approach. Instead of explicitly computing the full N×N MA weight matrix, WAVE uses residuals from AR output as value inputs for the MA component, enabling efficient linear attention computation. The method employs weight sharing between AR and MA components (sharing Q matrix, setting V to identity) to maintain parameter efficiency. Specific activation functions (-LeakyReLU for ϕMAq and sigmoid for ϕMAk) are chosen to create MA weights that decay appropriately from the diagonal while maintaining flexibility. The approach achieves O(N) time complexity while matching the parameter size of baseline models.

## Key Results
- WAVE attention improves average rankings by 3.7 positions compared to pure AR attention
- Greatest gains observed in gated linear attention (4.3 position improvement)
- Consistent performance improvements across 12 diverse time series datasets
- Maintains linear time complexity O(N) and parameter size parity with baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Indirect MA weight generation allows WAVE attention to incorporate moving-average components without increasing time complexity or parameter size.
- Mechanism: Uses residuals from AR output as value inputs for MA component, enabling efficient linear attention computation while maintaining MA properties.
- Core assumption: Implicitly generated MA weights through beta matrix accurately capture short-term effects needed for forecasting.
- Evidence anchors: [abstract] indirect MA weight generation method; [section 2.7] linear RNN approach for efficient computation.

### Mechanism 2
- Claim: Decoupling short-term fluctuations into MA weights allows AR weights to focus on long-term patterns.
- Mechanism: Assigns short-term effects to MA component through indirect weight generation, letting AR component focus on long-term dependencies.
- Core assumption: Time series data contains distinct short-term and long-term patterns that can be effectively separated.
- Evidence anchors: [section 2.5] decoupling local effects into short-term MA weights; [section 2.8] MA term models short-term effects.

### Mechanism 3
- Claim: Specific activation function choices ensure proper MA weight characteristics for time series forecasting.
- Mechanism: -LeakyReLU for ϕMAq and sigmoid for ϕMAk create MA weights that decay appropriately from diagonal while maintaining flexibility.
- Core assumption: MA weights should have larger values near diagonal and gradually decrease, with negative values preferred.
- Evidence anchors: [section 2.8] choice of -LeakyReLU and sigmoid; Figure 4 showing balanced lag weight pattern.

## Foundational Learning

- Concept: Autoregressive (AR) vs Moving-Average (MA) components in time series models
  - Why needed here: Understanding the distinction between AR and MA components is crucial for grasping how WAVE attention separates short-term and long-term effects.
  - Quick check question: What is the fundamental difference between how AR and MA components model time series data?

- Concept: Linear attention mechanisms and their computational advantages
  - Why needed here: WAVE builds on linear attention, so understanding how it reduces O(N²) to O(N) complexity is essential for appreciating the efficiency gains.
  - Quick check question: How does linear attention achieve O(N) complexity compared to standard attention's O(N²)?

- Concept: Weight sharing and parameter efficiency in neural networks
  - Why needed here: WAVE uses weight sharing to maintain parameter size while adding MA components, so understanding this technique is important.
  - Quick check question: How does weight sharing between AR and MA components help maintain parameter efficiency in WAVE attention?

## Architecture Onboarding

- Component map: Input tokenization → AR attention computation → Residual calculation → MA attention computation → Output projection
- Critical path: Input → AR attention computation → Residual calculation → MA attention computation → Output projection
- Design tradeoffs:
  - Linear vs softmax attention: Linear attention offers O(N) complexity but may sacrifice some modeling capacity
  - Activation function choices: Different combinations affect MA weight patterns and model performance
  - Lookback length: Longer input sequences improve performance but increase computational cost
- Failure signatures:
  - Training instability: May indicate issues with indirect MA weight generation or activation function choices
  - Degraded performance on long-term patterns: Suggests MA component is capturing too much or AR component isn't focusing on long-term effects
  - No improvement over pure AR: Indicates decoupling between short-term and long-term effects isn't working effectively
- First 3 experiments:
  1. Compare pure AR attention vs WAVE attention on a simple dataset to verify performance improvement
  2. Test different activation function combinations for ϕMAq and ϕMAk to find optimal weight patterns
  3. Evaluate impact of different lookback lengths (LI) on both AR and WAVE attention performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of ϕMA_q activation function affect the performance of WAVE attention on different types of time series data?
- Basis in paper: [explicit] Discussion of activation function selection and their impact on MA weights
- Why unresolved: Limited empirical comparison on diverse time series datasets
- What evidence would resolve it: Experiments comparing WAVE attention with different ϕMA_q activation functions on various time series datasets

### Open Question 2
- Question: Can WAVE attention be effectively combined with other attention mechanisms, such as sparse attention or local attention?
- Basis in paper: [inferred] Mention of WAVE attention adapting to various attention mechanisms
- Why unresolved: Focus on standard attention variants without exploring combinations
- What evidence would resolve it: Implementing and evaluating WAVE attention with sparse or local attention on long time series

### Open Question 3
- Question: How does the parameter α in the key activation function ϕMA_k affect the balance between long-term and short-term modeling?
- Basis in paper: [explicit] Discussion of α's role in controlling B matrix variance and MA weights' focus
- Why unresolved: Limited systematic study of different α values on various datasets
- What evidence would resolve it: Experiments with different α values on diverse time series datasets

## Limitations

- Evaluation focuses exclusively on one-step-ahead forecasting without exploring multi-step prediction performance
- Experiments use relatively small model architectures (m=3 layers, 8 heads) compared to state-of-the-art transformers
- Limited ablation studies to isolate the contribution of each component in the indirect MA weight generation method

## Confidence

- High Confidence: Fundamental mechanism of combining AR and MA components through indirect weight generation
- Medium Confidence: Experimental results and performance improvements across 12 datasets
- Low Confidence: Generalizability of activation function choices and indirect MA generation method

## Next Checks

1. **Ablation Study on Activation Functions**: Systematically test alternative activation function combinations for ϕMAq and ϕMAk to quantify their impact on performance.

2. **Multi-Step Forecasting Evaluation**: Extend evaluation to multi-step-ahead forecasting to assess whether WAVE attention maintains advantages for longer prediction horizons.

3. **Scalability Testing**: Evaluate WAVE attention with larger model configurations and compare against state-of-the-art large-scale time series models to assess scalability.