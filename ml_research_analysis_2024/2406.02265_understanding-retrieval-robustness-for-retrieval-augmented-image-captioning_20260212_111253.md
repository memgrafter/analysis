---
ver: rpa2
title: Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning
arxiv_id: '2406.02265'
source_url: https://arxiv.org/abs/2406.02265
tags:
- captions
- retrieved
- head
- tokens
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the robustness of retrieval-augmented image
  captioning models, focusing on the SmallCap model. The research reveals that SmallCap
  is sensitive to majority tokens in retrieved captions, which can lead to misleading
  generations when irrelevant information is present.
---

# Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning

## Quick Facts
- arXiv ID: 2406.02265
- Source URL: https://arxiv.org/abs/2406.02265
- Reference count: 18
- Primary result: Training with sampled retrieved captions improves robustness of retrieval-augmented image captioning models by reducing sensitivity to majority tokens

## Executive Summary
This paper investigates the robustness of retrieval-augmented image captioning models, specifically focusing on the SmallCap model. The authors identify that SmallCap is sensitive to majority tokens in retrieved captions, leading to potential generation errors when irrelevant information is present. To address this, they propose training the model with sampled retrieved captions from a larger list, which exposes the model to more diverse context and improves both in-domain and cross-domain performance by reducing reliance on majority tokens.

## Method Summary
The authors propose a training method that samples retrieved captions from a larger list rather than using fixed top-k relevant captions. They implement two sampling strategies: sample-k (randomly selecting k captions from the larger list) and controlled sample-k (keeping the top relevant caption while randomly sampling the remaining k-1). The model is evaluated on COCO for in-domain performance and NoCaps and VizWiz for cross-domain evaluation, using CIDEr score as the primary metric. Input attribution analysis is used to understand how tokens from retrieved captions influence generation.

## Key Results
- SmallCap is sensitive to tokens appearing in majority of retrieved captions, which are often copied into generated output
- Training with sampled retrieved captions improves performance across all tested datasets and k values
- Controlled sampling (keeping top caption while sampling others) further improves cross-domain performance
- The proposed method effectively mitigates the impact of noisy retrieval context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SmallCap is sensitive to majority tokens in retrieved captions.
- Mechanism: The model copies majority tokens from retrieved captions into generated output, especially when irrelevant information is present.
- Core assumption: Tokens appearing in more than half of retrieved captions act as dominant signals that the model prioritizes during generation.
- Evidence anchors:
  - [abstract]: "Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output."
  - [section 4.2]: "We find that the probability of majority vote in the 2G1B setting is 86.47%. This high probability suggests that the majority tokens in the good captions could be being used to guide the model generation."
  - [corpus]: Weak - corpus contains related retrieval-augmented captioning papers but none specifically address majority token sensitivity.
- Break condition: If training data contains diverse ground truth captions where majority tokens in retrieved captions rarely match ground truth, the copying behavior would be less reinforced.

### Mechanism 2
- Claim: Training with sampled retrieved captions from a larger list improves robustness.
- Mechanism: Random sampling exposes the model to diverse context including both top and lower-ranked captions, preventing overfitting to top-relevant captions.
- Core assumption: The model learns to distinguish relevant from irrelevant context when exposed to more varied training examples.
- Evidence anchors:
  - [abstract]: "We propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance."
  - [section 5.2]: "The application of sampling improves across all values of k for Vizwiz. On the NoCaps dataset, with the COCO datastore, sampling consistently improves near and out-domain performance."
  - [corpus]: Weak - corpus contains related retrieval-augmented captioning papers but none specifically address sampling methods for robustness.
- Break condition: If sampling rate is too high, the model might not learn to effectively use relevant context, reducing performance.

### Mechanism 3
- Claim: Controlled sampling (keeping top caption while sampling others) further improves cross-domain performance.
- Mechanism: The model learns to prioritize truly relevant information while being exposed to diverse context.
- Core assumption: Including the top-relevant caption ensures the model always has a reliable reference point while learning to filter noise from other sampled captions.
- Evidence anchors:
  - [section 5.2]: "Finally, on top of our best performing sample-k model, controlled sample-k further improves performance for both NoCaps and VizWiz."
  - [section 5.1]: "Aiming to train the model that better distinguishes irrelevant context, we design a controlled sampling process — selecting k −1 randomly from the larger list while keeping the top relevant caption of the image during training."
  - [corpus]: Weak - corpus contains related retrieval-augmented captioning papers but none specifically address controlled sampling methods.
- Break condition: If the top-relevant caption is not actually relevant, this method could reinforce copying of misleading information.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Understanding how SmallCap incorporates retrieved captions as additional context for generation
  - Quick check question: What is the difference between retrieval-augmented generation and traditional image captioning?

- Concept: Input attribution methods
  - Why needed here: Analyzing how individual tokens in retrieved captions influence model predictions
  - Quick check question: What does high attribution from a retrieved token to a generated token indicate about model behavior?

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how the model attends to different parts of the input (image patches, retrieved captions, generation tokens)
  - Quick check question: In cross-attention, what does it mean if the model pays maximum attention to retrieved captions rather than image patches?

## Architecture Onboarding

- Component map:
  Image encoder (ResNet-50x64 or CLIP-ViT-B/32) -> Text encoder for retrieval (CLIP) -> Retrieval system (image-to-text similarity using CLIP embeddings) -> SmallCap model (frozen encoder + trainable cross-attention layer + GPT-2/OPT decoder)

- Critical path:
  1. Input image → image encoder → CLIP embedding
  2. CLIP embedding → retrieval system → top-N captions
  3. Top-N captions + image → SmallCap cross-attention layer
  4. Cross-attention output → decoder → generated caption

- Design tradeoffs:
  - Using frozen encoders reduces training parameters but limits fine-tuning capability
  - Retrieving multiple captions provides more context but increases risk of including irrelevant information
  - Sampling during training improves robustness but may reduce in-domain performance

- Failure signatures:
  - Performance drop when using randomly retrieved captions
  - High attribution scores to majority tokens in retrieved captions
  - Cross-attention focusing on text tokens rather than image patches

- First 3 experiments:
  1. Evaluate SmallCap with randomly retrieved captions vs top-k captions
  2. Calculate majority token probability in 2 good + 1 bad caption setup
  3. Train SmallCap with sampled captions and compare performance on cross-domain datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sensitivity to majority tokens observed in SmallCap extend to other retrieval-augmented image captioning models?
- Basis in paper: [explicit] The authors state, "We expect that our analysis can inspire better retrieval-robust captioning models in the field," implying their findings may be applicable beyond SmallCap.
- Why unresolved: The paper only evaluates SmallCap, leaving the generalizability of the findings to other models unclear.
- What evidence would resolve it: Conducting similar analyses on other retrieval-augmented image captioning models to determine if they also exhibit sensitivity to majority tokens.

### Open Question 2
- Question: What is the optimal balance between training time and the level of noise exposure for achieving model robustness?
- Basis in paper: [inferred] The authors mention, "We are curious if there exists an optimal balance between training time and the level of noise exposure for achieving model robustness," indicating this as an open area of investigation.
- Why unresolved: The paper trains the model for the same number of epochs as SmallCap, without exploring the impact of varying training duration on robustness.
- What evidence would resolve it: Systematically varying the number of training epochs and the level of noise in the training data to identify the point of optimal robustness.

### Open Question 3
- Question: How does the choice of visual encoder impact the robustness of retrieval-augmented image captioning models?
- Basis in paper: [inferred] The authors note, "Investigating how model robustness varies with different visual encoders would enhance the scope of our study," suggesting this as a potential direction for future research.
- Why unresolved: The paper uses only the CLIP-ViT-B/32 backbone as the image encoder, limiting the generalizability of the findings to other visual encoders.
- What evidence would resolve it: Evaluating the robustness of retrieval-augmented image captioning models using different visual encoders, such as ResNet or Swin Transformer, to assess the impact on model performance.

## Limitations
- Analysis focused on single model architecture (SmallCap) limits generalizability to other retrieval-augmented captioning models
- Input attribution analysis may not fully capture complex attention patterns in the model
- Effectiveness of controlled sampling depends on quality of retrieval system which is not explicitly evaluated

## Confidence

- **High Confidence**: The observation that SmallCap is sensitive to majority tokens in retrieved captions is well-supported by multiple experiments and analyses.
- **Medium Confidence**: The mechanism explaining why majority tokens influence generation is supported by attribution analysis but could benefit from additional validation.
- **Low Confidence**: The claim that the proposed methods would generalize to other retrieval-augmented models is not tested.

## Next Checks

1. **Controlled Token Manipulation Experiment**: Create synthetic retrieved captions where specific tokens appear with controlled frequencies (e.g., 100%, 75%, 50%, 25%) and measure how generation changes. This would directly test whether token frequency in retrieved captions drives copying behavior rather than other factors like semantic relevance.

2. **Ablation of Sampling Methods**: Systematically vary the sampling rate (k) and measure the trade-off between robustness and in-domain performance across all datasets. This would clarify the optimal sampling strategy and determine whether the controlled sampling method provides benefits beyond simple random sampling.

3. **Cross-Model Generalization Test**: Apply the sampling training method to at least two other retrieval-augmented captioning architectures (e.g., dual retrieval models or transformer-based approaches) and compare robustness improvements. This would validate whether the findings extend beyond the SmallCap model and identify which architectural features are critical for the proposed solution.