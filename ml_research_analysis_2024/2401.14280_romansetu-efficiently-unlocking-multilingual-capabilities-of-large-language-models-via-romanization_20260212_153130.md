---
ver: rpa2
title: 'RomanSetu: Efficiently unlocking multilingual capabilities of Large Language
  Models via Romanization'
arxiv_id: '2401.14280'
source_url: https://arxiv.org/abs/2401.14280
tags:
- romanized
- native
- script
- languages
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the extension of English language models
  to non-English languages with non-Roman scripts using romanized text as an interface.
  The authors propose a two-stage approach: first, continually pretraining an English
  LLM on romanized data of the target languages, then performing instruction tuning
  on the romanized data.'
---

# RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization

## Quick Facts
- **arXiv ID**: 2401.14280
- **Source URL**: https://arxiv.org/abs/2401.14280
- **Reference count**: 19
- **Primary result**: Romanization extends English LLMs to non-Latin scripts with 2-4x token efficiency gains and competitive performance

## Executive Summary
This work proposes RomanSetu, a method to extend English language models to non-English languages with non-Roman scripts using romanized text as an interface. The authors demonstrate that romanization reduces token fertility by 2-4x compared to native scripts while maintaining competitive or superior performance across NLU, NLG, and MT tasks. Their two-stage approach combines continual pretraining on romanized data with instruction tuning, showing that romanized representations are better aligned with English than native scripts. Experiments with 5 Indic languages across 4 scripts and 2 language families validate the effectiveness of this approach.

## Method Summary
The method uses a two-stage approach: (1) continual pretraining of an English LLM on mixed English and romanized data to prevent catastrophic forgetting while learning cross-lingual representations, and (2) instruction tuning on romanized instruction datasets translated from English. The approach uses full fine-tuning without vocabulary expansion, processing romanized text through the existing Llama 2 tokenizer. The authors use IndicXlit, a lossy romanization scheme, and balance native and romanized data during pretraining to maintain English capabilities while acquiring multilingual knowledge.

## Key Results
- Romanized text reduces token fertility by 2-4x compared to native scripts
- Romanized representations match or outperform native script representations across NLU, NLG, and MT tasks
- For generation tasks like translation, romanized representations show significant improvements over native scripts

## Why This Works (Mechanism)

### Mechanism 1
Romanized text reduces token fertility by 2x-4x compared to native scripts. The Llama 2 tokenizer is optimized for Latin scripts, so romanized text maps more directly to tokens while native scripts require more tokens per word due to script-specific tokenization patterns.

### Mechanism 2
Romanized representations are better aligned with English representations than native script representations. Shared Latin script creates overlapping token embeddings between English and romanized text, leading to more similar embedding spaces compared to native scripts which have distinct token distributions.

### Mechanism 3
Romanized instruction tuning produces competitive or superior results across NLU, NLG, and MT tasks. The combination of reduced token length (efficiency) and better cross-lingual alignment (representational proximity) enables more effective knowledge transfer from English LLM capabilities to target languages.

## Foundational Learning

- **Cross-lingual transfer in language models**: Why needed - the entire approach relies on transferring English LLM capabilities to non-English languages via romanization. Quick check - What factors influence how effectively knowledge transfers between languages in multilingual models?

- **Tokenization and fertility in subword tokenizers**: Why needed - the efficiency gains claimed are based on reduced token fertility. Quick check - How does token fertility affect model performance and resource utilization?

- **Transliteration vs romanization**: Why needed - the work uses romanization (lossy) rather than transliteration (reversible), which has implications for output quality. Quick check - What's the difference between transliteration and romanization, and why might one be preferred over the other?

## Architecture Onboarding

- **Component map**: Llama 2 7B base model → Continuous pretraining with romanized data → Instruction fine-tuning with romanized data → Evaluation
- **Critical path**: Romanization → CPT → IFT → Evaluation
- **Design tradeoffs**: Using romanization vs native script (efficiency gains vs potential loss of script-specific information), using IndicXlit (lossy) vs ITRANS (reversible) (better performance vs reversible mapping capability), full fine-tuning vs parameter-efficient methods (potentially better performance vs lower computational cost)
- **Failure signatures**: Poor task performance despite romanization (likely issues with romanization quality or instruction data), no efficiency gains (tokenizer not optimized for Latin script or romanization scheme creates long tokens), translation errors in output (issues with romanization quality or transliteration model)
- **First 3 experiments**: 
  1. Measure token fertility on a sample of native vs romanized text to verify the 2-4x reduction claim
  2. Compare sentence embeddings between English, native script, and romanized text to verify representational alignment
  3. Run a small-scale translation task (e.g., English to Hindi) with both native and romanized approaches to compare performance

## Open Questions the Paper Calls Out

1. **Cross-lingual generalizability**: How does the effectiveness of romanization compare across different language families and scripts beyond the Indic languages studied? The paper acknowledges that findings should generalize but calls for further experimentation to confirm this hypothesis across other language families and scripts.

2. **Optimal data balance**: What is the optimal balance between native script and romanized data in the continual pretraining phase for maximizing cross-lingual transfer? The paper uses a 1:1 ratio but does not explore whether this ratio is optimal or if different ratios might yield better results.

3. **Romanization quality impact**: How does the quality of romanization affect the performance of cross-lingual transfer, particularly when using deterministic vs. natural romanization schemes? The paper uses natural romanization and acknowledges the trade-off with deterministic schemes but does not empirically compare the performance impact of different romanization quality levels.

## Limitations

- Limited evaluation scope focused on translation and classification tasks with minimal analysis of generation quality beyond chrF and Rouge-L metrics
- Lack of comparison to alternative approaches for extending English LLMs to non-English languages (native script pretraining, vocabulary expansion, parameter-efficient methods)
- Missing implementation details including exact hyperparameters, data preprocessing specifics, and instruction-tuning dataset characteristics

## Confidence

- **High Confidence**: Romanized text reduces token fertility by 2-4x; the two-stage approach is feasible and produces working models; romanized representations show better cross-lingual alignment with English
- **Medium Confidence**: Romanized representations match or outperform native script representations across tasks; romanization is an efficient alternative for extending English LLMs; the approach generalizes to other non-Latin script languages
- **Low Confidence**: Romanization is superior to all alternative approaches; the approach comprehensively solves the challenge of extending English LLMs; benefits of romanization outweigh all potential drawbacks in practical applications

## Next Checks

1. **Cross-Lingual Embedding Analysis**: Perform systematic analysis of sentence embeddings from English, native script, and romanized representations across all 5 languages. Compute and compare cosine similarity scores between English and target language embeddings in both native and romanized forms to quantify the claimed representational alignment.

2. **Output Quality Assessment**: Implement and evaluate a transliteration model to convert romanized outputs back to native scripts for translation tasks. Assess the quality of these reconstructed outputs using both automatic metrics and human evaluation to quantify any degradation from the romanization round-trip.

3. **Alternative Approach Comparison**: Implement a baseline approach using continued pretraining on native scripts without romanization, using the same data quantities and model architecture. Compare performance across all evaluated tasks to determine whether the benefits observed are specific to romanization or could be achieved through simpler methods.