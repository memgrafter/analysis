---
ver: rpa2
title: 'Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data'
arxiv_id: '2403.17091'
source_url: https://arxiv.org/abs/2403.17091
tags:
- divides
- alt0
- udcurlymod
- disp
- alt1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offline reinforcement learning with value function
  realizability but without Bellman completeness. The authors show that the sample
  complexity of offline policy evaluation is governed by the concentrability coefficient
  in an aggregated Markov Transition Model, which may grow exponentially with the
  horizon length even when the concentrability coefficient in the original MDP is
  small and the offline data is admissible.
---

# Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data

## Quick Facts
- **arXiv ID**: 2403.17091
- **Source URL**: https://arxiv.org/abs/2403.17091
- **Reference count**: 40
- **Primary result**: Sample complexity of offline RL is governed by aggregated concentrability coefficient, which may grow exponentially with horizon even when original concentrability is small.

## Executive Summary
This paper establishes fundamental limits for offline reinforcement learning under value function realizability without Bellman completeness. The key insight is that sample complexity is determined by the concentrability coefficient in an aggregated Markov transition model rather than the original MDP. The authors show that even with admissible offline data, the aggregated concentrability can grow exponentially with horizon length, making sample-efficient learning impossible. A generic reduction proves trajectory data offers no additional benefits over admissible data for this setting.

## Method Summary
The paper provides both lower and upper bounds for offline policy evaluation. The lower bound construction (Theorem 3.1) creates Markov transition models and aggregation schemes where standard concentrability remains polynomial while aggregated concentrability grows exponentially. The upper bound adapts the BVFT algorithm (Xie and Jiang, 2021) to solve a minimax problem over function class pairs, estimating Bellman error in the aggregated MDP. The algorithm's sample complexity depends on the aggregated concentrability coefficient, which is upper bounded by the pushforward concentrability used in prior works.

## Key Results
- Sample complexity for offline policy evaluation is governed by aggregated concentrability coefficient, not original MDP's concentrability
- Even with admissible offline data, aggregated concentrability can grow exponentially with horizon length (2^Ω(H))
- A generic reduction converts hard instances with admissible data to hard instances with trajectory data, showing no statistical benefits of trajectory data
- The aggregated concentrability coefficient can be exponentially larger than standard concentrability while preserving boundedness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample complexity governed by aggregated concentrability coefficient in Markov transition model
- Mechanism: Algorithm cannot distinguish states with identical value functions under function class, forcing aggregation that creates different transition structure
- Core assumption: Value function realizability without Bellman completeness; function class doesn't distinguish certain states
- Evidence anchors:
  - [abstract] "sample complexity... governed by the concentrability coefficient in an aggregated Markov Transition Model"
  - [section] "jointly determined by the function class and the offline data distribution"
  - [corpus] Weak evidence - neighbors focus on trajectory sufficiency

### Mechanism 2
- Claim: Aggregated concentrability grows exponentially with horizon even with admissible data
- Mechanism: Construction shows admissible data and small original concentrability still yield 2^Ω(H) aggregated concentrability
- Core assumption: Aggregation scheme and data distribution create bottleneck amplifying with horizon
- Evidence anchors:
  - [abstract] "concentrability coefficient... may grow exponentially with the horizon length"
  - [section] "Example 1... standard concentrability is O(H^3) whereas aggregated concentrability is 2^Ω(H)"
  - [corpus] No direct evidence in neighbors

### Mechanism 3
- Claim: Trajectory data offers no extra benefits over admissible data
- Mechanism: Generic reduction converts admissible data hard instances to trajectory data hard instances
- Core assumption: Reduction maintains bounded concentrability while preserving statistical hardness
- Evidence anchors:
  - [abstract] "generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data"
  - [section] "There exists a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data"
  - [corpus] Weak evidence - neighbors focus on trajectory sufficiency

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) structure
  - Why needed here: Understanding layered state space, transition dynamics, and occupancy measures is fundamental to concentrability definitions
  - Quick check question: In an H-horizon MDP with layered states, what is the dimension of the state space at layer h?

- **Concept**: Value function approximation and Bellman equations
  - Why needed here: Realizability (Q^π_e ∈ F) without Bellman completeness requires understanding value function-transition relationships
  - Quick check question: If a value function class F contains Q^π_e but not Th+1(F), what does this imply about Bellman completeness?

- **Concept**: Concentrability coefficient
  - Why needed here: Measures mismatch between data distribution and target policy distribution; aggregated variant governs sample complexity
  - Quick check question: How does concentrability coefficient change if offline data distribution equals occupancy measure of some policy?

## Architecture Onboarding

- **Component map**: Original MDP -> Aggregation scheme -> Aggregated MDP -> Concentrability computation -> Sample complexity bound
- **Critical path**: 1) Compute aggregated transition model from original MDP and data distribution 2) Determine aggregated concentrability coefficient 3) Use to bound sample complexity or establish hardness
- **Design tradeoffs**: More expressive function classes reduce aggregation but increase computational complexity; trajectory vs admissible data affects reduction arguments
- **Failure signatures**: Exponentially growing aggregated concentrability despite bounded original concentrability indicates need for additional assumptions
- **First 3 experiments**:
  1. Implement aggregation scheme for simple MDP with known value function class, verify states with identical value functions are aggregated
  2. Compute concentrability coefficient in aggregated MDP and compare to original MDP's coefficient
  3. Test reduction from admissible to trajectory data on small MDP instance, verify statistical hardness preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what additional structural assumptions on MDP or value function classes can trajectory data provide statistical or computational improvements beyond bounded concentrability?
- Basis in paper: [inferred] Paper shows worst-case no benefits of trajectory data, but notes not all MDPs are worst-case
- Why unresolved: Provides worst-case lower bound but doesn't explore beneficial scenarios under additional assumptions
- What evidence would resolve it: Theoretical analysis showing specific MDP structures enabling polynomial sample complexity with trajectory data, or empirical study demonstrating improved performance

### Open Question 2
- Question: Can the gap between upper and lower bounds in ε dependence be closed?
- Basis in paper: [explicit] Notes upper bound has 1/ε^4 dependence while lower bound has 1/ε dependence
- Why unresolved: Provides both bounds but doesn't attempt to tighten ε dependence
- What evidence would resolve it: Refined BVFT analysis reducing 1/ε^4 to 1/ε^2, or lower bound construction increasing accuracy dependence to 1/ε^4

### Open Question 3
- Question: Is statistically efficient offline policy optimization feasible under bounded concentrability and value function realizability?
- Basis in paper: [inferred] Focuses on policy evaluation, leaves policy optimization as future research
- Why unresolved: Provides evaluation lower bounds but doesn't address more challenging optimization problem
- What evidence would resolve it: Lower bound showing exponential sample complexity for optimization, or algorithm with polynomial sample complexity for optimization

## Limitations

- Lower bound construction relies on specific aggregation schemes that may not capture all practical scenarios
- Exponential growth of aggregated concentrability requires careful verification in concrete implementations
- Reduction from admissible to trajectory data depends on preservation of statistical hardness through transformation
- Upper bound algorithm's practical performance depends heavily on function class choice and aggregation scheme compatibility

## Confidence

- **High Confidence**: Theoretical framework connecting aggregated concentrability to sample complexity is well-established
- **Medium Confidence**: Reduction argument showing no trajectory data benefits, while theoretically sound, requires careful implementation
- **Medium Confidence**: Upper bound algorithm's theoretical guarantees depend on specific function class properties

## Next Checks

1. Implement aggregation scheme from Theorem 3.1 on simple MDP instance, empirically verify states with identical value functions are correctly aggregated, measure resulting concentrability change

2. Construct concrete example demonstrating reduction from admissible to trajectory data, verify through simulation that statistical hardness (sample complexity requirements) is preserved through transformation

3. Implement BVFT-based upper bound algorithm, test on synthetic MDPs with known true value function, measure whether sample complexity scales as predicted by aggregated concentrability coefficient