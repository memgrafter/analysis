---
ver: rpa2
title: 'Falcon: Faster and Parallel Inference of Large Language Models through Enhanced
  Semi-Autoregressive Drafting and Custom-Designed Decoding Tree'
arxiv_id: '2412.12639'
source_url: https://arxiv.org/abs/2412.12639
tags: []
core_contribution: This paper addresses the challenge of balancing minimal drafting
  latency and high speculation accuracy in speculative decoding of Large Language
  Models (LLMs). The authors introduce Falcon, an innovative semi-autoregressive speculative
  decoding framework that enhances both the drafter's parallelism and output quality.
---

# Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree

## Quick Facts
- arXiv ID: 2412.12639
- Source URL: https://arxiv.org/abs/2412.12639
- Reference count: 12
- Primary result: Achieves 2.91x-3.51x lossless speedup on Vicuna and LLaMA2-Chat models using semi-autoregressive drafting

## Executive Summary
This paper introduces Falcon, a speculative decoding framework that addresses the trade-off between drafting latency and speculation accuracy in Large Language Model inference. Falcon combines semi-autoregressive drafting with Coupled Sequential Glancing Distillation and a custom-designed decoding tree to generate multiple tokens in parallel while maintaining high acceptance rates. The framework demonstrates significant acceleration (2.91x-3.51x) on benchmark datasets without compromising output quality, outperforming existing speculative decoding methods while using a compact drafter architecture.

## Method Summary
Falcon implements semi-autoregressive (SAR) speculative decoding by generating k tokens in parallel through a custom decoding tree. The framework uses Coupled Sequential Glancing Distillation (CSGD) to improve the drafter's accuracy by strengthening inter-token dependencies within blocks, leveraging tokens from one time step ahead. The drafter consists of only two Transformer layers and is trained on ShareGPT dialogue data using AdamW optimizer. The decoding tree organizes generated tokens for parallel verification by the LLM, enabling multiple forward passes and significantly improving the overall acceptance rate.

## Key Results
- Achieves lossless speedup ratio of 2.91x to 3.51x on Vicuna and LLaMA2-Chat model series
- Maintains high speculation accuracy through Coupled Sequential Glancing Distillation
- Uses compact drafter architecture equivalent to only two Transformer layers
- Demonstrates superior acceleration compared to existing speculative decoding methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CSGD improves SAR drafter accuracy by strengthening inter-token dependencies within each block
- Mechanism: Replaces continuous predicted sequences with ground-truth ones from LLM based on Hamming distance, allowing drafter to access tokens from one time step ahead for richer context
- Core assumption: Drafter can effectively utilize one-step-ahead information to improve prediction accuracy
- Evidence anchors: Abstract states CSGD "fortifies inter-token dependencies within the same block, leading to increased speculation accuracy"; section explains leveraging features and tokens from one time step ahead

### Mechanism 2
- Claim: Custom-Designed Decoding Tree enables multiple forward passes and generates more tokens per pass, increasing acceptance rate
- Mechanism: Tree structure allows drafter to perform m forward passes and generate n*k tokens per pass, organized for parallel verification by LLM
- Core assumption: Tree structure and parallel verification can handle increased candidate tokens without significant overhead
- Evidence anchors: Abstract mentions tree "accommodating multiple forward passes as needed, thereby boosting the number of drafted tokens and significantly improving the overall acceptance rate"

### Mechanism 3
- Claim: Falcon achieves better balance between low drafting latency and high speculation accuracy compared to existing methods
- Mechanism: Combines SAR drafting (reduces latency) with CSGD (improves accuracy) and decoding tree (increases token throughput)
- Core assumption: SAR drafting enhanced with CSGD and decoding tree can outperform both pure AR and SAR methods in latency-accuracy tradeoff
- Evidence anchors: Abstract states framework achieves "superior acceleration capabilities" with 2.91x-3.51x speedup; comprehensive evaluations demonstrate performance

## Foundational Learning

- Concept: Transformer architecture and autoregressive decoding
  - Why needed here: Falcon builds upon and modifies Transformer architecture to implement SAR drafting approach
  - Quick check question: What is the key difference between autoregressive and semi-autoregressive decoding in terms of token generation order?

- Concept: Speculative decoding and its components (drafter, verifier)
  - Why needed here: Falcon is a speculative decoding framework using drafter to generate tokens and verifier (LLM) to check them
  - Quick check question: In speculative decoding, what is the role of the drafter and how does it differ from the verifier?

- Concept: Attention mechanisms and their role in capturing dependencies
  - Why needed here: Falcon uses Relaxed Causal-Masked Multi-Head Self-Attention to process input sequences while preserving causality
  - Quick check question: How does Relaxed Causal-Masked Multi-Head Self-Attention differ from standard self-attention in terms of information flow?

## Architecture Onboarding

- Component map: Embedding Layer -> Language Model Head -> Relaxed Causal SAR Decoding Head -> Custom-Designed Decoding Tree -> LLM Verifier

- Critical path: Token sequence → Embedding Layer → SAR Decoding Head → Generate k tokens in parallel → Tokens organized in Decoding Tree → LLM verifies tokens in parallel → Accepted tokens passed to next iteration

- Design tradeoffs: SAR drafting vs. AR drafting (SAR reduces latency but may sacrifice accuracy; Falcon uses CSGD to mitigate this); Number of forward passes (more passes increase token generation but also computational cost); Tree complexity (more complex trees can handle more tokens but may introduce overhead)

- Failure signatures: Low acceptance rate (indicates issues with drafter accuracy or CSGD implementation); High latency (may suggest problems with SAR decoding efficiency or tree structure); Memory overflow (could indicate issues with token organization in decoding tree)

- First 3 experiments: 1) Test Falcon's basic functionality with small LLM and simple dataset to verify token generation and acceptance; 2) Compare Falcon's performance with pure AR and SAR methods on benchmark dataset to measure speedup and accuracy; 3) Vary k parameter in Falcon to find optimal balance between token generation and accuracy for different model sizes

## Open Questions the Paper Calls Out
- What is the theoretical limit of speedup achievable by Falcon-like frameworks before acceptance rate becomes too low to be practical?
- How does Falcon's performance scale with increasingly large language models (e.g., 70B+ parameters) compared to smaller models?
- What is the optimal k value for different types of language tasks (e.g., code generation vs. mathematical reasoning vs. dialogue)?

## Limitations
- Training data domain transfer from ShareGPT dialogue to code generation and mathematical reasoning tasks may affect generalization
- Memory and computational overhead of SAR drafter, decoding tree, and parallel verification process is not fully characterized
- Optimal parameter selection for k, m, and n values requires per-deployment tuning without clear guidance

## Confidence
- **High Confidence**: Architectural innovations are clearly described and logically sound; 2.91x-3.51x speedup is well-supported by evaluation
- **Medium Confidence**: "Lossless" speedup claim is supported by automatic metrics but lacks human evaluation; generalization across task domains is demonstrated but not thoroughly analyzed
- **Low Confidence**: Drafter architecture being "compact" needs verification of actual memory and computational overhead during inference

## Next Checks
- Implement parameter sensitivity analysis varying k, m, and n on held-out validation set to determine optimal configurations for different hardware constraints
- Conduct human evaluation studies comparing Falcon-generated outputs with target LLM across benchmark datasets to verify "lossless" claim
- Test Falcon's performance on out-of-domain datasets (scientific papers, legal documents, low-resource languages) to assess training approach robustness