---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment
  Analysis
arxiv_id: '2412.02279'
source_url: https://arxiv.org/abs/2412.02279
tags:
- llms
- subtasks
- absa
- sentiment
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive evaluation of large language
  models (LLMs) on 13 aspect-based sentiment analysis (ABSA) datasets across 8 subtasks.
  The study explores two paradigms: fine-tuning-dependent, where LLMs are efficiently
  fine-tuned using instruction-based multi-task learning and LoRA, and fine-tuning-free,
  where LLMs use in-context learning (ICL) with three demonstration selection strategies
  (random, BM25, SimCSE).'
---

# A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis

## Quick Facts
- arXiv ID: 2412.02279
- Source URL: https://arxiv.org/abs/2412.02279
- Authors: Changzhi Zhou; Dandan Song; Yuhang Tian; Zhijing Wu; Hao Wang; Xinyu Zhang; Jun Yang; Ziyi Yang; Shuhao Zhang
- Reference count: 23
- Large language models efficiently fine-tuned with LoRA outperform full fine-tuned small language models on all ABSA subtasks.

## Executive Summary
This paper provides the first comprehensive evaluation of large language models (LLMs) on aspect-based sentiment analysis (ABSA) across 13 datasets and 8 subtasks. The study compares two paradigms: fine-tuning-dependent, where LLMs are efficiently fine-tuned using instruction-based multi-task learning and LoRA, and fine-tuning-free, where LLMs use in-context learning with demonstration selection strategies. Results demonstrate that efficiently fine-tuned LLMs significantly outperform fine-tuned small language models, even with minimal parameter tuning. In the fine-tuning-free paradigm, LLMs achieve remarkable performance through in-context learning, particularly when using sophisticated demonstration selection strategies that combine keyword and semantic information.

## Method Summary
The study evaluates LLMs on ABSA using two paradigms: fine-tuning-dependent and fine-tuning-free. For fine-tuning-dependent, the approach employs instruction-based multi-task learning with LoRA for parameter-efficient fine-tuning across 13 datasets and 8 subtasks. For fine-tuning-free, the study uses in-context learning (ICL) with three demonstration selection strategies: random selection, BM25-based keyword matching, and SimCSE-based semantic similarity. The evaluation includes 6 LLMs (LLaMA3-8B, ChatGLM3-6B, QWen1.5-7B, Mistral-7B-v0.2, LLaMA3-70B, GPT4) across all subtasks, measuring performance using F1 scores. Datasets are prepared by combining training and validation sets while removing overlaps with test sets.

## Key Results
- Efficiently fine-tuned LLMs (e.g., LLaMA3-8B with 3.4M parameters) outperform full fine-tuned SLMs (e.g., T5-base with 220M parameters) on all ABSA subtasks
- In the fine-tuning-free paradigm, LLMs with ICL achieve impressive performance, matching fine-tuned SLMs on some tasks
- Demonstration selection strategies significantly impact ICL effectiveness, with BM25 and SimCSE outperforming random selection
- Combining keyword and semantic information yields the best results for demonstration selection in ICL

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned LLMs with LoRA can outperform full fine-tuned SLMs using minimal parameter tuning. Instruction-based multi-task learning combined with LoRA allows LLMs to learn sentiment analysis and structured extraction tasks efficiently, leveraging their strong pre-trained language understanding. Core assumption: LLMs' pre-trained language understanding generalizes well to ABSA tasks without requiring full fine-tuning. Evidence: Extensive experiments show LLMs achieve state-of-the-art performance compared to fine-tuned SLMs. Break condition: If pre-training data lacks coverage of ABSA-relevant patterns or instruction formulation fails to guide proper task execution.

### Mechanism 2
BM25 and SimCSE demonstration selection strategies significantly improve ICL performance compared to random selection. BM25 retrieves sentences with keyword overlap to target sentences, aiding aspect term extraction, while SimCSE retrieves semantically similar sentences, improving sentiment understanding. Core assumption: Keyword and semantic information are complementary and both contribute to effective ICL for ABSA tasks. Evidence: The keyword and semantic information in ICL can significantly improve LLM performance, and combining these two orthogonal types of information leads to better effectiveness. Break condition: If demonstration selection strategies retrieve irrelevant or misleading examples that confuse the model's learning process.

### Mechanism 3
LLMs exhibit strong zero-shot performance on ALSC subtask, rivaling fine-tuned SLMs. LLMs can understand task instructions and sentiment analysis requirements without additional fine-tuning, leveraging their pre-trained language understanding. Core assumption: LLMs' pre-training includes sufficient sentiment analysis and natural language understanding capabilities. Evidence: In the fine-tuning-free paradigm where SLMs fail completely, API-based LLMs with ICL achieve remarkable performance, even matching that of fine-tuned SLMs in some subtasks. Break condition: If task instructions are ambiguous or if LLMs' pre-training lacks sufficient sentiment analysis examples.

## Foundational Learning

- Concept: Aspect-Based Sentiment Analysis (ABSA) subtasks
  - Why needed here: Understanding the 8 ABSA subtasks (AE, OE, ALSC, AOE, AESC, AOPE, ASTE, ASQP) is crucial for evaluating LLM performance across different task formulations.
  - Quick check question: What are the four sentiment elements involved in ABSA and how do they differ across subtasks?

- Concept: In-Context Learning (ICL) and demonstration selection strategies
  - Why needed here: ICL is the key mechanism for evaluating LLMs in the fine-tuning-free paradigm, and understanding different demonstration selection strategies is essential for analyzing performance differences.
  - Quick check question: How do BM25 and SimCSE differ in their approach to selecting demonstrations for ICL?

- Concept: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA
  - Why needed here: LoRA is the primary method for efficiently fine-tuning LLMs, and understanding its mechanism is crucial for interpreting the comparison with full fine-tuning.
  - Quick check question: What is the key advantage of LoRA over full fine-tuning in terms of parameter efficiency and performance?

## Architecture Onboarding

- Component map: LLaMA3-8B, ChatGLM3-6B, QWen1.5-7B, Mistral-7B-v0.2, LLaMA3-70B, GPT4 -> 13 ABSA datasets (D17, D19, D20, D21) -> 8 ABSA subtasks (AE, OE, ALSC, AOE, AESC, AOPE, ASTE, ASQP) -> fine-tuning-dependent (instruction-based multi-task learning + LoRA) or fine-tuning-free (ICL with demonstration selection) -> F1 score evaluation
- Critical path: Dataset preparation -> Model fine-tuning (with or without LoRA) -> Demonstration selection strategy implementation -> ICL execution -> Performance evaluation using F1 scores
- Design tradeoffs: Choice between fine-tuning and ICL involves balancing performance requirements with computational costs and data availability. Selection of demonstration strategies involves balancing keyword relevance and semantic similarity.
- Failure signatures: Performance degradation in ICL can occur due to irrelevant demonstrations, overly complex task formulations, or insufficient model pre-training. Fine-tuning failures can occur due to poor instruction formulation or insufficient training data.
- First 3 experiments:
  1. Evaluate zero-shot performance of LLaMA3-70B on ALSC subtask to establish baseline ICL capabilities.
  2. Compare BM25 vs SimCSE demonstration selection strategies on ASTE subtask to quantify the impact of different selection approaches.
  3. Evaluate LoRA fine-tuning efficiency by comparing LLaMA3-8B performance with T5-Instruct on AESC subtask while tracking parameter count and training time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause ICL to sometimes degrade LLM performance on ABSA tasks, and can these be systematically predicted or mitigated?
- Basis in paper: The paper notes that ICL can cause adverse effects in rare cases, particularly with LLaMA3-70B using random demonstrations on relatively simple subtasks, and suggests this may be due to increased prompt length degrading performance.
- Why unresolved: The paper observes this phenomenon but does not provide a comprehensive explanation of the underlying mechanisms or develop predictive models for when ICL might fail.
- What evidence would resolve it: Systematic experiments varying prompt length, demonstration quality, and task complexity across different LLMs, combined with analysis of attention patterns and loss landscapes during ICL.

### Open Question 2
- Question: How does the effectiveness of cross-task transfer in low-resource fine-tuning settings vary across different ABSA subtasks, and what are the optimal transfer sequences?
- Basis in paper: The paper demonstrates that warming up on existing subtasks improves performance on new ABSA subtasks in low-resource settings, but does not extensively explore which transfer sequences are most effective or why certain transfers work better than others.
- Why unresolved: The paper shows that cross-task transfer is beneficial but does not investigate the hierarchical relationships between subtasks or develop a framework for determining optimal transfer sequences.
- What evidence would resolve it: Comprehensive experiments testing all possible transfer sequences across subtasks, analysis of task similarity metrics, and development of transfer learning guidelines specific to ABSA.

### Open Question 3
- Question: What is the optimal balance between keyword-based and semantic-based demonstration selection strategies for different ABSA subtasks and LLM architectures?
- Basis in paper: The paper finds that combining BM25 (keyword-based) and SimCSE (semantic-based) demonstrations yields better performance than either strategy alone, but does not extensively explore how this balance should vary by task or model.
- Why unresolved: The paper demonstrates the complementarity of keyword and semantic information but does not provide guidance on how to dynamically adjust this balance based on task characteristics or model capabilities.
- What evidence would resolve it: Extensive experiments varying the ratio of keyword to semantic demonstrations across all subtasks and LLM variants, combined with analysis of which information type is more critical for different task types.

## Limitations
- Evaluation focuses exclusively on English ABSA datasets, limiting generalizability to other languages
- Instruction-based multi-task learning approach lacks detailed implementation specifications for exact replication
- Study primarily evaluates open-source LLMs with parameter counts up to 70B, leaving questions about performance scaling with larger models

## Confidence
- High confidence: The comparative performance advantage of fine-tuned LLMs over SLMs using LoRA is well-supported by systematic experiments across multiple datasets and tasks
- Medium confidence: The effectiveness of BM25 and SimCSE demonstration selection strategies for ICL is demonstrated but could benefit from more extensive ablation studies across different task types
- Medium confidence: The claim that LLMs achieve zero-shot performance rivaling fine-tuned SLMs on ALSC subtasks is supported but requires verification across additional model architectures and dataset variations

## Next Checks
1. **Cross-lingual validation**: Evaluate the fine-tuning-dependent and fine-tuning-free paradigms on multilingual ABSA datasets to assess the approach's generalizability beyond English
2. **Scaling analysis**: Test the performance trends with larger LLMs (e.g., LLaMA3-405B) and smaller variants to establish clear scaling laws and identify optimal model sizes for different ABSA subtasks
3. **Alternative demonstration strategies**: Implement and compare additional demonstration selection methods (e.g., contrastive learning-based retrieval, learned demonstration selection) against BM25 and SimCSE to determine if further performance gains are achievable