---
ver: rpa2
title: 'LEMMA-RCA: A Large Multi-modal Multi-domain Dataset for Root Cause Analysis'
arxiv_id: '2406.05375'
source_url: https://arxiv.org/abs/2406.05375
tags:
- system
- data
- root
- cause
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEMMA-RCA, a large-scale, multi-modal, multi-domain
  dataset for root cause analysis (RCA). The dataset addresses the lack of real-world,
  open-source RCA datasets by providing diverse fault scenarios from IT and OT operation
  systems.
---

# LEMMA-RCA: A Large Multi-modal Multi-domain Dataset for Root Cause Analysis

## Quick Facts
- arXiv ID: 2406.05375
- Source URL: https://arxiv.org/abs/2406.05375
- Reference count: 28
- Authors: Lecheng Zheng; Zhengzhang Chen; Dongjie Wang; Chengyuan Deng; Reon Matsuoka; Haifeng Chen

## Executive Summary
This paper introduces LEMMA-RCA, a large-scale, multi-modal, multi-domain dataset designed for root cause analysis (RCA) in complex systems. The dataset addresses the critical gap in real-world, open-source RCA datasets by providing diverse fault scenarios from both IT (microservices) and OT (water systems) operation systems. LEMMA-RCA includes hundreds of system entities with various data modalities including logs, metrics, and KPIs, enabling comprehensive evaluation of RCA methods across offline/online and single/multi-modal settings.

## Method Summary
The paper presents LEMMA-RCA as a multi-modal, multi-domain dataset for root cause analysis, evaluated using eight baseline RCA methods across different settings. The dataset includes microservice platforms (Product Review, Cloud Computing) and operational technology systems (SWaT, WADI) with logs, metrics, and KPIs. Baseline methods include causal discovery algorithms (PC, DYNOTEARS, C-LSTM, GOLEM, REASON), multi-modal approaches (NeZha, MULAN), and online methods (CORAL). The evaluation framework uses ranking metrics including PR@K, MAP@K, and MRR to assess performance in both offline and online settings.

## Key Results
- Multi-modal RCA methods (e.g., MULAN) achieved superior performance, with PR@1 of 100% on certain datasets by leveraging complementary information from metrics and logs
- Online RCA methods significantly outperformed offline methods on LEMMA-RCA due to its large scale (100,000+ timestamps) and dynamic fault patterns
- Causal graph-based methods provided better interpretability and performance by learning dependency structures between system entities and KPIs
- The dataset is publicly available at https://lemma-rca.github.io/

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal RCA methods outperform single-modal methods by leveraging complementary information from metrics and logs
- Mechanism: Combining metric and log data allows RCA models to capture both quantitative system behavior and qualitative anomaly signals, improving root cause identification accuracy
- Core assumption: Different data modalities contain complementary information about system faults, and causal discovery methods can effectively integrate these signals
- Evidence anchors:
  - [abstract]: "We evaluate the quality of LEMMA-RCA by testing the performance of eight baseline methods on this dataset under various settings, including offline and online modes as well as single and multiple modalities."
  - [section]: "Integrating both metric and log data enhances the performance of most RCA methods in terms of MRR, compared to using only metric data. This suggests that log data complements these methods, aiding in more accurate identification of potential root causes."
  - [corpus]: Weak evidence. The corpus contains related papers on multi-modal RCA but doesn't directly confirm this mechanism
- Break condition: If the causal discovery methods cannot effectively integrate multi-modal data or if modalities contain redundant rather than complementary information

### Mechanism 2
- Claim: Online RCA methods outperform offline methods on LEMMA-RCA due to its large scale and real-world fault dynamics
- Mechanism: Online methods can iteratively update causal graphs with new data batches, capturing changing fault patterns and reducing noise accumulation that affects offline methods
- Core assumption: LEMMA-RCA's large scale (100,000+ timestamps) and real-world fault scenarios create dynamic patterns that require continuous adaptation
- Evidence anchors:
  - [abstract]: "The experimental results demonstrate the high quality of LEMMA-RCA. The dataset is publicly available at https://lemma-rca.github.io/"
  - [section]: "Among online methods, CORAL significantly outperforms NOTEARS* and GOLEM* due to the design of state-invariant and state-dependent representations learning tailored for the online setting. Notably, LEMMA-RCA is a large-scale real-world dataset, consisting of more than 100,000 timestamps across several days with various system fault scenarios, which can be naturally transformed to the online setting, compared with small datasets (e.g., NeZha [12]) with limited timestamps for online RCA."
  - [corpus]: Moderate evidence. Related papers discuss online RCA but don't directly validate this specific mechanism
- Break condition: If the online adaptation mechanism introduces instability or if the fault patterns are too sparse to benefit from incremental learning

### Mechanism 3
- Claim: Causal graph-based RCA methods provide better interpretability and performance than non-causal approaches
- Mechanism: By learning dependency structures between system entities and KPIs, causal methods can trace fault propagation paths and identify root causes rather than just symptoms
- Core assumption: System faults follow causal propagation patterns that can be discovered from monitoring data
- Evidence anchors:
  - [abstract]: "Particularly, causal structure learning technique has proven effective in constructing causal or dependency graphs between different system entities and key performance indicators (KPIs), thereby enabling the tracing of underlying causes through these structures."
  - [section]: "The experimental results demonstrate the high quality of LEMMA-RCA and its extensive utility for advanced research in root cause analysis."
  - [corpus]: Moderate evidence. The corpus contains papers on causal RCA methods that support this general principle
- Break condition: If the causal assumptions don't hold in real systems or if the causal discovery methods are too computationally expensive for practical use

## Foundational Learning

- Concept: Root Cause Analysis (RCA)
  - Why needed here: RCA is the fundamental problem being addressed, requiring understanding of failure diagnosis in complex systems
  - Quick check question: What distinguishes root cause analysis from simple anomaly detection?

- Concept: Causal Structure Learning
  - Why needed here: The dataset is designed for causal RCA methods, which require understanding of causal discovery techniques
  - Quick check question: How do constraint-based causal discovery methods like PC algorithm differ from score-based methods like GOLEM?

- Concept: Multi-modal Data Integration
  - Why needed here: LEMMA-RCA contains both metrics and logs, requiring methods that can effectively combine these different data types
  - Quick check question: What are the main challenges in integrating structured metric data with unstructured log data for RCA?

## Architecture Onboarding

- Component map:
  - Data Collection Layer: Microservice platforms (Product Review, Cloud Computing), OT systems (SWaT, WADI)
  - Preprocessing Pipeline: Log parsing, feature extraction, KPI construction, stationarity assessment
  - Baseline Methods: Causal discovery algorithms (PC, DYNOTEARS, C-LSTM, GOLEM, REASON), multi-modal methods (NeZha, MULAN), online methods (CORAL)
  - Evaluation Framework: PR@K, MAP@K, MRR metrics for offline and online settings

- Critical path:
  1. Data preprocessing and feature extraction
  2. Causal structure learning from historical data
  3. Root cause identification using learned causal graphs
  4. Performance evaluation with ranking metrics

- Design tradeoffs:
  - Single-modal vs. multi-modal: Simpler models vs. potentially better accuracy
  - Offline vs. online: Computational efficiency vs. real-time adaptation capability
  - Complexity vs. interpretability: More complex models may be harder to interpret

- Failure signatures:
  - Poor PR@K scores: Model cannot distinguish root causes from non-root causes
  - Low MRR scores: Model ranks root causes too low in the list
  - Inconsistent performance across domains: Model doesn't generalize well

- First 3 experiments:
  1. Run single-modal offline RCA on Product Review dataset using only metrics
  2. Run multi-modal offline RCA on same dataset using both metrics and logs
  3. Compare online vs offline performance on Cloud Computing dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific metrics or performance indicators would best capture the effectiveness of multi-modal RCA methods in real-world, dynamic environments?
- Basis in paper: [explicit] The paper mentions the need for online, multi-modal RCA methods and their importance in dynamic environments like industrial automation and real-time monitoring services
- Why unresolved: While the paper identifies the gap in current RCA methods, it does not specify which metrics would be most effective in evaluating these methods in real-world, dynamic environments
- What evidence would resolve it: Conducting empirical studies comparing different metrics' effectiveness in evaluating multi-modal RCA methods across various dynamic environments would provide concrete evidence

### Open Question 2
- Question: How can LEMMA-RCA be expanded to include more diverse fault scenarios that better reflect the complexity of real-world systems?
- Basis in paper: [explicit] The paper acknowledges that the fault scenarios in LEMMA-RCA may not adequately reflect the diversity of conditions prevalent in broader real-world applications
- Why unresolved: The paper does not provide specific strategies or methodologies for expanding the dataset to include more diverse fault scenarios
- What evidence would resolve it: Developing and implementing a framework for systematically incorporating a wider range of fault scenarios into LEMMA-RCA, followed by validation studies, would provide evidence for effective expansion

### Open Question 3
- Question: What are the most effective techniques for handling missing data in system metric readings within the context of RCA?
- Basis in paper: [explicit] The paper mentions that LEMMA-RCA has some missing data in system metric readings due to factors like system interruptions
- Why unresolved: The paper does not explore or recommend specific techniques for handling missing data in RCA applications
- What evidence would resolve it: Comparative studies evaluating different missing data handling techniques on LEMMA-RCA and their impact on RCA performance would provide concrete evidence for the most effective approaches

## Limitations

- The paper's claims about multi-modal superiority and online method performance are based on evaluations against eight baseline methods, but specific implementations and hyperparameters are not fully detailed
- While the dataset spans multiple domains (IT and OT systems), the generalization of results across these domains has not been thoroughly validated
- The specific mechanisms by which multi-modal integration improves performance and why online methods outperform offline ones are not fully explained or validated

## Confidence

- **High confidence**: The dataset's existence and basic characteristics (multi-modal, multi-domain, real-world IT and OT systems) are well-established
- **Medium confidence**: The comparative performance of different RCA methods (multi-modal vs single-modal, online vs offline) is supported by experimental results but requires implementation of complex baselines for full validation
- **Low confidence**: The specific mechanisms by which multi-modal integration improves performance and why online methods outperform offline ones in this context are not fully explained or validated

## Next Checks

1. Implement and run a subset of the baseline methods (e.g., PC and MULAN) on a single dataset (e.g., Product Review) to verify the reported performance metrics (PR@1, MRR)
2. Conduct ablation studies by running methods with only metrics and only logs separately to quantify the claimed complementary benefits of multi-modal data
3. Test the online RCA methods on a small time window of the dataset to verify the claimed advantage of iterative causal graph updates over offline approaches