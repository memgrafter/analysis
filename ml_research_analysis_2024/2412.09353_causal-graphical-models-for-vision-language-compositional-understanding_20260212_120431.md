---
ver: rpa2
title: Causal Graphical Models for Vision-Language Compositional Understanding
arxiv_id: '2412.09353'
source_url: https://arxiv.org/abs/2412.09353
tags:
- which
- made
- cogt
- dependency
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of vision-language models (VLMs)
  struggling with compositional language understanding, where models often treat sentences
  as bags of words rather than considering syntactic and semantic relationships. The
  proposed solution, COGT, uses causal graphical models (CGMs) guided by dependency
  parsing to structure the decoding process.
---

# Causal Graphical Models for Vision-Language Compositional Understanding

## Quick Facts
- **arXiv ID**: 2412.09353
- **Source URL**: https://arxiv.org/abs/2412.09353
- **Reference count**: 29
- **Key outcome**: COGT significantly outperforms state-of-the-art approaches on compositional vision-language tasks by using causal graphical models to structure decoding and avoid spurious correlations.

## Executive Summary
This paper addresses the challenge of vision-language models (VLMs) struggling with compositional language understanding, where models often treat sentences as bags of words rather than considering syntactic and semantic relationships. The proposed solution, COGT, uses causal graphical models (CGMs) guided by dependency parsing to structure the decoding process. Instead of standard autoregressive or parallel prediction, COGT decodes tokens in a partially-ordered sequence following the CGM, focusing on main causal dependencies and ignoring spurious correlations. The method trains a decoder conditioned on a frozen VLM encoder, using syntactic and visual features.

## Method Summary
COGT uses dependency parsing to extract syntactic trees from captions, then builds a CGM where each word is a node connected by syntactic dependencies. A decoder is trained conditioned on frozen VLM visual features, using dependency-guided attention to predict tokens following the CGM structure rather than sequential order. During inference, likelihood estimation follows the same dependency structure, computing P(Wj|Parents(Wj),Z) where Parents(Wj) are determined by the dependency tree. This partially-ordered decoding strategy aims to learn only main causal dependencies while discarding spurious correlations from sequential word order.

## Key Results
- COGT significantly outperforms all prior state-of-the-art approaches on five compositional benchmarks (ARO, SugarCrepe, VL-Checklist, ColorSwap, and FG-OVD)
- COGT-CLIP+ (trained on COCO, CC3M, and Visual Genome) achieves the best performance, outperforming models trained on much larger datasets
- COGT improves VLM performance on standard downstream tasks without degradation

## Why This Works (Mechanism)

### Mechanism 1
COGT's partially-ordered decoding guided by dependency parsing avoids spurious correlations that standard autoregressive models learn. The dependency parser extracts syntactic and semantic relationships between words in a sentence. COGT uses these relationships to structure the decoder's prediction order, ensuring each word is predicted only after its syntactic head is known. This breaks the strong conditioning on previous words that introduces spurious associations in standard autoregressive models. Core assumption: Dependency relations extracted by a parser represent causal dependencies between words in natural language.

### Mechanism 2
The disentangled factorization of conditional distributions in COGT reduces the number of parameters needed to learn the joint distribution. Instead of learning P(Wj|W1,...,Wj-1) as in standard autoregressive models, COGT learns P(Wj|Parents(Wj),Z), where Parents(Wj) is determined by the dependency tree. This factorization is sparser than the full autoregressive factorization, reducing overfitting risk and improving generalization. Core assumption: The joint distribution of words in a sentence can be factorized according to the sparse dependency structure.

### Mechanism 3
COGT's conditional independence assumptions during inference prevent spurious associations from affecting likelihood estimation. During inference, COGT predicts tokens following the dependency tree structure rather than sequential order. This means the likelihood of a caption is computed based on the actual syntactic dependencies rather than the sequential order of words, eliminating spurious associations introduced by word order. Core assumption: The likelihood of a caption should be computed based on its syntactic dependencies rather than sequential word order.

## Foundational Learning

- **Concept: Dependency parsing**
  - Why needed here: COGT relies on dependency trees to structure the decoding process. Understanding how parsers extract syntactic relationships is fundamental to understanding how COGT works.
  - Quick check question: What is the difference between a dependency relation and a constituency relation in syntactic parsing?

- **Concept: Causal graphical models**
  - Why needed here: COGT builds a CGM from the dependency tree to model causal relationships between visual features and textual tokens. Understanding CGMs is essential for grasping the theoretical foundation.
  - Quick check question: How does a CGM differ from a standard Bayesian network in terms of the assumptions about the relationships between variables?

- **Concept: Disentangled factorization**
  - Why needed here: COGT's advantage comes from learning a disentangled factorization of the joint distribution. Understanding this concept is key to appreciating why COGT generalizes better.
  - Quick check question: In what way does a disentangled factorization reduce the risk of overfitting compared to a standard autoregressive factorization?

## Architecture Onboarding

- **Component map**: Frozen VLM visual encoder (CLIP/VXLM/InstructBLIP) → Feature extraction (Z) → Mapping network (M) → Cross-attention in decoder ← Dependency-guided self-attention ← Masked tokens with syntactic conditioning → Vocabulary distribution

- **Critical path**: Image → Visual encoder → Mapping network → Cross-attention in decoder ← Dependency-guided self-attention ← Masked tokens with syntactic conditioning → Vocabulary distribution

- **Design tradeoffs**: Using frozen visual encoder vs. fine-tuning: Preserves pre-trained knowledge but limits adaptation; Dependency-guided vs. standard attention: Reduces spurious correlations but requires accurate parsing; Multiple syntactic masks vs. single mask: Better syntactic conditioning but more parameters

- **Failure signatures**: Poor parsing quality → Incorrect dependency trees → Wrong prediction order; Missing syntactic categories → Wrong mask tokens → Poor conditioning; Over-reliance on visual features → Ignores linguistic structure → Degraded performance

- **First 3 experiments**: 1) Compare COGT with standard autoregressive decoding on a simple compositional benchmark (ARO); 2) Test different dependency parsers (Deep Biaffine vs CRFPar) to assess sensitivity to parsing quality; 3) Evaluate the impact of using only the last layer features vs. both last and penultimate layers of the visual encoder

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implicit in the work: how different dependency parsers affect performance, whether larger training datasets would further improve results, and how COGT's performance varies across different types of compositional tasks.

## Limitations
- The assumption that dependency relations represent causal dependencies between words is not empirically validated
- The paper doesn't address how parsing errors propagate through the system or provide ablation studies showing the relative contributions of CGM structure versus other architectural choices
- Claims about universal improvement on downstream tasks without degradation are based on limited evidence

## Confidence
- **High confidence**: Experimental results showing COGT's performance gains on compositional benchmarks are well-documented and reproducible
- **Medium confidence**: Theoretical framing using causal graphical models is compelling but relies on assumptions about the causal nature of syntactic dependencies that aren't empirically validated
- **Low confidence**: Assertion that COGT's gains transfer to standard downstream tasks without degradation is based on limited evidence

## Next Checks
1. **Ablation study on parsing quality**: Systematically vary the quality of the dependency parser (using gold-standard vs. automatic parses) to quantify how parsing errors affect COGT's performance
2. **Causal mechanism validation**: Design controlled experiments where spurious correlations are artificially introduced or removed to test whether COGT's CGM structure actually prevents learning these correlations
3. **Downstream task analysis**: Conduct a comprehensive study of COGT's impact across diverse downstream tasks, including failure cases and conditions under which performance degrades