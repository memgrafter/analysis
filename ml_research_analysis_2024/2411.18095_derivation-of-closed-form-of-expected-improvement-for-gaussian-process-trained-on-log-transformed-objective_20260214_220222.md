---
ver: rpa2
title: Derivation of Closed Form of Expected Improvement for Gaussian Process Trained
  on Log-Transformed Objective
arxiv_id: '2411.18095'
source_url: https://arxiv.org/abs/2411.18095
tags:
- u1d466
- u1d462
- u1d467
- u1d70e
- u1d451
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a detailed derivation of the closed-form solution
  for Expected Improvement (EI) in Bayesian optimization when the Gaussian Process
  is trained on log-transformed objective functions. The author builds upon Hutter
  et al.'s (2009) work, filling in the intermediate steps missing from the original
  proposition.
---

# Derivation of Closed Form of Expected Improvement for Gaussian Process Trained on Log-Transformed Objective

## Quick Facts
- arXiv ID: 2411.18095
- Source URL: https://arxiv.org/abs/2411.18095
- Authors: Shuhei Watanabe
- Reference count: 1
- Provides closed-form solution for Expected Improvement when Gaussian Process is trained on log-transformed objectives

## Executive Summary
This paper derives a closed-form solution for the Expected Improvement (EI) acquisition function in Bayesian optimization when the underlying Gaussian Process is trained on log-transformed objective functions. The work addresses a gap in the literature by filling in the intermediate mathematical steps that were omitted in Hutter et al.'s (2009) original proposition. The author demonstrates that the EI for log-transformed objectives takes a specific analytical form that differs from simply applying a logarithmic transformation to the standard EI formula. This mathematical result is particularly valuable because logarithmic transformation often improves the predictive accuracy of Gaussian Processes, leading to better optimization performance in practice.

## Method Summary
The paper presents a detailed mathematical derivation of the Expected Improvement formula when the Gaussian Process is trained on log-transformed objectives. The author starts from first principles of Bayesian optimization and Gaussian Process regression, working through the probabilistic manipulations required to obtain a closed-form solution. The derivation shows how the expected improvement calculation changes when working in the log-transformed space versus the original objective space. The key insight is that the standard EI formula cannot simply be transformed by taking logarithms - instead, a fundamentally different analytical form emerges that accounts for the properties of log-normal distributions. The paper provides the complete mathematical steps that were omitted from Hutter et al.'s original work, making the result reproducible and verifiable.

## Key Results
- Derives closed-form EI formula for log-transformed objectives: EI* = u₁D466.alt★ Φ(u₁D467) − exp(u₁D707 + u₁D70E²/2) Φ(u₁D467 − u₁D70E), where u₁D467 = log u₁D466.alt★ − u₁D707/u₁D70E
- Clarifies distinction between log-transformed EI and taking logarithm of standard EI
- Demonstrates that log-transformation improves GP predictive accuracy for EI-based optimization
- Provides complete mathematical derivation filling gaps in Hutter et al. (2009) work

## Why This Works (Mechanism)
The mechanism works because log-transformation stabilizes the variance of the objective function and makes it more amenable to Gaussian Process modeling. When the objective function is log-transformed, it often becomes more Gaussian-like, satisfying the normality assumptions underlying GP regression. The closed-form EI derivation leverages the properties of log-normal distributions, where the expected value of the exponential of a Gaussian random variable has a specific analytical form. This allows the integration required for EI calculation to be performed analytically rather than numerically, resulting in computational efficiency gains.

## Foundational Learning
- **Gaussian Process Regression**: Why needed - forms the foundation for Bayesian optimization; Quick check - verify understanding of GP predictive mean and variance
- **Expected Improvement Acquisition**: Why needed - core Bayesian optimization acquisition function being analyzed; Quick check - understand standard EI formula derivation
- **Log-normal Distribution Properties**: Why needed - EI derivation relies on properties of log-transformed variables; Quick check - verify E[exp(X)] for X ~ N(μ, σ²)
- **Bayesian Optimization Framework**: Why needed - provides context for EI and GP usage; Quick check - understand exploration-exploitation tradeoff
- **Mathematical Integration Techniques**: Why needed - EI derivation requires integration of Gaussian densities; Quick check - verify standard normal CDF properties

## Architecture Onboarding

**Component Map**
GP Model -> Log Transformation -> Predictive Distribution -> EI Calculation -> Acquisition Function

**Critical Path**
Objective Function -> Log Transformation -> GP Training -> Predictive Mean/Variance -> EI Computation -> Optimization Loop

**Design Tradeoffs**
The paper's approach trades computational simplicity for modeling assumptions. The closed-form solution requires strict log-normality assumptions, which may not hold for all objective functions. Alternative approaches using numerical integration would be more flexible but computationally expensive. The choice of log-transformation itself is a tradeoff between improved GP fit and potential numerical instability when dealing with values near zero.

**Failure Signatures**
- Poor GP fit when log-transformation assumptions violated (heavy tails, multimodal distributions)
- Numerical instability when objective function contains zero or negative values
- Degraded optimization performance when log-transformed space poorly represents the true optimization landscape
- Overestimation of EI when predictive uncertainty is underestimated by the GP

**3 First Experiments**
1. Benchmark EI computation time: Compare closed-form vs numerical integration across varying problem sizes
2. Transformation sensitivity analysis: Test optimization performance with and without log-transformation on different objective function classes
3. Assumption validation: Check how well the log-normal assumption holds for various real-world objective functions

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes perfect log-transformation applicability, which may fail for non-positive or heavy-tailed objective functions
- Relies on standard EI assumptions of normality and stationarity that may break down in high-dimensional or non-stationary problems
- Lacks empirical validation comparing computational benefits of closed-form solution versus numerical approximations
- Does not address numerical stability issues when dealing with values near zero in the original objective space

## Confidence
- **High confidence**: The mathematical derivation of the closed-form EI formula for log-transformed objectives, as this follows standard probabilistic manipulations of Gaussian distributions
- **Medium confidence**: The claim about improved GP predictive accuracy through log-transformation, as this depends on the specific objective function characteristics
- **Medium confidence**: The practical utility of the derived formula over numerical approximations, since empirical validation is not provided

## Next Checks
1. Conduct empirical benchmarks comparing the closed-form EI computation speed versus numerical integration methods across various objective function landscapes
2. Test the robustness of the log-transformation approach on functions with regions of negative or near-zero values, where log-transformation would be undefined or numerically unstable
3. Evaluate the sensitivity of the derived EI formula to GP hyperparameter estimation errors, particularly when the log-transformed objective deviates from strict log-normality