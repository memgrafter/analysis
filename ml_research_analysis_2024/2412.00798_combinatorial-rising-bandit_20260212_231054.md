---
ver: rpa2
title: Combinatorial Rising Bandit
arxiv_id: '2412.00798'
source_url: https://arxiv.org/abs/2412.00798
tags:
- regret
- reward
- policy
- goal
- crucb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Combinatorial Rising Bandit (CRB) framework addresses regret
  minimization in scenarios where combinatorial actions consist of base arms that
  improve with repeated use, introducing complex dependencies through partially shared
  enhancement. The proposed Combinatorial Rising UCB (CRUCB) algorithm estimates future
  rewards for base arms using a Future-UCB index that combines recent outcomes, predicted
  improvement, and exploration bonuses, then solves a combinatorial optimization to
  select actions.
---

# Combinatorial Rising Bandit

## Quick Facts
- **arXiv ID**: 2412.00798
- **Source URL**: https://arxiv.org/abs/2412.00798
- **Reference count**: 40
- **Primary result**: Introduces CRB framework with CRUCB algorithm achieving near-optimal regret bounds for combinatorial actions with improving base arms

## Executive Summary
This paper introduces the Combinatorial Rising Bandit (CRB) framework for regret minimization in scenarios where combinatorial actions consist of base arms that improve with repeated use. The proposed Combinatorial Rising UCB (CRUCB) algorithm addresses the challenge of partially shared enhancement, where selecting a super arm containing a base arm benefits all other super arms containing that base arm. CRUCB employs a Future-UCB index that estimates future rewards by combining recent outcomes, predicted improvement, and exploration bonuses, then solves a combinatorial optimization to select actions. Theoretical analysis establishes a regret upper bound for CRUCB that nearly matches a derived regret lower bound, demonstrating near-optimality. Empirical evaluation shows CRUCB outperforms existing rising and non-stationary bandit algorithms in both synthetic environments and deep reinforcement learning tasks.

## Method Summary
The CRUCB algorithm operates by estimating future rewards for each base arm using a Future-UCB index that combines recent outcomes, predicted improvement, and exploration bonuses. The algorithm uses a sliding window approach to capture recent performance and extrapolates improvement based on observed slopes. CRUCB then solves a combinatorial optimization problem using these estimated indices to select the optimal super arm. The method handles partially shared enhancement by updating reward estimates for all super arms containing a base arm when that arm is selected. Theoretical analysis establishes regret bounds that scale with the problem's difficulty through the cumulative increment parameter, while empirical evaluation demonstrates superior performance across diverse combinatorial settings.

## Key Results
- CRUCB achieves near-optimal regret bounds that nearly match the derived lower bound
- Outperforms existing rising and non-stationary bandit algorithms in synthetic shortest path planning tasks
- Demonstrates strong performance in deep RL environments including AntMaze navigation
- Maintains effectiveness even when theoretical assumptions about concave outcomes are violated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRUCB achieves near-optimal regret by estimating future rewards of base arms using a Future-UCB index that combines recent outcomes, predicted improvement, and exploration bonus.
- Mechanism: The Future-UCB index uses a sliding window to capture recent performance, extrapolates improvement based on observed slopes, and adds an exploration bonus scaled by uncertainty in non-stationary setting.
- Core assumption: Base arm outcomes are concave and σ²-subgaussian, allowing bounded uncertainty and predictable improvement trends.
- Evidence anchors:
  - [abstract] "CRUCB employs a Future-UCB index that optimistically estimates the future outcome of each base arm by combining its recent mean, slope, and uncertainty term."
  - [section 4] "The index μ̂ᵢ(t) consists of three parts: recent average, predicted upper bound of improvement, and exploration bonus."
- Break condition: If base arm outcomes are non-concave or have unbounded variance, the optimism bias and exploration bonuses become unreliable.

### Mechanism 2
- Claim: CRUCB adapts its regret bound to the difficulty of the problem instance through the cumulative increment Υ(M,q).
- Mechanism: The regret bound scales with Υ(M,q), which quantifies total expected outcome growth. When outcomes saturate quickly (small Υ), regret remains sub-linear; when outcomes grow slowly (large Υ), regret becomes linear.
- Core assumption: The reward function is Lipschitz continuous and maximum super arm size L is known or bounded.
- Evidence anchors:
  - [section 5.1] "Term (i) captures the regret caused by the inherent difficulty of the CRB problem, which is related to the rising nature of expected outcomes and the size of a super arm."
  - [section 5.1] "When outcomes of base arms evolve continuously, i.e., Υµ is large, identifying the optimal super arm becomes significantly more difficult."
- Break condition: If Lipschitz assumption fails or problem contains hidden combinatorial structure not captured by L.

### Mechanism 3
- Claim: CRUCB's sliding window size εNᵢ,t balances bias-variance tradeoff in reward estimation.
- Mechanism: Small window sizes reduce bias by focusing on recent outcomes but increase variance; large windows reduce variance but introduce bias. CRUCB uses εNᵢ,t to adapt window size to number of pulls.
- Core assumption: Sliding window parameter ε is chosen appropriately (0 < ε < 1/2) and base arm outcomes have consistent improvement patterns.
- Evidence anchors:
  - [section 4] "The index μ̂ᵢ(t) consists of three parts: recent average, predicted upper bound of improvement, and exploration bonus."
  - [section 5.1] "CRUCB employs a Future-UCB index that optimistically estimates the future outcome of each base arm by combining its recent mean, slope, and uncertainty term."
- Break condition: If base arm outcomes have abrupt changes or inconsistent patterns, sliding window approach cannot capture true improvement trend.

## Foundational Learning

- Concept: Combinatorial optimization and semi-bandit feedback
  - Why needed here: CRUCB selects super arms by solving combinatorial optimization problem using estimated Future-UCB indices, and receives semi-bandit feedback for each base arm in selected super arm.
  - Quick check question: How does semi-bandit feedback differ from full-bandit feedback, and why is it important for CRUCB's operation?

- Concept: Concave reward functions and Lipschitz continuity
  - Why needed here: CRUCB relies on concave reward functions to ensure predictable improvement trends and Lipschitz continuity to bound regret when comparing estimated and true rewards.
  - Quick check question: What happens to CRUCB's regret bound if the reward function is not Lipschitz continuous?

- Concept: Non-stationary bandit algorithms and sliding window techniques
  - Why needed here: CRUCB uses sliding window approach to handle rising nature of rewards, which is key characteristic of non-stationary bandit problems.
  - Quick check question: How does CRUCB's sliding window approach differ from traditional non-stationary bandit algorithms?

## Architecture Onboarding

- Component map: Future-UCB index estimator -> Combinatorial solver -> Regret analyzer

- Critical path:
  1. Estimate Future-UCB indices for each base arm using recent outcomes and improvement trends
  2. Solve combinatorial optimization problem to select super arm with highest estimated reward
  3. Play selected super arm and observe outcomes
  4. Update Future-UCB indices based on new observations
  5. Calculate and bound the regret

- Design tradeoffs:
  - Window size: Small windows reduce bias but increase variance; large windows reduce variance but introduce bias
  - Exploration bonus: Larger bonuses encourage exploration but may lead to suboptimal exploitation
  - Solver complexity: More complex solvers can find better solutions but increase computational cost

- Failure signatures:
  - High regret: Indicates poor estimation of Future-UCB indices or inefficient combinatorial optimization
  - Oscillating performance: Suggests inconsistent base arm outcomes or inappropriate window size
  - Slow convergence: Indicates insufficient exploration or overly conservative exploration bonus

- First 3 experiments:
  1. Synthetic environment with known outcome functions: Test CRUCB's ability to estimate Future-UCB indices accurately
  2. Deep reinforcement learning environment: Evaluate CRUCB's performance in realistic, complex setting
  3. Combinatorial optimization problem with varying difficulty: Assess CRUCB's adaptability to different problem complexities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CRUCB perform when the combinatorial structure itself evolves dynamically during learning?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, noting that analysis assumes fixed set of base arms and static combinatorial structure, while certain applications like robotic systems with skill discovery may involve evolving action spaces.
- Why unresolved: Theoretical framework and regret bounds are derived under static assumptions, and empirical evaluation focuses on environments with fixed combinatorial structures.
- What evidence would resolve it: Experiments comparing CRUCB against baselines in environments where set of valid super arms changes over time, along with theoretical analysis extending regret bounds to dynamic combinatorial structures.

### Open Question 2
- Question: Can CRUCB be extended to handle non-concave outcome functions that violate Assumption 1, as observed in the AntMaze experiments?
- Basis in paper: [inferred] The paper notes that while AntMaze experiments showed non-concave behavior due to extended zero-reward periods, CRUCB still outperformed baselines, suggesting potential robustness beyond theoretical assumptions.
- Why unresolved: Theoretical analysis relies on concavity for tractable prediction of future rewards, and no alternative theoretical framework is provided for non-concave cases.
- What evidence would resolve it: Theoretical analysis establishing regret bounds for CRUCB with relaxed concavity assumptions, combined with experiments in environments with explicitly non-concave outcome functions.

### Open Question 3
- Question: How does the performance of CRUCB compare to Thompson Sampling-based variants in combinatorial rising bandit settings?
- Basis in paper: [explicit] The paper evaluates against SW-CTS, a sliding-window combinatorial Thompson Sampling baseline, but only in context of stationary non-rising settings, and notes that non-combinatorial algorithms outperformed combinatorial ones in complex task.
- Why unresolved: Evaluation doesn't include rising-bandit Thompson Sampling variant that operates at base arm level with combinatorial optimization, which would be direct analog to CRUCB.
- What evidence would resolve it: Empirical comparison of CRUCB against Thompson Sampling-based algorithm that estimates base arm potentials and solves combinatorial optimization, in both synthetic and RL environments.

## Limitations

- CRUCB assumes concave, σ²-subgaussian base arm outcomes, which may not hold in many real-world scenarios
- Algorithm performance relies on appropriate selection of sliding window parameter ε and combinatorial solver efficiency
- Regret bound's dependence on Υ(M,q) introduces uncertainty when outcomes have complex growth patterns
- Theoretical framework assumes static combinatorial structure, limiting applicability to dynamic environments

## Confidence

- Mechanism 1 (Future-UCB index): **High** - Well-supported by theoretical analysis and empirical evidence
- Mechanism 2 (Regret bound scaling): **Medium** - Theoretical justification is strong, but empirical validation across diverse problem instances is limited
- Mechanism 3 (Sliding window balance): **Medium** - Theoretical foundation is sound, but practical performance may vary depending on base arm outcome patterns

## Next Checks

1. Test CRUCB on synthetic environments with non-concave or non-subgaussian base arm outcomes to assess robustness beyond theoretical assumptions.
2. Evaluate CRUCB's performance with different sliding window parameters ε to determine sensitivity and identify optimal settings for various problem instances.
3. Compare CRUCB's regret bound scaling with Υ(M,q) across a wider range of combinatorial optimization problems to validate theoretical predictions empirically.