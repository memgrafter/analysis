---
ver: rpa2
title: 'EfficientRAG: Efficient Retriever for Multi-Hop Question Answering'
arxiv_id: '2408.04259'
source_url: https://arxiv.org/abs/2408.04259
tags:
- answer
- question
- json
- your
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EfficientRAG introduces an efficient retriever for multi-hop question
  answering that avoids costly LLM calls by using lightweight models to iteratively
  generate new queries and filter irrelevant information. It consists of a Labeler
  & Tagger and a Filter module that identify useful information from retrieved chunks
  and construct follow-up queries.
---

# EfficientRAG: Efficient Retriever for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2408.04259
- Source URL: https://arxiv.org/abs/2408.04259
- Authors: Ziyuan Zhuang; Zhiyang Zhang; Sitao Cheng; Fangkai Yang; Jia Liu; Shujian Huang; Qingwei Lin; Saravan Rajmohan; Dongmei Zhang; Qi Zhang
- Reference count: 20
- Primary result: Achieves 81.84% recall on HotpotQA with only 3-6 chunks retrieved per query

## Executive Summary
EfficientRAG introduces an efficient retriever for multi-hop question answering that avoids costly LLM calls by using lightweight models to iteratively generate new queries and filter irrelevant information. The system consists of a Labeler & Tagger and a Filter module that identify useful information from retrieved chunks and construct follow-up queries. Experimental results show EfficientRAG achieves high recall (81.84% on HotpotQA, 84.08% on 2WikiMQA) while retrieving only 3-6 chunks per query, outperforming iterative RAG baselines. The approach maintains comparable accuracy to LLM-based methods while improving inference speed by 60-80% and demonstrating strong out-of-domain generalization.

## Method Summary
EfficientRAG is a multi-hop question answering system that uses lightweight models instead of LLM calls for iterative retrieval. It employs a Labeler & Tagger to identify useful tokens and tag chunks as Continue or Terminate, while a Filter module constructs new queries based on labeled tokens. The system is trained on synthetic data generated by Llama-3-70B-Instruct and fine-tuned using DeBERTa-v3-large models. EfficientRAG retrieves only 3-6 chunks per query while achieving high recall, and can adapt to different scenarios without further downstream training.

## Key Results
- Achieves 81.84% recall on HotpotQA and 84.08% recall on 2WikiMQA with only 3-6 chunks retrieved per query
- Improves inference speed by 60-80% compared to iterative LLM-based approaches
- Demonstrates strong out-of-domain generalization across multiple datasets
- Maintains comparable accuracy to LLM-based methods while using lightweight models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EfficientRAG avoids LLM calls by using lightweight models for query generation and filtering
- Mechanism: The system uses a Labeler & Tagger and a Filter module to iteratively generate new queries and filter irrelevant information
- Core assumption: Small models can effectively identify relations and associated entities from retrieved information
- Evidence anchors:
  - [abstract] "EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information"
  - [section 3.1] "Thus, we propose EfficientRAG consists of a Labeler and a Filter to iteratively generate new queries for retrieval and in the meanwhile keep the most relevant retrieved information"
  - [corpus] Weak evidence - no direct mentions of similar lightweight model approaches in related papers
- Break condition: If the lightweight models cannot accurately identify useful information or generate relevant follow-up queries, the system's performance would degrade significantly

### Mechanism 2
- Claim: EfficientRAG maintains high recall while retrieving fewer chunks
- Mechanism: By filtering out irrelevant information and focusing on useful tokens, the system can retrieve only 3-6 chunks per query while achieving comparable or better recall
- Core assumption: The number of relation types in multi-hop questions is limited compared to the number of entities
- Evidence anchors:
  - [abstract] "Experimental results show EfficientRAG achieves high recall (81.84% on HotpotQA, 84.08% on 2WikiMQA) while retrieving only 3-6 chunks per query"
  - [section 5.1] "EfficientRAG achieves notably high recall scores on HotpotQA and 2WikiMQA datasets, with recall values of 81.84 and 84.08, respectively. These results are impressive considering the minimal number of chunks retrieved"
  - [corpus] No direct evidence - related papers focus on different optimization approaches
- Break condition: If the assumption about limited relation types is incorrect for a given dataset, the filtering mechanism may discard too much relevant information

### Mechanism 3
- Claim: EfficientRAG demonstrates strong out-of-domain generalization
- Mechanism: The system can adapt to different scenarios without further downstream training, as shown by its performance on out-of-domain datasets
- Core assumption: The training data synthesis process creates generalizable patterns that apply across different datasets
- Evidence anchors:
  - [abstract] "demonstrating strong out-of-domain generalization"
  - [section 5.4] "Table 6 shows that our model adapts well to different datasets, and even surpasses the model trained on the original data in some cases"
  - [corpus] Weak evidence - no similar out-of-domain generalization claims in related papers
- Break condition: If the datasets have fundamentally different structures or the relations between entities vary significantly, the generalization may fail

## Foundational Learning

- Concept: Multi-hop question answering
  - Why needed here: EfficientRAG is specifically designed to handle multi-hop questions that require information from multiple sources
  - Quick check question: What is the difference between a one-hop and a multi-hop question? (Answer: A one-hop question can be answered with a single retrieval, while a multi-hop question requires information from multiple sources)

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: EfficientRAG is a type of RAG system that combines retrieval with generation, but optimizes the retrieval process
  - Quick check question: How does RAG differ from standard generation models? (Answer: RAG retrieves relevant information from external sources before generating a response, while standard models rely only on their training data)

- Concept: Token-level classification
  - Why needed here: The Labeler and Filter modules in EfficientRAG use token-level classification to identify useful information and generate new queries
  - Quick check question: What is the difference between sequence-level and token-level classification? (Answer: Sequence-level classification assigns a label to the entire sequence, while token-level classification assigns labels to individual tokens within the sequence)

## Architecture Onboarding

- Component map: Retriever → Labeler & Tagger → Filter → (Loop) → LLM Generator → Answer
- Critical path: Query → Retriever → Labeler & Tagger → Filter → (Loop) → LLM Generator → Answer
- Design tradeoffs:
  - Fewer chunks retrieved (3-6) vs. higher recall vs. potential information loss
  - Lightweight models vs. accuracy vs. inference speed
  - Out-of-domain generalization vs. domain-specific optimization
- Failure signatures:
  - Low recall indicates the filtering is too aggressive
  - Slow performance suggests the lightweight models are inefficient
  - Poor out-of-domain performance indicates overfitting to training data
- First 3 experiments:
  1. Test baseline recall with 10 chunks retrieval vs. EfficientRAG's 3-6 chunks
  2. Measure inference time difference between EfficientRAG and iterative LLM-based approaches
  3. Evaluate out-of-domain performance by training on HotpotQA and testing on 2WikiMQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EfficientRAG's performance be further improved by using larger LLMs as the final QnA generator?
- Basis in paper: [explicit] The paper states "The EfficientRAG framework can theoretically adapt to other models, but we opt not to implement a larger LLM as the final QnA reasoner due to time and resource limits."
- Why unresolved: The paper did not implement or test larger LLMs due to practical constraints, leaving open the question of whether performance could be enhanced with more powerful generators.
- What evidence would resolve it: Experimental results comparing EfficientRAG's performance using different sizes of LLMs (e.g., GPT-4, Claude 3) as the final QnA generator would provide clear evidence of potential performance gains.

### Open Question 2
- Question: How would EfficientRAG perform on in-domain multi-hop question-answering datasets compared to open-domain ones?
- Basis in paper: [inferred] The paper mentions "We analyze our method mainly on open-domain datasets, as it is hard to identify multi-hop question-answering datasets in in-domain settings."
- Why unresolved: The paper focused on open-domain datasets and acknowledged the difficulty of finding in-domain multi-hop datasets, leaving the performance in domain-specific contexts unexplored.
- What evidence would resolve it: Evaluating EfficientRAG on various in-domain multi-hop QA datasets (e.g., biomedical, legal, financial) would demonstrate its effectiveness in specialized domains.

### Open Question 3
- Question: What is the impact of different chunk sizes on EfficientRAG's retrieval performance and efficiency?
- Basis in paper: [inferred] While the paper mentions "we utilize Contriever-MSMARCO as the retriever," it does not discuss the effect of varying chunk sizes on performance.
- Why unresolved: The paper does not explore how different chunk sizes might affect recall, precision, or computational efficiency, which could be crucial for optimizing the system.
- What evidence would resolve it: Systematic experiments varying chunk sizes (e.g., 100 words, 500 words, full documents) and measuring their impact on retrieval metrics and computational resources would clarify the optimal chunk size for EfficientRAG.

## Limitations

- Reliance on synthetic training data quality generated by large language models
- Assumption that relation types in multi-hop questions are limited compared to entities may not hold for all domains
- Focus on recall and inference speed metrics with less emphasis on final answer accuracy

## Confidence

**High Confidence Claims:**
- EfficientRAG achieves high recall (81.84% on HotpotQA, 84.08% on 2WikiMQA) while retrieving only 3-6 chunks per query
- The system demonstrates improved inference speed (60-80% faster than baselines)
- EfficientRAG shows strong out-of-domain generalization across multiple datasets

**Medium Confidence Claims:**
- The lightweight model approach can effectively replace iterative LLM calls without significant accuracy loss
- The synthetic data generation process adequately captures multi-hop question complexity
- The filtering mechanism maintains sufficient information for accurate final answer generation

**Low Confidence Claims:**
- The system's performance with generators smaller than GPT-3.5 (mentioned as suboptimal but without specific metrics)
- The scalability of the approach to significantly larger or more complex knowledge bases
- The robustness of the system to adversarial or highly ambiguous queries

## Next Checks

1. **Synthetic Data Quality Validation**: Conduct a human evaluation of a sample of synthetic training data to assess whether the generated multi-hop questions, token labels, and next-hop questions accurately represent real query patterns and information needs. Compare this against human-annotated examples from the original datasets.

2. **Edge Case Robustness Testing**: Design a test suite of edge cases including highly ambiguous queries, questions requiring temporal reasoning, and queries spanning vastly different domains. Measure how the filtering mechanism handles these cases and whether information loss occurs in scenarios with complex relational patterns.

3. **Scaling Performance Analysis**: Evaluate the system's performance and efficiency as the knowledge base size scales from the current datasets to orders of magnitude larger (100x-1000x). Measure recall degradation, inference time scaling, and memory requirements to assess practical deployment limitations.