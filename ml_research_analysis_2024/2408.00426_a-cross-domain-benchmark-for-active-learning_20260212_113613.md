---
ver: rpa2
title: A Cross-Domain Benchmark for Active Learning
arxiv_id: '2408.00426'
source_url: https://arxiv.org/abs/2408.00426
tags:
- dataset
- methods
- learning
- datasets
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CDALBench, the first cross-domain benchmark
  for active learning that spans computer vision, natural language processing, and
  tabular data domains. The benchmark addresses the challenge of evaluating active
  learning methods across different data types by providing a standardized evaluation
  framework with 50 repetitions per experiment to ensure statistical significance.
---

# A Cross-Domain Benchmark for Active Learning

## Quick Facts
- arXiv ID: 2408.00426
- Source URL: https://arxiv.org/abs/2408.00426
- Reference count: 40
- Primary result: No single active learning method dominates across all domains

## Executive Summary
This paper introduces CDALBench, the first benchmark designed to evaluate active learning methods across computer vision, natural language processing, and tabular data domains. The benchmark addresses the challenge of cross-domain evaluation by providing standardized protocols with 50 repetitions per experiment to ensure statistical significance. A key contribution is an efficient greedy oracle algorithm that approximates optimal active learning performance while avoiding prohibitive computational costs. Experimental results reveal that active learning performance is strongly domain-dependent, with margin sampling excelling in text and tabular data while least confident sampling performs best for images.

## Method Summary
The benchmark spans nine datasets (14 with encoded versions) across three domains, using domain-specific models: ResNet18 for images, BiLSTM for text, and MLP for tabular data. Active learning methods include margin sampling, BALD, BADGE, Coreset, TypiClust, CoreGCN, DSA/LSA, Galaxy, and entropy/least confident sampling. The evaluation protocol uses 50 repetitions per experiment to ensure statistical significance, with normalized area under accuracy curve (AUC) as the primary metric. A novel greedy oracle algorithm provides an upper bound on performance by iteratively testing small subsets of unlabeled points. The benchmark code is publicly available for further research.

## Key Results
- No single active learning method dominates across all domains - performance is strongly dataset- and domain-dependent
- Cross-domain evaluation reveals that results from image-only benchmarks cannot be generalized to other domains
- Two synthetic datasets (Honeypot and Diverging Sine) expose fundamental weaknesses in common active learning approaches
- High repetition count (50 runs) is essential for drawing valid conclusions about active learning method performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's cross-domain character is crucial for sophisticated evaluation of active learning methods.
- Mechanism: By including datasets from computer vision, natural language processing, and tabular domains, the benchmark exposes domain-specific strengths and weaknesses of different active learning methods that would remain hidden in single-domain evaluations.
- Core assumption: Active learning methods that work well in one domain will not necessarily generalize to other domains.
- Evidence anchors: [abstract] "We show, that both the cross-domain character and a large amount of repetitions are crucial for sophisticated evaluation of AL research." [section] "Our experimental evaluation shows that there exists no clear SOTA method for AL. The superiority of methods is strongly dataset- and domain-dependent"

### Mechanism 2
- Claim: High repetition count (50 runs) is essential for drawing valid conclusions about active learning method performance.
- Mechanism: Multiple runs reduce variance and reveal the true performance distribution of methods, preventing misleading conclusions from single or few experiments.
- Core assumption: Active learning method performance is sensitive to random initialization and data sampling.
- Evidence anchors: [abstract] "Furthermore, by providing an efficient, greedy oracle, CDALBench can be evaluated with 50 runs for each experiment." [section] "With only conducting three runs as often done in the literature, the superiority of specific methods can strongly vary with the specific runs."

### Mechanism 3
- Claim: The greedy oracle algorithm efficiently approximates optimal active learning performance while avoiding prohibitive computational costs.
- Mechanism: By iteratively testing small subsets of unlabeled points and selecting the one that maximizes classifier performance, the oracle provides an upper bound for comparison without exhaustive search.
- Core assumption: Small samples of unlabeled points can reliably indicate which point will yield the greatest performance improvement.
- Evidence anchors: [abstract] "A key contribution is an efficient greedy oracle algorithm that approximates optimal active learning performance while avoiding prohibitive computational costs." [section] "To enable the computation of an oracle performance for a protocol with large amounts of restarts, we propose a greedy oracle algorithm which uses only a small amount of search steps to estimate the optimal solution."

## Foundational Learning

- Concept: Active Learning fundamentals
  - Why needed here: Understanding the core problem of selecting informative samples for labeling is essential for grasping why cross-domain evaluation matters
  - Quick check question: What is the primary goal of active learning in supervised learning tasks?

- Concept: Statistical significance and variance
  - Why needed here: The paper emphasizes the importance of multiple runs for drawing valid conclusions, requiring understanding of how variance affects experimental results
  - Quick check question: Why might three experimental runs be insufficient to determine if one method truly outperforms another?

- Concept: Benchmark design principles
  - Why needed here: The paper introduces a new benchmark framework, requiring understanding of what makes benchmarks effective for method comparison
  - Quick check question: What are the key components that make a benchmark useful for comparing active learning methods across domains?

## Architecture Onboarding

- Component map: Dataset preprocessing pipelines -> Domain-specific model architectures (ResNet18/BiLSTM/MLP) -> Embedding models for semi-supervised learning -> Active learning method implementations -> Evaluation protocols with 50 repetitions -> Oracle computation modules

- Critical path: For each experiment, the system must: (1) initialize with seed samples per class, (2) run active learning iterations with the selected method, (3) train/update the model at each iteration, (4) evaluate on test set, (5) repeat 50 times, and (6) aggregate results across runs and query sizes.

- Design tradeoffs: The benchmark prioritizes statistical validity (50 repetitions) over computational efficiency, uses smaller models rather than state-of-the-art for faster training, and includes synthetic datasets to expose method weaknesses rather than focusing solely on real-world performance.

- Failure signatures: If results show no significant differences between methods, it may indicate insufficient variance in the datasets; if results vary drastically between runs, it may indicate poor hyperparameter tuning or unstable training procedures.

- First 3 experiments:
  1. Run margin sampling on Splice dataset with 1 query size and 3 repetitions to verify basic functionality
  2. Compare random sampling vs. margin sampling on DNA dataset with query size 5 to validate method differences
  3. Run the oracle algorithm on USPS dataset with small budget to verify upper bound computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the poor performance of active learning methods on semi-supervised tabular data represent a fundamental limitation of these approaches for this data type, or can it be resolved through better method design or parameter tuning?
- Basis in paper: [explicit] The paper notes that semi-supervised tabular data shows "highly irregular behavior" with random sampling performing second-best and the oracle method underperforming other approaches, stating "Both the reasons, for the sub-random performance of most methods, and the bad performance of our oracle are currently unknown and require further research."
- Why unresolved: The paper explicitly acknowledges this as an open research question without providing explanations or solutions for the observed phenomena.
- What evidence would resolve it: Systematic experimentation varying model architectures, embedding techniques, and active learning algorithms specifically on semi-supervised tabular data to identify whether performance can be improved beyond random sampling, or theoretical analysis explaining why these methods fail in this setting.

### Open Question 2
- Question: Are the observed differences between domains (image vs. text vs. tabular) caused primarily by inherent characteristics of the data types, or by the specific model architectures commonly associated with each domain?
- Basis in paper: [inferred] The paper observes that "the superiority of specific methods varies over the different domains" and that "the image domain works fundamentally different than the tabular and text domain," but notes this could be influenced by "the type of model that is common to those domains" and suggests testing "different model archetypes per domain" as future work.
- Why unresolved: The benchmark uses domain-specific models (ResNet18 for images, BiLSTM for text, MLP for tabular) making it impossible to separate domain effects from model architecture effects with the current experimental design.
- What evidence would resolve it: Cross-domain experiments where the same model architecture is applied across all data types, or systematic ablation studies varying model architectures within each domain while keeping data constant.

### Open Question 3
- Question: What is the optimal number of repetitions needed to reliably evaluate active learning methods, and does this number vary by dataset complexity or domain?
- Basis in paper: [explicit] The paper demonstrates that "with only conducting three runs as often done in the literature, the superiority of specific methods can strongly vary with the specific runs" and shows that even with 5 repetitions "the variance between the samples is extremely high," while concluding that "50 repetitions" provides more stable results, but acknowledges this may not be universally optimal.
- Why unresolved: While the paper shows 3 repetitions are insufficient and 50 provide stability, it doesn't systematically determine whether 50 is optimal or whether different datasets/domains require different numbers of repetitions for reliable evaluation.
- What evidence would resolve it: Statistical analysis correlating dataset characteristics (size, complexity, domain) with the number of repetitions needed to achieve stable method rankings, potentially establishing guidelines for appropriate repetition counts based on dataset properties.

## Limitations

- The benchmark uses domain-specific models that may conflate domain effects with model architecture effects
- The greedy oracle provides an approximation of optimal performance but its approximation quality is not rigorously validated against exhaustive search
- The paper does not establish whether 50 repetitions is the optimal number for all datasets or if this varies by dataset complexity

## Confidence

- Cross-domain evaluation necessity: High confidence - the experimental results clearly show performance variations across domains
- 50-repetition requirement: Medium confidence - while the paper demonstrates importance, the specific number 50 versus alternative values is not rigorously justified
- Greedy oracle approximation quality: Medium confidence - the paper claims efficiency but provides limited analysis of approximation error relative to true optimal performance
- No single SOTA method: High confidence - experimental results across multiple datasets consistently support this conclusion

## Next Checks

1. Test the benchmark with additional datasets from underrepresented domains (e.g., time series, graph data) to validate the cross-domain framework's generalizability
2. Conduct ablation studies varying the number of repetitions (10, 25, 50, 100) to determine the minimum required for statistically significant conclusions
3. Compare the greedy oracle performance against a smaller subset of datasets using exhaustive search to quantify approximation error and establish confidence intervals for the upper bound estimates