---
ver: rpa2
title: Spoken Grammar Assessment Using LLM
arxiv_id: '2410.01579'
source_url: https://arxiv.org/abs/2410.01579
tags:
- grammar
- assessment
- language
- spoken
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end Spoken Language Assessment (SLA)
  system that can evaluate grammar from spoken utterances, a capability traditionally
  handled by separate Written Language Assessment (WLA) systems. The key innovation
  is using a Large Language Model (LLM) to generate unique paragraphs for assessment,
  making the system unteachable, and a custom-built language model (CLM) integrated
  with a hybrid ASR to accurately transcribe spoken grammar variations.
---

# Spoken Grammar Assessment Using LLM

## Quick Facts
- arXiv ID: 2410.01579
- Source URL: https://arxiv.org/abs/2410.01579
- Reference count: 25
- This paper proposes an end-to-end SLA system using LLM-generated paragraphs and custom CLM for accurate grammar assessment from spoken utterances.

## Executive Summary
This paper presents a novel Spoken Language Assessment (SLA) system that evaluates grammar from spoken utterances using a Large Language Model (LLM) and a custom-built Language Model (CLM) integrated with hybrid Automatic Speech Recognition (ASR). The system addresses the limitations of traditional Written Language Assessment (WLA) approaches by creating unique, unteachable assessment instances and improving ASR transcription accuracy for grammatically incorrect sentences. Experiments demonstrate significant improvements in both ASR performance and grammar assessment accuracy compared to state-of-the-art approaches.

## Method Summary
The proposed SLA system uses an LLM to generate unique assessment paragraphs with grammar test words, then employs a custom-built CLM integrated with a hybrid ASR system to accurately transcribe spoken words, including grammatical errors. The grammar scoring module compares the ASR output to a gold standard paragraph using set operations on a subset of grammar words. The system is evaluated on an in-house dataset of 17 students, showing substantial improvements in both ASR word recognition accuracy and grammar assessment performance.

## Key Results
- Custom CLM-based ASR achieves 84.7% word accuracy versus 46% for state-of-the-art Whisper
- Grammar assessment errors reduced from 20 to 3 on in-house dataset
- System successfully handles grammatically incorrect spoken sentences that standard ASR systems struggle with

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Custom CLM improves ASR transcription accuracy for grammatically incorrect sentences
- Mechanism: CLM trained on all grammatical variations allows ASR to recognize spoken words without correcting errors; shallow fusion combines acoustic and LM scores
- Core assumption: CLM can be effectively trained to include all possible grammatical variations
- Evidence anchors: Abstract shows 84.7% vs 46% accuracy; section explains CLM's role in recognizing grammatically incorrect sentences
- Break condition: CLM cannot adequately represent all grammatical variations or ASR cannot integrate CLM effectively

### Mechanism 2
- Claim: LLM-generated paragraphs make assessment unteachable
- Mechanism: LLM creates unique paragraphs with grammar variations marked by <grammar> tags for each student, preventing memorization
- Core assumption: LLM can consistently generate diverse, grammatically varied paragraphs
- Evidence anchors: Abstract mentions unteachable system; section discusses LLM's role in generating non-repetitive paragraphs
- Break condition: LLM generates repetitive patterns or students can reverse-engineer generation process

### Mechanism 3
- Claim: Grammar scoring is robust to ASR errors by focusing on grammar words subset
- Mechanism: Scoring system compares only grammar words (Gw) using set operations, filtering out ASR errors on non-Gw words
- Core assumption: Gw subset is representative of overall grammar proficiency and set operations effectively filter errors
- Evidence anchors: Abstract shows error reduction from 20 to 3; section explains how scoring is designed to be unaffected by non-Gw errors
- Break condition: Gw subset is too small/non-representative or ASR errors systematically affect Gw recognition

## Foundational Learning

- Language Model (LM) integration with ASR
  - Why needed here: Understanding how LMs improve ASR accuracy for grammatically incorrect sentences
  - Quick check question: How does a language model influence ASR output during decoding?

- Automatic Speech Recognition (ASR) evaluation metrics
  - Why needed here: Measuring ASR performance using metrics like Word Error Rate (WER)
  - Quick check question: What is Word Error Rate (WER) and how is it calculated in ASR systems?

- Large Language Model (LLM) prompting techniques
  - Why needed here: Effectively prompting LLMs to generate diverse, grammatically varied paragraphs
  - Quick check question: What is 1-shot learning prompting and how can it generate specific text types with LLMs?

## Architecture Onboarding

- Component map: LLM paragraph generator -> Custom Language Model trainer -> Hybrid ASR system -> Grammar scoring module -> Web interface
- Critical path: LLM generation → CLM training → ASR transcription → Grammar scoring
- Design tradeoffs: CLM complexity vs. ASR performance; grammar word selection size; LLM prompt specificity vs. paragraph diversity
- Failure signatures: High WER indicates CLM-ASR integration issues; inconsistent scores suggest Gw selection problems; repetitive patterns indicate LLM generation issues
- First 3 experiments:
  1. Test CLM-ASR performance on grammatically varied sentences to validate custom LM approach
  2. Evaluate consistency of LLM-generated paragraphs by analyzing multiple generations of same prompt
  3. Assess grammar scoring robustness by introducing controlled ASR errors and measuring impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SLA system's performance generalize to different proficiency levels of learners beyond the in-house dataset of 17 students?
- Basis in paper: [explicit] Paper uses 17-student in-house dataset but doesn't discuss testing across proficiency levels
- Why unresolved: Limited sample size and lack of proficiency diversity means effectiveness for various learner stages remains unknown
- What evidence would resolve it: Testing with larger, more diverse group across multiple proficiency levels including beginners, intermediate, and advanced speakers

### Open Question 2
- Question: How does the custom-built LM perform compared to other state-of-the-art ASR systems with domain-specific fine-tuning, beyond just comparing to Whisper?
- Basis in paper: [inferred] Paper compares to Whisper but doesn't explore other ASR systems with domain-specific adaptation
- Why unresolved: Limited comparison leaves uncertainty about whether performance advantage is unique or if other ASR systems could achieve similar results with fine-tuning
- What evidence would resolve it: Benchmarking against other state-of-the-art ASR systems (e.g., Google Speech-to-Text, Microsoft Azure Speech) with domain-specific fine-tuning

### Open Question 3
- Question: What is the impact of spontaneous speech characteristics (e.g., disfluencies, repetitions) on ASR performance and subsequent grammar assessment accuracy when using the custom-built LM?
- Basis in paper: [explicit] Paper notes design ensures "read" speech but doesn't address spontaneous speech scenarios
- Why unresolved: System's performance in real-world spontaneous speech scenarios is not evaluated
- What evidence would resolve it: Testing with spontaneous speech samples containing disfluencies, repetitions, and comparing accuracy to read speech

## Limitations

- Limited evaluation dataset size (17 students) restricts generalizability of results
- ASR system comparison only against Whisper, not exploring other state-of-the-art systems with domain adaptation
- System design assumes "read" speech format, not evaluating performance on spontaneous speech with natural disfluencies

## Confidence

- High Confidence: Core concept of using CLM to improve ASR for grammatically incorrect sentences (84.7% vs 46% accuracy)
- Medium Confidence: Unteachable assessment claim based on LLM generation diversity (lacks quantitative evidence)
- Low Confidence: Grammar scoring robustness to ASR errors (small error reduction from 20 to 3 suggests sensitivity)

## Next Checks

1. Replicate CLM-ASR performance on separate dataset with controlled grammatical variations to verify 84.7% accuracy claim
2. Generate multiple instances of assessment paragraphs and conduct statistical analysis to quantify diversity and uniqueness
3. Systematically introduce varying levels of ASR errors into transcripts and measure corresponding impact on grammar scores