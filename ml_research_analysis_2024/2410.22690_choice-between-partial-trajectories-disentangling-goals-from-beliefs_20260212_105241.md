---
ver: rpa2
title: 'Choice Between Partial Trajectories: Disentangling Goals from Beliefs'
arxiv_id: '2410.22690'
source_url: https://arxiv.org/abs/2410.22690
tags:
- human
- reward
- function
- return
- partial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning human preferences
  from choices between partial trajectories in reinforcement learning. The key challenge
  is disentangling human goals from beliefs about the environment.
---

# Choice Between Partial Trajectories: Disentangling Goals from Beliefs

## Quick Facts
- **arXiv ID**: 2410.22690
- **Source URL**: https://arxiv.org/abs/2410.22690
- **Reference count**: 19
- **Primary result**: A bootstrapped return model disentangles human goals from beliefs in preference learning from partial trajectory choices.

## Executive Summary
This paper addresses the challenge of learning human preferences from choices between partial trajectories in reinforcement learning, specifically tackling how to disentangle human goals from their beliefs about the environment. The authors propose a bootstrapped return model that adds an estimate of future return to the partial return, allowing the model to account for how human beliefs influence choices. They prove an Alignment Theorem showing that under certain axioms, this model can recover a reward function expressing human preferences regardless of belief accuracy, contrasting with previous approaches that either ignore beliefs or require correct beliefs.

## Method Summary
The method uses a bootstrapped return choice model that combines partial return with a value function estimate to capture how human beliefs affect trajectory evaluation. A logit choice model converts these bootstrapped returns into choice probabilities using the logistic function. The learning algorithm employs SGD to minimize negative log-likelihood, recovering both reward and value function parameters from stochastic choice data. The approach is designed to work even when human choices are based on erroneous beliefs about the environment.

## Key Results
- The bootstrapped return model generalizes partial return choices, ensuring alignment recovery even when humans choose based on partial returns
- The Alignment Theorem proves that bootstrapped return can disentangle goals from beliefs under two specific axioms
- Computational experiments demonstrate the model can recover correct reward functions when humans make choices based on erroneous beliefs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bootstrapped return disentangles human goals from beliefs by adding a value estimate to the partial return.
- **Mechanism**: The bootstrapped return model (partial return + value estimate) captures how human beliefs about future rewards influence choices, while the reward function itself remains goal-oriented. This separation allows learning the reward function without requiring correct beliefs.
- **Core assumption**: Human preferences between partial trajectories can be expressed through bootstrapped returns.
- **Evidence anchors**:
  - [abstract] "Benefits of the bootstrapped return model stem from its treatment of human beliefs. Unlike partial return, choices based on bootstrapped return reflect human beliefs about the environment."
  - [section] "The bootstrapped return adds to the partial return an estimate of expected future reward, expressed by a value function. In the reinforcement learning literature, what we refer to as the bootstrapped return is often called the N-step return."
  - [corpus] Found 25 related papers - weak correlation with stated mechanism.
- **Break condition**: If the value estimate V is not well-defined or does not capture human beliefs accurately, the disentanglement fails.

### Mechanism 2
- **Claim**: Learning via bootstrapped return recovers an aligned reward function even when choices are based on partial return.
- **Mechanism**: Since bootstrapped return with V=0 everywhere simplifies to partial return, the bootstrapped return model generalizes the partial return model. Therefore, learning via bootstrapped return can recover the same reward function that would be learned from partial return choices.
- **Core assumption**: The bootstrapped return model is a strict generalization of the partial return model.
- **Evidence anchors**:
  - [abstract] "The bootstrapped return model also affords greater robustness to choice behavior. Even if human choices are based on partial return, learning via a bootstrapped return model recovers an aligned reward function."
  - [section] "Thus, the bootstrapped return choice model is a strict generalization of the partial return model. Therefore, if the human makes choices based on partial return, a learning model based on bootstrapped return will recover a reward function that expresses ⪰."
  - [corpus] No direct corpus evidence for this generalization property.
- **Break condition**: If the learning algorithm cannot properly handle the generalization from partial to bootstrapped return, alignment may fail.

### Mechanism 3
- **Claim**: The Alignment Theorem proves that bootstrapped return can disentangle goals from beliefs for a general class of preferences.
- **Mechanism**: Under two axioms (reward representation and alignment), the theorem establishes that preferences between partial trajectories determine a reward function that expresses preferences between infinite trajectory lotteries, regardless of human beliefs.
- **Core assumption**: The preference relations satisfy Axioms 1 and 2.
- **Evidence anchors**:
  - [abstract] "To motivate the bootstrapped return model, we formulate axioms and prove an Alignment Theorem. This result formalizes how, for a general class of human preferences, such models are able to disentangle goals from beliefs."
  - [section] "Theorem 1. (alignment) Suppose that (⪰, ⪰∂) satisfies Axioms 1 and 2. 1. There exists a pair (r, V ) that expresses ⪰∂. 2. If (r, V ) expresses ⪰∂ then r expresses ⪰."
  - [corpus] No corpus evidence found for the Alignment Theorem itself.
- **Break condition**: If human preferences violate the axioms (e.g., preferences not expressible by a reward function), the theorem's guarantees do not apply.

## Foundational Learning

- **Concept**: Bootstrapped return as N-step return
  - Why needed here: Understanding this terminology helps connect to existing reinforcement learning literature and clarifies how the model estimates future rewards.
  - Quick check question: What is the relationship between bootstrapped return and N-step return in standard RL terminology?

- **Concept**: Choice models (partial return, cumulative advantage, bootstrapped return)
  - Why needed here: Different choice models make different assumptions about how human beliefs influence choices, which affects the ability to recover aligned reward functions.
  - Quick check question: How does each choice model treat the influence of human beliefs on trajectory evaluation?

- **Concept**: Axiomatic characterization of preferences
  - Why needed here: The Alignment Theorem relies on specific axioms to prove that bootstrapped return can disentangle goals from beliefs.
  - Quick check question: What are the two axioms required for the Alignment Theorem, and what do they guarantee?

## Architecture Onboarding

- **Component map**: Choice data collection → Bootstrapped return model (r, V) → Logit choice model → Negative log-likelihood optimization → Reward function recovery
- **Critical path**: Data collection → Model parameterization → Choice probability computation → Likelihood optimization → Reward function recovery
- **Design tradeoffs**: Tabular representation of r and V requires O(|S×A| + |S|) parameters but is practical only for small state spaces; neural network representations scale better but may require more data and careful architecture design
- **Failure signatures**: If the learned reward function does not align with human preferences, possible causes include insufficient data, incorrect value function estimation, or violations of the model's assumptions about choice behavior
- **First 3 experiments**:
  1. Implement the tabular bootstrapped return model on a simple grid environment with known reward function and test recovery accuracy with varying amounts of choice data
  2. Compare learning speed when humans choose between trajectories with shared vs. different terminal states, as the paper suggests this affects learning efficiency
  3. Test robustness by having humans make choices based on partial return while the agent learns via bootstrapped return, verifying alignment is maintained

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the bootstrapped return model perform in high-dimensional state spaces where value function approximation is necessary?
- **Basis in paper**: [inferred] The paper demonstrates the bootstrapped return model in tabular settings but does not address scalability to high-dimensional spaces requiring function approximation.
- **Why unresolved**: The paper focuses on tabular representations for simplicity and clarity of the main theoretical contributions, but does not explore neural network-based implementations or the challenges of high-dimensional spaces.
- **What evidence would resolve it**: Experiments comparing bootstrapped return learning with partial return and cumulative advantage in high-dimensional environments (e.g., Atari games, continuous control tasks) using neural network function approximators.

### Open Question 2
- **Question**: What is the impact of human belief uncertainty on the learned reward function when using the bootstrapped return model?
- **Basis in paper**: [inferred] The paper assumes humans have fixed beliefs about the environment but does not explore scenarios where humans have uncertainty or distributions over beliefs.
- **Why unresolved**: The theoretical analysis and experiments focus on deterministic belief structures, leaving the question of how belief uncertainty affects learning unaddressed.
- **What evidence would resolve it**: Empirical studies comparing reward learning performance when humans have point estimates versus distributions over transition probabilities, measuring alignment with true preferences.

### Open Question 3
- **Question**: How sensitive is the bootstrapped return model to the choice of value function parameterization and initialization?
- **Basis in paper**: [explicit] The paper mentions that value functions can be parameterized by vectors but does not investigate how different parameterizations or initializations affect learning outcomes.
- **Why unresolved**: While the paper establishes theoretical properties of the bootstrapped return model, it does not explore practical considerations around function approximation choices.
- **What evidence would resolve it**: Systematic experiments varying neural network architectures, initialization schemes, and regularization methods for value function learning, measuring impact on reward function recovery accuracy.

## Limitations
- Theoretical guarantees rely on specific axioms that may not hold for all human preference structures
- Computational experiments lack extensive detail, making real-world applicability assessment difficult
- Focus on tabular representations limits scalability to complex, high-dimensional environments

## Confidence
- **High confidence**: The core mechanism that bootstrapped return can disentangle goals from beliefs is well-supported by the Alignment Theorem and the theoretical framework
- **Medium confidence**: The practical effectiveness of the learning algorithm in recovering reward functions from stochastic choice data is demonstrated in computational experiments, but validation extent is limited
- **Low confidence**: Claims about robustness to erroneous human beliefs are theoretically justified but lack extensive empirical validation across diverse preference structures

## Next Checks
1. **Empirical validation of the Alignment Theorem**: Test whether the bootstrapped return model consistently recovers aligned reward functions across a range of synthetic environments with varying degrees of belief error, measuring alignment quality as a function of belief error magnitude
2. **Scalability testing**: Implement parameterized neural network representations of reward and value functions and evaluate learning performance on environments with larger state spaces, comparing against tabular baselines to identify performance bottlenecks
3. **Cross-preference generalization**: Test the bootstrapped return model's ability to recover aligned reward functions when human preferences violate the stated axioms, characterizing the extent to which the theoretical guarantees break down in practice