---
ver: rpa2
title: 'Causal Generative Explainers using Counterfactual Inference: A Case Study
  on the Morpho-MNIST Dataset'
arxiv_id: '2401.11394'
source_url: https://arxiv.org/abs/2401.11394
tags:
- counterfactual
- causal
- generative
- attribute
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces methods for explaining image classifiers
  using causal generative models that perform counterfactual inference. The authors
  propose three approaches: analyzing pixel-based explanations across varying attribute
  levels using SHAP and contrastive explainers, ranking human-interpretable attribute
  importances through Monte Carlo sampling, and generating counterfactual explanations
  via both gradient-based and model-agnostic attribute space searches.'
---

# Causal Generative Explainers using Counterfactual Inference: A Case Study on the Morpho-MNIST Dataset

## Quick Facts
- arXiv ID: 2401.11394
- Source URL: https://arxiv.org/abs/2401.11394
- Reference count: 5
- One-line primary result: Proposed methods generate more interpretable counterfactual explanations than OmnixAI, with lower IM1 scores (0.65-1.24 vs 1.54-2.89) and higher oracle scores (0.75-0.79 vs 0.56-0.64) on Morpho-MNIST

## Executive Summary
This paper introduces methods for explaining image classifiers using causal generative models that perform counterfactual inference. The authors propose three approaches: analyzing pixel-based explanations across varying attribute levels using SHAP and contrastive explainers, ranking human-interpretable attribute importances through Monte Carlo sampling, and generating counterfactual explanations via both gradient-based and model-agnostic attribute space searches. Experiments on the Morpho-MNIST dataset demonstrate that their methods produce more interpretable counterfactual explanations than OmnixAI, with lower IM1 scores (0.65-1.24 vs 1.54-2.89) indicating better reconstruction by target-class autoencoders, and higher oracle scores (0.75-0.79 vs 0.56-0.64) showing greater agreement between classifier and oracle on generated examples. The work expands upon prior binary-attribute methods to handle arbitrary classifiers and continuous/categorical attributes.

## Method Summary
The authors propose using causal generative models (DeepSCM and ImageCFGen) to perform counterfactual inference for explaining image classifiers. The methods include: (1) SHAP and contrastive explainers to analyze pixel-based explanations across varying attribute levels, (2) Monte Carlo sampling to rank human-interpretable attribute importances, and (3) optimization-based approaches in attribute space to generate counterfactual explanations. The Morpho-MNIST dataset provides the causal structure and attributes (thickness, intensity, slant) for the experiments. The explanation methods generate pixel and attribute-based explanations using the counterfactual images, which are then evaluated using quantitative metrics (IM1, IM2, oracle score).

## Key Results
- Proposed methods achieve lower IM1 scores (0.65-1.24) compared to OmnixAI (1.54-2.89), indicating better reconstruction by target-class autoencoders
- Oracle scores for proposed methods range from 0.75-0.79, outperforming OmnixAI's 0.56-0.64, showing greater agreement between classifier and oracle on generated examples
- The methods successfully extend prior binary-attribute approaches to handle arbitrary classifiers and continuous/categorical attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual inference via causal generative models enables targeted manipulation of causal attributes while preserving image style.
- Mechanism: The encoder recovers latent noise variables ϵ, and the generator produces counterfactuals by combining ϵ with new attribute values a′, ensuring causal validity (Equation 1).
- Core assumption: The SCM learned by the generative model accurately captures the true causal relationships between attributes and image pixels.
- Evidence anchors:
  - [abstract] "we present a generative counterfactual inference approach to study the influence of visual features...through generative learning"
  - [section] "the counterfactual x′ is of the form: x′ = G(E(x, a), a′)" (Equation 1)
  - [corpus] Weak evidence - no directly comparable mechanism found in neighbors
- Break condition: If the learned SCM is misspecified or if the encoder fails to recover the true latent noise variables.

### Mechanism 2
- Claim: Monte Carlo sampling in the latent space of conditional generative models enables attribute-based explanations for image classifiers.
- Mechanism: Random latent vectors z sampled from p(z) are combined with fixed attributes a to generate multiple images, and their classifications are averaged to create an approximate attribute classifier ˆf(a).
- Core assumption: The classifier's predictions are consistent across images generated with the same attributes, allowing the Monte Carlo approximation to converge.
- Evidence anchors:
  - [abstract] "We then establish a Monte-Carlo mechanism using the generator of a causal generative model in order to adapt Shapley explainers to produce feature importances for the human-interpretable attributes"
  - [section] "ˆf (a) = Ep(z)[f (G(z, a))] ≈ 1 m mX i=1 f (G(zi, a))" (Equation 2)
  - [corpus] Weak evidence - no directly comparable mechanism found in neighbors
- Break condition: If the generative model fails to produce diverse images for the same attribute values, or if the classifier's predictions are highly unstable.

### Mechanism 3
- Claim: Optimization in attribute space using loss functions based on classifier outputs and image similarity generates interpretable counterfactual explanations.
- Mechanism: Gradient-based and model-agnostic approaches search for attribute values a′ that minimize a loss function combining classification score differences and image reconstruction error (Equations 4 and 5).
- Core assumption: The attribute space is smooth enough for gradient-based optimization to find meaningful counterfactuals, and the loss function balances interpretability and realism.
- Evidence anchors:
  - [abstract] "we present optimization methods for creating counterfactual explanations of classifiers by means of counterfactual inference, proposing straightforward approaches for both differentiable and arbitrary classifiers"
  - [section] "min a′ max λ λ (max y′̸=yt fy′(x′(a′)) − fyt (x′(a′))) + ||x′(a′) − x||1" (Equation 5)
  - [corpus] Weak evidence - no directly comparable mechanism found in neighbors
- Break condition: If the attribute space is highly non-convex, or if the loss function's weighting leads to unrealistic or uninterpretable counterfactuals.

## Foundational Learning

- Concept: Counterfactual inference and its role in causal reasoning
  - Why needed here: The entire paper relies on generating counterfactual images by intervening on causal attributes, which requires understanding the principles of counterfactual inference.
  - Quick check question: What are the three steps of counterfactual inference in the context of causal models?

- Concept: Generative models (VAEs and BiGANs) and their latent space structure
  - Why needed here: The proposed methods leverage the latent spaces of VAEs and BiGANs to generate counterfactual images and perform attribute-based explanations.
  - Quick check question: How do VAEs and BiGANs differ in their approach to modeling the data generation process?

- Concept: SHAP values and their application to feature importance in structured data
  - Why needed here: The paper uses SHAP values to quantify the importance of pixels and attributes in explaining classifier decisions.
  - Quick check question: How do SHAP values differ from other feature importance methods like LIME or saliency maps?

## Architecture Onboarding

- Component map:
  - Morpho-MNIST dataset -> Causal generative models (DeepSCM and ImageCFGen) -> Image classifier -> Explanation methods (SHAP, contrastive explainers, optimization-based)

- Critical path:
  1. Train causal generative models on Morpho-MNIST data.
  2. Train an image classifier on Morpho-MNIST images.
  3. Generate counterfactual images by intervening on attributes using the causal generative models.
  4. Apply explanation methods (SHAP, contrastive, optimization) to the counterfactual images and classifier.
  5. Evaluate the interpretability of the explanations using quantitative metrics (IM1, IM2, oracle score).

- Design tradeoffs:
  - Choice of generative model (VAE vs. BiGAN): VAEs are more stable but less deterministic, while BiGANs are fully deterministic but may be less stable during training.
  - Choice of explanation method: SHAP provides pixel-level explanations, while contrastive explainers provide more interpretable positive and negative regions.
  - Choice of optimization approach: Gradient-based methods are faster but require differentiable classifiers, while model-agnostic methods are slower but more general.

- Failure signatures:
  - Poor reconstruction of counterfactual images: Indicates issues with the learned SCM or the generative model's capacity.
  - Inconsistent or uninterpretable explanations: Suggests problems with the explanation method or the classifier's decision boundary.
  - Low IM1 or IM2 scores: Implies that the counterfactuals are not realistic or interpretable enough.

- First 3 experiments:
  1. Train a VAE and BiGAN on the Morpho-MNIST dataset and evaluate their reconstruction quality on a held-out test set.
  2. Train an image classifier on the Morpho-MNIST images and evaluate its accuracy on a held-out test set.
  3. Generate counterfactual images by varying the thickness attribute using both the VAE and BiGAN, and visualize the evolution of SHAP values and contrastive explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform on real-world datasets with more complex causal structures beyond Morpho-MNIST?
- Basis in paper: [inferred] The paper only tests methods on the Morpho-MNIST dataset with a relatively simple causal structure (thickness, intensity, slant, and digit label). The authors acknowledge this limitation and suggest future work on real-world datasets.
- Why unresolved: The current experiments are limited to Morpho-MNIST, which has a controlled and simple causal structure. Real-world datasets likely have more complex, interdependent causal relationships that could affect the performance of the proposed methods.
- What evidence would resolve it: Testing the proposed methods on real-world datasets with known causal structures (e.g., healthcare, finance) and comparing performance metrics (IM1, IM2, oracle scores) to those achieved on Morpho-MNIST.

### Open Question 2
- Question: How sensitive are the counterfactual explanations to the choice of hyperparameters (e.g., λ in gradient-based method, grid search resolution in model-agnostic method)?
- Basis in paper: [explicit] The authors set λ=10 without explicit search and use a grid of size 100 for the model-agnostic method, noting they "observed empirically that changing its value did not significantly impact performance." This suggests hyperparameter sensitivity is not fully explored.
- Why unresolved: The paper does not systematically explore the sensitivity of counterfactual explanations to hyperparameter choices. Different values could lead to significantly different explanations, affecting interpretability and validity.
- What evidence would resolve it: Conducting a comprehensive sensitivity analysis by varying hyperparameters across a wide range and measuring their impact on explanation quality metrics (IM1, IM2, oracle scores) and visual interpretability.

### Open Question 3
- Question: Can the proposed attribute explanation method be extended to handle non-linear relationships between attributes and classifier outputs?
- Basis in paper: [inferred] The current attribute explanation method uses Monte Carlo sampling and SHAP to estimate attribute importances, which may not capture complex non-linear relationships between attributes and classifier outputs, especially for highly non-linear classifiers.
- Why unresolved: The paper does not investigate the performance of the attribute explanation method on datasets with known non-linear attribute-classifier relationships. The method's effectiveness for highly non-linear classifiers is not established.
- What evidence would resolve it: Testing the attribute explanation method on datasets with known non-linear attribute-classifier relationships (e.g., synthetic datasets with controlled non-linearities) and comparing the estimated attribute importances to ground truth importances.

## Limitations
- The methods rely on the accuracy of the learned causal structure from Morpho-MNIST, which may not generalize to real-world datasets with more complex attribute relationships
- The quantitative evaluation focuses on reconstruction metrics (IM1) and oracle agreement, but lacks user studies to validate the actual interpretability of the explanations
- The comparison with OmnixAI is limited to a single baseline, and the paper does not explore the scalability of the methods to higher-resolution images or more complex attribute spaces

## Confidence
- High confidence: The core mechanism of using causal generative models for counterfactual inference is well-established and the mathematical formulations are sound
- Medium confidence: The quantitative evaluation results on Morpho-MNIST are promising, but the lack of user studies and comparison with more baselines limits the generalizability of the findings
- Low confidence: The scalability of the methods to real-world datasets and the actual interpretability of the explanations to human users remain uncertain

## Next Checks
1. Conduct a user study to evaluate the interpretability of the proposed explanations compared to baseline methods, using metrics such as explanation satisfaction and task completion time
2. Apply the methods to a real-world dataset (e.g., CelebA or a medical imaging dataset) and evaluate the performance and scalability of the approaches
3. Explore the use of more advanced generative models (e.g., StyleGAN or Diffusion Models) and explanation methods (e.g., LIME or Anchors) to improve the quality and interpretability of the counterfactual explanations