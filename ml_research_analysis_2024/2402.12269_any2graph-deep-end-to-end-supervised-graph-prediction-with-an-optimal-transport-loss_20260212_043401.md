---
ver: rpa2
title: 'Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport
  Loss'
arxiv_id: '2402.12269'
source_url: https://arxiv.org/abs/2402.12269
tags:
- graph
- loss
- any2graph
- graphs
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Any2Graph, a framework for end-to-end supervised
  graph prediction (SGP) that can handle arbitrary input modalities and output graphs
  of varying sizes. The core method introduces a novel Partially-Masked Fused Gromov-Wasserstein
  (PMFGW) loss function that is differentiable, permutation invariant, and size-agnostic.
---

# Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss

## Quick Facts
- arXiv ID: 2402.12269
- Source URL: https://arxiv.org/abs/2402.12269
- Authors: Paul Krzakala, Junjie Yang, Rémi Flamary, Florence d'Alché-Buc, Charlotte Laclau, Matthieu Labeau
- Reference count: 40
- Primary result: Novel PMFGW loss enables differentiable, permutation-invariant graph prediction for arbitrary input modalities and output graphs of varying sizes

## Executive Summary
Any2Graph is a framework for end-to-end supervised graph prediction that can handle arbitrary input modalities (images, text, graphs, vectors) and output graphs of varying sizes. The core innovation is the Partially-Masked Fused Gromov-Wasserstein (PMFGW) loss function, which enables differentiable, permutation-invariant graph prediction without Hungarian matching by embedding graphs into a continuous space and using optimal transport over padded nodes. The framework consists of an encoder for extracting features from various inputs, a transformer for converting these features into node embeddings, and a graph decoder for predicting graph properties. Experimental results demonstrate state-of-the-art performance across different tasks including map construction from satellite images and molecule prediction from fingerprints.

## Method Summary
Any2Graph addresses supervised graph prediction (SGP) by introducing a novel PMFGW loss function that compares a continuous predicted graph with a padded target graph. The framework uses an encoder to extract features from arbitrary input modalities, a transformer to convert these features into node embeddings, and a graph decoder to predict graph properties including node mask, features, and adjacency matrix. The PMFGW loss leverages optimal transport to achieve permutation invariance and size-agnosticity without Hungarian matching. The method is trained end-to-end and shows superior performance on both synthetic and real-world datasets compared to existing approaches.

## Key Results
- Achieves state-of-the-art performance across different tasks including map construction from satellite images and molecule prediction from fingerprints
- Demonstrates superior ability to correctly predict graph sizes while maintaining high prediction accuracy
- Shows computational efficiency and robustness to the choice of maximum graph size hyperparameter M
- Outperforms existing approaches in terms of prediction accuracy and graph size prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PMFGW loss enables differentiable, permutation-invariant graph prediction without Hungarian matching.
- Mechanism: By embedding graphs into a continuous space of fixed dimension and using an optimal transport plan over the full set of nodes (including padded ones), the loss inherently respects node permutation symmetry. The transport plan is computed directly on the continuous representation, avoiding the need for discrete matching.
- Core assumption: The relaxation from permutations to doubly stochastic matrices preserves the essential properties needed for learning.
- Evidence anchors:
  - [abstract] "a novel Partially-Masked Fused Gromov-Wasserstein (PMFGW) loss function that is differentiable, permutation invariant, and size-agnostic"
  - [section] "From the GM perspective, FGW is cast as an approximation of the original problem, and the optimal transport plan is typically projected back to the space of permutation via Hungarian Matching [47]."
  - [corpus] Weak corpus evidence; related papers focus on partial matching and unbalanced transport but do not directly validate the differentiable relaxation claim.
- Break condition: If the OT relaxation becomes too loose for highly heterophilic graphs, the gradient signal may degrade and require explicit permutation alignment.

### Mechanism 2
- Claim: Feature diffusion accelerates learning by moving structural information into the linear loss terms.
- Mechanism: By augmenting node features with their diffused versions (e.g., concatenated with A·F), the model can predict adjacency patterns through the linear feature term rather than the quadratic structure term, which is harder to optimize.
- Core assumption: The diffused features capture enough structural signal to guide the prediction of edges.
- Evidence anchors:
  - [section] "Interestingly Relationformer uses a loss that presents similarities with FGW but where the matching is done on the node features only, before computing a quadratic-linear loss similar to PMFGW."
  - [section] "The diffused node features carry a portion of the structural information. This makes the node feature term slightly harder to minimize but in turn, the subsequent prediction of the structure is much easier"
  - [corpus] No direct corpus support; diffusion as a graph preprocessing step is known, but its specific role here is inferred from ablation experiments.
- Break condition: If node features are already highly informative (e.g., unique 2D positions), diffusion may add little benefit and could slow convergence.

### Mechanism 3
- Claim: The encoder-decoder transformer architecture can handle variable-sized feature sets from diverse input modalities.
- Mechanism: The encoder maps each input modality to a set of feature vectors whose size depends on the input (e.g., image patches, tokens, graph nodes). The transformer encoder-decoder processes these sets and outputs a fixed number of node embeddings, which the decoder converts into graph predictions.
- Core assumption: The transformer's permutation invariance and positional encoding can adequately represent the input structure.
- Evidence anchors:
  - [section] "The encoder might highly benefit from pre-training whenever applicable; but this goes beyond the scope of this paper."
  - [section] "This resembles the approach taken in machine translation, and we used an architecture based on a stack of transformer encoder-decoders"
  - [corpus] Weak corpus evidence; while transformers are standard in NLP, their application to graph prediction from arbitrary inputs is novel here.
- Break condition: If the input modality requires a specific ordering (e.g., temporal sequences), the permutation invariance may discard critical information.

## Foundational Learning

- Concept: Optimal transport (OT) basics (Kantorovich formulation, Wasserstein distance)
  - Why needed here: PMFGW is an extension of Fused Gromov-Wasserstein; understanding OT relaxation and tensor products is essential to grasp why the loss is differentiable and scalable.
  - Quick check question: What is the difference between the Monge and Kantorovich formulations of OT, and why does the latter enable the relaxation used in PMFGW?

- Concept: Graph isomorphism and node permutation invariance
  - Why needed here: The model must predict graphs without assuming a canonical node ordering; the loss must be invariant under permutation to avoid learning spurious orderings.
  - Quick check question: How does the definition of graph isomorphism relate to the permutation invariance requirement in the PMFGW loss?

- Concept: Transformer encoder-decoder architecture and positional encoding
  - Why needed here: The encoder-decoder must process sets of variable size and inject order information where needed; understanding self-attention and cross-attention is key to modifying the architecture for new modalities.
  - Quick check question: In what scenarios would adding positional encoding to the transformer's input features be critical for correct graph prediction?

## Architecture Onboarding

- Component map:
  - Encoder: modality-specific feature extractor (CNN for images, Transformer for tokens, GNN for graphs)
  - Transformer encoder-decoder: maps variable-sized feature sets to M node embeddings
  - Graph decoder: predicts node mask, features, and adjacency from embeddings
  - PMFGW loss: compares continuous prediction to padded target via optimal transport

- Critical path:
  1. Input → Encoder → k feature vectors
  2. k feature vectors → Transformer → M node embeddings
  3. M node embeddings → Graph decoder → (ĥ, F̂, Â)
  4. (ĥ, F̂, Â) → PMFGW loss → gradient update

- Design tradeoffs:
  - Fixed M vs. dynamic graph size: Larger M increases expressiveness but O(M³) solver cost
  - Feature diffusion: Improves structure prediction but adds computation and may be redundant for highly informative features
  - Continuous vs. discrete matching: Continuous OT plan is differentiable but slightly less precise than Hungarian matching

- Failure signatures:
  - Poor SIZE ACC but good EDGE PREC/REC: Model predicts correct edges but wrong number of nodes
  - Low GI ACC but high NODE ACC: Node features predicted well but graph topology wrong
  - Slow convergence: Try reducing αA or adding feature diffusion

- First 3 experiments:
  1. Ablation: Remove feature diffusion and compare NODE and EDGE metrics
  2. Sensitivity: Vary M and plot active nodes vs. EDIT distance
  3. Loss component analysis: Monitor mask, feature, and structure loss curves during training to diagnose learning bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Any2Graph scale to larger graphs (e.g., hundreds or thousands of nodes)?
- Basis in paper: [explicit] The authors state "The main limitation of Any2Graph is its scalability to graphs of larger size" and "we limit ourselves to relatively small graphs of up to 20 nodes."
- Why unresolved: The paper only experiments with small graphs (up to 20 nodes) and suggests future work on acceleration techniques but doesn't provide concrete solutions or experimental results.
- What evidence would resolve it: Experimental results showing Any2Graph performance on larger graphs, or theoretical analysis of computational complexity scaling with graph size.

### Open Question 2
- Question: How robust is Any2Graph to noise in the input data across different modalities?
- Basis in paper: [inferred] The paper shows results on synthetic Coloring data but doesn't systematically study noise robustness. The authors mention "flexible dataset" properties of Coloring but don't explore noise levels.
- Why unresolved: No experiments are shown varying input noise levels across different datasets, nor is there theoretical analysis of noise robustness.
- What evidence would resolve it: Experiments showing performance degradation as input noise increases across different modalities, or theoretical bounds on robustness to input perturbations.

### Open Question 3
- Question: What is the impact of the maximum graph size hyperparameter M on Any2Graph's performance?
- Basis in paper: [explicit] The authors state "Interestingly, we observe that performances are robust w.r.t. the choice of M" but only test M values up to 25 on the Coloring dataset.
- Why unresolved: The paper only explores a limited range of M values (10-25) on one dataset, leaving questions about performance on much larger graphs or different datasets.
- What evidence would resolve it: Systematic experiments varying M across different datasets and graph sizes, or theoretical analysis of how M affects model capacity and performance.

### Open Question 4
- Question: How does Any2Graph perform on more complex graph structures like graphs with loops or multiple connected components?
- Basis in paper: [explicit] The authors note "Fingerprint2Graph molecules exhibit unique substructures such as loops" but don't provide quantitative analysis of performance on such structures.
- Why unresolved: While the paper mentions complex structures exist in some datasets, it doesn't specifically measure performance on graphs with loops vs. without, or analyze connected components.
- What evidence would resolve it: Quantitative comparison of Any2Graph performance on graphs with and without loops/connected components, or ablation studies removing such structures from datasets.

### Open Question 5
- Question: How does the choice of ground loss functions (ℓh, ℓf, ℓA) affect Any2Graph's performance?
- Basis in paper: [explicit] The authors state "For ℓA and ℓh we use the cross-entropy... For ℓf we use the squared ℓ2 or the cross-entropy" but don't explore alternative choices or their impact.
- Why unresolved: The paper uses specific loss functions without exploring alternatives or providing analysis of how different choices affect performance.
- What evidence would resolve it: Experiments comparing Any2Graph with different ground loss function combinations, or theoretical analysis of how loss function choice affects the PMFGW properties.

## Limitations

- Scalability to larger graphs is limited by O(M³) computational complexity of the optimal transport solver
- The framework's generalization to entirely unseen input modalities not tested in the paper
- Performance with highly heterophilic graphs where the OT relaxation may be less effective
- Several hyperparameters (M, αA, number of transformer layers, etc.) are tuned for specific datasets without general guidance

## Confidence

- **High confidence**: The framework's core architecture (encoder → transformer → decoder) is sound and well-established; the synthetic dataset results are reliable and show clear improvements
- **Medium confidence**: The PMFGW loss design and its permutation invariance properties; performance on real-world datasets (Sat2Graph, Fingerprint2Graph) where the method shows state-of-the-art results
- **Lower confidence**: Generalization to entirely unseen input modalities not tested in the paper; performance with highly heterophilic graphs where the OT relaxation may be less effective

## Next Checks

1. **Hungarian matching baseline comparison**: Implement and compare against exact permutation matching on the synthetic Coloring dataset to quantify the impact of the continuous relaxation
2. **Feature diffusion ablation on diverse features**: Test the diffusion mechanism on datasets with both highly informative (unique positions) and uninformative features to map when it's beneficial
3. **Scalability benchmark**: Systematically evaluate runtime and EDIT distance performance as target graph size increases beyond the current datasets to identify practical limits of the O(M³) solver