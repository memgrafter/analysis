---
ver: rpa2
title: Learning to Coordinate without Communication under Incomplete Information
arxiv_id: '2409.12397'
source_url: https://arxiv.org/abs/2409.12397
tags:
- seeker
- helper
- action
- coordination
- dfas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of coordination between agents\
  \ in shared-control games under incomplete information, specifically when direct\
  \ communication is unavailable. The authors propose an automata-learning-based approach\
  \ where a helper agent infers the seeker\u2019s intent by constructing intent-response\
  \ deterministic finite automata (DFAs) for each possible helper action, learning\
  \ them from observed seeker action sequences using Angluin\u2019s L algorithm."
---

# Learning to Coordinate without Communication under Incomplete Information

## Quick Facts
- arXiv ID: 2409.12397
- Source URL: https://arxiv.org/abs/2409.12397
- Authors: Shenghui Chen; Shufang Zhu; Giuseppe De Giacomo; Ufuk Topcu
- Reference count: 40
- One-line primary result: Learning intent-response DFAs via Angluin's L* algorithm enables coordination without communication, achieving success rates within 4-7% of oracle with direct communication.

## Executive Summary
This paper addresses the challenge of coordinating between agents in shared-control games under incomplete information when direct communication is unavailable. The authors propose an automata-learning-based approach where a helper agent infers the seeker's intent by constructing intent-response deterministic finite automata (DFAs) for each possible helper action. These DFAs are learned from observed seeker action sequences using Angluin's L* algorithm and combined into a finite-state transducer to generate the helper's strategy. Experiments in the Gnomes at Night testbed demonstrate that this no-communication coordination (NCC) method significantly outperforms uncoordinated play while achieving success rates within 4-7% of an oracle with direct communication.

## Method Summary
The method employs a helper agent that learns intent-response DFAs for each possible action using Angluin's L* algorithm, which infers DFAs from membership and equivalence queries. The helper observes the seeker's action sequences, extracts informative segments that deviate from shortest paths, and uses these as positive/negative examples for DFA learning. Each DFA represents sequences indicating a specific helper action is needed. These DFAs are combined into a finite-state transducer that maps seeker sequences to helper actions. The seeker uses a modified A* algorithm that minimizes wall violations and expresses intent through deviations when assistance is needed. The approach is evaluated on 9×9 and 12×12 maze configurations, comparing success rates, steps taken, and seeker memory against no coordination and direct communication baselines.

## Key Results
- NCC achieves success rates within 4-7% of the oracle with direct communication
- NCC significantly outperforms no coordination baseline in both success rate and steps taken
- NCC reduces seeker memory requirements compared to uncoordinated play

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The helper can infer the seeker's intent from observed action sequences by learning intent-response DFAs.
- Mechanism: The helper observes sequences of the seeker's actions, extracts segments that deviate from the shortest path, and uses these as positive examples for the DFA corresponding to the helper's expected action. Segments not matching the intent are used as negative examples.
- Core assumption: Deviations from the shortest path indicate the seeker's need for assistance, and these deviations can be reliably segmented and labeled.
- Evidence anchors:
  - [abstract] "...enables an agent to develop a strategy by interpreting its partner's action sequences as intent signals..."
  - [section] "To identify these 'informative' segments, the helper constructs a subgraph of visited states, computes the shortest path from prior to current location, and removes it from the action sequence."
  - [corpus] Weak evidence - no direct mention of DFA-based intent inference in related papers, though similar concepts appear in "Successful Misunderstandings: Learning to Coordinate Without Being Understood."
- Break condition: If the seeker's deviations don't consistently indicate intent, or if the shortest path computation fails due to complex state spaces.

### Mechanism 2
- Claim: Combining learned DFAs into a finite-state transducer enables the helper to map seeker sequences to appropriate actions.
- Mechanism: Each DFA learned for a helper action accepts sequences that indicate that action is needed. These DFAs are combined into a transducer that outputs the most frequently accepted action for a given sequence.
- Core assumption: The union of DFAs can effectively represent the complete helper strategy without exponential blowup.
- Evidence anchors:
  - [abstract] "...constructing a finite-state transducer built from deterministic finite automata, one for each possible action the agent can take."
  - [section] "The learned DFAs are then combined into a finite-state transducer that encodes the helper's overall strategy."
  - [corpus] Moderate evidence - related work on "Unifying Agent Interaction and World Information for Multi-agent Coordination" suggests similar compositional approaches.
- Break condition: If the transducer becomes too large or if multiple actions are equally frequent, leading to ambiguity.

### Mechanism 3
- Claim: The seeker's modified A* algorithm with wall constraints helps the helper learn more accurate intent-response DFAs.
- Mechanism: By planning paths that minimize wall violations and expressing intent through deviations, the seeker provides clearer signals for the helper to learn from.
- Core assumption: The seeker's behavior is consistent and the wall constraints are effectively communicated through deviations.
- Evidence anchors:
  - [section] "The seeker plans paths with a modified A* algorithm that minimizes wall violations under its own and inferred partner constraints."
  - [section] "Upon violation, it replans and inserts intent-expressive actions."
  - [corpus] Weak evidence - no direct mention of A* modifications in related papers, though "Multiple Ships Cooperative Navigation and Collision Avoidance using Multi-agent Reinforcement Learning with Communication" suggests similar planning approaches.
- Break condition: If the seeker's behavior is too stochastic or if wall constraints change dynamically.

## Foundational Learning

- Concept: Deterministic Finite Automata (DFA)
  - Why needed here: DFAs provide a structured way to recognize patterns in the seeker's action sequences that indicate intent.
  - Quick check question: What is the difference between a DFA and a non-deterministic finite automaton (NFA)?

- Concept: Angluin's L* Algorithm
  - Why needed here: L* enables the helper to learn DFAs from membership and equivalence queries without prior knowledge of the seeker's policy.
  - Quick check question: How does Angluin's L* algorithm use counterexamples to refine its hypotheses?

- Concept: Finite-State Transducer
  - Why needed here: Transducers combine multiple DFAs to map input sequences (seeker actions) to output actions (helper responses) efficiently.
  - Quick check question: What is the advantage of using a transducer over simply composing multiple DFAs?

## Architecture Onboarding

- Component map: Seeker -> Helper -> Learning Module -> Strategy Module
- Critical path:
  1. Seeker executes action sequence.
  2. Helper observes sequence and extracts informative segments.
  3. Helper queries seeker for membership/equivalence.
  4. Helper learns DFAs and constructs transducer.
  5. Helper uses transducer to select actions.
- Design tradeoffs:
  - Using DFAs provides explainability but may limit expressiveness compared to neural networks.
  - Learning DFAs requires interaction with the seeker, which may be time-consuming.
  - The transducer approach avoids exponential blowup but may introduce ambiguity if multiple actions are equally frequent.
- Failure signatures:
  - Helper consistently fails to learn accurate DFAs (low Jaccard similarity with oracle DFAs).
  - Success rate plateaus below direct communication baseline.
  - Seeker's memorized wall constraints grow excessively large.
- First 3 experiments:
  1. Test DFA learning on a simple maze with clear intent signals to verify basic functionality.
  2. Evaluate transducer construction on a maze with multiple possible helper actions to check for ambiguity handling.
  3. Measure success rate improvement over no coordination baseline in a larger maze to assess scalability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the automata-learning-based approach degrade as the maze size increases beyond 12×12?
- Basis in paper: [inferred] The paper tests on 9×9 and 12×12 mazes but does not explore larger sizes or report scaling behavior.
- Why unresolved: The paper only evaluates on two maze sizes, leaving open whether the approach maintains effectiveness on significantly larger mazes where state spaces grow exponentially.
- What evidence would resolve it: Systematic testing on larger maze sizes (e.g., 15×15, 20×20) showing success rates, steps taken, and learning times would clarify scaling limits.

### Open Question 2
- Question: What is the impact of noisy or suboptimal seeker behavior on the learned intent-response DFAs?
- Basis in paper: [explicit] The paper mentions that the seeker might follow sub-optimal heuristics like weighted A*, but does not quantify how this affects DFA quality.
- Why unresolved: The paper acknowledges potential noise but does not measure how deviations from optimal behavior influence the accuracy of inferred DFAs or coordination performance.
- What evidence would resolve it: Controlled experiments introducing varying degrees of seeker sub-optimality or random deviations, measuring DFA Jaccard similarity and coordination success, would quantify this impact.

### Open Question 3
- Question: Can the approach be extended to handle non-deterministic transitions or probabilistic action outcomes?
- Basis in paper: [inferred] The current framework assumes deterministic transitions and does not address uncertainty in action effects or environment dynamics.
- Why unresolved: The paper focuses on deterministic settings, but real-world applications often involve stochasticity, which could break the deterministic DFA assumptions.
- What evidence would resolve it: Extending the formalism to probabilistic transitions and testing on environments with action uncertainty would show whether the automata-learning method can adapt or requires modification.

## Limitations

- The approach assumes seeker deviations from shortest paths reliably indicate intent, which may not generalize to more complex environments
- Experimental evaluation is limited to small maze sizes (9×9 and 12×12), raising scalability concerns
- The paper lacks ablation studies on key design choices like transducer combination versus simpler approaches

## Confidence

- **High confidence**: The core mechanism of using DFAs to represent intent-response patterns is well-established and the L* algorithm is a standard tool in automata learning. The experimental comparison against baselines (no coordination and direct communication) is appropriately designed.
- **Medium confidence**: The effectiveness of the approach in the Gnomes at Night testbed is supported by quantitative results, but the limited maze sizes and single domain reduce generalizability. The claim that this approach "achieves a success rate within 4-7% of direct communication" is specific but may not hold in more complex environments.
- **Low confidence**: The assumption that seeker deviations consistently indicate intent is plausible but not rigorously validated. The paper does not provide detailed analysis of failure cases or the robustness of the approach to noisy or adversarial seekers.

## Next Checks

1. **Scalability Test**: Evaluate the helper's performance on larger mazes (e.g., 15×15 or 20×20) to assess whether the success rate and step count remain competitive with the direct communication baseline. This will reveal if the transducer approach scales effectively.

2. **Robustness to Noisy Seekers**: Test the DFA learning process with seekers that occasionally deviate from their intended paths due to random noise or errors. Measure how the helper's success rate degrades and whether the DFAs remain accurate.

3. **Ablation Study on Transducer Design**: Compare the current transducer-based approach against simpler alternatives, such as using a single DFA or a rule-based system, to determine if the added complexity is necessary for good performance.