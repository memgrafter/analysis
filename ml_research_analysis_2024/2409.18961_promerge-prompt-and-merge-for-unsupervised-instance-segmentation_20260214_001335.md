---
ver: rpa2
title: 'ProMerge: Prompt and Merge for Unsupervised Instance Segmentation'
arxiv_id: '2409.18961'
source_url: https://arxiv.org/abs/2409.18961
tags:
- mask
- masks
- promerge
- image
- merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ProMerge, a training-free approach for unsupervised
  instance segmentation. The method leverages self-supervised visual features to generate
  initial mask proposals through point-prompting, then iteratively merges these masks
  based on pixel overlap and feature similarity, aided by a background-based mask
  pruning technique.
---

# ProMerge: Prompt and Merge for Unsupervised Instance Segmentation

## Quick Facts
- **arXiv ID:** 2409.18961
- **Source URL:** https://arxiv.org/abs/2409.18961
- **Authors:** Dylan Li; Gyungin Shin
- **Reference count:** 37
- **Primary result:** Training-free unsupervised instance segmentation method using self-supervised features and iterative mask merging, achieving competitive performance with state-of-the-art approaches while being significantly faster.

## Executive Summary
ProMerge presents a novel training-free approach for unsupervised instance segmentation that leverages self-supervised visual features to generate initial mask proposals through point-prompting, then iteratively merges these masks based on pixel overlap and feature similarity. The method achieves competitive performance compared to state-of-the-art normalized-cut-based methods while being approximately 3.6 times faster. When used to generate pseudo-labels for training an object detector, ProMerge predictions outperform the current leading unsupervised model across various challenging instance segmentation benchmarks including COCO2017, LVIS, KITTI, Objects365, and SA-1B.

## Method Summary
ProMerge uses DINO self-supervised visual features to represent images as patch tokens, which are then processed through point-prompting to generate initial mask proposals by computing cosine similarities between prompt tokens and all patch tokens. The method employs background-based mask pruning to remove noisy background masks before merging, using edge pixel concentration heuristics and pixel-wise voting to identify and filter background regions. Masks are then processed in descending order of area, with smaller masks merging with larger ones based on Intersection-over-Area (IoA) and feature similarity thresholds. The final output undergoes connected component splitting and CRF post-processing to obtain refined instance masks.

## Key Results
- Achieves competitive performance with state-of-the-art normalized-cut-based methods while being ~3.6x faster
- When used to generate pseudo-labels for object detector training, outperforms current leading unsupervised model on COCO2017, LVIS, KITTI, Objects365, and SA-1B benchmarks
- Demonstrates effectiveness across diverse datasets with varying object densities and image complexities

## Why This Works (Mechanism)

### Mechanism 1
Using self-supervised visual features from models like DINO enables effective initial mask proposals without training. Self-supervised features encode strong local correspondences and inherent grouping ability, allowing mask proposals to be generated via point-prompting that compares prompt tokens to all patch tokens using cosine similarity. Core assumption: The self-supervised features preserve sufficient spatial and semantic information for meaningful affinity computations between patches.

### Mechanism 2
Iterative mask merging based on pixel overlap (IoA) and feature similarity produces semantically coherent final masks. Masks are processed in descending area order; smaller masks merge with larger ones if IoA exceeds τmerge_IoA or feature similarity exceeds τmerge_f, allowing combination of spatially close or semantically similar segments. Core assumption: Pixel overlap and feature similarity are sufficient signals to determine when masks should be merged without over-merging distinct objects.

### Mechanism 3
Background-based mask pruning removes noisy background masks before merging, improving precision and recall. Background masks are identified by edge pixel concentration, aggregated via pixel-wise voting to form a single background mask, then used to filter foreground proposals via IoA and feature similarity thresholds. Core assumption: Background masks have distinguishable edge pixel patterns that can be reliably aggregated and used to filter out false positives.

## Foundational Learning

- **Self-supervised visual representation learning (e.g., DINO)**: Provides rich visual features without labeled data, enabling unsupervised instance segmentation through inherent grouping ability. *Quick check:* What property of DINO features makes them suitable for unsupervised segmentation tasks?

- **Normalized cut and spectral clustering**: Understanding the computational bottleneck that ProMerge avoids by using raw feature affinities instead of solving eigenvalue systems. *Quick check:* Why is solving the normalized cut eigenvalue problem computationally expensive compared to ProMerge's approach?

- **Intersection-over-Area (IoA) vs Intersection-over-Union (IoU)**: IoA is more effective for merging masks of different sizes, crucial for combining object parts without penalizing large mask sizes. *Quick check:* In what scenario would IoU be less effective than IoA for merging two masks?

## Architecture Onboarding

- **Component map:** Image encoder (DINO-ViT) → Feature extraction → Point-prompting module → Initial mask proposals → Background aggregation → Cascade filtering → Iterative merging → CRF post-processing → Output masks

- **Critical path:** Image → DINO features → Prompting → Background pruning → Merging → CRF → Output masks

- **Design tradeoffs:**
  - Number of prompt tokens (K) vs inference speed and mask quality
  - τb threshold for initial mask binarization vs mask size and noise
  - τmerge thresholds vs merging granularity and instance separation
  - Background filtering method (IoA vs feature similarity vs Cascade) vs precision/recall balance

- **Failure signatures:**
  - Low AP/AR with high recall: Background pruning too aggressive, merging too strict
  - Low AP/AR with high precision: Merging too permissive, background pruning insufficient
  - Slow inference: Too many prompt tokens (high K), inefficient filtering
  - Over-segmentation: τmerge thresholds too strict, background pruning too weak
  - Under-segmentation: τmerge thresholds too permissive, background pruning too aggressive

- **First 3 experiments:**
  1. Vary K (number of prompt tokens) to find optimal speed-quality tradeoff on COCO2017 validation
  2. Compare Cascade filtering vs IoA-only vs feature-similarity-only filtering on precision-recall curve
  3. Test different τb values to optimize initial mask quality before merging

## Open Questions the Paper Calls Out

### Open Question 1
**What is the optimal number of prompt tokens (K) for different image types and datasets in ProMerge?**
The paper shows that K=100 yields the best trade-off between inference speed and performance, but also demonstrates that performance varies with K across different values (Fig. 5). The optimal K likely depends on image complexity, object density, and dataset characteristics. Systematic experiments varying K across multiple datasets with different object densities and image complexities, potentially using adaptive K selection based on image content, would resolve this question.

### Open Question 2
**How does ProMerge performance scale with larger backbone architectures beyond ViT-B/8?**
The paper uses DINO with ViT-B/8 architecture and demonstrates good performance, but doesn't explore larger backbones. Larger architectures might provide richer feature representations that could improve segmentation quality, but may also increase computational cost. Experiments comparing ProMerge with different backbone sizes (e.g., ViT-L/16, ViT-H/14) on the same benchmarks, measuring both performance and inference speed trade-offs, would resolve this question.

### Open Question 3
**Can the background-based mask pruning be improved by incorporating additional cues beyond edge proximity?**
The current background pruning uses a simple heuristic based on edge proximity and pixel-wise voting. More sophisticated background detection could further improve mask quality by better distinguishing between foreground and background regions. Experiments testing alternative background detection methods, such as using saliency maps, depth information, or more advanced clustering techniques to identify background regions, would resolve this question.

## Limitations

- **Hyperparameter sensitivity:** The merging strategy based on IoA and feature similarity thresholds is sensitive to hyperparameter selection, with poor threshold selection potentially leading to over-segmentation or under-segmentation.
- **Generalization of background pruning:** The background pruning mechanism relies on edge pixel concentration heuristics that may not generalize well to objects with complex shapes or those positioned away from image boundaries.
- **Reliance on self-supervised features:** The method's effectiveness depends on the quality and consistency of DINO self-supervised features across different image types and object categories.

## Confidence

**High confidence:** The core architectural components (point-prompting with DINO features, iterative merging based on overlap and similarity, background pruning) are clearly described and logically sound. The method is computationally efficient compared to normalized cut approaches, which is a verifiable claim.

**Medium confidence:** The competitive performance claims are based on reported metrics, but without access to the code or ability to reproduce results, there's uncertainty about whether the method achieves stated performance consistently across different runs and datasets. The pseudo-label quality claims for downstream object detection training are reasonable but not extensively validated.

**Low confidence:** The paper's claims about the method working "out-of-the-box" without training or fine-tuning may overstate practical applicability. The sensitivity to hyperparameters and the need for careful threshold selection suggests the method may require dataset-specific tuning in practice.

## Next Checks

1. **Ablation study on feature extractors:** Test ProMerge with alternative self-supervised feature extractors (MAE, BEiT, VICReg) on COCO2017 validation to determine how sensitive performance is to the choice of visual features.

2. **Threshold sensitivity analysis:** Systematically vary τmerge_IoA and τmerge_f across a grid (0.5-0.95 in 0.05 increments) and plot AP vs threshold combinations to identify optimal settings and measure performance variance.

3. **Background pruning robustness:** Create synthetic test cases with objects positioned away from boundaries and objects with high edge density, then measure how well the background pruning mechanism maintains precision without removing valid foreground masks.