---
ver: rpa2
title: 'DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information
  for Dynamics Model Learning'
arxiv_id: '2410.21758'
source_url: https://arxiv.org/abs/2410.21758
tags:
- data
- dataset
- manipulation
- plasticine
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present DOFS, a real-world dataset of 3D deformable
  object manipulation with full spatial information. They introduce a low-cost hardware
  platform with a transparent operating plane and 6 RGB-D cameras to capture multi-view
  images of objects from top, side, and bottom perspectives.
---

# DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning

## Quick Facts
- arXiv ID: 2410.21758
- Source URL: https://arxiv.org/abs/2410.21758
- Reference count: 14
- Key outcome: Real-world dataset of 3D deformable object manipulation with full spatial information from multi-view RGB-D capture

## Executive Summary
The DOFS dataset addresses the critical need for real-world data with full 3D spatial information for deformable object manipulation. Using a low-cost hardware platform with 6 RGB-D cameras positioned around a transparent operating plane, the system captures complete 3D information of objects from top, side, and bottom perspectives. The dataset includes registered point clouds, 3D deformed meshes, and 3D occupancy data with semantic labels for 150 pinch manipulation sequences totaling 4,500 frames. A preliminary experiment demonstrates the utility of this data by training a dynamics model that successfully predicts post-manipulation states of plasticine after pinching actions within 8 training epochs on standard hardware.

## Method Summary
The authors present a real-world dataset of 3D deformable object manipulation with full spatial information. The hardware platform consists of an aluminum frame with a transparent acrylic operating plane and 6 Intel RealSense D435i cameras (4 above, 2 below) to capture multi-view RGB-D images. A UR5e robotic arm with PGI-140-80 gripper performs pinch manipulations on plasticine objects. The software pipeline uses ROS for camera synchronization, ICP for point cloud registration, and voxelization to create 3D occupancy data with semantic labels. The dataset includes 150 manipulation sequences with 4,500 frames total, capturing registered point clouds, 3D deformed meshes, and 3D occupancy data. A preliminary dynamics model is trained using downsampled 3D occupancy and action inputs to predict post-manipulation states.

## Key Results
- 150 pinch manipulation sequences captured with full spatial information from 6 RGB-D cameras
- Point clouds registered using ICP to capture both external surfaces and internal hollow structures
- 3D occupancy data with semantic labels generated using 0.002m voxel resolution
- Dynamics model trained on downsampled 3D occupancy and action inputs achieves successful state prediction within 8 epochs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-view RGB-D capture from top, side, and bottom enables complete spatial reconstruction of deformable objects
- Mechanism: By placing 6 RGB-D cameras around a transparent operating plane, the system captures images from all three spatial dimensions, allowing registration of point clouds that reveal both external surface and internal hollow structures
- Core assumption: The transparent plane does not interfere with camera depth sensing and that all camera views are properly calibrated
- Evidence anchors:
  - [abstract] "full spatial information (i.e., top, side, and bottom information)"
  - [section 2.1] "we used aluminum profiles to build a frame and installed a transparent acrylic board in the middle of the frame as the operating plane"
  - [section 2.2] "use Iterative Closest Point (ICP) [13] to register the point cloud generated from 6 RGB-D cameras to obtain the full spatial information"
- Break condition: If the transparent plane causes depth sensing artifacts or if camera calibration fails, point cloud registration will be incomplete

### Mechanism 2
- Claim: 3D occupancy data with semantic labels provides dense, high-resolution representation suitable for dynamics model training
- Mechanism: Voxelizing the registered point cloud into a uniform grid with assigned semantic labels creates a structured representation that neural networks can efficiently process as input for state prediction
- Core assumption: The voxel resolution (0.002m) is fine enough to capture relevant deformation features while remaining computationally tractable
- Evidence anchors:
  - [section 2.2] "we selected the voxel size [0.002, 0.002, 0.002]m to generate the 3D occupancy data and assign the semantic label"
  - [section 3] "we downsample the generated 3D occupancy and use it to train a dynamics model of the plasticine"
- Break condition: If voxel resolution is too coarse, fine deformation details will be lost; if too fine, computational requirements will exceed hardware capabilities

### Mechanism 3
- Claim: Learning dynamics from 3D occupancy and action inputs enables prediction of post-manipulation states
- Mechanism: The neural network learns the mapping from current 3D occupancy state and applied action (gripper positions) to predicted next state, capturing the physics of elasto-plastic deformation
- Core assumption: The quasi-static assumption holds (deformation reaches equilibrium faster than action application)
- Evidence anchors:
  - [abstract] "we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object"
  - [section 3] "preliminary experiment... shows that our model can predict the state of plasticine after an action is applied"
- Break condition: If deformation dynamics are too fast or highly non-linear, the quasi-static assumption breaks and predictions become inaccurate

## Foundational Learning

- Concept: Point cloud registration and ICP algorithm
  - Why needed here: To combine partial views from multiple cameras into a unified 3D representation of the object
  - Quick check question: What is the mathematical objective function that ICP minimizes when aligning two point clouds?

- Concept: Voxel grid representation and occupancy encoding
  - Why needed here: To convert continuous 3D point clouds into discrete tensor format suitable for neural network processing
  - Quick check question: How does voxel resolution affect the trade-off between representation fidelity and computational complexity?

- Concept: Neural dynamics modeling for deformable objects
  - Why needed here: To learn the state transition function that maps current object state and action to predicted next state
  - Quick check question: What loss function would you use to train a dynamics model for predicting 3D occupancy states?

## Architecture Onboarding

- Component map: 6 RGB-D cameras -> ROS synchronization -> Point cloud registration (ICP) -> Mesh reconstruction -> Voxelization -> Neural network training
- Critical path: Camera capture → ROS synchronization → Point cloud registration → Voxelization → Model training → Prediction
- Design tradeoffs: Multi-view vs. single-view capture (completeness vs. complexity), voxel resolution (accuracy vs. memory), real-time vs. offline processing
- Failure signatures: Incomplete point cloud registration (holes in reconstructed mesh), semantic labeling errors (fingers misclassified as object), dynamics model convergence issues (high prediction error)
- First 3 experiments:
  1. Validate point cloud registration by comparing single-view and multi-view reconstructions of a simple geometric object
  2. Test voxelization pipeline with synthetic data to ensure semantic labels are preserved through the conversion
  3. Train a simplified dynamics model on synthetic deformation data to verify the network architecture before using real data

## Open Questions the Paper Calls Out
1. How does the dataset size and diversity scale with more manipulation actions beyond pinching? The authors state they will "choose 3D DOs with different physical properties (e.g., stiffness) and geometry shapes to diversify our dataset" and "change the gripper shape to apply other manipulation strategies (e.g., cut, roll)." The current dataset only includes 150 pinches with 4,500 frames.

2. What is the impact of the 3D occupancy voxel size on the accuracy of dynamics model predictions? The authors selected a voxel size of [0.002, 0.002, 0.002]m but did not explore how different voxel sizes affect model performance.

3. How well does the learned dynamics model generalize to objects with different stiffness properties? The authors mention they will include objects with different physical properties but have not yet done so, and the current model was only trained on plasticine.

## Limitations
- Dataset size limited to 150 sequences with 4,500 frames total
- Only one object type (plasticine) and one manipulation primitive (pinching) included
- Quasi-static assumption may not hold for faster or more complex manipulations
- Neural network architecture details not fully specified for exact replication

## Confidence
- **High**: The hardware platform design and data collection methodology are well-specified and reproducible
- **Medium**: The registration pipeline using ICP and voxelization process is clearly described
- **Low**: The dynamics model architecture and training hyperparameters lack sufficient detail for exact replication

## Next Checks
1. **Point Cloud Completeness**: Validate that the multi-view registration captures full 3D geometry by comparing registered point clouds against ground truth for simple geometric test objects
2. **Dynamics Model Generalization**: Test the trained dynamics model on held-out sequences and on plasticine with different initial geometries to assess generalization beyond the training distribution
3. **Alternative Representation Ablation**: Compare dynamics model performance using 3D occupancy versus alternative representations (point clouds, meshes) to quantify the benefit of the voxelized representation for this task