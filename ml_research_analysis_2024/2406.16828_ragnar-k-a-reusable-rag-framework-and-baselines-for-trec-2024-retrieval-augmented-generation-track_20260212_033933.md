---
ver: rpa2
title: "Ragnar\xF6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented\
  \ Generation Track"
arxiv_id: '2406.16828'
source_url: https://arxiv.org/abs/2406.16828
tags:
- https
- arxiv
- potty
- retrieval
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Ragnar\xF6k, a reusable and open-source framework\
  \ for retrieval-augmented generation (RAG) systems, designed to support the TREC\
  \ 2024 RAG Track. Ragnar\xF6k provides a modular pipeline with retrieval (BM25,\
  \ RankZephyr) and augmented generation (Command R+, GPT-4o) components, along with\
  \ a web-based evaluation arena for human pairwise comparisons."
---

# Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track

## Quick Facts
- arXiv ID: 2406.16828
- Source URL: https://arxiv.org/abs/2406.16828
- Reference count: 40
- Primary result: Introduces Ragnarök, an open-source modular framework for RAG systems with curated datasets and baselines for TREC 2024 RAG Track

## Executive Summary
Ragnarök is a reusable, open-source framework designed to support the TREC 2024 Retrieval-Augmented Generation Track. It provides a modular pipeline with retrieval (BM25, RankZephyr) and augmented generation (Command R+, GPT-4o) components, along with a web-based evaluation arena for human pairwise comparisons. The framework includes a deduplicated MS MARCO V2.1 document collection and two development topic sets (TREC-RAGgy 2024 and TREC-Researchy 2024) curated to ensure non-factoid, knowledge-intensive queries. Baseline RAG pipelines demonstrate that GPT-4o generates longer, more detailed answers while Command R+ produces shorter, more citation-dense responses.

## Method Summary
The Ragnarök framework implements a two-stage RAG pipeline: first retrieving relevant segments using BM25 followed by RankZephyr reranking, then generating answers with citations using either Command R+ or GPT-4o. The system uses a deduplicated version of the MS MARCO V2.1 document and segment collections, created through MinHash-based LSH to identify and remove near-duplicate documents. Two development topic sets are provided: TREC-RAGgy 2024 (filtered TREC DL topics) and TREC-Researchy 2024 (Researchy Questions with intrinsic attributes). The framework is released as the pyragnarok Python package with a Gradio WebUI for pairwise comparison evaluation.

## Key Results
- Deduplication reduces MS MARCO V2.1 collection by 8.35% and segment count by ~9% while improving retrieval diversity
- GPT-4o generates longer, more detailed answers compared to Command R+'s shorter, citation-dense responses
- TREC-RAGgy 2024 and TREC-Researchy 2024 provide curated non-factoid topics requiring long-form answers and multi-hop reasoning
- Framework provides reproducible baselines for TREC 2024 RAG Track with modular components that can be independently tuned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular retrieval-augmented generation pipeline ensures that each stage can be independently tuned and optimized without breaking the whole system.
- Mechanism: By splitting the system into a retrieval module (BM25 → RankZephyr) and an augmented generation module (LLM prompt + generation), the framework allows swapping retrievers, rerankers, and generators without modifying the other parts.
- Core assumption: Each module's input/output format is standardized (e.g., top-k segments in consistent JSON).
- Evidence anchors:
  - [abstract]: "Ragnarök comprises two key modules: (R) Retrieval and (AG) Augmented Generation."
  - [section 3.1]: "The retrieval system searches for relevant segments... The augmented generation module uses the user-provided topic and retrieved segments..."
- Break condition: If the output schema changes incompatibly between modules, the pipeline breaks.

### Mechanism 2
- Claim: Deduplication of the MS MARCO V2 collection improves retrieval diversity and reduces near-duplicate bias in RAG outputs.
- Mechanism: Locality Sensitive Hashing with MinHash 9-gram shingles identifies equivalence classes of near-duplicate documents; one representative is kept, reducing the collection size by 8.35% and segment count by ~9%.
- Core assumption: Near-duplicates degrade downstream retrieval and generation quality by biasing relevance scores toward similar content.
- Evidence anchors:
  - [section 4]: "We conduct a deduplication strategy in the MS MARCO V2 document collection... reducing the duplicates in the original MS MARCO V2 document collection by 8.35%."
  - [section 4]: "When left intact, these near-duplicates degrade the downstream retrieval accuracy and reduce the diversity of the collected documents."
- Break condition: If deduplication removes documents that are not true duplicates but still useful for coverage, retrieval recall may suffer.

### Mechanism 3
- Claim: Using non-factoid, long-form topics from TREC-RAGgy and TREC-Researchy ensures that RAG systems are evaluated on realistic, complex queries that require synthesis rather than simple lookup.
- Mechanism: Topics are filtered to include only those requiring aggregation, multi-hop reasoning, or long-form answers, and intrinsic attributes (knowledge-intensive, multi-faceted) are maximized via a diversity sampler.
- Core assumption: Factoid queries are easily memorized by LLMs and do not test true RAG capability.
- Evidence anchors:
  - [section 5]: "Topics, i.e., user queries, are crucial for robust evaluation of RAG systems... these topics lack complexity, leading to short answers that can be easily memorized by LLMs."
  - [section 5]: "To avoid short-form answers in RAG, we utilize two collections containing non-factoid topics covering information about diverse topics and requiring long-form answers."
- Break condition: If topic filtering removes all queries that have short answers, evaluation may miss edge cases where RAG systems excel on simpler tasks.

## Foundational Learning

- Concept: Information retrieval ranking metrics (e.g., MRR, NDCG)
  - Why needed here: Understanding how BM25 and RankZephyr rank and rerank segments is critical for diagnosing retrieval performance.
  - Quick check question: What metric would you use to evaluate the quality of the top-20 reranked segments for a RAG system?

- Concept: Text chunking and sliding window segmentation
  - Why needed here: The sliding window technique (window=10 sentences, stride=5) creates overlapping segments to preserve context for RAG generation.
  - Quick check question: Why use a stride smaller than the window size in chunking?

- Concept: Large language model prompting and citation extraction
  - Why needed here: Different LLMs (Command R+, GPT-4o) cite differently (span vs. IEEE format); the prompt must align with the expected citation style.
  - Quick check question: How would you modify the prompt if switching from GPT-4o to a model that does not auto-cite?

## Architecture Onboarding

- Component map: Topic → BM25 retrieval → RankZephyr reranking → LLM generation → JSON output with citations
- Critical path: Topic → Retrieval → Rerank → Generation → Output
- Design tradeoffs:
  - More segments (larger k) increases recall but may introduce noise.
  - Longer prompts improve context but increase token costs.
  - Blinded evaluation avoids bias but hides model identity until after voting.
- Failure signatures:
  - Retrieval returns no relevant segments → generation fails or hallucinates.
  - Reranker degrades top-100 list → worse generation quality.
  - LLM prompt mismatch → no citations or wrong format.
- First 3 experiments:
  1. Run a topic through the pipeline with BM25 only (no reranker) and inspect the top-20 segments.
  2. Swap RankZephyr for RankGPT and compare citation counts and answer length.
  3. Evaluate the same topic on both Command R+ and GPT-4o and compare answer length and citation density.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal segmentation strategy for RAG systems to maximize answer quality?
- Basis in paper: [inferred] The paper discusses using a sliding window technique with 10 sentences and a stride of 5 sentences, but notes that chunking is a key challenge in RAG and that retrieved chunk representations correlate with RAG answer quality.
- Why unresolved: The paper does not empirically compare different segmentation strategies or provide evidence for the optimality of their chosen approach.
- What evidence would resolve it: Comparative studies of various segmentation strategies (e.g., different window sizes, strides, or chunking algorithms) on downstream RAG performance metrics.

### Open Question 2
- Question: How do different LLM generation models compare in terms of RAG answer quality and citation accuracy?
- Basis in paper: [explicit] The paper provides qualitative analysis showing GPT-4o generates longer, more detailed answers while Command R+ produces shorter, more citation-dense responses, but does not provide quantitative win rates between baselines.
- Why unresolved: The paper only provides a qualitative comparison and mentions that empirical computation of win rates between baselines is left for future work.
- What evidence would resolve it: Quantitative evaluation of multiple LLM models on RAG benchmarks, including metrics for answer quality, citation accuracy, and user preference.

### Open Question 3
- Question: What is the most effective evaluation methodology for RAG systems?
- Basis in paper: [explicit] The paper mentions ongoing work to finalize evaluation methodology using automatic nugget-based evaluation, following earlier work in Lin and Demner-Fushman [30], and notes that nugget-based evaluation is becoming the de facto strategy for RAG evaluation.
- Why unresolved: The paper states that the next phase of efforts will focus on finalizing the evaluation methodology, indicating it is not yet complete.
- What evidence would resolve it: Empirical comparison of different evaluation methodologies (e.g., nugget-based, LLM-as-a-judge, human evaluation) on RAG benchmarks, with analysis of their strengths, weaknesses, and correlation with user satisfaction.

## Limitations

- The paper lacks detailed quantitative evaluation metrics beyond qualitative comparison of GPT-4o and Command R+ outputs.
- Effectiveness of deduplication on downstream RAG quality is asserted but not empirically validated through controlled experiments.
- Framework's generalizability beyond TREC 2024 RAG Track context is assumed but not demonstrated.

## Confidence

- High confidence: The modular pipeline architecture and its design rationale are well-supported by the described implementation details and code availability.
- Medium confidence: The qualitative differences between GPT-4o and Command R+ outputs are clearly documented, though systematic quantitative comparison is missing.
- Low confidence: Claims about the superiority of curated topic sets (TREC-RAGgy, TREC-Researchy) for RAG evaluation require validation through comparative studies with other topic sets.

## Next Checks

1. Conduct ablation studies comparing RAG performance on the deduplicated MS MARCO V2.1 collection versus the original collection to quantify the impact of deduplication on retrieval quality and answer generation.
2. Perform systematic quantitative evaluation of answer quality using established metrics (e.g., answer relevance, citation accuracy, answer length consistency) across multiple LLMs beyond the two presented.
3. Validate the framework's portability by applying it to a different domain corpus (e.g., biomedical or legal documents) and assessing whether the same modular components perform effectively without modification.