---
ver: rpa2
title: Evaluation of Google Translate for Mandarin Chinese translation using sentiment
  and semantic analysis
arxiv_id: '2409.04964'
source_url: https://arxiv.org/abs/2409.04964
tags:
- translation
- translate
- google
- translations
- chapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated Google Translate's accuracy for Mandarin Chinese
  to English translation using sentiment and semantic analysis. The authors compared
  Google Translate outputs against two expert human translations of the classic Chinese
  novel "The True Story of Ah Q" using BERT-based sentiment analysis and MPNet-based
  semantic similarity.
---

# Evaluation of Google Translate for Mandarin Chinese translation using sentiment and semantic analysis

## Quick Facts
- arXiv ID: 2409.04964
- Source URL: https://arxiv.org/abs/2409.04964
- Reference count: 40
- Primary result: Google Translate performs significantly worse than human translators for Mandarin Chinese literary translation in terms of both semantic similarity and sentiment preservation

## Executive Summary
This study evaluates Google Translate's accuracy for Mandarin Chinese to English translation using sentiment and semantic analysis. The authors compared Google Translate outputs against two expert human translations of "The True Story of Ah Q" using BERT-based sentiment analysis and MPNet-based semantic similarity. Results showed that Google Translate struggled with contextual meaning, often mistranslating phrases and failing to capture the intended sentiment. The analysis revealed that Google Translate frequently mistranslated cultural allusions, vernacular expressions, and context-dependent phrases, demonstrating that it cannot yet match human translators in preserving both meaning and sentiment in literary Chinese texts.

## Method Summary
The study obtained the Mandarin Chinese text of "The True Story of Ah Q" and two human expert English translations (Yang Xianyi & Gladys Taylor Yang, and Julia Lovell). Google Translate was used to generate a third English translation. The text was preprocessed by removing chapter numbers and splitting into verses. A BERT model was fine-tuned on the SenWave dataset for sentiment analysis across 9 sentiment categories. MPNet was used for semantic similarity comparison. The study calculated Jaccard similarity scores for sentiment predictions and cosine similarity scores for semantic similarity between all three translations.

## Key Results
- Google Translate's semantic similarity scores averaged 0.577 compared to human translations at 0.653
- Google Translate frequently mistranslated cultural allusions and vernacular expressions
- The mistranslations may be due to lack of contextual significance and historical knowledge of China
- Google Translate inconsistently translated proper nouns and character names across the text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Google Translate lacks contextual and logical significance for translating vernacular Chinese, especially when the text depends on narrative interconnections.
- Mechanism: Google Translate segments and translates sentences independently without leveraging cross-sentence or cross-chapter context, leading to loss of semantic coherence and misinterpretation of idioms, allusions, and character references.
- Core assumption: Contextual coherence is critical for accurate translation of narrative texts.
- Evidence anchors:
  - [abstract] "The analysis revealed that Google Translate frequently mistranslated cultural allusions, vernacular expressions, and context-dependent phrases"
  - [section] "Novella as a type of text depends on contextual connections for accurate translation...Google Translate has the inability to maintain the contextual connections"
  - [corpus] Weak; no direct corpus evidence supporting context dependency in this specific paper.
- Break condition: When the input text is highly self-contained or formal, with minimal reliance on prior context.

### Mechanism 2
- Claim: Google Translate struggles with domain-specific linguistic features such as Chinese idioms (chengyu) and historical allusions.
- Mechanism: Google Translate performs literal character-to-word mapping without recognizing idiomatic or cultural meaning, resulting in translations that preserve form but lose intended meaning.
- Core assumption: Recognition of idiomatic and cultural references requires specialized linguistic knowledge or training data.
- Evidence anchors:
  - [abstract] "The mistranslations may be due to lack of contextual significance and historical knowledge of China"
  - [section] "Google Translate did not convey the intended meaning; it merely translated the allusion from Chinese characters into pinyin"
  - [corpus] Weak; corpus does not provide explicit examples of idiom translation failure.
- Break condition: When the text contains no idioms or cultural references.

### Mechanism 3
- Claim: Google Translate inconsistently translates proper nouns and character names across a text, causing reader confusion.
- Mechanism: Google Translate treats each occurrence of a name independently, applying different translations based on local context or segmentation, without maintaining consistent identity mapping.
- Core assumption: Consistent translation of proper nouns is essential for reader comprehension in narrative texts.
- Evidence anchors:
  - [section] "Zhao Taiye (赵太爷) is one of the characters throughout the whole story, but his name is not translated consistently the same in Google Translate"
  - [abstract] No direct mention of consistency issues.
  - [corpus] Weak; corpus does not discuss consistency of proper noun translation.
- Break condition: When the text uses few or no proper nouns.

## Foundational Learning

- Concept: Semantic similarity measures (cosine similarity, Jaccard similarity)
  - Why needed here: To quantify how closely the meaning of translated verses aligns between Google Translate and human expert versions.
  - Quick check question: What does a cosine similarity score of 0.9 indicate about two translated sentences?

- Concept: Sentiment analysis with multi-label classification
  - Why needed here: To evaluate whether translated texts preserve the emotional tone and nuanced sentiments intended by the original author.
  - Quick check question: Why might a sentence be assigned multiple sentiment labels?

- Concept: N-gram frequency analysis
  - Why needed here: To identify key thematic vocabulary differences between translations and detect stylistic biases.
  - Quick check question: How do bigrams differ from trigrams in revealing text patterns?

## Architecture Onboarding

- Component map: PDF scraping -> Text preprocessing -> Translation API calls -> BERT sentiment model -> MPNet semantic model -> Similarity computation -> Visualization
- Critical path: Text preprocessing -> Translation -> Model inference (sentiment & semantic) -> Similarity scoring -> Result analysis
- Design tradeoffs: BERT fine-tuning on SenWave dataset introduces bias toward COVID-19 related sentiments; MPNet chosen for semantic encoding but may not capture cultural nuance.
- Failure signatures: Inconsistent sentiment labels across chapters; low cosine similarity scores; mistranslation of idioms; inconsistent naming.
- First 3 experiments:
  1. Run sentiment analysis on a small sample chapter and compare label distribution to human annotation.
  2. Compute cosine similarity between human translations only to establish baseline semantic similarity.
  3. Translate a single verse with and without context window to measure impact on translation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do cultural allusions in Mandarin Chinese texts impact translation quality when using Google Translate versus human experts?
- Basis in paper: [explicit] The study highlights that Google Translate mistranslates cultural allusions like "塞翁失马安知非福" and cannot convey their intended meaning.
- Why unresolved: The paper provides examples of mistranslations but does not quantify how often cultural allusions appear in the text or their impact on overall translation quality.
- What evidence would resolve it: A systematic analysis of all cultural allusions in the text and their translation accuracy by Google Translate versus human experts, with metrics on frequency and contextual preservation.

### Open Question 2
- Question: Does Google Translate's performance vary significantly across different types of Mandarin Chinese texts (e.g., novels, newspapers, academic papers)?
- Basis in paper: [inferred] The study uses a single novel, limiting generalizability to other text types.
- Why unresolved: The paper acknowledges this limitation but does not test other text types.
- What evidence would resolve it: Comparative evaluations of Google Translate across multiple Mandarin text genres, with metrics on semantic and sentiment accuracy for each.

### Open Question 3
- Question: How does the choice of sentiment analysis dataset affect the evaluation of translation quality for Mandarin Chinese texts?
- Basis in paper: [explicit] The authors note that the SenWave dataset, which focuses on COVID-19 tweets, may not be appropriate for analyzing sentiments in a century-old novel.
- Why unresolved: The paper does not test alternative sentiment analysis datasets or manually created ones.
- What evidence would resolve it: Testing multiple sentiment analysis datasets (including domain-specific ones) on the same text and comparing their effectiveness in capturing sentiments.

## Limitations
- The study's findings are primarily based on a single literary text, "The True Story of Ah Q," which limits generalizability to other domains or text types.
- The SenWave dataset used for BERT fine-tuning is COVID-19 focused rather than literary, potentially introducing bias in sentiment analysis of classical Chinese literature.
- The semantic similarity analysis using MPNet may not fully capture cultural and historical nuances critical for literary translation evaluation.

## Confidence

- **High Confidence**: The general finding that Google Translate struggles with literary Chinese translation compared to human experts, supported by multiple similarity metrics and sentiment analysis.
- **Medium Confidence**: The specific mechanisms explaining translation failures (contextual loss, idiom mistranslation, naming inconsistency) are well-supported by examples but may not represent all failure modes.
- **Medium Confidence**: The quantitative comparisons between Google Translate and human translations are methodologically sound but limited by the single-source nature of the evaluation corpus.

## Next Checks
1. Test Google Translate against human translations for multiple genres (modern prose, technical documents, poetry) to determine if performance patterns hold across different text types.

2. Develop a systematic error taxonomy and manually annotate translation errors in Google Translate output to quantify the frequency and types of specific translation failures (idioms, allusions, naming consistency).

3. Translate identical sentences both in isolation and within their original narrative context using Google Translate to measure the impact of contextual information on translation quality, directly testing the proposed mechanism of contextual dependency.