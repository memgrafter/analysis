---
ver: rpa2
title: Towards Better Question Generation in QA-based Event Extraction
arxiv_id: '2405.10517'
source_url: https://arxiv.org/abs/2405.10517
tags:
- question
- context
- questions
- event
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning-based question generation
  framework (RLQG) for QA-based event extraction (EE). The method generates context-dependent,
  high-quality questions by combining supervised fine-tuning with inverse prompting
  and question-answering rewards.
---

# Towards Better Question Generation in QA-based Event Extraction

## Quick Facts
- **arXiv ID**: 2405.10517
- **Source URL**: https://arxiv.org/abs/2405.10517
- **Authors**: Zijin Hong; Jian Liu
- **Reference count**: 7
- **Primary result**: RLQG achieves 2.69% and 1.96% improvements in exact match accuracy over previous methods on ACE and RAMS benchmarks respectively

## Executive Summary
This paper addresses the challenge of generating high-quality context-dependent questions for QA-based event extraction (EE). The proposed RLQG framework combines supervised fine-tuning (SFT) with reinforcement learning (RL) refinement using inverse prompting and question-answering rewards. The method demonstrates significant improvements over existing approaches, particularly in data-scarce scenarios where it achieves comparable performance with only 40% of training data. The approach shows robustness across different QA models and achieves good performance with minimal manual intervention.

## Method Summary
RLQG employs a two-stage training process: first, supervised fine-tuning on template questions using a sequence-to-sequence framework, then reinforcement learning refinement with proximal policy optimization (PPO). The RL stage uses two reward components: inverse prompting (measuring context recovery from generated questions) and QA rewards (measuring how well questions elicit correct answers). The framework generates context-dependent questions by taking event triggers and roles as input, augmented with beam search for diversity.

## Key Results
- RLQG outperforms previous methods by 2.69% and 1.96% in exact match accuracy on ACE and RAMS benchmarks respectively
- In data-scarce scenarios, RLQG matches full-data performance with only 40% of training data
- The approach shows robustness across different QA models (GPT-3.5-turbo and LLaMA-2-13b-chat) with only 2.27% performance difference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLQG's inverse prompting mechanism improves question quality by ensuring context dependence
- Mechanism: The inverse prompting model attempts to recover the original context from the generated question. Questions that enable better context recovery are considered more context-dependent and thus of higher quality
- Core assumption: A question that can help recover the original context must be more context-dependent than one that cannot
- Evidence anchors:
  - [abstract] "we developed a Reinforcement Learning-based Question Generation framework, RLQG to refine the generation process... an inverse prompting mechanism is proposed to evaluate whether the question matches the context"
  - [section 3.2] "we developed an inverse prompting model to achieve context recovery... we utilize semantic similarity (SemSim) to evaluate the quality of recovery, which provides an inverse prompting reward for further use"
  - [corpus] Weak evidence - only 5 related papers found, none directly discussing inverse prompting for question generation

### Mechanism 2
- Claim: QA reward mechanism ensures questions provide clear guidance to QA models
- Mechanism: Questions that successfully guide a QA model to produce correct answers receive higher rewards, encouraging the generation of more indicative questions
- Core assumption: A good question for event extraction should be able to elicit the correct answer from a QA model
- Evidence anchors:
  - [abstract] "a question-answering reward is used to quantify the degree to which the question is indicative"
  - [section 3.2] "a good question should successfully guide a QA model to yield a correct answer. Therefore, for each candidate question... we use the context overlap ratio (COR) to evaluate the predicted answer"
  - [corpus] Weak evidence - corpus neighbors don't directly address QA reward mechanisms

### Mechanism 3
- Claim: Combining SFT with RL refinement allows learning from both annotated data and dynamic question evaluation
- Mechanism: SFT provides a starting point from template questions, while RL refinement using inverse prompting and QA rewards further improves question quality beyond what templates alone can achieve
- Core assumption: The template-based SFT provides reasonable initial questions that can be improved through RL, and the RL rewards are informative enough to guide improvement
- Evidence anchors:
  - [abstract] "we utilize a general sequence-to-sequence framework as the backbone... we use template questions as the targets for learning"
  - [section 3.2] "Typically, RL fine-tuning is employed subsequent to supervised fine-tuning for further refinement"
  - [section 5.2] "our method outperforms the SFT method in data scarcity, especially when the data is limited to around 40% to 60%"

## Foundational Learning

- **Concept**: Supervised Fine-Tuning (SFT)
  - Why needed here: Provides a starting point for the question generation model using template questions before RL refinement
  - Quick check question: What is the difference between training with cross-entropy loss (SFT) versus maximizing reward signals (RL)?

- **Concept**: Reinforcement Learning with PPO
  - Why needed here: Enables refinement of question quality based on composite rewards (inverse prompting + QA rewards) rather than just supervised targets
  - Quick check question: How does PPO's clipped objective help stabilize training compared to vanilla policy gradient methods?

- **Concept**: Inverse Prompting
  - Why needed here: Provides a way to evaluate whether generated questions contain sufficient context information by attempting to recover the original context
  - Quick check question: What metric is used to evaluate the quality of context recovery in the inverse prompting mechanism?

## Architecture Onboarding

- **Component map**: Question Generation Model (SFT + RL) → Question Answering Model → Evaluation (EM/COR/SemSim)
- **Critical path**: Context + Trigger + Role → QG Model → Question → QA Model → Answer
- **Design tradeoffs**: Using off-the-shelf QA models vs. training specialized QA models for EE
- **Failure signatures**: Poor question quality manifests as low EM scores, questions that don't match context, or questions that fail to elicit correct answers
- **First 3 experiments**:
  1. Test baseline template question generation (Simple-Q, Standard-Q) on ACE dataset
  2. Implement and test SFT model using Dynamic-Q templates as targets
  3. Add inverse prompting reward component and evaluate impact on question quality metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: How would incorporating event detection (ED) into the QA-based event extraction pipeline affect the performance of RLQG, particularly in real-world scenarios where triggers are not pre-identified?
  - Basis in paper: [explicit] The authors acknowledge that most QA-based EE approaches assume known triggers and plan to incorporate ED in future work for a comprehensive approach to EE.
  - Why unresolved: The paper does not evaluate the method's performance when triggers are not pre-identified, leaving a gap in understanding its real-world applicability.
  - What evidence would resolve it: Experiments comparing RLQG's performance with and without pre-identified triggers, using datasets where triggers must be detected, would provide insights into the impact of incorporating ED.

- **Open Question 2**: To what extent does the quality of template questions influence the effectiveness of the supervised fine-tuning (SFT) stage, and how can this be optimized for different event types and roles?
  - Basis in paper: [explicit] The authors discuss that the quality of template questions determines the ability of the SFT model for question generation, and higher quality templates improve SFT performance.
  - Why unresolved: While the paper uses different templates, it does not systematically analyze how variations in template quality affect the downstream performance of RLQG across diverse event types and roles.
  - What evidence would resolve it: A study comparing RLQG's performance using various template qualities and complexities, tailored to specific event types and roles, would clarify the optimal template design for SFT.

- **Open Question 3**: How does RLQG perform when evaluated on real-world data, which is often more complex, diverse, and noisy compared to standard benchmarks like ACE and RAMS?
  - Basis in paper: [explicit] The authors acknowledge that the method's generalizability to real-world scenarios remains to be determined, as it has only been evaluated on standard datasets.
  - Why unresolved: The paper does not provide empirical evidence of RLQG's effectiveness on real-world data, which may contain complexities not present in curated benchmarks.
  - What evidence would resolve it: Applying RLQG to real-world event extraction tasks, such as extracting events from social media or news articles, and comparing its performance to existing methods would demonstrate its practical applicability.

- **Open Question 4**: How sensitive is RLQG's performance to the choice of hyperparameters, such as the beam size in beam search augmentation and the regularization parameter in the reinforcement learning objective?
  - Basis in paper: [inferred] The authors mention using specific hyperparameter values (e.g., beam size of 10, regularization parameter µ > 0) but do not explore the sensitivity of the model's performance to these choices.
  - Why unresolved: Without analyzing hyperparameter sensitivity, it is unclear how robust RLQG is to different configurations and whether optimal performance requires extensive tuning.
  - What evidence would resolve it: Conducting a systematic hyperparameter search, varying values for beam size, regularization parameter, and other relevant hyperparameters, and analyzing their impact on RLQG's performance would provide insights into its sensitivity and robustness.

## Limitations

- The inverse prompting mechanism relies on ChatGPT-generated training pairs, raising concerns about reproducibility and consistency across implementations
- The composite reward function combining inverse prompting and QA rewards is not fully specified, making optimal weightings unclear
- The method has only been evaluated on standard benchmarks (ACE and RAMS) and its generalizability to real-world, more complex data remains unproven

## Confidence

- **High confidence**: The general RLQG framework architecture and its components (SFT + RL refinement) are well-described and implementable
- **Medium confidence**: The effectiveness claims are supported by EM score improvements, but the relative contribution of each mechanism (inverse prompting vs QA rewards) is not clearly isolated
- **Low confidence**: The inverse prompting mechanism's implementation details and the exact reward function composition are underspecified

## Next Checks

1. Implement ablation studies to isolate the impact of inverse prompting rewards versus QA rewards on final question quality
2. Test the inverse prompting mechanism's reliability by evaluating context recovery quality on a held-out validation set with human evaluation
3. Compare performance across different base language models (LLaMA vs GPT family) to assess the framework's generalizability beyond the specific models used in the paper