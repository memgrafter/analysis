---
ver: rpa2
title: 'Position: Theory of Mind Benchmarks are Broken for Large Language Models'
arxiv_id: '2412.19726'
source_url: https://arxiv.org/abs/2412.19726
tags:
- theory
- mind
- action
- large
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that current LLM theory of mind benchmarks are\
  \ flawed because they measure only literal theory of mind\u2014the ability to predict\
  \ others\u2019 actions\u2014while ignoring functional theory of mind, the ability\
  \ to adapt behavior in response to others\u2019 actions. The authors demonstrate\
  \ this gap using simple matrix games (Rock, Paper, Scissors, Iterated Battle of\
  \ Sexes, Iterated Prisoner\u2019s Dilemma) against both fixed and adaptive partners."
---

# Position: Theory of Mind Benchmarks are Broken for Large Language Models

## Quick Facts
- arXiv ID: 2412.19726
- Source URL: https://arxiv.org/abs/2412.19726
- Authors: Matthew Riemer; Zahra Ashktorab; Djallel Bouneffouf; Payel Das; Miao Liu; Justin D. Weisz; Murray Campbell
- Reference count: 40
- Primary result: Current LLM theory of mind benchmarks measure only literal ToM (predicting others' actions) while ignoring functional ToM (adapting behavior based on those predictions)

## Executive Summary
This paper argues that existing LLM theory of mind benchmarks are fundamentally flawed because they conflate literal theory of mind (predicting others' actions) with functional theory of mind (adapting behavior based on those predictions). Through experiments with matrix games against both fixed and adaptive partners, the authors demonstrate that several open-source LLMs can accurately predict opponents' actions but fail to act rationally based on these predictions. The gap persists even when actual actions are provided as input, suggesting current benchmarks overstate LLM capabilities for real-world multi-agent interactions.

## Method Summary
The authors test various open-source LLMs (LLAMA-2, LLAMA-3, Mixtral, Mistral Large 2, DeepSeek-R1) across three matrix games (Rock, Paper, Scissors, Iterated Battle of Sexes, Iterated Prisoner's Dilemma) using different prompting strategies. They measure both literal theory of mind accuracy (predicting opponents' actions) and functional theory of mind performance (adapting behavior based on predictions) through metrics like regret per step. The experiments include scenarios with fixed single-action opponents and adaptive tit-for-tat opponents, with some tests using oracle inputs where actual opponent actions are provided directly.

## Key Results
- LLMs show high literal ToM accuracy (predicting others' actions) but poor functional ToM performance (acting rationally based on predictions)
- The performance gap persists even when actual opponent actions are provided as oracle inputs
- Long interaction histories impair LLM reasoning, causing suboptimal decisions despite accurate predictions
- Different prompting strategies (LM vs QA vs CoT) significantly affect the alignment between literal and functional performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail functional theory of mind because they do not use predictions of others' behavior to rationally adapt their own actions.
- Mechanism: LLMs can model literal theory of mind (predicting others' actions) but lack a separate reasoning module that translates these predictions into strategic responses. Without this link, high literal accuracy does not imply effective functional performance.
- Core assumption: A model can predict behavior without integrating that prediction into its own decision-making process.
- Evidence anchors:
  - [abstract] "strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance"
  - [section] "the LLM still does not generate rational responses to these predicted actions" (Section 4.1)
  - [corpus] Weak - no direct citations in related work about this mechanism; needs experimental validation
- Break condition: If future models explicitly train a joint literal-to-functional mapping, the gap could close.

### Mechanism 2
- Claim: Long interaction histories impair LLM reasoning because context length exceeds effective reasoning capacity.
- Mechanism: As the number of interaction steps grows, the complexity of reasoning over the full history overwhelms the LLM's in-context learning, causing suboptimal decisions even when literal predictions are accurate.
- Core assumption: Reasoning quality degrades with history length beyond a threshold.
- Evidence anchors:
  - [abstract] "LLMs still struggle to reason effectively over long interaction histories"
  - [section] "Gap Remains with Literal Theory of Mind Inputs" and "Gap Remains with Oracle Inputs" (Section 4.2)
  - [corpus] Weak - corpus neighbors do not discuss history length; missing direct evidence
- Break condition: If models are given explicit memory or summarization mechanisms, the long-context bottleneck may be alleviated.

### Mechanism 3
- Claim: The framing of prompts (e.g., QA vs LM vs CoT) strongly affects the alignment between literal and functional performance.
- Mechanism: Different prompting strategies shift the LLM's internal processing from prediction to decision-making in varying degrees, altering how literal predictions influence final actions. QA framing pushes closer to decision-making, sometimes reducing literal accuracy but improving functional alignment.
- Core assumption: Prompt format controls the degree of integration between prediction and decision.
- Evidence anchors:
  - [abstract] "we report the accuracy of the literal theory of mind predictions with respect to individual actions as ToM %"
  - [section] "Adding the QA format seems to make literal theory of mind a lot worse by taking the input distribution further from the LLM task and closer to the kind of processing needed for actual decision making" (Section 4.1)
  - [corpus] Weak - corpus neighbors focus on ToM benchmarks but not prompt framing effects
- Break condition: If a unified prompt strategy is found that preserves both literal accuracy and functional alignment, this mechanism's variability would be reduced.

## Foundational Learning

- Concept: Distinction between literal and functional theory of mind
  - Why needed here: The paper's central claim is that existing benchmarks conflate these two, leading to misleading conclusions about LLM capabilities.
  - Quick check question: Can you describe a scenario where an agent predicts another's action correctly but still acts irrationally?

- Concept: Partially Observable Stochastic Games (POSGs)
  - Why needed here: The paper models multi-agent interaction as POSGs to formalize the interaction histories and reward structures used in experiments.
  - Quick check question: In a POSG, what constitutes an agent's observation versus its action?

- Concept: Prompt engineering and its impact on LLM outputs
  - Why needed here: The experiments test multiple prompting strategies to show how literal predictions translate (or fail to translate) into functional behavior.
  - Quick check question: How might a QA prompt versus a Chain-of-Thought prompt affect an LLM's decision-making in a repeated game?

## Architecture Onboarding

- Component map:
  - Literal ToM predictor: Generates predictions about opponent actions
  - Functional ToM executor: Uses predictions to choose rational actions
  - History encoder: Converts interaction sequences into token representations
  - Prompt generator: Formats input prompts for the LLM
  - Evaluation module: Computes regret and accuracy metrics

- Critical path:
  1. Record interaction history
  2. Encode history into tokens
  3. Generate literal ToM predictions
  4. Apply prompting strategy
  5. Generate action
  6. Execute and record outcome
  7. Update history

- Design tradeoffs:
  - Context length vs. reasoning depth: Longer histories provide more information but may overwhelm the LLM
  - Prompt simplicity vs. performance: Simple LM prompts may yield higher literal accuracy but worse functional alignment
  - Oracle input vs. prediction input: Oracle inputs test upper bounds but may not reflect real-world use

- Failure signatures:
  - High literal accuracy but high regret → gap between prediction and action selection
  - Consistent action patterns regardless of opponent → lack of adaptation
  - Degradation over long histories → context overload

- First 3 experiments:
  1. Replicate Rock, Paper, Scissors against fixed single-action opponents to measure literal vs functional gaps
  2. Test prompt framing (LM vs QA vs CoT) on the same game to observe impact on alignment
  3. Use oracle action input to determine if reasoning or prediction is the bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can functional theory of mind be reliably measured in more complex real-world interactive tasks beyond matrix games?
- Basis in paper: [explicit] The authors advocate for benchmarks that directly assess functional theory of mind to better reflect real-world multi-agent interaction needs, but note current simple matrix game benchmarks may not fully capture real-world complexity.
- Why unresolved: Current benchmarks like matrix games only test very simple partner policies (single action or tit-for-tat), while real-world interactions involve diverse personas and social contexts that require more nuanced adaptation.
- What evidence would resolve it: Development and validation of new benchmarks that measure functional theory of mind in complex social games (like Codenames, Hanabi, Taboo, Wavelength) where success requires adapting to specific partners' behaviors and reasoning patterns over extended interactions.

### Open Question 2
- Question: What architectural or training modifications could enable LLMs to achieve strong functional theory of mind without sacrificing literal theory of mind performance?
- Basis in paper: [explicit] The authors show that even when literal theory of mind predictions are provided as input (oracle setting), LLMs still struggle to reason effectively over long interaction histories, suggesting a fundamental gap in current architectures.
- Why unresolved: The paper demonstrates that current LLMs can predict opponents' actions well but fail to act rationally based on these predictions, even with oracle inputs. This suggests limitations in how LLMs process and utilize theory of mind information.
- What evidence would resolve it: Development of LLM architectures or training approaches that successfully close the gap between literal and functional theory of mind performance, demonstrated through consistent high performance on both metrics across multiple game types and partner policies.

### Open Question 3
- Question: Does the training process for reasoning-focused models like DeepSeek-R1 fundamentally alter their ability to perform literal theory of mind tasks?
- Basis in paper: [explicit] The authors observe that DeepSeek-R1 demonstrates strong functional theory of mind performance that paradoxically exceeds its literal theory of mind capabilities, suggesting a loss of prediction abilities during training.
- Why unresolved: This finding challenges the assumption that functional theory of mind must be built upon literal theory of mind capabilities, but the mechanism behind this phenomenon remains unclear.
- What evidence would resolve it: Detailed analysis of how reasoning-focused training affects the internal representations and capabilities of LLMs, particularly regarding their ability to predict versus reason about others' actions in multi-agent contexts.

## Limitations

- The distinction between literal and functional theory of mind, while intuitively appealing, lacks robust empirical validation across diverse LLM architectures and training paradigms.
- The experiments focus on a limited set of matrix games, which may not generalize to more complex multi-agent scenarios.
- The prompt engineering effects, while observed, are not fully explained mechanistically - it's unclear whether these effects are consistent across different model families or prompt strategies.

## Confidence

- **High Confidence**: The empirical observation that LLMs show high literal ToM accuracy but poor functional ToM performance is well-supported by the experimental results across multiple games and models.
- **Medium Confidence**: The mechanism that LLMs lack a separate reasoning module to translate predictions into actions is plausible but not definitively proven; alternative explanations (like context length limitations) remain viable.
- **Low Confidence**: The claim that current benchmarks are fundamentally broken for all LLM evaluation purposes is overstated; while the paper demonstrates important limitations, the benchmarks may still be useful for specific narrow purposes.

## Next Checks

1. **Cross-Model Validation**: Test the literal vs functional ToM gap across a broader range of LLM architectures (including proprietary models like GPT-4 and Claude) to determine if the observed pattern is universal or model-specific.

2. **Prompt Strategy Analysis**: Systematically vary prompt strategies across the same models to isolate whether the literal/functional gap is primarily driven by prompt format or by deeper architectural limitations.

3. **Complex Game Scenarios**: Extend the evaluation to more complex multi-agent games (like poker variants or negotiation scenarios) to test whether the observed limitations persist in richer interaction spaces.