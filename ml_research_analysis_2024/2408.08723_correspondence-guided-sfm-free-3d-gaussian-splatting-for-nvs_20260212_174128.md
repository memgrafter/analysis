---
ver: rpa2
title: Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS
arxiv_id: '2408.08723'
source_url: https://arxiv.org/abs/2408.08723
tags:
- pose
- camera
- gaussians
- poses
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses novel view synthesis (NVS) without pre-processed
  camera poses, known as SfM-free methods, which are crucial for rapid response capabilities
  and robustness against variable conditions. The proposed method, CG-3DGS, integrates
  correspondence detection with 3D Gaussian splatting to achieve better pixel alignment
  and stable optimization.
---

# Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS

## Quick Facts
- arXiv ID: 2408.08723
- Source URL: https://arxiv.org/abs/2408.08723
- Authors: Wei Sun; Xiaosong Zhang; Fang Wan; Yanzhao Zhou; Yuan Li; Qixiang Ye; Jianbin Jiao
- Reference count: 15
- One-line primary result: Achieves superior NVS performance without pre-processed camera poses using correspondence-guided pose optimization

## Executive Summary
This paper addresses the challenge of novel view synthesis (NVS) without pre-processed camera poses, known as SfM-free methods. The proposed method, CG-3DGS, integrates correspondence detection with 3D Gaussian splatting to achieve better pixel alignment and stable optimization. By using correspondence-guided pose optimization to estimate relative camera poses between frames and an approximated surface rendering technique to propagate gradients from 2D to 3D space, the method demonstrates superior performance and time efficiency compared to state-of-the-art baselines on datasets like Tanks and Temples and CO3D-V2.

## Method Summary
The method employs a two-step optimization pipeline for SfM-free NVS. First, it estimates relative camera poses by optimizing SE-3 transformations between adjacent frames using correspondence-based loss, which reduces gradient instability from pixel misalignment. Second, it fixes these learned poses and optimizes the 3D Gaussian representation using standard 3DGS training with photometric loss. The approach uses monocular depth estimates for point cloud initialization, LoFTR/Flow for 2D correspondence detection, and an approximated surface renderer to enable differentiable gradient flow from 2D screen-space to 3D Gaussian parameters.

## Key Results
- Achieves state-of-the-art NVS performance on Tanks and Temples and CO3D-V2 datasets
- Demonstrates significant improvements in PSNR, SSIM, and LPIPS metrics
- Provides time efficiency benefits over traditional SfM-based approaches
- Shows robust performance across varying scene complexities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correspondence-based loss reduces gradient instability caused by per-pixel misalignment in SfM-free NVS.
- Mechanism: Instead of computing L2 loss on all pixels regardless of alignment, the method detects 2D correspondences between the rendered image and the target, then propagates gradients only through aligned pixel pairs. This ensures that gradient signals are meaningful and avoids excessive gradients from misaligned regions.
- Core assumption: Misalignment between rendered and target images is the primary cause of unstable gradients when using per-pixel losses.
- Evidence anchors:
  - [abstract] "Inaccurate initial poses lead to misalignment issue, which, under the constraints of per-pixel image loss functions, results in excessive gradients, causing unstable optimization and poor convergence for NVS."
  - [section] "We use correspondences between the target and the rendered result to achieve better pixel alignment, facilitating the optimization of relative poses between frames."
  - [corpus] Weak or missing - no direct evidence in neighboring papers about correspondence-based loss for SfM-free NVS.
- Break condition: If correspondence detection fails to find meaningful matches due to large viewpoint changes or textureless regions, the alignment benefit disappears and gradient instability returns.

### Mechanism 2
- Claim: Approximated surface rendering enables differentiable gradient flow from 2D screen-space to 3D Gaussian parameters without explicit surface reconstruction.
- Mechanism: The method uses alpha-blending depth and weighted center positions of overlapping Gaussians to approximate a 3D surface point for each screen-space pixel. This allows gradients to propagate from 2D pixel differences back to 3D Gaussian positions, scales, and colors.
- Core assumption: The alpha-blending weighted average provides a reasonable approximation of the true surface location for gradient purposes.
- Evidence anchors:
  - [section] "We employ a differentiable approximate surface renderer... to render the screen-space coordinates at k'(i)... as q(k'(i)). The resulting loss function is expressed as..."
  - [section] "According to previous studies... the depth of an expected 3D surface point D(k) relative to a 2D screen-space point k is computed as follows: D(k) = Σi diαi(k) / Σj(1 - αj(k))"
- Break condition: If the approximation poorly represents the true surface (e.g., in thin structures or highly overlapping Gaussians), gradient signals may point in wrong directions, degrading optimization.

### Mechanism 3
- Claim: Two-step optimization (pose estimation then scene optimization) separates camera motion learning from scene geometry learning, improving stability.
- Mechanism: First, the method estimates relative camera poses by optimizing SE-3 transformations between adjacent frames using correspondence-based loss. Second, it fixes these poses and optimizes the 3D Gaussian representation using standard 3DGS training with photometric loss.
- Core assumption: Camera pose estimation and scene geometry optimization interfere with each other when optimized jointly, and separating them improves convergence.
- Evidence anchors:
  - [section] "We construct a two-step optimization pipeline: (i) We initialize an auxiliary 3D Gaussian set... Our goal is to learn an affine transformation... (ii). We initialize another 3D Gaussians set, where we perform scene optimization with all the frames and their corresponding learned camera poses."
  - [section] "During this optimization phase, we preserve the attributes of the pretrained 3D Gaussians Gt* unchanged to distinctly separate the effects of camera motion from changes..."
- Break condition: If initial pose estimates are too inaccurate, errors propagate to the scene optimization phase, limiting final quality regardless of the separation.

## Foundational Learning

- Concept: Structure-from-Motion (SfM) and its limitations
  - Why needed here: Understanding why SfM-free methods are valuable and what problems they solve (time consumption, failure on textureless scenes)
  - Quick check question: What are the two main drawbacks of using SfM as a preprocessing step for NVS methods like NeRF and 3DGS?

- Concept: 3D Gaussian Splatting representation and rendering
  - Why needed here: The paper builds directly on 3DGS, so understanding how Gaussians are represented (position, scale, rotation, color, opacity) and rendered (alpha-blending, projection) is essential
  - Quick check question: How does 3DGS approximate the projection of a 3D Gaussian onto the 2D image plane using the Jacobian?

- Concept: Differentiable rendering and gradient propagation
  - Why needed here: The core innovation involves making the correspondence detection and surface approximation differentiable to enable end-to-end training
  - Quick check question: In the context of 3DGS, what is the role of the Jacobian matrix in making rendering differentiable?

## Architecture Onboarding

- Component map: Monocular depth network (DPT/Zoe) -> Point cloud initialization -> 3D Gaussian initialization -> Correspondence detector (LoFTR/Flow) -> 2D-2D matches -> Approximated surface renderer -> 3D surface point estimation -> 3D Gaussian representation -> Scene geometry and appearance -> SE-3 transformation module -> Relative pose estimation -> 3DGS renderer -> Image synthesis -> Loss functions (Lcor-rgb, Lpix-rgb, Lcor-depth) -> Training signals

- Critical path:
  1. Depth estimation → Point cloud → 3D Gaussian initialization
  2. Correspondence detection between rendered and target frames
  3. Approximated surface rendering to link 2D pixels to 3D Gaussians
  4. Gradient propagation through correspondence-based loss
  5. SE-3 transformation optimization for pose estimation
  6. Fixed pose scene optimization with standard 3DGS training

- Design tradeoffs:
  - Correspondence detection adds computation but improves alignment and stability
  - Approximated surface rendering avoids expensive SDF reconstruction but may introduce approximation errors
  - Two-step optimization simplifies the problem but may propagate initial pose errors
  - Using monocular depth instead of SfM points avoids preprocessing but may have lower accuracy

- Failure signatures:
  - Poor correspondence matches → Gradient signals become noisy or incorrect
  - Overlapping Gaussians with similar alpha values → Surface approximation becomes unstable
  - Large camera motions between frames → Monocular depth initialization may be inaccurate
  - Textureless regions → Correspondence detection fails, leading to misalignment

- First 3 experiments:
  1. Verify correspondence detection works on simple synthetic data with known ground truth poses and correspondences
  2. Test approximated surface rendering by comparing estimated surface points against ground truth SDF values on a simple mesh
  3. Run ablation study comparing per-pixel loss vs correspondence-based loss on a simple scene with known misalignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with increasing scene complexity and number of frames?
- Basis in paper: [inferred] The paper mentions evaluation on Tanks and Temples and CO3D-V2 datasets, but does not extensively explore scalability with varying scene complexity or frame counts.
- Why unresolved: The paper does not provide a detailed analysis of how the method's performance is affected by increasing scene complexity or the number of frames, which is crucial for understanding its practical applicability.
- What evidence would resolve it: Conducting experiments with scenes of varying complexity and different numbers of frames, and analyzing the performance metrics (PSNR, SSIM, LPIPS) to determine scalability trends.

### Open Question 2
- Question: How robust is the method to variations in camera intrinsics and different types of camera motion?
- Basis in paper: [inferred] The paper discusses the method's performance on datasets with specific camera motions but does not explicitly test robustness to variations in camera intrinsics or different types of camera motion.
- Why unresolved: The paper does not provide evidence on how the method handles different camera intrinsics or various types of camera motion, which is important for real-world applications.
- What evidence would resolve it: Testing the method on datasets with varying camera intrinsics and different types of camera motion (e.g., rotational, translational) and evaluating its performance to determine robustness.

### Open Question 3
- Question: What are the computational and memory requirements of the method, and how do they compare to other state-of-the-art methods?
- Basis in paper: [explicit] The paper mentions that the method is efficient but does not provide a detailed comparison of computational and memory requirements with other methods.
- Why unresolved: The paper does not provide a comprehensive analysis of the computational and memory requirements, which is essential for understanding the method's efficiency in practical scenarios.
- What evidence would resolve it: Conducting a detailed analysis of the computational and memory requirements of the method and comparing them with other state-of-the-art methods to provide a clear understanding of its efficiency.

## Limitations
- Performance on highly dynamic scenes or extreme lighting conditions is not demonstrated
- Memory and computational overhead of correspondence detection is not quantified relative to performance gains
- Approximation quality of the surface renderer and its impact on gradient accuracy remains unverified through ablation studies

## Confidence
- **High**: The two-step optimization framework is clearly described and logically sound
- **Medium**: The correspondence-guided loss mechanism is well-motivated but relies on approximation quality
- **Medium**: The approximated surface rendering approach is theoretically justified but not empirically validated against ground truth

## Next Checks
1. Perform controlled experiments varying initial pose errors to measure sensitivity of the correspondence-based optimization to initialization quality
2. Conduct ablation studies comparing different correspondence detection methods (LoFTR vs Flow) to isolate their impact on performance
3. Test the method on datasets with known challenging scenarios (textureless regions, large viewpoint changes) to identify failure modes