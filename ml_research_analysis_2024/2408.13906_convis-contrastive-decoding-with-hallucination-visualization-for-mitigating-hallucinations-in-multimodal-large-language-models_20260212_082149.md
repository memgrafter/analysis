---
ver: rpa2
title: 'ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating
  Hallucinations in Multimodal Large Language Models'
arxiv_id: '2408.13906'
source_url: https://arxiv.org/abs/2408.13906
tags:
- image
- hallucinations
- generated
- images
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in Multimodal
  Large Language Models (MLLMs), where generated responses fail to accurately reflect
  the given image. The proposed ConVis method is a training-free contrastive decoding
  approach that leverages a text-to-image (T2I) generation model to semantically reconstruct
  the given image from hallucinated captions.
---

# ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2408.13906
- Source URL: https://arxiv.org/abs/2408.13906
- Authors: Yeji Park; Deokyeong Lee; Junsuk Choe; Buru Chang
- Reference count: 12
- One-line primary result: ConVis reduces hallucinations in MLLMs through training-free contrastive decoding without requiring additional data or model updates

## Executive Summary
This paper addresses the critical problem of hallucinations in Multimodal Large Language Models (MLLMs), where generated responses fail to accurately reflect the given image. The proposed ConVis method is a training-free contrastive decoding approach that leverages text-to-image generation to semantically reconstruct images from hallucinated captions, enabling MLLMs to capture visual contrastive signals that penalize hallucination generation. By comparing probability distributions between original and reconstructed images, ConVis identifies and mitigates hallucinated content during the decoding process.

The method operates purely within the decoding process, eliminating the need for additional data or model updates. Extensive experiments on five popular benchmarks demonstrate that ConVis effectively reduces hallucinations across various MLLMs while maintaining overall response generation performance, offering a practical solution to a persistent challenge in multimodal AI systems.

## Method Summary
ConVis is a training-free contrastive decoding method that mitigates hallucinations in MLLMs by leveraging text-to-image generation for hallucination visualization. The approach generates a caption using the MLLM, reconstructs an image using a T2I model based on this caption, and compares probability distributions between the original and reconstructed images. Tokens associated with hallucinations show amplified contrastive logit distributions, allowing for their penalization during generation. The method employs Nucleus sampling to generate diverse captions, creating multiple reconstructed images that enhance hallucination detection robustness. ConVis operates entirely within the decoding process without requiring additional training data or model modifications.

## Key Results
- ConVis achieves significant hallucination reduction across five benchmarks (CHAIR, HallusionBench, POPE, MME, and LLaVA-Bench)
- The method effectively reduces hallucinations in multiple MLLMs including LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2
- ConVis maintains overall response generation performance while improving visual accuracy
- Higher-quality T2I models (measured by CLIPScore) correlate with better hallucination mitigation performance
- Nucleus sampling outperforms greedy decoding for caption generation, with improved performance as the number of generated images increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual contrastive signals from hallucination visualization can effectively penalize hallucinated tokens during decoding
- Mechanism: The method generates a caption using the MLLM, reconstructs an image using a T2I model based on this caption, and compares probability distributions between the original and reconstructed images. Tokens associated with hallucinations show amplified contrastive logit distributions, allowing for their penalization during generation.
- Core assumption: The T2I model faithfully visualizes hallucinations present in the caption, creating visual discrepancies that can be captured as contrastive signals
- Evidence anchors:
  - [abstract] "ConVis leverages a text-to-image (T2I) generation model to semantically reconstruct the given image from hallucinated captions. By comparing the contrasting probability distributions produced by the original and reconstructed images, ConVis enables MLLMs to capture visual contrastive signals that penalize hallucination generation."
  - [section] "If the generated caption contains hallucinations (e.g., a book'), there will be visual discrepancies between the original and reconstructed images (e.g., a missing clock'). ConVis then uses the original and reconstructed images to compare the probability distributions, capturing visual contrast signals that highlight hallucinations."
  - [corpus] Weak - The corpus contains related papers but lacks direct evidence for this specific hallucination visualization mechanism
- Break condition: The T2I model fails to faithfully visualize hallucinations, or the visual discrepancies do not translate into meaningful contrastive signals for the MLLM

### Mechanism 2
- Claim: Increasing caption and image diversity through Nucleus sampling improves hallucination mitigation
- Mechanism: Instead of using greedy decoding, the method generates diverse captions using Nucleus sampling, which are then used to generate multiple images. This approach increases coverage of potential hallucinations and enhances robustness against T2I model misalignment.
- Core assumption: Diverse captions generated through Nucleus sampling capture a broader range of potential hallucinations than greedy decoding
- Evidence anchors:
  - [section] "To address this limitation, we increase the diversity of the generated images using the following approaches: (1) We first generate a diverse set of n captions using Nucleus Decoding instead of Greedy Decoding. (2) Then, the T2I model uses these n captions to generate n corresponding images."
  - [section] "Nucleus sampling outperforms Greedy search, demonstrating its potential to generate more diverse captions."
  - [section] "Furthermore, in Figure 4, we investigate how the number of generated images from different captions using Nucleus sampling affects CHAIR scores. We observe that as the number of images n increases, both CHAIR S and CHAIR I scores improve."
  - [corpus] Missing - No direct corpus evidence for this specific diversity enhancement mechanism
- Break condition: Nucleus sampling fails to generate meaningfully diverse captions, or the additional computational cost outweighs the benefits

### Mechanism 3
- Claim: Higher-quality T2I models produce better-aligned images that more effectively mitigate hallucinations
- Mechanism: The method uses T2I models with higher CLIPScore values (indicating better image-text alignment) to generate reconstructed images. Better alignment between generated images and captions leads to more effective contrastive signals for hallucination mitigation.
- Core assumption: T2I models with higher CLIPScores generate images that more faithfully represent the input captions
- Evidence anchors:
  - [section] "To investigate the impact of generated image quality on hallucination mitigation, we evaluate the performance of our method using various text-to-image (T2I) models."
  - [section] "The results indicate a clear trend: as the CLIPScore improves, so does the CHAIR score."
  - [section] "These findings suggest that using higher-quality T2I models, which are better aligned with the original captions, can more effectively mitigate hallucination issues."
  - [corpus] Weak - The corpus contains related papers but lacks direct evidence for the relationship between T2I model quality and hallucination mitigation effectiveness
- Break condition: CLIPScore does not correlate with hallucination mitigation effectiveness, or higher-quality T2I models introduce other issues that negate their benefits

## Foundational Learning

- Concept: Contrastive learning and decoding strategies
  - Why needed here: The method relies on comparing probability distributions between original and reconstructed images to identify and penalize hallucinated tokens
  - Quick check question: How does contrastive decoding differ from traditional decoding methods in language models?

- Concept: Text-to-image generation and image-text alignment metrics
  - Why needed here: The method uses T2I models to visualize hallucinations, and understanding how these models align generated images with input text is crucial for implementation
  - Quick check question: What does CLIPScore measure, and why is it relevant for evaluating T2I model performance?

- Concept: Multimodal model architectures and hallucination issues
  - Why needed here: Understanding how MLLMs process visual and textual information and why hallucinations occur is essential for implementing effective mitigation strategies
  - Quick check question: What are the primary causes of hallucinations in multimodal large language models?

## Architecture Onboarding

- Component map:
  MLLM backbone (LLaVA-1.5, mPLUG-Owl2, or MiniGPT-4) -> Nucleus sampling module -> T2I generation model (Hyper-SDXL) -> Contrastive decoding implementation -> Evaluation framework

- Critical path:
  1. Generate initial caption using MLLM with simple instruction
  2. Generate diverse captions using Nucleus sampling
  3. Generate reconstructed images using T2I model
  4. Compute contrastive logit distributions between original and reconstructed images
  5. Apply contrastive decoding to penalize hallucinated tokens
  6. Evaluate performance across benchmarks

- Design tradeoffs:
  - Computational cost vs. hallucination mitigation effectiveness
  - Number of generated images (n) vs. diversity and coverage
  - T2I model quality vs. generation speed and resource requirements
  - Nucleus sampling temperature vs. caption diversity and coherence

- Failure signatures:
  - CHAIR scores do not improve despite contrastive decoding
  - Generated images do not visually represent hallucinated content
  - Contrastive signals are too weak to effectively penalize hallucinations
  - Computational overhead becomes prohibitive for real-time applications

- First 3 experiments:
  1. Compare CHAIR scores using greedy decoding vs. nucleus sampling for caption generation
  2. Evaluate the impact of different numbers of generated images (n) on hallucination mitigation performance
  3. Test the method with different T2I models (Hyper-SD1.5, SDXL-Turbo, Hyper-SDXL) to assess the relationship between image quality and hallucination reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ConVis performance scale with the quality of the text-to-image (T2I) generation model?
- Basis in paper: [explicit] The authors state "We believe that as more advanced T2I models are developed, the performance of our method will continue to improve" and show that higher CLIPScore T2I models correlate with better CHAIR scores.
- Why unresolved: The paper only tests three T2I models with inference steps fixed at 1, leaving open questions about how performance would change with higher quality models, different inference steps, or newer T2I architectures.
- What evidence would resolve it: Systematic evaluation of ConVis across a wider range of T2I models with varying qualities, inference steps, and architectures, measuring performance on hallucination benchmarks.

### Open Question 2
- Question: Can ConVis be effectively adapted for VQA tasks where the question content may not align with caption-based hallucinations?
- Basis in paper: [explicit] The authors note "This limitation is particularly evident in our experiments with the POPE benchmark, where the performance gain is not as significant as expected" and explain that POPE questions may not align with caption-based hallucinations.
- Why unresolved: The paper identifies this limitation but only suggests "adapting the prompt to respond more specifically to the given question" as a potential solution without testing this approach or exploring alternative methods.
- What evidence would resolve it: Experiments comparing ConVis performance on VQA tasks using adapted prompts, question-aware caption generation, or alternative approaches to hallucination visualization that better align with question content.

### Open Question 3
- Question: What is the optimal number of generated images (n) for balancing hallucination reduction and computational efficiency?
- Basis in paper: [explicit] The authors show that increasing n from 1 to 4 improves CHAIR scores, but they only test up to n=4 and use a fixed value of n=4 in all experiments.
- Why unresolved: The paper demonstrates improvement up to n=4 but doesn't explore whether performance plateaus, whether diminishing returns occur, or what the computational cost implications are for larger n values.
- What evidence would resolve it: Systematic testing of ConVis with varying n values (e.g., 1-16) measuring both hallucination reduction metrics and computational costs to identify optimal trade-offs.

## Limitations

- The method's effectiveness depends on the quality of the text-to-image generation model, with performance improvements tied to CLIPScore values
- The hallucination visualization mechanism may not effectively address VQA tasks where question content doesn't align with caption-based hallucinations
- Computational overhead increases with the number of generated images, creating trade-offs between hallucination reduction and efficiency

## Confidence

**High Confidence**: The overall framework of contrastive decoding for hallucination mitigation is theoretically sound and aligns with established principles in multimodal learning. The method's training-free nature and compatibility with existing MLLMs are well-established.

**Medium Confidence**: The specific implementation details, including the choice of Nucleus sampling for diversity and the use of CLIPScore as a quality metric, show promise but lack comprehensive validation across different scenarios and model configurations.

**Low Confidence**: The hallucination visualization mechanism itself has the weakest support, as it relies on an implicit assumption about T2I model behavior that isn't empirically validated in the paper or supported by the corpus evidence.

## Next Checks

1. **Hallucination Visualization Fidelity Test**: Systematically evaluate whether the T2I model accurately visualizes specific hallucinated tokens by comparing generated images against ground truth scenarios where hallucinations are known to occur. This should include both successful and failed visualization cases to establish reliability bounds.

2. **Diversity vs. Coherence Analysis**: Compare Nucleus sampling against alternative diversity methods (temperature sampling, top-k sampling) while measuring both hallucination mitigation effectiveness and caption semantic coherence. This will determine whether Nucleus sampling provides optimal tradeoffs or if alternative approaches might be superior.

3. **CLIPScore Correlation Study**: Conduct controlled experiments varying T2I model quality while holding other factors constant to isolate the relationship between CLIPScore and hallucination mitigation performance. This should include models with similar CLIPScores but different architectural approaches to determine if CLIPScore is the appropriate quality metric.