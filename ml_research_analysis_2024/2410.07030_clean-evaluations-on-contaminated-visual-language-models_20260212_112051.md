---
ver: rpa2
title: Clean Evaluations on Contaminated Visual Language Models
arxiv_id: '2410.07030'
source_url: https://arxiv.org/abs/2410.07030
tags:
- data
- augmentation
- evaluation
- visual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data contamination in visual
  language model (VLM) evaluation, where benchmark data overlaps with training data,
  artificially inflating performance metrics. The authors propose using data augmentation
  techniques on visual inputs to mitigate contamination effects and introduce a new
  clean evaluation benchmark with 1,000 images and 2,561 question-answer pairs.
---

# Clean Evaluations on Contaminated Visual Language Models

## Quick Facts
- arXiv ID: 2410.07030
- Source URL: https://arxiv.org/abs/2410.07030
- Reference count: 9
- Key outcome: BGR channel swapping serves as an effective clean evaluation method for VLMs that cannot be exploited during training

## Executive Summary
This paper addresses the critical problem of data contamination in visual language model (VLM) evaluation, where benchmark data overlaps with training data, artificially inflating performance metrics. The authors propose using data augmentation techniques on visual inputs to mitigate contamination effects and introduce a new clean evaluation benchmark with 1,000 images and 2,561 question-answer pairs. Their key finding is that BGR channel swapping - changing the color channel order of images - serves as an effective clean evaluation method that cannot be exploited during training. Traditional augmentations like rotations and flips reduce contaminated model performance toward uncontaminated levels, but BGR augmentation shows particularly strong resistance to manipulation while effectively revealing true model capabilities.

## Method Summary
The authors propose using data augmentation on visual inputs to evaluate VLMs under clean conditions when benchmark data may be contaminated with training data. They introduce a new clean evaluation benchmark with 1,000 images and 2,561 QA pairs collected after model releases to avoid contamination. The method involves training baseline and contaminated models, then applying augmentations (rotations, flips, BGR channel swapping) to test samples to reveal true performance. BGR channel swapping, which randomly changes color channel order, was found to be particularly effective as it cannot be exploited during training. The evaluation uses ROUGE-1, ROUGE-2, and BLEU metrics to measure linguistic overlap and semantic similarity between generated and reference answers.

## Key Results
- BGR channel swapping effectively reduces data contamination effects in VLM evaluation
- Traditional augmentations (flips, rotations) help but can be exploited during training
- Contaminated models show significant performance drops under BGR augmentation, revealing true capabilities
- The proposed Gamersky dataset provides a contamination-free benchmark for VLM evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BGR channel swapping cannot be exploited during training as a data augmentation method
- Mechanism: By randomly switching color channels during evaluation, the model cannot memorize specific channel orders since training data would use varied color spaces
- Core assumption: Models trained on mixed color space data would not learn to predict specific channel arrangements
- Evidence anchors:
  - [abstract]: "BGR channel swapping to the visual input, which we fortunately found could not be used as a training technique"
  - [section]: "we discovered that the BGR channel-swapping method exhibited particularly strong resistance to potential manipulation"
  - [corpus]: No direct evidence found in corpus papers about BGR channel swapping as anti-contamination technique

### Mechanism 2
- Claim: Traditional augmentations (flips, rotations) reduce contamination effects but can be exploited during training
- Mechanism: These transformations create variations of training data that overlap with test data, allowing models to memorize augmented patterns
- Core assumption: Data augmentation during training can be reverse-engineered to match evaluation augmentations
- Evidence anchors:
  - [abstract]: "traditional visual data augmentation methods are useful, but they are at risk of being used as a part of the training data as a workaround"
  - [section]: "traditional data augmentation such as flipping and rotation on the image can help with the problem of data augmentation, making the performance closer to the uncontaminated model, they are yet at risk"
  - [corpus]: No direct evidence found in corpus papers about augmentation exploitation

### Mechanism 3
- Claim: Clean evaluation reveals true model capabilities by exposing contamination vulnerabilities
- Mechanism: Contaminated models show performance drops under augmentation because they rely on memorized patterns rather than genuine understanding
- Core assumption: Performance degradation under augmentation indicates memorization rather than true capability
- Evidence anchors:
  - [abstract]: "BGR channel swapping...is a simple yet effective method for reducing the effect of data contamination"
  - [section]: "Using augmentations like rotations, flips, and our proposed BGR channel swaps to the test data helped reveal the true performance of contaminated models"
  - [corpus]: No direct evidence found in corpus papers about clean evaluation methodology

## Foundational Learning

- Concept: Data contamination in ML evaluation
  - Why needed here: Understanding how test data leakage artificially inflates model performance metrics
  - Quick check question: What distinguishes genuine model capability from memorization of evaluation data?

- Concept: Visual data augmentation techniques
  - Why needed here: Different augmentation methods have varying effectiveness for clean evaluation
  - Quick check question: How do different image transformations affect model robustness to contamination?

- Concept: Cross-modal model evaluation
  - Why needed here: VLMs require evaluation methods that account for both visual and language components
  - Quick check question: Why can't traditional text-based clean evaluation methods be directly applied to VLMs?

## Architecture Onboarding

- Component map: VLM (visual encoder + text encoder) → augmentation module → evaluation metrics
- Critical path: Image → augmentation → VLM → prediction → ROUGE/BLEU scoring
- Design tradeoffs: BGR augmentation is harder to exploit but may reduce performance more than traditional methods
- Failure signatures: Performance gaps between original and augmented evaluations indicate contamination
- First 3 experiments:
  1. Train baseline model M1 on uncontaminated data, measure baseline performance
  2. Train contaminated model, measure inflated performance, apply traditional augmentations
  3. Apply BGR augmentation to contaminated model, measure reduction in performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BGR channel swapping consistently reduce contaminated model performance across different VLM architectures and datasets beyond the gaming domain?
- Basis in paper: [explicit] The authors note BGR augmentation is effective for reducing data contamination effects, but only test it on two specific VLM models (bunny-4B and internvl2.0-2B) using a gaming dataset
- Why unresolved: The study's limited scope to gaming images and two specific models prevents generalization to other domains and architectures
- What evidence would resolve it: Testing BGR augmentation on VLMs trained on diverse datasets (medical imaging, natural scenes, etc.) and different architectures (ViT, ConvNeXt, etc.) would establish its broader applicability

### Open Question 2
- Question: Can BGR channel swapping be effectively integrated into VLM training pipelines without losing its clean evaluation benefits?
- Basis in paper: [explicit] The authors state BGR augmentation is "harmful to be used as a data augmentation method during training" and note it cannot be exploited through training data manipulation
- Why unresolved: While the paper demonstrates BGR's resistance to exploitation during training, it doesn't explore whether it could be beneficial if carefully incorporated into training
- What evidence would resolve it: Systematic experiments testing BGR augmentation at different stages of training (pre-training, fine-tuning, adversarial training) would reveal whether it can enhance robustness while maintaining clean evaluation properties

### Open Question 3
- Question: What is the optimal level of BGR augmentation severity for clean evaluation that balances performance reduction with maintaining meaningful evaluation capability?
- Basis in paper: [inferred] The authors show BGR works well for clean evaluation but don't explore parameter tuning or severity levels of the augmentation
- Why unresolved: The paper only applies a binary BGR channel swap without exploring intermediate levels or combinations with other augmentations
- What evidence would resolve it: Testing various BGR augmentation strengths (partial channel mixing, gradient-based intensity) and their correlation with contamination detection sensitivity would identify optimal parameters

### Open Question 4
- Question: How does BGR augmentation compare to other clean evaluation methods for VLMs in terms of effectiveness and implementation complexity?
- Basis in paper: [explicit] The authors compare BGR to traditional augmentations (rotations, flips) but don't benchmark against other clean evaluation techniques like those used for LLMs
- Why unresolved: The paper focuses on demonstrating BGR's effectiveness but lacks comparative analysis with alternative clean evaluation approaches
- What evidence would resolve it: Direct comparison of BGR with methods like output distribution analysis, neural paraphrasing, or adversarial perturbation techniques across multiple contamination scenarios would establish its relative strengths and weaknesses

## Limitations
- The paper lacks theoretical justification for why BGR channel swapping resists training exploitation better than other augmentations
- The mechanism by which BGR swapping prevents contamination exploitation remains inadequately explained
- Limited scope to gaming images and two specific VLM models prevents generalization to other domains and architectures

## Confidence
**High Confidence**: The observation that contaminated models show performance degradation under augmentation is well-supported by the experimental design and results.
**Medium Confidence**: The claim that BGR channel swapping cannot be exploited during training is supported by empirical evidence but lacks theoretical justification.
**Low Confidence**: The assertion that traditional augmentations can be reverse-engineered for contamination exploitation is plausible but not directly evidenced.

## Next Checks
1. **Theoretical Analysis of BGR Invariance**: Conduct a systematic study examining why BGR channel swapping resists training exploitation while other augmentations do not.
2. **Cross-Model Generalization**: Test the BGR augmentation methodology across a broader range of VLM architectures and training datasets.
3. **Controlled Contamination Experiments**: Design experiments that deliberately introduce augmentation-aware training patterns to test whether models can learn to exploit BGR channel ordering.