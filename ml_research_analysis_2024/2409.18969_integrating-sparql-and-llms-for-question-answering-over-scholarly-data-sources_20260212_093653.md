---
ver: rpa2
title: Integrating SPARQL and LLMs for Question Answering over Scholarly Data Sources
arxiv_id: '2409.18969'
source_url: https://arxiv.org/abs/2409.18969
tags:
- data
- question
- answering
- scholarly
- sparql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid approach combining SPARQL queries
  and large language models for question answering over scholarly data sources including
  DBLP, SemOpenAlex, and Wikipedia-based texts. The methodology employs divide-and-conquer
  algorithms to manage diverse question types, using BERT-base-cased-Squad2 for context-specific
  predictions and SPARQL for structured data retrieval.
---

# Integrating SPARQL and LLMs for Question Answering over Scholarly Data Sources
## Quick Facts
- arXiv ID: 2409.18969
- Source URL: https://arxiv.org/abs/2409.18969
- Reference count: 10
- Key outcome: Hybrid approach combining SPARQL queries and large language models improves scholarly question answering accuracy over DBLP, SemOpenAlex, and Wikipedia-based sources

## Executive Summary
This paper presents a hybrid methodology that integrates SPARQL queries with large language model predictions for question answering over scholarly data sources. The approach employs divide-and-conquer algorithms to handle diverse question types, leveraging BERT-base-cased-Squad2 for context-specific predictions alongside structured SPARQL queries for data retrieval. The system was evaluated on 5,000 training questions and 702 test questions, demonstrating improved accuracy when combining both methods compared to using either approach individually. The work provides a robust solution for complex scholarly queries, particularly those involving author information and citation data across multiple knowledge graphs.

## Method Summary
The methodology combines divide-and-conquer algorithms with BERT-base-cased-Squad2 for context-specific predictions and SPARQL queries for structured data retrieval. The approach processes diverse question types by decomposing complex queries into manageable components, routing each through the most appropriate processing pipeline. BERT-base-cased-Squad2 handles contextual understanding and prediction tasks, while SPARQL manages structured data extraction from knowledge graphs. The hybrid system integrates these components through a coordination mechanism that determines when to use LLM predictions versus structured queries, with the combination showing superior performance to either method alone.

## Key Results
- Hybrid approach achieved improved accuracy compared to using SPARQL or LLM predictions individually
- System demonstrated effectiveness on author-specific queries and citation data across DBLP, SemOpenAlex, and Wikipedia
- Evaluation showed promising results using Exact Match and F-score metrics on test dataset of 702 questions

## Why This Works (Mechanism)
The hybrid approach succeeds by leveraging complementary strengths of structured queries and neural models. SPARQL provides precise, deterministic access to structured knowledge graph data, ensuring reliable retrieval of factual information like author affiliations and citation counts. LLMs contribute contextual understanding and flexibility in handling natural language variations, making them effective for interpreting complex scholarly queries. The divide-and-conquer strategy allows the system to decompose difficult questions into components that can be processed by the most appropriate method, with coordination mechanisms determining optimal routing. This combination addresses the limitations of both approaches individually: SPARQL's rigidity with natural language and LLMs' potential for hallucination when dealing with structured data.

## Foundational Learning
- **SPARQL Query Processing**: Essential for extracting structured data from knowledge graphs like DBLP and SemOpenAlex; quick check: verify query syntax and endpoint connectivity
- **BERT-base-cased-Squad2 Architecture**: Provides context-specific predictions for natural language understanding; quick check: validate tokenization and output formatting
- **Divide-and-Conquer Algorithms**: Enables decomposition of complex queries into manageable components; quick check: test with multi-part questions
- **Knowledge Graph Integration**: Combines multiple data sources (DBLP, SemOpenAlex, Wikipedia) into unified framework; quick check: ensure consistent entity resolution across sources
- **Hybrid Coordination Mechanisms**: Determines routing between SPARQL and LLM processing; quick check: validate decision logic with edge cases
- **Evaluation Metrics (Exact Match, F-score)**: Measures system accuracy for scholarly question answering; quick check: confirm metric calculations match paper specifications

## Architecture Onboarding
**Component Map**: User Query -> Divide-and-Conquer Parser -> Decision Module -> (SPARQL Query Engine / BERT-base-cased-Squad2) -> Answer Generator -> Final Response

**Critical Path**: User query enters divide-and-conquer parser, decision module routes to appropriate processing component (SPARQL or LLM), results combine in answer generator, final response produced

**Design Tradeoffs**: BERT-base-cased-Squad2 chosen for balance between performance and computational efficiency versus larger models; smaller test set used for practical evaluation constraints; focus on author/citation queries for domain specificity

**Failure Signatures**: SPARQL failures occur with ambiguous natural language queries; LLM failures manifest as hallucinations on structured data; coordination module failures when routing decisions are suboptimal

**3 First Experiments**:
1. Test with simple author name queries to verify basic SPARQL integration
2. Evaluate complex multi-part questions requiring both components
3. Assess performance degradation when coordination module is disabled

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on relatively small test set of 702 questions compared to 5,000 training examples
- BERT-base-cased-Squad2 may not capture full complexity of scholarly queries compared to larger modern models
- Focus primarily on author-specific and citation queries leaves uncertainty about performance on broader scholarly question types

## Confidence
- **Method Effectiveness**: Medium - promising results but limited comparative analysis with state-of-the-art alternatives
- **Generalization**: Medium - small test set and domain-specific focus raise concerns about broader applicability
- **Scalability**: Medium - computational efficiency demonstrated but not extensively evaluated across different model sizes

## Next Checks
1. Expand evaluation to include larger, more diverse scholarly question sets across multiple domains beyond author and citation queries
2. Conduct ablation studies comparing different LLM sizes and architectures while controlling for training data and computational resources
3. Perform detailed error analysis categorizing failure modes by question type, knowledge graph, and component (SPARQL vs LLM) to identify specific improvement opportunities