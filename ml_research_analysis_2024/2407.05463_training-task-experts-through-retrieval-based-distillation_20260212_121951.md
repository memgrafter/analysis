---
ver: rpa2
title: Training Task Experts through Retrieval Based Distillation
arxiv_id: '2407.05463'
source_url: https://arxiv.org/abs/2407.05463
tags:
- data
- dataset
- rebase
- task
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retrieval Based Distillation (ReBase), a
  method that addresses the challenge of obtaining high-quality task-specific data
  by first retrieving relevant examples from diverse online sources and then transforming
  them into domain-specific data using large language models (LLMs). Unlike previous
  approaches that rely solely on LLM-generated synthetic data, ReBase leverages existing
  datasets to enhance data diversity and reduce repetitive or incorrect outputs.
---

# Training Task Experts through Retrieval Based Distillation

## Quick Facts
- arXiv ID: 2407.05463
- Source URL: https://arxiv.org/abs/2407.05463
- Authors: Jiaxin Ge; Xueying Jia; Vijay Viswanathan; Hongyin Luo; Graham Neubig
- Reference count: 6
- Primary result: ReBase improves performance by up to 7.8% on SQuAD, 1.37% on MNLI, and 1.94% on BIG-Bench Hard compared to traditional synthetic data generation methods

## Executive Summary
This paper introduces Retrieval Based Distillation (ReBase), a method that addresses the challenge of obtaining high-quality task-specific data by first retrieving relevant examples from diverse online sources and then transforming them into domain-specific data using large language models (LLMs). Unlike previous approaches that rely solely on LLM-generated synthetic data, ReBase leverages existing datasets to enhance data diversity and reduce repetitive or incorrect outputs. The method includes a Chain-of-Thought reasoning step to distill complex reasoning capabilities into smaller models. Experiments on four benchmarks—MNLI, SQuAD, MCoNaLa, and BIG-Bench Hard—demonstrate that ReBase significantly improves performance by up to 7.8% on SQuAD, 1.37% on MNLI, and 1.94% on BIG-Bench Hard compared to traditional synthetic data generation methods.

## Method Summary
ReBase is a retrieval-augmented data generation method that constructs a large datastore from Hugging Face datasets, retrieves relevant examples using cosine similarity search, and transforms them into task-specific data using LLMs. The method optionally includes Chain-of-Thought reasoning for complex tasks. A Llama3-8B model is then fine-tuned on the transformed data using QLoRA for efficient adaptation. The approach addresses limitations of pure synthetic generation by leveraging diverse real-world data sources and demonstrates superior performance across multiple benchmarks compared to traditional synthetic data generation methods.

## Key Results
- ReBase improves SQuAD performance by up to 7.8% compared to synthetic methods
- Achieves 1.37% improvement on MNLI and 1.94% on BIG-Bench Hard benchmarks
- Increases dataset diversity from 25.9% to 75.4% unique samples on MCoNaLa
- Demonstrates cost-effectiveness, with ReBase using Claude 3 Haiku outperforming GPT-4 without ReBase on MCoNaLa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-based data generation increases task-specific dataset diversity compared to purely synthetic generation.
- Mechanism: ReBase retrieves relevant data points from a large heterogeneous datastore containing over 75,000 datasets, then transforms them into task-specific format using LLM. This multi-source retrieval approach exposes the model to broader content variations.
- Core assumption: Retrieved examples contain diverse and relevant information that synthetic generation from a few in-context examples cannot replicate.
- Evidence anchors:
  - [abstract] "This method greatly enhances data diversity" and improves performance by up to 7.8% on SQuAD.
  - [section 4.3] "ReBase significantly increases the percentage of unique samples" from 25.9% to 75.4% on