---
ver: rpa2
title: Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification
arxiv_id: '2401.03097'
source_url: https://arxiv.org/abs/2401.03097
tags:
- fairness
- accuracy
- classification
- loss
- adaboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fair AdaBoost (FAB) method to address unfairness
  in machine learning classification. FAB improves classification fairness by introducing
  a fairness-aware reweighting technique for base classifiers.
---

# Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification

## Quick Facts
- arXiv ID: 2401.03097
- Source URL: https://arxiv.org/abs/2401.03097
- Reference count: 10
- Primary result: Proposes FAB method that reduces fairness loss by 83.5-88.4% with minimal accuracy reduction compared to AdaBoost

## Executive Summary
This paper introduces Fair AdaBoost (FAB), a method that enhances classification fairness by incorporating a fairness-aware reweighting technique into the AdaBoost framework. FAB addresses unfairness in machine learning by considering three fairness indicators: accuracy, false positive rate, and false negative rate. The method introduces a hyperparameter to balance the trade-off between accuracy and fairness, making it adaptable to different fairness requirements.

Experimental results on three real-world datasets (Adult, COMPAS, and HSLS) demonstrate that FAB significantly improves classification fairness with only a slight reduction in accuracy compared to the original AdaBoost algorithm. FAB outperforms state-of-the-art fair classification methods in terms of the fairness-accuracy trade-off, achieving 83.5-88.4% reduction in fairness loss while maintaining high classification accuracy.

## Method Summary
FAB enhances the AdaBoost algorithm by introducing a fairness-aware reweighting technique for base classifiers. The method considers three fairness indicators: accuracy, false positive rate (FPR), and false negative rate (FNR). FAB incorporates a hyperparameter that allows users to balance the trade-off between accuracy and fairness according to their specific requirements. The algorithm theoretically derives an upper bound for the target loss function, providing a foundation for the method's effectiveness. During training, FAB adjusts the weights of training samples based on both their classification performance and their contribution to fairness metrics, ensuring that subsequent weak learners focus on samples that help improve fairness.

## Key Results
- FAB reduces fairness loss by 83.5% to 88.4% across three real-world datasets
- Achieves only a slight reduction in classification accuracy compared to original AdaBoost
- Outperforms state-of-the-art fair classification methods in fairness-accuracy trade-off
- Validated on Adult, COMPAS, and HSLS datasets with consistent improvements

## Why This Works (Mechanism)
FAB works by modifying the AdaBoost reweighting scheme to incorporate fairness considerations. When a base classifier misclassifies a sample, instead of simply increasing its weight based on classification error, FAB adjusts the weight based on how the misclassification affects fairness metrics (FPR, FNR, and overall accuracy). This creates a feedback loop where subsequent weak learners are incentivized to correct not just classification errors, but specifically those errors that contribute to unfairness. The hyperparameter allows fine-tuning of this trade-off, enabling the model to prioritize either fairness or accuracy depending on the application context.

## Foundational Learning
- AdaBoost algorithm: The original boosting method that combines weak learners sequentially - needed to understand FAB's baseline and modifications
- Fairness metrics (FPR, FNR): Measures of classification equity across different groups - needed to quantify and optimize for fairness
- Reweighting techniques: Methods for adjusting sample importance during training - needed to understand how FAB directs learning
- Upper bound theory: Mathematical proofs establishing performance guarantees - needed to validate FAB's theoretical foundation
- Trade-off optimization: Balancing competing objectives in machine learning - needed to understand the hyperparameter's role

## Architecture Onboarding

**Component Map:** Data -> Base Classifier Training -> Fairness-aware Reweighting -> Combined Classifier -> Output

**Critical Path:** Training samples are weighted based on both classification error and fairness impact. Each weak learner is trained on these weighted samples, then contributes to the final ensemble. The fairness-aware reweighting occurs at each boosting iteration, directly influencing which samples subsequent learners focus on.

**Design Tradeoffs:** FAB trades some accuracy for fairness improvement, controlled by a hyperparameter. This allows flexibility but requires careful tuning. The method adds computational overhead compared to standard AdaBoost due to fairness metric calculations at each iteration.

**Failure Signatures:** If the hyperparameter is set too high for fairness, accuracy may degrade significantly. If set too low, fairness improvements may be minimal. The method may not perform well if fairness metrics are not well-defined for the specific problem domain.

**3 First Experiments:** 1) Run FAB with default hyperparameter on a simple binary classification dataset to observe basic behavior, 2) Test FAB on a dataset with known fairness issues to verify improvement in FPR and FNR, 3) Compare FAB's accuracy-fairness trade-off against standard AdaBoost on the same dataset.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results validated on only three datasets, limiting generalizability
- Theoretical upper bound lacks independent verification and detailed sensitivity analysis
- Limited to three fairness metrics (accuracy, FPR, FNR), not testing broader fairness measures
- Optimal hyperparameter settings not fully characterized across different problem domains

## Confidence
- Experimental results: Medium-High confidence due to clear quantitative improvements across multiple datasets
- Theoretical contributions: Medium confidence due to lack of independent verification and detailed sensitivity analysis
- Generalizability claims: Medium confidence due to limited dataset diversity and scope of fairness metrics tested

## Next Checks
1) Test FAB on additional fairness metrics such as demographic parity and equalized odds to verify broader applicability
2) Conduct ablation studies to isolate the contribution of the fairness-aware reweighting technique versus other components of the algorithm
3) Perform sensitivity analysis on the fairness-accuracy trade-off hyperparameter to determine optimal settings across different dataset characteristics