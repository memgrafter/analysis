---
ver: rpa2
title: 'VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values'
arxiv_id: '2407.03000'
source_url: https://arxiv.org/abs/2407.03000
tags:
- action
- values
- human
- situation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VIVA is a new benchmark for vision-grounded decision-making driven
  by human values, designed to evaluate how well vision language models (VLMs) can
  make decisions in real-world situations by incorporating human values. The benchmark
  includes 1,240 images depicting diverse scenarios, each annotated with possible
  actions, relevant human values, and reasons for decisions.
---

# VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values

## Quick Facts
- **arXiv ID**: 2407.03000
- **Source URL**: https://arxiv.org/abs/2407.03000
- **Reference count**: 24
- **Key outcome**: VIVA is a new benchmark for vision-grounded decision-making driven by human values, designed to evaluate how well vision language models (VLMs) can make decisions in real-world situations by incorporating human values.

## Executive Summary
VIVA is a new benchmark for vision-grounded decision-making driven by human values, designed to evaluate how well vision language models (VLMs) can make decisions in real-world situations by incorporating human values. The benchmark includes 1,240 images depicting diverse scenarios, each annotated with possible actions, relevant human values, and reasons for decisions. The task involves two levels: selecting the most appropriate action and inferring the underlying values and reasons for the choice. Experiments show that even advanced models like GPT4-V struggle with these tasks, achieving 74.9% accuracy in combined action selection and value inference. The study also demonstrates that incorporating predicted action consequences and human values can improve decision-making performance, highlighting the need for better alignment of VLMs with human principles for reliable and socially responsible AI.

## Method Summary
VIVA evaluates VLMs on vision-grounded decision-making through two levels: action selection and value inference. The benchmark uses 1,240 images with annotated actions, values, and reasons. Models are evaluated zero-shot on selecting appropriate actions and inferring underlying human values and reasons. A consequence prediction module trained on GPT4-generated data is used to predict outcomes of actions, which improves decision-making performance. The evaluation measures accuracy for action selection, value inference, and combined action-value tasks, along with semantic explanation scores for reason generation.

## Key Results
- GPT4-V achieves 74.9% accuracy on combined action selection and value inference tasks
- Incorporating predicted action consequences improves decision-making performance across all models
- Oracle values significantly enhance action selection performance, with GPT4-V-generated values being most effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT4-V performs best because it has the strongest multimodal reasoning capabilities.
- Mechanism: GPT4-V integrates visual perception with deep language reasoning, allowing it to infer human values from complex real-world images.
- Core assumption: Visual encoder and language model are tightly integrated, enabling cross-modal alignment of actions and values.
- Evidence anchors:
  - [abstract] "even advanced models like GPT4-V struggle with these tasks, achieving 74.9% accuracy"
  - [section 5.1] "GPT4-V shows superiority in action selection and value inference"
  - [corpus] Found related paper VIVA+ that extends this line of work; supports this claim.
- Break condition: If the visual encoder fails to capture fine-grained scene details, reasoning based on those details will be unreliable.

### Mechanism 2
- Claim: Predicting action consequences improves decision-making.
- Mechanism: By simulating outcomes before choosing an action, models mimic human deliberation and avoid poor choices.
- Core assumption: Outcome prediction provides additional context that constrains the action selection process.
- Evidence anchors:
  - [section 5.2] "incorporating either action consequences or predicted human values can improve decision-making performance"
  - [section 5.2] "incorporating the predictions improves the performance of all models"
  - [corpus] VS-Bench evaluates VLMs for strategic abilities; outcome prediction is a form of strategy.
- Break condition: If consequence prediction is inaccurate, it can mislead the model into selecting worse actions.

### Mechanism 3
- Claim: Explicit values guide more accurate action selection.
- Mechanism: Providing human values narrows the decision space to options that align with social norms.
- Core assumption: Human values are strong priors for socially acceptable behavior.
- Evidence anchors:
  - [section 5.3] "augments with oracle values significantly enhances the performance of all models"
  - [section 5.3] "GPT4-V-generated values leads to more accurate action selection"
  - [corpus] VIVA+ builds on this idea with human-centered situational decision-making.
- Break condition: If the provided values are irrelevant or contradictory, they can confuse the model and degrade performance.

## Foundational Learning

- Concept: Multimodal integration
  - Why needed here: VIVA requires both visual understanding and language reasoning to evaluate actions and values.
  - Quick check question: Can the model correctly link an image of a person drowning to the value "Duty to help"?

- Concept: Value inference as classification
  - Why needed here: Level-2 tasks frame value detection as binary classification to allow measurable evaluation.
  - Quick check question: Does the model correctly classify "Promotion of personal safety" as positive for the flotation device scenario?

- Concept: Consequence prediction
  - Why needed here: Predicting outcomes before choosing actions mirrors human deliberation and improves accuracy.
  - Quick check question: Does the model predict that offering food to a fallen elderly person could cause harm?

## Architecture Onboarding

- Component map: Image → Visual encoder → LLM context → Action selection → Value inference → Reason generation
- Critical path: Image → Vision encoder → LLM context → Action selection → Value inference → Reason generation
- Design tradeoffs:
  - Larger models give better reasoning but are slower and costlier.
  - Adding consequence prediction increases accuracy but requires extra inference steps.
  - Using oracle values improves performance but isn't available in real deployments.
- Failure signatures:
  - Misrecognizing situations → incorrect actions
  - Misaligned value associations → socially inappropriate actions
  - Overconfidence in predictions → poor generalization
- First 3 experiments:
  1. Measure accuracy drop when removing consequence prediction.
  2. Test impact of oracle vs. predicted values on action selection.
  3. Evaluate robustness across distraction scenarios (normal vs. emergency).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VLMs on vision-grounded decision-making tasks vary across different cultural contexts, and what are the implications for developing culturally sensitive AI systems?
- Basis in paper: [inferred] The paper acknowledges the potential for cultural differences in values and their influence on decision-making, but does not extensively explore this aspect.
- Why unresolved: The paper focuses on universal human values and does not account for in-group variation of values or cultural nuances in decision-making.
- What evidence would resolve it: Conducting experiments with VLMs on vision-grounded decision-making tasks using culturally diverse datasets and evaluating their performance across different cultural contexts would provide insights into the need for culturally sensitive AI systems.

### Open Question 2
- Question: How can the consequence prediction module be improved to better anticipate the outcomes of actions in vision-grounded decision-making tasks, especially for smaller models?
- Basis in paper: [explicit] The paper discusses the use of a consequence prediction module to improve model decision-making, but notes that smaller models often struggle to accurately predict action consequences.
- Why unresolved: The paper suggests that the effectiveness of the consequence prediction module is contingent upon the model's ability to recognize situational nuances from the input image, which remains a challenge for current VLMs.
- What evidence would resolve it: Developing and testing more advanced consequence prediction methods, such as incorporating attention mechanisms or leveraging external knowledge bases, could enhance the ability of VLMs to anticipate action outcomes and make better decisions.

### Open Question 3
- Question: What are the specific challenges faced by VLMs in associating situations with relevant human values, and how can these challenges be addressed to improve value-driven decision-making?
- Basis in paper: [explicit] The paper highlights that current open-source VLMs struggle to associate situations with relevant human values, as evidenced by their inferior performance in Level-2 value inference tasks.
- Why unresolved: The paper suggests that future research is needed to enhance VLMs' alignment with human values for improved decision-making, but does not provide specific solutions to address this challenge.
- What evidence would resolve it: Investigating the factors that contribute to VLMs' difficulties in value association, such as limitations in their training data or model architectures, and developing targeted approaches to address these issues would provide insights into improving value-driven decision-making.

## Limitations

- Reliance on GPT4-generated data for consequence prediction and value labeling introduces potential bias and limits training example diversity
- Zero-shot evaluation may not capture full performance potential when models are fine-tuned on similar tasks
- 74.9% accuracy for combined tasks doesn't establish clear performance ceilings for current VLMs
- Oracle values in experiments create evaluation gaps between idealized conditions and real-world deployment

## Confidence

- **High confidence**: GPT4-V performs best due to strongest multimodal reasoning capabilities
- **Medium confidence**: Predicting action consequences improves decision-making (depends on quality of consequence prediction module)
- **Medium confidence**: Explicit values guide more accurate action selection (may not generalize when models must infer values independently)

## Next Checks

1. Validate consequence prediction generalization: Test the consequence prediction module on held-out scenarios and scenarios with adversarial perturbations to assess robustness and identify failure modes where predictions mislead action selection.

2. Compare oracle vs. predicted values in real deployment: Evaluate action selection performance using predicted values alone (without oracle augmentation) across all model types to establish realistic performance bounds for real-world applications.

3. Analyze value inference accuracy by category: Break down value inference accuracy across different value categories (e.g., safety, duty, social norms) to identify which types of values models struggle with most, and test whether providing context-specific value examples improves performance.