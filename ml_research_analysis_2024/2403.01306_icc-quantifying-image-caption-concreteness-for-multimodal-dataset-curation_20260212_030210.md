---
ver: rpa2
title: 'ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation'
arxiv_id: '2403.01306'
source_url: https://arxiv.org/abs/2403.01306
tags:
- caption
- captions
- concreteness
- scores
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces ICC (Image Caption Concreteness), a metric
  for quantifying the visual concreteness of image captions without requiring the
  image itself. The core idea is to measure visual-semantic information loss using
  multimodal autoencoders: a Visual-Bottleneck Autoencoder (VBA) using text-to-image
  models and a Semantic-Bottleneck Autoencoder (SBA) using CLIP embeddings with a
  language model.'
---

# ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation

## Quick Facts
- arXiv ID: 2403.01306
- Source URL: https://arxiv.org/abs/2403.01306
- Reference count: 35
- Key outcome: ICC metric achieves up to 35.4 CIDEr on MS-COCO image captioning when filtering datasets, outperforming CLIPScore and other methods

## Executive Summary
ICC (Image Caption Concreteness) introduces a novel metric for quantifying the visual concreteness of image captions without requiring the image itself. The method leverages multimodal autoencoders to measure visual-semantic information loss, capturing how well captions can be reconstructed through visual representations. Due to computational constraints of the full model, ICC is distilled into a lightweight model that maintains strong correlation with human judgments while enabling efficient inference. The metric demonstrates superior performance in dataset curation for vision-and-language tasks, particularly in resource-constrained training scenarios.

## Method Summary
ICC quantifies visual concreteness by measuring reconstruction fidelity through two multimodal autoencoders: a Visual-Bottleneck Autoencoder (VBA) using text-to-image models and a Semantic-Bottleneck Autoencoder (SBA) using CLIP embeddings with a language model. These pipelines measure information loss when captions pass through visual-semantic bottlenecks, with concrete captions reconstructing more faithfully than abstract ones. Due to computational costs, the reconstruction scores from VBA and SBA are distilled into a smaller DistilRoBERTa model, enabling fast, scalable inference while preserving accuracy. ICC strongly correlates with human evaluations and significantly outperforms existing filtering methods in dataset curation tasks.

## Key Results
- ICC achieves up to 35.4 CIDEr score on MS-COCO image captioning when filtering training data
- Outperforms CLIPScore, Complexity and Action, T-MARS, and PACScore in both captioning and retrieval tasks
- Demonstrates strong correlation with human evaluations (r=0.95) while enabling efficient inference through distillation
- Particularly effective in resource-constrained settings with limited training iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICC scores capture visual concreteness by measuring reconstruction fidelity through multimodal autoencoders.
- Mechanism: Visual-Bottleneck Autoencoder (VBA) and Semantic-Bottleneck Autoencoder (SBA) encode captions through visual-semantic bottlenecks and measure information loss during reconstruction. Concrete captions reconstruct more faithfully than abstract ones.
- Core assumption: Visual concreteness corresponds to the ability to reconstruct the original text with minimal information loss when passing through visual-semantic representations.
- Evidence anchors:
  - [abstract] "Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations."
  - [section 2] "We model this effect with multimodal autoencoders... In our setting, we use multiple autoencoder components that convert text to and from visual-semantic representations using foundation VLMs, and quantify the information loss of this process as a proxy for visual concreteness."
  - [corpus] Weak evidence - only 25 related papers found with average FMR=0.5, suggesting limited direct research on visual-semantic information loss as a concreteness metric.

### Mechanism 2
- Claim: ICC distillation enables efficient inference by compressing computationally expensive model outputs into a small, fast model.
- Mechanism: SBA and VBA reconstruction scores are collected over a small dataset and used to train a lightweight DistilRoBERTa model to predict combined scores directly from text, enabling rapid inference.
- Core assumption: The relationship between reconstruction scores and concreteness can be learned by a small model, preserving accuracy while enabling practical deployment.
- Evidence anchors:
  - [abstract] "As these models require costly inference through large generative models, they cannot feasibly run on a large scale; therefore, our ICC metric is distilled from these pipelines, enabling fast, computationally-efficient inference."
  - [section 2] "Using the aforementioned pipelines to quantify the concreteness at scale is not feasible... Therefore, we assemble SBA and VBA reconstruction scores over a relatively small collection of image-caption pairs and distill their aggregated values into our final ICC score."
  - [corpus] No direct evidence found in related papers about distillation techniques for multimodal concreteness metrics.

### Mechanism 3
- Claim: ICC filtering improves downstream task performance by selecting visually concrete samples that provide stronger learning signals.
- Mechanism: Filtering datasets using ICC retains samples with high visual concreteness, which provide clearer supervision signals for vision-and-language tasks, particularly when training resources are limited.
- Core assumption: Visually concrete captions provide stronger supervision signals than abstract or subjective captions for learning vision-and-language tasks.
- Evidence anchors:
  - [abstract] "Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality image-caption samples from web-scale multimodal datasets to allow for efficient training in resource-constrained settings."
  - [section 3.1] "Our results demonstrate that ICC is effective at selecting a core of high-quality image-caption samples from web-scale multimodal datasets for training models in the resource-constrained setting."
  - [corpus] No direct evidence in related papers about the impact of visual concreteness filtering on downstream task performance.

## Foundational Learning

- Concept: Visual-semantic information loss
  - Why needed here: Understanding how ICC measures visual concreteness requires grasping that it quantifies information loss when text passes through visual-semantic bottlenecks.
  - Quick check question: What happens to abstract captions when passed through a visual bottleneck compared to concrete captions?

- Concept: Autoencoder architectures
  - Why needed here: ICC relies on two types of autoencoders (VBA and SBA) that reconstruct text through visual-semantic representations.
  - Quick check question: How do VBA and SBA differ in their approach to measuring visual-semantic information loss?

- Concept: Model distillation
  - Why needed here: The computationally expensive SBA and VBA models are distilled into a small, efficient model for practical deployment.
  - Quick check question: Why is distillation necessary for ICC, and what are the key components being distilled?

## Architecture Onboarding

- Component map: Text → SBA/VBA pipelines → Reconstruction scores → Distilled ICC model → Final score
- Critical path: Text → SBA/VBA pipelines → Reconstruction scores → Distilled ICC model → Final score
- Design tradeoffs:
  - Computational efficiency vs. accuracy: Using distillation enables fast inference but may lose some nuance from the full models
  - Information bottleneck strength: VBA uses image as bottleneck (stronger) while SBA uses CLIP embeddings (weaker but more semantic)
  - Reconstruction metric choice: Edit distance for SBA (fine-grained) vs. BERTScore for VBA (semantic)

- Failure signatures:
  - Low ICC scores for captions that humans judge as concrete (false negatives)
  - High ICC scores for captions that humans judge as abstract (false positives)
  - Inconsistent performance across different dataset domains
  - Performance degradation when training data distribution differs from CC3M

- First 3 experiments:
  1. Run ICC on a small set of manually labeled captions to verify correlation with human judgments
  2. Compare ICC scores with and without distillation on a validation set to measure accuracy loss
  3. Test ICC filtering on a small dataset and measure downstream task performance compared to random filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ICC generalize well to non-English image captions and cross-lingual settings?
- Basis in paper: [inferred] The paper evaluates ICC on English captions and demonstrates strong correlation with human judgments for English texts, but does not test performance on multilingual or cross-lingual datasets.
- Why unresolved: The current evaluation is limited to English-language benchmarks (MS-COCO, NoCaps, LAION-400M) without testing on non-English captions or comparing performance across languages.
- What evidence would resolve it: Evaluating ICC on multilingual datasets (e.g., Multi30K, cross-lingual caption datasets) and measuring correlation with human judgments across different languages would determine its cross-lingual generalization.

### Open Question 2
- Question: How does ICC perform when applied to video captioning tasks, where temporal context is important?
- Basis in paper: [inferred] The paper focuses exclusively on image captioning tasks, and while the metric is designed for text evaluation without image reference, it does not explore temporal aspects of video data.
- Why unresolved: Video captioning involves dynamic scenes and temporal relationships that are not present in static images, which could affect how ICC's visual-semantic consistency measurements apply to video data.
- What evidence would resolve it: Testing ICC on video captioning datasets (e.g., MSR-VTT, ActivityNet Captions) and comparing its effectiveness in filtering video captions versus image captions would reveal its applicability to temporal data.

### Open Question 3
- Question: What is the optimal balance between SBA and VBA components for different downstream tasks (e.g., VQA vs. image captioning)?
- Basis in paper: [explicit] The paper shows that combining SBA and VBA scores works well for image captioning and retrieval, but does not explore task-specific weighting or whether different tasks benefit from different ratios of these components.
- Why unresolved: Different vision-and-language tasks may require different types of visual concreteness - image captioning may prioritize detailed visual descriptions while VQA might benefit more from conceptual understanding, suggesting task-specific optimization could improve performance.
- What evidence would resolve it: Conducting ablation studies where SBA and VBA weights are optimized separately for different downstream tasks (captioning, VQA, retrieval) would reveal whether task-specific configurations outperform the current fixed combination.

## Limitations

- Limited evaluation on non-English captions and cross-lingual settings
- Reliance on GPT-3.5 and GPT-4o for human evaluations without full prompt specification
- Computational cost of baseline models not empirically compared during actual filtering operations
- Fundamental assumption that reconstruction fidelity directly corresponds to visual concreteness lacks causal validation

## Confidence

- **High Confidence**: ICC's strong correlation with human evaluations of caption concreteness (r=0.95 with human annotations) is well-supported by the experimental results. The distillation approach enabling efficient inference is technically sound.
- **Medium Confidence**: ICC's superiority over existing filtering methods (CLIPScore, Complexity and Action, T-MARS, PACScore) is demonstrated on specific datasets and tasks, but the generalizability across different domains and tasks requires further validation.
- **Low Confidence**: The claim that ICC "complements existing approaches" for web-scale dataset curation needs more rigorous ablation studies. The paper shows performance improvements but doesn't definitively prove that ICC captures complementary information beyond what existing methods provide.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate ICC filtering performance on diverse datasets beyond COCO and Flickr30k (e.g., medical imaging captions, historical document descriptions, multilingual datasets) to assess domain robustness.

2. **Runtime Efficiency Benchmark**: Conduct head-to-head runtime comparisons between ICC and baseline filtering methods (CLIPScore, T-MARS, etc.) on identical hardware, measuring wall-clock time for filtering datasets of varying sizes (10K, 100K, 1M samples).

3. **Ablation Study on Distillation**: Compare ICC performance with and without distillation on a held-out validation set to quantify the accuracy-efficiency tradeoff. Additionally, test whether SBA and VBA scores capture truly complementary information by evaluating ICC performance when using only one of the two components.